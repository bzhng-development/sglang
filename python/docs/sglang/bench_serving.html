<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.bench_serving API documentation</title>
<meta name="description" content="Benchmark online serving with dynamic requests …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.bench_serving</code></h1>
</header>
<section id="section-intro">
<p>Benchmark online serving with dynamic requests.</p>
<p>Usage:
python3 -m sglang.bench_serving &ndash;backend sglang &ndash;num-prompt 10</p>
<p>python3 -m sglang.bench_serving &ndash;backend sglang &ndash;dataset-name random &ndash;num-prompts 3000 &ndash;random-input 1024 &ndash;random-output 1024 &ndash;random-range-ratio 0.5</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.bench_serving.async_request_gserver"><code class="name flex">
<span>async def <span class="ident">async_request_gserver</span></span>(<span>request_func_input: <a title="sglang.bench_serving.RequestFuncInput" href="#sglang.bench_serving.RequestFuncInput">RequestFuncInput</a>,<br>pbar: tqdm.asyncio.tqdm_asyncio | None = None) ‑> <a title="sglang.bench_serving.RequestFuncOutput" href="#sglang.bench_serving.RequestFuncOutput">RequestFuncOutput</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def async_request_gserver(
    request_func_input: RequestFuncInput,
    pbar: Optional[tqdm] = None,
) -&gt; RequestFuncOutput:
    raise NotImplementedError()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.async_request_openai_chat_completions"><code class="name flex">
<span>async def <span class="ident">async_request_openai_chat_completions</span></span>(<span>request_func_input: <a title="sglang.bench_serving.RequestFuncInput" href="#sglang.bench_serving.RequestFuncInput">RequestFuncInput</a>,<br>pbar: tqdm.asyncio.tqdm_asyncio | None = None) ‑> <a title="sglang.bench_serving.RequestFuncOutput" href="#sglang.bench_serving.RequestFuncOutput">RequestFuncOutput</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def async_request_openai_chat_completions(
    request_func_input: RequestFuncInput,
    pbar: Optional[tqdm] = None,
) -&gt; RequestFuncOutput:
    &#34;&#34;&#34;Makes a request to the OpenAI Chat Completions API.

    Handles both streaming and non-streaming responses, including support
    for image data in messages. Calculates and returns various performance
    metrics.

    Args:
        request_func_input: Input parameters for the request.
        pbar: Optional tqdm progress bar to update.

    Returns:
        RequestFuncOutput: Output of the request, including generated text,
                           latency, TTFT, ITL, and success status.
    &#34;&#34;&#34;
    api_url = request_func_input.api_url
    assert api_url.endswith(
        &#34;chat/completions&#34;
    ), &#34;OpenAI Chat Completions API URL must end with &#39;chat/completions&#39;.&#34;

    if request_func_input.image_data:
        # Build multi-image content: a list of image_url entries followed by the text
        content_items = [
            {
                &#34;type&#34;: &#34;image_url&#34;,
                &#34;image_url&#34;: {&#34;url&#34;: img_url},
            }
            for img_url in request_func_input.image_data
        ]
        content_items.append({&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: request_func_input.prompt})
        messages = [
            {
                &#34;role&#34;: &#34;user&#34;,
                &#34;content&#34;: content_items,
            },
        ]
    else:
        messages = [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: request_func_input.prompt}]

    async with _create_bench_client_session() as session:
        payload = {
            &#34;model&#34;: request_func_input.model,
            &#34;messages&#34;: messages,
            &#34;temperature&#34;: 0.0,
            &#34;max_tokens&#34;: request_func_input.output_len,
            &#34;stream&#34;: not args.disable_stream,
            **request_func_input.extra_request_body,
        }
        headers = get_auth_headers()

        output = RequestFuncOutput.init_new(request_func_input)

        generated_text = &#34;&#34;
        output_len = request_func_input.output_len
        ttft = 0.0
        st = time.perf_counter()
        most_recent_timestamp = st
        try:
            async with session.post(
                url=api_url, json=payload, headers=headers
            ) as response:
                if response.status == 200:
                    if args.disable_stream:
                        # Non-streaming response
                        response_json = await response.json()
                        output.generated_text = response_json[&#34;choices&#34;][0][&#34;message&#34;][
                            &#34;content&#34;
                        ]
                        output.success = True
                        output.latency = time.perf_counter() - st
                        output.ttft = (
                            output.latency
                        )  # For non-streaming, TTFT = total latency
                        output.output_len = response_json.get(&#34;usage&#34;, {}).get(
                            &#34;completion_tokens&#34;, output_len
                        )
                    else:
                        # Streaming response
                        async for chunk_bytes in response.content:
                            chunk_bytes = chunk_bytes.strip()
                            if not chunk_bytes:
                                continue

                            chunk = remove_prefix(chunk_bytes.decode(&#34;utf-8&#34;), &#34;data: &#34;)
                            latency = time.perf_counter() - st
                            if chunk == &#34;[DONE]&#34;:
                                pass
                            else:
                                data = json.loads(chunk)

                                # Check if this chunk contains content
                                delta = data.get(&#34;choices&#34;, [{}])[0].get(&#34;delta&#34;, {})
                                content = delta.get(&#34;content&#34;, &#34;&#34;)

                                if content:
                                    timestamp = time.perf_counter()
                                    # First token
                                    if ttft == 0.0:
                                        ttft = timestamp - st
                                        output.ttft = ttft

                                    # Decoding phase
                                    else:
                                        output.itl.append(
                                            timestamp - most_recent_timestamp
                                        )

                                    most_recent_timestamp = timestamp
                                    generated_text += content

                                # Check for usage info in final chunk
                                output_len = (data.get(&#34;usage&#34;) or {}).get(
                                    &#34;completion_tokens&#34;, output_len
                                )

                        output.generated_text = generated_text
                        output.success = True
                        output.latency = latency
                        output.output_len = output_len
                else:
                    output.error = response.reason or &#34;&#34;
                    output.success = False
        except Exception:
            output.success = False
            exc_info = sys.exc_info()
            output.error = &#34;&#34;.join(traceback.format_exception(*exc_info))

    if pbar:
        pbar.update(1)
    return output</code></pre>
</details>
<div class="desc"><p>Makes a request to the OpenAI Chat Completions API.</p>
<p>Handles both streaming and non-streaming responses, including support
for image data in messages. Calculates and returns various performance
metrics.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>request_func_input</code></strong></dt>
<dd>Input parameters for the request.</dd>
<dt><strong><code>pbar</code></strong></dt>
<dd>Optional tqdm progress bar to update.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="sglang.bench_serving.RequestFuncOutput" href="#sglang.bench_serving.RequestFuncOutput">RequestFuncOutput</a></code></dt>
<dd>Output of the request, including generated text,
latency, TTFT, ITL, and success status.</dd>
</dl></div>
</dd>
<dt id="sglang.bench_serving.async_request_openai_completions"><code class="name flex">
<span>async def <span class="ident">async_request_openai_completions</span></span>(<span>request_func_input: <a title="sglang.bench_serving.RequestFuncInput" href="#sglang.bench_serving.RequestFuncInput">RequestFuncInput</a>,<br>pbar: tqdm.asyncio.tqdm_asyncio | None = None) ‑> <a title="sglang.bench_serving.RequestFuncOutput" href="#sglang.bench_serving.RequestFuncOutput">RequestFuncOutput</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def async_request_openai_completions(
    request_func_input: RequestFuncInput,
    pbar: Optional[tqdm] = None,
) -&gt; RequestFuncOutput:
    api_url = request_func_input.api_url
    assert api_url.endswith(
        &#34;completions&#34;
    ), &#34;OpenAI Completions API URL must end with &#39;completions&#39;.&#34;

    prompt = request_func_input.prompt

    async with _create_bench_client_session() as session:
        payload = {
            &#34;model&#34;: request_func_input.model,
            &#34;prompt&#34;: prompt,
            &#34;temperature&#34;: 0.0,
            &#34;best_of&#34;: 1,
            &#34;max_tokens&#34;: request_func_input.output_len,
            &#34;stream&#34;: not args.disable_stream,
            &#34;ignore_eos&#34;: not args.disable_ignore_eos,
            **request_func_input.extra_request_body,
        }
        headers = get_auth_headers()

        output = RequestFuncOutput.init_new(request_func_input)

        generated_text = &#34;&#34;
        output_len = request_func_input.output_len
        ttft = 0.0
        st = time.perf_counter()
        most_recent_timestamp = st
        try:
            async with session.post(
                url=api_url, json=payload, headers=headers
            ) as response:
                if response.status == 200:
                    async for chunk_bytes in response.content:
                        chunk_bytes = chunk_bytes.strip()
                        if not chunk_bytes:
                            continue

                        chunk = remove_prefix(chunk_bytes.decode(&#34;utf-8&#34;), &#34;data: &#34;)
                        latency = time.perf_counter() - st
                        if chunk == &#34;[DONE]&#34;:
                            pass
                        else:
                            data = json.loads(chunk)

                            # NOTE: Some completion API might have a last
                            # usage summary response without a token so we
                            # want to check a token was generated
                            if data[&#34;choices&#34;][0][&#34;text&#34;]:
                                timestamp = time.perf_counter()
                                # First token
                                if ttft == 0.0:
                                    ttft = time.perf_counter() - st
                                    output.ttft = ttft

                                # Decoding phase
                                else:
                                    output.itl.append(timestamp - most_recent_timestamp)

                                most_recent_timestamp = timestamp
                                generated_text += data[&#34;choices&#34;][0][&#34;text&#34;]
                                output_len = (data.get(&#34;usage&#34;) or {}).get(
                                    &#34;completion_tokens&#34;, output_len
                                )

                    output.generated_text = generated_text
                    output.success = True
                    output.latency = latency
                    output.output_len = output_len
                else:
                    output.error = response.reason or &#34;&#34;
                    output.success = False
        except Exception:
            output.success = False
            exc_info = sys.exc_info()
            output.error = &#34;&#34;.join(traceback.format_exception(*exc_info))

    if pbar:
        pbar.update(1)
    return output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.async_request_profile"><code class="name flex">
<span>async def <span class="ident">async_request_profile</span></span>(<span>api_url: str) ‑> <a title="sglang.bench_serving.RequestFuncOutput" href="#sglang.bench_serving.RequestFuncOutput">RequestFuncOutput</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def async_request_profile(api_url: str) -&gt; RequestFuncOutput:
    async with _create_bench_client_session() as session:
        output = RequestFuncOutput()
        try:
            async with session.post(url=api_url) as response:
                if response.status == 200:
                    output.success = True
                else:
                    output.error = response.reason or &#34;&#34;
                    output.success = False
        except Exception:
            output.success = False
            exc_info = sys.exc_info()
            output.error = &#34;&#34;.join(traceback.format_exception(*exc_info))

    return output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.async_request_sglang_generate"><code class="name flex">
<span>async def <span class="ident">async_request_sglang_generate</span></span>(<span>request_func_input: <a title="sglang.bench_serving.RequestFuncInput" href="#sglang.bench_serving.RequestFuncInput">RequestFuncInput</a>,<br>pbar: tqdm.asyncio.tqdm_asyncio | None = None) ‑> <a title="sglang.bench_serving.RequestFuncOutput" href="#sglang.bench_serving.RequestFuncOutput">RequestFuncOutput</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def async_request_sglang_generate(
    request_func_input: RequestFuncInput,
    pbar: Optional[tqdm] = None,
) -&gt; RequestFuncOutput:
    api_url = request_func_input.api_url
    prompt = request_func_input.prompt

    async with _create_bench_client_session() as session:
        payload = {
            (&#34;text&#34; if isinstance(prompt, str) else &#34;input_ids&#34;): prompt,
            &#34;sampling_params&#34;: {
                &#34;temperature&#34;: 0.0,
                &#34;max_new_tokens&#34;: request_func_input.output_len,
                &#34;ignore_eos&#34;: not args.disable_ignore_eos,
            },
            &#34;stream&#34;: not args.disable_stream,
            &#34;lora_path&#34;: request_func_input.lora_name,
            &#34;return_logprob&#34;: args.return_logprob,
            &#34;logprob_start_len&#34;: -1,
            **request_func_input.extra_request_body,
        }

        # Add image data if available (list of image urls/base64)
        if request_func_input.image_data:
            payload[&#34;image_data&#34;] = request_func_input.image_data

        headers = get_auth_headers()

        output = RequestFuncOutput.init_new(request_func_input)

        generated_text = &#34;&#34;
        output_len = request_func_input.output_len
        ttft = 0.0
        st = time.perf_counter()
        most_recent_timestamp = st
        last_output_len = 0
        try:
            async with session.post(
                url=api_url, json=payload, headers=headers
            ) as response:
                if response.status == 200:
                    async for chunk_bytes in response.content:
                        chunk_bytes = chunk_bytes.strip()
                        if not chunk_bytes:
                            continue

                        chunk = remove_prefix(chunk_bytes.decode(&#34;utf-8&#34;), &#34;data: &#34;)
                        latency = time.perf_counter() - st
                        if chunk == &#34;[DONE]&#34;:
                            pass
                        else:
                            data = json.loads(chunk)

                            # NOTE: Some completion API might have a last
                            # usage summary response without a token so we
                            # want to check a token was generated
                            if &#34;text&#34; in data and data[&#34;text&#34;]:
                                timestamp = time.perf_counter()
                                generated_text = data[&#34;text&#34;]
                                output_len = data[&#34;meta_info&#34;][&#34;completion_tokens&#34;]

                                # First token
                                if ttft == 0.0:
                                    ttft = time.perf_counter() - st
                                    output.ttft = ttft

                                # Decoding phase
                                else:
                                    num_new_tokens = output_len - last_output_len
                                    if num_new_tokens == 0:
                                        continue
                                    adjust_itl = (
                                        timestamp - most_recent_timestamp
                                    ) / num_new_tokens
                                    output.itl.extend([adjust_itl] * num_new_tokens)

                                most_recent_timestamp = timestamp
                                last_output_len = output_len

                    output.generated_text = generated_text
                    output.success = True
                    output.latency = latency
                    output.output_len = output_len
                else:
                    output.error = response.reason or &#34;&#34;
                    output.success = False
        except Exception:
            output.success = False
            exc_info = sys.exc_info()
            output.error = &#34;&#34;.join(traceback.format_exception(*exc_info))
            print(f&#34;{output.error=}&#34;)

    if pbar:
        pbar.update(1)
    return output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.async_request_trt_llm"><code class="name flex">
<span>async def <span class="ident">async_request_trt_llm</span></span>(<span>request_func_input: <a title="sglang.bench_serving.RequestFuncInput" href="#sglang.bench_serving.RequestFuncInput">RequestFuncInput</a>,<br>pbar: tqdm.asyncio.tqdm_asyncio | None = None) ‑> <a title="sglang.bench_serving.RequestFuncOutput" href="#sglang.bench_serving.RequestFuncOutput">RequestFuncOutput</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def async_request_trt_llm(
    request_func_input: RequestFuncInput,
    pbar: Optional[tqdm] = None,
) -&gt; RequestFuncOutput:
    api_url = request_func_input.api_url
    assert api_url.endswith(&#34;generate_stream&#34;)

    async with _create_bench_client_session() as session:
        payload = {
            &#34;accumulate_tokens&#34;: True,
            &#34;text_input&#34;: request_func_input.prompt,
            &#34;temperature&#34;: 0.000001,
            &#34;top_p&#34;: 1.0,
            &#34;max_tokens&#34;: request_func_input.output_len,
            &#34;stream&#34;: True,
            &#34;min_length&#34;: request_func_input.output_len,
            &#34;end_id&#34;: 1048576,
            **request_func_input.extra_request_body,
        }
        if args.disable_ignore_eos:
            del payload[&#34;min_length&#34;]
            del payload[&#34;end_id&#34;]
        output = RequestFuncOutput.init_new(request_func_input)

        ttft = 0.0
        st = time.perf_counter()
        most_recent_timestamp = st
        try:
            async with session.post(url=api_url, json=payload) as response:
                if response.status == 200:
                    async for chunk_bytes in response.content:
                        chunk_bytes = chunk_bytes.strip()
                        if not chunk_bytes:
                            continue

                        chunk = remove_prefix(chunk_bytes.decode(&#34;utf-8&#34;), &#34;data:&#34;)

                        data = json.loads(chunk)
                        output.generated_text += data[&#34;text_output&#34;]
                        timestamp = time.perf_counter()
                        # First token
                        if ttft == 0.0:
                            ttft = timestamp - st
                            output.ttft = ttft

                        # Decoding phase
                        else:
                            output.itl.append(timestamp - most_recent_timestamp)

                        most_recent_timestamp = timestamp

                    output.latency = most_recent_timestamp - st
                    output.success = True
                    output.output_len = request_func_input.output_len

                else:
                    output.error = response.reason or &#34;&#34;
                    output.success = False
        except Exception:
            output.success = False
            exc_info = sys.exc_info()
            output.error = &#34;&#34;.join(traceback.format_exception(*exc_info))

        if pbar:
            pbar.update(1)
        return output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.async_request_truss"><code class="name flex">
<span>async def <span class="ident">async_request_truss</span></span>(<span>request_func_input: <a title="sglang.bench_serving.RequestFuncInput" href="#sglang.bench_serving.RequestFuncInput">RequestFuncInput</a>,<br>pbar: tqdm.asyncio.tqdm_asyncio | None = None) ‑> <a title="sglang.bench_serving.RequestFuncOutput" href="#sglang.bench_serving.RequestFuncOutput">RequestFuncOutput</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def async_request_truss(
    request_func_input: RequestFuncInput,
    pbar: Optional[tqdm] = None,
) -&gt; RequestFuncOutput:
    api_url = request_func_input.api_url

    prompt = request_func_input.prompt

    async with _create_bench_client_session() as session:
        payload = {
            &#34;model&#34;: request_func_input.model,
            &#34;prompt&#34;: prompt,
            &#34;temperature&#34;: 0.0,
            &#34;best_of&#34;: 1,
            &#34;max_tokens&#34;: request_func_input.output_len,
            &#34;stream&#34;: not args.disable_stream,
            &#34;ignore_eos&#34;: not args.disable_ignore_eos,
            **request_func_input.extra_request_body,
        }
        headers = get_auth_headers()

        output = RequestFuncOutput.init_new(request_func_input)

        generated_text = &#34;&#34;
        ttft = 0.0
        st = time.perf_counter()
        most_recent_timestamp = st
        try:
            async with session.post(
                url=api_url, json=payload, headers=headers
            ) as response:
                if response.status == 200:
                    async for chunk_bytes in response.content:
                        chunk_bytes = chunk_bytes.strip()
                        if not chunk_bytes:
                            continue

                        chunk = remove_prefix(chunk_bytes.decode(&#34;utf-8&#34;), &#34;data: &#34;)
                        latency = time.perf_counter() - st
                        if chunk == &#34;[DONE]&#34;:
                            pass
                        else:
                            data = json.loads(chunk)

                            # NOTE: Some completion API might have a last
                            # usage summary response without a token so we
                            # want to check a token was generated
                            if data[&#34;choices&#34;][0][&#34;text&#34;]:
                                timestamp = time.perf_counter()
                                # First token
                                if ttft == 0.0:
                                    ttft = time.perf_counter() - st
                                    output.ttft = ttft

                                # Decoding phase
                                else:
                                    output.itl.append(timestamp - most_recent_timestamp)

                                most_recent_timestamp = timestamp
                                generated_text += data[&#34;choices&#34;][0][&#34;text&#34;]

                    output.generated_text = generated_text
                    output.success = True
                    output.latency = latency
                    output.output_len = request_func_input.output_len
                else:
                    output.error = response.reason or &#34;&#34;
                    output.success = False
        except Exception:
            output.success = False
            exc_info = sys.exc_info()
            output.error = &#34;&#34;.join(traceback.format_exception(*exc_info))

    if pbar:
        pbar.update(1)
    return output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.benchmark"><code class="name flex">
<span>async def <span class="ident">benchmark</span></span>(<span>backend: str,<br>api_url: str,<br>base_url: str,<br>model_id: str,<br>tokenizer: transformers.tokenization_utils_base.PreTrainedTokenizerBase,<br>input_requests: List[<a title="sglang.bench_serving.DatasetRow" href="#sglang.bench_serving.DatasetRow">DatasetRow</a>],<br>request_rate: float,<br>max_concurrency: int | None,<br>disable_tqdm: bool,<br>lora_names: List[str],<br>extra_request_body: Dict[str, Any],<br>profile: bool,<br>pd_separated: bool = False,<br>flush_cache: bool = False,<br>warmup_requests: int = 1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def benchmark(
    backend: str,
    api_url: str,
    base_url: str,
    model_id: str,
    tokenizer: PreTrainedTokenizerBase,
    input_requests: List[DatasetRow],
    request_rate: float,
    max_concurrency: Optional[int],
    disable_tqdm: bool,
    lora_names: List[str],
    extra_request_body: Dict[str, Any],
    profile: bool,
    pd_separated: bool = False,
    flush_cache: bool = False,
    warmup_requests: int = 1,
):
    if backend in ASYNC_REQUEST_FUNCS:
        request_func = ASYNC_REQUEST_FUNCS[backend]
    else:
        raise ValueError(f&#34;Unknown backend: {backend}&#34;)

    # Limit concurrency
    # From https://github.com/vllm-project/vllm/pull/9390
    semaphore = asyncio.Semaphore(max_concurrency) if max_concurrency else None

    async def limited_request_func(request_func_input, pbar):
        if semaphore is None:
            return await request_func(request_func_input=request_func_input, pbar=pbar)
        async with semaphore:
            return await request_func(request_func_input=request_func_input, pbar=pbar)

    # Warmup
    print(f&#34;Starting warmup with {warmup_requests} sequences...&#34;)

    # Use the first request for all warmup iterations
    test_request = input_requests[0]

    if lora_names is not None and len(lora_names) != 0:
        lora_name = lora_names[0]
    else:
        lora_name = None

    # Create the test input once
    test_input = RequestFuncInput(
        model=model_id,
        prompt=test_request.prompt,
        api_url=api_url,
        prompt_len=test_request.prompt_len,
        output_len=min(test_request.output_len, 32),
        lora_name=lora_name,
        image_data=test_request.image_data,
        extra_request_body=extra_request_body,
    )

    # Run warmup requests
    warmup_tasks = []
    for _ in range(warmup_requests):
        warmup_tasks.append(
            asyncio.create_task(request_func(request_func_input=test_input))
        )

    warmup_outputs = await asyncio.gather(*warmup_tasks)

    # Check if at least one warmup request succeeded
    if warmup_requests &gt; 0 and not any(output.success for output in warmup_outputs):
        raise ValueError(
            &#34;Warmup failed - Please make sure benchmark arguments &#34;
            f&#34;are correctly specified. Error: {warmup_outputs[0].error}&#34;
        )
    else:
        print(
            f&#34;Warmup completed with {args.warmup_requests} sequences. Starting main benchmark run...&#34;
        )

    # Flush cache
    if (&#34;sglang&#34; in backend and _get_bool_env_var(&#34;SGLANG_IS_IN_CI&#34;)) or flush_cache:
        requests.post(base_url + &#34;/flush_cache&#34;, headers=get_auth_headers())

    time.sleep(1.0)

    # Start profiler
    if profile:
        print(&#34;Starting profiler...&#34;)
        profile_output = await async_request_profile(
            api_url=base_url + &#34;/start_profile&#34;
        )
        if profile_output.success:
            print(&#34;Profiler started&#34;)

    pbar = None if disable_tqdm else tqdm(total=len(input_requests))

    # Run all requests
    benchmark_start_time = time.perf_counter()
    tasks: List[asyncio.Task] = []
    async for request in get_request(input_requests, request_rate):
        if lora_names is not None and len(lora_names) != 0:
            idx = random.randint(0, len(lora_names) - 1)
            lora_name = lora_names[idx]
        else:
            lora_name = None

        request_func_input = RequestFuncInput(
            model=model_id,
            prompt=request.prompt,
            api_url=api_url,
            prompt_len=request.prompt_len,
            output_len=request.output_len,
            lora_name=lora_name,
            image_data=request.image_data,
            extra_request_body=extra_request_body,
        )

        tasks.append(
            asyncio.create_task(
                limited_request_func(request_func_input=request_func_input, pbar=pbar)
            )
        )
    outputs: List[RequestFuncOutput] = await asyncio.gather(*tasks)

    # Stop profiler
    if profile:
        print(&#34;Stopping profiler...&#34;)
        profile_output = await async_request_profile(api_url=base_url + &#34;/stop_profile&#34;)
        if profile_output.success:
            print(&#34;Profiler stopped&#34;)

    if pbar is not None:
        pbar.close()

    if &#34;sglang&#34; in backend:
        server_info = requests.get(base_url + &#34;/get_server_info&#34;)
        if server_info.status_code == 200:
            server_info_json = server_info.json()
            if &#34;decode&#34; in server_info_json:
                server_info_json = server_info_json[&#34;decode&#34;][0]
            accept_length = server_info_json[&#34;internal_states&#34;][0].get(
                &#34;avg_spec_accept_length&#34;, None
            )
        else:
            accept_length = None
    else:
        accept_length = None

    # Compute metrics and print results
    benchmark_duration = time.perf_counter() - benchmark_start_time
    metrics, output_lens = calculate_metrics(
        input_requests=input_requests,
        outputs=outputs,
        dur_s=benchmark_duration,
        tokenizer=tokenizer,
        backend=backend,
    )

    print(&#34;\n{s:{c}^{n}}&#34;.format(s=&#34; Serving Benchmark Result &#34;, n=50, c=&#34;=&#34;))
    print(&#34;{:&lt;40} {:&lt;10}&#34;.format(&#34;Backend:&#34;, backend))
    print(&#34;{:&lt;40} {:&lt;10}&#34;.format(&#34;Traffic request rate:&#34;, request_rate))
    print(
        &#34;{:&lt;40} {:&lt;10}&#34;.format(
            &#34;Max request concurrency:&#34;,
            max_concurrency if max_concurrency else &#34;not set&#34;,
        )
    )
    print(&#34;{:&lt;40} {:&lt;10}&#34;.format(&#34;Successful requests:&#34;, metrics.completed))
    print(&#34;{:&lt;40} {:&lt;10.2f}&#34;.format(&#34;Benchmark duration (s):&#34;, benchmark_duration))
    print(&#34;{:&lt;40} {:&lt;10}&#34;.format(&#34;Total input tokens:&#34;, metrics.total_input))
    print(&#34;{:&lt;40} {:&lt;10}&#34;.format(&#34;Total generated tokens:&#34;, metrics.total_output))
    print(
        &#34;{:&lt;40} {:&lt;10}&#34;.format(
            &#34;Total generated tokens (retokenized):&#34;, metrics.total_output_retokenized
        )
    )
    print(
        &#34;{:&lt;40} {:&lt;10.2f}&#34;.format(
            &#34;Request throughput (req/s):&#34;, metrics.request_throughput
        )
    )
    print(
        &#34;{:&lt;40} {:&lt;10.2f}&#34;.format(
            &#34;Input token throughput (tok/s):&#34;, metrics.input_throughput
        )
    )
    print(
        &#34;{:&lt;40} {:&lt;10.2f}&#34;.format(
            &#34;Output token throughput (tok/s):&#34;, metrics.output_throughput
        )
    )
    print(
        &#34;{:&lt;40} {:&lt;10.2f}&#34;.format(
            &#34;Total token throughput (tok/s):&#34;, metrics.total_throughput
        )
    )
    print(&#34;{:&lt;40} {:&lt;10.2f}&#34;.format(&#34;Concurrency:&#34;, metrics.concurrency))
    if accept_length:
        print(&#34;{:&lt;40} {:&lt;10.2f}&#34;.format(&#34;Accept length:&#34;, accept_length))
    print(&#34;{s:{c}^{n}}&#34;.format(s=&#34;End-to-End Latency&#34;, n=50, c=&#34;-&#34;))
    print(
        &#34;{:&lt;40} {:&lt;10.2f}&#34;.format(&#34;Mean E2E Latency (ms):&#34;, metrics.mean_e2e_latency_ms)
    )
    print(
        &#34;{:&lt;40} {:&lt;10.2f}&#34;.format(
            &#34;Median E2E Latency (ms):&#34;, metrics.median_e2e_latency_ms
        )
    )
    print(&#34;{s:{c}^{n}}&#34;.format(s=&#34;Time to First Token&#34;, n=50, c=&#34;-&#34;))
    print(&#34;{:&lt;40} {:&lt;10.2f}&#34;.format(&#34;Mean TTFT (ms):&#34;, metrics.mean_ttft_ms))
    print(&#34;{:&lt;40} {:&lt;10.2f}&#34;.format(&#34;Median TTFT (ms):&#34;, metrics.median_ttft_ms))
    print(&#34;{:&lt;40} {:&lt;10.2f}&#34;.format(&#34;P99 TTFT (ms):&#34;, metrics.p99_ttft_ms))
    print(&#34;{s:{c}^{n}}&#34;.format(s=&#34;Inter-Token Latency&#34;, n=50, c=&#34;-&#34;))
    print(&#34;{:&lt;40} {:&lt;10.2f}&#34;.format(&#34;Mean ITL (ms):&#34;, metrics.mean_itl_ms))
    print(&#34;{:&lt;40} {:&lt;10.2f}&#34;.format(&#34;Median ITL (ms):&#34;, metrics.median_itl_ms))
    print(&#34;{:&lt;40} {:&lt;10.2f}&#34;.format(&#34;P95 ITL (ms):&#34;, metrics.p95_itl_ms))
    print(&#34;{:&lt;40} {:&lt;10.2f}&#34;.format(&#34;P99 ITL (ms):&#34;, metrics.p99_itl_ms))
    print(&#34;{:&lt;40} {:&lt;10.2f}&#34;.format(&#34;Max ITL (ms):&#34;, metrics.max_itl_ms))
    print(&#34;=&#34; * 50)

    if (
        metrics.median_ttft_ms is not None
        and metrics.mean_itl_ms is not None
        and metrics.output_throughput is not None
    ):
        result = {
            # Arguments
            &#34;backend&#34;: args.backend,
            &#34;dataset_name&#34;: args.dataset_name,
            &#34;request_rate&#34;: request_rate,
            &#34;max_concurrency&#34;: max_concurrency,
            &#34;sharegpt_output_len&#34;: args.sharegpt_output_len,
            &#34;random_input_len&#34;: args.random_input_len,
            &#34;random_output_len&#34;: args.random_output_len,
            &#34;random_range_ratio&#34;: args.random_range_ratio,
            # Results
            &#34;duration&#34;: benchmark_duration,
            &#34;completed&#34;: metrics.completed,
            &#34;total_input_tokens&#34;: metrics.total_input,
            &#34;total_output_tokens&#34;: metrics.total_output,
            &#34;total_output_tokens_retokenized&#34;: metrics.total_output_retokenized,
            &#34;request_throughput&#34;: metrics.request_throughput,
            &#34;input_throughput&#34;: metrics.input_throughput,
            &#34;output_throughput&#34;: metrics.output_throughput,
            &#34;mean_e2e_latency_ms&#34;: metrics.mean_e2e_latency_ms,
            &#34;median_e2e_latency_ms&#34;: metrics.median_e2e_latency_ms,
            &#34;std_e2e_latency_ms&#34;: metrics.std_e2e_latency_ms,
            &#34;p99_e2e_latency_ms&#34;: metrics.p99_e2e_latency_ms,
            &#34;mean_ttft_ms&#34;: metrics.mean_ttft_ms,
            &#34;median_ttft_ms&#34;: metrics.median_ttft_ms,
            &#34;std_ttft_ms&#34;: metrics.std_ttft_ms,
            &#34;p99_ttft_ms&#34;: metrics.p99_ttft_ms,
            &#34;mean_tpot_ms&#34;: metrics.mean_tpot_ms,
            &#34;median_tpot_ms&#34;: metrics.median_tpot_ms,
            &#34;std_tpot_ms&#34;: metrics.std_tpot_ms,
            &#34;p99_tpot_ms&#34;: metrics.p99_tpot_ms,
            &#34;mean_itl_ms&#34;: metrics.mean_itl_ms,
            &#34;median_itl_ms&#34;: metrics.median_itl_ms,
            &#34;std_itl_ms&#34;: metrics.std_itl_ms,
            &#34;p95_itl_ms&#34;: metrics.p95_itl_ms,
            &#34;p99_itl_ms&#34;: metrics.p99_itl_ms,
            &#34;concurrency&#34;: metrics.concurrency,
            &#34;accept_length&#34;: accept_length,
        }
    else:
        print(f&#34;Error running benchmark for request rate: {request_rate}&#34;)
        print(&#34;-&#34; * 30)

    # Determine output file name
    if args.output_file:
        output_file_name = args.output_file
    else:
        now = datetime.now().strftime(&#34;%m%d&#34;)
        if args.dataset_name == &#34;random-image&#34;:
            output_file_name = (
                f&#34;{args.backend}_{now}_{args.num_prompts}_{args.random_input_len}_&#34;
                f&#34;{args.random_output_len}_{args.random_image_num_images}imgs_&#34;
                f&#34;{args.random_image_resolution}.jsonl&#34;
            )
        elif args.dataset_name.startswith(&#34;random&#34;):
            output_file_name = f&#34;{args.backend}_{now}_{args.num_prompts}_{args.random_input_len}_{args.random_output_len}.jsonl&#34;
        else:
            output_file_name = f&#34;{args.backend}_{now}_{args.num_prompts}_sharegpt.jsonl&#34;

    result_details = {
        &#34;input_lens&#34;: [output.prompt_len for output in outputs],
        &#34;output_lens&#34;: output_lens,
        &#34;ttfts&#34;: [output.ttft for output in outputs],
        &#34;itls&#34;: [output.itl for output in outputs],
        &#34;generated_texts&#34;: [output.generated_text for output in outputs],
        &#34;errors&#34;: [output.error for output in outputs],
    }

    # Append results to a JSONL file
    with open(output_file_name, &#34;a&#34;) as file:
        if args.output_details:
            result_for_dump = result | result_details
        else:
            result_for_dump = result
        file.write(json.dumps(result_for_dump) + &#34;\n&#34;)

    return result | result_details</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.calculate_metrics"><code class="name flex">
<span>def <span class="ident">calculate_metrics</span></span>(<span>input_requests: List[<a title="sglang.bench_serving.DatasetRow" href="#sglang.bench_serving.DatasetRow">DatasetRow</a>],<br>outputs: List[<a title="sglang.bench_serving.RequestFuncOutput" href="#sglang.bench_serving.RequestFuncOutput">RequestFuncOutput</a>],<br>dur_s: float,<br>tokenizer: transformers.tokenization_utils_base.PreTrainedTokenizerBase,<br>backend: str) ‑> Tuple[<a title="sglang.bench_serving.BenchmarkMetrics" href="#sglang.bench_serving.BenchmarkMetrics">BenchmarkMetrics</a>, List[int]]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_metrics(
    input_requests: List[DatasetRow],
    outputs: List[RequestFuncOutput],
    dur_s: float,
    tokenizer: PreTrainedTokenizerBase,
    backend: str,
) -&gt; Tuple[BenchmarkMetrics, List[int]]:
    output_lens: List[int] = []
    retokenized_output_lens: List[int] = []
    total_input = 0
    completed = 0
    itls: List[float] = []
    tpots: List[float] = []
    ttfts: List[float] = []
    e2e_latencies: List[float] = []
    for i in range(len(outputs)):
        if outputs[i].success:
            output_len = outputs[i].output_len
            output_lens.append(output_len)
            retokenized_output_len = len(
                tokenizer.encode(outputs[i].generated_text, add_special_tokens=False)
            )
            retokenized_output_lens.append(retokenized_output_len)
            total_input += input_requests[i].prompt_len
            if output_len &gt; 1:
                tpots.append((outputs[i].latency - outputs[i].ttft) / (output_len - 1))
            itls += outputs[i].itl
            ttfts.append(outputs[i].ttft)

            e2e_latencies.append(outputs[i].latency)

            completed += 1
        else:
            output_lens.append(0)
            retokenized_output_lens.append(0)

    if completed == 0:
        warnings.warn(
            &#34;All requests failed. This is likely due to a misconfiguration &#34;
            &#34;on the benchmark arguments.&#34;,
            stacklevel=2,
        )
    metrics = BenchmarkMetrics(
        completed=completed,
        total_input=total_input,
        total_output=sum(output_lens),
        total_output_retokenized=sum(retokenized_output_lens),
        request_throughput=completed / dur_s,
        input_throughput=total_input / dur_s,
        output_throughput=sum(output_lens) / dur_s,
        output_throughput_retokenized=sum(retokenized_output_lens) / dur_s,
        total_throughput=(total_input + sum(output_lens)) / dur_s,
        total_throughput_retokenized=(total_input + sum(retokenized_output_lens))
        / dur_s,
        mean_ttft_ms=np.mean(ttfts or 0)
        * 1000,  # ttfts is empty if streaming is not supported by backend
        median_ttft_ms=np.median(ttfts or 0) * 1000,
        std_ttft_ms=np.std(ttfts or 0) * 1000,
        p99_ttft_ms=np.percentile(ttfts or 0, 99) * 1000,
        mean_tpot_ms=np.mean(tpots or 0) * 1000,
        median_tpot_ms=np.median(tpots or 0) * 1000,
        std_tpot_ms=np.std(tpots or 0) * 1000,
        p99_tpot_ms=np.percentile(tpots or 0, 99) * 1000,
        mean_itl_ms=np.mean(itls or 0) * 1000,
        median_itl_ms=np.median(itls or 0) * 1000,
        std_itl_ms=np.std(itls or 0) * 1000,
        p95_itl_ms=np.percentile(itls or 0, 95) * 1000,
        p99_itl_ms=np.percentile(itls or 0, 99) * 1000,
        max_itl_ms=np.max(itls or 0) * 1000,
        mean_e2e_latency_ms=np.mean(e2e_latencies) * 1000,
        median_e2e_latency_ms=np.median(e2e_latencies) * 1000,
        std_e2e_latency_ms=np.std(e2e_latencies) * 1000,
        p99_e2e_latency_ms=np.percentile(e2e_latencies, 99) * 1000,
        concurrency=np.sum(e2e_latencies) / dur_s,
    )

    return metrics, output_lens</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.check_chat_template"><code class="name flex">
<span>def <span class="ident">check_chat_template</span></span>(<span>model_path)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_chat_template(model_path):
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        return &#34;chat_template&#34; in tokenizer.init_kwargs
    except Exception as e:
        print(f&#34;Fail to load tokenizer config with error={e}&#34;)
        return False</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.download_and_cache_file"><code class="name flex">
<span>def <span class="ident">download_and_cache_file</span></span>(<span>url: str, filename: str | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def download_and_cache_file(url: str, filename: Optional[str] = None):
    &#34;&#34;&#34;Read and cache a file from a url.&#34;&#34;&#34;
    if filename is None:
        filename = os.path.join(&#34;/tmp&#34;, url.split(&#34;/&#34;)[-1])

    # Check if the cache file already exists
    if is_file_valid_json(filename):
        return filename

    print(f&#34;Downloading from {url} to {filename}&#34;)

    # Stream the response to show the progress bar
    response = requests.get(url, stream=True)
    response.raise_for_status()  # Check for request errors

    # Total size of the file in bytes
    total_size = int(response.headers.get(&#34;content-length&#34;, 0))
    chunk_size = 1024  # Download in chunks of 1KB

    # Use tqdm to display the progress bar
    with open(filename, &#34;wb&#34;) as f, tqdm(
        desc=filename,
        total=total_size,
        unit=&#34;B&#34;,
        unit_scale=True,
        unit_divisor=1024,
    ) as bar:
        for chunk in response.iter_content(chunk_size=chunk_size):
            f.write(chunk)
            bar.update(len(chunk))

    return filename</code></pre>
</details>
<div class="desc"><p>Read and cache a file from a url.</p></div>
</dd>
<dt id="sglang.bench_serving.gen_prompt"><code class="name flex">
<span>def <span class="ident">gen_prompt</span></span>(<span>tokenizer, token_num)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gen_prompt(tokenizer, token_num):
    &#34;&#34;&#34;Generate a random prompt of specified token length using tokenizer vocabulary.&#34;&#34;&#34;
    all_available_tokens = list(tokenizer.get_vocab().values())
    selected_tokens = random.choices(all_available_tokens, k=token_num)
    return tokenizer.decode(selected_tokens)</code></pre>
</details>
<div class="desc"><p>Generate a random prompt of specified token length using tokenizer vocabulary.</p></div>
</dd>
<dt id="sglang.bench_serving.get_auth_headers"><code class="name flex">
<span>def <span class="ident">get_auth_headers</span></span>(<span>) ‑> Dict[str, str]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_auth_headers() -&gt; Dict[str, str]:
    api_key = os.environ.get(&#34;OPENAI_API_KEY&#34;)
    if api_key:
        return {&#34;Authorization&#34;: f&#34;Bearer {api_key}&#34;}
    else:
        return {}</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.get_dataset"><code class="name flex">
<span>def <span class="ident">get_dataset</span></span>(<span>args, tokenizer)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dataset(args, tokenizer):
    tokenize_prompt = getattr(args, &#34;tokenize_prompt&#34;, False)
    if args.dataset_name == &#34;sharegpt&#34;:
        assert not tokenize_prompt
        input_requests = sample_sharegpt_requests(
            dataset_path=args.dataset_path,
            num_requests=args.num_prompts,
            tokenizer=tokenizer,
            fixed_output_len=args.sharegpt_output_len,
            context_len=args.sharegpt_context_len,
            prompt_suffix=args.prompt_suffix,
            apply_chat_template=args.apply_chat_template,
        )
    elif args.dataset_name.startswith(&#34;random&#34;) and args.dataset_name != &#34;random-image&#34;:
        input_requests = sample_random_requests(
            input_len=args.random_input_len,
            output_len=args.random_output_len,
            num_prompts=args.num_prompts,
            range_ratio=args.random_range_ratio,
            tokenizer=tokenizer,
            dataset_path=args.dataset_path,
            random_sample=args.dataset_name == &#34;random&#34;,
            return_text=not tokenize_prompt,
        )
    elif args.dataset_name == &#34;random-image&#34;:
        assert not tokenize_prompt, &#34;random-image does not support --tokenize-prompt&#34;
        input_requests = sample_random_image_requests(
            num_requests=args.num_prompts,
            num_images=args.random_image_num_images,
            input_len=args.random_input_len,
            output_len=args.random_output_len,
            range_ratio=args.random_range_ratio,
            tokenizer=tokenizer,
            apply_chat_template=args.apply_chat_template,
            image_resolution=args.random_image_resolution,
        )
    elif args.dataset_name == &#34;generated-shared-prefix&#34;:
        assert not tokenize_prompt
        input_requests = sample_generated_shared_prefix_requests(
            num_groups=args.gsp_num_groups,
            prompts_per_group=args.gsp_prompts_per_group,
            system_prompt_len=args.gsp_system_prompt_len,
            question_len=args.gsp_question_len,
            output_len=args.gsp_output_len,
            tokenizer=tokenizer,
            args=args,
        )
    elif args.dataset_name == &#34;mmmu&#34;:
        assert not tokenize_prompt
        input_requests = sample_mmmu_requests(
            num_requests=args.num_prompts,
            tokenizer=tokenizer,
            fixed_output_len=args.random_output_len,
            apply_chat_template=args.apply_chat_template,
            random_sample=True,
        )
    else:
        raise ValueError(f&#34;Unknown dataset: {args.dataset_name}&#34;)
    return input_requests</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.get_gen_prefix_cache_path"><code class="name flex">
<span>def <span class="ident">get_gen_prefix_cache_path</span></span>(<span>args, tokenizer)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_gen_prefix_cache_path(args, tokenizer):
    &#34;&#34;&#34;Create cache directory under ~/.cache/sglang/benchmark&#34;&#34;&#34;
    cache_dir = Path.home() / &#34;.cache&#34; / &#34;sglang&#34; / &#34;benchmark&#34;

    # Create a unique cache filename based on the generation parameters
    cache_key = (
        f&#34;gen_shared_prefix_{args.gsp_num_groups}_{args.gsp_prompts_per_group}_&#34;
        f&#34;{args.gsp_system_prompt_len}_{args.gsp_question_len}_{args.gsp_output_len}_&#34;
        f&#34;{tokenizer.__class__.__name__}.pkl&#34;
    )
    return cache_dir / cache_key</code></pre>
</details>
<div class="desc"><p>Create cache directory under ~/.cache/sglang/benchmark</p></div>
</dd>
<dt id="sglang.bench_serving.get_model"><code class="name flex">
<span>def <span class="ident">get_model</span></span>(<span>pretrained_model_name_or_path: str) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_model(pretrained_model_name_or_path: str) -&gt; str:
    if os.getenv(&#34;SGLANG_USE_MODELSCOPE&#34;, &#34;false&#34;).lower() == &#34;true&#34;:
        import huggingface_hub.constants
        from modelscope import snapshot_download

        model_path = snapshot_download(
            model_id=pretrained_model_name_or_path,
            local_files_only=huggingface_hub.constants.HF_HUB_OFFLINE,
            ignore_file_pattern=[&#34;.*.pt&#34;, &#34;.*.safetensors&#34;, &#34;.*.bin&#34;],
        )

        return model_path
    return pretrained_model_name_or_path</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.get_request"><code class="name flex">
<span>async def <span class="ident">get_request</span></span>(<span>input_requests: List[<a title="sglang.bench_serving.DatasetRow" href="#sglang.bench_serving.DatasetRow">DatasetRow</a>],<br>request_rate: float) ‑> AsyncGenerator[<a title="sglang.bench_serving.DatasetRow" href="#sglang.bench_serving.DatasetRow">DatasetRow</a>, None]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def get_request(
    input_requests: List[DatasetRow],
    request_rate: float,
) -&gt; AsyncGenerator[DatasetRow, None]:
    input_requests = iter(input_requests)
    for request in input_requests:
        yield request

        if request_rate == float(&#34;inf&#34;):
            # If the request rate is infinity, then we don&#39;t need to wait.
            continue

        # Sample the request interval from the exponential distribution.
        interval = np.random.exponential(1.0 / request_rate)
        # The next request will be sent after the interval.
        await asyncio.sleep(interval)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.get_tokenizer"><code class="name flex">
<span>def <span class="ident">get_tokenizer</span></span>(<span>pretrained_model_name_or_path: str) ‑> transformers.tokenization_utils.PreTrainedTokenizer | transformers.tokenization_utils_fast.PreTrainedTokenizerFast</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_tokenizer(
    pretrained_model_name_or_path: str,
) -&gt; Union[PreTrainedTokenizer, PreTrainedTokenizerFast]:
    assert (
        pretrained_model_name_or_path is not None
        and pretrained_model_name_or_path != &#34;&#34;
    )
    if pretrained_model_name_or_path.endswith(
        &#34;.json&#34;
    ) or pretrained_model_name_or_path.endswith(&#34;.model&#34;):
        from sglang.srt.hf_transformers_utils import get_tokenizer

        return get_tokenizer(pretrained_model_name_or_path)

    if pretrained_model_name_or_path is not None and not os.path.exists(
        pretrained_model_name_or_path
    ):
        pretrained_model_name_or_path = get_model(pretrained_model_name_or_path)
    return AutoTokenizer.from_pretrained(
        pretrained_model_name_or_path, trust_remote_code=True
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.is_file_valid_json"><code class="name flex">
<span>def <span class="ident">is_file_valid_json</span></span>(<span>path)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_file_valid_json(path):
    if not os.path.isfile(path):
        return False

    # TODO can fuse into the real file open later
    try:
        with open(path) as f:
            json.load(f)
        return True
    except JSONDecodeError as e:
        print(
            f&#34;{path} exists but json loading fails ({e=}), thus treat as invalid file&#34;
        )
        return False</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.parse_random_image_resolution"><code class="name flex">
<span>def <span class="ident">parse_random_image_resolution</span></span>(<span>image_resolution: str) ‑> Tuple[int, int]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_random_image_resolution(image_resolution: str) -&gt; Tuple[int, int]:
    &#34;&#34;&#34;Parse image resolution into (width, height).

    Supports presets &#39;1080p&#39;, &#39;720p&#39;, &#39;360p&#39; and custom &#39;heightxwidth&#39; format
    (e.g., &#39;1080x1920&#39; means height=1080, width=1920).
    &#34;&#34;&#34;
    resolution_to_size = {
        &#34;4k&#34;: (3840, 2160),
        &#34;1080p&#34;: (1920, 1080),
        &#34;720p&#34;: (1280, 720),
        &#34;360p&#34;: (640, 360),
    }
    if image_resolution in resolution_to_size:
        return resolution_to_size[image_resolution]

    res = image_resolution.strip().lower()
    if &#34;x&#34; in res:
        parts = res.split(&#34;x&#34;)
        if len(parts) == 2 and parts[0].isdigit() and parts[1].isdigit():
            height = int(parts[0])
            width = int(parts[1])
            if height &gt; 0 and width &gt; 0:
                return (width, height)

    raise ValueError(
        f&#34;Unsupported random-image resolution: {image_resolution}. &#34;
        &#34;Choose from 4k, 1080p, 720p, 360p, or provide custom &#39;heightxwidth&#39; (e.g., 1080x1920).&#34;
    )</code></pre>
</details>
<div class="desc"><p>Parse image resolution into (width, height).</p>
<p>Supports presets '1080p', '720p', '360p' and custom 'heightxwidth' format
(e.g., '1080x1920' means height=1080, width=1920).</p></div>
</dd>
<dt id="sglang.bench_serving.remove_prefix"><code class="name flex">
<span>def <span class="ident">remove_prefix</span></span>(<span>text: str, prefix: str) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_prefix(text: str, prefix: str) -&gt; str:
    return text[len(prefix) :] if text.startswith(prefix) else text</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.remove_suffix"><code class="name flex">
<span>def <span class="ident">remove_suffix</span></span>(<span>text: str, suffix: str) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_suffix(text: str, suffix: str) -&gt; str:
    return text[: -len(suffix)] if text.endswith(suffix) else text</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.run_benchmark"><code class="name flex">
<span>def <span class="ident">run_benchmark</span></span>(<span>args_: argparse.Namespace)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_benchmark(args_: argparse.Namespace):
    global args
    args = args_

    # Set default value for max_concurrency if not present
    if not hasattr(args, &#34;max_concurrency&#34;):
        args.max_concurrency = None

    # Set default value for warmup_requests if not present
    if not hasattr(args, &#34;warmup_requests&#34;):
        args.warmup_requests = 1

    if not hasattr(args, &#34;output_details&#34;):
        args.output_details = False

    if not hasattr(args, &#34;tokenize_prompt&#34;):
        args.tokenize_prompt = False

    print(f&#34;benchmark_args={args}&#34;)

    # Set global environments
    set_ulimit()
    random.seed(args.seed)
    np.random.seed(args.seed)

    extra_request_body = {}
    if args.extra_request_body:
        extra_request_body = json.loads(args.extra_request_body)

    if args.tokenize_prompt:
        assert (
            args.backend == &#34;sglang&#34;
        ), &#34;`--tokenize-prompt` only compatible with `--backend sglang` currently&#34;

    # Set url
    if args.port is None:
        args.port = {
            &#34;sglang&#34;: 30000,
            &#34;sglang-native&#34;: 30000,
            &#34;sglang-oai&#34;: 30000,
            &#34;lmdeploy&#34;: 23333,
            &#34;vllm&#34;: 8000,
            &#34;trt&#34;: 8000,
            &#34;gserver&#34;: 9988,
            &#34;truss&#34;: 8080,
        }.get(args.backend, 30000)

    model_url = (
        f&#34;{args.base_url}/v1/models&#34;
        if args.base_url
        else f&#34;http://{args.host}:{args.port}/v1/models&#34;
    )

    if args.backend in [&#34;sglang&#34;, &#34;sglang-native&#34;]:
        api_url = (
            f&#34;{args.base_url}/generate&#34;
            if args.base_url
            else f&#34;http://{args.host}:{args.port}/generate&#34;
        )
    elif args.backend in [&#34;sglang-oai&#34;, &#34;vllm&#34;, &#34;lmdeploy&#34;]:
        api_url = (
            f&#34;{args.base_url}/v1/completions&#34;
            if args.base_url
            else f&#34;http://{args.host}:{args.port}/v1/completions&#34;
        )
    elif args.backend in [&#34;sglang-oai-chat&#34;, &#34;vllm-chat&#34;, &#34;lmdeploy-chat&#34;]:
        api_url = (
            f&#34;{args.base_url}/v1/chat/completions&#34;
            if args.base_url
            else f&#34;http://{args.host}:{args.port}/v1/chat/completions&#34;
        )
    elif args.backend == &#34;trt&#34;:
        api_url = (
            f&#34;{args.base_url}/v2/models/ensemble/generate_stream&#34;
            if args.base_url
            else f&#34;http://{args.host}:{args.port}/v2/models/ensemble/generate_stream&#34;
        )
        if args.model is None:
            print(&#34;Please provide a model using `--model` when using `trt` backend.&#34;)
            sys.exit(1)
    elif args.backend == &#34;gserver&#34;:
        api_url = args.base_url if args.base_url else f&#34;{args.host}:{args.port}&#34;
        args.model = args.model or &#34;default&#34;
    elif args.backend == &#34;truss&#34;:
        api_url = (
            f&#34;{args.base_url}/v1/models/model:predict&#34;
            if args.base_url
            else f&#34;http://{args.host}:{args.port}/v1/models/model:predict&#34;
        )
    base_url = (
        f&#34;http://{args.host}:{args.port}&#34; if args.base_url is None else args.base_url
    )

    # Get model name
    if args.model is None:
        if args.backend == &#34;truss&#34;:
            print(
                &#34;Please provide a model with `--model` when using truss backend. e.g. --model meta-llama/Llama-3.1-8B-Instruct&#34;
            )
            sys.exit(1)
        try:
            response = requests.get(model_url, headers=get_auth_headers())
            model_list = response.json().get(&#34;data&#34;, [])
            args.model = model_list[0][&#34;id&#34;] if model_list else None
        except Exception as e:
            print(f&#34;Failed to fetch model from {model_url}. Error: {e}&#34;)
            print(
                &#34;Please specify the correct host and port using `--host` and `--port`.&#34;
            )
            sys.exit(1)

    if args.model is None:
        print(&#34;No model specified or found. Please provide a model using `--model`.&#34;)
        sys.exit(1)

    if not check_chat_template(args.model):
        print(
            &#34;\nWARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.\n&#34;
            &#34;Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.\n&#34;
        )

    print(f&#34;{args}\n&#34;)

    # Read dataset
    backend = args.backend
    model_id = args.model
    tokenizer_id = args.tokenizer if args.tokenizer is not None else args.model
    tokenizer = get_tokenizer(tokenizer_id)
    input_requests = get_dataset(args, tokenizer)

    # compatible with SimpleNamespace
    if not hasattr(args, &#34;flush_cache&#34;):
        args.flush_cache = False

    return asyncio.run(
        benchmark(
            backend=backend,
            api_url=api_url,
            base_url=base_url,
            model_id=model_id,
            tokenizer=tokenizer,
            input_requests=input_requests,
            request_rate=args.request_rate,
            max_concurrency=args.max_concurrency,
            disable_tqdm=args.disable_tqdm,
            lora_names=args.lora_name,
            extra_request_body=extra_request_body,
            profile=args.profile,
            pd_separated=args.pd_separated,
            flush_cache=args.flush_cache,
            warmup_requests=args.warmup_requests,
        )
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.sample_generated_shared_prefix_requests"><code class="name flex">
<span>def <span class="ident">sample_generated_shared_prefix_requests</span></span>(<span>num_groups: int,<br>prompts_per_group: int,<br>system_prompt_len: int,<br>question_len: int,<br>output_len: int,<br>tokenizer: transformers.tokenization_utils_base.PreTrainedTokenizerBase,<br>args: argparse.Namespace) ‑> List[<a title="sglang.bench_serving.DatasetRow" href="#sglang.bench_serving.DatasetRow">DatasetRow</a>]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_generated_shared_prefix_requests(
    num_groups: int,
    prompts_per_group: int,
    system_prompt_len: int,
    question_len: int,
    output_len: int,
    tokenizer: PreTrainedTokenizerBase,
    args: argparse.Namespace,
) -&gt; List[DatasetRow]:
    &#34;&#34;&#34;Generate benchmark requests with shared system prompts using random tokens and caching.&#34;&#34;&#34;
    cache_path = get_gen_prefix_cache_path(args, tokenizer)

    # Try to load from cache first
    if cache_path.exists():
        print(f&#34;\nLoading cached generated input data from {cache_path}&#34;)
        with open(cache_path, &#34;rb&#34;) as f:
            return pickle.load(f)

    print(&#34;\nGenerating new input data...&#34;)

    # Generate system prompts for each group
    system_prompts = []
    for _ in range(num_groups):
        system_prompt = gen_prompt(tokenizer, system_prompt_len)
        system_prompts.append(system_prompt)

    # Generate questions
    questions = []
    for _ in range(num_groups * prompts_per_group):
        question = gen_prompt(tokenizer, question_len)
        questions.append(question)

    # Combine system prompts with questions
    input_requests = []
    total_input_tokens = 0
    total_output_tokens = 0

    for group_idx in tqdm(range(num_groups), desc=&#34;Generating system prompt&#34;):
        system_prompt = system_prompts[group_idx]
        for prompt_idx in tqdm(
            range(prompts_per_group), desc=&#34;Generating questions&#34;, leave=False
        ):
            question = questions[group_idx * prompts_per_group + prompt_idx]
            full_prompt = f&#34;{system_prompt}\n\n{question}&#34;
            prompt_len = len(tokenizer.encode(full_prompt))

            input_requests.append(
                DatasetRow(
                    prompt=full_prompt, prompt_len=prompt_len, output_len=output_len
                )
            )
            total_input_tokens += prompt_len
            total_output_tokens += output_len

    # Shuffle questions
    random.shuffle(input_requests)

    # Print statistics
    print(f&#34;\nGenerated shared prefix dataset statistics:&#34;)
    print(f&#34;Number of groups: {num_groups}&#34;)
    print(f&#34;Prompts per group: {prompts_per_group}&#34;)
    print(f&#34;Total prompts: {len(input_requests)}&#34;)
    print(f&#34;Total input tokens: {total_input_tokens}&#34;)
    print(f&#34;Total output tokens: {total_output_tokens}&#34;)
    print(
        f&#34;Average system prompt length: {sum(len(tokenizer.encode(sp)) for sp in system_prompts) / len(system_prompts):.1f} tokens&#34;
    )
    print(
        f&#34;Average question length: {sum(len(tokenizer.encode(q)) for q in questions) / len(questions):.1f} tokens\n&#34;
    )

    # Save to cache
    cache_path.parent.mkdir(parents=True, exist_ok=True)
    print(f&#34;Caching generated input data to {cache_path}&#34;)
    with open(cache_path, &#34;wb&#34;) as f:
        pickle.dump(input_requests, f)

    return input_requests</code></pre>
</details>
<div class="desc"><p>Generate benchmark requests with shared system prompts using random tokens and caching.</p></div>
</dd>
<dt id="sglang.bench_serving.sample_mmmu_requests"><code class="name flex">
<span>def <span class="ident">sample_mmmu_requests</span></span>(<span>num_requests: int,<br>tokenizer: transformers.tokenization_utils_base.PreTrainedTokenizerBase,<br>fixed_output_len: int | None = None,<br>apply_chat_template: bool = True,<br>random_sample: bool = True) ‑> List[<a title="sglang.bench_serving.DatasetRow" href="#sglang.bench_serving.DatasetRow">DatasetRow</a>]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_mmmu_requests(
    num_requests: int,
    tokenizer: PreTrainedTokenizerBase,
    fixed_output_len: Optional[int] = None,
    apply_chat_template: bool = True,
    random_sample: bool = True,
) -&gt; List[DatasetRow]:
    &#34;&#34;&#34;
    Sample requests from the MMMU dataset using HuggingFace datasets.

    Args:
        num_requests: Number of requests to sample.
        tokenizer: Tokenizer to use for token counting.
        fixed_output_len: If provided, use this fixed output length for all requests.
        apply_chat_template: Whether to apply the chat template to the prompt.
        random_sample: Whether to randomly sample or take the first N.

    Returns:
        List of tuples (prompt, prompt_token_len, output_token_len).
    &#34;&#34;&#34;
    try:
        import io

        import pybase64
        from datasets import load_dataset
    except ImportError:
        raise ImportError(&#34;Please install datasets: pip install datasets&#34;)

    print(&#34;Loading MMMU dataset from HuggingFace...&#34;)

    try:
        print(&#34;Attempting to load MMMU Math dataset...&#34;)
        mmmu_dataset = load_dataset(&#34;MMMU/MMMU&#34;, &#34;Math&#34;, split=&#34;test&#34;)
        print(
            f&#34;Successfully loaded MMMU Math dataset from HuggingFace with {len(mmmu_dataset)} examples&#34;
        )
    except Exception as e:
        print(f&#34;Failed to load MMMU Math dataset: {e}&#34;)
        raise ValueError(f&#34;Failed to load MMMU dataset: {e}&#34;)

    # Sample from the dataset
    if len(mmmu_dataset) &gt; num_requests:
        if random_sample:
            # Random sample
            indices = random.sample(range(len(mmmu_dataset)), num_requests)
            sample_dataset = mmmu_dataset.select(indices)
        else:
            # Take first N
            sample_dataset = mmmu_dataset.select(
                range(min(num_requests, len(mmmu_dataset)))
            )
    else:
        print(f&#34;Dataset has less than {num_requests} examples, using all examples&#34;)
        sample_dataset = mmmu_dataset

    print(f&#34;Selected {len(sample_dataset)} examples for benchmarking&#34;)

    # Create prompts
    filtered_dataset = []

    for i, example in enumerate(sample_dataset):
        try:
            # Extract image_1
            image = example.get(&#34;image_1&#34;)

            if image is not None:
                if hasattr(image, &#34;save&#34;):
                    # Convert RGBA images to RGB before encoding
                    if image.mode == &#34;RGBA&#34;:
                        image = image.convert(&#34;RGB&#34;)

                    # Encode image to base64 (save as PNG to support palette/alpha modes)
                    buffered = io.BytesIO()
                    image.save(buffered, format=&#34;PNG&#34;)
                    img_str = pybase64.b64encode(buffered.getvalue()).decode(&#34;utf-8&#34;)
                    image_data = f&#34;data:image/png;base64,{img_str}&#34;
                else:
                    continue

                # Extract the question
                question = example.get(&#34;question&#34;)

                # Construct the prompt
                prompt = f&#34;Question: {question}\n\nAnswer: &#34;
                if apply_chat_template:
                    try:
                        prompt = tokenizer.apply_chat_template(
                            [
                                {
                                    &#34;role&#34;: &#34;user&#34;,
                                    &#34;content&#34;: [
                                        {
                                            &#34;type&#34;: &#34;image_url&#34;,
                                            &#34;image_url&#34;: {&#34;url&#34;: image_data},
                                        },
                                        {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: prompt},
                                    ],
                                }
                            ],
                            add_generation_prompt=True,
                            tokenize=False,
                        )
                    except Exception as e:
                        # Note (Xinyuan): This is a workaround for an issue where some tokenizers do not support content as a list. (e.g. InternVL)
                        print(
                            f&#34;Error applying chat template: {e}, fallback to &lt;image&gt; tag&#34;
                        )
                        prompt = f&#34;&lt;image&gt;{prompt}&#34;

                # Calculate token lengths for text only (without image data)
                prompt_token_ids = tokenizer.encode(prompt)
                prompt_len = len(prompt_token_ids)

                output_len = fixed_output_len if fixed_output_len is not None else 256

                filtered_dataset.append(
                    DatasetRow(
                        prompt=prompt,
                        prompt_len=prompt_len,
                        output_len=output_len,
                        image_data=[image_data],
                    )
                )

        except Exception as e:
            print(f&#34;Error processing example {i}: {e}&#34;)

    print(f&#34;\nCreated {len(filtered_dataset)} MMMU prompts&#34;)
    return filtered_dataset</code></pre>
</details>
<div class="desc"><p>Sample requests from the MMMU dataset using HuggingFace datasets.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_requests</code></strong></dt>
<dd>Number of requests to sample.</dd>
<dt><strong><code>tokenizer</code></strong></dt>
<dd>Tokenizer to use for token counting.</dd>
<dt><strong><code>fixed_output_len</code></strong></dt>
<dd>If provided, use this fixed output length for all requests.</dd>
<dt><strong><code>apply_chat_template</code></strong></dt>
<dd>Whether to apply the chat template to the prompt.</dd>
<dt><strong><code>random_sample</code></strong></dt>
<dd>Whether to randomly sample or take the first N.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>List of tuples (prompt, prompt_token_len, output_token_len).</p></div>
</dd>
<dt id="sglang.bench_serving.sample_random_image_requests"><code class="name flex">
<span>def <span class="ident">sample_random_image_requests</span></span>(<span>num_requests: int,<br>num_images: int,<br>input_len: int,<br>output_len: int,<br>range_ratio: float,<br>tokenizer: transformers.tokenization_utils_base.PreTrainedTokenizerBase,<br>apply_chat_template: bool = True,<br>image_resolution: str = '1080p') ‑> List[<a title="sglang.bench_serving.DatasetRow" href="#sglang.bench_serving.DatasetRow">DatasetRow</a>]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_random_image_requests(
    num_requests: int,
    num_images: int,
    input_len: int,
    output_len: int,
    range_ratio: float,
    tokenizer: PreTrainedTokenizerBase,
    apply_chat_template: bool = True,
    image_resolution: str = &#34;1080p&#34;,
) -&gt; List[DatasetRow]:
    &#34;&#34;&#34;Generate requests with random images.

    - Each request includes ``num_images`` random images.
    - Supported resolutions: 4k (3840x2160), 1080p (1920x1080), 720p (1280x720), 360p (640x360),
      or custom &#39;heightxwidth&#39; (e.g., 1080x1920).
    - Text lengths follow the &#39;random&#39; dataset sampling rule. ``prompt_len``
      only counts text tokens and excludes image data.
    &#34;&#34;&#34;
    try:
        import pybase64
        from PIL import Image
    except ImportError as e:
        raise ImportError(
            &#34;Please install Pillow to generate random images: pip install pillow&#34;
        ) from e

    # Parse resolution (supports presets and &#39;heightxwidth&#39;)
    width, height = parse_random_image_resolution(image_resolution)

    # Check for potentially problematic combinations and warn user
    if width * height &gt;= 1920 * 1080 and num_images * num_requests &gt;= 100:
        warnings.warn(
            f&#34;High resolution ({width}x{height}) with {num_images * num_requests} total images &#34;
            f&#34;may take a long time. Consider reducing resolution or image count.&#34;,
            UserWarning,
            stacklevel=2,
        )

    # Sample text lengths
    input_lens = np.random.randint(
        max(int(input_len * range_ratio), 1), input_len + 1, size=num_requests
    )
    output_lens = np.random.randint(
        int(output_len * range_ratio), output_len + 1, size=num_requests
    )

    def _gen_random_image_data_uri(width: int = width, height: int = height) -&gt; str:
        arr = (np.random.rand(height, width, 3) * 255).astype(np.uint8)
        img = Image.fromarray(arr, mode=&#34;RGB&#34;)
        buf = io.BytesIO()
        img.save(buf, format=&#34;JPEG&#34;, quality=85)
        encoded = pybase64.b64encode(buf.getvalue()).decode(&#34;utf-8&#34;)
        return f&#34;data:image/jpeg;base64,{encoded}&#34;

    dataset: List[DatasetRow] = []
    for i in range(num_requests):
        # Generate text prompt
        text_prompt = gen_prompt(tokenizer, int(input_lens[i]))

        # Generate image list
        images = [_gen_random_image_data_uri() for _ in range(num_images)]

        prompt_str = text_prompt
        if apply_chat_template:
            try:
                content_items = [
                    {&#34;type&#34;: &#34;image_url&#34;, &#34;image_url&#34;: {&#34;url&#34;: img_url}}
                    for img_url in images
                ]
                content_items.append({&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: text_prompt})
                prompt_str = tokenizer.apply_chat_template(
                    [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: content_items}],
                    add_generation_prompt=True,
                    tokenize=False,
                )
            except Exception:
                # Some tokenizers do not support list content; fall back to a placeholder in the text
                prompt_str = f&#34;&lt;image&gt;{text_prompt}&#34;

        prompt_token_ids = tokenizer.encode(prompt_str)
        prompt_token_len = len(prompt_token_ids)

        dataset.append(
            DatasetRow(
                prompt=prompt_str,
                prompt_len=prompt_token_len,
                output_len=int(output_lens[i]),
                image_data=images,
            )
        )

    print(f&#34;#Input tokens: {np.sum([x.prompt_len for x in dataset])}&#34;)
    print(f&#34;#Output tokens: {np.sum([x.output_len for x in dataset])}&#34;)
    return dataset</code></pre>
</details>
<div class="desc"><p>Generate requests with random images.</p>
<ul>
<li>Each request includes <code>num_images</code> random images.</li>
<li>Supported resolutions: 4k (3840x2160), 1080p (1920x1080), 720p (1280x720), 360p (640x360),
or custom 'heightxwidth' (e.g., 1080x1920).</li>
<li>Text lengths follow the 'random' dataset sampling rule. <code>prompt_len</code>
only counts text tokens and excludes image data.</li>
</ul></div>
</dd>
<dt id="sglang.bench_serving.sample_random_requests"><code class="name flex">
<span>def <span class="ident">sample_random_requests</span></span>(<span>input_len: int,<br>output_len: int,<br>num_prompts: int,<br>range_ratio: float,<br>tokenizer: transformers.tokenization_utils_base.PreTrainedTokenizerBase,<br>dataset_path: str,<br>random_sample: bool = True,<br>return_text: bool = True) ‑> List[<a title="sglang.bench_serving.DatasetRow" href="#sglang.bench_serving.DatasetRow">DatasetRow</a>]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_random_requests(
    input_len: int,
    output_len: int,
    num_prompts: int,
    range_ratio: float,
    tokenizer: PreTrainedTokenizerBase,
    dataset_path: str,
    random_sample: bool = True,
    return_text: bool = True,
) -&gt; List[DatasetRow]:
    input_lens = np.random.randint(
        max(int(input_len * range_ratio), 1),
        input_len + 1,
        size=num_prompts,
    )
    output_lens = np.random.randint(
        int(output_len * range_ratio),
        output_len + 1,
        size=num_prompts,
    )

    if random_sample:
        # Sample token ids from ShareGPT and repeat/truncate them to satisfy the input_lens

        # Download sharegpt if necessary
        if not is_file_valid_json(dataset_path):
            dataset_path = download_and_cache_file(SHAREGPT_URL)

        # Load the dataset.
        with open(dataset_path) as f:
            dataset = json.load(f)
        # Filter out the conversations with less than 2 turns.
        dataset = [
            data
            for data in dataset
            if len(data.get(&#34;conversations&#34;, data.get(&#34;conversation&#34;, []))) &gt;= 2
        ]
        # Only keep the first two turns of each conversation.
        dataset = [
            (
                data.get(&#34;conversations&#34;, data.get(&#34;conversation&#34;, []))[0][&#34;value&#34;],
                data.get(&#34;conversations&#34;, data.get(&#34;conversation&#34;, []))[1][&#34;value&#34;],
            )
            for data in dataset
        ]
        # Shuffle the dataset.
        random.shuffle(dataset)

        # Filter out sequences that are too long or too short
        input_requests: List[DatasetRow] = []
        for data in dataset:
            i = len(input_requests)
            if i == num_prompts:
                break

            # Tokenize the prompts and completions.
            prompt = data[0]
            prompt_token_ids = tokenizer.encode(prompt)
            prompt_len = len(prompt_token_ids)

            # Skip empty prompt
            if prompt_len == 0:
                continue

            if prompt_len &gt; input_lens[i]:
                input_ids = prompt_token_ids[: input_lens[i]]
            else:
                ratio = (input_lens[i] + prompt_len - 1) // prompt_len
                input_ids = (prompt_token_ids * ratio)[: input_lens[i]]
            input_content = input_ids
            if return_text:
                input_content = tokenizer.decode(input_content)
            input_requests.append(
                DatasetRow(
                    prompt=input_content,
                    prompt_len=int(input_lens[i]),
                    output_len=int(output_lens[i]),
                )
            )
    else:
        # Sample token ids from random integers. This can cause some NaN issues.
        offsets = np.random.randint(0, tokenizer.vocab_size, size=num_prompts)
        input_requests = []
        for i in range(num_prompts):
            input_content = [
                (offsets[i] + i + j) % tokenizer.vocab_size
                for j in range(input_lens[i])
            ]
            if return_text:
                input_content = tokenizer.decode(input_content)
            input_requests.append(
                DatasetRow(
                    prompt=input_content,
                    prompt_len=int(input_lens[i]),
                    output_len=int(output_lens[i]),
                )
            )

    print(f&#34;#Input tokens: {np.sum(input_lens)}&#34;)
    print(f&#34;#Output tokens: {np.sum(output_lens)}&#34;)
    return input_requests</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.sample_sharegpt_requests"><code class="name flex">
<span>def <span class="ident">sample_sharegpt_requests</span></span>(<span>dataset_path: str,<br>num_requests: int,<br>tokenizer: transformers.tokenization_utils_base.PreTrainedTokenizerBase,<br>fixed_output_len: int | None = None,<br>context_len: int | None = None,<br>prompt_suffix: str | None = '',<br>apply_chat_template=False) ‑> List[<a title="sglang.bench_serving.DatasetRow" href="#sglang.bench_serving.DatasetRow">DatasetRow</a>]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_sharegpt_requests(
    dataset_path: str,
    num_requests: int,
    tokenizer: PreTrainedTokenizerBase,
    fixed_output_len: Optional[int] = None,
    context_len: Optional[int] = None,
    prompt_suffix: Optional[str] = &#34;&#34;,
    apply_chat_template=False,
) -&gt; List[DatasetRow]:
    if fixed_output_len is not None and fixed_output_len &lt; 4:
        raise ValueError(&#34;output_len too small&#34;)

    # Download sharegpt if necessary
    if not is_file_valid_json(dataset_path) and dataset_path == &#34;&#34;:
        dataset_path = download_and_cache_file(SHAREGPT_URL)

    # Load the dataset.
    with open(dataset_path) as f:
        dataset = json.load(f)

    # Filter out the conversations with less than 2 turns.
    dataset = [
        data
        for data in dataset
        if len(data.get(&#34;conversations&#34;, data.get(&#34;conversation&#34;, []))) &gt;= 2
    ]
    # Only keep the first two turns of each conversation.
    dataset = [
        (
            data.get(&#34;conversations&#34;, data.get(&#34;conversation&#34;, []))[0][&#34;value&#34;],
            data.get(&#34;conversations&#34;, data.get(&#34;conversation&#34;, []))[1][&#34;value&#34;],
        )
        for data in dataset
    ]

    # Shuffle the dataset.
    random.shuffle(dataset)

    # Filter out sequences that are too long or too short
    filtered_dataset: List[DatasetRow] = []
    for i in range(len(dataset)):
        if len(filtered_dataset) == num_requests:
            break

        # Tokenize the prompts and completions.
        prompt = dataset[i][0]
        if prompt_suffix:
            prompt = (
                remove_suffix(prompt, ASSISTANT_SUFFIX)
                + prompt_suffix
                + ASSISTANT_SUFFIX
            )

        if apply_chat_template:
            prompt = tokenizer.apply_chat_template(
                [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}],
                add_generation_prompt=True,
                tokenize=False,
            )
            prompt = prompt.replace(tokenizer.bos_token, &#34;&#34;)

        prompt_token_ids = tokenizer.encode(prompt)
        completion = dataset[i][1]
        completion_token_ids = tokenizer.encode(completion)
        prompt_len = len(prompt_token_ids)
        output_len = (
            len(completion_token_ids) if fixed_output_len is None else fixed_output_len
        )

        if prompt_len &lt; 2 or output_len &lt; 2:
            # Prune too short sequences.
            continue

        if context_len and prompt_len + output_len &gt; context_len:
            # Prune too long sequences.
            continue

        filtered_dataset.append(
            DatasetRow(prompt=prompt, prompt_len=prompt_len, output_len=output_len)
        )

    print(f&#34;#Input tokens: {np.sum([x.prompt_len for x in filtered_dataset])}&#34;)
    print(f&#34;#Output tokens: {np.sum([x.output_len for x in filtered_dataset])}&#34;)
    return filtered_dataset</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.set_global_args"><code class="name flex">
<span>def <span class="ident">set_global_args</span></span>(<span>args_: argparse.Namespace)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_global_args(args_: argparse.Namespace):
    &#34;&#34;&#34;Set the global args.&#34;&#34;&#34;
    global args
    args = args_</code></pre>
</details>
<div class="desc"><p>Set the global args.</p></div>
</dd>
<dt id="sglang.bench_serving.set_ulimit"><code class="name flex">
<span>def <span class="ident">set_ulimit</span></span>(<span>target_soft_limit=65535)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_ulimit(target_soft_limit=65535):
    resource_type = resource.RLIMIT_NOFILE
    current_soft, current_hard = resource.getrlimit(resource_type)

    if current_soft &lt; target_soft_limit:
        try:
            resource.setrlimit(resource_type, (target_soft_limit, current_hard))
        except ValueError as e:
            print(f&#34;Fail to set RLIMIT_NOFILE: {e}&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.bench_serving.BenchmarkMetrics"><code class="flex name class">
<span>class <span class="ident">BenchmarkMetrics</span></span>
<span>(</span><span>completed: int,<br>total_input: int,<br>total_output: int,<br>total_output_retokenized: int,<br>request_throughput: float,<br>input_throughput: float,<br>output_throughput: float,<br>output_throughput_retokenized: float,<br>total_throughput: float,<br>total_throughput_retokenized: float,<br>mean_ttft_ms: float,<br>median_ttft_ms: float,<br>std_ttft_ms: float,<br>p99_ttft_ms: float,<br>mean_tpot_ms: float,<br>median_tpot_ms: float,<br>std_tpot_ms: float,<br>p99_tpot_ms: float,<br>mean_itl_ms: float,<br>median_itl_ms: float,<br>std_itl_ms: float,<br>p95_itl_ms: float,<br>p99_itl_ms: float,<br>max_itl_ms: float,<br>mean_e2e_latency_ms: float,<br>median_e2e_latency_ms: float,<br>std_e2e_latency_ms: float,<br>p99_e2e_latency_ms: float,<br>concurrency: float)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class BenchmarkMetrics:
    completed: int
    total_input: int
    total_output: int
    total_output_retokenized: int
    request_throughput: float
    input_throughput: float
    output_throughput: float
    output_throughput_retokenized: float
    total_throughput: float
    total_throughput_retokenized: float
    mean_ttft_ms: float
    median_ttft_ms: float
    std_ttft_ms: float
    p99_ttft_ms: float
    mean_tpot_ms: float
    median_tpot_ms: float
    std_tpot_ms: float
    p99_tpot_ms: float
    mean_itl_ms: float
    median_itl_ms: float
    std_itl_ms: float
    p95_itl_ms: float
    p99_itl_ms: float
    max_itl_ms: float
    mean_e2e_latency_ms: float
    median_e2e_latency_ms: float
    std_e2e_latency_ms: float
    p99_e2e_latency_ms: float
    concurrency: float</code></pre>
</details>
<div class="desc"><p>BenchmarkMetrics(completed: int, total_input: int, total_output: int, total_output_retokenized: int, request_throughput: float, input_throughput: float, output_throughput: float, output_throughput_retokenized: float, total_throughput: float, total_throughput_retokenized: float, mean_ttft_ms: float, median_ttft_ms: float, std_ttft_ms: float, p99_ttft_ms: float, mean_tpot_ms: float, median_tpot_ms: float, std_tpot_ms: float, p99_tpot_ms: float, mean_itl_ms: float, median_itl_ms: float, std_itl_ms: float, p95_itl_ms: float, p99_itl_ms: float, max_itl_ms: float, mean_e2e_latency_ms: float, median_e2e_latency_ms: float, std_e2e_latency_ms: float, p99_e2e_latency_ms: float, concurrency: float)</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.bench_serving.BenchmarkMetrics.completed"><code class="name">var <span class="ident">completed</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.concurrency"><code class="name">var <span class="ident">concurrency</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.input_throughput"><code class="name">var <span class="ident">input_throughput</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.max_itl_ms"><code class="name">var <span class="ident">max_itl_ms</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.mean_e2e_latency_ms"><code class="name">var <span class="ident">mean_e2e_latency_ms</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.mean_itl_ms"><code class="name">var <span class="ident">mean_itl_ms</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.mean_tpot_ms"><code class="name">var <span class="ident">mean_tpot_ms</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.mean_ttft_ms"><code class="name">var <span class="ident">mean_ttft_ms</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.median_e2e_latency_ms"><code class="name">var <span class="ident">median_e2e_latency_ms</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.median_itl_ms"><code class="name">var <span class="ident">median_itl_ms</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.median_tpot_ms"><code class="name">var <span class="ident">median_tpot_ms</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.median_ttft_ms"><code class="name">var <span class="ident">median_ttft_ms</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.output_throughput"><code class="name">var <span class="ident">output_throughput</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.output_throughput_retokenized"><code class="name">var <span class="ident">output_throughput_retokenized</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.p95_itl_ms"><code class="name">var <span class="ident">p95_itl_ms</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.p99_e2e_latency_ms"><code class="name">var <span class="ident">p99_e2e_latency_ms</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.p99_itl_ms"><code class="name">var <span class="ident">p99_itl_ms</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.p99_tpot_ms"><code class="name">var <span class="ident">p99_tpot_ms</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.p99_ttft_ms"><code class="name">var <span class="ident">p99_ttft_ms</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.request_throughput"><code class="name">var <span class="ident">request_throughput</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.std_e2e_latency_ms"><code class="name">var <span class="ident">std_e2e_latency_ms</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.std_itl_ms"><code class="name">var <span class="ident">std_itl_ms</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.std_tpot_ms"><code class="name">var <span class="ident">std_tpot_ms</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.std_ttft_ms"><code class="name">var <span class="ident">std_ttft_ms</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.total_input"><code class="name">var <span class="ident">total_input</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.total_output"><code class="name">var <span class="ident">total_output</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.total_output_retokenized"><code class="name">var <span class="ident">total_output_retokenized</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.total_throughput"><code class="name">var <span class="ident">total_throughput</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.BenchmarkMetrics.total_throughput_retokenized"><code class="name">var <span class="ident">total_throughput_retokenized</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.bench_serving.DatasetRow"><code class="flex name class">
<span>class <span class="ident">DatasetRow</span></span>
<span>(</span><span>prompt: str,<br>prompt_len: int,<br>output_len: int,<br>image_data: List[str] | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class DatasetRow:
    prompt: str
    prompt_len: int
    output_len: int
    image_data: Optional[List[str]] = None</code></pre>
</details>
<div class="desc"><p>DatasetRow(prompt: str, prompt_len: int, output_len: int, image_data: Optional[List[str]] = None)</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.bench_serving.DatasetRow.image_data"><code class="name">var <span class="ident">image_data</span> : List[str] | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.DatasetRow.output_len"><code class="name">var <span class="ident">output_len</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.DatasetRow.prompt"><code class="name">var <span class="ident">prompt</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.DatasetRow.prompt_len"><code class="name">var <span class="ident">prompt_len</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.bench_serving.LoRAPathAction"><code class="flex name class">
<span>class <span class="ident">LoRAPathAction</span></span>
<span>(</span><span>option_strings,<br>dest,<br>nargs=None,<br>const=None,<br>default=None,<br>type=None,<br>choices=None,<br>required=False,<br>help=None,<br>metavar=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LoRAPathAction(argparse.Action):
    def __call__(self, parser, namespace, values, option_string=None):
        setattr(namespace, self.dest, [])
        for lora_name in values:
            getattr(namespace, self.dest).append(lora_name)</code></pre>
</details>
<div class="desc"><p>Information about how to convert command line strings to Python objects.</p>
<p>Action objects are used by an ArgumentParser to represent the information
needed to parse a single argument from one or more strings from the
command line. The keyword arguments to the Action constructor are also
all attributes of Action instances.</p>
<p>Keyword Arguments:</p>
<pre><code>- option_strings -- A list of command-line option strings which
    should be associated with this action.

- dest -- The name of the attribute to hold the created object(s)

- nargs -- The number of command-line arguments that should be
    consumed. By default, one argument will be consumed and a single
    value will be produced.  Other values include:
        - N (an integer) consumes N arguments (and produces a list)
        - '?' consumes zero or one arguments
        - '*' consumes zero or more arguments (and produces a list)
        - '+' consumes one or more arguments (and produces a list)
    Note that the difference between the default and nargs=1 is that
    with the default, a single value will be produced, while with
    nargs=1, a list containing a single value will be produced.

- const -- The value to be produced if the option is specified and the
    option uses an action that takes no values.

- default -- The value to be produced if the option is not specified.

- type -- A callable that accepts a single string argument, and
    returns the converted value.  The standard Python types str, int,
    float, and complex are useful examples of such callables.  If None,
    str is used.

- choices -- A container of values that should be allowed. If not None,
    after a command-line argument has been converted to the appropriate
    type, an exception will be raised if it is not a member of this
    collection.

- required -- True if the action must always be specified at the
    command line. This is only meaningful for optional command-line
    arguments.

- help -- The help string describing the argument.

- metavar -- The name to be used for the option's argument with the
    help string. If None, the 'dest' value will be used as the name.
</code></pre></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>argparse.Action</li>
<li>argparse._AttributeHolder</li>
</ul>
</dd>
<dt id="sglang.bench_serving.RequestFuncInput"><code class="flex name class">
<span>class <span class="ident">RequestFuncInput</span></span>
<span>(</span><span>prompt: str,<br>api_url: str,<br>prompt_len: int,<br>output_len: int,<br>model: str,<br>lora_name: str,<br>image_data: List[str] | None,<br>extra_request_body: Dict[str, Any])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class RequestFuncInput:
    prompt: str
    api_url: str
    prompt_len: int
    output_len: int
    model: str
    lora_name: str
    image_data: Optional[List[str]]
    extra_request_body: Dict[str, Any]</code></pre>
</details>
<div class="desc"><p>RequestFuncInput(prompt: str, api_url: str, prompt_len: int, output_len: int, model: str, lora_name: str, image_data: Optional[List[str]], extra_request_body: Dict[str, Any])</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.bench_serving.RequestFuncInput.api_url"><code class="name">var <span class="ident">api_url</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.RequestFuncInput.extra_request_body"><code class="name">var <span class="ident">extra_request_body</span> : Dict[str, Any]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.RequestFuncInput.image_data"><code class="name">var <span class="ident">image_data</span> : List[str] | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.RequestFuncInput.lora_name"><code class="name">var <span class="ident">lora_name</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.RequestFuncInput.model"><code class="name">var <span class="ident">model</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.RequestFuncInput.output_len"><code class="name">var <span class="ident">output_len</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.RequestFuncInput.prompt"><code class="name">var <span class="ident">prompt</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.RequestFuncInput.prompt_len"><code class="name">var <span class="ident">prompt_len</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.bench_serving.RequestFuncOutput"><code class="flex name class">
<span>class <span class="ident">RequestFuncOutput</span></span>
<span>(</span><span>generated_text: str = '',<br>success: bool = False,<br>latency: float = 0.0,<br>ttft: float = 0.0,<br>itl: List[float] = &lt;factory&gt;,<br>prompt_len: int = 0,<br>error: str = '',<br>output_len: int = 0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class RequestFuncOutput:
    generated_text: str = &#34;&#34;
    success: bool = False
    latency: float = 0.0
    ttft: float = 0.0  # Time to first token
    itl: List[float] = field(default_factory=list)  # List of inter-token latencies
    prompt_len: int = 0
    error: str = &#34;&#34;
    output_len: int = 0

    @staticmethod
    def init_new(request_func_input: RequestFuncInput):
        output = RequestFuncOutput()
        output.prompt_len = request_func_input.prompt_len
        return output</code></pre>
</details>
<div class="desc"><p>RequestFuncOutput(generated_text: str = '', success: bool = False, latency: float = 0.0, ttft: float = 0.0, itl: List[float] = <factory>, prompt_len: int = 0, error: str = '', output_len: int = 0)</p></div>
<h3>Static methods</h3>
<dl>
<dt id="sglang.bench_serving.RequestFuncOutput.init_new"><code class="name flex">
<span>def <span class="ident">init_new</span></span>(<span>request_func_input: <a title="sglang.bench_serving.RequestFuncInput" href="#sglang.bench_serving.RequestFuncInput">RequestFuncInput</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def init_new(request_func_input: RequestFuncInput):
    output = RequestFuncOutput()
    output.prompt_len = request_func_input.prompt_len
    return output</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.bench_serving.RequestFuncOutput.error"><code class="name">var <span class="ident">error</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.RequestFuncOutput.generated_text"><code class="name">var <span class="ident">generated_text</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.RequestFuncOutput.itl"><code class="name">var <span class="ident">itl</span> : List[float]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.RequestFuncOutput.latency"><code class="name">var <span class="ident">latency</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.RequestFuncOutput.output_len"><code class="name">var <span class="ident">output_len</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.RequestFuncOutput.prompt_len"><code class="name">var <span class="ident">prompt_len</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.RequestFuncOutput.success"><code class="name">var <span class="ident">success</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.bench_serving.RequestFuncOutput.ttft"><code class="name">var <span class="ident">ttft</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang" href="index.html">sglang</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.bench_serving.async_request_gserver" href="#sglang.bench_serving.async_request_gserver">async_request_gserver</a></code></li>
<li><code><a title="sglang.bench_serving.async_request_openai_chat_completions" href="#sglang.bench_serving.async_request_openai_chat_completions">async_request_openai_chat_completions</a></code></li>
<li><code><a title="sglang.bench_serving.async_request_openai_completions" href="#sglang.bench_serving.async_request_openai_completions">async_request_openai_completions</a></code></li>
<li><code><a title="sglang.bench_serving.async_request_profile" href="#sglang.bench_serving.async_request_profile">async_request_profile</a></code></li>
<li><code><a title="sglang.bench_serving.async_request_sglang_generate" href="#sglang.bench_serving.async_request_sglang_generate">async_request_sglang_generate</a></code></li>
<li><code><a title="sglang.bench_serving.async_request_trt_llm" href="#sglang.bench_serving.async_request_trt_llm">async_request_trt_llm</a></code></li>
<li><code><a title="sglang.bench_serving.async_request_truss" href="#sglang.bench_serving.async_request_truss">async_request_truss</a></code></li>
<li><code><a title="sglang.bench_serving.benchmark" href="#sglang.bench_serving.benchmark">benchmark</a></code></li>
<li><code><a title="sglang.bench_serving.calculate_metrics" href="#sglang.bench_serving.calculate_metrics">calculate_metrics</a></code></li>
<li><code><a title="sglang.bench_serving.check_chat_template" href="#sglang.bench_serving.check_chat_template">check_chat_template</a></code></li>
<li><code><a title="sglang.bench_serving.download_and_cache_file" href="#sglang.bench_serving.download_and_cache_file">download_and_cache_file</a></code></li>
<li><code><a title="sglang.bench_serving.gen_prompt" href="#sglang.bench_serving.gen_prompt">gen_prompt</a></code></li>
<li><code><a title="sglang.bench_serving.get_auth_headers" href="#sglang.bench_serving.get_auth_headers">get_auth_headers</a></code></li>
<li><code><a title="sglang.bench_serving.get_dataset" href="#sglang.bench_serving.get_dataset">get_dataset</a></code></li>
<li><code><a title="sglang.bench_serving.get_gen_prefix_cache_path" href="#sglang.bench_serving.get_gen_prefix_cache_path">get_gen_prefix_cache_path</a></code></li>
<li><code><a title="sglang.bench_serving.get_model" href="#sglang.bench_serving.get_model">get_model</a></code></li>
<li><code><a title="sglang.bench_serving.get_request" href="#sglang.bench_serving.get_request">get_request</a></code></li>
<li><code><a title="sglang.bench_serving.get_tokenizer" href="#sglang.bench_serving.get_tokenizer">get_tokenizer</a></code></li>
<li><code><a title="sglang.bench_serving.is_file_valid_json" href="#sglang.bench_serving.is_file_valid_json">is_file_valid_json</a></code></li>
<li><code><a title="sglang.bench_serving.parse_random_image_resolution" href="#sglang.bench_serving.parse_random_image_resolution">parse_random_image_resolution</a></code></li>
<li><code><a title="sglang.bench_serving.remove_prefix" href="#sglang.bench_serving.remove_prefix">remove_prefix</a></code></li>
<li><code><a title="sglang.bench_serving.remove_suffix" href="#sglang.bench_serving.remove_suffix">remove_suffix</a></code></li>
<li><code><a title="sglang.bench_serving.run_benchmark" href="#sglang.bench_serving.run_benchmark">run_benchmark</a></code></li>
<li><code><a title="sglang.bench_serving.sample_generated_shared_prefix_requests" href="#sglang.bench_serving.sample_generated_shared_prefix_requests">sample_generated_shared_prefix_requests</a></code></li>
<li><code><a title="sglang.bench_serving.sample_mmmu_requests" href="#sglang.bench_serving.sample_mmmu_requests">sample_mmmu_requests</a></code></li>
<li><code><a title="sglang.bench_serving.sample_random_image_requests" href="#sglang.bench_serving.sample_random_image_requests">sample_random_image_requests</a></code></li>
<li><code><a title="sglang.bench_serving.sample_random_requests" href="#sglang.bench_serving.sample_random_requests">sample_random_requests</a></code></li>
<li><code><a title="sglang.bench_serving.sample_sharegpt_requests" href="#sglang.bench_serving.sample_sharegpt_requests">sample_sharegpt_requests</a></code></li>
<li><code><a title="sglang.bench_serving.set_global_args" href="#sglang.bench_serving.set_global_args">set_global_args</a></code></li>
<li><code><a title="sglang.bench_serving.set_ulimit" href="#sglang.bench_serving.set_ulimit">set_ulimit</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.bench_serving.BenchmarkMetrics" href="#sglang.bench_serving.BenchmarkMetrics">BenchmarkMetrics</a></code></h4>
<ul class="">
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.completed" href="#sglang.bench_serving.BenchmarkMetrics.completed">completed</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.concurrency" href="#sglang.bench_serving.BenchmarkMetrics.concurrency">concurrency</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.input_throughput" href="#sglang.bench_serving.BenchmarkMetrics.input_throughput">input_throughput</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.max_itl_ms" href="#sglang.bench_serving.BenchmarkMetrics.max_itl_ms">max_itl_ms</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.mean_e2e_latency_ms" href="#sglang.bench_serving.BenchmarkMetrics.mean_e2e_latency_ms">mean_e2e_latency_ms</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.mean_itl_ms" href="#sglang.bench_serving.BenchmarkMetrics.mean_itl_ms">mean_itl_ms</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.mean_tpot_ms" href="#sglang.bench_serving.BenchmarkMetrics.mean_tpot_ms">mean_tpot_ms</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.mean_ttft_ms" href="#sglang.bench_serving.BenchmarkMetrics.mean_ttft_ms">mean_ttft_ms</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.median_e2e_latency_ms" href="#sglang.bench_serving.BenchmarkMetrics.median_e2e_latency_ms">median_e2e_latency_ms</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.median_itl_ms" href="#sglang.bench_serving.BenchmarkMetrics.median_itl_ms">median_itl_ms</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.median_tpot_ms" href="#sglang.bench_serving.BenchmarkMetrics.median_tpot_ms">median_tpot_ms</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.median_ttft_ms" href="#sglang.bench_serving.BenchmarkMetrics.median_ttft_ms">median_ttft_ms</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.output_throughput" href="#sglang.bench_serving.BenchmarkMetrics.output_throughput">output_throughput</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.output_throughput_retokenized" href="#sglang.bench_serving.BenchmarkMetrics.output_throughput_retokenized">output_throughput_retokenized</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.p95_itl_ms" href="#sglang.bench_serving.BenchmarkMetrics.p95_itl_ms">p95_itl_ms</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.p99_e2e_latency_ms" href="#sglang.bench_serving.BenchmarkMetrics.p99_e2e_latency_ms">p99_e2e_latency_ms</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.p99_itl_ms" href="#sglang.bench_serving.BenchmarkMetrics.p99_itl_ms">p99_itl_ms</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.p99_tpot_ms" href="#sglang.bench_serving.BenchmarkMetrics.p99_tpot_ms">p99_tpot_ms</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.p99_ttft_ms" href="#sglang.bench_serving.BenchmarkMetrics.p99_ttft_ms">p99_ttft_ms</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.request_throughput" href="#sglang.bench_serving.BenchmarkMetrics.request_throughput">request_throughput</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.std_e2e_latency_ms" href="#sglang.bench_serving.BenchmarkMetrics.std_e2e_latency_ms">std_e2e_latency_ms</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.std_itl_ms" href="#sglang.bench_serving.BenchmarkMetrics.std_itl_ms">std_itl_ms</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.std_tpot_ms" href="#sglang.bench_serving.BenchmarkMetrics.std_tpot_ms">std_tpot_ms</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.std_ttft_ms" href="#sglang.bench_serving.BenchmarkMetrics.std_ttft_ms">std_ttft_ms</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.total_input" href="#sglang.bench_serving.BenchmarkMetrics.total_input">total_input</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.total_output" href="#sglang.bench_serving.BenchmarkMetrics.total_output">total_output</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.total_output_retokenized" href="#sglang.bench_serving.BenchmarkMetrics.total_output_retokenized">total_output_retokenized</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.total_throughput" href="#sglang.bench_serving.BenchmarkMetrics.total_throughput">total_throughput</a></code></li>
<li><code><a title="sglang.bench_serving.BenchmarkMetrics.total_throughput_retokenized" href="#sglang.bench_serving.BenchmarkMetrics.total_throughput_retokenized">total_throughput_retokenized</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.bench_serving.DatasetRow" href="#sglang.bench_serving.DatasetRow">DatasetRow</a></code></h4>
<ul class="">
<li><code><a title="sglang.bench_serving.DatasetRow.image_data" href="#sglang.bench_serving.DatasetRow.image_data">image_data</a></code></li>
<li><code><a title="sglang.bench_serving.DatasetRow.output_len" href="#sglang.bench_serving.DatasetRow.output_len">output_len</a></code></li>
<li><code><a title="sglang.bench_serving.DatasetRow.prompt" href="#sglang.bench_serving.DatasetRow.prompt">prompt</a></code></li>
<li><code><a title="sglang.bench_serving.DatasetRow.prompt_len" href="#sglang.bench_serving.DatasetRow.prompt_len">prompt_len</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.bench_serving.LoRAPathAction" href="#sglang.bench_serving.LoRAPathAction">LoRAPathAction</a></code></h4>
</li>
<li>
<h4><code><a title="sglang.bench_serving.RequestFuncInput" href="#sglang.bench_serving.RequestFuncInput">RequestFuncInput</a></code></h4>
<ul class="two-column">
<li><code><a title="sglang.bench_serving.RequestFuncInput.api_url" href="#sglang.bench_serving.RequestFuncInput.api_url">api_url</a></code></li>
<li><code><a title="sglang.bench_serving.RequestFuncInput.extra_request_body" href="#sglang.bench_serving.RequestFuncInput.extra_request_body">extra_request_body</a></code></li>
<li><code><a title="sglang.bench_serving.RequestFuncInput.image_data" href="#sglang.bench_serving.RequestFuncInput.image_data">image_data</a></code></li>
<li><code><a title="sglang.bench_serving.RequestFuncInput.lora_name" href="#sglang.bench_serving.RequestFuncInput.lora_name">lora_name</a></code></li>
<li><code><a title="sglang.bench_serving.RequestFuncInput.model" href="#sglang.bench_serving.RequestFuncInput.model">model</a></code></li>
<li><code><a title="sglang.bench_serving.RequestFuncInput.output_len" href="#sglang.bench_serving.RequestFuncInput.output_len">output_len</a></code></li>
<li><code><a title="sglang.bench_serving.RequestFuncInput.prompt" href="#sglang.bench_serving.RequestFuncInput.prompt">prompt</a></code></li>
<li><code><a title="sglang.bench_serving.RequestFuncInput.prompt_len" href="#sglang.bench_serving.RequestFuncInput.prompt_len">prompt_len</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.bench_serving.RequestFuncOutput" href="#sglang.bench_serving.RequestFuncOutput">RequestFuncOutput</a></code></h4>
<ul class="two-column">
<li><code><a title="sglang.bench_serving.RequestFuncOutput.error" href="#sglang.bench_serving.RequestFuncOutput.error">error</a></code></li>
<li><code><a title="sglang.bench_serving.RequestFuncOutput.generated_text" href="#sglang.bench_serving.RequestFuncOutput.generated_text">generated_text</a></code></li>
<li><code><a title="sglang.bench_serving.RequestFuncOutput.init_new" href="#sglang.bench_serving.RequestFuncOutput.init_new">init_new</a></code></li>
<li><code><a title="sglang.bench_serving.RequestFuncOutput.itl" href="#sglang.bench_serving.RequestFuncOutput.itl">itl</a></code></li>
<li><code><a title="sglang.bench_serving.RequestFuncOutput.latency" href="#sglang.bench_serving.RequestFuncOutput.latency">latency</a></code></li>
<li><code><a title="sglang.bench_serving.RequestFuncOutput.output_len" href="#sglang.bench_serving.RequestFuncOutput.output_len">output_len</a></code></li>
<li><code><a title="sglang.bench_serving.RequestFuncOutput.prompt_len" href="#sglang.bench_serving.RequestFuncOutput.prompt_len">prompt_len</a></code></li>
<li><code><a title="sglang.bench_serving.RequestFuncOutput.success" href="#sglang.bench_serving.RequestFuncOutput.success">success</a></code></li>
<li><code><a title="sglang.bench_serving.RequestFuncOutput.ttft" href="#sglang.bench_serving.RequestFuncOutput.ttft">ttft</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
