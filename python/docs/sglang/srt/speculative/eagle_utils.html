<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.speculative.eagle_utils API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.speculative.eagle_utils</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.speculative.eagle_utils.create_accept_length_filter"><code class="name flex">
<span>def <span class="ident">create_accept_length_filter</span></span>(<span>accept_length: torch.Tensor,<br>unfinished_index_device: torch.Tensor,<br>seq_lens: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.compile(dynamic=True)
def create_accept_length_filter(
    accept_length: torch.Tensor,
    unfinished_index_device: torch.Tensor,
    seq_lens: torch.Tensor,
):
    accept_length_filter = torch.zeros_like(accept_length)
    accept_length_filter[unfinished_index_device] = (
        accept_length[unfinished_index_device] + 1
    )
    seq_lens.add_(accept_length + 1)
    return accept_length_filter</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.generate_token_bitmask"><code class="name flex">
<span>def <span class="ident">generate_token_bitmask</span></span>(<span>reqs: List[Req],<br>verify_input: <a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput">EagleVerifyInput</a>,<br>retrieve_next_token_cpu: torch.Tensor,<br>retrieve_next_sibling_cpu: torch.Tensor,<br>draft_tokens_cpu: torch.Tensor,<br>vocab_size: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_token_bitmask(
    reqs: List[Req],
    verify_input: EagleVerifyInput,
    retrieve_next_token_cpu: torch.Tensor,
    retrieve_next_sibling_cpu: torch.Tensor,
    draft_tokens_cpu: torch.Tensor,
    vocab_size: int,
):
    &#34;&#34;&#34;
    Generate the logit mask for structured output.
    Draft model&#39;s token can be either valid or invalid with respect to the grammar.
    We need to perform DFS to
    1. figure out which tokens are accepted by the grammar.
    2. if so, what is the corresponding logit mask.
    &#34;&#34;&#34;

    num_draft_tokens = draft_tokens_cpu.shape[-1]

    allocate_token_bitmask = None
    assert len(reqs) == retrieve_next_token_cpu.shape[0]
    grammar = None
    for i, req in enumerate(reqs):
        if req.grammar is not None:
            if allocate_token_bitmask is None:
                allocate_token_bitmask = req.grammar.allocate_vocab_mask(
                    vocab_size=vocab_size,
                    batch_size=draft_tokens_cpu.numel(),
                    device=&#34;cpu&#34;,
                )
            grammar = req.grammar
            s = time.perf_counter()
            traverse_tree(
                retrieve_next_token_cpu[i],
                retrieve_next_sibling_cpu[i],
                draft_tokens_cpu[i],
                req.grammar,
                allocate_token_bitmask[
                    i * num_draft_tokens : (i + 1) * num_draft_tokens
                ],
            )
            tree_traverse_time = time.perf_counter() - s
            if tree_traverse_time &gt; TREE_TRAVERSE_TIME_THRESHOLD:
                logger.warning(
                    f&#34;Bit mask generation took {tree_traverse_time} seconds with &#34;
                    f&#34;grammar: {req.grammar}&#34;
                )

    verify_input.grammar = grammar
    return allocate_token_bitmask</code></pre>
</details>
<div class="desc"><p>Generate the logit mask for structured output.
Draft model's token can be either valid or invalid with respect to the grammar.
We need to perform DFS to
1. figure out which tokens are accepted by the grammar.
2. if so, what is the corresponding logit mask.</p></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.get_src_tgt_cache_loc"><code class="name flex">
<span>def <span class="ident">get_src_tgt_cache_loc</span></span>(<span>seq_lens: torch.Tensor,<br>out_cache_loc: torch.Tensor,<br>accept_index: torch.Tensor,<br>accept_length: torch.Tensor,<br>draft_token_num: int,<br>page_size: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.compile(dynamic=True)
def get_src_tgt_cache_loc(
    seq_lens: torch.Tensor,
    out_cache_loc: torch.Tensor,
    accept_index: torch.Tensor,
    accept_length: torch.Tensor,
    draft_token_num: int,
    page_size: int,
):
    src_cache_loc = out_cache_loc[accept_index]
    tgt_cache_loc = torch.empty_like(src_cache_loc)
    extended_len = seq_lens + draft_token_num
    keep_len = torch.minimum(
        (seq_lens + accept_length + 1 + page_size - 1) // page_size * page_size,
        extended_len,
    )
    to_free_num_slots = extended_len - keep_len
    return src_cache_loc, tgt_cache_loc, to_free_num_slots</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.select_top_k_tokens"><code class="name flex">
<span>def <span class="ident">select_top_k_tokens</span></span>(<span>i: int,<br>topk_p: torch.Tensor,<br>topk_index: torch.Tensor,<br>hidden_states: torch.Tensor,<br>scores: torch.Tensor,<br>topk: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.compile(dynamic=True)
def select_top_k_tokens(
    i: int,
    topk_p: torch.Tensor,
    topk_index: torch.Tensor,
    hidden_states: torch.Tensor,
    scores: torch.Tensor,
    topk: int,
):
    if i == 0:
        # The first step after extend
        input_ids = topk_index.flatten()
        hidden_states = hidden_states.repeat_interleave(topk, dim=0)
        scores = topk_p  # shape: (b, topk)

        tree_info = (
            topk_p.unsqueeze(1),  # shape: (b, 1, topk)
            topk_index,  # shape: (b, topk)
            torch.arange(-1, topk, dtype=torch.long, device=&#34;cuda&#34;)
            .unsqueeze(0)
            .repeat(topk_p.shape[0], 1),  # shape: (b, topk + 1)
        )
    else:
        # The later decode steps
        expand_scores = torch.mul(
            scores.unsqueeze(2), topk_p.reshape(-1, topk, topk)
        )  # (b, topk, 1) x (b, topk ,topk) -&gt; (b, topk, topk)
        topk_cs_p, topk_cs_index = fast_topk(
            expand_scores.flatten(start_dim=1), topk, dim=-1
        )  # (b, topk)
        scores = topk_cs_p  # shape: (b, topk)

        topk_index = topk_index.reshape(-1, topk**2)
        input_ids = torch.gather(topk_index, index=topk_cs_index, dim=1).flatten()

        if hidden_states.shape[0] &gt; 0:
            selected_input_index = topk_cs_index.flatten() // topk + torch.arange(
                0, hidden_states.shape[0], step=topk, device=&#34;cuda&#34;
            ).repeat_interleave(topk)
            hidden_states = hidden_states[selected_input_index, :]

        tree_info = (
            expand_scores,  # shape: (b, topk, topk)
            topk_index,  # shape: (b, topk * topk)
            topk_cs_index + (topk**2 * (i - 1) + topk),  # shape: (b, topk)
        )

    return input_ids, hidden_states, scores, tree_info</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.traverse_tree"><code class="name flex">
<span>def <span class="ident">traverse_tree</span></span>(<span>retrieve_next_token: torch.Tensor,<br>retrieve_next_sibling: torch.Tensor,<br>draft_tokens: torch.Tensor,<br>grammar: BaseGrammarObject,<br>allocate_token_bitmask: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def traverse_tree(
    retrieve_next_token: torch.Tensor,
    retrieve_next_sibling: torch.Tensor,
    draft_tokens: torch.Tensor,
    grammar: BaseGrammarObject,
    allocate_token_bitmask: torch.Tensor,
):
    &#34;&#34;&#34;
    Traverse the tree constructed by the draft model to generate the logits mask.
    &#34;&#34;&#34;
    assert (
        retrieve_next_token.shape == retrieve_next_sibling.shape == draft_tokens.shape
    )

    allocate_token_bitmask.fill_(0)

    def dfs(
        curr: int,
        retrieve_next_token: torch.Tensor,
        retrieve_next_sibling: torch.Tensor,
        parent_pos: int,
    ):
        if curr == 0:
            # the first token generated by the target model, and thus it is always
            # accepted from the previous iteration
            accepted = True
        else:
            parent_bitmask = allocate_token_bitmask[parent_pos]
            curr_token_id = draft_tokens[curr]
            # 32 boolean bitmask values are packed into 32-bit integers
            accepted = (
                parent_bitmask[curr_token_id // 32] &amp; (1 &lt;&lt; (curr_token_id % 32))
            ) != 0

        if accepted:
            if curr != 0:
                # Accept the current token
                grammar.accept_token(draft_tokens[curr])
            if not grammar.is_terminated():
                # Generate the bitmask for the current token
                grammar.fill_vocab_mask(allocate_token_bitmask, curr)
                if retrieve_next_token[curr] != -1:
                    # Visit the child node
                    dfs(
                        retrieve_next_token[curr],
                        retrieve_next_token,
                        retrieve_next_sibling,
                        curr,
                    )

            if curr != 0:
                # Rollback the current token
                grammar.rollback(1)

        if retrieve_next_sibling[curr] != -1:
            # Visit the sibling node
            dfs(
                retrieve_next_sibling[curr],
                retrieve_next_token,
                retrieve_next_sibling,
                parent_pos,
            )

    dfs(0, retrieve_next_token, retrieve_next_sibling, -1)</code></pre>
</details>
<div class="desc"><p>Traverse the tree constructed by the draft model to generate the logits mask.</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput"><code class="flex name class">
<span>class <span class="ident">EagleDraftInput</span></span>
<span>(</span><span>topk_p: torch.Tensor = None,<br>topk_index: torch.Tensor = None,<br>hidden_states: torch.Tensor = None,<br>capture_hidden_mode: CaptureHiddenMode = 2,<br>verified_id: torch.Tensor = None,<br>accept_length: torch.Tensor = None,<br>accept_length_cpu: List[int] = None,<br>kv_indptr: torch.Tensor = None,<br>kv_indices: torch.Tensor = None,<br>num_tokens_per_batch: int = -1,<br>num_tokens_for_logprob_per_batch: int = -1,<br>seq_lens_for_draft_extend: torch.Tensor = None,<br>req_pool_indices_for_draft_extend: torch.Tensor = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class EagleDraftInput:
    # The inputs for decode
    # shape: (b, topk)
    topk_p: torch.Tensor = None
    topk_index: torch.Tensor = None
    # shape: (b, hidden_size)
    hidden_states: torch.Tensor = None
    capture_hidden_mode: CaptureHiddenMode = CaptureHiddenMode.FULL

    # Inputs for extend
    # shape: (b,)
    verified_id: torch.Tensor = None
    accept_length: torch.Tensor = None
    accept_length_cpu: List[int] = None

    # Inputs for the attention backends
    # shape: (b + 1,)
    kv_indptr: torch.Tensor = None
    kv_indices: torch.Tensor = None

    # Shape info for padding
    num_tokens_per_batch: int = -1
    num_tokens_for_logprob_per_batch: int = -1

    # Inputs for draft extend
    # shape: (b,)
    seq_lens_for_draft_extend: torch.Tensor = None
    req_pool_indices_for_draft_extend: torch.Tensor = None

    def prepare_for_extend(self, batch: ScheduleBatch):

        if batch.forward_mode.is_idle():
            return

        # Prefill only generate 1 token.
        assert len(self.verified_id) == len(batch.seq_lens)

        pt = 0
        for i, extend_len in enumerate(batch.extend_lens):
            input_ids = batch.input_ids[pt : pt + extend_len]
            batch.input_ids[pt : pt + extend_len] = torch.cat(
                (input_ids[1:], self.verified_id[i].reshape(1))
            )
            pt += extend_len

    @classmethod
    def create_idle_input(
        cls,
        device: torch.device,
        hidden_size: int,
        dtype: torch.dtype,
        topk: int,
        capture_hidden_mode: CaptureHiddenMode,
    ):
        return cls(
            verified_id=torch.empty((0,), device=device, dtype=torch.int32),
            hidden_states=torch.empty((0, hidden_size), device=device, dtype=dtype),
            topk_p=torch.empty((0, topk), device=device, dtype=torch.float32),
            topk_index=torch.empty((0, topk), device=device, dtype=torch.int64),
            capture_hidden_mode=capture_hidden_mode,
            accept_length=torch.empty((0,), device=device, dtype=torch.int32),
            accept_length_cpu=[],
        )

    def prepare_extend_after_decode(
        self,
        batch: ScheduleBatch,
        speculative_num_steps: int,
    ):

        if batch.forward_mode.is_idle():
            return

        batch.input_ids = self.verified_id
        batch.extend_lens = [x + 1 for x in batch.spec_info.accept_length_cpu]
        batch.extend_num_tokens = sum(batch.extend_lens)
        batch.seq_lens = batch.spec_info.seq_lens_for_draft_extend
        batch.req_pool_indices = batch.spec_info.req_pool_indices_for_draft_extend
        batch.return_logprob = False
        batch.return_hidden_states = False

        self.capture_hidden_mode = CaptureHiddenMode.LAST
        self.accept_length.add_(1)
        self.positions = torch.empty_like(batch.input_ids, dtype=torch.long)
        self.verified_id = torch.empty_like(self.accept_length, dtype=torch.int32)

        create_extend_after_decode_spec_info[(len(batch.seq_lens),)](
            batch.input_ids,
            batch.seq_lens,
            self.accept_length,
            self.positions,
            self.verified_id,
            next_power_of_2(max(speculative_num_steps + 1, len(batch.seq_lens))),
        )

    def generate_attn_arg_prefill(
        self,
        req_pool_indices: torch.Tensor,
        paged_kernel_lens: torch.Tensor,
        paged_kernel_lens_sum: int,
        req_to_token: torch.Tensor,
    ):
        bs = self.accept_length.numel()
        qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device=&#34;cuda&#34;)
        qo_indptr[1:] = torch.cumsum(self.accept_length, dim=0)
        cum_kv_seq_len = torch.zeros((bs + 1,), dtype=torch.int32, device=&#34;cuda&#34;)
        cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

        if paged_kernel_lens_sum is None:
            paged_kernel_lens_sum = cum_kv_seq_len[-1]

        kv_indices = torch.empty(
            paged_kernel_lens_sum, dtype=torch.int32, device=&#34;cuda&#34;
        )

        create_flashinfer_kv_indices_triton[(bs,)](
            req_to_token,
            req_pool_indices,
            paged_kernel_lens,
            cum_kv_seq_len,
            None,
            kv_indices,
            req_to_token.size(1),
        )
        return kv_indices, cum_kv_seq_len, qo_indptr, None

    def filter_batch(self, new_indices: torch.Tensor, has_been_filtered: bool = True):
        if has_been_filtered:
            # in eagle_utils.py:verify, we have already filtered the batch by `unfinished_index`
            # therefore, we don&#39;t need to filter the batch again in scheduler
            if len(new_indices) != len(self.topk_p):
                logger.warning(
                    f&#34;length of new_indices: {len(new_indices)} != length of topk_p: {len(self.topk_p)}, this should not happen&#34;
                )
            self.topk_p = self.topk_p[: len(new_indices)]
            self.topk_index = self.topk_index[: len(new_indices)]
            self.hidden_states = self.hidden_states[: len(new_indices)]
            self.verified_id = self.verified_id[: len(new_indices)]
        else:
            # in some cases(e.g draft_extend), we have not filtered the batch by `unfinished_index`
            self.topk_p = self.topk_p[new_indices]
            self.topk_index = self.topk_index[new_indices]
            self.hidden_states = self.hidden_states[new_indices]
            self.verified_id = self.verified_id[new_indices]

    def merge_batch(self, spec_info: EagleDraftInput):
        if self.hidden_states is None:
            self.hidden_states = spec_info.hidden_states
            self.verified_id = spec_info.verified_id
            self.topk_p = spec_info.topk_p
            self.topk_index = spec_info.topk_index
            return
        if spec_info.hidden_states is None:
            return
        self.hidden_states = torch.cat(
            [self.hidden_states, spec_info.hidden_states], axis=0
        )
        self.verified_id = torch.cat([self.verified_id, spec_info.verified_id], axis=0)
        self.topk_p = torch.cat([self.topk_p, spec_info.topk_p])
        self.topk_index = torch.cat([self.topk_index, spec_info.topk_index])</code></pre>
</details>
<div class="desc"><p>EagleDraftInput(topk_p: 'torch.Tensor' = None, topk_index: 'torch.Tensor' = None, hidden_states: 'torch.Tensor' = None, capture_hidden_mode: 'CaptureHiddenMode' = <CaptureHiddenMode.FULL: 2>, verified_id: 'torch.Tensor' = None, accept_length: 'torch.Tensor' = None, accept_length_cpu: 'List[int]' = None, kv_indptr: 'torch.Tensor' = None, kv_indices: 'torch.Tensor' = None, num_tokens_per_batch: 'int' = -1, num_tokens_for_logprob_per_batch: 'int' = -1, seq_lens_for_draft_extend: 'torch.Tensor' = None, req_pool_indices_for_draft_extend: 'torch.Tensor' = None)</p></div>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput.create_idle_input"><code class="name flex">
<span>def <span class="ident">create_idle_input</span></span>(<span>device: torch.device,<br>hidden_size: int,<br>dtype: torch.dtype,<br>topk: int,<br>capture_hidden_mode: CaptureHiddenMode)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput.accept_length"><code class="name">var <span class="ident">accept_length</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput.accept_length_cpu"><code class="name">var <span class="ident">accept_length_cpu</span> : List[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput.capture_hidden_mode"><code class="name">var <span class="ident">capture_hidden_mode</span> : <a title="sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode">CaptureHiddenMode</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput.hidden_states"><code class="name">var <span class="ident">hidden_states</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput.kv_indices"><code class="name">var <span class="ident">kv_indices</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput.kv_indptr"><code class="name">var <span class="ident">kv_indptr</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput.num_tokens_for_logprob_per_batch"><code class="name">var <span class="ident">num_tokens_for_logprob_per_batch</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput.num_tokens_per_batch"><code class="name">var <span class="ident">num_tokens_per_batch</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput.req_pool_indices_for_draft_extend"><code class="name">var <span class="ident">req_pool_indices_for_draft_extend</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput.seq_lens_for_draft_extend"><code class="name">var <span class="ident">seq_lens_for_draft_extend</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput.topk_index"><code class="name">var <span class="ident">topk_index</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput.topk_p"><code class="name">var <span class="ident">topk_p</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput.verified_id"><code class="name">var <span class="ident">verified_id</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput.filter_batch"><code class="name flex">
<span>def <span class="ident">filter_batch</span></span>(<span>self, new_indices: torch.Tensor, has_been_filtered: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_batch(self, new_indices: torch.Tensor, has_been_filtered: bool = True):
    if has_been_filtered:
        # in eagle_utils.py:verify, we have already filtered the batch by `unfinished_index`
        # therefore, we don&#39;t need to filter the batch again in scheduler
        if len(new_indices) != len(self.topk_p):
            logger.warning(
                f&#34;length of new_indices: {len(new_indices)} != length of topk_p: {len(self.topk_p)}, this should not happen&#34;
            )
        self.topk_p = self.topk_p[: len(new_indices)]
        self.topk_index = self.topk_index[: len(new_indices)]
        self.hidden_states = self.hidden_states[: len(new_indices)]
        self.verified_id = self.verified_id[: len(new_indices)]
    else:
        # in some cases(e.g draft_extend), we have not filtered the batch by `unfinished_index`
        self.topk_p = self.topk_p[new_indices]
        self.topk_index = self.topk_index[new_indices]
        self.hidden_states = self.hidden_states[new_indices]
        self.verified_id = self.verified_id[new_indices]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput.generate_attn_arg_prefill"><code class="name flex">
<span>def <span class="ident">generate_attn_arg_prefill</span></span>(<span>self,<br>req_pool_indices: torch.Tensor,<br>paged_kernel_lens: torch.Tensor,<br>paged_kernel_lens_sum: int,<br>req_to_token: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_attn_arg_prefill(
    self,
    req_pool_indices: torch.Tensor,
    paged_kernel_lens: torch.Tensor,
    paged_kernel_lens_sum: int,
    req_to_token: torch.Tensor,
):
    bs = self.accept_length.numel()
    qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device=&#34;cuda&#34;)
    qo_indptr[1:] = torch.cumsum(self.accept_length, dim=0)
    cum_kv_seq_len = torch.zeros((bs + 1,), dtype=torch.int32, device=&#34;cuda&#34;)
    cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

    if paged_kernel_lens_sum is None:
        paged_kernel_lens_sum = cum_kv_seq_len[-1]

    kv_indices = torch.empty(
        paged_kernel_lens_sum, dtype=torch.int32, device=&#34;cuda&#34;
    )

    create_flashinfer_kv_indices_triton[(bs,)](
        req_to_token,
        req_pool_indices,
        paged_kernel_lens,
        cum_kv_seq_len,
        None,
        kv_indices,
        req_to_token.size(1),
    )
    return kv_indices, cum_kv_seq_len, qo_indptr, None</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput.merge_batch"><code class="name flex">
<span>def <span class="ident">merge_batch</span></span>(<span>self,<br>spec_info: <a title="sglang.srt.speculative.eagle_utils.EagleDraftInput" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput">EagleDraftInput</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_batch(self, spec_info: EagleDraftInput):
    if self.hidden_states is None:
        self.hidden_states = spec_info.hidden_states
        self.verified_id = spec_info.verified_id
        self.topk_p = spec_info.topk_p
        self.topk_index = spec_info.topk_index
        return
    if spec_info.hidden_states is None:
        return
    self.hidden_states = torch.cat(
        [self.hidden_states, spec_info.hidden_states], axis=0
    )
    self.verified_id = torch.cat([self.verified_id, spec_info.verified_id], axis=0)
    self.topk_p = torch.cat([self.topk_p, spec_info.topk_p])
    self.topk_index = torch.cat([self.topk_index, spec_info.topk_index])</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput.prepare_extend_after_decode"><code class="name flex">
<span>def <span class="ident">prepare_extend_after_decode</span></span>(<span>self, batch: ScheduleBatch, speculative_num_steps: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_extend_after_decode(
    self,
    batch: ScheduleBatch,
    speculative_num_steps: int,
):

    if batch.forward_mode.is_idle():
        return

    batch.input_ids = self.verified_id
    batch.extend_lens = [x + 1 for x in batch.spec_info.accept_length_cpu]
    batch.extend_num_tokens = sum(batch.extend_lens)
    batch.seq_lens = batch.spec_info.seq_lens_for_draft_extend
    batch.req_pool_indices = batch.spec_info.req_pool_indices_for_draft_extend
    batch.return_logprob = False
    batch.return_hidden_states = False

    self.capture_hidden_mode = CaptureHiddenMode.LAST
    self.accept_length.add_(1)
    self.positions = torch.empty_like(batch.input_ids, dtype=torch.long)
    self.verified_id = torch.empty_like(self.accept_length, dtype=torch.int32)

    create_extend_after_decode_spec_info[(len(batch.seq_lens),)](
        batch.input_ids,
        batch.seq_lens,
        self.accept_length,
        self.positions,
        self.verified_id,
        next_power_of_2(max(speculative_num_steps + 1, len(batch.seq_lens))),
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleDraftInput.prepare_for_extend"><code class="name flex">
<span>def <span class="ident">prepare_for_extend</span></span>(<span>self, batch: ScheduleBatch)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_for_extend(self, batch: ScheduleBatch):

    if batch.forward_mode.is_idle():
        return

    # Prefill only generate 1 token.
    assert len(self.verified_id) == len(batch.seq_lens)

    pt = 0
    for i, extend_len in enumerate(batch.extend_lens):
        input_ids = batch.input_ids[pt : pt + extend_len]
        batch.input_ids[pt : pt + extend_len] = torch.cat(
            (input_ids[1:], self.verified_id[i].reshape(1))
        )
        pt += extend_len</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyInput"><code class="flex name class">
<span>class <span class="ident">EagleVerifyInput</span></span>
<span>(</span><span>draft_token: torch.Tensor,<br>custom_mask: torch.Tensor,<br>positions: torch.Tensor,<br>retrive_index: torch.Tensor,<br>retrive_next_token: torch.Tensor,<br>retrive_next_sibling: torch.Tensor,<br>retrive_cum_len: torch.Tensor,<br>spec_steps: int,<br>topk: int,<br>draft_token_num: int,<br>capture_hidden_mode: CaptureHiddenMode,<br>seq_lens_sum: int,<br>seq_lens_cpu: torch.Tensor,<br>grammar: BaseGrammarObject = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class EagleVerifyInput:
    draft_token: torch.Tensor
    custom_mask: torch.Tensor
    positions: torch.Tensor
    retrive_index: torch.Tensor
    retrive_next_token: torch.Tensor
    retrive_next_sibling: torch.Tensor
    retrive_cum_len: torch.Tensor
    spec_steps: int
    topk: int
    draft_token_num: int
    capture_hidden_mode: CaptureHiddenMode
    seq_lens_sum: int
    seq_lens_cpu: torch.Tensor
    grammar: BaseGrammarObject = None

    @classmethod
    def create_idle_input(cls, topk: int, spec_steps: int, num_verify_tokens: int):
        return cls(
            draft_token=torch.empty((0,), dtype=torch.long, device=&#34;cuda&#34;),
            custom_mask=torch.full((0,), True, dtype=torch.bool, device=&#34;cuda&#34;),
            positions=torch.empty((0,), dtype=torch.int64, device=&#34;cuda&#34;),
            retrive_index=torch.full(
                (0, num_verify_tokens), -1, dtype=torch.long, device=&#34;cuda&#34;
            ),
            retrive_next_token=torch.full(
                (0, num_verify_tokens), -1, dtype=torch.long, device=&#34;cuda&#34;
            ),
            retrive_next_sibling=torch.full(
                (0, num_verify_tokens), -1, dtype=torch.long, device=&#34;cuda&#34;
            ),
            retrive_cum_len=None,
            topk=topk,
            draft_token_num=num_verify_tokens,
            spec_steps=spec_steps,
            capture_hidden_mode=CaptureHiddenMode.FULL,
            seq_lens_sum=0,
            seq_lens_cpu=torch.empty((0,), dtype=torch.int32),
        )

    def prepare_for_verify(self, batch: ScheduleBatch, page_size: int):

        if batch.forward_mode.is_idle():
            return

        batch.input_ids = self.draft_token

        if page_size == 1:
            batch.out_cache_loc = batch.alloc_token_slots(len(batch.input_ids))
            end_offset = batch.seq_lens + self.draft_token_num
        else:
            prefix_lens = batch.seq_lens
            end_offset = prefix_lens + self.draft_token_num
            last_loc = get_last_loc(
                batch.req_to_token_pool.req_to_token,
                batch.req_pool_indices,
                prefix_lens,
            )
            batch.out_cache_loc = batch.alloc_paged_token_slots_extend(
                prefix_lens, end_offset, last_loc, len(batch.input_ids)
            )
            self.last_loc = last_loc

        bs = batch.batch_size()
        assign_req_to_token_pool[(bs,)](
            batch.req_pool_indices,
            batch.req_to_token_pool.req_to_token,
            batch.seq_lens,
            end_offset,
            batch.out_cache_loc,
            batch.req_to_token_pool.req_to_token.shape[1],
            next_power_of_2(bs),
        )

    def generate_attn_arg_prefill(
        self,
        req_pool_indices: torch.Tensor,
        paged_kernel_lens: torch.Tensor,
        paged_kernel_lens_sum: int,
        req_to_token: torch.Tensor,
    ):
        batch_size = len(req_pool_indices)
        qo_indptr = torch.arange(
            0,
            (1 + batch_size) * self.draft_token_num,
            step=self.draft_token_num,
            dtype=torch.int32,
            device=&#34;cuda&#34;,
        )
        cum_kv_seq_len = torch.zeros(
            (batch_size + 1,), dtype=torch.int32, device=&#34;cuda&#34;
        )

        paged_kernel_lens = paged_kernel_lens + self.draft_token_num
        cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

        kv_indices = torch.empty(
            paged_kernel_lens_sum + self.draft_token_num * batch_size,
            dtype=torch.int32,
            device=&#34;cuda&#34;,
        )
        create_flashinfer_kv_indices_triton[(batch_size,)](
            req_to_token,
            req_pool_indices,
            paged_kernel_lens,
            cum_kv_seq_len,
            None,
            kv_indices,
            req_to_token.size(1),
        )
        return kv_indices, cum_kv_seq_len, qo_indptr, self.custom_mask

    def verify(
        self,
        batch: ScheduleBatch,
        logits_output: LogitsProcessorOutput,
        token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,
        page_size: int,
        vocab_mask: Optional[torch.Tensor] = None,  # For grammar
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Verify and find accepted tokens based on logits output and batch
        (which contains spec decoding information).

        WARNING: This API in-place modifies the states of logits_output

        This API updates values inside logits_output based on the accepted
        tokens. I.e., logits_output.next_token_logits only contains
        accepted token logits.
        &#34;&#34;&#34;
        if batch.forward_mode.is_idle():
            return EagleVerifyOutput(
                draft_input=EagleDraftInput.create_idle_input(
                    device=batch.device,
                    hidden_size=batch.model_config.hidden_size,
                    dtype=batch.model_config.dtype,
                    topk=self.topk,
                    capture_hidden_mode=CaptureHiddenMode.LAST,
                ),
                logits_output=logits_output,
                verified_id=torch.empty(0, dtype=torch.long, device=batch.device),
                accept_length_per_req_cpu=[],
                accepted_indices=torch.full(
                    (0, self.spec_steps + 1),
                    -1,
                    dtype=torch.int32,
                    device=batch.device,
                ),
            )

        bs = self.retrive_index.shape[0]
        candidates = self.draft_token.reshape(bs, self.draft_token_num)
        sampling_info = batch.sampling_info

        predict_shape = list(logits_output.next_token_logits.shape)[:-1]
        predict_shape[-1] += 1
        predict = torch.empty(predict_shape, dtype=torch.int32, device=&#34;cuda&#34;)
        accept_index = torch.full(
            (bs, self.spec_steps + 1), -1, dtype=torch.int32, device=&#34;cuda&#34;
        )
        accept_length = torch.empty((bs,), dtype=torch.int32, device=&#34;cuda&#34;)

        if bs != len(sampling_info):
            sampling_info = copy.deepcopy(sampling_info)
            # NOTE: retrive_index are the indices of the requests that are kept.
            sampling_info.filter_batch(self.retrive_index.tolist(), self.retrive_index)

        # Apply the custom logit processors if registered in the sampling info.
        if sampling_info.has_custom_logit_processor:
            apply_custom_logit_processor(
                logits_output.next_token_logits,
                sampling_info,
                num_tokens_in_batch=self.draft_token_num,
            )

        # Apply penalty
        if sampling_info.penalizer_orchestrator.is_required:
            # This is a relaxed version of penalties for speculative decoding.
            linear_penalty = torch.zeros(
                (bs, logits_output.next_token_logits.shape[1]),
                dtype=torch.float32,
                device=&#34;cuda&#34;,
            )
            sampling_info.apply_logits_bias(linear_penalty)
            logits_output.next_token_logits.add_(
                torch.repeat_interleave(linear_penalty, self.draft_token_num, dim=0)
            )

        # Apply grammar mask
        if vocab_mask is not None:
            assert self.grammar is not None
            self.grammar.apply_vocab_mask(
                logits=logits_output.next_token_logits, vocab_mask=vocab_mask
            )

        # Sample tokens. Force greedy sampling on AMD
        is_all_greedy = sampling_info.is_all_greedy
        if (not is_all_greedy) and (not TREE_SPEC_KERNEL_AVAILABLE):
            logger.warning(
                &#34;Tree speculative sampling kernel unavailable (likely AMD/HIP build). &#34;
                &#34;Falling back to greedy verification.&#34;
            )

        if is_all_greedy or not TREE_SPEC_KERNEL_AVAILABLE:
            target_predict = torch.argmax(logits_output.next_token_logits, dim=-1)
            target_predict = target_predict.reshape(bs, self.draft_token_num)

            verify_tree_greedy(
                predicts=predict,  # mutable
                accept_index=accept_index,  # mutable
                accept_token_num=accept_length,  # mutable
                candidates=candidates,
                retrive_index=self.retrive_index,
                retrive_next_token=self.retrive_next_token,
                retrive_next_sibling=self.retrive_next_sibling,
                target_predict=target_predict,
            )
        else:
            # apply temperature and get target probs
            expanded_temperature = torch.repeat_interleave(
                sampling_info.temperatures, self.draft_token_num, dim=0
            )  # (bs * draft_token_num, 1)

            target_probs = F.softmax(
                logits_output.next_token_logits / expanded_temperature, dim=-1
            )  # (bs * draft_token_num, vocab_size)
            target_probs = top_k_renorm_prob(
                target_probs,
                torch.repeat_interleave(
                    sampling_info.top_ks, self.draft_token_num, dim=0
                ),
            )  # (bs * draft_token_num, vocab_size)
            if not torch.all(sampling_info.top_ps == 1.0):
                target_probs = top_p_renorm_prob(
                    target_probs,
                    torch.repeat_interleave(
                        sampling_info.top_ps, self.draft_token_num, dim=0
                    ),
                )
            target_probs = target_probs.reshape(bs, self.draft_token_num, -1)

            draft_probs = torch.zeros(
                target_probs.shape, dtype=torch.float32, device=&#34;cuda&#34;
            )

            # coins for rejection sampling
            coins = torch.rand_like(candidates, dtype=torch.float32, device=&#34;cuda&#34;)
            # coins for final sampling
            coins_for_final_sampling = torch.rand(
                (bs,), dtype=torch.float32, device=&#34;cuda&#34;
            )
            tree_speculative_sampling_target_only(
                predicts=predict,  # mutable
                accept_index=accept_index,  # mutable
                accept_token_num=accept_length,  # mutable
                candidates=candidates,
                retrive_index=self.retrive_index,
                retrive_next_token=self.retrive_next_token,
                retrive_next_sibling=self.retrive_next_sibling,
                uniform_samples=coins,
                uniform_samples_for_final_sampling=coins_for_final_sampling,
                target_probs=target_probs,
                draft_probs=draft_probs,
                threshold_single=global_server_args_dict[
                    &#34;speculative_accept_threshold_single&#34;
                ],
                threshold_acc=global_server_args_dict[
                    &#34;speculative_accept_threshold_acc&#34;
                ],
                deterministic=True,
            )

        if SIMULATE_ACC_LEN:
            # Do simulation
            accept_index = _generate_simulated_accept_index(
                accept_index=accept_index,
                predict=predict,  # mutable
                accept_length=accept_length,  # mutable
                simulate_acc_len=SIMULATE_ACC_LEN,
                bs=bs,
                spec_steps=self.spec_steps,
            )

        unfinished_index = []
        unfinished_accept_index = []
        accept_index_cpu = accept_index.tolist()
        predict_cpu = predict.tolist()
        has_finished = False

        # Iterate every accepted token and check if req has finished after append the token
        # should be checked BEFORE free kv cache slots
        for i, (req, accept_index_row) in enumerate(zip(batch.reqs, accept_index_cpu)):
            for j, idx in enumerate(accept_index_row):
                if idx == -1:
                    break
                id = predict_cpu[idx]
                req.output_ids.append(id)
                req.check_finished()
                if req.finished():
                    has_finished = True
                    # set all tokens after finished token to -1 and break
                    accept_index[i, j + 1 :] = -1
                    break
                else:
                    if req.grammar is not None:
                        try:
                            req.grammar.accept_token(id)
                        except ValueError as e:
                            logger.info(
                                f&#34;{i=}, {req=}\n&#34; f&#34;{accept_index=}\n&#34; f&#34;{predict=}\n&#34;
                            )
                            raise e
            if not req.finished():
                unfinished_index.append(i)
                if idx == -1:
                    unfinished_accept_index.append(accept_index[i, :j])
                else:
                    unfinished_accept_index.append(accept_index[i])
            req.spec_verify_ct += 1

        if has_finished:
            accept_length = (accept_index != -1).sum(dim=1) - 1

        # Free the KV cache for unaccepted tokens
        # TODO: fuse them
        accept_index = accept_index[accept_index != -1]
        verified_id = predict[accept_index]
        evict_mask = torch.full_like(self.draft_token, True, dtype=torch.bool)
        evict_mask[accept_index] = False

        if page_size == 1:
            # TODO: boolean array index leads to a device sync. Remove it.
            token_to_kv_pool_allocator.free(batch.out_cache_loc[evict_mask])
        else:
            if self.topk == 1:
                # Only evict full empty page. Do not evict partial empty page
                align_evict_mask_to_page_size[len(batch.seq_lens),](
                    batch.seq_lens,
                    evict_mask,
                    page_size,
                    self.draft_token_num,
                    next_power_of_2(self.draft_token_num),
                )
                token_to_kv_pool_allocator.free(batch.out_cache_loc[evict_mask])
            else:
                # Shift the accepted tokens to the beginning.
                # Only evict the last part
                src_cache_loc, tgt_cache_loc, to_free_num_slots = get_src_tgt_cache_loc(
                    batch.seq_lens,
                    batch.out_cache_loc,
                    accept_index,
                    accept_length,
                    self.draft_token_num,
                    page_size,
                )
                to_free_slots = torch.empty(
                    (to_free_num_slots.sum().item(),),
                    dtype=torch.int64,
                    device=to_free_num_slots.device,
                )

                # out_cache_loc: [0  1  2,  3  4  5,  6  7  8]
                # accept_index:  [0 -1  2,  3  4 -1,  6 -1 -1]
                # tgt_cache_loc: [0  1   ,  3  4   ,  6      ]
                # to_free_slots: [      2,        5,     7  8]
                # to_free_slots also needs to be page-aligned without the first partial page
                #
                # split each row of out_cache_loc into two parts.
                # 1. the first part goes to tgt_cache_loc. length = accept_length[i] + 1
                # 2. the second part goes to to_free_slots.
                get_target_cache_loc[(bs,)](
                    tgt_cache_loc,
                    to_free_slots,
                    accept_length,
                    to_free_num_slots,
                    batch.out_cache_loc,
                    self.draft_token_num,
                    next_power_of_2(self.draft_token_num),
                    next_power_of_2(bs),
                )

                # Free the kv cache
                token_to_kv_pool_allocator.free(to_free_slots)

                # Copy the kv cache
                batch.token_to_kv_pool_allocator.get_kvcache().move_kv_cache(
                    tgt_cache_loc, src_cache_loc
                )

        # Construct EagleVerifyOutput
        if not has_finished:
            if page_size == 1 or self.topk == 1:
                batch.out_cache_loc = batch.out_cache_loc[accept_index]
                assign_req_to_token_pool[(bs,)](
                    batch.req_pool_indices,
                    batch.req_to_token_pool.req_to_token,
                    batch.seq_lens,
                    batch.seq_lens + accept_length + 1,
                    batch.out_cache_loc,
                    batch.req_to_token_pool.req_to_token.shape[1],
                    next_power_of_2(bs),
                )
            else:
                batch.out_cache_loc = tgt_cache_loc
            batch.seq_lens.add_(accept_length + 1)

            draft_input = EagleDraftInput(
                hidden_states=batch.spec_info.hidden_states[accept_index],
                verified_id=verified_id,
                accept_length=accept_length,
                accept_length_cpu=accept_length.tolist(),
                seq_lens_for_draft_extend=batch.seq_lens,
                req_pool_indices_for_draft_extend=batch.req_pool_indices,
            )

            return EagleVerifyOutput(
                draft_input=draft_input,
                logits_output=logits_output,
                verified_id=verified_id,
                accept_length_per_req_cpu=draft_input.accept_length_cpu,
                accepted_indices=accept_index,
            )
        else:
            if page_size == 1 or self.topk == 1:
                assign_req_to_token_pool[(bs,)](
                    batch.req_pool_indices,
                    batch.req_to_token_pool.req_to_token,
                    batch.seq_lens,
                    batch.seq_lens + accept_length + 1,
                    batch.out_cache_loc[accept_index],
                    batch.req_to_token_pool.req_to_token.shape[1],
                    next_power_of_2(bs),
                )
                batch.seq_lens.add_(accept_length + 1)

            accept_length_cpu = accept_length.tolist()
            if len(unfinished_accept_index) &gt; 0:
                unfinished_accept_index = torch.cat(unfinished_accept_index)
                unfinished_index_device = torch.tensor(
                    unfinished_index, dtype=torch.int64, device=predict.device
                )
                draft_input_accept_length_cpu = [
                    accept_length_cpu[i] for i in unfinished_index
                ]
                if page_size == 1 or self.topk == 1:
                    batch.out_cache_loc = batch.out_cache_loc[unfinished_accept_index]
                else:
                    batch.out_cache_loc = torch.empty(
                        len(unfinished_index) + sum(draft_input_accept_length_cpu),
                        dtype=torch.int64,
                        device=predict.device,
                    )
                    accept_length_filter = create_accept_length_filter(
                        accept_length,
                        unfinished_index_device,
                        batch.seq_lens,
                    )
                    filter_finished_cache_loc_kernel[(bs,)](
                        batch.out_cache_loc,
                        tgt_cache_loc,
                        accept_length,
                        accept_length_filter,
                        next_power_of_2(bs),
                        next_power_of_2(self.draft_token_num),
                    )

                draft_input = EagleDraftInput(
                    hidden_states=batch.spec_info.hidden_states[
                        unfinished_accept_index
                    ],
                    verified_id=predict[unfinished_accept_index],
                    accept_length_cpu=draft_input_accept_length_cpu,
                    accept_length=accept_length[unfinished_index_device],
                    seq_lens_for_draft_extend=batch.seq_lens[unfinished_index_device],
                    req_pool_indices_for_draft_extend=batch.req_pool_indices[
                        unfinished_index_device
                    ],
                )
            else:
                draft_input = EagleDraftInput.create_idle_input(
                    device=batch.device,
                    hidden_size=batch.model_config.hidden_size,
                    dtype=batch.model_config.dtype,
                    topk=self.topk,
                    capture_hidden_mode=CaptureHiddenMode.LAST,
                )

            return EagleVerifyOutput(
                draft_input=draft_input,
                logits_output=logits_output,
                verified_id=verified_id,
                accept_length_per_req_cpu=accept_length_cpu,
                accepted_indices=accept_index,
            )</code></pre>
</details>
<div class="desc"><p>EagleVerifyInput(draft_token: 'torch.Tensor', custom_mask: 'torch.Tensor', positions: 'torch.Tensor', retrive_index: 'torch.Tensor', retrive_next_token: 'torch.Tensor', retrive_next_sibling: 'torch.Tensor', retrive_cum_len: 'torch.Tensor', spec_steps: 'int', topk: 'int', draft_token_num: 'int', capture_hidden_mode: 'CaptureHiddenMode', seq_lens_sum: 'int', seq_lens_cpu: 'torch.Tensor', grammar: 'BaseGrammarObject' = None)</p></div>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyInput.create_idle_input"><code class="name flex">
<span>def <span class="ident">create_idle_input</span></span>(<span>topk: int, spec_steps: int, num_verify_tokens: int)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyInput.capture_hidden_mode"><code class="name">var <span class="ident">capture_hidden_mode</span> : <a title="sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode">CaptureHiddenMode</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyInput.custom_mask"><code class="name">var <span class="ident">custom_mask</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyInput.draft_token"><code class="name">var <span class="ident">draft_token</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyInput.draft_token_num"><code class="name">var <span class="ident">draft_token_num</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyInput.grammar"><code class="name">var <span class="ident">grammar</span> : <a title="sglang.srt.constrained.base_grammar_backend.BaseGrammarObject" href="../constrained/base_grammar_backend.html#sglang.srt.constrained.base_grammar_backend.BaseGrammarObject">BaseGrammarObject</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyInput.positions"><code class="name">var <span class="ident">positions</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyInput.retrive_cum_len"><code class="name">var <span class="ident">retrive_cum_len</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyInput.retrive_index"><code class="name">var <span class="ident">retrive_index</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyInput.retrive_next_sibling"><code class="name">var <span class="ident">retrive_next_sibling</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyInput.retrive_next_token"><code class="name">var <span class="ident">retrive_next_token</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyInput.seq_lens_cpu"><code class="name">var <span class="ident">seq_lens_cpu</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyInput.seq_lens_sum"><code class="name">var <span class="ident">seq_lens_sum</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyInput.spec_steps"><code class="name">var <span class="ident">spec_steps</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyInput.topk"><code class="name">var <span class="ident">topk</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyInput.generate_attn_arg_prefill"><code class="name flex">
<span>def <span class="ident">generate_attn_arg_prefill</span></span>(<span>self,<br>req_pool_indices: torch.Tensor,<br>paged_kernel_lens: torch.Tensor,<br>paged_kernel_lens_sum: int,<br>req_to_token: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_attn_arg_prefill(
    self,
    req_pool_indices: torch.Tensor,
    paged_kernel_lens: torch.Tensor,
    paged_kernel_lens_sum: int,
    req_to_token: torch.Tensor,
):
    batch_size = len(req_pool_indices)
    qo_indptr = torch.arange(
        0,
        (1 + batch_size) * self.draft_token_num,
        step=self.draft_token_num,
        dtype=torch.int32,
        device=&#34;cuda&#34;,
    )
    cum_kv_seq_len = torch.zeros(
        (batch_size + 1,), dtype=torch.int32, device=&#34;cuda&#34;
    )

    paged_kernel_lens = paged_kernel_lens + self.draft_token_num
    cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)

    kv_indices = torch.empty(
        paged_kernel_lens_sum + self.draft_token_num * batch_size,
        dtype=torch.int32,
        device=&#34;cuda&#34;,
    )
    create_flashinfer_kv_indices_triton[(batch_size,)](
        req_to_token,
        req_pool_indices,
        paged_kernel_lens,
        cum_kv_seq_len,
        None,
        kv_indices,
        req_to_token.size(1),
    )
    return kv_indices, cum_kv_seq_len, qo_indptr, self.custom_mask</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyInput.prepare_for_verify"><code class="name flex">
<span>def <span class="ident">prepare_for_verify</span></span>(<span>self, batch: ScheduleBatch, page_size: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_for_verify(self, batch: ScheduleBatch, page_size: int):

    if batch.forward_mode.is_idle():
        return

    batch.input_ids = self.draft_token

    if page_size == 1:
        batch.out_cache_loc = batch.alloc_token_slots(len(batch.input_ids))
        end_offset = batch.seq_lens + self.draft_token_num
    else:
        prefix_lens = batch.seq_lens
        end_offset = prefix_lens + self.draft_token_num
        last_loc = get_last_loc(
            batch.req_to_token_pool.req_to_token,
            batch.req_pool_indices,
            prefix_lens,
        )
        batch.out_cache_loc = batch.alloc_paged_token_slots_extend(
            prefix_lens, end_offset, last_loc, len(batch.input_ids)
        )
        self.last_loc = last_loc

    bs = batch.batch_size()
    assign_req_to_token_pool[(bs,)](
        batch.req_pool_indices,
        batch.req_to_token_pool.req_to_token,
        batch.seq_lens,
        end_offset,
        batch.out_cache_loc,
        batch.req_to_token_pool.req_to_token.shape[1],
        next_power_of_2(bs),
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyInput.verify"><code class="name flex">
<span>def <span class="ident">verify</span></span>(<span>self,<br>batch: ScheduleBatch,<br>logits_output: LogitsProcessorOutput,<br>token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,<br>page_size: int,<br>vocab_mask: Optional[torch.Tensor] = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def verify(
    self,
    batch: ScheduleBatch,
    logits_output: LogitsProcessorOutput,
    token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,
    page_size: int,
    vocab_mask: Optional[torch.Tensor] = None,  # For grammar
) -&gt; torch.Tensor:
    &#34;&#34;&#34;
    Verify and find accepted tokens based on logits output and batch
    (which contains spec decoding information).

    WARNING: This API in-place modifies the states of logits_output

    This API updates values inside logits_output based on the accepted
    tokens. I.e., logits_output.next_token_logits only contains
    accepted token logits.
    &#34;&#34;&#34;
    if batch.forward_mode.is_idle():
        return EagleVerifyOutput(
            draft_input=EagleDraftInput.create_idle_input(
                device=batch.device,
                hidden_size=batch.model_config.hidden_size,
                dtype=batch.model_config.dtype,
                topk=self.topk,
                capture_hidden_mode=CaptureHiddenMode.LAST,
            ),
            logits_output=logits_output,
            verified_id=torch.empty(0, dtype=torch.long, device=batch.device),
            accept_length_per_req_cpu=[],
            accepted_indices=torch.full(
                (0, self.spec_steps + 1),
                -1,
                dtype=torch.int32,
                device=batch.device,
            ),
        )

    bs = self.retrive_index.shape[0]
    candidates = self.draft_token.reshape(bs, self.draft_token_num)
    sampling_info = batch.sampling_info

    predict_shape = list(logits_output.next_token_logits.shape)[:-1]
    predict_shape[-1] += 1
    predict = torch.empty(predict_shape, dtype=torch.int32, device=&#34;cuda&#34;)
    accept_index = torch.full(
        (bs, self.spec_steps + 1), -1, dtype=torch.int32, device=&#34;cuda&#34;
    )
    accept_length = torch.empty((bs,), dtype=torch.int32, device=&#34;cuda&#34;)

    if bs != len(sampling_info):
        sampling_info = copy.deepcopy(sampling_info)
        # NOTE: retrive_index are the indices of the requests that are kept.
        sampling_info.filter_batch(self.retrive_index.tolist(), self.retrive_index)

    # Apply the custom logit processors if registered in the sampling info.
    if sampling_info.has_custom_logit_processor:
        apply_custom_logit_processor(
            logits_output.next_token_logits,
            sampling_info,
            num_tokens_in_batch=self.draft_token_num,
        )

    # Apply penalty
    if sampling_info.penalizer_orchestrator.is_required:
        # This is a relaxed version of penalties for speculative decoding.
        linear_penalty = torch.zeros(
            (bs, logits_output.next_token_logits.shape[1]),
            dtype=torch.float32,
            device=&#34;cuda&#34;,
        )
        sampling_info.apply_logits_bias(linear_penalty)
        logits_output.next_token_logits.add_(
            torch.repeat_interleave(linear_penalty, self.draft_token_num, dim=0)
        )

    # Apply grammar mask
    if vocab_mask is not None:
        assert self.grammar is not None
        self.grammar.apply_vocab_mask(
            logits=logits_output.next_token_logits, vocab_mask=vocab_mask
        )

    # Sample tokens. Force greedy sampling on AMD
    is_all_greedy = sampling_info.is_all_greedy
    if (not is_all_greedy) and (not TREE_SPEC_KERNEL_AVAILABLE):
        logger.warning(
            &#34;Tree speculative sampling kernel unavailable (likely AMD/HIP build). &#34;
            &#34;Falling back to greedy verification.&#34;
        )

    if is_all_greedy or not TREE_SPEC_KERNEL_AVAILABLE:
        target_predict = torch.argmax(logits_output.next_token_logits, dim=-1)
        target_predict = target_predict.reshape(bs, self.draft_token_num)

        verify_tree_greedy(
            predicts=predict,  # mutable
            accept_index=accept_index,  # mutable
            accept_token_num=accept_length,  # mutable
            candidates=candidates,
            retrive_index=self.retrive_index,
            retrive_next_token=self.retrive_next_token,
            retrive_next_sibling=self.retrive_next_sibling,
            target_predict=target_predict,
        )
    else:
        # apply temperature and get target probs
        expanded_temperature = torch.repeat_interleave(
            sampling_info.temperatures, self.draft_token_num, dim=0
        )  # (bs * draft_token_num, 1)

        target_probs = F.softmax(
            logits_output.next_token_logits / expanded_temperature, dim=-1
        )  # (bs * draft_token_num, vocab_size)
        target_probs = top_k_renorm_prob(
            target_probs,
            torch.repeat_interleave(
                sampling_info.top_ks, self.draft_token_num, dim=0
            ),
        )  # (bs * draft_token_num, vocab_size)
        if not torch.all(sampling_info.top_ps == 1.0):
            target_probs = top_p_renorm_prob(
                target_probs,
                torch.repeat_interleave(
                    sampling_info.top_ps, self.draft_token_num, dim=0
                ),
            )
        target_probs = target_probs.reshape(bs, self.draft_token_num, -1)

        draft_probs = torch.zeros(
            target_probs.shape, dtype=torch.float32, device=&#34;cuda&#34;
        )

        # coins for rejection sampling
        coins = torch.rand_like(candidates, dtype=torch.float32, device=&#34;cuda&#34;)
        # coins for final sampling
        coins_for_final_sampling = torch.rand(
            (bs,), dtype=torch.float32, device=&#34;cuda&#34;
        )
        tree_speculative_sampling_target_only(
            predicts=predict,  # mutable
            accept_index=accept_index,  # mutable
            accept_token_num=accept_length,  # mutable
            candidates=candidates,
            retrive_index=self.retrive_index,
            retrive_next_token=self.retrive_next_token,
            retrive_next_sibling=self.retrive_next_sibling,
            uniform_samples=coins,
            uniform_samples_for_final_sampling=coins_for_final_sampling,
            target_probs=target_probs,
            draft_probs=draft_probs,
            threshold_single=global_server_args_dict[
                &#34;speculative_accept_threshold_single&#34;
            ],
            threshold_acc=global_server_args_dict[
                &#34;speculative_accept_threshold_acc&#34;
            ],
            deterministic=True,
        )

    if SIMULATE_ACC_LEN:
        # Do simulation
        accept_index = _generate_simulated_accept_index(
            accept_index=accept_index,
            predict=predict,  # mutable
            accept_length=accept_length,  # mutable
            simulate_acc_len=SIMULATE_ACC_LEN,
            bs=bs,
            spec_steps=self.spec_steps,
        )

    unfinished_index = []
    unfinished_accept_index = []
    accept_index_cpu = accept_index.tolist()
    predict_cpu = predict.tolist()
    has_finished = False

    # Iterate every accepted token and check if req has finished after append the token
    # should be checked BEFORE free kv cache slots
    for i, (req, accept_index_row) in enumerate(zip(batch.reqs, accept_index_cpu)):
        for j, idx in enumerate(accept_index_row):
            if idx == -1:
                break
            id = predict_cpu[idx]
            req.output_ids.append(id)
            req.check_finished()
            if req.finished():
                has_finished = True
                # set all tokens after finished token to -1 and break
                accept_index[i, j + 1 :] = -1
                break
            else:
                if req.grammar is not None:
                    try:
                        req.grammar.accept_token(id)
                    except ValueError as e:
                        logger.info(
                            f&#34;{i=}, {req=}\n&#34; f&#34;{accept_index=}\n&#34; f&#34;{predict=}\n&#34;
                        )
                        raise e
        if not req.finished():
            unfinished_index.append(i)
            if idx == -1:
                unfinished_accept_index.append(accept_index[i, :j])
            else:
                unfinished_accept_index.append(accept_index[i])
        req.spec_verify_ct += 1

    if has_finished:
        accept_length = (accept_index != -1).sum(dim=1) - 1

    # Free the KV cache for unaccepted tokens
    # TODO: fuse them
    accept_index = accept_index[accept_index != -1]
    verified_id = predict[accept_index]
    evict_mask = torch.full_like(self.draft_token, True, dtype=torch.bool)
    evict_mask[accept_index] = False

    if page_size == 1:
        # TODO: boolean array index leads to a device sync. Remove it.
        token_to_kv_pool_allocator.free(batch.out_cache_loc[evict_mask])
    else:
        if self.topk == 1:
            # Only evict full empty page. Do not evict partial empty page
            align_evict_mask_to_page_size[len(batch.seq_lens),](
                batch.seq_lens,
                evict_mask,
                page_size,
                self.draft_token_num,
                next_power_of_2(self.draft_token_num),
            )
            token_to_kv_pool_allocator.free(batch.out_cache_loc[evict_mask])
        else:
            # Shift the accepted tokens to the beginning.
            # Only evict the last part
            src_cache_loc, tgt_cache_loc, to_free_num_slots = get_src_tgt_cache_loc(
                batch.seq_lens,
                batch.out_cache_loc,
                accept_index,
                accept_length,
                self.draft_token_num,
                page_size,
            )
            to_free_slots = torch.empty(
                (to_free_num_slots.sum().item(),),
                dtype=torch.int64,
                device=to_free_num_slots.device,
            )

            # out_cache_loc: [0  1  2,  3  4  5,  6  7  8]
            # accept_index:  [0 -1  2,  3  4 -1,  6 -1 -1]
            # tgt_cache_loc: [0  1   ,  3  4   ,  6      ]
            # to_free_slots: [      2,        5,     7  8]
            # to_free_slots also needs to be page-aligned without the first partial page
            #
            # split each row of out_cache_loc into two parts.
            # 1. the first part goes to tgt_cache_loc. length = accept_length[i] + 1
            # 2. the second part goes to to_free_slots.
            get_target_cache_loc[(bs,)](
                tgt_cache_loc,
                to_free_slots,
                accept_length,
                to_free_num_slots,
                batch.out_cache_loc,
                self.draft_token_num,
                next_power_of_2(self.draft_token_num),
                next_power_of_2(bs),
            )

            # Free the kv cache
            token_to_kv_pool_allocator.free(to_free_slots)

            # Copy the kv cache
            batch.token_to_kv_pool_allocator.get_kvcache().move_kv_cache(
                tgt_cache_loc, src_cache_loc
            )

    # Construct EagleVerifyOutput
    if not has_finished:
        if page_size == 1 or self.topk == 1:
            batch.out_cache_loc = batch.out_cache_loc[accept_index]
            assign_req_to_token_pool[(bs,)](
                batch.req_pool_indices,
                batch.req_to_token_pool.req_to_token,
                batch.seq_lens,
                batch.seq_lens + accept_length + 1,
                batch.out_cache_loc,
                batch.req_to_token_pool.req_to_token.shape[1],
                next_power_of_2(bs),
            )
        else:
            batch.out_cache_loc = tgt_cache_loc
        batch.seq_lens.add_(accept_length + 1)

        draft_input = EagleDraftInput(
            hidden_states=batch.spec_info.hidden_states[accept_index],
            verified_id=verified_id,
            accept_length=accept_length,
            accept_length_cpu=accept_length.tolist(),
            seq_lens_for_draft_extend=batch.seq_lens,
            req_pool_indices_for_draft_extend=batch.req_pool_indices,
        )

        return EagleVerifyOutput(
            draft_input=draft_input,
            logits_output=logits_output,
            verified_id=verified_id,
            accept_length_per_req_cpu=draft_input.accept_length_cpu,
            accepted_indices=accept_index,
        )
    else:
        if page_size == 1 or self.topk == 1:
            assign_req_to_token_pool[(bs,)](
                batch.req_pool_indices,
                batch.req_to_token_pool.req_to_token,
                batch.seq_lens,
                batch.seq_lens + accept_length + 1,
                batch.out_cache_loc[accept_index],
                batch.req_to_token_pool.req_to_token.shape[1],
                next_power_of_2(bs),
            )
            batch.seq_lens.add_(accept_length + 1)

        accept_length_cpu = accept_length.tolist()
        if len(unfinished_accept_index) &gt; 0:
            unfinished_accept_index = torch.cat(unfinished_accept_index)
            unfinished_index_device = torch.tensor(
                unfinished_index, dtype=torch.int64, device=predict.device
            )
            draft_input_accept_length_cpu = [
                accept_length_cpu[i] for i in unfinished_index
            ]
            if page_size == 1 or self.topk == 1:
                batch.out_cache_loc = batch.out_cache_loc[unfinished_accept_index]
            else:
                batch.out_cache_loc = torch.empty(
                    len(unfinished_index) + sum(draft_input_accept_length_cpu),
                    dtype=torch.int64,
                    device=predict.device,
                )
                accept_length_filter = create_accept_length_filter(
                    accept_length,
                    unfinished_index_device,
                    batch.seq_lens,
                )
                filter_finished_cache_loc_kernel[(bs,)](
                    batch.out_cache_loc,
                    tgt_cache_loc,
                    accept_length,
                    accept_length_filter,
                    next_power_of_2(bs),
                    next_power_of_2(self.draft_token_num),
                )

            draft_input = EagleDraftInput(
                hidden_states=batch.spec_info.hidden_states[
                    unfinished_accept_index
                ],
                verified_id=predict[unfinished_accept_index],
                accept_length_cpu=draft_input_accept_length_cpu,
                accept_length=accept_length[unfinished_index_device],
                seq_lens_for_draft_extend=batch.seq_lens[unfinished_index_device],
                req_pool_indices_for_draft_extend=batch.req_pool_indices[
                    unfinished_index_device
                ],
            )
        else:
            draft_input = EagleDraftInput.create_idle_input(
                device=batch.device,
                hidden_size=batch.model_config.hidden_size,
                dtype=batch.model_config.dtype,
                topk=self.topk,
                capture_hidden_mode=CaptureHiddenMode.LAST,
            )

        return EagleVerifyOutput(
            draft_input=draft_input,
            logits_output=logits_output,
            verified_id=verified_id,
            accept_length_per_req_cpu=accept_length_cpu,
            accepted_indices=accept_index,
        )</code></pre>
</details>
<div class="desc"><p>Verify and find accepted tokens based on logits output and batch
(which contains spec decoding information).</p>
<p>WARNING: This API in-place modifies the states of logits_output</p>
<p>This API updates values inside logits_output based on the accepted
tokens. I.e., logits_output.next_token_logits only contains
accepted token logits.</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyOutput"><code class="flex name class">
<span>class <span class="ident">EagleVerifyOutput</span></span>
<span>(</span><span>draft_input: <a title="sglang.srt.speculative.eagle_utils.EagleDraftInput" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput">EagleDraftInput</a>,<br>logits_output: LogitsProcessorOutput,<br>verified_id: torch.Tensor,<br>accept_length_per_req_cpu: List[int],<br>accepted_indices: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class EagleVerifyOutput:
    # Draft input batch
    draft_input: EagleDraftInput
    # Logit outputs from target worker
    logits_output: LogitsProcessorOutput
    # Accepted token ids including the bonus token
    verified_id: torch.Tensor
    # Accepted token length per sequence in a batch in CPU.
    accept_length_per_req_cpu: List[int]
    # Accepted indices from logits_output.next_token_logits
    accepted_indices: torch.Tensor</code></pre>
</details>
<div class="desc"><p>EagleVerifyOutput(draft_input: 'EagleDraftInput', logits_output: 'LogitsProcessorOutput', verified_id: 'torch.Tensor', accept_length_per_req_cpu: 'List[int]', accepted_indices: 'torch.Tensor')</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyOutput.accept_length_per_req_cpu"><code class="name">var <span class="ident">accept_length_per_req_cpu</span> : List[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyOutput.accepted_indices"><code class="name">var <span class="ident">accepted_indices</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyOutput.draft_input"><code class="name">var <span class="ident">draft_input</span> : <a title="sglang.srt.speculative.eagle_utils.EagleDraftInput" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput">EagleDraftInput</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyOutput.logits_output"><code class="name">var <span class="ident">logits_output</span> : <a title="sglang.srt.layers.logits_processor.LogitsProcessorOutput" href="../layers/logits_processor.html#sglang.srt.layers.logits_processor.LogitsProcessorOutput">LogitsProcessorOutput</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_utils.EagleVerifyOutput.verified_id"><code class="name">var <span class="ident">verified_id</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.speculative" href="index.html">sglang.srt.speculative</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.speculative.eagle_utils.create_accept_length_filter" href="#sglang.srt.speculative.eagle_utils.create_accept_length_filter">create_accept_length_filter</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.generate_token_bitmask" href="#sglang.srt.speculative.eagle_utils.generate_token_bitmask">generate_token_bitmask</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.get_src_tgt_cache_loc" href="#sglang.srt.speculative.eagle_utils.get_src_tgt_cache_loc">get_src_tgt_cache_loc</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.select_top_k_tokens" href="#sglang.srt.speculative.eagle_utils.select_top_k_tokens">select_top_k_tokens</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.traverse_tree" href="#sglang.srt.speculative.eagle_utils.traverse_tree">traverse_tree</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput">EagleDraftInput</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput.accept_length" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput.accept_length">accept_length</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput.accept_length_cpu" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput.accept_length_cpu">accept_length_cpu</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput.capture_hidden_mode" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput.capture_hidden_mode">capture_hidden_mode</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput.create_idle_input" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput.create_idle_input">create_idle_input</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput.filter_batch" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput.filter_batch">filter_batch</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput.generate_attn_arg_prefill" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput.generate_attn_arg_prefill">generate_attn_arg_prefill</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput.hidden_states" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput.hidden_states">hidden_states</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput.kv_indices" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput.kv_indices">kv_indices</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput.kv_indptr" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput.kv_indptr">kv_indptr</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput.merge_batch" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput.merge_batch">merge_batch</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput.num_tokens_for_logprob_per_batch" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput.num_tokens_for_logprob_per_batch">num_tokens_for_logprob_per_batch</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput.num_tokens_per_batch" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput.num_tokens_per_batch">num_tokens_per_batch</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput.prepare_extend_after_decode" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput.prepare_extend_after_decode">prepare_extend_after_decode</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput.prepare_for_extend" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput.prepare_for_extend">prepare_for_extend</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput.req_pool_indices_for_draft_extend" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput.req_pool_indices_for_draft_extend">req_pool_indices_for_draft_extend</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput.seq_lens_for_draft_extend" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput.seq_lens_for_draft_extend">seq_lens_for_draft_extend</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput.topk_index" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput.topk_index">topk_index</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput.topk_p" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput.topk_p">topk_p</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleDraftInput.verified_id" href="#sglang.srt.speculative.eagle_utils.EagleDraftInput.verified_id">verified_id</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput">EagleVerifyInput</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput.capture_hidden_mode" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput.capture_hidden_mode">capture_hidden_mode</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput.create_idle_input" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput.create_idle_input">create_idle_input</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput.custom_mask" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput.custom_mask">custom_mask</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput.draft_token" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput.draft_token">draft_token</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput.draft_token_num" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput.draft_token_num">draft_token_num</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput.generate_attn_arg_prefill" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput.generate_attn_arg_prefill">generate_attn_arg_prefill</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput.grammar" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput.grammar">grammar</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput.positions" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput.positions">positions</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput.prepare_for_verify" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput.prepare_for_verify">prepare_for_verify</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput.retrive_cum_len" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput.retrive_cum_len">retrive_cum_len</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput.retrive_index" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput.retrive_index">retrive_index</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput.retrive_next_sibling" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput.retrive_next_sibling">retrive_next_sibling</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput.retrive_next_token" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput.retrive_next_token">retrive_next_token</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput.seq_lens_cpu" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput.seq_lens_cpu">seq_lens_cpu</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput.seq_lens_sum" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput.seq_lens_sum">seq_lens_sum</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput.spec_steps" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput.spec_steps">spec_steps</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput.topk" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput.topk">topk</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput.verify" href="#sglang.srt.speculative.eagle_utils.EagleVerifyInput.verify">verify</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyOutput" href="#sglang.srt.speculative.eagle_utils.EagleVerifyOutput">EagleVerifyOutput</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyOutput.accept_length_per_req_cpu" href="#sglang.srt.speculative.eagle_utils.EagleVerifyOutput.accept_length_per_req_cpu">accept_length_per_req_cpu</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyOutput.accepted_indices" href="#sglang.srt.speculative.eagle_utils.EagleVerifyOutput.accepted_indices">accepted_indices</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyOutput.draft_input" href="#sglang.srt.speculative.eagle_utils.EagleVerifyOutput.draft_input">draft_input</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyOutput.logits_output" href="#sglang.srt.speculative.eagle_utils.EagleVerifyOutput.logits_output">logits_output</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_utils.EagleVerifyOutput.verified_id" href="#sglang.srt.speculative.eagle_utils.EagleVerifyOutput.verified_id">verified_id</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
