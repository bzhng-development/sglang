<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.speculative.eagle_worker API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.speculative.eagle_worker</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.speculative.eagle_worker.draft_tp_context"><code class="name flex">
<span>def <span class="ident">draft_tp_context</span></span>(<span>tp_group: <a title="sglang.srt.distributed.parallel_state.GroupCoordinator" href="../distributed/parallel_state.html#sglang.srt.distributed.parallel_state.GroupCoordinator">GroupCoordinator</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def draft_tp_context(tp_group: GroupCoordinator):
    # Draft model doesn&#39;t use dp and has its own tp group.
    # We disable mscclpp now because it doesn&#39;t support 2 comm groups.
    with patch_tensor_parallel_group(tp_group):
        yield</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_worker.get_last_loc_large_page_size_large_top_k"><code class="name flex">
<span>def <span class="ident">get_last_loc_large_page_size_large_top_k</span></span>(<span>req_to_token: torch.Tensor,<br>req_pool_indices: torch.Tensor,<br>seq_lens: torch.Tensor,<br>speculative_num_steps: int,<br>topk: int,<br>page_size: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_last_loc_large_page_size_large_top_k(
    req_to_token: torch.Tensor,
    req_pool_indices: torch.Tensor,
    seq_lens: torch.Tensor,
    speculative_num_steps: int,
    topk: int,
    page_size: int,
):
    prefix_lens = seq_lens
    last_page_lens = prefix_lens % page_size
    num_new_pages_per_topk = (
        last_page_lens + speculative_num_steps + page_size - 1
    ) // page_size
    seq_lens = prefix_lens // page_size * page_size + num_new_pages_per_topk * (
        page_size * topk
    )
    extend_lens = seq_lens - prefix_lens
    last_loc = get_last_loc(
        req_to_token,
        req_pool_indices,
        prefix_lens,
    )

    return prefix_lens, seq_lens, last_loc, num_new_pages_per_topk, extend_lens</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_worker.get_last_loc_large_page_size_top_k_1"><code class="name flex">
<span>def <span class="ident">get_last_loc_large_page_size_top_k_1</span></span>(<span>req_to_token: torch.Tensor,<br>req_pool_indices: torch.Tensor,<br>seq_lens,<br>speculative_num_steps: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.compile(dynamic=True)
def get_last_loc_large_page_size_top_k_1(
    req_to_token: torch.Tensor,
    req_pool_indices: torch.Tensor,
    seq_lens,
    speculative_num_steps: int,
):
    prefix_lens = seq_lens
    seq_lens = prefix_lens + speculative_num_steps
    last_loc = get_last_loc(
        req_to_token,
        req_pool_indices,
        prefix_lens,
    )
    return prefix_lens, seq_lens, last_loc</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_worker.load_token_map"><code class="name flex">
<span>def <span class="ident">load_token_map</span></span>(<span>token_map_path: str) ‑> List[int]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_token_map(token_map_path: str) -&gt; List[int]:
    if not os.path.exists(token_map_path):
        cache_dir = snapshot_download(
            os.path.dirname(token_map_path),
            ignore_patterns=[&#34;*.bin&#34;, &#34;*.safetensors&#34;],
        )
        token_map_path = os.path.join(cache_dir, os.path.basename(token_map_path))
    hot_token_id = torch.load(token_map_path, weights_only=True)
    return torch.tensor(hot_token_id, dtype=torch.int64)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.speculative.eagle_worker.EAGLEWorker"><code class="flex name class">
<span>class <span class="ident">EAGLEWorker</span></span>
<span>(</span><span>server_args: <a title="sglang.srt.server_args.ServerArgs" href="../server_args.html#sglang.srt.server_args.ServerArgs">ServerArgs</a>,<br>gpu_id: int,<br>tp_rank: int,<br>dp_rank: int | None,<br>moe_ep_rank: int,<br>nccl_port: int,<br>target_worker: <a title="sglang.srt.managers.tp_worker.TpModelWorker" href="../managers/tp_worker.html#sglang.srt.managers.tp_worker.TpModelWorker">TpModelWorker</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EAGLEWorker(TpModelWorker):

    def __init__(
        self,
        server_args: ServerArgs,
        gpu_id: int,
        tp_rank: int,
        dp_rank: Optional[int],
        moe_ep_rank: int,
        nccl_port: int,
        target_worker: TpModelWorker,
    ):
        # Parse arguments
        self.server_args = server_args
        self.topk = server_args.speculative_eagle_topk
        self.speculative_num_steps = server_args.speculative_num_steps
        self.speculative_num_draft_tokens = server_args.speculative_num_draft_tokens
        self.enable_nan_detection = server_args.enable_nan_detection
        self.gpu_id = gpu_id
        self.device = server_args.device
        self.target_worker = target_worker
        self.page_size = server_args.page_size
        self.speculative_algorithm = SpeculativeAlgorithm.from_string(
            server_args.speculative_algorithm
        )
        self.padded_static_len = -1

        # Override the context length of the draft model to be the same as the target model.
        server_args.context_length = target_worker.model_runner.model_config.context_len

        # Do not capture cuda graph in `super().__init__()`
        # It will be captured later.
        backup_disable_cuda_graph = server_args.disable_cuda_graph
        server_args.disable_cuda_graph = True
        # Share the allocator with a target worker.
        # Draft and target worker own their own KV cache pools.
        self.req_to_token_pool, self.token_to_kv_pool_allocator = (
            target_worker.get_memory_pool()
        )

        # Load hot token ids
        if self.speculative_algorithm.is_eagle3():
            if server_args.speculative_token_map is not None:
                logger.warning(
                    &#34;Speculative token map specified, but EAGLE3 models already have this. Ignoring the specified token map.&#34;
                )
            self.hot_token_id = None
        elif server_args.speculative_token_map is not None:
            self.hot_token_id = load_token_map(server_args.speculative_token_map)
            server_args.json_model_override_args = (
                f&#39;{{&#34;hot_vocab_size&#34;: {len(self.hot_token_id)}}}&#39;
            )
        else:
            self.hot_token_id = None

        # Init draft worker
        with empty_context():
            super().__init__(
                server_args=server_args,
                gpu_id=gpu_id,
                tp_rank=tp_rank,
                pp_rank=0,  # FIXME
                dp_rank=dp_rank,
                moe_ep_rank=moe_ep_rank,
                nccl_port=nccl_port,
                is_draft_worker=True,
                req_to_token_pool=self.req_to_token_pool,
                token_to_kv_pool_allocator=self.token_to_kv_pool_allocator,
            )

        embed, head = self.target_worker.model_runner.model.get_embed_and_head()

        if self.speculative_algorithm.is_eagle3():
            # most cases EAGLE3 models don&#39;t share lm_head
            # but some models (e.g. nvidia/gpt-oss-120b-Eagle3) shares
            if (
                hasattr(self.draft_model_runner.model, &#34;load_lm_head_from_target&#34;)
                and self.draft_model_runner.model.load_lm_head_from_target
            ):
                self.draft_model_runner.model.set_embed_and_head(embed, head)
            else:
                self.draft_model_runner.model.set_embed(embed)

            # grab hot token ids
            if self.draft_model_runner.model.hot_token_id is not None:
                self.hot_token_id = self.draft_model_runner.model.hot_token_id.to(
                    embed.device
                )

        else:
            if self.hot_token_id is not None:
                head = head.clone()
                self.hot_token_id = self.hot_token_id.to(head.device)
                head.data = head.data[self.hot_token_id]

            # Share the embedding and lm_head
            self.draft_model_runner.model.set_embed_and_head(embed, head)

        # Init attention backend and cuda graphs
        self.draft_model_runner.server_args.disable_cuda_graph = (
            backup_disable_cuda_graph
        )
        self.draft_tp_context = (
            draft_tp_context if server_args.enable_dp_attention else empty_context
        )
        with self.draft_tp_context(self.draft_model_runner.tp_group):
            self.init_attention_backend()
            self.init_cuda_graphs()

        # Some dummy tensors
        self.num_new_pages_per_topk = torch.empty(
            (), dtype=torch.int64, device=self.device
        )
        self.extend_lens = torch.empty((), dtype=torch.int64, device=self.device)

    def init_attention_backend(self):
        # Create multi-step attn backends and cuda graph runners

        self.has_prefill_wrapper_verify = False
        self.draft_extend_attn_backend = None

        if self.server_args.attention_backend == &#34;flashinfer&#34;:
            if not global_server_args_dict[&#34;use_mla_backend&#34;]:
                from sglang.srt.layers.attention.flashinfer_backend import (
                    FlashInferAttnBackend,
                    FlashInferMultiStepDraftBackend,
                )

                self.draft_attn_backend = FlashInferMultiStepDraftBackend(
                    self.draft_model_runner,
                    self.topk,
                    self.speculative_num_steps,
                )
                self.draft_extend_attn_backend = FlashInferAttnBackend(
                    self.draft_model_runner,
                    skip_prefill=False,
                )
            else:
                from sglang.srt.layers.attention.flashinfer_mla_backend import (
                    FlashInferMLAAttnBackend,
                    FlashInferMLAMultiStepDraftBackend,
                )

                self.draft_attn_backend = FlashInferMLAMultiStepDraftBackend(
                    self.draft_model_runner,
                    self.topk,
                    self.speculative_num_steps,
                )
                self.draft_extend_attn_backend = FlashInferMLAAttnBackend(
                    self.draft_model_runner,
                    skip_prefill=False,
                )
            self.has_prefill_wrapper_verify = True
        elif self.server_args.attention_backend == &#34;triton&#34;:
            from sglang.srt.layers.attention.triton_backend import (
                TritonAttnBackend,
                TritonMultiStepDraftBackend,
            )

            self.draft_attn_backend = TritonMultiStepDraftBackend(
                self.draft_model_runner,
                self.topk,
                self.speculative_num_steps,
            )
            self.draft_extend_attn_backend = TritonAttnBackend(
                self.draft_model_runner,
                skip_prefill=False,
            )
        elif self.server_args.attention_backend == &#34;aiter&#34;:
            from sglang.srt.layers.attention.aiter_backend import (
                AiterAttnBackend,
                AiterMultiStepDraftBackend,
            )

            self.draft_attn_backend = AiterMultiStepDraftBackend(
                self.draft_model_runner,
                self.topk,
                self.speculative_num_steps,
            )
            self.draft_extend_attn_backend = AiterAttnBackend(
                self.draft_model_runner,
                skip_prefill=False,
            )
            self.has_prefill_wrapper_verify = False
        elif self.server_args.attention_backend == &#34;fa3&#34;:
            from sglang.srt.layers.attention.flashattention_backend import (
                FlashAttentionBackend,
                FlashAttentionMultiStepBackend,
            )

            self.draft_attn_backend = FlashAttentionMultiStepBackend(
                self.draft_model_runner,
                self.topk,
                self.speculative_num_steps,
            )
            self.draft_extend_attn_backend = FlashAttentionBackend(
                self.draft_model_runner,
                skip_prefill=False,
            )
        elif self.server_args.attention_backend == &#34;flashmla&#34;:
            from sglang.srt.layers.attention.flashmla_backend import (
                FlashMLAMultiStepDraftBackend,
            )

            self.draft_attn_backend = FlashMLAMultiStepDraftBackend(
                self.draft_model_runner,
                self.topk,
                self.speculative_num_steps,
            )
        elif self.server_args.attention_backend == &#34;trtllm_mha&#34;:
            from sglang.srt.layers.attention.trtllm_mha_backend import (
                TRTLLMHAAttnBackend,
                TRTLLMHAAttnMultiStepDraftBackend,
            )

            self.draft_attn_backend = TRTLLMHAAttnMultiStepDraftBackend(
                self.draft_model_runner,
                self.topk,
                self.speculative_num_steps,
            )
            self.draft_extend_attn_backend = TRTLLMHAAttnBackend(
                self.draft_model_runner,
                skip_prefill=False,
            )
            self.has_prefill_wrapper_verify = True
        elif self.server_args.attention_backend == &#34;trtllm_mla&#34;:
            if not global_server_args_dict[&#34;use_mla_backend&#34;]:
                raise ValueError(
                    &#34;trtllm_mla backend requires MLA model (use_mla_backend=True).&#34;
                )

            from sglang.srt.layers.attention.trtllm_mla_backend import (
                TRTLLMMLABackend,
                TRTLLMMLAMultiStepDraftBackend,
            )

            self.draft_attn_backend = TRTLLMMLAMultiStepDraftBackend(
                self.draft_model_runner,
                self.topk,
                self.speculative_num_steps,
            )
            self.draft_extend_attn_backend = TRTLLMMLABackend(
                self.draft_model_runner,
                skip_prefill=False,
            )
            self.has_prefill_wrapper_verify = True
        else:
            raise ValueError(
                f&#34;EAGLE is not supported in attention backend {self.server_args.attention_backend}&#34;
            )

        self.draft_model_runner.draft_attn_backend = self.draft_attn_backend

    def init_cuda_graphs(self):
        &#34;&#34;&#34;Capture cuda graphs.&#34;&#34;&#34;
        self.cuda_graph_runner = None
        self.cuda_graph_runner_for_draft_extend = None

        if self.server_args.disable_cuda_graph:
            return

        # Capture draft
        tic = time.perf_counter()
        before_mem = get_available_gpu_memory(self.device, self.gpu_id)
        logger.info(
            f&#34;Capture draft cuda graph begin. This can take up to several minutes. avail mem={before_mem:.2f} GB&#34;
        )
        self.cuda_graph_runner = EAGLEDraftCudaGraphRunner(self)
        after_mem = get_available_gpu_memory(self.device, self.gpu_id)
        logger.info(
            f&#34;Capture draft cuda graph end. Time elapsed: {time.perf_counter() - tic:.2f} s. mem usage={(before_mem - after_mem):.2f} GB. avail mem={after_mem:.2f} GB.&#34;
        )

        # Capture extend
        if self.draft_extend_attn_backend:
            tic = time.perf_counter()
            before_mem = get_available_gpu_memory(self.device, self.gpu_id)
            logger.info(
                f&#34;Capture draft extend cuda graph begin. This can take up to several minutes. avail mem={before_mem:.2f} GB&#34;
            )
            self.cuda_graph_runner_for_draft_extend = EAGLEDraftExtendCudaGraphRunner(
                self
            )
            after_mem = get_available_gpu_memory(self.device, self.gpu_id)
            logger.info(
                f&#34;Capture draft extend cuda graph end. Time elapsed: {time.perf_counter() - tic:.2f} s. mem usage={(before_mem - after_mem):.2f} GB. avail mem={after_mem:.2f} GB.&#34;
            )

    @property
    def draft_model_runner(self):
        return self.model_runner

    def forward_batch_speculative_generation(
        self, batch: ScheduleBatch
    ) -&gt; Tuple[LogitsProcessorOutput, torch.Tensor, int, int, bool]:
        &#34;&#34;&#34;Run speculative decoding forward.

        NOTE: Many states of batch is modified as you go through. It is not guaranteed that
        the final output batch have the same state as the input.

        Args:
            batch: The batch to run forward. The state of the batch is modified as it runs.
        Returns:
            A tuple of the final logit output of the target model, next tokens accepted,
            the batch id (used for overlap schedule), and number of accepted tokens.
        &#34;&#34;&#34;
        if batch.forward_mode.is_extend() or batch.is_extend_in_batch:
            logits_output, next_token_ids, bid, seq_lens_cpu = (
                self.forward_target_extend(batch)
            )
            with self.draft_tp_context(self.draft_model_runner.tp_group):
                self.forward_draft_extend(
                    batch, logits_output.hidden_states, next_token_ids, seq_lens_cpu
                )
            return logits_output, next_token_ids, bid, 0, False
        else:
            with self.draft_tp_context(self.draft_model_runner.tp_group):
                spec_info = self.draft(batch)
            logits_output, verify_output, model_worker_batch, can_run_cuda_graph = (
                self.verify(batch, spec_info)
            )

            with self.draft_tp_context(self.draft_model_runner.tp_group):
                # NOTE: We should use `check_forward_draft_extend_after_decode`
                # when DP attention is enabled, but it is slow. Skip it for now.
                if (
                    self.server_args.enable_dp_attention
                    or batch.spec_info.verified_id.shape[0] &gt; 0
                ):
                    # decode is not finished
                    self.forward_draft_extend_after_decode(batch)

            return (
                logits_output,
                verify_output.verified_id,
                model_worker_batch.bid,
                sum(verify_output.accept_length_per_req_cpu),
                can_run_cuda_graph,
            )

    def check_forward_draft_extend_after_decode(self, batch: ScheduleBatch):
        local_need_forward = batch.spec_info.verified_id.shape[0] &gt; 0
        if not self.server_args.enable_dp_attention:
            return local_need_forward

        global_need_forward = torch.tensor(
            [
                (local_need_forward),
            ],
            dtype=torch.int64,
        )
        torch.distributed.all_reduce(
            global_need_forward, group=get_tp_group().cpu_group
        )
        global_need_forward_cnt = global_need_forward[0].item()
        need_forward = global_need_forward_cnt &gt; 0
        return need_forward

    def forward_target_extend(
        self, batch: ScheduleBatch
    ) -&gt; Tuple[LogitsProcessorOutput, torch.Tensor, int, Optional[torch.Tensor]]:
        &#34;&#34;&#34;Run the target extend.

        Args:
            batch: The batch to run. States could be modified.

        Returns:
            logits_output: The output of logits. It will contain the full hidden states.
            next_token_ids: Next token ids generated.
            bid: The model batch ID. Used for overlap schedule.
        &#34;&#34;&#34;
        # Forward with the target model and get hidden states.
        # We need the full hidden states to prefill the KV cache of the draft model.
        model_worker_batch = batch.get_model_worker_batch()
        model_worker_batch.capture_hidden_mode = CaptureHiddenMode.FULL
        logits_output, next_token_ids, _ = self.target_worker.forward_batch_generation(
            model_worker_batch
        )
        return (
            logits_output,
            next_token_ids,
            model_worker_batch.bid,
            model_worker_batch.seq_lens_cpu,
        )

    def _draft_preprocess_decode(self, batch: ScheduleBatch):
        # Parse args
        num_seqs = batch.batch_size()
        spec_info = batch.spec_info

        # Accumulate penalty
        if batch.sampling_info.penalizer_orchestrator.is_required:
            # This is a relaxed version of penalties for speculative decoding.
            batch.sampling_info.penalizer_orchestrator.cumulate_output_tokens(
                spec_info.verified_id.to(torch.int64)
            )

        # Allocate cache locations
        # Layout of the out_cache_loc
        # [       topk 0         ] [       topk 1         ]
        # [iter=0, iter=1, iter=2] [iter=0, iter=1, iter=2]
        if self.page_size == 1:
            out_cache_loc, token_to_kv_pool_state_backup = batch.alloc_token_slots(
                num_seqs * self.speculative_num_steps * self.topk, backup_state=True
            )
        else:
            if self.topk == 1:
                prefix_lens, seq_lens, last_loc = get_last_loc_large_page_size_top_k_1(
                    batch.req_to_token_pool.req_to_token,
                    batch.req_pool_indices,
                    batch.seq_lens,
                    self.speculative_num_steps,
                )
                extend_num_tokens = num_seqs * self.speculative_num_steps
            else:
                # In this case, the last partial page needs to be duplicated.
                # KV cache layout in batch.req_to_token_pool.req_to_token:
                #
                # | -------- | -- xxxx .. | -- xxxx .. | -- xxxx .. |
                #    prefix     top-k = 0    tok-k = 1    top-k = 2
                #
                #  &#34;-&#34; means prefix tokens
                #  &#34;x&#34; means speculative draft tokens
                #  &#34;.&#34; means padded tokens

                # TODO(lmzheng): The current implementation is still a fake support
                # for page size &gt; 1. In the `assign_draft_cache_locs` below,
                # we directly move the indices instead of the real kv cache.
                # This only works when the kernel backend runs with page size = 1.
                # If the kernel backend runs with page size &gt; 1, we need to
                # duplicate the real KV cache. The overhead of duplicating KV
                # cache seems okay because the draft KV cache only has one layer.
                # see a related copy operation in MHATokenToKVPool::move_kv_cache.

                (
                    prefix_lens,
                    seq_lens,
                    last_loc,
                    self.num_new_pages_per_topk,
                    self.extend_lens,
                ) = get_last_loc_large_page_size_large_top_k(
                    batch.req_to_token_pool.req_to_token,
                    batch.req_pool_indices,
                    batch.seq_lens,
                    self.speculative_num_steps,
                    self.topk,
                    self.page_size,
                )

                # TODO(lmzheng): remove this device sync
                extend_num_tokens = torch.sum(self.extend_lens).item()

            out_cache_loc, token_to_kv_pool_state_backup = (
                batch.alloc_paged_token_slots_extend(
                    prefix_lens,
                    seq_lens,
                    last_loc,
                    extend_num_tokens,
                    backup_state=True,
                )
            )

        assign_draft_cache_locs[(num_seqs,)](
            batch.req_pool_indices,
            batch.req_to_token_pool.req_to_token,
            batch.seq_lens,
            self.extend_lens,
            self.num_new_pages_per_topk,
            out_cache_loc,
            batch.req_to_token_pool.req_to_token.shape[1],
            self.topk,
            self.speculative_num_steps,
            self.page_size,
            next_power_of_2(num_seqs),
            next_power_of_2(self.speculative_num_steps),
        )

        if self.page_size &gt; 1 and self.topk &gt; 1:
            # Remove padded slots
            out_cache_loc = out_cache_loc[
                : num_seqs * self.topk * self.speculative_num_steps
            ]

        batch.out_cache_loc = out_cache_loc
        batch.seq_lens_sum = torch.sum(batch.seq_lens).item()
        batch.return_hidden_states = False
        spec_info.positions = batch.seq_lens.repeat_interleave(self.topk, dim=0)
        self.token_to_kv_pool_allocator.restore_state(token_to_kv_pool_state_backup)

    def _draft_preprocess_idle(self, batch: ScheduleBatch):
        batch.spec_info = EagleDraftInput.create_idle_input(
            device=self.device,
            hidden_size=self.model_config.hidden_size,
            dtype=self.model_config.dtype,
            topk=self.topk,
            capture_hidden_mode=CaptureHiddenMode.LAST,
        )

    def draft(self, batch: ScheduleBatch):
        # Parse args
        if batch.forward_mode.is_idle():
            self._draft_preprocess_idle(batch)
        else:
            self._draft_preprocess_decode(batch)

        spec_info = batch.spec_info
        assert isinstance(spec_info, EagleDraftInput)

        spec_info.capture_hidden_mode = CaptureHiddenMode.LAST
        spec_info.num_tokens_per_batch = self.topk
        spec_info.num_tokens_for_logprob_per_batch = self.topk
        batch.return_hidden_states = False

        # Get forward batch
        model_worker_batch = batch.get_model_worker_batch()
        assert model_worker_batch.capture_hidden_mode == CaptureHiddenMode.LAST
        forward_batch = ForwardBatch.init_new(
            model_worker_batch, self.draft_model_runner
        )
        can_cuda_graph = self.cuda_graph_runner and self.cuda_graph_runner.can_run(
            forward_batch
        )
        if can_cuda_graph:
            score_list, token_list, parents_list = self.cuda_graph_runner.replay(
                forward_batch
            )
        else:
            forward_batch.can_run_dp_cuda_graph = False
            if not forward_batch.forward_mode.is_idle():
                # Initialize attention backend
                self.draft_attn_backend.init_forward_metadata(forward_batch)
            # Run forward steps
            score_list, token_list, parents_list = self.draft_forward(forward_batch)

        if batch.forward_mode.is_idle():
            return EagleVerifyInput.create_idle_input(
                self.topk,
                self.speculative_num_steps,
                self.speculative_num_draft_tokens,
            )

        (
            tree_mask,
            position,
            retrive_index,
            retrive_next_token,
            retrive_next_sibling,
            draft_tokens,
        ) = build_tree_kernel_efficient(
            spec_info.verified_id,
            score_list,
            token_list,
            parents_list,
            batch.seq_lens,
            batch.seq_lens_sum,
            self.topk,
            self.speculative_num_steps,
            self.speculative_num_draft_tokens,
        )

        return EagleVerifyInput(
            draft_token=draft_tokens,
            custom_mask=tree_mask,
            positions=position,
            retrive_index=retrive_index,
            retrive_next_token=retrive_next_token,
            retrive_next_sibling=retrive_next_sibling,
            retrive_cum_len=None,
            spec_steps=self.speculative_num_steps,
            topk=self.topk,
            draft_token_num=self.server_args.speculative_num_draft_tokens,
            capture_hidden_mode=CaptureHiddenMode.FULL,
            seq_lens_sum=forward_batch.seq_lens_sum,
            seq_lens_cpu=forward_batch.seq_lens_cpu,
        )

    def draft_forward(self, forward_batch: ForwardBatch):
        # Parse args
        spec_info = forward_batch.spec_info
        assert isinstance(spec_info, EagleDraftInput)
        out_cache_loc = forward_batch.out_cache_loc
        topk_p, topk_index, hidden_states = (
            spec_info.topk_p,
            spec_info.topk_index,
            spec_info.hidden_states,
        )
        if self.hot_token_id is not None:
            topk_index = self.hot_token_id[topk_index]

        out_cache_loc = out_cache_loc.reshape(
            forward_batch.batch_size, self.topk, self.speculative_num_steps
        )
        out_cache_loc = out_cache_loc.permute((2, 0, 1)).reshape(
            self.speculative_num_steps, -1
        )

        # Return values
        score_list: List[torch.Tensor] = []
        token_list: List[torch.Tensor] = []
        parents_list: List[torch.Tensor] = []

        # Forward multiple steps
        scores = None
        for i in range(self.speculative_num_steps):
            input_ids, hidden_states, scores, tree_info = select_top_k_tokens(
                i, topk_p, topk_index, hidden_states, scores, self.topk
            )
            score_list.append(tree_info[0])
            token_list.append(tree_info[1])
            parents_list.append(tree_info[2])

            # We don&#39;t need to run the last forward. we get 1 token from draft prefill and (#spec steps - 1) tokens here
            if i == self.speculative_num_steps - 1:
                break

            # Set inputs
            forward_batch.input_ids = input_ids
            forward_batch.out_cache_loc = out_cache_loc[i]
            forward_batch.positions.add_(1)
            forward_batch.attn_backend = self.draft_attn_backend.attn_backends[i]
            spec_info.hidden_states = hidden_states

            # Run forward
            logits_output, _ = self.draft_model_runner.forward(
                forward_batch, skip_attn_backend_init=True
            )
            self._detect_nan_if_needed(logits_output)
            probs = torch.softmax(logits_output.next_token_logits, dim=-1)
            topk_p, topk_index = fast_topk(probs, self.topk, dim=-1)
            if self.hot_token_id is not None:
                topk_index = self.hot_token_id[topk_index]
            hidden_states = logits_output.hidden_states

        return score_list, token_list, parents_list

    def verify(self, batch: ScheduleBatch, spec_info: EagleVerifyInput):
        spec_info.prepare_for_verify(batch, self.page_size)
        batch.return_hidden_states = False
        batch.forward_mode = (
            ForwardMode.TARGET_VERIFY
            if not batch.forward_mode.is_idle()
            else ForwardMode.IDLE
        )
        batch.spec_info = spec_info

        model_worker_batch = batch.get_model_worker_batch(
            seq_lens_cpu_cache=spec_info.seq_lens_cpu
        )
        assert model_worker_batch.capture_hidden_mode == spec_info.capture_hidden_mode

        if batch.has_grammar:
            retrieve_next_token_cpu = spec_info.retrive_next_token.cpu()
            retrieve_next_sibling_cpu = spec_info.retrive_next_sibling.cpu()
            draft_tokens_cpu = spec_info.draft_token.view(
                spec_info.retrive_next_token.shape
            ).cpu()

        # Forward
        logits_output, _, can_run_cuda_graph = (
            self.target_worker.forward_batch_generation(
                model_worker_batch, skip_sample=True
            )
        )

        vocab_mask = None
        if batch.has_grammar:
            # Generate the logit mask for structured output.
            # Overlap the CPU operations for bitmask generation with the forward pass.
            vocab_mask = generate_token_bitmask(
                batch.reqs,
                spec_info,
                retrieve_next_token_cpu,
                retrieve_next_sibling_cpu,
                draft_tokens_cpu,
                batch.sampling_info.vocab_size,
            )

            if vocab_mask is not None:
                assert spec_info.grammar is not None
                vocab_mask = vocab_mask.to(spec_info.retrive_next_token.device)
                # NOTE (sk): otherwise, this vocab mask will be the one from the previous extend stage
                # and will be applied to produce wrong results
                batch.sampling_info.vocab_mask = None

        self._detect_nan_if_needed(logits_output)
        spec_info.hidden_states = logits_output.hidden_states
        res: EagleVerifyOutput = spec_info.verify(
            batch,
            logits_output,
            self.token_to_kv_pool_allocator,
            self.page_size,
            vocab_mask,
        )

        # Post process based on verified outputs.
        # Pick indices that we care (accepted)
        logits_output.next_token_logits = logits_output.next_token_logits[
            res.accepted_indices
        ]
        logits_output.hidden_states = logits_output.hidden_states[res.accepted_indices]

        if batch.return_logprob:
            self.add_logprob_values(batch, res, logits_output)

        # Prepare the batch for the next draft forwards.
        batch.forward_mode = (
            ForwardMode.DECODE if not batch.forward_mode.is_idle() else ForwardMode.IDLE
        )
        batch.spec_info = res.draft_input

        return logits_output, res, model_worker_batch, can_run_cuda_graph

    def add_logprob_values(
        self,
        batch: ScheduleBatch,
        res: EagleVerifyOutput,
        logits_output: LogitsProcessorOutput,
    ):
        # Extract args
        logits_output = res.logits_output
        top_logprobs_nums = batch.top_logprobs_nums
        token_ids_logprobs = batch.token_ids_logprobs
        accepted_indices = res.accepted_indices
        assert len(accepted_indices) == len(logits_output.next_token_logits)

        temperatures = batch.sampling_info.temperatures
        num_draft_tokens = batch.spec_info.draft_token_num
        # acceptance indices are the indices in a &#34;flattened&#34; batch.
        # dividing it to num_draft_tokens will yield the actual batch index.
        temperatures = temperatures[accepted_indices // num_draft_tokens]
        if RETURN_ORIGINAL_LOGPROB:
            logprobs = torch.nn.functional.log_softmax(
                logits_output.next_token_logits, dim=-1
            )
        else:
            logprobs = torch.nn.functional.log_softmax(
                logits_output.next_token_logits / temperatures, dim=-1
            )
        batch_next_token_ids = res.verified_id
        num_tokens_per_req = [accept + 1 for accept in res.accept_length_per_req_cpu]

        # We should repeat top_logprobs_nums to match num_tokens_per_req.
        top_logprobs_nums_repeat_interleaved = []
        token_ids_logprobs_repeat_interleaved = []
        for num, num_tokens in zip(top_logprobs_nums, num_tokens_per_req):
            top_logprobs_nums_repeat_interleaved.extend([num] * num_tokens)
        for token_ids, num_tokens in zip(token_ids_logprobs, num_tokens_per_req):
            token_ids_logprobs_repeat_interleaved.extend([token_ids] * num_tokens)

        # Extract logprobs
        if any(x &gt; 0 for x in top_logprobs_nums):
            (
                logits_output.next_token_top_logprobs_val,
                logits_output.next_token_top_logprobs_idx,
            ) = get_top_logprobs(
                logprobs,
                top_logprobs_nums_repeat_interleaved,
            )

        if any(x is not None for x in token_ids_logprobs):
            (
                logits_output.next_token_token_ids_logprobs_val,
                logits_output.next_token_token_ids_logprobs_idx,
            ) = get_token_ids_logprobs(
                logprobs,
                token_ids_logprobs_repeat_interleaved,
            )

        logits_output.next_token_logprobs = logprobs[
            torch.arange(len(batch_next_token_ids), device=batch.sampling_info.device),
            batch_next_token_ids,
        ]

        # Add output logprobs to the request
        pt = 0
        next_token_logprobs = logits_output.next_token_logprobs.tolist()
        verified_ids = batch_next_token_ids.tolist()
        for req, num_tokens in zip(batch.reqs, num_tokens_per_req, strict=True):
            for _ in range(num_tokens):
                if req.return_logprob:
                    req.output_token_logprobs_val.append(next_token_logprobs[pt])
                    req.output_token_logprobs_idx.append(verified_ids[pt])
                    if req.top_logprobs_num &gt; 0:
                        req.output_top_logprobs_val.append(
                            res.logits_output.next_token_top_logprobs_val[pt]
                        )
                        req.output_top_logprobs_idx.append(
                            res.logits_output.next_token_top_logprobs_idx[pt]
                        )
                pt += 1

    def forward_draft_extend(
        self,
        batch: ScheduleBatch,
        hidden_states: torch.Tensor,
        next_token_ids: torch.Tensor,
        seq_lens_cpu: Optional[torch.Tensor],
    ):
        &#34;&#34;&#34;Run draft model extend. This API modifies the states of the batch.

        Args:
            batch: The batch to run.
            hidden_states: Hidden states from the target model forward
            next_token_ids: Next token ids generated from the target forward.
        &#34;&#34;&#34;
        batch.spec_info = EagleDraftInput(
            hidden_states=hidden_states,
            verified_id=next_token_ids,
            num_tokens_per_batch=1,
            num_tokens_for_logprob_per_batch=1,
        )
        batch.return_hidden_states = False
        batch.spec_info.prepare_for_extend(batch)
        batch.spec_info.capture_hidden_mode = CaptureHiddenMode.LAST
        model_worker_batch = batch.get_model_worker_batch(
            seq_lens_cpu_cache=seq_lens_cpu
        )
        forward_batch = ForwardBatch.init_new(
            model_worker_batch, self.draft_model_runner
        )
        forward_batch.return_logprob = False
        logits_output, _ = self.draft_model_runner.forward(forward_batch)
        self._detect_nan_if_needed(logits_output)
        assert isinstance(forward_batch.spec_info, EagleDraftInput)
        assert forward_batch.spec_info is batch.spec_info
        self.capture_for_decode(logits_output, forward_batch.spec_info)
        has_finished, unfinished_req_index = False, []
        for i, req in enumerate(batch.reqs):
            if req.finished():
                has_finished = True
            else:
                unfinished_req_index.append(i)
        if has_finished:
            unfinished_index_device = torch.tensor(
                unfinished_req_index,
                dtype=torch.int64,
                device=batch.spec_info.topk_p.device,
            )
            batch.spec_info.filter_batch(
                unfinished_index_device, has_been_filtered=False
            )

    def forward_draft_extend_after_decode(self, batch: ScheduleBatch):
        assert isinstance(batch.spec_info, EagleDraftInput)
        # Backup fields that will be modified in-place
        seq_lens_backup = batch.seq_lens.clone()
        req_pool_indices_backup = batch.req_pool_indices
        accept_length_backup = batch.spec_info.accept_length
        return_logprob_backup = batch.return_logprob

        input_is_idle = batch.forward_mode.is_idle()

        if not input_is_idle and batch.spec_info.verified_id.numel() == 0:
            batch = batch.copy()
            batch.prepare_for_idle()
            hidden_size = (
                self.model_config.hidden_size * 3
                if self.speculative_algorithm.is_eagle3()
                else self.model_config.hidden_size
            )
            batch.spec_info = EagleDraftInput.create_idle_input(
                device=self.device,
                hidden_size=hidden_size,
                dtype=self.model_config.dtype,
                topk=self.topk,
                capture_hidden_mode=CaptureHiddenMode.LAST,
            )

        batch.spec_info.num_tokens_per_batch = self.speculative_num_steps + 1
        batch.spec_info.num_tokens_for_logprob_per_batch = 1
        batch.spec_info.prepare_extend_after_decode(
            batch,
            self.speculative_num_steps,
        )
        batch.forward_mode = (
            ForwardMode.DRAFT_EXTEND
            if not batch.forward_mode.is_idle()
            else ForwardMode.IDLE
        )

        batch.return_hidden_states = False
        model_worker_batch = batch.get_model_worker_batch()
        assert model_worker_batch.capture_hidden_mode == CaptureHiddenMode.LAST
        forward_batch = ForwardBatch.init_new(
            model_worker_batch, self.draft_model_runner
        )
        if forward_batch.seq_lens_cpu is not None:
            forward_batch.seq_lens_sum = forward_batch.seq_lens_cpu.sum().item()
        else:
            forward_batch.seq_lens_sum = batch.seq_lens.sum().item()

        # Run
        can_cuda_graph = (
            self.cuda_graph_runner_for_draft_extend
            and self.cuda_graph_runner_for_draft_extend.can_run(forward_batch)
        )
        if can_cuda_graph:
            logits_output = self.cuda_graph_runner_for_draft_extend.replay(
                forward_batch
            )
            forward_batch.spec_info.topk_p, forward_batch.spec_info.topk_index = (
                logits_output.topk_p,
                logits_output.topk_index,
            )
            forward_batch.spec_info.hidden_states = logits_output.hidden_states
        else:
            forward_batch.can_run_dp_cuda_graph = False
            if not forward_batch.forward_mode.is_idle():
                self.draft_model_runner.attn_backend.init_forward_metadata(
                    forward_batch
                )
            logits_output, _ = self.draft_model_runner.forward(
                forward_batch, skip_attn_backend_init=True
            )
            self.capture_for_decode(logits_output, forward_batch.spec_info)

        self._detect_nan_if_needed(logits_output)

        # Restore backup.
        # This is because `seq_lens` can be modified in `prepare_extend_after_decode`
        batch.forward_mode = (
            ForwardMode.DECODE if not input_is_idle else ForwardMode.IDLE
        )
        batch.seq_lens = seq_lens_backup
        batch.req_pool_indices = req_pool_indices_backup
        batch.spec_info.accept_length = accept_length_backup
        batch.return_logprob = return_logprob_backup

    def capture_for_decode(
        self, logits_output: LogitsProcessorOutput, draft_input: EagleDraftInput
    ):
        probs = torch.softmax(logits_output.next_token_logits, dim=-1)
        draft_input.topk_p, draft_input.topk_index = fast_topk(probs, self.topk, dim=-1)
        draft_input.hidden_states = logits_output.hidden_states

    def _detect_nan_if_needed(self, logits_output: LogitsProcessorOutput):
        if self.enable_nan_detection:
            logits = logits_output.next_token_logits
            if torch.any(torch.isnan(logits)):
                logger.error(&#34;Detected errors during sampling! NaN in the logits.&#34;)
                raise ValueError(&#34;Detected errors during sampling! NaN in the logits.&#34;)</code></pre>
</details>
<div class="desc"><p>A tensor parallel model worker.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.managers.tp_worker.TpModelWorker" href="../managers/tp_worker.html#sglang.srt.managers.tp_worker.TpModelWorker">TpModelWorker</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.speculative.eagle_worker.EAGLEWorker.draft_model_runner"><code class="name">prop <span class="ident">draft_model_runner</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def draft_model_runner(self):
    return self.model_runner</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.speculative.eagle_worker.EAGLEWorker.add_logprob_values"><code class="name flex">
<span>def <span class="ident">add_logprob_values</span></span>(<span>self,<br>batch: <a title="sglang.srt.managers.schedule_batch.ScheduleBatch" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.ScheduleBatch">ScheduleBatch</a>,<br>res: <a title="sglang.srt.speculative.eagle_utils.EagleVerifyOutput" href="eagle_utils.html#sglang.srt.speculative.eagle_utils.EagleVerifyOutput">EagleVerifyOutput</a>,<br>logits_output: <a title="sglang.srt.layers.logits_processor.LogitsProcessorOutput" href="../layers/logits_processor.html#sglang.srt.layers.logits_processor.LogitsProcessorOutput">LogitsProcessorOutput</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_logprob_values(
    self,
    batch: ScheduleBatch,
    res: EagleVerifyOutput,
    logits_output: LogitsProcessorOutput,
):
    # Extract args
    logits_output = res.logits_output
    top_logprobs_nums = batch.top_logprobs_nums
    token_ids_logprobs = batch.token_ids_logprobs
    accepted_indices = res.accepted_indices
    assert len(accepted_indices) == len(logits_output.next_token_logits)

    temperatures = batch.sampling_info.temperatures
    num_draft_tokens = batch.spec_info.draft_token_num
    # acceptance indices are the indices in a &#34;flattened&#34; batch.
    # dividing it to num_draft_tokens will yield the actual batch index.
    temperatures = temperatures[accepted_indices // num_draft_tokens]
    if RETURN_ORIGINAL_LOGPROB:
        logprobs = torch.nn.functional.log_softmax(
            logits_output.next_token_logits, dim=-1
        )
    else:
        logprobs = torch.nn.functional.log_softmax(
            logits_output.next_token_logits / temperatures, dim=-1
        )
    batch_next_token_ids = res.verified_id
    num_tokens_per_req = [accept + 1 for accept in res.accept_length_per_req_cpu]

    # We should repeat top_logprobs_nums to match num_tokens_per_req.
    top_logprobs_nums_repeat_interleaved = []
    token_ids_logprobs_repeat_interleaved = []
    for num, num_tokens in zip(top_logprobs_nums, num_tokens_per_req):
        top_logprobs_nums_repeat_interleaved.extend([num] * num_tokens)
    for token_ids, num_tokens in zip(token_ids_logprobs, num_tokens_per_req):
        token_ids_logprobs_repeat_interleaved.extend([token_ids] * num_tokens)

    # Extract logprobs
    if any(x &gt; 0 for x in top_logprobs_nums):
        (
            logits_output.next_token_top_logprobs_val,
            logits_output.next_token_top_logprobs_idx,
        ) = get_top_logprobs(
            logprobs,
            top_logprobs_nums_repeat_interleaved,
        )

    if any(x is not None for x in token_ids_logprobs):
        (
            logits_output.next_token_token_ids_logprobs_val,
            logits_output.next_token_token_ids_logprobs_idx,
        ) = get_token_ids_logprobs(
            logprobs,
            token_ids_logprobs_repeat_interleaved,
        )

    logits_output.next_token_logprobs = logprobs[
        torch.arange(len(batch_next_token_ids), device=batch.sampling_info.device),
        batch_next_token_ids,
    ]

    # Add output logprobs to the request
    pt = 0
    next_token_logprobs = logits_output.next_token_logprobs.tolist()
    verified_ids = batch_next_token_ids.tolist()
    for req, num_tokens in zip(batch.reqs, num_tokens_per_req, strict=True):
        for _ in range(num_tokens):
            if req.return_logprob:
                req.output_token_logprobs_val.append(next_token_logprobs[pt])
                req.output_token_logprobs_idx.append(verified_ids[pt])
                if req.top_logprobs_num &gt; 0:
                    req.output_top_logprobs_val.append(
                        res.logits_output.next_token_top_logprobs_val[pt]
                    )
                    req.output_top_logprobs_idx.append(
                        res.logits_output.next_token_top_logprobs_idx[pt]
                    )
            pt += 1</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_worker.EAGLEWorker.capture_for_decode"><code class="name flex">
<span>def <span class="ident">capture_for_decode</span></span>(<span>self,<br>logits_output: <a title="sglang.srt.layers.logits_processor.LogitsProcessorOutput" href="../layers/logits_processor.html#sglang.srt.layers.logits_processor.LogitsProcessorOutput">LogitsProcessorOutput</a>,<br>draft_input: <a title="sglang.srt.speculative.eagle_utils.EagleDraftInput" href="eagle_utils.html#sglang.srt.speculative.eagle_utils.EagleDraftInput">EagleDraftInput</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def capture_for_decode(
    self, logits_output: LogitsProcessorOutput, draft_input: EagleDraftInput
):
    probs = torch.softmax(logits_output.next_token_logits, dim=-1)
    draft_input.topk_p, draft_input.topk_index = fast_topk(probs, self.topk, dim=-1)
    draft_input.hidden_states = logits_output.hidden_states</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_worker.EAGLEWorker.check_forward_draft_extend_after_decode"><code class="name flex">
<span>def <span class="ident">check_forward_draft_extend_after_decode</span></span>(<span>self,<br>batch: <a title="sglang.srt.managers.schedule_batch.ScheduleBatch" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.ScheduleBatch">ScheduleBatch</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_forward_draft_extend_after_decode(self, batch: ScheduleBatch):
    local_need_forward = batch.spec_info.verified_id.shape[0] &gt; 0
    if not self.server_args.enable_dp_attention:
        return local_need_forward

    global_need_forward = torch.tensor(
        [
            (local_need_forward),
        ],
        dtype=torch.int64,
    )
    torch.distributed.all_reduce(
        global_need_forward, group=get_tp_group().cpu_group
    )
    global_need_forward_cnt = global_need_forward[0].item()
    need_forward = global_need_forward_cnt &gt; 0
    return need_forward</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_worker.EAGLEWorker.draft"><code class="name flex">
<span>def <span class="ident">draft</span></span>(<span>self,<br>batch: <a title="sglang.srt.managers.schedule_batch.ScheduleBatch" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.ScheduleBatch">ScheduleBatch</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def draft(self, batch: ScheduleBatch):
    # Parse args
    if batch.forward_mode.is_idle():
        self._draft_preprocess_idle(batch)
    else:
        self._draft_preprocess_decode(batch)

    spec_info = batch.spec_info
    assert isinstance(spec_info, EagleDraftInput)

    spec_info.capture_hidden_mode = CaptureHiddenMode.LAST
    spec_info.num_tokens_per_batch = self.topk
    spec_info.num_tokens_for_logprob_per_batch = self.topk
    batch.return_hidden_states = False

    # Get forward batch
    model_worker_batch = batch.get_model_worker_batch()
    assert model_worker_batch.capture_hidden_mode == CaptureHiddenMode.LAST
    forward_batch = ForwardBatch.init_new(
        model_worker_batch, self.draft_model_runner
    )
    can_cuda_graph = self.cuda_graph_runner and self.cuda_graph_runner.can_run(
        forward_batch
    )
    if can_cuda_graph:
        score_list, token_list, parents_list = self.cuda_graph_runner.replay(
            forward_batch
        )
    else:
        forward_batch.can_run_dp_cuda_graph = False
        if not forward_batch.forward_mode.is_idle():
            # Initialize attention backend
            self.draft_attn_backend.init_forward_metadata(forward_batch)
        # Run forward steps
        score_list, token_list, parents_list = self.draft_forward(forward_batch)

    if batch.forward_mode.is_idle():
        return EagleVerifyInput.create_idle_input(
            self.topk,
            self.speculative_num_steps,
            self.speculative_num_draft_tokens,
        )

    (
        tree_mask,
        position,
        retrive_index,
        retrive_next_token,
        retrive_next_sibling,
        draft_tokens,
    ) = build_tree_kernel_efficient(
        spec_info.verified_id,
        score_list,
        token_list,
        parents_list,
        batch.seq_lens,
        batch.seq_lens_sum,
        self.topk,
        self.speculative_num_steps,
        self.speculative_num_draft_tokens,
    )

    return EagleVerifyInput(
        draft_token=draft_tokens,
        custom_mask=tree_mask,
        positions=position,
        retrive_index=retrive_index,
        retrive_next_token=retrive_next_token,
        retrive_next_sibling=retrive_next_sibling,
        retrive_cum_len=None,
        spec_steps=self.speculative_num_steps,
        topk=self.topk,
        draft_token_num=self.server_args.speculative_num_draft_tokens,
        capture_hidden_mode=CaptureHiddenMode.FULL,
        seq_lens_sum=forward_batch.seq_lens_sum,
        seq_lens_cpu=forward_batch.seq_lens_cpu,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_worker.EAGLEWorker.draft_forward"><code class="name flex">
<span>def <span class="ident">draft_forward</span></span>(<span>self,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def draft_forward(self, forward_batch: ForwardBatch):
    # Parse args
    spec_info = forward_batch.spec_info
    assert isinstance(spec_info, EagleDraftInput)
    out_cache_loc = forward_batch.out_cache_loc
    topk_p, topk_index, hidden_states = (
        spec_info.topk_p,
        spec_info.topk_index,
        spec_info.hidden_states,
    )
    if self.hot_token_id is not None:
        topk_index = self.hot_token_id[topk_index]

    out_cache_loc = out_cache_loc.reshape(
        forward_batch.batch_size, self.topk, self.speculative_num_steps
    )
    out_cache_loc = out_cache_loc.permute((2, 0, 1)).reshape(
        self.speculative_num_steps, -1
    )

    # Return values
    score_list: List[torch.Tensor] = []
    token_list: List[torch.Tensor] = []
    parents_list: List[torch.Tensor] = []

    # Forward multiple steps
    scores = None
    for i in range(self.speculative_num_steps):
        input_ids, hidden_states, scores, tree_info = select_top_k_tokens(
            i, topk_p, topk_index, hidden_states, scores, self.topk
        )
        score_list.append(tree_info[0])
        token_list.append(tree_info[1])
        parents_list.append(tree_info[2])

        # We don&#39;t need to run the last forward. we get 1 token from draft prefill and (#spec steps - 1) tokens here
        if i == self.speculative_num_steps - 1:
            break

        # Set inputs
        forward_batch.input_ids = input_ids
        forward_batch.out_cache_loc = out_cache_loc[i]
        forward_batch.positions.add_(1)
        forward_batch.attn_backend = self.draft_attn_backend.attn_backends[i]
        spec_info.hidden_states = hidden_states

        # Run forward
        logits_output, _ = self.draft_model_runner.forward(
            forward_batch, skip_attn_backend_init=True
        )
        self._detect_nan_if_needed(logits_output)
        probs = torch.softmax(logits_output.next_token_logits, dim=-1)
        topk_p, topk_index = fast_topk(probs, self.topk, dim=-1)
        if self.hot_token_id is not None:
            topk_index = self.hot_token_id[topk_index]
        hidden_states = logits_output.hidden_states

    return score_list, token_list, parents_list</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_worker.EAGLEWorker.forward_batch_speculative_generation"><code class="name flex">
<span>def <span class="ident">forward_batch_speculative_generation</span></span>(<span>self,<br>batch: <a title="sglang.srt.managers.schedule_batch.ScheduleBatch" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.ScheduleBatch">ScheduleBatch</a>) ‑> Tuple[<a title="sglang.srt.layers.logits_processor.LogitsProcessorOutput" href="../layers/logits_processor.html#sglang.srt.layers.logits_processor.LogitsProcessorOutput">LogitsProcessorOutput</a>, torch.Tensor, int, int, bool]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_batch_speculative_generation(
    self, batch: ScheduleBatch
) -&gt; Tuple[LogitsProcessorOutput, torch.Tensor, int, int, bool]:
    &#34;&#34;&#34;Run speculative decoding forward.

    NOTE: Many states of batch is modified as you go through. It is not guaranteed that
    the final output batch have the same state as the input.

    Args:
        batch: The batch to run forward. The state of the batch is modified as it runs.
    Returns:
        A tuple of the final logit output of the target model, next tokens accepted,
        the batch id (used for overlap schedule), and number of accepted tokens.
    &#34;&#34;&#34;
    if batch.forward_mode.is_extend() or batch.is_extend_in_batch:
        logits_output, next_token_ids, bid, seq_lens_cpu = (
            self.forward_target_extend(batch)
        )
        with self.draft_tp_context(self.draft_model_runner.tp_group):
            self.forward_draft_extend(
                batch, logits_output.hidden_states, next_token_ids, seq_lens_cpu
            )
        return logits_output, next_token_ids, bid, 0, False
    else:
        with self.draft_tp_context(self.draft_model_runner.tp_group):
            spec_info = self.draft(batch)
        logits_output, verify_output, model_worker_batch, can_run_cuda_graph = (
            self.verify(batch, spec_info)
        )

        with self.draft_tp_context(self.draft_model_runner.tp_group):
            # NOTE: We should use `check_forward_draft_extend_after_decode`
            # when DP attention is enabled, but it is slow. Skip it for now.
            if (
                self.server_args.enable_dp_attention
                or batch.spec_info.verified_id.shape[0] &gt; 0
            ):
                # decode is not finished
                self.forward_draft_extend_after_decode(batch)

        return (
            logits_output,
            verify_output.verified_id,
            model_worker_batch.bid,
            sum(verify_output.accept_length_per_req_cpu),
            can_run_cuda_graph,
        )</code></pre>
</details>
<div class="desc"><p>Run speculative decoding forward.</p>
<p>NOTE: Many states of batch is modified as you go through. It is not guaranteed that
the final output batch have the same state as the input.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>The batch to run forward. The state of the batch is modified as it runs.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A tuple of the final logit output of the target model, next tokens accepted,
the batch id (used for overlap schedule), and number of accepted tokens.</p></div>
</dd>
<dt id="sglang.srt.speculative.eagle_worker.EAGLEWorker.forward_draft_extend"><code class="name flex">
<span>def <span class="ident">forward_draft_extend</span></span>(<span>self,<br>batch: <a title="sglang.srt.managers.schedule_batch.ScheduleBatch" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.ScheduleBatch">ScheduleBatch</a>,<br>hidden_states: torch.Tensor,<br>next_token_ids: torch.Tensor,<br>seq_lens_cpu: torch.Tensor | None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_draft_extend(
    self,
    batch: ScheduleBatch,
    hidden_states: torch.Tensor,
    next_token_ids: torch.Tensor,
    seq_lens_cpu: Optional[torch.Tensor],
):
    &#34;&#34;&#34;Run draft model extend. This API modifies the states of the batch.

    Args:
        batch: The batch to run.
        hidden_states: Hidden states from the target model forward
        next_token_ids: Next token ids generated from the target forward.
    &#34;&#34;&#34;
    batch.spec_info = EagleDraftInput(
        hidden_states=hidden_states,
        verified_id=next_token_ids,
        num_tokens_per_batch=1,
        num_tokens_for_logprob_per_batch=1,
    )
    batch.return_hidden_states = False
    batch.spec_info.prepare_for_extend(batch)
    batch.spec_info.capture_hidden_mode = CaptureHiddenMode.LAST
    model_worker_batch = batch.get_model_worker_batch(
        seq_lens_cpu_cache=seq_lens_cpu
    )
    forward_batch = ForwardBatch.init_new(
        model_worker_batch, self.draft_model_runner
    )
    forward_batch.return_logprob = False
    logits_output, _ = self.draft_model_runner.forward(forward_batch)
    self._detect_nan_if_needed(logits_output)
    assert isinstance(forward_batch.spec_info, EagleDraftInput)
    assert forward_batch.spec_info is batch.spec_info
    self.capture_for_decode(logits_output, forward_batch.spec_info)
    has_finished, unfinished_req_index = False, []
    for i, req in enumerate(batch.reqs):
        if req.finished():
            has_finished = True
        else:
            unfinished_req_index.append(i)
    if has_finished:
        unfinished_index_device = torch.tensor(
            unfinished_req_index,
            dtype=torch.int64,
            device=batch.spec_info.topk_p.device,
        )
        batch.spec_info.filter_batch(
            unfinished_index_device, has_been_filtered=False
        )</code></pre>
</details>
<div class="desc"><p>Run draft model extend. This API modifies the states of the batch.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>The batch to run.</dd>
<dt><strong><code>hidden_states</code></strong></dt>
<dd>Hidden states from the target model forward</dd>
<dt><strong><code>next_token_ids</code></strong></dt>
<dd>Next token ids generated from the target forward.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.speculative.eagle_worker.EAGLEWorker.forward_draft_extend_after_decode"><code class="name flex">
<span>def <span class="ident">forward_draft_extend_after_decode</span></span>(<span>self,<br>batch: <a title="sglang.srt.managers.schedule_batch.ScheduleBatch" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.ScheduleBatch">ScheduleBatch</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_draft_extend_after_decode(self, batch: ScheduleBatch):
    assert isinstance(batch.spec_info, EagleDraftInput)
    # Backup fields that will be modified in-place
    seq_lens_backup = batch.seq_lens.clone()
    req_pool_indices_backup = batch.req_pool_indices
    accept_length_backup = batch.spec_info.accept_length
    return_logprob_backup = batch.return_logprob

    input_is_idle = batch.forward_mode.is_idle()

    if not input_is_idle and batch.spec_info.verified_id.numel() == 0:
        batch = batch.copy()
        batch.prepare_for_idle()
        hidden_size = (
            self.model_config.hidden_size * 3
            if self.speculative_algorithm.is_eagle3()
            else self.model_config.hidden_size
        )
        batch.spec_info = EagleDraftInput.create_idle_input(
            device=self.device,
            hidden_size=hidden_size,
            dtype=self.model_config.dtype,
            topk=self.topk,
            capture_hidden_mode=CaptureHiddenMode.LAST,
        )

    batch.spec_info.num_tokens_per_batch = self.speculative_num_steps + 1
    batch.spec_info.num_tokens_for_logprob_per_batch = 1
    batch.spec_info.prepare_extend_after_decode(
        batch,
        self.speculative_num_steps,
    )
    batch.forward_mode = (
        ForwardMode.DRAFT_EXTEND
        if not batch.forward_mode.is_idle()
        else ForwardMode.IDLE
    )

    batch.return_hidden_states = False
    model_worker_batch = batch.get_model_worker_batch()
    assert model_worker_batch.capture_hidden_mode == CaptureHiddenMode.LAST
    forward_batch = ForwardBatch.init_new(
        model_worker_batch, self.draft_model_runner
    )
    if forward_batch.seq_lens_cpu is not None:
        forward_batch.seq_lens_sum = forward_batch.seq_lens_cpu.sum().item()
    else:
        forward_batch.seq_lens_sum = batch.seq_lens.sum().item()

    # Run
    can_cuda_graph = (
        self.cuda_graph_runner_for_draft_extend
        and self.cuda_graph_runner_for_draft_extend.can_run(forward_batch)
    )
    if can_cuda_graph:
        logits_output = self.cuda_graph_runner_for_draft_extend.replay(
            forward_batch
        )
        forward_batch.spec_info.topk_p, forward_batch.spec_info.topk_index = (
            logits_output.topk_p,
            logits_output.topk_index,
        )
        forward_batch.spec_info.hidden_states = logits_output.hidden_states
    else:
        forward_batch.can_run_dp_cuda_graph = False
        if not forward_batch.forward_mode.is_idle():
            self.draft_model_runner.attn_backend.init_forward_metadata(
                forward_batch
            )
        logits_output, _ = self.draft_model_runner.forward(
            forward_batch, skip_attn_backend_init=True
        )
        self.capture_for_decode(logits_output, forward_batch.spec_info)

    self._detect_nan_if_needed(logits_output)

    # Restore backup.
    # This is because `seq_lens` can be modified in `prepare_extend_after_decode`
    batch.forward_mode = (
        ForwardMode.DECODE if not input_is_idle else ForwardMode.IDLE
    )
    batch.seq_lens = seq_lens_backup
    batch.req_pool_indices = req_pool_indices_backup
    batch.spec_info.accept_length = accept_length_backup
    batch.return_logprob = return_logprob_backup</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_worker.EAGLEWorker.forward_target_extend"><code class="name flex">
<span>def <span class="ident">forward_target_extend</span></span>(<span>self,<br>batch: <a title="sglang.srt.managers.schedule_batch.ScheduleBatch" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.ScheduleBatch">ScheduleBatch</a>) ‑> Tuple[<a title="sglang.srt.layers.logits_processor.LogitsProcessorOutput" href="../layers/logits_processor.html#sglang.srt.layers.logits_processor.LogitsProcessorOutput">LogitsProcessorOutput</a>, torch.Tensor, int, torch.Tensor | None]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_target_extend(
    self, batch: ScheduleBatch
) -&gt; Tuple[LogitsProcessorOutput, torch.Tensor, int, Optional[torch.Tensor]]:
    &#34;&#34;&#34;Run the target extend.

    Args:
        batch: The batch to run. States could be modified.

    Returns:
        logits_output: The output of logits. It will contain the full hidden states.
        next_token_ids: Next token ids generated.
        bid: The model batch ID. Used for overlap schedule.
    &#34;&#34;&#34;
    # Forward with the target model and get hidden states.
    # We need the full hidden states to prefill the KV cache of the draft model.
    model_worker_batch = batch.get_model_worker_batch()
    model_worker_batch.capture_hidden_mode = CaptureHiddenMode.FULL
    logits_output, next_token_ids, _ = self.target_worker.forward_batch_generation(
        model_worker_batch
    )
    return (
        logits_output,
        next_token_ids,
        model_worker_batch.bid,
        model_worker_batch.seq_lens_cpu,
    )</code></pre>
</details>
<div class="desc"><p>Run the target extend.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong></dt>
<dd>The batch to run. States could be modified.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>logits_output</code></dt>
<dd>The output of logits. It will contain the full hidden states.</dd>
<dt><code>next_token_ids</code></dt>
<dd>Next token ids generated.</dd>
<dt><code>bid</code></dt>
<dd>The model batch ID. Used for overlap schedule.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.speculative.eagle_worker.EAGLEWorker.init_attention_backend"><code class="name flex">
<span>def <span class="ident">init_attention_backend</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_attention_backend(self):
    # Create multi-step attn backends and cuda graph runners

    self.has_prefill_wrapper_verify = False
    self.draft_extend_attn_backend = None

    if self.server_args.attention_backend == &#34;flashinfer&#34;:
        if not global_server_args_dict[&#34;use_mla_backend&#34;]:
            from sglang.srt.layers.attention.flashinfer_backend import (
                FlashInferAttnBackend,
                FlashInferMultiStepDraftBackend,
            )

            self.draft_attn_backend = FlashInferMultiStepDraftBackend(
                self.draft_model_runner,
                self.topk,
                self.speculative_num_steps,
            )
            self.draft_extend_attn_backend = FlashInferAttnBackend(
                self.draft_model_runner,
                skip_prefill=False,
            )
        else:
            from sglang.srt.layers.attention.flashinfer_mla_backend import (
                FlashInferMLAAttnBackend,
                FlashInferMLAMultiStepDraftBackend,
            )

            self.draft_attn_backend = FlashInferMLAMultiStepDraftBackend(
                self.draft_model_runner,
                self.topk,
                self.speculative_num_steps,
            )
            self.draft_extend_attn_backend = FlashInferMLAAttnBackend(
                self.draft_model_runner,
                skip_prefill=False,
            )
        self.has_prefill_wrapper_verify = True
    elif self.server_args.attention_backend == &#34;triton&#34;:
        from sglang.srt.layers.attention.triton_backend import (
            TritonAttnBackend,
            TritonMultiStepDraftBackend,
        )

        self.draft_attn_backend = TritonMultiStepDraftBackend(
            self.draft_model_runner,
            self.topk,
            self.speculative_num_steps,
        )
        self.draft_extend_attn_backend = TritonAttnBackend(
            self.draft_model_runner,
            skip_prefill=False,
        )
    elif self.server_args.attention_backend == &#34;aiter&#34;:
        from sglang.srt.layers.attention.aiter_backend import (
            AiterAttnBackend,
            AiterMultiStepDraftBackend,
        )

        self.draft_attn_backend = AiterMultiStepDraftBackend(
            self.draft_model_runner,
            self.topk,
            self.speculative_num_steps,
        )
        self.draft_extend_attn_backend = AiterAttnBackend(
            self.draft_model_runner,
            skip_prefill=False,
        )
        self.has_prefill_wrapper_verify = False
    elif self.server_args.attention_backend == &#34;fa3&#34;:
        from sglang.srt.layers.attention.flashattention_backend import (
            FlashAttentionBackend,
            FlashAttentionMultiStepBackend,
        )

        self.draft_attn_backend = FlashAttentionMultiStepBackend(
            self.draft_model_runner,
            self.topk,
            self.speculative_num_steps,
        )
        self.draft_extend_attn_backend = FlashAttentionBackend(
            self.draft_model_runner,
            skip_prefill=False,
        )
    elif self.server_args.attention_backend == &#34;flashmla&#34;:
        from sglang.srt.layers.attention.flashmla_backend import (
            FlashMLAMultiStepDraftBackend,
        )

        self.draft_attn_backend = FlashMLAMultiStepDraftBackend(
            self.draft_model_runner,
            self.topk,
            self.speculative_num_steps,
        )
    elif self.server_args.attention_backend == &#34;trtllm_mha&#34;:
        from sglang.srt.layers.attention.trtllm_mha_backend import (
            TRTLLMHAAttnBackend,
            TRTLLMHAAttnMultiStepDraftBackend,
        )

        self.draft_attn_backend = TRTLLMHAAttnMultiStepDraftBackend(
            self.draft_model_runner,
            self.topk,
            self.speculative_num_steps,
        )
        self.draft_extend_attn_backend = TRTLLMHAAttnBackend(
            self.draft_model_runner,
            skip_prefill=False,
        )
        self.has_prefill_wrapper_verify = True
    elif self.server_args.attention_backend == &#34;trtllm_mla&#34;:
        if not global_server_args_dict[&#34;use_mla_backend&#34;]:
            raise ValueError(
                &#34;trtllm_mla backend requires MLA model (use_mla_backend=True).&#34;
            )

        from sglang.srt.layers.attention.trtllm_mla_backend import (
            TRTLLMMLABackend,
            TRTLLMMLAMultiStepDraftBackend,
        )

        self.draft_attn_backend = TRTLLMMLAMultiStepDraftBackend(
            self.draft_model_runner,
            self.topk,
            self.speculative_num_steps,
        )
        self.draft_extend_attn_backend = TRTLLMMLABackend(
            self.draft_model_runner,
            skip_prefill=False,
        )
        self.has_prefill_wrapper_verify = True
    else:
        raise ValueError(
            f&#34;EAGLE is not supported in attention backend {self.server_args.attention_backend}&#34;
        )

    self.draft_model_runner.draft_attn_backend = self.draft_attn_backend</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.speculative.eagle_worker.EAGLEWorker.init_cuda_graphs"><code class="name flex">
<span>def <span class="ident">init_cuda_graphs</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_cuda_graphs(self):
    &#34;&#34;&#34;Capture cuda graphs.&#34;&#34;&#34;
    self.cuda_graph_runner = None
    self.cuda_graph_runner_for_draft_extend = None

    if self.server_args.disable_cuda_graph:
        return

    # Capture draft
    tic = time.perf_counter()
    before_mem = get_available_gpu_memory(self.device, self.gpu_id)
    logger.info(
        f&#34;Capture draft cuda graph begin. This can take up to several minutes. avail mem={before_mem:.2f} GB&#34;
    )
    self.cuda_graph_runner = EAGLEDraftCudaGraphRunner(self)
    after_mem = get_available_gpu_memory(self.device, self.gpu_id)
    logger.info(
        f&#34;Capture draft cuda graph end. Time elapsed: {time.perf_counter() - tic:.2f} s. mem usage={(before_mem - after_mem):.2f} GB. avail mem={after_mem:.2f} GB.&#34;
    )

    # Capture extend
    if self.draft_extend_attn_backend:
        tic = time.perf_counter()
        before_mem = get_available_gpu_memory(self.device, self.gpu_id)
        logger.info(
            f&#34;Capture draft extend cuda graph begin. This can take up to several minutes. avail mem={before_mem:.2f} GB&#34;
        )
        self.cuda_graph_runner_for_draft_extend = EAGLEDraftExtendCudaGraphRunner(
            self
        )
        after_mem = get_available_gpu_memory(self.device, self.gpu_id)
        logger.info(
            f&#34;Capture draft extend cuda graph end. Time elapsed: {time.perf_counter() - tic:.2f} s. mem usage={(before_mem - after_mem):.2f} GB. avail mem={after_mem:.2f} GB.&#34;
        )</code></pre>
</details>
<div class="desc"><p>Capture cuda graphs.</p></div>
</dd>
<dt id="sglang.srt.speculative.eagle_worker.EAGLEWorker.verify"><code class="name flex">
<span>def <span class="ident">verify</span></span>(<span>self,<br>batch: <a title="sglang.srt.managers.schedule_batch.ScheduleBatch" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.ScheduleBatch">ScheduleBatch</a>,<br>spec_info: <a title="sglang.srt.speculative.eagle_utils.EagleVerifyInput" href="eagle_utils.html#sglang.srt.speculative.eagle_utils.EagleVerifyInput">EagleVerifyInput</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def verify(self, batch: ScheduleBatch, spec_info: EagleVerifyInput):
    spec_info.prepare_for_verify(batch, self.page_size)
    batch.return_hidden_states = False
    batch.forward_mode = (
        ForwardMode.TARGET_VERIFY
        if not batch.forward_mode.is_idle()
        else ForwardMode.IDLE
    )
    batch.spec_info = spec_info

    model_worker_batch = batch.get_model_worker_batch(
        seq_lens_cpu_cache=spec_info.seq_lens_cpu
    )
    assert model_worker_batch.capture_hidden_mode == spec_info.capture_hidden_mode

    if batch.has_grammar:
        retrieve_next_token_cpu = spec_info.retrive_next_token.cpu()
        retrieve_next_sibling_cpu = spec_info.retrive_next_sibling.cpu()
        draft_tokens_cpu = spec_info.draft_token.view(
            spec_info.retrive_next_token.shape
        ).cpu()

    # Forward
    logits_output, _, can_run_cuda_graph = (
        self.target_worker.forward_batch_generation(
            model_worker_batch, skip_sample=True
        )
    )

    vocab_mask = None
    if batch.has_grammar:
        # Generate the logit mask for structured output.
        # Overlap the CPU operations for bitmask generation with the forward pass.
        vocab_mask = generate_token_bitmask(
            batch.reqs,
            spec_info,
            retrieve_next_token_cpu,
            retrieve_next_sibling_cpu,
            draft_tokens_cpu,
            batch.sampling_info.vocab_size,
        )

        if vocab_mask is not None:
            assert spec_info.grammar is not None
            vocab_mask = vocab_mask.to(spec_info.retrive_next_token.device)
            # NOTE (sk): otherwise, this vocab mask will be the one from the previous extend stage
            # and will be applied to produce wrong results
            batch.sampling_info.vocab_mask = None

    self._detect_nan_if_needed(logits_output)
    spec_info.hidden_states = logits_output.hidden_states
    res: EagleVerifyOutput = spec_info.verify(
        batch,
        logits_output,
        self.token_to_kv_pool_allocator,
        self.page_size,
        vocab_mask,
    )

    # Post process based on verified outputs.
    # Pick indices that we care (accepted)
    logits_output.next_token_logits = logits_output.next_token_logits[
        res.accepted_indices
    ]
    logits_output.hidden_states = logits_output.hidden_states[res.accepted_indices]

    if batch.return_logprob:
        self.add_logprob_values(batch, res, logits_output)

    # Prepare the batch for the next draft forwards.
    batch.forward_mode = (
        ForwardMode.DECODE if not batch.forward_mode.is_idle() else ForwardMode.IDLE
    )
    batch.spec_info = res.draft_input

    return logits_output, res, model_worker_batch, can_run_cuda_graph</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.speculative" href="index.html">sglang.srt.speculative</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.speculative.eagle_worker.draft_tp_context" href="#sglang.srt.speculative.eagle_worker.draft_tp_context">draft_tp_context</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_worker.get_last_loc_large_page_size_large_top_k" href="#sglang.srt.speculative.eagle_worker.get_last_loc_large_page_size_large_top_k">get_last_loc_large_page_size_large_top_k</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_worker.get_last_loc_large_page_size_top_k_1" href="#sglang.srt.speculative.eagle_worker.get_last_loc_large_page_size_top_k_1">get_last_loc_large_page_size_top_k_1</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_worker.load_token_map" href="#sglang.srt.speculative.eagle_worker.load_token_map">load_token_map</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.speculative.eagle_worker.EAGLEWorker" href="#sglang.srt.speculative.eagle_worker.EAGLEWorker">EAGLEWorker</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.speculative.eagle_worker.EAGLEWorker.add_logprob_values" href="#sglang.srt.speculative.eagle_worker.EAGLEWorker.add_logprob_values">add_logprob_values</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_worker.EAGLEWorker.capture_for_decode" href="#sglang.srt.speculative.eagle_worker.EAGLEWorker.capture_for_decode">capture_for_decode</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_worker.EAGLEWorker.check_forward_draft_extend_after_decode" href="#sglang.srt.speculative.eagle_worker.EAGLEWorker.check_forward_draft_extend_after_decode">check_forward_draft_extend_after_decode</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_worker.EAGLEWorker.draft" href="#sglang.srt.speculative.eagle_worker.EAGLEWorker.draft">draft</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_worker.EAGLEWorker.draft_forward" href="#sglang.srt.speculative.eagle_worker.EAGLEWorker.draft_forward">draft_forward</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_worker.EAGLEWorker.draft_model_runner" href="#sglang.srt.speculative.eagle_worker.EAGLEWorker.draft_model_runner">draft_model_runner</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_worker.EAGLEWorker.forward_batch_speculative_generation" href="#sglang.srt.speculative.eagle_worker.EAGLEWorker.forward_batch_speculative_generation">forward_batch_speculative_generation</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_worker.EAGLEWorker.forward_draft_extend" href="#sglang.srt.speculative.eagle_worker.EAGLEWorker.forward_draft_extend">forward_draft_extend</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_worker.EAGLEWorker.forward_draft_extend_after_decode" href="#sglang.srt.speculative.eagle_worker.EAGLEWorker.forward_draft_extend_after_decode">forward_draft_extend_after_decode</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_worker.EAGLEWorker.forward_target_extend" href="#sglang.srt.speculative.eagle_worker.EAGLEWorker.forward_target_extend">forward_target_extend</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_worker.EAGLEWorker.init_attention_backend" href="#sglang.srt.speculative.eagle_worker.EAGLEWorker.init_attention_backend">init_attention_backend</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_worker.EAGLEWorker.init_cuda_graphs" href="#sglang.srt.speculative.eagle_worker.EAGLEWorker.init_cuda_graphs">init_cuda_graphs</a></code></li>
<li><code><a title="sglang.srt.speculative.eagle_worker.EAGLEWorker.verify" href="#sglang.srt.speculative.eagle_worker.EAGLEWorker.verify">verify</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
