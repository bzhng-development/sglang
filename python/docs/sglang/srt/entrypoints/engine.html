<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.entrypoints.engine API documentation</title>
<meta name="description" content="The entry point of inference server. (SRT = SGLang Runtime) â€¦">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.entrypoints.engine</code></h1>
</header>
<section id="section-intro">
<p>The entry point of inference server. (SRT = SGLang Runtime)</p>
<p>This file implements python APIs for the inference engine.</p>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.entrypoints.engine.Engine"><code class="flex name class">
<span>class <span class="ident">Engine</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Engine(EngineBase):
    &#34;&#34;&#34;
    The entry point to the inference engine.

    - The engine consists of three components:
        1. TokenizerManager: Tokenizes the requests and sends them to the scheduler.
        2. Scheduler (subprocess): Receives requests from the Tokenizer Manager, schedules batches, forwards them, and sends the output tokens to the Detokenizer Manager.
        3. DetokenizerManager (subprocess): Detokenizes the output tokens and sends the result back to the Tokenizer Manager.

    Note:
    1. The HTTP server, Engine, and TokenizerManager all run in the main process.
    2. Inter-process communication (IPC) is handled via the ZMQ library, with each process using a different port.
    &#34;&#34;&#34;

    def __init__(self, **kwargs):
        &#34;&#34;&#34;
        The arguments of this function is the same as `sglang/srt/server_args.py::ServerArgs`.
        Please refer to `ServerArgs` for the documentation.
        &#34;&#34;&#34;
        if &#34;server_args&#34; in kwargs:
            # Directly load server_args
            server_args = kwargs[&#34;server_args&#34;]
        else:
            # Construct server_args from kwargs
            if &#34;log_level&#34; not in kwargs:
                # Do not print logs by default
                kwargs[&#34;log_level&#34;] = &#34;error&#34;
            server_args = ServerArgs(**kwargs)

        # Shutdown the subprocesses automatically when the program exits
        atexit.register(self.shutdown)

        # Allocate ports for inter-process communications
        self.port_args = PortArgs.init_new(server_args)
        logger.info(f&#34;{server_args=}&#34;)

        # Launch subprocesses
        tokenizer_manager, template_manager, scheduler_info = _launch_subprocesses(
            server_args=server_args,
            port_args=self.port_args,
        )
        self.server_args = server_args
        self.tokenizer_manager = tokenizer_manager
        self.template_manager = template_manager
        self.scheduler_info = scheduler_info

        context = zmq.Context(2)
        self.send_to_rpc = get_zmq_socket(
            context, zmq.DEALER, self.port_args.rpc_ipc_name, True
        )

    def generate(
        self,
        # The input prompt. It can be a single prompt or a batch of prompts.
        prompt: Optional[Union[List[str], str]] = None,
        sampling_params: Optional[Union[List[Dict], Dict]] = None,
        # The token ids for text; one can either specify text or input_ids.
        input_ids: Optional[Union[List[List[int]], List[int]]] = None,
        # The image input. It can be an image instance, file name, URL, or base64 encoded string.
        # Can be formatted as:
        # - Single image for a single request
        # - List of images (one per request in a batch)
        # - List of lists of images (multiple images per request)
        # See also python/sglang/srt/utils.py:load_image for more details.
        image_data: Optional[MultimodalDataInputFormat] = None,
        audio_data: Optional[MultimodalDataInputFormat] = None,
        video_data: Optional[MultimodalDataInputFormat] = None,
        return_logprob: Optional[Union[List[bool], bool]] = False,
        logprob_start_len: Optional[Union[List[int], int]] = None,
        top_logprobs_num: Optional[Union[List[int], int]] = None,
        token_ids_logprob: Optional[Union[List[List[int]], List[int]]] = None,
        lora_path: Optional[List[Optional[str]]] = None,
        custom_logit_processor: Optional[Union[List[str], str]] = None,
        return_hidden_states: bool = False,
        stream: bool = False,
        bootstrap_host: Optional[Union[List[str], str]] = None,
        bootstrap_port: Optional[Union[List[int], int]] = None,
        bootstrap_room: Optional[Union[List[int], int]] = None,
        data_parallel_rank: Optional[int] = None,
    ) -&gt; Union[Dict, Iterator[Dict]]:
        &#34;&#34;&#34;
        The arguments of this function is the same as `sglang/srt/managers/io_struct.py::GenerateReqInput`.
        Please refer to `GenerateReqInput` for the documentation.
        &#34;&#34;&#34;
        if self.server_args.enable_dp_attention:
            if data_parallel_rank is None:
                logger.debug(&#34;data_parallel_rank not provided, using default dispatch&#34;)
            elif data_parallel_rank &lt; 0:
                raise ValueError(&#34;data_parallel_rank must be non-negative&#34;)
            elif data_parallel_rank &gt;= self.server_args.dp_size:
                raise ValueError(
                    f&#34;data_parallel_rank must be less than dp_size: {self.server_args.dp_size}&#34;
                )

        obj = GenerateReqInput(
            text=prompt,
            input_ids=input_ids,
            sampling_params=sampling_params,
            image_data=image_data,
            audio_data=audio_data,
            video_data=video_data,
            return_logprob=return_logprob,
            logprob_start_len=logprob_start_len,
            top_logprobs_num=top_logprobs_num,
            token_ids_logprob=token_ids_logprob,
            lora_path=lora_path,
            custom_logit_processor=custom_logit_processor,
            return_hidden_states=return_hidden_states,
            stream=stream,
            bootstrap_host=bootstrap_host,
            bootstrap_port=bootstrap_port,
            bootstrap_room=bootstrap_room,
            data_parallel_rank=data_parallel_rank,
        )
        loop = asyncio.get_event_loop()
        generator = self.tokenizer_manager.generate_request(obj, None)

        if stream:

            def generator_wrapper():
                while True:
                    try:
                        chunk = loop.run_until_complete(generator.__anext__())
                        yield chunk
                    except StopAsyncIteration:
                        break

            return generator_wrapper()
        else:
            ret = loop.run_until_complete(generator.__anext__())
            return ret

    async def async_generate(
        self,
        # The input prompt. It can be a single prompt or a batch of prompts.
        prompt: Optional[Union[List[str], str]] = None,
        sampling_params: Optional[Union[List[Dict], Dict]] = None,
        # The token ids for text; one can either specify text or input_ids.
        input_ids: Optional[Union[List[List[int]], List[int]]] = None,
        # The image input. It can be an image instance, file name, URL, or base64 encoded string.
        # Can be formatted as:
        # - Single image for a single request
        # - List of images (one per request in a batch)
        # - List of lists of images (multiple images per request)
        # See also python/sglang/srt/utils.py:load_image for more details.
        image_data: Optional[MultimodalDataInputFormat] = None,
        audio_data: Optional[MultimodalDataInputFormat] = None,
        video_data: Optional[MultimodalDataInputFormat] = None,
        return_logprob: Optional[Union[List[bool], bool]] = False,
        logprob_start_len: Optional[Union[List[int], int]] = None,
        top_logprobs_num: Optional[Union[List[int], int]] = None,
        token_ids_logprob: Optional[Union[List[List[int]], List[int]]] = None,
        lora_path: Optional[List[Optional[str]]] = None,
        custom_logit_processor: Optional[Union[List[str], str]] = None,
        return_hidden_states: bool = False,
        stream: bool = False,
        bootstrap_host: Optional[Union[List[str], str]] = None,
        bootstrap_port: Optional[Union[List[int], int]] = None,
        bootstrap_room: Optional[Union[List[int], int]] = None,
        data_parallel_rank: Optional[int] = None,
    ) -&gt; Union[Dict, AsyncIterator[Dict]]:
        &#34;&#34;&#34;
        The arguments of this function is the same as `sglang/srt/managers/io_struct.py::GenerateReqInput`.
        Please refer to `GenerateReqInput` for the documentation.
        &#34;&#34;&#34;

        if self.server_args.enable_dp_attention:
            if data_parallel_rank is None:
                logger.debug(&#34;data_parallel_rank not provided, using default dispatch&#34;)
            elif data_parallel_rank &lt; 0:
                raise ValueError(&#34;data_parallel_rank must be non-negative&#34;)
            elif data_parallel_rank &gt;= self.server_args.dp_size:
                raise ValueError(
                    f&#34;data_parallel_rank must be in range [0, {self.server_args.dp_size-1}]&#34;
                )

        logger.debug(f&#34;data_parallel_rank: {data_parallel_rank}&#34;)
        obj = GenerateReqInput(
            text=prompt,
            input_ids=input_ids,
            sampling_params=sampling_params,
            image_data=image_data,
            audio_data=audio_data,
            video_data=video_data,
            return_logprob=return_logprob,
            logprob_start_len=logprob_start_len,
            top_logprobs_num=top_logprobs_num,
            token_ids_logprob=token_ids_logprob,
            lora_path=lora_path,
            return_hidden_states=return_hidden_states,
            stream=stream,
            custom_logit_processor=custom_logit_processor,
            bootstrap_host=bootstrap_host,
            bootstrap_port=bootstrap_port,
            bootstrap_room=bootstrap_room,
            data_parallel_rank=data_parallel_rank,
        )
        generator = self.tokenizer_manager.generate_request(obj, None)

        if stream is True:
            return generator
        else:
            return await generator.__anext__()

    def encode(
        self,
        prompt: Union[str, List[str], List[Dict], List[List[Dict]]],
        image_data: Optional[MultimodalDataInputFormat] = None,
        audio_data: Optional[MultimodalDataInputFormat] = None,
        video_data: Optional[MultimodalDataInputFormat] = None,
    ) -&gt; Dict:
        &#34;&#34;&#34;
        The arguments of this function is the same as `sglang/srt/managers/io_struct.py::EmbeddingReqInput`.
        Please refer to `EmbeddingReqInput` for the documentation.
        &#34;&#34;&#34;
        obj = EmbeddingReqInput(
            text=prompt,
            image_data=image_data,
            audio_data=audio_data,
            video_data=video_data,
        )
        loop = asyncio.get_event_loop()
        generator = self.tokenizer_manager.generate_request(obj, None)
        ret = loop.run_until_complete(generator.__anext__())
        return ret

    async def async_encode(
        self,
        prompt: Union[str, List[str], List[Dict], List[List[Dict]]],
        image_data: Optional[MultimodalDataInputFormat] = None,
        audio_data: Optional[MultimodalDataInputFormat] = None,
        video_data: Optional[MultimodalDataInputFormat] = None,
    ) -&gt; Dict:
        &#34;&#34;&#34;
        Asynchronous version of encode method.

        The arguments of this function is the same as `sglang/srt/managers/io_struct.py::EmbeddingReqInput`.
        Please refer to `EmbeddingReqInput` for the documentation.
        &#34;&#34;&#34;
        obj = EmbeddingReqInput(
            text=prompt,
            image_data=image_data,
            audio_data=audio_data,
            video_data=video_data,
        )
        generator = self.tokenizer_manager.generate_request(obj, None)
        return await generator.__anext__()

    def rerank(
        self,
        prompt: Union[List[List[str]]],
    ) -&gt; Dict:
        &#34;&#34;&#34;
        The arguments of this function is the same as `sglang/srt/managers/io_struct.py::EmbeddingReqInput`.
        Please refer to `EmbeddingReqInput` for the documentation.
        &#34;&#34;&#34;
        obj = EmbeddingReqInput(text=prompt, is_cross_encoder_request=True)
        loop = asyncio.get_event_loop()
        generator = self.tokenizer_manager.generate_request(obj, None)
        ret = loop.run_until_complete(generator.__anext__())
        return ret

    def shutdown(self):
        &#34;&#34;&#34;Shutdown the engine&#34;&#34;&#34;
        kill_process_tree(os.getpid(), include_parent=False)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.shutdown()
        return False

    def flush_cache(self):
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(self.tokenizer_manager.flush_cache())

    def start_profile(self):
        loop = asyncio.get_event_loop()
        loop.run_until_complete(self.tokenizer_manager.start_profile())

    def stop_profile(self):
        loop = asyncio.get_event_loop()
        loop.run_until_complete(self.tokenizer_manager.stop_profile())

    def start_expert_distribution_record(self):
        loop = asyncio.get_event_loop()
        loop.run_until_complete(
            self.tokenizer_manager.start_expert_distribution_record()
        )

    def stop_expert_distribution_record(self):
        loop = asyncio.get_event_loop()
        loop.run_until_complete(
            self.tokenizer_manager.stop_expert_distribution_record()
        )

    def dump_expert_distribution_record(self):
        loop = asyncio.get_event_loop()
        loop.run_until_complete(
            self.tokenizer_manager.dump_expert_distribution_record()
        )

    def get_server_info(self):
        loop = asyncio.get_event_loop()
        internal_states = loop.run_until_complete(
            self.tokenizer_manager.get_internal_state()
        )
        return {
            **dataclasses.asdict(self.tokenizer_manager.server_args),
            **self.scheduler_info,
            &#34;internal_states&#34;: internal_states,
            &#34;version&#34;: __version__,
        }

    def init_weights_update_group(
        self,
        master_address: str,
        master_port: int,
        rank_offset: int,
        world_size: int,
        group_name: str,
        backend: str = &#34;nccl&#34;,
    ):
        &#34;&#34;&#34;Initialize parameter update group.&#34;&#34;&#34;
        obj = InitWeightsUpdateGroupReqInput(
            master_address=master_address,
            master_port=master_port,
            rank_offset=rank_offset,
            world_size=world_size,
            group_name=group_name,
            backend=backend,
        )
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(
            self.tokenizer_manager.init_weights_update_group(obj, None)
        )

    def update_weights_from_distributed(
        self,
        names: list[str],
        dtypes: list[str],
        shapes: list[list[int]],
        group_name: str = &#34;weight_update_group&#34;,
        flush_cache: bool = True,
    ):
        &#34;&#34;&#34;Update weights from distributed source.&#34;&#34;&#34;
        obj = UpdateWeightsFromDistributedReqInput(
            names=names,
            dtypes=dtypes,
            shapes=shapes,
            group_name=group_name,
            flush_cache=flush_cache,
        )
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(
            self.tokenizer_manager.update_weights_from_distributed(obj, None)
        )

    def update_weights_from_tensor(
        self,
        named_tensors: List[Tuple[str, torch.Tensor]],
        load_format: Optional[str] = None,
        flush_cache: bool = True,
    ):
        &#34;&#34;&#34;Update weights from distributed source. If there are going to be more updates, set `flush_cache` to be false
        to avoid duplicated cache cleaning operation.&#34;&#34;&#34;
        if load_format == &#34;flattened_bucket&#34;:
            serialized_named_tensors = named_tensors
        else:
            serialized_named_tensors = [
                MultiprocessingSerializer.serialize(named_tensors)
                for _ in range(self.server_args.tp_size)
            ]
        obj = UpdateWeightsFromTensorReqInput(
            serialized_named_tensors=serialized_named_tensors,
            load_format=load_format,
            flush_cache=flush_cache,
        )
        loop = asyncio.get_event_loop()

        return loop.run_until_complete(
            self.tokenizer_manager.update_weights_from_tensor(obj, None)
        )

    def update_weights_from_disk(
        self,
        model_path: str,
        load_format: Optional[str] = None,
    ):
        &#34;&#34;&#34;Update the weights from disk inplace without re-launching the engine.

        This method allows updating the model weights from disk without restarting
        the engine. It can be used to load a different model or update weights with
        new training.
        &#34;&#34;&#34;
        obj = UpdateWeightFromDiskReqInput(
            model_path=model_path,
            load_format=load_format,
        )

        loop = asyncio.get_event_loop()
        return loop.run_until_complete(
            self.tokenizer_manager.update_weights_from_disk(obj, None)
        )

    def get_weights_by_name(self, name: str, truncate_size: int = 100):
        &#34;&#34;&#34;Get weights by parameter name.&#34;&#34;&#34;
        obj = GetWeightsByNameReqInput(name=name, truncate_size=truncate_size)
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(
            self.tokenizer_manager.get_weights_by_name(obj, None)
        )

    def load_lora_adapter(self, lora_name: str, lora_path: str, pinned: bool = False):
        &#34;&#34;&#34;Load a new LoRA adapter without re-launching the engine.&#34;&#34;&#34;

        obj = LoadLoRAAdapterReqInput(
            lora_name=lora_name,
            lora_path=lora_path,
            pinned=pinned,
        )

        loop = asyncio.get_event_loop()
        return loop.run_until_complete(
            self.tokenizer_manager.load_lora_adapter(obj, None)
        )

    def unload_lora_adapter(self, lora_name: str):
        &#34;&#34;&#34;Unload a LoRA adapter without re-launching the engine.&#34;&#34;&#34;

        obj = UnloadLoRAAdapterReqInput(lora_name=lora_name)

        loop = asyncio.get_event_loop()
        return loop.run_until_complete(
            self.tokenizer_manager.unload_lora_adapter(obj, None)
        )

    def release_memory_occupation(self, tags: Optional[List[str]] = None):
        obj = ReleaseMemoryOccupationReqInput(tags=tags)
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(
            self.tokenizer_manager.release_memory_occupation(obj, None)
        )

    def resume_memory_occupation(self, tags: Optional[List[str]] = None):
        obj = ResumeMemoryOccupationReqInput(tags=tags)
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(
            self.tokenizer_manager.resume_memory_occupation(obj, None)
        )

    def freeze_gc(self):
        &#34;&#34;&#34;
        To maintain a high performance server with low latency, we want to reduce the
        stalls caused by the garbage collector scanning through a large number of objects.

        It is usually helpful to start the server and warm it up with real requests to
        initialize many of the long-lived objects that do not need to be garbage collected.

        After sufficient warmup, we can call this function to freeze the garbage collector
        so that all objects created before this point are considered out of scope for garbage
        collection.
        &#34;&#34;&#34;

        loop = asyncio.get_event_loop()
        loop.run_until_complete(self.tokenizer_manager.freeze_gc())

    &#34;&#34;&#34;
    Execute an RPC call on all scheduler processes.
    &#34;&#34;&#34;

    def collective_rpc(self, method: str, **kwargs):
        obj = RpcReqInput(method=method, parameters=kwargs)
        self.send_to_rpc.send_pyobj(obj)
        recv_req = self.send_to_rpc.recv_pyobj(zmq.BLOCKY)
        assert isinstance(recv_req, RpcReqOutput)
        assert recv_req.success, recv_req.message

    def save_remote_model(self, **kwargs):
        self.collective_rpc(&#34;save_remote_model&#34;, **kwargs)

    def save_sharded_model(self, **kwargs):
        self.collective_rpc(&#34;save_sharded_model&#34;, **kwargs)

    def score(
        self,
        query: Optional[Union[str, List[int]]] = None,
        items: Optional[Union[str, List[str], List[List[int]]]] = None,
        label_token_ids: Optional[List[int]] = None,
        apply_softmax: bool = False,
        item_first: bool = False,
    ) -&gt; List[List[float]]:
        &#34;&#34;&#34;
        Score the probability of specified token IDs appearing after the given (query + item) pair. For example:
        query = &#34;&lt;|user|&gt;Is the following city the capital of France? &#34;
        items = [&#34;Paris &lt;|assistant|&gt;&#34;, &#34;London &lt;|assistant|&gt;&#34;, &#34;Berlin &lt;|assistant|&gt;&#34;]
        label_token_ids = [2332, 1223] # Token IDs for &#34;Yes&#34; and &#34;No&#34;
        item_first = False

        This would pass the following prompts to the model:
        &#34;&lt;|user|&gt;Is the following city the capital of France? Paris &lt;|assistant|&gt;&#34;
        &#34;&lt;|user|&gt;Is the following city the capital of France? London &lt;|assistant|&gt;&#34;
        &#34;&lt;|user|&gt;Is the following city the capital of France? Berlin &lt;|assistant|&gt;&#34;
        The api would then return the probabilities of the model producing &#34;Yes&#34; and &#34;No&#34; as the next token.
        The output would look like:
        [[0.9, 0.1], [0.2, 0.8], [0.1, 0.9]]


        Args:
            query: The query text or pre-tokenized query token IDs. Must be provided.
            items: The item text(s) or pre-tokenized item token IDs. Must be provided.
            label_token_ids: List of token IDs to compute probabilities for. If None, no token probabilities will be computed.
            apply_softmax: Whether to normalize probabilities using softmax.
            item_first: If True, prepend items to query. Otherwise append items to query.

        Returns:
            List of dictionaries mapping token IDs to their probabilities for each item.
            Each dictionary in the list corresponds to one item input.

        Raises:
            ValueError: If query is not provided, or if items is not provided,
                      or if token IDs are out of vocabulary, or if logprobs are not available for the specified tokens.
        &#34;&#34;&#34;
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(
            self.tokenizer_manager.score_request(
                query=query,
                items=items,
                label_token_ids=label_token_ids,
                apply_softmax=apply_softmax,
                item_first=item_first,
                request=None,
            )
        )

    async def async_score(
        self,
        query: Optional[Union[str, List[int]]] = None,
        items: Optional[Union[str, List[str], List[List[int]]]] = None,
        label_token_ids: Optional[List[int]] = None,
        apply_softmax: bool = False,
        item_first: bool = False,
    ) -&gt; List[List[float]]:
        &#34;&#34;&#34;
        Asynchronous version of score method.

        See score() for detailed documentation.
        &#34;&#34;&#34;
        return await self.tokenizer_manager.score_request(
            query=query,
            items=items,
            label_token_ids=label_token_ids,
            apply_softmax=apply_softmax,
            item_first=item_first,
            request=None,
        )</code></pre>
</details>
<div class="desc"><p>The entry point to the inference engine.</p>
<ul>
<li>The engine consists of three components:<ol>
<li>TokenizerManager: Tokenizes the requests and sends them to the scheduler.</li>
<li>Scheduler (subprocess): Receives requests from the Tokenizer Manager, schedules batches, forwards them, and sends the output tokens to the Detokenizer Manager.</li>
<li>DetokenizerManager (subprocess): Detokenizes the output tokens and sends the result back to the Tokenizer Manager.</li>
</ol>
</li>
</ul>
<p>Note:
1. The HTTP server, Engine, and TokenizerManager all run in the main process.
2. Inter-process communication (IPC) is handled via the ZMQ library, with each process using a different port.</p>
<p>The arguments of this function is the same as <code>sglang/srt/server_args.py::ServerArgs</code>.
Please refer to <code>ServerArgs</code> for the documentation.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.entrypoints.EngineBase.EngineBase" href="EngineBase.html#sglang.srt.entrypoints.EngineBase.EngineBase">EngineBase</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.entrypoints.engine.Engine.async_encode"><code class="name flex">
<span>async def <span class="ident">async_encode</span></span>(<span>self,<br>prompt:Â strÂ |Â List[str]Â |Â List[Dict]Â |Â List[List[Dict]],<br>image_data:Â List[List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]]Â |Â List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]Â |Â AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â DictÂ |Â NoneÂ =Â None,<br>audio_data:Â List[List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]]Â |Â List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]Â |Â AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â DictÂ |Â NoneÂ =Â None,<br>video_data:Â List[List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]]Â |Â List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]Â |Â AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â DictÂ |Â NoneÂ =Â None) â€‘>Â Dict</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def async_encode(
    self,
    prompt: Union[str, List[str], List[Dict], List[List[Dict]]],
    image_data: Optional[MultimodalDataInputFormat] = None,
    audio_data: Optional[MultimodalDataInputFormat] = None,
    video_data: Optional[MultimodalDataInputFormat] = None,
) -&gt; Dict:
    &#34;&#34;&#34;
    Asynchronous version of encode method.

    The arguments of this function is the same as `sglang/srt/managers/io_struct.py::EmbeddingReqInput`.
    Please refer to `EmbeddingReqInput` for the documentation.
    &#34;&#34;&#34;
    obj = EmbeddingReqInput(
        text=prompt,
        image_data=image_data,
        audio_data=audio_data,
        video_data=video_data,
    )
    generator = self.tokenizer_manager.generate_request(obj, None)
    return await generator.__anext__()</code></pre>
</details>
<div class="desc"><p>Asynchronous version of encode method.</p>
<p>The arguments of this function is the same as <code>sglang/srt/managers/io_struct.py::EmbeddingReqInput</code>.
Please refer to <code>EmbeddingReqInput</code> for the documentation.</p></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.async_generate"><code class="name flex">
<span>async def <span class="ident">async_generate</span></span>(<span>self,<br>prompt:Â List[str]Â |Â strÂ |Â NoneÂ =Â None,<br>sampling_params:Â List[Dict]Â |Â DictÂ |Â NoneÂ =Â None,<br>input_ids:Â List[List[int]]Â |Â List[int]Â |Â NoneÂ =Â None,<br>image_data:Â List[List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]]Â |Â List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]Â |Â AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â DictÂ |Â NoneÂ =Â None,<br>audio_data:Â List[List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]]Â |Â List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]Â |Â AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â DictÂ |Â NoneÂ =Â None,<br>video_data:Â List[List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]]Â |Â List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]Â |Â AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â DictÂ |Â NoneÂ =Â None,<br>return_logprob:Â List[bool]Â |Â boolÂ |Â NoneÂ =Â False,<br>logprob_start_len:Â List[int]Â |Â intÂ |Â NoneÂ =Â None,<br>top_logprobs_num:Â List[int]Â |Â intÂ |Â NoneÂ =Â None,<br>token_ids_logprob:Â List[List[int]]Â |Â List[int]Â |Â NoneÂ =Â None,<br>lora_path:Â List[strÂ |Â None]Â |Â NoneÂ =Â None,<br>custom_logit_processor:Â List[str]Â |Â strÂ |Â NoneÂ =Â None,<br>return_hidden_states:Â boolÂ =Â False,<br>stream:Â boolÂ =Â False,<br>bootstrap_host:Â List[str]Â |Â strÂ |Â NoneÂ =Â None,<br>bootstrap_port:Â List[int]Â |Â intÂ |Â NoneÂ =Â None,<br>bootstrap_room:Â List[int]Â |Â intÂ |Â NoneÂ =Â None,<br>data_parallel_rank:Â intÂ |Â NoneÂ =Â None) â€‘>Â DictÂ |Â AsyncIterator[Dict]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def async_generate(
    self,
    # The input prompt. It can be a single prompt or a batch of prompts.
    prompt: Optional[Union[List[str], str]] = None,
    sampling_params: Optional[Union[List[Dict], Dict]] = None,
    # The token ids for text; one can either specify text or input_ids.
    input_ids: Optional[Union[List[List[int]], List[int]]] = None,
    # The image input. It can be an image instance, file name, URL, or base64 encoded string.
    # Can be formatted as:
    # - Single image for a single request
    # - List of images (one per request in a batch)
    # - List of lists of images (multiple images per request)
    # See also python/sglang/srt/utils.py:load_image for more details.
    image_data: Optional[MultimodalDataInputFormat] = None,
    audio_data: Optional[MultimodalDataInputFormat] = None,
    video_data: Optional[MultimodalDataInputFormat] = None,
    return_logprob: Optional[Union[List[bool], bool]] = False,
    logprob_start_len: Optional[Union[List[int], int]] = None,
    top_logprobs_num: Optional[Union[List[int], int]] = None,
    token_ids_logprob: Optional[Union[List[List[int]], List[int]]] = None,
    lora_path: Optional[List[Optional[str]]] = None,
    custom_logit_processor: Optional[Union[List[str], str]] = None,
    return_hidden_states: bool = False,
    stream: bool = False,
    bootstrap_host: Optional[Union[List[str], str]] = None,
    bootstrap_port: Optional[Union[List[int], int]] = None,
    bootstrap_room: Optional[Union[List[int], int]] = None,
    data_parallel_rank: Optional[int] = None,
) -&gt; Union[Dict, AsyncIterator[Dict]]:
    &#34;&#34;&#34;
    The arguments of this function is the same as `sglang/srt/managers/io_struct.py::GenerateReqInput`.
    Please refer to `GenerateReqInput` for the documentation.
    &#34;&#34;&#34;

    if self.server_args.enable_dp_attention:
        if data_parallel_rank is None:
            logger.debug(&#34;data_parallel_rank not provided, using default dispatch&#34;)
        elif data_parallel_rank &lt; 0:
            raise ValueError(&#34;data_parallel_rank must be non-negative&#34;)
        elif data_parallel_rank &gt;= self.server_args.dp_size:
            raise ValueError(
                f&#34;data_parallel_rank must be in range [0, {self.server_args.dp_size-1}]&#34;
            )

    logger.debug(f&#34;data_parallel_rank: {data_parallel_rank}&#34;)
    obj = GenerateReqInput(
        text=prompt,
        input_ids=input_ids,
        sampling_params=sampling_params,
        image_data=image_data,
        audio_data=audio_data,
        video_data=video_data,
        return_logprob=return_logprob,
        logprob_start_len=logprob_start_len,
        top_logprobs_num=top_logprobs_num,
        token_ids_logprob=token_ids_logprob,
        lora_path=lora_path,
        return_hidden_states=return_hidden_states,
        stream=stream,
        custom_logit_processor=custom_logit_processor,
        bootstrap_host=bootstrap_host,
        bootstrap_port=bootstrap_port,
        bootstrap_room=bootstrap_room,
        data_parallel_rank=data_parallel_rank,
    )
    generator = self.tokenizer_manager.generate_request(obj, None)

    if stream is True:
        return generator
    else:
        return await generator.__anext__()</code></pre>
</details>
<div class="desc"><p>The arguments of this function is the same as <code>sglang/srt/managers/io_struct.py::GenerateReqInput</code>.
Please refer to <code>GenerateReqInput</code> for the documentation.</p></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.async_score"><code class="name flex">
<span>async def <span class="ident">async_score</span></span>(<span>self,<br>query:Â strÂ |Â List[int]Â |Â NoneÂ =Â None,<br>items:Â strÂ |Â List[str]Â |Â List[List[int]]Â |Â NoneÂ =Â None,<br>label_token_ids:Â List[int]Â |Â NoneÂ =Â None,<br>apply_softmax:Â boolÂ =Â False,<br>item_first:Â boolÂ =Â False) â€‘>Â List[List[float]]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def async_score(
    self,
    query: Optional[Union[str, List[int]]] = None,
    items: Optional[Union[str, List[str], List[List[int]]]] = None,
    label_token_ids: Optional[List[int]] = None,
    apply_softmax: bool = False,
    item_first: bool = False,
) -&gt; List[List[float]]:
    &#34;&#34;&#34;
    Asynchronous version of score method.

    See score() for detailed documentation.
    &#34;&#34;&#34;
    return await self.tokenizer_manager.score_request(
        query=query,
        items=items,
        label_token_ids=label_token_ids,
        apply_softmax=apply_softmax,
        item_first=item_first,
        request=None,
    )</code></pre>
</details>
<div class="desc"><p>Asynchronous version of score method.</p>
<p>See score() for detailed documentation.</p></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.collective_rpc"><code class="name flex">
<span>def <span class="ident">collective_rpc</span></span>(<span>self, method:Â str, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def collective_rpc(self, method: str, **kwargs):
    obj = RpcReqInput(method=method, parameters=kwargs)
    self.send_to_rpc.send_pyobj(obj)
    recv_req = self.send_to_rpc.recv_pyobj(zmq.BLOCKY)
    assert isinstance(recv_req, RpcReqOutput)
    assert recv_req.success, recv_req.message</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.dump_expert_distribution_record"><code class="name flex">
<span>def <span class="ident">dump_expert_distribution_record</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump_expert_distribution_record(self):
    loop = asyncio.get_event_loop()
    loop.run_until_complete(
        self.tokenizer_manager.dump_expert_distribution_record()
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.encode"><code class="name flex">
<span>def <span class="ident">encode</span></span>(<span>self,<br>prompt:Â strÂ |Â List[str]Â |Â List[Dict]Â |Â List[List[Dict]],<br>image_data:Â List[List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]]Â |Â List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]Â |Â AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â DictÂ |Â NoneÂ =Â None,<br>audio_data:Â List[List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]]Â |Â List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]Â |Â AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â DictÂ |Â NoneÂ =Â None,<br>video_data:Â List[List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]]Â |Â List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]Â |Â AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â DictÂ |Â NoneÂ =Â None) â€‘>Â Dict</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def encode(
    self,
    prompt: Union[str, List[str], List[Dict], List[List[Dict]]],
    image_data: Optional[MultimodalDataInputFormat] = None,
    audio_data: Optional[MultimodalDataInputFormat] = None,
    video_data: Optional[MultimodalDataInputFormat] = None,
) -&gt; Dict:
    &#34;&#34;&#34;
    The arguments of this function is the same as `sglang/srt/managers/io_struct.py::EmbeddingReqInput`.
    Please refer to `EmbeddingReqInput` for the documentation.
    &#34;&#34;&#34;
    obj = EmbeddingReqInput(
        text=prompt,
        image_data=image_data,
        audio_data=audio_data,
        video_data=video_data,
    )
    loop = asyncio.get_event_loop()
    generator = self.tokenizer_manager.generate_request(obj, None)
    ret = loop.run_until_complete(generator.__anext__())
    return ret</code></pre>
</details>
<div class="desc"><p>The arguments of this function is the same as <code>sglang/srt/managers/io_struct.py::EmbeddingReqInput</code>.
Please refer to <code>EmbeddingReqInput</code> for the documentation.</p></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.freeze_gc"><code class="name flex">
<span>def <span class="ident">freeze_gc</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def freeze_gc(self):
    &#34;&#34;&#34;
    To maintain a high performance server with low latency, we want to reduce the
    stalls caused by the garbage collector scanning through a large number of objects.

    It is usually helpful to start the server and warm it up with real requests to
    initialize many of the long-lived objects that do not need to be garbage collected.

    After sufficient warmup, we can call this function to freeze the garbage collector
    so that all objects created before this point are considered out of scope for garbage
    collection.
    &#34;&#34;&#34;

    loop = asyncio.get_event_loop()
    loop.run_until_complete(self.tokenizer_manager.freeze_gc())</code></pre>
</details>
<div class="desc"><p>To maintain a high performance server with low latency, we want to reduce the
stalls caused by the garbage collector scanning through a large number of objects.</p>
<p>It is usually helpful to start the server and warm it up with real requests to
initialize many of the long-lived objects that do not need to be garbage collected.</p>
<p>After sufficient warmup, we can call this function to freeze the garbage collector
so that all objects created before this point are considered out of scope for garbage
collection.</p></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.generate"><code class="name flex">
<span>def <span class="ident">generate</span></span>(<span>self,<br>prompt:Â List[str]Â |Â strÂ |Â NoneÂ =Â None,<br>sampling_params:Â List[Dict]Â |Â DictÂ |Â NoneÂ =Â None,<br>input_ids:Â List[List[int]]Â |Â List[int]Â |Â NoneÂ =Â None,<br>image_data:Â List[List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]]Â |Â List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]Â |Â AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â DictÂ |Â NoneÂ =Â None,<br>audio_data:Â List[List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]]Â |Â List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]Â |Â AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â DictÂ |Â NoneÂ =Â None,<br>video_data:Â List[List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]]Â |Â List[AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â Dict]Â |Â AnyÂ |Â strÂ |Â <a title="sglang.srt.utils.ImageData" href="../utils.html#sglang.srt.utils.ImageData">ImageData</a>Â |Â DictÂ |Â NoneÂ =Â None,<br>return_logprob:Â List[bool]Â |Â boolÂ |Â NoneÂ =Â False,<br>logprob_start_len:Â List[int]Â |Â intÂ |Â NoneÂ =Â None,<br>top_logprobs_num:Â List[int]Â |Â intÂ |Â NoneÂ =Â None,<br>token_ids_logprob:Â List[List[int]]Â |Â List[int]Â |Â NoneÂ =Â None,<br>lora_path:Â List[strÂ |Â None]Â |Â NoneÂ =Â None,<br>custom_logit_processor:Â List[str]Â |Â strÂ |Â NoneÂ =Â None,<br>return_hidden_states:Â boolÂ =Â False,<br>stream:Â boolÂ =Â False,<br>bootstrap_host:Â List[str]Â |Â strÂ |Â NoneÂ =Â None,<br>bootstrap_port:Â List[int]Â |Â intÂ |Â NoneÂ =Â None,<br>bootstrap_room:Â List[int]Â |Â intÂ |Â NoneÂ =Â None,<br>data_parallel_rank:Â intÂ |Â NoneÂ =Â None) â€‘>Â DictÂ |Â Iterator[Dict]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate(
    self,
    # The input prompt. It can be a single prompt or a batch of prompts.
    prompt: Optional[Union[List[str], str]] = None,
    sampling_params: Optional[Union[List[Dict], Dict]] = None,
    # The token ids for text; one can either specify text or input_ids.
    input_ids: Optional[Union[List[List[int]], List[int]]] = None,
    # The image input. It can be an image instance, file name, URL, or base64 encoded string.
    # Can be formatted as:
    # - Single image for a single request
    # - List of images (one per request in a batch)
    # - List of lists of images (multiple images per request)
    # See also python/sglang/srt/utils.py:load_image for more details.
    image_data: Optional[MultimodalDataInputFormat] = None,
    audio_data: Optional[MultimodalDataInputFormat] = None,
    video_data: Optional[MultimodalDataInputFormat] = None,
    return_logprob: Optional[Union[List[bool], bool]] = False,
    logprob_start_len: Optional[Union[List[int], int]] = None,
    top_logprobs_num: Optional[Union[List[int], int]] = None,
    token_ids_logprob: Optional[Union[List[List[int]], List[int]]] = None,
    lora_path: Optional[List[Optional[str]]] = None,
    custom_logit_processor: Optional[Union[List[str], str]] = None,
    return_hidden_states: bool = False,
    stream: bool = False,
    bootstrap_host: Optional[Union[List[str], str]] = None,
    bootstrap_port: Optional[Union[List[int], int]] = None,
    bootstrap_room: Optional[Union[List[int], int]] = None,
    data_parallel_rank: Optional[int] = None,
) -&gt; Union[Dict, Iterator[Dict]]:
    &#34;&#34;&#34;
    The arguments of this function is the same as `sglang/srt/managers/io_struct.py::GenerateReqInput`.
    Please refer to `GenerateReqInput` for the documentation.
    &#34;&#34;&#34;
    if self.server_args.enable_dp_attention:
        if data_parallel_rank is None:
            logger.debug(&#34;data_parallel_rank not provided, using default dispatch&#34;)
        elif data_parallel_rank &lt; 0:
            raise ValueError(&#34;data_parallel_rank must be non-negative&#34;)
        elif data_parallel_rank &gt;= self.server_args.dp_size:
            raise ValueError(
                f&#34;data_parallel_rank must be less than dp_size: {self.server_args.dp_size}&#34;
            )

    obj = GenerateReqInput(
        text=prompt,
        input_ids=input_ids,
        sampling_params=sampling_params,
        image_data=image_data,
        audio_data=audio_data,
        video_data=video_data,
        return_logprob=return_logprob,
        logprob_start_len=logprob_start_len,
        top_logprobs_num=top_logprobs_num,
        token_ids_logprob=token_ids_logprob,
        lora_path=lora_path,
        custom_logit_processor=custom_logit_processor,
        return_hidden_states=return_hidden_states,
        stream=stream,
        bootstrap_host=bootstrap_host,
        bootstrap_port=bootstrap_port,
        bootstrap_room=bootstrap_room,
        data_parallel_rank=data_parallel_rank,
    )
    loop = asyncio.get_event_loop()
    generator = self.tokenizer_manager.generate_request(obj, None)

    if stream:

        def generator_wrapper():
            while True:
                try:
                    chunk = loop.run_until_complete(generator.__anext__())
                    yield chunk
                except StopAsyncIteration:
                    break

        return generator_wrapper()
    else:
        ret = loop.run_until_complete(generator.__anext__())
        return ret</code></pre>
</details>
<div class="desc"><p>The arguments of this function is the same as <code>sglang/srt/managers/io_struct.py::GenerateReqInput</code>.
Please refer to <code>GenerateReqInput</code> for the documentation.</p></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.get_server_info"><code class="name flex">
<span>def <span class="ident">get_server_info</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_server_info(self):
    loop = asyncio.get_event_loop()
    internal_states = loop.run_until_complete(
        self.tokenizer_manager.get_internal_state()
    )
    return {
        **dataclasses.asdict(self.tokenizer_manager.server_args),
        **self.scheduler_info,
        &#34;internal_states&#34;: internal_states,
        &#34;version&#34;: __version__,
    }</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.get_weights_by_name"><code class="name flex">
<span>def <span class="ident">get_weights_by_name</span></span>(<span>self, name:Â str, truncate_size:Â intÂ =Â 100)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_weights_by_name(self, name: str, truncate_size: int = 100):
    &#34;&#34;&#34;Get weights by parameter name.&#34;&#34;&#34;
    obj = GetWeightsByNameReqInput(name=name, truncate_size=truncate_size)
    loop = asyncio.get_event_loop()
    return loop.run_until_complete(
        self.tokenizer_manager.get_weights_by_name(obj, None)
    )</code></pre>
</details>
<div class="desc"><p>Get weights by parameter name.</p></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.init_weights_update_group"><code class="name flex">
<span>def <span class="ident">init_weights_update_group</span></span>(<span>self,<br>master_address:Â str,<br>master_port:Â int,<br>rank_offset:Â int,<br>world_size:Â int,<br>group_name:Â str,<br>backend:Â strÂ =Â 'nccl')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_weights_update_group(
    self,
    master_address: str,
    master_port: int,
    rank_offset: int,
    world_size: int,
    group_name: str,
    backend: str = &#34;nccl&#34;,
):
    &#34;&#34;&#34;Initialize parameter update group.&#34;&#34;&#34;
    obj = InitWeightsUpdateGroupReqInput(
        master_address=master_address,
        master_port=master_port,
        rank_offset=rank_offset,
        world_size=world_size,
        group_name=group_name,
        backend=backend,
    )
    loop = asyncio.get_event_loop()
    return loop.run_until_complete(
        self.tokenizer_manager.init_weights_update_group(obj, None)
    )</code></pre>
</details>
<div class="desc"><p>Initialize parameter update group.</p></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.rerank"><code class="name flex">
<span>def <span class="ident">rerank</span></span>(<span>self, prompt:Â List[List[str]]) â€‘>Â Dict</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rerank(
    self,
    prompt: Union[List[List[str]]],
) -&gt; Dict:
    &#34;&#34;&#34;
    The arguments of this function is the same as `sglang/srt/managers/io_struct.py::EmbeddingReqInput`.
    Please refer to `EmbeddingReqInput` for the documentation.
    &#34;&#34;&#34;
    obj = EmbeddingReqInput(text=prompt, is_cross_encoder_request=True)
    loop = asyncio.get_event_loop()
    generator = self.tokenizer_manager.generate_request(obj, None)
    ret = loop.run_until_complete(generator.__anext__())
    return ret</code></pre>
</details>
<div class="desc"><p>The arguments of this function is the same as <code>sglang/srt/managers/io_struct.py::EmbeddingReqInput</code>.
Please refer to <code>EmbeddingReqInput</code> for the documentation.</p></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.save_remote_model"><code class="name flex">
<span>def <span class="ident">save_remote_model</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_remote_model(self, **kwargs):
    self.collective_rpc(&#34;save_remote_model&#34;, **kwargs)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.save_sharded_model"><code class="name flex">
<span>def <span class="ident">save_sharded_model</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_sharded_model(self, **kwargs):
    self.collective_rpc(&#34;save_sharded_model&#34;, **kwargs)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.score"><code class="name flex">
<span>def <span class="ident">score</span></span>(<span>self,<br>query:Â strÂ |Â List[int]Â |Â NoneÂ =Â None,<br>items:Â strÂ |Â List[str]Â |Â List[List[int]]Â |Â NoneÂ =Â None,<br>label_token_ids:Â List[int]Â |Â NoneÂ =Â None,<br>apply_softmax:Â boolÂ =Â False,<br>item_first:Â boolÂ =Â False) â€‘>Â List[List[float]]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def score(
    self,
    query: Optional[Union[str, List[int]]] = None,
    items: Optional[Union[str, List[str], List[List[int]]]] = None,
    label_token_ids: Optional[List[int]] = None,
    apply_softmax: bool = False,
    item_first: bool = False,
) -&gt; List[List[float]]:
    &#34;&#34;&#34;
    Score the probability of specified token IDs appearing after the given (query + item) pair. For example:
    query = &#34;&lt;|user|&gt;Is the following city the capital of France? &#34;
    items = [&#34;Paris &lt;|assistant|&gt;&#34;, &#34;London &lt;|assistant|&gt;&#34;, &#34;Berlin &lt;|assistant|&gt;&#34;]
    label_token_ids = [2332, 1223] # Token IDs for &#34;Yes&#34; and &#34;No&#34;
    item_first = False

    This would pass the following prompts to the model:
    &#34;&lt;|user|&gt;Is the following city the capital of France? Paris &lt;|assistant|&gt;&#34;
    &#34;&lt;|user|&gt;Is the following city the capital of France? London &lt;|assistant|&gt;&#34;
    &#34;&lt;|user|&gt;Is the following city the capital of France? Berlin &lt;|assistant|&gt;&#34;
    The api would then return the probabilities of the model producing &#34;Yes&#34; and &#34;No&#34; as the next token.
    The output would look like:
    [[0.9, 0.1], [0.2, 0.8], [0.1, 0.9]]


    Args:
        query: The query text or pre-tokenized query token IDs. Must be provided.
        items: The item text(s) or pre-tokenized item token IDs. Must be provided.
        label_token_ids: List of token IDs to compute probabilities for. If None, no token probabilities will be computed.
        apply_softmax: Whether to normalize probabilities using softmax.
        item_first: If True, prepend items to query. Otherwise append items to query.

    Returns:
        List of dictionaries mapping token IDs to their probabilities for each item.
        Each dictionary in the list corresponds to one item input.

    Raises:
        ValueError: If query is not provided, or if items is not provided,
                  or if token IDs are out of vocabulary, or if logprobs are not available for the specified tokens.
    &#34;&#34;&#34;
    loop = asyncio.get_event_loop()
    return loop.run_until_complete(
        self.tokenizer_manager.score_request(
            query=query,
            items=items,
            label_token_ids=label_token_ids,
            apply_softmax=apply_softmax,
            item_first=item_first,
            request=None,
        )
    )</code></pre>
</details>
<div class="desc"><p>Score the probability of specified token IDs appearing after the given (query + item) pair. For example:
query = "&lt;|user|&gt;Is the following city the capital of France? "
items = ["Paris &lt;|assistant|&gt;", "London &lt;|assistant|&gt;", "Berlin &lt;|assistant|&gt;"]
label_token_ids = [2332, 1223] # Token IDs for "Yes" and "No"
item_first = False</p>
<p>This would pass the following prompts to the model:
"&lt;|user|&gt;Is the following city the capital of France? Paris &lt;|assistant|&gt;"
"&lt;|user|&gt;Is the following city the capital of France? London &lt;|assistant|&gt;"
"&lt;|user|&gt;Is the following city the capital of France? Berlin &lt;|assistant|&gt;"
The api would then return the probabilities of the model producing "Yes" and "No" as the next token.
The output would look like:
[[0.9, 0.1], [0.2, 0.8], [0.1, 0.9]]</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>query</code></strong></dt>
<dd>The query text or pre-tokenized query token IDs. Must be provided.</dd>
<dt><strong><code>items</code></strong></dt>
<dd>The item text(s) or pre-tokenized item token IDs. Must be provided.</dd>
<dt><strong><code>label_token_ids</code></strong></dt>
<dd>List of token IDs to compute probabilities for. If None, no token probabilities will be computed.</dd>
<dt><strong><code>apply_softmax</code></strong></dt>
<dd>Whether to normalize probabilities using softmax.</dd>
<dt><strong><code>item_first</code></strong></dt>
<dd>If True, prepend items to query. Otherwise append items to query.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>List of dictionaries mapping token IDs to their probabilities for each item.
Each dictionary in the list corresponds to one item input.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If query is not provided, or if items is not provided,
or if token IDs are out of vocabulary, or if logprobs are not available for the specified tokens.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.shutdown"><code class="name flex">
<span>def <span class="ident">shutdown</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shutdown(self):
    &#34;&#34;&#34;Shutdown the engine&#34;&#34;&#34;
    kill_process_tree(os.getpid(), include_parent=False)</code></pre>
</details>
<div class="desc"><p>Shutdown the engine</p></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.start_expert_distribution_record"><code class="name flex">
<span>def <span class="ident">start_expert_distribution_record</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def start_expert_distribution_record(self):
    loop = asyncio.get_event_loop()
    loop.run_until_complete(
        self.tokenizer_manager.start_expert_distribution_record()
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.start_profile"><code class="name flex">
<span>def <span class="ident">start_profile</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def start_profile(self):
    loop = asyncio.get_event_loop()
    loop.run_until_complete(self.tokenizer_manager.start_profile())</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.stop_expert_distribution_record"><code class="name flex">
<span>def <span class="ident">stop_expert_distribution_record</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stop_expert_distribution_record(self):
    loop = asyncio.get_event_loop()
    loop.run_until_complete(
        self.tokenizer_manager.stop_expert_distribution_record()
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.stop_profile"><code class="name flex">
<span>def <span class="ident">stop_profile</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stop_profile(self):
    loop = asyncio.get_event_loop()
    loop.run_until_complete(self.tokenizer_manager.stop_profile())</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.update_weights_from_disk"><code class="name flex">
<span>def <span class="ident">update_weights_from_disk</span></span>(<span>self, model_path:Â str, load_format:Â strÂ |Â NoneÂ =Â None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_weights_from_disk(
    self,
    model_path: str,
    load_format: Optional[str] = None,
):
    &#34;&#34;&#34;Update the weights from disk inplace without re-launching the engine.

    This method allows updating the model weights from disk without restarting
    the engine. It can be used to load a different model or update weights with
    new training.
    &#34;&#34;&#34;
    obj = UpdateWeightFromDiskReqInput(
        model_path=model_path,
        load_format=load_format,
    )

    loop = asyncio.get_event_loop()
    return loop.run_until_complete(
        self.tokenizer_manager.update_weights_from_disk(obj, None)
    )</code></pre>
</details>
<div class="desc"><p>Update the weights from disk inplace without re-launching the engine.</p>
<p>This method allows updating the model weights from disk without restarting
the engine. It can be used to load a different model or update weights with
new training.</p></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.update_weights_from_distributed"><code class="name flex">
<span>def <span class="ident">update_weights_from_distributed</span></span>(<span>self,<br>names:Â list[str],<br>dtypes:Â list[str],<br>shapes:Â list[list[int]],<br>group_name:Â strÂ =Â 'weight_update_group',<br>flush_cache:Â boolÂ =Â True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_weights_from_distributed(
    self,
    names: list[str],
    dtypes: list[str],
    shapes: list[list[int]],
    group_name: str = &#34;weight_update_group&#34;,
    flush_cache: bool = True,
):
    &#34;&#34;&#34;Update weights from distributed source.&#34;&#34;&#34;
    obj = UpdateWeightsFromDistributedReqInput(
        names=names,
        dtypes=dtypes,
        shapes=shapes,
        group_name=group_name,
        flush_cache=flush_cache,
    )
    loop = asyncio.get_event_loop()
    return loop.run_until_complete(
        self.tokenizer_manager.update_weights_from_distributed(obj, None)
    )</code></pre>
</details>
<div class="desc"><p>Update weights from distributed source.</p></div>
</dd>
<dt id="sglang.srt.entrypoints.engine.Engine.update_weights_from_tensor"><code class="name flex">
<span>def <span class="ident">update_weights_from_tensor</span></span>(<span>self,<br>named_tensors:Â List[Tuple[str,Â torch.Tensor]],<br>load_format:Â strÂ |Â NoneÂ =Â None,<br>flush_cache:Â boolÂ =Â True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_weights_from_tensor(
    self,
    named_tensors: List[Tuple[str, torch.Tensor]],
    load_format: Optional[str] = None,
    flush_cache: bool = True,
):
    &#34;&#34;&#34;Update weights from distributed source. If there are going to be more updates, set `flush_cache` to be false
    to avoid duplicated cache cleaning operation.&#34;&#34;&#34;
    if load_format == &#34;flattened_bucket&#34;:
        serialized_named_tensors = named_tensors
    else:
        serialized_named_tensors = [
            MultiprocessingSerializer.serialize(named_tensors)
            for _ in range(self.server_args.tp_size)
        ]
    obj = UpdateWeightsFromTensorReqInput(
        serialized_named_tensors=serialized_named_tensors,
        load_format=load_format,
        flush_cache=flush_cache,
    )
    loop = asyncio.get_event_loop()

    return loop.run_until_complete(
        self.tokenizer_manager.update_weights_from_tensor(obj, None)
    )</code></pre>
</details>
<div class="desc"><p>Update weights from distributed source. If there are going to be more updates, set <code>flush_cache</code> to be false
to avoid duplicated cache cleaning operation.</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.entrypoints.EngineBase.EngineBase" href="EngineBase.html#sglang.srt.entrypoints.EngineBase.EngineBase">EngineBase</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.entrypoints.EngineBase.EngineBase.flush_cache" href="EngineBase.html#sglang.srt.entrypoints.EngineBase.EngineBase.flush_cache">flush_cache</a></code></li>
<li><code><a title="sglang.srt.entrypoints.EngineBase.EngineBase.load_lora_adapter" href="EngineBase.html#sglang.srt.entrypoints.EngineBase.EngineBase.load_lora_adapter">load_lora_adapter</a></code></li>
<li><code><a title="sglang.srt.entrypoints.EngineBase.EngineBase.release_memory_occupation" href="EngineBase.html#sglang.srt.entrypoints.EngineBase.EngineBase.release_memory_occupation">release_memory_occupation</a></code></li>
<li><code><a title="sglang.srt.entrypoints.EngineBase.EngineBase.resume_memory_occupation" href="EngineBase.html#sglang.srt.entrypoints.EngineBase.EngineBase.resume_memory_occupation">resume_memory_occupation</a></code></li>
<li><code><a title="sglang.srt.entrypoints.EngineBase.EngineBase.unload_lora_adapter" href="EngineBase.html#sglang.srt.entrypoints.EngineBase.EngineBase.unload_lora_adapter">unload_lora_adapter</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.entrypoints" href="index.html">sglang.srt.entrypoints</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.entrypoints.engine.Engine" href="#sglang.srt.entrypoints.engine.Engine">Engine</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.entrypoints.engine.Engine.async_encode" href="#sglang.srt.entrypoints.engine.Engine.async_encode">async_encode</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.async_generate" href="#sglang.srt.entrypoints.engine.Engine.async_generate">async_generate</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.async_score" href="#sglang.srt.entrypoints.engine.Engine.async_score">async_score</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.collective_rpc" href="#sglang.srt.entrypoints.engine.Engine.collective_rpc">collective_rpc</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.dump_expert_distribution_record" href="#sglang.srt.entrypoints.engine.Engine.dump_expert_distribution_record">dump_expert_distribution_record</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.encode" href="#sglang.srt.entrypoints.engine.Engine.encode">encode</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.freeze_gc" href="#sglang.srt.entrypoints.engine.Engine.freeze_gc">freeze_gc</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.generate" href="#sglang.srt.entrypoints.engine.Engine.generate">generate</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.get_server_info" href="#sglang.srt.entrypoints.engine.Engine.get_server_info">get_server_info</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.get_weights_by_name" href="#sglang.srt.entrypoints.engine.Engine.get_weights_by_name">get_weights_by_name</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.init_weights_update_group" href="#sglang.srt.entrypoints.engine.Engine.init_weights_update_group">init_weights_update_group</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.rerank" href="#sglang.srt.entrypoints.engine.Engine.rerank">rerank</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.save_remote_model" href="#sglang.srt.entrypoints.engine.Engine.save_remote_model">save_remote_model</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.save_sharded_model" href="#sglang.srt.entrypoints.engine.Engine.save_sharded_model">save_sharded_model</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.score" href="#sglang.srt.entrypoints.engine.Engine.score">score</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.shutdown" href="#sglang.srt.entrypoints.engine.Engine.shutdown">shutdown</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.start_expert_distribution_record" href="#sglang.srt.entrypoints.engine.Engine.start_expert_distribution_record">start_expert_distribution_record</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.start_profile" href="#sglang.srt.entrypoints.engine.Engine.start_profile">start_profile</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.stop_expert_distribution_record" href="#sglang.srt.entrypoints.engine.Engine.stop_expert_distribution_record">stop_expert_distribution_record</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.stop_profile" href="#sglang.srt.entrypoints.engine.Engine.stop_profile">stop_profile</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.update_weights_from_disk" href="#sglang.srt.entrypoints.engine.Engine.update_weights_from_disk">update_weights_from_disk</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.update_weights_from_distributed" href="#sglang.srt.entrypoints.engine.Engine.update_weights_from_distributed">update_weights_from_distributed</a></code></li>
<li><code><a title="sglang.srt.entrypoints.engine.Engine.update_weights_from_tensor" href="#sglang.srt.entrypoints.engine.Engine.update_weights_from_tensor">update_weights_from_tensor</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
