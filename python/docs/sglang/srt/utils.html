<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.utils API documentation</title>
<meta name="description" content="Common utilities.">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.utils</code></h1>
</header>
<section id="section-intro">
<p>Common utilities.</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.utils.add_api_key_middleware"><code class="name flex">
<span>def <span class="ident">add_api_key_middleware</span></span>(<span>app, api_key: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_api_key_middleware(app, api_key: str):
    @app.middleware(&#34;http&#34;)
    async def authentication(request, call_next):
        if request.method == &#34;OPTIONS&#34;:
            return await call_next(request)
        if request.url.path.startswith(&#34;/health&#34;):
            return await call_next(request)
        if request.url.path.startswith(&#34;/metrics&#34;):
            return await call_next(request)
        if request.headers.get(&#34;Authorization&#34;) != &#34;Bearer &#34; + api_key:
            return ORJSONResponse(content={&#34;error&#34;: &#34;Unauthorized&#34;}, status_code=401)
        return await call_next(request)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.add_prefix"><code class="name flex">
<span>def <span class="ident">add_prefix</span></span>(<span>name: str, prefix: str) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_prefix(name: str, prefix: str) -&gt; str:
    &#34;&#34;&#34;Add a weight path prefix to a module name.

    Args:
        name: base module name.
        prefix: weight prefix str to added to the front of `name` concatenated with `.`.

    Returns:
        The string `prefix.name` if prefix is non-empty, otherwise just `name`.
    &#34;&#34;&#34;
    return name if not prefix else f&#34;{prefix}.{name}&#34;</code></pre>
</details>
<div class="desc"><p>Add a weight path prefix to a module name.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>base module name.</dd>
<dt><strong><code>prefix</code></strong></dt>
<dd>weight prefix str to added to the front of <code>name</code> concatenated with <code>.</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The string <code>prefix.name</code> if prefix is non-empty, otherwise just <code>name</code>.</p></div>
</dd>
<dt id="sglang.srt.utils.add_prometheus_middleware"><code class="name flex">
<span>def <span class="ident">add_prometheus_middleware</span></span>(<span>app)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_prometheus_middleware(app):
    # We need to import prometheus_client after setting the env variable `PROMETHEUS_MULTIPROC_DIR`
    from prometheus_client import CollectorRegistry, make_asgi_app, multiprocess

    registry = CollectorRegistry()
    multiprocess.MultiProcessCollector(registry)
    metrics_route = Mount(&#34;/metrics&#34;, make_asgi_app(registry=registry))

    # Workaround for 307 Redirect for /metrics
    metrics_route.path_regex = re.compile(&#34;^/metrics(?P&lt;path&gt;.*)$&#34;)
    app.routes.append(metrics_route)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.align"><code class="name flex">
<span>def <span class="ident">align</span></span>(<span>x: int, y: int) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def align(x: int, y: int) -&gt; int:
    return ceil_div(x, y) * y</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.apply_module_patch"><code class="name flex">
<span>def <span class="ident">apply_module_patch</span></span>(<span>target_module, target_function, wrappers)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_module_patch(target_module, target_function, wrappers):
    original_module, original_function = parse_module_path(
        target_module, target_function, False
    )

    original_function_id = id(original_function)

    candidate = original_function
    for wrapper in wrappers:
        candidate = wrapper(candidate)
    if target_function is not None:
        setattr(original_module, target_function, candidate)

    for key, value in sys.modules.copy().items():
        if (
            target_function is not None
            and hasattr(value, target_function)
            and id(getattr(value, target_function)) == original_function_id
        ):
            setattr(value, target_function, candidate)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.assert_pkg_version"><code class="name flex">
<span>def <span class="ident">assert_pkg_version</span></span>(<span>pkg: str, min_version: str, message: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def assert_pkg_version(pkg: str, min_version: str, message: str):
    try:
        installed_version = version(pkg)
        if pkg_version.parse(installed_version) &lt; pkg_version.parse(min_version):
            raise Exception(
                f&#34;{pkg} is installed with version {installed_version}, which &#34;
                f&#34;is less than the minimum required version {min_version}. &#34; + message
            )
    except PackageNotFoundError:
        raise Exception(
            f&#34;{pkg} with minimum required version {min_version} is not installed. &#34;
            + message
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.bind_or_assign"><code class="name flex">
<span>def <span class="ident">bind_or_assign</span></span>(<span>target, source)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def bind_or_assign(target, source):
    if target is not None:
        target.copy_(source)
        return target
    else:
        return source</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.bind_port"><code class="name flex">
<span>def <span class="ident">bind_port</span></span>(<span>port)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def bind_port(port):
    &#34;&#34;&#34;Bind to a specific port, assuming it&#39;s available.&#34;&#34;&#34;
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)  # Allows address reuse
    sock.bind((&#34;&#34;, port))
    sock.listen(1)
    return sock</code></pre>
</details>
<div class="desc"><p>Bind to a specific port, assuming it's available.</p></div>
</dd>
<dt id="sglang.srt.utils.broadcast_pyobj"><code class="name flex">
<span>def <span class="ident">broadcast_pyobj</span></span>(<span>data: List[Any],<br>rank: int,<br>dist_group: Optional[torch.distributed.ProcessGroup] = None,<br>src: int = 0,<br>force_cpu_device: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def broadcast_pyobj(
    data: List[Any],
    rank: int,
    dist_group: Optional[torch.distributed.ProcessGroup] = None,
    src: int = 0,
    force_cpu_device: bool = True,
):
    &#34;&#34;&#34;Broadcast inputs from src rank to all other ranks with torch.dist backend.
    The `rank` here refer to the source rank on global process group (regardless
    of dist_group argument).
    &#34;&#34;&#34;
    device = torch.device(
        &#34;cuda&#34; if torch.cuda.is_available() and not force_cpu_device else &#34;cpu&#34;
    )

    if rank == src:
        if len(data) == 0:
            tensor_size = torch.tensor([0], dtype=torch.long, device=device)
            dist.broadcast(tensor_size, src=src, group=dist_group)
        else:
            serialized_data = pickle.dumps(data)
            size = len(serialized_data)

            tensor_data = torch.ByteTensor(
                np.frombuffer(serialized_data, dtype=np.uint8)
            ).to(device)
            tensor_size = torch.tensor([size], dtype=torch.long, device=device)

            dist.broadcast(tensor_size, src=src, group=dist_group)
            dist.broadcast(tensor_data, src=src, group=dist_group)
        return data
    else:
        tensor_size = torch.tensor([0], dtype=torch.long, device=device)
        dist.broadcast(tensor_size, src=src, group=dist_group)
        size = tensor_size.item()

        if size == 0:
            return []

        tensor_data = torch.empty(size, dtype=torch.uint8, device=device)
        dist.broadcast(tensor_data, src=src, group=dist_group)

        serialized_data = bytes(tensor_data.cpu().numpy())
        data = pickle.loads(serialized_data)
        return data</code></pre>
</details>
<div class="desc"><p>Broadcast inputs from src rank to all other ranks with torch.dist backend.
The <code>rank</code> here refer to the source rank on global process group (regardless
of dist_group argument).</p></div>
</dd>
<dt id="sglang.srt.utils.calculate_time"><code class="name flex">
<span>def <span class="ident">calculate_time</span></span>(<span>show=False, min_cost_ms=0.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_time(show=False, min_cost_ms=0.0):
    def wrapper(func):
        def inner_func(*args, **kwargs):
            torch.cuda.synchronize()
            if show:
                start_time = time.perf_counter()
            result = func(*args, **kwargs)
            torch.cuda.synchronize()
            if show:
                cost_time = (time.perf_counter() - start_time) * 1000
                if cost_time &gt; min_cost_ms:
                    print(f&#34;Function {func.__name__} took {cost_time} ms to run.&#34;)
            return result

        return inner_func

    return wrapper</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.ceil_div"><code class="name flex">
<span>def <span class="ident">ceil_div</span></span>(<span>x: int, y: int) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ceil_div(x: int, y: int) -&gt; int:
    return (x + y - 1) // y</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.check_cuda_result"><code class="name flex">
<span>def <span class="ident">check_cuda_result</span></span>(<span>raw_output)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_cuda_result(raw_output):
    import cuda.bindings.runtime as cuda_rt

    err, *results = raw_output
    if err != cuda_rt.cudaError_t.cudaSuccess:
        raise Exception(f&#34;CUDA error: {err}&#34;)

    return results</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.configure_gc_logger"><code class="name flex">
<span>def <span class="ident">configure_gc_logger</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_gc_logger():
    logger.info(&#34;Enable GC Logger&#34;)

    import gc

    gc_start_time = {}

    def gc_callback(phase, info):
        gen = info.get(&#34;generation&#34;, &#34;?&#34;)
        if phase == &#34;start&#34;:
            gc_start_time[gen] = time.time()
            logger.info(f&#34;GC start: Time {time.time()} | Generation {gen}&#34;)
        elif phase == &#34;stop&#34;:
            duration = time.time() - gc_start_time.get(gen, time.time())
            collected = info.get(&#34;collected&#34;, &#34;?&#34;)
            uncollectable = info.get(&#34;uncollectable&#34;, &#34;?&#34;)
            logger.info(
                f&#34;GC end: Time {time.time()} | Generation {gen} | &#34;
                f&#34;Duration: {duration:.4f}s | Collected: {collected} | Uncollectable: {uncollectable} &#34;
                f&#39;{&#34;(LONG GC)&#34; if duration &gt; 0.1 else &#34;&#34;}&#39;
            )

    gc.callbacks.append(gc_callback)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.configure_gc_warning"><code class="name flex">
<span>def <span class="ident">configure_gc_warning</span></span>(<span>warn_threshold_secs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_gc_warning(warn_threshold_secs):
    import gc

    gc_start_time = {}

    def gc_callback(phase, info):
        gen = info.get(&#34;generation&#34;, &#34;?&#34;)
        if phase == &#34;start&#34;:
            gc_start_time[gen] = time.time()
        elif phase == &#34;stop&#34;:
            duration = time.time() - gc_start_time.get(gen, time.time())
            if duration &gt; warn_threshold_secs:
                g0, g1, g2 = gc_object_counts()
                logger.warn(
                    f&#34;LONG GARBAGE COLLECTION DETECTED | Generation {gen} | Duration: {duration:.4f}s | # Objects: gen0={g0}, gen1={g1}, gen2={g2} | &#34;
                    f&#34;This may cause latency jitter. Consider calling the freeze_gc API after sending a few warmup requests.&#34;
                )

    gc.callbacks.append(gc_callback)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.configure_ipv6"><code class="name flex">
<span>def <span class="ident">configure_ipv6</span></span>(<span>dist_init_addr)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_ipv6(dist_init_addr):
    addr = dist_init_addr
    end = addr.find(&#34;]&#34;)
    if end == -1:
        raise ValueError(&#34;invalid IPv6 address format: missing &#39;]&#39;&#34;)

    host = addr[: end + 1]

    # this only validates the address without brackets: we still need the below checks.
    # if it&#39;s invalid, immediately raise an error so we know it&#39;s not formatting issues.
    if not is_valid_ipv6_address(host[1:end]):
        raise ValueError(f&#34;invalid IPv6 address: {host}&#34;)

    port_str = None
    if len(addr) &gt; end + 1:
        if addr[end + 1] == &#34;:&#34;:
            port_str = addr[end + 2 :]
        else:
            raise ValueError(&#34;received IPv6 address format: expected &#39;:&#39; after &#39;]&#39;&#34;)

    if not port_str:
        raise ValueError(
            &#34;a port must be specified in IPv6 address (format: [ipv6]:port)&#34;
        )

    try:
        port = int(port_str)
    except ValueError:
        raise ValueError(f&#34;invalid port in IPv6 address: &#39;{port_str}&#39;&#34;)
    return port, host</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.configure_logger"><code class="name flex">
<span>def <span class="ident">configure_logger</span></span>(<span>server_args, prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_logger(server_args, prefix: str = &#34;&#34;):
    if SGLANG_LOGGING_CONFIG_PATH := os.getenv(&#34;SGLANG_LOGGING_CONFIG_PATH&#34;):
        if not os.path.exists(SGLANG_LOGGING_CONFIG_PATH):
            raise Exception(
                &#34;Setting SGLANG_LOGGING_CONFIG_PATH from env with &#34;
                f&#34;{SGLANG_LOGGING_CONFIG_PATH} but it does not exist!&#34;
            )
        with open(SGLANG_LOGGING_CONFIG_PATH, encoding=&#34;utf-8&#34;) as file:
            custom_config = json.loads(file.read())
        logging.config.dictConfig(custom_config)
        return
    format = f&#34;[%(asctime)s{prefix}] %(message)s&#34;
    # format = f&#34;[%(asctime)s.%(msecs)03d{prefix}] %(message)s&#34;
    logging.basicConfig(
        level=getattr(logging, server_args.log_level.upper()),
        format=format,
        datefmt=&#34;%Y-%m-%d %H:%M:%S&#34;,
        force=True,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.cpu_has_amx_support"><code class="name flex">
<span>def <span class="ident">cpu_has_amx_support</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cpu_has_amx_support():
    return torch._C._cpu._is_amx_tile_supported() and is_intel_amx_backend_available</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.crash_on_warnings"><code class="name flex">
<span>def <span class="ident">crash_on_warnings</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def crash_on_warnings():
    # Crash on warning if we are running CI tests
    return get_bool_env_var(&#34;SGLANG_IS_IN_CI&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.create_checksum"><code class="name flex">
<span>def <span class="ident">create_checksum</span></span>(<span>directory: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_checksum(directory: str):
    raise NotImplementedError()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.dataclass_to_string_truncated"><code class="name flex">
<span>def <span class="ident">dataclass_to_string_truncated</span></span>(<span>data, max_length=2048, skip_names: Optional[Set[str]] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dataclass_to_string_truncated(
    data, max_length=2048, skip_names: Optional[Set[str]] = None
):
    if skip_names is None:
        skip_names = set()
    if isinstance(data, str):
        if len(data) &gt; max_length:
            half_length = max_length // 2
            return f&#34;{repr(data[:half_length])} ... {repr(data[-half_length:])}&#34;
        else:
            return f&#34;{repr(data)}&#34;
    elif isinstance(data, (list, tuple)):
        if len(data) &gt; max_length:
            half_length = max_length // 2
            return str(data[:half_length]) + &#34; ... &#34; + str(data[-half_length:])
        else:
            return str(data)
    elif isinstance(data, dict):
        return (
            &#34;{&#34;
            + &#34;, &#34;.join(
                f&#34;&#39;{k}&#39;: {dataclass_to_string_truncated(v, max_length)}&#34;
                for k, v in data.items()
                if k not in skip_names
            )
            + &#34;}&#34;
        )
    elif dataclasses.is_dataclass(data):
        fields = dataclasses.fields(data)
        return (
            f&#34;{data.__class__.__name__}(&#34;
            + &#34;, &#34;.join(
                f&#34;{f.name}={dataclass_to_string_truncated(getattr(data, f.name), max_length)}&#34;
                for f in fields
                if f.name not in skip_names
            )
            + &#34;)&#34;
        )
    else:
        return str(data)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.debug_timing"><code class="name flex">
<span>def <span class="ident">debug_timing</span></span>(<span>func)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def debug_timing(func):
    # todo: replace with a more organized instrumentation
    def wrapper(*args, **kwargs):
        if logger.isEnabledFor(logging.DEBUG):
            tic = torch.cuda.Event(enable_timing=True)
            toc = torch.cuda.Event(enable_timing=True)
            tic.record()
            result = func(*args, **kwargs)
            toc.record()
            toc.synchronize()  # Wait for the function to complete without synchronizing all ops on the GPU
            elapsed = tic.elapsed_time(toc)
            indices = kwargs.get(&#34;indices&#34;, args[1] if len(args) &gt; 1 else None)
            num_tokens = len(indices) if indices is not None else 0
            throughput = num_tokens / elapsed * 1000 if elapsed &gt; 0 else 0
            logger.debug(
                f&#34;Transfer time: {elapsed} ms, throughput: {throughput} tokens/s&#34;
            )
            return result
        else:
            return func(*args, **kwargs)

    return wrapper</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.decode_video_base64"><code class="name flex">
<span>def <span class="ident">decode_video_base64</span></span>(<span>video_base64)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decode_video_base64(video_base64):
    from PIL import Image

    # Decode the base64 string
    video_bytes = pybase64.b64decode(video_base64, validate=True)

    # Placeholder for the start indices of each PNG image
    img_starts = []

    frame_format = &#34;PNG&#34;  # str(os.getenv(&#39;FRAME_FORMAT&#39;, &#34;JPEG&#34;))

    assert frame_format in [
        &#34;PNG&#34;,
        &#34;JPEG&#34;,
    ], &#34;FRAME_FORMAT must be either &#39;PNG&#39; or &#39;JPEG&#39;&#34;

    if frame_format == &#34;PNG&#34;:
        # Find each PNG start signature to isolate images
        i = 0
        while i &lt; len(video_bytes) - 7:  # Adjusted for the length of the PNG signature
            # Check if we found the start of a PNG file
            if (
                video_bytes[i] == 0x89
                and video_bytes[i + 1] == 0x50
                and video_bytes[i + 2] == 0x4E
                and video_bytes[i + 3] == 0x47
                and video_bytes[i + 4] == 0x0D
                and video_bytes[i + 5] == 0x0A
                and video_bytes[i + 6] == 0x1A
                and video_bytes[i + 7] == 0x0A
            ):
                img_starts.append(i)
                i += 8  # Skip the PNG signature
            else:
                i += 1
    else:
        # Find each JPEG start (0xFFD8) to isolate images
        i = 0
        while (
            i &lt; len(video_bytes) - 1
        ):  # Adjusted for the length of the JPEG SOI signature
            # Check if we found the start of a JPEG file
            if video_bytes[i] == 0xFF and video_bytes[i + 1] == 0xD8:
                img_starts.append(i)
                # Move to the next byte to continue searching for the next image start
                i += 2
            else:
                i += 1

    frames = []
    for start_idx in img_starts:
        # Assuming each image is back-to-back, the end of one image is the start of another
        # The last image goes until the end of the byte string
        end_idx = (
            img_starts[img_starts.index(start_idx) + 1]
            if img_starts.index(start_idx) + 1 &lt; len(img_starts)
            else len(video_bytes)
        )
        img_bytes = video_bytes[start_idx:end_idx]

        # Convert bytes to a PIL Image
        img = Image.open(BytesIO(img_bytes))

        # Convert PIL Image to a NumPy array
        frame = np.array(img)

        # Append the frame to the list of frames
        frames.append(frame)

    # Ensure there&#39;s at least one frame to avoid errors with np.stack
    if frames:
        return np.stack(frames, axis=0), img.size
    else:
        return np.array([]), (
            0,
            0,
        )  # Return an empty array and size tuple if no frames were found</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.delete_directory"><code class="name flex">
<span>def <span class="ident">delete_directory</span></span>(<span>dirpath)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_directory(dirpath):
    try:
        # This will remove the directory and all its contents
        shutil.rmtree(dirpath)
    except OSError as e:
        print(f&#34;Warning: {dirpath} : {e.strerror}&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.dim_is_supported"><code class="name flex">
<span>def <span class="ident">dim_is_supported</span></span>(<span>weight)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dim_is_supported(weight):
    return weight.size(0) % 16 == 0 and weight.size(1) % 32 == 0</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.direct_register_custom_op"><code class="name flex">
<span>def <span class="ident">direct_register_custom_op</span></span>(<span>op_name: str,<br>op_func: Callable,<br>mutates_args: List[str],<br>fake_impl: Optional[Callable] = None,<br>target_lib: Optional[Library] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def direct_register_custom_op(
    op_name: str,
    op_func: Callable,
    mutates_args: List[str],
    fake_impl: Optional[Callable] = None,
    target_lib: Optional[Library] = None,
):
    &#34;&#34;&#34;
    `torch.library.custom_op` can have significant overhead because it
    needs to consider complicated dispatching logic. This function
    directly registers a custom op and dispatches it to the CUDA backend.
    See https://gist.github.com/youkaichao/ecbea9ec9fc79a45d2adce1784d7a9a5
    for more details.

    By default, the custom op is registered to the vLLM library. If you
    want to register it to a different library, you can pass the library
    object to the `target_lib` argument.

    IMPORTANT: the lifetime of the operator is tied to the lifetime of the
    library object. If you want to bind the operator to a different library,
    make sure the library object is alive when the operator is used.

    Note: This function will silently skip registration if the operator
    with the same name is already registered to avoid RuntimeError in
    multi-engine scenarios (e.g., VERL framework).
    &#34;&#34;&#34;
    import torch.library

    my_lib = target_lib or sglang_lib

    # Check if operator is already registered to avoid duplicate registration
    # This is important for scenarios where multiple SGLang engines run in the same process
    try:
        # Try to access the operator to see if it&#39;s already registered
        lib_name = my_lib.m.name if hasattr(my_lib.m, &#34;name&#34;) else &#34;sglang&#34;
        if hasattr(torch.ops, lib_name) and hasattr(
            getattr(torch.ops, lib_name), op_name
        ):
            # Operator already exists, skip registration
            return
    except (AttributeError, RuntimeError):
        # Operator doesn&#39;t exist, proceed with registration
        pass

    if hasattr(torch.library, &#34;infer_schema&#34;):
        schema_str = torch.library.infer_schema(op_func, mutates_args=mutates_args)
    else:
        # for pytorch 2.4
        import torch._custom_op.impl

        schema_str = torch._custom_op.impl.infer_schema(op_func, mutates_args)

    try:
        my_lib.define(op_name + schema_str)
        my_lib.impl(op_name, op_func, &#34;CUDA&#34;)
        if fake_impl is not None:
            my_lib._register_fake(op_name, fake_impl)
    except RuntimeError as error:
        if &#34;Tried to register an operator&#34; in str(e) and &#34;multiple times&#34; in str(e):
            # Silently ignore duplicate registration errors
            # This can happen in multi-engine scenarios
            pass
        else:
            # Re-raise other RuntimeErrors
            raise error
    except AttributeError as error:
        # Always re-raise AttributeError as it indicates missing dependencies
        raise error</code></pre>
</details>
<div class="desc"><p><code>torch.library.custom_op</code> can have significant overhead because it
needs to consider complicated dispatching logic. This function
directly registers a custom op and dispatches it to the CUDA backend.
See <a href="https://gist.github.com/youkaichao/ecbea9ec9fc79a45d2adce1784d7a9a5">https://gist.github.com/youkaichao/ecbea9ec9fc79a45d2adce1784d7a9a5</a>
for more details.</p>
<p>By default, the custom op is registered to the vLLM library. If you
want to register it to a different library, you can pass the library
object to the <code>target_lib</code> argument.</p>
<p>IMPORTANT: the lifetime of the operator is tied to the lifetime of the
library object. If you want to bind the operator to a different library,
make sure the library object is alive when the operator is used.</p>
<p>Note: This function will silently skip registration if the operator
with the same name is already registered to avoid RuntimeError in
multi-engine scenarios (e.g., VERL framework).</p></div>
</dd>
<dt id="sglang.srt.utils.disable_request_logging"><code class="name flex">
<span>def <span class="ident">disable_request_logging</span></span>(<span>) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@lru_cache(maxsize=2)
def disable_request_logging() -&gt; bool:
    return get_bool_env_var(&#34;SGLANG_DISABLE_REQUEST_LOGGING&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.dispose_tensor"><code class="name flex">
<span>def <span class="ident">dispose_tensor</span></span>(<span>x: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dispose_tensor(x: torch.Tensor):
    x.set_(torch.empty((0,), device=x.device, dtype=x.dtype))</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.dump_to_file"><code class="name flex">
<span>def <span class="ident">dump_to_file</span></span>(<span>dirpath, name, value)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump_to_file(dirpath, name, value):
    from sglang.srt.distributed import get_tensor_model_parallel_rank

    if get_tensor_model_parallel_rank() != 0:
        return

    os.makedirs(dirpath, exist_ok=True)
    if value.dtype is torch.bfloat16:
        value = value.float()
    value = value.cpu().numpy()
    output_filename = os.path.join(dirpath, f&#34;pytorch_dump_{name}.npy&#34;)
    logger.info(f&#34;Dump a tensor to {output_filename}. Shape = {value.shape}&#34;)
    np.save(output_filename, value)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.dynamic_import"><code class="name flex">
<span>def <span class="ident">dynamic_import</span></span>(<span>func_path: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dynamic_import(func_path: str):
    parts = func_path.split(&#34;.&#34;)
    if len(parts) &lt; 2:
        raise ValueError(
            &#34;func_path should contain both module name and func name (such as &#39;module.func&#39;)&#34;
        )
    module_path = &#34;.&#34;.join(parts[:-1])
    func_name = parts[-1]
    module = importlib.import_module(module_path)
    func = getattr(module, func_name)
    return func</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.empty_context"><code class="name flex">
<span>def <span class="ident">empty_context</span></span>(<span>*args, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def empty_context(*args, **kwargs):
    return EmptyContextManager()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.enable_show_time_cost"><code class="name flex">
<span>def <span class="ident">enable_show_time_cost</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def enable_show_time_cost():
    global show_time_cost
    show_time_cost = True</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.fast_topk"><code class="name flex">
<span>def <span class="ident">fast_topk</span></span>(<span>values, topk, dim)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fast_topk(values, topk, dim):
    if topk == 1:
        # Use max along the specified dimension to get both value and index
        return torch.max(values, dim=dim, keepdim=True)
    else:
        # Use topk for efficiency with larger k values
        return torch.topk(values, topk, dim=dim)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.find_local_repo_dir"><code class="name flex">
<span>def <span class="ident">find_local_repo_dir</span></span>(<span>repo_id: str, revision: Optional[str] = None) ‑> str | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_local_repo_dir(repo_id: str, revision: Optional[str] = None) -&gt; Optional[str]:
    import huggingface_hub as hf

    # Build cache path
    cache_path = os.path.join(
        hf.constants.HF_HUB_CACHE,
        hf.constants.REPO_ID_SEPARATOR.join([&#34;models&#34;, *repo_id.split(&#34;/&#34;)]),
    )

    # Get revision from main ref if not specified
    if not revision:
        ref_path = os.path.join(cache_path, &#34;refs&#34;, &#34;main&#34;)
        if os.path.isfile(ref_path):
            with open(ref_path) as f:
                revision = f.read().strip()

    # List files from revision directory
    if revision:
        rev_dir = os.path.join(cache_path, &#34;snapshots&#34;, revision)
        if os.path.isdir(rev_dir):
            return rev_dir

    return None</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.find_process_using_port"><code class="name flex">
<span>def <span class="ident">find_process_using_port</span></span>(<span>port: int) ‑> psutil.Process | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_process_using_port(port: int) -&gt; Optional[psutil.Process]:
    for conn in psutil.net_connections(kind=&#34;inet&#34;):
        if conn.laddr.port == port:
            try:
                return psutil.Process(conn.pid)
            except psutil.NoSuchProcess:
                # It could happen by race condition (the proc dies when psutil.Process is called).
                pass

    return None</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.flatten_nested_list"><code class="name flex">
<span>def <span class="ident">flatten_nested_list</span></span>(<span>nested_list)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flatten_nested_list(nested_list):
    if isinstance(nested_list, list):
        return [
            item for sublist in nested_list for item in flatten_nested_list(sublist)
        ]
    else:
        return [nested_list]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.format_tcp_address"><code class="name flex">
<span>def <span class="ident">format_tcp_address</span></span>(<span>ip: str, port: int) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def format_tcp_address(ip: str, port: int) -&gt; str:
    return f&#34;tcp://{maybe_wrap_ipv6_address(ip)}:{port}&#34;</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.freeze_gc"><code class="name flex">
<span>def <span class="ident">freeze_gc</span></span>(<span>context: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def freeze_gc(context: str):
    import gc

    g0_before, g1_before, g2_before = gc_object_counts()
    gc.freeze()
    g0_after, g1_after, g2_after = gc_object_counts()
    logger.info(
        f&#34;Freezing GC in {context} process. &#34;
        f&#34;gen0: {g0_before}-&gt;{g0_after}, &#34;
        f&#34;gen1: {g1_before}-&gt;{g1_after}, &#34;
        f&#34;gen2: {g2_before}-&gt;{g2_after}&#34;
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.gc_object_counts"><code class="name flex">
<span>def <span class="ident">gc_object_counts</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gc_object_counts():
    import gc

    g0 = len(gc.get_objects(0))
    g1 = len(gc.get_objects(1))
    g2 = len(gc.get_objects(2))
    return g0, g1, g2</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_amdgpu_memory_capacity"><code class="name flex">
<span>def <span class="ident">get_amdgpu_memory_capacity</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_amdgpu_memory_capacity():
    try:
        # Run rocm-smi and capture the output
        result = subprocess.run(
            [
                &#34;rocminfo | grep &#39;gfx&#39; -A 100 | grep &#39;Pool 1&#39; -A 5 | grep &#39;Size:&#39; | awk &#39;{print $2}&#39;&#34;
            ],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            shell=True,
            text=True,
        )
        if result.returncode != 0:
            raise RuntimeError(f&#34;rocm-smi error: {result.stderr.strip()}&#34;)

        # Parse the output to extract memory values in MiB
        memory_values = [
            float(mem.split(&#34;(&#34;)[0].strip()) / 1024
            for mem in result.stdout.strip().split(&#34;\n&#34;)
        ]

        if not memory_values:
            raise ValueError(&#34;No GPU memory values found.&#34;)

        # Return the minimum memory value
        return min(memory_values)

    except FileNotFoundError:
        raise RuntimeError(
            &#34;rocm-smi not found. Ensure AMD ROCm drivers are installed and accessible.&#34;
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_available_gpu_memory"><code class="name flex">
<span>def <span class="ident">get_available_gpu_memory</span></span>(<span>device, gpu_id, distributed=False, empty_cache=True, cpu_group=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_available_gpu_memory(
    device, gpu_id, distributed=False, empty_cache=True, cpu_group=None
):
    &#34;&#34;&#34;
    Get available memory for cuda:gpu_id device.
    When distributed is True, the available memory is the minimum available memory of all GPUs.
    &#34;&#34;&#34;
    if device == &#34;cuda&#34;:
        num_gpus = torch.cuda.device_count()
        assert gpu_id &lt; num_gpus

        if torch.cuda.current_device() != gpu_id:
            print(
                f&#34;WARNING: current device is not {gpu_id}, but {torch.cuda.current_device()}, &#34;,
                &#34;which may cause useless memory allocation for torch CUDA context.&#34;,
            )

        if empty_cache:
            torch.cuda.empty_cache()
        free_gpu_memory, _ = torch.cuda.mem_get_info(gpu_id)

    elif device == &#34;xpu&#34;:
        num_gpus = torch.xpu.device_count()
        assert gpu_id &lt; num_gpus

        if torch.xpu.current_device() != gpu_id:
            print(
                f&#34;WARNING: current device is not {gpu_id}, but {torch.xpu.current_device()}, &#34;,
                &#34;which may cause useless memory allocation for torch XPU context.&#34;,
            )

        if empty_cache:
            torch.xpu.empty_cache()
        used_memory = torch.xpu.memory_allocated()
        total_gpu_memory = torch.xpu.get_device_properties(gpu_id).total_memory
        free_gpu_memory = total_gpu_memory - used_memory

    elif device == &#34;hpu&#34;:
        num_gpus = torch.hpu.device_count()
        assert gpu_id &lt; num_gpus

        if torch.hpu.current_device() != gpu_id:
            print(
                f&#34;WARNING: current device is not {gpu_id}, but {torch.hpu.current_device()}, &#34;,
                &#34;which may cause useless memory allocation for torch HPU context.&#34;,
            )

        free_gpu_memory, total_gpu_memory = torch.hpu.mem_get_info()

    elif device == &#34;cpu&#34;:
        # TODO: rename the variables in the current function to be not GPU specific
        free_gpu_memory = psutil.virtual_memory().available
    elif device == &#34;npu&#34;:
        num_gpus = torch.npu.device_count()
        assert gpu_id &lt; num_gpus

        if torch.npu.current_device() != gpu_id:
            print(
                f&#34;WARNING: current device is not {gpu_id}, but {torch.npu.current_device()}, &#34;,
                &#34;which may cause useless memory allocation for torch NPU context.&#34;,
            )
        free_gpu_memory, total_gpu_memory = torch.npu.mem_get_info()

    if distributed:
        tensor = torch.tensor(free_gpu_memory, dtype=torch.float32)
        torch.distributed.all_reduce(
            tensor, op=torch.distributed.ReduceOp.MIN, group=cpu_group
        )
        free_gpu_memory = tensor.item()

    return free_gpu_memory / (1 &lt;&lt; 30)</code></pre>
</details>
<div class="desc"><p>Get available memory for cuda:gpu_id device.
When distributed is True, the available memory is the minimum available memory of all GPUs.</p></div>
</dd>
<dt id="sglang.srt.utils.get_bool_env_var"><code class="name flex">
<span>def <span class="ident">get_bool_env_var</span></span>(<span>name: str, default: str = 'false') ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_bool_env_var(name: str, default: str = &#34;false&#34;) -&gt; bool:
    value = os.getenv(name, default)
    value = value.lower()

    truthy_values = (&#34;true&#34;, &#34;1&#34;)
    falsy_values = (&#34;false&#34;, &#34;0&#34;)

    if (value not in truthy_values) and (value not in falsy_values):
        if value not in _warned_bool_env_var_keys:
            logger.warning(
                f&#34;get_bool_env_var({name}) see non-understandable value={value} and treat as false&#34;
            )
        _warned_bool_env_var_keys.add(value)

    return value in truthy_values</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_compiler_backend"><code class="name flex">
<span>def <span class="ident">get_compiler_backend</span></span>(<span>) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_compiler_backend() -&gt; str:
    if hasattr(torch, &#34;hpu&#34;) and torch.hpu.is_available():
        return &#34;hpu_backend&#34;

    if hasattr(torch, &#34;npu&#34;) and torch.npu.is_available():
        try:
            import torchair
            import torchair.ge_concrete_graph.ge_converter.experimental.patch_for_hcom_allreduce
            from torchair.configs.compiler_config import CompilerConfig
        except ImportError as e:
            raise ImportError(
                &#34;NPU detected, but torchair package is not installed. &#34;
                &#34;Please install torchair for torch.compile support on NPU.&#34;
            )
        compiler_config = CompilerConfig()
        predefined_config = get_npu_compiler_config()
        for k, v in predefined_config.items():
            setattr(compiler_config.experimental_config, k, v)

        npu_backend = torchair.get_npu_backend(compiler_config=compiler_config)
        return npu_backend

    return &#34;inductor&#34;</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_cpu_ids_by_node"><code class="name flex">
<span>def <span class="ident">get_cpu_ids_by_node</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_cpu_ids_by_node():
    node_to_cpus = get_physical_cpus_by_numa()
    # Sort by NUMA node index
    cpu_ids = [
        &#34;,&#34;.join(map(str, sorted(node_to_cpus[node]))) for node in sorted(node_to_cpus)
    ]

    # [&#39;0,1,2,3&#39;, &#39;4,5,6,7&#39;, &#39;8,9,10,11&#39;, &#39;12,13,14,15&#39;, &#39;16,17,18,19&#39;, &#39;20,21,22,23&#39;]
    return cpu_ids</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_cuda_version"><code class="name flex">
<span>def <span class="ident">get_cuda_version</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_cuda_version():
    if torch.version.cuda:
        return tuple(map(int, torch.version.cuda.split(&#34;.&#34;)))
    return (0, 0)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_device"><code class="name flex">
<span>def <span class="ident">get_device</span></span>(<span>device_id: Optional[int] = None) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@lru_cache(maxsize=8)
def get_device(device_id: Optional[int] = None) -&gt; str:
    if is_cpu():
        if cpu_has_amx_support():
            logger.info(&#34;Intel AMX is detected, using CPU with Intel AMX support.&#34;)
        else:
            logger.warning(
                &#34;CPU device enabled, using torch native backend, low performance expected.&#34;
            )
        return &#34;cpu&#34;

    if hasattr(torch, &#34;cuda&#34;) and torch.cuda.is_available():
        if device_id is None:
            return &#34;cuda&#34;
        return &#34;cuda:{}&#34;.format(device_id)

    if hasattr(torch, &#34;xpu&#34;) and torch.xpu.is_available():
        if device_id == None:
            return &#34;xpu&#34;
        return &#34;xpu:{}&#34;.format(device_id)

    if hasattr(torch, &#34;npu&#34;) and torch.npu.is_available():
        if device_id == None:
            return &#34;npu&#34;
        return &#34;npu:{}&#34;.format(device_id)

    if is_habana_available():
        try:
            import habana_frameworks.torch.hpu

            if torch.hpu.is_available():
                if device_id == None:
                    return &#34;hpu&#34;
                return &#34;hpu:{}&#34;.format(device_id)
        except ImportError as e:
            raise ImportError(
                &#34;Habana frameworks detected, but failed to import &#39;habana_frameworks.torch.hpu&#39;.&#34;
            )

    raise RuntimeError(&#34;No accelerator (CUDA, XPU, HPU) is available.&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_device_capability"><code class="name flex">
<span>def <span class="ident">get_device_capability</span></span>(<span>device_id: int = 0) ‑> Tuple[int, int]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_device_capability(device_id: int = 0) -&gt; Tuple[int, int]:
    major, minor = None, None
    if hasattr(torch, &#34;cuda&#34;) and torch.cuda.is_available():
        major, minor = torch.cuda.get_device_capability(device_id)

    if hasattr(torch, &#34;xpu&#34;) and torch.xpu.is_available():
        major, minor, *_ = torch.xpu.get_device_capability(device_id)[&#34;version&#34;].split(
            &#34;.&#34;
        )
        major, minor = int(major), int(minor)

    if hasattr(torch, &#34;hpu&#34;) and torch.hpu.is_available():
        try:
            # TODO(HandH1998): `get_device_capability` is not supported by `torch.hpu` for now.
            # Update this once the support is available.
            # major, minor = torch.hpu.get_device_capability(device_id)
            major, minor = None, None
        except Exception as e:
            raise RuntimeError(
                f&#34;An error occurred while getting device capability of hpu: {e}.&#34;
            ) from e

    return major, minor</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_device_core_count"><code class="name flex">
<span>def <span class="ident">get_device_core_count</span></span>(<span>device_id: int = 0) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_device_core_count(device_id: int = 0) -&gt; int:
    if hasattr(torch, &#34;cuda&#34;) and torch.cuda.is_available():
        return torch.cuda.get_device_properties(device_id).multi_processor_count

    return 0</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_device_count"><code class="name flex">
<span>def <span class="ident">get_device_count</span></span>(<span>) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@lru_cache(maxsize=1)
def get_device_count() -&gt; int:
    if hasattr(torch, &#34;cuda&#34;) and torch.cuda.is_available():
        try:
            return torch.cuda.device_count()
        except RuntimeError:
            return 0

    if hasattr(torch, &#34;xpu&#34;) and torch.xpu.is_available():
        try:
            return torch.xpu.device_count()
        except RuntimeError:
            return 0

    if is_habana_available():
        try:
            import habana_frameworks.torch.hpu

            if torch.hpu.is_available():
                return torch.hpu.device_count()
        except (ImportError, RuntimeError):
            return 0

    return 0  # No accelerators available</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_device_memory_capacity"><code class="name flex">
<span>def <span class="ident">get_device_memory_capacity</span></span>(<span>device: str = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_device_memory_capacity(device: str = None):
    if is_cuda():
        gpu_mem = get_nvgpu_memory_capacity()
    elif is_hip():
        gpu_mem = get_amdgpu_memory_capacity()
    elif device == &#34;hpu&#34;:
        gpu_mem = get_hpu_memory_capacity()
    elif device == &#34;npu&#34;:
        gpu_mem = get_npu_memory_capacity()
    else:
        # GPU memory is not known yet or no GPU is available.
        gpu_mem = None

    return gpu_mem</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_device_name"><code class="name flex">
<span>def <span class="ident">get_device_name</span></span>(<span>device_id: int = 0) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_device_name(device_id: int = 0) -&gt; str:
    if hasattr(torch, &#34;cuda&#34;) and torch.cuda.is_available():
        return torch.cuda.get_device_name(device_id)

    if hasattr(torch, &#34;xpu&#34;) and torch.xpu.is_available():
        return torch.xpu.get_device_name(device_id)

    if hasattr(torch, &#34;hpu&#34;) and torch.hpu.is_available():
        return torch.hpu.get_device_name(device_id)

    if hasattr(torch, &#34;npu&#34;) and torch.npu.is_available():
        return torch.npu.get_device_name(device_id)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_device_sm"><code class="name flex">
<span>def <span class="ident">get_device_sm</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_device_sm():
    if torch.cuda.is_available():
        major, minor = torch.cuda.get_device_capability()
        return major * 10 + minor
    return 0</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_free_port"><code class="name flex">
<span>def <span class="ident">get_free_port</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_free_port():
    # try ipv4
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind((&#34;&#34;, 0))
            return s.getsockname()[1]
    except OSError:
        # try ipv6
        with socket.socket(socket.AF_INET6, socket.SOCK_STREAM) as s:
            s.bind((&#34;&#34;, 0))
            return s.getsockname()[1]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_hpu_memory_capacity"><code class="name flex">
<span>def <span class="ident">get_hpu_memory_capacity</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_hpu_memory_capacity():
    try:
        # Run hl-smi and capture the output
        result = subprocess.run(
            [&#34;hl-smi --query | grep &#39;Total&#39;&#34;],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            shell=True,
            text=True,
        )

        if result.returncode != 0:
            raise RuntimeError(f&#34;hl-smi error: {result.stderr.strip()}&#34;)

        # Parse the output to extract memory values in MiB
        memory_values = [
            float(mem.split(&#34; &#34;)[-2]) for mem in result.stdout.strip().split(&#34;\n&#34;)
        ]

        if not memory_values:
            raise ValueError(&#34;No GPU memory values found.&#34;)

        # Return the minimum memory value
        return min(memory_values)

    except FileNotFoundError:
        raise RuntimeError(
            &#34;hl-smi not found. Ensure Habana drivers are installed and accessible.&#34;
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_int_env_var"><code class="name flex">
<span>def <span class="ident">get_int_env_var</span></span>(<span>name: str, default: int = 0) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_int_env_var(name: str, default: int = 0) -&gt; int:
    value = os.getenv(name)
    if value is None or not value.strip():
        return default
    try:
        return int(value)
    except ValueError:
        return default</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_ip"><code class="name flex">
<span>def <span class="ident">get_ip</span></span>(<span>) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_ip() -&gt; str:
    # SGLANG_HOST_IP env can be ignore
    host_ip = os.getenv(&#34;SGLANG_HOST_IP&#34;, &#34;&#34;) or os.getenv(&#34;HOST_IP&#34;, &#34;&#34;)
    if host_ip:
        return host_ip

    # IP is not set, try to get it from the network interface

    # try ipv4
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    try:
        s.connect((&#34;8.8.8.8&#34;, 80))  # Doesn&#39;t need to be reachable
        return s.getsockname()[0]
    except Exception:
        pass

    # try ipv6
    try:
        s = socket.socket(socket.AF_INET6, socket.SOCK_DGRAM)
        # Google&#39;s public DNS server, see
        # https://developers.google.com/speed/public-dns/docs/using#addresses
        s.connect((&#34;2001:4860:4860::8888&#34;, 80))  # Doesn&#39;t need to be reachable
        return s.getsockname()[0]
    except Exception:
        pass

    # try  using hostname
    hostname = socket.gethostname()
    try:
        ip_addr = socket.gethostbyname(hostname)
        warnings.warn(&#34;using local ip address: {}&#34;.format(ip_addr))
        return ip_addr
    except Exception:
        pass

    warnings.warn(
        &#34;Failed to get the IP address, using 0.0.0.0 by default.&#34;
        &#34;The value can be set by the environment variable&#34;
        &#34; SGLANG_HOST_IP or HOST_IP.&#34;,
        stacklevel=2,
    )
    return &#34;0.0.0.0&#34;</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_local_ip_auto"><code class="name flex">
<span>def <span class="ident">get_local_ip_auto</span></span>(<span>) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_local_ip_auto() -&gt; str:
    interface = os.environ.get(&#34;SGLANG_LOCAL_IP_NIC&#34;, None)
    return (
        get_local_ip_by_nic(interface)
        if interface is not None
        else get_local_ip_by_remote()
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_local_ip_by_nic"><code class="name flex">
<span>def <span class="ident">get_local_ip_by_nic</span></span>(<span>interface: str) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_local_ip_by_nic(interface: str) -&gt; str:
    try:
        import netifaces
    except ImportError as e:
        raise ImportError(
            &#34;Environment variable SGLANG_LOCAL_IP_NIC requires package netifaces, please install it through &#39;pip install netifaces&#39;&#34;
        ) from e

    try:
        addresses = netifaces.ifaddresses(interface)
        if netifaces.AF_INET in addresses:
            for addr_info in addresses[netifaces.AF_INET]:
                ip = addr_info.get(&#34;addr&#34;)
                if ip and ip != &#34;127.0.0.1&#34; and ip != &#34;0.0.0.0&#34;:
                    return ip
        if netifaces.AF_INET6 in addresses:
            for addr_info in addresses[netifaces.AF_INET6]:
                ip = addr_info.get(&#34;addr&#34;)
                if ip and not ip.startswith(&#34;fe80::&#34;) and ip != &#34;::1&#34;:
                    return ip.split(&#34;%&#34;)[0]
    except (ValueError, OSError) as e:
        raise ValueError(
            &#34;Can not get local ip from NIC. Please verify whether SGLANG_LOCAL_IP_NIC is set correctly.&#34;
        )

    # Fallback
    return get_local_ip_by_remote()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_local_ip_by_remote"><code class="name flex">
<span>def <span class="ident">get_local_ip_by_remote</span></span>(<span>) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_local_ip_by_remote() -&gt; str:
    # try ipv4
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    try:
        s.connect((&#34;8.8.8.8&#34;, 80))  # Doesn&#39;t need to be reachable
        return s.getsockname()[0]
    except Exception:
        pass

    try:
        hostname = socket.gethostname()
        ip = socket.gethostbyname(hostname)
        if ip and ip != &#34;127.0.0.1&#34; and ip != &#34;0.0.0.0&#34;:
            return ip
    except Exception:
        pass

    # try ipv6
    try:
        s = socket.socket(socket.AF_INET6, socket.SOCK_DGRAM)
        # Google&#39;s public DNS server, see
        # https://developers.google.com/speed/public-dns/docs/using#addresses
        s.connect((&#34;2001:4860:4860::8888&#34;, 80))  # Doesn&#39;t need to be reachable
        return s.getsockname()[0]
    except Exception:
        raise ValueError(&#34;Can not get local ip&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_npu_compiler_config"><code class="name flex">
<span>def <span class="ident">get_npu_compiler_config</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_npu_compiler_config():
    config = {
        &#34;frozen_parameter&#34;: True,
        &#34;tiling_schedule_optimize&#34;: True,
        &#34;topology_sorting_strategy&#34;: &#34;StableRDFS&#34;,
    }
    return config</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_npu_memory_capacity"><code class="name flex">
<span>def <span class="ident">get_npu_memory_capacity</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_npu_memory_capacity():
    try:
        import torch_npu

        return torch.npu.mem_get_info()[1] // 1024 // 1024  # unit: MB
    except ImportError as e:
        raise ImportError(&#34;torch_npu is required when run on npu device.&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_nvgpu_memory_capacity"><code class="name flex">
<span>def <span class="ident">get_nvgpu_memory_capacity</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_nvgpu_memory_capacity():
    try:
        # Run nvidia-smi and capture the output
        result = subprocess.run(
            [&#34;nvidia-smi&#34;, &#34;--query-gpu=memory.total&#34;, &#34;--format=csv,noheader,nounits&#34;],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        )

        if result.returncode != 0:
            raise RuntimeError(f&#34;nvidia-smi error: {result.stderr.strip()}&#34;)

        # Parse the output to extract memory values
        memory_values = [
            float(mem)
            for mem in result.stdout.strip().split(&#34;\n&#34;)
            if re.match(r&#34;^\d+(\.\d+)?$&#34;, mem.strip())
        ]

        if not memory_values:
            # Fallback to torch.cuda.mem_get_info() when failed to get memory capacity from nvidia-smi,
            # typically in NVIDIA MIG mode.
            if torch.cuda.is_available():
                logger.warning(
                    &#34;Failed to get GPU memory capacity from nvidia-smi, falling back to torch.cuda.mem_get_info().&#34;
                )
                return torch.cuda.mem_get_info()[1] // 1024 // 1024  # unit: MB
            raise ValueError(&#34;No GPU memory values found.&#34;)

        # Return the minimum memory value
        return min(memory_values)

    except FileNotFoundError:
        raise RuntimeError(
            &#34;nvidia-smi not found. Ensure NVIDIA drivers are installed and accessible.&#34;
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_open_port"><code class="name flex">
<span>def <span class="ident">get_open_port</span></span>(<span>) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_open_port() -&gt; int:
    port = os.getenv(&#34;SGLANG_PORT&#34;)
    if port is not None:
        port = int(port)
        while True:
            try:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                    s.bind((&#34;&#34;, port))
                    return port
            except OSError:
                port += 1  # Increment port number if already in use
                logger.info(&#34;Port %d is already in use, trying port %d&#34;, port - 1, port)
    # try ipv4
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind((&#34;&#34;, 0))
            return s.getsockname()[1]
    except OSError:
        # try ipv6
        with socket.socket(socket.AF_INET6, socket.SOCK_STREAM) as s:
            s.bind((&#34;&#34;, 0))
            return s.getsockname()[1]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_physical_cpus_by_numa"><code class="name flex">
<span>def <span class="ident">get_physical_cpus_by_numa</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_physical_cpus_by_numa():
    cpu_info = parse_lscpu_topology()

    # Map NUMA node -&gt; set of (core_id, socket) to avoid duplicates
    # 0: {(0,0): 0, (1, 0): 1,...}
    # ...
    # 5: {(214,1): 214, (215,1): 215}
    physical_by_node = defaultdict(dict)  # node -&gt; core_id -&gt; cpu_id

    for cpu, core, socket, node in cpu_info:
        key = (core, socket)
        if key not in physical_by_node[node]:
            physical_by_node[node][
                key
            ] = cpu  # pick first CPU seen for that physical core

    # Retrieves CPUs that the current process is allowed to run on
    cpus_allowed_list = psutil.Process().cpu_affinity()

    # Convert to list of physical CPUs per node
    # 0: [0,1,2,...,42]
    # ...
    # 2: [86,87,...,127]
    # ...
    # 5: [214,215,...,255]
    node_to_cpus = {}
    for node, core_to_cpu in physical_by_node.items():
        cpus = sorted(core_to_cpu.values())
        allowed_cpus = set(cpus).intersection(cpus_allowed_list)
        node_to_cpus[node] = allowed_cpus

    return node_to_cpus</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.get_zmq_socket"><code class="name flex">
<span>def <span class="ident">get_zmq_socket</span></span>(<span>context: zmq.Context, socket_type: zmq.SocketType, endpoint: str, bind: bool)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_zmq_socket(
    context: zmq.Context, socket_type: zmq.SocketType, endpoint: str, bind: bool
):
    mem = psutil.virtual_memory()
    total_mem = mem.total / 1024**3
    available_mem = mem.available / 1024**3
    if total_mem &gt; 32 and available_mem &gt; 16:
        buf_size = int(0.5 * 1024**3)
    else:
        buf_size = -1

    socket = context.socket(socket_type)
    if endpoint.find(&#34;[&#34;) != -1:
        socket.setsockopt(zmq.IPV6, 1)

    def set_send_opt():
        socket.setsockopt(zmq.SNDHWM, 0)
        socket.setsockopt(zmq.SNDBUF, buf_size)

    def set_recv_opt():
        socket.setsockopt(zmq.RCVHWM, 0)
        socket.setsockopt(zmq.RCVBUF, buf_size)

    if socket_type == zmq.PUSH:
        set_send_opt()
    elif socket_type == zmq.PULL:
        set_recv_opt()
    elif socket_type == zmq.DEALER:
        set_send_opt()
        set_recv_opt()
    else:
        raise ValueError(f&#34;Unsupported socket type: {socket_type}&#34;)

    if bind:
        socket.bind(endpoint)
    else:
        socket.connect(endpoint)

    return socket</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.init_custom_process_group"><code class="name flex">
<span>def <span class="ident">init_custom_process_group</span></span>(<span>backend=None,<br>init_method=None,<br>timeout=None,<br>world_size=-1,<br>rank=-1,<br>store=None,<br>group_name=None,<br>pg_options=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_custom_process_group(
    backend=None,
    init_method=None,
    timeout=None,
    world_size=-1,
    rank=-1,
    store=None,
    group_name=None,
    pg_options=None,
):
    from torch.distributed.distributed_c10d import (
        Backend,
        PrefixStore,
        _new_process_group_helper,
        _world,
        default_pg_timeout,
        rendezvous,
    )

    assert (store is None) or (
        init_method is None
    ), &#34;Cannot specify both init_method and store.&#34;

    if store is not None:
        assert world_size &gt; 0, &#34;world_size must be positive if using store&#34;
        assert rank &gt;= 0, &#34;rank must be non-negative if using store&#34;
    elif init_method is None:
        init_method = &#34;env://&#34;

    if backend:
        backend = Backend(backend)
    else:
        backend = Backend(&#34;undefined&#34;)

    if timeout is None:
        timeout = default_pg_timeout

    # backward compatible API
    if store is None:
        rendezvous_iterator = rendezvous(init_method, rank, world_size, timeout=timeout)
        store, rank, world_size = next(rendezvous_iterator)
        store.set_timeout(timeout)

        # Use a PrefixStore to avoid accidental overrides of keys used by
        # different systems (e.g. RPC) in case the store is multi-tenant.
        store = PrefixStore(group_name, store)

    # NOTE: The pg_options parameter was renamed into backend_options in PyTorch 2.6.0
    # https://github.com/pytorch/pytorch/commit/a0c7029a75628cd5fa8df83c0de0ea98ee7fd844
    # We need to determine the appropriate parameter name based on PyTorch version
    pg_options_param_name = (
        &#34;backend_options&#34; if str(torch.__version__) &gt;= &#34;2.6&#34; else &#34;pg_options&#34;
    )
    pg, _ = _new_process_group_helper(
        world_size,
        rank,
        [],
        backend,
        store,
        group_name=group_name,
        **{pg_options_param_name: pg_options},
        timeout=timeout,
    )

    _world.pg_group_ranks[pg] = {i: i for i in range(world_size)}

    return pg</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_ampere_with_cuda_12_3"><code class="name flex">
<span>def <span class="ident">is_ampere_with_cuda_12_3</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">is_ampere_with_cuda_12_3 = lambda: _check(8)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_blackwell"><code class="name flex">
<span>def <span class="ident">is_blackwell</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_blackwell():
    if not is_cuda():
        return False
    return torch.cuda.get_device_capability()[0] == 10</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_cpu"><code class="name flex">
<span>def <span class="ident">is_cpu</span></span>(<span>) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_cpu() -&gt; bool:
    return os.getenv(&#34;SGLANG_USE_CPU_ENGINE&#34;, &#34;0&#34;) == &#34;1&#34; and is_host_cpu_x86()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_cuda"><code class="name flex">
<span>def <span class="ident">is_cuda</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_cuda():
    return torch.cuda.is_available() and torch.version.cuda</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_cuda_alike"><code class="name flex">
<span>def <span class="ident">is_cuda_alike</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_cuda_alike():
    return is_cuda() or is_hip()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_fa3_default_architecture"><code class="name flex">
<span>def <span class="ident">is_fa3_default_architecture</span></span>(<span>hf_config)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_fa3_default_architecture(hf_config):
    architectures = getattr(hf_config, &#34;architectures&#34;, None)
    if not isinstance(architectures, list) or not architectures:
        return False
    default_archs = {
        &#34;Qwen2ForCausalLM&#34;,
        &#34;Llama4ForConditionalGeneration&#34;,
        &#34;LlamaForCausalLM&#34;,
        &#34;Gemma2ForCausalLM&#34;,
        &#34;Gemma3ForConditionalGeneration&#34;,
        &#34;Qwen3ForCausalLM&#34;,
        &#34;Qwen3MoeForCausalLM&#34;,
        &#34;Glm4MoeForCausalLM&#34;,
        &#34;Glm4vMoeForConditionalGeneration&#34;,
        &#34;Step3VLForConditionalGeneration&#34;,
    }
    return architectures[0] in default_archs</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_flashinfer_available"><code class="name flex">
<span>def <span class="ident">is_flashinfer_available</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_flashinfer_available():
    &#34;&#34;&#34;
    Check whether flashinfer is available.
    As of Oct. 6, 2024, it is only available on NVIDIA GPUs.
    &#34;&#34;&#34;
    if not get_bool_env_var(&#34;SGLANG_IS_FLASHINFER_AVAILABLE&#34;, default=&#34;true&#34;):
        return False
    return importlib.util.find_spec(&#34;flashinfer&#34;) is not None and is_cuda()</code></pre>
</details>
<div class="desc"><p>Check whether flashinfer is available.
As of Oct. 6, 2024, it is only available on NVIDIA GPUs.</p></div>
</dd>
<dt id="sglang.srt.utils.is_habana_available"><code class="name flex">
<span>def <span class="ident">is_habana_available</span></span>(<span>) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@lru_cache(maxsize=1)
def is_habana_available() -&gt; bool:
    return find_spec(&#34;habana_frameworks&#34;) is not None</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_hip"><code class="name flex">
<span>def <span class="ident">is_hip</span></span>(<span>) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_hip() -&gt; bool:
    return torch.version.hip is not None</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_hopper_with_cuda_12_3"><code class="name flex">
<span>def <span class="ident">is_hopper_with_cuda_12_3</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">is_hopper_with_cuda_12_3 = lambda: _check(9)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_host_cpu_x86"><code class="name flex">
<span>def <span class="ident">is_host_cpu_x86</span></span>(<span>) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_host_cpu_x86() -&gt; bool:
    machine = platform.machine().lower()
    return (
        machine in (&#34;x86_64&#34;, &#34;amd64&#34;, &#34;i386&#34;, &#34;i686&#34;)
        and hasattr(torch, &#34;cpu&#34;)
        and torch.cpu.is_available()
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_hpu"><code class="name flex">
<span>def <span class="ident">is_hpu</span></span>(<span>) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_hpu() -&gt; bool:
    return hasattr(torch, &#34;hpu&#34;) and torch.hpu.is_available()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_no_spec_infer_or_topk_one"><code class="name flex">
<span>def <span class="ident">is_no_spec_infer_or_topk_one</span></span>(<span>server_args)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_no_spec_infer_or_topk_one(server_args):
    return server_args.speculative_eagle_topk is None or (
        server_args.speculative_eagle_topk is not None
        and server_args.speculative_eagle_topk == 1
        and is_page_size_one(server_args)
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_non_idle_and_non_empty"><code class="name flex">
<span>def <span class="ident">is_non_idle_and_non_empty</span></span>(<span>forward_mode, hidden_states)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_non_idle_and_non_empty(forward_mode, hidden_states):
    return (
        (forward_mode is not None)
        and not forward_mode.is_idle()
        and hidden_states.shape[0] &gt; 0
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_npu"><code class="name flex">
<span>def <span class="ident">is_npu</span></span>(<span>) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_npu() -&gt; bool:
    return hasattr(torch, &#34;npu&#34;) and torch.npu.is_available()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_page_size_one"><code class="name flex">
<span>def <span class="ident">is_page_size_one</span></span>(<span>server_args)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_page_size_one(server_args):
    return server_args.page_size == 1</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_pin_memory_available"><code class="name flex">
<span>def <span class="ident">is_pin_memory_available</span></span>(<span>) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_pin_memory_available() -&gt; bool:
    return torch.cuda.is_available()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_port_available"><code class="name flex">
<span>def <span class="ident">is_port_available</span></span>(<span>port)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_port_available(port):
    &#34;&#34;&#34;Return whether a port is available.&#34;&#34;&#34;
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        try:
            s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            s.bind((&#34;&#34;, port))
            s.listen(1)
            return True
        except socket.error:
            return False
        except OverflowError:
            return False</code></pre>
</details>
<div class="desc"><p>Return whether a port is available.</p></div>
</dd>
<dt id="sglang.srt.utils.is_remote_url"><code class="name flex">
<span>def <span class="ident">is_remote_url</span></span>(<span>url: Union[str, Path]) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_remote_url(url: Union[str, Path]) -&gt; bool:
    &#34;&#34;&#34;
    Check if the URL is a remote URL of the format:
    &lt;connector_type&gt;://&lt;host&gt;:&lt;port&gt;/&lt;model_name&gt;
    &#34;&#34;&#34;
    if isinstance(url, Path):
        return False

    pattern = r&#34;(.+)://(.*)&#34;
    m = re.match(pattern, url)
    return m is not None</code></pre>
</details>
<div class="desc"><p>Check if the URL is a remote URL of the format:
<connector_type>://<host>:<port>/<model_name></p></div>
</dd>
<dt id="sglang.srt.utils.is_shm_available"><code class="name flex">
<span>def <span class="ident">is_shm_available</span></span>(<span>dtype, world_size, local_size)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_shm_available(dtype, world_size, local_size):
    return (
        cpu_has_amx_support()
        and dtype in [torch.bfloat16, torch.float]
        and world_size &gt;= 1
        and world_size == local_size
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_sm100_supported"><code class="name flex">
<span>def <span class="ident">is_sm100_supported</span></span>(<span>device=None) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@lru_cache(maxsize=1)
def is_sm100_supported(device=None) -&gt; bool:
    return (torch.cuda.get_device_capability(device)[0] == 10) and (
        torch.version.cuda &gt;= &#34;12.8&#34;
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_sm90_supported"><code class="name flex">
<span>def <span class="ident">is_sm90_supported</span></span>(<span>device=None) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@lru_cache(maxsize=1)
def is_sm90_supported(device=None) -&gt; bool:
    return (torch.cuda.get_device_capability(device)[0] == 9) and (
        torch.version.cuda &gt;= &#34;12.3&#34;
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_triton_3"><code class="name flex">
<span>def <span class="ident">is_triton_3</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_triton_3():
    return triton.__version__.startswith(&#34;3.&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_triton_kernels_available"><code class="name flex">
<span>def <span class="ident">is_triton_kernels_available</span></span>(<span>) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@lru_cache(maxsize=1)
def is_triton_kernels_available() -&gt; bool:
    return importlib.util.find_spec(&#34;triton_kernels&#34;) is not None</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_valid_ipv6_address"><code class="name flex">
<span>def <span class="ident">is_valid_ipv6_address</span></span>(<span>address: str) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_valid_ipv6_address(address: str) -&gt; bool:
    try:
        ipaddress.IPv6Address(address)
        return True
    except ValueError:
        return False</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.is_xpu"><code class="name flex">
<span>def <span class="ident">is_xpu</span></span>(<span>) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_xpu() -&gt; bool:
    return hasattr(torch, &#34;xpu&#34;) and torch.xpu.is_available()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.kill_itself_when_parent_died"><code class="name flex">
<span>def <span class="ident">kill_itself_when_parent_died</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kill_itself_when_parent_died():
    if sys.platform == &#34;linux&#34;:
        # sigkill this process when parent worker manager dies
        PR_SET_PDEATHSIG = 1
        libc = ctypes.CDLL(&#34;libc.so.6&#34;)
        libc.prctl(PR_SET_PDEATHSIG, signal.SIGKILL)
    else:
        logger.warning(&#34;kill_itself_when_parent_died is only supported in linux.&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.kill_process_tree"><code class="name flex">
<span>def <span class="ident">kill_process_tree</span></span>(<span>parent_pid, include_parent: bool = True, skip_pid: int = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kill_process_tree(parent_pid, include_parent: bool = True, skip_pid: int = None):
    &#34;&#34;&#34;Kill the process and all its child processes.&#34;&#34;&#34;
    # Remove sigchld handler to avoid spammy logs.
    if threading.current_thread() is threading.main_thread():
        signal.signal(signal.SIGCHLD, signal.SIG_DFL)

    if parent_pid is None:
        parent_pid = os.getpid()
        include_parent = False

    try:
        itself = psutil.Process(parent_pid)
    except psutil.NoSuchProcess:
        return

    children = itself.children(recursive=True)
    for child in children:
        if child.pid == skip_pid:
            continue
        try:
            child.kill()
        except psutil.NoSuchProcess:
            pass

    if include_parent:
        try:
            if parent_pid == os.getpid():
                itself.kill()
                sys.exit(0)

            itself.kill()

            # Sometime processes cannot be killed with SIGKILL (e.g, PID=1 launched by kubernetes),
            # so we send an additional signal to kill them.
            itself.send_signal(signal.SIGQUIT)
        except psutil.NoSuchProcess:
            pass</code></pre>
</details>
<div class="desc"><p>Kill the process and all its child processes.</p></div>
</dd>
<dt id="sglang.srt.utils.launch_dummy_health_check_server"><code class="name flex">
<span>def <span class="ident">launch_dummy_health_check_server</span></span>(<span>host, port, enable_metrics)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def launch_dummy_health_check_server(host, port, enable_metrics):
    import asyncio

    import uvicorn
    from fastapi import FastAPI, Response

    app = FastAPI()

    @app.get(&#34;/health&#34;)
    async def health():
        &#34;&#34;&#34;Check the health of the http server.&#34;&#34;&#34;
        return Response(status_code=200)

    @app.get(&#34;/health_generate&#34;)
    async def health_generate():
        &#34;&#34;&#34;Check the health of the http server.&#34;&#34;&#34;
        return Response(status_code=200)

    # Add prometheus middleware
    if enable_metrics:
        add_prometheus_middleware(app)
        enable_func_timer()

    config = uvicorn.Config(
        app,
        host=host,
        port=port,
        timeout_keep_alive=5,
        loop=&#34;auto&#34;,
        log_config=None,
        log_level=&#34;warning&#34;,
    )
    server = uvicorn.Server(config=config)

    try:
        loop = asyncio.get_running_loop()
        logger.info(
            f&#34;Dummy health check server scheduled on existing loop at {host}:{port}&#34;
        )
        loop.create_task(server.serve())

    except RuntimeError:
        logger.info(f&#34;Starting dummy health check server at {host}:{port}&#34;)
        server.run()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.load_audio"><code class="name flex">
<span>def <span class="ident">load_audio</span></span>(<span>audio_file: str, sr: Optional[int] = None, mono: bool = True) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_audio(
    audio_file: str, sr: Optional[int] = None, mono: bool = True
) -&gt; np.ndarray:
    # Use soundfile here, since librosa use it under the hood,
    # and librosa will not support audio loading in the future
    import soundfile as sf
    from scipy.signal import resample

    if sr is None:
        sr = 16000

    # Load audio data
    if isinstance(audio_file, bytes):
        audio, original_sr = sf.read(BytesIO(audio_file))
    elif audio_file.startswith(&#34;data:&#34;):
        audio_file = audio_file.split(&#34;,&#34;)[1]
        audio, original_sr = sf.read(
            BytesIO(pybase64.b64decode(audio_file, validate=True))
        )
    elif audio_file.startswith(&#34;http://&#34;) or audio_file.startswith(&#34;https://&#34;):
        timeout = int(os.getenv(&#34;REQUEST_TIMEOUT&#34;, &#34;5&#34;))
        response = requests.get(audio_file, stream=True, timeout=timeout)
        audio_file = BytesIO(response.content)
        response.close()
        audio, original_sr = sf.read(audio_file)
    elif isinstance(audio_file, str):
        audio, original_sr = sf.read(audio_file)
    else:
        raise ValueError(f&#34;Invalid audio format: {audio_file}&#34;)

    # Resample audio if the original sample rate is different from the desired sample rate
    if original_sr != sr:
        num_samples = int(len(audio) * float(sr) / original_sr)
        audio = resample(audio, num_samples)

    # Convert to mono if requested and audio is stereo
    if mono and len(audio.shape) &gt; 1:
        audio = np.mean(audio, axis=1)

    return audio</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.load_image"><code class="name flex">
<span>def <span class="ident">load_image</span></span>(<span>image_file: Union[Image.Image, str, <a title="sglang.srt.utils.ImageData" href="#sglang.srt.utils.ImageData">ImageData</a>, bytes]) ‑> tuple[PIL.Image.Image, tuple[int, int]]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_image(
    image_file: Union[Image.Image, str, ImageData, bytes],
) -&gt; tuple[Image.Image, tuple[int, int]]:
    if isinstance(image_file, ImageData):
        image_file = image_file.url

    image = image_size = None
    if isinstance(image_file, Image.Image):
        image = image_file
        image_size = (image.width, image.height)
    elif isinstance(image_file, bytes):
        image = Image.open(BytesIO(image_file))
    elif image_file.startswith(&#34;http://&#34;) or image_file.startswith(&#34;https://&#34;):
        timeout = int(os.getenv(&#34;REQUEST_TIMEOUT&#34;, &#34;3&#34;))
        response = requests.get(image_file, stream=True, timeout=timeout)
        try:
            response.raise_for_status()
            image = Image.open(response.raw)
            image.load()  # Force loading to avoid issues after closing the stream
        finally:
            response.close()
    elif image_file.lower().endswith((&#34;png&#34;, &#34;jpg&#34;, &#34;jpeg&#34;, &#34;webp&#34;, &#34;gif&#34;)):
        image = Image.open(image_file)
    elif image_file.startswith(&#34;data:&#34;):
        image_file = image_file.split(&#34;,&#34;)[1]
        image = Image.open(BytesIO(pybase64.b64decode(image_file, validate=True)))
    elif isinstance(image_file, str):
        image = Image.open(BytesIO(pybase64.b64decode(image_file, validate=True)))
    else:
        raise ValueError(f&#34;Invalid image: {image_file}&#34;)

    return image, image_size</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.load_json_config"><code class="name flex">
<span>def <span class="ident">load_json_config</span></span>(<span>data: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_json_config(data: str):
    try:
        return json.loads(data)
    except JSONDecodeError:
        return json.loads(Path(data).read_text())</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.load_video"><code class="name flex">
<span>def <span class="ident">load_video</span></span>(<span>video_file: Union[str, bytes], use_gpu: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_video(video_file: Union[str, bytes], use_gpu: bool = True):
    # We import decord here to avoid a strange Segmentation fault (core dumped) issue.
    from decord import VideoReader, cpu, gpu

    try:
        from decord.bridge import decord_bridge

        ctx = gpu(0)
        _ = decord_bridge.get_ctx_device(ctx)
    except Exception:
        ctx = cpu(0)

    tmp_file = None
    vr = None
    try:
        if isinstance(video_file, bytes):
            tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=&#34;.mp4&#34;)
            tmp_file.write(video_file)
            tmp_file.close()
            vr = VideoReader(tmp_file.name, ctx=ctx)
        elif isinstance(video_file, str):
            if video_file.startswith((&#34;http://&#34;, &#34;https://&#34;)):
                timeout = int(os.getenv(&#34;REQUEST_TIMEOUT&#34;, &#34;10&#34;))
                response = requests.get(video_file, stream=True, timeout=timeout)
                response.raise_for_status()
                tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=&#34;.mp4&#34;)
                for chunk in response.iter_content(chunk_size=8192):
                    tmp_file.write(chunk)
                tmp_file.close()
                vr = VideoReader(tmp_file.name, ctx=ctx)
            elif video_file.startswith(&#34;data:&#34;):
                _, encoded = video_file.split(&#34;,&#34;, 1)
                video_bytes = pybase64.b64decode(encoded)
                tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=&#34;.mp4&#34;)
                tmp_file.write(video_bytes)
                tmp_file.close()
                vr = VideoReader(tmp_file.name, ctx=ctx)
            elif os.path.isfile(video_file):
                vr = VideoReader(video_file, ctx=ctx)
            else:
                video_bytes = pybase64.b64decode(video_file)
                tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=&#34;.mp4&#34;)
                tmp_file.write(video_bytes)
                tmp_file.close()
                vr = VideoReader(tmp_file.name, ctx=ctx)
        else:
            raise ValueError(f&#34;Unsupported video input type: {type(video_file)}&#34;)

        return vr

    finally:
        if tmp_file and os.path.exists(tmp_file.name):
            os.unlink(tmp_file.name)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.log_info_on_rank0"><code class="name flex">
<span>def <span class="ident">log_info_on_rank0</span></span>(<span>logger, msg)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_info_on_rank0(logger, msg):
    from sglang.srt.distributed import get_tensor_model_parallel_rank

    if get_tensor_model_parallel_rank() == 0:
        logger.info(msg)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.lru_cache_frozenset"><code class="name flex">
<span>def <span class="ident">lru_cache_frozenset</span></span>(<span>maxsize=128)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lru_cache_frozenset(maxsize=128):
    def _to_hashable(o):
        try:
            hash(o)
            return o
        except TypeError:
            # Not hashable; convert based on type
            if isinstance(o, (dict)):
                return frozenset(
                    (_to_hashable(k), _to_hashable(v)) for k, v in o.items()
                )
            elif isinstance(o, set):
                return frozenset(_to_hashable(v) for v in o)
            elif isinstance(o, (list, tuple)) or (
                isinstance(o, Sequence) and not isinstance(o, (str, bytes))
            ):
                return tuple(_to_hashable(v) for v in o)
            else:
                raise TypeError(f&#34;Cannot make hashable: {type(o)}&#34;)

    def decorator(func):
        cache = OrderedDict()

        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            h_args = tuple(_to_hashable(a) for a in args)
            h_kwargs = frozenset(
                (_to_hashable(k), _to_hashable(v)) for k, v in kwargs.items()
            )
            key = (h_args, h_kwargs)
            if key in cache:
                cache.move_to_end(key)
                return cache[key]
            result = func(*args, **kwargs)
            cache[key] = result
            if maxsize is not None and len(cache) &gt; maxsize:
                cache.popitem(last=False)
            return result

        wrapper.cache_clear = cache.clear  # For manual cache clearing
        return wrapper

    return decorator</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.make_layers"><code class="name flex">
<span>def <span class="ident">make_layers</span></span>(<span>num_hidden_layers: int,<br>layer_fn: <a title="sglang.srt.utils.LayerFn" href="#sglang.srt.utils.LayerFn">LayerFn</a>,<br>pp_rank: Optional[int] = None,<br>pp_size: Optional[int] = None,<br>prefix: str = '',<br>return_tuple: bool = False,<br>offloader_kwargs: Dict[str, Any] = {}) ‑> Tuple[int, int, torch.nn.modules.container.ModuleList]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_layers(
    num_hidden_layers: int,
    layer_fn: LayerFn,
    pp_rank: Optional[int] = None,
    pp_size: Optional[int] = None,
    prefix: str = &#34;&#34;,
    return_tuple: bool = False,
    offloader_kwargs: Dict[str, Any] = {},
) -&gt; Tuple[int, int, torch.nn.ModuleList]:
    &#34;&#34;&#34;Make a list of layers with the given layer function&#34;&#34;&#34;
    # circula imports
    from sglang.srt.distributed import get_pp_indices
    from sglang.srt.layers.utils import PPMissingLayer
    from sglang.srt.offloader import get_offloader

    assert not pp_size or num_hidden_layers &gt;= pp_size
    start_layer, end_layer = (
        get_pp_indices(
            num_hidden_layers,
            pp_rank,
            pp_size,
        )
        if pp_rank is not None and pp_size is not None
        else (0, num_hidden_layers)
    )
    modules = torch.nn.ModuleList(
        [PPMissingLayer(return_tuple=return_tuple) for _ in range(start_layer)]
        + get_offloader().wrap_modules(
            (
                layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
                for idx in range(start_layer, end_layer)
            ),
            **offloader_kwargs,
        )
        + [
            PPMissingLayer(return_tuple=return_tuple)
            for _ in range(end_layer, num_hidden_layers)
        ]
    )
    if pp_rank is None or pp_size is None:
        return modules
    return modules, start_layer, end_layer</code></pre>
</details>
<div class="desc"><p>Make a list of layers with the given layer function</p></div>
</dd>
<dt id="sglang.srt.utils.mark_end"><code class="name flex">
<span>def <span class="ident">mark_end</span></span>(<span>name)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mark_end(name):
    global time_infos, show_time_cost
    if not show_time_cost:
        return
    torch.cuda.synchronize()
    time_infos[name].acc_time += time.perf_counter()
    if time_infos[name].check():
        time_infos[name].pretty_print()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.mark_start"><code class="name flex">
<span>def <span class="ident">mark_start</span></span>(<span>name, interval=0.1, color=0, indent=0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mark_start(name, interval=0.1, color=0, indent=0):
    global time_infos, show_time_cost
    if not show_time_cost:
        return
    torch.cuda.synchronize()
    if time_infos.get(name, None) is None:
        time_infos[name] = TimeInfo(name, interval, color, indent)
    time_infos[name].acc_time -= time.perf_counter()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.maybe_torch_compile"><code class="name flex">
<span>def <span class="ident">maybe_torch_compile</span></span>(<span>*args, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def maybe_torch_compile(*args, **kwargs):
    &#34;&#34;&#34;
    torch.compile does not work for triton 2.2.0, which is needed in xlm1&#39;s jax.
    Therefore, we disable it here.
    &#34;&#34;&#34;

    def decorator(func):
        if is_triton_3():
            return torch.compile(*args, **kwargs)(func)
        return func

    return decorator</code></pre>
</details>
<div class="desc"><p>torch.compile does not work for triton 2.2.0, which is needed in xlm1's jax.
Therefore, we disable it here.</p></div>
</dd>
<dt id="sglang.srt.utils.maybe_wrap_ipv6_address"><code class="name flex">
<span>def <span class="ident">maybe_wrap_ipv6_address</span></span>(<span>address: str) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def maybe_wrap_ipv6_address(address: str) -&gt; str:
    if is_valid_ipv6_address(address):
        return f&#34;[{address}]&#34;
    return address</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.monkey_patch_p2p_access_check"><code class="name flex">
<span>def <span class="ident">monkey_patch_p2p_access_check</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def monkey_patch_p2p_access_check():
    &#34;&#34;&#34;
    Monkey patch the slow p2p access check.
    NOTE: We assume the p2p access is always allowed, which can be wrong for some setups.
    &#34;&#34;&#34;

    import sglang.srt.distributed.device_communicators.custom_all_reduce_utils as tgt

    setattr(tgt, &#34;gpu_p2p_access_check&#34;, lambda *arg, **kwargs: True)

    # Suppress the warnings from this delete function when using sglang.bench_one_batch
    from sglang.srt.distributed.device_communicators.custom_all_reduce import (
        CustomAllreduce,
    )

    setattr(CustomAllreduce, &#34;__del__&#34;, lambda *args, **kwargs: None)</code></pre>
</details>
<div class="desc"><p>Monkey patch the slow p2p access check.
NOTE: We assume the p2p access is always allowed, which can be wrong for some setups.</p></div>
</dd>
<dt id="sglang.srt.utils.monkey_patch_vllm_gguf_config"><code class="name flex">
<span>def <span class="ident">monkey_patch_vllm_gguf_config</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def monkey_patch_vllm_gguf_config():
    try:
        from vllm.model_executor.layers.quantization.gguf import (
            GGUFConfig,
            GGUFEmbeddingMethod,
            GGUFLinearMethod,
        )
    except ImportError:
        return

    from sglang.srt.layers.linear import LinearBase
    from sglang.srt.layers.vocab_parallel_embedding import VocabParallelEmbedding

    def get_quant_method_with_embedding_replaced(
        self, layer: torch.nn.Module, prefix: str
    ) -&gt; Optional[&#34;QuantizeMethodBase&#34;]:
        if isinstance(layer, LinearBase):
            return GGUFLinearMethod(self)
        elif isinstance(layer, VocabParallelEmbedding):
            # patch to own VocabParallelEmbedding
            return GGUFEmbeddingMethod(self)
        return None

    setattr(GGUFConfig, &#34;get_quant_method&#34;, get_quant_method_with_embedding_replaced)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.mxfp_supported"><code class="name flex">
<span>def <span class="ident">mxfp_supported</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mxfp_supported():
    &#34;&#34;&#34;
    Returns whether the current platform supports MX types.
    &#34;&#34;&#34;
    if torch.version.hip:
        gcn_arch = torch.cuda.get_device_properties(0).gcnArchName
        return any(gfx in gcn_arch for gfx in [&#34;gfx95&#34;])
    else:
        return False</code></pre>
</details>
<div class="desc"><p>Returns whether the current platform supports MX types.</p></div>
</dd>
<dt id="sglang.srt.utils.next_power_of_2"><code class="name flex">
<span>def <span class="ident">next_power_of_2</span></span>(<span>n: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def next_power_of_2(n: int):
    return 1 &lt;&lt; (n - 1).bit_length() if n &gt; 0 else 1</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.nullable_str"><code class="name flex">
<span>def <span class="ident">nullable_str</span></span>(<span>val: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def nullable_str(val: str):
    if not val or val == &#34;None&#34;:
        return None
    return val</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.parse_connector_type"><code class="name flex">
<span>def <span class="ident">parse_connector_type</span></span>(<span>url: str) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_connector_type(url: str) -&gt; str:
    &#34;&#34;&#34;
    Parse the connector type from the URL of the format:
    &lt;connector_type&gt;://&lt;path&gt;
    &#34;&#34;&#34;
    pattern = r&#34;(.+)://(.*)&#34;
    m = re.match(pattern, url)
    if m is None:
        return &#34;&#34;

    return m.group(1)</code></pre>
</details>
<div class="desc"><p>Parse the connector type from the URL of the format:
<connector_type>://<path></p></div>
</dd>
<dt id="sglang.srt.utils.parse_lscpu_topology"><code class="name flex">
<span>def <span class="ident">parse_lscpu_topology</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_lscpu_topology():
    try:
        # Get CPU topology: CPU,Core,Socket,Node
        output = subprocess.check_output(
            [&#34;lscpu&#34;, &#34;-p=CPU,Core,Socket,Node&#34;], text=True
        )
    except Exception as e:
        raise RuntimeError(f&#34;Unexpected error running &#39;lscpu&#39;: {e}&#34;)

    # Parse only data lines (skip comments)
    cpu_info = []
    for line in output.splitlines():
        if not line.startswith(&#34;#&#34;):
            cpu, core, socket, node = map(int, line.strip().split(&#34;,&#34;))
            cpu_info.append((cpu, core, socket, node))

    # [(0,0,0,0),(1,1,0,0),...,(43,43,0,1),...,(256,0,0,0),...]
    return cpu_info</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.parse_module_path"><code class="name flex">
<span>def <span class="ident">parse_module_path</span></span>(<span>module_path, function_name, create_dummy)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_module_path(module_path, function_name, create_dummy):
    from importlib.machinery import ModuleSpec

    def create_dummy_module(full_path, parent=None):
        &#34;&#34;&#34;Create and register a placeholder module&#34;&#34;&#34;
        dummy = types.ModuleType(full_path)
        dummy.__file__ = &#34;vllm_ascend.dummy_module.py&#34;
        dummy.__spec__ = ModuleSpec(full_path, None)
        sys.modules[full_path] = dummy
        if parent:
            setattr(parent, full_path.split(&#34;.&#34;)[-1], dummy)
        return dummy

    def create_placeholder_function(func_name):
        &#34;&#34;&#34;Create dummy function that raises when called&#34;&#34;&#34;

        def placeholder(*args, **kwargs):
            raise NotImplementedError(f&#34;Function {func_name} is a placeholder&#34;)

        placeholder.__name__ = func_name
        return placeholder

    modules = module_path.split(&#34;.&#34;)
    current_module = None
    processed_path = []

    for idx, part in enumerate(modules):
        current_path = &#34;.&#34;.join(modules[: idx + 1])
        parent_path = &#34;.&#34;.join(modules[:idx]) if idx &gt; 0 else None

        try:
            current_module = importlib.import_module(current_path)
        except ModuleNotFoundError:
            # Handle missing module
            parent = importlib.import_module(parent_path) if parent_path else None
            if parent and hasattr(parent, part):
                # Use existing attribute from parent
                current_module = getattr(parent, part)
                # Check for early function resolution
                if function_name and hasattr(current_module, function_name):
                    return current_module, getattr(current_module, function_name)
                if function_name and create_dummy:
                    ph_func = create_placeholder_function(function_name)
                    setattr(current_module, function_name, ph_func)
                    return current_module, ph_func
                if function_name:
                    raise AttributeError(
                        f&#34;Function {function_name} missing in {current_path}&#34;
                    )
            else:
                if not create_dummy:
                    raise
                # Create and register dummy module
                current_module = create_dummy_module(
                    current_path,
                    parent=(
                        importlib.import_module(parent_path) if parent_path else None
                    ),
                )

        processed_path.append(part)

    # Final function handling
    final_module = sys.modules[module_path]
    if function_name is not None:
        if not hasattr(final_module, function_name):
            if create_dummy:
                ph_func = create_placeholder_function(function_name)
                setattr(final_module, function_name, ph_func)
            else:
                setattr(final_module, function_name, None)
        return final_module, getattr(final_module, function_name)

    return final_module, None</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.permute_weight"><code class="name flex">
<span>def <span class="ident">permute_weight</span></span>(<span>x: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def permute_weight(x: torch.Tensor) -&gt; torch.Tensor:
    b_ = x.shape[0]
    n_ = x.shape[1]
    k_ = x.shape[2]

    x_ = x
    if x.dtype == torch.bfloat16 or x.dtype == torch.float16:
        x_ = x_.view(int(b_), int(n_ / 16), 16, int(k_ / 32), 4, 8)
    elif x.dtype == torch.float8_e4m3fnuz or x.dtype == torch.int8:
        x_ = x_.view(int(b_), int(n_ / 16), 16, int(k_ / 64), 4, 16)
    else:
        # return x_
        x_ = x_.view(int(b_), int(n_ / 16), 16, int(k_ / 8), 2, 4)

    x_ = x_.permute(0, 1, 3, 4, 2, 5)
    x_ = x_.contiguous()
    x_ = x_.view(*x.shape)
    return x_</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.point_to_point_pyobj"><code class="name flex">
<span>def <span class="ident">point_to_point_pyobj</span></span>(<span>data: List[Any],<br>rank: int,<br>group: Optional[torch.distributed.ProcessGroup] = None,<br>src: int = 0,<br>dst: int = 1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def point_to_point_pyobj(
    data: List[Any],
    rank: int,
    group: Optional[torch.distributed.ProcessGroup] = None,
    src: int = 0,
    dst: int = 1,
):
    &#34;&#34;&#34;Send data from src to dst in group using DeviceToDevice communication.&#34;&#34;&#34;

    if rank == src:
        if len(data) == 0:
            tensor_size = torch.tensor(
                [0], dtype=torch.long, device=torch.cuda.current_device()
            )
            dist.send(tensor_size, dst=dst, group=group)
        else:
            serialized_data = pickle.dumps(data)
            size = len(serialized_data)
            tensor_data = torch.ByteTensor(
                np.frombuffer(serialized_data, dtype=np.uint8)
            ).cuda(
                device=torch.cuda.current_device()
            )  # Move to GPU
            tensor_size = torch.tensor(
                [size], dtype=torch.long, device=torch.cuda.current_device()
            )

            dist.send(tensor_size, dst=dst, group=group)
            dist.send(tensor_data, dst=dst, group=group)
        return data

    elif rank == dst:
        tensor_size = torch.tensor(
            [0], dtype=torch.long, device=torch.cuda.current_device()
        )
        dist.recv(tensor_size, src=src, group=group)
        size = tensor_size.item()

        if size == 0:
            return []

        tensor_data = torch.empty(
            size, dtype=torch.uint8, device=torch.cuda.current_device()
        )
        dist.recv(tensor_data, src=src, group=group)

        serialized_data = bytes(
            tensor_data.cpu().numpy()
        )  # Move back to host for deserialization
        data = pickle.loads(serialized_data)
        return data

    # Other ranks in pp_group do nothing
    return []</code></pre>
</details>
<div class="desc"><p>Send data from src to dst in group using DeviceToDevice communication.</p></div>
</dd>
<dt id="sglang.srt.utils.prepack_weight_if_needed"><code class="name flex">
<span>def <span class="ident">prepack_weight_if_needed</span></span>(<span>weight)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepack_weight_if_needed(weight):
    if weight.device != torch.device(&#34;cpu&#34;):
        return weight
    if not cpu_has_amx_support():
        return weight

    return torch.ops.sgl_kernel.convert_weight_packed(weight)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.prepare_model_and_tokenizer"><code class="name flex">
<span>def <span class="ident">prepare_model_and_tokenizer</span></span>(<span>model_path: str, tokenizer_path: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_model_and_tokenizer(model_path: str, tokenizer_path: str):
    if get_bool_env_var(&#34;SGLANG_USE_MODELSCOPE&#34;):
        if not os.path.exists(model_path):
            from modelscope import snapshot_download

            model_path = snapshot_download(model_path)
            tokenizer_path = snapshot_download(
                tokenizer_path, ignore_patterns=[&#34;*.bin&#34;, &#34;*.safetensors&#34;]
            )
    return model_path, tokenizer_path</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.print_info_once"><code class="name flex">
<span>def <span class="ident">print_info_once</span></span>(<span>msg: str) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@functools.lru_cache(None)
def print_info_once(msg: str) -&gt; None:
    logger.info(msg)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.print_warning_once"><code class="name flex">
<span>def <span class="ident">print_warning_once</span></span>(<span>msg: str) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_warning_once(msg: str) -&gt; None:
    # Set the stacklevel to 2 to print the caller&#39;s line info
    logger.warning(msg, stacklevel=2)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.pyspy_dump_schedulers"><code class="name flex">
<span>def <span class="ident">pyspy_dump_schedulers</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pyspy_dump_schedulers():
    &#34;&#34;&#34;py-spy dump on all scheduler in a local node.&#34;&#34;&#34;
    try:
        pid = psutil.Process().pid
        # Command to run py-spy with the PID
        cmd = f&#34;py-spy dump --pid {pid}&#34;
        result = subprocess.run(
            cmd, shell=True, capture_output=True, text=True, check=True
        )
        logger.error(f&#34;Pyspy dump for PID {pid}:\n{result.stdout}&#34;)
    except subprocess.CalledProcessError as e:
        logger.error(f&#34;Pyspy failed to dump PID {pid}. Error: {e.stderr}&#34;)</code></pre>
</details>
<div class="desc"><p>py-spy dump on all scheduler in a local node.</p></div>
</dd>
<dt id="sglang.srt.utils.pytorch_profile"><code class="name flex">
<span>def <span class="ident">pytorch_profile</span></span>(<span>name, func, *args, data_size=-1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pytorch_profile(name, func, *args, data_size=-1):
    &#34;&#34;&#34;
    Args:
        name (string): the name of recorded function.
        func: the function to be profiled.
        args: the arguments of the profiled function.
        data_size (int): some measurement of the computation complexity.
            Usually, it could be the batch size.
    &#34;&#34;&#34;
    global step_counter
    os.makedirs(&#34;trace&#34;, exist_ok=True)
    with profile(
        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
        # schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),
        # on_trace_ready=tensorboard_trace_handler(&#39;./log_dir&#39;),
        record_shapes=True,
        profile_memory=True,
        with_stack=True,
    ) as prof:
        with record_function(name):
            with open(f&#34;trace/size_{step_counter}.json&#34;, &#34;w&#34;) as f:
                json.dump({&#34;size&#34;: data_size}, f)
            result = func(*args)
    prof.export_chrome_trace(f&#34;trace/{name}_{step_counter}.json&#34;)
    step_counter += 1
    return result</code></pre>
</details>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>string</code></dt>
<dd>the name of recorded function.</dd>
<dt><strong><code>func</code></strong></dt>
<dd>the function to be profiled.</dd>
<dt><strong><code>args</code></strong></dt>
<dd>the arguments of the profiled function.</dd>
<dt><strong><code>data_size</code></strong> :&ensp;<code>int</code></dt>
<dd>some measurement of the computation complexity.
Usually, it could be the batch size.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.utils.random_uuid"><code class="name flex">
<span>def <span class="ident">random_uuid</span></span>(<span>) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_uuid() -&gt; str:
    return str(uuid.uuid4().hex)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.read_system_prompt_from_file"><code class="name flex">
<span>def <span class="ident">read_system_prompt_from_file</span></span>(<span>model_name: str) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_system_prompt_from_file(model_name: str) -&gt; str:
    &#34;&#34;&#34;Read system prompt from a file in the HuggingFace cache directory.

    Args:
        model_name: The model name to construct the file path

    Returns:
        The system prompt content from the file, or empty string if file not found
    &#34;&#34;&#34;
    try:
        local_repo_dir = find_local_repo_dir(model_name)
        if local_repo_dir:
            system_prompt_file = os.path.join(local_repo_dir, &#34;SYSTEM_PROMPT.txt&#34;)
            if os.path.exists(system_prompt_file):
                with open(system_prompt_file, &#34;r&#34;, encoding=&#34;utf-8&#34;) as f:
                    return f.read()

        return &#34;&#34;
    except Exception:
        # If anything fails, return empty string
        return &#34;&#34;</code></pre>
</details>
<div class="desc"><p>Read system prompt from a file in the HuggingFace cache directory.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_name</code></strong></dt>
<dd>The model name to construct the file path</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The system prompt content from the file, or empty string if file not found</p></div>
</dd>
<dt id="sglang.srt.utils.replace_submodule"><code class="name flex">
<span>def <span class="ident">replace_submodule</span></span>(<span>model: nn.Module, module_name: str, new_module: nn.Module) ‑> torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def replace_submodule(
    model: nn.Module, module_name: str, new_module: nn.Module
) -&gt; nn.Module:
    &#34;&#34;&#34;Replace a submodule in a model with a new module.&#34;&#34;&#34;
    parent = model.get_submodule(&#34;.&#34;.join(module_name.split(&#34;.&#34;)[:-1]))
    target_name = module_name.split(&#34;.&#34;)[-1]
    setattr(parent, target_name, new_module)
    return new_module</code></pre>
</details>
<div class="desc"><p>Replace a submodule in a model with a new module.</p></div>
</dd>
<dt id="sglang.srt.utils.require_attn_tp_gather"><code class="name flex">
<span>def <span class="ident">require_attn_tp_gather</span></span>(<span>server_args)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def require_attn_tp_gather(server_args):
    &#34;&#34;&#34;
    Check if the input of attention is scattered.
    &#34;&#34;&#34;
    assert server_args.moe_dense_tp_size in [1, None]
    if server_args.moe_a2a_backend != &#34;none&#34; or server_args.moe_dense_tp_size == 1:
        if server_args.enable_dp_attention:
            return server_args.dp_size &lt; server_args.tp_size
        else:
            return True
    else:
        return False</code></pre>
</details>
<div class="desc"><p>Check if the input of attention is scattered.</p></div>
</dd>
<dt id="sglang.srt.utils.require_gathered_buffer"><code class="name flex">
<span>def <span class="ident">require_gathered_buffer</span></span>(<span>server_args)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def require_gathered_buffer(server_args):
    return require_mlp_tp_gather(server_args) or require_attn_tp_gather(server_args)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.require_mlp_sync"><code class="name flex">
<span>def <span class="ident">require_mlp_sync</span></span>(<span>server_args)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def require_mlp_sync(server_args):
    return server_args.enable_dp_attention or require_gathered_buffer(server_args)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.require_mlp_tp_gather"><code class="name flex">
<span>def <span class="ident">require_mlp_tp_gather</span></span>(<span>server_args)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def require_mlp_tp_gather(server_args):
    &#34;&#34;&#34;
    Check if the input of MLP is obtained by all-gather rather than all-reduce. This only happens when each MLP TP group contains multiple attention DP groups.
    &#34;&#34;&#34;
    if server_args.enable_dp_attention:
        assert server_args.dp_size &gt; 1, &#34;dp_size must be greater than 1&#34;
        if (
            server_args.moe_dense_tp_size is None
        ):  # TODO(ch-wan): some MoE models do not have dense layers
            return True
        elif not server_args.enable_dp_lm_head:
            return True
        elif server_args.moe_a2a_backend == &#34;none&#34;:
            return True
        else:
            return (
                server_args.moe_dense_tp_size
                &gt; server_args.tp_size // server_args.dp_size
            )
    else:
        return False</code></pre>
</details>
<div class="desc"><p>Check if the input of MLP is obtained by all-gather rather than all-reduce. This only happens when each MLP TP group contains multiple attention DP groups.</p></div>
</dd>
<dt id="sglang.srt.utils.retry"><code class="name flex">
<span>def <span class="ident">retry</span></span>(<span>fn,<br>max_retry: int,<br>initial_delay: float = 2.0,<br>max_delay: float = 60.0,<br>should_retry: Callable[[Any], bool] = &lt;function &lt;lambda&gt;&gt;)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def retry(
    fn,
    max_retry: int,
    initial_delay: float = 2.0,
    max_delay: float = 60.0,
    should_retry: Callable[[Any], bool] = lambda e: True,
):
    for try_index in itertools.count():
        try:
            return fn()
        except Exception as e:
            if try_index &gt;= max_retry:
                raise Exception(f&#34;retry() exceed maximum number of retries.&#34;)

            if not should_retry(e):
                raise Exception(f&#34;retry() observe errors that should not be retried.&#34;)

            delay = min(initial_delay * (2**try_index), max_delay) * (
                0.75 + 0.25 * random.random()
            )

            logger.warning(
                f&#34;retry() failed once ({try_index}th try, maximum {max_retry} retries). Will delay {delay:.2f}s and retry. Error: {e}&#34;
            )
            traceback.print_exc()

            time.sleep(delay)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.round_up"><code class="name flex">
<span>def <span class="ident">round_up</span></span>(<span>x: int, y: int) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def round_up(x: int, y: int) -&gt; int:
    return ((x - 1) // y + 1) * y</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.set_cuda_arch"><code class="name flex">
<span>def <span class="ident">set_cuda_arch</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_cuda_arch():
    if is_flashinfer_available():
        capability = torch.cuda.get_device_capability()
        arch = f&#34;{capability[0]}.{capability[1]}&#34;
        os.environ[&#34;TORCH_CUDA_ARCH_LIST&#34;] = f&#34;{arch}{&#39;+PTX&#39; if arch == &#39;9.0&#39; else &#39;&#39;}&#34;</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.set_gpu_proc_affinity"><code class="name flex">
<span>def <span class="ident">set_gpu_proc_affinity</span></span>(<span>tp_size: int, nnodes: int, gpu_id: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_gpu_proc_affinity(
    tp_size: int,
    nnodes: int,
    gpu_id: int,
):
    # current process
    pid = os.getpid()
    p = psutil.Process(pid)

    tp_size_per_node = tp_size // nnodes

    # total physical cores
    total_pcores = psutil.cpu_count(logical=False)
    # physical cores per TP (N.B. more Cores than GPUs on node)
    num_cores_bind = total_pcores // tp_size_per_node

    # able to handle multiple DP per node
    start_cpu_id = (gpu_id * num_cores_bind) % total_pcores
    end_cpu_id = start_cpu_id + num_cores_bind

    if psutil.cpu_count() != psutil.cpu_count(logical=False):
        # HT on
        lower_cpu_ids = [id for id in range(start_cpu_id, end_cpu_id)]
        upper_cpu_ids = [id + total_pcores for id in range(start_cpu_id, end_cpu_id)]
        bind_cpu_ids = list(itertools.chain(lower_cpu_ids, upper_cpu_ids))
    else:
        # HT off
        bind_cpu_ids = [id for id in range(start_cpu_id, end_cpu_id)]

    # set cpu_affinity to current process
    p.cpu_affinity(bind_cpu_ids)
    logger.info(f&#34;Process {pid} gpu_id {gpu_id} is running on CPUs: {p.cpu_affinity()}&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.set_prometheus_multiproc_dir"><code class="name flex">
<span>def <span class="ident">set_prometheus_multiproc_dir</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_prometheus_multiproc_dir():
    # Set prometheus multiprocess directory
    # sglang uses prometheus multiprocess mode
    # we need to set this before importing prometheus_client
    # https://prometheus.github.io/client_python/multiprocess/
    global prometheus_multiproc_dir

    if &#34;PROMETHEUS_MULTIPROC_DIR&#34; in os.environ:
        logger.debug(&#34;User set PROMETHEUS_MULTIPROC_DIR detected.&#34;)
        prometheus_multiproc_dir = tempfile.TemporaryDirectory(
            dir=os.environ[&#34;PROMETHEUS_MULTIPROC_DIR&#34;]
        )
    else:
        prometheus_multiproc_dir = tempfile.TemporaryDirectory()
        os.environ[&#34;PROMETHEUS_MULTIPROC_DIR&#34;] = prometheus_multiproc_dir.name
    logger.debug(f&#34;PROMETHEUS_MULTIPROC_DIR: {os.environ[&#39;PROMETHEUS_MULTIPROC_DIR&#39;]}&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.set_random_seed"><code class="name flex">
<span>def <span class="ident">set_random_seed</span></span>(<span>seed: int) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_random_seed(seed: int) -&gt; None:
    &#34;&#34;&#34;Set the random seed for all libraries.&#34;&#34;&#34;
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)</code></pre>
</details>
<div class="desc"><p>Set the random seed for all libraries.</p></div>
</dd>
<dt id="sglang.srt.utils.set_ulimit"><code class="name flex">
<span>def <span class="ident">set_ulimit</span></span>(<span>target_soft_limit=65535)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_ulimit(target_soft_limit=65535):
    # number of open files
    resource_type = resource.RLIMIT_NOFILE
    current_soft, current_hard = resource.getrlimit(resource_type)

    if current_soft &lt; target_soft_limit:
        try:
            resource.setrlimit(resource_type, (target_soft_limit, current_hard))
        except ValueError as e:
            logger.warning(f&#34;Fail to set RLIMIT_NOFILE: {e}&#34;)

    # stack size
    resource_type = resource.RLIMIT_STACK
    current_soft, current_hard = resource.getrlimit(resource_type)
    target_soft_limit_stack_size = 1024 * target_soft_limit
    if current_soft &lt; target_soft_limit_stack_size:
        try:
            resource.setrlimit(
                resource_type, (target_soft_limit_stack_size, current_hard)
            )
        except ValueError as e:
            logger.warning(f&#34;Fail to set RLIMIT_STACK: {e}&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.set_uvicorn_logging_configs"><code class="name flex">
<span>def <span class="ident">set_uvicorn_logging_configs</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_uvicorn_logging_configs():
    from uvicorn.config import LOGGING_CONFIG

    LOGGING_CONFIG[&#34;formatters&#34;][&#34;default&#34;][
        &#34;fmt&#34;
    ] = &#34;[%(asctime)s] %(levelprefix)s %(message)s&#34;
    LOGGING_CONFIG[&#34;formatters&#34;][&#34;default&#34;][&#34;datefmt&#34;] = &#34;%Y-%m-%d %H:%M:%S&#34;
    LOGGING_CONFIG[&#34;formatters&#34;][&#34;access&#34;][
        &#34;fmt&#34;
    ] = &#39;[%(asctime)s] %(levelprefix)s %(client_addr)s - &#34;%(request_line)s&#34; %(status_code)s&#39;
    LOGGING_CONFIG[&#34;formatters&#34;][&#34;access&#34;][&#34;datefmt&#34;] = &#34;%Y-%m-%d %H:%M:%S&#34;</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.set_weight_attrs"><code class="name flex">
<span>def <span class="ident">set_weight_attrs</span></span>(<span>weight: torch.Tensor, weight_attrs: Optional[Dict[str, Any]])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_weight_attrs(
    weight: torch.Tensor,
    weight_attrs: Optional[Dict[str, Any]],
):
    &#34;&#34;&#34;Set attributes on a weight tensor.

    This method is used to set attributes on a weight tensor. This method
    will not overwrite existing attributes.

    Args:
        weight: The weight tensor.
        weight_attrs: A dictionary of attributes to set on the weight tensor.
    &#34;&#34;&#34;
    if weight_attrs is None:
        return
    for key, value in weight_attrs.items():
        assert not hasattr(weight, key), f&#34;Overwriting existing tensor attribute: {key}&#34;
        setattr(weight, key, value)</code></pre>
</details>
<div class="desc"><p>Set attributes on a weight tensor.</p>
<p>This method is used to set attributes on a weight tensor. This method
will not overwrite existing attributes.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>weight</code></strong></dt>
<dd>The weight tensor.</dd>
<dt><strong><code>weight_attrs</code></strong></dt>
<dd>A dictionary of attributes to set on the weight tensor.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.utils.support_triton"><code class="name flex">
<span>def <span class="ident">support_triton</span></span>(<span>backend: str) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def support_triton(backend: str) -&gt; bool:
    return backend not in [&#34;torch_native&#34;, &#34;intel_amx&#34;, &#34;ascend&#34;]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.supports_custom_op"><code class="name flex">
<span>def <span class="ident">supports_custom_op</span></span>(<span>) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def supports_custom_op() -&gt; bool:
    return hasattr(torch.library, &#34;custom_op&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.suppress_other_loggers"><code class="name flex">
<span>def <span class="ident">suppress_other_loggers</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def suppress_other_loggers():
    warnings.filterwarnings(
        &#34;ignore&#34;, category=UserWarning, message=&#34;The given NumPy array is not writable&#34;
    )

    try:
        from vllm.logger import logger as vllm_default_logger
    except ImportError:
        return

    vllm_default_logger.setLevel(logging.WARN)
    logging.getLogger(&#34;vllm.distributed.device_communicators.pynccl&#34;).setLevel(
        logging.WARN
    )
    logging.getLogger(&#34;vllm.distributed.device_communicators.shm_broadcast&#34;).setLevel(
        logging.WARN
    )
    logging.getLogger(&#34;vllm.config&#34;).setLevel(logging.ERROR)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.use_intel_amx_backend"><code class="name flex">
<span>def <span class="ident">use_intel_amx_backend</span></span>(<span>layer)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def use_intel_amx_backend(layer):
    return getattr(layer, &#34;use_intel_amx_backend&#34;, False)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.wait_port_available"><code class="name flex">
<span>def <span class="ident">wait_port_available</span></span>(<span>port: int, port_name: str, timeout_s: int = 30, raise_exception: bool = True) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_port_available(
    port: int, port_name: str, timeout_s: int = 30, raise_exception: bool = True
) -&gt; bool:
    for i in range(timeout_s):
        if is_port_available(port):
            return True

        if i &gt; 10 and i % 5 == 0:
            process = find_process_using_port(port)
            if process is None:
                logger.warning(
                    f&#34;The port {port} is in use, but we could not find the process that uses it.&#34;
                )

            pid = process.pid
            error_message = f&#34;{port_name} is used by a process already. {process.name()=}&#39; {process.cmdline()=} {process.status()=} {pid=}&#34;
            logger.info(
                f&#34;port {port} is in use. Waiting for {i} seconds for {port_name} to be available. {error_message}&#34;
            )
        time.sleep(0.1)

    if raise_exception:
        raise ValueError(
            f&#34;{port_name} at {port} is not available in {timeout_s} seconds. {error_message}&#34;
        )
    return False</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.utils.BumpAllocator"><code class="flex name class">
<span>class <span class="ident">BumpAllocator</span></span>
<span>(</span><span>buffer_size: int, dtype, device)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BumpAllocator:
    def __init__(self, buffer_size: int, dtype, device):
        self._buffer = torch.zeros((buffer_size,), dtype=dtype, device=device)
        self._pointer = 0

    def allocate(self, size: int):
        assert self._pointer + size &lt;= len(self._buffer)
        output = self._buffer[self._pointer : self._pointer + size]
        self._pointer += size
        return output</code></pre>
</details>
<div class="desc"></div>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.utils.BumpAllocator.allocate"><code class="name flex">
<span>def <span class="ident">allocate</span></span>(<span>self, size: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def allocate(self, size: int):
    assert self._pointer + size &lt;= len(self._buffer)
    output = self._buffer[self._pointer : self._pointer + size]
    self._pointer += size
    return output</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.utils.ConcurrentCounter"><code class="flex name class">
<span>class <span class="ident">ConcurrentCounter</span></span>
<span>(</span><span>initial: int = 0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConcurrentCounter:
    &#34;&#34;&#34;
    An asynchronous counter for managing concurrent tasks that need
    coordinated increments, decrements, and waiting until the count reaches zero.

    This class is useful for scenarios like tracking the number of in-flight tasks
    and waiting for them to complete.
    &#34;&#34;&#34;

    def __init__(self, initial: int = 0):
        &#34;&#34;&#34;
        Initialize the counter with an optional initial value.

        Args:
            initial (int): The initial value of the counter. Default is 0.
        &#34;&#34;&#34;
        self._count = initial
        self._condition = asyncio.Condition()

    def value(self) -&gt; int:
        &#34;&#34;&#34;
        Return the current value of the counter.

        Note:
            This method is not synchronized. It may return a stale value
            if other coroutines are concurrently modifying the counter.

        Returns:
            int: The current counter value.
        &#34;&#34;&#34;
        return self._count

    def __repr__(self) -&gt; str:
        &#34;&#34;&#34;Return an informative string representation of the counter.&#34;&#34;&#34;
        return f&#34;&lt;ConcurrentCounter value={self.value()}&gt;&#34;

    async def increment(self, n: int = 1, notify_all: bool = True):
        &#34;&#34;&#34;
        Atomically increment the counter by a given amount and notify all waiters.

        Args:
            n (int): The amount to increment the counter by. Default is 1.
            notify_all (bool): Whether to notify all waiters after incrementing. Default is True.
        &#34;&#34;&#34;
        async with self._condition:
            self._count += n
            if notify_all:
                self._condition.notify_all()

    async def decrement(self, n: int = 1, notify_all: bool = True):
        &#34;&#34;&#34;
        Atomically decrement the counter by a given amount and notify all waiters.

        Args:
            n (int): The amount to decrement the counter by. Default is 1.
            notify_all (bool): Whether to notify all waiters after decrementing. Default is True.
        &#34;&#34;&#34;
        async with self._condition:
            self._count -= n
            if notify_all:
                self._condition.notify_all()

    async def wait_for(self, condition: Callable[[int], bool]):
        &#34;&#34;&#34;
        Asynchronously wait until the counter satisfies a given condition.

        This suspends the calling coroutine without blocking the thread, allowing
        other tasks to run while waiting. When the condition is met, the coroutine resumes.

        Args:
            condition (Callable[[int], bool]): A function that takes the current counter value
                and returns True when the condition is satisfied.
        &#34;&#34;&#34;
        async with self._condition:
            await self._condition.wait_for(lambda: condition(self._count))

    async def wait_for_zero(self):
        &#34;&#34;&#34;
        Asynchronously wait until the counter reaches zero.

        This suspends the calling coroutine without blocking the thread, allowing
        other tasks to run while waiting. When the counter becomes zero, the coroutine resumes.
        &#34;&#34;&#34;
        await self.wait_for(lambda count: count == 0)</code></pre>
</details>
<div class="desc"><p>An asynchronous counter for managing concurrent tasks that need
coordinated increments, decrements, and waiting until the count reaches zero.</p>
<p>This class is useful for scenarios like tracking the number of in-flight tasks
and waiting for them to complete.</p>
<p>Initialize the counter with an optional initial value.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>initial</code></strong> :&ensp;<code>int</code></dt>
<dd>The initial value of the counter. Default is 0.</dd>
</dl></div>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.utils.ConcurrentCounter.decrement"><code class="name flex">
<span>async def <span class="ident">decrement</span></span>(<span>self, n: int = 1, notify_all: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def decrement(self, n: int = 1, notify_all: bool = True):
    &#34;&#34;&#34;
    Atomically decrement the counter by a given amount and notify all waiters.

    Args:
        n (int): The amount to decrement the counter by. Default is 1.
        notify_all (bool): Whether to notify all waiters after decrementing. Default is True.
    &#34;&#34;&#34;
    async with self._condition:
        self._count -= n
        if notify_all:
            self._condition.notify_all()</code></pre>
</details>
<div class="desc"><p>Atomically decrement the counter by a given amount and notify all waiters.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>The amount to decrement the counter by. Default is 1.</dd>
<dt><strong><code>notify_all</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to notify all waiters after decrementing. Default is True.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.utils.ConcurrentCounter.increment"><code class="name flex">
<span>async def <span class="ident">increment</span></span>(<span>self, n: int = 1, notify_all: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def increment(self, n: int = 1, notify_all: bool = True):
    &#34;&#34;&#34;
    Atomically increment the counter by a given amount and notify all waiters.

    Args:
        n (int): The amount to increment the counter by. Default is 1.
        notify_all (bool): Whether to notify all waiters after incrementing. Default is True.
    &#34;&#34;&#34;
    async with self._condition:
        self._count += n
        if notify_all:
            self._condition.notify_all()</code></pre>
</details>
<div class="desc"><p>Atomically increment the counter by a given amount and notify all waiters.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>The amount to increment the counter by. Default is 1.</dd>
<dt><strong><code>notify_all</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to notify all waiters after incrementing. Default is True.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.utils.ConcurrentCounter.value"><code class="name flex">
<span>def <span class="ident">value</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def value(self) -&gt; int:
    &#34;&#34;&#34;
    Return the current value of the counter.

    Note:
        This method is not synchronized. It may return a stale value
        if other coroutines are concurrently modifying the counter.

    Returns:
        int: The current counter value.
    &#34;&#34;&#34;
    return self._count</code></pre>
</details>
<div class="desc"><p>Return the current value of the counter.</p>
<h2 id="note">Note</h2>
<p>This method is not synchronized. It may return a stale value
if other coroutines are concurrently modifying the counter.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>The current counter value.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.utils.ConcurrentCounter.wait_for"><code class="name flex">
<span>async def <span class="ident">wait_for</span></span>(<span>self, condition: Callable[[int], bool])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def wait_for(self, condition: Callable[[int], bool]):
    &#34;&#34;&#34;
    Asynchronously wait until the counter satisfies a given condition.

    This suspends the calling coroutine without blocking the thread, allowing
    other tasks to run while waiting. When the condition is met, the coroutine resumes.

    Args:
        condition (Callable[[int], bool]): A function that takes the current counter value
            and returns True when the condition is satisfied.
    &#34;&#34;&#34;
    async with self._condition:
        await self._condition.wait_for(lambda: condition(self._count))</code></pre>
</details>
<div class="desc"><p>Asynchronously wait until the counter satisfies a given condition.</p>
<p>This suspends the calling coroutine without blocking the thread, allowing
other tasks to run while waiting. When the condition is met, the coroutine resumes.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>condition</code></strong> :&ensp;<code>Callable[[int], bool]</code></dt>
<dd>A function that takes the current counter value
and returns True when the condition is satisfied.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.utils.ConcurrentCounter.wait_for_zero"><code class="name flex">
<span>async def <span class="ident">wait_for_zero</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def wait_for_zero(self):
    &#34;&#34;&#34;
    Asynchronously wait until the counter reaches zero.

    This suspends the calling coroutine without blocking the thread, allowing
    other tasks to run while waiting. When the counter becomes zero, the coroutine resumes.
    &#34;&#34;&#34;
    await self.wait_for(lambda count: count == 0)</code></pre>
</details>
<div class="desc"><p>Asynchronously wait until the counter reaches zero.</p>
<p>This suspends the calling coroutine without blocking the thread, allowing
other tasks to run while waiting. When the counter becomes zero, the coroutine resumes.</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.utils.DynamicGradMode"><code class="flex name class">
<span>class <span class="ident">DynamicGradMode</span></span>
<span>(</span><span>mode=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DynamicGradMode(_DecoratorContextManager):
    &#34;&#34;&#34;
    A combination of torch.no_grad and torch.inference_mode,
    with their behavior controlled by an environment variable. Just refer to them.
    &#34;&#34;&#34;

    @staticmethod
    def set_inference_mode(mode: bool):
        if isinstance(mode, bool):
            global _ENABLE_TORCH_INFERENCE_MODE

            _ENABLE_TORCH_INFERENCE_MODE = mode
        else:
            logger.warning(&#34;mode is not a boolean object&#34;)

    def __init__(self, mode=True):
        if not torch._jit_internal.is_scripting():
            super().__init__()
        if _ENABLE_TORCH_INFERENCE_MODE:
            self.mode = mode
        else:
            self.prev = False

    def __new__(cls, mode_or_orig_func=True if _ENABLE_TORCH_INFERENCE_MODE else None):
        if mode_or_orig_func is None or isinstance(mode_or_orig_func, bool):
            return super().__new__(cls)
        return cls()(mode_or_orig_func)

    def __enter__(self) -&gt; None:
        if _ENABLE_TORCH_INFERENCE_MODE:
            self._inference_mode_context = torch._C._InferenceMode(self.mode)
            self._inference_mode_context.__enter__()
        else:
            self.prev = torch.is_grad_enabled()
            torch.set_grad_enabled(False)

    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -&gt; None:
        if _ENABLE_TORCH_INFERENCE_MODE:
            self._inference_mode_context.__exit__(exc_type, exc_value, traceback)
        else:
            torch.set_grad_enabled(self.prev)

    def clone(self) -&gt; &#34;DynamicGradMode&#34;:
        r&#34;&#34;&#34;
        Create a copy of this class
        &#34;&#34;&#34;
        if _ENABLE_TORCH_INFERENCE_MODE:
            return self.__class__(self.mode)
        else:
            return self.__class__()</code></pre>
</details>
<div class="desc"><p>A combination of torch.no_grad and torch.inference_mode,
with their behavior controlled by an environment variable. Just refer to them.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils._contextlib._DecoratorContextManager</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.utils.DynamicGradMode.set_inference_mode"><code class="name flex">
<span>def <span class="ident">set_inference_mode</span></span>(<span>mode: bool)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def set_inference_mode(mode: bool):
    if isinstance(mode, bool):
        global _ENABLE_TORCH_INFERENCE_MODE

        _ENABLE_TORCH_INFERENCE_MODE = mode
    else:
        logger.warning(&#34;mode is not a boolean object&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.utils.DynamicGradMode.clone"><code class="name flex">
<span>def <span class="ident">clone</span></span>(<span>self) ‑> <a title="sglang.srt.utils.DynamicGradMode" href="#sglang.srt.utils.DynamicGradMode">DynamicGradMode</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clone(self) -&gt; &#34;DynamicGradMode&#34;:
    r&#34;&#34;&#34;
    Create a copy of this class
    &#34;&#34;&#34;
    if _ENABLE_TORCH_INFERENCE_MODE:
        return self.__class__(self.mode)
    else:
        return self.__class__()</code></pre>
</details>
<div class="desc"><p>Create a copy of this class</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.utils.EmptyContextManager"><code class="flex name class">
<span>class <span class="ident">EmptyContextManager</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EmptyContextManager:
    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        pass</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.ImageData"><code class="flex name class">
<span>class <span class="ident">ImageData</span></span>
<span>(</span><span>url: str, detail: "Optional[Literal['auto', 'low', 'high']]" = 'auto')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class ImageData:
    url: str
    detail: Optional[Literal[&#34;auto&#34;, &#34;low&#34;, &#34;high&#34;]] = &#34;auto&#34;</code></pre>
</details>
<div class="desc"><p>ImageData(url: 'str', detail: "Optional[Literal['auto', 'low', 'high']]" = 'auto')</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.utils.ImageData.detail"><code class="name">var <span class="ident">detail</span> : Literal['auto', 'low', 'high'] | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.ImageData.url"><code class="name">var <span class="ident">url</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.utils.LayerFn"><code class="flex name class">
<span>class <span class="ident">LayerFn</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LayerFn(Protocol):

    def __call__(self, layer_id: int, prefix: str) -&gt; torch.nn.Module: ...</code></pre>
</details>
<div class="desc"><p>Base class for protocol classes.</p>
<p>Protocol classes are defined as::</p>
<pre><code>class Proto(Protocol):
    def meth(self) -&gt; int:
        ...
</code></pre>
<p>Such classes are primarily used with static type checkers that recognize
structural subtyping (static duck-typing).</p>
<p>For example::</p>
<pre><code>class C:
    def meth(self) -&gt; int:
        return 0

def func(x: Proto) -&gt; int:
    return x.meth()

func(C())  # Passes static type check
</code></pre>
<p>See PEP 544 for details. Protocol classes decorated with
@typing.runtime_checkable act as simple-minded runtime protocols that check
only the presence of given attributes, ignoring their type signatures.
Protocol classes can be generic, they are defined as::</p>
<pre><code>class GenProto[T](Protocol):
    def meth(self) -&gt; T:
        ...
</code></pre></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>typing.Protocol</li>
<li>typing.Generic</li>
</ul>
</dd>
<dt id="sglang.srt.utils.LazyValue"><code class="flex name class">
<span>class <span class="ident">LazyValue</span></span>
<span>(</span><span>creator: Callable)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LazyValue:
    def __init__(self, creator: Callable):
        self._creator = creator
        self._value = None

    @property
    def value(self):
        if self._creator is not None:
            self._value = self._creator()
            self._creator = None
        return self._value</code></pre>
</details>
<div class="desc"></div>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.utils.LazyValue.value"><code class="name">prop <span class="ident">value</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def value(self):
    if self._creator is not None:
        self._value = self._creator()
        self._creator = None
    return self._value</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.utils.MultiprocessingSerializer"><code class="flex name class">
<span>class <span class="ident">MultiprocessingSerializer</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiprocessingSerializer:
    @staticmethod
    def serialize(obj, output_str: bool = False):
        &#34;&#34;&#34;
        Serialize a Python object using ForkingPickler.

        Args:
            obj: The object to serialize.
            output_str (bool): If True, return a base64-encoded string instead of raw bytes.

        Returns:
            bytes or str: The serialized object.
        &#34;&#34;&#34;
        buf = io.BytesIO()
        ForkingPickler(buf).dump(obj)
        buf.seek(0)
        output = buf.read()

        if output_str:
            # Convert bytes to base64-encoded string
            output = pybase64.b64encode(output).decode(&#34;utf-8&#34;)

        return output

    @staticmethod
    def deserialize(data):
        &#34;&#34;&#34;
        Deserialize a previously serialized object.

        Args:
            data (bytes or str): The serialized data, optionally base64-encoded.

        Returns:
            The deserialized Python object.
        &#34;&#34;&#34;
        if isinstance(data, str):
            # Decode base64 string to bytes
            data = pybase64.b64decode(data, validate=True)

        return ForkingPickler.loads(data)</code></pre>
</details>
<div class="desc"></div>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.utils.MultiprocessingSerializer.deserialize"><code class="name flex">
<span>def <span class="ident">deserialize</span></span>(<span>data)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def deserialize(data):
    &#34;&#34;&#34;
    Deserialize a previously serialized object.

    Args:
        data (bytes or str): The serialized data, optionally base64-encoded.

    Returns:
        The deserialized Python object.
    &#34;&#34;&#34;
    if isinstance(data, str):
        # Decode base64 string to bytes
        data = pybase64.b64decode(data, validate=True)

    return ForkingPickler.loads(data)</code></pre>
</details>
<div class="desc"><p>Deserialize a previously serialized object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>bytes</code> or <code>str</code></dt>
<dd>The serialized data, optionally base64-encoded.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>The deserialized Python object.</p></div>
</dd>
<dt id="sglang.srt.utils.MultiprocessingSerializer.serialize"><code class="name flex">
<span>def <span class="ident">serialize</span></span>(<span>obj, output_str: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def serialize(obj, output_str: bool = False):
    &#34;&#34;&#34;
    Serialize a Python object using ForkingPickler.

    Args:
        obj: The object to serialize.
        output_str (bool): If True, return a base64-encoded string instead of raw bytes.

    Returns:
        bytes or str: The serialized object.
    &#34;&#34;&#34;
    buf = io.BytesIO()
    ForkingPickler(buf).dump(obj)
    buf.seek(0)
    output = buf.read()

    if output_str:
        # Convert bytes to base64-encoded string
        output = pybase64.b64encode(output).decode(&#34;utf-8&#34;)

    return output</code></pre>
</details>
<div class="desc"><p>Serialize a Python object using ForkingPickler.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd>The object to serialize.</dd>
<dt><strong><code>output_str</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, return a base64-encoded string instead of raw bytes.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bytes</code> or <code>str</code></dt>
<dd>The serialized object.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.utils.PackWeightMethod"><code class="flex name class">
<span>class <span class="ident">PackWeightMethod</span></span>
<span>(</span><span>weight_names, transpose_dims=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PackWeightMethod:
    def __init__(self, weight_names, transpose_dims=None):
        self.weight_names = weight_names
        self.transpose_dims = transpose_dims

    def process_weights_after_loading(self, module) -&gt; None:
        _process_weight_after_loading(module, self.weight_names, self.transpose_dims)</code></pre>
</details>
<div class="desc"></div>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.utils.PackWeightMethod.process_weights_after_loading"><code class="name flex">
<span>def <span class="ident">process_weights_after_loading</span></span>(<span>self, module) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_weights_after_loading(self, module) -&gt; None:
    _process_weight_after_loading(module, self.weight_names, self.transpose_dims)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.utils.TimeInfo"><code class="flex name class">
<span>class <span class="ident">TimeInfo</span></span>
<span>(</span><span>name, interval=0.1, color=0, indent=0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TimeInfo:
    def __init__(self, name, interval=0.1, color=0, indent=0):
        self.name = name
        self.interval = interval
        self.color = color
        self.indent = indent

        self.acc_time = 0
        self.last_acc_time = 0

    def check(self):
        if self.acc_time - self.last_acc_time &gt; self.interval:
            self.last_acc_time = self.acc_time
            return True
        return False

    def pretty_print(self):
        print(f&#34;\x1b[{self.color}m&#34;, end=&#34;&#34;)
        print(&#34;-&#34; * self.indent * 2, end=&#34;&#34;)
        print(f&#34;{self.name}: {self.acc_time:.3f}s\x1b[0m&#34;)</code></pre>
</details>
<div class="desc"></div>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.utils.TimeInfo.check"><code class="name flex">
<span>def <span class="ident">check</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check(self):
    if self.acc_time - self.last_acc_time &gt; self.interval:
        self.last_acc_time = self.acc_time
        return True
    return False</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.utils.TimeInfo.pretty_print"><code class="name flex">
<span>def <span class="ident">pretty_print</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pretty_print(self):
    print(f&#34;\x1b[{self.color}m&#34;, end=&#34;&#34;)
    print(&#34;-&#34; * self.indent * 2, end=&#34;&#34;)
    print(f&#34;{self.name}: {self.acc_time:.3f}s\x1b[0m&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.utils.Withable"><code class="flex name class">
<span>class <span class="ident">Withable</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Withable(Generic[T]):
    def __init__(self):
        self._value: Optional[T] = None

    @property
    def value(self) -&gt; T:
        return self._value

    @contextmanager
    def with_value(self, new_value: T):
        assert self._value is None
        self._value = new_value
        try:
            yield
        finally:
            assert self._value is new_value
            self._value = None</code></pre>
</details>
<div class="desc"><p>Abstract base class for generic types.</p>
<p>On Python 3.12 and newer, generic classes implicitly inherit from
Generic when they declare a parameter list after the class's name::</p>
<pre><code>class Mapping[KT, VT]:
    def __getitem__(self, key: KT) -&gt; VT:
        ...
    # Etc.
</code></pre>
<p>On older versions of Python, however, generic classes have to
explicitly inherit from Generic.</p>
<p>After a class has been declared to be generic, it can then be used as
follows::</p>
<pre><code>def lookup_name[KT, VT](mapping: Mapping[KT, VT], key: KT, default: VT) -&gt; VT:
    try:
        return mapping[key]
    except KeyError:
        return default
</code></pre></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>typing.Generic</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.utils.Withable.value"><code class="name">prop <span class="ident">value</span> : T</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def value(self) -&gt; T:
    return self._value</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.utils.Withable.with_value"><code class="name flex">
<span>def <span class="ident">with_value</span></span>(<span>self, new_value: T)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def with_value(self, new_value: T):
    assert self._value is None
    self._value = new_value
    try:
        yield
    finally:
        assert self._value is new_value
        self._value = None</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt" href="index.html">sglang.srt</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.utils.add_api_key_middleware" href="#sglang.srt.utils.add_api_key_middleware">add_api_key_middleware</a></code></li>
<li><code><a title="sglang.srt.utils.add_prefix" href="#sglang.srt.utils.add_prefix">add_prefix</a></code></li>
<li><code><a title="sglang.srt.utils.add_prometheus_middleware" href="#sglang.srt.utils.add_prometheus_middleware">add_prometheus_middleware</a></code></li>
<li><code><a title="sglang.srt.utils.align" href="#sglang.srt.utils.align">align</a></code></li>
<li><code><a title="sglang.srt.utils.apply_module_patch" href="#sglang.srt.utils.apply_module_patch">apply_module_patch</a></code></li>
<li><code><a title="sglang.srt.utils.assert_pkg_version" href="#sglang.srt.utils.assert_pkg_version">assert_pkg_version</a></code></li>
<li><code><a title="sglang.srt.utils.bind_or_assign" href="#sglang.srt.utils.bind_or_assign">bind_or_assign</a></code></li>
<li><code><a title="sglang.srt.utils.bind_port" href="#sglang.srt.utils.bind_port">bind_port</a></code></li>
<li><code><a title="sglang.srt.utils.broadcast_pyobj" href="#sglang.srt.utils.broadcast_pyobj">broadcast_pyobj</a></code></li>
<li><code><a title="sglang.srt.utils.calculate_time" href="#sglang.srt.utils.calculate_time">calculate_time</a></code></li>
<li><code><a title="sglang.srt.utils.ceil_div" href="#sglang.srt.utils.ceil_div">ceil_div</a></code></li>
<li><code><a title="sglang.srt.utils.check_cuda_result" href="#sglang.srt.utils.check_cuda_result">check_cuda_result</a></code></li>
<li><code><a title="sglang.srt.utils.configure_gc_logger" href="#sglang.srt.utils.configure_gc_logger">configure_gc_logger</a></code></li>
<li><code><a title="sglang.srt.utils.configure_gc_warning" href="#sglang.srt.utils.configure_gc_warning">configure_gc_warning</a></code></li>
<li><code><a title="sglang.srt.utils.configure_ipv6" href="#sglang.srt.utils.configure_ipv6">configure_ipv6</a></code></li>
<li><code><a title="sglang.srt.utils.configure_logger" href="#sglang.srt.utils.configure_logger">configure_logger</a></code></li>
<li><code><a title="sglang.srt.utils.cpu_has_amx_support" href="#sglang.srt.utils.cpu_has_amx_support">cpu_has_amx_support</a></code></li>
<li><code><a title="sglang.srt.utils.crash_on_warnings" href="#sglang.srt.utils.crash_on_warnings">crash_on_warnings</a></code></li>
<li><code><a title="sglang.srt.utils.create_checksum" href="#sglang.srt.utils.create_checksum">create_checksum</a></code></li>
<li><code><a title="sglang.srt.utils.dataclass_to_string_truncated" href="#sglang.srt.utils.dataclass_to_string_truncated">dataclass_to_string_truncated</a></code></li>
<li><code><a title="sglang.srt.utils.debug_timing" href="#sglang.srt.utils.debug_timing">debug_timing</a></code></li>
<li><code><a title="sglang.srt.utils.decode_video_base64" href="#sglang.srt.utils.decode_video_base64">decode_video_base64</a></code></li>
<li><code><a title="sglang.srt.utils.delete_directory" href="#sglang.srt.utils.delete_directory">delete_directory</a></code></li>
<li><code><a title="sglang.srt.utils.dim_is_supported" href="#sglang.srt.utils.dim_is_supported">dim_is_supported</a></code></li>
<li><code><a title="sglang.srt.utils.direct_register_custom_op" href="#sglang.srt.utils.direct_register_custom_op">direct_register_custom_op</a></code></li>
<li><code><a title="sglang.srt.utils.disable_request_logging" href="#sglang.srt.utils.disable_request_logging">disable_request_logging</a></code></li>
<li><code><a title="sglang.srt.utils.dispose_tensor" href="#sglang.srt.utils.dispose_tensor">dispose_tensor</a></code></li>
<li><code><a title="sglang.srt.utils.dump_to_file" href="#sglang.srt.utils.dump_to_file">dump_to_file</a></code></li>
<li><code><a title="sglang.srt.utils.dynamic_import" href="#sglang.srt.utils.dynamic_import">dynamic_import</a></code></li>
<li><code><a title="sglang.srt.utils.empty_context" href="#sglang.srt.utils.empty_context">empty_context</a></code></li>
<li><code><a title="sglang.srt.utils.enable_show_time_cost" href="#sglang.srt.utils.enable_show_time_cost">enable_show_time_cost</a></code></li>
<li><code><a title="sglang.srt.utils.fast_topk" href="#sglang.srt.utils.fast_topk">fast_topk</a></code></li>
<li><code><a title="sglang.srt.utils.find_local_repo_dir" href="#sglang.srt.utils.find_local_repo_dir">find_local_repo_dir</a></code></li>
<li><code><a title="sglang.srt.utils.find_process_using_port" href="#sglang.srt.utils.find_process_using_port">find_process_using_port</a></code></li>
<li><code><a title="sglang.srt.utils.flatten_nested_list" href="#sglang.srt.utils.flatten_nested_list">flatten_nested_list</a></code></li>
<li><code><a title="sglang.srt.utils.format_tcp_address" href="#sglang.srt.utils.format_tcp_address">format_tcp_address</a></code></li>
<li><code><a title="sglang.srt.utils.freeze_gc" href="#sglang.srt.utils.freeze_gc">freeze_gc</a></code></li>
<li><code><a title="sglang.srt.utils.gc_object_counts" href="#sglang.srt.utils.gc_object_counts">gc_object_counts</a></code></li>
<li><code><a title="sglang.srt.utils.get_amdgpu_memory_capacity" href="#sglang.srt.utils.get_amdgpu_memory_capacity">get_amdgpu_memory_capacity</a></code></li>
<li><code><a title="sglang.srt.utils.get_available_gpu_memory" href="#sglang.srt.utils.get_available_gpu_memory">get_available_gpu_memory</a></code></li>
<li><code><a title="sglang.srt.utils.get_bool_env_var" href="#sglang.srt.utils.get_bool_env_var">get_bool_env_var</a></code></li>
<li><code><a title="sglang.srt.utils.get_compiler_backend" href="#sglang.srt.utils.get_compiler_backend">get_compiler_backend</a></code></li>
<li><code><a title="sglang.srt.utils.get_cpu_ids_by_node" href="#sglang.srt.utils.get_cpu_ids_by_node">get_cpu_ids_by_node</a></code></li>
<li><code><a title="sglang.srt.utils.get_cuda_version" href="#sglang.srt.utils.get_cuda_version">get_cuda_version</a></code></li>
<li><code><a title="sglang.srt.utils.get_device" href="#sglang.srt.utils.get_device">get_device</a></code></li>
<li><code><a title="sglang.srt.utils.get_device_capability" href="#sglang.srt.utils.get_device_capability">get_device_capability</a></code></li>
<li><code><a title="sglang.srt.utils.get_device_core_count" href="#sglang.srt.utils.get_device_core_count">get_device_core_count</a></code></li>
<li><code><a title="sglang.srt.utils.get_device_count" href="#sglang.srt.utils.get_device_count">get_device_count</a></code></li>
<li><code><a title="sglang.srt.utils.get_device_memory_capacity" href="#sglang.srt.utils.get_device_memory_capacity">get_device_memory_capacity</a></code></li>
<li><code><a title="sglang.srt.utils.get_device_name" href="#sglang.srt.utils.get_device_name">get_device_name</a></code></li>
<li><code><a title="sglang.srt.utils.get_device_sm" href="#sglang.srt.utils.get_device_sm">get_device_sm</a></code></li>
<li><code><a title="sglang.srt.utils.get_free_port" href="#sglang.srt.utils.get_free_port">get_free_port</a></code></li>
<li><code><a title="sglang.srt.utils.get_hpu_memory_capacity" href="#sglang.srt.utils.get_hpu_memory_capacity">get_hpu_memory_capacity</a></code></li>
<li><code><a title="sglang.srt.utils.get_int_env_var" href="#sglang.srt.utils.get_int_env_var">get_int_env_var</a></code></li>
<li><code><a title="sglang.srt.utils.get_ip" href="#sglang.srt.utils.get_ip">get_ip</a></code></li>
<li><code><a title="sglang.srt.utils.get_local_ip_auto" href="#sglang.srt.utils.get_local_ip_auto">get_local_ip_auto</a></code></li>
<li><code><a title="sglang.srt.utils.get_local_ip_by_nic" href="#sglang.srt.utils.get_local_ip_by_nic">get_local_ip_by_nic</a></code></li>
<li><code><a title="sglang.srt.utils.get_local_ip_by_remote" href="#sglang.srt.utils.get_local_ip_by_remote">get_local_ip_by_remote</a></code></li>
<li><code><a title="sglang.srt.utils.get_npu_compiler_config" href="#sglang.srt.utils.get_npu_compiler_config">get_npu_compiler_config</a></code></li>
<li><code><a title="sglang.srt.utils.get_npu_memory_capacity" href="#sglang.srt.utils.get_npu_memory_capacity">get_npu_memory_capacity</a></code></li>
<li><code><a title="sglang.srt.utils.get_nvgpu_memory_capacity" href="#sglang.srt.utils.get_nvgpu_memory_capacity">get_nvgpu_memory_capacity</a></code></li>
<li><code><a title="sglang.srt.utils.get_open_port" href="#sglang.srt.utils.get_open_port">get_open_port</a></code></li>
<li><code><a title="sglang.srt.utils.get_physical_cpus_by_numa" href="#sglang.srt.utils.get_physical_cpus_by_numa">get_physical_cpus_by_numa</a></code></li>
<li><code><a title="sglang.srt.utils.get_zmq_socket" href="#sglang.srt.utils.get_zmq_socket">get_zmq_socket</a></code></li>
<li><code><a title="sglang.srt.utils.init_custom_process_group" href="#sglang.srt.utils.init_custom_process_group">init_custom_process_group</a></code></li>
<li><code><a title="sglang.srt.utils.is_ampere_with_cuda_12_3" href="#sglang.srt.utils.is_ampere_with_cuda_12_3">is_ampere_with_cuda_12_3</a></code></li>
<li><code><a title="sglang.srt.utils.is_blackwell" href="#sglang.srt.utils.is_blackwell">is_blackwell</a></code></li>
<li><code><a title="sglang.srt.utils.is_cpu" href="#sglang.srt.utils.is_cpu">is_cpu</a></code></li>
<li><code><a title="sglang.srt.utils.is_cuda" href="#sglang.srt.utils.is_cuda">is_cuda</a></code></li>
<li><code><a title="sglang.srt.utils.is_cuda_alike" href="#sglang.srt.utils.is_cuda_alike">is_cuda_alike</a></code></li>
<li><code><a title="sglang.srt.utils.is_fa3_default_architecture" href="#sglang.srt.utils.is_fa3_default_architecture">is_fa3_default_architecture</a></code></li>
<li><code><a title="sglang.srt.utils.is_flashinfer_available" href="#sglang.srt.utils.is_flashinfer_available">is_flashinfer_available</a></code></li>
<li><code><a title="sglang.srt.utils.is_habana_available" href="#sglang.srt.utils.is_habana_available">is_habana_available</a></code></li>
<li><code><a title="sglang.srt.utils.is_hip" href="#sglang.srt.utils.is_hip">is_hip</a></code></li>
<li><code><a title="sglang.srt.utils.is_hopper_with_cuda_12_3" href="#sglang.srt.utils.is_hopper_with_cuda_12_3">is_hopper_with_cuda_12_3</a></code></li>
<li><code><a title="sglang.srt.utils.is_host_cpu_x86" href="#sglang.srt.utils.is_host_cpu_x86">is_host_cpu_x86</a></code></li>
<li><code><a title="sglang.srt.utils.is_hpu" href="#sglang.srt.utils.is_hpu">is_hpu</a></code></li>
<li><code><a title="sglang.srt.utils.is_no_spec_infer_or_topk_one" href="#sglang.srt.utils.is_no_spec_infer_or_topk_one">is_no_spec_infer_or_topk_one</a></code></li>
<li><code><a title="sglang.srt.utils.is_non_idle_and_non_empty" href="#sglang.srt.utils.is_non_idle_and_non_empty">is_non_idle_and_non_empty</a></code></li>
<li><code><a title="sglang.srt.utils.is_npu" href="#sglang.srt.utils.is_npu">is_npu</a></code></li>
<li><code><a title="sglang.srt.utils.is_page_size_one" href="#sglang.srt.utils.is_page_size_one">is_page_size_one</a></code></li>
<li><code><a title="sglang.srt.utils.is_pin_memory_available" href="#sglang.srt.utils.is_pin_memory_available">is_pin_memory_available</a></code></li>
<li><code><a title="sglang.srt.utils.is_port_available" href="#sglang.srt.utils.is_port_available">is_port_available</a></code></li>
<li><code><a title="sglang.srt.utils.is_remote_url" href="#sglang.srt.utils.is_remote_url">is_remote_url</a></code></li>
<li><code><a title="sglang.srt.utils.is_shm_available" href="#sglang.srt.utils.is_shm_available">is_shm_available</a></code></li>
<li><code><a title="sglang.srt.utils.is_sm100_supported" href="#sglang.srt.utils.is_sm100_supported">is_sm100_supported</a></code></li>
<li><code><a title="sglang.srt.utils.is_sm90_supported" href="#sglang.srt.utils.is_sm90_supported">is_sm90_supported</a></code></li>
<li><code><a title="sglang.srt.utils.is_triton_3" href="#sglang.srt.utils.is_triton_3">is_triton_3</a></code></li>
<li><code><a title="sglang.srt.utils.is_triton_kernels_available" href="#sglang.srt.utils.is_triton_kernels_available">is_triton_kernels_available</a></code></li>
<li><code><a title="sglang.srt.utils.is_valid_ipv6_address" href="#sglang.srt.utils.is_valid_ipv6_address">is_valid_ipv6_address</a></code></li>
<li><code><a title="sglang.srt.utils.is_xpu" href="#sglang.srt.utils.is_xpu">is_xpu</a></code></li>
<li><code><a title="sglang.srt.utils.kill_itself_when_parent_died" href="#sglang.srt.utils.kill_itself_when_parent_died">kill_itself_when_parent_died</a></code></li>
<li><code><a title="sglang.srt.utils.kill_process_tree" href="#sglang.srt.utils.kill_process_tree">kill_process_tree</a></code></li>
<li><code><a title="sglang.srt.utils.launch_dummy_health_check_server" href="#sglang.srt.utils.launch_dummy_health_check_server">launch_dummy_health_check_server</a></code></li>
<li><code><a title="sglang.srt.utils.load_audio" href="#sglang.srt.utils.load_audio">load_audio</a></code></li>
<li><code><a title="sglang.srt.utils.load_image" href="#sglang.srt.utils.load_image">load_image</a></code></li>
<li><code><a title="sglang.srt.utils.load_json_config" href="#sglang.srt.utils.load_json_config">load_json_config</a></code></li>
<li><code><a title="sglang.srt.utils.load_video" href="#sglang.srt.utils.load_video">load_video</a></code></li>
<li><code><a title="sglang.srt.utils.log_info_on_rank0" href="#sglang.srt.utils.log_info_on_rank0">log_info_on_rank0</a></code></li>
<li><code><a title="sglang.srt.utils.lru_cache_frozenset" href="#sglang.srt.utils.lru_cache_frozenset">lru_cache_frozenset</a></code></li>
<li><code><a title="sglang.srt.utils.make_layers" href="#sglang.srt.utils.make_layers">make_layers</a></code></li>
<li><code><a title="sglang.srt.utils.mark_end" href="#sglang.srt.utils.mark_end">mark_end</a></code></li>
<li><code><a title="sglang.srt.utils.mark_start" href="#sglang.srt.utils.mark_start">mark_start</a></code></li>
<li><code><a title="sglang.srt.utils.maybe_torch_compile" href="#sglang.srt.utils.maybe_torch_compile">maybe_torch_compile</a></code></li>
<li><code><a title="sglang.srt.utils.maybe_wrap_ipv6_address" href="#sglang.srt.utils.maybe_wrap_ipv6_address">maybe_wrap_ipv6_address</a></code></li>
<li><code><a title="sglang.srt.utils.monkey_patch_p2p_access_check" href="#sglang.srt.utils.monkey_patch_p2p_access_check">monkey_patch_p2p_access_check</a></code></li>
<li><code><a title="sglang.srt.utils.monkey_patch_vllm_gguf_config" href="#sglang.srt.utils.monkey_patch_vllm_gguf_config">monkey_patch_vllm_gguf_config</a></code></li>
<li><code><a title="sglang.srt.utils.mxfp_supported" href="#sglang.srt.utils.mxfp_supported">mxfp_supported</a></code></li>
<li><code><a title="sglang.srt.utils.next_power_of_2" href="#sglang.srt.utils.next_power_of_2">next_power_of_2</a></code></li>
<li><code><a title="sglang.srt.utils.nullable_str" href="#sglang.srt.utils.nullable_str">nullable_str</a></code></li>
<li><code><a title="sglang.srt.utils.parse_connector_type" href="#sglang.srt.utils.parse_connector_type">parse_connector_type</a></code></li>
<li><code><a title="sglang.srt.utils.parse_lscpu_topology" href="#sglang.srt.utils.parse_lscpu_topology">parse_lscpu_topology</a></code></li>
<li><code><a title="sglang.srt.utils.parse_module_path" href="#sglang.srt.utils.parse_module_path">parse_module_path</a></code></li>
<li><code><a title="sglang.srt.utils.permute_weight" href="#sglang.srt.utils.permute_weight">permute_weight</a></code></li>
<li><code><a title="sglang.srt.utils.point_to_point_pyobj" href="#sglang.srt.utils.point_to_point_pyobj">point_to_point_pyobj</a></code></li>
<li><code><a title="sglang.srt.utils.prepack_weight_if_needed" href="#sglang.srt.utils.prepack_weight_if_needed">prepack_weight_if_needed</a></code></li>
<li><code><a title="sglang.srt.utils.prepare_model_and_tokenizer" href="#sglang.srt.utils.prepare_model_and_tokenizer">prepare_model_and_tokenizer</a></code></li>
<li><code><a title="sglang.srt.utils.print_info_once" href="#sglang.srt.utils.print_info_once">print_info_once</a></code></li>
<li><code><a title="sglang.srt.utils.print_warning_once" href="#sglang.srt.utils.print_warning_once">print_warning_once</a></code></li>
<li><code><a title="sglang.srt.utils.pyspy_dump_schedulers" href="#sglang.srt.utils.pyspy_dump_schedulers">pyspy_dump_schedulers</a></code></li>
<li><code><a title="sglang.srt.utils.pytorch_profile" href="#sglang.srt.utils.pytorch_profile">pytorch_profile</a></code></li>
<li><code><a title="sglang.srt.utils.random_uuid" href="#sglang.srt.utils.random_uuid">random_uuid</a></code></li>
<li><code><a title="sglang.srt.utils.read_system_prompt_from_file" href="#sglang.srt.utils.read_system_prompt_from_file">read_system_prompt_from_file</a></code></li>
<li><code><a title="sglang.srt.utils.replace_submodule" href="#sglang.srt.utils.replace_submodule">replace_submodule</a></code></li>
<li><code><a title="sglang.srt.utils.require_attn_tp_gather" href="#sglang.srt.utils.require_attn_tp_gather">require_attn_tp_gather</a></code></li>
<li><code><a title="sglang.srt.utils.require_gathered_buffer" href="#sglang.srt.utils.require_gathered_buffer">require_gathered_buffer</a></code></li>
<li><code><a title="sglang.srt.utils.require_mlp_sync" href="#sglang.srt.utils.require_mlp_sync">require_mlp_sync</a></code></li>
<li><code><a title="sglang.srt.utils.require_mlp_tp_gather" href="#sglang.srt.utils.require_mlp_tp_gather">require_mlp_tp_gather</a></code></li>
<li><code><a title="sglang.srt.utils.retry" href="#sglang.srt.utils.retry">retry</a></code></li>
<li><code><a title="sglang.srt.utils.round_up" href="#sglang.srt.utils.round_up">round_up</a></code></li>
<li><code><a title="sglang.srt.utils.set_cuda_arch" href="#sglang.srt.utils.set_cuda_arch">set_cuda_arch</a></code></li>
<li><code><a title="sglang.srt.utils.set_gpu_proc_affinity" href="#sglang.srt.utils.set_gpu_proc_affinity">set_gpu_proc_affinity</a></code></li>
<li><code><a title="sglang.srt.utils.set_prometheus_multiproc_dir" href="#sglang.srt.utils.set_prometheus_multiproc_dir">set_prometheus_multiproc_dir</a></code></li>
<li><code><a title="sglang.srt.utils.set_random_seed" href="#sglang.srt.utils.set_random_seed">set_random_seed</a></code></li>
<li><code><a title="sglang.srt.utils.set_ulimit" href="#sglang.srt.utils.set_ulimit">set_ulimit</a></code></li>
<li><code><a title="sglang.srt.utils.set_uvicorn_logging_configs" href="#sglang.srt.utils.set_uvicorn_logging_configs">set_uvicorn_logging_configs</a></code></li>
<li><code><a title="sglang.srt.utils.set_weight_attrs" href="#sglang.srt.utils.set_weight_attrs">set_weight_attrs</a></code></li>
<li><code><a title="sglang.srt.utils.support_triton" href="#sglang.srt.utils.support_triton">support_triton</a></code></li>
<li><code><a title="sglang.srt.utils.supports_custom_op" href="#sglang.srt.utils.supports_custom_op">supports_custom_op</a></code></li>
<li><code><a title="sglang.srt.utils.suppress_other_loggers" href="#sglang.srt.utils.suppress_other_loggers">suppress_other_loggers</a></code></li>
<li><code><a title="sglang.srt.utils.use_intel_amx_backend" href="#sglang.srt.utils.use_intel_amx_backend">use_intel_amx_backend</a></code></li>
<li><code><a title="sglang.srt.utils.wait_port_available" href="#sglang.srt.utils.wait_port_available">wait_port_available</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.utils.BumpAllocator" href="#sglang.srt.utils.BumpAllocator">BumpAllocator</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.utils.BumpAllocator.allocate" href="#sglang.srt.utils.BumpAllocator.allocate">allocate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.utils.ConcurrentCounter" href="#sglang.srt.utils.ConcurrentCounter">ConcurrentCounter</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.utils.ConcurrentCounter.decrement" href="#sglang.srt.utils.ConcurrentCounter.decrement">decrement</a></code></li>
<li><code><a title="sglang.srt.utils.ConcurrentCounter.increment" href="#sglang.srt.utils.ConcurrentCounter.increment">increment</a></code></li>
<li><code><a title="sglang.srt.utils.ConcurrentCounter.value" href="#sglang.srt.utils.ConcurrentCounter.value">value</a></code></li>
<li><code><a title="sglang.srt.utils.ConcurrentCounter.wait_for" href="#sglang.srt.utils.ConcurrentCounter.wait_for">wait_for</a></code></li>
<li><code><a title="sglang.srt.utils.ConcurrentCounter.wait_for_zero" href="#sglang.srt.utils.ConcurrentCounter.wait_for_zero">wait_for_zero</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.utils.DynamicGradMode" href="#sglang.srt.utils.DynamicGradMode">DynamicGradMode</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.utils.DynamicGradMode.clone" href="#sglang.srt.utils.DynamicGradMode.clone">clone</a></code></li>
<li><code><a title="sglang.srt.utils.DynamicGradMode.set_inference_mode" href="#sglang.srt.utils.DynamicGradMode.set_inference_mode">set_inference_mode</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.utils.EmptyContextManager" href="#sglang.srt.utils.EmptyContextManager">EmptyContextManager</a></code></h4>
</li>
<li>
<h4><code><a title="sglang.srt.utils.ImageData" href="#sglang.srt.utils.ImageData">ImageData</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.utils.ImageData.detail" href="#sglang.srt.utils.ImageData.detail">detail</a></code></li>
<li><code><a title="sglang.srt.utils.ImageData.url" href="#sglang.srt.utils.ImageData.url">url</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.utils.LayerFn" href="#sglang.srt.utils.LayerFn">LayerFn</a></code></h4>
</li>
<li>
<h4><code><a title="sglang.srt.utils.LazyValue" href="#sglang.srt.utils.LazyValue">LazyValue</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.utils.LazyValue.value" href="#sglang.srt.utils.LazyValue.value">value</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.utils.MultiprocessingSerializer" href="#sglang.srt.utils.MultiprocessingSerializer">MultiprocessingSerializer</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.utils.MultiprocessingSerializer.deserialize" href="#sglang.srt.utils.MultiprocessingSerializer.deserialize">deserialize</a></code></li>
<li><code><a title="sglang.srt.utils.MultiprocessingSerializer.serialize" href="#sglang.srt.utils.MultiprocessingSerializer.serialize">serialize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.utils.PackWeightMethod" href="#sglang.srt.utils.PackWeightMethod">PackWeightMethod</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.utils.PackWeightMethod.process_weights_after_loading" href="#sglang.srt.utils.PackWeightMethod.process_weights_after_loading">process_weights_after_loading</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.utils.TimeInfo" href="#sglang.srt.utils.TimeInfo">TimeInfo</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.utils.TimeInfo.check" href="#sglang.srt.utils.TimeInfo.check">check</a></code></li>
<li><code><a title="sglang.srt.utils.TimeInfo.pretty_print" href="#sglang.srt.utils.TimeInfo.pretty_print">pretty_print</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.utils.Withable" href="#sglang.srt.utils.Withable">Withable</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.utils.Withable.value" href="#sglang.srt.utils.Withable.value">value</a></code></li>
<li><code><a title="sglang.srt.utils.Withable.with_value" href="#sglang.srt.utils.Withable.with_value">with_value</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
