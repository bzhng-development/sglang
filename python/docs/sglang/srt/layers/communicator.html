<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.layers.communicator API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.layers.communicator</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.layers.communicator.enable_moe_dense_fully_dp"><code class="name flex">
<span>def <span class="ident">enable_moe_dense_fully_dp</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def enable_moe_dense_fully_dp():
    return global_server_args_dict[&#34;moe_dense_tp_size&#34;] == 1</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.layers.communicator.CommunicateContext"><code class="flex name class">
<span>class <span class="ident">CommunicateContext</span></span>
<span>(</span><span>process_group_sizes: Dict[<a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a>, int],<br>attn_tp_rank: int,<br>attn_tp_size: int,<br>attn_dp_size: int,<br>tp_size: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class CommunicateContext:
    process_group_sizes: Dict[ScatterMode, int]
    attn_tp_rank: int
    attn_tp_size: int
    attn_dp_size: int
    tp_size: int

    def is_same_group_size(self, a: ScatterMode, b: ScatterMode):
        return self.process_group_sizes[a] == self.process_group_sizes[b]

    @classmethod
    def init_new(cls):
        attn_tp_rank = get_attention_tp_rank()
        attn_tp_size = get_attention_tp_size()
        attn_dp_size = get_attention_dp_size()
        tp_size = get_tensor_model_parallel_world_size()
        process_group_sizes = {
            ScatterMode.SCATTERED: 1,
            ScatterMode.TP_ATTN_FULL: attn_tp_size,
            # TODO: support --moe-dense-tp-size &gt; 1
            ScatterMode.FULL: tp_size,
        }
        return cls(
            process_group_sizes=process_group_sizes,
            attn_tp_rank=attn_tp_rank,
            attn_tp_size=attn_tp_size,
            attn_dp_size=attn_dp_size,
            tp_size=tp_size,
        )</code></pre>
</details>
<div class="desc"><p>CommunicateContext(process_group_sizes: Dict[sglang.srt.layers.communicator.ScatterMode, int], attn_tp_rank: int, attn_tp_size: int, attn_dp_size: int, tp_size: int)</p></div>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.layers.communicator.CommunicateContext.init_new"><code class="name flex">
<span>def <span class="ident">init_new</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.layers.communicator.CommunicateContext.attn_dp_size"><code class="name">var <span class="ident">attn_dp_size</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.communicator.CommunicateContext.attn_tp_rank"><code class="name">var <span class="ident">attn_tp_rank</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.communicator.CommunicateContext.attn_tp_size"><code class="name">var <span class="ident">attn_tp_size</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.communicator.CommunicateContext.process_group_sizes"><code class="name">var <span class="ident">process_group_sizes</span> : Dict[<a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a>, int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.communicator.CommunicateContext.tp_size"><code class="name">var <span class="ident">tp_size</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.communicator.CommunicateContext.is_same_group_size"><code class="name flex">
<span>def <span class="ident">is_same_group_size</span></span>(<span>self,<br>a: <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a>,<br>b: <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_same_group_size(self, a: ScatterMode, b: ScatterMode):
    return self.process_group_sizes[a] == self.process_group_sizes[b]</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.layers.communicator.CommunicateSimpleFn"><code class="flex name class">
<span>class <span class="ident">CommunicateSimpleFn</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CommunicateSimpleFn:
    @staticmethod
    def get_fn(
        input_mode: ScatterMode,
        output_mode: ScatterMode,
        context: CommunicateContext,
    ):
        if context.is_same_group_size(input_mode, output_mode):
            return CommunicateSimpleFn._trivial

        if (input_mode == ScatterMode.SCATTERED) and (
            output_mode == ScatterMode.TP_ATTN_FULL
        ):
            return CommunicateSimpleFn._scattered_to_tp_attn_full

        raise NotImplementedError(f&#34;{input_mode=} {output_mode=}&#34;)

    @staticmethod
    def _trivial(
        hidden_states: torch.Tensor,
        forward_batch: ForwardBatch,
        context: CommunicateContext,
    ) -&gt; torch.Tensor:
        return hidden_states

    @staticmethod
    def _scattered_to_tp_attn_full(
        hidden_states: torch.Tensor,
        forward_batch: ForwardBatch,
        context: CommunicateContext,
    ) -&gt; torch.Tensor:
        hidden_states, local_hidden_states = (
            get_local_dp_buffer(),
            hidden_states,
        )
        attn_tp_all_gather_into_tensor(
            hidden_states,
            local_hidden_states,
        )
        return hidden_states</code></pre>
</details>
<div class="desc"></div>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.layers.communicator.CommunicateSimpleFn.get_fn"><code class="name flex">
<span>def <span class="ident">get_fn</span></span>(<span>input_mode: <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a>,<br>output_mode: <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a>,<br>context: <a title="sglang.srt.layers.communicator.CommunicateContext" href="#sglang.srt.layers.communicator.CommunicateContext">CommunicateContext</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_fn(
    input_mode: ScatterMode,
    output_mode: ScatterMode,
    context: CommunicateContext,
):
    if context.is_same_group_size(input_mode, output_mode):
        return CommunicateSimpleFn._trivial

    if (input_mode == ScatterMode.SCATTERED) and (
        output_mode == ScatterMode.TP_ATTN_FULL
    ):
        return CommunicateSimpleFn._scattered_to_tp_attn_full

    raise NotImplementedError(f&#34;{input_mode=} {output_mode=}&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.layers.communicator.CommunicateSummableTensorPairFn"><code class="flex name class">
<span>class <span class="ident">CommunicateSummableTensorPairFn</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CommunicateSummableTensorPairFn:
    &#34;&#34;&#34;It is allowed to make (hidden_states, residual) := (hidden_states + residual, None) if needed.&#34;&#34;&#34;

    @classmethod
    def execute(
        cls,
        hidden_states_input_mode,
        residual_input_mode,
        output_mode,
        context,
        **kwargs,
    ):
        return cls.get_fn(
            hidden_states_input_mode=hidden_states_input_mode,
            residual_input_mode=residual_input_mode,
            output_mode=output_mode,
            context=context,
        )(context=context, **kwargs)

    @staticmethod
    def get_fn(
        hidden_states_input_mode: ScatterMode,
        residual_input_mode: ScatterMode,
        output_mode: ScatterMode,
        context: CommunicateContext,
    ):
        if context.is_same_group_size(
            hidden_states_input_mode, output_mode
        ) and context.is_same_group_size(residual_input_mode, output_mode):
            return CommunicateSummableTensorPairFn._trivial

        if (
            (hidden_states_input_mode == ScatterMode.FULL)
            and (residual_input_mode == ScatterMode.TP_ATTN_FULL)
            and (output_mode == ScatterMode.TP_ATTN_FULL)
        ):
            return CommunicateSummableTensorPairFn._scatter_hidden_states

        if (
            (hidden_states_input_mode == ScatterMode.SCATTERED)
            and (residual_input_mode == ScatterMode.SCATTERED)
            and (output_mode == ScatterMode.TP_ATTN_FULL)
        ):
            return CommunicateSummableTensorPairFn._gather

        if (
            (hidden_states_input_mode == ScatterMode.TP_ATTN_FULL)
            and (residual_input_mode == ScatterMode.TP_ATTN_FULL)
            and (output_mode == ScatterMode.SCATTERED)
        ):
            return CommunicateSummableTensorPairFn._scatter

        raise NotImplementedError(
            f&#34;{hidden_states_input_mode=} {residual_input_mode=} {output_mode=}&#34;
        )

    @staticmethod
    def _trivial(
        hidden_states: torch.Tensor,
        residual: torch.Tensor,
        forward_batch: ForwardBatch,
        context: CommunicateContext,
        **kwargs,
    ):
        return hidden_states, residual

    @staticmethod
    def _scatter_hidden_states(
        hidden_states: torch.Tensor,
        residual: torch.Tensor,
        forward_batch: ForwardBatch,
        context: CommunicateContext,
        allow_reduce_scatter: bool = False,
    ):
        hidden_states, global_hidden_states = (
            get_local_dp_buffer(),
            hidden_states,
        )
        if allow_reduce_scatter and forward_batch.dp_padding_mode.is_max_len():
            # When using padding, all_reduce is skipped after MLP and MOE and reduce scatter is used here instead.
            dp_reduce_scatter_tensor(hidden_states, global_hidden_states)
        else:
            dp_scatter(hidden_states, global_hidden_states, forward_batch)
        return hidden_states, residual

    @staticmethod
    def _gather(
        hidden_states: torch.Tensor,
        residual: torch.Tensor,
        forward_batch: ForwardBatch,
        context: CommunicateContext,
        **kwargs,
    ):
        hidden_states += residual
        residual = None
        hidden_states, local_hidden_states = (
            get_local_dp_buffer(),
            hidden_states,
        )
        attn_tp_all_gather_into_tensor(
            hidden_states,
            local_hidden_states,
        )
        return hidden_states, residual

    @staticmethod
    def _scatter(
        hidden_states: torch.Tensor,
        residual: torch.Tensor,
        forward_batch: ForwardBatch,
        context: CommunicateContext,
    ):
        assert residual is None, &#34;not yet handled residual!=None&#34;
        tensor_list = list(hidden_states.tensor_split(context.attn_tp_size))
        hidden_states = tensor_list[context.attn_tp_rank]
        return hidden_states, residual</code></pre>
</details>
<div class="desc"><p>It is allowed to make (hidden_states, residual) := (hidden_states + residual, None) if needed.</p></div>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.layers.communicator.CommunicateSummableTensorPairFn.execute"><code class="name flex">
<span>def <span class="ident">execute</span></span>(<span>hidden_states_input_mode, residual_input_mode, output_mode, context, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.communicator.CommunicateSummableTensorPairFn.get_fn"><code class="name flex">
<span>def <span class="ident">get_fn</span></span>(<span>hidden_states_input_mode: <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a>,<br>residual_input_mode: <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a>,<br>output_mode: <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a>,<br>context: <a title="sglang.srt.layers.communicator.CommunicateContext" href="#sglang.srt.layers.communicator.CommunicateContext">CommunicateContext</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_fn(
    hidden_states_input_mode: ScatterMode,
    residual_input_mode: ScatterMode,
    output_mode: ScatterMode,
    context: CommunicateContext,
):
    if context.is_same_group_size(
        hidden_states_input_mode, output_mode
    ) and context.is_same_group_size(residual_input_mode, output_mode):
        return CommunicateSummableTensorPairFn._trivial

    if (
        (hidden_states_input_mode == ScatterMode.FULL)
        and (residual_input_mode == ScatterMode.TP_ATTN_FULL)
        and (output_mode == ScatterMode.TP_ATTN_FULL)
    ):
        return CommunicateSummableTensorPairFn._scatter_hidden_states

    if (
        (hidden_states_input_mode == ScatterMode.SCATTERED)
        and (residual_input_mode == ScatterMode.SCATTERED)
        and (output_mode == ScatterMode.TP_ATTN_FULL)
    ):
        return CommunicateSummableTensorPairFn._gather

    if (
        (hidden_states_input_mode == ScatterMode.TP_ATTN_FULL)
        and (residual_input_mode == ScatterMode.TP_ATTN_FULL)
        and (output_mode == ScatterMode.SCATTERED)
    ):
        return CommunicateSummableTensorPairFn._scatter

    raise NotImplementedError(
        f&#34;{hidden_states_input_mode=} {residual_input_mode=} {output_mode=}&#34;
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.layers.communicator.CommunicateWithAllReduceAndLayerNormFn"><code class="flex name class">
<span>class <span class="ident">CommunicateWithAllReduceAndLayerNormFn</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CommunicateWithAllReduceAndLayerNormFn:
    &#34;&#34;&#34;Besides communication, needs to
    1. All reduce in tp_attn_group on hidden_states
    2. Apply layer norm
    &#34;&#34;&#34;

    @staticmethod
    def get_fn(
        hidden_states_input_mode: ScatterMode,
        residual_input_mode: ScatterMode,
        hidden_states_output_mode: ScatterMode,
        residual_output_mode: ScatterMode,
        context: CommunicateContext,
    ):

        if (
            context.is_same_group_size(
                hidden_states_input_mode, hidden_states_output_mode
            )
            and context.is_same_group_size(residual_input_mode, residual_output_mode)
            and context.attn_tp_size == 1
        ):
            return CommunicateWithAllReduceAndLayerNormFn._simple

        if (
            (hidden_states_input_mode == ScatterMode.TP_ATTN_FULL)
            and (
                residual_input_mode in [ScatterMode.SCATTERED, ScatterMode.TP_ATTN_FULL]
            )
            and (hidden_states_output_mode == ScatterMode.FULL)
            and (residual_output_mode == ScatterMode.TP_ATTN_FULL)
        ):
            return partial(
                CommunicateWithAllReduceAndLayerNormFn._gather_hidden_states_and_residual,
                residual_input_mode=residual_input_mode,
            )

        if (
            (hidden_states_input_mode == ScatterMode.TP_ATTN_FULL)
            and (
                residual_input_mode in [ScatterMode.SCATTERED, ScatterMode.TP_ATTN_FULL]
            )
            and (hidden_states_output_mode == ScatterMode.SCATTERED)
            and (residual_output_mode == ScatterMode.SCATTERED)
        ):
            return partial(
                CommunicateWithAllReduceAndLayerNormFn._scatter_hidden_states_and_residual,
                residual_input_mode=residual_input_mode,
            )

        raise NotImplementedError(
            f&#34;{hidden_states_input_mode=} {residual_input_mode=} {hidden_states_output_mode=} {residual_output_mode=}&#34;
        )

    @staticmethod
    def _simple(
        hidden_states: torch.Tensor,
        residual: torch.Tensor,
        forward_batch: ForwardBatch,
        layernorm: torch.nn.Module,
        context: CommunicateContext,
    ):
        # TODO move these `if shape != 0` into LayerNorm itself
        if hidden_states.shape[0] != 0:
            hidden_states, residual = layernorm(hidden_states, residual)
        return hidden_states, residual

    @staticmethod
    def _gather_hidden_states_and_residual(
        hidden_states: torch.Tensor,
        residual: torch.Tensor,
        forward_batch: ForwardBatch,
        layernorm: torch.nn.Module,
        context: CommunicateContext,
        *,
        residual_input_mode,
    ):
        if residual_input_mode == ScatterMode.SCATTERED and context.attn_tp_size &gt; 1:
            residual, local_residual = (
                get_local_dp_buffer(),
                residual,
            )
            attn_tp_all_gather_into_tensor(residual, local_residual)
        if context.attn_dp_size != 1:
            if context.attn_tp_rank == 0:
                hidden_states += residual

            # Perform layernorm on smaller data before comm. Only valid when attn_tp_size is 1 (tp_size == dp_size)
            use_layer_norm_before_gather = context.attn_tp_size == 1
            if use_layer_norm_before_gather and hidden_states.shape[0] != 0:
                residual = hidden_states
                hidden_states = layernorm(hidden_states)
            hidden_states, local_hidden_states = (
                get_global_dp_buffer(),
                hidden_states,
            )
            dp_gather_partial(hidden_states, local_hidden_states, forward_batch)

            if not use_layer_norm_before_gather:
                dp_scatter(residual, hidden_states, forward_batch)
                if hidden_states.shape[0] != 0:
                    hidden_states = layernorm(hidden_states)
        else:
            # According to the discussion in https://github.com/flashinfer-ai/flashinfer/issues/1223#issuecomment-3047256465
            # We set the max token num to 128 for allreduce fusion with min-latency case(use_oneshot=True).
            if (
                _is_sm100_supported
                and _is_flashinfer_available
                and hasattr(layernorm, &#34;forward_with_allreduce_fusion&#34;)
                and global_server_args_dict[&#34;enable_flashinfer_allreduce_fusion&#34;]
                and hidden_states.shape[0] &lt;= 2048
            ):
                hidden_states, residual = layernorm.forward_with_allreduce_fusion(
                    hidden_states, residual
                )
            else:
                hidden_states = tensor_model_parallel_all_reduce(hidden_states)
                hidden_states, residual = layernorm(hidden_states, residual)
        return hidden_states, residual

    @staticmethod
    def _scatter_hidden_states_and_residual(
        hidden_states: torch.Tensor,
        residual: torch.Tensor,
        forward_batch: ForwardBatch,
        layernorm: torch.nn.Module,
        context: CommunicateContext,
        *,
        residual_input_mode,
    ):
        input_hidden_states = hidden_states
        hidden_states = hidden_states.tensor_split(context.attn_tp_size)[
            context.attn_tp_rank
        ]
        attn_tp_reduce_scatter_tensor(hidden_states, input_hidden_states)
        if residual_input_mode == ScatterMode.TP_ATTN_FULL:
            residual = residual.tensor_split(context.attn_tp_size)[context.attn_tp_rank]
        if hidden_states.shape[0] != 0:
            hidden_states, residual = layernorm(hidden_states, residual)
        return hidden_states, residual</code></pre>
</details>
<div class="desc"><p>Besides communication, needs to
1. All reduce in tp_attn_group on hidden_states
2. Apply layer norm</p></div>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.layers.communicator.CommunicateWithAllReduceAndLayerNormFn.get_fn"><code class="name flex">
<span>def <span class="ident">get_fn</span></span>(<span>hidden_states_input_mode: <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a>,<br>residual_input_mode: <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a>,<br>hidden_states_output_mode: <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a>,<br>residual_output_mode: <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a>,<br>context: <a title="sglang.srt.layers.communicator.CommunicateContext" href="#sglang.srt.layers.communicator.CommunicateContext">CommunicateContext</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_fn(
    hidden_states_input_mode: ScatterMode,
    residual_input_mode: ScatterMode,
    hidden_states_output_mode: ScatterMode,
    residual_output_mode: ScatterMode,
    context: CommunicateContext,
):

    if (
        context.is_same_group_size(
            hidden_states_input_mode, hidden_states_output_mode
        )
        and context.is_same_group_size(residual_input_mode, residual_output_mode)
        and context.attn_tp_size == 1
    ):
        return CommunicateWithAllReduceAndLayerNormFn._simple

    if (
        (hidden_states_input_mode == ScatterMode.TP_ATTN_FULL)
        and (
            residual_input_mode in [ScatterMode.SCATTERED, ScatterMode.TP_ATTN_FULL]
        )
        and (hidden_states_output_mode == ScatterMode.FULL)
        and (residual_output_mode == ScatterMode.TP_ATTN_FULL)
    ):
        return partial(
            CommunicateWithAllReduceAndLayerNormFn._gather_hidden_states_and_residual,
            residual_input_mode=residual_input_mode,
        )

    if (
        (hidden_states_input_mode == ScatterMode.TP_ATTN_FULL)
        and (
            residual_input_mode in [ScatterMode.SCATTERED, ScatterMode.TP_ATTN_FULL]
        )
        and (hidden_states_output_mode == ScatterMode.SCATTERED)
        and (residual_output_mode == ScatterMode.SCATTERED)
    ):
        return partial(
            CommunicateWithAllReduceAndLayerNormFn._scatter_hidden_states_and_residual,
            residual_input_mode=residual_input_mode,
        )

    raise NotImplementedError(
        f&#34;{hidden_states_input_mode=} {residual_input_mode=} {hidden_states_output_mode=} {residual_output_mode=}&#34;
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.layers.communicator.LayerCommunicator"><code class="flex name class">
<span>class <span class="ident">LayerCommunicator</span></span>
<span>(</span><span>layer_scatter_modes: <a title="sglang.srt.layers.communicator.LayerScatterModes" href="#sglang.srt.layers.communicator.LayerScatterModes">LayerScatterModes</a>,<br>input_layernorm: torch.nn.modules.module.Module,<br>post_attention_layernorm: torch.nn.modules.module.Module,<br>allow_reduce_scatter: bool = False,<br>is_last_layer: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LayerCommunicator:
    def __init__(
        self,
        layer_scatter_modes: LayerScatterModes,
        input_layernorm: torch.nn.Module,
        post_attention_layernorm: torch.nn.Module,
        # Reduce scatter requires skipping all-reduce in model code after MoE/MLP, so only enable for models which have that implemented. Remove flag once done for all models that use LayerCommunicator.
        allow_reduce_scatter: bool = False,
        is_last_layer: bool = False,
    ):
        self.layer_scatter_modes = layer_scatter_modes
        self.input_layernorm = input_layernorm
        self.post_attention_layernorm = post_attention_layernorm
        self.allow_reduce_scatter = allow_reduce_scatter
        self.is_last_layer = is_last_layer

        self._context = CommunicateContext.init_new()
        self._communicate_simple_fn = CommunicateSimpleFn.get_fn(
            input_mode=self.layer_scatter_modes.layer_input_mode,
            output_mode=self.layer_scatter_modes.attn_mode,
            context=self._context,
        )
        self._communicate_with_all_reduce_and_layer_norm_fn = (
            CommunicateWithAllReduceAndLayerNormFn.get_fn(
                hidden_states_input_mode=self.layer_scatter_modes.attn_mode,
                residual_input_mode=self.layer_scatter_modes.layer_input_mode,
                hidden_states_output_mode=self.layer_scatter_modes.mlp_mode,
                residual_output_mode=self.layer_scatter_modes.middle_residual_mode,
                context=self._context,
            )
        )
        self._communicate_summable_tensor_pair_fn = (
            CommunicateSummableTensorPairFn.get_fn(
                hidden_states_input_mode=self.layer_scatter_modes.mlp_mode,
                residual_input_mode=self.layer_scatter_modes.middle_residual_mode,
                output_mode=self.layer_scatter_modes.layer_output_mode,
                context=self._context,
            )
        )

    def prepare_attn(
        self,
        hidden_states: torch.Tensor,
        residual: torch.Tensor,
        forward_batch: ForwardBatch,
    ):
        if hidden_states.shape[0] == 0:
            residual = hidden_states
        else:
            if (
                residual is not None
                and hasattr(hidden_states, &#34;_sglang_needs_allreduce_fusion&#34;)
                and hidden_states._sglang_needs_allreduce_fusion
            ):
                hidden_states, residual = (
                    self.input_layernorm.forward_with_allreduce_fusion(
                        hidden_states, residual
                    )
                )
            else:
                if residual is None:
                    residual = hidden_states
                    hidden_states = self.input_layernorm(hidden_states)
                else:
                    hidden_states, residual = self.input_layernorm(
                        hidden_states, residual
                    )

        hidden_states = self._communicate_simple_fn(
            hidden_states=hidden_states,
            forward_batch=forward_batch,
            context=self._context,
        )

        return hidden_states, residual

    def prepare_mlp(
        self,
        hidden_states: torch.Tensor,
        residual: torch.Tensor,
        forward_batch: ForwardBatch,
    ):
        return self._communicate_with_all_reduce_and_layer_norm_fn(
            hidden_states=hidden_states,
            residual=residual,
            forward_batch=forward_batch,
            layernorm=self.post_attention_layernorm,
            context=self._context,
        )

    def postprocess_layer(
        self,
        hidden_states: torch.Tensor,
        residual: torch.Tensor,
        forward_batch: ForwardBatch,
    ):
        return self._communicate_summable_tensor_pair_fn(
            hidden_states=hidden_states,
            residual=residual,
            forward_batch=forward_batch,
            context=self._context,
            allow_reduce_scatter=self.allow_reduce_scatter,
        )

    def should_use_reduce_scatter(self, forward_batch: ForwardBatch):
        return (
            self.allow_reduce_scatter
            and self._communicate_summable_tensor_pair_fn
            is CommunicateSummableTensorPairFn._scatter_hidden_states
            and forward_batch.dp_padding_mode.is_max_len()
        )

    def should_fuse_mlp_allreduce_with_next_layer(
        self, forward_batch: ForwardBatch
    ) -&gt; bool:
        speculative_algo = global_server_args_dict.get(&#34;speculative_algorithm&#34;, None)
        if (
            is_dp_attention_enabled()
            and speculative_algo is not None
            and speculative_algo.is_eagle()
        ):
            return False

        batch_size = (
            forward_batch.input_ids.shape[0]
            if hasattr(forward_batch, &#34;input_ids&#34;)
            else 0
        )
        if batch_size &gt; FUSE_ALLREDUCE_MAX_BATCH_SIZE:
            return False

        static_conditions_met = (
            (not self.is_last_layer)
            and (self._context.tp_size &gt; 1)
            and global_server_args_dict.get(&#34;enable_flashinfer_allreduce_fusion&#34;, False)
            and _is_flashinfer_available
        )

        if not static_conditions_met:
            return False

        return (
            batch_size &gt; 0
            and batch_size &lt;= FUSE_ALLREDUCE_MAX_BATCH_SIZE
            and (not self.is_last_layer)
        )</code></pre>
</details>
<div class="desc"></div>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.communicator.LayerCommunicator.postprocess_layer"><code class="name flex">
<span>def <span class="ident">postprocess_layer</span></span>(<span>self,<br>hidden_states: torch.Tensor,<br>residual: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def postprocess_layer(
    self,
    hidden_states: torch.Tensor,
    residual: torch.Tensor,
    forward_batch: ForwardBatch,
):
    return self._communicate_summable_tensor_pair_fn(
        hidden_states=hidden_states,
        residual=residual,
        forward_batch=forward_batch,
        context=self._context,
        allow_reduce_scatter=self.allow_reduce_scatter,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.communicator.LayerCommunicator.prepare_attn"><code class="name flex">
<span>def <span class="ident">prepare_attn</span></span>(<span>self,<br>hidden_states: torch.Tensor,<br>residual: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_attn(
    self,
    hidden_states: torch.Tensor,
    residual: torch.Tensor,
    forward_batch: ForwardBatch,
):
    if hidden_states.shape[0] == 0:
        residual = hidden_states
    else:
        if (
            residual is not None
            and hasattr(hidden_states, &#34;_sglang_needs_allreduce_fusion&#34;)
            and hidden_states._sglang_needs_allreduce_fusion
        ):
            hidden_states, residual = (
                self.input_layernorm.forward_with_allreduce_fusion(
                    hidden_states, residual
                )
            )
        else:
            if residual is None:
                residual = hidden_states
                hidden_states = self.input_layernorm(hidden_states)
            else:
                hidden_states, residual = self.input_layernorm(
                    hidden_states, residual
                )

    hidden_states = self._communicate_simple_fn(
        hidden_states=hidden_states,
        forward_batch=forward_batch,
        context=self._context,
    )

    return hidden_states, residual</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.communicator.LayerCommunicator.prepare_mlp"><code class="name flex">
<span>def <span class="ident">prepare_mlp</span></span>(<span>self,<br>hidden_states: torch.Tensor,<br>residual: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_mlp(
    self,
    hidden_states: torch.Tensor,
    residual: torch.Tensor,
    forward_batch: ForwardBatch,
):
    return self._communicate_with_all_reduce_and_layer_norm_fn(
        hidden_states=hidden_states,
        residual=residual,
        forward_batch=forward_batch,
        layernorm=self.post_attention_layernorm,
        context=self._context,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.communicator.LayerCommunicator.should_fuse_mlp_allreduce_with_next_layer"><code class="name flex">
<span>def <span class="ident">should_fuse_mlp_allreduce_with_next_layer</span></span>(<span>self,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def should_fuse_mlp_allreduce_with_next_layer(
    self, forward_batch: ForwardBatch
) -&gt; bool:
    speculative_algo = global_server_args_dict.get(&#34;speculative_algorithm&#34;, None)
    if (
        is_dp_attention_enabled()
        and speculative_algo is not None
        and speculative_algo.is_eagle()
    ):
        return False

    batch_size = (
        forward_batch.input_ids.shape[0]
        if hasattr(forward_batch, &#34;input_ids&#34;)
        else 0
    )
    if batch_size &gt; FUSE_ALLREDUCE_MAX_BATCH_SIZE:
        return False

    static_conditions_met = (
        (not self.is_last_layer)
        and (self._context.tp_size &gt; 1)
        and global_server_args_dict.get(&#34;enable_flashinfer_allreduce_fusion&#34;, False)
        and _is_flashinfer_available
    )

    if not static_conditions_met:
        return False

    return (
        batch_size &gt; 0
        and batch_size &lt;= FUSE_ALLREDUCE_MAX_BATCH_SIZE
        and (not self.is_last_layer)
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.communicator.LayerCommunicator.should_use_reduce_scatter"><code class="name flex">
<span>def <span class="ident">should_use_reduce_scatter</span></span>(<span>self,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def should_use_reduce_scatter(self, forward_batch: ForwardBatch):
    return (
        self.allow_reduce_scatter
        and self._communicate_summable_tensor_pair_fn
        is CommunicateSummableTensorPairFn._scatter_hidden_states
        and forward_batch.dp_padding_mode.is_max_len()
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.layers.communicator.LayerScatterModes"><code class="flex name class">
<span>class <span class="ident">LayerScatterModes</span></span>
<span>(</span><span>layer_input_mode: <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a>,<br>attn_mode: <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a>,<br>mlp_mode: <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a>,<br>middle_residual_mode: <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a>,<br>layer_output_mode: <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class LayerScatterModes:
    layer_input_mode: ScatterMode
    attn_mode: ScatterMode
    # Can be further split into e.g. mlp_input_mode and mlp_output_mode if needed
    mlp_mode: ScatterMode
    middle_residual_mode: ScatterMode
    layer_output_mode: ScatterMode

    @classmethod
    def init_new(cls, **kwargs):
        context = _LayerModeComputationContext(**kwargs)
        return cls(
            layer_input_mode=cls._compute_layer_input_mode(context),
            attn_mode=ScatterMode.TP_ATTN_FULL,
            mlp_mode=cls._compute_mlp_mode(context),
            middle_residual_mode=cls._compute_middle_residual_mode(context),
            layer_output_mode=cls._compute_layer_output_mode(context),
        )

    @classmethod
    def _compute_layer_input_mode(cls, context: _LayerModeComputationContext):
        if context.layer_id == 0:
            return ScatterMode.model_input_output()
        return cls._compute_layer_output_mode(context.previous_layer())

    @classmethod
    def _compute_mlp_mode(cls, context: _LayerModeComputationContext):
        if context.is_layer_sparse:
            return (
                ScatterMode.SCATTERED
                if (
                    # Token dispatch/combine will be handled outside of LayerCommunicator for these modes.
                    not get_moe_a2a_backend().is_none()
                    or should_use_flashinfer_cutlass_moe_fp4_allgather()
                )
                else ScatterMode.FULL
            )
        else:
            return (
                ScatterMode.SCATTERED
                if enable_moe_dense_fully_dp()
                else ScatterMode.FULL
            )

    @classmethod
    def _compute_middle_residual_mode(cls, context: _LayerModeComputationContext):
        mlp_mode = cls._compute_mlp_mode(context)
        if mlp_mode == ScatterMode.SCATTERED:
            return ScatterMode.SCATTERED
        if mlp_mode == ScatterMode.FULL:
            return ScatterMode.TP_ATTN_FULL
        raise NotImplementedError

    @classmethod
    def _compute_layer_output_mode(cls, context: _LayerModeComputationContext):
        mlp_mode = cls._compute_mlp_mode(context)
        if context.layer_id == context.num_layers - 1:
            return ScatterMode.model_input_output()
        if mlp_mode == ScatterMode.SCATTERED:
            return ScatterMode.SCATTERED
        if mlp_mode == ScatterMode.FULL:
            return ScatterMode.TP_ATTN_FULL
        raise NotImplementedError</code></pre>
</details>
<div class="desc"><p>LayerScatterModes(layer_input_mode: sglang.srt.layers.communicator.ScatterMode, attn_mode: sglang.srt.layers.communicator.ScatterMode, mlp_mode: sglang.srt.layers.communicator.ScatterMode, middle_residual_mode: sglang.srt.layers.communicator.ScatterMode, layer_output_mode: sglang.srt.layers.communicator.ScatterMode)</p></div>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.layers.communicator.LayerScatterModes.init_new"><code class="name flex">
<span>def <span class="ident">init_new</span></span>(<span>**kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.layers.communicator.LayerScatterModes.attn_mode"><code class="name">var <span class="ident">attn_mode</span> : <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.communicator.LayerScatterModes.layer_input_mode"><code class="name">var <span class="ident">layer_input_mode</span> : <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.communicator.LayerScatterModes.layer_output_mode"><code class="name">var <span class="ident">layer_output_mode</span> : <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.communicator.LayerScatterModes.middle_residual_mode"><code class="name">var <span class="ident">middle_residual_mode</span> : <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.communicator.LayerScatterModes.mlp_mode"><code class="name">var <span class="ident">mlp_mode</span> : <a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.layers.communicator.ScatterMode"><code class="flex name class">
<span>class <span class="ident">ScatterMode</span></span>
<span>(</span><span>*args, **kwds)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ScatterMode(Enum):
    &#34;&#34;&#34;
    Suppose we have TP=4, DP=2, enable-dp-attention, and the system handles seq a,b,c,d
    Model input/output: [ab, ab, cd, cd] for four ranks respectively
    SCATTERED: [a, b, c, d]
    TP_ATTN_FULL: [ab, ab, cd, cd], i.e. all ranks inside a TP attn group have full data of the group
    FULL: [abcd, abcd, abcd, abcd]
    &#34;&#34;&#34;

    SCATTERED = auto()
    TP_ATTN_FULL = auto()
    FULL = auto()

    @staticmethod
    def model_input_output():
        &#34;&#34;&#34;The scatter mode for model forward pass input and output data&#34;&#34;&#34;
        return ScatterMode.TP_ATTN_FULL</code></pre>
</details>
<div class="desc"><p>Suppose we have TP=4, DP=2, enable-dp-attention, and the system handles seq a,b,c,d
Model input/output: [ab, ab, cd, cd] for four ranks respectively
SCATTERED: [a, b, c, d]
TP_ATTN_FULL: [ab, ab, cd, cd], i.e. all ranks inside a TP attn group have full data of the group
FULL: [abcd, abcd, abcd, abcd]</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.layers.communicator.ScatterMode.FULL"><code class="name">var <span class="ident">FULL</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.communicator.ScatterMode.SCATTERED"><code class="name">var <span class="ident">SCATTERED</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.communicator.ScatterMode.TP_ATTN_FULL"><code class="name">var <span class="ident">TP_ATTN_FULL</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.layers.communicator.ScatterMode.model_input_output"><code class="name flex">
<span>def <span class="ident">model_input_output</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def model_input_output():
    &#34;&#34;&#34;The scatter mode for model forward pass input and output data&#34;&#34;&#34;
    return ScatterMode.TP_ATTN_FULL</code></pre>
</details>
<div class="desc"><p>The scatter mode for model forward pass input and output data</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.layers" href="index.html">sglang.srt.layers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.layers.communicator.enable_moe_dense_fully_dp" href="#sglang.srt.layers.communicator.enable_moe_dense_fully_dp">enable_moe_dense_fully_dp</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.layers.communicator.CommunicateContext" href="#sglang.srt.layers.communicator.CommunicateContext">CommunicateContext</a></code></h4>
<ul class="two-column">
<li><code><a title="sglang.srt.layers.communicator.CommunicateContext.attn_dp_size" href="#sglang.srt.layers.communicator.CommunicateContext.attn_dp_size">attn_dp_size</a></code></li>
<li><code><a title="sglang.srt.layers.communicator.CommunicateContext.attn_tp_rank" href="#sglang.srt.layers.communicator.CommunicateContext.attn_tp_rank">attn_tp_rank</a></code></li>
<li><code><a title="sglang.srt.layers.communicator.CommunicateContext.attn_tp_size" href="#sglang.srt.layers.communicator.CommunicateContext.attn_tp_size">attn_tp_size</a></code></li>
<li><code><a title="sglang.srt.layers.communicator.CommunicateContext.init_new" href="#sglang.srt.layers.communicator.CommunicateContext.init_new">init_new</a></code></li>
<li><code><a title="sglang.srt.layers.communicator.CommunicateContext.is_same_group_size" href="#sglang.srt.layers.communicator.CommunicateContext.is_same_group_size">is_same_group_size</a></code></li>
<li><code><a title="sglang.srt.layers.communicator.CommunicateContext.process_group_sizes" href="#sglang.srt.layers.communicator.CommunicateContext.process_group_sizes">process_group_sizes</a></code></li>
<li><code><a title="sglang.srt.layers.communicator.CommunicateContext.tp_size" href="#sglang.srt.layers.communicator.CommunicateContext.tp_size">tp_size</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.communicator.CommunicateSimpleFn" href="#sglang.srt.layers.communicator.CommunicateSimpleFn">CommunicateSimpleFn</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.communicator.CommunicateSimpleFn.get_fn" href="#sglang.srt.layers.communicator.CommunicateSimpleFn.get_fn">get_fn</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.communicator.CommunicateSummableTensorPairFn" href="#sglang.srt.layers.communicator.CommunicateSummableTensorPairFn">CommunicateSummableTensorPairFn</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.communicator.CommunicateSummableTensorPairFn.execute" href="#sglang.srt.layers.communicator.CommunicateSummableTensorPairFn.execute">execute</a></code></li>
<li><code><a title="sglang.srt.layers.communicator.CommunicateSummableTensorPairFn.get_fn" href="#sglang.srt.layers.communicator.CommunicateSummableTensorPairFn.get_fn">get_fn</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.communicator.CommunicateWithAllReduceAndLayerNormFn" href="#sglang.srt.layers.communicator.CommunicateWithAllReduceAndLayerNormFn">CommunicateWithAllReduceAndLayerNormFn</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.communicator.CommunicateWithAllReduceAndLayerNormFn.get_fn" href="#sglang.srt.layers.communicator.CommunicateWithAllReduceAndLayerNormFn.get_fn">get_fn</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.communicator.LayerCommunicator" href="#sglang.srt.layers.communicator.LayerCommunicator">LayerCommunicator</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.communicator.LayerCommunicator.postprocess_layer" href="#sglang.srt.layers.communicator.LayerCommunicator.postprocess_layer">postprocess_layer</a></code></li>
<li><code><a title="sglang.srt.layers.communicator.LayerCommunicator.prepare_attn" href="#sglang.srt.layers.communicator.LayerCommunicator.prepare_attn">prepare_attn</a></code></li>
<li><code><a title="sglang.srt.layers.communicator.LayerCommunicator.prepare_mlp" href="#sglang.srt.layers.communicator.LayerCommunicator.prepare_mlp">prepare_mlp</a></code></li>
<li><code><a title="sglang.srt.layers.communicator.LayerCommunicator.should_fuse_mlp_allreduce_with_next_layer" href="#sglang.srt.layers.communicator.LayerCommunicator.should_fuse_mlp_allreduce_with_next_layer">should_fuse_mlp_allreduce_with_next_layer</a></code></li>
<li><code><a title="sglang.srt.layers.communicator.LayerCommunicator.should_use_reduce_scatter" href="#sglang.srt.layers.communicator.LayerCommunicator.should_use_reduce_scatter">should_use_reduce_scatter</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.communicator.LayerScatterModes" href="#sglang.srt.layers.communicator.LayerScatterModes">LayerScatterModes</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.communicator.LayerScatterModes.attn_mode" href="#sglang.srt.layers.communicator.LayerScatterModes.attn_mode">attn_mode</a></code></li>
<li><code><a title="sglang.srt.layers.communicator.LayerScatterModes.init_new" href="#sglang.srt.layers.communicator.LayerScatterModes.init_new">init_new</a></code></li>
<li><code><a title="sglang.srt.layers.communicator.LayerScatterModes.layer_input_mode" href="#sglang.srt.layers.communicator.LayerScatterModes.layer_input_mode">layer_input_mode</a></code></li>
<li><code><a title="sglang.srt.layers.communicator.LayerScatterModes.layer_output_mode" href="#sglang.srt.layers.communicator.LayerScatterModes.layer_output_mode">layer_output_mode</a></code></li>
<li><code><a title="sglang.srt.layers.communicator.LayerScatterModes.middle_residual_mode" href="#sglang.srt.layers.communicator.LayerScatterModes.middle_residual_mode">middle_residual_mode</a></code></li>
<li><code><a title="sglang.srt.layers.communicator.LayerScatterModes.mlp_mode" href="#sglang.srt.layers.communicator.LayerScatterModes.mlp_mode">mlp_mode</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.communicator.ScatterMode" href="#sglang.srt.layers.communicator.ScatterMode">ScatterMode</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.communicator.ScatterMode.FULL" href="#sglang.srt.layers.communicator.ScatterMode.FULL">FULL</a></code></li>
<li><code><a title="sglang.srt.layers.communicator.ScatterMode.SCATTERED" href="#sglang.srt.layers.communicator.ScatterMode.SCATTERED">SCATTERED</a></code></li>
<li><code><a title="sglang.srt.layers.communicator.ScatterMode.TP_ATTN_FULL" href="#sglang.srt.layers.communicator.ScatterMode.TP_ATTN_FULL">TP_ATTN_FULL</a></code></li>
<li><code><a title="sglang.srt.layers.communicator.ScatterMode.model_input_output" href="#sglang.srt.layers.communicator.ScatterMode.model_input_output">model_input_output</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
