<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.layers.quantization.modelopt_quant API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.layers.quantization.modelopt_quant</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp4Config"><code class="flex name class">
<span>class <span class="ident">ModelOptFp4Config</span></span>
<span>(</span><span>is_checkpoint_nvfp4_serialized: bool = False,<br>kv_cache_quant_algo: str = None,<br>group_size: int = None,<br>exclude_modules: List[str] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelOptFp4Config(QuantizationConfig):
    &#34;&#34;&#34;Config class for FP4.&#34;&#34;&#34;

    def __init__(
        self,
        is_checkpoint_nvfp4_serialized: bool = False,
        kv_cache_quant_algo: str = None,
        group_size: int = None,
        exclude_modules: List[str] = None,
    ) -&gt; None:
        self.is_checkpoint_nvfp4_serialized = is_checkpoint_nvfp4_serialized
        if is_checkpoint_nvfp4_serialized:
            logger.warning(
                &#34;Detected nvfp4 checkpoint. Please note that the &#34;
                &#34;format is experimental and subject to change.&#34;
            )
        self.group_size = group_size
        self.kv_cache_quant_algo = kv_cache_quant_algo
        self.exclude_modules = exclude_modules

    @classmethod
    def get_name(cls) -&gt; str:
        return &#34;modelopt_fp4&#34;

    @classmethod
    def get_supported_act_dtypes(cls) -&gt; List[torch.dtype]:
        return [torch.bfloat16, torch.half, torch.float8_e4m3fn]

    @classmethod
    def get_min_capability(cls) -&gt; int:
        return 100

    @classmethod
    def get_config_filenames(cls) -&gt; List[str]:
        return [&#34;hf_quant_config.json&#34;]

    @classmethod
    def from_config(cls, config: Dict[str, Any]) -&gt; ModelOptFp4Config:
        # Handle two different config formats:
        # 1. hf_quant_config.json format: {&#34;quantization&#34;: {&#34;quant_algo&#34;: &#34;NVFP4&#34;, ...}}
        # 2. config.json quantization_config format: {&#34;quant_algo&#34;: &#34;NVFP4&#34;, ...}
        # In future modelopt will deprecate hf_quant_config.json, and only keep config.json.
        # For legacy reasons, we keep hf_quant_config.json for now.

        # Initialize variables
        kv_cache_quant_algo = None
        group_size = None
        exclude_modules = []

        # Try flat format first (config.json quantization_config - preferred format)
        quant_method = config.get(&#34;quant_algo&#34;)
        if quant_method is not None:
            # Flat format (config.json quantization_config)
            # Note: FP4 models in config.json format may not have all the detailed fields
            # that are present in hf_quant_config.json, so we need to handle defaults
            kv_cache_quant_algo = config.get(&#34;kv_cache_quant_algo&#34;)
            if not kv_cache_quant_algo:
                # For config.json format, derive from kv_cache_scheme if available
                kv_cache_scheme = config.get(&#34;kv_cache_scheme&#34;)
                if (
                    kv_cache_scheme
                    and kv_cache_scheme.get(&#34;type&#34;) == &#34;float&#34;
                    and kv_cache_scheme.get(&#34;num_bits&#34;) == 8
                ):
                    kv_cache_quant_algo = &#34;FP8&#34;
                else:
                    kv_cache_quant_algo = &#34;auto&#34;

            group_size = config.get(&#34;group_size&#34;)
            exclude_modules = config.get(&#34;ignore&#34;, [])
        else:
            # Fall back to nested format (hf_quant_config.json - legacy format)
            try:
                quant_config = cls.get_from_keys(config, [&#34;quantization&#34;])
                quant_method = quant_config[&#34;quant_algo&#34;]
                kv_cache_quant_algo = quant_config.get(&#34;kv_cache_quant_algo&#34;)
                if not kv_cache_quant_algo:
                    kv_cache_quant_algo = &#34;auto&#34;
                group_size = quant_config.get(&#34;group_size&#34;)
                exclude_modules = quant_config.get(&#34;exclude_modules&#34;, [])
            except (ValueError, KeyError):
                raise ValueError(
                    &#34;Cannot find &#39;quant_algo&#39; in the model&#39;s quantization config. &#34;
                    &#34;Expected either flat format (config.json) or nested format (hf_quant_config.json).&#34;
                )

        if not quant_method in [&#34;FP8&#34;, &#34;NVFP4&#34;]:
            raise ValueError(
                f&#34;ModelOpt currently only supports: FP8, NVFP4&#34;
                &#34; quantizations in sglang. Please check the &#34;
                &#34;quantization config for your model&#39;s configuration.&#34;
            )
        is_checkpoint_nvfp4_serialized = &#34;NVFP4&#34; in quant_method

        if not (group_size and kv_cache_quant_algo) or exclude_modules is None:
            logger.warning(
                f&#34;group_size: {group_size},&#34;
                f&#34;kv_cache_quant_algo: {kv_cache_quant_algo},&#34;
                f&#34;exclude_modules: {exclude_modules}&#34;
            )
            raise ValueError(
                &#34;NVFP4 quantization requires group size and &#34;
                &#34;kv_cache_quant_algo specified in the quantization config&#34;
            )
        return cls(
            is_checkpoint_nvfp4_serialized,
            kv_cache_quant_algo,
            group_size,
            exclude_modules,
        )

    def is_layer_excluded(self, prefix: str, exclude_modules: list):
        import regex as re

        for pattern in exclude_modules:
            regex_str = pattern.replace(&#34;.&#34;, r&#34;\.&#34;).replace(&#34;*&#34;, r&#34;.*&#34;)
            if re.fullmatch(regex_str, prefix):
                return True

            # Check if the last part of the excluded pattern is contained in the last part of the prefix
            # This handles fused modules like fused_qkv_a_proj_with_mqa that contain q_a_proj and kv_a_proj_with_mqa
            pattern_last_part = pattern.split(&#34;.&#34;)[-1]
            prefix_last_part = prefix.split(&#34;.&#34;)[-1]
            if pattern_last_part in prefix_last_part:
                return True
        return False

    def get_quant_method(
        self, layer: torch.nn.Module, prefix: str
    ) -&gt; Optional[QuantizeMethodBase]:
        from sglang.srt.layers.linear import LinearBase
        from sglang.srt.layers.moe.fused_moe_triton import FusedMoE
        from sglang.srt.layers.moe.fused_moe_triton.layer import FlashInferFP4MoE

        if isinstance(layer, LinearBase):
            if is_layer_skipped(prefix, self.exclude_modules) or self.is_layer_excluded(
                prefix, self.exclude_modules
            ):
                return UnquantizedLinearMethod()
            return ModelOptFp4LinearMethod(self)
        if self.kv_cache_quant_algo and isinstance(layer, RadixAttention):
            return ModelOptFp8KVCacheMethod(self)
        elif isinstance(layer, FlashInferFP4MoE):
            # FlashInferFP4MoE needs the same quantization method but with compatible attribute handling
            return ModelOptNvFp4FusedMoEMethod(self)
        elif isinstance(layer, FusedMoE):
            return ModelOptNvFp4FusedMoEMethod(self)
        return None

    def get_scaled_act_names(self) -&gt; List[str]:
        return []</code></pre>
</details>
<div class="desc"><p>Config class for FP4.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp4Config.is_layer_excluded"><code class="name flex">
<span>def <span class="ident">is_layer_excluded</span></span>(<span>self, prefix: str, exclude_modules: list)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_layer_excluded(self, prefix: str, exclude_modules: list):
    import regex as re

    for pattern in exclude_modules:
        regex_str = pattern.replace(&#34;.&#34;, r&#34;\.&#34;).replace(&#34;*&#34;, r&#34;.*&#34;)
        if re.fullmatch(regex_str, prefix):
            return True

        # Check if the last part of the excluded pattern is contained in the last part of the prefix
        # This handles fused modules like fused_qkv_a_proj_with_mqa that contain q_a_proj and kv_a_proj_with_mqa
        pattern_last_part = pattern.split(&#34;.&#34;)[-1]
        prefix_last_part = prefix.split(&#34;.&#34;)[-1]
        if pattern_last_part in prefix_last_part:
            return True
    return False</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.from_config" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.from_config">from_config</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.get_config_filenames" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.get_config_filenames">get_config_filenames</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.get_from_keys" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.get_from_keys">get_from_keys</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.get_from_keys_or" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.get_from_keys_or">get_from_keys_or</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.get_min_capability" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.get_min_capability">get_min_capability</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.get_name" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.get_name">get_name</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.get_quant_method" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.get_quant_method">get_quant_method</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.get_scaled_act_names" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.get_scaled_act_names">get_scaled_act_names</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.get_supported_act_dtypes" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.get_supported_act_dtypes">get_supported_act_dtypes</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.override_quantization_method" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.override_quantization_method">override_quantization_method</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp4LinearMethod"><code class="flex name class">
<span>class <span class="ident">ModelOptFp4LinearMethod</span></span>
<span>(</span><span>quant_config: <a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp4Config" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptFp4Config">ModelOptFp4Config</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelOptFp4LinearMethod(LinearMethodBase):
    &#34;&#34;&#34;Linear method for NVFP4.
    Supports loading NVFP4 checkpoints with the following structure:

    |Tensor Name           | datatype      |  shape      |
    |----------------------------------------------------|
    |input_scale           | torch.float32 | scalar      |
    |weight                | NVFP4(SE2M1)  | [1, X, y/2] |
    |weight_scale          | FP8-E4M3      | [X, Y]      |
    |weight_scale_2        | torch.float32 | scalar      |

    The weights are quantized per block of 16 elements.
    Args: quant_config: The ModelOpt quantization config.
    &#34;&#34;&#34;

    def __init__(self, quant_config: ModelOptFp4Config):
        self.quant_config = quant_config

    def create_weights(
        self,
        layer: torch.nn.Module,
        input_size_per_partition: int,
        output_partition_sizes: List[int],
        input_size: int,
        output_size: int,
        params_dtype: torch.dtype,
        **extra_weight_attrs,
    ):
        del input_size, output_size
        if not self.quant_config.is_checkpoint_nvfp4_serialized:
            raise ValueError(
                &#34;NVFP4 quantization was selected, &#34;
                &#34; dynamic quantization is not supported.&#34;
            )

        output_size_per_partition = sum(output_partition_sizes)
        weight_loader = extra_weight_attrs.get(&#34;weight_loader&#34;)

        layer.logical_widths = output_partition_sizes

        layer.input_size_per_partition = input_size_per_partition
        layer.output_size_per_partition = output_size_per_partition
        if input_size_per_partition % 16 != 0:
            raise ValueError(
                &#34;Unsupported model when in features size is &#34; &#34;not multiple of 16&#34;
            )

        weight_dtype = (
            torch.float8_e4m3fn
            if self.quant_config.is_checkpoint_nvfp4_serialized
            else params_dtype
        )

        weight = ModelWeightParameter(
            data=torch.empty(
                # 2 fp4 data is packed in one uint8 in the input dimension
                output_size_per_partition,
                input_size_per_partition // 2,
                dtype=torch.uint8,
            ),
            input_dim=1,
            output_dim=0,
            weight_loader=weight_loader,
        )
        layer.register_parameter(&#34;weight&#34;, weight)

        input_scale = PerTensorScaleParameter(
            data=torch.empty(len(output_partition_sizes), dtype=torch.float32),
            weight_loader=weight_loader,
        )

        layer.register_parameter(&#34;input_scale&#34;, input_scale)

        weight_scale_2 = PerTensorScaleParameter(
            data=torch.empty(len(output_partition_sizes), dtype=torch.float32),
            weight_loader=weight_loader,
        )
        layer.register_parameter(&#34;weight_scale_2&#34;, weight_scale_2)

        weight_scale = ModelWeightParameter(
            data=torch.empty(
                output_size_per_partition,
                input_size_per_partition // self.quant_config.group_size,
                dtype=weight_dtype,
            ),
            input_dim=1,
            output_dim=0,
            weight_loader=weight_loader,
        )

        layer.register_parameter(&#34;weight_scale&#34;, weight_scale)

    def process_weights_after_loading(self, layer: torch.nn.Module) -&gt; None:
        input_scale_2 = layer.input_scale.max().to(torch.float32)
        weight_scale_2 = layer.weight_scale_2.max().to(torch.float32)
        layer.input_scale = Parameter(input_scale_2, requires_grad=False)
        layer.weight_scale_2 = Parameter(weight_scale_2, requires_grad=False)
        layer.alpha = Parameter(
            layer.input_scale * layer.weight_scale_2, requires_grad=False
        )
        layer.input_scale_inv = Parameter(
            (1 / input_scale_2).to(torch.float32), requires_grad=False
        )

        # Pad and blockwise interleave weight_scale
        scales = layer.weight_scale
        scale_ndim = scales.ndim
        if scale_ndim == 2:
            scales = scales.unsqueeze(0)
        assert scales.ndim == 3
        B, M, K = scales.shape
        round_up_multiple = lambda x, m: (x + m - 1) // m * m
        M_padded = round_up_multiple(M, 128)
        K_padded = round_up_multiple(K, 4)
        padded_scales = torch.zeros((B, M_padded, K_padded), dtype=scales.dtype)
        padded_scales[:B, :M, :K] = scales
        batches, rows, cols = padded_scales.shape
        assert rows % 128 == 0
        assert cols % 4 == 0
        padded_scales = padded_scales.reshape(batches, rows // 128, 4, 32, cols // 4, 4)
        padded_scales = padded_scales.permute((0, 1, 4, 3, 2, 5))
        padded_scales = padded_scales.contiguous().cuda()
        padded_scales = (
            padded_scales.reshape(M_padded, K_padded)
            if scale_ndim == 2
            else padded_scales.reshape(B, M_padded, K_padded)
        )
        layer.weight_scale_interleaved = Parameter(padded_scales, requires_grad=False)

    def apply(
        self,
        layer: torch.nn.Module,
        x: torch.Tensor,
        bias: Optional[torch.Tensor] = None,
    ) -&gt; torch.Tensor:
        output_dtype = x.dtype
        x_m, _ = x.shape
        w_n, _ = layer.weight.shape
        output_shape = [x_m, w_n]

        # Quantize BF16 or FP16 to (FP4 and interleaved block scale)
        x_fp4, x_scale_interleaved = scaled_fp4_quant(x, layer.input_scale_inv)

        assert x_fp4.dtype == torch.uint8
        assert x_scale_interleaved.dtype == torch.float8_e4m3fn
        assert layer.weight.dtype == torch.uint8
        assert layer.weight_scale_interleaved.dtype == torch.float8_e4m3fn
        assert layer.alpha.dtype == torch.float32

        w = layer.weight
        w_scale_interleaved = layer.weight_scale_interleaved
        if enable_flashinfer_fp4_gemm:
            w = layer.weight.T
            w_scale_interleaved = layer.weight_scale_interleaved.T
        out = fp4_gemm(
            x_fp4,
            w,
            x_scale_interleaved,
            w_scale_interleaved,
            layer.alpha,
            output_dtype,
        )
        if bias is not None:
            out = out + bias
        return out.view(*output_shape)</code></pre>
</details>
<div class="desc"><p>Linear method for NVFP4.
Supports loading NVFP4 checkpoints with the following structure:</p>
<p>|Tensor Name
| datatype
|
shape
|
|----------------------------------------------------|
|input_scale
| torch.float32 | scalar
|
|weight
| NVFP4(SE2M1)
| [1, X, y/2] |
|weight_scale
| FP8-E4M3
| [X, Y]
|
|weight_scale_2
| torch.float32 | scalar
|</p>
<p>The weights are quantized per block of 16 elements.
Args: quant_config: The ModelOpt quantization config.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.quantization.base_config.LinearMethodBase" href="base_config.html#sglang.srt.layers.quantization.base_config.LinearMethodBase">LinearMethodBase</a></li>
<li><a title="sglang.srt.layers.quantization.base_config.QuantizeMethodBase" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizeMethodBase">QuantizeMethodBase</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.layers.quantization.base_config.LinearMethodBase" href="base_config.html#sglang.srt.layers.quantization.base_config.LinearMethodBase">LinearMethodBase</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.layers.quantization.base_config.LinearMethodBase.apply" href="base_config.html#sglang.srt.layers.quantization.base_config.LinearMethodBase.apply">apply</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.LinearMethodBase.create_weights" href="base_config.html#sglang.srt.layers.quantization.base_config.LinearMethodBase.create_weights">create_weights</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.LinearMethodBase.process_weights_after_loading" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizeMethodBase.process_weights_after_loading">process_weights_after_loading</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8Config"><code class="flex name class">
<span>class <span class="ident">ModelOptFp8Config</span></span>
<span>(</span><span>is_checkpoint_fp8_serialized: bool = False,<br>kv_cache_quant_method: Optional[str] = None,<br>exclude_modules: Optional[List[str]] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelOptFp8Config(QuantizationConfig):
    &#34;&#34;&#34;Configuration for ModelOpt FP8 quantization, including serialization and compatibility checks.&#34;&#34;&#34;

    def __init__(
        self,
        is_checkpoint_fp8_serialized: bool = False,
        kv_cache_quant_method: Optional[str] = None,
        exclude_modules: Optional[List[str]] = None,
    ) -&gt; None:
        &#34;&#34;&#34;
        Args:
            is_checkpoint_fp8_serialized (bool): Indicates if the checkpoint uses serialized FP8 format.
        &#34;&#34;&#34;
        self.is_checkpoint_fp8_serialized = is_checkpoint_fp8_serialized
        self.kv_cache_quant_method = kv_cache_quant_method
        self.exclude_modules = exclude_modules
        if is_checkpoint_fp8_serialized:
            logger.warning(
                &#34;Detected ModelOpt FP8 checkpoint. The format is experimental and subject to change.&#34;
            )

    @classmethod
    def get_name(cls) -&gt; str:
        return &#34;modelopt&#34;

    @classmethod
    def get_supported_act_dtypes(cls) -&gt; List[torch.dtype]:
        return [torch.bfloat16, torch.half]

    @classmethod
    def get_min_capability(cls) -&gt; int:
        return 89  # Minimum hardware capability (e.g., Hopper GPUs).

    @classmethod
    def get_config_filenames(cls) -&gt; List[str]:
        return [&#34;hf_quant_config.json&#34;]

    @classmethod
    def from_config(cls, config: Dict[str, Any]) -&gt; ModelOptFp8Config:
        # Handle two different config formats:
        # 1. hf_quant_config.json format: {&#34;quantization&#34;: {&#34;quant_algo&#34;: &#34;FP8&#34;, ...}}
        # 2. config.json quantization_config format: {&#34;quant_algo&#34;: &#34;FP8&#34;, ...}
        # In future modelopt will deprecate hf_quant_config.json, and only keep config.json.
        # For legacy reasons, we keep hf_quant_config.json for now.

        # Initialize variables
        kv_cache_quant_method = None
        exclude_modules = None

        # Try flat format first (config.json quantization_config - preferred format)
        quant_method = config.get(&#34;quant_algo&#34;)
        if quant_method is not None:
            # Flat format (config.json quantization_config)
            # For kv_cache, check if kv_cache_scheme exists and extract algo
            kv_cache_scheme = config.get(&#34;kv_cache_scheme&#34;)
            if (
                kv_cache_scheme
                and kv_cache_scheme.get(&#34;type&#34;) == &#34;float&#34;
                and kv_cache_scheme.get(&#34;num_bits&#34;) == 8
            ):
                kv_cache_quant_method = &#34;FP8&#34;

            # Map &#39;ignore&#39; field to &#39;exclude_modules&#39;
            exclude_modules = config.get(&#34;ignore&#34;)
        else:
            # Fall back to nested format (hf_quant_config.json - legacy format)
            try:
                quantization_section = cls.get_from_keys(config, [&#34;quantization&#34;])
                quant_method = quantization_section.get(&#34;quant_algo&#34;)
                kv_cache_quant_method = quantization_section.get(&#34;kv_cache_quant_algo&#34;)
                exclude_modules = quantization_section.get(&#34;exclude_modules&#34;)
            except ValueError:
                raise ValueError(
                    &#34;Cannot find &#39;quant_algo&#39; in the model&#39;s quantization config. &#34;
                    &#34;Expected either flat format (config.json) or nested format (hf_quant_config.json).&#34;
                )
        if quant_method is None:
            raise ValueError(
                &#34;Cannot find &#39;quant_algo&#39; in the model&#39;s quantization config. &#34;
            )
        if &#34;FP8&#34; not in quant_method:
            raise ValueError(
                &#34;ModelOptFp8Config only supports static FP8 quantization in SGLang. &#34;
                &#34;For FP4 quantization, use ModelOptFp4Config. &#34;
                &#34;Check the quantization config for your model&#39;s configuration.&#34;
            )

        return cls(
            is_checkpoint_fp8_serialized=True,
            kv_cache_quant_method=kv_cache_quant_method,
            exclude_modules=exclude_modules,
        )

    def get_quant_method(
        self, layer: torch.nn.Module, prefix: str
    ) -&gt; Optional[QuantizeMethodBase]:

        from sglang.srt.layers.linear import LinearBase
        from sglang.srt.layers.moe.fused_moe_triton import FusedMoE

        if self.exclude_modules and any(
            module in prefix
            or (
                prefix.startswith(&#34;language_model.&#34;)
                and module in prefix.removeprefix(&#34;language_model.&#34;)
            )
            for module in self.exclude_modules
        ):
            return None

        if isinstance(layer, LinearBase):
            return ModelOptFp8LinearMethod(self)
        if self.kv_cache_quant_method and isinstance(layer, RadixAttention):
            return ModelOptFp8KVCacheMethod(self)

        if isinstance(layer, FusedMoE):
            return ModelOptFp8MoEMethod(self)

        return None

    def get_scaled_act_names(self) -&gt; List[str]:
        return []</code></pre>
</details>
<div class="desc"><p>Configuration for ModelOpt FP8 quantization, including serialization and compatibility checks.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>is_checkpoint_fp8_serialized</code></strong> :&ensp;<code>bool</code></dt>
<dd>Indicates if the checkpoint uses serialized FP8 format.</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.from_config" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.from_config">from_config</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.get_config_filenames" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.get_config_filenames">get_config_filenames</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.get_from_keys" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.get_from_keys">get_from_keys</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.get_from_keys_or" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.get_from_keys_or">get_from_keys_or</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.get_min_capability" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.get_min_capability">get_min_capability</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.get_name" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.get_name">get_name</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.get_quant_method" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.get_quant_method">get_quant_method</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.get_scaled_act_names" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.get_scaled_act_names">get_scaled_act_names</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.get_supported_act_dtypes" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.get_supported_act_dtypes">get_supported_act_dtypes</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.QuantizationConfig.override_quantization_method" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig.override_quantization_method">override_quantization_method</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8KVCacheMethod"><code class="flex name class">
<span>class <span class="ident">ModelOptFp8KVCacheMethod</span></span>
<span>(</span><span>quant_config: <a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8Config" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8Config">ModelOptFp8Config</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelOptFp8KVCacheMethod(BaseKVCacheMethod):
    &#34;&#34;&#34;
    Handles loading FP8 kv-cache scaling factors from modelopt quantized checkpoints.
    &#34;&#34;&#34;

    def __init__(self, quant_config: ModelOptFp8Config):
        super().__init__(quant_config)</code></pre>
</details>
<div class="desc"><p>Handles loading FP8 kv-cache scaling factors from modelopt quantized checkpoints.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.quantization.kv_cache.BaseKVCacheMethod" href="kv_cache.html#sglang.srt.layers.quantization.kv_cache.BaseKVCacheMethod">BaseKVCacheMethod</a></li>
<li><a title="sglang.srt.layers.quantization.base_config.QuantizeMethodBase" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizeMethodBase">QuantizeMethodBase</a></li>
<li>abc.ABC</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.layers.quantization.kv_cache.BaseKVCacheMethod" href="kv_cache.html#sglang.srt.layers.quantization.kv_cache.BaseKVCacheMethod">BaseKVCacheMethod</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.layers.quantization.kv_cache.BaseKVCacheMethod.apply" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizeMethodBase.apply">apply</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.kv_cache.BaseKVCacheMethod.create_weights" href="kv_cache.html#sglang.srt.layers.quantization.kv_cache.BaseKVCacheMethod.create_weights">create_weights</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.kv_cache.BaseKVCacheMethod.process_weights_after_loading" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizeMethodBase.process_weights_after_loading">process_weights_after_loading</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8LinearMethod"><code class="flex name class">
<span>class <span class="ident">ModelOptFp8LinearMethod</span></span>
<span>(</span><span>quant_config: <a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8Config" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8Config">ModelOptFp8Config</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelOptFp8LinearMethod(LinearMethodBase):
    &#34;&#34;&#34;Linear method for ModelOpt static FP8 quantization.

    Supports loading FP8 checkpoints with static weight and activation scales.
    Future support may include dynamic scales.

    **Limitations**:
    1. Only supports per-tensor quantization due to `torch._scaled_mm` limitations.
    2. Only supports the `float8_e4m3fn` data type.

    Args:
        quant_config (ModelOptFp8Config): The ModelOpt quantization configuration.
    &#34;&#34;&#34;

    def __init__(self, quant_config: ModelOptFp8Config):
        super().__init__()
        self.quant_config = quant_config
        self.cutlass_fp8_supported = cutlass_fp8_supported()

    def create_weights(
        self,
        layer: torch.nn.Module,
        input_size_per_partition: int,
        output_partition_sizes: List[int],
        params_dtype: torch.dtype,
        **extra_weight_attrs,
    ) -&gt; None:
        &#34;&#34;&#34;Creates and registers weights, weight scales, and input scales for FP8 quantization.&#34;&#34;&#34;
        output_size_per_partition = sum(output_partition_sizes)
        weight_loader = extra_weight_attrs.get(&#34;weight_loader&#34;)
        weight_dtype = (
            torch.float8_e4m3fn
            if self.quant_config.is_checkpoint_fp8_serialized
            else params_dtype
        )

        # Set layer attributes
        layer.logical_widths = output_partition_sizes
        layer.input_size_per_partition = input_size_per_partition
        layer.output_size_per_partition = output_size_per_partition

        # Register weight
        layer.register_parameter(
            &#34;weight&#34;,
            ModelWeightParameter(
                data=torch.empty(
                    output_size_per_partition,
                    input_size_per_partition,
                    dtype=weight_dtype,
                ),
                input_dim=1,
                output_dim=0,
                weight_loader=weight_loader,
            ),
        )

        if self.quant_config.is_checkpoint_fp8_serialized:
            # Register weight and input scales
            for scale_name in [&#34;weight_scale&#34;, &#34;input_scale&#34;]:
                layer.register_parameter(
                    scale_name,
                    PerTensorScaleParameter(
                        data=torch.full(
                            (len(output_partition_sizes),),
                            torch.finfo(torch.float32).min,
                            dtype=torch.float32,
                        ),
                        weight_loader=weight_loader,
                    ),
                )

    def process_weights_after_loading(self, layer: torch.nn.Module) -&gt; None:
        &#34;&#34;&#34;Requantizes weights after loading using the maximum scale.&#34;&#34;&#34;
        max_w_scale, quantized_weight = requantize_with_max_scale(
            layer.weight, layer.weight_scale, layer.logical_widths
        )
        layer.weight = Parameter(quantized_weight.t(), requires_grad=False)
        # cutlass sgl-kernel only supports per-channel scale
        if self.cutlass_fp8_supported:
            max_w_scale = convert_to_channelwise(max_w_scale, layer.logical_widths)
        layer.weight_scale = Parameter(max_w_scale, requires_grad=False)
        layer.input_scale = Parameter(layer.input_scale.max(), requires_grad=False)

    def apply(
        self,
        layer: torch.nn.Module,
        x: torch.Tensor,
        bias: Optional[torch.Tensor] = None,
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;Applies FP8 linear transformation.&#34;&#34;&#34;
        return apply_fp8_linear(
            input=x,
            weight=layer.weight,
            weight_scale=layer.weight_scale,
            input_scale=layer.input_scale,
            bias=bias,
            cutlass_fp8_supported=self.cutlass_fp8_supported,
        )</code></pre>
</details>
<div class="desc"><p>Linear method for ModelOpt static FP8 quantization.</p>
<p>Supports loading FP8 checkpoints with static weight and activation scales.
Future support may include dynamic scales.</p>
<p><strong>Limitations</strong>:
1. Only supports per-tensor quantization due to <code>torch._scaled_mm</code> limitations.
2. Only supports the <code>float8_e4m3fn</code> data type.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>quant_config</code></strong> :&ensp;<code><a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8Config" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8Config">ModelOptFp8Config</a></code></dt>
<dd>The ModelOpt quantization configuration.</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.quantization.base_config.LinearMethodBase" href="base_config.html#sglang.srt.layers.quantization.base_config.LinearMethodBase">LinearMethodBase</a></li>
<li><a title="sglang.srt.layers.quantization.base_config.QuantizeMethodBase" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizeMethodBase">QuantizeMethodBase</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8LinearMethod.apply"><code class="name flex">
<span>def <span class="ident">apply</span></span>(<span>self,<br>layer: torch.nn.Module,<br>x: torch.Tensor,<br>bias: Optional[torch.Tensor] = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply(
    self,
    layer: torch.nn.Module,
    x: torch.Tensor,
    bias: Optional[torch.Tensor] = None,
) -&gt; torch.Tensor:
    &#34;&#34;&#34;Applies FP8 linear transformation.&#34;&#34;&#34;
    return apply_fp8_linear(
        input=x,
        weight=layer.weight,
        weight_scale=layer.weight_scale,
        input_scale=layer.input_scale,
        bias=bias,
        cutlass_fp8_supported=self.cutlass_fp8_supported,
    )</code></pre>
</details>
<div class="desc"><p>Applies FP8 linear transformation.</p></div>
</dd>
<dt id="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8LinearMethod.create_weights"><code class="name flex">
<span>def <span class="ident">create_weights</span></span>(<span>self,<br>layer: torch.nn.Module,<br>input_size_per_partition: int,<br>output_partition_sizes: List[int],<br>params_dtype: torch.dtype,<br>**extra_weight_attrs) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_weights(
    self,
    layer: torch.nn.Module,
    input_size_per_partition: int,
    output_partition_sizes: List[int],
    params_dtype: torch.dtype,
    **extra_weight_attrs,
) -&gt; None:
    &#34;&#34;&#34;Creates and registers weights, weight scales, and input scales for FP8 quantization.&#34;&#34;&#34;
    output_size_per_partition = sum(output_partition_sizes)
    weight_loader = extra_weight_attrs.get(&#34;weight_loader&#34;)
    weight_dtype = (
        torch.float8_e4m3fn
        if self.quant_config.is_checkpoint_fp8_serialized
        else params_dtype
    )

    # Set layer attributes
    layer.logical_widths = output_partition_sizes
    layer.input_size_per_partition = input_size_per_partition
    layer.output_size_per_partition = output_size_per_partition

    # Register weight
    layer.register_parameter(
        &#34;weight&#34;,
        ModelWeightParameter(
            data=torch.empty(
                output_size_per_partition,
                input_size_per_partition,
                dtype=weight_dtype,
            ),
            input_dim=1,
            output_dim=0,
            weight_loader=weight_loader,
        ),
    )

    if self.quant_config.is_checkpoint_fp8_serialized:
        # Register weight and input scales
        for scale_name in [&#34;weight_scale&#34;, &#34;input_scale&#34;]:
            layer.register_parameter(
                scale_name,
                PerTensorScaleParameter(
                    data=torch.full(
                        (len(output_partition_sizes),),
                        torch.finfo(torch.float32).min,
                        dtype=torch.float32,
                    ),
                    weight_loader=weight_loader,
                ),
            )</code></pre>
</details>
<div class="desc"><p>Creates and registers weights, weight scales, and input scales for FP8 quantization.</p></div>
</dd>
<dt id="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8LinearMethod.process_weights_after_loading"><code class="name flex">
<span>def <span class="ident">process_weights_after_loading</span></span>(<span>self, layer: torch.nn.Module) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_weights_after_loading(self, layer: torch.nn.Module) -&gt; None:
    &#34;&#34;&#34;Requantizes weights after loading using the maximum scale.&#34;&#34;&#34;
    max_w_scale, quantized_weight = requantize_with_max_scale(
        layer.weight, layer.weight_scale, layer.logical_widths
    )
    layer.weight = Parameter(quantized_weight.t(), requires_grad=False)
    # cutlass sgl-kernel only supports per-channel scale
    if self.cutlass_fp8_supported:
        max_w_scale = convert_to_channelwise(max_w_scale, layer.logical_widths)
    layer.weight_scale = Parameter(max_w_scale, requires_grad=False)
    layer.input_scale = Parameter(layer.input_scale.max(), requires_grad=False)</code></pre>
</details>
<div class="desc"><p>Requantizes weights after loading using the maximum scale.</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8MoEMethod"><code class="flex name class">
<span>class <span class="ident">ModelOptFp8MoEMethod</span></span>
<span>(</span><span>quant_config: <a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8Config" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8Config">ModelOptFp8Config</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelOptFp8MoEMethod(FusedMoEMethodBase):
    &#34;&#34;&#34;MoE method for ModelOpt FP8.
    Supports loading FP8 checkpoints with static weight scale and activation scale.

    Args:
        quant_config: The ModelOpt quantization config.
    &#34;&#34;&#34;

    def __init__(self, quant_config: ModelOptFp8Config):
        self.quant_config = quant_config
        self.cutlass_fp8_supported = cutlass_fp8_supported()

    def create_weights(
        self,
        layer: torch.nn.Module,
        num_experts: int,
        hidden_size: int,
        intermediate_size: int,
        params_dtype: torch.dtype,
        **extra_weight_attrs,
    ):
        from sglang.srt.layers.moe.fused_moe_triton import FusedMoeWeightScaleSupported

        # Use FP8 dtype if checkpoint is serialized, otherwise use the default dtype
        weight_dtype = (
            torch.float8_e4m3fn
            if self.quant_config.is_checkpoint_fp8_serialized
            else params_dtype
        )
        weight_loader = extra_weight_attrs.get(&#34;weight_loader&#34;)

        w13_weight = ModelWeightParameter(
            data=torch.empty(
                num_experts, 2 * intermediate_size, hidden_size, dtype=weight_dtype
            ),
            input_dim=2,
            output_dim=1,
            weight_loader=weight_loader,
        )
        layer.register_parameter(&#34;w13_weight&#34;, w13_weight)

        w2_weight = ModelWeightParameter(
            data=torch.empty(
                num_experts, hidden_size, intermediate_size, dtype=weight_dtype
            ),
            input_dim=2,
            output_dim=1,
            weight_loader=weight_loader,
        )
        layer.register_parameter(&#34;w2_weight&#34;, w2_weight)

        if self.quant_config.is_checkpoint_fp8_serialized:
            # WEIGHT SCALES - Per-tensor scaling for ModelOpts
            # Allocate 2 scales for w1 and w3 respectively.
            # They will be combined to a single scale after weight loading.
            w13_weight_scale = PerTensorScaleParameter(
                data=torch.full(
                    (num_experts, 2),
                    torch.finfo(torch.float32).min,
                    dtype=torch.float32,
                ),
                weight_loader=weight_loader,
            )
            w2_weight_scale = PerTensorScaleParameter(
                data=torch.full(
                    (num_experts,), torch.finfo(torch.float32).min, dtype=torch.float32
                ),
                weight_loader=weight_loader,
            )
            layer.register_parameter(&#34;w13_weight_scale&#34;, w13_weight_scale)
            layer.register_parameter(&#34;w2_weight_scale&#34;, w2_weight_scale)

            # Set weight loader attributes for scales
            extra_weight_attrs.update(
                {&#34;quant_method&#34;: FusedMoeWeightScaleSupported.TENSOR.value}
            )

            # INPUT SCALES - Per-tensor scaling for ModelOpt
            w13_input_scale = PerTensorScaleParameter(
                data=torch.full((num_experts,), 1.0, dtype=torch.float32),
                weight_loader=weight_loader,
            )
            w2_input_scale = PerTensorScaleParameter(
                data=torch.full((num_experts,), 1.0, dtype=torch.float32),
                weight_loader=weight_loader,
            )
            layer.register_parameter(&#34;w13_input_scale&#34;, w13_input_scale)
            layer.register_parameter(&#34;w2_input_scale&#34;, w2_input_scale)

    def process_weights_after_loading(self, layer: torch.nn.Module) -&gt; None:
        &#34;&#34;&#34;Process FP8 MoE weights after loading from serialized checkpoint.

        Only supports pre-quantized checkpoints with FP8 weights and scales.
        &#34;&#34;&#34;

        layer.w13_weight = Parameter(layer.w13_weight.data, requires_grad=False)
        layer.w2_weight = Parameter(layer.w2_weight.data, requires_grad=False)

        # Handle scale parameters
        if hasattr(layer, &#34;w13_weight_scale&#34;) and layer.w13_weight_scale is not None:
            # Fp8 moe kernel needs single weight scale for w13 per expert.
            # We take the max of the w1 and w3 scales then dequant and requant each expert.
            if layer.w13_weight_scale.dim() == 2:  # Shape: (num_experts, 2)
                from sglang.srt.layers.quantization.fp8_kernel import scaled_fp8_quant

                # Get the maximum scale across w1 and w3 for each expert
                max_w13_scales = layer.w13_weight_scale.max(dim=1).values

                # Requantize each expert&#39;s weights using the combined scale
                # w13_weight has shape (num_experts, 2 * intermediate_size, hidden_size)
                # where the first intermediate_size rows are w1, the next are w3
                intermediate_size = layer.w13_weight.shape[1] // 2
                for expert_id in range(layer.w13_weight.shape[0]):
                    start = 0
                    for shard_id in range(2):  # w1 and w3
                        # Dequantize using the original scale for this shard
                        dq_weight = per_tensor_dequantize(
                            layer.w13_weight[expert_id][
                                start : start + intermediate_size, :
                            ],
                            layer.w13_weight_scale[expert_id][shard_id],
                        )
                        # Requantize using the combined max scale
                        (
                            layer.w13_weight[expert_id][
                                start : start + intermediate_size, :
                            ],
                            _,
                        ) = scaled_fp8_quant(dq_weight, max_w13_scales[expert_id])

                        start += intermediate_size

                # Update the scale parameter to be per-expert instead of per-shard
                layer.w13_weight_scale = Parameter(max_w13_scales, requires_grad=False)
            else:
                layer.w13_weight_scale = Parameter(
                    layer.w13_weight_scale.data, requires_grad=False
                )

        if hasattr(layer, &#34;w2_weight_scale&#34;) and layer.w2_weight_scale is not None:
            layer.w2_weight_scale = Parameter(
                layer.w2_weight_scale.data, requires_grad=False
            )
        if hasattr(layer, &#34;w13_input_scale&#34;) and layer.w13_input_scale is not None:
            layer.w13_input_scale = Parameter(
                layer.w13_input_scale.max(), requires_grad=False
            )
        if hasattr(layer, &#34;w2_input_scale&#34;) and layer.w2_input_scale is not None:
            layer.w2_input_scale = Parameter(
                layer.w2_input_scale.max(), requires_grad=False
            )

    def apply(
        self,
        layer: torch.nn.Module,
        x: torch.Tensor,
        topk_output: TopKOutput,
        moe_runner_config: MoeRunnerConfig,
    ) -&gt; torch.Tensor:
        from sglang.srt.layers.moe.fused_moe_triton.fused_moe import fused_experts

        return fused_experts(
            x,
            layer.w13_weight,
            layer.w2_weight,
            topk_output=topk_output,
            moe_runner_config=moe_runner_config,
            use_fp8_w8a8=True,
            per_channel_quant=False,  # ModelOpt uses per-tensor quantization
            w1_scale=layer.w13_weight_scale,
            w2_scale=layer.w2_weight_scale,
            a1_scale=layer.w13_input_scale,
            a2_scale=layer.w2_input_scale,
        )</code></pre>
</details>
<div class="desc"><p>MoE method for ModelOpt FP8.
Supports loading FP8 checkpoints with static weight scale and activation scale.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>quant_config</code></strong></dt>
<dd>The ModelOpt quantization config.</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.quantization.base_config.FusedMoEMethodBase" href="base_config.html#sglang.srt.layers.quantization.base_config.FusedMoEMethodBase">FusedMoEMethodBase</a></li>
<li><a title="sglang.srt.layers.quantization.base_config.QuantizeMethodBase" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizeMethodBase">QuantizeMethodBase</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8MoEMethod.process_weights_after_loading"><code class="name flex">
<span>def <span class="ident">process_weights_after_loading</span></span>(<span>self, layer: torch.nn.Module) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_weights_after_loading(self, layer: torch.nn.Module) -&gt; None:
    &#34;&#34;&#34;Process FP8 MoE weights after loading from serialized checkpoint.

    Only supports pre-quantized checkpoints with FP8 weights and scales.
    &#34;&#34;&#34;

    layer.w13_weight = Parameter(layer.w13_weight.data, requires_grad=False)
    layer.w2_weight = Parameter(layer.w2_weight.data, requires_grad=False)

    # Handle scale parameters
    if hasattr(layer, &#34;w13_weight_scale&#34;) and layer.w13_weight_scale is not None:
        # Fp8 moe kernel needs single weight scale for w13 per expert.
        # We take the max of the w1 and w3 scales then dequant and requant each expert.
        if layer.w13_weight_scale.dim() == 2:  # Shape: (num_experts, 2)
            from sglang.srt.layers.quantization.fp8_kernel import scaled_fp8_quant

            # Get the maximum scale across w1 and w3 for each expert
            max_w13_scales = layer.w13_weight_scale.max(dim=1).values

            # Requantize each expert&#39;s weights using the combined scale
            # w13_weight has shape (num_experts, 2 * intermediate_size, hidden_size)
            # where the first intermediate_size rows are w1, the next are w3
            intermediate_size = layer.w13_weight.shape[1] // 2
            for expert_id in range(layer.w13_weight.shape[0]):
                start = 0
                for shard_id in range(2):  # w1 and w3
                    # Dequantize using the original scale for this shard
                    dq_weight = per_tensor_dequantize(
                        layer.w13_weight[expert_id][
                            start : start + intermediate_size, :
                        ],
                        layer.w13_weight_scale[expert_id][shard_id],
                    )
                    # Requantize using the combined max scale
                    (
                        layer.w13_weight[expert_id][
                            start : start + intermediate_size, :
                        ],
                        _,
                    ) = scaled_fp8_quant(dq_weight, max_w13_scales[expert_id])

                    start += intermediate_size

            # Update the scale parameter to be per-expert instead of per-shard
            layer.w13_weight_scale = Parameter(max_w13_scales, requires_grad=False)
        else:
            layer.w13_weight_scale = Parameter(
                layer.w13_weight_scale.data, requires_grad=False
            )

    if hasattr(layer, &#34;w2_weight_scale&#34;) and layer.w2_weight_scale is not None:
        layer.w2_weight_scale = Parameter(
            layer.w2_weight_scale.data, requires_grad=False
        )
    if hasattr(layer, &#34;w13_input_scale&#34;) and layer.w13_input_scale is not None:
        layer.w13_input_scale = Parameter(
            layer.w13_input_scale.max(), requires_grad=False
        )
    if hasattr(layer, &#34;w2_input_scale&#34;) and layer.w2_input_scale is not None:
        layer.w2_input_scale = Parameter(
            layer.w2_input_scale.max(), requires_grad=False
        )</code></pre>
</details>
<div class="desc"><p>Process FP8 MoE weights after loading from serialized checkpoint.</p>
<p>Only supports pre-quantized checkpoints with FP8 weights and scales.</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.layers.quantization.base_config.FusedMoEMethodBase" href="base_config.html#sglang.srt.layers.quantization.base_config.FusedMoEMethodBase">FusedMoEMethodBase</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.layers.quantization.base_config.FusedMoEMethodBase.apply" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizeMethodBase.apply">apply</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.FusedMoEMethodBase.create_weights" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizeMethodBase.create_weights">create_weights</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.layers.quantization.modelopt_quant.ModelOptNvFp4FusedMoEMethod"><code class="flex name class">
<span>class <span class="ident">ModelOptNvFp4FusedMoEMethod</span></span>
<span>(</span><span>quant_config: <a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp4Config" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptFp4Config">ModelOptFp4Config</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelOptNvFp4FusedMoEMethod(FusedMoEMethodBase):
    &#34;&#34;&#34;
       MoE Method for FP4 Quantization with Blockscales and PerTensorScales
    Args:
        quant_config: NVFP4 Quant Config
    &#34;&#34;&#34;

    def __init__(self, quant_config: ModelOptFp4Config):
        self.quant_config = quant_config
        if not is_sm100_supported():
            raise ValueError(
                &#34;Current platform does not support NVFP4&#34;
                &#34; quantization. Please use Blackwell and&#34;
                &#34; above.&#34;
            )
        self.enable_flashinfer_trtllm_moe = should_use_flashinfer_trtllm_moe()
        self._cache_permute_indices = {}

    @property
    def enable_flashinfer_cutlass_moe(self) -&gt; bool:
        from sglang.srt.layers.moe import get_moe_runner_backend

        &#34;&#34;&#34;Access the global enable_flashinfer_cutlass_moe setting.&#34;&#34;&#34;
        return get_moe_runner_backend().is_flashinfer_cutlass()

    def create_weights(
        self,
        layer: torch.nn.Module,
        num_experts: int,
        hidden_size: int,
        intermediate_size_per_partition: int,
        params_dtype: torch.dtype,
        **extra_weight_attrs,
    ):
        if not self.quant_config.is_checkpoint_nvfp4_serialized:
            raise ValueError(
                &#34;NVFP4 quantization was selected, &#34;
                &#34; dynamic quantization is not supported.&#34;
            )

        # TODO(ch-wan): check if this is needed
        layer.intermediate_size_per_partition = intermediate_size_per_partition
        layer.params_dtype = params_dtype
        layer.quant_config = self.quant_config

        weight_dtype = torch.uint8
        weight_scale_dtype = torch.float8_e4m3fn
        weight_loader = extra_weight_attrs.get(&#34;weight_loader&#34;)
        # GEMM 1
        w13_weight = ModelWeightParameter(
            data=torch.empty(
                layer.num_local_experts,
                2 * intermediate_size_per_partition,
                # 2 fp4 items are packed in the input dimension
                hidden_size // 2,
                dtype=weight_dtype,
            ),
            input_dim=1,
            output_dim=2,
            weight_loader=weight_loader,
        )
        layer.register_parameter(&#34;w13_weight&#34;, w13_weight)

        # GEMM 2
        w2_weight = ModelWeightParameter(
            data=torch.empty(
                layer.num_local_experts,
                hidden_size,
                # 2 fp4 items are packed in the input dimension
                intermediate_size_per_partition // 2,
                dtype=weight_dtype,
            ),
            input_dim=1,
            output_dim=2,
            weight_loader=weight_loader,
        )
        layer.register_parameter(&#34;w2_weight&#34;, w2_weight)

        w13_weight_scale = ModelWeightParameter(
            data=torch.empty(
                layer.num_local_experts,
                2 * intermediate_size_per_partition,
                hidden_size // self.quant_config.group_size,
                dtype=weight_scale_dtype,
            ),
            input_dim=1,
            output_dim=2,
            weight_loader=weight_loader,
        )
        layer.register_parameter(&#34;w13_weight_scale&#34;, w13_weight_scale)

        # Only use `swizzle_blockscale` for shapes, not for real content
        layer.w13_blockscale_swizzled = Parameter(
            self.swizzle_blockscale(layer.w13_weight_scale), requires_grad=False
        )

        w2_weight_scale = ModelWeightParameter(
            data=torch.empty(
                layer.num_local_experts,
                hidden_size,
                intermediate_size_per_partition // self.quant_config.group_size,
                dtype=weight_scale_dtype,
            ),
            input_dim=1,
            output_dim=2,
            weight_loader=weight_loader,
        )
        layer.register_parameter(&#34;w2_weight_scale&#34;, w2_weight_scale)

        layer.w2_blockscale_swizzled = Parameter(
            self.swizzle_blockscale(layer.w2_weight_scale), requires_grad=False
        )

        from sglang.srt.layers.moe.fused_moe_triton import FusedMoeWeightScaleSupported

        extra_weight_attrs.update(
            {&#34;quant_method&#34;: FusedMoeWeightScaleSupported.BLOCK.value}
        )

        w13_weight_scale_2 = PerTensorScaleParameter(
            data=torch.empty(layer.num_local_experts, 2, dtype=torch.float32),
            weight_loader=weight_loader,
        )
        layer.register_parameter(&#34;w13_weight_scale_2&#34;, w13_weight_scale_2)

        w2_weight_scale_2 = PerTensorScaleParameter(
            data=torch.empty(layer.num_local_experts, dtype=torch.float32),
            weight_loader=weight_loader,
        )
        layer.register_parameter(&#34;w2_weight_scale_2&#34;, w2_weight_scale_2)

        extra_weight_attrs.update(
            {&#34;quant_method&#34;: FusedMoeWeightScaleSupported.TENSOR.value}
        )

        w13_input_scale = PerTensorScaleParameter(
            data=torch.empty(layer.num_local_experts, 2, dtype=torch.float32),
            weight_loader=weight_loader,
        )
        layer.register_parameter(&#34;w13_input_scale&#34;, w13_input_scale)

        w2_input_scale = PerTensorScaleParameter(
            data=torch.empty(layer.num_local_experts, dtype=torch.float32),
            weight_loader=weight_loader,
        )
        layer.register_parameter(&#34;w2_input_scale&#34;, w2_input_scale)

    def swizzle_blockscale(self, scale: torch.Tensor):
        assert scale.dtype == torch.float8_e4m3fn
        # Pad and blockwise interleave weight_scale
        scale_ndim = scale.ndim
        if scale.ndim == 2:
            scale = scale.unsqueeze(0)
        assert scale.ndim == 3
        B, M, K = scale.shape
        round_up_multiple = lambda x, m: (x + m - 1) // m * m
        M_padded = round_up_multiple(M, 128)
        K_padded = round_up_multiple(K, 4)
        padded_scale = torch.zeros((B, M_padded, K_padded), dtype=scale.dtype)
        padded_scale[:B, :M, :K] = scale
        batches, rows, cols = padded_scale.shape
        assert rows % 128 == 0
        assert cols % 4 == 0
        padded_scale = padded_scale.reshape(batches, rows // 128, 4, 32, cols // 4, 4)
        swizzled_scale = padded_scale.permute((0, 1, 4, 3, 2, 5))
        swizzled_scale = swizzled_scale.contiguous().cuda()
        return (
            swizzled_scale.reshape(M_padded, K_padded)
            if scale_ndim == 2
            else swizzled_scale.reshape(B, M_padded, K_padded)
        )

    def prepare_static_weights_for_kernel(
        self,
        # args_dequant,
        # args,
        gemm1_weights,
        gemm2_weights,
        gemm1_scales_linear_fp4_bytes,
        gemm2_scales_linear_fp4_bytes,
        hidden_size,
        intermediate_size,
        num_experts,
    ):
        from flashinfer import (
            RoutingMethodType,
            e2m1_and_ufp8sf_scale_to_float,
            fp4_quantize,
            next_positive_power_of_2,
            nvfp4_block_scale_interleave,
            reorder_rows_for_gated_act_gemm,
            shuffle_matrix_a,
            shuffle_matrix_sf_a,
        )
        from flashinfer.fused_moe.core import (
            _maybe_get_cached_w2_permute_indices,
            _maybe_get_cached_w3_w1_permute_indices,
        )

        &#34;&#34;&#34;Prepare quantized weights for kernel (done offline with weights).&#34;&#34;&#34;
        epilogue_tile_m = 128  # FIXME: this depends on the kernel internals

        # Convert quantized weights to proper formats
        gemm1_weights_fp4 = gemm1_weights.view(torch.float8_e4m3fn).reshape(
            num_experts, 2 * intermediate_size, hidden_size // 2
        )  # packed fp4
        gemm1_scales_linear_fp4 = gemm1_scales_linear_fp4_bytes.view(
            torch.float8_e4m3fn
        ).reshape(
            num_experts, 2 * intermediate_size, hidden_size // 16
        )  # fp8 scaling factors

        gemm2_weights_fp4 = gemm2_weights.view(torch.float8_e4m3fn).reshape(
            num_experts, hidden_size, intermediate_size // 2
        )  # packed fp4
        gemm2_scales_linear_fp4 = gemm2_scales_linear_fp4_bytes.view(
            torch.float8_e4m3fn
        ).reshape(
            num_experts, hidden_size, intermediate_size // 16
        )  # fp8 scaling factors

        gemm1_weights_fp4_shuffled = []
        gemm1_scales_fp4_shuffled = []
        gemm2_weights_fp4_shuffled = []
        gemm2_scales_fp4_shuffled = []
        for i in range(num_experts):
            # Calculate the permute indices for the following:
            # 1. Reorder rows of W1 and scales for fused gated activation
            # 2. Shuffle weights and scaling factors for transposed mma output
            # for both w3_w1 and w2 weights and scale factors
            permute_indices = _maybe_get_cached_w3_w1_permute_indices(
                self._cache_permute_indices,
                gemm1_weights_fp4[i].view(torch.uint8),
                epilogue_tile_m,
            )
            gemm1_weights_fp4_shuffled.append(
                gemm1_weights_fp4[i]
                .view(torch.uint8)[permute_indices.to(gemm1_weights_fp4.device)]
                .contiguous()
            )

            permute_sf_indices = _maybe_get_cached_w3_w1_permute_indices(
                self._cache_permute_indices,
                gemm1_scales_linear_fp4[i].view(torch.uint8),
                epilogue_tile_m,
                num_elts_per_sf=16,
            )
            gemm1_scales_fp4_shuffled.append(
                nvfp4_block_scale_interleave(
                    gemm1_scales_linear_fp4[i]
                    .view(torch.uint8)[
                        permute_sf_indices.to(gemm1_scales_linear_fp4.device)
                    ]
                    .contiguous()
                )
            )

            permute_indices = _maybe_get_cached_w2_permute_indices(
                self._cache_permute_indices,
                gemm2_weights_fp4[i].view(torch.uint8),
                epilogue_tile_m,
            )
            gemm2_weights_fp4_shuffled.append(
                gemm2_weights_fp4[i]
                .view(torch.uint8)[permute_indices.to(gemm2_weights_fp4.device)]
                .contiguous()
            )

            permute_sf_indices = _maybe_get_cached_w2_permute_indices(
                self._cache_permute_indices,
                gemm2_scales_linear_fp4[i].view(torch.uint8),
                epilogue_tile_m,
                num_elts_per_sf=16,
            )
            gemm2_scales_fp4_shuffled.append(
                nvfp4_block_scale_interleave(
                    gemm2_scales_linear_fp4[i]
                    .view(torch.uint8)[
                        permute_sf_indices.to(gemm2_scales_linear_fp4.device)
                    ]
                    .contiguous()
                )
            )

        # Stack weights for all experts
        gemm1_weights_fp4_shuffled = torch.stack(gemm1_weights_fp4_shuffled)
        gemm1_scales_fp4_shuffled = (
            torch.stack(gemm1_scales_fp4_shuffled)
            .view(torch.float8_e4m3fn)
            .reshape(num_experts, 2 * intermediate_size, hidden_size // 16)
        )

        gemm2_weights_fp4_shuffled = torch.stack(gemm2_weights_fp4_shuffled)
        gemm2_scales_fp4_shuffled = (
            torch.stack(gemm2_scales_fp4_shuffled)
            .view(torch.float8_e4m3fn)
            .reshape(num_experts, hidden_size, intermediate_size // 16)
        )
        return (
            gemm1_weights_fp4_shuffled,
            gemm1_scales_fp4_shuffled,
            gemm2_weights_fp4_shuffled,
            gemm2_scales_fp4_shuffled,
        )

    def process_weights_after_loading(self, layer: torch.nn.Module) -&gt; None:
        &#34;&#34;&#34;Process FP4 MoE weights after loading from serialized checkpoint.

        Only supports pre-quantized checkpoints with FP8 weights and scales.
        &#34;&#34;&#34;

        # GEMM 1 scale processing
        if not torch.allclose(
            layer.w13_weight_scale_2[:, 0], layer.w13_weight_scale_2[:, 1]
        ):
            logger.warning_once(
                &#34;w1_weight_scale_2 must match w3_weight_scale_2. &#34;
                &#34;Accuracy may be affected.&#34;
            )

        w13_weight_scale_2 = layer.w13_weight_scale_2[:, 0]
        layer.w13_weight_scale_2 = Parameter(w13_weight_scale_2, requires_grad=False)

        # Calculate input scales based on strategy
        if self.enable_flashinfer_cutlass_moe or self.enable_flashinfer_trtllm_moe:
            w13_input_scale = layer.w13_input_scale.max().to(torch.float32)
            w2_input_scale = layer.w2_input_scale.max().to(torch.float32)
        else:
            w13_input_scale = layer.w13_input_scale.max(dim=1).values.to(torch.float32)
            w2_input_scale = layer.w2_input_scale

        # Create shared parameters
        layer.g1_alphas = Parameter(
            (w13_input_scale * w13_weight_scale_2).to(torch.float32),
            requires_grad=False,
        )
        layer.g2_alphas = Parameter(
            (w2_input_scale * layer.w2_weight_scale_2).to(torch.float32),
            requires_grad=False,
        )
        layer.w13_input_scale_quant = Parameter(
            (1 / w13_input_scale).to(torch.float32), requires_grad=False
        )
        layer.w2_input_scale_quant = Parameter(
            (1 / w2_input_scale).to(torch.float32), requires_grad=False
        )

        # Validate weight scales
        for name, weight_scale in [
            (&#34;w13&#34;, layer.w13_weight_scale),
            (&#34;w2&#34;, layer.w2_weight_scale),
        ]:
            assert (
                weight_scale.shape[2] % 16 == 0
            ), f&#34;Expected {name}_weight_scale.dim(2) to be divisible by 16&#34;
            assert (
                weight_scale.dtype == torch.float8_e4m3fn
            ), f&#34;{name} Weight Blockscale must be represented as FP8-E4M3&#34;

        # Weight processing based on strategy
        if (
            self.enable_flashinfer_trtllm_moe
            and reorder_rows_for_gated_act_gemm is not None
            and shuffle_matrix_sf_a is not None
        ):
            # FlashInfer TRTLLM processing - handles both w13 and w2
            (
                gemm1_weights_fp4_shuffled,
                gemm1_scales_fp4_shuffled,
                gemm2_weights_fp4_shuffled,
                gemm2_scales_fp4_shuffled,
            ) = self.prepare_static_weights_for_kernel(
                layer.w13_weight,
                layer.w2_weight,
                layer.w13_weight_scale,
                layer.w2_weight_scale,
                layer.w2_weight.size(-2),  # hidden_size
                layer.w13_weight.size(-2) // 2,  # intermediate_size
                layer.w13_weight.size(0),  # num_experts
            )

            # Set flashinfer parameters
            layer.gemm1_weights_fp4_shuffled = Parameter(
                gemm1_weights_fp4_shuffled, requires_grad=False
            )
            layer.gemm2_weights_fp4_shuffled = Parameter(
                gemm2_weights_fp4_shuffled, requires_grad=False
            )
            layer.gemm1_scales_fp4_shuffled = Parameter(
                gemm1_scales_fp4_shuffled, requires_grad=False
            )
            layer.gemm2_scales_fp4_shuffled = Parameter(
                gemm2_scales_fp4_shuffled, requires_grad=False
            )

            # Additional parameter needed for TRT-LLM
            layer.g1_scale_c = Parameter(
                (layer.w2_input_scale_quant * layer.g1_alphas).to(torch.float32),
                requires_grad=False,
            )

            # Clean up weights that won&#39;t be used by TRT-LLM
            del (
                layer.w2_weight,
                layer.w2_weight_scale,
                layer.w13_weight,
                layer.w13_weight_scale,
            )

            logger.info_once(&#34;Applied flashinfer weight processing for both w13 and w2&#34;)

        else:
            # CUTLASS processing - handle w13 and w2 separately

            # Process w13 weights
            w13_blockscale_swizzled = self.swizzle_blockscale(layer.w13_weight_scale)
            del layer.w13_weight_scale
            layer.w13_blockscale_swizzled.data.copy_(w13_blockscale_swizzled)
            layer.w13_weight = Parameter(layer.w13_weight.data, requires_grad=False)

            # Process w2 weights
            w2_blockscale_swizzled = self.swizzle_blockscale(layer.w2_weight_scale)
            del layer.w2_weight_scale
            layer.w2_blockscale_swizzled.data.copy_(w2_blockscale_swizzled)
            layer.w2_weight = Parameter(layer.w2_weight.data, requires_grad=False)

            # Both flashinfer cutlass and regular cutlass use same processing for w2
            logger.info_once(&#34;Applied weight processing for both w13 and w2&#34;)

            # Set up CUTLASS MoE parameters
            device = layer.w13_weight.device
            layer.cutlass_moe_params = CutlassMoEParams(
                CutlassMoEType.BlockscaledFP4,
                device,
                num_experts=layer.num_experts,  # global num experts
                intermediate_size_per_partition=layer.w2_weight.shape[2] * 2,  # n
                hidden_size=layer.w13_weight.shape[2] * 2,
            )  # k

    @property
    def load_up_proj_weight_first(self) -&gt; bool:
        # FlashInfer CUTLASS kernel assumes [Up, Gate] Proj as W13
        return self.enable_flashinfer_cutlass_moe

    def apply(
        self,
        layer: FusedMoE,
        x: torch.Tensor,
        topk_output: TopKOutput,
        moe_runner_config: MoeRunnerConfig,
    ) -&gt; torch.Tensor:
        assert (
            moe_runner_config.activation == &#34;silu&#34;
        ), &#34;Only SiLU activation is supported.&#34;

        # Check if this is a FlashInferFP4MoE layer that should handle its own forward
        if hasattr(layer, &#34;gemm1_weights_fp4_shuffled&#34;):
            # This layer was processed with flashinfer TRTLLM - delegate to its own forward
            return layer.forward(x, topk_output)

        if self.enable_flashinfer_cutlass_moe:
            assert (
                not moe_runner_config.apply_router_weight_on_input
            ), &#34;apply_router_weight_on_input is not supported for Flashinfer&#34;
            # TRTLLM Cutlass moe takes in activations in BF16/Half/nvfp4 precision
            # and fp4 quantized weights loaded from the checkpoint
            topk_weights, topk_ids = topk_output.topk_weights, topk_output.topk_ids

            output_dtype = x.dtype
            x_sf = None
            if should_use_flashinfer_cutlass_moe_fp4_allgather():
                from flashinfer import fp4_quantize, nvfp4_block_scale_interleave

                # Quantize before comm, swizzle after.
                if x.shape[0] &gt; 0:
                    x, x_sf = fp4_quantize(
                        x, layer.w13_input_scale_quant, is_sf_swizzled_layout=False
                    )
                else:
                    x_col = x.shape[1]
                    x = torch.zeros(0, x_col // 2, dtype=torch.uint8, device=x.device)
                    x_sf = torch.zeros(
                        0, x_col // 16, dtype=torch.uint8, device=x.device
                    )
                topk_weights, topk_ids, x, x_sf = get_tp_group().all_gatherv(
                    [topk_weights, topk_ids, x, x_sf], sizes=get_dp_global_num_tokens()
                )
                x_sf = nvfp4_block_scale_interleave(x_sf)

            output = flashinfer_cutlass_fused_moe(
                input=x,
                token_selected_experts=topk_ids.to(torch.int),
                token_final_scales=topk_weights,
                fc1_expert_weights=layer.w13_weight.view(torch.long),
                fc2_expert_weights=layer.w2_weight.view(torch.long),
                output_dtype=output_dtype,
                input_sf=x_sf,
                quant_scales=[
                    layer.w13_input_scale_quant,
                    layer.w13_blockscale_swizzled.view(torch.int32),
                    layer.g1_alphas,
                    layer.w2_input_scale_quant,
                    layer.w2_blockscale_swizzled.view(torch.int32),
                    layer.g2_alphas,
                ],
                ep_size=layer.moe_ep_size,
                ep_rank=layer.moe_ep_rank,
                tp_size=layer.moe_tp_size,
                tp_rank=layer.moe_tp_rank,
                tune_max_num_tokens=next_power_of_2(x.shape[0]),
            )[0]
            # Scale by routed_scaling_factor is fused into select_experts.
            if should_use_flashinfer_cutlass_moe_fp4_allgather():
                output, global_output = get_local_dp_buffer(), output
                get_tp_group().reduce_scatterv(
                    global_output, output=output, sizes=get_dp_global_num_tokens()
                )
            return output

        from sglang.srt.layers.moe.cutlass_moe import cutlass_moe_fp4

        topk_weights, topk_ids = topk_output.topk_weights, topk_output.topk_ids
        output = cutlass_moe_fp4(
            a=x,
            a1_gscale=layer.w13_input_scale_quant,
            w1_fp4=layer.w13_weight,
            w1_blockscale=layer.w13_blockscale_swizzled,
            w1_alphas=layer.g1_alphas,
            a2_gscale=layer.w2_input_scale_quant,
            w2_fp4=layer.w2_weight,
            w2_blockscale=layer.w2_blockscale_swizzled,
            w2_alphas=layer.g2_alphas,
            topk_weights=topk_weights,
            topk_ids=topk_ids,
            params=layer.cutlass_moe_params,
            apply_router_weight_on_input=moe_runner_config.apply_router_weight_on_input,
        ).to(x.dtype)
        # Scale by routed_scaling_factor is fused into select_experts.
        return output</code></pre>
</details>
<div class="desc"><p>MoE Method for FP4 Quantization with Blockscales and PerTensorScales</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>quant_config</code></strong></dt>
<dd>NVFP4 Quant Config</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.quantization.base_config.FusedMoEMethodBase" href="base_config.html#sglang.srt.layers.quantization.base_config.FusedMoEMethodBase">FusedMoEMethodBase</a></li>
<li><a title="sglang.srt.layers.quantization.base_config.QuantizeMethodBase" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizeMethodBase">QuantizeMethodBase</a></li>
<li>abc.ABC</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.layers.quantization.modelopt_quant.ModelOptNvFp4FusedMoEMethod.enable_flashinfer_cutlass_moe"><code class="name">prop <span class="ident">enable_flashinfer_cutlass_moe</span> : bool</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def enable_flashinfer_cutlass_moe(self) -&gt; bool:
    from sglang.srt.layers.moe import get_moe_runner_backend

    &#34;&#34;&#34;Access the global enable_flashinfer_cutlass_moe setting.&#34;&#34;&#34;
    return get_moe_runner_backend().is_flashinfer_cutlass()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.quantization.modelopt_quant.ModelOptNvFp4FusedMoEMethod.load_up_proj_weight_first"><code class="name">prop <span class="ident">load_up_proj_weight_first</span> : bool</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def load_up_proj_weight_first(self) -&gt; bool:
    # FlashInfer CUTLASS kernel assumes [Up, Gate] Proj as W13
    return self.enable_flashinfer_cutlass_moe</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.quantization.modelopt_quant.ModelOptNvFp4FusedMoEMethod.prepare_static_weights_for_kernel"><code class="name flex">
<span>def <span class="ident">prepare_static_weights_for_kernel</span></span>(<span>self,<br>gemm1_weights,<br>gemm2_weights,<br>gemm1_scales_linear_fp4_bytes,<br>gemm2_scales_linear_fp4_bytes,<br>hidden_size,<br>intermediate_size,<br>num_experts)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_static_weights_for_kernel(
    self,
    # args_dequant,
    # args,
    gemm1_weights,
    gemm2_weights,
    gemm1_scales_linear_fp4_bytes,
    gemm2_scales_linear_fp4_bytes,
    hidden_size,
    intermediate_size,
    num_experts,
):
    from flashinfer import (
        RoutingMethodType,
        e2m1_and_ufp8sf_scale_to_float,
        fp4_quantize,
        next_positive_power_of_2,
        nvfp4_block_scale_interleave,
        reorder_rows_for_gated_act_gemm,
        shuffle_matrix_a,
        shuffle_matrix_sf_a,
    )
    from flashinfer.fused_moe.core import (
        _maybe_get_cached_w2_permute_indices,
        _maybe_get_cached_w3_w1_permute_indices,
    )

    &#34;&#34;&#34;Prepare quantized weights for kernel (done offline with weights).&#34;&#34;&#34;
    epilogue_tile_m = 128  # FIXME: this depends on the kernel internals

    # Convert quantized weights to proper formats
    gemm1_weights_fp4 = gemm1_weights.view(torch.float8_e4m3fn).reshape(
        num_experts, 2 * intermediate_size, hidden_size // 2
    )  # packed fp4
    gemm1_scales_linear_fp4 = gemm1_scales_linear_fp4_bytes.view(
        torch.float8_e4m3fn
    ).reshape(
        num_experts, 2 * intermediate_size, hidden_size // 16
    )  # fp8 scaling factors

    gemm2_weights_fp4 = gemm2_weights.view(torch.float8_e4m3fn).reshape(
        num_experts, hidden_size, intermediate_size // 2
    )  # packed fp4
    gemm2_scales_linear_fp4 = gemm2_scales_linear_fp4_bytes.view(
        torch.float8_e4m3fn
    ).reshape(
        num_experts, hidden_size, intermediate_size // 16
    )  # fp8 scaling factors

    gemm1_weights_fp4_shuffled = []
    gemm1_scales_fp4_shuffled = []
    gemm2_weights_fp4_shuffled = []
    gemm2_scales_fp4_shuffled = []
    for i in range(num_experts):
        # Calculate the permute indices for the following:
        # 1. Reorder rows of W1 and scales for fused gated activation
        # 2. Shuffle weights and scaling factors for transposed mma output
        # for both w3_w1 and w2 weights and scale factors
        permute_indices = _maybe_get_cached_w3_w1_permute_indices(
            self._cache_permute_indices,
            gemm1_weights_fp4[i].view(torch.uint8),
            epilogue_tile_m,
        )
        gemm1_weights_fp4_shuffled.append(
            gemm1_weights_fp4[i]
            .view(torch.uint8)[permute_indices.to(gemm1_weights_fp4.device)]
            .contiguous()
        )

        permute_sf_indices = _maybe_get_cached_w3_w1_permute_indices(
            self._cache_permute_indices,
            gemm1_scales_linear_fp4[i].view(torch.uint8),
            epilogue_tile_m,
            num_elts_per_sf=16,
        )
        gemm1_scales_fp4_shuffled.append(
            nvfp4_block_scale_interleave(
                gemm1_scales_linear_fp4[i]
                .view(torch.uint8)[
                    permute_sf_indices.to(gemm1_scales_linear_fp4.device)
                ]
                .contiguous()
            )
        )

        permute_indices = _maybe_get_cached_w2_permute_indices(
            self._cache_permute_indices,
            gemm2_weights_fp4[i].view(torch.uint8),
            epilogue_tile_m,
        )
        gemm2_weights_fp4_shuffled.append(
            gemm2_weights_fp4[i]
            .view(torch.uint8)[permute_indices.to(gemm2_weights_fp4.device)]
            .contiguous()
        )

        permute_sf_indices = _maybe_get_cached_w2_permute_indices(
            self._cache_permute_indices,
            gemm2_scales_linear_fp4[i].view(torch.uint8),
            epilogue_tile_m,
            num_elts_per_sf=16,
        )
        gemm2_scales_fp4_shuffled.append(
            nvfp4_block_scale_interleave(
                gemm2_scales_linear_fp4[i]
                .view(torch.uint8)[
                    permute_sf_indices.to(gemm2_scales_linear_fp4.device)
                ]
                .contiguous()
            )
        )

    # Stack weights for all experts
    gemm1_weights_fp4_shuffled = torch.stack(gemm1_weights_fp4_shuffled)
    gemm1_scales_fp4_shuffled = (
        torch.stack(gemm1_scales_fp4_shuffled)
        .view(torch.float8_e4m3fn)
        .reshape(num_experts, 2 * intermediate_size, hidden_size // 16)
    )

    gemm2_weights_fp4_shuffled = torch.stack(gemm2_weights_fp4_shuffled)
    gemm2_scales_fp4_shuffled = (
        torch.stack(gemm2_scales_fp4_shuffled)
        .view(torch.float8_e4m3fn)
        .reshape(num_experts, hidden_size, intermediate_size // 16)
    )
    return (
        gemm1_weights_fp4_shuffled,
        gemm1_scales_fp4_shuffled,
        gemm2_weights_fp4_shuffled,
        gemm2_scales_fp4_shuffled,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.quantization.modelopt_quant.ModelOptNvFp4FusedMoEMethod.process_weights_after_loading"><code class="name flex">
<span>def <span class="ident">process_weights_after_loading</span></span>(<span>self, layer: torch.nn.Module) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_weights_after_loading(self, layer: torch.nn.Module) -&gt; None:
    &#34;&#34;&#34;Process FP4 MoE weights after loading from serialized checkpoint.

    Only supports pre-quantized checkpoints with FP8 weights and scales.
    &#34;&#34;&#34;

    # GEMM 1 scale processing
    if not torch.allclose(
        layer.w13_weight_scale_2[:, 0], layer.w13_weight_scale_2[:, 1]
    ):
        logger.warning_once(
            &#34;w1_weight_scale_2 must match w3_weight_scale_2. &#34;
            &#34;Accuracy may be affected.&#34;
        )

    w13_weight_scale_2 = layer.w13_weight_scale_2[:, 0]
    layer.w13_weight_scale_2 = Parameter(w13_weight_scale_2, requires_grad=False)

    # Calculate input scales based on strategy
    if self.enable_flashinfer_cutlass_moe or self.enable_flashinfer_trtllm_moe:
        w13_input_scale = layer.w13_input_scale.max().to(torch.float32)
        w2_input_scale = layer.w2_input_scale.max().to(torch.float32)
    else:
        w13_input_scale = layer.w13_input_scale.max(dim=1).values.to(torch.float32)
        w2_input_scale = layer.w2_input_scale

    # Create shared parameters
    layer.g1_alphas = Parameter(
        (w13_input_scale * w13_weight_scale_2).to(torch.float32),
        requires_grad=False,
    )
    layer.g2_alphas = Parameter(
        (w2_input_scale * layer.w2_weight_scale_2).to(torch.float32),
        requires_grad=False,
    )
    layer.w13_input_scale_quant = Parameter(
        (1 / w13_input_scale).to(torch.float32), requires_grad=False
    )
    layer.w2_input_scale_quant = Parameter(
        (1 / w2_input_scale).to(torch.float32), requires_grad=False
    )

    # Validate weight scales
    for name, weight_scale in [
        (&#34;w13&#34;, layer.w13_weight_scale),
        (&#34;w2&#34;, layer.w2_weight_scale),
    ]:
        assert (
            weight_scale.shape[2] % 16 == 0
        ), f&#34;Expected {name}_weight_scale.dim(2) to be divisible by 16&#34;
        assert (
            weight_scale.dtype == torch.float8_e4m3fn
        ), f&#34;{name} Weight Blockscale must be represented as FP8-E4M3&#34;

    # Weight processing based on strategy
    if (
        self.enable_flashinfer_trtllm_moe
        and reorder_rows_for_gated_act_gemm is not None
        and shuffle_matrix_sf_a is not None
    ):
        # FlashInfer TRTLLM processing - handles both w13 and w2
        (
            gemm1_weights_fp4_shuffled,
            gemm1_scales_fp4_shuffled,
            gemm2_weights_fp4_shuffled,
            gemm2_scales_fp4_shuffled,
        ) = self.prepare_static_weights_for_kernel(
            layer.w13_weight,
            layer.w2_weight,
            layer.w13_weight_scale,
            layer.w2_weight_scale,
            layer.w2_weight.size(-2),  # hidden_size
            layer.w13_weight.size(-2) // 2,  # intermediate_size
            layer.w13_weight.size(0),  # num_experts
        )

        # Set flashinfer parameters
        layer.gemm1_weights_fp4_shuffled = Parameter(
            gemm1_weights_fp4_shuffled, requires_grad=False
        )
        layer.gemm2_weights_fp4_shuffled = Parameter(
            gemm2_weights_fp4_shuffled, requires_grad=False
        )
        layer.gemm1_scales_fp4_shuffled = Parameter(
            gemm1_scales_fp4_shuffled, requires_grad=False
        )
        layer.gemm2_scales_fp4_shuffled = Parameter(
            gemm2_scales_fp4_shuffled, requires_grad=False
        )

        # Additional parameter needed for TRT-LLM
        layer.g1_scale_c = Parameter(
            (layer.w2_input_scale_quant * layer.g1_alphas).to(torch.float32),
            requires_grad=False,
        )

        # Clean up weights that won&#39;t be used by TRT-LLM
        del (
            layer.w2_weight,
            layer.w2_weight_scale,
            layer.w13_weight,
            layer.w13_weight_scale,
        )

        logger.info_once(&#34;Applied flashinfer weight processing for both w13 and w2&#34;)

    else:
        # CUTLASS processing - handle w13 and w2 separately

        # Process w13 weights
        w13_blockscale_swizzled = self.swizzle_blockscale(layer.w13_weight_scale)
        del layer.w13_weight_scale
        layer.w13_blockscale_swizzled.data.copy_(w13_blockscale_swizzled)
        layer.w13_weight = Parameter(layer.w13_weight.data, requires_grad=False)

        # Process w2 weights
        w2_blockscale_swizzled = self.swizzle_blockscale(layer.w2_weight_scale)
        del layer.w2_weight_scale
        layer.w2_blockscale_swizzled.data.copy_(w2_blockscale_swizzled)
        layer.w2_weight = Parameter(layer.w2_weight.data, requires_grad=False)

        # Both flashinfer cutlass and regular cutlass use same processing for w2
        logger.info_once(&#34;Applied weight processing for both w13 and w2&#34;)

        # Set up CUTLASS MoE parameters
        device = layer.w13_weight.device
        layer.cutlass_moe_params = CutlassMoEParams(
            CutlassMoEType.BlockscaledFP4,
            device,
            num_experts=layer.num_experts,  # global num experts
            intermediate_size_per_partition=layer.w2_weight.shape[2] * 2,  # n
            hidden_size=layer.w13_weight.shape[2] * 2,
        )  # k</code></pre>
</details>
<div class="desc"><p>Process FP4 MoE weights after loading from serialized checkpoint.</p>
<p>Only supports pre-quantized checkpoints with FP8 weights and scales.</p></div>
</dd>
<dt id="sglang.srt.layers.quantization.modelopt_quant.ModelOptNvFp4FusedMoEMethod.swizzle_blockscale"><code class="name flex">
<span>def <span class="ident">swizzle_blockscale</span></span>(<span>self, scale: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def swizzle_blockscale(self, scale: torch.Tensor):
    assert scale.dtype == torch.float8_e4m3fn
    # Pad and blockwise interleave weight_scale
    scale_ndim = scale.ndim
    if scale.ndim == 2:
        scale = scale.unsqueeze(0)
    assert scale.ndim == 3
    B, M, K = scale.shape
    round_up_multiple = lambda x, m: (x + m - 1) // m * m
    M_padded = round_up_multiple(M, 128)
    K_padded = round_up_multiple(K, 4)
    padded_scale = torch.zeros((B, M_padded, K_padded), dtype=scale.dtype)
    padded_scale[:B, :M, :K] = scale
    batches, rows, cols = padded_scale.shape
    assert rows % 128 == 0
    assert cols % 4 == 0
    padded_scale = padded_scale.reshape(batches, rows // 128, 4, 32, cols // 4, 4)
    swizzled_scale = padded_scale.permute((0, 1, 4, 3, 2, 5))
    swizzled_scale = swizzled_scale.contiguous().cuda()
    return (
        swizzled_scale.reshape(M_padded, K_padded)
        if scale_ndim == 2
        else swizzled_scale.reshape(B, M_padded, K_padded)
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.layers.quantization.base_config.FusedMoEMethodBase" href="base_config.html#sglang.srt.layers.quantization.base_config.FusedMoEMethodBase">FusedMoEMethodBase</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.layers.quantization.base_config.FusedMoEMethodBase.apply" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizeMethodBase.apply">apply</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.base_config.FusedMoEMethodBase.create_weights" href="base_config.html#sglang.srt.layers.quantization.base_config.QuantizeMethodBase.create_weights">create_weights</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.layers.quantization" href="index.html">sglang.srt.layers.quantization</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp4Config" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptFp4Config">ModelOptFp4Config</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp4Config.is_layer_excluded" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptFp4Config.is_layer_excluded">is_layer_excluded</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp4LinearMethod" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptFp4LinearMethod">ModelOptFp4LinearMethod</a></code></h4>
</li>
<li>
<h4><code><a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8Config" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8Config">ModelOptFp8Config</a></code></h4>
</li>
<li>
<h4><code><a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8KVCacheMethod" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8KVCacheMethod">ModelOptFp8KVCacheMethod</a></code></h4>
</li>
<li>
<h4><code><a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8LinearMethod" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8LinearMethod">ModelOptFp8LinearMethod</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8LinearMethod.apply" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8LinearMethod.apply">apply</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8LinearMethod.create_weights" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8LinearMethod.create_weights">create_weights</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8LinearMethod.process_weights_after_loading" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8LinearMethod.process_weights_after_loading">process_weights_after_loading</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8MoEMethod" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8MoEMethod">ModelOptFp8MoEMethod</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8MoEMethod.process_weights_after_loading" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptFp8MoEMethod.process_weights_after_loading">process_weights_after_loading</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptNvFp4FusedMoEMethod" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptNvFp4FusedMoEMethod">ModelOptNvFp4FusedMoEMethod</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptNvFp4FusedMoEMethod.enable_flashinfer_cutlass_moe" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptNvFp4FusedMoEMethod.enable_flashinfer_cutlass_moe">enable_flashinfer_cutlass_moe</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptNvFp4FusedMoEMethod.load_up_proj_weight_first" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptNvFp4FusedMoEMethod.load_up_proj_weight_first">load_up_proj_weight_first</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptNvFp4FusedMoEMethod.prepare_static_weights_for_kernel" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptNvFp4FusedMoEMethod.prepare_static_weights_for_kernel">prepare_static_weights_for_kernel</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptNvFp4FusedMoEMethod.process_weights_after_loading" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptNvFp4FusedMoEMethod.process_weights_after_loading">process_weights_after_loading</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.modelopt_quant.ModelOptNvFp4FusedMoEMethod.swizzle_blockscale" href="#sglang.srt.layers.quantization.modelopt_quant.ModelOptNvFp4FusedMoEMethod.swizzle_blockscale">swizzle_blockscale</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
