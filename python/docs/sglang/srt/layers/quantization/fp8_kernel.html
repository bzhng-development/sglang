<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.layers.quantization.fp8_kernel API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.layers.quantization.fp8_kernel</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.layers.quantization.fp8_kernel.create_per_token_group_quant_fp8_output_scale"><code class="name flex">
<span>def <span class="ident">create_per_token_group_quant_fp8_output_scale</span></span>(<span>x_shape,<br>device,<br>group_size,<br>column_major_scales: bool,<br>scale_tma_aligned: bool,<br>scale_ue8m0: bool)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_per_token_group_quant_fp8_output_scale(
    x_shape,
    device,
    group_size,
    column_major_scales: bool,
    scale_tma_aligned: bool,
    scale_ue8m0: bool,
):
    if scale_ue8m0:
        assert column_major_scales and scale_tma_aligned
        *x_batch, x_q_mn, x_q_k = x_shape
        x_s_mn, x_s_k = x_q_mn, x_q_k // 128
        aligned_mn = align(x_s_mn, 4)
        aligned_k = align(x_s_k, 4)
        # TODO(FIXME): Fix cuda kernel and recover here to empty.
        return torch.empty(
            (*x_batch, aligned_k // 4, aligned_mn),
            device=device,
            dtype=torch.int,
        ).transpose(-1, -2)[..., :x_s_mn, :]
    elif column_major_scales:
        if scale_tma_aligned:
            # TODO extract &#34;align&#34; function
            # aligned to 4 * sizeof(float)
            aligned_size = (x_shape[-2] + 3) // 4 * 4
            return torch.empty(
                x_shape[:-2] + (x_shape[-1] // group_size, aligned_size),
                device=device,
                dtype=torch.float32,
            ).permute(-1, -2)[: x_shape[-2], :]
        else:
            return torch.empty(
                (x_shape[-1] // group_size,) + x_shape[:-1],
                device=device,
                dtype=torch.float32,
            ).permute(-1, -2)
    else:
        return torch.empty(
            x_shape[:-1] + (x_shape[-1] // group_size,),
            device=device,
            dtype=torch.float32,
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.deep_gemm_fp8_fp8_bf16_nt"><code class="name flex">
<span>def <span class="ident">deep_gemm_fp8_fp8_bf16_nt</span></span>(<span>A: torch.Tensor,<br>As: torch.Tensor,<br>B: torch.Tensor,<br>Bs: torch.Tensor,<br>C: torch.Tensor) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deep_gemm_fp8_fp8_bf16_nt(
    A: torch.Tensor,
    As: torch.Tensor,
    B: torch.Tensor,
    Bs: torch.Tensor,
    C: torch.Tensor,
) -&gt; None:
    deep_gemm_wrapper.gemm_nt_f8f8bf16((A, As), (B, Bs), C)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.deep_gemm_fp8_fp8_bf16_nt_fake"><code class="name flex">
<span>def <span class="ident">deep_gemm_fp8_fp8_bf16_nt_fake</span></span>(<span>A: torch.Tensor,<br>As: torch.Tensor,<br>B: torch.Tensor,<br>Bs: torch.Tensor,<br>C: torch.Tensor) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deep_gemm_fp8_fp8_bf16_nt_fake(
    A: torch.Tensor,
    As: torch.Tensor,
    B: torch.Tensor,
    Bs: torch.Tensor,
    C: torch.Tensor,
) -&gt; None:
    return</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.get_w8a8_block_fp8_configs"><code class="name flex">
<span>def <span class="ident">get_w8a8_block_fp8_configs</span></span>(<span>N: int, K: int, block_n: int, block_k: int) ‑> Dict[int, Any] | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@functools.lru_cache
def get_w8a8_block_fp8_configs(
    N: int, K: int, block_n: int, block_k: int
) -&gt; Optional[Dict[int, Any]]:
    &#34;&#34;&#34;
    Return optimized configurations for the w8a8 block fp8 kernel.

    The return value will be a dictionary that maps an irregular grid of
    batch sizes to configurations of the w8a8 block fp8 kernel. To evaluate the
    kernel on a given batch size bs, the closest batch size in the grid should
    be picked and the associated configuration chosen to invoke the kernel.
    &#34;&#34;&#34;

    # First look up if an optimized configuration is available in the configs
    # directory
    device_name = get_device_name().replace(&#34; &#34;, &#34;_&#34;)
    json_file_name = f&#34;N={N},K={K},device_name={device_name},dtype=fp8_w8a8,block_shape=[{block_n}, {block_k}].json&#34;

    config_file_path = os.path.join(
        os.path.dirname(os.path.realpath(__file__)), &#34;configs&#34;, json_file_name
    )
    if os.path.exists(config_file_path):
        with open(config_file_path) as f:
            log_info_on_rank0(
                logger,
                f&#34;Using configuration from {config_file_path} for W8A8 Block FP8 kernel.&#34;,
            )
            # If a configuration has been found, return it
            return {int(key): val for key, val in json.load(f).items()}

    # If no optimized configuration is available, we will use the default
    # configuration
    logger.warning(
        (
            &#34;Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! &#34;
            &#34;Config file not found at %s&#34;
        ),
        config_file_path,
    )
    return None</code></pre>
</details>
<div class="desc"><p>Return optimized configurations for the w8a8 block fp8 kernel.</p>
<p>The return value will be a dictionary that maps an irregular grid of
batch sizes to configurations of the w8a8 block fp8 kernel. To evaluate the
kernel on a given batch size bs, the closest batch size in the grid should
be picked and the associated configuration chosen to invoke the kernel.</p></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.is_fp8_fnuz"><code class="name flex">
<span>def <span class="ident">is_fp8_fnuz</span></span>(<span>) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@lru_cache()
def is_fp8_fnuz() -&gt; bool:
    if _is_hip:
        # only device 0 is checked, this assumes MI300 platforms are homogeneous
        return &#34;gfx94&#34; in torch.cuda.get_device_properties(0).gcnArchName
    return False</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.is_weak_contiguous"><code class="name flex">
<span>def <span class="ident">is_weak_contiguous</span></span>(<span>x: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_weak_contiguous(x: torch.Tensor):
    strides = x.stride()
    sizes = x.shape
    is_not_transpose = strides[0] == 1 and (strides[1] &gt;= max(1, sizes[0]))
    is_transpose = strides[1] == 1 and (strides[0] &gt;= max(1, sizes[1]))
    return is_transpose or is_not_transpose</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.per_group_transpose"><code class="name flex">
<span>def <span class="ident">per_group_transpose</span></span>(<span>a: torch.Tensor, expert_offsets: torch.Tensor, M_ALIGNMENT: int = 1) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def per_group_transpose(
    a: torch.Tensor,
    expert_offsets: torch.Tensor,
    M_ALIGNMENT: int = 1,
) -&gt; torch.Tensor:
    assert a.dim() == 2
    assert a.is_contiguous(), &#34;`a` is not contiguous&#34;

    m, k = a.size()
    trans_a = torch.empty_like(a)
    num_experts = expert_offsets.size(0) - 1

    grid = lambda META: (
        num_experts,
        triton.cdiv((m + num_experts - 1) // num_experts, META[&#34;BLOCK_SIZE_M&#34;]),
        triton.cdiv(k, META[&#34;BLOCK_SIZE_K&#34;]),
    )
    _per_group_transpose[grid](
        a, trans_a, expert_offsets, k, M_ALIGNMENT, BLOCK_SIZE_M=16, BLOCK_SIZE_K=8
    )
    return trans_a</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.per_tensor_quant_mla_fp8"><code class="name flex">
<span>def <span class="ident">per_tensor_quant_mla_fp8</span></span>(<span>x: torch.Tensor, x_s_out: torch.Tensor, eps: float = 1e-12) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def per_tensor_quant_mla_fp8(
    x: torch.Tensor, x_s_out: torch.Tensor, eps: float = 1e-12
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;
    This function quantizes input values to float8 values with tensor-wise quantization
    and specialized for mla absorbed case.
    &#34;&#34;&#34;
    assert x.dim() == 3, &#34;`x` is not a 3d-tensor&#34;
    assert (
        x_s_out.shape == (1,)
        and x_s_out.dtype == torch.float32
        and x_s_out.device == x.device
    )

    x_q = x.new_empty(x.size(), dtype=fp8_dtype)

    num_head, num_seq, head_size = x.shape
    BLOCK_SIZE = triton.next_power_of_2(head_size)
    grid = (num_seq, num_head)

    _per_tensor_quant_mla_fp8_stage1[grid](
        x,
        x_s_out,
        head_size,
        x.stride(0),
        x.stride(1),
        eps,
        fp8_max,
        BLOCK_SIZE,
    )
    _per_tensor_quant_mla_fp8_stage2[grid](
        x,
        x_s_out,
        x_q,
        num_seq,
        head_size,
        x.stride(0),
        x.stride(1),
        fp8_min,
        fp8_max,
        BLOCK_SIZE,
    )

    return x_q, x_s_out</code></pre>
</details>
<div class="desc"><p>This function quantizes input values to float8 values with tensor-wise quantization
and specialized for mla absorbed case.</p></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.per_token_group_quant_8bit"><code class="name flex">
<span>def <span class="ident">per_token_group_quant_8bit</span></span>(<span>x: torch.Tensor,<br>group_size: int,<br>dst_dtype: torch.dtype,<br>eps: float = 1e-10,<br>column_major_scales: bool = False,<br>scale_tma_aligned: bool = False,<br>scale_ue8m0: bool = False,<br>fuse_silu_and_mul: bool = False,<br>masked_m: torch.Tensor | None = None) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def per_token_group_quant_8bit(
    x: torch.Tensor,
    group_size: int,
    dst_dtype: torch.dtype,
    eps: float = 1e-10,
    column_major_scales: bool = False,
    scale_tma_aligned: bool = False,
    scale_ue8m0: bool = False,
    fuse_silu_and_mul: bool = False,
    masked_m: Optional[torch.Tensor] = None,
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    if fuse_silu_and_mul:
        return _per_token_group_quant_8bit_fuse_silu_and_mul(
            x=x,
            group_size=group_size,
            dst_dtype=dst_dtype,
            column_major_scales=column_major_scales,
            scale_tma_aligned=scale_tma_aligned,
            scale_ue8m0=scale_ue8m0,
            masked_m=masked_m,
        )
    else:
        return _per_token_group_quant_8bit_raw(
            x=x,
            group_size=group_size,
            eps=eps,
            column_major_scales=column_major_scales,
            scale_tma_aligned=scale_tma_aligned,
            scale_ue8m0=scale_ue8m0,
            dtype=dst_dtype,
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.per_token_group_quant_fp8"><code class="name flex">
<span>def <span class="ident">per_token_group_quant_fp8</span></span>(<span>x: torch.Tensor,<br>group_size: int,<br>eps: float = 1e-10,<br>dtype: torch.dtype = torch.float8_e4m3fn,<br>column_major_scales: bool = False,<br>scale_tma_aligned: bool = False,<br>scale_ue8m0: bool = False) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def _per_token_group_quant_8bit_raw(
    x: torch.Tensor,
    group_size: int,
    eps: float = 1e-10,
    dtype: torch.dtype = fp8_dtype,
    column_major_scales: bool = False,
    scale_tma_aligned: bool = False,
    scale_ue8m0: bool = False,
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;Function to perform per-token-group quantization on an input tensor `x`.

    It converts the tensor values into signed float8 values and returns the
    quantized tensor along with the scaling factor used for quantization.

    Args:
        x: The input tenosr with ndim &gt;= 2.
        group_size: The group size used for quantization.
        eps: The minimum to avoid dividing zero.
        dtype: The dype of output tensor.

    Returns:
        Tuple[torch.Tensor, torch.Tensor]: The quantized tensor and the scaling factor for quantization.
    &#34;&#34;&#34;
    assert (
        x.shape[-1] % group_size == 0
    ), &#34;the last dimension of `x` cannot be divisible by `group_size`&#34;
    assert x.is_contiguous(), &#34;`x` is not contiguous&#34;

    if _is_hip:
        if dtype == torch.int8:
            bit8_max = 127.0
        else:
            bit8_max = 224.0
        bit8_min = -bit8_max  # TODO incorrect for int8
    else:
        if dtype == torch.int8:
            info = torch.iinfo(dtype)
        else:
            info = torch.finfo(dtype)
        bit8_max = info.max
        bit8_min = info.min

    x_q = torch.empty_like(x, device=x.device, dtype=dtype)
    x_s = create_per_token_group_quant_fp8_output_scale(
        x_shape=x.shape,
        device=x.device,
        group_size=group_size,
        column_major_scales=column_major_scales,
        scale_tma_aligned=scale_tma_aligned,
        scale_ue8m0=False,
    )

    M = x.numel() // group_size
    N = group_size

    BLOCK = triton.next_power_of_2(N)
    # heuristics for number of warps
    num_warps = min(max(BLOCK // 256, 1), 8)
    num_stages = 1
    if column_major_scales:
        _per_token_group_quant_8bit_colmajor[(M,)](
            x,
            x_q,
            x_s,
            group_size,
            x.shape[1],
            x_s.stride(1),
            eps,
            bit8_min=bit8_min,
            bit8_max=bit8_max,
            BLOCK=BLOCK,
            num_warps=num_warps,
            num_stages=num_stages,
            SCALE_UE8M0=scale_ue8m0,
        )
    else:
        assert not scale_ue8m0
        _per_token_group_quant_8bit[(M,)](
            x,
            x_q,
            x_s,
            group_size,
            N,
            eps,
            bit8_min=bit8_min,
            bit8_max=bit8_max,
            BLOCK=BLOCK,
            num_warps=num_warps,
            num_stages=num_stages,
        )

    if scale_ue8m0:
        from deep_gemm import transform_sf_into_required_layout

        assert group_size == 128
        x_s = transform_sf_into_required_layout(
            x_s,
            num_groups=None,
            mn=x_q.shape[0],
            k=x_q.shape[1],
            recipe=(1, group_size, group_size),
            is_sfa=True,
        )

    return x_q, x_s</code></pre>
</details>
<div class="desc"><p>Function to perform per-token-group quantization on an input tensor <code>x</code>.</p>
<p>It converts the tensor values into signed float8 values and returns the
quantized tensor along with the scaling factor used for quantization.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>The input tenosr with ndim &gt;= 2.</dd>
<dt><strong><code>group_size</code></strong></dt>
<dd>The group size used for quantization.</dd>
<dt><strong><code>eps</code></strong></dt>
<dd>The minimum to avoid dividing zero.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>The dype of output tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[torch.Tensor, torch.Tensor]</code></dt>
<dd>The quantized tensor and the scaling factor for quantization.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.per_token_group_quant_fp8_hopper_moe_mn_major"><code class="name flex">
<span>def <span class="ident">per_token_group_quant_fp8_hopper_moe_mn_major</span></span>(<span>A: torch.Tensor,<br>expert_offsets: torch.Tensor,<br>problem_sizes: torch.Tensor,<br>group_size: int,<br>expert_tokens_alignment: int = 1) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def per_token_group_quant_fp8_hopper_moe_mn_major(
    A: torch.Tensor,
    expert_offsets: torch.Tensor,
    problem_sizes: torch.Tensor,
    group_size: int,
    expert_tokens_alignment: int = 1,
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    assert A.dim() == 2
    assert A.is_contiguous(), &#34;`A` is not contiguous&#34;
    assert (
        A.shape[-1] % group_size == 0
    ), &#34;the last dimension of `A` cannot be divisible by `group_size`&#34;

    a_q = torch.empty_like(A, device=A.device, dtype=fp8_dtype)
    M, K = A.shape[0], A.shape[1]
    k = K // group_size
    sfa = torch.empty((M, k), device=A.device, dtype=torch.float32)
    num_experts = problem_sizes.shape[0]
    grid = (k, num_experts)
    _per_token_group_quant_fp8_hopper_moe_mn_major[grid](
        A,
        expert_offsets,
        problem_sizes,
        a_q,
        sfa,
        K,
        group_size,
        expert_tokens_alignment,
    )
    return a_q, sfa</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.per_token_group_quant_mla_deep_gemm_masked_fp8"><code class="name flex">
<span>def <span class="ident">per_token_group_quant_mla_deep_gemm_masked_fp8</span></span>(<span>x: torch.Tensor,<br>group_size: int = 128,<br>eps: float = 1e-12,<br>dtype: torch.dtype = torch.float8_e4m3fn) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def per_token_group_quant_mla_deep_gemm_masked_fp8(
    x: torch.Tensor,
    group_size: int = 128,
    eps: float = 1e-12,
    dtype: torch.dtype = fp8_dtype,
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;
    This function quantizes input values to float8 values with per-token-group-quantization
    for deep_gemm grouped_gemm_masked and specialized for mla absorbed case.
    &#34;&#34;&#34;
    assert x.dim() == 3, &#34;`x` is not a 3d-tensor&#34;

    b, m, k = x.shape
    aligned_m = (m + 255) // 256 * 256  # 256 is the max block_m of the gemm kernel
    num_tiles_k = k // group_size
    assert num_tiles_k * group_size == k, f&#34;k % {group_size} must be zero&#34;

    x_q = x.new_empty((b, aligned_m, k), dtype=dtype)
    x_s = x.new_empty((b, num_tiles_k, aligned_m), dtype=torch.float32)
    masked_m = x.new_empty((b,), dtype=torch.int32)

    BLOCK_SIZE = triton.next_power_of_2(group_size)
    grid = (m, b)

    _per_token_group_quant_mla_deep_gemm_masked_fp8[grid](
        x,
        x_q,
        x_s,
        masked_m,
        group_size,
        x.stride(0),
        x.stride(1),
        x_q.stride(0),
        x_q.stride(1),
        x_s.stride(0),
        x_s.stride(1),
        eps,
        -fp8_max,
        fp8_max,
        num_tiles_k,
        BLOCK_SIZE,
    )

    return x_q, x_s.transpose(1, 2), masked_m, m, aligned_m</code></pre>
</details>
<div class="desc"><p>This function quantizes input values to float8 values with per-token-group-quantization
for deep_gemm grouped_gemm_masked and specialized for mla absorbed case.</p></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.prepare_block_fp8_matmul_inputs"><code class="name flex">
<span>def <span class="ident">prepare_block_fp8_matmul_inputs</span></span>(<span>A: torch.Tensor,<br>B: torch.Tensor,<br>As: torch.Tensor,<br>Bs: torch.Tensor,<br>block_size: List[int],<br>output_dtype: torch.dtype = torch.float16) ‑> Tuple[int, int, int]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_block_fp8_matmul_inputs(
    A: torch.Tensor,
    B: torch.Tensor,
    As: torch.Tensor,
    Bs: torch.Tensor,
    block_size: List[int],
    output_dtype: torch.dtype = torch.float16,
) -&gt; Tuple[int, int, int]:
    assert len(block_size) == 2
    block_n, block_k = block_size[0], block_size[1]

    assert A.shape[-1] == B.shape[-1]
    assert A.shape[:-1] == As.shape[:-1]
    assert A.is_contiguous()

    if As.dtype == torch.float:
        assert triton.cdiv(A.shape[-1], block_k) == As.shape[-1]
    elif As.dtype == torch.int:
        assert (
            triton.cdiv(triton.cdiv(A.shape[-1], block_k), 4) == As.shape[-1]
        ), f&#34;{A.shape=} {As.shape=} {block_size=}&#34;
    else:
        raise NotImplementedError

    M = A.numel() // A.shape[-1]

    assert B.ndim == 2
    assert B.is_contiguous()
    assert Bs.ndim == 2
    N, K = B.shape

    if Bs.dtype == torch.float:
        assert triton.cdiv(N, block_n) == Bs.shape[0]
        assert triton.cdiv(K, block_k) == Bs.shape[1]
    elif Bs.dtype == torch.int:
        assert N == Bs.shape[0], f&#34;{B.shape=} {Bs.shape=} {block_size=}&#34;
        assert (
            triton.cdiv(triton.cdiv(K, block_k), 4) == Bs.shape[1]
        ), f&#34;{B.shape=} {Bs.shape=} {block_size=}&#34;
    else:
        raise NotImplementedError

    C_shape = A.shape[:-1] + (N,)
    C = A.new_empty(C_shape, dtype=output_dtype)

    return M, N, K, C</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.scaled_fp8_quant"><code class="name flex">
<span>def <span class="ident">scaled_fp8_quant</span></span>(<span>input: torch.Tensor,<br>scale: torch.Tensor | None = None,<br>num_token_padding: int | None = None,<br>use_per_token_if_dynamic: bool = False) ‑> tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scaled_fp8_quant(
    input: torch.Tensor,
    scale: Optional[torch.Tensor] = None,
    num_token_padding: Optional[int] = None,
    use_per_token_if_dynamic: bool = False,
) -&gt; tuple[torch.Tensor, torch.Tensor]:

    assert input.ndim == 2, f&#34;Expected 2D input tensor, got {input.ndim}D&#34;
    shape = input.shape
    if num_token_padding:
        shape = (max(num_token_padding, input.shape[0]), shape[1])
    output = torch.empty(shape, device=input.device, dtype=fp8_dtype)

    if scale is None:
        # Dynamic scaling
        if use_per_token_if_dynamic:
            scale = torch.empty(
                (shape[0], 1), device=input.device, dtype=torch.float32
            )
            sgl_per_token_quant_fp8(input, output, scale)
        else:
            scale = torch.zeros(1, device=input.device, dtype=torch.float32)
            sgl_per_tensor_quant_fp8(
                input, output, scale, is_static=False
            )  # False for dynamic
    else:
        # Static scaling
        assert (
            scale.numel() == 1
        ), f&#34;Expected scalar scale, got numel={scale.numel()}&#34;
        sgl_per_tensor_quant_fp8(
            input, output, scale, is_static=True
        )  # True for static

    return output, scale</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.select_w8a8_block_fp8_matmul_kernel"><code class="name flex">
<span>def <span class="ident">select_w8a8_block_fp8_matmul_kernel</span></span>(<span>M, N, META)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_w8a8_block_fp8_matmul_kernel(M, N, META):
    return _w8a8_block_fp8_matmul</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.sglang_per_token_group_quant_8bit"><code class="name flex">
<span>def <span class="ident">sglang_per_token_group_quant_8bit</span></span>(<span>x: torch.Tensor,<br>group_size: int,<br>dst_dtype: torch.dtype,<br>eps: float = 1e-10,<br>column_major_scales: bool = False,<br>scale_tma_aligned: bool = False,<br>scale_ue8m0: bool = False,<br>fuse_silu_and_mul: bool = False,<br>masked_m: torch.Tensor | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sglang_per_token_group_quant_8bit(
    x: torch.Tensor,
    group_size: int,
    dst_dtype: torch.dtype,
    eps: float = 1e-10,
    column_major_scales: bool = False,
    scale_tma_aligned: bool = False,
    scale_ue8m0: bool = False,
    fuse_silu_and_mul: bool = False,
    masked_m: Optional[torch.Tensor] = None,
):
    from sglang.srt.layers.quantization.int8_kernel import (
        sglang_per_token_group_quant_int8,
    )

    if dst_dtype == torch.int8:
        assert not column_major_scales
        assert not scale_tma_aligned
        assert not fuse_silu_and_mul
        assert masked_m is None
        return sglang_per_token_group_quant_int8(
            x=x,
            group_size=group_size,
            eps=eps,
            dtype=dst_dtype,
        )

    return sglang_per_token_group_quant_fp8(
        x=x,
        group_size=group_size,
        eps=eps,
        column_major_scales=column_major_scales,
        scale_tma_aligned=scale_tma_aligned,
        scale_ue8m0=scale_ue8m0,
        fuse_silu_and_mul=fuse_silu_and_mul,
        masked_m=masked_m,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.sglang_per_token_group_quant_fp8"><code class="name flex">
<span>def <span class="ident">sglang_per_token_group_quant_fp8</span></span>(<span>x: torch.Tensor,<br>group_size: int,<br>eps: float = 1e-10,<br>column_major_scales: bool = False,<br>scale_tma_aligned: bool = False,<br>scale_ue8m0: bool = False,<br>fuse_silu_and_mul: bool = False,<br>masked_m: torch.Tensor | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sglang_per_token_group_quant_fp8(
    x: torch.Tensor,
    group_size: int,
    eps: float = 1e-10,
    column_major_scales: bool = False,
    scale_tma_aligned: bool = False,
    scale_ue8m0: bool = False,
    fuse_silu_and_mul: bool = False,
    masked_m: Optional[torch.Tensor] = None,
):
    assert (
        x.shape[-1] % group_size == 0
    ), &#34;the last dimension of `x` cannot be divisible by `group_size`&#34;
    assert x.is_contiguous(), &#34;`x` is not contiguous&#34;

    out_shape = (*x.shape[:-1], x.shape[-1] // (2 if fuse_silu_and_mul else 1))

    x_q = torch.empty(out_shape, device=x.device, dtype=fp8_dtype)
    x_s = create_per_token_group_quant_fp8_output_scale(
        x_shape=out_shape,
        device=x.device,
        group_size=group_size,
        column_major_scales=column_major_scales,
        scale_tma_aligned=scale_tma_aligned,
        scale_ue8m0=scale_ue8m0,
    )

    if x.shape[0] &gt; 0:
        sgl_per_token_group_quant_fp8(
            x, x_q, x_s, group_size, eps, fp8_min, fp8_max, scale_ue8m0
        )

    return x_q, x_s</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.sglang_per_token_quant_fp8"><code class="name flex">
<span>def <span class="ident">sglang_per_token_quant_fp8</span></span>(<span>x: torch.Tensor, dtype: torch.dtype = torch.float8_e4m3fn)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sglang_per_token_quant_fp8(
    x: torch.Tensor,
    dtype: torch.dtype = fp8_dtype,
):
    assert x.is_contiguous(), &#34;`x` is not contiguous&#34;

    x_q = torch.empty_like(x, device=x.device, dtype=dtype)
    x_s = torch.empty(
        x.shape[0],
        1,
        device=x.device,
        dtype=torch.float32,
    )

    sgl_per_token_quant_fp8(x, x_q, x_s)

    return x_q, x_s</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.static_quant_fp8"><code class="name flex">
<span>def <span class="ident">static_quant_fp8</span></span>(<span>x: torch.Tensor, x_s: torch.Tensor, repeat_scale: bool = False) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def static_quant_fp8(
    x: torch.Tensor,
    x_s: torch.Tensor,
    repeat_scale: bool = False,
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;Function to perform static quantization using the given scale on an input tensor `x`.

    It converts the tensor values into signed float8 values and returns the
    quantized tensor along with the scaling factor used for quantization.

    Args:
        x: The input tenosr with ndim &gt;= 2.
        x_s: The quantization scale.
        repeat_scale: Whether to broadcast per-tensor scale to per-channel scale.
        dtype: The dype of output tensor.

    Returns:
        Tuple[torch.Tensor, torch.Tensor]: The quantized tensor and the scaling factor for quantization.
    &#34;&#34;&#34;
    assert x.is_contiguous(), &#34;`x` is not contiguous&#34;
    assert x_s.numel() == 1, &#34;only supports per-tensor scale&#34;

    x_q = torch.empty_like(x, device=x.device, dtype=fp8_dtype)
    M = x.numel() // x.shape[-1]
    N = x.shape[-1]
    if repeat_scale:
        x_s_repeat = torch.empty(
            (M, 1),
            device=x.device,
            dtype=torch.float32,
        )
    else:
        x_s_repeat = None

    BLOCK = triton.next_power_of_2(N)
    # heuristics for number of warps
    num_warps = min(max(BLOCK // 256, 1), 8)
    num_stages = 1
    _static_quant_fp8[(M,)](
        x,
        x_q,
        x_s,
        x_s_repeat,
        N,
        N,
        fp8_min=fp8_min,
        fp8_max=fp8_max,
        BLOCK=BLOCK,
        REPEAT_SCALE=repeat_scale,
        num_warps=num_warps,
        num_stages=num_stages,
    )
    x_s = x_s_repeat if repeat_scale else x_s
    return x_q, x_s</code></pre>
</details>
<div class="desc"><p>Function to perform static quantization using the given scale on an input tensor <code>x</code>.</p>
<p>It converts the tensor values into signed float8 values and returns the
quantized tensor along with the scaling factor used for quantization.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>The input tenosr with ndim &gt;= 2.</dd>
<dt><strong><code>x_s</code></strong></dt>
<dd>The quantization scale.</dd>
<dt><strong><code>repeat_scale</code></strong></dt>
<dd>Whether to broadcast per-tensor scale to per-channel scale.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>The dype of output tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[torch.Tensor, torch.Tensor]</code></dt>
<dd>The quantized tensor and the scaling factor for quantization.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.triton_scaled_mm"><code class="name flex">
<span>def <span class="ident">triton_scaled_mm</span></span>(<span>input: torch.Tensor,<br>weight: torch.Tensor,<br>scale_a: torch.Tensor,<br>scale_b: torch.Tensor,<br>out_dtype: type[torch.dtype],<br>bias: torch.Tensor | None = None,<br>block_size_m: int = 32,<br>block_size_n: int = 32,<br>block_size_k: int = 32,<br>use_heuristic=True) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def triton_scaled_mm(
    input: torch.Tensor,
    weight: torch.Tensor,
    scale_a: torch.Tensor,
    scale_b: torch.Tensor,
    out_dtype: type[torch.dtype],
    bias: Optional[torch.Tensor] = None,
    block_size_m: int = 32,
    block_size_n: int = 32,
    block_size_k: int = 32,
    use_heuristic=True,
) -&gt; torch.Tensor:
    M, K = input.shape
    N = weight.shape[1]

    assert N &gt; 0 and K &gt; 0 and M &gt; 0
    assert weight.shape[0] == K
    assert input.dtype == weight.dtype

    scale_a = scale_a.reshape(-1, 1) if scale_a.dim() &lt;= 1 else scale_a
    scale_b = scale_b.reshape(-1, 1) if scale_b.dim() &lt;= 1 else scale_b

    assert scale_a.dtype == scale_b.dtype and scale_a.is_floating_point()
    assert scale_a.shape[1] == 1 and (scale_a.shape[0] == 1 or scale_a.shape[0] == M)
    assert scale_b.shape[1] == 1 and (scale_b.shape[0] == 1 or scale_b.shape[0] == N)
    assert out_dtype.is_floating_point
    assert bias is None or bias.is_floating_point()
    assert is_weak_contiguous(input)
    assert is_weak_contiguous(weight)

    grid = lambda META: (
        triton.cdiv(M, META[&#34;BLOCK_SIZE_M&#34;]) * triton.cdiv(N, META[&#34;BLOCK_SIZE_N&#34;]),
    )

    result = torch.empty((M, N), dtype=out_dtype, device=input.device)

    has_scalar = lambda x: x.shape[0] == 1 and x.shape[1] == 1

    if use_heuristic:
        is_small_N = N &lt; 8192
        next_power_of_2_M = max(32, triton.next_power_of_2(M))
        if next_power_of_2_M &lt;= 32:
            tile_shape = (64, 64, 256) if is_small_N else (64, 128, 256)
        elif next_power_of_2_M &lt;= 64:
            tile_shape = (64, 64, 256)
        elif next_power_of_2_M &lt;= 128:
            tile_shape = (64, 128, 128)
        else:
            tile_shape = (128, 128, 128)

    block_size_m, block_size_n, block_size_k = tile_shape

    block_size_sa = 1 if has_scalar(scale_a) else block_size_m
    block_size_sb = 1 if has_scalar(scale_b) else block_size_n

    accumulator_dtype = tl.float32 if input.is_floating_point() else tl.int32

    # A = input, B = weight, C = result
    # A = M x K, B = K x N, C = M x N
    scaled_mm_kernel[grid](
        input,
        weight,
        scale_a,
        scale_b,
        result,
        bias,
        M,
        N,
        K,
        input.stride(0),
        input.stride(1),
        weight.stride(0),
        weight.stride(1),
        result.stride(0),
        result.stride(1),
        accumulator_dtype,
        BLOCK_SIZE_M=block_size_m,
        BLOCK_SIZE_N=block_size_n,
        BLOCK_SIZE_K=block_size_k,
        BLOCK_SIZE_SCALE_A=block_size_sa,
        BLOCK_SIZE_SCALE_B=block_size_sb,
    )

    return result.to(out_dtype)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.w8a8_block_fp8_matmul"><code class="name flex">
<span>def <span class="ident">w8a8_block_fp8_matmul</span></span>(<span>A: torch.Tensor,<br>B: torch.Tensor,<br>As: torch.Tensor,<br>Bs: torch.Tensor,<br>block_size: List[int],<br>output_dtype: torch.dtype = torch.float16) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def w8a8_block_fp8_matmul(
    A: torch.Tensor,
    B: torch.Tensor,
    As: torch.Tensor,
    Bs: torch.Tensor,
    block_size: List[int],
    output_dtype: torch.dtype = torch.float16,
) -&gt; torch.Tensor:
    if output_dtype == torch.bfloat16 and deep_gemm_wrapper.ENABLE_JIT_DEEPGEMM:
        return w8a8_block_fp8_matmul_deepgemm(
            A, B, As, Bs, block_size, output_dtype=output_dtype
        )

    return w8a8_block_fp8_matmul_triton(
        A, B, As, Bs, block_size, output_dtype=output_dtype
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.w8a8_block_fp8_matmul_deepgemm"><code class="name flex">
<span>def <span class="ident">w8a8_block_fp8_matmul_deepgemm</span></span>(<span>A: torch.Tensor,<br>B: torch.Tensor,<br>As: torch.Tensor,<br>Bs: torch.Tensor,<br>block_size: List[int],<br>output_dtype: torch.dtype) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def w8a8_block_fp8_matmul_deepgemm(
    A: torch.Tensor,
    B: torch.Tensor,
    As: torch.Tensor,
    Bs: torch.Tensor,
    block_size: List[int],
    output_dtype: torch.dtype,
) -&gt; torch.Tensor:
    M, N, K, C = prepare_block_fp8_matmul_inputs(A, B, As, Bs, block_size, output_dtype)

    # Deepgemm only supports output tensor type as bfloat16
    assert C.dtype == torch.bfloat16 and deep_gemm_wrapper.ENABLE_JIT_DEEPGEMM

    if supports_custom_op():
        torch.ops.sglang.deep_gemm_fp8_fp8_bf16_nt(A, As, B, Bs, C)
    else:
        deep_gemm_wrapper.gemm_nt_f8f8bf16((A, As), (B, Bs), C)

    return C</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.quantization.fp8_kernel.w8a8_block_fp8_matmul_triton"><code class="name flex">
<span>def <span class="ident">w8a8_block_fp8_matmul_triton</span></span>(<span>A: torch.Tensor,<br>B: torch.Tensor,<br>As: torch.Tensor,<br>Bs: torch.Tensor,<br>block_size: List[int],<br>output_dtype: torch.dtype = torch.float16) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def w8a8_block_fp8_matmul_triton(
    A: torch.Tensor,
    B: torch.Tensor,
    As: torch.Tensor,
    Bs: torch.Tensor,
    block_size: List[int],
    output_dtype: torch.dtype = torch.float16,
) -&gt; torch.Tensor:
    &#34;&#34;&#34;This function performs matrix multiplication with block-wise quantization.

    It takes two input tensors `A` and `B` with scales `As` and `Bs`.
    The output is returned in the specified `output_dtype`.

    Args:
        A: The input tensor, e.g., activation.
        B: The input tensor, e.g., weight.
        As: The per-token-group quantization scale for `A`.
        Bs: The per-block quantization scale for `B`.
        block_size: The block size for per-block quantization. It should be 2-dim, e.g., [128, 128].
        output_dytpe: The dtype of the returned tensor.

    Returns:
        torch.Tensor: The result of matmul.
    &#34;&#34;&#34;

    M, N, K, C = prepare_block_fp8_matmul_inputs(A, B, As, Bs, block_size, output_dtype)

    block_n, block_k = block_size

    configs = get_w8a8_block_fp8_configs(N, K, block_size[0], block_size[1])
    if configs:
        # If an optimal configuration map has been found, look up the
        # optimal config
        config = configs[min(configs.keys(), key=lambda x: abs(x - M))]
    else:
        # Default config
        # Block-wise quant: BLOCK_SIZE_K must be divisible by block_size[1]
        config = {
            &#34;BLOCK_SIZE_M&#34;: 64,
            &#34;BLOCK_SIZE_N&#34;: block_size[0],
            &#34;BLOCK_SIZE_K&#34;: block_size[1],
            &#34;GROUP_SIZE_M&#34;: 32,
            &#34;num_warps&#34;: 4,
            &#34;num_stages&#34;: 3,
        }

    def grid(META):
        return (
            triton.cdiv(M, META[&#34;BLOCK_SIZE_M&#34;]) * triton.cdiv(N, META[&#34;BLOCK_SIZE_N&#34;]),
        )

    kernel = select_w8a8_block_fp8_matmul_kernel(M, N, config)

    kernel[grid](
        A,
        B,
        C,
        As,
        Bs,
        M,
        N,
        K,
        block_n,
        block_k,
        A.stride(-2),
        A.stride(-1),
        B.stride(1),
        B.stride(0),
        C.stride(-2),
        C.stride(-1),
        As.stride(-2),
        As.stride(-1),
        Bs.stride(1),
        Bs.stride(0),
        **config,
    )

    return C</code></pre>
</details>
<div class="desc"><p>This function performs matrix multiplication with block-wise quantization.</p>
<p>It takes two input tensors <code>A</code> and <code>B</code> with scales <code>As</code> and <code>Bs</code>.
The output is returned in the specified <code>output_dtype</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>A</code></strong></dt>
<dd>The input tensor, e.g., activation.</dd>
<dt><strong><code>B</code></strong></dt>
<dd>The input tensor, e.g., weight.</dd>
<dt><strong><code>As</code></strong></dt>
<dd>The per-token-group quantization scale for <code>A</code>.</dd>
<dt><strong><code>Bs</code></strong></dt>
<dd>The per-block quantization scale for <code>B</code>.</dd>
<dt><strong><code>block_size</code></strong></dt>
<dd>The block size for per-block quantization. It should be 2-dim, e.g., [128, 128].</dd>
<dt><strong><code>output_dytpe</code></strong></dt>
<dd>The dtype of the returned tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>The result of matmul.</dd>
</dl></div>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.layers.quantization" href="index.html">sglang.srt.layers.quantization</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.create_per_token_group_quant_fp8_output_scale" href="#sglang.srt.layers.quantization.fp8_kernel.create_per_token_group_quant_fp8_output_scale">create_per_token_group_quant_fp8_output_scale</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.deep_gemm_fp8_fp8_bf16_nt" href="#sglang.srt.layers.quantization.fp8_kernel.deep_gemm_fp8_fp8_bf16_nt">deep_gemm_fp8_fp8_bf16_nt</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.deep_gemm_fp8_fp8_bf16_nt_fake" href="#sglang.srt.layers.quantization.fp8_kernel.deep_gemm_fp8_fp8_bf16_nt_fake">deep_gemm_fp8_fp8_bf16_nt_fake</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.get_w8a8_block_fp8_configs" href="#sglang.srt.layers.quantization.fp8_kernel.get_w8a8_block_fp8_configs">get_w8a8_block_fp8_configs</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.is_fp8_fnuz" href="#sglang.srt.layers.quantization.fp8_kernel.is_fp8_fnuz">is_fp8_fnuz</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.is_weak_contiguous" href="#sglang.srt.layers.quantization.fp8_kernel.is_weak_contiguous">is_weak_contiguous</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.per_group_transpose" href="#sglang.srt.layers.quantization.fp8_kernel.per_group_transpose">per_group_transpose</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.per_tensor_quant_mla_fp8" href="#sglang.srt.layers.quantization.fp8_kernel.per_tensor_quant_mla_fp8">per_tensor_quant_mla_fp8</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.per_token_group_quant_8bit" href="#sglang.srt.layers.quantization.fp8_kernel.per_token_group_quant_8bit">per_token_group_quant_8bit</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.per_token_group_quant_fp8" href="#sglang.srt.layers.quantization.fp8_kernel.per_token_group_quant_fp8">per_token_group_quant_fp8</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.per_token_group_quant_fp8_hopper_moe_mn_major" href="#sglang.srt.layers.quantization.fp8_kernel.per_token_group_quant_fp8_hopper_moe_mn_major">per_token_group_quant_fp8_hopper_moe_mn_major</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.per_token_group_quant_mla_deep_gemm_masked_fp8" href="#sglang.srt.layers.quantization.fp8_kernel.per_token_group_quant_mla_deep_gemm_masked_fp8">per_token_group_quant_mla_deep_gemm_masked_fp8</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.prepare_block_fp8_matmul_inputs" href="#sglang.srt.layers.quantization.fp8_kernel.prepare_block_fp8_matmul_inputs">prepare_block_fp8_matmul_inputs</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.scaled_fp8_quant" href="#sglang.srt.layers.quantization.fp8_kernel.scaled_fp8_quant">scaled_fp8_quant</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.select_w8a8_block_fp8_matmul_kernel" href="#sglang.srt.layers.quantization.fp8_kernel.select_w8a8_block_fp8_matmul_kernel">select_w8a8_block_fp8_matmul_kernel</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.sglang_per_token_group_quant_8bit" href="#sglang.srt.layers.quantization.fp8_kernel.sglang_per_token_group_quant_8bit">sglang_per_token_group_quant_8bit</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.sglang_per_token_group_quant_fp8" href="#sglang.srt.layers.quantization.fp8_kernel.sglang_per_token_group_quant_fp8">sglang_per_token_group_quant_fp8</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.sglang_per_token_quant_fp8" href="#sglang.srt.layers.quantization.fp8_kernel.sglang_per_token_quant_fp8">sglang_per_token_quant_fp8</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.static_quant_fp8" href="#sglang.srt.layers.quantization.fp8_kernel.static_quant_fp8">static_quant_fp8</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.triton_scaled_mm" href="#sglang.srt.layers.quantization.fp8_kernel.triton_scaled_mm">triton_scaled_mm</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.w8a8_block_fp8_matmul" href="#sglang.srt.layers.quantization.fp8_kernel.w8a8_block_fp8_matmul">w8a8_block_fp8_matmul</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.w8a8_block_fp8_matmul_deepgemm" href="#sglang.srt.layers.quantization.fp8_kernel.w8a8_block_fp8_matmul_deepgemm">w8a8_block_fp8_matmul_deepgemm</a></code></li>
<li><code><a title="sglang.srt.layers.quantization.fp8_kernel.w8a8_block_fp8_matmul_triton" href="#sglang.srt.layers.quantization.fp8_kernel.w8a8_block_fp8_matmul_triton">w8a8_block_fp8_matmul_triton</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
