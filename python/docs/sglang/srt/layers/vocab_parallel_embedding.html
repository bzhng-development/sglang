<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.layers.vocab_parallel_embedding API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.layers.vocab_parallel_embedding</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.layers.vocab_parallel_embedding.get_masked_input_and_mask"><code class="name flex">
<span>def <span class="ident">get_masked_input_and_mask</span></span>(<span>input_: torch.Tensor,<br>org_vocab_start_index: int,<br>org_vocab_end_index: int,<br>num_org_vocab_padding: int,<br>added_vocab_start_index: int,<br>added_vocab_end_index: int) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.compile(dynamic=True, backend=get_compiler_backend())
def get_masked_input_and_mask(
    input_: torch.Tensor,
    org_vocab_start_index: int,
    org_vocab_end_index: int,
    num_org_vocab_padding: int,
    added_vocab_start_index: int,
    added_vocab_end_index: int,
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    # torch.compile will fuse all of the pointwise ops below
    # into a single kernel, making it very fast
    org_vocab_mask = (input_ &gt;= org_vocab_start_index) &amp; (input_ &lt; org_vocab_end_index)
    added_vocab_mask = (input_ &gt;= added_vocab_start_index) &amp; (
        input_ &lt; added_vocab_end_index
    )
    added_offset = (
        added_vocab_start_index
        - (org_vocab_end_index - org_vocab_start_index)
        - num_org_vocab_padding
    )
    valid_offset = (org_vocab_start_index * org_vocab_mask) + (
        added_offset * added_vocab_mask
    )
    vocab_mask = org_vocab_mask | added_vocab_mask
    input_ = vocab_mask * (input_ - valid_offset)
    return input_, ~vocab_mask</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.pad_vocab_size"><code class="name flex">
<span>def <span class="ident">pad_vocab_size</span></span>(<span>vocab_size: int, pad_to: int = 64) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad_vocab_size(vocab_size: int, pad_to: int = DEFAULT_VOCAB_PADDING_SIZE) -&gt; int:
    &#34;&#34;&#34;Pad the vocab size to the given value.&#34;&#34;&#34;
    return ((vocab_size + pad_to - 1) // pad_to) * pad_to</code></pre>
</details>
<div class="desc"><p>Pad the vocab size to the given value.</p></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.vocab_range_from_global_vocab_size"><code class="name flex">
<span>def <span class="ident">vocab_range_from_global_vocab_size</span></span>(<span>global_vocab_size: int, rank: int, world_size: int, offset: int = 0) ‑> Sequence[int]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vocab_range_from_global_vocab_size(
    global_vocab_size: int, rank: int, world_size: int, offset: int = 0
) -&gt; Sequence[int]:
    per_partition_vocab_size = divide(global_vocab_size, world_size)
    return vocab_range_from_per_partition_vocab_size(
        per_partition_vocab_size, rank, offset=offset
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.vocab_range_from_per_partition_vocab_size"><code class="name flex">
<span>def <span class="ident">vocab_range_from_per_partition_vocab_size</span></span>(<span>per_partition_vocab_size: int, rank: int, offset: int = 0) ‑> Sequence[int]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vocab_range_from_per_partition_vocab_size(
    per_partition_vocab_size: int, rank: int, offset: int = 0
) -&gt; Sequence[int]:
    index_f = rank * per_partition_vocab_size
    index_l = index_f + per_partition_vocab_size
    return index_f + offset, index_l + offset</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.layers.vocab_parallel_embedding.ParallelLMHead"><code class="flex name class">
<span>class <span class="ident">ParallelLMHead</span></span>
<span>(</span><span>num_embeddings: int,<br>embedding_dim: int,<br>*,<br>bias: bool = False,<br>params_dtype: torch.dtype | None = None,<br>org_num_embeddings: int | None = None,<br>padding_size: int = 64,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '',<br>use_attn_tp_group: bool = False,<br>use_presharded_weights: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ParallelLMHead(VocabParallelEmbedding):
    &#34;&#34;&#34;Parallelized LM head.

    Output logits weight matrices used in the Sampler. The weight and bias
    tensors are padded to make sure they are divisible by the number of
    model parallel GPUs.

    Args:
        num_embeddings: vocabulary size.
        embedding_dim: size of hidden state.
        bias: whether to use bias.
        params_dtype: type of the parameters.
        org_num_embeddings: original vocabulary size (without LoRA).
        padding_size: padding size for the vocabulary.
    &#34;&#34;&#34;

    def __init__(
        self,
        num_embeddings: int,
        embedding_dim: int,
        *,
        bias: bool = False,
        params_dtype: Optional[torch.dtype] = None,
        org_num_embeddings: Optional[int] = None,
        padding_size: int = DEFAULT_VOCAB_PADDING_SIZE,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
        use_attn_tp_group: bool = False,
        use_presharded_weights: bool = False,
    ):
        super().__init__(
            num_embeddings,
            embedding_dim,
            params_dtype=params_dtype,
            org_num_embeddings=org_num_embeddings,
            padding_size=padding_size,
            quant_config=quant_config,
            prefix=prefix,
            use_attn_tp_group=use_attn_tp_group,
            use_presharded_weights=use_presharded_weights,
        )
        self.quant_config = quant_config

        # We only support pack LMHead if it&#39;s not quantized.
        if _is_cpu and _is_cpu_amx_available:
            if hasattr(self, &#34;weight&#34;) and self.weight.dtype == torch.bfloat16:
                self.quant_method = PackWeightMethod(weight_names=[&#34;weight&#34;])

        if bias:
            self.bias = Parameter(
                torch.empty(self.num_embeddings_per_partition, dtype=params_dtype)
            )
            set_weight_attrs(
                self.bias,
                {
                    &#34;output_dim&#34;: 0,
                    &#34;weight_loader&#34;: self.weight_loader,
                },
            )
        else:
            self.register_parameter(&#34;bias&#34;, None)

    def tie_weights(self, embed_tokens: VocabParallelEmbedding):
        &#34;&#34;&#34;Tie the weights with word embeddings.&#34;&#34;&#34;
        # GGUF quantized embed_tokens.
        if self.quant_config and self.quant_config.get_name() == &#34;gguf&#34;:
            return embed_tokens
        else:
            self.weight = embed_tokens.weight
            return self

    def forward(self, input_):
        del input_
        raise RuntimeError(&#34;LMHead&#39;s weights should be used in the sampler.&#34;)</code></pre>
</details>
<div class="desc"><p>Parallelized LM head.</p>
<p>Output logits weight matrices used in the Sampler. The weight and bias
tensors are padded to make sure they are divisible by the number of
model parallel GPUs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_embeddings</code></strong></dt>
<dd>vocabulary size.</dd>
<dt><strong><code>embedding_dim</code></strong></dt>
<dd>size of hidden state.</dd>
<dt><strong><code>bias</code></strong></dt>
<dd>whether to use bias.</dd>
<dt><strong><code>params_dtype</code></strong></dt>
<dd>type of the parameters.</dd>
<dt><strong><code>org_num_embeddings</code></strong></dt>
<dd>original vocabulary size (without LoRA).</dd>
<dt><strong><code>padding_size</code></strong></dt>
<dd>padding size for the vocabulary.</dd>
</dl>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding">VocabParallelEmbedding</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.vocab_parallel_embedding.ParallelLMHead.tie_weights"><code class="name flex">
<span>def <span class="ident">tie_weights</span></span>(<span>self,<br>embed_tokens: <a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding">VocabParallelEmbedding</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tie_weights(self, embed_tokens: VocabParallelEmbedding):
    &#34;&#34;&#34;Tie the weights with word embeddings.&#34;&#34;&#34;
    # GGUF quantized embed_tokens.
    if self.quant_config and self.quant_config.get_name() == &#34;gguf&#34;:
        return embed_tokens
    else:
        self.weight = embed_tokens.weight
        return self</code></pre>
</details>
<div class="desc"><p>Tie the weights with word embeddings.</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding">VocabParallelEmbedding</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding.extra_repr" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding.extra_repr">extra_repr</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding.forward" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding.forward">forward</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding.get_sharded_to_full_mapping" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding.get_sharded_to_full_mapping">get_sharded_to_full_mapping</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding"><code class="flex name class">
<span>class <span class="ident">VocabParallelEmbedding</span></span>
<span>(</span><span>num_embeddings: int,<br>embedding_dim: int,<br>*,<br>params_dtype: torch.dtype | None = None,<br>org_num_embeddings: int | None = None,<br>padding_size: int = 64,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '',<br>enable_tp: bool = True,<br>use_attn_tp_group: bool = False,<br>use_presharded_weights: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VocabParallelEmbedding(torch.nn.Module):
    &#34;&#34;&#34;Embedding parallelized in the vocabulary dimension.

    Adapted from torch.nn.Embedding, note that we pad the vocabulary size to
    make sure it is divisible by the number of model parallel GPUs.

    In order to support various loading methods, we ensure that LoRA-added
    embeddings are always at the end of TP-sharded tensors. In other words,
    we shard base embeddings and LoRA embeddings separately (both padded),
    and place them in the same tensor.
    In this example, we will have the original vocab size = 1010,
    added vocab size = 16 and padding to 64. Therefore, the total
    vocab size with padding will be 1088 (because we first pad 1010 to
    1024, add 16, and then pad to 1088).
    Therefore, the tensor format looks like the following:
    TP1, rank 0 (no sharding):
                            |&lt; --------BASE-------- &gt;|&lt; -BASE PADDING-- &gt;|&lt; -----LORA------ &gt;|&lt; -LORA PADDING-- &gt;|
    corresponding token_id: |  0  |  1  | ... | 1009 |  -1  | ... |  -1  | 1010 | ... | 1015 |  -1  | ... |  -1  |
                     index: |  0  |  1  | ... | 1009 | 1010 | ... | 1023 | 1024 | ... | 1039 | 1040 | ... | 1087 |

    TP2, rank 0:
                            |&lt; --------------------BASE--------------------- &gt;|&lt; -----LORA------ &gt;|&lt; -LORA PADDING- &gt;|
    corresponding token_id: |  0  |  1  |  2  | ... | 497  | 498 | ...  | 511 | 1000 | ... | 1015 |  -1  | ... |  -1 |
                     index: |  0  |  1  |  2  | ... | 497  | 498 | ...  | 511 | 512  | ... | 527  |  520 | ... | 543 |
    TP2, rank 1:
                            |&lt; -----------BASE----------- &gt;|&lt; -BASE PADDING- &gt;|&lt; -----------LORA PADDING----------- &gt;|
    corresponding token_id: | 512 | 513 | 514 | ... | 1009 | -1  | ...  | -1  |  -1  | ... |  -1  | -1  | ... |   -1 |
                     index: |  0  |  1  |  2  | ... | 497  | 498 | ...  | 511 | 512  | ... | 519  | 520 | ... |  543 |

    Args:
        num_embeddings: vocabulary size.
        embedding_dim: size of hidden state.
        params_dtype: type of the parameters.
        org_num_embeddings: original vocabulary size (without LoRA).
        padding_size: padding size for the vocabulary.
        quant_config: quant config for the layer
        prefix: full name of the layer in the state dict
    &#34;&#34;&#34;  # noqa: E501

    def __init__(
        self,
        num_embeddings: int,
        embedding_dim: int,
        *,
        params_dtype: Optional[torch.dtype] = None,
        org_num_embeddings: Optional[int] = None,
        padding_size: int = DEFAULT_VOCAB_PADDING_SIZE,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
        enable_tp: bool = True,
        use_attn_tp_group: bool = False,
        use_presharded_weights: bool = False,
    ):
        super().__init__()
        self.quant_config = quant_config

        self.enable_tp = enable_tp
        if self.enable_tp:
            if use_attn_tp_group:
                tp_rank = get_attention_tp_rank()
                self.tp_size = get_attention_tp_size()
            else:
                tp_rank = get_tensor_model_parallel_rank()
                self.tp_size = get_tensor_model_parallel_world_size()
        else:
            assert use_attn_tp_group is False
            tp_rank = 0
            self.tp_size = 1

        self.num_embeddings = num_embeddings
        self.org_vocab_size = org_num_embeddings or num_embeddings

        # Support the case where the vocab size is not divisible by the TP size.
        if (
            _is_cpu
            and pad_vocab_size(self.org_vocab_size, padding_size) % self.tp_size != 0
        ):
            padding_size *= self.tp_size
        self.padding_size = padding_size

        num_added_embeddings = num_embeddings - self.org_vocab_size
        self.use_presharded_weights = use_presharded_weights
        if use_presharded_weights:
            assert (
                num_added_embeddings == 0
            ), &#34;Lora is not supported with presharded weights.&#34;

        self.org_vocab_size_padded = pad_vocab_size(
            self.org_vocab_size, self.padding_size
        )
        self.num_embeddings_padded = pad_vocab_size(
            self.org_vocab_size_padded + num_added_embeddings, self.padding_size
        )
        assert self.org_vocab_size_padded &lt;= self.num_embeddings_padded

        self.shard_indices = self._get_indices(
            self.num_embeddings_padded,
            self.org_vocab_size_padded,
            self.num_embeddings,
            self.org_vocab_size,
            tp_rank,
            self.tp_size,
        )
        self.embedding_dim = embedding_dim

        quant_method = None
        if quant_config is not None:
            quant_method = quant_config.get_quant_method(self, prefix=prefix)
        if quant_method is None:
            quant_method = UnquantizedEmbeddingMethod()

        # If we are making an embedding layer, then our quantization linear
        # method must implement the embedding operation. If we are another
        # layer type like ParallelLMHead, this is not important.
        is_embedding_layer = type(self.__class__) is VocabParallelEmbedding
        quant_method_implements_embedding = method_has_implemented_embedding(
            type(quant_method)
        )
        if is_embedding_layer and not quant_method_implements_embedding:
            raise NotImplementedError(
                f&#34;The class {type(quant_method).__name__} must implement &#34;
                &#34;the &#39;embedding&#39; method, see UnquantizedEmbeddingMethod.&#34;
            )

        self.quant_method: QuantizeMethodBase = quant_method

        if params_dtype is None:
            params_dtype = torch.get_default_dtype()
        # Divide the weight matrix along the vocaburaly dimension.
        self.num_added_embeddings = self.num_embeddings - self.org_vocab_size
        self.num_embeddings_per_partition = divide(
            self.num_embeddings_padded, self.tp_size
        )
        assert (
            self.shard_indices.num_elements_padded == self.num_embeddings_per_partition
        )
        self.num_org_embeddings_per_partition = (
            self.shard_indices.org_vocab_end_index
            - self.shard_indices.org_vocab_start_index
        )
        self.num_added_embeddings_per_partition = (
            self.shard_indices.added_vocab_end_index
            - self.shard_indices.added_vocab_start_index
        )

        self.quant_method.create_weights(
            self,
            self.embedding_dim,
            [self.num_embeddings_per_partition],
            self.embedding_dim,
            self.num_embeddings_padded,
            params_dtype=params_dtype,
            weight_loader=self.weight_loader,
        )

    @classmethod
    def _get_indices(
        cls,
        vocab_size_padded: int,
        org_vocab_size_padded: int,
        vocab_size: int,
        org_vocab_size: int,
        tp_rank: int,
        tp_size: int,
    ) -&gt; VocabParallelEmbeddingShardIndices:
        &#34;&#34;&#34;Get start and end indices for vocab parallel embedding, following the
        layout outlined in the class docstring, based on the given tp_rank and
        tp_size.&#34;&#34;&#34;
        num_added_embeddings_padded = vocab_size_padded - org_vocab_size_padded
        padded_org_vocab_start_index, padded_org_vocab_end_index = (
            vocab_range_from_global_vocab_size(org_vocab_size_padded, tp_rank, tp_size)
        )
        padded_added_vocab_start_index, padded_added_vocab_end_index = (
            vocab_range_from_global_vocab_size(
                num_added_embeddings_padded, tp_rank, tp_size, offset=org_vocab_size
            )
        )
        # remove padding
        org_vocab_start_index = min(padded_org_vocab_start_index, org_vocab_size)
        org_vocab_end_index = min(padded_org_vocab_end_index, org_vocab_size)
        added_vocab_start_index = min(padded_added_vocab_start_index, vocab_size)
        added_vocab_end_index = min(padded_added_vocab_end_index, vocab_size)
        return VocabParallelEmbeddingShardIndices(
            padded_org_vocab_start_index,
            padded_org_vocab_end_index,
            padded_added_vocab_start_index,
            padded_added_vocab_end_index,
            org_vocab_start_index,
            org_vocab_end_index,
            added_vocab_start_index,
            added_vocab_end_index,
        )

    def get_sharded_to_full_mapping(self) -&gt; Optional[List[int]]:
        &#34;&#34;&#34;Get a mapping that can be used to reindex the gathered
        logits for sampling.

        During sampling, we gather logits from all ranks. The relationship
        of index-&gt;token_id will follow the same format as outlined in the class
        docstring. However, after the gather, we want to reindex the final
        logits tensor to map index-&gt;token_id one-to-one (the index is always
        equal the token_id it corresponds to). The indices returned by this
        method allow us to do that.
        &#34;&#34;&#34;
        if self.tp_size &lt; 2:
            return None

        base_embeddings: List[int] = []
        added_embeddings: List[int] = []
        padding: List[int] = []
        for tp_rank in range(self.tp_size):
            shard_indices = self._get_indices(
                self.num_embeddings_padded,
                self.org_vocab_size_padded,
                self.num_embeddings,
                self.org_vocab_size,
                tp_rank,
                self.tp_size,
            )
            range_start = self.num_embeddings_per_partition * tp_rank
            range_end = self.num_embeddings_per_partition * (tp_rank + 1)
            base_embeddings.extend(
                range(range_start, range_start + shard_indices.num_org_elements)
            )
            padding.extend(
                range(
                    range_start + shard_indices.num_org_elements,
                    range_start + shard_indices.num_org_elements_padded,
                )
            )
            added_embeddings.extend(
                range(
                    range_start + shard_indices.num_org_elements_padded,
                    range_start
                    + shard_indices.num_org_elements_padded
                    + shard_indices.num_added_elements,
                )
            )
            padding.extend(
                range(
                    range_start
                    + shard_indices.num_org_elements_padded
                    + shard_indices.num_added_elements,
                    range_start
                    + shard_indices.num_org_elements_padded
                    + shard_indices.num_added_elements_padded,
                )
            )
            assert (
                range_start
                + shard_indices.num_org_elements_padded
                + shard_indices.num_added_elements_padded
                == range_end
            )
        ret = base_embeddings + added_embeddings + padding
        assert len(ret) == self.num_embeddings_padded
        return ret

    def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
        output_dim = getattr(param, &#34;output_dim&#34;, None)
        packed_dim = getattr(param, &#34;packed_dim&#34;, None)

        # If the parameter is a gguf weight, then load it directly.
        if getattr(param, &#34;is_gguf_weight_type&#34;, None):
            param.data.copy_(loaded_weight)
            param.weight_type = loaded_weight.item()
            return
        elif isinstance(param, UninitializedParameter):
            shape = list(loaded_weight.shape)
            if output_dim is not None:
                shape[output_dim] = shape[output_dim] // self.tp_size
            param.materialize(tuple(shape), dtype=loaded_weight.dtype)

        # If parameter does not have output dim, then it should
        # be copied onto all gpus (e.g. g_idx for act_order gptq).
        if output_dim is None:
            assert param.data.shape == loaded_weight.shape
            param.data.copy_(loaded_weight)
            return

        # Shard indexes for loading the weight
        start_idx = self.shard_indices.org_vocab_start_index
        shard_size = self.shard_indices.org_vocab_end_index - start_idx

        # If param packed on the same dim we are sharding on, then
        # need to adjust offsets of loaded weight by pack_factor.
        if packed_dim is not None and packed_dim == output_dim:
            packed_factor = (
                param.packed_factor
                if isinstance(param, BasevLLMParameter)
                else param.packed_factor
            )
            assert loaded_weight.shape[output_dim] == (
                self.org_vocab_size // param.packed_factor
            )
            start_idx = start_idx // packed_factor
            shard_size = shard_size // packed_factor
        else:
            assert loaded_weight.shape[output_dim] == (
                self.org_vocab_size
                // (self.tp_size if self.use_presharded_weights else 1)
            ), f&#34;{self.org_vocab_size=} {self.use_presharded_weights=} {loaded_weight.shape[output_dim]=}&#34;

        # Copy the data.
        if not self.use_presharded_weights:
            loaded_weight = loaded_weight.narrow(output_dim, start_idx, shard_size)
        param[: loaded_weight.shape[0]].data.copy_(loaded_weight)
        param[loaded_weight.shape[0] :].data.fill_(0)

    def forward(self, input_):
        if self.tp_size &gt; 1:
            # Build the mask.
            masked_input, input_mask = get_masked_input_and_mask(
                input_,
                self.shard_indices.org_vocab_start_index,
                self.shard_indices.org_vocab_end_index,
                self.shard_indices.num_org_vocab_padding,
                self.shard_indices.added_vocab_start_index,
                self.shard_indices.added_vocab_end_index,
            )
        else:
            masked_input = input_
        # Get the embeddings.
        with use_symmetric_memory(parallel_state.get_tp_group()) as sm:
            output_parallel = self.quant_method.embedding(self, masked_input.long())
            sm.tag(output_parallel)
        # Mask the output embedding.
        if self.tp_size &gt; 1:
            output_parallel.masked_fill_(input_mask.unsqueeze(-1), 0)
            # Reduce across all the model parallel GPUs.
            output = tensor_model_parallel_all_reduce(output_parallel)
        else:
            output = output_parallel
        return output

    def extra_repr(self) -&gt; str:
        s = f&#34;num_embeddings={self.num_embeddings_per_partition}&#34;
        s += f&#34;, embedding_dim={self.embedding_dim}&#34;
        s += f&#34;, org_vocab_size={self.org_vocab_size}&#34;
        s += f&#34;, num_embeddings_padded={self.num_embeddings_padded}&#34;
        if self.enable_tp:
            s += f&#34;, tp_size={self.tp_size}&#34;
        return s</code></pre>
</details>
<div class="desc"><p>Embedding parallelized in the vocabulary dimension.</p>
<p>Adapted from torch.nn.Embedding, note that we pad the vocabulary size to
make sure it is divisible by the number of model parallel GPUs.</p>
<p>In order to support various loading methods, we ensure that LoRA-added
embeddings are always at the end of TP-sharded tensors. In other words,
we shard base embeddings and LoRA embeddings separately (both padded),
and place them in the same tensor.
In this example, we will have the original vocab size = 1010,
added vocab size = 16 and padding to 64. Therefore, the total
vocab size with padding will be 1088 (because we first pad 1010 to
1024, add 16, and then pad to 1088).
Therefore, the tensor format looks like the following:
TP1, rank 0 (no sharding):
|&lt; --------BASE-------- &gt;|&lt; -BASE PADDING&ndash; &gt;|&lt; -----LORA------ &gt;|&lt; -LORA PADDING&ndash; &gt;|
corresponding token_id: |
0
|
1
| &hellip; | 1009 |
-1
| &hellip; |
-1
| 1010 | &hellip; | 1015 |
-1
| &hellip; |
-1
|
index: |
0
|
1
| &hellip; | 1009 | 1010 | &hellip; | 1023 | 1024 | &hellip; | 1039 | 1040 | &hellip; | 1087 |</p>
<p>TP2, rank 0:
|&lt; --------------------BASE--------------------- &gt;|&lt; -----LORA------ &gt;|&lt; -LORA PADDING- &gt;|
corresponding token_id: |
0
|
1
|
2
| &hellip; | 497
| 498 | &hellip;
| 511 | 1000 | &hellip; | 1015 |
-1
| &hellip; |
-1 |
index: |
0
|
1
|
2
| &hellip; | 497
| 498 | &hellip;
| 511 | 512
| &hellip; | 527
|
520 | &hellip; | 543 |
TP2, rank 1:
|&lt; -----------BASE----------- &gt;|&lt; -BASE PADDING- &gt;|&lt; -----------LORA PADDING----------- &gt;|
corresponding token_id: | 512 | 513 | 514 | &hellip; | 1009 | -1
| &hellip;
| -1
|
-1
| &hellip; |
-1
| -1
| &hellip; |
-1 |
index: |
0
|
1
|
2
| &hellip; | 497
| 498 | &hellip;
| 511 | 512
| &hellip; | 519
| 520 | &hellip; |
543 |</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_embeddings</code></strong></dt>
<dd>vocabulary size.</dd>
<dt><strong><code>embedding_dim</code></strong></dt>
<dd>size of hidden state.</dd>
<dt><strong><code>params_dtype</code></strong></dt>
<dd>type of the parameters.</dd>
<dt><strong><code>org_num_embeddings</code></strong></dt>
<dd>original vocabulary size (without LoRA).</dd>
<dt><strong><code>padding_size</code></strong></dt>
<dd>padding size for the vocabulary.</dd>
<dt><strong><code>quant_config</code></strong></dt>
<dd>quant config for the layer</dd>
<dt><strong><code>prefix</code></strong></dt>
<dd>full name of the layer in the state dict</dd>
</dl>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.vocab_parallel_embedding.ParallelLMHead" href="#sglang.srt.layers.vocab_parallel_embedding.ParallelLMHead">ParallelLMHead</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding.extra_repr"><code class="name flex">
<span>def <span class="ident">extra_repr</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extra_repr(self) -&gt; str:
    s = f&#34;num_embeddings={self.num_embeddings_per_partition}&#34;
    s += f&#34;, embedding_dim={self.embedding_dim}&#34;
    s += f&#34;, org_vocab_size={self.org_vocab_size}&#34;
    s += f&#34;, num_embeddings_padded={self.num_embeddings_padded}&#34;
    if self.enable_tp:
        s += f&#34;, tp_size={self.tp_size}&#34;
    return s</code></pre>
</details>
<div class="desc"><p>Return the extra representation of the module.</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, input_) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, input_):
    if self.tp_size &gt; 1:
        # Build the mask.
        masked_input, input_mask = get_masked_input_and_mask(
            input_,
            self.shard_indices.org_vocab_start_index,
            self.shard_indices.org_vocab_end_index,
            self.shard_indices.num_org_vocab_padding,
            self.shard_indices.added_vocab_start_index,
            self.shard_indices.added_vocab_end_index,
        )
    else:
        masked_input = input_
    # Get the embeddings.
    with use_symmetric_memory(parallel_state.get_tp_group()) as sm:
        output_parallel = self.quant_method.embedding(self, masked_input.long())
        sm.tag(output_parallel)
    # Mask the output embedding.
    if self.tp_size &gt; 1:
        output_parallel.masked_fill_(input_mask.unsqueeze(-1), 0)
        # Reduce across all the model parallel GPUs.
        output = tensor_model_parallel_all_reduce(output_parallel)
    else:
        output = output_parallel
    return output</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding.get_sharded_to_full_mapping"><code class="name flex">
<span>def <span class="ident">get_sharded_to_full_mapping</span></span>(<span>self) ‑> List[int] | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_sharded_to_full_mapping(self) -&gt; Optional[List[int]]:
    &#34;&#34;&#34;Get a mapping that can be used to reindex the gathered
    logits for sampling.

    During sampling, we gather logits from all ranks. The relationship
    of index-&gt;token_id will follow the same format as outlined in the class
    docstring. However, after the gather, we want to reindex the final
    logits tensor to map index-&gt;token_id one-to-one (the index is always
    equal the token_id it corresponds to). The indices returned by this
    method allow us to do that.
    &#34;&#34;&#34;
    if self.tp_size &lt; 2:
        return None

    base_embeddings: List[int] = []
    added_embeddings: List[int] = []
    padding: List[int] = []
    for tp_rank in range(self.tp_size):
        shard_indices = self._get_indices(
            self.num_embeddings_padded,
            self.org_vocab_size_padded,
            self.num_embeddings,
            self.org_vocab_size,
            tp_rank,
            self.tp_size,
        )
        range_start = self.num_embeddings_per_partition * tp_rank
        range_end = self.num_embeddings_per_partition * (tp_rank + 1)
        base_embeddings.extend(
            range(range_start, range_start + shard_indices.num_org_elements)
        )
        padding.extend(
            range(
                range_start + shard_indices.num_org_elements,
                range_start + shard_indices.num_org_elements_padded,
            )
        )
        added_embeddings.extend(
            range(
                range_start + shard_indices.num_org_elements_padded,
                range_start
                + shard_indices.num_org_elements_padded
                + shard_indices.num_added_elements,
            )
        )
        padding.extend(
            range(
                range_start
                + shard_indices.num_org_elements_padded
                + shard_indices.num_added_elements,
                range_start
                + shard_indices.num_org_elements_padded
                + shard_indices.num_added_elements_padded,
            )
        )
        assert (
            range_start
            + shard_indices.num_org_elements_padded
            + shard_indices.num_added_elements_padded
            == range_end
        )
    ret = base_embeddings + added_embeddings + padding
    assert len(ret) == self.num_embeddings_padded
    return ret</code></pre>
</details>
<div class="desc"><p>Get a mapping that can be used to reindex the gathered
logits for sampling.</p>
<p>During sampling, we gather logits from all ranks. The relationship
of index-&gt;token_id will follow the same format as outlined in the class
docstring. However, after the gather, we want to reindex the final
logits tensor to map index-&gt;token_id one-to-one (the index is always
equal the token_id it corresponds to). The indices returned by this
method allow us to do that.</p></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding.weight_loader"><code class="name flex">
<span>def <span class="ident">weight_loader</span></span>(<span>self, param: torch.nn.parameter.Parameter, loaded_weight: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor):
    output_dim = getattr(param, &#34;output_dim&#34;, None)
    packed_dim = getattr(param, &#34;packed_dim&#34;, None)

    # If the parameter is a gguf weight, then load it directly.
    if getattr(param, &#34;is_gguf_weight_type&#34;, None):
        param.data.copy_(loaded_weight)
        param.weight_type = loaded_weight.item()
        return
    elif isinstance(param, UninitializedParameter):
        shape = list(loaded_weight.shape)
        if output_dim is not None:
            shape[output_dim] = shape[output_dim] // self.tp_size
        param.materialize(tuple(shape), dtype=loaded_weight.dtype)

    # If parameter does not have output dim, then it should
    # be copied onto all gpus (e.g. g_idx for act_order gptq).
    if output_dim is None:
        assert param.data.shape == loaded_weight.shape
        param.data.copy_(loaded_weight)
        return

    # Shard indexes for loading the weight
    start_idx = self.shard_indices.org_vocab_start_index
    shard_size = self.shard_indices.org_vocab_end_index - start_idx

    # If param packed on the same dim we are sharding on, then
    # need to adjust offsets of loaded weight by pack_factor.
    if packed_dim is not None and packed_dim == output_dim:
        packed_factor = (
            param.packed_factor
            if isinstance(param, BasevLLMParameter)
            else param.packed_factor
        )
        assert loaded_weight.shape[output_dim] == (
            self.org_vocab_size // param.packed_factor
        )
        start_idx = start_idx // packed_factor
        shard_size = shard_size // packed_factor
    else:
        assert loaded_weight.shape[output_dim] == (
            self.org_vocab_size
            // (self.tp_size if self.use_presharded_weights else 1)
        ), f&#34;{self.org_vocab_size=} {self.use_presharded_weights=} {loaded_weight.shape[output_dim]=}&#34;

    # Copy the data.
    if not self.use_presharded_weights:
        loaded_weight = loaded_weight.narrow(output_dim, start_idx, shard_size)
    param[: loaded_weight.shape[0]].data.copy_(loaded_weight)
    param[loaded_weight.shape[0] :].data.fill_(0)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices"><code class="flex name class">
<span>class <span class="ident">VocabParallelEmbeddingShardIndices</span></span>
<span>(</span><span>padded_org_vocab_start_index: int,<br>padded_org_vocab_end_index: int,<br>padded_added_vocab_start_index: int,<br>padded_added_vocab_end_index: int,<br>org_vocab_start_index: int,<br>org_vocab_end_index: int,<br>added_vocab_start_index: int,<br>added_vocab_end_index: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class VocabParallelEmbeddingShardIndices:
    &#34;&#34;&#34;Indices for a shard of a vocab parallel embedding.&#34;&#34;&#34;

    padded_org_vocab_start_index: int
    padded_org_vocab_end_index: int
    padded_added_vocab_start_index: int
    padded_added_vocab_end_index: int

    org_vocab_start_index: int
    org_vocab_end_index: int
    added_vocab_start_index: int
    added_vocab_end_index: int

    @property
    def num_org_elements(self) -&gt; int:
        return self.org_vocab_end_index - self.org_vocab_start_index

    @property
    def num_added_elements(self) -&gt; int:
        return self.added_vocab_end_index - self.added_vocab_start_index

    @property
    def num_org_elements_padded(self) -&gt; int:
        return self.padded_org_vocab_end_index - self.padded_org_vocab_start_index

    @property
    def num_added_elements_padded(self) -&gt; int:
        return self.padded_added_vocab_end_index - self.padded_added_vocab_start_index

    @property
    def num_org_vocab_padding(self) -&gt; int:
        return self.num_org_elements_padded - self.num_org_elements

    @property
    def num_added_vocab_padding(self) -&gt; int:
        return self.num_added_elements_padded - self.num_added_elements

    @property
    def num_elements_padded(self) -&gt; int:
        return self.num_org_elements_padded + self.num_added_elements_padded

    def __post_init__(self):
        # sanity checks
        assert self.padded_org_vocab_start_index &lt;= self.padded_org_vocab_end_index
        assert self.padded_added_vocab_start_index &lt;= self.padded_added_vocab_end_index

        assert self.org_vocab_start_index &lt;= self.org_vocab_end_index
        assert self.added_vocab_start_index &lt;= self.added_vocab_end_index

        assert self.org_vocab_start_index &lt;= self.padded_org_vocab_start_index
        assert self.added_vocab_start_index &lt;= self.padded_added_vocab_start_index
        assert self.org_vocab_end_index &lt;= self.padded_org_vocab_end_index
        assert self.added_vocab_end_index &lt;= self.padded_added_vocab_end_index

        assert self.num_org_elements &lt;= self.num_org_elements_padded
        assert self.num_added_elements &lt;= self.num_added_elements_padded</code></pre>
</details>
<div class="desc"><p>Indices for a shard of a vocab parallel embedding.</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.added_vocab_end_index"><code class="name">var <span class="ident">added_vocab_end_index</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.added_vocab_start_index"><code class="name">var <span class="ident">added_vocab_start_index</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_added_elements"><code class="name">prop <span class="ident">num_added_elements</span> : int</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def num_added_elements(self) -&gt; int:
    return self.added_vocab_end_index - self.added_vocab_start_index</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_added_elements_padded"><code class="name">prop <span class="ident">num_added_elements_padded</span> : int</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def num_added_elements_padded(self) -&gt; int:
    return self.padded_added_vocab_end_index - self.padded_added_vocab_start_index</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_added_vocab_padding"><code class="name">prop <span class="ident">num_added_vocab_padding</span> : int</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def num_added_vocab_padding(self) -&gt; int:
    return self.num_added_elements_padded - self.num_added_elements</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_elements_padded"><code class="name">prop <span class="ident">num_elements_padded</span> : int</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def num_elements_padded(self) -&gt; int:
    return self.num_org_elements_padded + self.num_added_elements_padded</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_org_elements"><code class="name">prop <span class="ident">num_org_elements</span> : int</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def num_org_elements(self) -&gt; int:
    return self.org_vocab_end_index - self.org_vocab_start_index</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_org_elements_padded"><code class="name">prop <span class="ident">num_org_elements_padded</span> : int</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def num_org_elements_padded(self) -&gt; int:
    return self.padded_org_vocab_end_index - self.padded_org_vocab_start_index</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_org_vocab_padding"><code class="name">prop <span class="ident">num_org_vocab_padding</span> : int</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def num_org_vocab_padding(self) -&gt; int:
    return self.num_org_elements_padded - self.num_org_elements</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.org_vocab_end_index"><code class="name">var <span class="ident">org_vocab_end_index</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.org_vocab_start_index"><code class="name">var <span class="ident">org_vocab_start_index</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.padded_added_vocab_end_index"><code class="name">var <span class="ident">padded_added_vocab_end_index</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.padded_added_vocab_start_index"><code class="name">var <span class="ident">padded_added_vocab_start_index</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.padded_org_vocab_end_index"><code class="name">var <span class="ident">padded_org_vocab_end_index</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.padded_org_vocab_start_index"><code class="name">var <span class="ident">padded_org_vocab_start_index</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.layers" href="index.html">sglang.srt.layers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.get_masked_input_and_mask" href="#sglang.srt.layers.vocab_parallel_embedding.get_masked_input_and_mask">get_masked_input_and_mask</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.pad_vocab_size" href="#sglang.srt.layers.vocab_parallel_embedding.pad_vocab_size">pad_vocab_size</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.vocab_range_from_global_vocab_size" href="#sglang.srt.layers.vocab_parallel_embedding.vocab_range_from_global_vocab_size">vocab_range_from_global_vocab_size</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.vocab_range_from_per_partition_vocab_size" href="#sglang.srt.layers.vocab_parallel_embedding.vocab_range_from_per_partition_vocab_size">vocab_range_from_per_partition_vocab_size</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.layers.vocab_parallel_embedding.ParallelLMHead" href="#sglang.srt.layers.vocab_parallel_embedding.ParallelLMHead">ParallelLMHead</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.ParallelLMHead.tie_weights" href="#sglang.srt.layers.vocab_parallel_embedding.ParallelLMHead.tie_weights">tie_weights</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding">VocabParallelEmbedding</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding.extra_repr" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding.extra_repr">extra_repr</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding.forward" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding.forward">forward</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding.get_sharded_to_full_mapping" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding.get_sharded_to_full_mapping">get_sharded_to_full_mapping</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding.weight_loader" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding.weight_loader">weight_loader</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices">VocabParallelEmbeddingShardIndices</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.added_vocab_end_index" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.added_vocab_end_index">added_vocab_end_index</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.added_vocab_start_index" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.added_vocab_start_index">added_vocab_start_index</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_added_elements" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_added_elements">num_added_elements</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_added_elements_padded" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_added_elements_padded">num_added_elements_padded</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_added_vocab_padding" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_added_vocab_padding">num_added_vocab_padding</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_elements_padded" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_elements_padded">num_elements_padded</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_org_elements" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_org_elements">num_org_elements</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_org_elements_padded" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_org_elements_padded">num_org_elements_padded</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_org_vocab_padding" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.num_org_vocab_padding">num_org_vocab_padding</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.org_vocab_end_index" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.org_vocab_end_index">org_vocab_end_index</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.org_vocab_start_index" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.org_vocab_start_index">org_vocab_start_index</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.padded_added_vocab_end_index" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.padded_added_vocab_end_index">padded_added_vocab_end_index</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.padded_added_vocab_start_index" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.padded_added_vocab_start_index">padded_added_vocab_start_index</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.padded_org_vocab_end_index" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.padded_org_vocab_end_index">padded_org_vocab_end_index</a></code></li>
<li><code><a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.padded_org_vocab_start_index" href="#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbeddingShardIndices.padded_org_vocab_start_index">padded_org_vocab_start_index</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
