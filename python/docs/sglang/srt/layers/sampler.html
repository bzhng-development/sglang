<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.layers.sampler API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.layers.sampler</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.layers.sampler.apply_custom_logit_processor"><code class="name flex">
<span>def <span class="ident">apply_custom_logit_processor</span></span>(<span>logits: torch.Tensor,<br>sampling_batch_info: <a title="sglang.srt.sampling.sampling_batch_info.SamplingBatchInfo" href="../sampling/sampling_batch_info.html#sglang.srt.sampling.sampling_batch_info.SamplingBatchInfo">SamplingBatchInfo</a>,<br>num_tokens_in_batch: int = 1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_custom_logit_processor(
    logits: torch.Tensor,
    sampling_batch_info: SamplingBatchInfo,
    num_tokens_in_batch: int = 1,
):
    &#34;&#34;&#34;Apply custom logit processors to the logits.
    This function will modify the logits in-place.
    num_tokens_in_batch is needed to support spec decoding, where each batch can contain multiple
    tokens. By default, we assume each batch contains only 1 token.
    &#34;&#34;&#34;

    assert logits.shape[0] == len(sampling_batch_info) * num_tokens_in_batch, (
        f&#34;The batch size of logits ({logits.shape[0]}) does not match the batch size of &#34;
        f&#34;sampling_batch_info ({len(sampling_batch_info)}) x num_tokens_in_batch &#34;
        f&#34;({num_tokens_in_batch})&#34;
    )

    for _, (
        processor,
        batch_mask,
    ) in sampling_batch_info.custom_logit_processor.items():
        # Get the batch indices that need to be processed
        batch_indices = batch_mask.nonzero(as_tuple=True)[0]

        assert batch_mask.shape[0] == len(sampling_batch_info), (
            f&#34;The number of batch mask ({batch_mask.shape[0]}) does not match the number of &#34;
            f&#34;sampling_batch_info ({len(sampling_batch_info)})&#34;
        )
        batch_mask = torch.repeat_interleave(batch_mask, num_tokens_in_batch)

        # Apply the processor to the logits
        logits[batch_mask] = processor(
            logits[batch_mask],
            [sampling_batch_info.custom_params[i] for i in batch_indices],
        )

        logger.debug(
            f&#34;Custom logit processor {processor.__class__.__name__} is applied.&#34;
        )</code></pre>
</details>
<div class="desc"><p>Apply custom logit processors to the logits.
This function will modify the logits in-place.
num_tokens_in_batch is needed to support spec decoding, where each batch can contain multiple
tokens. By default, we assume each batch contains only 1 token.</p></div>
</dd>
<dt id="sglang.srt.layers.sampler.get_token_ids_logprobs"><code class="name flex">
<span>def <span class="ident">get_token_ids_logprobs</span></span>(<span>logprobs: torch.Tensor, token_ids_logprobs: List[List[int]])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_token_ids_logprobs(
    logprobs: torch.Tensor,
    token_ids_logprobs: List[List[int]],
):
    output_token_ids_logprobs_val = []
    output_token_ids_logprobs_idx = []
    for i, token_ids in enumerate(token_ids_logprobs):
        if token_ids is not None:
            output_token_ids_logprobs_val.append(logprobs[i, token_ids].tolist())
            output_token_ids_logprobs_idx.append(token_ids)
        else:
            output_token_ids_logprobs_val.append([])
            output_token_ids_logprobs_idx.append([])

    return (
        output_token_ids_logprobs_val,
        output_token_ids_logprobs_idx,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.sampler.get_top_logprobs"><code class="name flex">
<span>def <span class="ident">get_top_logprobs</span></span>(<span>logprobs: torch.Tensor, top_logprobs_nums: List[int])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_top_logprobs(
    logprobs: torch.Tensor,
    top_logprobs_nums: List[int],
):
    max_k = max(top_logprobs_nums)
    ret = logprobs.topk(max_k, dim=1)
    values = ret.values.tolist()
    indices = ret.indices.tolist()

    output_top_logprobs_val = []
    output_top_logprobs_idx = []
    for i, k in enumerate(top_logprobs_nums):
        output_top_logprobs_val.append(values[i][:k])
        output_top_logprobs_idx.append(indices[i][:k])

    return (
        output_top_logprobs_val,
        output_top_logprobs_idx,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.sampler.sampling_from_probs_torch"><code class="name flex">
<span>def <span class="ident">sampling_from_probs_torch</span></span>(<span>probs: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sampling_from_probs_torch(probs: torch.Tensor):
    &#34;&#34;&#34;A sampling implementation with native pytorch operations, without
    top-k, top-p, or min-p filtering.&#34;&#34;&#34;
    sampled_index = torch.multinomial(probs, num_samples=1)
    batch_next_token_ids = sampled_index.view(-1).to(torch.int32)
    return batch_next_token_ids</code></pre>
</details>
<div class="desc"><p>A sampling implementation with native pytorch operations, without
top-k, top-p, or min-p filtering.</p></div>
</dd>
<dt id="sglang.srt.layers.sampler.top_k_top_p_min_p_sampling_from_probs_torch"><code class="name flex">
<span>def <span class="ident">top_k_top_p_min_p_sampling_from_probs_torch</span></span>(<span>probs: torch.Tensor,<br>top_ks: torch.Tensor,<br>top_ps: torch.Tensor,<br>min_ps: torch.Tensor,<br>need_min_p_sampling: bool)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def top_k_top_p_min_p_sampling_from_probs_torch(
    probs: torch.Tensor,
    top_ks: torch.Tensor,
    top_ps: torch.Tensor,
    min_ps: torch.Tensor,
    need_min_p_sampling: bool,
):
    &#34;&#34;&#34;A top-k, top-p and min-p sampling implementation with native pytorch operations.&#34;&#34;&#34;
    probs_sort, probs_idx = probs.sort(dim=-1, descending=True)
    probs_sum = torch.cumsum(probs_sort, dim=-1)
    probs_sort[
        torch.arange(0, probs.shape[-1], device=probs.device).view(1, -1)
        &gt;= top_ks.view(-1, 1)
    ] = 0.0
    probs_sort[(probs_sum - probs_sort) &gt; top_ps.view(-1, 1)] = 0.0

    if need_min_p_sampling:
        min_p_thresholds = probs_sort[:, 0] * min_ps
        probs_sort[probs_sort &lt; min_p_thresholds.view(-1, 1)] = 0.0

    sampled_index = torch.multinomial(probs_sort, num_samples=1)
    # int32 range is enough to represent the token ids
    probs_idx = probs_idx.to(torch.int32)
    batch_next_token_ids = torch.gather(probs_idx, dim=1, index=sampled_index).view(-1)
    return batch_next_token_ids</code></pre>
</details>
<div class="desc"><p>A top-k, top-p and min-p sampling implementation with native pytorch operations.</p></div>
</dd>
<dt id="sglang.srt.layers.sampler.top_p_normalize_probs_torch"><code class="name flex">
<span>def <span class="ident">top_p_normalize_probs_torch</span></span>(<span>probs: torch.Tensor, top_ps: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def top_p_normalize_probs_torch(
    probs: torch.Tensor,
    top_ps: torch.Tensor,
):
    # See also top_k_top_p_min_p_sampling_from_probs_torch
    probs_sort, probs_idx = probs.sort(dim=-1, descending=True)
    probs_sum = torch.cumsum(probs_sort, dim=-1)
    probs_sort[(probs_sum - probs_sort) &gt; top_ps.view(-1, 1)] = 0.0
    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))
    return torch.zeros_like(probs_sort).scatter_(-1, probs_idx, probs_sort)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.layers.sampler.Sampler"><code class="flex name class">
<span>class <span class="ident">Sampler</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Sampler(nn.Module):
    def __init__(self):
        super().__init__()
        self.use_nan_detection = global_server_args_dict[&#34;enable_nan_detection&#34;]
        self.tp_sync_group = get_tp_group().device_group

        if is_dp_attention_enabled():
            self.tp_sync_group = get_attention_tp_group().device_group

    def forward(
        self,
        logits_output: LogitsProcessorOutput,
        sampling_info: SamplingBatchInfo,
        return_logprob: bool,
        top_logprobs_nums: List[int],
        token_ids_logprobs: List[List[int]],
    ):
        &#34;&#34;&#34;Run a sampler &amp; compute logprobs and update logits_output accordingly.

        Args:
            logits_output: The logits from the model forward
            sampling_info: Metadata for sampling
            return_logprob: If set, store the output logprob information to
                logits_output
            top_logprobs_nums: Number of top lobprobs per sequence in a batch
            batch_next_token_ids: next token IDs. If set, skip sampling and only
                compute output logprobs It is used for speculative decoding which
                performs sampling in draft workers.
        &#34;&#34;&#34;
        logits = logits_output.next_token_logits

        # Apply the custom logit processors if registered in the sampling info.
        if sampling_info.has_custom_logit_processor:
            apply_custom_logit_processor(logits, sampling_info)

        if self.use_nan_detection and torch.any(torch.isnan(logits)):
            logger.warning(&#34;Detected errors during sampling! NaN in the logits.&#34;)
            logits = torch.where(
                torch.isnan(logits), torch.full_like(logits, -1e5), logits
            )
            if crash_on_warnings():
                raise ValueError(&#34;Detected errors during sampling! NaN in the logits.&#34;)

        if sampling_info.is_all_greedy:
            # Use torch.argmax if all requests use greedy sampling
            batch_next_token_ids = torch.argmax(logits, -1)
            if return_logprob:
                logprobs = torch.nn.functional.log_softmax(logits, dim=-1)

        else:
            # Post process original logits. if temperatures are all 1.0, no need to rescale
            if return_logprob and RETURN_ORIGINAL_LOGPROB:
                logprobs = torch.softmax(logits, dim=-1)

            # Post process logits
            logits.div_(sampling_info.temperatures)
            logits[:] = torch.softmax(logits, dim=-1)
            probs = logits
            del logits

            if True:  # Keep this redundant check to simplify some internal code sync
                if global_server_args_dict[&#34;sampling_backend&#34;] == &#34;flashinfer&#34;:
                    if sampling_info.need_min_p_sampling:
                        probs = top_k_renorm_prob(probs, sampling_info.top_ks)
                        probs = top_p_renorm_prob(probs, sampling_info.top_ps)
                        batch_next_token_ids = min_p_sampling_from_probs(
                            probs, sampling_info.min_ps
                        )
                    else:
                        batch_next_token_ids = top_k_top_p_sampling_from_probs(
                            probs.contiguous(),
                            sampling_info.top_ks,
                            sampling_info.top_ps,
                            filter_apply_order=&#34;joint&#34;,
                            check_nan=self.use_nan_detection,
                        )
                elif global_server_args_dict[&#34;sampling_backend&#34;] == &#34;pytorch&#34;:
                    # A slower fallback implementation with torch native operations.
                    batch_next_token_ids = top_k_top_p_min_p_sampling_from_probs_torch(
                        probs,
                        sampling_info.top_ks,
                        sampling_info.top_ps,
                        sampling_info.min_ps,
                        sampling_info.need_min_p_sampling,
                    )
                else:
                    raise ValueError(
                        f&#34;Invalid sampling backend: {global_server_args_dict[&#39;sampling_backend&#39;]}&#34;
                    )

            if return_logprob:
                # clamp to avoid -inf
                if RETURN_ORIGINAL_LOGPROB:
                    logprobs = torch.log(logprobs).clamp(
                        min=torch.finfo(logprobs.dtype).min
                    )
                else:
                    logprobs = torch.log(probs).clamp(min=torch.finfo(probs.dtype).min)

        # Attach logprobs to logits_output (in-place modification)
        if return_logprob:
            if any(x &gt; 0 for x in top_logprobs_nums):
                (
                    logits_output.next_token_top_logprobs_val,
                    logits_output.next_token_top_logprobs_idx,
                ) = get_top_logprobs(logprobs, top_logprobs_nums)

            if any(x is not None for x in token_ids_logprobs):
                (
                    logits_output.next_token_token_ids_logprobs_val,
                    logits_output.next_token_token_ids_logprobs_idx,
                ) = get_token_ids_logprobs(logprobs, token_ids_logprobs)

            logits_output.next_token_logprobs = logprobs[
                torch.arange(len(batch_next_token_ids), device=sampling_info.device),
                batch_next_token_ids,
            ]

        if SYNC_TOKEN_IDS_ACROSS_TP or sampling_info.grammars:
            # For performance reasons, SGLang does not sync the final token IDs across TP ranks by default.
            # This saves one all-reduce, but the correctness of this approach depends on the determinism of several operators:
            # the last all-reduce, the last lm_head matmul, and all sampling kernels.
            # These kernels are deterministic in most cases, but there are some rare instances where they are not deterministic.
            # In such cases, enable this env variable to prevent hanging due to TP ranks becoming desynchronized.
            # When using xgrammar, this becomes more likely so we also do the sync when grammar is used.

            torch.distributed.all_reduce(
                batch_next_token_ids,
                op=dist.ReduceOp.MIN,
                group=self.tp_sync_group,
            )

        return batch_next_token_ids</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.sampler.Sampler.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>logits_output: <a title="sglang.srt.layers.logits_processor.LogitsProcessorOutput" href="logits_processor.html#sglang.srt.layers.logits_processor.LogitsProcessorOutput">LogitsProcessorOutput</a>,<br>sampling_info: <a title="sglang.srt.sampling.sampling_batch_info.SamplingBatchInfo" href="../sampling/sampling_batch_info.html#sglang.srt.sampling.sampling_batch_info.SamplingBatchInfo">SamplingBatchInfo</a>,<br>return_logprob: bool,<br>top_logprobs_nums: List[int],<br>token_ids_logprobs: List[List[int]]) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    logits_output: LogitsProcessorOutput,
    sampling_info: SamplingBatchInfo,
    return_logprob: bool,
    top_logprobs_nums: List[int],
    token_ids_logprobs: List[List[int]],
):
    &#34;&#34;&#34;Run a sampler &amp; compute logprobs and update logits_output accordingly.

    Args:
        logits_output: The logits from the model forward
        sampling_info: Metadata for sampling
        return_logprob: If set, store the output logprob information to
            logits_output
        top_logprobs_nums: Number of top lobprobs per sequence in a batch
        batch_next_token_ids: next token IDs. If set, skip sampling and only
            compute output logprobs It is used for speculative decoding which
            performs sampling in draft workers.
    &#34;&#34;&#34;
    logits = logits_output.next_token_logits

    # Apply the custom logit processors if registered in the sampling info.
    if sampling_info.has_custom_logit_processor:
        apply_custom_logit_processor(logits, sampling_info)

    if self.use_nan_detection and torch.any(torch.isnan(logits)):
        logger.warning(&#34;Detected errors during sampling! NaN in the logits.&#34;)
        logits = torch.where(
            torch.isnan(logits), torch.full_like(logits, -1e5), logits
        )
        if crash_on_warnings():
            raise ValueError(&#34;Detected errors during sampling! NaN in the logits.&#34;)

    if sampling_info.is_all_greedy:
        # Use torch.argmax if all requests use greedy sampling
        batch_next_token_ids = torch.argmax(logits, -1)
        if return_logprob:
            logprobs = torch.nn.functional.log_softmax(logits, dim=-1)

    else:
        # Post process original logits. if temperatures are all 1.0, no need to rescale
        if return_logprob and RETURN_ORIGINAL_LOGPROB:
            logprobs = torch.softmax(logits, dim=-1)

        # Post process logits
        logits.div_(sampling_info.temperatures)
        logits[:] = torch.softmax(logits, dim=-1)
        probs = logits
        del logits

        if True:  # Keep this redundant check to simplify some internal code sync
            if global_server_args_dict[&#34;sampling_backend&#34;] == &#34;flashinfer&#34;:
                if sampling_info.need_min_p_sampling:
                    probs = top_k_renorm_prob(probs, sampling_info.top_ks)
                    probs = top_p_renorm_prob(probs, sampling_info.top_ps)
                    batch_next_token_ids = min_p_sampling_from_probs(
                        probs, sampling_info.min_ps
                    )
                else:
                    batch_next_token_ids = top_k_top_p_sampling_from_probs(
                        probs.contiguous(),
                        sampling_info.top_ks,
                        sampling_info.top_ps,
                        filter_apply_order=&#34;joint&#34;,
                        check_nan=self.use_nan_detection,
                    )
            elif global_server_args_dict[&#34;sampling_backend&#34;] == &#34;pytorch&#34;:
                # A slower fallback implementation with torch native operations.
                batch_next_token_ids = top_k_top_p_min_p_sampling_from_probs_torch(
                    probs,
                    sampling_info.top_ks,
                    sampling_info.top_ps,
                    sampling_info.min_ps,
                    sampling_info.need_min_p_sampling,
                )
            else:
                raise ValueError(
                    f&#34;Invalid sampling backend: {global_server_args_dict[&#39;sampling_backend&#39;]}&#34;
                )

        if return_logprob:
            # clamp to avoid -inf
            if RETURN_ORIGINAL_LOGPROB:
                logprobs = torch.log(logprobs).clamp(
                    min=torch.finfo(logprobs.dtype).min
                )
            else:
                logprobs = torch.log(probs).clamp(min=torch.finfo(probs.dtype).min)

    # Attach logprobs to logits_output (in-place modification)
    if return_logprob:
        if any(x &gt; 0 for x in top_logprobs_nums):
            (
                logits_output.next_token_top_logprobs_val,
                logits_output.next_token_top_logprobs_idx,
            ) = get_top_logprobs(logprobs, top_logprobs_nums)

        if any(x is not None for x in token_ids_logprobs):
            (
                logits_output.next_token_token_ids_logprobs_val,
                logits_output.next_token_token_ids_logprobs_idx,
            ) = get_token_ids_logprobs(logprobs, token_ids_logprobs)

        logits_output.next_token_logprobs = logprobs[
            torch.arange(len(batch_next_token_ids), device=sampling_info.device),
            batch_next_token_ids,
        ]

    if SYNC_TOKEN_IDS_ACROSS_TP or sampling_info.grammars:
        # For performance reasons, SGLang does not sync the final token IDs across TP ranks by default.
        # This saves one all-reduce, but the correctness of this approach depends on the determinism of several operators:
        # the last all-reduce, the last lm_head matmul, and all sampling kernels.
        # These kernels are deterministic in most cases, but there are some rare instances where they are not deterministic.
        # In such cases, enable this env variable to prevent hanging due to TP ranks becoming desynchronized.
        # When using xgrammar, this becomes more likely so we also do the sync when grammar is used.

        torch.distributed.all_reduce(
            batch_next_token_ids,
            op=dist.ReduceOp.MIN,
            group=self.tp_sync_group,
        )

    return batch_next_token_ids</code></pre>
</details>
<div class="desc"><p>Run a sampler &amp; compute logprobs and update logits_output accordingly.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>logits_output</code></strong></dt>
<dd>The logits from the model forward</dd>
<dt><strong><code>sampling_info</code></strong></dt>
<dd>Metadata for sampling</dd>
<dt><strong><code>return_logprob</code></strong></dt>
<dd>If set, store the output logprob information to
logits_output</dd>
<dt><strong><code>top_logprobs_nums</code></strong></dt>
<dd>Number of top lobprobs per sequence in a batch</dd>
<dt><strong><code>batch_next_token_ids</code></strong></dt>
<dd>next token IDs. If set, skip sampling and only
compute output logprobs It is used for speculative decoding which
performs sampling in draft workers.</dd>
</dl></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.layers" href="index.html">sglang.srt.layers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.layers.sampler.apply_custom_logit_processor" href="#sglang.srt.layers.sampler.apply_custom_logit_processor">apply_custom_logit_processor</a></code></li>
<li><code><a title="sglang.srt.layers.sampler.get_token_ids_logprobs" href="#sglang.srt.layers.sampler.get_token_ids_logprobs">get_token_ids_logprobs</a></code></li>
<li><code><a title="sglang.srt.layers.sampler.get_top_logprobs" href="#sglang.srt.layers.sampler.get_top_logprobs">get_top_logprobs</a></code></li>
<li><code><a title="sglang.srt.layers.sampler.sampling_from_probs_torch" href="#sglang.srt.layers.sampler.sampling_from_probs_torch">sampling_from_probs_torch</a></code></li>
<li><code><a title="sglang.srt.layers.sampler.top_k_top_p_min_p_sampling_from_probs_torch" href="#sglang.srt.layers.sampler.top_k_top_p_min_p_sampling_from_probs_torch">top_k_top_p_min_p_sampling_from_probs_torch</a></code></li>
<li><code><a title="sglang.srt.layers.sampler.top_p_normalize_probs_torch" href="#sglang.srt.layers.sampler.top_p_normalize_probs_torch">top_p_normalize_probs_torch</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.layers.sampler.Sampler" href="#sglang.srt.layers.sampler.Sampler">Sampler</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.sampler.Sampler.forward" href="#sglang.srt.layers.sampler.Sampler.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
