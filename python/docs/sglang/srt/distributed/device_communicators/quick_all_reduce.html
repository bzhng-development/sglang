<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.distributed.device_communicators.quick_all_reduce API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.distributed.device_communicators.quick_all_reduce</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.distributed.device_communicators.quick_all_reduce.qr_rocm_arch_available"><code class="name flex">
<span>def <span class="ident">qr_rocm_arch_available</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def qr_rocm_arch_available():
    if not _is_hip:
        return False
    try:
        props = torch.cuda.get_device_properties(0)
        gcn_arch = getattr(props, &#34;gcnArchName&#34;, &#34;&#34;)
        supported_archs = [&#34;gfx94&#34;, &#34;gfx95&#34;]
        return any(gfx in gcn_arch for gfx in supported_archs)
    except Exception as e:
        logger.warning(&#34;Failed to determine ROCm for quick allreduce: %s&#34;, e)
        return False</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickAllReduce"><code class="flex name class">
<span>class <span class="ident">QuickAllReduce</span></span>
<span>(</span><span>group: torch.distributed.distributed_c10d.ProcessGroup,<br>device: int | str | torch.device)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class QuickAllReduce:

    _SUPPORTED_WORLD_SIZES = [2, 4, 8]
    _SUPPORTED_DTYPES = [torch.float16, torch.bfloat16]
    # The following data is based on kernel tests.
    # In this order [FP, INT8, INT6, INT4].
    _QR_MIN_SIZE = {
        (torch.float16, 2): [1 * MB, 2 * MB, 2 * MB, 1 * MB],
        (torch.float16, 4): [1 * MB, 16 * MB, 4 * MB, 2 * MB],
        (torch.float16, 8): [16 * MB, 4 * MB, 4 * MB, 2 * MB],
        (torch.bfloat16, 2): [2 * MB, 8 * MB, 8 * MB, 8 * MB],
        (torch.bfloat16, 4): [8 * MB, 64 * MB, 64 * MB, 16 * MB],
        (torch.bfloat16, 8): [16 * MB, 2048 * MB, 2048 * MB, 2048 * MB],
    }

    def __init__(
        self, group: ProcessGroup, device: Union[int, str, torch.device]
    ) -&gt; None:
        &#34;&#34;&#34;
        Custom allreduce provides non-destructive acceleration and is
        available for CUDA and ROCm MI300 series.
        Custom quick allreduce leverages quantization for further
        acceleration on ROCm. It currently supports Q8, Q6, and Q4
        quantization formats and FP(float16, bfloat16).
        Quick allreduce is designed as a complement to custom allreduce.
        Its initialization requires even stricter conditions.
        Only the ROCm MI300 series is supported for quick allreduce at
        this time.
        Args:
            group: the process group to work on. If None, it will use the
                default process group.
            device: the device to bind the CustomAllreduce to. If None,
                it will be bind to f&#34;cuda:{local_rank}&#34;.
        It is the caller&#39;s responsibility to make sure each communicator
        is bind to a unique device, and all communicators in this group
        are in the same node.
        &#34;&#34;&#34;
        self.disabled = True
        if not qr_rocm_arch_available():
            logger.debug(
                &#34;Custom quick allreduce is only supported on ROCm MI300 series.&#34;
            )
            return

        if not quick_ar:
            # disable because of missing quick reduce library
            # e.g. in a cuda environment
            logger.info(
                &#34;Custom quick allreduce is disabled because &#34;
                &#34;of missing custom quick allreduce library&#34;
            )
            return

        self.group = group
        assert (
            dist.get_backend(group) != dist.Backend.NCCL
        ), &#34;Custom quick allreduce should be attached to a non-NCCL group.&#34;
        if not all(in_the_same_node_as(group, source_rank=0)):
            # No need to initialize custom quick allreduce for
            # multi-node case.
            logger.warning(
                &#34;Custom quick allreduce is disabled because this &#34;
                &#34;process group spans across nodes.&#34;
            )
            return
        rank = dist.get_rank(group=self.group)
        world_size = dist.get_world_size(group=self.group)
        self.rank = rank
        self.world_size = world_size
        if world_size == 1:
            # No need to initialize QuickReduce for single GPU case.
            return

        if world_size not in QuickAllReduce._SUPPORTED_WORLD_SIZES:
            logger.warning(
                &#34;Custom quick allreduce is disabled due to an &#34;
                &#34;unsupported world size: %d. Supported world sizes: %s.&#34;,
                world_size,
                str(QuickAllReduce._SUPPORTED_WORLD_SIZES),
            )
            return

        if isinstance(device, int):
            device = torch.device(f&#34;cuda:{device}&#34;)
        elif isinstance(device, str):
            device = torch.device(device)
        assert isinstance(device, torch.device)
        self.device = device

        cuda_visible_devices = os.environ.get(&#34;CUDA_VISIBLE_DEVICES&#34;, None)
        if cuda_visible_devices:
            device_ids = list(map(int, cuda_visible_devices.split(&#34;,&#34;)))
        else:
            device_ids = list(range(torch.cuda.device_count()))
        physical_device_id = device_ids[device.index]
        tensor = torch.tensor([physical_device_id], dtype=torch.int, device=&#34;cpu&#34;)
        gather_list = [
            torch.tensor([0], dtype=torch.int, device=&#34;cpu&#34;)
            for _ in range(self.world_size)
        ]
        dist.all_gather(gather_list, tensor, group=self.group)
        physical_device_ids = [t.item() for t in gather_list]

        # test nvlink first, this will filter out most of the cases
        # where custom quick allreduce is not supported
        # this checks hardware and driver support for NVLink
        if _is_cuda or _is_hip:
            self.fully_connected = is_full_nvlink(physical_device_ids, self.world_size)
        if self.world_size &gt; 2 and not self.fully_connected:
            logger.debug(
                &#34;Custom quick allreduce is disabled because it&#39;s not supported &#34;
                &#34;on more than two PCIe-only GPUs. &#34;
            )
            return

        self.init_quick_all_reduce()

    def init_quick_all_reduce(self):
        # On RocM, bfloat16 kernels are slower than fp16
        # due to slower match operations
        # If environment variable is set to 1, we convert input to fp16
        self.use_fp16_kernels = int(
            os.environ.get(&#34;ROCM_QUICK_REDUCE_CAST_BF16_TO_FP16&#34;, 1)
        )
        regime_str = os.environ.get(&#34;ROCM_QUICK_REDUCE_QUANTIZATION&#34;, &#34;NONE&#34;)
        if regime_str not in QuickReduceRegime.__members__:
            logger.warning(
                &#34;Custom quick allreduce:&#34;,
                f&#34;Invalid quantization level: {regime_str}. &#34;
                &#34;Supported levels: &#34;
                f&#34;{list(QuickReduceRegime.__members__.keys())}&#34;,
            )
            return

        if regime_str == &#34;NONE&#34;:
            logger.debug(
                &#34;Custom quick allreduce is disabled based &#34;
                &#34;on env variable &#34;
                &#34;ROCM_QUICK_REDUCE_QUANTIZATION=&#39;NONE&#39;&#34;
            )
            return
        self.qr_quant_level = QuickReduceRegime[regime_str]

        # TODO: If the dtype is not bfloat16 or then float16,
        # quickallreduce should not be created.

        # ROCM_QUICK_REDUCE_MAX_SIZE_BYTES_MB is specified in MB
        qr_max_size = int(os.environ.get(&#34;ROCM_QUICK_REDUCE_MAX_SIZE_BYTES_MB&#34;, 0))
        if qr_max_size &gt; 0:
            if qr_max_size &lt; 1:
                logger.info(
                    &#34;You should not set a max_size smaller than 1MB, which can &#34;
                    &#34;lead to error or degradation to custom allreduce or rccl.&#34;
                )
            qr_max_size = qr_max_size * MB
        # If qr_max_size is None, then 2GB is used by default.
        self._ptr = ops.init_custom_qr(self.rank, self.world_size, qr_max_size)
        self.qr_max_size = qr_max_size if qr_max_size &gt; 0 else ops.qr_max_size()
        self.create_shared_buffer()
        self.disabled = False

    def create_shared_buffer(self):
        &#34;&#34;&#34;
        Creates a shared buffer for quickreduce.
        Has to be called after init_custom_qr
        &#34;&#34;&#34;
        handle = ops.qr_get_handle(self._ptr)
        world_size = dist.get_world_size(group=self.group)
        handles = [None] * world_size
        dist.all_gather_object(handles, handle, group=self.group)
        ops.qr_open_handles(self._ptr, handles)

    def should_quick_allreduce(self, inp: torch.Tensor):
        &#34;&#34;&#34;
        Check if quickreduce is available
        &#34;&#34;&#34;
        if self.disabled:
            return False
        if inp.dtype not in self._SUPPORTED_DTYPES:
            return False
        inp_size = inp.numel() * inp.element_size()
        # custom quick allreduce requires input byte size to be
        # multiples of 16
        if inp_size % 16 != 0:
            return False
        if not is_weak_contiguous(inp):
            return False
        dtype = inp.dtype
        if self.use_fp16_kernels:
            dtype = torch.float16
        return (
            inp_size &lt;= self.qr_max_size
            and inp_size
            &gt;= self._QR_MIN_SIZE[(dtype, self.world_size)][self.qr_quant_level.value]
        )

    def quick_all_reduce(self, inp: torch.Tensor, *, out: torch.Tensor = None):
        &#34;&#34;&#34;Performs an out-of-place custom quick all reduce.&#34;&#34;&#34;
        # quick allreduce doesn&#39;t require a separate graph mode,
        # as QR uses static IPC buffer.
        if out is None:
            out = torch.empty_like(inp)
        ops.qr_all_reduce(
            self._ptr, inp, out, self.qr_quant_level.value, self.use_fp16_kernels
        )
        return out

    def close(self):
        if not self.disabled and getattr(self, &#34;_ptr&#34;, None):
            if ops is not None:
                ops.qr_destroy(self._ptr)
            self._ptr = 0
            self.disabled = True

    def __del__(self):
        self.close()</code></pre>
</details>
<div class="desc"><p>Custom allreduce provides non-destructive acceleration and is
available for CUDA and ROCm MI300 series.
Custom quick allreduce leverages quantization for further
acceleration on ROCm. It currently supports Q8, Q6, and Q4
quantization formats and FP(float16, bfloat16).
Quick allreduce is designed as a complement to custom allreduce.
Its initialization requires even stricter conditions.
Only the ROCm MI300 series is supported for quick allreduce at
this time.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>group</code></strong></dt>
<dd>the process group to work on. If None, it will use the
default process group.</dd>
<dt><strong><code>device</code></strong></dt>
<dd>the device to bind the CustomAllreduce to. If None,
it will be bind to f"cuda:{local_rank}".</dd>
</dl>
<p>It is the caller's responsibility to make sure each communicator
is bind to a unique device, and all communicators in this group
are in the same node.</p></div>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickAllReduce.close"><code class="name flex">
<span>def <span class="ident">close</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def close(self):
    if not self.disabled and getattr(self, &#34;_ptr&#34;, None):
        if ops is not None:
            ops.qr_destroy(self._ptr)
        self._ptr = 0
        self.disabled = True</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickAllReduce.create_shared_buffer"><code class="name flex">
<span>def <span class="ident">create_shared_buffer</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_shared_buffer(self):
    &#34;&#34;&#34;
    Creates a shared buffer for quickreduce.
    Has to be called after init_custom_qr
    &#34;&#34;&#34;
    handle = ops.qr_get_handle(self._ptr)
    world_size = dist.get_world_size(group=self.group)
    handles = [None] * world_size
    dist.all_gather_object(handles, handle, group=self.group)
    ops.qr_open_handles(self._ptr, handles)</code></pre>
</details>
<div class="desc"><p>Creates a shared buffer for quickreduce.
Has to be called after init_custom_qr</p></div>
</dd>
<dt id="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickAllReduce.init_quick_all_reduce"><code class="name flex">
<span>def <span class="ident">init_quick_all_reduce</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_quick_all_reduce(self):
    # On RocM, bfloat16 kernels are slower than fp16
    # due to slower match operations
    # If environment variable is set to 1, we convert input to fp16
    self.use_fp16_kernels = int(
        os.environ.get(&#34;ROCM_QUICK_REDUCE_CAST_BF16_TO_FP16&#34;, 1)
    )
    regime_str = os.environ.get(&#34;ROCM_QUICK_REDUCE_QUANTIZATION&#34;, &#34;NONE&#34;)
    if regime_str not in QuickReduceRegime.__members__:
        logger.warning(
            &#34;Custom quick allreduce:&#34;,
            f&#34;Invalid quantization level: {regime_str}. &#34;
            &#34;Supported levels: &#34;
            f&#34;{list(QuickReduceRegime.__members__.keys())}&#34;,
        )
        return

    if regime_str == &#34;NONE&#34;:
        logger.debug(
            &#34;Custom quick allreduce is disabled based &#34;
            &#34;on env variable &#34;
            &#34;ROCM_QUICK_REDUCE_QUANTIZATION=&#39;NONE&#39;&#34;
        )
        return
    self.qr_quant_level = QuickReduceRegime[regime_str]

    # TODO: If the dtype is not bfloat16 or then float16,
    # quickallreduce should not be created.

    # ROCM_QUICK_REDUCE_MAX_SIZE_BYTES_MB is specified in MB
    qr_max_size = int(os.environ.get(&#34;ROCM_QUICK_REDUCE_MAX_SIZE_BYTES_MB&#34;, 0))
    if qr_max_size &gt; 0:
        if qr_max_size &lt; 1:
            logger.info(
                &#34;You should not set a max_size smaller than 1MB, which can &#34;
                &#34;lead to error or degradation to custom allreduce or rccl.&#34;
            )
        qr_max_size = qr_max_size * MB
    # If qr_max_size is None, then 2GB is used by default.
    self._ptr = ops.init_custom_qr(self.rank, self.world_size, qr_max_size)
    self.qr_max_size = qr_max_size if qr_max_size &gt; 0 else ops.qr_max_size()
    self.create_shared_buffer()
    self.disabled = False</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickAllReduce.quick_all_reduce"><code class="name flex">
<span>def <span class="ident">quick_all_reduce</span></span>(<span>self, inp: torch.Tensor, *, out: torch.Tensor = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def quick_all_reduce(self, inp: torch.Tensor, *, out: torch.Tensor = None):
    &#34;&#34;&#34;Performs an out-of-place custom quick all reduce.&#34;&#34;&#34;
    # quick allreduce doesn&#39;t require a separate graph mode,
    # as QR uses static IPC buffer.
    if out is None:
        out = torch.empty_like(inp)
    ops.qr_all_reduce(
        self._ptr, inp, out, self.qr_quant_level.value, self.use_fp16_kernels
    )
    return out</code></pre>
</details>
<div class="desc"><p>Performs an out-of-place custom quick all reduce.</p></div>
</dd>
<dt id="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickAllReduce.should_quick_allreduce"><code class="name flex">
<span>def <span class="ident">should_quick_allreduce</span></span>(<span>self, inp: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def should_quick_allreduce(self, inp: torch.Tensor):
    &#34;&#34;&#34;
    Check if quickreduce is available
    &#34;&#34;&#34;
    if self.disabled:
        return False
    if inp.dtype not in self._SUPPORTED_DTYPES:
        return False
    inp_size = inp.numel() * inp.element_size()
    # custom quick allreduce requires input byte size to be
    # multiples of 16
    if inp_size % 16 != 0:
        return False
    if not is_weak_contiguous(inp):
        return False
    dtype = inp.dtype
    if self.use_fp16_kernels:
        dtype = torch.float16
    return (
        inp_size &lt;= self.qr_max_size
        and inp_size
        &gt;= self._QR_MIN_SIZE[(dtype, self.world_size)][self.qr_quant_level.value]
    )</code></pre>
</details>
<div class="desc"><p>Check if quickreduce is available</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickReduceRegime"><code class="flex name class">
<span>class <span class="ident">QuickReduceRegime</span></span>
<span>(</span><span>*args, **kwds)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class QuickReduceRegime(Enum):
    FP = 0
    INT8 = 1
    INT6 = 2
    INT4 = 3
    NONE = 4</code></pre>
</details>
<div class="desc"><p>Create a collection of name/value pairs.</p>
<p>Example enumeration:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; class Color(Enum):
...     RED = 1
...     BLUE = 2
...     GREEN = 3
</code></pre>
<p>Access them by:</p>
<ul>
<li>attribute access:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color.RED
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<ul>
<li>value lookup:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color(1)
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<ul>
<li>name lookup:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color['RED']
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<p>Enumerations can be iterated over, and know how many members they have:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; len(Color)
3
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; list(Color)
[&lt;Color.RED: 1&gt;, &lt;Color.BLUE: 2&gt;, &lt;Color.GREEN: 3&gt;]
</code></pre>
<p>Methods can be added to enumerations, and members can have their own
attributes &ndash; see the documentation for details.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickReduceRegime.FP"><code class="name">var <span class="ident">FP</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickReduceRegime.INT4"><code class="name">var <span class="ident">INT4</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickReduceRegime.INT6"><code class="name">var <span class="ident">INT6</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickReduceRegime.INT8"><code class="name">var <span class="ident">INT8</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickReduceRegime.NONE"><code class="name">var <span class="ident">NONE</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.distributed.device_communicators" href="index.html">sglang.srt.distributed.device_communicators</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.distributed.device_communicators.quick_all_reduce.qr_rocm_arch_available" href="#sglang.srt.distributed.device_communicators.quick_all_reduce.qr_rocm_arch_available">qr_rocm_arch_available</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickAllReduce" href="#sglang.srt.distributed.device_communicators.quick_all_reduce.QuickAllReduce">QuickAllReduce</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickAllReduce.close" href="#sglang.srt.distributed.device_communicators.quick_all_reduce.QuickAllReduce.close">close</a></code></li>
<li><code><a title="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickAllReduce.create_shared_buffer" href="#sglang.srt.distributed.device_communicators.quick_all_reduce.QuickAllReduce.create_shared_buffer">create_shared_buffer</a></code></li>
<li><code><a title="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickAllReduce.init_quick_all_reduce" href="#sglang.srt.distributed.device_communicators.quick_all_reduce.QuickAllReduce.init_quick_all_reduce">init_quick_all_reduce</a></code></li>
<li><code><a title="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickAllReduce.quick_all_reduce" href="#sglang.srt.distributed.device_communicators.quick_all_reduce.QuickAllReduce.quick_all_reduce">quick_all_reduce</a></code></li>
<li><code><a title="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickAllReduce.should_quick_allreduce" href="#sglang.srt.distributed.device_communicators.quick_all_reduce.QuickAllReduce.should_quick_allreduce">should_quick_allreduce</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickReduceRegime" href="#sglang.srt.distributed.device_communicators.quick_all_reduce.QuickReduceRegime">QuickReduceRegime</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickReduceRegime.FP" href="#sglang.srt.distributed.device_communicators.quick_all_reduce.QuickReduceRegime.FP">FP</a></code></li>
<li><code><a title="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickReduceRegime.INT4" href="#sglang.srt.distributed.device_communicators.quick_all_reduce.QuickReduceRegime.INT4">INT4</a></code></li>
<li><code><a title="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickReduceRegime.INT6" href="#sglang.srt.distributed.device_communicators.quick_all_reduce.QuickReduceRegime.INT6">INT6</a></code></li>
<li><code><a title="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickReduceRegime.INT8" href="#sglang.srt.distributed.device_communicators.quick_all_reduce.QuickReduceRegime.INT8">INT8</a></code></li>
<li><code><a title="sglang.srt.distributed.device_communicators.quick_all_reduce.QuickReduceRegime.NONE" href="#sglang.srt.distributed.device_communicators.quick_all_reduce.QuickReduceRegime.NONE">NONE</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
