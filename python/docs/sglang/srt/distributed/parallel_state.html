<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.distributed.parallel_state API documentation</title>
<meta name="description" content="vLLM distributed state.
It takes over the control of the distributed environment from PyTorch.
The typical workflow is: …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.distributed.parallel_state</code></h1>
</header>
<section id="section-intro">
<p>vLLM distributed state.
It takes over the control of the distributed environment from PyTorch.
The typical workflow is:</p>
<ul>
<li>call <code><a title="sglang.srt.distributed.parallel_state.init_distributed_environment" href="#sglang.srt.distributed.parallel_state.init_distributed_environment">init_distributed_environment()</a></code> to initialize the distributed environment.</li>
<li>
<p>call <code><a title="sglang.srt.distributed.parallel_state.initialize_model_parallel" href="#sglang.srt.distributed.parallel_state.initialize_model_parallel">initialize_model_parallel()</a></code> or <code><a title="sglang.srt.distributed.parallel_state.ensure_model_parallel_initialized" href="#sglang.srt.distributed.parallel_state.ensure_model_parallel_initialized">ensure_model_parallel_initialized()</a></code> to
initialize the model parallel groups.</p>
</li>
<li>
<p>any code dealing with the distributed stuff</p>
</li>
<li>
<p>call <code><a title="sglang.srt.distributed.parallel_state.destroy_model_parallel" href="#sglang.srt.distributed.parallel_state.destroy_model_parallel">destroy_model_parallel()</a></code> to destroy the model parallel groups.</p>
</li>
<li>call <code><a title="sglang.srt.distributed.parallel_state.destroy_distributed_environment" href="#sglang.srt.distributed.parallel_state.destroy_distributed_environment">destroy_distributed_environment()</a></code> to destroy the distributed environment.</li>
</ul>
<p>If you only need to use the distributed environment without model/pipeline
parallelism, you can skip the model parallel initialization and destruction
steps.</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.distributed.parallel_state.cleanup_dist_env_and_memory"><code class="name flex">
<span>def <span class="ident">cleanup_dist_env_and_memory</span></span>(<span>shutdown_ray: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cleanup_dist_env_and_memory(shutdown_ray: bool = False):
    destroy_model_parallel()
    destroy_distributed_environment()
    with contextlib.suppress(AssertionError):
        torch.distributed.destroy_process_group()
    if shutdown_ray:
        import ray  # Lazy import Ray

        ray.shutdown()
    gc.collect()
    if not current_platform.is_cpu():
        if hasattr(torch, &#34;cuda&#34;) and torch.cuda.is_available():
            torch.cuda.empty_cache()
            if hasattr(torch._C, &#34;_host_emptyCache&#34;):
                torch._C._host_emptyCache()
            else:
                logger.warning(
                    &#34;torch._C._host_emptyCache() only available in Pytorch &gt;=2.5&#34;
                )
        elif hasattr(torch, &#34;xpu&#34;) and torch.xpu.is_available():
            torch.xpu.empty_cache()
        elif hasattr(torch, &#34;npu&#34;) and torch.npu.is_available():
            torch.npu.empty_cache()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.destroy_distributed_environment"><code class="name flex">
<span>def <span class="ident">destroy_distributed_environment</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def destroy_distributed_environment():
    global _WORLD
    if _WORLD:
        _WORLD.destroy()
    _WORLD = None
    if torch.distributed.is_initialized():
        torch.distributed.destroy_process_group()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.destroy_model_parallel"><code class="name flex">
<span>def <span class="ident">destroy_model_parallel</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def destroy_model_parallel():
    &#34;&#34;&#34;Set the groups to none and destroy them.&#34;&#34;&#34;
    global _TP
    if _TP:
        _TP.destroy()
    _TP = None

    global _PP
    if _PP:
        _PP.destroy()
    _PP = None</code></pre>
</details>
<div class="desc"><p>Set the groups to none and destroy them.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.ensure_model_parallel_initialized"><code class="name flex">
<span>def <span class="ident">ensure_model_parallel_initialized</span></span>(<span>tensor_model_parallel_size: int,<br>expert_model_parallel_size: int,<br>pipeline_model_parallel_size: int,<br>backend: str | None = None) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ensure_model_parallel_initialized(
    tensor_model_parallel_size: int,
    expert_model_parallel_size: int,
    pipeline_model_parallel_size: int,
    backend: Optional[str] = None,
) -&gt; None:
    &#34;&#34;&#34;Helper to initialize model parallel groups if they are not initialized,
    or ensure tensor-parallel and pipeline-parallel sizes are equal to expected
    values if the model parallel groups are initialized.
    &#34;&#34;&#34;
    backend = backend or torch.distributed.get_backend(get_world_group().device_group)
    if not model_parallel_is_initialized():
        initialize_model_parallel(
            tensor_model_parallel_size,
            expert_model_parallel_size,
            pipeline_model_parallel_size,
            backend,
        )
        return

    assert get_tensor_model_parallel_world_size() == tensor_model_parallel_size, (
        &#34;tensor parallel group already initialized, but of unexpected size: &#34;
        f&#34;{get_tensor_model_parallel_world_size()=} vs. &#34;
        f&#34;{tensor_model_parallel_size=}&#34;
    )
    pp_world_size = get_pp_group().world_size
    assert pp_world_size == pipeline_model_parallel_size, (
        &#34;pipeline parallel group already initialized, but of unexpected size: &#34;
        f&#34;{pp_world_size=} vs. &#34;
        f&#34;{pipeline_model_parallel_size=}&#34;
    )</code></pre>
</details>
<div class="desc"><p>Helper to initialize model parallel groups if they are not initialized,
or ensure tensor-parallel and pipeline-parallel sizes are equal to expected
values if the model parallel groups are initialized.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.get_moe_ep_group"><code class="name flex">
<span>def <span class="ident">get_moe_ep_group</span></span>(<span>) ‑> <a title="sglang.srt.distributed.parallel_state.GroupCoordinator" href="#sglang.srt.distributed.parallel_state.GroupCoordinator">GroupCoordinator</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_moe_ep_group() -&gt; GroupCoordinator:
    assert _MOE_EP is not None, &#34;expert model parallel group is not initialized&#34;
    return _MOE_EP</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.get_moe_expert_parallel_rank"><code class="name flex">
<span>def <span class="ident">get_moe_expert_parallel_rank</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_moe_expert_parallel_rank():
    &#34;&#34;&#34;Return my rank for the moe expert parallel group.&#34;&#34;&#34;
    return get_moe_ep_group().rank_in_group</code></pre>
</details>
<div class="desc"><p>Return my rank for the moe expert parallel group.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.get_moe_expert_parallel_world_size"><code class="name flex">
<span>def <span class="ident">get_moe_expert_parallel_world_size</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_moe_expert_parallel_world_size():
    &#34;&#34;&#34;Return world size for the moe expert parallel group.&#34;&#34;&#34;
    return get_moe_ep_group().world_size</code></pre>
</details>
<div class="desc"><p>Return world size for the moe expert parallel group.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.get_moe_tensor_parallel_rank"><code class="name flex">
<span>def <span class="ident">get_moe_tensor_parallel_rank</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_moe_tensor_parallel_rank():
    &#34;&#34;&#34;Return my rank for the moe tensor parallel group.&#34;&#34;&#34;
    return get_moe_tp_group().rank_in_group</code></pre>
</details>
<div class="desc"><p>Return my rank for the moe tensor parallel group.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.get_moe_tensor_parallel_world_size"><code class="name flex">
<span>def <span class="ident">get_moe_tensor_parallel_world_size</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_moe_tensor_parallel_world_size():
    &#34;&#34;&#34;Return world size for the moe tensor parallel group.&#34;&#34;&#34;
    return get_moe_tp_group().world_size</code></pre>
</details>
<div class="desc"><p>Return world size for the moe tensor parallel group.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.get_moe_tp_group"><code class="name flex">
<span>def <span class="ident">get_moe_tp_group</span></span>(<span>) ‑> <a title="sglang.srt.distributed.parallel_state.GroupCoordinator" href="#sglang.srt.distributed.parallel_state.GroupCoordinator">GroupCoordinator</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_moe_tp_group() -&gt; GroupCoordinator:
    assert _MOE_TP is not None, &#34;expert model parallel group is not initialized&#34;
    return _MOE_TP</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.get_pipeline_model_parallel_group"><code class="name flex">
<span>def <span class="ident">get_pipeline_model_parallel_group</span></span>(<span>) ‑> <a title="sglang.srt.distributed.parallel_state.GroupCoordinator" href="#sglang.srt.distributed.parallel_state.GroupCoordinator">GroupCoordinator</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_pp_group() -&gt; GroupCoordinator:
    assert _PP is not None, &#34;pipeline model parallel group is not initialized&#34;
    return _PP</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.get_pp_group"><code class="name flex">
<span>def <span class="ident">get_pp_group</span></span>(<span>) ‑> <a title="sglang.srt.distributed.parallel_state.GroupCoordinator" href="#sglang.srt.distributed.parallel_state.GroupCoordinator">GroupCoordinator</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_pp_group() -&gt; GroupCoordinator:
    assert _PP is not None, &#34;pipeline model parallel group is not initialized&#34;
    return _PP</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.get_tensor_model_parallel_group"><code class="name flex">
<span>def <span class="ident">get_tensor_model_parallel_group</span></span>(<span>) ‑> <a title="sglang.srt.distributed.parallel_state.GroupCoordinator" href="#sglang.srt.distributed.parallel_state.GroupCoordinator">GroupCoordinator</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_tp_group() -&gt; GroupCoordinator:
    if _ENABLE_PDMUX_P_TP:
        assert (
            _PDMUX_PREFILL_TP_GROUP is not None
        ), &#34;tensor model parallel group for PD-Multiplexing Prefill is not initialized&#34;
        return _PDMUX_PREFILL_TP_GROUP
    assert _TP is not None, &#34;tensor model parallel group is not initialized&#34;
    return _TP</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.get_tensor_model_parallel_rank"><code class="name flex">
<span>def <span class="ident">get_tensor_model_parallel_rank</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_tensor_model_parallel_rank():
    &#34;&#34;&#34;Return my rank for the tensor model parallel group.&#34;&#34;&#34;
    return get_tp_group().rank_in_group</code></pre>
</details>
<div class="desc"><p>Return my rank for the tensor model parallel group.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.get_tensor_model_parallel_world_size"><code class="name flex">
<span>def <span class="ident">get_tensor_model_parallel_world_size</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_tensor_model_parallel_world_size():
    &#34;&#34;&#34;Return world size for the tensor model parallel group.&#34;&#34;&#34;
    return get_tp_group().world_size</code></pre>
</details>
<div class="desc"><p>Return world size for the tensor model parallel group.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.get_tp_group"><code class="name flex">
<span>def <span class="ident">get_tp_group</span></span>(<span>) ‑> <a title="sglang.srt.distributed.parallel_state.GroupCoordinator" href="#sglang.srt.distributed.parallel_state.GroupCoordinator">GroupCoordinator</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_tp_group() -&gt; GroupCoordinator:
    if _ENABLE_PDMUX_P_TP:
        assert (
            _PDMUX_PREFILL_TP_GROUP is not None
        ), &#34;tensor model parallel group for PD-Multiplexing Prefill is not initialized&#34;
        return _PDMUX_PREFILL_TP_GROUP
    assert _TP is not None, &#34;tensor model parallel group is not initialized&#34;
    return _TP</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.get_world_group"><code class="name flex">
<span>def <span class="ident">get_world_group</span></span>(<span>) ‑> <a title="sglang.srt.distributed.parallel_state.GroupCoordinator" href="#sglang.srt.distributed.parallel_state.GroupCoordinator">GroupCoordinator</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_world_group() -&gt; GroupCoordinator:
    assert _WORLD is not None, &#34;world group is not initialized&#34;
    return _WORLD</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.graph_capture"><code class="name flex">
<span>def <span class="ident">graph_capture</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def graph_capture():
    &#34;&#34;&#34;
    `graph_capture` is a context manager which should surround the code that
    is capturing the CUDA graph. Its main purpose is to ensure that the
    some operations will be run after the graph is captured, before the graph
    is replayed. It returns a `GraphCaptureContext` object which contains the
    necessary data for the graph capture. Currently, it only contains the
    stream that the graph capture is running on. This stream is set to the
    current CUDA stream when the context manager is entered and reset to the
    default stream when the context manager is exited. This is to ensure that
    the graph capture is running on a separate stream from the default stream,
    in order to explicitly distinguish the kernels to capture
    from other kernels possibly launched on background in the default stream.
    &#34;&#34;&#34;
    with get_tp_group().graph_capture() as context, get_pp_group().graph_capture(
        context
    ):
        yield context</code></pre>
</details>
<div class="desc"><p><code><a title="sglang.srt.distributed.parallel_state.graph_capture" href="#sglang.srt.distributed.parallel_state.graph_capture">graph_capture()</a></code> is a context manager which should surround the code that
is capturing the CUDA graph. Its main purpose is to ensure that the
some operations will be run after the graph is captured, before the graph
is replayed. It returns a <code><a title="sglang.srt.distributed.parallel_state.GraphCaptureContext" href="#sglang.srt.distributed.parallel_state.GraphCaptureContext">GraphCaptureContext</a></code> object which contains the
necessary data for the graph capture. Currently, it only contains the
stream that the graph capture is running on. This stream is set to the
current CUDA stream when the context manager is entered and reset to the
default stream when the context manager is exited. This is to ensure that
the graph capture is running on a separate stream from the default stream,
in order to explicitly distinguish the kernels to capture
from other kernels possibly launched on background in the default stream.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.in_the_same_node_as"><code class="name flex">
<span>def <span class="ident">in_the_same_node_as</span></span>(<span>pg: torch.distributed.distributed_c10d.ProcessGroup, source_rank: int = 0) ‑> List[bool]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def in_the_same_node_as(pg: ProcessGroup, source_rank: int = 0) -&gt; List[bool]:
    &#34;&#34;&#34;
    This is a collective operation that returns if each rank is in the same node
    as the source rank. It tests if processes are attached to the same
    memory system (shared access to shared memory).
    &#34;&#34;&#34;
    assert (
        torch.distributed.get_backend(pg) != torch.distributed.Backend.NCCL
    ), &#34;in_the_same_node_as should be tested with a non-NCCL group.&#34;
    # local rank inside the group
    rank = torch.distributed.get_rank(group=pg)
    world_size = torch.distributed.get_world_size(group=pg)

    # local tensor in each process to store the result
    is_in_the_same_node = torch.tensor([0] * world_size, dtype=torch.int32)

    # global ranks of the processes in the group
    ranks = torch.distributed.get_process_group_ranks(pg)

    magic_message = b&#34;magic_message&#34;
    shm = None

    try:
        with contextlib.suppress(OSError):
            if rank == source_rank:
                # create a shared memory segment
                shm = shared_memory.SharedMemory(create=True, size=128)
                shm.buf[: len(magic_message)] = magic_message
                torch.distributed.broadcast_object_list(
                    [shm.name], src=ranks[source_rank], group=pg
                )
                is_in_the_same_node[rank] = 1
            else:
                # try to open the shared memory segment
                recv = [None]
                torch.distributed.broadcast_object_list(
                    recv, src=ranks[source_rank], group=pg
                )
                name = recv[0]
                # fix to https://stackoverflow.com/q/62748654/9191338
                # Python incorrectly tracks shared memory even if it is not
                # created by the process. The following patch is a workaround.
                with patch(
                    &#34;multiprocessing.resource_tracker.register&#34;,
                    lambda *args, **kwargs: None,
                ):
                    shm = shared_memory.SharedMemory(name=name)
                if shm.buf[: len(magic_message)] == magic_message:
                    is_in_the_same_node[rank] = 1
    except Exception as e:
        logger.error(&#34;Error ignored in is_in_the_same_node: %s&#34;, e)
    finally:
        if shm:
            shm.close()

    torch.distributed.barrier(group=pg)

    # clean up the shared memory segment
    with contextlib.suppress(OSError):
        if rank == source_rank and shm:
            shm.unlink()
    torch.distributed.all_reduce(is_in_the_same_node, group=pg)

    return [x == 1 for x in is_in_the_same_node.tolist()]</code></pre>
</details>
<div class="desc"><p>This is a collective operation that returns if each rank is in the same node
as the source rank. It tests if processes are attached to the same
memory system (shared access to shared memory).</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.init_distributed_environment"><code class="name flex">
<span>def <span class="ident">init_distributed_environment</span></span>(<span>world_size: int = -1,<br>rank: int = -1,<br>distributed_init_method: str = 'env://',<br>local_rank: int = -1,<br>backend: str = 'nccl',<br>timeout: int | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_distributed_environment(
    world_size: int = -1,
    rank: int = -1,
    distributed_init_method: str = &#34;env://&#34;,
    local_rank: int = -1,
    backend: str = &#34;nccl&#34;,
    timeout: Optional[int] = None,
):
    logger.debug(
        &#34;world_size=%d rank=%d local_rank=%d &#34; &#34;distributed_init_method=%s backend=%s&#34;,
        world_size,
        rank,
        local_rank,
        distributed_init_method,
        backend,
    )
    if not torch.distributed.is_initialized():
        assert distributed_init_method is not None, (
            &#34;distributed_init_method must be provided when initializing &#34;
            &#34;distributed environment&#34;
        )
        if timeout is not None:
            assert isinstance(timeout, (int)), &#34;timeout must be a number&#34;
            assert timeout &gt; 0, &#34;timeout must be positive&#34;
            timeout = timedelta(seconds=timeout)

        # this backend is used for WORLD
        torch.distributed.init_process_group(
            backend=backend,
            init_method=distributed_init_method,
            world_size=world_size,
            rank=rank,
            timeout=timeout,
        )

    # set the local rank
    # local_rank is not available in torch ProcessGroup,
    # see https://github.com/pytorch/pytorch/issues/122816
    if local_rank == -1:
        # local rank not set, this usually happens in single-node
        # setting, where we can use rank as local rank
        if distributed_init_method == &#34;env://&#34;:
            local_rank = int(os.environ.get(&#34;LOCAL_RANK&#34;, &#34;0&#34;))
        else:
            local_rank = rank
    global _WORLD
    if _WORLD is None:
        ranks = list(range(torch.distributed.get_world_size()))
        _WORLD = init_world_group(ranks, local_rank, backend)
    else:
        assert (
            _WORLD.world_size == torch.distributed.get_world_size()
        ), &#34;world group already initialized with a different world size&#34;</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.init_model_parallel_group"><code class="name flex">
<span>def <span class="ident">init_model_parallel_group</span></span>(<span>group_ranks: List[List[int]],<br>local_rank: int,<br>backend: str,<br>use_custom_allreduce: bool | None = None,<br>use_message_queue_broadcaster: bool = False,<br>group_name: str | None = None,<br>use_mscclpp_allreduce: bool | None = None) ‑> <a title="sglang.srt.distributed.parallel_state.GroupCoordinator" href="#sglang.srt.distributed.parallel_state.GroupCoordinator">GroupCoordinator</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_model_parallel_group(
    group_ranks: List[List[int]],
    local_rank: int,
    backend: str,
    use_custom_allreduce: Optional[bool] = None,
    use_message_queue_broadcaster: bool = False,
    group_name: Optional[str] = None,
    use_mscclpp_allreduce: Optional[bool] = None,
) -&gt; GroupCoordinator:
    if use_custom_allreduce is None:
        use_custom_allreduce = _ENABLE_CUSTOM_ALL_REDUCE
    if use_mscclpp_allreduce is None:
        use_mscclpp_allreduce = _ENABLE_MSCCLPP_ALL_REDUCE
    return GroupCoordinator(
        group_ranks=group_ranks,
        local_rank=local_rank,
        torch_distributed_backend=backend,
        use_pynccl=not _is_npu,
        use_pymscclpp=use_mscclpp_allreduce,
        use_custom_allreduce=use_custom_allreduce,
        use_hpu_communicator=True,
        use_xpu_communicator=True,
        use_npu_communicator=True,
        use_message_queue_broadcaster=use_message_queue_broadcaster,
        group_name=group_name,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.init_world_group"><code class="name flex">
<span>def <span class="ident">init_world_group</span></span>(<span>ranks: List[int], local_rank: int, backend: str) ‑> <a title="sglang.srt.distributed.parallel_state.GroupCoordinator" href="#sglang.srt.distributed.parallel_state.GroupCoordinator">GroupCoordinator</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_world_group(
    ranks: List[int], local_rank: int, backend: str
) -&gt; GroupCoordinator:
    return GroupCoordinator(
        group_ranks=[ranks],
        local_rank=local_rank,
        torch_distributed_backend=backend,
        use_pynccl=False,
        use_pymscclpp=False,
        use_custom_allreduce=False,
        use_hpu_communicator=False,
        use_xpu_communicator=False,
        use_npu_communicator=False,
        group_name=&#34;world&#34;,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.initialize_model_parallel"><code class="name flex">
<span>def <span class="ident">initialize_model_parallel</span></span>(<span>tensor_model_parallel_size: int = 1,<br>expert_model_parallel_size: int = 1,<br>pipeline_model_parallel_size: int = 1,<br>backend: str | None = None,<br>duplicate_tp_group: bool = False) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize_model_parallel(
    tensor_model_parallel_size: int = 1,
    expert_model_parallel_size: int = 1,
    pipeline_model_parallel_size: int = 1,
    backend: Optional[str] = None,
    duplicate_tp_group: bool = False,
) -&gt; None:
    &#34;&#34;&#34;
    Initialize model parallel groups.

    Arguments:
        tensor_model_parallel_size: number of GPUs used for tensor model
            parallelism.
        pipeline_model_parallel_size: number of GPUs used for pipeline model
            parallelism.

    Let&#39;s say we have a total of 8 GPUs denoted by g0 ... g7 and we
    use 2 GPUs to parallelize the model tensor, and 4 GPUs to parallelize
    the model pipeline. The present function will
    create 4 tensor model-parallel groups and 2 pipeline model-parallel groups:
        4 tensor model-parallel groups:
            [g0, g1], [g2, g3], [g4, g5], [g6, g7]
        2 pipeline model-parallel groups:
            [g0, g2, g4, g6], [g1, g3, g5, g7]
    Note that for efficiency, the caller should make sure adjacent ranks
    are on the same DGX box. For example if we are using 2 DGX-1 boxes
    with a total of 16 GPUs, rank 0 to 7 belong to the first box and
    ranks 8 to 15 belong to the second box.
    &#34;&#34;&#34;
    # Get world size and rank. Ensure some consistencies.
    assert torch.distributed.is_initialized()
    world_size: int = torch.distributed.get_world_size()
    backend = backend or torch.distributed.get_backend(get_world_group().device_group)

    if world_size != tensor_model_parallel_size * pipeline_model_parallel_size:
        raise RuntimeError(
            f&#34;world_size ({world_size}) is not equal to &#34;
            f&#34;tensor_model_parallel_size ({tensor_model_parallel_size}) x &#34;
            f&#34;pipeline_model_parallel_size ({pipeline_model_parallel_size})&#34;
        )

    # Build the tensor model-parallel groups.
    num_tensor_model_parallel_groups: int = world_size // tensor_model_parallel_size
    global _TP
    assert _TP is None, &#34;tensor model parallel group is already initialized&#34;
    group_ranks = []
    for i in range(num_tensor_model_parallel_groups):
        ranks = list(
            range(i * tensor_model_parallel_size, (i + 1) * tensor_model_parallel_size)
        )
        group_ranks.append(ranks)

    # message queue broadcaster is only used in tensor model parallel group
    _TP = init_model_parallel_group(
        group_ranks,
        get_world_group().local_rank,
        backend,
        use_message_queue_broadcaster=get_bool_env_var(
            &#34;SGLANG_USE_MESSAGE_QUEUE_BROADCASTER&#34;, &#34;true&#34;
        ),
        group_name=&#34;tp&#34;,
    )

    if duplicate_tp_group:
        global _PDMUX_PREFILL_TP_GROUP
        assert (
            _PDMUX_PREFILL_TP_GROUP is None
        ), &#34;tensor model parallel group for PD-Multiplexing Prefill is already initialized&#34;
        _PDMUX_PREFILL_TP_GROUP = init_model_parallel_group(
            group_ranks,
            get_world_group().local_rank,
            backend,
            use_message_queue_broadcaster=get_bool_env_var(
                &#34;SGLANG_USE_MESSAGE_QUEUE_BROADCASTER&#34;, &#34;true&#34;
            ),
            group_name=&#34;pdmux_prefill_tp&#34;,
        )
        _TP.pynccl_comm.disabled = False
        _PDMUX_PREFILL_TP_GROUP.pynccl_comm.disabled = False

    moe_ep_size = expert_model_parallel_size

    moe_tp_size = tensor_model_parallel_size // moe_ep_size
    global _MOE_EP
    assert _MOE_EP is None, &#34;expert model parallel group is already initialized&#34;
    group_ranks = []
    for i in range(num_tensor_model_parallel_groups):
        for j in range(moe_tp_size):
            st = i * tensor_model_parallel_size + j
            en = (i + 1) * tensor_model_parallel_size + j
            ranks = list(range(st, en, moe_tp_size))
            group_ranks.append(ranks)

    _MOE_EP = init_model_parallel_group(
        group_ranks,
        get_world_group().local_rank,
        backend,
        use_custom_allreduce=False,
        group_name=&#34;moe_ep&#34;,
    )

    global _MOE_TP
    assert _MOE_TP is None, &#34;expert model parallel group is already initialized&#34;
    group_ranks = []
    for i in range(num_tensor_model_parallel_groups):
        for j in range(moe_ep_size):
            st = i * tensor_model_parallel_size + j * moe_tp_size
            en = i * tensor_model_parallel_size + (j + 1) * moe_tp_size
            ranks = list(range(st, en))
            group_ranks.append(ranks)

    _MOE_TP = init_model_parallel_group(
        group_ranks,
        get_world_group().local_rank,
        backend,
        use_custom_allreduce=False,
        group_name=&#34;moe_tp&#34;,
    )

    # Build the pipeline model-parallel groups.
    num_pipeline_model_parallel_groups: int = world_size // pipeline_model_parallel_size
    global _PP
    assert _PP is None, &#34;pipeline model parallel group is already initialized&#34;
    group_ranks = []
    for i in range(num_pipeline_model_parallel_groups):
        ranks = list(range(i, world_size, num_pipeline_model_parallel_groups))
        group_ranks.append(ranks)
    # pipeline parallel does not need custom allreduce
    _PP = init_model_parallel_group(
        group_ranks,
        get_world_group().local_rank,
        backend,
        use_custom_allreduce=False,
        group_name=&#34;pp&#34;,
    )</code></pre>
</details>
<div class="desc"><p>Initialize model parallel groups.</p>
<h2 id="arguments">Arguments</h2>
<p>tensor_model_parallel_size: number of GPUs used for tensor model
parallelism.
pipeline_model_parallel_size: number of GPUs used for pipeline model
parallelism.</p>
<p>Let's say we have a total of 8 GPUs denoted by g0 &hellip; g7 and we
use 2 GPUs to parallelize the model tensor, and 4 GPUs to parallelize
the model pipeline. The present function will
create 4 tensor model-parallel groups and 2 pipeline model-parallel groups:
4 tensor model-parallel groups:
[g0, g1], [g2, g3], [g4, g5], [g6, g7]
2 pipeline model-parallel groups:
[g0, g2, g4, g6], [g1, g3, g5, g7]
Note that for efficiency, the caller should make sure adjacent ranks
are on the same DGX box. For example if we are using 2 DGX-1 boxes
with a total of 16 GPUs, rank 0 to 7 belong to the first box and
ranks 8 to 15 belong to the second box.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.inplace_all_reduce"><code class="name flex">
<span>def <span class="ident">inplace_all_reduce</span></span>(<span>tensor: torch.Tensor, group_name: str) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inplace_all_reduce(tensor: torch.Tensor, group_name: str) -&gt; None:
    assert group_name in _groups, f&#34;Group {group_name} is not found.&#34;
    group = _groups[group_name]()
    if group is None:
        raise ValueError(f&#34;Group {group_name} is destroyed.&#34;)
    group._all_reduce_in_place(tensor)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.inplace_all_reduce_fake"><code class="name flex">
<span>def <span class="ident">inplace_all_reduce_fake</span></span>(<span>tensor: torch.Tensor, group_name: str) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inplace_all_reduce_fake(tensor: torch.Tensor, group_name: str) -&gt; None:
    return</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.model_parallel_is_initialized"><code class="name flex">
<span>def <span class="ident">model_parallel_is_initialized</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model_parallel_is_initialized():
    &#34;&#34;&#34;Check if tensor and pipeline parallel groups are initialized.&#34;&#34;&#34;
    return _TP is not None and _PP is not None</code></pre>
</details>
<div class="desc"><p>Check if tensor and pipeline parallel groups are initialized.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.monkey_patch_vllm_parallel_state"><code class="name flex">
<span>def <span class="ident">monkey_patch_vllm_parallel_state</span></span>(<span>reverse: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def monkey_patch_vllm_parallel_state(reverse: bool = False):
    try:
        import vllm.distributed.parallel_state as vllm_parrlel_state
    except ImportError:
        return

    global vllm_get_pp_group, vllm_get_tp_group, vllm_get_world_group
    if vllm_get_pp_group is None:
        vllm_get_pp_group = vllm_parrlel_state.get_pp_group
        vllm_get_tp_group = vllm_parrlel_state.get_tp_group
        vllm_get_world_group = vllm_parrlel_state.get_world_group
    if reverse:
        setattr(vllm_parrlel_state, &#34;get_pp_group&#34;, vllm_get_pp_group)
        setattr(vllm_parrlel_state, &#34;get_tp_group&#34;, vllm_get_tp_group)
        setattr(vllm_parrlel_state, &#34;get_world_group&#34;, vllm_get_world_group)
    else:
        setattr(vllm_parrlel_state, &#34;get_pp_group&#34;, get_pp_group)
        setattr(vllm_parrlel_state, &#34;get_tp_group&#34;, get_tp_group)
        setattr(vllm_parrlel_state, &#34;get_world_group&#34;, get_world_group)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.outplace_all_reduce"><code class="name flex">
<span>def <span class="ident">outplace_all_reduce</span></span>(<span>tensor: torch.Tensor, group_name: str, outplace_all_reduce_method: str) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def outplace_all_reduce(
    tensor: torch.Tensor, group_name: str, outplace_all_reduce_method: str
) -&gt; torch.Tensor:
    assert group_name in _groups, f&#34;Group {group_name} is not found.&#34;
    group = _groups[group_name]()
    if group is None:
        raise ValueError(f&#34;Group {group_name} is destroyed.&#34;)
    return group._all_reduce_out_place(tensor, outplace_all_reduce_method)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.outplace_all_reduce_fake"><code class="name flex">
<span>def <span class="ident">outplace_all_reduce_fake</span></span>(<span>tensor: torch.Tensor, group_name: str, outplace_all_reduce_method: str) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def outplace_all_reduce_fake(
    tensor: torch.Tensor, group_name: str, outplace_all_reduce_method: str
) -&gt; torch.Tensor:
    return torch.empty_like(tensor)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.patch_tensor_parallel_group"><code class="name flex">
<span>def <span class="ident">patch_tensor_parallel_group</span></span>(<span>tp_group: <a title="sglang.srt.distributed.parallel_state.GroupCoordinator" href="#sglang.srt.distributed.parallel_state.GroupCoordinator">GroupCoordinator</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def patch_tensor_parallel_group(tp_group: GroupCoordinator):
    &#34;&#34;&#34;Patch the tp group temporarily until this function ends.

    This method is for draft workers of speculative decoding to run draft model
    with different tp degree from that of target model workers.

    Args:
        tp_group (GroupCoordinator): the tp group coordinator
    &#34;&#34;&#34;
    global _TP_STATE_PATCHED
    assert not _TP_STATE_PATCHED, &#34;Should not call when it&#39;s already patched&#34;

    _TP_STATE_PATCHED = True
    old_tp_group = get_tp_group()
    global _TP
    _TP = tp_group
    try:
        yield
    finally:
        # restore the original state
        _TP_STATE_PATCHED = False
        _TP = old_tp_group</code></pre>
</details>
<div class="desc"><p>Patch the tp group temporarily until this function ends.</p>
<p>This method is for draft workers of speculative decoding to run draft model
with different tp degree from that of target model workers.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tp_group</code></strong> :&ensp;<code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator" href="#sglang.srt.distributed.parallel_state.GroupCoordinator">GroupCoordinator</a></code></dt>
<dd>the tp group coordinator</dd>
</dl></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.reg_all_gather_into_tensor"><code class="name flex">
<span>def <span class="ident">reg_all_gather_into_tensor</span></span>(<span>output: torch.Tensor, input: torch.Tensor, group_name: str) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reg_all_gather_into_tensor(
    output: torch.Tensor, input: torch.Tensor, group_name: str
) -&gt; None:
    assert group_name in _groups, f&#34;Group {group_name} is not found.&#34;
    group = _groups[group_name]()
    if group is None:
        raise ValueError(f&#34;Group {group_name} is destroyed.&#34;)
    group._all_gather_into_tensor(output, input)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.reg_all_gather_into_tensor_fake"><code class="name flex">
<span>def <span class="ident">reg_all_gather_into_tensor_fake</span></span>(<span>output: torch.Tensor, input: torch.Tensor, group_name: str) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reg_all_gather_into_tensor_fake(
    output: torch.Tensor, input: torch.Tensor, group_name: str
) -&gt; None:
    pass</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.set_custom_all_reduce"><code class="name flex">
<span>def <span class="ident">set_custom_all_reduce</span></span>(<span>enable: bool)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_custom_all_reduce(enable: bool):
    global _ENABLE_CUSTOM_ALL_REDUCE
    _ENABLE_CUSTOM_ALL_REDUCE = enable</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.set_mscclpp_all_reduce"><code class="name flex">
<span>def <span class="ident">set_mscclpp_all_reduce</span></span>(<span>enable: bool)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_mscclpp_all_reduce(enable: bool):
    global _ENABLE_MSCCLPP_ALL_REDUCE
    _ENABLE_MSCCLPP_ALL_REDUCE = enable</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.set_pdmux_status"><code class="name flex">
<span>def <span class="ident">set_pdmux_status</span></span>(<span>enable_prefill_multiplexing: bool)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_pdmux_status(enable_prefill_multiplexing: bool):
    global _ENABLE_PDMUX_P_TP
    _ENABLE_PDMUX_P_TP = enable_prefill_multiplexing</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.distributed.parallel_state.GraphCaptureContext"><code class="flex name class">
<span>class <span class="ident">GraphCaptureContext</span></span>
<span>(</span><span>stream: torch.cuda.streams.Stream)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class GraphCaptureContext:
    stream: torch.cuda.Stream if not _is_npu else torch.npu.Stream</code></pre>
</details>
<div class="desc"><p>GraphCaptureContext(stream: torch.cuda.streams.Stream)</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.distributed.parallel_state.GraphCaptureContext.stream"><code class="name">var <span class="ident">stream</span> : torch.cuda.streams.Stream</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator"><code class="flex name class">
<span>class <span class="ident">GroupCoordinator</span></span>
<span>(</span><span>group_ranks: List[List[int]],<br>local_rank: int,<br>torch_distributed_backend: str | torch.distributed.distributed_c10d.Backend,<br>use_pynccl: bool,<br>use_pymscclpp: bool,<br>use_custom_allreduce: bool,<br>use_hpu_communicator: bool,<br>use_xpu_communicator: bool,<br>use_npu_communicator: bool,<br>use_message_queue_broadcaster: bool = False,<br>group_name: str | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GroupCoordinator:
    &#34;&#34;&#34;
    PyTorch ProcessGroup wrapper for a group of processes.
    PyTorch ProcessGroup is bound to one specific communication backend,
        e.g. NCCL, Gloo, MPI, etc.
    GroupCoordinator takes charge of all the communication operations among
        the processes in the group. It can route the communication to
        a specific implementation (e.g. switch allreduce implementation
        based on the tensor size and cuda graph mode).
    &#34;&#34;&#34;

    # available attributes:
    rank: int  # global rank
    ranks: List[int]  # global ranks in the group
    world_size: int  # size of the group
    # difference between `local_rank` and `rank_in_group`:
    # if we have a group of size 4 across two nodes:
    # Process | Node | Rank | Local Rank | Rank in Group
    #   0     |   0  |  0   |     0      |       0
    #   1     |   0  |  1   |     1      |       1
    #   2     |   1  |  2   |     0      |       2
    #   3     |   1  |  3   |     1      |       3
    local_rank: int  # local rank used to assign devices
    rank_in_group: int  # rank inside the group
    cpu_group: ProcessGroup  # group for CPU communication
    device_group: ProcessGroup  # group for device communication
    use_pynccl: bool  # a hint of whether to use PyNccl
    use_pymscclpp: bool  # a hint of whether to use PyMsccl
    use_custom_allreduce: bool  # a hint of whether to use CustomAllreduce
    use_message_queue_broadcaster: (
        bool  # a hint of whether to use message queue broadcaster
    )
    # communicators are only created for world size &gt; 1
    pynccl_comm: Optional[Any]  # PyNccl communicator
    ca_comm: Optional[Any]  # Custom allreduce communicator
    mq_broadcaster: Optional[Any]  # shared memory broadcaster

    def __init__(
        self,
        group_ranks: List[List[int]],
        local_rank: int,
        torch_distributed_backend: Union[str, Backend],
        use_pynccl: bool,
        use_pymscclpp: bool,
        use_custom_allreduce: bool,
        use_hpu_communicator: bool,
        use_xpu_communicator: bool,
        use_npu_communicator: bool,
        use_message_queue_broadcaster: bool = False,
        group_name: Optional[str] = None,
    ):
        # Set group info
        group_name = group_name or &#34;anonymous&#34;
        self.unique_name = _get_unique_name(group_name)
        _register_group(self)

        # Set rank info
        self.rank = torch.distributed.get_rank()
        self.local_rank = local_rank
        self.device_group = None
        self.cpu_group = None
        self.local_size = get_int_env_var(&#34;LOCAL_SIZE&#34;, 0)

        for ranks in group_ranks:
            device_group = torch.distributed.new_group(
                ranks, backend=torch_distributed_backend
            )
            # a group with `gloo` backend, to allow direct coordination between
            # processes through the CPU.
            cpu_group = torch.distributed.new_group(ranks, backend=&#34;gloo&#34;)
            if self.rank in ranks:
                self.ranks = ranks
                self.world_size = len(ranks)
                self.rank_in_group = ranks.index(self.rank)
                self.device_group = device_group
                self.cpu_group = cpu_group

        assert self.cpu_group is not None
        assert self.device_group is not None

        device_id = 0 if IS_ONE_DEVICE_PER_PROCESS else local_rank
        if is_cuda_alike():
            self.device = torch.device(f&#34;cuda:{device_id}&#34;)
        elif _is_npu:
            self.device = torch.device(f&#34;npu:{device_id}&#34;)
        else:
            self.device = torch.device(&#34;cpu&#34;)
        self.device_module = torch.get_device_module(self.device)

        # Import communicators
        self.use_pynccl = use_pynccl
        self.use_pymscclpp = use_pymscclpp
        self.use_custom_allreduce = use_custom_allreduce
        self.use_hpu_communicator = use_hpu_communicator
        self.use_xpu_communicator = use_xpu_communicator
        self.use_npu_communicator = use_npu_communicator
        self.use_message_queue_broadcaster = use_message_queue_broadcaster

        # lazy import to avoid documentation build error
        from sglang.srt.distributed.device_communicators.custom_all_reduce import (
            CustomAllreduce,
        )
        from sglang.srt.distributed.device_communicators.pymscclpp import (
            PyMscclppCommunicator,
        )
        from sglang.srt.distributed.device_communicators.pynccl import (
            PyNcclCommunicator,
        )

        if is_hip():
            from sglang.srt.distributed.device_communicators.quick_all_reduce import (
                QuickAllReduce,
                qr_rocm_arch_available,
            )

        self.pynccl_comm: Optional[PyNcclCommunicator] = None
        if use_pynccl and self.world_size &gt; 1:
            self.pynccl_comm = PyNcclCommunicator(
                group=self.cpu_group,
                device=self.device,
            )

        self.pymscclpp_comm: Optional[PyMscclppCommunicator] = None
        if use_pymscclpp and self.world_size &gt; 1:
            self.pymscclpp_comm = PyMscclppCommunicator(
                group=self.cpu_group,
                device=self.device,
            )

        self.ca_comm: Optional[CustomAllreduce] = None
        self.qr_comm: Optional[QuickAllReduce] = None
        if use_custom_allreduce and self.world_size &gt; 1:
            # Initialize a custom fast all-reduce implementation.
            try:
                self.ca_comm = CustomAllreduce(
                    group=self.cpu_group,
                    device=self.device,
                )
            except Exception as e:
                logger.warning(
                    f&#34;Setup Custom allreduce failed with {e}. To silence this &#34;
                    &#34;warning, specify --disable-custom-all-reduce explicitly.&#34;
                )
            if is_hip():
                try:
                    # Initialize a custom quick all-reduce implementation for AMD
                    # when rocm &gt;= gfx942. Quick reduce is designed as a
                    # complement to custom allreduce.
                    # Based on quickreduce (https://github.com/mk1-project/quickreduce).
                    if qr_rocm_arch_available():
                        self.qr_comm = QuickAllReduce(
                            group=self.cpu_group, device=self.device
                        )
                except Exception as e:
                    logger.warning(f&#34;Failed to initialize QuickAllReduce: {e}&#34;)

        # Create communicator for other hardware backends
        from sglang.srt.distributed.device_communicators.hpu_communicator import (
            HpuCommunicator,
        )
        from sglang.srt.distributed.device_communicators.npu_communicator import (
            NpuCommunicator,
        )
        from sglang.srt.distributed.device_communicators.xpu_communicator import (
            XpuCommunicator,
        )

        self.hpu_communicator: Optional[HpuCommunicator] = None
        if use_hpu_communicator and self.world_size &gt; 1:
            self.hpu_communicator = HpuCommunicator(group=self.device_group)

        self.xpu_communicator: Optional[XpuCommunicator] = None
        if use_xpu_communicator and self.world_size &gt; 1:
            self.xpu_communicator = XpuCommunicator(group=self.device_group)

        self.npu_communicator: Optional[NpuCommunicator] = None
        if use_npu_communicator and self.world_size &gt; 1:
            self.npu_communicator = NpuCommunicator(group=self.device_group)

        # Create message queue
        from sglang.srt.distributed.device_communicators.shm_broadcast import (
            MessageQueue,
        )

        self.mq_broadcaster: Optional[MessageQueue] = None
        if use_message_queue_broadcaster and self.world_size &gt; 1:
            self.mq_broadcaster = MessageQueue.create_from_process_group(
                self.cpu_group, 1 &lt;&lt; 22, 6
            )

    def __repr__(self):
        return (
            f&#34;ranks={self.ranks} rank={self.rank} local_rank={self.local_rank} use_pynccl={self.use_pynccl} &#34;
            f&#34;device_group={self.device_group} cpu_group={self.cpu_group} unique_name={self.unique_name} &#34;
            f&#34;world_size={self.world_size} rank_in_group={self.rank_in_group}&#34;
        )

    @property
    def first_rank(self):
        &#34;&#34;&#34;Return the global rank of the first process in the group&#34;&#34;&#34;
        return self.ranks[0]

    @property
    def last_rank(self):
        &#34;&#34;&#34;Return the global rank of the last process in the group&#34;&#34;&#34;
        return self.ranks[-1]

    @property
    def is_first_rank(self):
        &#34;&#34;&#34;Return whether the caller is the first process in the group&#34;&#34;&#34;
        return self.rank == self.first_rank

    @property
    def is_last_rank(self):
        &#34;&#34;&#34;Return whether the caller is the last process in the group&#34;&#34;&#34;
        return self.rank == self.last_rank

    @property
    def next_rank(self):
        &#34;&#34;&#34;Return the global rank of the process that follows the caller&#34;&#34;&#34;
        rank_in_group = self.rank_in_group
        world_size = self.world_size
        return self.ranks[(rank_in_group + 1) % world_size]

    @property
    def prev_rank(self):
        &#34;&#34;&#34;Return the global rank of the process that precedes the caller&#34;&#34;&#34;
        rank_in_group = self.rank_in_group
        world_size = self.world_size
        return self.ranks[(rank_in_group - 1) % world_size]

    @contextmanager
    def graph_capture(
        self, graph_capture_context: Optional[GraphCaptureContext] = None
    ):
        if graph_capture_context is None:
            stream = self.device_module.Stream()
            graph_capture_context = GraphCaptureContext(stream)
        else:
            stream = graph_capture_context.stream
        # We don&#39;t need the context of custom quick allreduce because the ipc access
        # is already collected in init() and we can capture the quick allreduce directly.
        ca_comm = self.ca_comm
        maybe_ca_context = nullcontext() if ca_comm is None else ca_comm.capture()

        # ensure all initialization operations complete before attempting to
        # capture the graph on another stream
        curr_stream = self.device_module.current_stream()
        if curr_stream != stream:
            stream.wait_stream(curr_stream)

        with self.device_module.stream(stream), maybe_ca_context:
            # In graph mode, we have to be very careful about the collective
            # operations. The current status is:
            #     allreduce \ Mode   |  Eager  |  Graph  |
            # --------------------------------------------
            # quick allreduce        | enabled | enabled |
            # custom allreduce       | enabled | enabled |
            # PyNccl                 | disabled| enabled |
            # PyMscclpp              | disabled| enabled |
            # torch.distributed      | enabled | disabled|
            #
            # Note: When custom quick allreduce is enabled, a runtime check
            #  will be performed. If the tensor size is too small, it will
            #  automatically fall back to the next available option.
            # Note that custom allreduce will have a runtime check, if the
            #  tensor size is too large, it will fallback to the next
            #  available option.
            # Note that the PyMsccl needs to register the tensor in ahead,
            #  which will introduce large overhead in the eager case,
            #  therefore it is only supported in the graph case.
            # In summary: We select the appropriate allreduce method for
            #  each mode based on the algorithm order in the table and
            #  their usage conditions.
            pynccl_comm = self.pynccl_comm
            maybe_pynccl_context: Any
            if not pynccl_comm:
                maybe_pynccl_context = nullcontext()
            else:
                maybe_pynccl_context = pynccl_comm.change_state(
                    enable=True, stream=torch.cuda.current_stream()
                )

            pymscclpp_comm = self.pymscclpp_comm
            maybe_pymscclpp_context: Any
            if not pymscclpp_comm:
                maybe_pymscclpp_context = nullcontext()
            else:
                maybe_pymscclpp_context = pymscclpp_comm.change_state(enable=True)
            with maybe_pynccl_context, maybe_pymscclpp_context:
                yield graph_capture_context

    def all_reduce(self, input_: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        User-facing all-reduce function before we actually call the
        all-reduce operation.

        We need this because Dynamo does not support passing an arbitrary
        object (`self` in this case) to a custom op. We need to pass the
         group name as a string, and then look up the group coordinator from
         the group name, dispatch the all-reduce operation to the group
         coordinator.

        In addition, PyTorch custom ops do not support mutation or returning
        a new tensor in the same op. So we need to figure out if the op is
        in-place or out-of-place ahead of time.
        &#34;&#34;&#34;
        # Bypass the function if we are using only 1 GPU.
        if self.world_size == 1:
            return input_

        if input_.is_cpu:
            if is_shm_available(input_.dtype, self.world_size, self.local_size):
                torch.ops.sgl_kernel.shm_allreduce(
                    input_, torch.distributed.ReduceOp.SUM
                )
            else:
                torch.distributed.all_reduce(input_, group=self.device_group)
            return input_

        if not supports_custom_op():
            self._all_reduce_in_place(input_)
            return input_

        if self.hpu_communicator is not None and not self.hpu_communicator.disabled:
            return self.hpu_communicator.all_reduce(input_)

        if self.xpu_communicator is not None and not self.xpu_communicator.disabled:
            return self.xpu_communicator.all_reduce(input_)

        if self.npu_communicator is not None and not self.npu_communicator.disabled:
            return self.npu_communicator.all_reduce(input_)

        if (
            self.pynccl_comm is not None
            and hasattr(input_, &#34;symmetric_memory&#34;)
            and input_.symmetric_memory
        ):
            with self.pynccl_comm.change_state(
                enable=True, stream=torch.cuda.current_stream()
            ):
                self.pynccl_comm.all_reduce(input_)
                return input_

        outplace_all_reduce_method = None
        if (
            self.qr_comm is not None
            and not self.qr_comm.disabled
            and self.qr_comm.should_quick_allreduce(input_)
        ):
            outplace_all_reduce_method = &#34;qr&#34;
        elif (
            self.ca_comm is not None
            and not self.ca_comm.disabled
            and self.ca_comm.should_custom_ar(input_)
        ):
            outplace_all_reduce_method = &#34;ca&#34;
        elif (
            self.pymscclpp_comm is not None
            and not self.pymscclpp_comm.disabled
            and self.pymscclpp_comm.should_mscclpp_allreduce(input_)
        ):
            outplace_all_reduce_method = &#34;pymscclpp&#34;
        if outplace_all_reduce_method is not None:
            return torch.ops.sglang.outplace_all_reduce(
                input_,
                group_name=self.unique_name,
                outplace_all_reduce_method=outplace_all_reduce_method,
            )
        else:
            torch.ops.sglang.inplace_all_reduce(input_, group_name=self.unique_name)
            return input_

    def _all_reduce_out_place(
        self, input_: torch.Tensor, outplace_all_reduce_method: str
    ) -&gt; torch.Tensor:
        qr_comm = self.qr_comm
        ca_comm = self.ca_comm
        pymscclpp_comm = self.pymscclpp_comm
        assert any([qr_comm, ca_comm, pymscclpp_comm])
        if outplace_all_reduce_method == &#34;qr&#34;:
            assert not qr_comm.disabled
            out = qr_comm.quick_all_reduce(input_)
        elif outplace_all_reduce_method == &#34;ca&#34;:
            assert not ca_comm.disabled
            out = ca_comm.custom_all_reduce(input_)
        else:
            assert not pymscclpp_comm.disabled
            out = pymscclpp_comm.all_reduce(input_)
        assert out is not None
        return out

    def _all_reduce_in_place(self, input_: torch.Tensor) -&gt; None:
        pynccl_comm = self.pynccl_comm
        if pynccl_comm is not None and not pynccl_comm.disabled:
            pynccl_comm.all_reduce(input_)
        else:
            torch.distributed.all_reduce(input_, group=self.device_group)

    def reduce_scatter_tensor(
        self,
        output: torch.Tensor,
        input: torch.Tensor,
    ) -&gt; None:
        # TODO(ch-wan): support other backends
        torch.distributed.reduce_scatter_tensor(output, input, group=self.device_group)
        return output

    def reduce_scatter(
        self,
        output: torch.Tensor,
        input_list: List[torch.Tensor],
    ) -&gt; None:
        # TODO(ch-wan): support other backends
        torch.distributed.reduce_scatter(output, input_list, group=self.device_group)
        return output

    def reduce_scatterv(
        self,
        input_: torch.Tensor,
        output: Optional[torch.Tensor] = None,
        sizes: Optional[List[int]] = None,
    ) -&gt; torch.Tensor:
        world_size = self.world_size
        pynccl_comm = self.pynccl_comm

        with pynccl_comm.change_state(enable=True, stream=torch.cuda.current_stream()):
            assert (
                pynccl_comm is not None and not pynccl_comm.disabled
            ), &#34;pynccl is required for reduce_scatterv&#34;

            if sizes is not None:
                assert len(sizes) == world_size
                assert input_.shape[0] == sum(sizes)
                chunk_size = sizes[self.rank_in_group]
            else:
                assert input_.shape[0] % world_size == 0
                chunk_size = input_.shape[0] // world_size
            output_shape = (chunk_size,) + input_.shape[1:]

            if output is None:
                output = torch.empty(
                    output_shape, dtype=input_.dtype, device=input_.device
                )
            else:
                assert output.shape == output_shape

            pynccl_comm.reduce_scatter(output, input_, sizes=sizes)
            return output

    def _all_gather_into_tensor(self, output: torch.Tensor, input: torch.Tensor):
        pynccl_comm = self.pynccl_comm
        if pynccl_comm is not None and not pynccl_comm.disabled:
            pynccl_comm.all_gather(output, input)
        else:
            torch.distributed.all_gather_into_tensor(
                output, input, group=self.device_group
            )

    def all_gather_into_tensor(self, output: torch.Tensor, input: torch.Tensor):
        if _is_npu or not supports_custom_op():
            self._all_gather_into_tensor(output, input)
        else:
            torch.ops.sglang.reg_all_gather_into_tensor(
                output, input, group_name=self.unique_name
            )

    def all_gather(
        self,
        input_: torch.Tensor,
        dim: int = -1,
        output_tensor_list: Optional[List[torch.Tensor]] = None,
    ) -&gt; torch.Tensor:
        world_size = self.world_size
        # Bypass the function if we are using only 1 GPU.
        if world_size == 1:
            if output_tensor_list is not None:
                logger.warning(
                    &#34;Performing in-place all-gather with a group size of 1. &#34;
                    &#34;This may be unnecessary; consider bypassing it for better efficiency.&#34;
                )
                output_tensor_list[0].copy_(input_)
                return None
            else:
                return input_

        if output_tensor_list is not None:
            # TODO(ch-wan): support other backends
            return torch.distributed.all_gather(
                output_tensor_list, input_, group=self.device_group
            )

        assert (
            -input_.dim() &lt;= dim &lt; input_.dim()
        ), f&#34;Invalid dim ({dim}) for input tensor with shape {input_.size()}&#34;

        # For HPUs, use HPU communicator.
        hpu_comm = self.hpu_communicator
        if hpu_comm is not None and not hpu_comm.disabled:
            return hpu_comm.all_gather(input_, dim)

        # For NPUs, use NPU communicator.
        npu_comm = self.npu_communicator
        if npu_comm is not None and not npu_comm.disabled:
            return npu_comm.all_gather(input_, dim)

        if dim &lt; 0:
            # Convert negative dim to positive.
            dim += input_.dim()
        input_size = input_.size()
        # NOTE: we have to use concat-style all-gather here,
        # stack-style all-gather has compatibility issues with
        # torch.compile . see https://github.com/pytorch/pytorch/issues/138795
        output_size = (input_size[0] * world_size,) + input_size[1:]
        # Allocate output tensor.
        output_tensor = torch.empty(
            output_size, dtype=input_.dtype, device=input_.device
        )

        # All-gather.
        if input_.is_cpu and is_shm_available(
            input_.dtype, self.world_size, self.local_size
        ):
            return torch.ops.sgl_kernel.shm_allgather(input_, dim)

        if input_.is_cpu:
            torch.distributed.all_gather_into_tensor(
                output_tensor, input_, group=self.device_group
            )
        else:
            self.all_gather_into_tensor(output_tensor, input_)

        # Reshape
        output_tensor = output_tensor.reshape((world_size,) + input_size)
        output_tensor = output_tensor.movedim(0, dim)
        output_tensor = output_tensor.reshape(
            input_size[:dim] + (world_size * input_size[dim],) + input_size[dim + 1 :]
        )
        return output_tensor

    def all_gatherv(
        self,
        input_: Union[torch.Tensor, List[torch.Tensor]],
        sizes: Optional[List[int]] = None,
    ) -&gt; Union[torch.Tensor, List[torch.Tensor]]:
        &#34;&#34;&#34;
        Supports varying sizes per rank and input tensor list.
        `sizes`: a list of len(world_size) with the number of items per rank to gather.
        &#34;&#34;&#34;
        world_size = self.world_size
        pynccl_comm = self.pynccl_comm

        with pynccl_comm.change_state(enable=True, stream=torch.cuda.current_stream()):
            assert (
                pynccl_comm is not None and not pynccl_comm.disabled
            ), &#34;pynccl is required for all_gatherv&#34;

            def _all_gather_single(
                input_: torch.Tensor, sizes: Optional[List[int]] = None
            ):
                input_size = input_.size()
                if sizes is not None:
                    assert len(sizes) == world_size
                    assert input_.shape[0] == sizes[self.rank_in_group]
                    output_size = (sum(sizes),) + input_size[1:]
                    # &#39;sizes&#39; is not needed if all inputs in the same group have the same shape
                    if all(s == sizes[0] for s in sizes):
                        sizes = None
                else:
                    output_size = (input_size[0] * world_size,) + input_size[1:]
                # Allocate output tensor.
                output_tensor = torch.empty(
                    output_size, dtype=input_.dtype, device=input_.device
                )
                pynccl_comm.all_gather(output_tensor, input_, sizes=sizes)
                return output_tensor

            if isinstance(input_, torch.Tensor):
                return _all_gather_single(input_, sizes)

            output_list = []
            pynccl_comm.group_start()
            for inp in input_:
                output_list.append(_all_gather_single(inp, sizes=sizes))
            pynccl_comm.group_end()

            return output_list

    def gather(
        self, input_: torch.Tensor, dst: int = 0, dim: int = -1
    ) -&gt; Optional[torch.Tensor]:
        &#34;&#34;&#34;
        NOTE: We assume that the input tensor is on the same device across
        all the ranks.
        NOTE: `dst` is the local rank of the destination rank.
        &#34;&#34;&#34;
        world_size = self.world_size
        # Bypass the function if we are using only 1 GPU.
        if world_size == 1:
            return input_
        assert (
            -input_.dim() &lt;= dim &lt; input_.dim()
        ), f&#34;Invalid dim ({dim}) for input tensor with shape {input_.size()}&#34;
        if dim &lt; 0:
            # Convert negative dim to positive.
            dim += input_.dim()
        if self.xpu_communicator is not None and not self.xpu_communicator.disabled:
            return self.xpu_communicator.gather(input_, self.rank_in_group, dst, dim)
        # Allocate output tensor.
        if self.rank_in_group == dst:
            gather_list = [torch.empty_like(input_) for _ in range(world_size)]
        else:
            gather_list = None
        # Gather.
        torch.distributed.gather(
            input_, gather_list, dst=self.ranks[dst], group=self.device_group
        )
        if self.rank_in_group == dst:
            output_tensor = torch.cat(gather_list, dim=dim)
        else:
            output_tensor = None
        return output_tensor

    def broadcast(self, input_: torch.Tensor, src: int = 0):
        &#34;&#34;&#34;Broadcast the input tensor.
        NOTE: `src` is the local rank of the source rank.
        &#34;&#34;&#34;
        assert src &lt; self.world_size, f&#34;Invalid src rank ({src})&#34;

        # Bypass the function if we are using only 1 GPU.
        if self.world_size == 1:
            return input_
        # Broadcast.
        torch.distributed.broadcast(
            input_, src=self.ranks[src], group=self.device_group
        )
        return input_

    def broadcast_object(self, obj: Optional[Any] = None, src: int = 0):
        &#34;&#34;&#34;Broadcast the input object.
        NOTE: `src` is the local rank of the source rank.
        &#34;&#34;&#34;
        assert src &lt; self.world_size, f&#34;Invalid src rank ({src})&#34;

        # Bypass the function if we are using only 1 GPU.
        if self.world_size == 1:
            return obj
        if self.mq_broadcaster is not None:
            assert src == 0, &#34;Message queue broadcaster only supports src=0&#34;
            return self.mq_broadcaster.broadcast_object(obj)
        if self.rank_in_group == src:
            torch.distributed.broadcast_object_list(
                [obj], src=self.ranks[src], group=self.cpu_group
            )
            return obj
        else:
            recv = [None]
            torch.distributed.broadcast_object_list(
                recv, src=self.ranks[src], group=self.cpu_group
            )
            return recv[0]

    def broadcast_object_list(
        self, obj_list: List[Any], src: int = 0, group: Optional[ProcessGroup] = None
    ):
        &#34;&#34;&#34;Broadcast the input object list.
        NOTE: `src` is the local rank of the source rank.
        &#34;&#34;&#34;
        assert src &lt; self.world_size, f&#34;Invalid src rank ({src})&#34;

        # Bypass the function if we are using only 1 GPU.
        if self.world_size == 1:
            return obj_list
        # Broadcast.
        torch.distributed.broadcast_object_list(
            obj_list, src=self.ranks[src], group=self.device_group
        )
        return obj_list

    def all_gather_object(self, obj: Any) -&gt; List[Any]:
        objs = [None] * self.world_size
        torch.distributed.all_gather_object(objs, obj, group=self.cpu_group)
        return objs

    def send_object(self, obj: Any, dst: int) -&gt; None:
        &#34;&#34;&#34;Send the input object list to the destination rank.&#34;&#34;&#34;
        &#34;&#34;&#34;NOTE: `dst` is the local rank of the destination rank.&#34;&#34;&#34;

        assert dst &lt; self.world_size, f&#34;Invalid dst rank ({dst})&#34;

        assert dst != self.rank_in_group, (
            &#34;Invalid destination rank. Destination rank is the same &#34;
            &#34;as the current rank.&#34;
        )

        # Serialize object to tensor and get the size as well
        object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8).cuda(
            device=torch.cuda.current_device()
        )

        size_tensor = torch.tensor(
            [object_tensor.numel()],
            dtype=torch.long,
            device=torch.cuda.current_device(),
        )

        # Send object size
        torch.distributed.send(
            size_tensor, dst=self.ranks[dst], group=self.device_group
        )

        # Send object
        torch.distributed.send(
            object_tensor, dst=self.ranks[dst], group=self.device_group
        )

        return None

    def recv_object(self, src: int) -&gt; Any:
        &#34;&#34;&#34;Receive the input object list from the source rank.&#34;&#34;&#34;
        &#34;&#34;&#34;NOTE: `src` is the local rank of the source rank.&#34;&#34;&#34;

        assert src &lt; self.world_size, f&#34;Invalid src rank ({src})&#34;

        assert (
            src != self.rank_in_group
        ), &#34;Invalid source rank. Source rank is the same as the current rank.&#34;

        size_tensor = torch.empty(
            1, dtype=torch.long, device=torch.cuda.current_device()
        )

        # Receive object size
        rank_size = torch.distributed.recv(
            size_tensor, src=self.ranks[src], group=self.device_group
        )

        # Tensor to receive serialized objects into.
        object_tensor = torch.empty(  # type: ignore[call-overload]
            size_tensor.item(),  # type: ignore[arg-type]
            dtype=torch.uint8,
            device=torch.cuda.current_device(),
        )

        rank_object = torch.distributed.recv(
            object_tensor, src=self.ranks[src], group=self.device_group
        )

        assert (
            rank_object == rank_size
        ), &#34;Received object sender rank does not match the size sender rank.&#34;

        obj = pickle.loads(object_tensor.cpu().numpy().tobytes())

        return obj

    def broadcast_tensor_dict(
        self,
        tensor_dict: Optional[Dict[str, Union[torch.Tensor, Any]]] = None,
        src: int = 0,
        group: Optional[ProcessGroup] = None,
        metadata_group: Optional[ProcessGroup] = None,
    ) -&gt; Optional[Dict[str, Union[torch.Tensor, Any]]]:
        &#34;&#34;&#34;Broadcast the input tensor dictionary.
        NOTE: `src` is the local rank of the source rank.
        &#34;&#34;&#34;
        # Bypass the function if we are using only 1 GPU.
        if not torch.distributed.is_initialized() or self.world_size == 1:
            return tensor_dict

        group = self.device_group
        metadata_group = self.cpu_group
        assert src &lt; self.world_size, f&#34;Invalid src rank ({src})&#34;

        rank_in_group = self.rank_in_group
        if rank_in_group == src:
            metadata_list: List[Tuple[Any, Any]] = []
            assert isinstance(
                tensor_dict, dict
            ), f&#34;Expecting a dictionary, got {type(tensor_dict)}&#34;
            metadata_list, tensor_list = _split_tensor_dict(tensor_dict)
            # `metadata_list` lives in CPU memory.
            # `broadcast_object_list` has serialization &amp; deserialization,
            # all happening on CPU. Therefore, we can use the CPU group.
            self.broadcast_object(metadata_list, src=src)
            async_handles = []
            for tensor in tensor_list:
                if tensor.numel() == 0:
                    # Skip broadcasting empty tensors.
                    continue
                if tensor.is_cpu:
                    # use metadata_group for CPU tensors
                    handle = torch.distributed.broadcast(
                        tensor, src=self.ranks[src], group=metadata_group, async_op=True
                    )
                else:
                    # use group for GPU tensors
                    handle = torch.distributed.broadcast(
                        tensor, src=self.ranks[src], group=group, async_op=True
                    )
                async_handles.append(handle)
            for async_handle in async_handles:
                async_handle.wait()

        else:
            metadata_list = self.broadcast_object(None, src=src)
            tensor_dict = {}
            async_handles = []
            for key, value in metadata_list:
                if isinstance(value, TensorMetadata):
                    tensor = torch.empty(
                        value.size, dtype=value.dtype, device=value.device
                    )
                    if tensor.numel() == 0:
                        # Skip broadcasting empty tensors.
                        tensor_dict[key] = tensor
                        continue
                    if tensor.is_cpu:
                        # use metadata_group for CPU tensors
                        handle = torch.distributed.broadcast(
                            tensor,
                            src=self.ranks[src],
                            group=metadata_group,
                            async_op=True,
                        )
                    else:
                        # use group for GPU tensors
                        handle = torch.distributed.broadcast(
                            tensor, src=self.ranks[src], group=group, async_op=True
                        )
                    async_handles.append(handle)
                    tensor_dict[key] = tensor
                else:
                    tensor_dict[key] = value
            for async_handle in async_handles:
                async_handle.wait()
        return tensor_dict

    def send_tensor_dict(
        self,
        tensor_dict: Dict[str, Union[torch.Tensor, Any]],
        dst: Optional[int] = None,
        all_gather_group: Optional[&#34;GroupCoordinator&#34;] = None,
    ) -&gt; Optional[Dict[str, Union[torch.Tensor, Any]]]:
        &#34;&#34;&#34;Send the input tensor dictionary.
        NOTE: `dst` is the local rank of the source rank.
        &#34;&#34;&#34;
        # Bypass the function if we are using only 1 GPU.
        if not torch.distributed.is_initialized() or self.world_size == 1:
            return tensor_dict

        all_gather_size = 1 if all_gather_group is None else all_gather_group.world_size
        all_gather_rank = (
            0 if all_gather_group is None else all_gather_group.rank_in_group
        )

        group = self.device_group
        metadata_group = self.cpu_group

        if dst is None:
            dst = (self.rank_in_group + 1) % self.world_size
        assert dst &lt; self.world_size, f&#34;Invalid dst rank ({dst})&#34;

        assert isinstance(
            tensor_dict, dict
        ), f&#34;Expecting a dictionary, got {type(tensor_dict)}&#34;
        metadata_list, tensor_list = _split_tensor_dict(tensor_dict)
        # Note: While switching to Device-to-Device (D2D) would introduce an extra
        # Device-to-Host (D2H) memory copy overhead for serialization, our benchmarks
        # show better overall transmission performance with D2D due to:
        # 1. Superior D2D transfer bandwidth
        # 2. Ability to overlap send and recv operations
        # Thus the net performance gain justifies this approach.
        self.send_object(metadata_list, dst=dst)
        for tensor in tensor_list:
            if tensor.numel() == 0:
                # Skip sending empty tensors.
                continue

            # send-allgather: send only a slice, then do allgather.
            if all_gather_group is not None and tensor.numel() % all_gather_size == 0:
                tensor = tensor.reshape(all_gather_size, -1)[all_gather_rank]

            if tensor.is_cpu:
                # use metadata_group for CPU tensors
                torch.distributed.send(
                    tensor, dst=self.ranks[dst], group=metadata_group
                )
            else:
                # use group for GPU tensors
                torch.distributed.send(tensor, dst=self.ranks[dst], group=group)
        return None

    def recv_tensor_dict(
        self,
        src: Optional[int] = None,
        all_gather_group: Optional[&#34;GroupCoordinator&#34;] = None,
    ) -&gt; Optional[Dict[str, Union[torch.Tensor, Any]]]:
        &#34;&#34;&#34;Recv the input tensor dictionary.
        NOTE: `src` is the local rank of the source rank.
        &#34;&#34;&#34;
        # Bypass the function if we are using only 1 GPU.
        if not torch.distributed.is_initialized() or self.world_size == 1:
            return None

        all_gather_size = 1 if all_gather_group is None else all_gather_group.world_size
        all_gather_rank = (
            0 if all_gather_group is None else all_gather_group.rank_in_group
        )

        group = self.device_group
        metadata_group = self.cpu_group

        if src is None:
            src = (self.rank_in_group - 1) % self.world_size
        assert src &lt; self.world_size, f&#34;Invalid src rank ({src})&#34;

        recv_metadata_list = self.recv_object(src=src)
        tensor_dict: Dict[str, Any] = {}
        for key, value in recv_metadata_list:
            if isinstance(value, TensorMetadata):
                tensor = torch.empty(value.size, dtype=value.dtype, device=value.device)
                if tensor.numel() == 0:
                    # Skip broadcasting empty tensors.
                    tensor_dict[key] = tensor
                    continue

                # send-allgather: send only a slice, then do allgather.
                use_all_gather = (
                    all_gather_group is not None
                    and tensor.numel() % all_gather_size == 0
                )

                if use_all_gather:
                    orig_shape = tensor.shape
                    tensor = tensor.reshape(all_gather_size, -1)[all_gather_rank]

                if tensor.is_cpu:
                    # use metadata_group for CPU tensors
                    torch.distributed.recv(
                        tensor, src=self.ranks[src], group=metadata_group
                    )
                else:
                    # use group for GPU tensors
                    torch.distributed.recv(tensor, src=self.ranks[src], group=group)
                if use_all_gather:
                    # do the allgather
                    tensor = all_gather_group.all_gather(tensor, dim=0)  # type: ignore
                    tensor = tensor.reshape(orig_shape)

                tensor_dict[key] = tensor
            else:
                tensor_dict[key] = value
        return tensor_dict

    def barrier(self):
        &#34;&#34;&#34;Barrier synchronization among the group.
        NOTE: don&#39;t use `device_group` here! `barrier` in NCCL is
        terrible because it is internally a broadcast operation with
        secretly created GPU tensors. It is easy to mess up the current
        device. Use the CPU group instead.
        &#34;&#34;&#34;
        torch.distributed.barrier(group=self.cpu_group)

    def send(self, tensor: torch.Tensor, dst: Optional[int] = None) -&gt; None:
        &#34;&#34;&#34;Sends a tensor to the destination rank in a non-blocking way&#34;&#34;&#34;
        &#34;&#34;&#34;NOTE: `dst` is the local rank of the destination rank.&#34;&#34;&#34;
        if dst is None:
            dst = (self.rank_in_group + 1) % self.world_size

        pynccl_comm = self.pynccl_comm
        if pynccl_comm is not None and not pynccl_comm.disabled:
            pynccl_comm.send(tensor, dst)
        else:
            torch.distributed.send(tensor, self.ranks[dst], self.device_group)

    def recv(
        self, size: torch.Size, dtype: torch.dtype, src: Optional[int] = None
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;Receives a tensor from the source rank.&#34;&#34;&#34;
        &#34;&#34;&#34;NOTE: `src` is the local rank of the source rank.&#34;&#34;&#34;
        if src is None:
            src = (self.rank_in_group - 1) % self.world_size

        tensor = torch.empty(size, dtype=dtype, device=self.device)
        pynccl_comm = self.pynccl_comm
        if pynccl_comm is not None and not pynccl_comm.disabled:
            pynccl_comm.recv(tensor, src)
        else:
            torch.distributed.recv(tensor, self.ranks[src], self.device_group)
        return tensor

    def destroy(self):
        if self.device_group is not None:
            torch.distributed.destroy_process_group(self.device_group)
            self.device_group = None
        if self.cpu_group is not None:
            torch.distributed.destroy_process_group(self.cpu_group)
            self.cpu_group = None
        if self.pynccl_comm is not None:
            self.pynccl_comm = None
        if self.ca_comm is not None:
            self.ca_comm = None
        if self.mq_broadcaster is not None:
            self.mq_broadcaster = None</code></pre>
</details>
<div class="desc"><p>PyTorch ProcessGroup wrapper for a group of processes.
PyTorch ProcessGroup is bound to one specific communication backend,
e.g. NCCL, Gloo, MPI, etc.
GroupCoordinator takes charge of all the communication operations among
the processes in the group. It can route the communication to
a specific implementation (e.g. switch allreduce implementation
based on the tensor size and cuda graph mode).</p></div>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.ca_comm"><code class="name">var <span class="ident">ca_comm</span> : Any | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.cpu_group"><code class="name">var <span class="ident">cpu_group</span> : torch.distributed.distributed_c10d.ProcessGroup</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.device_group"><code class="name">var <span class="ident">device_group</span> : torch.distributed.distributed_c10d.ProcessGroup</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.local_rank"><code class="name">var <span class="ident">local_rank</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.mq_broadcaster"><code class="name">var <span class="ident">mq_broadcaster</span> : Any | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.pynccl_comm"><code class="name">var <span class="ident">pynccl_comm</span> : Any | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.rank"><code class="name">var <span class="ident">rank</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.rank_in_group"><code class="name">var <span class="ident">rank_in_group</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.ranks"><code class="name">var <span class="ident">ranks</span> : List[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.use_custom_allreduce"><code class="name">var <span class="ident">use_custom_allreduce</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.use_message_queue_broadcaster"><code class="name">var <span class="ident">use_message_queue_broadcaster</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.use_pymscclpp"><code class="name">var <span class="ident">use_pymscclpp</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.use_pynccl"><code class="name">var <span class="ident">use_pynccl</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.world_size"><code class="name">var <span class="ident">world_size</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.first_rank"><code class="name">prop <span class="ident">first_rank</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def first_rank(self):
    &#34;&#34;&#34;Return the global rank of the first process in the group&#34;&#34;&#34;
    return self.ranks[0]</code></pre>
</details>
<div class="desc"><p>Return the global rank of the first process in the group</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.is_first_rank"><code class="name">prop <span class="ident">is_first_rank</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_first_rank(self):
    &#34;&#34;&#34;Return whether the caller is the first process in the group&#34;&#34;&#34;
    return self.rank == self.first_rank</code></pre>
</details>
<div class="desc"><p>Return whether the caller is the first process in the group</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.is_last_rank"><code class="name">prop <span class="ident">is_last_rank</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_last_rank(self):
    &#34;&#34;&#34;Return whether the caller is the last process in the group&#34;&#34;&#34;
    return self.rank == self.last_rank</code></pre>
</details>
<div class="desc"><p>Return whether the caller is the last process in the group</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.last_rank"><code class="name">prop <span class="ident">last_rank</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def last_rank(self):
    &#34;&#34;&#34;Return the global rank of the last process in the group&#34;&#34;&#34;
    return self.ranks[-1]</code></pre>
</details>
<div class="desc"><p>Return the global rank of the last process in the group</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.next_rank"><code class="name">prop <span class="ident">next_rank</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def next_rank(self):
    &#34;&#34;&#34;Return the global rank of the process that follows the caller&#34;&#34;&#34;
    rank_in_group = self.rank_in_group
    world_size = self.world_size
    return self.ranks[(rank_in_group + 1) % world_size]</code></pre>
</details>
<div class="desc"><p>Return the global rank of the process that follows the caller</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.prev_rank"><code class="name">prop <span class="ident">prev_rank</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def prev_rank(self):
    &#34;&#34;&#34;Return the global rank of the process that precedes the caller&#34;&#34;&#34;
    rank_in_group = self.rank_in_group
    world_size = self.world_size
    return self.ranks[(rank_in_group - 1) % world_size]</code></pre>
</details>
<div class="desc"><p>Return the global rank of the process that precedes the caller</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.all_gather"><code class="name flex">
<span>def <span class="ident">all_gather</span></span>(<span>self,<br>input_: torch.Tensor,<br>dim: int = -1,<br>output_tensor_list: List[torch.Tensor] | None = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def all_gather(
    self,
    input_: torch.Tensor,
    dim: int = -1,
    output_tensor_list: Optional[List[torch.Tensor]] = None,
) -&gt; torch.Tensor:
    world_size = self.world_size
    # Bypass the function if we are using only 1 GPU.
    if world_size == 1:
        if output_tensor_list is not None:
            logger.warning(
                &#34;Performing in-place all-gather with a group size of 1. &#34;
                &#34;This may be unnecessary; consider bypassing it for better efficiency.&#34;
            )
            output_tensor_list[0].copy_(input_)
            return None
        else:
            return input_

    if output_tensor_list is not None:
        # TODO(ch-wan): support other backends
        return torch.distributed.all_gather(
            output_tensor_list, input_, group=self.device_group
        )

    assert (
        -input_.dim() &lt;= dim &lt; input_.dim()
    ), f&#34;Invalid dim ({dim}) for input tensor with shape {input_.size()}&#34;

    # For HPUs, use HPU communicator.
    hpu_comm = self.hpu_communicator
    if hpu_comm is not None and not hpu_comm.disabled:
        return hpu_comm.all_gather(input_, dim)

    # For NPUs, use NPU communicator.
    npu_comm = self.npu_communicator
    if npu_comm is not None and not npu_comm.disabled:
        return npu_comm.all_gather(input_, dim)

    if dim &lt; 0:
        # Convert negative dim to positive.
        dim += input_.dim()
    input_size = input_.size()
    # NOTE: we have to use concat-style all-gather here,
    # stack-style all-gather has compatibility issues with
    # torch.compile . see https://github.com/pytorch/pytorch/issues/138795
    output_size = (input_size[0] * world_size,) + input_size[1:]
    # Allocate output tensor.
    output_tensor = torch.empty(
        output_size, dtype=input_.dtype, device=input_.device
    )

    # All-gather.
    if input_.is_cpu and is_shm_available(
        input_.dtype, self.world_size, self.local_size
    ):
        return torch.ops.sgl_kernel.shm_allgather(input_, dim)

    if input_.is_cpu:
        torch.distributed.all_gather_into_tensor(
            output_tensor, input_, group=self.device_group
        )
    else:
        self.all_gather_into_tensor(output_tensor, input_)

    # Reshape
    output_tensor = output_tensor.reshape((world_size,) + input_size)
    output_tensor = output_tensor.movedim(0, dim)
    output_tensor = output_tensor.reshape(
        input_size[:dim] + (world_size * input_size[dim],) + input_size[dim + 1 :]
    )
    return output_tensor</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.all_gather_into_tensor"><code class="name flex">
<span>def <span class="ident">all_gather_into_tensor</span></span>(<span>self, output: torch.Tensor, input: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def all_gather_into_tensor(self, output: torch.Tensor, input: torch.Tensor):
    if _is_npu or not supports_custom_op():
        self._all_gather_into_tensor(output, input)
    else:
        torch.ops.sglang.reg_all_gather_into_tensor(
            output, input, group_name=self.unique_name
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.all_gather_object"><code class="name flex">
<span>def <span class="ident">all_gather_object</span></span>(<span>self, obj: Any) ‑> List[Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def all_gather_object(self, obj: Any) -&gt; List[Any]:
    objs = [None] * self.world_size
    torch.distributed.all_gather_object(objs, obj, group=self.cpu_group)
    return objs</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.all_gatherv"><code class="name flex">
<span>def <span class="ident">all_gatherv</span></span>(<span>self, input_: torch.Tensor | List[torch.Tensor], sizes: List[int] | None = None) ‑> torch.Tensor | List[torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def all_gatherv(
    self,
    input_: Union[torch.Tensor, List[torch.Tensor]],
    sizes: Optional[List[int]] = None,
) -&gt; Union[torch.Tensor, List[torch.Tensor]]:
    &#34;&#34;&#34;
    Supports varying sizes per rank and input tensor list.
    `sizes`: a list of len(world_size) with the number of items per rank to gather.
    &#34;&#34;&#34;
    world_size = self.world_size
    pynccl_comm = self.pynccl_comm

    with pynccl_comm.change_state(enable=True, stream=torch.cuda.current_stream()):
        assert (
            pynccl_comm is not None and not pynccl_comm.disabled
        ), &#34;pynccl is required for all_gatherv&#34;

        def _all_gather_single(
            input_: torch.Tensor, sizes: Optional[List[int]] = None
        ):
            input_size = input_.size()
            if sizes is not None:
                assert len(sizes) == world_size
                assert input_.shape[0] == sizes[self.rank_in_group]
                output_size = (sum(sizes),) + input_size[1:]
                # &#39;sizes&#39; is not needed if all inputs in the same group have the same shape
                if all(s == sizes[0] for s in sizes):
                    sizes = None
            else:
                output_size = (input_size[0] * world_size,) + input_size[1:]
            # Allocate output tensor.
            output_tensor = torch.empty(
                output_size, dtype=input_.dtype, device=input_.device
            )
            pynccl_comm.all_gather(output_tensor, input_, sizes=sizes)
            return output_tensor

        if isinstance(input_, torch.Tensor):
            return _all_gather_single(input_, sizes)

        output_list = []
        pynccl_comm.group_start()
        for inp in input_:
            output_list.append(_all_gather_single(inp, sizes=sizes))
        pynccl_comm.group_end()

        return output_list</code></pre>
</details>
<div class="desc"><p>Supports varying sizes per rank and input tensor list.
<code>sizes</code>: a list of len(world_size) with the number of items per rank to gather.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.all_reduce"><code class="name flex">
<span>def <span class="ident">all_reduce</span></span>(<span>self, input_: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def all_reduce(self, input_: torch.Tensor) -&gt; torch.Tensor:
    &#34;&#34;&#34;
    User-facing all-reduce function before we actually call the
    all-reduce operation.

    We need this because Dynamo does not support passing an arbitrary
    object (`self` in this case) to a custom op. We need to pass the
     group name as a string, and then look up the group coordinator from
     the group name, dispatch the all-reduce operation to the group
     coordinator.

    In addition, PyTorch custom ops do not support mutation or returning
    a new tensor in the same op. So we need to figure out if the op is
    in-place or out-of-place ahead of time.
    &#34;&#34;&#34;
    # Bypass the function if we are using only 1 GPU.
    if self.world_size == 1:
        return input_

    if input_.is_cpu:
        if is_shm_available(input_.dtype, self.world_size, self.local_size):
            torch.ops.sgl_kernel.shm_allreduce(
                input_, torch.distributed.ReduceOp.SUM
            )
        else:
            torch.distributed.all_reduce(input_, group=self.device_group)
        return input_

    if not supports_custom_op():
        self._all_reduce_in_place(input_)
        return input_

    if self.hpu_communicator is not None and not self.hpu_communicator.disabled:
        return self.hpu_communicator.all_reduce(input_)

    if self.xpu_communicator is not None and not self.xpu_communicator.disabled:
        return self.xpu_communicator.all_reduce(input_)

    if self.npu_communicator is not None and not self.npu_communicator.disabled:
        return self.npu_communicator.all_reduce(input_)

    if (
        self.pynccl_comm is not None
        and hasattr(input_, &#34;symmetric_memory&#34;)
        and input_.symmetric_memory
    ):
        with self.pynccl_comm.change_state(
            enable=True, stream=torch.cuda.current_stream()
        ):
            self.pynccl_comm.all_reduce(input_)
            return input_

    outplace_all_reduce_method = None
    if (
        self.qr_comm is not None
        and not self.qr_comm.disabled
        and self.qr_comm.should_quick_allreduce(input_)
    ):
        outplace_all_reduce_method = &#34;qr&#34;
    elif (
        self.ca_comm is not None
        and not self.ca_comm.disabled
        and self.ca_comm.should_custom_ar(input_)
    ):
        outplace_all_reduce_method = &#34;ca&#34;
    elif (
        self.pymscclpp_comm is not None
        and not self.pymscclpp_comm.disabled
        and self.pymscclpp_comm.should_mscclpp_allreduce(input_)
    ):
        outplace_all_reduce_method = &#34;pymscclpp&#34;
    if outplace_all_reduce_method is not None:
        return torch.ops.sglang.outplace_all_reduce(
            input_,
            group_name=self.unique_name,
            outplace_all_reduce_method=outplace_all_reduce_method,
        )
    else:
        torch.ops.sglang.inplace_all_reduce(input_, group_name=self.unique_name)
        return input_</code></pre>
</details>
<div class="desc"><p>User-facing all-reduce function before we actually call the
all-reduce operation.</p>
<p>We need this because Dynamo does not support passing an arbitrary
object (<code>self</code> in this case) to a custom op. We need to pass the
group name as a string, and then look up the group coordinator from
the group name, dispatch the all-reduce operation to the group
coordinator.</p>
<p>In addition, PyTorch custom ops do not support mutation or returning
a new tensor in the same op. So we need to figure out if the op is
in-place or out-of-place ahead of time.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.barrier"><code class="name flex">
<span>def <span class="ident">barrier</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def barrier(self):
    &#34;&#34;&#34;Barrier synchronization among the group.
    NOTE: don&#39;t use `device_group` here! `barrier` in NCCL is
    terrible because it is internally a broadcast operation with
    secretly created GPU tensors. It is easy to mess up the current
    device. Use the CPU group instead.
    &#34;&#34;&#34;
    torch.distributed.barrier(group=self.cpu_group)</code></pre>
</details>
<div class="desc"><p>Barrier synchronization among the group.
NOTE: don't use <code>device_group</code> here! <code>barrier</code> in NCCL is
terrible because it is internally a broadcast operation with
secretly created GPU tensors. It is easy to mess up the current
device. Use the CPU group instead.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.broadcast"><code class="name flex">
<span>def <span class="ident">broadcast</span></span>(<span>self, input_: torch.Tensor, src: int = 0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def broadcast(self, input_: torch.Tensor, src: int = 0):
    &#34;&#34;&#34;Broadcast the input tensor.
    NOTE: `src` is the local rank of the source rank.
    &#34;&#34;&#34;
    assert src &lt; self.world_size, f&#34;Invalid src rank ({src})&#34;

    # Bypass the function if we are using only 1 GPU.
    if self.world_size == 1:
        return input_
    # Broadcast.
    torch.distributed.broadcast(
        input_, src=self.ranks[src], group=self.device_group
    )
    return input_</code></pre>
</details>
<div class="desc"><p>Broadcast the input tensor.
NOTE: <code>src</code> is the local rank of the source rank.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.broadcast_object"><code class="name flex">
<span>def <span class="ident">broadcast_object</span></span>(<span>self, obj: Any | None = None, src: int = 0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def broadcast_object(self, obj: Optional[Any] = None, src: int = 0):
    &#34;&#34;&#34;Broadcast the input object.
    NOTE: `src` is the local rank of the source rank.
    &#34;&#34;&#34;
    assert src &lt; self.world_size, f&#34;Invalid src rank ({src})&#34;

    # Bypass the function if we are using only 1 GPU.
    if self.world_size == 1:
        return obj
    if self.mq_broadcaster is not None:
        assert src == 0, &#34;Message queue broadcaster only supports src=0&#34;
        return self.mq_broadcaster.broadcast_object(obj)
    if self.rank_in_group == src:
        torch.distributed.broadcast_object_list(
            [obj], src=self.ranks[src], group=self.cpu_group
        )
        return obj
    else:
        recv = [None]
        torch.distributed.broadcast_object_list(
            recv, src=self.ranks[src], group=self.cpu_group
        )
        return recv[0]</code></pre>
</details>
<div class="desc"><p>Broadcast the input object.
NOTE: <code>src</code> is the local rank of the source rank.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.broadcast_object_list"><code class="name flex">
<span>def <span class="ident">broadcast_object_list</span></span>(<span>self,<br>obj_list: List[Any],<br>src: int = 0,<br>group: torch.distributed.distributed_c10d.ProcessGroup | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def broadcast_object_list(
    self, obj_list: List[Any], src: int = 0, group: Optional[ProcessGroup] = None
):
    &#34;&#34;&#34;Broadcast the input object list.
    NOTE: `src` is the local rank of the source rank.
    &#34;&#34;&#34;
    assert src &lt; self.world_size, f&#34;Invalid src rank ({src})&#34;

    # Bypass the function if we are using only 1 GPU.
    if self.world_size == 1:
        return obj_list
    # Broadcast.
    torch.distributed.broadcast_object_list(
        obj_list, src=self.ranks[src], group=self.device_group
    )
    return obj_list</code></pre>
</details>
<div class="desc"><p>Broadcast the input object list.
NOTE: <code>src</code> is the local rank of the source rank.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.broadcast_tensor_dict"><code class="name flex">
<span>def <span class="ident">broadcast_tensor_dict</span></span>(<span>self,<br>tensor_dict: Dict[str, torch.Tensor | Any] | None = None,<br>src: int = 0,<br>group: torch.distributed.distributed_c10d.ProcessGroup | None = None,<br>metadata_group: torch.distributed.distributed_c10d.ProcessGroup | None = None) ‑> Dict[str, torch.Tensor | Any] | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def broadcast_tensor_dict(
    self,
    tensor_dict: Optional[Dict[str, Union[torch.Tensor, Any]]] = None,
    src: int = 0,
    group: Optional[ProcessGroup] = None,
    metadata_group: Optional[ProcessGroup] = None,
) -&gt; Optional[Dict[str, Union[torch.Tensor, Any]]]:
    &#34;&#34;&#34;Broadcast the input tensor dictionary.
    NOTE: `src` is the local rank of the source rank.
    &#34;&#34;&#34;
    # Bypass the function if we are using only 1 GPU.
    if not torch.distributed.is_initialized() or self.world_size == 1:
        return tensor_dict

    group = self.device_group
    metadata_group = self.cpu_group
    assert src &lt; self.world_size, f&#34;Invalid src rank ({src})&#34;

    rank_in_group = self.rank_in_group
    if rank_in_group == src:
        metadata_list: List[Tuple[Any, Any]] = []
        assert isinstance(
            tensor_dict, dict
        ), f&#34;Expecting a dictionary, got {type(tensor_dict)}&#34;
        metadata_list, tensor_list = _split_tensor_dict(tensor_dict)
        # `metadata_list` lives in CPU memory.
        # `broadcast_object_list` has serialization &amp; deserialization,
        # all happening on CPU. Therefore, we can use the CPU group.
        self.broadcast_object(metadata_list, src=src)
        async_handles = []
        for tensor in tensor_list:
            if tensor.numel() == 0:
                # Skip broadcasting empty tensors.
                continue
            if tensor.is_cpu:
                # use metadata_group for CPU tensors
                handle = torch.distributed.broadcast(
                    tensor, src=self.ranks[src], group=metadata_group, async_op=True
                )
            else:
                # use group for GPU tensors
                handle = torch.distributed.broadcast(
                    tensor, src=self.ranks[src], group=group, async_op=True
                )
            async_handles.append(handle)
        for async_handle in async_handles:
            async_handle.wait()

    else:
        metadata_list = self.broadcast_object(None, src=src)
        tensor_dict = {}
        async_handles = []
        for key, value in metadata_list:
            if isinstance(value, TensorMetadata):
                tensor = torch.empty(
                    value.size, dtype=value.dtype, device=value.device
                )
                if tensor.numel() == 0:
                    # Skip broadcasting empty tensors.
                    tensor_dict[key] = tensor
                    continue
                if tensor.is_cpu:
                    # use metadata_group for CPU tensors
                    handle = torch.distributed.broadcast(
                        tensor,
                        src=self.ranks[src],
                        group=metadata_group,
                        async_op=True,
                    )
                else:
                    # use group for GPU tensors
                    handle = torch.distributed.broadcast(
                        tensor, src=self.ranks[src], group=group, async_op=True
                    )
                async_handles.append(handle)
                tensor_dict[key] = tensor
            else:
                tensor_dict[key] = value
        for async_handle in async_handles:
            async_handle.wait()
    return tensor_dict</code></pre>
</details>
<div class="desc"><p>Broadcast the input tensor dictionary.
NOTE: <code>src</code> is the local rank of the source rank.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.destroy"><code class="name flex">
<span>def <span class="ident">destroy</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def destroy(self):
    if self.device_group is not None:
        torch.distributed.destroy_process_group(self.device_group)
        self.device_group = None
    if self.cpu_group is not None:
        torch.distributed.destroy_process_group(self.cpu_group)
        self.cpu_group = None
    if self.pynccl_comm is not None:
        self.pynccl_comm = None
    if self.ca_comm is not None:
        self.ca_comm = None
    if self.mq_broadcaster is not None:
        self.mq_broadcaster = None</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.gather"><code class="name flex">
<span>def <span class="ident">gather</span></span>(<span>self, input_: torch.Tensor, dst: int = 0, dim: int = -1) ‑> torch.Tensor | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gather(
    self, input_: torch.Tensor, dst: int = 0, dim: int = -1
) -&gt; Optional[torch.Tensor]:
    &#34;&#34;&#34;
    NOTE: We assume that the input tensor is on the same device across
    all the ranks.
    NOTE: `dst` is the local rank of the destination rank.
    &#34;&#34;&#34;
    world_size = self.world_size
    # Bypass the function if we are using only 1 GPU.
    if world_size == 1:
        return input_
    assert (
        -input_.dim() &lt;= dim &lt; input_.dim()
    ), f&#34;Invalid dim ({dim}) for input tensor with shape {input_.size()}&#34;
    if dim &lt; 0:
        # Convert negative dim to positive.
        dim += input_.dim()
    if self.xpu_communicator is not None and not self.xpu_communicator.disabled:
        return self.xpu_communicator.gather(input_, self.rank_in_group, dst, dim)
    # Allocate output tensor.
    if self.rank_in_group == dst:
        gather_list = [torch.empty_like(input_) for _ in range(world_size)]
    else:
        gather_list = None
    # Gather.
    torch.distributed.gather(
        input_, gather_list, dst=self.ranks[dst], group=self.device_group
    )
    if self.rank_in_group == dst:
        output_tensor = torch.cat(gather_list, dim=dim)
    else:
        output_tensor = None
    return output_tensor</code></pre>
</details>
<div class="desc"><p>NOTE: We assume that the input tensor is on the same device across
all the ranks.
NOTE: <code>dst</code> is the local rank of the destination rank.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.graph_capture"><code class="name flex">
<span>def <span class="ident">graph_capture</span></span>(<span>self,<br>graph_capture_context: <a title="sglang.srt.distributed.parallel_state.GraphCaptureContext" href="#sglang.srt.distributed.parallel_state.GraphCaptureContext">GraphCaptureContext</a> | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def graph_capture(
    self, graph_capture_context: Optional[GraphCaptureContext] = None
):
    if graph_capture_context is None:
        stream = self.device_module.Stream()
        graph_capture_context = GraphCaptureContext(stream)
    else:
        stream = graph_capture_context.stream
    # We don&#39;t need the context of custom quick allreduce because the ipc access
    # is already collected in init() and we can capture the quick allreduce directly.
    ca_comm = self.ca_comm
    maybe_ca_context = nullcontext() if ca_comm is None else ca_comm.capture()

    # ensure all initialization operations complete before attempting to
    # capture the graph on another stream
    curr_stream = self.device_module.current_stream()
    if curr_stream != stream:
        stream.wait_stream(curr_stream)

    with self.device_module.stream(stream), maybe_ca_context:
        # In graph mode, we have to be very careful about the collective
        # operations. The current status is:
        #     allreduce \ Mode   |  Eager  |  Graph  |
        # --------------------------------------------
        # quick allreduce        | enabled | enabled |
        # custom allreduce       | enabled | enabled |
        # PyNccl                 | disabled| enabled |
        # PyMscclpp              | disabled| enabled |
        # torch.distributed      | enabled | disabled|
        #
        # Note: When custom quick allreduce is enabled, a runtime check
        #  will be performed. If the tensor size is too small, it will
        #  automatically fall back to the next available option.
        # Note that custom allreduce will have a runtime check, if the
        #  tensor size is too large, it will fallback to the next
        #  available option.
        # Note that the PyMsccl needs to register the tensor in ahead,
        #  which will introduce large overhead in the eager case,
        #  therefore it is only supported in the graph case.
        # In summary: We select the appropriate allreduce method for
        #  each mode based on the algorithm order in the table and
        #  their usage conditions.
        pynccl_comm = self.pynccl_comm
        maybe_pynccl_context: Any
        if not pynccl_comm:
            maybe_pynccl_context = nullcontext()
        else:
            maybe_pynccl_context = pynccl_comm.change_state(
                enable=True, stream=torch.cuda.current_stream()
            )

        pymscclpp_comm = self.pymscclpp_comm
        maybe_pymscclpp_context: Any
        if not pymscclpp_comm:
            maybe_pymscclpp_context = nullcontext()
        else:
            maybe_pymscclpp_context = pymscclpp_comm.change_state(enable=True)
        with maybe_pynccl_context, maybe_pymscclpp_context:
            yield graph_capture_context</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.recv"><code class="name flex">
<span>def <span class="ident">recv</span></span>(<span>self, size: torch.Size, dtype: torch.dtype, src: int | None = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def recv(
    self, size: torch.Size, dtype: torch.dtype, src: Optional[int] = None
) -&gt; torch.Tensor:
    &#34;&#34;&#34;Receives a tensor from the source rank.&#34;&#34;&#34;
    &#34;&#34;&#34;NOTE: `src` is the local rank of the source rank.&#34;&#34;&#34;
    if src is None:
        src = (self.rank_in_group - 1) % self.world_size

    tensor = torch.empty(size, dtype=dtype, device=self.device)
    pynccl_comm = self.pynccl_comm
    if pynccl_comm is not None and not pynccl_comm.disabled:
        pynccl_comm.recv(tensor, src)
    else:
        torch.distributed.recv(tensor, self.ranks[src], self.device_group)
    return tensor</code></pre>
</details>
<div class="desc"><p>Receives a tensor from the source rank.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.recv_object"><code class="name flex">
<span>def <span class="ident">recv_object</span></span>(<span>self, src: int) ‑> Any</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def recv_object(self, src: int) -&gt; Any:
    &#34;&#34;&#34;Receive the input object list from the source rank.&#34;&#34;&#34;
    &#34;&#34;&#34;NOTE: `src` is the local rank of the source rank.&#34;&#34;&#34;

    assert src &lt; self.world_size, f&#34;Invalid src rank ({src})&#34;

    assert (
        src != self.rank_in_group
    ), &#34;Invalid source rank. Source rank is the same as the current rank.&#34;

    size_tensor = torch.empty(
        1, dtype=torch.long, device=torch.cuda.current_device()
    )

    # Receive object size
    rank_size = torch.distributed.recv(
        size_tensor, src=self.ranks[src], group=self.device_group
    )

    # Tensor to receive serialized objects into.
    object_tensor = torch.empty(  # type: ignore[call-overload]
        size_tensor.item(),  # type: ignore[arg-type]
        dtype=torch.uint8,
        device=torch.cuda.current_device(),
    )

    rank_object = torch.distributed.recv(
        object_tensor, src=self.ranks[src], group=self.device_group
    )

    assert (
        rank_object == rank_size
    ), &#34;Received object sender rank does not match the size sender rank.&#34;

    obj = pickle.loads(object_tensor.cpu().numpy().tobytes())

    return obj</code></pre>
</details>
<div class="desc"><p>Receive the input object list from the source rank.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.recv_tensor_dict"><code class="name flex">
<span>def <span class="ident">recv_tensor_dict</span></span>(<span>self,<br>src: int | None = None,<br>all_gather_group: ForwardRef('<a title="sglang.srt.distributed.parallel_state.GroupCoordinator" href="#sglang.srt.distributed.parallel_state.GroupCoordinator">GroupCoordinator</a>') | None = None) ‑> Dict[str, torch.Tensor | Any] | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def recv_tensor_dict(
    self,
    src: Optional[int] = None,
    all_gather_group: Optional[&#34;GroupCoordinator&#34;] = None,
) -&gt; Optional[Dict[str, Union[torch.Tensor, Any]]]:
    &#34;&#34;&#34;Recv the input tensor dictionary.
    NOTE: `src` is the local rank of the source rank.
    &#34;&#34;&#34;
    # Bypass the function if we are using only 1 GPU.
    if not torch.distributed.is_initialized() or self.world_size == 1:
        return None

    all_gather_size = 1 if all_gather_group is None else all_gather_group.world_size
    all_gather_rank = (
        0 if all_gather_group is None else all_gather_group.rank_in_group
    )

    group = self.device_group
    metadata_group = self.cpu_group

    if src is None:
        src = (self.rank_in_group - 1) % self.world_size
    assert src &lt; self.world_size, f&#34;Invalid src rank ({src})&#34;

    recv_metadata_list = self.recv_object(src=src)
    tensor_dict: Dict[str, Any] = {}
    for key, value in recv_metadata_list:
        if isinstance(value, TensorMetadata):
            tensor = torch.empty(value.size, dtype=value.dtype, device=value.device)
            if tensor.numel() == 0:
                # Skip broadcasting empty tensors.
                tensor_dict[key] = tensor
                continue

            # send-allgather: send only a slice, then do allgather.
            use_all_gather = (
                all_gather_group is not None
                and tensor.numel() % all_gather_size == 0
            )

            if use_all_gather:
                orig_shape = tensor.shape
                tensor = tensor.reshape(all_gather_size, -1)[all_gather_rank]

            if tensor.is_cpu:
                # use metadata_group for CPU tensors
                torch.distributed.recv(
                    tensor, src=self.ranks[src], group=metadata_group
                )
            else:
                # use group for GPU tensors
                torch.distributed.recv(tensor, src=self.ranks[src], group=group)
            if use_all_gather:
                # do the allgather
                tensor = all_gather_group.all_gather(tensor, dim=0)  # type: ignore
                tensor = tensor.reshape(orig_shape)

            tensor_dict[key] = tensor
        else:
            tensor_dict[key] = value
    return tensor_dict</code></pre>
</details>
<div class="desc"><p>Recv the input tensor dictionary.
NOTE: <code>src</code> is the local rank of the source rank.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.reduce_scatter"><code class="name flex">
<span>def <span class="ident">reduce_scatter</span></span>(<span>self, output: torch.Tensor, input_list: List[torch.Tensor]) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reduce_scatter(
    self,
    output: torch.Tensor,
    input_list: List[torch.Tensor],
) -&gt; None:
    # TODO(ch-wan): support other backends
    torch.distributed.reduce_scatter(output, input_list, group=self.device_group)
    return output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.reduce_scatter_tensor"><code class="name flex">
<span>def <span class="ident">reduce_scatter_tensor</span></span>(<span>self, output: torch.Tensor, input: torch.Tensor) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reduce_scatter_tensor(
    self,
    output: torch.Tensor,
    input: torch.Tensor,
) -&gt; None:
    # TODO(ch-wan): support other backends
    torch.distributed.reduce_scatter_tensor(output, input, group=self.device_group)
    return output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.reduce_scatterv"><code class="name flex">
<span>def <span class="ident">reduce_scatterv</span></span>(<span>self,<br>input_: torch.Tensor,<br>output: torch.Tensor | None = None,<br>sizes: List[int] | None = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reduce_scatterv(
    self,
    input_: torch.Tensor,
    output: Optional[torch.Tensor] = None,
    sizes: Optional[List[int]] = None,
) -&gt; torch.Tensor:
    world_size = self.world_size
    pynccl_comm = self.pynccl_comm

    with pynccl_comm.change_state(enable=True, stream=torch.cuda.current_stream()):
        assert (
            pynccl_comm is not None and not pynccl_comm.disabled
        ), &#34;pynccl is required for reduce_scatterv&#34;

        if sizes is not None:
            assert len(sizes) == world_size
            assert input_.shape[0] == sum(sizes)
            chunk_size = sizes[self.rank_in_group]
        else:
            assert input_.shape[0] % world_size == 0
            chunk_size = input_.shape[0] // world_size
        output_shape = (chunk_size,) + input_.shape[1:]

        if output is None:
            output = torch.empty(
                output_shape, dtype=input_.dtype, device=input_.device
            )
        else:
            assert output.shape == output_shape

        pynccl_comm.reduce_scatter(output, input_, sizes=sizes)
        return output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.send"><code class="name flex">
<span>def <span class="ident">send</span></span>(<span>self, tensor: torch.Tensor, dst: int | None = None) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def send(self, tensor: torch.Tensor, dst: Optional[int] = None) -&gt; None:
    &#34;&#34;&#34;Sends a tensor to the destination rank in a non-blocking way&#34;&#34;&#34;
    &#34;&#34;&#34;NOTE: `dst` is the local rank of the destination rank.&#34;&#34;&#34;
    if dst is None:
        dst = (self.rank_in_group + 1) % self.world_size

    pynccl_comm = self.pynccl_comm
    if pynccl_comm is not None and not pynccl_comm.disabled:
        pynccl_comm.send(tensor, dst)
    else:
        torch.distributed.send(tensor, self.ranks[dst], self.device_group)</code></pre>
</details>
<div class="desc"><p>Sends a tensor to the destination rank in a non-blocking way</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.send_object"><code class="name flex">
<span>def <span class="ident">send_object</span></span>(<span>self, obj: Any, dst: int) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def send_object(self, obj: Any, dst: int) -&gt; None:
    &#34;&#34;&#34;Send the input object list to the destination rank.&#34;&#34;&#34;
    &#34;&#34;&#34;NOTE: `dst` is the local rank of the destination rank.&#34;&#34;&#34;

    assert dst &lt; self.world_size, f&#34;Invalid dst rank ({dst})&#34;

    assert dst != self.rank_in_group, (
        &#34;Invalid destination rank. Destination rank is the same &#34;
        &#34;as the current rank.&#34;
    )

    # Serialize object to tensor and get the size as well
    object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8).cuda(
        device=torch.cuda.current_device()
    )

    size_tensor = torch.tensor(
        [object_tensor.numel()],
        dtype=torch.long,
        device=torch.cuda.current_device(),
    )

    # Send object size
    torch.distributed.send(
        size_tensor, dst=self.ranks[dst], group=self.device_group
    )

    # Send object
    torch.distributed.send(
        object_tensor, dst=self.ranks[dst], group=self.device_group
    )

    return None</code></pre>
</details>
<div class="desc"><p>Send the input object list to the destination rank.</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.GroupCoordinator.send_tensor_dict"><code class="name flex">
<span>def <span class="ident">send_tensor_dict</span></span>(<span>self,<br>tensor_dict: Dict[str, torch.Tensor | Any],<br>dst: int | None = None,<br>all_gather_group: ForwardRef('<a title="sglang.srt.distributed.parallel_state.GroupCoordinator" href="#sglang.srt.distributed.parallel_state.GroupCoordinator">GroupCoordinator</a>') | None = None) ‑> Dict[str, torch.Tensor | Any] | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def send_tensor_dict(
    self,
    tensor_dict: Dict[str, Union[torch.Tensor, Any]],
    dst: Optional[int] = None,
    all_gather_group: Optional[&#34;GroupCoordinator&#34;] = None,
) -&gt; Optional[Dict[str, Union[torch.Tensor, Any]]]:
    &#34;&#34;&#34;Send the input tensor dictionary.
    NOTE: `dst` is the local rank of the source rank.
    &#34;&#34;&#34;
    # Bypass the function if we are using only 1 GPU.
    if not torch.distributed.is_initialized() or self.world_size == 1:
        return tensor_dict

    all_gather_size = 1 if all_gather_group is None else all_gather_group.world_size
    all_gather_rank = (
        0 if all_gather_group is None else all_gather_group.rank_in_group
    )

    group = self.device_group
    metadata_group = self.cpu_group

    if dst is None:
        dst = (self.rank_in_group + 1) % self.world_size
    assert dst &lt; self.world_size, f&#34;Invalid dst rank ({dst})&#34;

    assert isinstance(
        tensor_dict, dict
    ), f&#34;Expecting a dictionary, got {type(tensor_dict)}&#34;
    metadata_list, tensor_list = _split_tensor_dict(tensor_dict)
    # Note: While switching to Device-to-Device (D2D) would introduce an extra
    # Device-to-Host (D2H) memory copy overhead for serialization, our benchmarks
    # show better overall transmission performance with D2D due to:
    # 1. Superior D2D transfer bandwidth
    # 2. Ability to overlap send and recv operations
    # Thus the net performance gain justifies this approach.
    self.send_object(metadata_list, dst=dst)
    for tensor in tensor_list:
        if tensor.numel() == 0:
            # Skip sending empty tensors.
            continue

        # send-allgather: send only a slice, then do allgather.
        if all_gather_group is not None and tensor.numel() % all_gather_size == 0:
            tensor = tensor.reshape(all_gather_size, -1)[all_gather_rank]

        if tensor.is_cpu:
            # use metadata_group for CPU tensors
            torch.distributed.send(
                tensor, dst=self.ranks[dst], group=metadata_group
            )
        else:
            # use group for GPU tensors
            torch.distributed.send(tensor, dst=self.ranks[dst], group=group)
    return None</code></pre>
</details>
<div class="desc"><p>Send the input tensor dictionary.
NOTE: <code>dst</code> is the local rank of the source rank.</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.distributed.parallel_state.TensorMetadata"><code class="flex name class">
<span>class <span class="ident">TensorMetadata</span></span>
<span>(</span><span>device, dtype, size)</span>
</code></dt>
<dd>
<div class="desc"><p>TensorMetadata(device, dtype, size)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.distributed.parallel_state.TensorMetadata.device"><code class="name">var <span class="ident">device</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 0</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.TensorMetadata.dtype"><code class="name">var <span class="ident">dtype</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 1</p></div>
</dd>
<dt id="sglang.srt.distributed.parallel_state.TensorMetadata.size"><code class="name">var <span class="ident">size</span></code></dt>
<dd>
<div class="desc"><p>Alias for field number 2</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.distributed" href="index.html">sglang.srt.distributed</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.distributed.parallel_state.cleanup_dist_env_and_memory" href="#sglang.srt.distributed.parallel_state.cleanup_dist_env_and_memory">cleanup_dist_env_and_memory</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.destroy_distributed_environment" href="#sglang.srt.distributed.parallel_state.destroy_distributed_environment">destroy_distributed_environment</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.destroy_model_parallel" href="#sglang.srt.distributed.parallel_state.destroy_model_parallel">destroy_model_parallel</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.ensure_model_parallel_initialized" href="#sglang.srt.distributed.parallel_state.ensure_model_parallel_initialized">ensure_model_parallel_initialized</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.get_moe_ep_group" href="#sglang.srt.distributed.parallel_state.get_moe_ep_group">get_moe_ep_group</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.get_moe_expert_parallel_rank" href="#sglang.srt.distributed.parallel_state.get_moe_expert_parallel_rank">get_moe_expert_parallel_rank</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.get_moe_expert_parallel_world_size" href="#sglang.srt.distributed.parallel_state.get_moe_expert_parallel_world_size">get_moe_expert_parallel_world_size</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.get_moe_tensor_parallel_rank" href="#sglang.srt.distributed.parallel_state.get_moe_tensor_parallel_rank">get_moe_tensor_parallel_rank</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.get_moe_tensor_parallel_world_size" href="#sglang.srt.distributed.parallel_state.get_moe_tensor_parallel_world_size">get_moe_tensor_parallel_world_size</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.get_moe_tp_group" href="#sglang.srt.distributed.parallel_state.get_moe_tp_group">get_moe_tp_group</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.get_pipeline_model_parallel_group" href="#sglang.srt.distributed.parallel_state.get_pipeline_model_parallel_group">get_pipeline_model_parallel_group</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.get_pp_group" href="#sglang.srt.distributed.parallel_state.get_pp_group">get_pp_group</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.get_tensor_model_parallel_group" href="#sglang.srt.distributed.parallel_state.get_tensor_model_parallel_group">get_tensor_model_parallel_group</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.get_tensor_model_parallel_rank" href="#sglang.srt.distributed.parallel_state.get_tensor_model_parallel_rank">get_tensor_model_parallel_rank</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.get_tensor_model_parallel_world_size" href="#sglang.srt.distributed.parallel_state.get_tensor_model_parallel_world_size">get_tensor_model_parallel_world_size</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.get_tp_group" href="#sglang.srt.distributed.parallel_state.get_tp_group">get_tp_group</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.get_world_group" href="#sglang.srt.distributed.parallel_state.get_world_group">get_world_group</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.graph_capture" href="#sglang.srt.distributed.parallel_state.graph_capture">graph_capture</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.in_the_same_node_as" href="#sglang.srt.distributed.parallel_state.in_the_same_node_as">in_the_same_node_as</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.init_distributed_environment" href="#sglang.srt.distributed.parallel_state.init_distributed_environment">init_distributed_environment</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.init_model_parallel_group" href="#sglang.srt.distributed.parallel_state.init_model_parallel_group">init_model_parallel_group</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.init_world_group" href="#sglang.srt.distributed.parallel_state.init_world_group">init_world_group</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.initialize_model_parallel" href="#sglang.srt.distributed.parallel_state.initialize_model_parallel">initialize_model_parallel</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.inplace_all_reduce" href="#sglang.srt.distributed.parallel_state.inplace_all_reduce">inplace_all_reduce</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.inplace_all_reduce_fake" href="#sglang.srt.distributed.parallel_state.inplace_all_reduce_fake">inplace_all_reduce_fake</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.model_parallel_is_initialized" href="#sglang.srt.distributed.parallel_state.model_parallel_is_initialized">model_parallel_is_initialized</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.monkey_patch_vllm_parallel_state" href="#sglang.srt.distributed.parallel_state.monkey_patch_vllm_parallel_state">monkey_patch_vllm_parallel_state</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.outplace_all_reduce" href="#sglang.srt.distributed.parallel_state.outplace_all_reduce">outplace_all_reduce</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.outplace_all_reduce_fake" href="#sglang.srt.distributed.parallel_state.outplace_all_reduce_fake">outplace_all_reduce_fake</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.patch_tensor_parallel_group" href="#sglang.srt.distributed.parallel_state.patch_tensor_parallel_group">patch_tensor_parallel_group</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.reg_all_gather_into_tensor" href="#sglang.srt.distributed.parallel_state.reg_all_gather_into_tensor">reg_all_gather_into_tensor</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.reg_all_gather_into_tensor_fake" href="#sglang.srt.distributed.parallel_state.reg_all_gather_into_tensor_fake">reg_all_gather_into_tensor_fake</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.set_custom_all_reduce" href="#sglang.srt.distributed.parallel_state.set_custom_all_reduce">set_custom_all_reduce</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.set_mscclpp_all_reduce" href="#sglang.srt.distributed.parallel_state.set_mscclpp_all_reduce">set_mscclpp_all_reduce</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.set_pdmux_status" href="#sglang.srt.distributed.parallel_state.set_pdmux_status">set_pdmux_status</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.distributed.parallel_state.GraphCaptureContext" href="#sglang.srt.distributed.parallel_state.GraphCaptureContext">GraphCaptureContext</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.distributed.parallel_state.GraphCaptureContext.stream" href="#sglang.srt.distributed.parallel_state.GraphCaptureContext.stream">stream</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator" href="#sglang.srt.distributed.parallel_state.GroupCoordinator">GroupCoordinator</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.all_gather" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.all_gather">all_gather</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.all_gather_into_tensor" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.all_gather_into_tensor">all_gather_into_tensor</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.all_gather_object" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.all_gather_object">all_gather_object</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.all_gatherv" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.all_gatherv">all_gatherv</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.all_reduce" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.all_reduce">all_reduce</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.barrier" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.barrier">barrier</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.broadcast" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.broadcast">broadcast</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.broadcast_object" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.broadcast_object">broadcast_object</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.broadcast_object_list" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.broadcast_object_list">broadcast_object_list</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.broadcast_tensor_dict" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.broadcast_tensor_dict">broadcast_tensor_dict</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.ca_comm" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.ca_comm">ca_comm</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.cpu_group" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.cpu_group">cpu_group</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.destroy" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.destroy">destroy</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.device_group" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.device_group">device_group</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.first_rank" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.first_rank">first_rank</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.gather" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.gather">gather</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.graph_capture" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.graph_capture">graph_capture</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.is_first_rank" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.is_first_rank">is_first_rank</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.is_last_rank" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.is_last_rank">is_last_rank</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.last_rank" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.last_rank">last_rank</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.local_rank" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.local_rank">local_rank</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.mq_broadcaster" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.mq_broadcaster">mq_broadcaster</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.next_rank" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.next_rank">next_rank</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.prev_rank" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.prev_rank">prev_rank</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.pynccl_comm" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.pynccl_comm">pynccl_comm</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.rank" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.rank">rank</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.rank_in_group" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.rank_in_group">rank_in_group</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.ranks" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.ranks">ranks</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.recv" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.recv">recv</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.recv_object" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.recv_object">recv_object</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.recv_tensor_dict" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.recv_tensor_dict">recv_tensor_dict</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.reduce_scatter" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.reduce_scatter">reduce_scatter</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.reduce_scatter_tensor" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.reduce_scatter_tensor">reduce_scatter_tensor</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.reduce_scatterv" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.reduce_scatterv">reduce_scatterv</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.send" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.send">send</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.send_object" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.send_object">send_object</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.send_tensor_dict" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.send_tensor_dict">send_tensor_dict</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.use_custom_allreduce" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.use_custom_allreduce">use_custom_allreduce</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.use_message_queue_broadcaster" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.use_message_queue_broadcaster">use_message_queue_broadcaster</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.use_pymscclpp" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.use_pymscclpp">use_pymscclpp</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.use_pynccl" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.use_pynccl">use_pynccl</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.GroupCoordinator.world_size" href="#sglang.srt.distributed.parallel_state.GroupCoordinator.world_size">world_size</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.distributed.parallel_state.TensorMetadata" href="#sglang.srt.distributed.parallel_state.TensorMetadata">TensorMetadata</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.distributed.parallel_state.TensorMetadata.device" href="#sglang.srt.distributed.parallel_state.TensorMetadata.device">device</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.TensorMetadata.dtype" href="#sglang.srt.distributed.parallel_state.TensorMetadata.dtype">dtype</a></code></li>
<li><code><a title="sglang.srt.distributed.parallel_state.TensorMetadata.size" href="#sglang.srt.distributed.parallel_state.TensorMetadata.size">size</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
