<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.configs.model_config API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.configs.model_config</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.configs.model_config.get_hybrid_layer_ids"><code class="name flex">
<span>def <span class="ident">get_hybrid_layer_ids</span></span>(<span>model_architectures: List[str], num_hidden_layers: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_hybrid_layer_ids(model_architectures: List[str], num_hidden_layers: int):
    if &#34;Llama4ForConditionalGeneration&#34; in model_architectures:
        swa_attention_layer_ids = [
            i for i in range(num_hidden_layers) if (i + 1) % 4 != 0
        ]
        full_attention_layer_ids = [
            i for i in range(num_hidden_layers) if (i + 1) % 4 == 0
        ]
    else:
        swa_attention_layer_ids = None
        full_attention_layer_ids = None
    return swa_attention_layer_ids, full_attention_layer_ids</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.model_config.is_audio_model"><code class="name flex">
<span>def <span class="ident">is_audio_model</span></span>(<span>model_architectures: List[str])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_audio_model(model_architectures: List[str]):
    return False</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.model_config.is_encoder_decoder_model"><code class="name flex">
<span>def <span class="ident">is_encoder_decoder_model</span></span>(<span>model_architectures: List[str])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_encoder_decoder_model(model_architectures: List[str]):
    return &#34;MllamaForConditionalGeneration&#34; in model_architectures</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.model_config.is_generation_model"><code class="name flex">
<span>def <span class="ident">is_generation_model</span></span>(<span>model_architectures: List[str], is_embedding: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_generation_model(model_architectures: List[str], is_embedding: bool = False):
    # We have two ways to determine whether a model is a generative model.
    # 1. Check the model architecture
    # 2. check the `is_embedding` server args

    if (
        &#34;LlamaEmbeddingModel&#34; in model_architectures
        or &#34;MistralModel&#34; in model_architectures
        or &#34;LlamaForSequenceClassification&#34; in model_architectures
        or &#34;LlamaForSequenceClassificationWithNormal_Weights&#34; in model_architectures
        or &#34;InternLM2ForRewardModel&#34; in model_architectures
        or &#34;Qwen2ForRewardModel&#34; in model_architectures
        or &#34;Qwen2ForSequenceClassification&#34; in model_architectures
        or &#34;Qwen3ForSequenceClassification&#34; in model_architectures
        or &#34;CLIPModel&#34; in model_architectures
        or &#34;BertModel&#34; in model_architectures
        or &#34;Contriever&#34; in model_architectures
        or &#34;BertForSequenceClassification&#34; in model_architectures
        or &#34;XLMRobertaModel&#34; in model_architectures
        or &#34;XLMRobertaForSequenceClassification&#34; in model_architectures
    ):
        return False
    else:
        return not is_embedding</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.model_config.is_hybrid_model"><code class="name flex">
<span>def <span class="ident">is_hybrid_model</span></span>(<span>model_architectures: List[str],<br>hybrid_kvcache_ratio: float | None,<br>context_length: int | None,<br>attention_chunk_size: int | None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_hybrid_model(
    model_architectures: List[str],
    hybrid_kvcache_ratio: Optional[float],
    context_length: Optional[int],
    attention_chunk_size: Optional[int],
):
    if hybrid_kvcache_ratio is None:
        return None
    elif (
        hybrid_kvcache_ratio &gt; 0
        and model_architectures[0] == &#34;Llama4ForConditionalGeneration&#34;
        and context_length &gt; attention_chunk_size
    ):
        return hybrid_kvcache_ratio
    else:
        return None</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.model_config.is_image_gen_model"><code class="name flex">
<span>def <span class="ident">is_image_gen_model</span></span>(<span>model_architectures: List[str])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_image_gen_model(model_architectures: List[str]):
    return False</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.model_config.is_multimodal_chunked_prefill_supported"><code class="name flex">
<span>def <span class="ident">is_multimodal_chunked_prefill_supported</span></span>(<span>model_architectures: List[str])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_multimodal_chunked_prefill_supported(model_architectures: List[str]):
    &#34;&#34;&#34;Check if chunked prefill is supported for a MultiModal model.&#34;&#34;&#34;
    unsupported = [
        &#34;Grok1VForCausalLM&#34;,
        &#34;Grok1AForCausalLM&#34;,
        &#34;LlavaLlamaForCausalLM&#34;,
        &#34;MllamaForConditionalGeneration&#34;,
        &#34;CLIPModel&#34;,
    ]
    if any(multi_model_arch in unsupported for multi_model_arch in model_architectures):
        return False
    else:
        return True</code></pre>
</details>
<div class="desc"><p>Check if chunked prefill is supported for a MultiModal model.</p></div>
</dd>
<dt id="sglang.srt.configs.model_config.is_multimodal_gen_model"><code class="name flex">
<span>def <span class="ident">is_multimodal_gen_model</span></span>(<span>model_architectures: List[str])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_multimodal_gen_model(model_architectures: List[str]):
    return False</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.model_config.is_multimodal_model"><code class="name flex">
<span>def <span class="ident">is_multimodal_model</span></span>(<span>model_architectures: List[str])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_multimodal_model(model_architectures: List[str]):
    if any(
        multi_model_arch in model_architectures
        for multi_model_arch in multimodal_model_archs
    ):
        return True
    else:
        return False</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.model_config.yarn_get_mscale"><code class="name flex">
<span>def <span class="ident">yarn_get_mscale</span></span>(<span>scale: float = 1, mscale: float = 1) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def yarn_get_mscale(scale: float = 1, mscale: float = 1) -&gt; float:
    if scale &lt;= 1:
        return 1.0
    return 0.1 * mscale * math.log(scale) + 1.0</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.configs.model_config.AttentionArch"><code class="flex name class">
<span>class <span class="ident">AttentionArch</span></span>
<span>(</span><span>*args, **kwds)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AttentionArch(IntEnum):
    MLA = auto()
    MHA = auto()</code></pre>
</details>
<div class="desc"><p>Enum where members are also (and must be) ints</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.IntEnum</li>
<li>builtins.int</li>
<li>enum.ReprEnum</li>
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.configs.model_config.AttentionArch.MHA"><code class="name">var <span class="ident">MHA</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.model_config.AttentionArch.MLA"><code class="name">var <span class="ident">MLA</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.configs.model_config.ModelConfig"><code class="flex name class">
<span>class <span class="ident">ModelConfig</span></span>
<span>(</span><span>model_path: str,<br>trust_remote_code: bool = True,<br>revision: str | None = None,<br>context_length: int | None = None,<br>model_override_args: str = '{}',<br>is_embedding: bool | None = None,<br>enable_multimodal: bool | None = None,<br>dtype: str = 'auto',<br>quantization: str | None = None,<br>override_config_file: str | None = None,<br>is_draft_model: bool = False,<br>hybrid_kvcache_ratio: float | None = None,<br>model_impl: str | <a title="sglang.srt.configs.model_config.ModelImpl" href="#sglang.srt.configs.model_config.ModelImpl">ModelImpl</a> = ModelImpl.AUTO)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelConfig:
    def __init__(
        self,
        model_path: str,
        trust_remote_code: bool = True,
        revision: Optional[str] = None,
        context_length: Optional[int] = None,
        model_override_args: str = &#34;{}&#34;,
        is_embedding: Optional[bool] = None,
        enable_multimodal: Optional[bool] = None,
        dtype: str = &#34;auto&#34;,
        quantization: Optional[str] = None,
        override_config_file: Optional[str] = None,
        is_draft_model: bool = False,
        hybrid_kvcache_ratio: Optional[float] = None,
        model_impl: Union[str, ModelImpl] = ModelImpl.AUTO,
    ) -&gt; None:
        # Parse args
        self.model_path = model_path
        self.revision = revision
        self.quantization = quantization
        self.model_impl = model_impl

        self.maybe_pull_model_tokenizer_from_remote()
        self.model_override_args = json.loads(model_override_args)
        kwargs = {}
        if override_config_file and override_config_file.strip():
            kwargs[&#34;_configuration_file&#34;] = override_config_file.strip()

        self.hf_config = get_config(
            self.model_path,
            trust_remote_code=trust_remote_code,
            revision=revision,
            model_override_args=self.model_override_args,
            **kwargs,
        )

        self.hf_generation_config = get_generation_config(
            self.model_path,
            trust_remote_code=trust_remote_code,
            revision=revision,
            **kwargs,
        )

        self.hf_text_config = get_hf_text_config(self.hf_config)
        self.attention_chunk_size = getattr(
            self.hf_text_config, &#34;attention_chunk_size&#34;, None
        )
        self.is_hybrid = is_hybrid_model(
            self.hf_config.architectures,
            hybrid_kvcache_ratio=hybrid_kvcache_ratio,
            context_length=context_length,
            attention_chunk_size=self.attention_chunk_size,
        )
        if self.is_hybrid is not None:
            self.swa_attention_layer_ids, self.full_attention_layer_ids = (
                get_hybrid_layer_ids(
                    self.hf_config.architectures, self.hf_text_config.num_hidden_layers
                )
            )

        if enable_multimodal is None:
            mm_disabled_models = [
                &#34;Gemma3ForConditionalGeneration&#34;,
                &#34;Llama4ForConditionalGeneration&#34;,
                &#34;Step3VLForConditionalGeneration&#34;,
            ]
            if self.hf_config.architectures[0] in mm_disabled_models:
                enable_multimodal = False
                logger.info(
                    f&#34;Multimodal is disabled for {self.hf_config.model_type}. To enable it, set --enable-multimodal.&#34;
                )
            else:
                enable_multimodal = True

        if (
            is_draft_model
            and self.hf_config.architectures[0] == &#34;DeepseekV3ForCausalLM&#34;
        ):
            self.hf_config.architectures[0] = &#34;DeepseekV3ForCausalLMNextN&#34;

        if is_draft_model and self.hf_config.architectures[0] == &#34;Glm4MoeForCausalLM&#34;:
            self.hf_config.architectures[0] = &#34;Glm4MoeForCausalLMNextN&#34;

        if (
            is_draft_model
            and self.hf_config.architectures[0] == &#34;LongcatFlashForCausalLM&#34;
        ):
            self.hf_config.architectures[0] = &#34;LongcatFlashForCausalLMNextN&#34;
            self.hf_config.num_hidden_layers = self.hf_config.num_nextn_predict_layers

        if is_draft_model and self.hf_config.architectures[0] == &#34;MiMoForCausalLM&#34;:
            self.hf_config.architectures[0] = &#34;MiMoMTP&#34;
        if (
            is_draft_model
            and self.hf_config.architectures[0] == &#34;Ernie4_5_MoeForCausalLM&#34;
        ):
            self.hf_config.architectures[0] = &#34;Ernie4_5_MoeForCausalLMMTP&#34;

        # Check model type
        self.is_generation = is_generation_model(
            self.hf_config.architectures, is_embedding
        )
        self.is_multimodal = enable_multimodal and is_multimodal_model(
            self.hf_config.architectures
        )
        self.is_multimodal_gen = enable_multimodal and is_multimodal_gen_model(
            self.hf_config.architectures
        )
        self.is_image_gen = enable_multimodal and is_image_gen_model(
            self.hf_config.architectures
        )
        self.is_audio_model = enable_multimodal and is_audio_model(
            self.hf_config.architectures
        )
        self.is_multimodal_chunked_prefill_supported = (
            enable_multimodal
            and is_multimodal_chunked_prefill_supported(self.hf_config.architectures)
        )
        self.is_encoder_decoder = is_encoder_decoder_model(self.hf_config.architectures)
        self.dtype = _get_and_verify_dtype(self.hf_text_config, dtype)

        # Derive context length
        derived_context_len = get_context_length(self.hf_text_config)
        if context_length is not None:
            if context_length &gt; derived_context_len:
                reason = &#34;Target model&#39;s&#34; if is_draft_model else &#34;User-specified&#34;
                msg = (
                    f&#34;Warning: {reason} context_length ({context_length}) is greater than the derived context_length ({derived_context_len}). &#34;
                    f&#34;This may lead to incorrect model outputs or CUDA errors. Note that the derived context_length may differ from max_position_embeddings in the model&#39;s config.&#34;
                )
                if (
                    get_bool_env_var(&#34;SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN&#34;)
                    or is_in_ci()  # FIXME: fix this special case
                ):
                    logger.warning(msg)
                    self.context_len = context_length
                else:
                    raise ValueError(
                        f&#34;{msg} To allow overriding this maximum, set the env var SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1&#34;
                    )
            else:
                self.context_len = context_length
        else:
            self.context_len = derived_context_len

        # Unify the config keys for hf_text_config
        self.head_dim = getattr(
            self.hf_text_config,
            &#34;head_dim&#34;,
            self.hf_text_config.hidden_size // self.hf_text_config.num_attention_heads,
        )

        # FIXME: temporary special judge for MLA architecture
        if (
            &#34;DeepseekV2ForCausalLM&#34; in self.hf_config.architectures
            or &#34;DeepseekV3ForCausalLM&#34; in self.hf_config.architectures
            or &#34;DeepseekV3ForCausalLMNextN&#34; in self.hf_config.architectures
            or &#34;LongcatFlashForCausalLM&#34; in self.hf_config.architectures
            or &#34;LongcatFlashForCausalLMNextN&#34; in self.hf_config.architectures
        ):
            self.head_dim = 256
            self.attention_arch = AttentionArch.MLA
            self.kv_lora_rank = self.hf_config.kv_lora_rank
            self.qk_nope_head_dim = self.hf_config.qk_nope_head_dim
            self.qk_rope_head_dim = self.hf_config.qk_rope_head_dim
            self.v_head_dim = self.hf_config.v_head_dim

            # Handle rope scaling with yarn
            self.scaling = 1 / math.sqrt(self.qk_nope_head_dim + self.qk_rope_head_dim)
            if self.hf_config.rope_scaling:
                mscale_all_dim = self.hf_config.rope_scaling.get(
                    &#34;mscale_all_dim&#34;, False
                )
                scaling_factor = self.hf_config.rope_scaling[&#34;factor&#34;]
                mscale = yarn_get_mscale(scaling_factor, float(mscale_all_dim))
                self.scaling = self.scaling * mscale * mscale

        elif &#34;MiniCPM3ForCausalLM&#34; in self.hf_config.architectures:
            self.head_dim = 128
            self.attention_arch = AttentionArch.MLA
            self.kv_lora_rank = self.hf_config.kv_lora_rank
            self.qk_rope_head_dim = self.hf_config.qk_rope_head_dim
        elif &#34;DeepseekVL2ForCausalLM&#34; in self.hf_config.architectures and getattr(
            self.hf_text_config, &#34;use_mla&#34;, True
        ):
            self.head_dim = 256
            self.attention_arch = AttentionArch.MLA
            self.kv_lora_rank = self.hf_text_config.kv_lora_rank
            self.qk_rope_head_dim = self.hf_text_config.qk_rope_head_dim
        elif &#34;KimiVLForConditionalGeneration&#34; in self.hf_config.architectures:
            self.head_dim = 256
            self.attention_arch = AttentionArch.MLA
            self.kv_lora_rank = self.hf_text_config.kv_lora_rank
            self.qk_rope_head_dim = self.hf_text_config.qk_rope_head_dim
            self.v_head_dim = self.hf_text_config.v_head_dim
            self.qk_nope_head_dim = self.hf_text_config.qk_nope_head_dim
        else:
            if (
                &#34;MistralModel&#34; in self.hf_config.architectures
                or &#34;MixtralForCausalLM&#34; in self.hf_config.architectures
                or &#34;MistralForCausalLM&#34; in self.hf_config.architectures
            ):
                if getattr(self, &#34;head_dim&#34;, None) is None:
                    self.head_dim = (
                        self.hf_config.hidden_size // self.hf_config.num_attention_heads
                    )
                    # In transformers==4.52.3, the head_dim is null in MistralConfig
                    if (
                        not hasattr(self.hf_text_config, &#34;head_dim&#34;)
                        or self.hf_text_config.head_dim is None
                    ):
                        setattr(self.hf_text_config, &#34;head_dim&#34;, self.head_dim)

            self.attention_arch = AttentionArch.MHA

        self.num_attention_heads = self.hf_text_config.num_attention_heads
        self.num_key_value_heads = getattr(
            self.hf_text_config, &#34;num_key_value_heads&#34;, None
        )

        # for Dbrx and MPT models
        if self.hf_config.model_type in [&#34;dbrx&#34;, &#34;mpt&#34;]:
            self.num_key_value_heads = getattr(
                self.hf_config.attn_config, &#34;kv_n_heads&#34;, None
            )

        if self.num_key_value_heads is None:
            self.num_key_value_heads = self.num_attention_heads
        self.hidden_size = self.hf_text_config.hidden_size
        self.num_hidden_layers = self.hf_text_config.num_hidden_layers
        self.num_attention_layers = self.num_hidden_layers
        if &#34;LongcatFlashForCausalLM&#34; in self.hf_config.architectures:
            self.num_attention_layers = self.num_hidden_layers * 2
        self.num_nextn_predict_layers = getattr(
            self.hf_text_config, &#34;num_nextn_predict_layers&#34;, None
        )
        self.vocab_size = self.hf_text_config.vocab_size

        # Verify quantization
        self._verify_quantization()

        # Verify dual-chunk attention config
        self._verify_dual_chunk_attention_config()

        # Cache attributes
        self.hf_eos_token_id = self.get_hf_eos_token_id()

        # multimodal
        self.image_token_id = getattr(
            self.hf_config, &#34;image_token_id&#34;, None
        ) or getattr(self.hf_config, &#34;image_token_index&#34;, None)

    @staticmethod
    def from_server_args(server_args: ServerArgs, model_path: str = None, **kwargs):
        return ModelConfig(
            model_path=model_path or server_args.model_path,
            trust_remote_code=server_args.trust_remote_code,
            revision=server_args.revision,
            context_length=server_args.context_length,
            model_override_args=server_args.json_model_override_args,
            is_embedding=server_args.is_embedding,
            enable_multimodal=server_args.enable_multimodal,
            dtype=server_args.dtype,
            quantization=server_args.quantization,
            hybrid_kvcache_ratio=server_args.hybrid_kvcache_ratio,
            model_impl=server_args.model_impl,
            **kwargs,
        )

    def get_total_num_attention_heads(self) -&gt; int:
        return self.num_attention_heads

    def get_num_attention_heads(self, tensor_parallel_size) -&gt; int:
        total_num_attention_heads = self.num_attention_heads
        return max(1, total_num_attention_heads // tensor_parallel_size)

    # adapted from https://github.com/vllm-project/vllm/blob/main/vllm/config.py#L289
    def get_total_num_kv_heads(self) -&gt; int:
        &#34;&#34;&#34;Returns the total number of KV heads.&#34;&#34;&#34;
        # For GPTBigCode &amp; Falcon:
        # NOTE: for falcon, when new_decoder_architecture is True, the
        # multi_query flag is ignored and we use n_head_kv for the number of
        # KV heads.
        falcon_model_types = [&#34;falcon&#34;, &#34;RefinedWeb&#34;, &#34;RefinedWebModel&#34;]
        new_decoder_arch_falcon = (
            self.hf_config.model_type in falcon_model_types
            and getattr(self.hf_config, &#34;new_decoder_architecture&#34;, False)
        )
        if not new_decoder_arch_falcon and getattr(
            self.hf_text_config, &#34;multi_query&#34;, False
        ):
            # Multi-query attention, only one KV head.
            # Currently, tensor parallelism is not supported in this case.
            return 1

        # For DBRX and MPT
        if self.hf_config.model_type in [&#34;mpt&#34;]:
            if &#34;kv_n_heads&#34; in self.hf_config.attn_config:
                return self.hf_config.attn_config[&#34;kv_n_heads&#34;]
            return self.hf_config.num_attention_heads
        if self.hf_config.model_type in [&#34;dbrx&#34;]:
            return getattr(
                self.hf_config.attn_config,
                &#34;kv_n_heads&#34;,
                self.hf_config.num_attention_heads,
            )
        if self.hf_config.model_type in [&#34;nemotron-nas&#34;]:
            nkvh = {
                self.hf_config.num_attention_heads // block.attention.n_heads_in_group
                for block in self.hf_config.block_configs
                if not block.attention.no_op
            }
            if len(nkvh) == 0:
                raise RuntimeError(&#34;Couldn&#39;t determine number of kv heads&#34;)
            if len(nkvh) &gt; 1:
                raise ValueError(
                    &#34;Variable GQA (VGQA) is not yet supported for nemotron-nas in sglang&#34;
                )
            return next(iter(nkvh))

        attributes = [
            # For Falcon:
            &#34;n_head_kv&#34;,
            &#34;num_kv_heads&#34;,
            # For LLaMA-2:
            &#34;num_key_value_heads&#34;,
            # For ChatGLM:
            &#34;multi_query_group_num&#34;,
            # For Step3
            &#34;num_attention_groups&#34;,
        ]
        for attr in attributes:
            num_kv_heads = getattr(self.hf_text_config, attr, None)
            if num_kv_heads is not None:
                return num_kv_heads

        # For non-grouped-query attention models, the number of KV heads is
        # equal to the number of attention heads.
        return self.hf_text_config.num_attention_heads

    def get_num_kv_heads(self, tensor_parallel_size) -&gt; int:
        &#34;&#34;&#34;Returns the number of KV heads per GPU.&#34;&#34;&#34;
        total_num_kv_heads = self.get_total_num_kv_heads()
        # If tensor parallelism is used, we divide the number of KV heads by
        # the tensor parallel size. We will replicate the KV heads in the
        # case where the number of KV heads is smaller than the tensor
        # parallel size so each GPU has at least one KV head.
        return max(1, total_num_kv_heads // tensor_parallel_size)

    # adapted from https://github.com/vllm-project/vllm/blob/v0.6.4.post1/vllm/config.py
    def _parse_quant_hf_config(self):
        quant_cfg = getattr(self.hf_config, &#34;quantization_config&#34;, None)
        if quant_cfg is None:
            # compressed-tensors uses a &#34;compression_config&#34; key
            quant_cfg = getattr(self.hf_config, &#34;compression_config&#34;, None)
        if quant_cfg is None:
            # check if is modelopt model -- modelopt doesn&#39;t have corresponding field
            # in hf `config.json` but has a standalone `hf_quant_config.json` in the root directory
            # example: https://huggingface.co/nvidia/Llama-3.1-8B-Instruct-FP8/tree/main
            is_local = os.path.exists(self.model_path)
            modelopt_quant_config = {&#34;quant_method&#34;: &#34;modelopt&#34;}
            if not is_local:
                from huggingface_hub import HfApi

                hf_api = HfApi()
                if hf_api.file_exists(self.model_path, &#34;hf_quant_config.json&#34;):
                    quant_cfg = modelopt_quant_config
            elif os.path.exists(os.path.join(self.model_path, &#34;hf_quant_config.json&#34;)):
                quant_config_file = os.path.join(
                    self.model_path, &#34;hf_quant_config.json&#34;
                )
                with open(quant_config_file) as f:
                    quant_config_dict = json.load(f)
                json_quant_configs = quant_config_dict[&#34;quantization&#34;]
                quant_algo = json_quant_configs.get(&#34;quant_algo&#34;, None)
                if quant_algo == &#34;MIXED_PRECISION&#34;:
                    quant_cfg = {&#34;quant_method&#34;: &#34;w4afp8&#34;}
                else:
                    quant_cfg = modelopt_quant_config
        return quant_cfg

    # adapted from https://github.com/vllm-project/vllm/blob/v0.6.4.post1/vllm/config.py
    def _verify_quantization(self) -&gt; None:
        supported_quantization = [*QUANTIZATION_METHODS]
        rocm_supported_quantization = [
            &#34;awq&#34;,
            &#34;gptq&#34;,
            &#34;fp8&#34;,
            &#34;compressed_tensors&#34;,
            &#34;compressed-tensors&#34;,
            &#34;fbgemm_fp8&#34;,
            &#34;w8a8_fp8&#34;,
            &#34;petit_nvfp4&#34;,
            &#34;quark&#34;,
            &#34;mxfp4&#34;,
        ]
        optimized_quantization_methods = [
            &#34;fp8&#34;,
            &#34;marlin&#34;,
            &#34;modelopt&#34;,
            &#34;gptq_marlin_24&#34;,
            &#34;gptq_marlin&#34;,
            &#34;awq_marlin&#34;,
            &#34;fbgemm_fp8&#34;,
            &#34;compressed_tensors&#34;,
            &#34;compressed-tensors&#34;,
            &#34;experts_int8&#34;,
            &#34;w8a8_int8&#34;,
            &#34;w8a8_fp8&#34;,
            &#34;moe_wna16&#34;,
            &#34;qoq&#34;,
            &#34;w4afp8&#34;,
            &#34;petit_nvfp4&#34;,
        ]
        compatible_quantization_methods = {
            &#34;modelopt_fp4&#34;: [&#34;modelopt&#34;],
            &#34;petit_nvfp4&#34;: [&#34;modelopt&#34;],
            &#34;w8a8_int8&#34;: [&#34;compressed-tensors&#34;, &#34;compressed_tensors&#34;],
            &#34;w8a8_fp8&#34;: [&#34;compressed-tensors&#34;, &#34;compressed_tensors&#34;],
        }
        if self.quantization is not None:
            self.quantization = self.quantization.lower()

        # Parse quantization method from the HF model config, if available.
        quant_cfg = self._parse_quant_hf_config()

        if quant_cfg is not None:
            quant_method = quant_cfg.get(
                &#34;quant_method&#34;, &#34;&#34; if not self.quantization else self.quantization
            ).lower()

            # Detect which checkpoint is it
            for _, method in QUANTIZATION_METHODS.items():
                quantization_override = method.override_quantization_method(
                    quant_cfg, self.quantization
                )
                if quantization_override:
                    quant_method = quantization_override
                    self.quantization = quantization_override
                    break

            # Verify quantization configurations.
            if self.quantization is None:
                self.quantization = quant_method
            elif self.quantization != quant_method:
                if (
                    self.quantization not in compatible_quantization_methods
                    or quant_method
                    not in compatible_quantization_methods[self.quantization]
                ):
                    raise ValueError(
                        &#34;Quantization method specified in the model config &#34;
                        f&#34;({quant_method}) does not match the quantization &#34;
                        f&#34;method specified in the `quantization` argument &#34;
                        f&#34;({self.quantization}).&#34;
                    )

        if self.quantization is not None:
            if self.quantization not in supported_quantization:
                raise ValueError(
                    f&#34;Unknown quantization method: {self.quantization}. Must &#34;
                    f&#34;be one of {supported_quantization}.&#34;
                )
            if is_hip() and self.quantization not in rocm_supported_quantization:
                raise ValueError(
                    f&#34;{self.quantization} quantization is currently not &#34;
                    f&#34;supported in ROCm.&#34;
                )
            if self.quantization not in optimized_quantization_methods:
                logger.warning(
                    &#34;%s quantization is not fully &#34;
                    &#34;optimized yet. The speed can be slower than &#34;
                    &#34;non-quantized models.&#34;,
                    self.quantization,
                )

    def _verify_dual_chunk_attention_config(self) -&gt; None:
        if hasattr(self.hf_config, &#34;dual_chunk_attention_config&#34;):
            # Try loading the sparse attention config
            sparse_attn_config = get_sparse_attention_config(self.model_path)
            if not sparse_attn_config:
                return
            self.hf_config.dual_chunk_attention_config[&#34;sparse_attention_config&#34;] = (
                sparse_attn_config
            )
            if (
                &#34;sparse_attention_enabled&#34;
                not in self.hf_config.dual_chunk_attention_config
            ):
                self.hf_config.dual_chunk_attention_config[
                    &#34;sparse_attention_enabled&#34;
                ] = True

    def get_hf_eos_token_id(self) -&gt; Optional[Set[int]]:
        eos_ids = getattr(self.hf_config, &#34;eos_token_id&#34;, None)
        if eos_ids is not None:
            # it can be either int or list of int
            eos_ids = {eos_ids} if isinstance(eos_ids, int) else set(eos_ids)
        if eos_ids is None:
            eos_ids = set()
        if self.hf_generation_config:
            generation_eos_ids = getattr(
                self.hf_generation_config, &#34;eos_token_id&#34;, None
            )
            if generation_eos_ids:
                generation_eos_ids = (
                    {generation_eos_ids}
                    if isinstance(generation_eos_ids, int)
                    else set(generation_eos_ids)
                )
                eos_ids = eos_ids | generation_eos_ids
        return eos_ids

    def maybe_pull_model_tokenizer_from_remote(self) -&gt; None:
        &#34;&#34;&#34;
        Pull the model config files to a temporary
        directory in case of remote.

        Args:
            model: The model name or path.

        &#34;&#34;&#34;
        from sglang.srt.connector import create_remote_connector
        from sglang.srt.utils import is_remote_url

        if is_remote_url(self.model_path):
            logger.info(&#34;Pulling model configs from remote...&#34;)
            # BaseConnector implements __del__() to clean up the local dir.
            # Since config files need to exist all the time, so we DO NOT use
            # with statement to avoid closing the client.
            client = create_remote_connector(self.model_path)
            if is_remote_url(self.model_path):
                client.pull_files(allow_pattern=[&#34;*config.json&#34;])
                self.model_weights = self.model_path
                self.model_path = client.get_local_dir()</code></pre>
</details>
<div class="desc"></div>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.configs.model_config.ModelConfig.from_server_args"><code class="name flex">
<span>def <span class="ident">from_server_args</span></span>(<span>server_args: <a title="sglang.srt.server_args.ServerArgs" href="../server_args.html#sglang.srt.server_args.ServerArgs">ServerArgs</a>,<br>model_path: str = None,<br>**kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_server_args(server_args: ServerArgs, model_path: str = None, **kwargs):
    return ModelConfig(
        model_path=model_path or server_args.model_path,
        trust_remote_code=server_args.trust_remote_code,
        revision=server_args.revision,
        context_length=server_args.context_length,
        model_override_args=server_args.json_model_override_args,
        is_embedding=server_args.is_embedding,
        enable_multimodal=server_args.enable_multimodal,
        dtype=server_args.dtype,
        quantization=server_args.quantization,
        hybrid_kvcache_ratio=server_args.hybrid_kvcache_ratio,
        model_impl=server_args.model_impl,
        **kwargs,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.configs.model_config.ModelConfig.get_hf_eos_token_id"><code class="name flex">
<span>def <span class="ident">get_hf_eos_token_id</span></span>(<span>self) ‑> Set[int] | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_hf_eos_token_id(self) -&gt; Optional[Set[int]]:
    eos_ids = getattr(self.hf_config, &#34;eos_token_id&#34;, None)
    if eos_ids is not None:
        # it can be either int or list of int
        eos_ids = {eos_ids} if isinstance(eos_ids, int) else set(eos_ids)
    if eos_ids is None:
        eos_ids = set()
    if self.hf_generation_config:
        generation_eos_ids = getattr(
            self.hf_generation_config, &#34;eos_token_id&#34;, None
        )
        if generation_eos_ids:
            generation_eos_ids = (
                {generation_eos_ids}
                if isinstance(generation_eos_ids, int)
                else set(generation_eos_ids)
            )
            eos_ids = eos_ids | generation_eos_ids
    return eos_ids</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.model_config.ModelConfig.get_num_attention_heads"><code class="name flex">
<span>def <span class="ident">get_num_attention_heads</span></span>(<span>self, tensor_parallel_size) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_num_attention_heads(self, tensor_parallel_size) -&gt; int:
    total_num_attention_heads = self.num_attention_heads
    return max(1, total_num_attention_heads // tensor_parallel_size)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.model_config.ModelConfig.get_num_kv_heads"><code class="name flex">
<span>def <span class="ident">get_num_kv_heads</span></span>(<span>self, tensor_parallel_size) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_num_kv_heads(self, tensor_parallel_size) -&gt; int:
    &#34;&#34;&#34;Returns the number of KV heads per GPU.&#34;&#34;&#34;
    total_num_kv_heads = self.get_total_num_kv_heads()
    # If tensor parallelism is used, we divide the number of KV heads by
    # the tensor parallel size. We will replicate the KV heads in the
    # case where the number of KV heads is smaller than the tensor
    # parallel size so each GPU has at least one KV head.
    return max(1, total_num_kv_heads // tensor_parallel_size)</code></pre>
</details>
<div class="desc"><p>Returns the number of KV heads per GPU.</p></div>
</dd>
<dt id="sglang.srt.configs.model_config.ModelConfig.get_total_num_attention_heads"><code class="name flex">
<span>def <span class="ident">get_total_num_attention_heads</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_total_num_attention_heads(self) -&gt; int:
    return self.num_attention_heads</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.model_config.ModelConfig.get_total_num_kv_heads"><code class="name flex">
<span>def <span class="ident">get_total_num_kv_heads</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_total_num_kv_heads(self) -&gt; int:
    &#34;&#34;&#34;Returns the total number of KV heads.&#34;&#34;&#34;
    # For GPTBigCode &amp; Falcon:
    # NOTE: for falcon, when new_decoder_architecture is True, the
    # multi_query flag is ignored and we use n_head_kv for the number of
    # KV heads.
    falcon_model_types = [&#34;falcon&#34;, &#34;RefinedWeb&#34;, &#34;RefinedWebModel&#34;]
    new_decoder_arch_falcon = (
        self.hf_config.model_type in falcon_model_types
        and getattr(self.hf_config, &#34;new_decoder_architecture&#34;, False)
    )
    if not new_decoder_arch_falcon and getattr(
        self.hf_text_config, &#34;multi_query&#34;, False
    ):
        # Multi-query attention, only one KV head.
        # Currently, tensor parallelism is not supported in this case.
        return 1

    # For DBRX and MPT
    if self.hf_config.model_type in [&#34;mpt&#34;]:
        if &#34;kv_n_heads&#34; in self.hf_config.attn_config:
            return self.hf_config.attn_config[&#34;kv_n_heads&#34;]
        return self.hf_config.num_attention_heads
    if self.hf_config.model_type in [&#34;dbrx&#34;]:
        return getattr(
            self.hf_config.attn_config,
            &#34;kv_n_heads&#34;,
            self.hf_config.num_attention_heads,
        )
    if self.hf_config.model_type in [&#34;nemotron-nas&#34;]:
        nkvh = {
            self.hf_config.num_attention_heads // block.attention.n_heads_in_group
            for block in self.hf_config.block_configs
            if not block.attention.no_op
        }
        if len(nkvh) == 0:
            raise RuntimeError(&#34;Couldn&#39;t determine number of kv heads&#34;)
        if len(nkvh) &gt; 1:
            raise ValueError(
                &#34;Variable GQA (VGQA) is not yet supported for nemotron-nas in sglang&#34;
            )
        return next(iter(nkvh))

    attributes = [
        # For Falcon:
        &#34;n_head_kv&#34;,
        &#34;num_kv_heads&#34;,
        # For LLaMA-2:
        &#34;num_key_value_heads&#34;,
        # For ChatGLM:
        &#34;multi_query_group_num&#34;,
        # For Step3
        &#34;num_attention_groups&#34;,
    ]
    for attr in attributes:
        num_kv_heads = getattr(self.hf_text_config, attr, None)
        if num_kv_heads is not None:
            return num_kv_heads

    # For non-grouped-query attention models, the number of KV heads is
    # equal to the number of attention heads.
    return self.hf_text_config.num_attention_heads</code></pre>
</details>
<div class="desc"><p>Returns the total number of KV heads.</p></div>
</dd>
<dt id="sglang.srt.configs.model_config.ModelConfig.maybe_pull_model_tokenizer_from_remote"><code class="name flex">
<span>def <span class="ident">maybe_pull_model_tokenizer_from_remote</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def maybe_pull_model_tokenizer_from_remote(self) -&gt; None:
    &#34;&#34;&#34;
    Pull the model config files to a temporary
    directory in case of remote.

    Args:
        model: The model name or path.

    &#34;&#34;&#34;
    from sglang.srt.connector import create_remote_connector
    from sglang.srt.utils import is_remote_url

    if is_remote_url(self.model_path):
        logger.info(&#34;Pulling model configs from remote...&#34;)
        # BaseConnector implements __del__() to clean up the local dir.
        # Since config files need to exist all the time, so we DO NOT use
        # with statement to avoid closing the client.
        client = create_remote_connector(self.model_path)
        if is_remote_url(self.model_path):
            client.pull_files(allow_pattern=[&#34;*config.json&#34;])
            self.model_weights = self.model_path
            self.model_path = client.get_local_dir()</code></pre>
</details>
<div class="desc"><p>Pull the model config files to a temporary
directory in case of remote.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong></dt>
<dd>The model name or path.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.configs.model_config.ModelImpl"><code class="flex name class">
<span>class <span class="ident">ModelImpl</span></span>
<span>(</span><span>*args, **kwds)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelImpl(str, Enum):
    AUTO = &#34;auto&#34;
    SGLANG = &#34;sglang&#34;
    TRANSFORMERS = &#34;transformers&#34;</code></pre>
</details>
<div class="desc"><p>str(object='') -&gt; str
str(bytes_or_buffer[, encoding[, errors]]) -&gt; str</p>
<p>Create a new string object from the given object. If encoding or
errors is specified, then the object must expose a data buffer
that will be decoded using the given encoding and error handler.
Otherwise, returns the result of object.<strong>str</strong>() (if defined)
or repr(object).
encoding defaults to sys.getdefaultencoding().
errors defaults to 'strict'.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.str</li>
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.configs.model_config.ModelImpl.AUTO"><code class="name">var <span class="ident">AUTO</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.model_config.ModelImpl.SGLANG"><code class="name">var <span class="ident">SGLANG</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.model_config.ModelImpl.TRANSFORMERS"><code class="name">var <span class="ident">TRANSFORMERS</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.configs" href="index.html">sglang.srt.configs</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.configs.model_config.get_hybrid_layer_ids" href="#sglang.srt.configs.model_config.get_hybrid_layer_ids">get_hybrid_layer_ids</a></code></li>
<li><code><a title="sglang.srt.configs.model_config.is_audio_model" href="#sglang.srt.configs.model_config.is_audio_model">is_audio_model</a></code></li>
<li><code><a title="sglang.srt.configs.model_config.is_encoder_decoder_model" href="#sglang.srt.configs.model_config.is_encoder_decoder_model">is_encoder_decoder_model</a></code></li>
<li><code><a title="sglang.srt.configs.model_config.is_generation_model" href="#sglang.srt.configs.model_config.is_generation_model">is_generation_model</a></code></li>
<li><code><a title="sglang.srt.configs.model_config.is_hybrid_model" href="#sglang.srt.configs.model_config.is_hybrid_model">is_hybrid_model</a></code></li>
<li><code><a title="sglang.srt.configs.model_config.is_image_gen_model" href="#sglang.srt.configs.model_config.is_image_gen_model">is_image_gen_model</a></code></li>
<li><code><a title="sglang.srt.configs.model_config.is_multimodal_chunked_prefill_supported" href="#sglang.srt.configs.model_config.is_multimodal_chunked_prefill_supported">is_multimodal_chunked_prefill_supported</a></code></li>
<li><code><a title="sglang.srt.configs.model_config.is_multimodal_gen_model" href="#sglang.srt.configs.model_config.is_multimodal_gen_model">is_multimodal_gen_model</a></code></li>
<li><code><a title="sglang.srt.configs.model_config.is_multimodal_model" href="#sglang.srt.configs.model_config.is_multimodal_model">is_multimodal_model</a></code></li>
<li><code><a title="sglang.srt.configs.model_config.yarn_get_mscale" href="#sglang.srt.configs.model_config.yarn_get_mscale">yarn_get_mscale</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.configs.model_config.AttentionArch" href="#sglang.srt.configs.model_config.AttentionArch">AttentionArch</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.configs.model_config.AttentionArch.MHA" href="#sglang.srt.configs.model_config.AttentionArch.MHA">MHA</a></code></li>
<li><code><a title="sglang.srt.configs.model_config.AttentionArch.MLA" href="#sglang.srt.configs.model_config.AttentionArch.MLA">MLA</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.configs.model_config.ModelConfig" href="#sglang.srt.configs.model_config.ModelConfig">ModelConfig</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.configs.model_config.ModelConfig.from_server_args" href="#sglang.srt.configs.model_config.ModelConfig.from_server_args">from_server_args</a></code></li>
<li><code><a title="sglang.srt.configs.model_config.ModelConfig.get_hf_eos_token_id" href="#sglang.srt.configs.model_config.ModelConfig.get_hf_eos_token_id">get_hf_eos_token_id</a></code></li>
<li><code><a title="sglang.srt.configs.model_config.ModelConfig.get_num_attention_heads" href="#sglang.srt.configs.model_config.ModelConfig.get_num_attention_heads">get_num_attention_heads</a></code></li>
<li><code><a title="sglang.srt.configs.model_config.ModelConfig.get_num_kv_heads" href="#sglang.srt.configs.model_config.ModelConfig.get_num_kv_heads">get_num_kv_heads</a></code></li>
<li><code><a title="sglang.srt.configs.model_config.ModelConfig.get_total_num_attention_heads" href="#sglang.srt.configs.model_config.ModelConfig.get_total_num_attention_heads">get_total_num_attention_heads</a></code></li>
<li><code><a title="sglang.srt.configs.model_config.ModelConfig.get_total_num_kv_heads" href="#sglang.srt.configs.model_config.ModelConfig.get_total_num_kv_heads">get_total_num_kv_heads</a></code></li>
<li><code><a title="sglang.srt.configs.model_config.ModelConfig.maybe_pull_model_tokenizer_from_remote" href="#sglang.srt.configs.model_config.ModelConfig.maybe_pull_model_tokenizer_from_remote">maybe_pull_model_tokenizer_from_remote</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.configs.model_config.ModelImpl" href="#sglang.srt.configs.model_config.ModelImpl">ModelImpl</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.configs.model_config.ModelImpl.AUTO" href="#sglang.srt.configs.model_config.ModelImpl.AUTO">AUTO</a></code></li>
<li><code><a title="sglang.srt.configs.model_config.ModelImpl.SGLANG" href="#sglang.srt.configs.model_config.ModelImpl.SGLANG">SGLANG</a></code></li>
<li><code><a title="sglang.srt.configs.model_config.ModelImpl.TRANSFORMERS" href="#sglang.srt.configs.model_config.ModelImpl.TRANSFORMERS">TRANSFORMERS</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
