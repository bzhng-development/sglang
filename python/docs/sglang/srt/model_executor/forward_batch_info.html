<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.model_executor.forward_batch_info API documentation</title>
<meta name="description" content="Store information about a forward batch …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.model_executor.forward_batch_info</code></h1>
</header>
<section id="section-intro">
<p>Store information about a forward batch.</p>
<p>The following is the flow of data structures for a batch:</p>
<p>ScheduleBatch -&gt; ModelWorkerBatch -&gt; ForwardBatch</p>
<ul>
<li>ScheduleBatch is managed by <code>scheduler.py::Scheduler</code>.
It contains high-level scheduling data. Most of the data is on the CPU.</li>
<li>ModelWorkerBatch is managed by <code>tp_worker.py::TpModelWorker</code>.
It is a subset of <code>ScheduleBatch</code> that only contains data related to the model forward on GPU.
It will be transformed from CPU scheduler to GPU model runner.</li>
<li>ForwardBatch is managed by <code>model_runner.py::ModelRunner</code>.
It contains low-level tensor data. Most of the data consists of GPU tensors.</li>
</ul>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.model_executor.forward_batch_info.clamp_position"><code class="name flex">
<span>def <span class="ident">clamp_position</span></span>(<span>seq_lens)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.compile(dynamic=True, backend=get_compiler_backend(), disable=_is_npu)
def clamp_position(seq_lens):
    return torch.clamp((seq_lens - 1), min=0).to(torch.int64)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.compute_position"><code class="name flex">
<span>def <span class="ident">compute_position</span></span>(<span>attn_backend: str,<br>extend_prefix_lens: torch.Tensor,<br>extend_seq_lens: torch.Tensor,<br>extend_seq_lens_sum: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_position(
    attn_backend: str,
    extend_prefix_lens: torch.Tensor,
    extend_seq_lens: torch.Tensor,
    extend_seq_lens_sum: int,
):
    if support_triton(attn_backend):
        positions, extend_start_loc = compute_position_triton(
            extend_prefix_lens,
            extend_seq_lens,
            extend_seq_lens_sum,
        )
    else:
        positions, extend_start_loc = compute_position_torch(
            extend_prefix_lens, extend_seq_lens
        )
    return positions, extend_start_loc</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.compute_position_torch"><code class="name flex">
<span>def <span class="ident">compute_position_torch</span></span>(<span>extend_prefix_lens: torch.Tensor, extend_seq_lens: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_position_torch(
    extend_prefix_lens: torch.Tensor, extend_seq_lens: torch.Tensor
):
    positions = torch.cat(
        [
            torch.arange(
                prefix_len, prefix_len + extend_len, device=extend_prefix_lens.device
            )
            for prefix_len, extend_len in zip(extend_prefix_lens, extend_seq_lens)
        ],
        axis=0,
    )
    extend_start_loc = torch.zeros_like(extend_seq_lens)
    extend_start_loc[1:] = torch.cumsum(extend_seq_lens[:-1], dim=0)
    return positions.to(torch.int64), extend_start_loc</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.compute_position_triton"><code class="name flex">
<span>def <span class="ident">compute_position_triton</span></span>(<span>extend_prefix_lens: torch.Tensor,<br>extend_seq_lens: torch.Tensor,<br>extend_seq_lens_sum)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_position_triton(
    extend_prefix_lens: torch.Tensor, extend_seq_lens: torch.Tensor, extend_seq_lens_sum
):
    &#34;&#34;&#34;Compute positions. It is a fused version of `compute_position_torch`.&#34;&#34;&#34;
    batch_size = extend_seq_lens.shape[0]
    has_prefix = extend_prefix_lens.shape[0] == batch_size

    positions = torch.empty(
        extend_seq_lens_sum, dtype=torch.int64, device=extend_seq_lens.device
    )
    extend_start_loc = torch.empty(
        batch_size, dtype=torch.int32, device=extend_seq_lens.device
    )

    # Launch kernel
    compute_position_kernel[(batch_size,)](
        positions,
        extend_start_loc,
        extend_prefix_lens,
        extend_seq_lens,
        has_prefix,
    )

    return positions, extend_start_loc</code></pre>
</details>
<div class="desc"><p>Compute positions. It is a fused version of <code><a title="sglang.srt.model_executor.forward_batch_info.compute_position_torch" href="#sglang.srt.model_executor.forward_batch_info.compute_position_torch">compute_position_torch()</a></code>.</p></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.enable_num_token_non_padded"><code class="name flex">
<span>def <span class="ident">enable_num_token_non_padded</span></span>(<span>server_args)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def enable_num_token_non_padded(server_args):
    return get_moe_expert_parallel_world_size() &gt; 1</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode"><code class="flex name class">
<span>class <span class="ident">CaptureHiddenMode</span></span>
<span>(</span><span>*args, **kwds)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@total_ordering
class CaptureHiddenMode(IntEnum):
    # Do not capture anything.
    NULL = 0
    # Capture a hidden state of the last token.
    LAST = 1
    # Capture hidden states of all tokens.
    FULL = 2

    def need_capture(self):
        return self != CaptureHiddenMode.NULL

    def is_full(self):
        return self == CaptureHiddenMode.FULL

    def is_last(self):
        return self == CaptureHiddenMode.LAST

    def __lt__(self, other):
        return self.value &lt; other.value</code></pre>
</details>
<div class="desc"><p>Enum where members are also (and must be) ints</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.IntEnum</li>
<li>builtins.int</li>
<li>enum.ReprEnum</li>
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode.FULL"><code class="name">var <span class="ident">FULL</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode.LAST"><code class="name">var <span class="ident">LAST</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode.NULL"><code class="name">var <span class="ident">NULL</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode.is_full"><code class="name flex">
<span>def <span class="ident">is_full</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_full(self):
    return self == CaptureHiddenMode.FULL</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode.is_last"><code class="name flex">
<span>def <span class="ident">is_last</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_last(self):
    return self == CaptureHiddenMode.LAST</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode.need_capture"><code class="name flex">
<span>def <span class="ident">need_capture</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def need_capture(self):
    return self != CaptureHiddenMode.NULL</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch"><code class="flex name class">
<span>class <span class="ident">ForwardBatch</span></span>
<span>(</span><span>forward_mode: <a title="sglang.srt.model_executor.forward_batch_info.ForwardMode" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode">ForwardMode</a>,<br>batch_size: int,<br>input_ids: torch.Tensor,<br>req_pool_indices: torch.Tensor,<br>seq_lens: torch.Tensor,<br>out_cache_loc: torch.Tensor,<br>seq_lens_sum: int,<br>orig_seq_lens: Optional[torch.Tensor] = None,<br>seq_lens_cpu: Optional[torch.Tensor] = None,<br>return_logprob: bool = False,<br>top_logprobs_nums: Optional[List[int]] = None,<br>token_ids_logprobs: Optional[List[List[int]]] = None,<br>next_token_logits_buffer: torch.Tensor = None,<br>temp_scaled_logprobs: bool = False,<br>temperature: torch.Tensor = None,<br>top_p_normalized_logprobs: bool = False,<br>top_p: torch.Tensor = None,<br>positions: torch.Tensor = None,<br>extend_num_tokens: Optional[int] = None,<br>extend_seq_lens: Optional[torch.Tensor] = None,<br>extend_prefix_lens: Optional[torch.Tensor] = None,<br>extend_start_loc: Optional[torch.Tensor] = None,<br>extend_prefix_lens_cpu: Optional[List[int]] = None,<br>extend_seq_lens_cpu: Optional[List[int]] = None,<br>extend_logprob_start_lens_cpu: Optional[List[int]] = None,<br>extend_input_logprob_token_ids_gpu: Optional[torch.Tensor] = None,<br>hidden_states: torch.Tensor = None,<br>residual: torch.Tensor = None,<br>model_specific_states: Dict[str, any] = None,<br>split_index: int = 0,<br>attn_attend_prefix_cache: Optional[bool] = None,<br>num_prefix_chunks: Optional[int] = None,<br>prefix_chunk_idx: Optional[int] = None,<br>prefix_chunk_len: Optional[int] = None,<br>prefix_chunk_starts: Optional[torch.Tensor] = None,<br>prefix_chunk_seq_lens: Optional[torch.Tensor] = None,<br>prefix_chunk_cu_seq_lens: Optional[torch.Tensor] = None,<br>prefix_chunk_max_seq_lens: Optional[List[int]] = None,<br>prefix_chunk_num_tokens: Optional[List[int]] = None,<br>prefix_chunk_kv_indices: Optional[List[torch.Tensor]] = None,<br>mha_return_lse: Optional[bool] = None,<br>mm_inputs: Optional[List[MultimodalInputs]] = None,<br>encoder_cached: Optional[List[bool]] = None,<br>encoder_lens: Optional[torch.Tensor] = None,<br>encoder_lens_cpu: Optional[List[int]] = None,<br>encoder_out_cache_loc: Optional[torch.Tensor] = None,<br>lora_ids: Optional[List[str]] = None,<br>input_embeds: Optional[torch.Tensor] = None,<br>token_type_ids: Optional[torch.Tensor] = None,<br>sampling_info: SamplingBatchInfo = None,<br>req_to_token_pool: ReqToTokenPool = None,<br>token_to_kv_pool: KVCache = None,<br>attn_backend: AttentionBackend = None,<br>global_num_tokens_cpu: Optional[List[int]] = None,<br>global_num_tokens_gpu: Optional[torch.Tensor] = None,<br>global_num_tokens_for_logprob_cpu: Optional[List[int]] = None,<br>global_num_tokens_for_logprob_gpu: Optional[torch.Tensor] = None,<br>dp_padding_mode: Optional[DpPaddingMode] = None,<br>dp_local_start_pos: Optional[torch.Tensor] = None,<br>dp_local_num_tokens: Optional[torch.Tensor] = None,<br>global_dp_buffer_len: Optional[int] = None,<br>is_extend_in_batch: bool = False,<br>can_run_dp_cuda_graph: bool = False,<br>global_forward_mode: Optional[<a title="sglang.srt.model_executor.forward_batch_info.ForwardMode" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode">ForwardMode</a>] = None,<br>spec_info: Optional[Union[EagleVerifyInput, EagleDraftInput]] = None,<br>spec_algorithm: SpeculativeAlgorithm = None,<br>capture_hidden_mode: <a title="sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode" href="#sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode">CaptureHiddenMode</a> = None,<br>padded_static_len: int = -1,<br>num_token_non_padded: Optional[torch.Tensor] = None,<br>mrope_positions: torch.Tensor = None,<br>tbo_split_seq_index: Optional[int] = None,<br>tbo_parent_token_range: Optional[Tuple[int, int]] = None,<br>tbo_children: Optional[List[<a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>]] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class ForwardBatch:
    &#34;&#34;&#34;Store all inputs of a forward pass.&#34;&#34;&#34;

    # The forward mode
    forward_mode: ForwardMode
    # The batch size
    batch_size: int
    # The input ids
    input_ids: torch.Tensor
    # The indices of requests in the req_to_token_pool
    req_pool_indices: torch.Tensor
    # The sequence length
    seq_lens: torch.Tensor
    # The indices of output tokens in the token_to_kv_pool
    out_cache_loc: torch.Tensor

    # The sum of all sequence lengths
    seq_lens_sum: int

    # The original sequence length without being chunked. Qwen-1M related.
    orig_seq_lens: Optional[torch.Tensor] = None

    # Optional seq_lens on cpu
    seq_lens_cpu: Optional[torch.Tensor] = None

    # For logprob
    return_logprob: bool = False
    top_logprobs_nums: Optional[List[int]] = None
    token_ids_logprobs: Optional[List[List[int]]] = None

    # For logits and logprobs post processing
    next_token_logits_buffer: torch.Tensor = None
    temp_scaled_logprobs: bool = False
    temperature: torch.Tensor = None
    top_p_normalized_logprobs: bool = False
    top_p: torch.Tensor = None

    # Position information
    positions: torch.Tensor = None

    # For extend
    extend_num_tokens: Optional[int] = None
    extend_seq_lens: Optional[torch.Tensor] = None
    extend_prefix_lens: Optional[torch.Tensor] = None
    extend_start_loc: Optional[torch.Tensor] = None
    extend_prefix_lens_cpu: Optional[List[int]] = None
    extend_seq_lens_cpu: Optional[List[int]] = None
    extend_logprob_start_lens_cpu: Optional[List[int]] = None
    extend_input_logprob_token_ids_gpu: Optional[torch.Tensor] = None

    # For split prefill
    # intermediate values for split prefill
    hidden_states: torch.Tensor = None
    residual: torch.Tensor = None
    model_specific_states: Dict[str, any] = None
    # current split index of layer
    split_index: int = 0

    # For MLA chunked prefix cache used in chunked prefill
    # Tell attention backend whether the kv cache needs to be attended in current pass
    attn_attend_prefix_cache: Optional[bool] = None
    # Number of prefix cache chunks
    num_prefix_chunks: Optional[int] = None
    # Index of current chunk, used by attention backend
    prefix_chunk_idx: Optional[int] = None
    # Maximum number of tokens in each chunk per sequence. Computed from maximum chunk capacity
    prefix_chunk_len: Optional[int] = None
    # Start positions of prefix cache for each chunk, (num_prefix_chunks, batch_size)
    prefix_chunk_starts: Optional[torch.Tensor] = None
    # Lengths of prefix cache for each chunk, (num_prefix_chunks, batch_size)
    prefix_chunk_seq_lens: Optional[torch.Tensor] = None
    # Accumulated lengths of prefix cache for each chunk, (num_prefix_chunks, batch_size + 1)
    prefix_chunk_cu_seq_lens: Optional[torch.Tensor] = None
    # Max lengths of prefix cache for each chunk, (num_prefix_chunks,)
    prefix_chunk_max_seq_lens: Optional[List[int]] = None
    # Number of tokens in each prefix cache chunk, (num_prefix_chunks,)
    prefix_chunk_num_tokens: Optional[List[int]] = None
    # KV Indices for each chunk
    prefix_chunk_kv_indices: Optional[List[torch.Tensor]] = None
    # For MLA chunked prefix cache used in chunked prefill
    # Tell attention backend whether lse needs to be returned
    mha_return_lse: Optional[bool] = None

    # For multimodal
    mm_inputs: Optional[List[MultimodalInputs]] = None

    # Encoder-decoder
    encoder_cached: Optional[List[bool]] = None
    encoder_lens: Optional[torch.Tensor] = None
    encoder_lens_cpu: Optional[List[int]] = None
    encoder_out_cache_loc: Optional[torch.Tensor] = None

    # For LoRA
    lora_ids: Optional[List[str]] = None

    # For input embeddings
    input_embeds: Optional[torch.Tensor] = None

    # For cross-encoder model
    token_type_ids: Optional[torch.Tensor] = None

    # Sampling info
    sampling_info: SamplingBatchInfo = None

    # Attention backend
    req_to_token_pool: ReqToTokenPool = None
    token_to_kv_pool: KVCache = None
    attn_backend: AttentionBackend = None

    # For DP attention
    global_num_tokens_cpu: Optional[List[int]] = None
    global_num_tokens_gpu: Optional[torch.Tensor] = None
    # Has to be None when cuda graph is captured.
    global_num_tokens_for_logprob_cpu: Optional[List[int]] = None
    global_num_tokens_for_logprob_gpu: Optional[torch.Tensor] = None
    # The padding mode for DP attention
    dp_padding_mode: Optional[DpPaddingMode] = None
    # for extend, local start pos and num tokens is different in logits processor
    # this will be computed in get_dp_local_info
    # this will be recomputed in LogitsMetadata.from_forward_batch
    dp_local_start_pos: Optional[torch.Tensor] = None  # cached info at runtime
    dp_local_num_tokens: Optional[torch.Tensor] = None  # cached info at runtime
    global_dp_buffer_len: Optional[int] = None
    is_extend_in_batch: bool = False
    can_run_dp_cuda_graph: bool = False
    global_forward_mode: Optional[ForwardMode] = None

    # Speculative decoding
    spec_info: Optional[Union[EagleVerifyInput, EagleDraftInput]] = None
    spec_algorithm: SpeculativeAlgorithm = None
    capture_hidden_mode: CaptureHiddenMode = None

    # For padding
    padded_static_len: int = -1  # -1 if not padded
    num_token_non_padded: Optional[torch.Tensor] = None  # scalar tensor

    # For Qwen2-VL
    mrope_positions: torch.Tensor = None

    # For two-batch overlap
    tbo_split_seq_index: Optional[int] = None
    tbo_parent_token_range: Optional[Tuple[int, int]] = None
    tbo_children: Optional[List[ForwardBatch]] = None

    @classmethod
    def init_new(
        cls,
        batch: ModelWorkerBatch,
        model_runner: ModelRunner,
    ):
        from sglang.srt.two_batch_overlap import TboForwardBatchPreparer

        ret = cls(
            forward_mode=batch.forward_mode,
            batch_size=len(batch.seq_lens),
            input_ids=batch.input_ids,
            req_pool_indices=batch.req_pool_indices,
            seq_lens=batch.seq_lens,
            out_cache_loc=batch.out_cache_loc,
            mm_inputs=batch.multimodal_inputs,
            encoder_cached=batch.encoder_cached,
            encoder_lens=batch.encoder_lens,
            encoder_lens_cpu=batch.encoder_lens_cpu,
            encoder_out_cache_loc=batch.encoder_out_cache_loc,
            seq_lens_sum=batch.seq_lens_sum,
            seq_lens_cpu=batch.seq_lens_cpu,
            orig_seq_lens=batch.orig_seq_lens,
            return_logprob=batch.return_logprob,
            top_logprobs_nums=batch.top_logprobs_nums,
            token_ids_logprobs=batch.token_ids_logprobs,
            is_extend_in_batch=batch.is_extend_in_batch,
            can_run_dp_cuda_graph=batch.can_run_dp_cuda_graph,
            global_forward_mode=batch.global_forward_mode,
            lora_ids=batch.lora_ids,
            sampling_info=batch.sampling_info,
            req_to_token_pool=model_runner.req_to_token_pool,
            token_to_kv_pool=model_runner.token_to_kv_pool,
            attn_backend=model_runner.attn_backend,
            spec_algorithm=batch.spec_algorithm,
            spec_info=batch.spec_info,
            capture_hidden_mode=batch.capture_hidden_mode,
            input_embeds=batch.input_embeds,
            token_type_ids=batch.token_type_ids,
            tbo_split_seq_index=batch.tbo_split_seq_index,
        )
        device = model_runner.device

        if batch.extend_input_logprob_token_ids is not None:
            ret.extend_input_logprob_token_ids_gpu = (
                batch.extend_input_logprob_token_ids.to(device, non_blocking=True)
            )

        if enable_num_token_non_padded(model_runner.server_args):
            ret.num_token_non_padded = torch.tensor(
                len(batch.input_ids), dtype=torch.int32
            ).to(device, non_blocking=True)

        # For MLP sync
        if batch.global_num_tokens is not None:
            from sglang.srt.speculative.eagle_utils import (
                EagleDraftInput,
                EagleVerifyInput,
            )

            assert batch.global_num_tokens_for_logprob is not None
            # process global_num_tokens and global_num_tokens_for_logprob
            if batch.spec_info is not None:
                if isinstance(batch.spec_info, EagleDraftInput):
                    global_num_tokens = [
                        x * batch.spec_info.num_tokens_per_batch
                        for x in batch.global_num_tokens
                    ]
                    global_num_tokens_for_logprob = [
                        x * batch.spec_info.num_tokens_for_logprob_per_batch
                        for x in batch.global_num_tokens_for_logprob
                    ]
                else:
                    assert isinstance(batch.spec_info, EagleVerifyInput)
                    global_num_tokens = [
                        x * batch.spec_info.draft_token_num
                        for x in batch.global_num_tokens
                    ]
                    global_num_tokens_for_logprob = [
                        x * batch.spec_info.draft_token_num
                        for x in batch.global_num_tokens_for_logprob
                    ]
            else:
                global_num_tokens = batch.global_num_tokens
                global_num_tokens_for_logprob = batch.global_num_tokens_for_logprob

            ret.global_num_tokens_cpu = global_num_tokens
            ret.global_num_tokens_gpu = torch.tensor(
                global_num_tokens, dtype=torch.int64
            ).to(device, non_blocking=True)

            ret.global_num_tokens_for_logprob_cpu = global_num_tokens_for_logprob
            ret.global_num_tokens_for_logprob_gpu = torch.tensor(
                global_num_tokens_for_logprob, dtype=torch.int64
            ).to(device, non_blocking=True)

        if ret.forward_mode.is_idle():
            ret.positions = torch.empty((0,), dtype=torch.int64, device=device)
            TboForwardBatchPreparer.prepare(
                ret, is_draft_worker=model_runner.is_draft_worker
            )
            return ret

        # Override the positions with spec_info
        if (
            ret.spec_info is not None
            and getattr(ret.spec_info, &#34;positions&#34;, None) is not None
        ):
            ret.positions = ret.spec_info.positions

        # Init position information
        if ret.forward_mode.is_decode():
            if ret.positions is None:
                ret.positions = clamp_position(batch.seq_lens)
        else:
            ret.extend_seq_lens = torch.tensor(
                batch.extend_seq_lens, dtype=torch.int32
            ).to(device, non_blocking=True)
            ret.extend_prefix_lens = torch.tensor(
                batch.extend_prefix_lens, dtype=torch.int32
            ).to(device, non_blocking=True)
            ret.extend_num_tokens = batch.extend_num_tokens
            positions, ret.extend_start_loc = compute_position(
                model_runner.server_args.attention_backend,
                ret.extend_prefix_lens,
                ret.extend_seq_lens,
                ret.extend_num_tokens,
            )
            if ret.positions is None:
                ret.positions = positions
            ret.extend_prefix_lens_cpu = batch.extend_prefix_lens
            ret.extend_seq_lens_cpu = batch.extend_seq_lens
            ret.extend_logprob_start_lens_cpu = batch.extend_logprob_start_lens

        if model_runner.model_is_mrope:
            ret._compute_mrope_positions(model_runner, batch)

        # Init lora information
        if model_runner.server_args.enable_lora:
            model_runner.lora_manager.prepare_lora_batch(ret)

        TboForwardBatchPreparer.prepare(
            ret, is_draft_worker=model_runner.is_draft_worker
        )

        return ret

    def merge_mm_inputs(self) -&gt; Optional[MultimodalInputs]:
        &#34;&#34;&#34;
        Merge all multimodal inputs in the batch into a single MultiModalInputs object.

        Returns:
            if none, current batch contains no multimodal input

        &#34;&#34;&#34;
        if not self.mm_inputs or all(x is None for x in self.mm_inputs):
            return None
        # Filter out None values
        valid_inputs = [x for x in self.mm_inputs if x is not None]

        # TODO: is it expensive?
        # a workaround to avoid importing `MultimodalInputs`
        merged = valid_inputs[0].__class__(mm_items=[])

        # Merge remaining inputs
        for mm_input in valid_inputs:
            merged.merge(mm_input)

        return merged

    def contains_image_inputs(self) -&gt; bool:
        if self.mm_inputs is None:
            return False
        return any(
            mm_input is not None and mm_input.contains_image_inputs()
            for mm_input in self.mm_inputs
        )

    def contains_audio_inputs(self) -&gt; bool:
        if self.mm_inputs is None:
            return False
        return any(
            mm_input is not None and mm_input.contains_audio_inputs()
            for mm_input in self.mm_inputs
        )

    def contains_video_inputs(self) -&gt; bool:
        if self.mm_inputs is None:
            return False
        return any(
            mm_input is not None and mm_input.contains_video_inputs()
            for mm_input in self.mm_inputs
        )

    def contains_mm_inputs(self) -&gt; bool:
        return (
            self.contains_audio_inputs()
            or self.contains_video_inputs()
            or self.contains_image_inputs()
        )

    def _compute_mrope_positions(
        self, model_runner: ModelRunner, batch: ModelWorkerBatch
    ):
        # batch_size * [3 * seq_len]
        batch_size = self.seq_lens.shape[0]
        mrope_positions_list = [[]] * batch_size
        for batch_idx in range(batch_size):
            mm_input = batch.multimodal_inputs[batch_idx]
            if self.forward_mode.is_decode():
                mrope_position_deltas = (
                    [0]
                    if mm_input is None
                    else flatten_nested_list(mm_input.mrope_position_delta.tolist())
                )
                next_input_positions = []
                for mrope_position_delta in mrope_position_deltas:
                    # batched deltas needs to be processed separately
                    # Convert list of lists to tensor with shape [3, seq_len]
                    next_input_positions += [
                        MRotaryEmbedding.get_next_input_positions(
                            mrope_position_delta,
                            int(self.seq_lens[batch_idx]) - 1,
                            int(self.seq_lens[batch_idx]),
                        )
                    ]
                # 3 * N
                mrope_positions_list[batch_idx] = torch.cat(next_input_positions, dim=1)
            elif self.forward_mode.is_extend():
                extend_seq_len, extend_prefix_len = (
                    batch.extend_seq_lens[batch_idx],
                    batch.extend_prefix_lens[batch_idx],
                )
                if mm_input is None:
                    # text only
                    mrope_positions = torch.tensor(
                        [
                            [
                                pos
                                for pos in range(
                                    extend_prefix_len,
                                    extend_prefix_len + extend_seq_len,
                                )
                            ]
                        ]
                        * 3
                    )
                else:
                    mrope_positions = mm_input.mrope_positions[
                        :,
                        extend_prefix_len : extend_prefix_len + extend_seq_len,
                    ]
                mrope_positions_list[batch_idx] = mrope_positions

        self.mrope_positions = torch.cat(
            [pos.to(device=model_runner.device) for pos in mrope_positions_list],
            dim=1,
        ).to(dtype=torch.int64, device=model_runner.device)

    def get_max_chunk_capacity(self):
        # Maximum number of tokens in each chunk
        # TODO: Should be changed to a better value, maybe passed through server args
        return 128 * 1024

    def set_prefix_chunk_idx(self, idx: int):
        self.prefix_chunk_idx = idx

    def set_attn_attend_prefix_cache(self, attn_attend_prefix_cache: bool):
        self.attn_attend_prefix_cache = attn_attend_prefix_cache

    def prepare_chunked_kv_indices(self, device: torch.device):
        self.prefix_chunk_kv_indices = []
        for idx in range(self.num_prefix_chunks):
            chunk_starts = self.prefix_chunk_starts[idx]
            chunk_seq_lens = self.prefix_chunk_seq_lens[idx]
            chunk_cu_seq_lens = self.prefix_chunk_cu_seq_lens[idx]
            num_chunk_tokens = self.prefix_chunk_num_tokens[idx]

            chunk_kv_indices = torch.empty(
                num_chunk_tokens, dtype=torch.int32, device=device
            )

            create_chunked_prefix_cache_kv_indices[(self.batch_size,)](
                self.req_to_token_pool.req_to_token,
                self.req_pool_indices,
                chunk_starts,
                chunk_seq_lens,
                chunk_cu_seq_lens,
                chunk_kv_indices,
                self.req_to_token_pool.req_to_token.shape[1],
            )
            self.prefix_chunk_kv_indices.append(chunk_kv_indices)

    def _pad_tensor_to_size(self, tensor: torch.Tensor, size: int, *, value: int = 0):
        if value == 0:
            return torch.cat(
                [tensor, tensor.new_zeros(size - tensor.shape[0], *tensor.shape[1:])],
                dim=0,
            )
        else:
            return torch.cat(
                [
                    tensor,
                    tensor.new_full((size - tensor.shape[0], *tensor.shape[1:]), value),
                ],
                dim=0,
            )

    def prepare_mlp_sync_batch(self, model_runner: ModelRunner):

        from sglang.srt.speculative.eagle_utils import EagleDraftInput

        assert self.global_num_tokens_cpu is not None
        assert self.global_num_tokens_for_logprob_cpu is not None

        global_num_tokens = self.global_num_tokens_cpu
        sync_group_size = len(global_num_tokens)
        attn_tp_size = get_attention_tp_size()

        for i in range(sync_group_size):
            # make sure that the padded length is divisible by attn_tp_size because we may need reduce-scatter across attn_tp dim.
            # there is no reduce-scatter in LM logprob, so we do not need to adjust the padded length for logprob
            global_num_tokens[i] = (
                (global_num_tokens[i] - 1) // attn_tp_size + 1
            ) * attn_tp_size

        dp_padding_mode = DpPaddingMode.get_dp_padding_mode(global_num_tokens)
        self.dp_padding_mode = dp_padding_mode

        if dp_padding_mode.is_max_len():
            # when DP gather mode is all gather, we will use
            # all_gather_into_tensor to gather hidden states, where transferred
            # tokens should be padded to the same length. We will also use
            # reduce-scatter instead of all-reduce after MLP.
            max_num_tokens = max(global_num_tokens)
            global_num_tokens = [max_num_tokens] * sync_group_size
            buffer_len = max_num_tokens * sync_group_size
        else:
            buffer_len = sum(global_num_tokens)

        if len(global_num_tokens) &gt; 1:
            num_tokens = global_num_tokens[get_attention_dp_rank()]
        else:
            num_tokens = global_num_tokens[0]

        self.global_dp_buffer_len = buffer_len
        set_dp_buffer_len(buffer_len, num_tokens, global_num_tokens)

        bs = self.batch_size

        if self.forward_mode.is_decode():
            if self.is_extend_in_batch and dp_padding_mode.is_max_len():
                setattr(self, &#34;_original_forward_mode&#34;, self.forward_mode)
                self.forward_mode = ForwardMode.EXTEND
                self.extend_num_tokens = bs
                self.extend_seq_lens = torch.full_like(self.seq_lens, 1)
                self.extend_prefix_lens = self.seq_lens - 1
                self.extend_start_loc = torch.arange(
                    bs, dtype=torch.int32, device=self.seq_lens.device
                )
                self.extend_prefix_lens_cpu = self.extend_prefix_lens.cpu()
                self.extend_seq_lens_cpu = self.extend_seq_lens.cpu()
                self.extend_logprob_start_lens_cpu = self.extend_prefix_lens_cpu
            else:
                setattr(self, &#34;_original_batch_size&#34;, self.batch_size)
                if self.spec_info is not None:
                    bs = self.batch_size = (
                        num_tokens // self.spec_info.num_tokens_per_batch
                    )
                else:
                    bs = self.batch_size = num_tokens

        # padding
        self.input_ids = self._pad_tensor_to_size(self.input_ids, num_tokens)
        self.req_pool_indices = self._pad_tensor_to_size(self.req_pool_indices, bs)

        seq_len_fill_value = (
            model_runner.attn_backend.get_cuda_graph_seq_len_fill_value()
        )
        self.seq_lens_sum = self.seq_lens_sum + seq_len_fill_value * (
            bs - self.seq_lens.shape[0]
        )
        self.seq_lens = self._pad_tensor_to_size(
            self.seq_lens, bs, value=seq_len_fill_value
        )
        if self.seq_lens_cpu is not None:
            self.seq_lens_cpu = self._pad_tensor_to_size(
                self.seq_lens_cpu, bs, value=seq_len_fill_value
            )

        self.out_cache_loc = self._pad_tensor_to_size(self.out_cache_loc, num_tokens)
        if self.encoder_lens is not None:
            self.encoder_lens = self._pad_tensor_to_size(self.encoder_lens, bs)
        self.positions = self._pad_tensor_to_size(self.positions, num_tokens)
        self.global_num_tokens_cpu = global_num_tokens
        self.global_num_tokens_gpu = self.global_num_tokens_gpu.new_tensor(
            global_num_tokens
        )

        if self.mrope_positions is not None:
            self.mrope_positions = self._pad_tensor_to_size(self.mrope_positions, bs)

        # TODO: check if we need to pad other tensors
        if self.extend_seq_lens is not None:
            self.extend_seq_lens = self._pad_tensor_to_size(self.extend_seq_lens, bs)

        if self.spec_info is not None and isinstance(self.spec_info, EagleDraftInput):
            spec_info = self.spec_info
            self.output_cache_loc_backup = self.out_cache_loc
            self.hidden_states_backup = spec_info.hidden_states
            if spec_info.topk_p is not None:
                spec_info.topk_p = self._pad_tensor_to_size(spec_info.topk_p, bs)
            if spec_info.topk_index is not None:
                spec_info.topk_index = self._pad_tensor_to_size(
                    spec_info.topk_index, bs
                )
            if spec_info.accept_length is not None:
                spec_info.accept_length = self._pad_tensor_to_size(
                    spec_info.accept_length, bs
                )
            spec_info.hidden_states = self._pad_tensor_to_size(
                spec_info.hidden_states, num_tokens
            )

    def post_forward_mlp_sync_batch(self, logits_output: LogitsProcessorOutput):

        self.forward_mode = getattr(self, &#34;_original_forward_mode&#34;, self.forward_mode)
        self.batch_size = getattr(self, &#34;_original_batch_size&#34;, self.batch_size)
        bs = self.batch_size

        if self.spec_info is not None:
            if self.forward_mode.is_decode():  # draft
                num_tokens = self.hidden_states_backup.shape[0]
                self.positions = self.positions[:num_tokens]
                self.seq_lens = self.seq_lens[:bs]
                self.req_pool_indices = self.req_pool_indices[:bs]
                if self.seq_lens_cpu is not None:
                    self.seq_lens_cpu = self.seq_lens_cpu[:bs]
                logits_output.next_token_logits = logits_output.next_token_logits[
                    :num_tokens
                ]
                logits_output.hidden_states = logits_output.hidden_states[:num_tokens]
            elif self.forward_mode.is_target_verify():  # verify
                num_tokens = bs * self.spec_info.draft_token_num
                logits_output.next_token_logits = logits_output.next_token_logits[
                    :num_tokens
                ]
                logits_output.hidden_states = logits_output.hidden_states[:num_tokens]
            elif self.forward_mode.is_draft_extend():  # draft extend
                self.spec_info.accept_length = self.spec_info.accept_length[:bs]
                logits_output.next_token_logits = logits_output.next_token_logits[:bs]
                logits_output.hidden_states = logits_output.hidden_states[:bs]
            elif self.forward_mode.is_extend() or self.forward_mode.is_idle():
                logits_output.next_token_logits = logits_output.next_token_logits[:bs]
                logits_output.hidden_states = logits_output.hidden_states[:bs]

            if hasattr(self, &#34;hidden_states_backup&#34;):
                self.spec_info.hidden_states = self.hidden_states_backup
            if hasattr(self, &#34;output_cache_loc_backup&#34;):
                self.out_cache_loc = self.output_cache_loc_backup

        elif self.forward_mode.is_decode() or self.forward_mode.is_idle():
            logits_output.next_token_logits = logits_output.next_token_logits[:bs]
            if logits_output.hidden_states is not None:
                logits_output.hidden_states = logits_output.hidden_states[:bs]
        elif self.forward_mode.is_extend():
            num_tokens = self.seq_lens_sum
            logits_output.next_token_logits = logits_output.next_token_logits[
                :num_tokens
            ]
            if logits_output.hidden_states is not None:
                logits_output.hidden_states = logits_output.hidden_states[:num_tokens]

    # Here we suppose the length of each chunk is equal
    # For example, if we have 4 sequences with prefix length [256, 512, 768, 1024], prefix_chunk_len = 256
    # num_prefix_chunks = cdiv(1024, 256) = 4
    # prefix_chunk_starts = [[0, 0, 0, 0], [256, 256, 256, 256], [512, 512, 512, 512], [768, 768, 768, 768]]
    # prefix_chunk_ends = [[256, 256, 256, 256], [256, 512, 512, 512], [256, 512, 768, 768], [256, 512, 768, 1024]]
    # prefix_chunk_seq_lens = [[256, 256, 256, 256], [0, 256, 256, 256], [0, 0, 256, 256], [0, 0, 0, 256]]
    # TODO: Implement a better way to allocate chunk lengths that uses memory spaces more efficiently.
    def get_prefix_chunk_seq_lens(
        self, prefix_lens: torch.Tensor, num_prefix_chunks: int, prefix_chunk_len: int
    ):
        device = prefix_lens.device
        prefix_chunk_starts = (
            torch.arange(num_prefix_chunks, device=device, dtype=torch.int32)
            .unsqueeze(1)
            .expand(-1, self.batch_size)
            * prefix_chunk_len
        )
        prefix_chunk_ends = torch.min(
            prefix_lens.unsqueeze(0),
            prefix_chunk_starts + prefix_chunk_len,
        ).to(torch.int32)

        prefix_chunk_seq_lens = (
            (prefix_chunk_ends - prefix_chunk_starts).clamp(min=0).to(torch.int32)
        )

        return prefix_chunk_starts, prefix_chunk_seq_lens

    # Called before each attention module if using chunked kv cache for prefill
    # Some of the codes are adapted from https://github.com/vllm-project/vllm/blob/main/vllm/v1/attention/backends/mla/common.py
    def prepare_chunked_prefix_cache_info(self, device: torch.device):

        from sglang.srt.mem_cache.memory_pool import MLATokenToKVPool

        assert isinstance(
            self.token_to_kv_pool, MLATokenToKVPool
        ), &#34;Currently chunked prefix cache can only be used by Deepseek models&#34;

        if self.prefix_chunk_len is not None:
            # Chunked kv cache info already prepared by prior modules
            return

        self.prefix_chunk_idx = -1

        # chunk_capacity is the maximum number of tokens in each chunk
        chunk_capacity = self.get_max_chunk_capacity()
        self.prefix_chunk_len = chunk_capacity // self.batch_size

        self.num_prefix_chunks = (
            max(self.extend_prefix_lens_cpu) + self.prefix_chunk_len - 1
        ) // self.prefix_chunk_len

        # Here we compute chunk lens twice to avoid stream sync, once on gpu and once on cpu.
        prefix_chunk_starts_cuda, prefix_chunk_seq_lens_cuda = (
            self.get_prefix_chunk_seq_lens(
                self.extend_prefix_lens,
                self.num_prefix_chunks,
                self.prefix_chunk_len,
            )
        )
        _, prefix_chunk_seq_lens_cpu = self.get_prefix_chunk_seq_lens(
            torch.tensor(self.extend_prefix_lens_cpu),
            self.num_prefix_chunks,
            self.prefix_chunk_len,
        )
        self.prefix_chunk_starts = prefix_chunk_starts_cuda
        self.prefix_chunk_seq_lens = prefix_chunk_seq_lens_cuda

        # Metadata for attention backend
        self.prefix_chunk_cu_seq_lens = torch.zeros(
            self.num_prefix_chunks,
            self.batch_size + 1,
            device=device,
            dtype=torch.int32,
        )
        self.prefix_chunk_cu_seq_lens[:, 1:] = prefix_chunk_seq_lens_cuda.cumsum(
            dim=1
        ).to(torch.int32)
        self.prefix_chunk_max_seq_lens = prefix_chunk_seq_lens_cpu.max(
            dim=1
        ).values.tolist()

        self.prefix_chunk_num_tokens = prefix_chunk_seq_lens_cpu.sum(dim=1).tolist()
        assert max(self.prefix_chunk_num_tokens) &lt;= self.get_max_chunk_capacity()

        # Precompute the kv indices for each chunk
        self.prepare_chunked_kv_indices(device)

    @property
    def can_run_tbo(self):
        return self.tbo_split_seq_index is not None</code></pre>
</details>
<div class="desc"><p>Store all inputs of a forward pass.</p></div>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.test.attention.test_prefix_chunk_info.MockForwardBatch" href="../../test/attention/test_prefix_chunk_info.html#sglang.test.attention.test_prefix_chunk_info.MockForwardBatch">MockForwardBatch</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.init_new"><code class="name flex">
<span>def <span class="ident">init_new</span></span>(<span>batch: ModelWorkerBatch, model_runner: ModelRunner)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.attn_attend_prefix_cache"><code class="name">var <span class="ident">attn_attend_prefix_cache</span> : Optional[bool]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.attn_backend"><code class="name">var <span class="ident">attn_backend</span> : AttentionBackend</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.batch_size"><code class="name">var <span class="ident">batch_size</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.can_run_dp_cuda_graph"><code class="name">var <span class="ident">can_run_dp_cuda_graph</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.can_run_tbo"><code class="name">prop <span class="ident">can_run_tbo</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def can_run_tbo(self):
    return self.tbo_split_seq_index is not None</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.capture_hidden_mode"><code class="name">var <span class="ident">capture_hidden_mode</span> : <a title="sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode" href="#sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode">CaptureHiddenMode</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.dp_local_num_tokens"><code class="name">var <span class="ident">dp_local_num_tokens</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.dp_local_start_pos"><code class="name">var <span class="ident">dp_local_start_pos</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.dp_padding_mode"><code class="name">var <span class="ident">dp_padding_mode</span> : Optional[DpPaddingMode]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.encoder_cached"><code class="name">var <span class="ident">encoder_cached</span> : Optional[List[bool]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.encoder_lens"><code class="name">var <span class="ident">encoder_lens</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.encoder_lens_cpu"><code class="name">var <span class="ident">encoder_lens_cpu</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.encoder_out_cache_loc"><code class="name">var <span class="ident">encoder_out_cache_loc</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_input_logprob_token_ids_gpu"><code class="name">var <span class="ident">extend_input_logprob_token_ids_gpu</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_logprob_start_lens_cpu"><code class="name">var <span class="ident">extend_logprob_start_lens_cpu</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_num_tokens"><code class="name">var <span class="ident">extend_num_tokens</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_prefix_lens"><code class="name">var <span class="ident">extend_prefix_lens</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_prefix_lens_cpu"><code class="name">var <span class="ident">extend_prefix_lens_cpu</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_seq_lens"><code class="name">var <span class="ident">extend_seq_lens</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_seq_lens_cpu"><code class="name">var <span class="ident">extend_seq_lens_cpu</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_start_loc"><code class="name">var <span class="ident">extend_start_loc</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.forward_mode"><code class="name">var <span class="ident">forward_mode</span> : <a title="sglang.srt.model_executor.forward_batch_info.ForwardMode" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode">ForwardMode</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.global_dp_buffer_len"><code class="name">var <span class="ident">global_dp_buffer_len</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.global_forward_mode"><code class="name">var <span class="ident">global_forward_mode</span> : Optional[<a title="sglang.srt.model_executor.forward_batch_info.ForwardMode" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode">ForwardMode</a>]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.global_num_tokens_cpu"><code class="name">var <span class="ident">global_num_tokens_cpu</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.global_num_tokens_for_logprob_cpu"><code class="name">var <span class="ident">global_num_tokens_for_logprob_cpu</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.global_num_tokens_for_logprob_gpu"><code class="name">var <span class="ident">global_num_tokens_for_logprob_gpu</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.global_num_tokens_gpu"><code class="name">var <span class="ident">global_num_tokens_gpu</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.hidden_states"><code class="name">var <span class="ident">hidden_states</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.input_embeds"><code class="name">var <span class="ident">input_embeds</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.input_ids"><code class="name">var <span class="ident">input_ids</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.is_extend_in_batch"><code class="name">var <span class="ident">is_extend_in_batch</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.lora_ids"><code class="name">var <span class="ident">lora_ids</span> : Optional[List[str]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.mha_return_lse"><code class="name">var <span class="ident">mha_return_lse</span> : Optional[bool]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.mm_inputs"><code class="name">var <span class="ident">mm_inputs</span> : Optional[List[MultimodalInputs]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.model_specific_states"><code class="name">var <span class="ident">model_specific_states</span> : Dict[str, any]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.mrope_positions"><code class="name">var <span class="ident">mrope_positions</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.next_token_logits_buffer"><code class="name">var <span class="ident">next_token_logits_buffer</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.num_prefix_chunks"><code class="name">var <span class="ident">num_prefix_chunks</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.num_token_non_padded"><code class="name">var <span class="ident">num_token_non_padded</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.orig_seq_lens"><code class="name">var <span class="ident">orig_seq_lens</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.out_cache_loc"><code class="name">var <span class="ident">out_cache_loc</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.padded_static_len"><code class="name">var <span class="ident">padded_static_len</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.positions"><code class="name">var <span class="ident">positions</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_cu_seq_lens"><code class="name">var <span class="ident">prefix_chunk_cu_seq_lens</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_idx"><code class="name">var <span class="ident">prefix_chunk_idx</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_kv_indices"><code class="name">var <span class="ident">prefix_chunk_kv_indices</span> : Optional[List[torch.Tensor]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_len"><code class="name">var <span class="ident">prefix_chunk_len</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_max_seq_lens"><code class="name">var <span class="ident">prefix_chunk_max_seq_lens</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_num_tokens"><code class="name">var <span class="ident">prefix_chunk_num_tokens</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_seq_lens"><code class="name">var <span class="ident">prefix_chunk_seq_lens</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_starts"><code class="name">var <span class="ident">prefix_chunk_starts</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.req_pool_indices"><code class="name">var <span class="ident">req_pool_indices</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.req_to_token_pool"><code class="name">var <span class="ident">req_to_token_pool</span> : ReqToTokenPool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.residual"><code class="name">var <span class="ident">residual</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.return_logprob"><code class="name">var <span class="ident">return_logprob</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.sampling_info"><code class="name">var <span class="ident">sampling_info</span> : SamplingBatchInfo</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.seq_lens"><code class="name">var <span class="ident">seq_lens</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.seq_lens_cpu"><code class="name">var <span class="ident">seq_lens_cpu</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.seq_lens_sum"><code class="name">var <span class="ident">seq_lens_sum</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.spec_algorithm"><code class="name">var <span class="ident">spec_algorithm</span> : SpeculativeAlgorithm</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.spec_info"><code class="name">var <span class="ident">spec_info</span> : Optional[Union[EagleVerifyInput, EagleDraftInput]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.split_index"><code class="name">var <span class="ident">split_index</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.tbo_children"><code class="name">var <span class="ident">tbo_children</span> : Optional[List[<a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.tbo_parent_token_range"><code class="name">var <span class="ident">tbo_parent_token_range</span> : Optional[Tuple[int, int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.tbo_split_seq_index"><code class="name">var <span class="ident">tbo_split_seq_index</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.temp_scaled_logprobs"><code class="name">var <span class="ident">temp_scaled_logprobs</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.temperature"><code class="name">var <span class="ident">temperature</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.token_ids_logprobs"><code class="name">var <span class="ident">token_ids_logprobs</span> : Optional[List[List[int]]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.token_to_kv_pool"><code class="name">var <span class="ident">token_to_kv_pool</span> : KVCache</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.token_type_ids"><code class="name">var <span class="ident">token_type_ids</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.top_logprobs_nums"><code class="name">var <span class="ident">top_logprobs_nums</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.top_p"><code class="name">var <span class="ident">top_p</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.top_p_normalized_logprobs"><code class="name">var <span class="ident">top_p_normalized_logprobs</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.contains_audio_inputs"><code class="name flex">
<span>def <span class="ident">contains_audio_inputs</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def contains_audio_inputs(self) -&gt; bool:
    if self.mm_inputs is None:
        return False
    return any(
        mm_input is not None and mm_input.contains_audio_inputs()
        for mm_input in self.mm_inputs
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.contains_image_inputs"><code class="name flex">
<span>def <span class="ident">contains_image_inputs</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def contains_image_inputs(self) -&gt; bool:
    if self.mm_inputs is None:
        return False
    return any(
        mm_input is not None and mm_input.contains_image_inputs()
        for mm_input in self.mm_inputs
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.contains_mm_inputs"><code class="name flex">
<span>def <span class="ident">contains_mm_inputs</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def contains_mm_inputs(self) -&gt; bool:
    return (
        self.contains_audio_inputs()
        or self.contains_video_inputs()
        or self.contains_image_inputs()
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.contains_video_inputs"><code class="name flex">
<span>def <span class="ident">contains_video_inputs</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def contains_video_inputs(self) -&gt; bool:
    if self.mm_inputs is None:
        return False
    return any(
        mm_input is not None and mm_input.contains_video_inputs()
        for mm_input in self.mm_inputs
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.get_max_chunk_capacity"><code class="name flex">
<span>def <span class="ident">get_max_chunk_capacity</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_max_chunk_capacity(self):
    # Maximum number of tokens in each chunk
    # TODO: Should be changed to a better value, maybe passed through server args
    return 128 * 1024</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.get_prefix_chunk_seq_lens"><code class="name flex">
<span>def <span class="ident">get_prefix_chunk_seq_lens</span></span>(<span>self, prefix_lens: torch.Tensor, num_prefix_chunks: int, prefix_chunk_len: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_prefix_chunk_seq_lens(
    self, prefix_lens: torch.Tensor, num_prefix_chunks: int, prefix_chunk_len: int
):
    device = prefix_lens.device
    prefix_chunk_starts = (
        torch.arange(num_prefix_chunks, device=device, dtype=torch.int32)
        .unsqueeze(1)
        .expand(-1, self.batch_size)
        * prefix_chunk_len
    )
    prefix_chunk_ends = torch.min(
        prefix_lens.unsqueeze(0),
        prefix_chunk_starts + prefix_chunk_len,
    ).to(torch.int32)

    prefix_chunk_seq_lens = (
        (prefix_chunk_ends - prefix_chunk_starts).clamp(min=0).to(torch.int32)
    )

    return prefix_chunk_starts, prefix_chunk_seq_lens</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.merge_mm_inputs"><code class="name flex">
<span>def <span class="ident">merge_mm_inputs</span></span>(<span>self) ‑> Optional[MultimodalInputs]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_mm_inputs(self) -&gt; Optional[MultimodalInputs]:
    &#34;&#34;&#34;
    Merge all multimodal inputs in the batch into a single MultiModalInputs object.

    Returns:
        if none, current batch contains no multimodal input

    &#34;&#34;&#34;
    if not self.mm_inputs or all(x is None for x in self.mm_inputs):
        return None
    # Filter out None values
    valid_inputs = [x for x in self.mm_inputs if x is not None]

    # TODO: is it expensive?
    # a workaround to avoid importing `MultimodalInputs`
    merged = valid_inputs[0].__class__(mm_items=[])

    # Merge remaining inputs
    for mm_input in valid_inputs:
        merged.merge(mm_input)

    return merged</code></pre>
</details>
<div class="desc"><p>Merge all multimodal inputs in the batch into a single MultiModalInputs object.</p>
<h2 id="returns">Returns</h2>
<p>if none, current batch contains no multimodal input</p></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.post_forward_mlp_sync_batch"><code class="name flex">
<span>def <span class="ident">post_forward_mlp_sync_batch</span></span>(<span>self, logits_output: LogitsProcessorOutput)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def post_forward_mlp_sync_batch(self, logits_output: LogitsProcessorOutput):

    self.forward_mode = getattr(self, &#34;_original_forward_mode&#34;, self.forward_mode)
    self.batch_size = getattr(self, &#34;_original_batch_size&#34;, self.batch_size)
    bs = self.batch_size

    if self.spec_info is not None:
        if self.forward_mode.is_decode():  # draft
            num_tokens = self.hidden_states_backup.shape[0]
            self.positions = self.positions[:num_tokens]
            self.seq_lens = self.seq_lens[:bs]
            self.req_pool_indices = self.req_pool_indices[:bs]
            if self.seq_lens_cpu is not None:
                self.seq_lens_cpu = self.seq_lens_cpu[:bs]
            logits_output.next_token_logits = logits_output.next_token_logits[
                :num_tokens
            ]
            logits_output.hidden_states = logits_output.hidden_states[:num_tokens]
        elif self.forward_mode.is_target_verify():  # verify
            num_tokens = bs * self.spec_info.draft_token_num
            logits_output.next_token_logits = logits_output.next_token_logits[
                :num_tokens
            ]
            logits_output.hidden_states = logits_output.hidden_states[:num_tokens]
        elif self.forward_mode.is_draft_extend():  # draft extend
            self.spec_info.accept_length = self.spec_info.accept_length[:bs]
            logits_output.next_token_logits = logits_output.next_token_logits[:bs]
            logits_output.hidden_states = logits_output.hidden_states[:bs]
        elif self.forward_mode.is_extend() or self.forward_mode.is_idle():
            logits_output.next_token_logits = logits_output.next_token_logits[:bs]
            logits_output.hidden_states = logits_output.hidden_states[:bs]

        if hasattr(self, &#34;hidden_states_backup&#34;):
            self.spec_info.hidden_states = self.hidden_states_backup
        if hasattr(self, &#34;output_cache_loc_backup&#34;):
            self.out_cache_loc = self.output_cache_loc_backup

    elif self.forward_mode.is_decode() or self.forward_mode.is_idle():
        logits_output.next_token_logits = logits_output.next_token_logits[:bs]
        if logits_output.hidden_states is not None:
            logits_output.hidden_states = logits_output.hidden_states[:bs]
    elif self.forward_mode.is_extend():
        num_tokens = self.seq_lens_sum
        logits_output.next_token_logits = logits_output.next_token_logits[
            :num_tokens
        ]
        if logits_output.hidden_states is not None:
            logits_output.hidden_states = logits_output.hidden_states[:num_tokens]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prepare_chunked_kv_indices"><code class="name flex">
<span>def <span class="ident">prepare_chunked_kv_indices</span></span>(<span>self, device: torch.device)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_chunked_kv_indices(self, device: torch.device):
    self.prefix_chunk_kv_indices = []
    for idx in range(self.num_prefix_chunks):
        chunk_starts = self.prefix_chunk_starts[idx]
        chunk_seq_lens = self.prefix_chunk_seq_lens[idx]
        chunk_cu_seq_lens = self.prefix_chunk_cu_seq_lens[idx]
        num_chunk_tokens = self.prefix_chunk_num_tokens[idx]

        chunk_kv_indices = torch.empty(
            num_chunk_tokens, dtype=torch.int32, device=device
        )

        create_chunked_prefix_cache_kv_indices[(self.batch_size,)](
            self.req_to_token_pool.req_to_token,
            self.req_pool_indices,
            chunk_starts,
            chunk_seq_lens,
            chunk_cu_seq_lens,
            chunk_kv_indices,
            self.req_to_token_pool.req_to_token.shape[1],
        )
        self.prefix_chunk_kv_indices.append(chunk_kv_indices)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prepare_chunked_prefix_cache_info"><code class="name flex">
<span>def <span class="ident">prepare_chunked_prefix_cache_info</span></span>(<span>self, device: torch.device)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_chunked_prefix_cache_info(self, device: torch.device):

    from sglang.srt.mem_cache.memory_pool import MLATokenToKVPool

    assert isinstance(
        self.token_to_kv_pool, MLATokenToKVPool
    ), &#34;Currently chunked prefix cache can only be used by Deepseek models&#34;

    if self.prefix_chunk_len is not None:
        # Chunked kv cache info already prepared by prior modules
        return

    self.prefix_chunk_idx = -1

    # chunk_capacity is the maximum number of tokens in each chunk
    chunk_capacity = self.get_max_chunk_capacity()
    self.prefix_chunk_len = chunk_capacity // self.batch_size

    self.num_prefix_chunks = (
        max(self.extend_prefix_lens_cpu) + self.prefix_chunk_len - 1
    ) // self.prefix_chunk_len

    # Here we compute chunk lens twice to avoid stream sync, once on gpu and once on cpu.
    prefix_chunk_starts_cuda, prefix_chunk_seq_lens_cuda = (
        self.get_prefix_chunk_seq_lens(
            self.extend_prefix_lens,
            self.num_prefix_chunks,
            self.prefix_chunk_len,
        )
    )
    _, prefix_chunk_seq_lens_cpu = self.get_prefix_chunk_seq_lens(
        torch.tensor(self.extend_prefix_lens_cpu),
        self.num_prefix_chunks,
        self.prefix_chunk_len,
    )
    self.prefix_chunk_starts = prefix_chunk_starts_cuda
    self.prefix_chunk_seq_lens = prefix_chunk_seq_lens_cuda

    # Metadata for attention backend
    self.prefix_chunk_cu_seq_lens = torch.zeros(
        self.num_prefix_chunks,
        self.batch_size + 1,
        device=device,
        dtype=torch.int32,
    )
    self.prefix_chunk_cu_seq_lens[:, 1:] = prefix_chunk_seq_lens_cuda.cumsum(
        dim=1
    ).to(torch.int32)
    self.prefix_chunk_max_seq_lens = prefix_chunk_seq_lens_cpu.max(
        dim=1
    ).values.tolist()

    self.prefix_chunk_num_tokens = prefix_chunk_seq_lens_cpu.sum(dim=1).tolist()
    assert max(self.prefix_chunk_num_tokens) &lt;= self.get_max_chunk_capacity()

    # Precompute the kv indices for each chunk
    self.prepare_chunked_kv_indices(device)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prepare_mlp_sync_batch"><code class="name flex">
<span>def <span class="ident">prepare_mlp_sync_batch</span></span>(<span>self, model_runner: ModelRunner)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_mlp_sync_batch(self, model_runner: ModelRunner):

    from sglang.srt.speculative.eagle_utils import EagleDraftInput

    assert self.global_num_tokens_cpu is not None
    assert self.global_num_tokens_for_logprob_cpu is not None

    global_num_tokens = self.global_num_tokens_cpu
    sync_group_size = len(global_num_tokens)
    attn_tp_size = get_attention_tp_size()

    for i in range(sync_group_size):
        # make sure that the padded length is divisible by attn_tp_size because we may need reduce-scatter across attn_tp dim.
        # there is no reduce-scatter in LM logprob, so we do not need to adjust the padded length for logprob
        global_num_tokens[i] = (
            (global_num_tokens[i] - 1) // attn_tp_size + 1
        ) * attn_tp_size

    dp_padding_mode = DpPaddingMode.get_dp_padding_mode(global_num_tokens)
    self.dp_padding_mode = dp_padding_mode

    if dp_padding_mode.is_max_len():
        # when DP gather mode is all gather, we will use
        # all_gather_into_tensor to gather hidden states, where transferred
        # tokens should be padded to the same length. We will also use
        # reduce-scatter instead of all-reduce after MLP.
        max_num_tokens = max(global_num_tokens)
        global_num_tokens = [max_num_tokens] * sync_group_size
        buffer_len = max_num_tokens * sync_group_size
    else:
        buffer_len = sum(global_num_tokens)

    if len(global_num_tokens) &gt; 1:
        num_tokens = global_num_tokens[get_attention_dp_rank()]
    else:
        num_tokens = global_num_tokens[0]

    self.global_dp_buffer_len = buffer_len
    set_dp_buffer_len(buffer_len, num_tokens, global_num_tokens)

    bs = self.batch_size

    if self.forward_mode.is_decode():
        if self.is_extend_in_batch and dp_padding_mode.is_max_len():
            setattr(self, &#34;_original_forward_mode&#34;, self.forward_mode)
            self.forward_mode = ForwardMode.EXTEND
            self.extend_num_tokens = bs
            self.extend_seq_lens = torch.full_like(self.seq_lens, 1)
            self.extend_prefix_lens = self.seq_lens - 1
            self.extend_start_loc = torch.arange(
                bs, dtype=torch.int32, device=self.seq_lens.device
            )
            self.extend_prefix_lens_cpu = self.extend_prefix_lens.cpu()
            self.extend_seq_lens_cpu = self.extend_seq_lens.cpu()
            self.extend_logprob_start_lens_cpu = self.extend_prefix_lens_cpu
        else:
            setattr(self, &#34;_original_batch_size&#34;, self.batch_size)
            if self.spec_info is not None:
                bs = self.batch_size = (
                    num_tokens // self.spec_info.num_tokens_per_batch
                )
            else:
                bs = self.batch_size = num_tokens

    # padding
    self.input_ids = self._pad_tensor_to_size(self.input_ids, num_tokens)
    self.req_pool_indices = self._pad_tensor_to_size(self.req_pool_indices, bs)

    seq_len_fill_value = (
        model_runner.attn_backend.get_cuda_graph_seq_len_fill_value()
    )
    self.seq_lens_sum = self.seq_lens_sum + seq_len_fill_value * (
        bs - self.seq_lens.shape[0]
    )
    self.seq_lens = self._pad_tensor_to_size(
        self.seq_lens, bs, value=seq_len_fill_value
    )
    if self.seq_lens_cpu is not None:
        self.seq_lens_cpu = self._pad_tensor_to_size(
            self.seq_lens_cpu, bs, value=seq_len_fill_value
        )

    self.out_cache_loc = self._pad_tensor_to_size(self.out_cache_loc, num_tokens)
    if self.encoder_lens is not None:
        self.encoder_lens = self._pad_tensor_to_size(self.encoder_lens, bs)
    self.positions = self._pad_tensor_to_size(self.positions, num_tokens)
    self.global_num_tokens_cpu = global_num_tokens
    self.global_num_tokens_gpu = self.global_num_tokens_gpu.new_tensor(
        global_num_tokens
    )

    if self.mrope_positions is not None:
        self.mrope_positions = self._pad_tensor_to_size(self.mrope_positions, bs)

    # TODO: check if we need to pad other tensors
    if self.extend_seq_lens is not None:
        self.extend_seq_lens = self._pad_tensor_to_size(self.extend_seq_lens, bs)

    if self.spec_info is not None and isinstance(self.spec_info, EagleDraftInput):
        spec_info = self.spec_info
        self.output_cache_loc_backup = self.out_cache_loc
        self.hidden_states_backup = spec_info.hidden_states
        if spec_info.topk_p is not None:
            spec_info.topk_p = self._pad_tensor_to_size(spec_info.topk_p, bs)
        if spec_info.topk_index is not None:
            spec_info.topk_index = self._pad_tensor_to_size(
                spec_info.topk_index, bs
            )
        if spec_info.accept_length is not None:
            spec_info.accept_length = self._pad_tensor_to_size(
                spec_info.accept_length, bs
            )
        spec_info.hidden_states = self._pad_tensor_to_size(
            spec_info.hidden_states, num_tokens
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.set_attn_attend_prefix_cache"><code class="name flex">
<span>def <span class="ident">set_attn_attend_prefix_cache</span></span>(<span>self, attn_attend_prefix_cache: bool)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_attn_attend_prefix_cache(self, attn_attend_prefix_cache: bool):
    self.attn_attend_prefix_cache = attn_attend_prefix_cache</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardBatch.set_prefix_chunk_idx"><code class="name flex">
<span>def <span class="ident">set_prefix_chunk_idx</span></span>(<span>self, idx: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_prefix_chunk_idx(self, idx: int):
    self.prefix_chunk_idx = idx</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode"><code class="flex name class">
<span>class <span class="ident">ForwardMode</span></span>
<span>(</span><span>*args, **kwds)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ForwardMode(IntEnum):
    # Extend a sequence. The KV cache of the beginning part of the sequence is already computed (e.g., system prompt).
    # It is also called &#34;prefill&#34; in common terminology.
    EXTEND = auto()
    # Decode one token.
    DECODE = auto()
    # Contains both EXTEND and DECODE when doing chunked prefill.
    MIXED = auto()
    # No sequence to forward. For data parallel attention, some workers will be IDLE if no sequence are allocated.
    IDLE = auto()

    # Used in speculative decoding: verify a batch in the target model.
    TARGET_VERIFY = auto()
    # Used in speculative decoding: extend a batch in the draft model.
    DRAFT_EXTEND = auto()

    # A dummy first batch to start the pipeline for overlap scheduler.
    # It is now used for triggering the sampling_info_done event for the first prefill batch.
    DUMMY_FIRST = auto()

    # Split Prefill for PD multiplexing
    SPLIT_PREFILL = auto()

    def is_prefill(self):
        return self.is_extend()

    def is_extend(self):
        return (
            self == ForwardMode.EXTEND
            or self == ForwardMode.MIXED
            or self == ForwardMode.DRAFT_EXTEND
            or self == ForwardMode.TARGET_VERIFY
        )

    def is_decode(self):
        return self == ForwardMode.DECODE

    def is_mixed(self):
        return self == ForwardMode.MIXED

    def is_idle(self):
        return self == ForwardMode.IDLE

    def is_decode_or_idle(self):
        return self == ForwardMode.DECODE or self == ForwardMode.IDLE

    def is_target_verify(self):
        return self == ForwardMode.TARGET_VERIFY

    def is_draft_extend(self):
        return self == ForwardMode.DRAFT_EXTEND

    def is_extend_or_draft_extend_or_mixed(self):
        return (
            self == ForwardMode.EXTEND
            or self == ForwardMode.DRAFT_EXTEND
            or self == ForwardMode.MIXED
        )

    def is_cuda_graph(self):
        return (
            self == ForwardMode.DECODE
            or self == ForwardMode.TARGET_VERIFY
            or self == ForwardMode.IDLE
        )

    def is_dummy_first(self):
        return self == ForwardMode.DUMMY_FIRST

    def is_split_prefill(self):
        return self == ForwardMode.SPLIT_PREFILL</code></pre>
</details>
<div class="desc"><p>Enum where members are also (and must be) ints</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.IntEnum</li>
<li>builtins.int</li>
<li>enum.ReprEnum</li>
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.DECODE"><code class="name">var <span class="ident">DECODE</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.DRAFT_EXTEND"><code class="name">var <span class="ident">DRAFT_EXTEND</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.DUMMY_FIRST"><code class="name">var <span class="ident">DUMMY_FIRST</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.EXTEND"><code class="name">var <span class="ident">EXTEND</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.IDLE"><code class="name">var <span class="ident">IDLE</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.MIXED"><code class="name">var <span class="ident">MIXED</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.SPLIT_PREFILL"><code class="name">var <span class="ident">SPLIT_PREFILL</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.TARGET_VERIFY"><code class="name">var <span class="ident">TARGET_VERIFY</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_cuda_graph"><code class="name flex">
<span>def <span class="ident">is_cuda_graph</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_cuda_graph(self):
    return (
        self == ForwardMode.DECODE
        or self == ForwardMode.TARGET_VERIFY
        or self == ForwardMode.IDLE
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_decode"><code class="name flex">
<span>def <span class="ident">is_decode</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_decode(self):
    return self == ForwardMode.DECODE</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_decode_or_idle"><code class="name flex">
<span>def <span class="ident">is_decode_or_idle</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_decode_or_idle(self):
    return self == ForwardMode.DECODE or self == ForwardMode.IDLE</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_draft_extend"><code class="name flex">
<span>def <span class="ident">is_draft_extend</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_draft_extend(self):
    return self == ForwardMode.DRAFT_EXTEND</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_dummy_first"><code class="name flex">
<span>def <span class="ident">is_dummy_first</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_dummy_first(self):
    return self == ForwardMode.DUMMY_FIRST</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_extend"><code class="name flex">
<span>def <span class="ident">is_extend</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_extend(self):
    return (
        self == ForwardMode.EXTEND
        or self == ForwardMode.MIXED
        or self == ForwardMode.DRAFT_EXTEND
        or self == ForwardMode.TARGET_VERIFY
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_extend_or_draft_extend_or_mixed"><code class="name flex">
<span>def <span class="ident">is_extend_or_draft_extend_or_mixed</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_extend_or_draft_extend_or_mixed(self):
    return (
        self == ForwardMode.EXTEND
        or self == ForwardMode.DRAFT_EXTEND
        or self == ForwardMode.MIXED
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_idle"><code class="name flex">
<span>def <span class="ident">is_idle</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_idle(self):
    return self == ForwardMode.IDLE</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_mixed"><code class="name flex">
<span>def <span class="ident">is_mixed</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_mixed(self):
    return self == ForwardMode.MIXED</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_prefill"><code class="name flex">
<span>def <span class="ident">is_prefill</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_prefill(self):
    return self.is_extend()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_split_prefill"><code class="name flex">
<span>def <span class="ident">is_split_prefill</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_split_prefill(self):
    return self == ForwardMode.SPLIT_PREFILL</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_target_verify"><code class="name flex">
<span>def <span class="ident">is_target_verify</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_target_verify(self):
    return self == ForwardMode.TARGET_VERIFY</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.model_executor.forward_batch_info.PPProxyTensors"><code class="flex name class">
<span>class <span class="ident">PPProxyTensors</span></span>
<span>(</span><span>tensors)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PPProxyTensors:
    # adapted from https://github.com/vllm-project/vllm/blob/d14e98d924724b284dc5eaf8070d935e214e50c0/vllm/sequence.py#L1103
    tensors: Dict[str, torch.Tensor]

    def __init__(self, tensors):
        # manually define this function, so that
        # Dynamo knows `IntermediateTensors()` comes from this file.
        # Otherwise, dataclass will generate this function by evaluating
        # a string, and we will lose the information about the source file.
        self.tensors = tensors

    def __getitem__(self, key: Union[str, slice]):
        if isinstance(key, str):
            return self.tensors[key]
        elif isinstance(key, slice):
            return self.__class__({k: v[key] for k, v in self.tensors.items()})

    def __setitem__(self, key: str, value: torch.Tensor):
        self.tensors[key] = value

    def __len__(self):
        return len(self.tensors)

    def __eq__(self, other: object):
        return isinstance(other, self.__class__) and self

    def __repr__(self) -&gt; str:
        return f&#34;PPProxyTensors(tensors={self.tensors})&#34;</code></pre>
</details>
<div class="desc"></div>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.model_executor.forward_batch_info.PPProxyTensors.tensors"><code class="name">var <span class="ident">tensors</span> : Dict[str, torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.model_executor" href="index.html">sglang.srt.model_executor</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.model_executor.forward_batch_info.clamp_position" href="#sglang.srt.model_executor.forward_batch_info.clamp_position">clamp_position</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.compute_position" href="#sglang.srt.model_executor.forward_batch_info.compute_position">compute_position</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.compute_position_torch" href="#sglang.srt.model_executor.forward_batch_info.compute_position_torch">compute_position_torch</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.compute_position_triton" href="#sglang.srt.model_executor.forward_batch_info.compute_position_triton">compute_position_triton</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.enable_num_token_non_padded" href="#sglang.srt.model_executor.forward_batch_info.enable_num_token_non_padded">enable_num_token_non_padded</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode" href="#sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode">CaptureHiddenMode</a></code></h4>
<ul class="two-column">
<li><code><a title="sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode.FULL" href="#sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode.FULL">FULL</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode.LAST" href="#sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode.LAST">LAST</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode.NULL" href="#sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode.NULL">NULL</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode.is_full" href="#sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode.is_full">is_full</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode.is_last" href="#sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode.is_last">is_last</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode.need_capture" href="#sglang.srt.model_executor.forward_batch_info.CaptureHiddenMode.need_capture">need_capture</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.attn_attend_prefix_cache" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.attn_attend_prefix_cache">attn_attend_prefix_cache</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.attn_backend" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.attn_backend">attn_backend</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.batch_size" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.batch_size">batch_size</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.can_run_dp_cuda_graph" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.can_run_dp_cuda_graph">can_run_dp_cuda_graph</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.can_run_tbo" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.can_run_tbo">can_run_tbo</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.capture_hidden_mode" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.capture_hidden_mode">capture_hidden_mode</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.contains_audio_inputs" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.contains_audio_inputs">contains_audio_inputs</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.contains_image_inputs" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.contains_image_inputs">contains_image_inputs</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.contains_mm_inputs" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.contains_mm_inputs">contains_mm_inputs</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.contains_video_inputs" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.contains_video_inputs">contains_video_inputs</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.dp_local_num_tokens" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.dp_local_num_tokens">dp_local_num_tokens</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.dp_local_start_pos" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.dp_local_start_pos">dp_local_start_pos</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.dp_padding_mode" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.dp_padding_mode">dp_padding_mode</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.encoder_cached" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.encoder_cached">encoder_cached</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.encoder_lens" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.encoder_lens">encoder_lens</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.encoder_lens_cpu" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.encoder_lens_cpu">encoder_lens_cpu</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.encoder_out_cache_loc" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.encoder_out_cache_loc">encoder_out_cache_loc</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_input_logprob_token_ids_gpu" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_input_logprob_token_ids_gpu">extend_input_logprob_token_ids_gpu</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_logprob_start_lens_cpu" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_logprob_start_lens_cpu">extend_logprob_start_lens_cpu</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_num_tokens" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_num_tokens">extend_num_tokens</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_prefix_lens" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_prefix_lens">extend_prefix_lens</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_prefix_lens_cpu" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_prefix_lens_cpu">extend_prefix_lens_cpu</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_seq_lens" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_seq_lens">extend_seq_lens</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_seq_lens_cpu" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_seq_lens_cpu">extend_seq_lens_cpu</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_start_loc" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.extend_start_loc">extend_start_loc</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.forward_mode" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.forward_mode">forward_mode</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.get_max_chunk_capacity" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.get_max_chunk_capacity">get_max_chunk_capacity</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.get_prefix_chunk_seq_lens" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.get_prefix_chunk_seq_lens">get_prefix_chunk_seq_lens</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.global_dp_buffer_len" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.global_dp_buffer_len">global_dp_buffer_len</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.global_forward_mode" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.global_forward_mode">global_forward_mode</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.global_num_tokens_cpu" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.global_num_tokens_cpu">global_num_tokens_cpu</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.global_num_tokens_for_logprob_cpu" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.global_num_tokens_for_logprob_cpu">global_num_tokens_for_logprob_cpu</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.global_num_tokens_for_logprob_gpu" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.global_num_tokens_for_logprob_gpu">global_num_tokens_for_logprob_gpu</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.global_num_tokens_gpu" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.global_num_tokens_gpu">global_num_tokens_gpu</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.hidden_states" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.hidden_states">hidden_states</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.init_new" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.init_new">init_new</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.input_embeds" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.input_embeds">input_embeds</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.input_ids" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.input_ids">input_ids</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.is_extend_in_batch" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.is_extend_in_batch">is_extend_in_batch</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.lora_ids" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.lora_ids">lora_ids</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.merge_mm_inputs" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.merge_mm_inputs">merge_mm_inputs</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.mha_return_lse" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.mha_return_lse">mha_return_lse</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.mm_inputs" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.mm_inputs">mm_inputs</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.model_specific_states" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.model_specific_states">model_specific_states</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.mrope_positions" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.mrope_positions">mrope_positions</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.next_token_logits_buffer" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.next_token_logits_buffer">next_token_logits_buffer</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.num_prefix_chunks" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.num_prefix_chunks">num_prefix_chunks</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.num_token_non_padded" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.num_token_non_padded">num_token_non_padded</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.orig_seq_lens" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.orig_seq_lens">orig_seq_lens</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.out_cache_loc" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.out_cache_loc">out_cache_loc</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.padded_static_len" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.padded_static_len">padded_static_len</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.positions" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.positions">positions</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.post_forward_mlp_sync_batch" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.post_forward_mlp_sync_batch">post_forward_mlp_sync_batch</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_cu_seq_lens" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_cu_seq_lens">prefix_chunk_cu_seq_lens</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_idx" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_idx">prefix_chunk_idx</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_kv_indices" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_kv_indices">prefix_chunk_kv_indices</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_len" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_len">prefix_chunk_len</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_max_seq_lens" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_max_seq_lens">prefix_chunk_max_seq_lens</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_num_tokens" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_num_tokens">prefix_chunk_num_tokens</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_seq_lens" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_seq_lens">prefix_chunk_seq_lens</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_starts" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.prefix_chunk_starts">prefix_chunk_starts</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prepare_chunked_kv_indices" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.prepare_chunked_kv_indices">prepare_chunked_kv_indices</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prepare_chunked_prefix_cache_info" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.prepare_chunked_prefix_cache_info">prepare_chunked_prefix_cache_info</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.prepare_mlp_sync_batch" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.prepare_mlp_sync_batch">prepare_mlp_sync_batch</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.req_pool_indices" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.req_pool_indices">req_pool_indices</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.req_to_token_pool" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.req_to_token_pool">req_to_token_pool</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.residual" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.residual">residual</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.return_logprob" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.return_logprob">return_logprob</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.sampling_info" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.sampling_info">sampling_info</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.seq_lens" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.seq_lens">seq_lens</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.seq_lens_cpu" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.seq_lens_cpu">seq_lens_cpu</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.seq_lens_sum" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.seq_lens_sum">seq_lens_sum</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.set_attn_attend_prefix_cache" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.set_attn_attend_prefix_cache">set_attn_attend_prefix_cache</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.set_prefix_chunk_idx" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.set_prefix_chunk_idx">set_prefix_chunk_idx</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.spec_algorithm" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.spec_algorithm">spec_algorithm</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.spec_info" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.spec_info">spec_info</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.split_index" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.split_index">split_index</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.tbo_children" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.tbo_children">tbo_children</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.tbo_parent_token_range" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.tbo_parent_token_range">tbo_parent_token_range</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.tbo_split_seq_index" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.tbo_split_seq_index">tbo_split_seq_index</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.temp_scaled_logprobs" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.temp_scaled_logprobs">temp_scaled_logprobs</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.temperature" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.temperature">temperature</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.token_ids_logprobs" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.token_ids_logprobs">token_ids_logprobs</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.token_to_kv_pool" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.token_to_kv_pool">token_to_kv_pool</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.token_type_ids" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.token_type_ids">token_type_ids</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.top_logprobs_nums" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.top_logprobs_nums">top_logprobs_nums</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.top_p" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.top_p">top_p</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch.top_p_normalized_logprobs" href="#sglang.srt.model_executor.forward_batch_info.ForwardBatch.top_p_normalized_logprobs">top_p_normalized_logprobs</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode">ForwardMode</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.DECODE" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.DECODE">DECODE</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.DRAFT_EXTEND" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.DRAFT_EXTEND">DRAFT_EXTEND</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.DUMMY_FIRST" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.DUMMY_FIRST">DUMMY_FIRST</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.EXTEND" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.EXTEND">EXTEND</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.IDLE" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.IDLE">IDLE</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.MIXED" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.MIXED">MIXED</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.SPLIT_PREFILL" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.SPLIT_PREFILL">SPLIT_PREFILL</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.TARGET_VERIFY" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.TARGET_VERIFY">TARGET_VERIFY</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_cuda_graph" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.is_cuda_graph">is_cuda_graph</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_decode" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.is_decode">is_decode</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_decode_or_idle" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.is_decode_or_idle">is_decode_or_idle</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_draft_extend" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.is_draft_extend">is_draft_extend</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_dummy_first" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.is_dummy_first">is_dummy_first</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_extend" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.is_extend">is_extend</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_extend_or_draft_extend_or_mixed" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.is_extend_or_draft_extend_or_mixed">is_extend_or_draft_extend_or_mixed</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_idle" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.is_idle">is_idle</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_mixed" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.is_mixed">is_mixed</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_prefill" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.is_prefill">is_prefill</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_split_prefill" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.is_split_prefill">is_split_prefill</a></code></li>
<li><code><a title="sglang.srt.model_executor.forward_batch_info.ForwardMode.is_target_verify" href="#sglang.srt.model_executor.forward_batch_info.ForwardMode.is_target_verify">is_target_verify</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.model_executor.forward_batch_info.PPProxyTensors" href="#sglang.srt.model_executor.forward_batch_info.PPProxyTensors">PPProxyTensors</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.model_executor.forward_batch_info.PPProxyTensors.tensors" href="#sglang.srt.model_executor.forward_batch_info.PPProxyTensors.tensors">tensors</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
