<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.model_executor.model_runner API documentation</title>
<meta name="description" content="ModelRunner runs the forward passes of the models.">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.model_executor.model_runner</code></h1>
</header>
<section id="section-intro">
<p>ModelRunner runs the forward passes of the models.</p>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.model_executor.model_runner.LocalSerializedTensor"><code class="flex name class">
<span>class <span class="ident">LocalSerializedTensor</span></span>
<span>(</span><span>values: List[bytes])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class LocalSerializedTensor:
    &#34;&#34;&#34;torch.Tensor that gets serialized by MultiprocessingSerializer (which only serializes a pointer and not the data).
    The i-th element in the list corresponds to i-th rank&#39;s GPU.&#34;&#34;&#34;

    values: List[bytes]

    def get(self, rank: int):
        return MultiprocessingSerializer.deserialize(self.values[rank])</code></pre>
</details>
<div class="desc"><p>torch.Tensor that gets serialized by MultiprocessingSerializer (which only serializes a pointer and not the data).
The i-th element in the list corresponds to i-th rank's GPU.</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.model_executor.model_runner.LocalSerializedTensor.values"><code class="name">var <span class="ident">values</span> : List[bytes]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.model_executor.model_runner.LocalSerializedTensor.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self, rank: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(self, rank: int):
    return MultiprocessingSerializer.deserialize(self.values[rank])</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner"><code class="flex name class">
<span>class <span class="ident">ModelRunner</span></span>
<span>(</span><span>model_config: <a title="sglang.srt.configs.model_config.ModelConfig" href="../configs/model_config.html#sglang.srt.configs.model_config.ModelConfig">ModelConfig</a>,<br>mem_fraction_static: float,<br>gpu_id: int,<br>tp_rank: int,<br>tp_size: int,<br>moe_ep_rank: int,<br>moe_ep_size: int,<br>pp_rank: int,<br>pp_size: int,<br>nccl_port: int,<br>server_args: <a title="sglang.srt.server_args.ServerArgs" href="../server_args.html#sglang.srt.server_args.ServerArgs">ServerArgs</a>,<br>dp_rank: int | None = None,<br>is_draft_worker: bool = False,<br>req_to_token_pool: <a title="sglang.srt.mem_cache.memory_pool.ReqToTokenPool" href="../mem_cache/memory_pool.html#sglang.srt.mem_cache.memory_pool.ReqToTokenPool">ReqToTokenPool</a> | None = None,<br>token_to_kv_pool_allocator: <a title="sglang.srt.mem_cache.allocator.BaseTokenToKVPoolAllocator" href="../mem_cache/allocator.html#sglang.srt.mem_cache.allocator.BaseTokenToKVPoolAllocator">BaseTokenToKVPoolAllocator</a> | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelRunner:
    &#34;&#34;&#34;ModelRunner runs the forward passes of the models.&#34;&#34;&#34;

    def __init__(
        self,
        model_config: ModelConfig,
        mem_fraction_static: float,
        gpu_id: int,
        tp_rank: int,
        tp_size: int,
        moe_ep_rank: int,
        moe_ep_size: int,
        pp_rank: int,
        pp_size: int,
        nccl_port: int,
        server_args: ServerArgs,
        dp_rank: Optional[int] = None,
        is_draft_worker: bool = False,
        req_to_token_pool: Optional[ReqToTokenPool] = None,
        token_to_kv_pool_allocator: Optional[BaseTokenToKVPoolAllocator] = None,
    ):
        # Parse args
        self.mem_fraction_static = mem_fraction_static
        self.device = server_args.device
        self.gpu_id = gpu_id
        self.tp_rank = tp_rank
        self.tp_size = tp_size
        self.moe_ep_rank = moe_ep_rank
        self.moe_ep_size = moe_ep_size
        self.dp_size = server_args.dp_size
        self.pp_rank = pp_rank
        self.pp_size = pp_size
        self.model_config = model_config
        self.dist_port = nccl_port
        self.server_args = server_args
        self.is_draft_worker = is_draft_worker
        self.is_generation = model_config.is_generation
        self.is_multimodal = model_config.is_multimodal
        self.is_multimodal_chunked_prefill_supported = (
            model_config.is_multimodal_chunked_prefill_supported
        )
        self.spec_algorithm = SpeculativeAlgorithm.from_string(
            server_args.speculative_algorithm
        )
        self.page_size = server_args.page_size
        self.req_to_token_pool = req_to_token_pool
        self.token_to_kv_pool_allocator = token_to_kv_pool_allocator
        self.is_hybrid = model_config.is_hybrid
        self.use_mla_backend = self.model_config.attention_arch == AttentionArch.MLA
        self.attention_chunk_size = model_config.attention_chunk_size
        self.forward_pass_id = 0

        # Apply the rank zero filter to logger
        if not any(isinstance(f, RankZeroFilter) for f in logger.filters):
            logger.addFilter(RankZeroFilter(tp_rank == 0))
        if server_args.show_time_cost:
            enable_show_time_cost()

        # Model-specific adjustment
        self.model_specific_adjustment()

        # Global vars
        global_server_args_dict.update(
            {k: getattr(server_args, k) for k in GLOBAL_SERVER_ARGS_KEYS}
            | {
                # TODO it is indeed not a &#34;server args&#34;
                &#34;use_mla_backend&#34;: self.use_mla_backend,
                &#34;speculative_algorithm&#34;: self.spec_algorithm,
            }
        )

        # Init OpenMP threads binding for CPU
        if self.device == &#34;cpu&#34;:
            self.init_threads_binding()

        # Get memory before model loading
        min_per_gpu_memory = self.init_torch_distributed()

        # CPU offload
        set_offloader(create_offloader_from_server_args(server_args, dp_rank=dp_rank))

        # Update deep gemm configure
        if deep_gemm_wrapper.ENABLE_JIT_DEEPGEMM:
            deep_gemm_wrapper.update_deep_gemm_config(gpu_id, server_args)

        # Initialize the model runner
        self.initialize(min_per_gpu_memory)

        # Temporary cached values
        self.support_pp = (
            &#34;pp_proxy_tensors&#34; in inspect.signature(self.model.forward).parameters
        )

        # For weight updates
        self._model_update_group = {}

    def initialize(self, min_per_gpu_memory: float):
        server_args = self.server_args

        self.memory_saver_adapter = TorchMemorySaverAdapter.create(
            enable=self.server_args.enable_memory_saver
        )

        if not self.is_draft_worker:
            set_global_expert_location_metadata(
                compute_initial_expert_location_metadata(server_args, self.model_config)
            )
            if self.tp_rank == 0 and get_bool_env_var(
                &#34;SGLANG_LOG_EXPERT_LOCATION_METADATA&#34;
            ):
                logger.info(
                    f&#34;Initial expert_location_metadata: {get_global_expert_location_metadata()}&#34;
                )

            set_global_expert_distribution_recorder(
                ExpertDistributionRecorder.init_new(
                    server_args,
                    get_global_expert_location_metadata(),
                    rank=self.tp_rank,
                )
            )

        # Expert parallelism
        self.eplb_manager = (
            EPLBManager(self)
            if self.server_args.enable_eplb and (not self.is_draft_worker)
            else None
        )
        self.expert_location_updater = ExpertLocationUpdater()

        # Load the model
        self.sampler = Sampler()
        self.load_model()

        # Check if the model is using hybrid SWA
        if (
            not self.server_args.disable_hybrid_swa_memory
            and self.sliding_window_size is not None
            and self.sliding_window_size &gt; 0
        ):
            architectures = self.model_config.hf_config.architectures
            if architectures and not any(&#34;Llama4&#34; in arch for arch in architectures):
                self.is_hybrid = self.model_config.is_hybrid = True

        # For MTP models like DeepSeek-V3 or GLM-4.5, the MTP layer(s) are used separately as draft
        # models for speculative decoding. In those cases, `num_nextn_predict_layers` is used to
        # determine the number of layers.
        model_has_mtp_layers = self.model_config.num_nextn_predict_layers is not None
        model_num_layers = (
            self.model_config.num_nextn_predict_layers
            if self.is_draft_worker and model_has_mtp_layers
            else max(
                self.model_config.num_hidden_layers,
                self.model_config.num_attention_layers,
            )
        )
        self.start_layer = getattr(self.model, &#34;start_layer&#34;, 0)
        self.end_layer = getattr(self.model, &#34;end_layer&#34;, model_num_layers)
        self.num_effective_layers = self.end_layer - self.start_layer
        assert (
            (not model_has_mtp_layers)
            or (self.spec_algorithm.is_none())
            or (
                (not self.spec_algorithm.is_none())
                and (self.num_effective_layers == model_num_layers)
            )
        ), &#34;PP is not compatible with MTP models.&#34;

        # Apply torchao quantization
        torchao_applied = getattr(self.model, &#34;torchao_applied&#34;, False)
        # In layered loading, torchao may have been applied
        if not torchao_applied:
            apply_torchao_config_to_model(
                self.model, global_server_args_dict[&#34;torchao_config&#34;]
            )

        # Apply torch TP if the model supports it
        supports_torch_tp = getattr(self.model, &#34;supports_torch_tp&#34;, False)
        if self.tp_size &gt; 1 and supports_torch_tp:
            self.apply_torch_tp()

        # Init lora
        if server_args.enable_lora:
            self.init_lora_manager()

        # Init memory pool and attention backends
        self.init_memory_pool(
            min_per_gpu_memory,
            server_args.max_running_requests,
            server_args.max_total_tokens,
        )
        if self.device == &#34;cuda&#34;:
            self.init_cublas()
            self.init_attention_backend()
            self.init_device_graphs()
        elif self.device == &#34;npu&#34;:
            self.init_attention_backend()
            self.init_device_graphs()
        else:
            self.graph_runner = None
            self.cuda_graph_mem_usage = 0
            self.init_attention_backend()

        # auxiliary hidden capture mode. TODO: expose this to server args?
        if self.spec_algorithm.is_eagle3() and not self.is_draft_worker:
            # load draft config
            draft_model_config = ModelConfig.from_server_args(
                server_args,
                model_path=(server_args.speculative_draft_model_path),
                is_draft_model=True,
            )

            try:
                # get the aux layer from draft model config
                eagle_config = getattr(
                    draft_model_config.hf_config, &#34;eagle_config&#34;, None
                )
                eagle_aux_hidden_state_layer_ids = eagle_config[
                    &#34;eagle_aux_hidden_state_layer_ids&#34;
                ]
            except:
                # if there is no aux layer, set to None
                eagle_aux_hidden_state_layer_ids = None

            self.model.set_eagle3_layers_to_capture(eagle_aux_hidden_state_layer_ids)

    def model_specific_adjustment(self):
        server_args = self.server_args

        if (
            server_args.attention_backend == &#34;intel_amx&#34;
            and server_args.device == &#34;cpu&#34;
            and not _is_cpu_amx_available
        ):
            logger.info(
                &#34;The current platform does not support Intel AMX, will fallback to torch_native backend.&#34;
            )
            server_args.attention_backend = &#34;torch_native&#34;

        if server_args.prefill_attention_backend is not None and (
            server_args.prefill_attention_backend
            == server_args.decode_attention_backend
        ):  # override the default attention backend
            server_args.attention_backend = server_args.prefill_attention_backend

        if (
            getattr(self.model_config.hf_config, &#34;dual_chunk_attention_config&#34;, None)
            is not None
        ):
            if server_args.attention_backend is None:
                server_args.attention_backend = &#34;dual_chunk_flash_attn&#34;
                logger.info(&#34;Dual chunk attention is turned on by default.&#34;)
            elif server_args.attention_backend != &#34;dual_chunk_flash_attn&#34;:
                raise ValueError(
                    &#34;Dual chunk attention is enabled, but attention backend is set to &#34;
                    f&#34;{server_args.attention_backend}. Please set it to &#39;dual_chunk_flash_attn&#39;.&#34;
                )

        if server_args.attention_backend is None:
            &#34;&#34;&#34;
            Auto select the fastest attention backend.

            1. Models with MHA Architecture (e.g: Llama, QWen)
                1.1 We will turn on FA3 on hopper unless user use spec decode with topk &gt; 1 or page_size &gt; 1.
                1.2 In other cases, we will use flashinfer if available, otherwise use triton.
            2. Models with MLA Architecture and using FA3
                2.1 We will use FA3 backend on hopper.
                2.2 We will use Flashinfer backend on blackwell.
                2.3 Otherwise, we will use triton backend.
            &#34;&#34;&#34;

            if not self.use_mla_backend:
                # MHA architecture
                if (
                    is_hopper_with_cuda_12_3()
                    and is_no_spec_infer_or_topk_one(server_args)
                    and is_fa3_default_architecture(self.model_config.hf_config)
                ):
                    server_args.attention_backend = &#34;fa3&#34;
                elif _is_hip:
                    server_args.attention_backend = &#34;aiter&#34;
                elif _is_npu:
                    server_args.attention_backend = &#34;ascend&#34;
                else:
                    server_args.attention_backend = (
                        &#34;flashinfer&#34; if is_flashinfer_available() else &#34;triton&#34;
                    )
            else:
                # MLA architecture
                if is_hopper_with_cuda_12_3():
                    server_args.attention_backend = &#34;fa3&#34;
                elif is_sm100_supported():
                    server_args.attention_backend = &#34;flashinfer&#34;
                elif _is_hip:
                    head_num = self.model_config.get_num_kv_heads(self.tp_size)
                    # TODO current aiter only support head number 16 or 128 head number
                    if (
                        head_num == 128 or head_num == 16
                    ) and self.spec_algorithm.is_none():
                        server_args.attention_backend = &#34;aiter&#34;
                    else:
                        server_args.attention_backend = &#34;triton&#34;
                elif _is_npu:
                    server_args.attention_backend = &#34;ascend&#34;
                else:
                    server_args.attention_backend = &#34;triton&#34;
            logger.info(
                f&#34;Attention backend not explicitly specified. Use {server_args.attention_backend} backend by default.&#34;
            )
        elif self.use_mla_backend:
            if server_args.device != &#34;cpu&#34;:
                if server_args.attention_backend in [
                    &#34;aiter&#34;,
                    &#34;flashinfer&#34;,
                    &#34;fa3&#34;,
                    &#34;triton&#34;,
                    &#34;flashmla&#34;,
                    &#34;cutlass_mla&#34;,
                    &#34;trtllm_mla&#34;,
                    &#34;ascend&#34;,
                ]:
                    logger.info(
                        f&#34;MLA optimization is turned on. Use {server_args.attention_backend} backend.&#34;
                    )
                else:
                    raise ValueError(
                        f&#34;Invalid attention backend for MLA: {server_args.attention_backend}&#34;
                    )
            else:
                if server_args.attention_backend != &#34;intel_amx&#34;:
                    raise ValueError(
                        &#34;MLA optimization not supported on CPU except for intel_amx backend.&#34;
                    )

        if (
            server_args.attention_backend == &#34;fa3&#34;
            and server_args.kv_cache_dtype == &#34;fp8_e5m2&#34;
        ):
            logger.warning(
                &#34;FlashAttention3 only supports fp8_e4m3 if using FP8; &#34;
                &#34;Setting attention backend to triton.&#34;
            )
            server_args.attention_backend = &#34;triton&#34;

        if server_args.enable_double_sparsity:
            logger.info(
                &#34;Double sparsity optimization is turned on. Use triton backend without CUDA graph.&#34;
            )
            server_args.attention_backend = &#34;triton&#34;
            server_args.disable_cuda_graph = True
            if server_args.ds_heavy_channel_type is None:
                raise ValueError(
                    &#34;Please specify the heavy channel type for double sparsity optimization.&#34;
                )
            self.init_double_sparsity_channel_config(server_args.ds_heavy_channel_type)

        if self.is_multimodal:
            if not self.is_multimodal_chunked_prefill_supported:
                server_args.chunked_prefill_size = -1
                logger.info(
                    f&#34;Automatically turn off --chunked-prefill-size as it is not supported for &#34;
                    f&#34;{self.model_config.hf_config.model_type}&#34;
                )

        if not self.use_mla_backend:
            server_args.disable_chunked_prefix_cache = True

        if not server_args.disable_chunked_prefix_cache:
            logger.info(&#34;Chunked prefix cache is turned on.&#34;)

        if server_args.attention_backend == &#34;aiter&#34;:
            if self.model_config.context_len &gt; 8192:
                self.mem_fraction_static *= 0.85

        if (
            server_args.enable_hierarchical_cache
            and server_args.hicache_io_backend == &#34;kernel&#34;
        ):
            # fix for the compatibility issue with FlashAttention3 decoding and HiCache kernel backend
            if server_args.decode_attention_backend is None:
                if not self.use_mla_backend:
                    server_args.decode_attention_backend = (
                        &#34;flashinfer&#34; if is_flashinfer_available() else &#34;triton&#34;
                    )
                else:
                    server_args.decode_attention_backend = (
                        &#34;flashinfer&#34; if is_sm100_supported() else &#34;triton&#34;
                    )
            elif server_args.decode_attention_backend == &#34;fa3&#34;:
                server_args.hicache_io_backend = &#34;direct&#34;
                logger.warning(
                    &#34;FlashAttention3 decode backend is not compatible with hierarchical cache. &#34;
                    f&#34;Setting hicache_io_backend to vanilla I/O, which may lead to suboptimal performance with small page sizes.&#34;
                )

    def init_torch_distributed(self):
        logger.info(&#34;Init torch distributed begin.&#34;)

        try:
            torch.get_device_module(self.device).set_device(self.gpu_id)
        except Exception:
            logger.warning(
                f&#34;Context: {self.device=} {self.gpu_id=} {os.environ.get(&#39;CUDA_VISIBLE_DEVICES&#39;)=} {self.tp_rank=} {self.tp_size=}&#34;
            )
            raise

        if self.device == &#34;cuda&#34;:
            backend = &#34;nccl&#34;
        elif self.device == &#34;xpu&#34;:
            backend = &#34;xccl&#34;
        elif self.device == &#34;hpu&#34;:
            backend = &#34;hccl&#34;
        elif self.device == &#34;cpu&#34;:
            backend = &#34;gloo&#34;
        elif self.device == &#34;npu&#34;:
            backend = &#34;hccl&#34;

        before_avail_memory = get_available_gpu_memory(self.device, self.gpu_id)
        if not self.server_args.enable_p2p_check:
            monkey_patch_p2p_access_check()

        if self.server_args.dist_init_addr:
            dist_init_method = f&#34;tcp://{self.server_args.dist_init_addr}&#34;
        else:
            dist_init_method = f&#34;tcp://127.0.0.1:{self.dist_port}&#34;
        set_custom_all_reduce(not self.server_args.disable_custom_all_reduce)
        set_mscclpp_all_reduce(self.server_args.enable_mscclpp)

        if not self.is_draft_worker:
            if self.device == &#34;cpu&#34;:
                if _is_cpu_amx_available:
                    # Bind OpenMP threads to CPU cores
                    torch.ops.sgl_kernel.init_cpu_threads_env(self.local_omp_cpuid)

                    # Set local size to hint SGLang to use shared memory based AllReduce
                    os.environ[&#34;LOCAL_SIZE&#34;] = str(self.tp_size)
                    torch.ops.sgl_kernel.initialize(self.tp_size, self.tp_rank)
                else:
                    logger.warning(
                        &#34;init_cpu_threads_env and shared memory based AllReduce is disabled since intel amx backend is not available&#34;
                    )

            # Only initialize the distributed environment on the target model worker.
            init_distributed_environment(
                backend=backend,
                world_size=self.tp_size * self.pp_size,
                rank=self.tp_size * self.pp_rank + self.tp_rank,
                local_rank=self.gpu_id,
                distributed_init_method=dist_init_method,
                timeout=self.server_args.dist_timeout,
            )
            initialize_model_parallel(
                tensor_model_parallel_size=self.tp_size,
                pipeline_model_parallel_size=self.pp_size,
                expert_model_parallel_size=self.moe_ep_size,
                duplicate_tp_group=self.server_args.enable_pdmux,
            )
            initialize_dp_attention(
                server_args=self.server_args,
                model_config=self.model_config,
            )

        min_per_gpu_memory = get_available_gpu_memory(
            self.device,
            self.gpu_id,
            distributed=get_world_group().world_size &gt; 1,
            cpu_group=get_world_group().cpu_group,
        )
        self.tp_group = get_tp_group()
        self.attention_tp_group = get_attention_tp_group()

        # Check memory for tensor parallelism
        local_gpu_memory = get_available_gpu_memory(self.device, self.gpu_id)
        if self.tp_size &gt; 1 and not self.is_draft_worker:
            if min_per_gpu_memory &lt; local_gpu_memory * 0.9:
                if get_bool_env_var(&#34;SGL_DISABLE_TP_MEMORY_INBALANCE_CHECK&#34;):
                    logger.warning(
                        &#34;The memory capacity is unbalanced. Some GPUs may be occupied by other processes. &#34;
                        f&#34;{min_per_gpu_memory=}, {local_gpu_memory=}, {local_gpu_memory * 0.9=}&#34;
                    )
                else:
                    raise ValueError(
                        &#34;The memory capacity is unbalanced. Some GPUs may be occupied by other processes. &#34;
                        f&#34;{min_per_gpu_memory=}, {local_gpu_memory=}, {local_gpu_memory * 0.9=}&#34;
                    )

        logger.info(
            f&#34;Init torch distributed ends. mem usage={(before_avail_memory - local_gpu_memory):.2f} GB&#34;
        )
        return min_per_gpu_memory

    def load_model(self):
        before_avail_memory = get_available_gpu_memory(self.device, self.gpu_id)
        logger.info(
            f&#34;Load weight begin. avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB&#34;
        )

        # This can reduce thread conflicts and speed up weight loading.
        if self.device != &#34;cpu&#34;:
            torch.set_num_threads(1)
        if self.device == &#34;cuda&#34;:
            if torch.cuda.get_device_capability()[0] &lt; 8:
                logger.info(
                    &#34;Compute capability below sm80. Use float16 due to lack of bfloat16 support.&#34;
                )
                self.server_args.dtype = &#34;float16&#34;
                self.model_config.dtype = torch.float16
                if torch.cuda.get_device_capability()[1] &lt; 5:
                    raise RuntimeError(&#34;SGLang only supports sm75 and above.&#34;)

        set_cuda_arch()

        # Prepare the model config
        self.load_config = LoadConfig(
            load_format=self.server_args.load_format,
            download_dir=self.server_args.download_dir,
            model_loader_extra_config=self.server_args.model_loader_extra_config,
        )
        if self.device == &#34;cpu&#34;:
            self.model_config = adjust_config_with_unaligned_cpu_tp(
                self.model_config, self.load_config, self.tp_size
            )
        if self.server_args.load_format == &#34;gguf&#34;:
            monkey_patch_vllm_gguf_config()

        # Load the model
        # Remove monkey_patch when linear.py quant remove dependencies with vllm
        monkey_patch_vllm_parallel_state()
        monkey_patch_isinstance_for_vllm_base_layer()

        with self.memory_saver_adapter.region(GPU_MEMORY_TYPE_WEIGHTS):
            self.model = get_model(
                model_config=self.model_config,
                load_config=self.load_config,
                device_config=DeviceConfig(self.device),
            )
        monkey_patch_vllm_parallel_state(reverse=True)
        monkey_patch_isinstance_for_vllm_base_layer(reverse=True)

        get_offloader().post_init()

        if self.server_args.kv_cache_dtype == &#34;fp8_e4m3&#34;:
            if self.server_args.quantization_param_path is not None:
                if callable(getattr(self.model, &#34;load_kv_cache_scales&#34;, None)):
                    self.model.load_kv_cache_scales(
                        self.server_args.quantization_param_path
                    )
                    logger.info(
                        &#34;Loaded KV cache scaling factors from %s&#34;,
                        self.server_args.quantization_param_path,
                    )
                else:
                    raise RuntimeError(
                        &#34;Using FP8 KV cache and scaling factors provided but &#34;
                        &#34;model %s does not support loading scaling factors.&#34;,
                        self.model.__class__,
                    )
            else:
                logger.warning(
                    &#34;Using FP8 KV cache but no scaling factors &#34;
                    &#34;provided. Defaulting to scaling factors of 1.0. &#34;
                    &#34;This may lead to less accurate results!&#34;
                )

        # Parse other args
        self.sliding_window_size = None
        if hasattr(self.model, &#34;get_attention_sliding_window_size&#34;):
            self.sliding_window_size = self.model.get_attention_sliding_window_size()
        elif self.model_config.attention_chunk_size is not None:
            self.sliding_window_size = self.model_config.attention_chunk_size
            logger.info(
                f&#34;Setting sliding_window_size to be attention_chunk_size: {self.sliding_window_size}&#34;
            )

        self.dtype = self.model_config.dtype

        after_avail_memory = get_available_gpu_memory(self.device, self.gpu_id)
        self.weight_load_mem_usage = before_avail_memory - after_avail_memory
        logger.info(
            f&#34;Load weight end. &#34;
            f&#34;type={type(self.model).__name__}, &#34;
            f&#34;dtype={self.dtype}, &#34;
            f&#34;avail mem={after_avail_memory:.2f} GB, &#34;
            f&#34;mem usage={self.weight_load_mem_usage:.2f} GB.&#34;
        )

        # Handle the case where some ranks do not finish loading.
        try:
            dist.monitored_barrier(
                group=get_tp_group().cpu_group,
                timeout=datetime.timedelta(seconds=UNBALANCED_MODEL_LOADING_TIMEOUT_S),
                wait_all_ranks=True,
            )
        except RuntimeError:
            raise ValueError(
                f&#34;TP rank {self.tp_rank} could finish the model loading, but there are other ranks that didn&#39;t finish loading. It is likely due to unexpected failures (e.g., OOM) or a slow node.&#34;
            ) from None

    def update_expert_location(
        self,
        new_expert_location_metadata: ExpertLocationMetadata,
        update_layer_ids: List[int],
    ):
        self.expert_location_updater.update(
            self.model.routed_experts_weights_of_layer,
            new_expert_location_metadata,
            update_layer_ids=update_layer_ids,
            nnodes=self.server_args.nnodes,
            rank=self.tp_rank,
        )

    def update_weights_from_disk(
        self, model_path: str, load_format: str
    ) -&gt; tuple[bool, str]:
        &#34;&#34;&#34;Update engine weights in-place from the disk.&#34;&#34;&#34;
        logger.info(
            f&#34;Update engine weights online from disk begin. &#34;
            f&#34;avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB&#34;
        )

        target_device = torch.device(self.device)
        self.model_config.model_path = model_path
        load_config = LoadConfig(load_format=load_format)

        # Only support DefaultModelLoader for now
        loader = get_model_loader(load_config)
        if not isinstance(loader, DefaultModelLoader):
            message = f&#34;Failed to get model loader: {loader}.&#34;
            return False, message

        def get_weight_iter(config):
            iter = loader._get_weights_iterator(
                DefaultModelLoader.Source.init_new(config, self.model)
            )
            return iter

        def model_load_weights(model, iter):
            DefaultModelLoader.load_weights_and_postprocess(model, iter, target_device)
            return model

        with set_default_torch_dtype(self.model_config.dtype):
            try:
                iter = get_weight_iter(self.model_config)
            except Exception as e:
                message = f&#34;Failed to get weights iterator: {e}.&#34;
                return False, message
            try:
                model = model_load_weights(self.model, iter)
            except Exception as e:
                message = (
                    f&#34;Failed to update weights: {e}.\nRolling back to original weights.&#34;
                )
                del iter
                gc.collect()
                iter = get_weight_iter(self.model_config)
                self.model = model_load_weights(self.model, iter)
                return False, message

        self.model = model
        self.server_args.model_path = model_path
        self.server_args.load_format = load_format
        self.load_config = load_config

        logger.info(&#34;Update weights end.&#34;)
        return True, &#34;Succeeded to update model weights.&#34;

    def init_weights_update_group(
        self,
        master_address,
        master_port,
        rank_offset,
        world_size,
        group_name,
        backend=&#34;nccl&#34;,
    ):
        &#34;&#34;&#34;Initialize the Torch process group for model parameter updates.

        `_model_update_group` is used in the RLHF workflow, where rank
        0 is the actor model in the training engine, and the other ranks are
        the inference engine, which is used for rollout.

        In the RLHF workflow, the training engine updates the model
        weights/parameters online, and broadcasts them to the inference
        engine through the `_model_update_group` process group.
        &#34;&#34;&#34;
        assert (
            torch.distributed.is_initialized()
        ), &#34;Default torch process group must be initialized&#34;
        assert group_name != &#34;&#34;, &#34;Group name cannot be empty&#34;

        rank = rank_offset + self.tp_rank

        logger.info(
            f&#34;init custom process group: master_address={master_address}, master_port={master_port}, &#34;
            f&#34;rank_offset={rank_offset}, rank={rank}, world_size={world_size}, group_name={group_name}, backend={backend}&#34;
        )

        try:
            self._model_update_group[group_name] = init_custom_process_group(
                backend=backend,
                init_method=f&#34;tcp://{master_address}:{master_port}&#34;,
                world_size=world_size,
                rank=rank,
                group_name=group_name,
            )
            return True, &#34;Succeeded to initialize custom process group.&#34;
        except Exception as e:
            message = f&#34;Failed to initialize custom process group: {e}.&#34;
            logger.error(message)
            return False, message

    def update_weights_from_distributed(self, names, dtypes, shapes, group_name):
        &#34;&#34;&#34;
        Update specific parameter in the model weights online
        through `_model_update_group` process group.

        Args:
            name: the name of the parameter to be updated.
            dtype: the data type of the parameter to be updated.
            shape: the shape of the parameter to be updated.
        &#34;&#34;&#34;

        assert group_name in self._model_update_group, (
            f&#34;Group {group_name} not in {list(self._model_update_group.keys())}. &#34;
            &#34;Please call `init_weights_update_group` first.&#34;
        )

        try:
            weights = []
            handles = []
            for name, dtype, shape in zip(names, dtypes, shapes):
                target_dtype = (
                    dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)
                )
                weight = torch.empty(shape, dtype=target_dtype, device=self.device)
                handles.append(
                    torch.distributed.broadcast(
                        weight,
                        src=0,
                        group=self._model_update_group[group_name],
                        async_op=True,
                    )
                )
                weights.append((name, weight))
            for handle in handles:
                handle.wait()

            self.model.load_weights(weights)
            return True, f&#34;Succeeded to update parameter online.&#34;

        except Exception as e:
            error_msg = (
                f&#34;Failed to update parameter online: {e}. &#34;
                f&#34;The full weights of the ModelRunner are partially updated. &#34;
                f&#34;Please discard the whole weights.&#34;
            )
            logger.error(error_msg)
            return False, error_msg

    def update_weights_from_tensor(
        self,
        named_tensors: List[Tuple[str, Union[torch.Tensor, &#34;LocalSerializedTensor&#34;]]],
        load_format: Optional[str] = None,
    ):
        monkey_patch_torch_reductions()
        if load_format == &#34;flattened_bucket&#34;:
            # Handle flattened bucket format
            return self._update_weights_from_flattened_bucket(
                flattened_tensor_bucket_dict=named_tensors
            )

        # We need to get device after patch otherwise the device would be wrong
        self.device_module = torch.get_device_module(self.device)
        infered_device = self.device_module.current_device()

        named_tensors = [
            (name, _unwrap_tensor(tensor, tp_rank=self.tp_rank, device=infered_device))
            for name, tensor in named_tensors
        ]
        if load_format == &#34;direct&#34;:
            _model_load_weights_direct(self.model, named_tensors)
        elif load_format in self.server_args.custom_weight_loader:
            custom_loader = dynamic_import(load_format)
            custom_loader(self.model, named_tensors)
        elif load_format is None:
            self.model.load_weights(named_tensors)
        else:
            raise NotImplementedError(f&#34;Unknown load_format={load_format}&#34;)
        return True, &#34;Success&#34;

    def _update_weights_from_flattened_bucket(
        self,
        flattened_tensor_bucket_dict,
    ):
        &#34;&#34;&#34;Handle flattened bucket format for weight updates&#34;&#34;&#34;
        flattened_tensor = flattened_tensor_bucket_dict[&#34;flattened_tensor&#34;]
        metadata = flattened_tensor_bucket_dict[&#34;metadata&#34;]

        # Convert metadata dict to our format
        converted_metadata = []
        for meta in metadata:
            converted_meta = FlattenedTensorMetadata(
                name=meta.name,
                shape=meta.shape,
                dtype=meta.dtype,
                start_idx=meta.start_idx,
                end_idx=meta.end_idx,
                numel=meta.numel,
            )
            converted_metadata.append(converted_meta)

        # Create bucket and reconstruct tensors
        bucket = FlattenedTensorBucket(
            flattened_tensor=flattened_tensor, metadata=converted_metadata
        )
        reconstructed_tensors = bucket.reconstruct_tensors()

        # Load the reconstructed tensors using the standard method
        self.model.load_weights(reconstructed_tensors)

        return True, &#34;Success&#34;

    def get_weights_by_name(
        self, name: str, truncate_size: int = 100
    ) -&gt; Optional[torch.Tensor]:
        &#34;&#34;&#34;Get the weights of the parameter by its name. Similar to `get_parameter` in Hugging Face.

        Only used for unit test with an unoptimized performance.
        For optimized performance, please use torch.save and torch.load.
        &#34;&#34;&#34;
        # TODO: (chenyang) Add support for Qwen models.
        try:
            return self.model.get_weights_by_name(
                name, truncate_size, tp_size=self.tp_size
            )
        except Exception as e:
            logger.error(f&#34;Error when getting parameter {name}: {e}&#34;)
            return None

    def init_lora_manager(self):
        self.lora_manager = LoRAManager(
            base_model=self.model,
            base_hf_config=self.model_config.hf_config,
            max_loras_per_batch=self.server_args.max_loras_per_batch,
            load_config=self.load_config,
            dtype=self.dtype,
            lora_backend=self.server_args.lora_backend,
            tp_size=self.tp_size,
            tp_rank=self.tp_rank,
            max_lora_rank=self.server_args.max_lora_rank,
            target_modules=self.server_args.lora_target_modules,
            lora_paths=self.server_args.lora_paths,
        )

    def load_lora_adapter(self, lora_ref: LoRARef):
        &#34;&#34;&#34;Load a new lora adapter from disk or huggingface.&#34;&#34;&#34;

        logger.info(
            f&#34;LoRA adapter loading starts: {lora_ref}. &#34;
            f&#34;avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB&#34;
        )

        result = self.lora_manager.load_lora_adapter(lora_ref)

        logger.info(
            f&#34;LoRA adapter loading completes: {lora_ref}. &#34;
            f&#34;avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB&#34;
        )

        return result

    def unload_lora_adapter(self, lora_ref: LoRARef):
        &#34;&#34;&#34;Unload a lora adapter that was previously loaded during initialization or dynamic loading.&#34;&#34;&#34;

        logger.info(
            f&#34;LoRA adapter unloading starts: {lora_ref}. &#34;
            f&#34;avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB&#34;
        )

        result = self.lora_manager.unload_lora_adapter(lora_ref)

        logger.info(
            f&#34;LoRA adapter unloading completes: {lora_ref}. &#34;
            f&#34;avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB&#34;
        )

        return result

    def profile_max_num_token(self, total_gpu_memory: int):
        available_gpu_memory = get_available_gpu_memory(
            self.device,
            self.gpu_id,
            distributed=get_world_group().world_size &gt; 1,
            cpu_group=get_world_group().cpu_group,
        )
        if self.is_draft_worker:
            num_layers = getattr(
                self.model_config.hf_config,
                &#34;num_nextn_predict_layers&#34;,
                self.num_effective_layers,
            )
        else:
            num_layers = self.num_effective_layers
        if self.use_mla_backend:
            cell_size = (
                (self.model_config.kv_lora_rank + self.model_config.qk_rope_head_dim)
                * num_layers
                * torch._utils._element_size(self.kv_cache_dtype)
            )
        else:
            cell_size = (
                self.model_config.get_num_kv_heads(get_attention_tp_size())
                * self.model_config.head_dim
                * num_layers
                * 2
                * torch._utils._element_size(self.kv_cache_dtype)
            )
        rest_memory = available_gpu_memory - total_gpu_memory * (
            1 - self.mem_fraction_static
        )
        max_num_token = int(rest_memory * (1 &lt;&lt; 30) // cell_size)
        return max_num_token

    def set_num_token_hybrid(self):
        if (
            &#34;Llama4ForConditionalGeneration&#34;
            in self.model_config.hf_config.architectures
        ):
            temp_ratio = (
                (1 - self.is_hybrid)
                + self.is_hybrid
                * self.attention_chunk_size
                / self.model_config.context_len
            )
            self.swa_max_total_num_tokens = (
                4 * self.max_total_num_tokens * temp_ratio // (3 * temp_ratio + 1)
            )
            self.full_max_total_num_tokens = (
                4 * self.max_total_num_tokens
                - 12 * self.max_total_num_tokens * temp_ratio // (3 * temp_ratio + 1)
            )
            self.swa_max_total_num_tokens = int(
                self.swa_max_total_num_tokens
                // self.server_args.page_size
                * self.server_args.page_size
            )
            self.full_max_total_num_tokens = int(
                self.full_max_total_num_tokens
                // self.server_args.page_size
                * self.server_args.page_size
            )
            self.max_total_num_tokens = self.full_max_total_num_tokens
        else:
            assert self.sliding_window_size is not None and self.sliding_window_size &gt; 0
            full_attention_layer_ids = []
            swa_attention_layer_ids = []

            try:
                layers = self.model.model.layers
            except:
                try:
                    layers = self.model.language_model.model.layers
                except:
                    try:
                        layers = self.model.language_model.layers
                    except:
                        self.is_hybrid = False
                        return

            for layer in layers:
                if (
                    layer.self_attn.attn.sliding_window_size is None
                    or layer.self_attn.attn.sliding_window_size == -1
                ):
                    full_attention_layer_ids.append(layer.layer_id)
                else:
                    swa_attention_layer_ids.append(layer.layer_id)
            self.model_config.swa_attention_layer_ids = swa_attention_layer_ids
            self.model_config.full_attention_layer_ids = full_attention_layer_ids

            # Algorithm:
            # Existing max_total_num_tokens is per layer and assume all layers have the same number of tokens.
            # - Find total # of tokens available across layers.
            # - Calculate full_max_total_num_tokens and swa_max_total_num_tokens based on the given swa_full_tokens_ratio.
            total_tokens = (
                self.max_total_num_tokens * self.model_config.num_hidden_layers
            )
            full_layers_num = len(full_attention_layer_ids)
            swa_layers_num = len(swa_attention_layer_ids)
            swa_full_tokens_ratio = self.server_args.swa_full_tokens_ratio

            # Solve the equations:
            # 1. swa_max_total_num_tokens * swa_layers_num + full_max_total_num_tokens * full_layers_num == total_tokens
            # 2. full_max_total_num_tokens * swa_full_tokens_ratio == swa_max_total_num_tokens
            denominator = swa_full_tokens_ratio * swa_layers_num + full_layers_num
            self.full_max_total_num_tokens = int(total_tokens / denominator)
            self.swa_max_total_num_tokens = int(
                self.full_max_total_num_tokens * swa_full_tokens_ratio
            )
            self.max_total_num_tokens = self.full_max_total_num_tokens

            logger.info(
                f&#34;Use Sliding window memory pool. full_layer_tokens={self.full_max_total_num_tokens}, swa_layer_tokens={self.swa_max_total_num_tokens}&#34;
            )

    def init_memory_pool(
        self,
        total_gpu_memory: int,
        max_num_reqs: Optional[int] = None,
        max_total_tokens: Optional[int] = None,
    ):
        # Determine the kv cache dtype
        if self.server_args.kv_cache_dtype == &#34;auto&#34;:
            self.kv_cache_dtype = self.dtype
        elif self.server_args.kv_cache_dtype == &#34;fp8_e5m2&#34;:
            if _is_hip:  # Using natively supported format
                self.kv_cache_dtype = torch.float8_e5m2fnuz
            else:
                self.kv_cache_dtype = torch.float8_e5m2
        elif self.server_args.kv_cache_dtype == &#34;fp8_e4m3&#34;:
            if _is_hip:  # Using natively supported format
                self.kv_cache_dtype = torch.float8_e4m3fnuz
            else:
                self.kv_cache_dtype = torch.float8_e4m3fn
        else:
            raise ValueError(
                f&#34;Unsupported kv_cache_dtype: {self.server_args.kv_cache_dtype}.&#34;
            )

        self.max_total_num_tokens = self.profile_max_num_token(total_gpu_memory)
        if SGLANG_CI_SMALL_KV_SIZE:
            self.max_total_num_tokens = int(SGLANG_CI_SMALL_KV_SIZE)

        if max_num_reqs is None:
            max_num_reqs = min(
                max(
                    int(
                        self.max_total_num_tokens / self.model_config.context_len * 512
                    ),
                    2048,
                ),
                4096,
            )

        if not self.spec_algorithm.is_none():
            if self.is_draft_worker:
                self.max_total_num_tokens = self.server_args.draft_runner_cache_size
                max_num_reqs = self.server_args.max_num_reqs
            else:
                # We are sharing the `token_to_kv_pool`, and both verify and draft tokens
                # can be concurrently allocated, so we should give a headroom for it.
                self.server_args.draft_runner_cache_size = (
                    self.max_total_num_tokens
                    # draft
                    + max_num_reqs
                    * self.server_args.speculative_num_steps
                    * self.server_args.speculative_eagle_topk
                    # verify
                    + max_num_reqs * self.server_args.speculative_num_draft_tokens
                    # buffer
                    + 100
                )
                # Target worker and draft worker shares the same indices for the
                # token_to_kv_pool, so we should make sure to match max_total_num_tokens.
                self.max_total_num_tokens = self.server_args.draft_runner_cache_size
                self.server_args.max_num_reqs = max_num_reqs

        if max_total_tokens is not None:
            if max_total_tokens &gt; self.max_total_num_tokens:
                logging.warning(
                    f&#34;max_total_tokens={max_total_tokens} is larger than the profiled value &#34;
                    f&#34;{self.max_total_num_tokens}. &#34;
                    f&#34;Use the profiled value instead.&#34;
                )
            self.max_total_num_tokens = min(self.max_total_num_tokens, max_total_tokens)

        self.max_total_num_tokens = (
            self.max_total_num_tokens
            // self.server_args.page_size
            * self.server_args.page_size
        )
        # create token size for hybrid cache
        if self.is_hybrid:
            self.set_num_token_hybrid()

        if self.max_total_num_tokens &lt;= 0:
            raise RuntimeError(
                &#34;Not enough memory. Please try to increase --mem-fraction-static.&#34;
            )

        # Initialize req_to_token_pool
        if self.req_to_token_pool is None:
            # FIXME(lsyin): this is the temporary fix for the context length issue when using speculative decoding
            extra_max_context_len = 4
            if self.server_args.speculative_num_draft_tokens is not None:
                extra_max_context_len += self.server_args.speculative_num_draft_tokens

            if self.server_args.disaggregation_mode == &#34;decode&#34;:
                from sglang.srt.disaggregation.decode import DecodeReqToTokenPool

                # subscribe memory for pre-allocated requests
                # if max_num_reqs &lt;= 32, we pre-allocate 2x requests
                pre_alloc_size = max_num_reqs * 2 if max_num_reqs &lt;= 32 else 0
                self.req_to_token_pool = DecodeReqToTokenPool(
                    size=max_num_reqs,
                    max_context_len=self.model_config.context_len
                    + extra_max_context_len,
                    device=self.device,
                    enable_memory_saver=self.server_args.enable_memory_saver,
                    pre_alloc_size=pre_alloc_size,
                )
            else:
                self.req_to_token_pool = ReqToTokenPool(
                    size=max_num_reqs,
                    max_context_len=self.model_config.context_len
                    + extra_max_context_len,
                    device=self.device,
                    enable_memory_saver=self.server_args.enable_memory_saver,
                )
        else:
            # Draft worker shares req_to_token_pool with the target worker.
            assert self.is_draft_worker

        # Initialize token_to_kv_pool
        if self.server_args.attention_backend == &#34;ascend&#34;:
            if self.use_mla_backend:
                self.token_to_kv_pool = AscendMLAPagedTokenToKVPool(
                    self.max_total_num_tokens,
                    page_size=self.page_size,
                    dtype=self.kv_cache_dtype,
                    kv_lora_rank=self.model_config.kv_lora_rank,
                    qk_rope_head_dim=self.model_config.qk_rope_head_dim,
                    layer_num=self.num_effective_layers,
                    device=self.device,
                    enable_memory_saver=self.server_args.enable_memory_saver,
                    start_layer=self.start_layer,
                    end_layer=self.end_layer,
                )
            else:
                self.token_to_kv_pool = AscendTokenToKVPool(
                    self.max_total_num_tokens,
                    page_size=self.page_size,
                    dtype=self.kv_cache_dtype,
                    head_num=self.model_config.get_num_kv_heads(
                        get_attention_tp_size()
                    ),
                    head_dim=self.model_config.head_dim,
                    layer_num=self.model_config.num_hidden_layers,
                    device=self.device,
                    enable_memory_saver=self.server_args.enable_memory_saver,
                )
        elif self.use_mla_backend:
            self.token_to_kv_pool = MLATokenToKVPool(
                self.max_total_num_tokens,
                page_size=self.page_size,
                dtype=self.kv_cache_dtype,
                kv_lora_rank=self.model_config.kv_lora_rank,
                qk_rope_head_dim=self.model_config.qk_rope_head_dim,
                layer_num=self.num_effective_layers,
                device=self.device,
                enable_memory_saver=self.server_args.enable_memory_saver,
                start_layer=self.start_layer,
                end_layer=self.end_layer,
            )
        elif self.server_args.enable_double_sparsity:
            self.token_to_kv_pool = DoubleSparseTokenToKVPool(
                self.max_total_num_tokens,
                page_size=self.page_size,
                dtype=self.kv_cache_dtype,
                head_num=self.model_config.get_num_kv_heads(get_attention_tp_size()),
                head_dim=self.model_config.head_dim,
                layer_num=self.num_effective_layers,
                device=self.device,
                heavy_channel_num=self.server_args.ds_heavy_channel_num,
                enable_memory_saver=self.server_args.enable_memory_saver,
                start_layer=self.start_layer,
                end_layer=self.end_layer,
            )
        else:
            if self.is_hybrid:
                self.token_to_kv_pool = SWAKVPool(
                    size=self.full_max_total_num_tokens,
                    size_swa=self.swa_max_total_num_tokens,
                    dtype=self.kv_cache_dtype,
                    head_num=self.model_config.get_num_kv_heads(
                        get_attention_tp_size()
                    ),
                    head_dim=self.model_config.head_dim,
                    swa_attention_layer_ids=self.model_config.swa_attention_layer_ids,
                    full_attention_layer_ids=self.model_config.full_attention_layer_ids,
                    enable_kvcache_transpose=False,
                    device=self.device,
                )
            else:
                self.token_to_kv_pool = MHATokenToKVPool(
                    self.max_total_num_tokens,
                    page_size=self.page_size,
                    dtype=self.kv_cache_dtype,
                    head_num=self.model_config.get_num_kv_heads(
                        get_attention_tp_size()
                    ),
                    head_dim=self.model_config.head_dim,
                    layer_num=self.num_effective_layers,
                    device=self.device,
                    enable_memory_saver=self.server_args.enable_memory_saver,
                    start_layer=self.start_layer,
                    end_layer=self.end_layer,
                )

        # Initialize token_to_kv_pool_allocator
        need_sort = self.server_args.disaggregation_mode in (&#34;decode&#34;, &#34;prefill&#34;)
        if self.token_to_kv_pool_allocator is None:
            if self.server_args.attention_backend == &#34;ascend&#34;:
                self.token_to_kv_pool_allocator = AscendPagedTokenToKVPoolAllocator(
                    self.max_total_num_tokens,
                    page_size=self.page_size,
                    dtype=self.kv_cache_dtype,
                    device=self.device,
                    kvcache=self.token_to_kv_pool,
                    need_sort=need_sort,
                )
            else:
                if self.page_size == 1:
                    if self.is_hybrid:
                        self.token_to_kv_pool_allocator = SWATokenToKVPoolAllocator(
                            self.full_max_total_num_tokens,
                            self.swa_max_total_num_tokens,
                            dtype=self.kv_cache_dtype,
                            device=self.device,
                            kvcache=self.token_to_kv_pool,
                            need_sort=need_sort,
                        )
                    else:
                        self.token_to_kv_pool_allocator = TokenToKVPoolAllocator(
                            self.max_total_num_tokens,
                            dtype=self.kv_cache_dtype,
                            device=self.device,
                            kvcache=self.token_to_kv_pool,
                            need_sort=need_sort,
                        )
                else:
                    assert not self.is_hybrid
                    self.token_to_kv_pool_allocator = PagedTokenToKVPoolAllocator(
                        self.max_total_num_tokens,
                        page_size=self.page_size,
                        dtype=self.kv_cache_dtype,
                        device=self.device,
                        kvcache=self.token_to_kv_pool,
                        need_sort=need_sort,
                    )
        else:
            assert self.is_draft_worker

        logger.info(
            f&#34;Memory pool end. &#34;
            f&#34;avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB&#34;
        )

    def init_cublas(self):
        &#34;&#34;&#34;We need to run a small matmul to init cublas. Otherwise, it will raise some errors later.&#34;&#34;&#34;
        dtype = torch.float16
        device = &#34;cuda&#34;
        a = torch.ones((16, 16), dtype=dtype, device=device)
        b = torch.ones((16, 16), dtype=dtype, device=device)
        c = a @ b
        return c

    def init_attention_backend(self):
        &#34;&#34;&#34;Init attention kernel backend.&#34;&#34;&#34;
        if self.server_args.enable_two_batch_overlap and not self.is_draft_worker:
            self.attn_backend = TboAttnBackend.init_new(self._get_attention_backend)
        else:
            self.attn_backend = self._get_attention_backend()

    def _get_attention_backend(self):
        &#34;&#34;&#34;Init attention kernel backend.&#34;&#34;&#34;
        self.decode_attention_backend_str = (
            self.server_args.decode_attention_backend
            if self.server_args.decode_attention_backend
            else self.server_args.attention_backend
        )
        self.prefill_attention_backend_str = (
            self.server_args.prefill_attention_backend
            if self.server_args.prefill_attention_backend
            else self.server_args.attention_backend
        )
        if self.decode_attention_backend_str != self.prefill_attention_backend_str:
            from sglang.srt.layers.attention.hybrid_attn_backend import (
                HybridAttnBackend,
            )

            attn_backend = HybridAttnBackend(
                self,
                decode_backend=self._get_attention_backend_from_str(
                    self.decode_attention_backend_str
                ),
                prefill_backend=self._get_attention_backend_from_str(
                    self.prefill_attention_backend_str
                ),
            )
            logger.info(
                f&#34;Using hybrid attention backend for decode and prefill: &#34;
                f&#34;decode_backend={self.decode_attention_backend_str}, &#34;
                f&#34;prefill_backend={self.prefill_attention_backend_str}.&#34;
            )
            logger.warning(
                f&#34;Warning: Attention backend specified by --attention-backend or default backend might be overridden.&#34;
                f&#34;The feature of hybrid attention backend is experimental and unstable. Please raise an issue if you encounter any problem.&#34;
            )
        else:
            attn_backend = self._get_attention_backend_from_str(
                self.server_args.attention_backend
            )

        global_server_args_dict.update(
            {
                &#34;decode_attention_backend&#34;: self.decode_attention_backend_str,
                &#34;prefill_attention_backend&#34;: self.prefill_attention_backend_str,
            }
        )
        return attn_backend

    def _get_attention_backend_from_str(self, backend_str: str):
        if backend_str == &#34;flashinfer&#34;:
            if not self.use_mla_backend:
                from sglang.srt.layers.attention.flashinfer_backend import (
                    FlashInferAttnBackend,
                )

                # Init streams
                if self.server_args.speculative_algorithm == &#34;EAGLE&#34;:
                    if (
                        not hasattr(self, &#34;plan_stream_for_flashinfer&#34;)
                        or not self.plan_stream_for_flashinfer
                    ):
                        self.plan_stream_for_flashinfer = torch.cuda.Stream()
                return FlashInferAttnBackend(self)
            else:
                from sglang.srt.layers.attention.flashinfer_mla_backend import (
                    FlashInferMLAAttnBackend,
                )

                return FlashInferMLAAttnBackend(self)
        elif backend_str == &#34;aiter&#34;:
            from sglang.srt.layers.attention.aiter_backend import AiterAttnBackend

            return AiterAttnBackend(self)
        elif self.server_args.attention_backend == &#34;wave&#34;:
            from sglang.srt.layers.attention.wave_backend import WaveAttnBackend

            return WaveAttnBackend(self)
        elif backend_str == &#34;ascend&#34;:
            from sglang.srt.layers.attention.ascend_backend import AscendAttnBackend

            return AscendAttnBackend(self)
        elif backend_str == &#34;triton&#34;:
            assert not self.model_config.is_encoder_decoder, (
                &#34;Cross attention is not supported in the triton attention backend. &#34;
                &#34;Please use `--attention-backend flashinfer`.&#34;
            )
            if self.server_args.enable_double_sparsity:
                from sglang.srt.layers.attention.double_sparsity_backend import (
                    DoubleSparseAttnBackend,
                )

                return DoubleSparseAttnBackend(self)
            else:
                from sglang.srt.layers.attention.triton_backend import TritonAttnBackend

                return TritonAttnBackend(self)
        elif backend_str == &#34;torch_native&#34;:
            from sglang.srt.layers.attention.torch_native_backend import (
                TorchNativeAttnBackend,
            )

            return TorchNativeAttnBackend(self)
        elif backend_str == &#34;flashmla&#34;:
            from sglang.srt.layers.attention.flashmla_backend import FlashMLABackend

            return FlashMLABackend(self)
        elif backend_str == &#34;fa3&#34;:
            assert (
                torch.cuda.get_device_capability()[0] == 8 and not self.use_mla_backend
            ) or torch.cuda.get_device_capability()[0] == 9, (
                &#34;FlashAttention v3 Backend requires SM&gt;=80 and SM&lt;=90. &#34;
                &#34;Please use `--attention-backend flashinfer`.&#34;
            )
            from sglang.srt.layers.attention.flashattention_backend import (
                FlashAttentionBackend,
            )

            return FlashAttentionBackend(self)
        elif backend_str == &#34;cutlass_mla&#34;:
            from sglang.srt.layers.attention.cutlass_mla_backend import (
                CutlassMLABackend,
            )

            return CutlassMLABackend(self)
        elif backend_str == &#34;trtllm_mla&#34;:
            if not self.use_mla_backend:
                raise ValueError(&#34;trtllm_mla backend can only be used with MLA models.&#34;)
            from sglang.srt.layers.attention.trtllm_mla_backend import TRTLLMMLABackend

            return TRTLLMMLABackend(self)
        elif backend_str == &#34;trtllm_mha&#34;:
            if self.use_mla_backend:
                raise ValueError(
                    &#34;trtllm_mha backend can only be used with non-MLA models.&#34;
                )
            from sglang.srt.layers.attention.trtllm_mha_backend import (
                TRTLLMHAAttnBackend,
            )

            return TRTLLMHAAttnBackend(self)
        elif backend_str == &#34;intel_amx&#34;:
            from sglang.srt.layers.attention.intel_amx_backend import (
                IntelAMXAttnBackend,
            )

            return IntelAMXAttnBackend(self)
        elif backend_str == &#34;dual_chunk_flash_attn&#34;:
            from sglang.srt.layers.attention.dual_chunk_flashattention_backend import (
                DualChunkFlashAttentionBackend,
            )

            return DualChunkFlashAttentionBackend(self)
        else:
            raise ValueError(f&#34;Invalid attention backend: {backend_str}&#34;)

    def init_double_sparsity_channel_config(self, selected_channel):
        selected_channel = &#34;.&#34; + selected_channel + &#34;_proj&#34;
        self.sorted_channels = []
        # load channel config
        with open(self.server_args.ds_channel_config_path, &#34;r&#34;) as f:
            channel_config = json.load(f)

        for i in range(self.start_layer, self.end_layer):
            key = &#34;model.layers.&#34; + str(i) + &#34;.self_attn&#34; + selected_channel
            self.sorted_channels.append(
                torch.tensor(channel_config[key])[
                    :, : self.server_args.ds_heavy_channel_num
                ]
                .contiguous()
                .cuda()
            )

    def init_device_graphs(self):
        &#34;&#34;&#34;Capture cuda graphs.&#34;&#34;&#34;
        self.graph_runner = None
        self.cuda_graph_mem_usage = 0

        if not self.is_generation:
            # TODO: Currently, cuda graph only captures decode steps, which only exists for generation models
            return

        if self.server_args.disable_cuda_graph:
            return

        tic = time.perf_counter()
        before_mem = get_available_gpu_memory(self.device, self.gpu_id)
        logger.info(
            f&#34;Capture cuda graph begin. This can take up to several minutes. avail mem={before_mem:.2f} GB&#34;
        )
        self.graph_runner = (
            CudaGraphRunner(self) if not _is_npu else NPUGraphRunner(self)
        )
        after_mem = get_available_gpu_memory(self.device, self.gpu_id)
        self.cuda_graph_mem_usage = before_mem - after_mem
        logger.info(
            f&#34;Capture cuda graph end. Time elapsed: {time.perf_counter() - tic:.2f} s. &#34;
            f&#34;mem usage={self.cuda_graph_mem_usage:.2f} GB. avail mem={after_mem:.2f} GB.&#34;
        )

    def init_threads_binding(self):
        omp_cpuids = os.environ.get(&#34;SGLANG_CPU_OMP_THREADS_BIND&#34;, &#34;all&#34;)
        if omp_cpuids == &#34;all&#34;:
            cpu_ids_by_node = get_cpu_ids_by_node()
            n_numa_node = len(cpu_ids_by_node)

            assert self.tp_size &lt;= n_numa_node, (
                f&#34;SGLANG_CPU_OMP_THREADS_BIND is not set, in this case, &#34;
                f&#34;tp_size {self.tp_size} should be smaller than or equal to number of numa node on the machine {n_numa_node}. &#34;
                f&#34;If you need tp_size to be larger than number of numa node, please set the CPU cores for each tp rank via SGLANG_CPU_OMP_THREADS_BIND explicitly. &#34;
                f&#34;For example, on a machine with 2 numa nodes, where core 0-31 are on numa node 0 and core 32-63 are on numa node 1, &#34;
                f&#34;it is suggested to use -tp 2 and bind tp rank 0 to core 0-31 and tp rank 1 to core 32-63. &#34;
                f&#34;This is the default behavior if SGLANG_CPU_OMP_THREADS_BIND is not set and it is the same as setting SGLANG_CPU_OMP_THREADS_BIND=0-31|32-63. &#34;
                f&#34;If you do need tp_size to be larger than the number of numa nodes, you could set SGLANG_CPU_OMP_THREADS_BIND explicitly for example SGLANG_CPU_OMP_THREADS_BIND=0-15|16-31|32-47|48-63 and run with -tp 4. &#34;
                f&#34;If you don&#39;t want each tp rank to use all the cores on one numa node, you could set for example SGLANG_CPU_OMP_THREADS_BIND=0-15|32-47 and run with -tp 2.&#34;
            )
            if self.tp_size &lt; n_numa_node:
                logger.warning(
                    f&#34;Detected the current machine has {n_numa_node} numa nodes available, but tp_size is set to {self.tp_size}, so only {self.tp_size} numa nodes are used.&#34;
                )
            self.local_omp_cpuid = cpu_ids_by_node[self.tp_rank]
        else:
            self.local_omp_cpuid = omp_cpuids.split(&#34;|&#34;)[self.tp_rank]

    def apply_torch_tp(self):
        logger.info(f&#34;Enabling torch tensor parallelism on {self.tp_size} devices.&#34;)
        from sglang.srt.model_parallel import tensor_parallel

        device_mesh = torch.distributed.init_device_mesh(self.device, (self.tp_size,))
        tensor_parallel(self.model, device_mesh)

    def forward_decode(
        self,
        forward_batch: ForwardBatch,
        skip_attn_backend_init: bool = False,
        pp_proxy_tensors=None,
    ) -&gt; LogitsProcessorOutput:
        if not skip_attn_backend_init:
            self.attn_backend.init_forward_metadata(forward_batch)
        # FIXME: add pp_proxy_tensors arg to all models
        kwargs = {}
        if self.support_pp:
            kwargs[&#34;pp_proxy_tensors&#34;] = pp_proxy_tensors
        return self.model.forward(
            forward_batch.input_ids,
            forward_batch.positions,
            forward_batch,
            **kwargs,
        )

    def forward_extend(
        self,
        forward_batch: ForwardBatch,
        skip_attn_backend_init: bool = False,
        pp_proxy_tensors=None,
    ) -&gt; LogitsProcessorOutput:
        if not skip_attn_backend_init:
            self.attn_backend.init_forward_metadata(forward_batch)

        kwargs = {}
        if self.support_pp:
            kwargs[&#34;pp_proxy_tensors&#34;] = pp_proxy_tensors
        if forward_batch.input_embeds is not None:
            kwargs[&#34;input_embeds&#34;] = forward_batch.input_embeds.bfloat16()
        if not self.is_generation:
            kwargs[&#34;get_embedding&#34;] = True
        return self.model.forward(
            forward_batch.input_ids,
            forward_batch.positions,
            forward_batch,
            **kwargs,
        )

    def forward_idle(
        self, forward_batch: ForwardBatch, pp_proxy_tensors=None
    ) -&gt; LogitsProcessorOutput:
        kwargs = {}
        if self.support_pp:
            kwargs[&#34;pp_proxy_tensors&#34;] = pp_proxy_tensors
        return self.model.forward(
            forward_batch.input_ids,
            forward_batch.positions,
            forward_batch,
            **kwargs,
        )

    def forward_split_prefill(
        self,
        forward_batch: ForwardBatch,
        reinit_attn_backend: bool = False,
        forward_count: int = 1,
    ) -&gt; LogitsProcessorOutput:
        if forward_batch.split_index == 0 or reinit_attn_backend:
            self.attn_backend.init_forward_metadata(forward_batch)
        next_split_index = min(
            forward_batch.split_index + forward_count,
            self.model_config.num_hidden_layers,
        )
        ret = self.model.forward_split_prefill(
            forward_batch.input_ids,
            forward_batch.positions,
            forward_batch,
            (forward_batch.split_index, next_split_index),
        )
        forward_batch.split_index = next_split_index
        return ret

    def forward(
        self,
        forward_batch: ForwardBatch,
        skip_attn_backend_init: bool = False,
        pp_proxy_tensors: Optional[PPProxyTensors] = None,
        reinit_attn_backend: bool = False,
        split_forward_count: int = 1,
    ) -&gt; Tuple[Union[LogitsProcessorOutput, PPProxyTensors], bool]:
        self.forward_pass_id += 1

        with get_global_expert_distribution_recorder().with_forward_pass(
            self.forward_pass_id,
            forward_batch,
        ):
            output = self._forward_raw(
                forward_batch,
                skip_attn_backend_init,
                pp_proxy_tensors,
                reinit_attn_backend,
                split_forward_count,
            )

        if self.eplb_manager is not None:
            self.eplb_manager.on_forward_pass_end()

        return output

    def _forward_raw(
        self,
        forward_batch: ForwardBatch,
        skip_attn_backend_init: bool,
        pp_proxy_tensors: Optional[PPProxyTensors],
        reinit_attn_backend: bool = False,
        split_forward_count: int = 1,
    ) -&gt; Tuple[Union[LogitsProcessorOutput, PPProxyTensors], bool]:
        can_run_cuda_graph = bool(
            forward_batch.forward_mode.is_cuda_graph()
            and self.graph_runner
            and self.graph_runner.can_run(forward_batch)
        )
        if can_run_cuda_graph:
            ret = self.graph_runner.replay(
                forward_batch,
                skip_attn_backend_init=skip_attn_backend_init,
                pp_proxy_tensors=pp_proxy_tensors,
            )
            return ret, can_run_cuda_graph

        # For MLP sync
        if forward_batch.global_num_tokens_cpu is not None:
            forward_batch.prepare_mlp_sync_batch(self)

        if forward_batch.forward_mode.is_decode():
            ret = self.forward_decode(
                forward_batch,
                skip_attn_backend_init=skip_attn_backend_init,
                pp_proxy_tensors=pp_proxy_tensors,
            )
        elif forward_batch.forward_mode.is_extend():
            ret = self.forward_extend(
                forward_batch,
                skip_attn_backend_init=skip_attn_backend_init,
                pp_proxy_tensors=pp_proxy_tensors,
            )
        elif forward_batch.forward_mode.is_split_prefill():
            ret = self.forward_split_prefill(
                forward_batch,
                reinit_attn_backend=reinit_attn_backend,
                forward_count=split_forward_count,
            )
        elif forward_batch.forward_mode.is_idle():
            ret = self.forward_idle(forward_batch, pp_proxy_tensors=pp_proxy_tensors)
        else:
            raise ValueError(f&#34;Invalid forward mode: {forward_batch.forward_mode}&#34;)

        if forward_batch.global_num_tokens_cpu is not None:
            forward_batch.post_forward_mlp_sync_batch(ret)

        return ret, can_run_cuda_graph

    def _preprocess_logits(
        self, logits_output: LogitsProcessorOutput, sampling_info: SamplingBatchInfo
    ):
        # Apply logit bias
        if sampling_info.sampling_info_done:
            # Overlap mode: the function update_regex_vocab_mask was executed
            # in process_batch_result of the last batch.
            if sampling_info.grammars:
                sampling_info.sampling_info_done.wait()
        else:
            # Normal mode: Put CPU-heavy tasks here. They will be overlapped with the forward pass.
            sampling_info.update_regex_vocab_mask()
        sampling_info.apply_logits_bias(logits_output.next_token_logits)

    def sample(
        self,
        logits_output: LogitsProcessorOutput,
        forward_batch: ForwardBatch,
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;Sample and compute logprobs and update logits_output.

        Args:
            logits_output: The logits output from the model forward
            forward_batch: The forward batch that generates logits_output

        Returns:
            A list of next_token_ids
        &#34;&#34;&#34;
        # For duplex models with multiple output streams.
        if isinstance(logits_output, tuple):
            return torch.stack(
                [self.sample(values, forward_batch) for values in logits_output],
                axis=-1,
            )

        self._preprocess_logits(logits_output, forward_batch.sampling_info)

        # Sample the next tokens
        next_token_ids = self.sampler(
            logits_output,
            forward_batch.sampling_info,
            forward_batch.return_logprob,
            forward_batch.top_logprobs_nums,
            forward_batch.token_ids_logprobs,
        )
        return next_token_ids

    @property
    def model_is_mrope(self) -&gt; bool:
        &#34;&#34;&#34;Detect if the model has &#34;mrope&#34; rope_scaling type.
        mrope requires keep &#34;rope_deltas&#34; between prompt and decoding phases.&#34;&#34;&#34;
        rope_scaling = getattr(self.model_config.hf_text_config, &#34;rope_scaling&#34;, {})
        if rope_scaling is None:
            return False
        is_mrope_enabled = &#34;mrope_section&#34; in rope_scaling
        return is_mrope_enabled

    def save_remote_model(self, url: str):
        from sglang.srt.model_loader.loader import RemoteModelLoader

        logger.info(f&#34;Saving model to {url}&#34;)
        RemoteModelLoader.save_model(self.model, self.model_config.model_path, url)

    def save_sharded_model(
        self, path: str, pattern: Optional[str] = None, max_size: Optional[int] = None
    ):
        from sglang.srt.model_loader.loader import ShardedStateLoader

        logger.info(
            f&#34;Save sharded model to {path} with pattern {pattern} and max_size {max_size}&#34;
        )
        ShardedStateLoader.save_model(self.model, path, pattern, max_size)</code></pre>
</details>
<div class="desc"><p>ModelRunner runs the forward passes of the models.</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.model_is_mrope"><code class="name">prop <span class="ident">model_is_mrope</span> : bool</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def model_is_mrope(self) -&gt; bool:
    &#34;&#34;&#34;Detect if the model has &#34;mrope&#34; rope_scaling type.
    mrope requires keep &#34;rope_deltas&#34; between prompt and decoding phases.&#34;&#34;&#34;
    rope_scaling = getattr(self.model_config.hf_text_config, &#34;rope_scaling&#34;, {})
    if rope_scaling is None:
        return False
    is_mrope_enabled = &#34;mrope_section&#34; in rope_scaling
    return is_mrope_enabled</code></pre>
</details>
<div class="desc"><p>Detect if the model has "mrope" rope_scaling type.
mrope requires keep "rope_deltas" between prompt and decoding phases.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.apply_torch_tp"><code class="name flex">
<span>def <span class="ident">apply_torch_tp</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_torch_tp(self):
    logger.info(f&#34;Enabling torch tensor parallelism on {self.tp_size} devices.&#34;)
    from sglang.srt.model_parallel import tensor_parallel

    device_mesh = torch.distributed.init_device_mesh(self.device, (self.tp_size,))
    tensor_parallel(self.model, device_mesh)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>skip_attn_backend_init: bool = False,<br>pp_proxy_tensors: <a title="sglang.srt.model_executor.forward_batch_info.PPProxyTensors" href="forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.PPProxyTensors">PPProxyTensors</a> | None = None,<br>reinit_attn_backend: bool = False,<br>split_forward_count: int = 1) ‑> Tuple[<a title="sglang.srt.layers.logits_processor.LogitsProcessorOutput" href="../layers/logits_processor.html#sglang.srt.layers.logits_processor.LogitsProcessorOutput">LogitsProcessorOutput</a> | <a title="sglang.srt.model_executor.forward_batch_info.PPProxyTensors" href="forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.PPProxyTensors">PPProxyTensors</a>, bool]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    forward_batch: ForwardBatch,
    skip_attn_backend_init: bool = False,
    pp_proxy_tensors: Optional[PPProxyTensors] = None,
    reinit_attn_backend: bool = False,
    split_forward_count: int = 1,
) -&gt; Tuple[Union[LogitsProcessorOutput, PPProxyTensors], bool]:
    self.forward_pass_id += 1

    with get_global_expert_distribution_recorder().with_forward_pass(
        self.forward_pass_id,
        forward_batch,
    ):
        output = self._forward_raw(
            forward_batch,
            skip_attn_backend_init,
            pp_proxy_tensors,
            reinit_attn_backend,
            split_forward_count,
        )

    if self.eplb_manager is not None:
        self.eplb_manager.on_forward_pass_end()

    return output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.forward_decode"><code class="name flex">
<span>def <span class="ident">forward_decode</span></span>(<span>self,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>skip_attn_backend_init: bool = False,<br>pp_proxy_tensors=None) ‑> <a title="sglang.srt.layers.logits_processor.LogitsProcessorOutput" href="../layers/logits_processor.html#sglang.srt.layers.logits_processor.LogitsProcessorOutput">LogitsProcessorOutput</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_decode(
    self,
    forward_batch: ForwardBatch,
    skip_attn_backend_init: bool = False,
    pp_proxy_tensors=None,
) -&gt; LogitsProcessorOutput:
    if not skip_attn_backend_init:
        self.attn_backend.init_forward_metadata(forward_batch)
    # FIXME: add pp_proxy_tensors arg to all models
    kwargs = {}
    if self.support_pp:
        kwargs[&#34;pp_proxy_tensors&#34;] = pp_proxy_tensors
    return self.model.forward(
        forward_batch.input_ids,
        forward_batch.positions,
        forward_batch,
        **kwargs,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.forward_extend"><code class="name flex">
<span>def <span class="ident">forward_extend</span></span>(<span>self,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>skip_attn_backend_init: bool = False,<br>pp_proxy_tensors=None) ‑> <a title="sglang.srt.layers.logits_processor.LogitsProcessorOutput" href="../layers/logits_processor.html#sglang.srt.layers.logits_processor.LogitsProcessorOutput">LogitsProcessorOutput</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_extend(
    self,
    forward_batch: ForwardBatch,
    skip_attn_backend_init: bool = False,
    pp_proxy_tensors=None,
) -&gt; LogitsProcessorOutput:
    if not skip_attn_backend_init:
        self.attn_backend.init_forward_metadata(forward_batch)

    kwargs = {}
    if self.support_pp:
        kwargs[&#34;pp_proxy_tensors&#34;] = pp_proxy_tensors
    if forward_batch.input_embeds is not None:
        kwargs[&#34;input_embeds&#34;] = forward_batch.input_embeds.bfloat16()
    if not self.is_generation:
        kwargs[&#34;get_embedding&#34;] = True
    return self.model.forward(
        forward_batch.input_ids,
        forward_batch.positions,
        forward_batch,
        **kwargs,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.forward_idle"><code class="name flex">
<span>def <span class="ident">forward_idle</span></span>(<span>self,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>pp_proxy_tensors=None) ‑> <a title="sglang.srt.layers.logits_processor.LogitsProcessorOutput" href="../layers/logits_processor.html#sglang.srt.layers.logits_processor.LogitsProcessorOutput">LogitsProcessorOutput</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_idle(
    self, forward_batch: ForwardBatch, pp_proxy_tensors=None
) -&gt; LogitsProcessorOutput:
    kwargs = {}
    if self.support_pp:
        kwargs[&#34;pp_proxy_tensors&#34;] = pp_proxy_tensors
    return self.model.forward(
        forward_batch.input_ids,
        forward_batch.positions,
        forward_batch,
        **kwargs,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.forward_split_prefill"><code class="name flex">
<span>def <span class="ident">forward_split_prefill</span></span>(<span>self,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>reinit_attn_backend: bool = False,<br>forward_count: int = 1) ‑> <a title="sglang.srt.layers.logits_processor.LogitsProcessorOutput" href="../layers/logits_processor.html#sglang.srt.layers.logits_processor.LogitsProcessorOutput">LogitsProcessorOutput</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_split_prefill(
    self,
    forward_batch: ForwardBatch,
    reinit_attn_backend: bool = False,
    forward_count: int = 1,
) -&gt; LogitsProcessorOutput:
    if forward_batch.split_index == 0 or reinit_attn_backend:
        self.attn_backend.init_forward_metadata(forward_batch)
    next_split_index = min(
        forward_batch.split_index + forward_count,
        self.model_config.num_hidden_layers,
    )
    ret = self.model.forward_split_prefill(
        forward_batch.input_ids,
        forward_batch.positions,
        forward_batch,
        (forward_batch.split_index, next_split_index),
    )
    forward_batch.split_index = next_split_index
    return ret</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.get_weights_by_name"><code class="name flex">
<span>def <span class="ident">get_weights_by_name</span></span>(<span>self, name: str, truncate_size: int = 100) ‑> torch.Tensor | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_weights_by_name(
    self, name: str, truncate_size: int = 100
) -&gt; Optional[torch.Tensor]:
    &#34;&#34;&#34;Get the weights of the parameter by its name. Similar to `get_parameter` in Hugging Face.

    Only used for unit test with an unoptimized performance.
    For optimized performance, please use torch.save and torch.load.
    &#34;&#34;&#34;
    # TODO: (chenyang) Add support for Qwen models.
    try:
        return self.model.get_weights_by_name(
            name, truncate_size, tp_size=self.tp_size
        )
    except Exception as e:
        logger.error(f&#34;Error when getting parameter {name}: {e}&#34;)
        return None</code></pre>
</details>
<div class="desc"><p>Get the weights of the parameter by its name. Similar to <code>get_parameter</code> in Hugging Face.</p>
<p>Only used for unit test with an unoptimized performance.
For optimized performance, please use torch.save and torch.load.</p></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.init_attention_backend"><code class="name flex">
<span>def <span class="ident">init_attention_backend</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_attention_backend(self):
    &#34;&#34;&#34;Init attention kernel backend.&#34;&#34;&#34;
    if self.server_args.enable_two_batch_overlap and not self.is_draft_worker:
        self.attn_backend = TboAttnBackend.init_new(self._get_attention_backend)
    else:
        self.attn_backend = self._get_attention_backend()</code></pre>
</details>
<div class="desc"><p>Init attention kernel backend.</p></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.init_cublas"><code class="name flex">
<span>def <span class="ident">init_cublas</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_cublas(self):
    &#34;&#34;&#34;We need to run a small matmul to init cublas. Otherwise, it will raise some errors later.&#34;&#34;&#34;
    dtype = torch.float16
    device = &#34;cuda&#34;
    a = torch.ones((16, 16), dtype=dtype, device=device)
    b = torch.ones((16, 16), dtype=dtype, device=device)
    c = a @ b
    return c</code></pre>
</details>
<div class="desc"><p>We need to run a small matmul to init cublas. Otherwise, it will raise some errors later.</p></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.init_device_graphs"><code class="name flex">
<span>def <span class="ident">init_device_graphs</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_device_graphs(self):
    &#34;&#34;&#34;Capture cuda graphs.&#34;&#34;&#34;
    self.graph_runner = None
    self.cuda_graph_mem_usage = 0

    if not self.is_generation:
        # TODO: Currently, cuda graph only captures decode steps, which only exists for generation models
        return

    if self.server_args.disable_cuda_graph:
        return

    tic = time.perf_counter()
    before_mem = get_available_gpu_memory(self.device, self.gpu_id)
    logger.info(
        f&#34;Capture cuda graph begin. This can take up to several minutes. avail mem={before_mem:.2f} GB&#34;
    )
    self.graph_runner = (
        CudaGraphRunner(self) if not _is_npu else NPUGraphRunner(self)
    )
    after_mem = get_available_gpu_memory(self.device, self.gpu_id)
    self.cuda_graph_mem_usage = before_mem - after_mem
    logger.info(
        f&#34;Capture cuda graph end. Time elapsed: {time.perf_counter() - tic:.2f} s. &#34;
        f&#34;mem usage={self.cuda_graph_mem_usage:.2f} GB. avail mem={after_mem:.2f} GB.&#34;
    )</code></pre>
</details>
<div class="desc"><p>Capture cuda graphs.</p></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.init_double_sparsity_channel_config"><code class="name flex">
<span>def <span class="ident">init_double_sparsity_channel_config</span></span>(<span>self, selected_channel)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_double_sparsity_channel_config(self, selected_channel):
    selected_channel = &#34;.&#34; + selected_channel + &#34;_proj&#34;
    self.sorted_channels = []
    # load channel config
    with open(self.server_args.ds_channel_config_path, &#34;r&#34;) as f:
        channel_config = json.load(f)

    for i in range(self.start_layer, self.end_layer):
        key = &#34;model.layers.&#34; + str(i) + &#34;.self_attn&#34; + selected_channel
        self.sorted_channels.append(
            torch.tensor(channel_config[key])[
                :, : self.server_args.ds_heavy_channel_num
            ]
            .contiguous()
            .cuda()
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.init_lora_manager"><code class="name flex">
<span>def <span class="ident">init_lora_manager</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_lora_manager(self):
    self.lora_manager = LoRAManager(
        base_model=self.model,
        base_hf_config=self.model_config.hf_config,
        max_loras_per_batch=self.server_args.max_loras_per_batch,
        load_config=self.load_config,
        dtype=self.dtype,
        lora_backend=self.server_args.lora_backend,
        tp_size=self.tp_size,
        tp_rank=self.tp_rank,
        max_lora_rank=self.server_args.max_lora_rank,
        target_modules=self.server_args.lora_target_modules,
        lora_paths=self.server_args.lora_paths,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.init_memory_pool"><code class="name flex">
<span>def <span class="ident">init_memory_pool</span></span>(<span>self,<br>total_gpu_memory: int,<br>max_num_reqs: int | None = None,<br>max_total_tokens: int | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_memory_pool(
    self,
    total_gpu_memory: int,
    max_num_reqs: Optional[int] = None,
    max_total_tokens: Optional[int] = None,
):
    # Determine the kv cache dtype
    if self.server_args.kv_cache_dtype == &#34;auto&#34;:
        self.kv_cache_dtype = self.dtype
    elif self.server_args.kv_cache_dtype == &#34;fp8_e5m2&#34;:
        if _is_hip:  # Using natively supported format
            self.kv_cache_dtype = torch.float8_e5m2fnuz
        else:
            self.kv_cache_dtype = torch.float8_e5m2
    elif self.server_args.kv_cache_dtype == &#34;fp8_e4m3&#34;:
        if _is_hip:  # Using natively supported format
            self.kv_cache_dtype = torch.float8_e4m3fnuz
        else:
            self.kv_cache_dtype = torch.float8_e4m3fn
    else:
        raise ValueError(
            f&#34;Unsupported kv_cache_dtype: {self.server_args.kv_cache_dtype}.&#34;
        )

    self.max_total_num_tokens = self.profile_max_num_token(total_gpu_memory)
    if SGLANG_CI_SMALL_KV_SIZE:
        self.max_total_num_tokens = int(SGLANG_CI_SMALL_KV_SIZE)

    if max_num_reqs is None:
        max_num_reqs = min(
            max(
                int(
                    self.max_total_num_tokens / self.model_config.context_len * 512
                ),
                2048,
            ),
            4096,
        )

    if not self.spec_algorithm.is_none():
        if self.is_draft_worker:
            self.max_total_num_tokens = self.server_args.draft_runner_cache_size
            max_num_reqs = self.server_args.max_num_reqs
        else:
            # We are sharing the `token_to_kv_pool`, and both verify and draft tokens
            # can be concurrently allocated, so we should give a headroom for it.
            self.server_args.draft_runner_cache_size = (
                self.max_total_num_tokens
                # draft
                + max_num_reqs
                * self.server_args.speculative_num_steps
                * self.server_args.speculative_eagle_topk
                # verify
                + max_num_reqs * self.server_args.speculative_num_draft_tokens
                # buffer
                + 100
            )
            # Target worker and draft worker shares the same indices for the
            # token_to_kv_pool, so we should make sure to match max_total_num_tokens.
            self.max_total_num_tokens = self.server_args.draft_runner_cache_size
            self.server_args.max_num_reqs = max_num_reqs

    if max_total_tokens is not None:
        if max_total_tokens &gt; self.max_total_num_tokens:
            logging.warning(
                f&#34;max_total_tokens={max_total_tokens} is larger than the profiled value &#34;
                f&#34;{self.max_total_num_tokens}. &#34;
                f&#34;Use the profiled value instead.&#34;
            )
        self.max_total_num_tokens = min(self.max_total_num_tokens, max_total_tokens)

    self.max_total_num_tokens = (
        self.max_total_num_tokens
        // self.server_args.page_size
        * self.server_args.page_size
    )
    # create token size for hybrid cache
    if self.is_hybrid:
        self.set_num_token_hybrid()

    if self.max_total_num_tokens &lt;= 0:
        raise RuntimeError(
            &#34;Not enough memory. Please try to increase --mem-fraction-static.&#34;
        )

    # Initialize req_to_token_pool
    if self.req_to_token_pool is None:
        # FIXME(lsyin): this is the temporary fix for the context length issue when using speculative decoding
        extra_max_context_len = 4
        if self.server_args.speculative_num_draft_tokens is not None:
            extra_max_context_len += self.server_args.speculative_num_draft_tokens

        if self.server_args.disaggregation_mode == &#34;decode&#34;:
            from sglang.srt.disaggregation.decode import DecodeReqToTokenPool

            # subscribe memory for pre-allocated requests
            # if max_num_reqs &lt;= 32, we pre-allocate 2x requests
            pre_alloc_size = max_num_reqs * 2 if max_num_reqs &lt;= 32 else 0
            self.req_to_token_pool = DecodeReqToTokenPool(
                size=max_num_reqs,
                max_context_len=self.model_config.context_len
                + extra_max_context_len,
                device=self.device,
                enable_memory_saver=self.server_args.enable_memory_saver,
                pre_alloc_size=pre_alloc_size,
            )
        else:
            self.req_to_token_pool = ReqToTokenPool(
                size=max_num_reqs,
                max_context_len=self.model_config.context_len
                + extra_max_context_len,
                device=self.device,
                enable_memory_saver=self.server_args.enable_memory_saver,
            )
    else:
        # Draft worker shares req_to_token_pool with the target worker.
        assert self.is_draft_worker

    # Initialize token_to_kv_pool
    if self.server_args.attention_backend == &#34;ascend&#34;:
        if self.use_mla_backend:
            self.token_to_kv_pool = AscendMLAPagedTokenToKVPool(
                self.max_total_num_tokens,
                page_size=self.page_size,
                dtype=self.kv_cache_dtype,
                kv_lora_rank=self.model_config.kv_lora_rank,
                qk_rope_head_dim=self.model_config.qk_rope_head_dim,
                layer_num=self.num_effective_layers,
                device=self.device,
                enable_memory_saver=self.server_args.enable_memory_saver,
                start_layer=self.start_layer,
                end_layer=self.end_layer,
            )
        else:
            self.token_to_kv_pool = AscendTokenToKVPool(
                self.max_total_num_tokens,
                page_size=self.page_size,
                dtype=self.kv_cache_dtype,
                head_num=self.model_config.get_num_kv_heads(
                    get_attention_tp_size()
                ),
                head_dim=self.model_config.head_dim,
                layer_num=self.model_config.num_hidden_layers,
                device=self.device,
                enable_memory_saver=self.server_args.enable_memory_saver,
            )
    elif self.use_mla_backend:
        self.token_to_kv_pool = MLATokenToKVPool(
            self.max_total_num_tokens,
            page_size=self.page_size,
            dtype=self.kv_cache_dtype,
            kv_lora_rank=self.model_config.kv_lora_rank,
            qk_rope_head_dim=self.model_config.qk_rope_head_dim,
            layer_num=self.num_effective_layers,
            device=self.device,
            enable_memory_saver=self.server_args.enable_memory_saver,
            start_layer=self.start_layer,
            end_layer=self.end_layer,
        )
    elif self.server_args.enable_double_sparsity:
        self.token_to_kv_pool = DoubleSparseTokenToKVPool(
            self.max_total_num_tokens,
            page_size=self.page_size,
            dtype=self.kv_cache_dtype,
            head_num=self.model_config.get_num_kv_heads(get_attention_tp_size()),
            head_dim=self.model_config.head_dim,
            layer_num=self.num_effective_layers,
            device=self.device,
            heavy_channel_num=self.server_args.ds_heavy_channel_num,
            enable_memory_saver=self.server_args.enable_memory_saver,
            start_layer=self.start_layer,
            end_layer=self.end_layer,
        )
    else:
        if self.is_hybrid:
            self.token_to_kv_pool = SWAKVPool(
                size=self.full_max_total_num_tokens,
                size_swa=self.swa_max_total_num_tokens,
                dtype=self.kv_cache_dtype,
                head_num=self.model_config.get_num_kv_heads(
                    get_attention_tp_size()
                ),
                head_dim=self.model_config.head_dim,
                swa_attention_layer_ids=self.model_config.swa_attention_layer_ids,
                full_attention_layer_ids=self.model_config.full_attention_layer_ids,
                enable_kvcache_transpose=False,
                device=self.device,
            )
        else:
            self.token_to_kv_pool = MHATokenToKVPool(
                self.max_total_num_tokens,
                page_size=self.page_size,
                dtype=self.kv_cache_dtype,
                head_num=self.model_config.get_num_kv_heads(
                    get_attention_tp_size()
                ),
                head_dim=self.model_config.head_dim,
                layer_num=self.num_effective_layers,
                device=self.device,
                enable_memory_saver=self.server_args.enable_memory_saver,
                start_layer=self.start_layer,
                end_layer=self.end_layer,
            )

    # Initialize token_to_kv_pool_allocator
    need_sort = self.server_args.disaggregation_mode in (&#34;decode&#34;, &#34;prefill&#34;)
    if self.token_to_kv_pool_allocator is None:
        if self.server_args.attention_backend == &#34;ascend&#34;:
            self.token_to_kv_pool_allocator = AscendPagedTokenToKVPoolAllocator(
                self.max_total_num_tokens,
                page_size=self.page_size,
                dtype=self.kv_cache_dtype,
                device=self.device,
                kvcache=self.token_to_kv_pool,
                need_sort=need_sort,
            )
        else:
            if self.page_size == 1:
                if self.is_hybrid:
                    self.token_to_kv_pool_allocator = SWATokenToKVPoolAllocator(
                        self.full_max_total_num_tokens,
                        self.swa_max_total_num_tokens,
                        dtype=self.kv_cache_dtype,
                        device=self.device,
                        kvcache=self.token_to_kv_pool,
                        need_sort=need_sort,
                    )
                else:
                    self.token_to_kv_pool_allocator = TokenToKVPoolAllocator(
                        self.max_total_num_tokens,
                        dtype=self.kv_cache_dtype,
                        device=self.device,
                        kvcache=self.token_to_kv_pool,
                        need_sort=need_sort,
                    )
            else:
                assert not self.is_hybrid
                self.token_to_kv_pool_allocator = PagedTokenToKVPoolAllocator(
                    self.max_total_num_tokens,
                    page_size=self.page_size,
                    dtype=self.kv_cache_dtype,
                    device=self.device,
                    kvcache=self.token_to_kv_pool,
                    need_sort=need_sort,
                )
    else:
        assert self.is_draft_worker

    logger.info(
        f&#34;Memory pool end. &#34;
        f&#34;avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB&#34;
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.init_threads_binding"><code class="name flex">
<span>def <span class="ident">init_threads_binding</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_threads_binding(self):
    omp_cpuids = os.environ.get(&#34;SGLANG_CPU_OMP_THREADS_BIND&#34;, &#34;all&#34;)
    if omp_cpuids == &#34;all&#34;:
        cpu_ids_by_node = get_cpu_ids_by_node()
        n_numa_node = len(cpu_ids_by_node)

        assert self.tp_size &lt;= n_numa_node, (
            f&#34;SGLANG_CPU_OMP_THREADS_BIND is not set, in this case, &#34;
            f&#34;tp_size {self.tp_size} should be smaller than or equal to number of numa node on the machine {n_numa_node}. &#34;
            f&#34;If you need tp_size to be larger than number of numa node, please set the CPU cores for each tp rank via SGLANG_CPU_OMP_THREADS_BIND explicitly. &#34;
            f&#34;For example, on a machine with 2 numa nodes, where core 0-31 are on numa node 0 and core 32-63 are on numa node 1, &#34;
            f&#34;it is suggested to use -tp 2 and bind tp rank 0 to core 0-31 and tp rank 1 to core 32-63. &#34;
            f&#34;This is the default behavior if SGLANG_CPU_OMP_THREADS_BIND is not set and it is the same as setting SGLANG_CPU_OMP_THREADS_BIND=0-31|32-63. &#34;
            f&#34;If you do need tp_size to be larger than the number of numa nodes, you could set SGLANG_CPU_OMP_THREADS_BIND explicitly for example SGLANG_CPU_OMP_THREADS_BIND=0-15|16-31|32-47|48-63 and run with -tp 4. &#34;
            f&#34;If you don&#39;t want each tp rank to use all the cores on one numa node, you could set for example SGLANG_CPU_OMP_THREADS_BIND=0-15|32-47 and run with -tp 2.&#34;
        )
        if self.tp_size &lt; n_numa_node:
            logger.warning(
                f&#34;Detected the current machine has {n_numa_node} numa nodes available, but tp_size is set to {self.tp_size}, so only {self.tp_size} numa nodes are used.&#34;
            )
        self.local_omp_cpuid = cpu_ids_by_node[self.tp_rank]
    else:
        self.local_omp_cpuid = omp_cpuids.split(&#34;|&#34;)[self.tp_rank]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.init_torch_distributed"><code class="name flex">
<span>def <span class="ident">init_torch_distributed</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_torch_distributed(self):
    logger.info(&#34;Init torch distributed begin.&#34;)

    try:
        torch.get_device_module(self.device).set_device(self.gpu_id)
    except Exception:
        logger.warning(
            f&#34;Context: {self.device=} {self.gpu_id=} {os.environ.get(&#39;CUDA_VISIBLE_DEVICES&#39;)=} {self.tp_rank=} {self.tp_size=}&#34;
        )
        raise

    if self.device == &#34;cuda&#34;:
        backend = &#34;nccl&#34;
    elif self.device == &#34;xpu&#34;:
        backend = &#34;xccl&#34;
    elif self.device == &#34;hpu&#34;:
        backend = &#34;hccl&#34;
    elif self.device == &#34;cpu&#34;:
        backend = &#34;gloo&#34;
    elif self.device == &#34;npu&#34;:
        backend = &#34;hccl&#34;

    before_avail_memory = get_available_gpu_memory(self.device, self.gpu_id)
    if not self.server_args.enable_p2p_check:
        monkey_patch_p2p_access_check()

    if self.server_args.dist_init_addr:
        dist_init_method = f&#34;tcp://{self.server_args.dist_init_addr}&#34;
    else:
        dist_init_method = f&#34;tcp://127.0.0.1:{self.dist_port}&#34;
    set_custom_all_reduce(not self.server_args.disable_custom_all_reduce)
    set_mscclpp_all_reduce(self.server_args.enable_mscclpp)

    if not self.is_draft_worker:
        if self.device == &#34;cpu&#34;:
            if _is_cpu_amx_available:
                # Bind OpenMP threads to CPU cores
                torch.ops.sgl_kernel.init_cpu_threads_env(self.local_omp_cpuid)

                # Set local size to hint SGLang to use shared memory based AllReduce
                os.environ[&#34;LOCAL_SIZE&#34;] = str(self.tp_size)
                torch.ops.sgl_kernel.initialize(self.tp_size, self.tp_rank)
            else:
                logger.warning(
                    &#34;init_cpu_threads_env and shared memory based AllReduce is disabled since intel amx backend is not available&#34;
                )

        # Only initialize the distributed environment on the target model worker.
        init_distributed_environment(
            backend=backend,
            world_size=self.tp_size * self.pp_size,
            rank=self.tp_size * self.pp_rank + self.tp_rank,
            local_rank=self.gpu_id,
            distributed_init_method=dist_init_method,
            timeout=self.server_args.dist_timeout,
        )
        initialize_model_parallel(
            tensor_model_parallel_size=self.tp_size,
            pipeline_model_parallel_size=self.pp_size,
            expert_model_parallel_size=self.moe_ep_size,
            duplicate_tp_group=self.server_args.enable_pdmux,
        )
        initialize_dp_attention(
            server_args=self.server_args,
            model_config=self.model_config,
        )

    min_per_gpu_memory = get_available_gpu_memory(
        self.device,
        self.gpu_id,
        distributed=get_world_group().world_size &gt; 1,
        cpu_group=get_world_group().cpu_group,
    )
    self.tp_group = get_tp_group()
    self.attention_tp_group = get_attention_tp_group()

    # Check memory for tensor parallelism
    local_gpu_memory = get_available_gpu_memory(self.device, self.gpu_id)
    if self.tp_size &gt; 1 and not self.is_draft_worker:
        if min_per_gpu_memory &lt; local_gpu_memory * 0.9:
            if get_bool_env_var(&#34;SGL_DISABLE_TP_MEMORY_INBALANCE_CHECK&#34;):
                logger.warning(
                    &#34;The memory capacity is unbalanced. Some GPUs may be occupied by other processes. &#34;
                    f&#34;{min_per_gpu_memory=}, {local_gpu_memory=}, {local_gpu_memory * 0.9=}&#34;
                )
            else:
                raise ValueError(
                    &#34;The memory capacity is unbalanced. Some GPUs may be occupied by other processes. &#34;
                    f&#34;{min_per_gpu_memory=}, {local_gpu_memory=}, {local_gpu_memory * 0.9=}&#34;
                )

    logger.info(
        f&#34;Init torch distributed ends. mem usage={(before_avail_memory - local_gpu_memory):.2f} GB&#34;
    )
    return min_per_gpu_memory</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.init_weights_update_group"><code class="name flex">
<span>def <span class="ident">init_weights_update_group</span></span>(<span>self, master_address, master_port, rank_offset, world_size, group_name, backend='nccl')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_weights_update_group(
    self,
    master_address,
    master_port,
    rank_offset,
    world_size,
    group_name,
    backend=&#34;nccl&#34;,
):
    &#34;&#34;&#34;Initialize the Torch process group for model parameter updates.

    `_model_update_group` is used in the RLHF workflow, where rank
    0 is the actor model in the training engine, and the other ranks are
    the inference engine, which is used for rollout.

    In the RLHF workflow, the training engine updates the model
    weights/parameters online, and broadcasts them to the inference
    engine through the `_model_update_group` process group.
    &#34;&#34;&#34;
    assert (
        torch.distributed.is_initialized()
    ), &#34;Default torch process group must be initialized&#34;
    assert group_name != &#34;&#34;, &#34;Group name cannot be empty&#34;

    rank = rank_offset + self.tp_rank

    logger.info(
        f&#34;init custom process group: master_address={master_address}, master_port={master_port}, &#34;
        f&#34;rank_offset={rank_offset}, rank={rank}, world_size={world_size}, group_name={group_name}, backend={backend}&#34;
    )

    try:
        self._model_update_group[group_name] = init_custom_process_group(
            backend=backend,
            init_method=f&#34;tcp://{master_address}:{master_port}&#34;,
            world_size=world_size,
            rank=rank,
            group_name=group_name,
        )
        return True, &#34;Succeeded to initialize custom process group.&#34;
    except Exception as e:
        message = f&#34;Failed to initialize custom process group: {e}.&#34;
        logger.error(message)
        return False, message</code></pre>
</details>
<div class="desc"><p>Initialize the Torch process group for model parameter updates.</p>
<p><code>_model_update_group</code> is used in the RLHF workflow, where rank
0 is the actor model in the training engine, and the other ranks are
the inference engine, which is used for rollout.</p>
<p>In the RLHF workflow, the training engine updates the model
weights/parameters online, and broadcasts them to the inference
engine through the <code>_model_update_group</code> process group.</p></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self, min_per_gpu_memory: float)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self, min_per_gpu_memory: float):
    server_args = self.server_args

    self.memory_saver_adapter = TorchMemorySaverAdapter.create(
        enable=self.server_args.enable_memory_saver
    )

    if not self.is_draft_worker:
        set_global_expert_location_metadata(
            compute_initial_expert_location_metadata(server_args, self.model_config)
        )
        if self.tp_rank == 0 and get_bool_env_var(
            &#34;SGLANG_LOG_EXPERT_LOCATION_METADATA&#34;
        ):
            logger.info(
                f&#34;Initial expert_location_metadata: {get_global_expert_location_metadata()}&#34;
            )

        set_global_expert_distribution_recorder(
            ExpertDistributionRecorder.init_new(
                server_args,
                get_global_expert_location_metadata(),
                rank=self.tp_rank,
            )
        )

    # Expert parallelism
    self.eplb_manager = (
        EPLBManager(self)
        if self.server_args.enable_eplb and (not self.is_draft_worker)
        else None
    )
    self.expert_location_updater = ExpertLocationUpdater()

    # Load the model
    self.sampler = Sampler()
    self.load_model()

    # Check if the model is using hybrid SWA
    if (
        not self.server_args.disable_hybrid_swa_memory
        and self.sliding_window_size is not None
        and self.sliding_window_size &gt; 0
    ):
        architectures = self.model_config.hf_config.architectures
        if architectures and not any(&#34;Llama4&#34; in arch for arch in architectures):
            self.is_hybrid = self.model_config.is_hybrid = True

    # For MTP models like DeepSeek-V3 or GLM-4.5, the MTP layer(s) are used separately as draft
    # models for speculative decoding. In those cases, `num_nextn_predict_layers` is used to
    # determine the number of layers.
    model_has_mtp_layers = self.model_config.num_nextn_predict_layers is not None
    model_num_layers = (
        self.model_config.num_nextn_predict_layers
        if self.is_draft_worker and model_has_mtp_layers
        else max(
            self.model_config.num_hidden_layers,
            self.model_config.num_attention_layers,
        )
    )
    self.start_layer = getattr(self.model, &#34;start_layer&#34;, 0)
    self.end_layer = getattr(self.model, &#34;end_layer&#34;, model_num_layers)
    self.num_effective_layers = self.end_layer - self.start_layer
    assert (
        (not model_has_mtp_layers)
        or (self.spec_algorithm.is_none())
        or (
            (not self.spec_algorithm.is_none())
            and (self.num_effective_layers == model_num_layers)
        )
    ), &#34;PP is not compatible with MTP models.&#34;

    # Apply torchao quantization
    torchao_applied = getattr(self.model, &#34;torchao_applied&#34;, False)
    # In layered loading, torchao may have been applied
    if not torchao_applied:
        apply_torchao_config_to_model(
            self.model, global_server_args_dict[&#34;torchao_config&#34;]
        )

    # Apply torch TP if the model supports it
    supports_torch_tp = getattr(self.model, &#34;supports_torch_tp&#34;, False)
    if self.tp_size &gt; 1 and supports_torch_tp:
        self.apply_torch_tp()

    # Init lora
    if server_args.enable_lora:
        self.init_lora_manager()

    # Init memory pool and attention backends
    self.init_memory_pool(
        min_per_gpu_memory,
        server_args.max_running_requests,
        server_args.max_total_tokens,
    )
    if self.device == &#34;cuda&#34;:
        self.init_cublas()
        self.init_attention_backend()
        self.init_device_graphs()
    elif self.device == &#34;npu&#34;:
        self.init_attention_backend()
        self.init_device_graphs()
    else:
        self.graph_runner = None
        self.cuda_graph_mem_usage = 0
        self.init_attention_backend()

    # auxiliary hidden capture mode. TODO: expose this to server args?
    if self.spec_algorithm.is_eagle3() and not self.is_draft_worker:
        # load draft config
        draft_model_config = ModelConfig.from_server_args(
            server_args,
            model_path=(server_args.speculative_draft_model_path),
            is_draft_model=True,
        )

        try:
            # get the aux layer from draft model config
            eagle_config = getattr(
                draft_model_config.hf_config, &#34;eagle_config&#34;, None
            )
            eagle_aux_hidden_state_layer_ids = eagle_config[
                &#34;eagle_aux_hidden_state_layer_ids&#34;
            ]
        except:
            # if there is no aux layer, set to None
            eagle_aux_hidden_state_layer_ids = None

        self.model.set_eagle3_layers_to_capture(eagle_aux_hidden_state_layer_ids)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.load_lora_adapter"><code class="name flex">
<span>def <span class="ident">load_lora_adapter</span></span>(<span>self,<br>lora_ref: <a title="sglang.srt.lora.lora_registry.LoRARef" href="../lora/lora_registry.html#sglang.srt.lora.lora_registry.LoRARef">LoRARef</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_lora_adapter(self, lora_ref: LoRARef):
    &#34;&#34;&#34;Load a new lora adapter from disk or huggingface.&#34;&#34;&#34;

    logger.info(
        f&#34;LoRA adapter loading starts: {lora_ref}. &#34;
        f&#34;avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB&#34;
    )

    result = self.lora_manager.load_lora_adapter(lora_ref)

    logger.info(
        f&#34;LoRA adapter loading completes: {lora_ref}. &#34;
        f&#34;avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB&#34;
    )

    return result</code></pre>
</details>
<div class="desc"><p>Load a new lora adapter from disk or huggingface.</p></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.load_model"><code class="name flex">
<span>def <span class="ident">load_model</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_model(self):
    before_avail_memory = get_available_gpu_memory(self.device, self.gpu_id)
    logger.info(
        f&#34;Load weight begin. avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB&#34;
    )

    # This can reduce thread conflicts and speed up weight loading.
    if self.device != &#34;cpu&#34;:
        torch.set_num_threads(1)
    if self.device == &#34;cuda&#34;:
        if torch.cuda.get_device_capability()[0] &lt; 8:
            logger.info(
                &#34;Compute capability below sm80. Use float16 due to lack of bfloat16 support.&#34;
            )
            self.server_args.dtype = &#34;float16&#34;
            self.model_config.dtype = torch.float16
            if torch.cuda.get_device_capability()[1] &lt; 5:
                raise RuntimeError(&#34;SGLang only supports sm75 and above.&#34;)

    set_cuda_arch()

    # Prepare the model config
    self.load_config = LoadConfig(
        load_format=self.server_args.load_format,
        download_dir=self.server_args.download_dir,
        model_loader_extra_config=self.server_args.model_loader_extra_config,
    )
    if self.device == &#34;cpu&#34;:
        self.model_config = adjust_config_with_unaligned_cpu_tp(
            self.model_config, self.load_config, self.tp_size
        )
    if self.server_args.load_format == &#34;gguf&#34;:
        monkey_patch_vllm_gguf_config()

    # Load the model
    # Remove monkey_patch when linear.py quant remove dependencies with vllm
    monkey_patch_vllm_parallel_state()
    monkey_patch_isinstance_for_vllm_base_layer()

    with self.memory_saver_adapter.region(GPU_MEMORY_TYPE_WEIGHTS):
        self.model = get_model(
            model_config=self.model_config,
            load_config=self.load_config,
            device_config=DeviceConfig(self.device),
        )
    monkey_patch_vllm_parallel_state(reverse=True)
    monkey_patch_isinstance_for_vllm_base_layer(reverse=True)

    get_offloader().post_init()

    if self.server_args.kv_cache_dtype == &#34;fp8_e4m3&#34;:
        if self.server_args.quantization_param_path is not None:
            if callable(getattr(self.model, &#34;load_kv_cache_scales&#34;, None)):
                self.model.load_kv_cache_scales(
                    self.server_args.quantization_param_path
                )
                logger.info(
                    &#34;Loaded KV cache scaling factors from %s&#34;,
                    self.server_args.quantization_param_path,
                )
            else:
                raise RuntimeError(
                    &#34;Using FP8 KV cache and scaling factors provided but &#34;
                    &#34;model %s does not support loading scaling factors.&#34;,
                    self.model.__class__,
                )
        else:
            logger.warning(
                &#34;Using FP8 KV cache but no scaling factors &#34;
                &#34;provided. Defaulting to scaling factors of 1.0. &#34;
                &#34;This may lead to less accurate results!&#34;
            )

    # Parse other args
    self.sliding_window_size = None
    if hasattr(self.model, &#34;get_attention_sliding_window_size&#34;):
        self.sliding_window_size = self.model.get_attention_sliding_window_size()
    elif self.model_config.attention_chunk_size is not None:
        self.sliding_window_size = self.model_config.attention_chunk_size
        logger.info(
            f&#34;Setting sliding_window_size to be attention_chunk_size: {self.sliding_window_size}&#34;
        )

    self.dtype = self.model_config.dtype

    after_avail_memory = get_available_gpu_memory(self.device, self.gpu_id)
    self.weight_load_mem_usage = before_avail_memory - after_avail_memory
    logger.info(
        f&#34;Load weight end. &#34;
        f&#34;type={type(self.model).__name__}, &#34;
        f&#34;dtype={self.dtype}, &#34;
        f&#34;avail mem={after_avail_memory:.2f} GB, &#34;
        f&#34;mem usage={self.weight_load_mem_usage:.2f} GB.&#34;
    )

    # Handle the case where some ranks do not finish loading.
    try:
        dist.monitored_barrier(
            group=get_tp_group().cpu_group,
            timeout=datetime.timedelta(seconds=UNBALANCED_MODEL_LOADING_TIMEOUT_S),
            wait_all_ranks=True,
        )
    except RuntimeError:
        raise ValueError(
            f&#34;TP rank {self.tp_rank} could finish the model loading, but there are other ranks that didn&#39;t finish loading. It is likely due to unexpected failures (e.g., OOM) or a slow node.&#34;
        ) from None</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.model_specific_adjustment"><code class="name flex">
<span>def <span class="ident">model_specific_adjustment</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model_specific_adjustment(self):
    server_args = self.server_args

    if (
        server_args.attention_backend == &#34;intel_amx&#34;
        and server_args.device == &#34;cpu&#34;
        and not _is_cpu_amx_available
    ):
        logger.info(
            &#34;The current platform does not support Intel AMX, will fallback to torch_native backend.&#34;
        )
        server_args.attention_backend = &#34;torch_native&#34;

    if server_args.prefill_attention_backend is not None and (
        server_args.prefill_attention_backend
        == server_args.decode_attention_backend
    ):  # override the default attention backend
        server_args.attention_backend = server_args.prefill_attention_backend

    if (
        getattr(self.model_config.hf_config, &#34;dual_chunk_attention_config&#34;, None)
        is not None
    ):
        if server_args.attention_backend is None:
            server_args.attention_backend = &#34;dual_chunk_flash_attn&#34;
            logger.info(&#34;Dual chunk attention is turned on by default.&#34;)
        elif server_args.attention_backend != &#34;dual_chunk_flash_attn&#34;:
            raise ValueError(
                &#34;Dual chunk attention is enabled, but attention backend is set to &#34;
                f&#34;{server_args.attention_backend}. Please set it to &#39;dual_chunk_flash_attn&#39;.&#34;
            )

    if server_args.attention_backend is None:
        &#34;&#34;&#34;
        Auto select the fastest attention backend.

        1. Models with MHA Architecture (e.g: Llama, QWen)
            1.1 We will turn on FA3 on hopper unless user use spec decode with topk &gt; 1 or page_size &gt; 1.
            1.2 In other cases, we will use flashinfer if available, otherwise use triton.
        2. Models with MLA Architecture and using FA3
            2.1 We will use FA3 backend on hopper.
            2.2 We will use Flashinfer backend on blackwell.
            2.3 Otherwise, we will use triton backend.
        &#34;&#34;&#34;

        if not self.use_mla_backend:
            # MHA architecture
            if (
                is_hopper_with_cuda_12_3()
                and is_no_spec_infer_or_topk_one(server_args)
                and is_fa3_default_architecture(self.model_config.hf_config)
            ):
                server_args.attention_backend = &#34;fa3&#34;
            elif _is_hip:
                server_args.attention_backend = &#34;aiter&#34;
            elif _is_npu:
                server_args.attention_backend = &#34;ascend&#34;
            else:
                server_args.attention_backend = (
                    &#34;flashinfer&#34; if is_flashinfer_available() else &#34;triton&#34;
                )
        else:
            # MLA architecture
            if is_hopper_with_cuda_12_3():
                server_args.attention_backend = &#34;fa3&#34;
            elif is_sm100_supported():
                server_args.attention_backend = &#34;flashinfer&#34;
            elif _is_hip:
                head_num = self.model_config.get_num_kv_heads(self.tp_size)
                # TODO current aiter only support head number 16 or 128 head number
                if (
                    head_num == 128 or head_num == 16
                ) and self.spec_algorithm.is_none():
                    server_args.attention_backend = &#34;aiter&#34;
                else:
                    server_args.attention_backend = &#34;triton&#34;
            elif _is_npu:
                server_args.attention_backend = &#34;ascend&#34;
            else:
                server_args.attention_backend = &#34;triton&#34;
        logger.info(
            f&#34;Attention backend not explicitly specified. Use {server_args.attention_backend} backend by default.&#34;
        )
    elif self.use_mla_backend:
        if server_args.device != &#34;cpu&#34;:
            if server_args.attention_backend in [
                &#34;aiter&#34;,
                &#34;flashinfer&#34;,
                &#34;fa3&#34;,
                &#34;triton&#34;,
                &#34;flashmla&#34;,
                &#34;cutlass_mla&#34;,
                &#34;trtllm_mla&#34;,
                &#34;ascend&#34;,
            ]:
                logger.info(
                    f&#34;MLA optimization is turned on. Use {server_args.attention_backend} backend.&#34;
                )
            else:
                raise ValueError(
                    f&#34;Invalid attention backend for MLA: {server_args.attention_backend}&#34;
                )
        else:
            if server_args.attention_backend != &#34;intel_amx&#34;:
                raise ValueError(
                    &#34;MLA optimization not supported on CPU except for intel_amx backend.&#34;
                )

    if (
        server_args.attention_backend == &#34;fa3&#34;
        and server_args.kv_cache_dtype == &#34;fp8_e5m2&#34;
    ):
        logger.warning(
            &#34;FlashAttention3 only supports fp8_e4m3 if using FP8; &#34;
            &#34;Setting attention backend to triton.&#34;
        )
        server_args.attention_backend = &#34;triton&#34;

    if server_args.enable_double_sparsity:
        logger.info(
            &#34;Double sparsity optimization is turned on. Use triton backend without CUDA graph.&#34;
        )
        server_args.attention_backend = &#34;triton&#34;
        server_args.disable_cuda_graph = True
        if server_args.ds_heavy_channel_type is None:
            raise ValueError(
                &#34;Please specify the heavy channel type for double sparsity optimization.&#34;
            )
        self.init_double_sparsity_channel_config(server_args.ds_heavy_channel_type)

    if self.is_multimodal:
        if not self.is_multimodal_chunked_prefill_supported:
            server_args.chunked_prefill_size = -1
            logger.info(
                f&#34;Automatically turn off --chunked-prefill-size as it is not supported for &#34;
                f&#34;{self.model_config.hf_config.model_type}&#34;
            )

    if not self.use_mla_backend:
        server_args.disable_chunked_prefix_cache = True

    if not server_args.disable_chunked_prefix_cache:
        logger.info(&#34;Chunked prefix cache is turned on.&#34;)

    if server_args.attention_backend == &#34;aiter&#34;:
        if self.model_config.context_len &gt; 8192:
            self.mem_fraction_static *= 0.85

    if (
        server_args.enable_hierarchical_cache
        and server_args.hicache_io_backend == &#34;kernel&#34;
    ):
        # fix for the compatibility issue with FlashAttention3 decoding and HiCache kernel backend
        if server_args.decode_attention_backend is None:
            if not self.use_mla_backend:
                server_args.decode_attention_backend = (
                    &#34;flashinfer&#34; if is_flashinfer_available() else &#34;triton&#34;
                )
            else:
                server_args.decode_attention_backend = (
                    &#34;flashinfer&#34; if is_sm100_supported() else &#34;triton&#34;
                )
        elif server_args.decode_attention_backend == &#34;fa3&#34;:
            server_args.hicache_io_backend = &#34;direct&#34;
            logger.warning(
                &#34;FlashAttention3 decode backend is not compatible with hierarchical cache. &#34;
                f&#34;Setting hicache_io_backend to vanilla I/O, which may lead to suboptimal performance with small page sizes.&#34;
            )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.profile_max_num_token"><code class="name flex">
<span>def <span class="ident">profile_max_num_token</span></span>(<span>self, total_gpu_memory: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def profile_max_num_token(self, total_gpu_memory: int):
    available_gpu_memory = get_available_gpu_memory(
        self.device,
        self.gpu_id,
        distributed=get_world_group().world_size &gt; 1,
        cpu_group=get_world_group().cpu_group,
    )
    if self.is_draft_worker:
        num_layers = getattr(
            self.model_config.hf_config,
            &#34;num_nextn_predict_layers&#34;,
            self.num_effective_layers,
        )
    else:
        num_layers = self.num_effective_layers
    if self.use_mla_backend:
        cell_size = (
            (self.model_config.kv_lora_rank + self.model_config.qk_rope_head_dim)
            * num_layers
            * torch._utils._element_size(self.kv_cache_dtype)
        )
    else:
        cell_size = (
            self.model_config.get_num_kv_heads(get_attention_tp_size())
            * self.model_config.head_dim
            * num_layers
            * 2
            * torch._utils._element_size(self.kv_cache_dtype)
        )
    rest_memory = available_gpu_memory - total_gpu_memory * (
        1 - self.mem_fraction_static
    )
    max_num_token = int(rest_memory * (1 &lt;&lt; 30) // cell_size)
    return max_num_token</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.sample"><code class="name flex">
<span>def <span class="ident">sample</span></span>(<span>self,<br>logits_output: <a title="sglang.srt.layers.logits_processor.LogitsProcessorOutput" href="../layers/logits_processor.html#sglang.srt.layers.logits_processor.LogitsProcessorOutput">LogitsProcessorOutput</a>,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample(
    self,
    logits_output: LogitsProcessorOutput,
    forward_batch: ForwardBatch,
) -&gt; torch.Tensor:
    &#34;&#34;&#34;Sample and compute logprobs and update logits_output.

    Args:
        logits_output: The logits output from the model forward
        forward_batch: The forward batch that generates logits_output

    Returns:
        A list of next_token_ids
    &#34;&#34;&#34;
    # For duplex models with multiple output streams.
    if isinstance(logits_output, tuple):
        return torch.stack(
            [self.sample(values, forward_batch) for values in logits_output],
            axis=-1,
        )

    self._preprocess_logits(logits_output, forward_batch.sampling_info)

    # Sample the next tokens
    next_token_ids = self.sampler(
        logits_output,
        forward_batch.sampling_info,
        forward_batch.return_logprob,
        forward_batch.top_logprobs_nums,
        forward_batch.token_ids_logprobs,
    )
    return next_token_ids</code></pre>
</details>
<div class="desc"><p>Sample and compute logprobs and update logits_output.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>logits_output</code></strong></dt>
<dd>The logits output from the model forward</dd>
<dt><strong><code>forward_batch</code></strong></dt>
<dd>The forward batch that generates logits_output</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A list of next_token_ids</p></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.save_remote_model"><code class="name flex">
<span>def <span class="ident">save_remote_model</span></span>(<span>self, url: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_remote_model(self, url: str):
    from sglang.srt.model_loader.loader import RemoteModelLoader

    logger.info(f&#34;Saving model to {url}&#34;)
    RemoteModelLoader.save_model(self.model, self.model_config.model_path, url)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.save_sharded_model"><code class="name flex">
<span>def <span class="ident">save_sharded_model</span></span>(<span>self, path: str, pattern: str | None = None, max_size: int | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_sharded_model(
    self, path: str, pattern: Optional[str] = None, max_size: Optional[int] = None
):
    from sglang.srt.model_loader.loader import ShardedStateLoader

    logger.info(
        f&#34;Save sharded model to {path} with pattern {pattern} and max_size {max_size}&#34;
    )
    ShardedStateLoader.save_model(self.model, path, pattern, max_size)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.set_num_token_hybrid"><code class="name flex">
<span>def <span class="ident">set_num_token_hybrid</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_num_token_hybrid(self):
    if (
        &#34;Llama4ForConditionalGeneration&#34;
        in self.model_config.hf_config.architectures
    ):
        temp_ratio = (
            (1 - self.is_hybrid)
            + self.is_hybrid
            * self.attention_chunk_size
            / self.model_config.context_len
        )
        self.swa_max_total_num_tokens = (
            4 * self.max_total_num_tokens * temp_ratio // (3 * temp_ratio + 1)
        )
        self.full_max_total_num_tokens = (
            4 * self.max_total_num_tokens
            - 12 * self.max_total_num_tokens * temp_ratio // (3 * temp_ratio + 1)
        )
        self.swa_max_total_num_tokens = int(
            self.swa_max_total_num_tokens
            // self.server_args.page_size
            * self.server_args.page_size
        )
        self.full_max_total_num_tokens = int(
            self.full_max_total_num_tokens
            // self.server_args.page_size
            * self.server_args.page_size
        )
        self.max_total_num_tokens = self.full_max_total_num_tokens
    else:
        assert self.sliding_window_size is not None and self.sliding_window_size &gt; 0
        full_attention_layer_ids = []
        swa_attention_layer_ids = []

        try:
            layers = self.model.model.layers
        except:
            try:
                layers = self.model.language_model.model.layers
            except:
                try:
                    layers = self.model.language_model.layers
                except:
                    self.is_hybrid = False
                    return

        for layer in layers:
            if (
                layer.self_attn.attn.sliding_window_size is None
                or layer.self_attn.attn.sliding_window_size == -1
            ):
                full_attention_layer_ids.append(layer.layer_id)
            else:
                swa_attention_layer_ids.append(layer.layer_id)
        self.model_config.swa_attention_layer_ids = swa_attention_layer_ids
        self.model_config.full_attention_layer_ids = full_attention_layer_ids

        # Algorithm:
        # Existing max_total_num_tokens is per layer and assume all layers have the same number of tokens.
        # - Find total # of tokens available across layers.
        # - Calculate full_max_total_num_tokens and swa_max_total_num_tokens based on the given swa_full_tokens_ratio.
        total_tokens = (
            self.max_total_num_tokens * self.model_config.num_hidden_layers
        )
        full_layers_num = len(full_attention_layer_ids)
        swa_layers_num = len(swa_attention_layer_ids)
        swa_full_tokens_ratio = self.server_args.swa_full_tokens_ratio

        # Solve the equations:
        # 1. swa_max_total_num_tokens * swa_layers_num + full_max_total_num_tokens * full_layers_num == total_tokens
        # 2. full_max_total_num_tokens * swa_full_tokens_ratio == swa_max_total_num_tokens
        denominator = swa_full_tokens_ratio * swa_layers_num + full_layers_num
        self.full_max_total_num_tokens = int(total_tokens / denominator)
        self.swa_max_total_num_tokens = int(
            self.full_max_total_num_tokens * swa_full_tokens_ratio
        )
        self.max_total_num_tokens = self.full_max_total_num_tokens

        logger.info(
            f&#34;Use Sliding window memory pool. full_layer_tokens={self.full_max_total_num_tokens}, swa_layer_tokens={self.swa_max_total_num_tokens}&#34;
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.unload_lora_adapter"><code class="name flex">
<span>def <span class="ident">unload_lora_adapter</span></span>(<span>self,<br>lora_ref: <a title="sglang.srt.lora.lora_registry.LoRARef" href="../lora/lora_registry.html#sglang.srt.lora.lora_registry.LoRARef">LoRARef</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unload_lora_adapter(self, lora_ref: LoRARef):
    &#34;&#34;&#34;Unload a lora adapter that was previously loaded during initialization or dynamic loading.&#34;&#34;&#34;

    logger.info(
        f&#34;LoRA adapter unloading starts: {lora_ref}. &#34;
        f&#34;avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB&#34;
    )

    result = self.lora_manager.unload_lora_adapter(lora_ref)

    logger.info(
        f&#34;LoRA adapter unloading completes: {lora_ref}. &#34;
        f&#34;avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB&#34;
    )

    return result</code></pre>
</details>
<div class="desc"><p>Unload a lora adapter that was previously loaded during initialization or dynamic loading.</p></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.update_expert_location"><code class="name flex">
<span>def <span class="ident">update_expert_location</span></span>(<span>self,<br>new_expert_location_metadata: <a title="sglang.srt.eplb.expert_location.ExpertLocationMetadata" href="../eplb/expert_location.html#sglang.srt.eplb.expert_location.ExpertLocationMetadata">ExpertLocationMetadata</a>,<br>update_layer_ids: List[int])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_expert_location(
    self,
    new_expert_location_metadata: ExpertLocationMetadata,
    update_layer_ids: List[int],
):
    self.expert_location_updater.update(
        self.model.routed_experts_weights_of_layer,
        new_expert_location_metadata,
        update_layer_ids=update_layer_ids,
        nnodes=self.server_args.nnodes,
        rank=self.tp_rank,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.update_weights_from_disk"><code class="name flex">
<span>def <span class="ident">update_weights_from_disk</span></span>(<span>self, model_path: str, load_format: str) ‑> tuple[bool, str]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_weights_from_disk(
    self, model_path: str, load_format: str
) -&gt; tuple[bool, str]:
    &#34;&#34;&#34;Update engine weights in-place from the disk.&#34;&#34;&#34;
    logger.info(
        f&#34;Update engine weights online from disk begin. &#34;
        f&#34;avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB&#34;
    )

    target_device = torch.device(self.device)
    self.model_config.model_path = model_path
    load_config = LoadConfig(load_format=load_format)

    # Only support DefaultModelLoader for now
    loader = get_model_loader(load_config)
    if not isinstance(loader, DefaultModelLoader):
        message = f&#34;Failed to get model loader: {loader}.&#34;
        return False, message

    def get_weight_iter(config):
        iter = loader._get_weights_iterator(
            DefaultModelLoader.Source.init_new(config, self.model)
        )
        return iter

    def model_load_weights(model, iter):
        DefaultModelLoader.load_weights_and_postprocess(model, iter, target_device)
        return model

    with set_default_torch_dtype(self.model_config.dtype):
        try:
            iter = get_weight_iter(self.model_config)
        except Exception as e:
            message = f&#34;Failed to get weights iterator: {e}.&#34;
            return False, message
        try:
            model = model_load_weights(self.model, iter)
        except Exception as e:
            message = (
                f&#34;Failed to update weights: {e}.\nRolling back to original weights.&#34;
            )
            del iter
            gc.collect()
            iter = get_weight_iter(self.model_config)
            self.model = model_load_weights(self.model, iter)
            return False, message

    self.model = model
    self.server_args.model_path = model_path
    self.server_args.load_format = load_format
    self.load_config = load_config

    logger.info(&#34;Update weights end.&#34;)
    return True, &#34;Succeeded to update model weights.&#34;</code></pre>
</details>
<div class="desc"><p>Update engine weights in-place from the disk.</p></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.update_weights_from_distributed"><code class="name flex">
<span>def <span class="ident">update_weights_from_distributed</span></span>(<span>self, names, dtypes, shapes, group_name)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_weights_from_distributed(self, names, dtypes, shapes, group_name):
    &#34;&#34;&#34;
    Update specific parameter in the model weights online
    through `_model_update_group` process group.

    Args:
        name: the name of the parameter to be updated.
        dtype: the data type of the parameter to be updated.
        shape: the shape of the parameter to be updated.
    &#34;&#34;&#34;

    assert group_name in self._model_update_group, (
        f&#34;Group {group_name} not in {list(self._model_update_group.keys())}. &#34;
        &#34;Please call `init_weights_update_group` first.&#34;
    )

    try:
        weights = []
        handles = []
        for name, dtype, shape in zip(names, dtypes, shapes):
            target_dtype = (
                dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)
            )
            weight = torch.empty(shape, dtype=target_dtype, device=self.device)
            handles.append(
                torch.distributed.broadcast(
                    weight,
                    src=0,
                    group=self._model_update_group[group_name],
                    async_op=True,
                )
            )
            weights.append((name, weight))
        for handle in handles:
            handle.wait()

        self.model.load_weights(weights)
        return True, f&#34;Succeeded to update parameter online.&#34;

    except Exception as e:
        error_msg = (
            f&#34;Failed to update parameter online: {e}. &#34;
            f&#34;The full weights of the ModelRunner are partially updated. &#34;
            f&#34;Please discard the whole weights.&#34;
        )
        logger.error(error_msg)
        return False, error_msg</code></pre>
</details>
<div class="desc"><p>Update specific parameter in the model weights online
through <code>_model_update_group</code> process group.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>the name of the parameter to be updated.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>the data type of the parameter to be updated.</dd>
<dt><strong><code>shape</code></strong></dt>
<dd>the shape of the parameter to be updated.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.model_executor.model_runner.ModelRunner.update_weights_from_tensor"><code class="name flex">
<span>def <span class="ident">update_weights_from_tensor</span></span>(<span>self,<br>named_tensors: List[Tuple[str, torch.Tensor | ForwardRef('<a title="sglang.srt.model_executor.model_runner.LocalSerializedTensor" href="#sglang.srt.model_executor.model_runner.LocalSerializedTensor">LocalSerializedTensor</a>')]],<br>load_format: str | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_weights_from_tensor(
    self,
    named_tensors: List[Tuple[str, Union[torch.Tensor, &#34;LocalSerializedTensor&#34;]]],
    load_format: Optional[str] = None,
):
    monkey_patch_torch_reductions()
    if load_format == &#34;flattened_bucket&#34;:
        # Handle flattened bucket format
        return self._update_weights_from_flattened_bucket(
            flattened_tensor_bucket_dict=named_tensors
        )

    # We need to get device after patch otherwise the device would be wrong
    self.device_module = torch.get_device_module(self.device)
    infered_device = self.device_module.current_device()

    named_tensors = [
        (name, _unwrap_tensor(tensor, tp_rank=self.tp_rank, device=infered_device))
        for name, tensor in named_tensors
    ]
    if load_format == &#34;direct&#34;:
        _model_load_weights_direct(self.model, named_tensors)
    elif load_format in self.server_args.custom_weight_loader:
        custom_loader = dynamic_import(load_format)
        custom_loader(self.model, named_tensors)
    elif load_format is None:
        self.model.load_weights(named_tensors)
    else:
        raise NotImplementedError(f&#34;Unknown load_format={load_format}&#34;)
    return True, &#34;Success&#34;</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.model_executor.model_runner.RankZeroFilter"><code class="flex name class">
<span>class <span class="ident">RankZeroFilter</span></span>
<span>(</span><span>is_rank_zero)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RankZeroFilter(logging.Filter):
    &#34;&#34;&#34;Filter that only allows INFO level logs from rank 0, but allows all other levels from any rank.&#34;&#34;&#34;

    def __init__(self, is_rank_zero):
        super().__init__()
        self.is_rank_zero = is_rank_zero

    def filter(self, record):
        if record.levelno == logging.INFO:
            return self.is_rank_zero
        return True</code></pre>
</details>
<div class="desc"><p>Filter that only allows INFO level logs from rank 0, but allows all other levels from any rank.</p>
<p>Initialize a filter.</p>
<p>Initialize with the name of the logger which, together with its
children, will have its events allowed through the filter. If no
name is specified, allow every event.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>logging.Filter</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.model_executor.model_runner.RankZeroFilter.filter"><code class="name flex">
<span>def <span class="ident">filter</span></span>(<span>self, record)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter(self, record):
    if record.levelno == logging.INFO:
        return self.is_rank_zero
    return True</code></pre>
</details>
<div class="desc"><p>Determine if the specified record is to be logged.</p>
<p>Returns True if the record should be logged, or False otherwise.
If deemed appropriate, the record may be modified in-place.</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.model_executor" href="index.html">sglang.srt.model_executor</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.model_executor.model_runner.LocalSerializedTensor" href="#sglang.srt.model_executor.model_runner.LocalSerializedTensor">LocalSerializedTensor</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.model_executor.model_runner.LocalSerializedTensor.get" href="#sglang.srt.model_executor.model_runner.LocalSerializedTensor.get">get</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.LocalSerializedTensor.values" href="#sglang.srt.model_executor.model_runner.LocalSerializedTensor.values">values</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.model_executor.model_runner.ModelRunner" href="#sglang.srt.model_executor.model_runner.ModelRunner">ModelRunner</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.apply_torch_tp" href="#sglang.srt.model_executor.model_runner.ModelRunner.apply_torch_tp">apply_torch_tp</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.forward" href="#sglang.srt.model_executor.model_runner.ModelRunner.forward">forward</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.forward_decode" href="#sglang.srt.model_executor.model_runner.ModelRunner.forward_decode">forward_decode</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.forward_extend" href="#sglang.srt.model_executor.model_runner.ModelRunner.forward_extend">forward_extend</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.forward_idle" href="#sglang.srt.model_executor.model_runner.ModelRunner.forward_idle">forward_idle</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.forward_split_prefill" href="#sglang.srt.model_executor.model_runner.ModelRunner.forward_split_prefill">forward_split_prefill</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.get_weights_by_name" href="#sglang.srt.model_executor.model_runner.ModelRunner.get_weights_by_name">get_weights_by_name</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.init_attention_backend" href="#sglang.srt.model_executor.model_runner.ModelRunner.init_attention_backend">init_attention_backend</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.init_cublas" href="#sglang.srt.model_executor.model_runner.ModelRunner.init_cublas">init_cublas</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.init_device_graphs" href="#sglang.srt.model_executor.model_runner.ModelRunner.init_device_graphs">init_device_graphs</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.init_double_sparsity_channel_config" href="#sglang.srt.model_executor.model_runner.ModelRunner.init_double_sparsity_channel_config">init_double_sparsity_channel_config</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.init_lora_manager" href="#sglang.srt.model_executor.model_runner.ModelRunner.init_lora_manager">init_lora_manager</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.init_memory_pool" href="#sglang.srt.model_executor.model_runner.ModelRunner.init_memory_pool">init_memory_pool</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.init_threads_binding" href="#sglang.srt.model_executor.model_runner.ModelRunner.init_threads_binding">init_threads_binding</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.init_torch_distributed" href="#sglang.srt.model_executor.model_runner.ModelRunner.init_torch_distributed">init_torch_distributed</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.init_weights_update_group" href="#sglang.srt.model_executor.model_runner.ModelRunner.init_weights_update_group">init_weights_update_group</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.initialize" href="#sglang.srt.model_executor.model_runner.ModelRunner.initialize">initialize</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.load_lora_adapter" href="#sglang.srt.model_executor.model_runner.ModelRunner.load_lora_adapter">load_lora_adapter</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.load_model" href="#sglang.srt.model_executor.model_runner.ModelRunner.load_model">load_model</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.model_is_mrope" href="#sglang.srt.model_executor.model_runner.ModelRunner.model_is_mrope">model_is_mrope</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.model_specific_adjustment" href="#sglang.srt.model_executor.model_runner.ModelRunner.model_specific_adjustment">model_specific_adjustment</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.profile_max_num_token" href="#sglang.srt.model_executor.model_runner.ModelRunner.profile_max_num_token">profile_max_num_token</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.sample" href="#sglang.srt.model_executor.model_runner.ModelRunner.sample">sample</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.save_remote_model" href="#sglang.srt.model_executor.model_runner.ModelRunner.save_remote_model">save_remote_model</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.save_sharded_model" href="#sglang.srt.model_executor.model_runner.ModelRunner.save_sharded_model">save_sharded_model</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.set_num_token_hybrid" href="#sglang.srt.model_executor.model_runner.ModelRunner.set_num_token_hybrid">set_num_token_hybrid</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.unload_lora_adapter" href="#sglang.srt.model_executor.model_runner.ModelRunner.unload_lora_adapter">unload_lora_adapter</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.update_expert_location" href="#sglang.srt.model_executor.model_runner.ModelRunner.update_expert_location">update_expert_location</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.update_weights_from_disk" href="#sglang.srt.model_executor.model_runner.ModelRunner.update_weights_from_disk">update_weights_from_disk</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.update_weights_from_distributed" href="#sglang.srt.model_executor.model_runner.ModelRunner.update_weights_from_distributed">update_weights_from_distributed</a></code></li>
<li><code><a title="sglang.srt.model_executor.model_runner.ModelRunner.update_weights_from_tensor" href="#sglang.srt.model_executor.model_runner.ModelRunner.update_weights_from_tensor">update_weights_from_tensor</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.model_executor.model_runner.RankZeroFilter" href="#sglang.srt.model_executor.model_runner.RankZeroFilter">RankZeroFilter</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.model_executor.model_runner.RankZeroFilter.filter" href="#sglang.srt.model_executor.model_runner.RankZeroFilter.filter">filter</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
