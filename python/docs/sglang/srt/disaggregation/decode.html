<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.disaggregation.decode API documentation</title>
<meta name="description" content="Life cycle of a request in the decode server …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.disaggregation.decode</code></h1>
</header>
<section id="section-intro">
<p>Life cycle of a request in the decode server</p>
<ol>
<li>
<p>PreallocQueue:
a. Initialize a receiver for each request
b. The request handshakes first, and pre-allocate kv once there is available kv.
c. Move the request to TransferQueue.</p>
</li>
<li>
<p>TransferQueue:
a. Poll the receiver to check the transfer state
b. If the transfer has finished, move the request to waiting queue</p>
</li>
<li>
<p>WaitingQueue:
a. Use the requests in the queue to construct a PrebuiltExtendBatch
b. Skip the prefill forward but only populate metadata</p>
</li>
<li>
<p>RunningBatch:
a. Merge the resolved PrebuiltExtendBatch into running batch to run decoding</p>
</li>
</ol>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.disaggregation.decode.DecodePreallocQueue"><code class="flex name class">
<span>class <span class="ident">DecodePreallocQueue</span></span>
<span>(</span><span>req_to_token_pool: ReqToTokenPool,<br>token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,<br>draft_token_to_kv_pool: Optional[KVCache],<br>req_to_metadata_buffer_idx_allocator: ReqToMetadataIdxAllocator,<br>metadata_buffers: MetadataBuffers,<br>scheduler: Scheduler,<br>transfer_queue: <a title="sglang.srt.disaggregation.decode.DecodeTransferQueue" href="#sglang.srt.disaggregation.decode.DecodeTransferQueue">DecodeTransferQueue</a>,<br>tree_cache: BasePrefixCache,<br>gloo_group: ProcessGroup,<br>tp_rank: int,<br>tp_size: int,<br>dp_size: int,<br>gpu_id: int,<br>bootstrap_port: int,<br>max_total_num_tokens: int,<br>prefill_pp_size: int,<br>num_reserved_decode_tokens: int,<br>transfer_backend: TransferBackend)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DecodePreallocQueue:
    &#34;&#34;&#34;
    Store the requests that are preallocating.
    &#34;&#34;&#34;

    def __init__(
        self,
        req_to_token_pool: ReqToTokenPool,
        token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,
        draft_token_to_kv_pool: Optional[KVCache],
        req_to_metadata_buffer_idx_allocator: ReqToMetadataIdxAllocator,
        metadata_buffers: MetadataBuffers,
        scheduler: Scheduler,
        transfer_queue: DecodeTransferQueue,
        tree_cache: BasePrefixCache,
        gloo_group: ProcessGroup,
        tp_rank: int,
        tp_size: int,
        dp_size: int,
        gpu_id: int,
        bootstrap_port: int,
        max_total_num_tokens: int,
        prefill_pp_size: int,
        num_reserved_decode_tokens: int,
        transfer_backend: TransferBackend,
    ):
        self.req_to_token_pool = req_to_token_pool
        self.token_to_kv_pool_allocator = token_to_kv_pool_allocator
        self.token_to_kv_pool = token_to_kv_pool_allocator.get_kvcache()
        self.draft_token_to_kv_pool = draft_token_to_kv_pool
        self.is_mla_backend = is_mla_backend(self.token_to_kv_pool)
        self.metadata_buffers = metadata_buffers
        self.req_to_metadata_buffer_idx_allocator = req_to_metadata_buffer_idx_allocator
        self.scheduler = scheduler
        self.transfer_queue = transfer_queue
        self.tree_cache = tree_cache  # this is always a chunk cache
        self.gloo_group = gloo_group
        self.tp_rank = tp_rank
        self.tp_size = tp_size
        self.dp_size = dp_size
        self.gpu_id = gpu_id
        self.bootstrap_port = bootstrap_port
        self.max_total_num_tokens = max_total_num_tokens
        self.prefill_pp_size = prefill_pp_size
        self.num_reserved_decode_tokens = num_reserved_decode_tokens
        self.transfer_backend = transfer_backend
        # Queue for requests pending pre-allocation
        self.queue: List[DecodeRequest] = []
        self.retracted_queue: List[Req] = []
        self.prefill_pp_size = prefill_pp_size
        self.kv_manager = self._init_kv_manager()

    def _init_kv_manager(self) -&gt; BaseKVManager:
        kv_args_class = get_kv_class(self.transfer_backend, KVClassType.KVARGS)
        kv_args = kv_args_class()

        attn_tp_size = get_attention_tp_size()
        kv_args.engine_rank = self.tp_rank % (attn_tp_size)

        kv_args.decode_tp_size = attn_tp_size
        # Note(shangming): pp is not supported on the decode side yet, so its rank is fixed to 0
        kv_args.pp_rank = 0
        kv_args.system_dp_rank = self.scheduler.dp_rank
        kv_args.prefill_pp_size = self.prefill_pp_size
        kv_data_ptrs, kv_data_lens, kv_item_lens = (
            self.token_to_kv_pool.get_contiguous_buf_infos()
        )
        if self.draft_token_to_kv_pool is not None:
            # We should also transfer draft model kv cache. The indices are
            # always shared with a target model.
            draft_kv_data_ptrs, draft_kv_data_lens, draft_kv_item_lens = (
                self.draft_token_to_kv_pool.get_contiguous_buf_infos()
            )
            kv_data_ptrs += draft_kv_data_ptrs
            kv_data_lens += draft_kv_data_lens
            kv_item_lens += draft_kv_item_lens

        kv_args.kv_data_ptrs = kv_data_ptrs
        kv_args.kv_data_lens = kv_data_lens
        kv_args.kv_item_lens = kv_item_lens

        kv_args.aux_data_ptrs, kv_args.aux_data_lens, kv_args.aux_item_lens = (
            self.metadata_buffers.get_buf_infos()
        )

        kv_args.ib_device = self.scheduler.server_args.disaggregation_ib_device
        kv_args.gpu_id = self.scheduler.gpu_id
        kv_manager_class = get_kv_class(self.transfer_backend, KVClassType.MANAGER)
        kv_manager = kv_manager_class(
            kv_args,
            DisaggregationMode.DECODE,
            self.scheduler.server_args,
            self.is_mla_backend,
        )
        return kv_manager

    def add(self, req: Req, is_retracted: bool = False) -&gt; None:
        &#34;&#34;&#34;Add a request to the pending queue.&#34;&#34;&#34;
        if self._check_if_req_exceed_kv_capacity(req):
            return

        if is_retracted:
            self.retracted_queue.append(req)
        else:
            if req.bootstrap_host == FAKE_BOOTSTRAP_HOST:
                kv_receiver_class = get_kv_class(
                    TransferBackend.FAKE, KVClassType.RECEIVER
                )
            else:
                kv_receiver_class = get_kv_class(
                    self.transfer_backend, KVClassType.RECEIVER
                )

            kv_receiver = kv_receiver_class(
                mgr=self.kv_manager,
                bootstrap_addr=f&#34;{req.bootstrap_host}:{req.bootstrap_port}&#34;,
                bootstrap_room=req.bootstrap_room,
                data_parallel_rank=req.data_parallel_rank,
            )

            self.queue.append(
                DecodeRequest(req=req, kv_receiver=kv_receiver, waiting_for_input=False)
            )

    def _check_if_req_exceed_kv_capacity(self, req: Req) -&gt; bool:
        if len(req.origin_input_ids) &gt; self.max_total_num_tokens:
            message = f&#34;Request {req.rid} exceeds the maximum number of tokens: {len(req.origin_input_ids)} &gt; {self.max_total_num_tokens}&#34;
            logger.error(message)
            prepare_abort(req, message, status_code=HTTPStatus.BAD_REQUEST)
            self.scheduler.stream_output([req], req.return_logprob)
            return True
        return False

    def extend(self, reqs: List[Req], is_retracted: bool = False) -&gt; None:
        &#34;&#34;&#34;Add a request to the pending queue.&#34;&#34;&#34;
        for req in reqs:
            self.add(req, is_retracted=is_retracted)

    def resume_retracted_reqs(self) -&gt; List[Req]:
        # TODO refactor the scheduling part, reuse with the unified engine logic as much as possible

        # allocate memory
        resumed_reqs = []
        indices_to_remove = set()
        allocatable_tokens = self._allocatable_tokens(count_retracted=False)

        for i, req in enumerate(self.retracted_queue):
            if self.req_to_token_pool.available_size() &lt;= 0:
                break

            required_tokens_for_request = (
                len(req.origin_input_ids)
                + len(req.output_ids)
                + self.num_reserved_decode_tokens
            )
            if required_tokens_for_request &gt; allocatable_tokens:
                break

            resumed_reqs.append(req)
            indices_to_remove.add(i)
            req.is_retracted = False
            self._pre_alloc(req)
            allocatable_tokens -= required_tokens_for_request

            # load from cpu, release the cpu copy
            req.load_kv_cache(self.req_to_token_pool, self.token_to_kv_pool_allocator)

        self.retracted_queue = [
            entry
            for i, entry in enumerate(self.retracted_queue)
            if i not in indices_to_remove
        ]

        return resumed_reqs

    def _update_handshake_waiters(self) -&gt; None:
        if not self.queue:
            return

        if all(decode_req.waiting_for_input for decode_req in self.queue):
            return

        polls = poll_and_all_reduce(
            [decode_req.kv_receiver for decode_req in self.queue], self.gloo_group
        )

        for i, (decode_req, poll) in enumerate(zip(self.queue, polls)):
            if poll == KVPoll.Bootstrapping:
                pass
            elif poll == KVPoll.WaitingForInput:
                decode_req.waiting_for_input = True
            elif poll == KVPoll.Failed:
                error_message = f&#34;Decode handshake failed for request rank={self.tp_rank} {decode_req.req.rid=} {decode_req.req.bootstrap_room=}&#34;
                try:
                    decode_req.kv_receiver.failure_exception()
                except Exception as e:
                    error_message += f&#34; with exception {e}&#34;
                logger.error(error_message)
                prepare_abort(
                    decode_req.req,
                    error_message,
                    status_code=HTTPStatus.INTERNAL_SERVER_ERROR,
                )
                if self.scheduler.enable_metrics:
                    self.scheduler.metrics_collector.increment_bootstrap_failed_reqs()
            else:
                raise ValueError(f&#34;Unexpected poll case: {poll}&#34;)

    def pop_preallocated(self) -&gt; List[DecodeRequest]:
        &#34;&#34;&#34;Pop the preallocated requests from the pending queue (FIFO).&#34;&#34;&#34;
        self._update_handshake_waiters()

        preallocated_reqs = []
        indices_to_remove = set()

        # We need to make sure that the sum of inflight tokens and allocatable tokens is greater than maximum input+output length of each inflight request
        # Otherwise it is possible for one request running decode out of memory, while all other requests are in the transfer queue that cannot be retracted.
        retractable_tokens = sum(
            len(r.origin_input_ids) + len(r.output_ids)
            for r in self.scheduler.running_batch.reqs
        )
        allocatable_tokens = self._allocatable_tokens(
            retractable_tokens=retractable_tokens, count_retracted=True
        )
        # First, remove all failed requests from the queue
        for i, decode_req in enumerate(self.queue):
            if isinstance(decode_req.req.finished_reason, FINISH_ABORT):
                self.scheduler.stream_output(
                    [decode_req.req], decode_req.req.return_logprob
                )
                indices_to_remove.add(i)

        # Then, preallocate the remaining requests if possible
        for i, decode_req in enumerate(self.queue):
            if i in indices_to_remove:
                continue

            if not decode_req.waiting_for_input:
                continue

            if self.req_to_token_pool.available_size() &lt;= 0:
                break

            if self.req_to_metadata_buffer_idx_allocator.available_size() &lt;= 0:
                break

            # Memory estimation: don&#39;t add if the projected memory cannot be met
            # TODO: add new_token ratio
            origin_input_len = len(decode_req.req.origin_input_ids)
            required_tokens_for_request = (
                origin_input_len + self.num_reserved_decode_tokens
            )

            if (
                max(
                    required_tokens_for_request,
                    origin_input_len
                    + min(
                        decode_req.req.sampling_params.max_new_tokens,
                        CLIP_MAX_NEW_TOKEN,
                    )
                    - retractable_tokens,
                )
                &gt; allocatable_tokens
            ):
                break
            if required_tokens_for_request &gt; allocatable_tokens:
                break

            allocatable_tokens -= required_tokens_for_request
            self._pre_alloc(decode_req.req)

            kv_indices = (
                self.req_to_token_pool.req_to_token[decode_req.req.req_pool_idx][
                    : len(decode_req.req.origin_input_ids)
                ]
                .cpu()
                .numpy()
            )

            decode_req.metadata_buffer_index = (
                self.req_to_metadata_buffer_idx_allocator.alloc()
            )
            assert decode_req.metadata_buffer_index is not None
            page_indices = kv_to_page_indices(
                kv_indices, self.token_to_kv_pool_allocator.page_size
            )
            decode_req.kv_receiver.init(page_indices, decode_req.metadata_buffer_index)
            preallocated_reqs.append(decode_req)
            indices_to_remove.add(i)

        self.queue = [
            entry for i, entry in enumerate(self.queue) if i not in indices_to_remove
        ]

        return preallocated_reqs

    @property
    def num_tokens_pre_allocated(self):
        return sum(
            len(decode_req.req.fill_ids) for decode_req in self.transfer_queue.queue
        )

    def _allocatable_tokens(
        self, retractable_tokens: Optional[int] = None, count_retracted: bool = True
    ) -&gt; int:
        need_space_for_single_req = (
            max(
                [
                    min(x.sampling_params.max_new_tokens, CLIP_MAX_NEW_TOKEN)
                    + len(x.origin_input_ids)
                    - retractable_tokens
                    for x in self.scheduler.running_batch.reqs
                ]
            )
            if retractable_tokens is not None
            and len(self.scheduler.running_batch.reqs) &gt; 0
            else 0
        )

        if self.scheduler.model_config.is_hybrid:
            available_size = min(
                self.token_to_kv_pool_allocator.full_available_size(),
                self.token_to_kv_pool_allocator.swa_available_size(),
            )
        else:
            available_size = self.token_to_kv_pool_allocator.available_size()

        allocatable_tokens = available_size - max(
            # preserve some space for future decode
            self.num_reserved_decode_tokens
            * (
                len(self.scheduler.running_batch.reqs)
                + len(self.transfer_queue.queue)
                + len(self.scheduler.waiting_queue)
            ),
            # make sure each request can finish if reach max_tokens with all other requests retracted
            need_space_for_single_req,
        )

        # Note: if the last fake extend just finishes, and we enter `pop_preallocated` immediately in the next iteration
        #       the extend batch is not in any queue, so we need to explicitly add the tokens slots here
        if (
            self.scheduler.last_batch
            and self.scheduler.last_batch.forward_mode.is_extend()
        ):
            allocatable_tokens -= self.num_reserved_decode_tokens * len(
                self.scheduler.last_batch.reqs
            )

        if count_retracted:
            allocatable_tokens -= sum(
                [
                    len(req.origin_input_ids)
                    + len(req.output_ids)
                    + self.num_reserved_decode_tokens
                    for req in self.retracted_queue
                ]
            )
        return allocatable_tokens

    def _pre_alloc(self, req: Req) -&gt; torch.Tensor:
        &#34;&#34;&#34;Pre-allocate the memory for req_to_token and token_kv_pool&#34;&#34;&#34;
        req_pool_indices = self.req_to_token_pool.alloc(1)

        assert (
            req_pool_indices is not None
        ), &#34;req_pool_indices is full! There is a bug in memory estimation.&#34;

        req.req_pool_idx = req_pool_indices[0]

        if self.token_to_kv_pool_allocator.page_size == 1:
            kv_loc = self.token_to_kv_pool_allocator.alloc(
                len(req.origin_input_ids) + max(len(req.output_ids) - 1, 0)
            )
        else:
            num_tokens = len(req.origin_input_ids) + max(len(req.output_ids) - 1, 0)
            kv_loc = self.token_to_kv_pool_allocator.alloc_extend(
                prefix_lens=torch.tensor(
                    [0],
                    dtype=torch.int64,
                    device=self.token_to_kv_pool_allocator.device,
                ),
                seq_lens=torch.tensor(
                    [num_tokens],
                    dtype=torch.int64,
                    device=self.token_to_kv_pool_allocator.device,
                ),
                last_loc=torch.tensor(
                    [-1],
                    dtype=torch.int64,
                    device=self.token_to_kv_pool_allocator.device,
                ),
                extend_num_tokens=num_tokens,
            )

        assert (
            kv_loc is not None
        ), &#34;KV cache is full! There is a bug in memory estimation.&#34;

        self.req_to_token_pool.write((req.req_pool_idx, slice(0, len(kv_loc))), kv_loc)

        # populate metadata
        req.fill_ids = req.origin_input_ids + req.output_ids
        req.extend_input_len = len(req.origin_input_ids)

        return kv_loc</code></pre>
</details>
<div class="desc"><p>Store the requests that are preallocating.</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.disaggregation.decode.DecodePreallocQueue.num_tokens_pre_allocated"><code class="name">prop <span class="ident">num_tokens_pre_allocated</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def num_tokens_pre_allocated(self):
    return sum(
        len(decode_req.req.fill_ids) for decode_req in self.transfer_queue.queue
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.disaggregation.decode.DecodePreallocQueue.add"><code class="name flex">
<span>def <span class="ident">add</span></span>(<span>self, req: Req, is_retracted: bool = False) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add(self, req: Req, is_retracted: bool = False) -&gt; None:
    &#34;&#34;&#34;Add a request to the pending queue.&#34;&#34;&#34;
    if self._check_if_req_exceed_kv_capacity(req):
        return

    if is_retracted:
        self.retracted_queue.append(req)
    else:
        if req.bootstrap_host == FAKE_BOOTSTRAP_HOST:
            kv_receiver_class = get_kv_class(
                TransferBackend.FAKE, KVClassType.RECEIVER
            )
        else:
            kv_receiver_class = get_kv_class(
                self.transfer_backend, KVClassType.RECEIVER
            )

        kv_receiver = kv_receiver_class(
            mgr=self.kv_manager,
            bootstrap_addr=f&#34;{req.bootstrap_host}:{req.bootstrap_port}&#34;,
            bootstrap_room=req.bootstrap_room,
            data_parallel_rank=req.data_parallel_rank,
        )

        self.queue.append(
            DecodeRequest(req=req, kv_receiver=kv_receiver, waiting_for_input=False)
        )</code></pre>
</details>
<div class="desc"><p>Add a request to the pending queue.</p></div>
</dd>
<dt id="sglang.srt.disaggregation.decode.DecodePreallocQueue.extend"><code class="name flex">
<span>def <span class="ident">extend</span></span>(<span>self, reqs: List[Req], is_retracted: bool = False) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extend(self, reqs: List[Req], is_retracted: bool = False) -&gt; None:
    &#34;&#34;&#34;Add a request to the pending queue.&#34;&#34;&#34;
    for req in reqs:
        self.add(req, is_retracted=is_retracted)</code></pre>
</details>
<div class="desc"><p>Add a request to the pending queue.</p></div>
</dd>
<dt id="sglang.srt.disaggregation.decode.DecodePreallocQueue.pop_preallocated"><code class="name flex">
<span>def <span class="ident">pop_preallocated</span></span>(<span>self) ‑> List[<a title="sglang.srt.disaggregation.decode.DecodeRequest" href="#sglang.srt.disaggregation.decode.DecodeRequest">DecodeRequest</a>]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pop_preallocated(self) -&gt; List[DecodeRequest]:
    &#34;&#34;&#34;Pop the preallocated requests from the pending queue (FIFO).&#34;&#34;&#34;
    self._update_handshake_waiters()

    preallocated_reqs = []
    indices_to_remove = set()

    # We need to make sure that the sum of inflight tokens and allocatable tokens is greater than maximum input+output length of each inflight request
    # Otherwise it is possible for one request running decode out of memory, while all other requests are in the transfer queue that cannot be retracted.
    retractable_tokens = sum(
        len(r.origin_input_ids) + len(r.output_ids)
        for r in self.scheduler.running_batch.reqs
    )
    allocatable_tokens = self._allocatable_tokens(
        retractable_tokens=retractable_tokens, count_retracted=True
    )
    # First, remove all failed requests from the queue
    for i, decode_req in enumerate(self.queue):
        if isinstance(decode_req.req.finished_reason, FINISH_ABORT):
            self.scheduler.stream_output(
                [decode_req.req], decode_req.req.return_logprob
            )
            indices_to_remove.add(i)

    # Then, preallocate the remaining requests if possible
    for i, decode_req in enumerate(self.queue):
        if i in indices_to_remove:
            continue

        if not decode_req.waiting_for_input:
            continue

        if self.req_to_token_pool.available_size() &lt;= 0:
            break

        if self.req_to_metadata_buffer_idx_allocator.available_size() &lt;= 0:
            break

        # Memory estimation: don&#39;t add if the projected memory cannot be met
        # TODO: add new_token ratio
        origin_input_len = len(decode_req.req.origin_input_ids)
        required_tokens_for_request = (
            origin_input_len + self.num_reserved_decode_tokens
        )

        if (
            max(
                required_tokens_for_request,
                origin_input_len
                + min(
                    decode_req.req.sampling_params.max_new_tokens,
                    CLIP_MAX_NEW_TOKEN,
                )
                - retractable_tokens,
            )
            &gt; allocatable_tokens
        ):
            break
        if required_tokens_for_request &gt; allocatable_tokens:
            break

        allocatable_tokens -= required_tokens_for_request
        self._pre_alloc(decode_req.req)

        kv_indices = (
            self.req_to_token_pool.req_to_token[decode_req.req.req_pool_idx][
                : len(decode_req.req.origin_input_ids)
            ]
            .cpu()
            .numpy()
        )

        decode_req.metadata_buffer_index = (
            self.req_to_metadata_buffer_idx_allocator.alloc()
        )
        assert decode_req.metadata_buffer_index is not None
        page_indices = kv_to_page_indices(
            kv_indices, self.token_to_kv_pool_allocator.page_size
        )
        decode_req.kv_receiver.init(page_indices, decode_req.metadata_buffer_index)
        preallocated_reqs.append(decode_req)
        indices_to_remove.add(i)

    self.queue = [
        entry for i, entry in enumerate(self.queue) if i not in indices_to_remove
    ]

    return preallocated_reqs</code></pre>
</details>
<div class="desc"><p>Pop the preallocated requests from the pending queue (FIFO).</p></div>
</dd>
<dt id="sglang.srt.disaggregation.decode.DecodePreallocQueue.resume_retracted_reqs"><code class="name flex">
<span>def <span class="ident">resume_retracted_reqs</span></span>(<span>self) ‑> List[Req]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resume_retracted_reqs(self) -&gt; List[Req]:
    # TODO refactor the scheduling part, reuse with the unified engine logic as much as possible

    # allocate memory
    resumed_reqs = []
    indices_to_remove = set()
    allocatable_tokens = self._allocatable_tokens(count_retracted=False)

    for i, req in enumerate(self.retracted_queue):
        if self.req_to_token_pool.available_size() &lt;= 0:
            break

        required_tokens_for_request = (
            len(req.origin_input_ids)
            + len(req.output_ids)
            + self.num_reserved_decode_tokens
        )
        if required_tokens_for_request &gt; allocatable_tokens:
            break

        resumed_reqs.append(req)
        indices_to_remove.add(i)
        req.is_retracted = False
        self._pre_alloc(req)
        allocatable_tokens -= required_tokens_for_request

        # load from cpu, release the cpu copy
        req.load_kv_cache(self.req_to_token_pool, self.token_to_kv_pool_allocator)

    self.retracted_queue = [
        entry
        for i, entry in enumerate(self.retracted_queue)
        if i not in indices_to_remove
    ]

    return resumed_reqs</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.disaggregation.decode.DecodeReqToTokenPool"><code class="flex name class">
<span>class <span class="ident">DecodeReqToTokenPool</span></span>
<span>(</span><span>size: int,<br>max_context_len: int,<br>device: str,<br>enable_memory_saver: bool,<br>pre_alloc_size: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DecodeReqToTokenPool:
    &#34;&#34;&#34;
    The difference of DecodeReqToTokenPool and ReqToTokenPool is that
    DecodeReqToTokenPool subscribes memory for pre-allocated requests.

    In ReqToTokenPool, if `--max-running-requests` is 8,
    #pre-allocated + #transfer + #running &lt;= 8, but there are in fact more memory can carry pre-allocated requests.

    In DecodeReqToTokenPool, if `--max-running-requests` is 8,
    #running &lt;= 8, #pre-allocated + #transfer &lt;= pre_alloc_size, so we can use the free memory to pre-allocate requests to unblock prefill.
    &#34;&#34;&#34;

    def __init__(
        self,
        size: int,
        max_context_len: int,
        device: str,
        enable_memory_saver: bool,
        pre_alloc_size: int,
    ):
        memory_saver_adapter = TorchMemorySaverAdapter.create(
            enable=enable_memory_saver
        )

        self.size = size
        self.max_context_len = max_context_len
        self.device = device
        self.pre_alloc_size = pre_alloc_size
        with memory_saver_adapter.region(tag=GPU_MEMORY_TYPE_KV_CACHE):
            self.req_to_token = torch.zeros(
                (size + pre_alloc_size, max_context_len),
                dtype=torch.int32,
                device=device,
            )

        self.free_slots = list(range(size + pre_alloc_size))

    def write(self, indices, values):
        self.req_to_token[indices] = values

    def available_size(self):
        return len(self.free_slots)

    def alloc(self, need_size: int) -&gt; List[int]:
        if need_size &gt; len(self.free_slots):
            return None

        select_index = self.free_slots[:need_size]
        self.free_slots = self.free_slots[need_size:]
        return select_index

    def free(self, free_index: Union[int, List[int]]):
        if isinstance(free_index, (int,)):
            self.free_slots.append(free_index)
        else:
            self.free_slots.extend(free_index)

    def clear(self):
        self.free_slots = list(range(self.size + self.pre_alloc_size))</code></pre>
</details>
<div class="desc"><p>The difference of DecodeReqToTokenPool and ReqToTokenPool is that
DecodeReqToTokenPool subscribes memory for pre-allocated requests.</p>
<p>In ReqToTokenPool, if <code>--max-running-requests</code> is 8,</p>
<h1 id="pre-allocated-transfer-running-8-but-there-are-in-fact-more-memory-can-carry-pre-allocated-requests">pre-allocated + #transfer + #running &lt;= 8, but there are in fact more memory can carry pre-allocated requests.</h1>
<p>In DecodeReqToTokenPool, if <code>--max-running-requests</code> is 8,</p>
<h1 id="running-8-pre-allocated-transfer-pre_alloc_size-so-we-can-use-the-free-memory-to-pre-allocate-requests-to-unblock-prefill">running &lt;= 8, #pre-allocated + #transfer &lt;= pre_alloc_size, so we can use the free memory to pre-allocate requests to unblock prefill.</h1></div>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.disaggregation.decode.DecodeReqToTokenPool.alloc"><code class="name flex">
<span>def <span class="ident">alloc</span></span>(<span>self, need_size: int) ‑> List[int]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def alloc(self, need_size: int) -&gt; List[int]:
    if need_size &gt; len(self.free_slots):
        return None

    select_index = self.free_slots[:need_size]
    self.free_slots = self.free_slots[need_size:]
    return select_index</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.disaggregation.decode.DecodeReqToTokenPool.available_size"><code class="name flex">
<span>def <span class="ident">available_size</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def available_size(self):
    return len(self.free_slots)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.disaggregation.decode.DecodeReqToTokenPool.clear"><code class="name flex">
<span>def <span class="ident">clear</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clear(self):
    self.free_slots = list(range(self.size + self.pre_alloc_size))</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.disaggregation.decode.DecodeReqToTokenPool.free"><code class="name flex">
<span>def <span class="ident">free</span></span>(<span>self, free_index: Union[int, List[int]])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def free(self, free_index: Union[int, List[int]]):
    if isinstance(free_index, (int,)):
        self.free_slots.append(free_index)
    else:
        self.free_slots.extend(free_index)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.disaggregation.decode.DecodeReqToTokenPool.write"><code class="name flex">
<span>def <span class="ident">write</span></span>(<span>self, indices, values)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write(self, indices, values):
    self.req_to_token[indices] = values</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.disaggregation.decode.DecodeRequest"><code class="flex name class">
<span>class <span class="ident">DecodeRequest</span></span>
<span>(</span><span>req: Req,<br>kv_receiver: BaseKVReceiver,<br>waiting_for_input: bool = False,<br>metadata_buffer_index: int = -1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class DecodeRequest:
    req: Req
    kv_receiver: BaseKVReceiver
    waiting_for_input: bool = False
    metadata_buffer_index: int = -1</code></pre>
</details>
<div class="desc"><p>DecodeRequest(req: 'Req', kv_receiver: 'BaseKVReceiver', waiting_for_input: 'bool' = False, metadata_buffer_index: 'int' = -1)</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.disaggregation.decode.DecodeRequest.kv_receiver"><code class="name">var <span class="ident">kv_receiver</span> : BaseKVReceiver</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.disaggregation.decode.DecodeRequest.metadata_buffer_index"><code class="name">var <span class="ident">metadata_buffer_index</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.disaggregation.decode.DecodeRequest.req"><code class="name">var <span class="ident">req</span> : Req</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.disaggregation.decode.DecodeRequest.waiting_for_input"><code class="name">var <span class="ident">waiting_for_input</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.disaggregation.decode.DecodeTransferQueue"><code class="flex name class">
<span>class <span class="ident">DecodeTransferQueue</span></span>
<span>(</span><span>gloo_group: ProcessGroup,<br>req_to_metadata_buffer_idx_allocator: ReqToMetadataIdxAllocator,<br>tp_rank: int,<br>metadata_buffers: MetadataBuffers,<br>scheduler: Scheduler,<br>tree_cache: BasePrefixCache)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DecodeTransferQueue:
    &#34;&#34;&#34;
    Store the requests that is polling kv
    &#34;&#34;&#34;

    def __init__(
        self,
        gloo_group: ProcessGroup,
        req_to_metadata_buffer_idx_allocator: ReqToMetadataIdxAllocator,
        tp_rank: int,
        metadata_buffers: MetadataBuffers,
        scheduler: Scheduler,
        tree_cache: BasePrefixCache,
    ):
        self.queue: List[DecodeRequest] = []
        self.gloo_group = gloo_group
        self.req_to_metadata_buffer_idx_allocator = req_to_metadata_buffer_idx_allocator
        self.tp_rank = tp_rank
        self.metadata_buffers = metadata_buffers
        self.scheduler = scheduler
        self.tree_cache = tree_cache
        self.spec_algorithm = scheduler.spec_algorithm

    def add(self, decode_req: DecodeRequest) -&gt; None:
        self.queue.append(decode_req)

    def extend(self, decode_reqs: List[DecodeRequest]) -&gt; None:
        self.queue.extend(decode_reqs)

    def pop_transferred(self) -&gt; List[Req]:
        if not self.queue:
            return []
        polls = poll_and_all_reduce(
            [decode_req.kv_receiver for decode_req in self.queue], self.gloo_group
        )

        transferred_reqs = []
        indices_to_remove = set()
        for i, (decode_req, poll) in enumerate(zip(self.queue, polls)):
            if poll == KVPoll.Failed:
                error_message = f&#34;Decode transfer failed for request rank={self.tp_rank} {decode_req.req.rid=} {decode_req.req.bootstrap_room=}&#34;
                try:
                    decode_req.kv_receiver.failure_exception()
                except Exception as e:
                    error_message += f&#34; with exception {e}&#34;
                logger.error(error_message)
                prepare_abort(
                    decode_req.req,
                    error_message,
                    status_code=HTTPStatus.INTERNAL_SERVER_ERROR,
                )
                self.scheduler.stream_output(
                    [decode_req.req], decode_req.req.return_logprob
                )
                # unlock the kv cache or it will have memory leak
                self.tree_cache.cache_finished_req(decode_req.req)
                indices_to_remove.add(i)
                if self.scheduler.enable_metrics:
                    self.scheduler.metrics_collector.increment_transfer_failed_reqs()
                continue
            elif poll == KVPoll.Success:

                idx = decode_req.metadata_buffer_index
                (
                    output_id,
                    output_token_logprobs_val,
                    output_token_logprobs_idx,
                    output_top_logprobs_val,
                    output_top_logprobs_idx,
                    output_hidden_states,
                ) = self.metadata_buffers.get_buf(idx)

                decode_req.req.output_ids.append(output_id[0].item())
                if not self.spec_algorithm.is_none():
                    decode_req.req.hidden_states_tensor = output_hidden_states
                if decode_req.req.return_logprob:
                    decode_req.req.output_token_logprobs_val.append(
                        output_token_logprobs_val[0].item()
                    )
                    decode_req.req.output_token_logprobs_idx.append(
                        output_token_logprobs_idx[0].item()
                    )
                    decode_req.req.output_top_logprobs_val.append(
                        output_top_logprobs_val[
                            : decode_req.req.top_logprobs_num
                        ].tolist()
                    )
                    decode_req.req.output_top_logprobs_idx.append(
                        output_top_logprobs_idx[
                            : decode_req.req.top_logprobs_num
                        ].tolist()
                    )

                if hasattr(decode_req.kv_receiver, &#34;clear&#34;):
                    decode_req.kv_receiver.clear()

                # special handling for sampling_params.max_new_tokens == 1
                if decode_req.req.sampling_params.max_new_tokens == 1:
                    # finish immediately
                    decode_req.req.check_finished()
                    self.scheduler.stream_output(
                        [decode_req.req], decode_req.req.return_logprob
                    )
                    self.tree_cache.cache_finished_req(decode_req.req)
                else:
                    transferred_reqs.append(decode_req.req)

                indices_to_remove.add(i)
            elif poll in [
                KVPoll.Bootstrapping,
                KVPoll.WaitingForInput,
                KVPoll.Transferring,
            ]:
                pass
            else:
                raise ValueError(f&#34;Unexpected poll case: {poll}&#34;)

        for i in indices_to_remove:
            idx = self.queue[i].metadata_buffer_index
            assert idx != -1
            self.req_to_metadata_buffer_idx_allocator.free(idx)

        self.queue = [
            entry for i, entry in enumerate(self.queue) if i not in indices_to_remove
        ]

        return transferred_reqs</code></pre>
</details>
<div class="desc"><p>Store the requests that is polling kv</p></div>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.disaggregation.decode.DecodeTransferQueue.add"><code class="name flex">
<span>def <span class="ident">add</span></span>(<span>self,<br>decode_req: <a title="sglang.srt.disaggregation.decode.DecodeRequest" href="#sglang.srt.disaggregation.decode.DecodeRequest">DecodeRequest</a>) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add(self, decode_req: DecodeRequest) -&gt; None:
    self.queue.append(decode_req)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.disaggregation.decode.DecodeTransferQueue.extend"><code class="name flex">
<span>def <span class="ident">extend</span></span>(<span>self,<br>decode_reqs: List[<a title="sglang.srt.disaggregation.decode.DecodeRequest" href="#sglang.srt.disaggregation.decode.DecodeRequest">DecodeRequest</a>]) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extend(self, decode_reqs: List[DecodeRequest]) -&gt; None:
    self.queue.extend(decode_reqs)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.disaggregation.decode.DecodeTransferQueue.pop_transferred"><code class="name flex">
<span>def <span class="ident">pop_transferred</span></span>(<span>self) ‑> List[Req]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pop_transferred(self) -&gt; List[Req]:
    if not self.queue:
        return []
    polls = poll_and_all_reduce(
        [decode_req.kv_receiver for decode_req in self.queue], self.gloo_group
    )

    transferred_reqs = []
    indices_to_remove = set()
    for i, (decode_req, poll) in enumerate(zip(self.queue, polls)):
        if poll == KVPoll.Failed:
            error_message = f&#34;Decode transfer failed for request rank={self.tp_rank} {decode_req.req.rid=} {decode_req.req.bootstrap_room=}&#34;
            try:
                decode_req.kv_receiver.failure_exception()
            except Exception as e:
                error_message += f&#34; with exception {e}&#34;
            logger.error(error_message)
            prepare_abort(
                decode_req.req,
                error_message,
                status_code=HTTPStatus.INTERNAL_SERVER_ERROR,
            )
            self.scheduler.stream_output(
                [decode_req.req], decode_req.req.return_logprob
            )
            # unlock the kv cache or it will have memory leak
            self.tree_cache.cache_finished_req(decode_req.req)
            indices_to_remove.add(i)
            if self.scheduler.enable_metrics:
                self.scheduler.metrics_collector.increment_transfer_failed_reqs()
            continue
        elif poll == KVPoll.Success:

            idx = decode_req.metadata_buffer_index
            (
                output_id,
                output_token_logprobs_val,
                output_token_logprobs_idx,
                output_top_logprobs_val,
                output_top_logprobs_idx,
                output_hidden_states,
            ) = self.metadata_buffers.get_buf(idx)

            decode_req.req.output_ids.append(output_id[0].item())
            if not self.spec_algorithm.is_none():
                decode_req.req.hidden_states_tensor = output_hidden_states
            if decode_req.req.return_logprob:
                decode_req.req.output_token_logprobs_val.append(
                    output_token_logprobs_val[0].item()
                )
                decode_req.req.output_token_logprobs_idx.append(
                    output_token_logprobs_idx[0].item()
                )
                decode_req.req.output_top_logprobs_val.append(
                    output_top_logprobs_val[
                        : decode_req.req.top_logprobs_num
                    ].tolist()
                )
                decode_req.req.output_top_logprobs_idx.append(
                    output_top_logprobs_idx[
                        : decode_req.req.top_logprobs_num
                    ].tolist()
                )

            if hasattr(decode_req.kv_receiver, &#34;clear&#34;):
                decode_req.kv_receiver.clear()

            # special handling for sampling_params.max_new_tokens == 1
            if decode_req.req.sampling_params.max_new_tokens == 1:
                # finish immediately
                decode_req.req.check_finished()
                self.scheduler.stream_output(
                    [decode_req.req], decode_req.req.return_logprob
                )
                self.tree_cache.cache_finished_req(decode_req.req)
            else:
                transferred_reqs.append(decode_req.req)

            indices_to_remove.add(i)
        elif poll in [
            KVPoll.Bootstrapping,
            KVPoll.WaitingForInput,
            KVPoll.Transferring,
        ]:
            pass
        else:
            raise ValueError(f&#34;Unexpected poll case: {poll}&#34;)

    for i in indices_to_remove:
        idx = self.queue[i].metadata_buffer_index
        assert idx != -1
        self.req_to_metadata_buffer_idx_allocator.free(idx)

    self.queue = [
        entry for i, entry in enumerate(self.queue) if i not in indices_to_remove
    ]

    return transferred_reqs</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.disaggregation.decode.SchedulerDisaggregationDecodeMixin"><code class="flex name class">
<span>class <span class="ident">SchedulerDisaggregationDecodeMixin</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SchedulerDisaggregationDecodeMixin:

    @torch.no_grad()
    def event_loop_normal_disagg_decode(self: Scheduler):
        &#34;&#34;&#34;A normal scheduler loop for decode worker in disaggregation mode.&#34;&#34;&#34;

        while True:
            recv_reqs = self.recv_requests()
            self.process_input_requests(recv_reqs)
            # polling and allocating kv cache
            self.process_decode_queue()
            batch = self.get_next_disagg_decode_batch_to_run()
            self.cur_batch = batch

            prepare_mlp_sync_flag = require_mlp_sync(self.server_args)

            if batch:
                # Generate fake extend output.
                if batch.forward_mode.is_extend():
                    # Note: Logprobs should be handled on the prefill engine.
                    self.stream_output(
                        batch.reqs, any(req.return_logprob for req in batch.reqs)
                    )
                    if prepare_mlp_sync_flag:
                        self._prepare_idle_batch_and_run(None)
                else:
                    if prepare_mlp_sync_flag:
                        self.prepare_mlp_sync_batch(batch)
                    result = self.run_batch(batch)
                    self.process_batch_result(batch, result)
            elif prepare_mlp_sync_flag:
                batch, _ = self._prepare_idle_batch_and_run(None)

            if batch is None and (
                len(self.waiting_queue)
                + len(self.disagg_decode_transfer_queue.queue)
                + len(self.disagg_decode_prealloc_queue.queue)
                == 0
            ):
                self.self_check_during_idle()

            self.last_batch = batch

    @torch.no_grad()
    def event_loop_overlap_disagg_decode(self: Scheduler):
        result_queue = deque()
        self.last_batch: Optional[ScheduleBatch] = None
        self.last_batch_in_queue = False  # last batch is modified in-place, so we need another variable to track if it&#39;s extend

        while True:
            recv_reqs = self.recv_requests()
            self.process_input_requests(recv_reqs)
            # polling and allocating kv cache
            self.process_decode_queue()
            batch = self.get_next_disagg_decode_batch_to_run()
            self.cur_batch = batch
            last_batch_in_queue = False

            prepare_mlp_sync_flag = require_mlp_sync(self.server_args)

            if batch:
                # Generate fake extend output.
                if batch.forward_mode.is_extend():
                    # Note: Logprobs should be handled on the prefill engine.
                    self.stream_output(
                        batch.reqs, any(req.return_logprob for req in batch.reqs)
                    )
                    if prepare_mlp_sync_flag:
                        batch_, result = self._prepare_idle_batch_and_run(
                            None, delay_process=True
                        )
                        if batch_:
                            result_queue.append((batch_.copy(), result))
                            last_batch_in_queue = True
                else:
                    if prepare_mlp_sync_flag:
                        self.prepare_mlp_sync_batch(batch)
                    result = self.run_batch(batch)
                    result_queue.append((batch.copy(), result))

                    if (self.last_batch is None) or (not self.last_batch_in_queue):
                        # Create a dummy first batch to start the pipeline for overlap schedule.
                        # It is now used for triggering the sampling_info_done event.
                        tmp_batch = ScheduleBatch(
                            reqs=None,
                            forward_mode=ForwardMode.DUMMY_FIRST,
                            next_batch_sampling_info=self.tp_worker.cur_sampling_info,
                        )
                        self.set_next_batch_sampling_info_done(tmp_batch)
                    last_batch_in_queue = True

            elif prepare_mlp_sync_flag:
                batch, result = self._prepare_idle_batch_and_run(
                    None, delay_process=True
                )
                if batch:
                    result_queue.append((batch.copy(), result))
                    last_batch_in_queue = True

            # Process the results of the previous batch but skip if the last batch is extend
            if self.last_batch and self.last_batch_in_queue:
                tmp_batch, tmp_result = result_queue.popleft()
                tmp_batch.next_batch_sampling_info = (
                    self.tp_worker.cur_sampling_info if batch else None
                )
                self.process_batch_result(tmp_batch, tmp_result)

            if batch is None and (
                len(self.waiting_queue)
                + len(self.disagg_decode_transfer_queue.queue)
                + len(self.disagg_decode_prealloc_queue.queue)
                == 0
            ):
                self.self_check_during_idle()

            self.last_batch = batch
            self.last_batch_in_queue = last_batch_in_queue

    def _prepare_idle_batch_and_run(self: Scheduler, batch, delay_process=False):
        batch = self.prepare_mlp_sync_batch(batch)
        result = None
        if batch:
            result = self.run_batch(batch)
            if not delay_process:
                self.process_batch_result(batch, result)
        return batch, result

    def get_next_disagg_decode_batch_to_run(
        self: Scheduler,
    ) -&gt; Optional[Tuple[ScheduleBatch, bool]]:
        &#34;&#34;&#34;Create fake completed prefill if possible and merge with running batch&#34;&#34;&#34;
        # Merge the prefill batch into the running batch
        last_batch = self.last_batch
        if last_batch and last_batch.forward_mode.is_extend():
            # chunked prefill doesn&#39;t happen in decode instance.
            assert self.chunked_req is None
            # Filter finished batches.
            last_batch.filter_batch()
            if not last_batch.is_empty():
                if self.running_batch.is_empty():
                    self.running_batch = last_batch
                else:
                    # merge running_batch with prefill batch
                    self.running_batch.merge_batch(last_batch)

        new_prebuilt_batch = self.get_new_prebuilt_batch()

        ret: Optional[ScheduleBatch] = None
        if new_prebuilt_batch:
            ret = new_prebuilt_batch
        else:
            if self.running_batch.is_empty():
                ret = None
            else:
                self.running_batch = self.update_running_batch(self.running_batch)
                ret = self.running_batch if not self.running_batch.is_empty() else None

        return ret

    def get_new_prebuilt_batch(self: Scheduler) -&gt; Optional[ScheduleBatch]:
        &#34;&#34;&#34;Create a schedulebatch for fake completed prefill&#34;&#34;&#34;
        if self.grammar_queue:
            self.move_ready_grammar_requests()

        if len(self.waiting_queue) == 0:
            return None

        curr_batch_size = self.running_batch.batch_size()

        batch_size = min(self.req_to_token_pool.size, self.max_running_requests)

        num_not_used_batch = batch_size - curr_batch_size

        # pop req from waiting queue
        can_run_list: List[Req] = []
        waiting_queue: List[Req] = []

        for i in range(len(self.waiting_queue)):
            req = self.waiting_queue[i]
            # we can only add at least `num_not_used_batch` new batch to the running queue
            if i &lt; num_not_used_batch:
                can_run_list.append(req)
                req.init_next_round_input(self.tree_cache)
            else:
                waiting_queue.append(req)

        self.waiting_queue = waiting_queue
        if len(can_run_list) == 0:
            return None

        # construct a schedule batch with those requests and mark as decode
        new_batch = ScheduleBatch.init_new(
            can_run_list,
            self.req_to_token_pool,
            self.token_to_kv_pool_allocator,
            self.tree_cache,
            self.model_config,
            self.enable_overlap,
            self.spec_algorithm,
        )

        # construct fake completed prefill
        new_batch.prepare_for_prebuilt_extend()
        new_batch.process_prebuilt_extend(self.server_args, self.model_config)

        return new_batch

    def process_decode_queue(self: Scheduler):
        # try to resume retracted requests if there are enough space for another `num_reserved_decode_tokens` decode steps
        resumed_reqs = self.disagg_decode_prealloc_queue.resume_retracted_reqs()
        self.waiting_queue.extend(resumed_reqs)
        if len(self.disagg_decode_prealloc_queue.retracted_queue) &gt; 0:
            # if there are still retracted requests, we do not allocate new requests
            return

        req_conns = self.disagg_decode_prealloc_queue.pop_preallocated()
        self.disagg_decode_transfer_queue.extend(req_conns)
        alloc_reqs = (
            self.disagg_decode_transfer_queue.pop_transferred()
        )  # the requests which kv has arrived
        self.waiting_queue.extend(alloc_reqs)</code></pre>
</details>
<div class="desc"></div>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.managers.scheduler.Scheduler" href="../managers/scheduler.html#sglang.srt.managers.scheduler.Scheduler">Scheduler</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.disaggregation.decode.SchedulerDisaggregationDecodeMixin.event_loop_normal_disagg_decode"><code class="name flex">
<span>def <span class="ident">event_loop_normal_disagg_decode</span></span>(<span>self: Scheduler)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def event_loop_normal_disagg_decode(self: Scheduler):
    &#34;&#34;&#34;A normal scheduler loop for decode worker in disaggregation mode.&#34;&#34;&#34;

    while True:
        recv_reqs = self.recv_requests()
        self.process_input_requests(recv_reqs)
        # polling and allocating kv cache
        self.process_decode_queue()
        batch = self.get_next_disagg_decode_batch_to_run()
        self.cur_batch = batch

        prepare_mlp_sync_flag = require_mlp_sync(self.server_args)

        if batch:
            # Generate fake extend output.
            if batch.forward_mode.is_extend():
                # Note: Logprobs should be handled on the prefill engine.
                self.stream_output(
                    batch.reqs, any(req.return_logprob for req in batch.reqs)
                )
                if prepare_mlp_sync_flag:
                    self._prepare_idle_batch_and_run(None)
            else:
                if prepare_mlp_sync_flag:
                    self.prepare_mlp_sync_batch(batch)
                result = self.run_batch(batch)
                self.process_batch_result(batch, result)
        elif prepare_mlp_sync_flag:
            batch, _ = self._prepare_idle_batch_and_run(None)

        if batch is None and (
            len(self.waiting_queue)
            + len(self.disagg_decode_transfer_queue.queue)
            + len(self.disagg_decode_prealloc_queue.queue)
            == 0
        ):
            self.self_check_during_idle()

        self.last_batch = batch</code></pre>
</details>
<div class="desc"><p>A normal scheduler loop for decode worker in disaggregation mode.</p></div>
</dd>
<dt id="sglang.srt.disaggregation.decode.SchedulerDisaggregationDecodeMixin.event_loop_overlap_disagg_decode"><code class="name flex">
<span>def <span class="ident">event_loop_overlap_disagg_decode</span></span>(<span>self: Scheduler)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def event_loop_overlap_disagg_decode(self: Scheduler):
    result_queue = deque()
    self.last_batch: Optional[ScheduleBatch] = None
    self.last_batch_in_queue = False  # last batch is modified in-place, so we need another variable to track if it&#39;s extend

    while True:
        recv_reqs = self.recv_requests()
        self.process_input_requests(recv_reqs)
        # polling and allocating kv cache
        self.process_decode_queue()
        batch = self.get_next_disagg_decode_batch_to_run()
        self.cur_batch = batch
        last_batch_in_queue = False

        prepare_mlp_sync_flag = require_mlp_sync(self.server_args)

        if batch:
            # Generate fake extend output.
            if batch.forward_mode.is_extend():
                # Note: Logprobs should be handled on the prefill engine.
                self.stream_output(
                    batch.reqs, any(req.return_logprob for req in batch.reqs)
                )
                if prepare_mlp_sync_flag:
                    batch_, result = self._prepare_idle_batch_and_run(
                        None, delay_process=True
                    )
                    if batch_:
                        result_queue.append((batch_.copy(), result))
                        last_batch_in_queue = True
            else:
                if prepare_mlp_sync_flag:
                    self.prepare_mlp_sync_batch(batch)
                result = self.run_batch(batch)
                result_queue.append((batch.copy(), result))

                if (self.last_batch is None) or (not self.last_batch_in_queue):
                    # Create a dummy first batch to start the pipeline for overlap schedule.
                    # It is now used for triggering the sampling_info_done event.
                    tmp_batch = ScheduleBatch(
                        reqs=None,
                        forward_mode=ForwardMode.DUMMY_FIRST,
                        next_batch_sampling_info=self.tp_worker.cur_sampling_info,
                    )
                    self.set_next_batch_sampling_info_done(tmp_batch)
                last_batch_in_queue = True

        elif prepare_mlp_sync_flag:
            batch, result = self._prepare_idle_batch_and_run(
                None, delay_process=True
            )
            if batch:
                result_queue.append((batch.copy(), result))
                last_batch_in_queue = True

        # Process the results of the previous batch but skip if the last batch is extend
        if self.last_batch and self.last_batch_in_queue:
            tmp_batch, tmp_result = result_queue.popleft()
            tmp_batch.next_batch_sampling_info = (
                self.tp_worker.cur_sampling_info if batch else None
            )
            self.process_batch_result(tmp_batch, tmp_result)

        if batch is None and (
            len(self.waiting_queue)
            + len(self.disagg_decode_transfer_queue.queue)
            + len(self.disagg_decode_prealloc_queue.queue)
            == 0
        ):
            self.self_check_during_idle()

        self.last_batch = batch
        self.last_batch_in_queue = last_batch_in_queue</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.disaggregation.decode.SchedulerDisaggregationDecodeMixin.get_new_prebuilt_batch"><code class="name flex">
<span>def <span class="ident">get_new_prebuilt_batch</span></span>(<span>self: Scheduler) ‑> Optional[ScheduleBatch]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_new_prebuilt_batch(self: Scheduler) -&gt; Optional[ScheduleBatch]:
    &#34;&#34;&#34;Create a schedulebatch for fake completed prefill&#34;&#34;&#34;
    if self.grammar_queue:
        self.move_ready_grammar_requests()

    if len(self.waiting_queue) == 0:
        return None

    curr_batch_size = self.running_batch.batch_size()

    batch_size = min(self.req_to_token_pool.size, self.max_running_requests)

    num_not_used_batch = batch_size - curr_batch_size

    # pop req from waiting queue
    can_run_list: List[Req] = []
    waiting_queue: List[Req] = []

    for i in range(len(self.waiting_queue)):
        req = self.waiting_queue[i]
        # we can only add at least `num_not_used_batch` new batch to the running queue
        if i &lt; num_not_used_batch:
            can_run_list.append(req)
            req.init_next_round_input(self.tree_cache)
        else:
            waiting_queue.append(req)

    self.waiting_queue = waiting_queue
    if len(can_run_list) == 0:
        return None

    # construct a schedule batch with those requests and mark as decode
    new_batch = ScheduleBatch.init_new(
        can_run_list,
        self.req_to_token_pool,
        self.token_to_kv_pool_allocator,
        self.tree_cache,
        self.model_config,
        self.enable_overlap,
        self.spec_algorithm,
    )

    # construct fake completed prefill
    new_batch.prepare_for_prebuilt_extend()
    new_batch.process_prebuilt_extend(self.server_args, self.model_config)

    return new_batch</code></pre>
</details>
<div class="desc"><p>Create a schedulebatch for fake completed prefill</p></div>
</dd>
<dt id="sglang.srt.disaggregation.decode.SchedulerDisaggregationDecodeMixin.get_next_disagg_decode_batch_to_run"><code class="name flex">
<span>def <span class="ident">get_next_disagg_decode_batch_to_run</span></span>(<span>self: Scheduler) ‑> Optional[Tuple[ScheduleBatch, bool]]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_next_disagg_decode_batch_to_run(
    self: Scheduler,
) -&gt; Optional[Tuple[ScheduleBatch, bool]]:
    &#34;&#34;&#34;Create fake completed prefill if possible and merge with running batch&#34;&#34;&#34;
    # Merge the prefill batch into the running batch
    last_batch = self.last_batch
    if last_batch and last_batch.forward_mode.is_extend():
        # chunked prefill doesn&#39;t happen in decode instance.
        assert self.chunked_req is None
        # Filter finished batches.
        last_batch.filter_batch()
        if not last_batch.is_empty():
            if self.running_batch.is_empty():
                self.running_batch = last_batch
            else:
                # merge running_batch with prefill batch
                self.running_batch.merge_batch(last_batch)

    new_prebuilt_batch = self.get_new_prebuilt_batch()

    ret: Optional[ScheduleBatch] = None
    if new_prebuilt_batch:
        ret = new_prebuilt_batch
    else:
        if self.running_batch.is_empty():
            ret = None
        else:
            self.running_batch = self.update_running_batch(self.running_batch)
            ret = self.running_batch if not self.running_batch.is_empty() else None

    return ret</code></pre>
</details>
<div class="desc"><p>Create fake completed prefill if possible and merge with running batch</p></div>
</dd>
<dt id="sglang.srt.disaggregation.decode.SchedulerDisaggregationDecodeMixin.process_decode_queue"><code class="name flex">
<span>def <span class="ident">process_decode_queue</span></span>(<span>self: Scheduler)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_decode_queue(self: Scheduler):
    # try to resume retracted requests if there are enough space for another `num_reserved_decode_tokens` decode steps
    resumed_reqs = self.disagg_decode_prealloc_queue.resume_retracted_reqs()
    self.waiting_queue.extend(resumed_reqs)
    if len(self.disagg_decode_prealloc_queue.retracted_queue) &gt; 0:
        # if there are still retracted requests, we do not allocate new requests
        return

    req_conns = self.disagg_decode_prealloc_queue.pop_preallocated()
    self.disagg_decode_transfer_queue.extend(req_conns)
    alloc_reqs = (
        self.disagg_decode_transfer_queue.pop_transferred()
    )  # the requests which kv has arrived
    self.waiting_queue.extend(alloc_reqs)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.disaggregation" href="index.html">sglang.srt.disaggregation</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.disaggregation.decode.DecodePreallocQueue" href="#sglang.srt.disaggregation.decode.DecodePreallocQueue">DecodePreallocQueue</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.disaggregation.decode.DecodePreallocQueue.add" href="#sglang.srt.disaggregation.decode.DecodePreallocQueue.add">add</a></code></li>
<li><code><a title="sglang.srt.disaggregation.decode.DecodePreallocQueue.extend" href="#sglang.srt.disaggregation.decode.DecodePreallocQueue.extend">extend</a></code></li>
<li><code><a title="sglang.srt.disaggregation.decode.DecodePreallocQueue.num_tokens_pre_allocated" href="#sglang.srt.disaggregation.decode.DecodePreallocQueue.num_tokens_pre_allocated">num_tokens_pre_allocated</a></code></li>
<li><code><a title="sglang.srt.disaggregation.decode.DecodePreallocQueue.pop_preallocated" href="#sglang.srt.disaggregation.decode.DecodePreallocQueue.pop_preallocated">pop_preallocated</a></code></li>
<li><code><a title="sglang.srt.disaggregation.decode.DecodePreallocQueue.resume_retracted_reqs" href="#sglang.srt.disaggregation.decode.DecodePreallocQueue.resume_retracted_reqs">resume_retracted_reqs</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.disaggregation.decode.DecodeReqToTokenPool" href="#sglang.srt.disaggregation.decode.DecodeReqToTokenPool">DecodeReqToTokenPool</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.disaggregation.decode.DecodeReqToTokenPool.alloc" href="#sglang.srt.disaggregation.decode.DecodeReqToTokenPool.alloc">alloc</a></code></li>
<li><code><a title="sglang.srt.disaggregation.decode.DecodeReqToTokenPool.available_size" href="#sglang.srt.disaggregation.decode.DecodeReqToTokenPool.available_size">available_size</a></code></li>
<li><code><a title="sglang.srt.disaggregation.decode.DecodeReqToTokenPool.clear" href="#sglang.srt.disaggregation.decode.DecodeReqToTokenPool.clear">clear</a></code></li>
<li><code><a title="sglang.srt.disaggregation.decode.DecodeReqToTokenPool.free" href="#sglang.srt.disaggregation.decode.DecodeReqToTokenPool.free">free</a></code></li>
<li><code><a title="sglang.srt.disaggregation.decode.DecodeReqToTokenPool.write" href="#sglang.srt.disaggregation.decode.DecodeReqToTokenPool.write">write</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.disaggregation.decode.DecodeRequest" href="#sglang.srt.disaggregation.decode.DecodeRequest">DecodeRequest</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.disaggregation.decode.DecodeRequest.kv_receiver" href="#sglang.srt.disaggregation.decode.DecodeRequest.kv_receiver">kv_receiver</a></code></li>
<li><code><a title="sglang.srt.disaggregation.decode.DecodeRequest.metadata_buffer_index" href="#sglang.srt.disaggregation.decode.DecodeRequest.metadata_buffer_index">metadata_buffer_index</a></code></li>
<li><code><a title="sglang.srt.disaggregation.decode.DecodeRequest.req" href="#sglang.srt.disaggregation.decode.DecodeRequest.req">req</a></code></li>
<li><code><a title="sglang.srt.disaggregation.decode.DecodeRequest.waiting_for_input" href="#sglang.srt.disaggregation.decode.DecodeRequest.waiting_for_input">waiting_for_input</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.disaggregation.decode.DecodeTransferQueue" href="#sglang.srt.disaggregation.decode.DecodeTransferQueue">DecodeTransferQueue</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.disaggregation.decode.DecodeTransferQueue.add" href="#sglang.srt.disaggregation.decode.DecodeTransferQueue.add">add</a></code></li>
<li><code><a title="sglang.srt.disaggregation.decode.DecodeTransferQueue.extend" href="#sglang.srt.disaggregation.decode.DecodeTransferQueue.extend">extend</a></code></li>
<li><code><a title="sglang.srt.disaggregation.decode.DecodeTransferQueue.pop_transferred" href="#sglang.srt.disaggregation.decode.DecodeTransferQueue.pop_transferred">pop_transferred</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.disaggregation.decode.SchedulerDisaggregationDecodeMixin" href="#sglang.srt.disaggregation.decode.SchedulerDisaggregationDecodeMixin">SchedulerDisaggregationDecodeMixin</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.disaggregation.decode.SchedulerDisaggregationDecodeMixin.event_loop_normal_disagg_decode" href="#sglang.srt.disaggregation.decode.SchedulerDisaggregationDecodeMixin.event_loop_normal_disagg_decode">event_loop_normal_disagg_decode</a></code></li>
<li><code><a title="sglang.srt.disaggregation.decode.SchedulerDisaggregationDecodeMixin.event_loop_overlap_disagg_decode" href="#sglang.srt.disaggregation.decode.SchedulerDisaggregationDecodeMixin.event_loop_overlap_disagg_decode">event_loop_overlap_disagg_decode</a></code></li>
<li><code><a title="sglang.srt.disaggregation.decode.SchedulerDisaggregationDecodeMixin.get_new_prebuilt_batch" href="#sglang.srt.disaggregation.decode.SchedulerDisaggregationDecodeMixin.get_new_prebuilt_batch">get_new_prebuilt_batch</a></code></li>
<li><code><a title="sglang.srt.disaggregation.decode.SchedulerDisaggregationDecodeMixin.get_next_disagg_decode_batch_to_run" href="#sglang.srt.disaggregation.decode.SchedulerDisaggregationDecodeMixin.get_next_disagg_decode_batch_to_run">get_next_disagg_decode_batch_to_run</a></code></li>
<li><code><a title="sglang.srt.disaggregation.decode.SchedulerDisaggregationDecodeMixin.process_decode_queue" href="#sglang.srt.disaggregation.decode.SchedulerDisaggregationDecodeMixin.process_decode_queue">process_decode_queue</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
