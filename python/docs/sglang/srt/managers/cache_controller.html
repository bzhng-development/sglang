<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.managers.cache_controller API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.managers.cache_controller</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.managers.cache_controller.CacheOperation"><code class="flex name class">
<span>class <span class="ident">CacheOperation</span></span>
<span>(</span><span>host_indices: torch.Tensor,<br>device_indices: torch.Tensor,<br>node_id: int,<br>priority: Optional[int] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CacheOperation:

    counter = 0

    def __init__(
        self,
        host_indices: torch.Tensor,
        device_indices: torch.Tensor,
        node_id: int,
        priority: Optional[int] = None,
    ):
        self.host_indices = host_indices
        self.device_indices = device_indices
        self.node_ids = [node_id]
        self.data = None

        self.id = CacheOperation.counter
        CacheOperation.counter += 1
        # default priority is the order of creation
        self.priority = priority if priority is not None else self.id

    def merge(self, other: &#34;CacheOperation&#34;) -&gt; None:
        # multiple operations can be merged into a single operation for batch processing
        self.host_indices = torch.cat([self.host_indices, other.host_indices])
        self.device_indices = torch.cat([self.device_indices, other.device_indices])
        self.priority = min(self.priority, other.priority)
        self.node_ids.extend(other.node_ids)

    def split(self, factor) -&gt; List[&#34;CacheOperation&#34;]:
        # split an operation into smaller operations to reduce the size of intermediate buffers
        if factor &lt;= 1:
            return [self]

        chunk_size = math.ceil(len(self.host_indices) / factor)
        split_ops = []
        for i in range(0, len(self.host_indices), chunk_size):
            split_ops.append(
                CacheOperation(
                    host_indices=self.host_indices[i : i + chunk_size],
                    device_indices=self.device_indices[i : i + chunk_size],
                    node_id=0,
                )
            )
        # Inherit the node_ids on the final chunk
        if split_ops:
            split_ops[-1].node_ids = self.node_ids

        return split_ops

    def __lt__(self, other: &#34;CacheOperation&#34;):
        return self.priority &lt; other.priority</code></pre>
</details>
<div class="desc"></div>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.managers.cache_controller.CacheOperation.counter"><code class="name">var <span class="ident">counter</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.managers.cache_controller.CacheOperation.merge"><code class="name flex">
<span>def <span class="ident">merge</span></span>(<span>self,<br>other: "'<a title="sglang.srt.managers.cache_controller.CacheOperation" href="#sglang.srt.managers.cache_controller.CacheOperation">CacheOperation</a>'") ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge(self, other: &#34;CacheOperation&#34;) -&gt; None:
    # multiple operations can be merged into a single operation for batch processing
    self.host_indices = torch.cat([self.host_indices, other.host_indices])
    self.device_indices = torch.cat([self.device_indices, other.device_indices])
    self.priority = min(self.priority, other.priority)
    self.node_ids.extend(other.node_ids)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.CacheOperation.split"><code class="name flex">
<span>def <span class="ident">split</span></span>(<span>self, factor) ‑> List[<a title="sglang.srt.managers.cache_controller.CacheOperation" href="#sglang.srt.managers.cache_controller.CacheOperation">CacheOperation</a>]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def split(self, factor) -&gt; List[&#34;CacheOperation&#34;]:
    # split an operation into smaller operations to reduce the size of intermediate buffers
    if factor &lt;= 1:
        return [self]

    chunk_size = math.ceil(len(self.host_indices) / factor)
    split_ops = []
    for i in range(0, len(self.host_indices), chunk_size):
        split_ops.append(
            CacheOperation(
                host_indices=self.host_indices[i : i + chunk_size],
                device_indices=self.device_indices[i : i + chunk_size],
                node_id=0,
            )
        )
    # Inherit the node_ids on the final chunk
    if split_ops:
        split_ops[-1].node_ids = self.node_ids

    return split_ops</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.managers.cache_controller.HiCacheController"><code class="flex name class">
<span>class <span class="ident">HiCacheController</span></span>
<span>(</span><span>token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,<br>mem_pool_host: HostKVCache,<br>page_size: int,<br>tp_group: torch.distributed.ProcessGroup,<br>load_cache_event: threading.Event = None,<br>write_policy: str = 'write_through_selective',<br>io_backend: str = '',<br>storage_backend: Optional[str] = None,<br>prefetch_threshold: int = 256,<br>model_name: Optional[str] = None,<br>storage_backend_extra_config: Optional[str] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HiCacheController:

    def __init__(
        self,
        token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,
        mem_pool_host: HostKVCache,
        page_size: int,
        tp_group: torch.distributed.ProcessGroup,
        load_cache_event: threading.Event = None,
        write_policy: str = &#34;write_through_selective&#34;,
        io_backend: str = &#34;&#34;,
        storage_backend: Optional[str] = None,
        prefetch_threshold: int = 256,
        model_name: Optional[str] = None,
        storage_backend_extra_config: Optional[str] = None,
    ):
        self.mem_pool_device_allocator = token_to_kv_pool_allocator
        self.mem_pool_device = token_to_kv_pool_allocator.get_kvcache()
        self.mem_pool_host = mem_pool_host
        self.write_policy = write_policy
        self.page_size = page_size
        self.io_backend = io_backend
        self.enable_storage = False

        if storage_backend is not None:
            self.storage_backend_type = storage_backend
            from sglang.srt.mem_cache.hicache_storage import get_hash_str

            self.get_hash_str = get_hash_str
            self.storage_config = self._generate_storage_config(
                model_name, storage_backend_extra_config
            )
            # for MLA models, only one rank needs to backup the KV cache
            self.backup_skip = (
                self.storage_config.is_mla_model
                # todo: load balancing
                and self.storage_config.tp_rank != 0
            )

            if storage_backend == &#34;file&#34;:
                from sglang.srt.mem_cache.hicache_storage import HiCacheFile

                self.storage_backend = HiCacheFile(self.storage_config)
            elif storage_backend == &#34;nixl&#34;:
                from sglang.srt.mem_cache.storage.nixl.hicache_nixl import HiCacheNixl

                self.storage_backend = HiCacheNixl()
            elif storage_backend == &#34;mooncake&#34;:
                from sglang.srt.mem_cache.storage.mooncake_store.mooncake_store import (
                    MooncakeStore,
                )

                self.storage_backend = MooncakeStore(self.storage_config)
                self.storage_backend.register_buffer(self.mem_pool_host.kv_buffer)
                assert self.mem_pool_host.layout == &#34;page_first&#34;
            elif storage_backend == &#34;hf3fs&#34;:
                from sglang.srt.mem_cache.storage.hf3fs.storage_hf3fs import (
                    HiCacheHF3FS,
                )

                if self.mem_pool_host.layout == &#34;page_first&#34;:
                    bytes_per_page = (
                        mem_pool_host.get_ksize_per_token() * mem_pool_host.page_size
                    )
                elif self.mem_pool_host.layout == &#34;layer_first&#34;:
                    bytes_per_page = (
                        mem_pool_host.get_size_per_token() * mem_pool_host.page_size
                    )
                dtype = mem_pool_host.dtype
                self.storage_backend = HiCacheHF3FS.from_env_config(
                    bytes_per_page, dtype, self.storage_config
                )
            else:
                raise NotImplementedError(
                    f&#34;Unsupported storage backend: {storage_backend}&#34;
                )

            self.enable_storage = True
            # todo: threshold policy for prefetching
            self.prefetch_threshold = max(prefetch_threshold, self.page_size)
            self.prefetch_capacity_limit = int(
                0.8 * (self.mem_pool_host.size - self.mem_pool_device.size)
            )
            # granularity of batch storage IO operations, in number of pages
            self.storage_batch_size = 128
            # tracking the number of tokens locked in prefetching, updated by the main scheduler thread
            self.prefetch_tokens_occupied = 0

            # create a new communication group for synchronizing storage operations across TP workers
            self.tp_world_size = torch.distributed.get_world_size(group=tp_group)
            if self.tp_world_size &gt; 1:
                group_ranks = torch.distributed.get_process_group_ranks(tp_group)
                self.prefetch_tp_group = torch.distributed.new_group(
                    group_ranks, backend=&#34;gloo&#34;
                )

        self.load_cache_event = load_cache_event
        self.layer_done_counter = LayerDoneCounter(self.mem_pool_device.layer_num)
        self.mem_pool_device.register_layer_transfer_counter(self.layer_done_counter)

        if write_policy not in [
            &#34;write_through&#34;,
            &#34;write_through_selective&#34;,
            &#34;write_back&#34;,
        ]:
            raise ValueError(f&#34;Invalid write policy: {write_policy}&#34;)

        self.write_queue = PriorityQueue()
        self.load_queue = PriorityQueue()

        self.ack_write_queue = Queue()
        self.ack_load_queue = Queue()

        self.stop_event = threading.Event()
        self.write_buffer = TransferBuffer(self.stop_event)
        self.load_buffer = TransferBuffer(
            self.stop_event, buffer_count=10, max_buffer_size=100
        )

        self.write_stream = torch.cuda.Stream()
        self.load_stream = torch.cuda.Stream()

        self.write_thread = threading.Thread(
            target=self.write_thread_func_direct, daemon=True
        )
        self.load_thread = threading.Thread(
            target=self.load_thread_func_layer_by_layer, daemon=True
        )

        self.write_thread.start()
        self.load_thread.start()

        if self.enable_storage:
            self.prefetch_thread = threading.Thread(
                target=self.prefetch_thread_func, daemon=True
            )
            self.backup_thread = threading.Thread(
                target=self.backup_thread_func, daemon=True
            )
            self.prefetch_queue = Queue()
            self.backup_queue = Queue()

            self.prefetch_revoke_queue = Queue()
            self.ack_backup_queue = Queue()
            self.host_mem_release_queue = Queue()

            self.prefetch_thread.start()
            self.backup_thread.start()

    def _generate_storage_config(
        self,
        model_name: Optional[str] = None,
        storage_backend_extra_config: Optional[str] = None,
    ):

        if is_dp_attention_enabled():
            self.tp_rank = get_attention_tp_rank()
            self.tp_size = get_attention_tp_size()
        else:
            self.tp_rank = get_tensor_model_parallel_rank()
            self.tp_size = get_tensor_model_parallel_world_size()

        # Currently, AscendMLAPagedTokenToKVPool is the subclass of MLATokenToKVPool.
        is_mla_backend = isinstance(self.mem_pool_device, MLATokenToKVPool)

        # Parse extra config JSON if provided
        extra_config = None
        if storage_backend_extra_config:
            try:
                import json

                extra_config = json.loads(storage_backend_extra_config)
            except Exception as e:
                logger.error(f&#34;Invalid backend extra config JSON: {e}&#34;)

        return HiCacheStorageConfig(
            tp_rank=self.tp_rank,
            tp_size=self.tp_size,
            is_mla_model=is_mla_backend,
            model_name=model_name,
            extra_config=extra_config,
        )

    def reset(self):
        self.stop_event.set()
        self.write_thread.join()
        self.load_thread.join()

        self.write_queue.queue.clear()
        self.load_queue.queue.clear()
        self.write_buffer.clear()
        self.load_buffer.clear()
        self.ack_write_queue.queue.clear()
        self.ack_load_queue.queue.clear()
        if self.enable_storage:
            self.prefetch_thread.join()
            self.backup_thread.join()
            self.prefetch_queue.queue.clear()
            self.backup_queue.queue.clear()
            self.prefetch_revoke_queue.queue.clear()
            self.ack_backup_queue.queue.clear()

        self.write_thread = threading.Thread(
            target=self.write_thread_func_direct, daemon=True
        )
        self.load_thread = threading.Thread(
            target=self.load_thread_func_layer_by_layer, daemon=True
        )
        self.stop_event.clear()
        self.write_thread.start()
        self.load_thread.start()

        if self.enable_storage:
            self.prefetch_thread = threading.Thread(
                target=self.prefetch_thread_func, daemon=True
            )
            self.backup_thread = threading.Thread(
                target=self.backup_thread_func, daemon=True
            )
            self.prefetch_thread.start()
            self.backup_thread.start()

    def write(
        self,
        device_indices: torch.Tensor,
        priority: Optional[int] = None,
        node_id: int = 0,
    ) -&gt; Optional[torch.Tensor]:
        &#34;&#34;&#34;
        Back up KV caches from device memory to host memory.
        &#34;&#34;&#34;
        host_indices = self.mem_pool_host.alloc(len(device_indices))
        if host_indices is None:
            return None
        self.mem_pool_host.protect_write(host_indices)
        torch.cuda.current_stream().synchronize()
        self.write_queue.put(
            CacheOperation(host_indices, device_indices, node_id, priority)
        )
        return host_indices

    def load(
        self,
        host_indices: torch.Tensor,
        priority: Optional[int] = None,
        node_id: int = 0,
    ) -&gt; Optional[torch.Tensor]:
        &#34;&#34;&#34;
        Load KV caches from host memory to device memory.
        &#34;&#34;&#34;
        device_indices = self.mem_pool_device_allocator.alloc(len(host_indices))
        if device_indices is None:
            return None
        self.mem_pool_host.protect_load(host_indices)
        # to ensure the device indices are ready before accessed by another CUDA stream
        torch.cuda.current_stream().synchronize()
        self.load_queue.put(
            CacheOperation(host_indices, device_indices, node_id, priority)
        )
        return device_indices

    def move_indices(self, host_indices, device_indices):
        # move indices to GPU if using kernels, to host if using direct indexing
        if self.io_backend == &#34;kernel&#34;:
            return host_indices.to(self.mem_pool_device.device), device_indices
        elif self.io_backend == &#34;direct&#34;:
            device_indices = device_indices.cpu()
            host_indices, idx = host_indices.sort()
            return host_indices, device_indices.index_select(0, idx)
        else:
            raise ValueError(f&#34;Unsupported io backend&#34;)

    def write_thread_func_direct(self):
        &#34;&#34;&#34;
        Directly write through KV caches to host memory without buffering.
        &#34;&#34;&#34;
        torch.cuda.set_stream(self.write_stream)
        while not self.stop_event.is_set():
            try:
                operation = self.write_queue.get(block=True, timeout=1)
                host_indices, device_indices = self.move_indices(
                    operation.host_indices, operation.device_indices
                )
                self.mem_pool_host.backup_from_device_all_layer(
                    self.mem_pool_device, host_indices, device_indices, self.io_backend
                )
                self.write_stream.synchronize()
                self.mem_pool_host.complete_io(operation.host_indices)
                for node_id in operation.node_ids:
                    if node_id != 0:
                        self.ack_write_queue.put(node_id)
            except Empty:
                continue
            except Exception as e:
                logger.error(e)

    def load_thread_func_layer_by_layer(self):
        &#34;&#34;&#34;
        Load KV caches from host memory to device memory layer by layer.
        &#34;&#34;&#34;
        torch.cuda.set_stream(self.load_stream)
        while not self.stop_event.is_set():
            self.load_cache_event.wait(timeout=1)
            if not self.load_cache_event.is_set():
                continue
            self.load_cache_event.clear()
            self.layer_done_counter.update_producer()

            batch_operation = None
            while self.load_queue.qsize() &gt; 0:
                op = self.load_queue.get(block=True)
                if batch_operation is None:
                    batch_operation = op
                else:
                    batch_operation.merge(op)
            if batch_operation is None:
                continue

            # start layer-wise KV cache transfer from CPU to GPU
            self.layer_done_counter.reset()
            host_indices, device_indices = self.move_indices(
                batch_operation.host_indices, batch_operation.device_indices
            )
            for i in range(self.mem_pool_host.layer_num):
                self.mem_pool_host.load_to_device_per_layer(
                    self.mem_pool_device,
                    host_indices,
                    device_indices,
                    i,
                    self.io_backend,
                )
                self.load_stream.synchronize()
                self.layer_done_counter.increment()

            self.mem_pool_host.complete_io(batch_operation.host_indices)
            for node_id in batch_operation.node_ids:
                if node_id != 0:
                    self.ack_load_queue.put(node_id)

    def evict_device(
        self, device_indices: torch.Tensor, host_indices: torch.Tensor
    ) -&gt; int:
        if self.mem_pool_host.is_synced(host_indices):
            self.mem_pool_device_allocator.free(device_indices)
            self.mem_pool_host.update_backup(host_indices)
            return len(device_indices)
        else:
            raise ValueError(
                f&#34;Inconsistent states: {self.mem_pool_host.get_state(host_indices)}&#34;
            )

    def evict_host(self, host_indices: torch.Tensor, backup_only: bool = True) -&gt; int:
        if not backup_only:
            raise ValueError(&#34;Other eviction policies are not supported yet.&#34;)

        if self.mem_pool_host.is_backup(host_indices):
            self.mem_pool_host.free(host_indices)
            return len(host_indices)
        else:
            raise ValueError(
                f&#34;Inconsistent states: {self.mem_pool_host.get_state(host_indices)}&#34;
            )

    def prefetch(
        self,
        request_id: str,
        host_indices: torch.Tensor,
        new_input_tokens: List[int],
        last_hash: Optional[str] = None,
    ) -&gt; PrefetchOperation:
        &#34;&#34;&#34;
        Prefetch KV caches from storage backend to host memory.
        &#34;&#34;&#34;
        operation = PrefetchOperation(
            request_id, host_indices, new_input_tokens, last_hash
        )
        self.prefetch_queue.put(operation)
        return operation

    def terminate_prefetch(self, operation):
        operation.mark_done()
        return operation.completed_tokens, operation.hash_value

    def append_host_mem_release(self, host_indices: torch.Tensor):
        chunks = host_indices.split(self.mem_pool_host.page_size)
        for chunk in chunks:
            self.host_mem_release_queue.put(chunk)

    def _3fs_zero_copy_page_get(self, operation, hash_values, host_indices):
        hashes, dsts = self.mem_pool_host.get_buffer_with_hash(
            hash_values, host_indices
        )
        page_data = self.storage_backend.batch_get(hashes, dsts)
        if page_data:
            operation.increment(self.page_size * len(hashes))
        else:
            logger.warning(
                f&#34;Prefetch operation {operation.request_id} failed to retrieve page {hashes}.&#34;
            )

    def _mooncake_page_get(self, operation, hash_values, host_indices):
        key_strs, buffer_ptrs, buffer_sizes = self.mem_pool_host.get_buffer_meta(
            hash_values,
            host_indices,
            self.storage_config.tp_rank,
        )
        get_result = self.storage_backend.batch_get(
            key_strs,
            target_location=buffer_ptrs,
            target_sizes=buffer_sizes,
        )
        if get_result != len(hash_values):
            logger.warning(
                f&#34;Prefetch operation {operation.request_id} failed or partially failed.&#34;
            )
        if get_result != 0:
            operation.increment(get_result * self.page_size)

    def _generic_page_get(self, operation, hash_values, host_indices):
        dummy_page_dst = [self.mem_pool_host.get_dummy_flat_data_page()] * len(
            hash_values
        )
        page_data = self.storage_backend.batch_get(hash_values, dummy_page_dst)
        if page_data is None:
            return
        for i in range(len(hash_values)):
            if page_data[i] is None:
                logger.warning(
                    f&#34;Prefetch operation {operation.request_id} failed to retrieve page {hash_values[i]}.&#34;
                )
                break
            if operation.increment(self.page_size):
                self.mem_pool_host.set_from_flat_data_page(
                    host_indices[i * self.page_size],
                    page_data[i],
                )
            else:
                break

    def _page_transfer(self, operation):
        # Select the get function and batch size
        if self.storage_backend_type == &#34;mooncake&#34;:
            get_func = self._mooncake_page_get
        elif (
            self.storage_backend_type == &#34;hf3fs&#34;
            and self.mem_pool_host.layout == &#34;page_first&#34;
        ):
            get_func = self._3fs_zero_copy_page_get
        else:
            get_func = self._generic_page_get

        # Transfer batch by batch
        for i in range(0, len(operation.hash_value), self.storage_batch_size):
            batch_hashes = operation.hash_value[i : i + self.storage_batch_size]
            batch_host_indices = operation.host_indices[
                i * self.page_size : (i + len(batch_hashes)) * self.page_size
            ]
            prev_completed_tokens = operation.completed_tokens
            # Get one batch token, and update the completed_tokens if succeed
            get_func(operation, batch_hashes, batch_host_indices)
            # Check termination
            if (
                operation.completed_tokens
                != prev_completed_tokens + len(batch_hashes) * self.page_size
            ):
                break  # Some operations fail or operation terminated by controller
        # release pre-allocated memory
        self.append_host_mem_release(
            operation.host_indices[operation.completed_tokens :]
        )

    def prefetch_io_aux_func(self):
        &#34;&#34;&#34;
        Auxiliary function conducting IO operations for prefetching.
        &#34;&#34;&#34;
        while not self.stop_event.is_set():
            try:
                operation = self.prefetch_buffer.get(block=True, timeout=1)
                self._page_transfer(operation)
                # operation terminated by controller, release pre-allocated memory
                self.append_host_mem_release(
                    operation.host_indices[operation.completed_tokens :]
                )
            except Empty:
                continue

    def prefetch_rate_limited(self) -&gt; bool:
        &#34;&#34;&#34;
        Rate limit the prefetching operations to avoid overwhelming the storage backend.
        &#34;&#34;&#34;
        # cancel prefetch if too much memory is occupied
        if self.prefetch_tokens_occupied &gt;= self.prefetch_capacity_limit:
            return True
        # todo: more sophisticated rate limiting based on storage backend performance
        return False

    def _storage_hit_query(self, operation) -&gt; tuple[list[str], int]:
        last_hash = operation.last_hash
        tokens_to_fetch = operation.token_ids

        storage_query_count = 0
        hash_value = []

        for start in range(
            0, len(tokens_to_fetch), self.page_size * self.storage_batch_size
        ):
            end = min(
                start + self.page_size * self.storage_batch_size, len(tokens_to_fetch)
            )
            batch_tokens = tokens_to_fetch[start:end]
            batch_hashes = []
            for i in range(0, len(batch_tokens), self.page_size):
                last_hash = self.get_hash_str(
                    batch_tokens[i : i + self.page_size], last_hash
                )
                batch_hashes.append(last_hash)
            hit_page_num = self.storage_backend.batch_exists(batch_hashes)
            hash_value.extend(batch_hashes[:hit_page_num])
            storage_query_count += hit_page_num * self.page_size
            if hit_page_num &lt; len(batch_hashes):
                break
        return hash_value, storage_query_count

    def prefetch_thread_func(self):
        &#34;&#34;&#34;
        Manage prefetching operations from storage backend to host memory.
        &#34;&#34;&#34;
        self.prefetch_buffer = Queue()
        aux_thread = threading.Thread(target=self.prefetch_io_aux_func, daemon=True)
        aux_thread.start()
        while (not self.stop_event.is_set()) or not self.prefetch_queue.empty():
            try:
                operation = self.prefetch_queue.get(block=True, timeout=1)
                if operation is None:
                    continue

                hash_value, storage_hit_count = self._storage_hit_query(operation)
                if self.tp_world_size &gt; 1:
                    storage_hit_count_tensor = torch.tensor(
                        storage_hit_count, dtype=torch.int
                    )
                    torch.distributed.all_reduce(
                        storage_hit_count_tensor,
                        op=torch.distributed.ReduceOp.MIN,
                        group=self.prefetch_tp_group,
                    )
                    storage_hit_count = storage_hit_count_tensor.item()

                if storage_hit_count &lt; self.prefetch_threshold:
                    # not to prefetch if not enough benefits
                    self.prefetch_revoke_queue.put(operation.request_id)
                    self.append_host_mem_release(operation.host_indices)
                    logger.debug(
                        f&#34;Revoking prefetch for request {operation.request_id} due to insufficient hits ({storage_hit_count}).&#34;
                    )
                else:
                    operation.hash_value = hash_value[
                        : (storage_hit_count // self.page_size)
                    ]
                    # free the pre-allocated memory for pages that are not hit
                    self.append_host_mem_release(
                        operation.host_indices[storage_hit_count:]
                    )
                    operation.host_indices = operation.host_indices[:storage_hit_count]
                    logger.debug(
                        f&#34;Prefetching {len(operation.hash_value)} pages for request {operation.request_id}.&#34;
                    )
                    self.prefetch_buffer.put(operation)

            except Empty:
                continue

    def write_storage(
        self,
        host_indices: torch.Tensor,
        token_ids: List[int],
        hash_value: Optional[List[str]] = None,
    ) -&gt; int:
        &#34;&#34;&#34;
        Write KV caches from host memory to storage backend.
        &#34;&#34;&#34;
        operation = StorageOperation(host_indices, token_ids, hash_value=hash_value)
        self.backup_queue.put(operation)
        return operation.id

    # non-zero copy
    def _generic_page_set(self, hash_values, host_indices) -&gt; bool:
        data = [
            self.mem_pool_host.get_flat_data_page(host_indices[i * self.page_size])
            for i in range(len(hash_values))
        ]
        return self.storage_backend.batch_set(hash_values, data)

    # zero copy
    def _mooncake_page_set(self, hash_values, host_indices) -&gt; bool:
        key_strs, buffer_ptrs, buffer_sizes = self.mem_pool_host.get_buffer_meta(
            hash_values,
            host_indices,
            self.storage_config.tp_rank,
        )
        success = self.storage_backend.batch_set(
            key_strs,
            target_location=buffer_ptrs,
            target_sizes=buffer_sizes,
        )
        return success

    # zero copy
    def _3fs_zero_copy_page_set(self, hash_values, host_indices) -&gt; bool:
        hashes, dsts = self.mem_pool_host.get_buffer_with_hash(
            hash_values, host_indices
        )
        return self.storage_backend.batch_set(hashes, dsts)

    # Backup batch by batch
    def _page_backup(self, operation):
        # Select the set function and batch size
        if self.storage_backend_type == &#34;mooncake&#34;:
            backup_set_func = self._mooncake_page_set
        elif (
            self.storage_backend_type == &#34;hf3fs&#34;
            and self.mem_pool_host.layout == &#34;page_first&#34;
        ):
            backup_set_func = self._3fs_zero_copy_page_set
        else:
            backup_set_func = self._generic_page_set
        # Backup batch by batch
        for i in range(0, len(operation.hash_value), self.storage_batch_size):
            batch_hashes = operation.hash_value[i : i + self.storage_batch_size]
            batch_host_indices = operation.host_indices[
                i * self.page_size : (i + len(batch_hashes)) * self.page_size
            ]
            # Set one batch token, and record if success.
            # todo: allow partial success
            success = backup_set_func(batch_hashes, batch_host_indices)
            if not success:
                logger.warning(
                    f&#34;Write page to storage: {len(batch_hashes)} pages failed.&#34;
                )
                break
            operation.completed_tokens += self.page_size * len(batch_hashes)

    def backup_thread_func(self):
        &#34;&#34;&#34;
        Manage backup operations from host memory to storage backend.
        &#34;&#34;&#34;
        while not self.stop_event.is_set():
            try:
                operation = self.backup_queue.get(block=True, timeout=1)
                if operation is None:
                    continue

                if not self.backup_skip:
                    self._page_backup(operation)
                self.ack_backup_queue.put(operation.id)

            except Empty:
                continue</code></pre>
</details>
<div class="desc"></div>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.managers.cache_controller.HiCacheController.append_host_mem_release"><code class="name flex">
<span>def <span class="ident">append_host_mem_release</span></span>(<span>self, host_indices: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append_host_mem_release(self, host_indices: torch.Tensor):
    chunks = host_indices.split(self.mem_pool_host.page_size)
    for chunk in chunks:
        self.host_mem_release_queue.put(chunk)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.HiCacheController.backup_thread_func"><code class="name flex">
<span>def <span class="ident">backup_thread_func</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backup_thread_func(self):
    &#34;&#34;&#34;
    Manage backup operations from host memory to storage backend.
    &#34;&#34;&#34;
    while not self.stop_event.is_set():
        try:
            operation = self.backup_queue.get(block=True, timeout=1)
            if operation is None:
                continue

            if not self.backup_skip:
                self._page_backup(operation)
            self.ack_backup_queue.put(operation.id)

        except Empty:
            continue</code></pre>
</details>
<div class="desc"><p>Manage backup operations from host memory to storage backend.</p></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.HiCacheController.evict_device"><code class="name flex">
<span>def <span class="ident">evict_device</span></span>(<span>self, device_indices: torch.Tensor, host_indices: torch.Tensor) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evict_device(
    self, device_indices: torch.Tensor, host_indices: torch.Tensor
) -&gt; int:
    if self.mem_pool_host.is_synced(host_indices):
        self.mem_pool_device_allocator.free(device_indices)
        self.mem_pool_host.update_backup(host_indices)
        return len(device_indices)
    else:
        raise ValueError(
            f&#34;Inconsistent states: {self.mem_pool_host.get_state(host_indices)}&#34;
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.HiCacheController.evict_host"><code class="name flex">
<span>def <span class="ident">evict_host</span></span>(<span>self, host_indices: torch.Tensor, backup_only: bool = True) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evict_host(self, host_indices: torch.Tensor, backup_only: bool = True) -&gt; int:
    if not backup_only:
        raise ValueError(&#34;Other eviction policies are not supported yet.&#34;)

    if self.mem_pool_host.is_backup(host_indices):
        self.mem_pool_host.free(host_indices)
        return len(host_indices)
    else:
        raise ValueError(
            f&#34;Inconsistent states: {self.mem_pool_host.get_state(host_indices)}&#34;
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.HiCacheController.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self,<br>host_indices: torch.Tensor,<br>priority: Optional[int] = None,<br>node_id: int = 0) ‑> torch.Tensor | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(
    self,
    host_indices: torch.Tensor,
    priority: Optional[int] = None,
    node_id: int = 0,
) -&gt; Optional[torch.Tensor]:
    &#34;&#34;&#34;
    Load KV caches from host memory to device memory.
    &#34;&#34;&#34;
    device_indices = self.mem_pool_device_allocator.alloc(len(host_indices))
    if device_indices is None:
        return None
    self.mem_pool_host.protect_load(host_indices)
    # to ensure the device indices are ready before accessed by another CUDA stream
    torch.cuda.current_stream().synchronize()
    self.load_queue.put(
        CacheOperation(host_indices, device_indices, node_id, priority)
    )
    return device_indices</code></pre>
</details>
<div class="desc"><p>Load KV caches from host memory to device memory.</p></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.HiCacheController.load_thread_func_layer_by_layer"><code class="name flex">
<span>def <span class="ident">load_thread_func_layer_by_layer</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_thread_func_layer_by_layer(self):
    &#34;&#34;&#34;
    Load KV caches from host memory to device memory layer by layer.
    &#34;&#34;&#34;
    torch.cuda.set_stream(self.load_stream)
    while not self.stop_event.is_set():
        self.load_cache_event.wait(timeout=1)
        if not self.load_cache_event.is_set():
            continue
        self.load_cache_event.clear()
        self.layer_done_counter.update_producer()

        batch_operation = None
        while self.load_queue.qsize() &gt; 0:
            op = self.load_queue.get(block=True)
            if batch_operation is None:
                batch_operation = op
            else:
                batch_operation.merge(op)
        if batch_operation is None:
            continue

        # start layer-wise KV cache transfer from CPU to GPU
        self.layer_done_counter.reset()
        host_indices, device_indices = self.move_indices(
            batch_operation.host_indices, batch_operation.device_indices
        )
        for i in range(self.mem_pool_host.layer_num):
            self.mem_pool_host.load_to_device_per_layer(
                self.mem_pool_device,
                host_indices,
                device_indices,
                i,
                self.io_backend,
            )
            self.load_stream.synchronize()
            self.layer_done_counter.increment()

        self.mem_pool_host.complete_io(batch_operation.host_indices)
        for node_id in batch_operation.node_ids:
            if node_id != 0:
                self.ack_load_queue.put(node_id)</code></pre>
</details>
<div class="desc"><p>Load KV caches from host memory to device memory layer by layer.</p></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.HiCacheController.move_indices"><code class="name flex">
<span>def <span class="ident">move_indices</span></span>(<span>self, host_indices, device_indices)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def move_indices(self, host_indices, device_indices):
    # move indices to GPU if using kernels, to host if using direct indexing
    if self.io_backend == &#34;kernel&#34;:
        return host_indices.to(self.mem_pool_device.device), device_indices
    elif self.io_backend == &#34;direct&#34;:
        device_indices = device_indices.cpu()
        host_indices, idx = host_indices.sort()
        return host_indices, device_indices.index_select(0, idx)
    else:
        raise ValueError(f&#34;Unsupported io backend&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.HiCacheController.prefetch"><code class="name flex">
<span>def <span class="ident">prefetch</span></span>(<span>self,<br>request_id: str,<br>host_indices: torch.Tensor,<br>new_input_tokens: List[int],<br>last_hash: Optional[str] = None) ‑> <a title="sglang.srt.managers.cache_controller.PrefetchOperation" href="#sglang.srt.managers.cache_controller.PrefetchOperation">PrefetchOperation</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prefetch(
    self,
    request_id: str,
    host_indices: torch.Tensor,
    new_input_tokens: List[int],
    last_hash: Optional[str] = None,
) -&gt; PrefetchOperation:
    &#34;&#34;&#34;
    Prefetch KV caches from storage backend to host memory.
    &#34;&#34;&#34;
    operation = PrefetchOperation(
        request_id, host_indices, new_input_tokens, last_hash
    )
    self.prefetch_queue.put(operation)
    return operation</code></pre>
</details>
<div class="desc"><p>Prefetch KV caches from storage backend to host memory.</p></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.HiCacheController.prefetch_io_aux_func"><code class="name flex">
<span>def <span class="ident">prefetch_io_aux_func</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prefetch_io_aux_func(self):
    &#34;&#34;&#34;
    Auxiliary function conducting IO operations for prefetching.
    &#34;&#34;&#34;
    while not self.stop_event.is_set():
        try:
            operation = self.prefetch_buffer.get(block=True, timeout=1)
            self._page_transfer(operation)
            # operation terminated by controller, release pre-allocated memory
            self.append_host_mem_release(
                operation.host_indices[operation.completed_tokens :]
            )
        except Empty:
            continue</code></pre>
</details>
<div class="desc"><p>Auxiliary function conducting IO operations for prefetching.</p></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.HiCacheController.prefetch_rate_limited"><code class="name flex">
<span>def <span class="ident">prefetch_rate_limited</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prefetch_rate_limited(self) -&gt; bool:
    &#34;&#34;&#34;
    Rate limit the prefetching operations to avoid overwhelming the storage backend.
    &#34;&#34;&#34;
    # cancel prefetch if too much memory is occupied
    if self.prefetch_tokens_occupied &gt;= self.prefetch_capacity_limit:
        return True
    # todo: more sophisticated rate limiting based on storage backend performance
    return False</code></pre>
</details>
<div class="desc"><p>Rate limit the prefetching operations to avoid overwhelming the storage backend.</p></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.HiCacheController.prefetch_thread_func"><code class="name flex">
<span>def <span class="ident">prefetch_thread_func</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prefetch_thread_func(self):
    &#34;&#34;&#34;
    Manage prefetching operations from storage backend to host memory.
    &#34;&#34;&#34;
    self.prefetch_buffer = Queue()
    aux_thread = threading.Thread(target=self.prefetch_io_aux_func, daemon=True)
    aux_thread.start()
    while (not self.stop_event.is_set()) or not self.prefetch_queue.empty():
        try:
            operation = self.prefetch_queue.get(block=True, timeout=1)
            if operation is None:
                continue

            hash_value, storage_hit_count = self._storage_hit_query(operation)
            if self.tp_world_size &gt; 1:
                storage_hit_count_tensor = torch.tensor(
                    storage_hit_count, dtype=torch.int
                )
                torch.distributed.all_reduce(
                    storage_hit_count_tensor,
                    op=torch.distributed.ReduceOp.MIN,
                    group=self.prefetch_tp_group,
                )
                storage_hit_count = storage_hit_count_tensor.item()

            if storage_hit_count &lt; self.prefetch_threshold:
                # not to prefetch if not enough benefits
                self.prefetch_revoke_queue.put(operation.request_id)
                self.append_host_mem_release(operation.host_indices)
                logger.debug(
                    f&#34;Revoking prefetch for request {operation.request_id} due to insufficient hits ({storage_hit_count}).&#34;
                )
            else:
                operation.hash_value = hash_value[
                    : (storage_hit_count // self.page_size)
                ]
                # free the pre-allocated memory for pages that are not hit
                self.append_host_mem_release(
                    operation.host_indices[storage_hit_count:]
                )
                operation.host_indices = operation.host_indices[:storage_hit_count]
                logger.debug(
                    f&#34;Prefetching {len(operation.hash_value)} pages for request {operation.request_id}.&#34;
                )
                self.prefetch_buffer.put(operation)

        except Empty:
            continue</code></pre>
</details>
<div class="desc"><p>Manage prefetching operations from storage backend to host memory.</p></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.HiCacheController.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    self.stop_event.set()
    self.write_thread.join()
    self.load_thread.join()

    self.write_queue.queue.clear()
    self.load_queue.queue.clear()
    self.write_buffer.clear()
    self.load_buffer.clear()
    self.ack_write_queue.queue.clear()
    self.ack_load_queue.queue.clear()
    if self.enable_storage:
        self.prefetch_thread.join()
        self.backup_thread.join()
        self.prefetch_queue.queue.clear()
        self.backup_queue.queue.clear()
        self.prefetch_revoke_queue.queue.clear()
        self.ack_backup_queue.queue.clear()

    self.write_thread = threading.Thread(
        target=self.write_thread_func_direct, daemon=True
    )
    self.load_thread = threading.Thread(
        target=self.load_thread_func_layer_by_layer, daemon=True
    )
    self.stop_event.clear()
    self.write_thread.start()
    self.load_thread.start()

    if self.enable_storage:
        self.prefetch_thread = threading.Thread(
            target=self.prefetch_thread_func, daemon=True
        )
        self.backup_thread = threading.Thread(
            target=self.backup_thread_func, daemon=True
        )
        self.prefetch_thread.start()
        self.backup_thread.start()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.HiCacheController.terminate_prefetch"><code class="name flex">
<span>def <span class="ident">terminate_prefetch</span></span>(<span>self, operation)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def terminate_prefetch(self, operation):
    operation.mark_done()
    return operation.completed_tokens, operation.hash_value</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.HiCacheController.write"><code class="name flex">
<span>def <span class="ident">write</span></span>(<span>self,<br>device_indices: torch.Tensor,<br>priority: Optional[int] = None,<br>node_id: int = 0) ‑> torch.Tensor | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write(
    self,
    device_indices: torch.Tensor,
    priority: Optional[int] = None,
    node_id: int = 0,
) -&gt; Optional[torch.Tensor]:
    &#34;&#34;&#34;
    Back up KV caches from device memory to host memory.
    &#34;&#34;&#34;
    host_indices = self.mem_pool_host.alloc(len(device_indices))
    if host_indices is None:
        return None
    self.mem_pool_host.protect_write(host_indices)
    torch.cuda.current_stream().synchronize()
    self.write_queue.put(
        CacheOperation(host_indices, device_indices, node_id, priority)
    )
    return host_indices</code></pre>
</details>
<div class="desc"><p>Back up KV caches from device memory to host memory.</p></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.HiCacheController.write_storage"><code class="name flex">
<span>def <span class="ident">write_storage</span></span>(<span>self,<br>host_indices: torch.Tensor,<br>token_ids: List[int],<br>hash_value: Optional[List[str]] = None) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_storage(
    self,
    host_indices: torch.Tensor,
    token_ids: List[int],
    hash_value: Optional[List[str]] = None,
) -&gt; int:
    &#34;&#34;&#34;
    Write KV caches from host memory to storage backend.
    &#34;&#34;&#34;
    operation = StorageOperation(host_indices, token_ids, hash_value=hash_value)
    self.backup_queue.put(operation)
    return operation.id</code></pre>
</details>
<div class="desc"><p>Write KV caches from host memory to storage backend.</p></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.HiCacheController.write_thread_func_direct"><code class="name flex">
<span>def <span class="ident">write_thread_func_direct</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_thread_func_direct(self):
    &#34;&#34;&#34;
    Directly write through KV caches to host memory without buffering.
    &#34;&#34;&#34;
    torch.cuda.set_stream(self.write_stream)
    while not self.stop_event.is_set():
        try:
            operation = self.write_queue.get(block=True, timeout=1)
            host_indices, device_indices = self.move_indices(
                operation.host_indices, operation.device_indices
            )
            self.mem_pool_host.backup_from_device_all_layer(
                self.mem_pool_device, host_indices, device_indices, self.io_backend
            )
            self.write_stream.synchronize()
            self.mem_pool_host.complete_io(operation.host_indices)
            for node_id in operation.node_ids:
                if node_id != 0:
                    self.ack_write_queue.put(node_id)
        except Empty:
            continue
        except Exception as e:
            logger.error(e)</code></pre>
</details>
<div class="desc"><p>Directly write through KV caches to host memory without buffering.</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.managers.cache_controller.LayerDoneCounter"><code class="flex name class">
<span>class <span class="ident">LayerDoneCounter</span></span>
<span>(</span><span>num_layers)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LayerDoneCounter:
    def __init__(self, num_layers):
        self.num_layers = num_layers
        # extra producer and consumer counters for overlap mode
        self.num_counters = 3
        self.counters = [num_layers] * self.num_counters
        self.conditions = [threading.Condition() for _ in range(self.num_counters)]
        self.producer_index = 0
        self.consumer_index = 0

    def next_producer(self):
        return (self.producer_index + 1) % self.num_counters

    def update_producer(self):
        self.producer_index = self.next_producer()
        return self.producer_index

    def set_consumer(self, index):
        self.consumer_index = index

    def increment(self):
        with self.conditions[self.producer_index]:
            self.counters[self.producer_index] += 1
            self.conditions[self.producer_index].notify_all()

    def wait_until(self, threshold):
        with self.conditions[self.consumer_index]:
            while self.counters[self.consumer_index] &lt;= threshold:
                self.conditions[self.consumer_index].wait()

    def reset(self):
        with self.conditions[self.producer_index]:
            self.counters[self.producer_index] = 0</code></pre>
</details>
<div class="desc"></div>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.managers.cache_controller.LayerDoneCounter.increment"><code class="name flex">
<span>def <span class="ident">increment</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def increment(self):
    with self.conditions[self.producer_index]:
        self.counters[self.producer_index] += 1
        self.conditions[self.producer_index].notify_all()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.LayerDoneCounter.next_producer"><code class="name flex">
<span>def <span class="ident">next_producer</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def next_producer(self):
    return (self.producer_index + 1) % self.num_counters</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.LayerDoneCounter.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    with self.conditions[self.producer_index]:
        self.counters[self.producer_index] = 0</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.LayerDoneCounter.set_consumer"><code class="name flex">
<span>def <span class="ident">set_consumer</span></span>(<span>self, index)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_consumer(self, index):
    self.consumer_index = index</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.LayerDoneCounter.update_producer"><code class="name flex">
<span>def <span class="ident">update_producer</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_producer(self):
    self.producer_index = self.next_producer()
    return self.producer_index</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.LayerDoneCounter.wait_until"><code class="name flex">
<span>def <span class="ident">wait_until</span></span>(<span>self, threshold)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wait_until(self, threshold):
    with self.conditions[self.consumer_index]:
        while self.counters[self.consumer_index] &lt;= threshold:
            self.conditions[self.consumer_index].wait()</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.managers.cache_controller.PrefetchOperation"><code class="flex name class">
<span>class <span class="ident">PrefetchOperation</span></span>
<span>(</span><span>request_id: str,<br>host_indices: torch.Tensor,<br>token_ids: List[int],<br>last_hash: Optional[str] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PrefetchOperation(StorageOperation):
    def __init__(
        self,
        request_id: str,
        host_indices: torch.Tensor,
        token_ids: List[int],
        last_hash: Optional[str] = None,
    ):
        self.request_id = request_id

        self._done_flag = False
        self._lock = threading.Lock()

        self.start_time = time.monotonic()

        super().__init__(host_indices, token_ids, last_hash)

    def increment(self, num_tokens: int):
        with self._lock:
            if self._done_flag:
                return False
            self.completed_tokens += num_tokens
            return True

    def mark_done(self):
        with self._lock:
            self._done_flag = True

    def is_done(self) -&gt; bool:
        return self._done_flag</code></pre>
</details>
<div class="desc"></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.managers.cache_controller.StorageOperation" href="#sglang.srt.managers.cache_controller.StorageOperation">StorageOperation</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.managers.cache_controller.PrefetchOperation.increment"><code class="name flex">
<span>def <span class="ident">increment</span></span>(<span>self, num_tokens: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def increment(self, num_tokens: int):
    with self._lock:
        if self._done_flag:
            return False
        self.completed_tokens += num_tokens
        return True</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.PrefetchOperation.is_done"><code class="name flex">
<span>def <span class="ident">is_done</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_done(self) -&gt; bool:
    return self._done_flag</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.PrefetchOperation.mark_done"><code class="name flex">
<span>def <span class="ident">mark_done</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mark_done(self):
    with self._lock:
        self._done_flag = True</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.managers.cache_controller.StorageOperation"><code class="flex name class">
<span>class <span class="ident">StorageOperation</span></span>
<span>(</span><span>host_indices: torch.Tensor,<br>token_ids: List[int],<br>last_hash: Optional[str] = None,<br>hash_value: Optional[List[str]] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StorageOperation:
    counter = 0

    def __init__(
        self,
        host_indices: torch.Tensor,
        token_ids: List[int],
        last_hash: Optional[str] = None,
        hash_value: Optional[List[str]] = None,
    ):
        self.host_indices = host_indices
        self.token_ids = token_ids
        self.last_hash = last_hash
        self.completed_tokens = 0
        self.hash_value = hash_value if hash_value is not None else []

        self.id = StorageOperation.counter
        StorageOperation.counter += 1

    def __lt__(self, other: &#34;StorageOperation&#34;):
        return self.id &lt; other.id</code></pre>
</details>
<div class="desc"></div>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.managers.cache_controller.PrefetchOperation" href="#sglang.srt.managers.cache_controller.PrefetchOperation">PrefetchOperation</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.managers.cache_controller.StorageOperation.counter"><code class="name">var <span class="ident">counter</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.managers.cache_controller.TransferBuffer"><code class="flex name class">
<span>class <span class="ident">TransferBuffer</span></span>
<span>(</span><span>stop_event, buffer_count: int = 3, max_buffer_size: int = 1024)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TransferBuffer:
    &#34;&#34;&#34;
    Overlapping buffer preparation and transfer operations to improve throughput.
    &#34;&#34;&#34;

    def __init__(
        self, stop_event, buffer_count: int = 3, max_buffer_size: int = 1024
    ) -&gt; None:
        self.stop_event = stop_event
        self.buffers = Queue(maxsize=buffer_count)
        # todo: adjust the buffer size based on throughput profile of the system
        self.max_buffer_size = max_buffer_size

    def full(self) -&gt; bool:
        return self.buffers.full()

    def empty(self) -&gt; bool:
        return self.buffers.empty()

    def put(self, item, block=True, timeout=1) -&gt; None:
        while not self.stop_event.is_set():
            try:
                self.buffers.put(item, block=block, timeout=timeout)
                break
            except Full:
                if not block:
                    break
                continue
            except Exception as e:
                logger.error(e)

    def get(self, block=True, timeout=1) -&gt; Optional[CacheOperation]:
        try:
            return self.buffers.get(block=block, timeout=timeout)
        except Empty:
            return None
        except Exception as e:
            logger.error(e)

    def clear(self):
        self.buffers.queue.clear()</code></pre>
</details>
<div class="desc"><p>Overlapping buffer preparation and transfer operations to improve throughput.</p></div>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.managers.cache_controller.TransferBuffer.clear"><code class="name flex">
<span>def <span class="ident">clear</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clear(self):
    self.buffers.queue.clear()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.TransferBuffer.empty"><code class="name flex">
<span>def <span class="ident">empty</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def empty(self) -&gt; bool:
    return self.buffers.empty()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.TransferBuffer.full"><code class="name flex">
<span>def <span class="ident">full</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def full(self) -&gt; bool:
    return self.buffers.full()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.TransferBuffer.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self, block=True, timeout=1) ‑> <a title="sglang.srt.managers.cache_controller.CacheOperation" href="#sglang.srt.managers.cache_controller.CacheOperation">CacheOperation</a> | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(self, block=True, timeout=1) -&gt; Optional[CacheOperation]:
    try:
        return self.buffers.get(block=block, timeout=timeout)
    except Empty:
        return None
    except Exception as e:
        logger.error(e)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.cache_controller.TransferBuffer.put"><code class="name flex">
<span>def <span class="ident">put</span></span>(<span>self, item, block=True, timeout=1) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def put(self, item, block=True, timeout=1) -&gt; None:
    while not self.stop_event.is_set():
        try:
            self.buffers.put(item, block=block, timeout=timeout)
            break
        except Full:
            if not block:
                break
            continue
        except Exception as e:
            logger.error(e)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.managers" href="index.html">sglang.srt.managers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.managers.cache_controller.CacheOperation" href="#sglang.srt.managers.cache_controller.CacheOperation">CacheOperation</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.cache_controller.CacheOperation.counter" href="#sglang.srt.managers.cache_controller.CacheOperation.counter">counter</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.CacheOperation.merge" href="#sglang.srt.managers.cache_controller.CacheOperation.merge">merge</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.CacheOperation.split" href="#sglang.srt.managers.cache_controller.CacheOperation.split">split</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.managers.cache_controller.HiCacheController" href="#sglang.srt.managers.cache_controller.HiCacheController">HiCacheController</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.cache_controller.HiCacheController.append_host_mem_release" href="#sglang.srt.managers.cache_controller.HiCacheController.append_host_mem_release">append_host_mem_release</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.HiCacheController.backup_thread_func" href="#sglang.srt.managers.cache_controller.HiCacheController.backup_thread_func">backup_thread_func</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.HiCacheController.evict_device" href="#sglang.srt.managers.cache_controller.HiCacheController.evict_device">evict_device</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.HiCacheController.evict_host" href="#sglang.srt.managers.cache_controller.HiCacheController.evict_host">evict_host</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.HiCacheController.load" href="#sglang.srt.managers.cache_controller.HiCacheController.load">load</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.HiCacheController.load_thread_func_layer_by_layer" href="#sglang.srt.managers.cache_controller.HiCacheController.load_thread_func_layer_by_layer">load_thread_func_layer_by_layer</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.HiCacheController.move_indices" href="#sglang.srt.managers.cache_controller.HiCacheController.move_indices">move_indices</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.HiCacheController.prefetch" href="#sglang.srt.managers.cache_controller.HiCacheController.prefetch">prefetch</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.HiCacheController.prefetch_io_aux_func" href="#sglang.srt.managers.cache_controller.HiCacheController.prefetch_io_aux_func">prefetch_io_aux_func</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.HiCacheController.prefetch_rate_limited" href="#sglang.srt.managers.cache_controller.HiCacheController.prefetch_rate_limited">prefetch_rate_limited</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.HiCacheController.prefetch_thread_func" href="#sglang.srt.managers.cache_controller.HiCacheController.prefetch_thread_func">prefetch_thread_func</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.HiCacheController.reset" href="#sglang.srt.managers.cache_controller.HiCacheController.reset">reset</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.HiCacheController.terminate_prefetch" href="#sglang.srt.managers.cache_controller.HiCacheController.terminate_prefetch">terminate_prefetch</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.HiCacheController.write" href="#sglang.srt.managers.cache_controller.HiCacheController.write">write</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.HiCacheController.write_storage" href="#sglang.srt.managers.cache_controller.HiCacheController.write_storage">write_storage</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.HiCacheController.write_thread_func_direct" href="#sglang.srt.managers.cache_controller.HiCacheController.write_thread_func_direct">write_thread_func_direct</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.managers.cache_controller.LayerDoneCounter" href="#sglang.srt.managers.cache_controller.LayerDoneCounter">LayerDoneCounter</a></code></h4>
<ul class="two-column">
<li><code><a title="sglang.srt.managers.cache_controller.LayerDoneCounter.increment" href="#sglang.srt.managers.cache_controller.LayerDoneCounter.increment">increment</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.LayerDoneCounter.next_producer" href="#sglang.srt.managers.cache_controller.LayerDoneCounter.next_producer">next_producer</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.LayerDoneCounter.reset" href="#sglang.srt.managers.cache_controller.LayerDoneCounter.reset">reset</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.LayerDoneCounter.set_consumer" href="#sglang.srt.managers.cache_controller.LayerDoneCounter.set_consumer">set_consumer</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.LayerDoneCounter.update_producer" href="#sglang.srt.managers.cache_controller.LayerDoneCounter.update_producer">update_producer</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.LayerDoneCounter.wait_until" href="#sglang.srt.managers.cache_controller.LayerDoneCounter.wait_until">wait_until</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.managers.cache_controller.PrefetchOperation" href="#sglang.srt.managers.cache_controller.PrefetchOperation">PrefetchOperation</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.cache_controller.PrefetchOperation.increment" href="#sglang.srt.managers.cache_controller.PrefetchOperation.increment">increment</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.PrefetchOperation.is_done" href="#sglang.srt.managers.cache_controller.PrefetchOperation.is_done">is_done</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.PrefetchOperation.mark_done" href="#sglang.srt.managers.cache_controller.PrefetchOperation.mark_done">mark_done</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.managers.cache_controller.StorageOperation" href="#sglang.srt.managers.cache_controller.StorageOperation">StorageOperation</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.cache_controller.StorageOperation.counter" href="#sglang.srt.managers.cache_controller.StorageOperation.counter">counter</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.managers.cache_controller.TransferBuffer" href="#sglang.srt.managers.cache_controller.TransferBuffer">TransferBuffer</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.cache_controller.TransferBuffer.clear" href="#sglang.srt.managers.cache_controller.TransferBuffer.clear">clear</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.TransferBuffer.empty" href="#sglang.srt.managers.cache_controller.TransferBuffer.empty">empty</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.TransferBuffer.full" href="#sglang.srt.managers.cache_controller.TransferBuffer.full">full</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.TransferBuffer.get" href="#sglang.srt.managers.cache_controller.TransferBuffer.get">get</a></code></li>
<li><code><a title="sglang.srt.managers.cache_controller.TransferBuffer.put" href="#sglang.srt.managers.cache_controller.TransferBuffer.put">put</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
