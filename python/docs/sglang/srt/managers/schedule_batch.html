<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.managers.schedule_batch API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.managers.schedule_batch</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.managers.schedule_batch.get_last_loc"><code class="name flex">
<span>def <span class="ident">get_last_loc</span></span>(<span>req_to_token: torch.Tensor,<br>req_pool_indices_tensor: torch.Tensor,<br>prefix_lens_tensor: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_last_loc(
    req_to_token: torch.Tensor,
    req_pool_indices_tensor: torch.Tensor,
    prefix_lens_tensor: torch.Tensor,
) -&gt; torch.Tensor:
    if (
        global_server_args_dict[&#34;attention_backend&#34;] != &#34;ascend&#34;
        and global_server_args_dict[&#34;attention_backend&#34;] != &#34;torch_native&#34;
    ):
        impl = get_last_loc_triton
    else:
        impl = get_last_loc_torch

    return impl(req_to_token, req_pool_indices_tensor, prefix_lens_tensor)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.get_last_loc_torch"><code class="name flex">
<span>def <span class="ident">get_last_loc_torch</span></span>(<span>req_to_token: torch.Tensor,<br>req_pool_indices_tensor: torch.Tensor,<br>prefix_lens_tensor: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_last_loc_torch(
    req_to_token: torch.Tensor,
    req_pool_indices_tensor: torch.Tensor,
    prefix_lens_tensor: torch.Tensor,
) -&gt; torch.Tensor:
    return torch.where(
        prefix_lens_tensor &gt; 0,
        req_to_token[req_pool_indices_tensor, prefix_lens_tensor - 1],
        torch.full_like(prefix_lens_tensor, -1),
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.get_last_loc_triton"><code class="name flex">
<span>def <span class="ident">get_last_loc_triton</span></span>(<span>req_to_token: torch.Tensor,<br>req_pool_indices_tensor: torch.Tensor,<br>prefix_lens_tensor: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_last_loc_triton(
    req_to_token: torch.Tensor,
    req_pool_indices_tensor: torch.Tensor,
    prefix_lens_tensor: torch.Tensor,
) -&gt; torch.Tensor:
    BLOCK_SIZE = 256
    num_tokens = prefix_lens_tensor.shape[0]
    result = torch.empty_like(prefix_lens_tensor)
    grid = (triton.cdiv(num_tokens, BLOCK_SIZE),)

    get_last_loc_kernel[grid](
        req_to_token,
        req_pool_indices_tensor,
        prefix_lens_tensor,
        result,
        num_tokens,
        req_to_token.stride(0),
        BLOCK_SIZE,
    )
    return result</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.managers.schedule_batch.BaseFinishReason"><code class="flex name class">
<span>class <span class="ident">BaseFinishReason</span></span>
<span>(</span><span>is_error: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseFinishReason:
    def __init__(self, is_error: bool = False):
        self.is_error = is_error

    def to_json(self):
        raise NotImplementedError()</code></pre>
</details>
<div class="desc"></div>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.managers.schedule_batch.FINISH_ABORT" href="#sglang.srt.managers.schedule_batch.FINISH_ABORT">FINISH_ABORT</a></li>
<li><a title="sglang.srt.managers.schedule_batch.FINISH_LENGTH" href="#sglang.srt.managers.schedule_batch.FINISH_LENGTH">FINISH_LENGTH</a></li>
<li><a title="sglang.srt.managers.schedule_batch.FINISH_MATCHED_STR" href="#sglang.srt.managers.schedule_batch.FINISH_MATCHED_STR">FINISH_MATCHED_STR</a></li>
<li><a title="sglang.srt.managers.schedule_batch.FINISH_MATCHED_TOKEN" href="#sglang.srt.managers.schedule_batch.FINISH_MATCHED_TOKEN">FINISH_MATCHED_TOKEN</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.managers.schedule_batch.BaseFinishReason.to_json"><code class="name flex">
<span>def <span class="ident">to_json</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_json(self):
    raise NotImplementedError()</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.managers.schedule_batch.FINISH_ABORT"><code class="flex name class">
<span>class <span class="ident">FINISH_ABORT</span></span>
<span>(</span><span>message=None, status_code=None, err_type=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FINISH_ABORT(BaseFinishReason):
    def __init__(self, message=None, status_code=None, err_type=None):
        super().__init__(is_error=True)
        self.message = message or &#34;Aborted&#34;
        self.status_code = status_code
        self.err_type = err_type

    def to_json(self):
        return {
            &#34;type&#34;: &#34;abort&#34;,
            &#34;message&#34;: self.message,
            &#34;status_code&#34;: self.status_code,
            &#34;err_type&#34;: self.err_type,
        }</code></pre>
</details>
<div class="desc"></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.managers.schedule_batch.BaseFinishReason" href="#sglang.srt.managers.schedule_batch.BaseFinishReason">BaseFinishReason</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.managers.schedule_batch.FINISH_ABORT.to_json"><code class="name flex">
<span>def <span class="ident">to_json</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_json(self):
    return {
        &#34;type&#34;: &#34;abort&#34;,
        &#34;message&#34;: self.message,
        &#34;status_code&#34;: self.status_code,
        &#34;err_type&#34;: self.err_type,
    }</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.managers.schedule_batch.FINISH_LENGTH"><code class="flex name class">
<span>class <span class="ident">FINISH_LENGTH</span></span>
<span>(</span><span>length: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FINISH_LENGTH(BaseFinishReason):
    def __init__(self, length: int):
        super().__init__()
        self.length = length

    def to_json(self):
        return {
            &#34;type&#34;: &#34;length&#34;,  # to match OpenAI API&#39;s return value
            &#34;length&#34;: self.length,
        }</code></pre>
</details>
<div class="desc"></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.managers.schedule_batch.BaseFinishReason" href="#sglang.srt.managers.schedule_batch.BaseFinishReason">BaseFinishReason</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.managers.schedule_batch.FINISH_LENGTH.to_json"><code class="name flex">
<span>def <span class="ident">to_json</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_json(self):
    return {
        &#34;type&#34;: &#34;length&#34;,  # to match OpenAI API&#39;s return value
        &#34;length&#34;: self.length,
    }</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.managers.schedule_batch.FINISH_MATCHED_STR"><code class="flex name class">
<span>class <span class="ident">FINISH_MATCHED_STR</span></span>
<span>(</span><span>matched: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FINISH_MATCHED_STR(BaseFinishReason):
    def __init__(self, matched: str):
        super().__init__()
        self.matched = matched

    def to_json(self):
        return {
            &#34;type&#34;: &#34;stop&#34;,  # to match OpenAI API&#39;s return value
            &#34;matched&#34;: self.matched,
        }</code></pre>
</details>
<div class="desc"></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.managers.schedule_batch.BaseFinishReason" href="#sglang.srt.managers.schedule_batch.BaseFinishReason">BaseFinishReason</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.managers.schedule_batch.FINISH_MATCHED_STR.to_json"><code class="name flex">
<span>def <span class="ident">to_json</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_json(self):
    return {
        &#34;type&#34;: &#34;stop&#34;,  # to match OpenAI API&#39;s return value
        &#34;matched&#34;: self.matched,
    }</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.managers.schedule_batch.FINISH_MATCHED_TOKEN"><code class="flex name class">
<span>class <span class="ident">FINISH_MATCHED_TOKEN</span></span>
<span>(</span><span>matched: Union[int, List[int]])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FINISH_MATCHED_TOKEN(BaseFinishReason):
    def __init__(self, matched: Union[int, List[int]]):
        super().__init__()
        self.matched = matched

    def to_json(self):
        return {
            &#34;type&#34;: &#34;stop&#34;,  # to match OpenAI API&#39;s return value
            &#34;matched&#34;: self.matched,
        }</code></pre>
</details>
<div class="desc"></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.managers.schedule_batch.BaseFinishReason" href="#sglang.srt.managers.schedule_batch.BaseFinishReason">BaseFinishReason</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.managers.schedule_batch.FINISH_MATCHED_TOKEN.to_json"><code class="name flex">
<span>def <span class="ident">to_json</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_json(self):
    return {
        &#34;type&#34;: &#34;stop&#34;,  # to match OpenAI API&#39;s return value
        &#34;matched&#34;: self.matched,
    }</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.managers.schedule_batch.Modality"><code class="flex name class">
<span>class <span class="ident">Modality</span></span>
<span>(</span><span>*args, **kwds)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Modality(Enum):
    IMAGE = auto()
    MULTI_IMAGES = auto()
    VIDEO = auto()
    AUDIO = auto()

    @staticmethod
    def from_str(modality_str: str):
        try:
            return Modality[modality_str.upper()]
        except KeyError:
            raise ValueError(
                f&#34;Invalid modality string: {modality_str}. Valid modalities are: {[m.name for m in Modality]}&#34;
            )

    @staticmethod
    def all():
        return [Modality.IMAGE, Modality.VIDEO, Modality.AUDIO]</code></pre>
</details>
<div class="desc"><p>Create a collection of name/value pairs.</p>
<p>Example enumeration:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; class Color(Enum):
...     RED = 1
...     BLUE = 2
...     GREEN = 3
</code></pre>
<p>Access them by:</p>
<ul>
<li>attribute access:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color.RED
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<ul>
<li>value lookup:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color(1)
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<ul>
<li>name lookup:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color['RED']
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<p>Enumerations can be iterated over, and know how many members they have:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; len(Color)
3
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; list(Color)
[&lt;Color.RED: 1&gt;, &lt;Color.BLUE: 2&gt;, &lt;Color.GREEN: 3&gt;]
</code></pre>
<p>Methods can be added to enumerations, and members can have their own
attributes &ndash; see the documentation for details.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.managers.schedule_batch.Modality.AUDIO"><code class="name">var <span class="ident">AUDIO</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.Modality.IMAGE"><code class="name">var <span class="ident">IMAGE</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.Modality.MULTI_IMAGES"><code class="name">var <span class="ident">MULTI_IMAGES</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.Modality.VIDEO"><code class="name">var <span class="ident">VIDEO</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.managers.schedule_batch.Modality.all"><code class="name flex">
<span>def <span class="ident">all</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def all():
    return [Modality.IMAGE, Modality.VIDEO, Modality.AUDIO]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.Modality.from_str"><code class="name flex">
<span>def <span class="ident">from_str</span></span>(<span>modality_str: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_str(modality_str: str):
    try:
        return Modality[modality_str.upper()]
    except KeyError:
        raise ValueError(
            f&#34;Invalid modality string: {modality_str}. Valid modalities are: {[m.name for m in Modality]}&#34;
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch"><code class="flex name class">
<span>class <span class="ident">ModelWorkerBatch</span></span>
<span>(</span><span>bid: int,<br>forward_mode: ForwardMode,<br>input_ids: torch.Tensor,<br>req_pool_indices: torch.Tensor,<br>seq_lens: torch.Tensor,<br>out_cache_loc: torch.Tensor,<br>seq_lens_cpu: Optional[torch.Tensor],<br>seq_lens_sum: int,<br>return_logprob: bool,<br>top_logprobs_nums: Optional[List[int]],<br>token_ids_logprobs: Optional[List[List[int]]],<br>global_num_tokens: Optional[List[int]],<br>global_num_tokens_for_logprob: Optional[List[int]],<br>is_extend_in_batch: bool,<br>can_run_dp_cuda_graph: bool,<br>tbo_split_seq_index: Optional[int],<br>global_forward_mode: Optional[ForwardMode],<br>extend_num_tokens: Optional[int],<br>extend_seq_lens: Optional[List[int]],<br>extend_prefix_lens: Optional[List[int]],<br>extend_logprob_start_lens: Optional[List[int]],<br>extend_input_logprob_token_ids: Optional[torch.Tensor],<br>multimodal_inputs: Optional[List[<a title="sglang.srt.managers.schedule_batch.MultimodalInputs" href="#sglang.srt.managers.schedule_batch.MultimodalInputs">MultimodalInputs</a>]],<br>encoder_cached: Optional[List[bool]],<br>encoder_lens: Optional[torch.Tensor],<br>encoder_lens_cpu: Optional[List[int]],<br>encoder_out_cache_loc: Optional[torch.Tensor],<br>lora_ids: Optional[List[str]],<br>sampling_info: SamplingBatchInfo,<br>orig_seq_lens: Optional[torch.Tensor] = None,<br>input_embeds: Optional[torch.Tensor] = None,<br>token_type_ids: Optional[torch.Tensor] = None,<br>spec_algorithm: SpeculativeAlgorithm = None,<br>spec_info: Optional[Union[EagleVerifyInput, EagleDraftInput]] = None,<br>capture_hidden_mode: CaptureHiddenMode = None,<br>hicache_consumer_index: int = 0,<br>launch_done: Optional[threading.Event] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclasses.dataclass
class ModelWorkerBatch:
    # The batch id
    bid: int
    # The forward mode
    forward_mode: ForwardMode
    # The input ids
    input_ids: torch.Tensor
    # The indices of requests in the req_to_token_pool
    req_pool_indices: torch.Tensor
    # The sequence length
    seq_lens: torch.Tensor
    # The indices of output tokens in the token_to_kv_pool_allocator
    out_cache_loc: torch.Tensor
    # The sequence length tensor on CPU
    seq_lens_cpu: Optional[torch.Tensor]
    seq_lens_sum: int

    # For logprob
    return_logprob: bool
    top_logprobs_nums: Optional[List[int]]
    token_ids_logprobs: Optional[List[List[int]]]

    # For DP attention
    global_num_tokens: Optional[List[int]]
    global_num_tokens_for_logprob: Optional[List[int]]
    is_extend_in_batch: bool
    can_run_dp_cuda_graph: bool
    tbo_split_seq_index: Optional[int]
    global_forward_mode: Optional[ForwardMode]

    # For extend
    extend_num_tokens: Optional[int]
    extend_seq_lens: Optional[List[int]]
    extend_prefix_lens: Optional[List[int]]
    extend_logprob_start_lens: Optional[List[int]]
    extend_input_logprob_token_ids: Optional[torch.Tensor]

    # For multimodal
    multimodal_inputs: Optional[List[MultimodalInputs]]

    # For encoder-decoder
    encoder_cached: Optional[List[bool]]
    encoder_lens: Optional[torch.Tensor]
    encoder_lens_cpu: Optional[List[int]]
    encoder_out_cache_loc: Optional[torch.Tensor]

    # For LoRA
    lora_ids: Optional[List[str]]

    # Sampling info
    sampling_info: SamplingBatchInfo

    # The original sequence lengths, Qwen-1M related
    orig_seq_lens: Optional[torch.Tensor] = None

    # The input Embeds
    input_embeds: Optional[torch.Tensor] = None

    # For corss-encoder model
    token_type_ids: Optional[torch.Tensor] = None

    # Speculative decoding
    spec_algorithm: SpeculativeAlgorithm = None
    spec_info: Optional[Union[EagleVerifyInput, EagleDraftInput]] = None
    # If set, the output of the batch contains the hidden states of the run.
    capture_hidden_mode: CaptureHiddenMode = None
    hicache_consumer_index: int = 0

    # Overlap event
    launch_done: Optional[threading.Event] = None</code></pre>
</details>
<div class="desc"><p>ModelWorkerBatch(bid: 'int', forward_mode: 'ForwardMode', input_ids: 'torch.Tensor', req_pool_indices: 'torch.Tensor', seq_lens: 'torch.Tensor', out_cache_loc: 'torch.Tensor', seq_lens_cpu: 'Optional[torch.Tensor]', seq_lens_sum: 'int', return_logprob: 'bool', top_logprobs_nums: 'Optional[List[int]]', token_ids_logprobs: 'Optional[List[List[int]]]', global_num_tokens: 'Optional[List[int]]', global_num_tokens_for_logprob: 'Optional[List[int]]', is_extend_in_batch: 'bool', can_run_dp_cuda_graph: 'bool', tbo_split_seq_index: 'Optional[int]', global_forward_mode: 'Optional[ForwardMode]', extend_num_tokens: 'Optional[int]', extend_seq_lens: 'Optional[List[int]]', extend_prefix_lens: 'Optional[List[int]]', extend_logprob_start_lens: 'Optional[List[int]]', extend_input_logprob_token_ids: 'Optional[torch.Tensor]', multimodal_inputs: 'Optional[List[MultimodalInputs]]', encoder_cached: 'Optional[List[bool]]', encoder_lens: 'Optional[torch.Tensor]', encoder_lens_cpu: 'Optional[List[int]]', encoder_out_cache_loc: 'Optional[torch.Tensor]', lora_ids: 'Optional[List[str]]', sampling_info: 'SamplingBatchInfo', orig_seq_lens: 'Optional[torch.Tensor]' = None, input_embeds: 'Optional[torch.Tensor]' = None, token_type_ids: 'Optional[torch.Tensor]' = None, spec_algorithm: 'SpeculativeAlgorithm' = None, spec_info: 'Optional[Union[EagleVerifyInput, EagleDraftInput]]' = None, capture_hidden_mode: 'CaptureHiddenMode' = None, hicache_consumer_index: 'int' = 0, launch_done: 'Optional[threading.Event]' = None)</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.bid"><code class="name">var <span class="ident">bid</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.can_run_dp_cuda_graph"><code class="name">var <span class="ident">can_run_dp_cuda_graph</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.capture_hidden_mode"><code class="name">var <span class="ident">capture_hidden_mode</span> : CaptureHiddenMode</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.encoder_cached"><code class="name">var <span class="ident">encoder_cached</span> : Optional[List[bool]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.encoder_lens"><code class="name">var <span class="ident">encoder_lens</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.encoder_lens_cpu"><code class="name">var <span class="ident">encoder_lens_cpu</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.encoder_out_cache_loc"><code class="name">var <span class="ident">encoder_out_cache_loc</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.extend_input_logprob_token_ids"><code class="name">var <span class="ident">extend_input_logprob_token_ids</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.extend_logprob_start_lens"><code class="name">var <span class="ident">extend_logprob_start_lens</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.extend_num_tokens"><code class="name">var <span class="ident">extend_num_tokens</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.extend_prefix_lens"><code class="name">var <span class="ident">extend_prefix_lens</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.extend_seq_lens"><code class="name">var <span class="ident">extend_seq_lens</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.forward_mode"><code class="name">var <span class="ident">forward_mode</span> : ForwardMode</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.global_forward_mode"><code class="name">var <span class="ident">global_forward_mode</span> : Optional[ForwardMode]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.global_num_tokens"><code class="name">var <span class="ident">global_num_tokens</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.global_num_tokens_for_logprob"><code class="name">var <span class="ident">global_num_tokens_for_logprob</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.hicache_consumer_index"><code class="name">var <span class="ident">hicache_consumer_index</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.input_embeds"><code class="name">var <span class="ident">input_embeds</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.input_ids"><code class="name">var <span class="ident">input_ids</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.is_extend_in_batch"><code class="name">var <span class="ident">is_extend_in_batch</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.launch_done"><code class="name">var <span class="ident">launch_done</span> : Optional[threading.Event]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.lora_ids"><code class="name">var <span class="ident">lora_ids</span> : Optional[List[str]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.multimodal_inputs"><code class="name">var <span class="ident">multimodal_inputs</span> : Optional[List[<a title="sglang.srt.managers.schedule_batch.MultimodalInputs" href="#sglang.srt.managers.schedule_batch.MultimodalInputs">MultimodalInputs</a>]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.orig_seq_lens"><code class="name">var <span class="ident">orig_seq_lens</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.out_cache_loc"><code class="name">var <span class="ident">out_cache_loc</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.req_pool_indices"><code class="name">var <span class="ident">req_pool_indices</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.return_logprob"><code class="name">var <span class="ident">return_logprob</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.sampling_info"><code class="name">var <span class="ident">sampling_info</span> : SamplingBatchInfo</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.seq_lens"><code class="name">var <span class="ident">seq_lens</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.seq_lens_cpu"><code class="name">var <span class="ident">seq_lens_cpu</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.seq_lens_sum"><code class="name">var <span class="ident">seq_lens_sum</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.spec_algorithm"><code class="name">var <span class="ident">spec_algorithm</span> : SpeculativeAlgorithm</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.spec_info"><code class="name">var <span class="ident">spec_info</span> : Optional[Union[EagleVerifyInput, EagleDraftInput]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.tbo_split_seq_index"><code class="name">var <span class="ident">tbo_split_seq_index</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.token_ids_logprobs"><code class="name">var <span class="ident">token_ids_logprobs</span> : Optional[List[List[int]]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.token_type_ids"><code class="name">var <span class="ident">token_type_ids</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ModelWorkerBatch.top_logprobs_nums"><code class="name">var <span class="ident">top_logprobs_nums</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalDataItem"><code class="flex name class">
<span>class <span class="ident">MultimodalDataItem</span></span>
<span>(</span><span>modality: <a title="sglang.srt.managers.schedule_batch.Modality" href="#sglang.srt.managers.schedule_batch.Modality">Modality</a>,<br>hash: int = None,<br>pad_value: int = None,<br>offsets: Optional[list] = None,<br>feature: Union[torch.Tensor, np.ndarray] = None,<br>precomputed_embeddings: Optional[Union[torch.Tensor, np.ndarray]] = None,<br>model_specific_data: dict[str, Any] = &lt;factory&gt;)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclasses.dataclass
class MultimodalDataItem:
    &#34;&#34;&#34;
    One MultimodalDataItem contains all inputs for one modality.
    For example, if there are 3 images and 1 audio inputs, there will be 2 MultimodalDataItem.
    One for images and one for audio.

    We put the common fields first and the model-specific fields in model_specific_data.
    &#34;&#34;&#34;

    modality: Modality
    hash: int = None
    pad_value: int = None
    offsets: Optional[list] = None

    # the raw features returned by processor, e.g. pixel_values or audio_features
    feature: Union[torch.Tensor, np.ndarray] = None
    # the precomputed embeddings, passed as final encoder embeddings
    # One and only one of the feature and precomputed_embeddings will be empty
    precomputed_embeddings: Optional[Union[torch.Tensor, np.ndarray]] = None

    # Model-specific data stored in a dictionary
    model_specific_data: dict[str, Any] = dataclasses.field(default_factory=dict)

    def __getattr__(self, name: str):
        if (
            &#34;model_specific_data&#34; in self.__dict__
            and name in self.__dict__[&#34;model_specific_data&#34;]
        ):
            return self.__dict__[&#34;model_specific_data&#34;][name]
        else:
            raise AttributeError(
                f&#34;&#39;{self.__class__.__name__}&#39; object has no attribute &#39;{name}&#39;&#34;
            )

    def __setitem__(self, key: str, value: Any):
        if key in self.__dict__:
            self.__dict__[key] = value
        else:
            self.model_specific_data[key] = value

    def set(self, key: str, value: Any):
        self.__setitem__(key, value)

    @staticmethod
    def is_empty_list(l):
        if l is None:
            return True
        return len([item for item in flatten_nested_list(l) if item is not None]) == 0

    def set_pad_value(self):
        &#34;&#34;&#34;
        Set the pad value after first hashing the data
        &#34;&#34;&#34;
        from sglang.srt.managers.mm_utils import hash_feature

        if self.hash is None:
            if self.feature is not None:
                hashed_feature = self.feature
            else:
                hashed_feature = self.precomputed_embeddings
            self.hash = hash_feature(hashed_feature)
        assert self.hash is not None
        self.pad_value = self.hash % (1 &lt;&lt; 30)

    def is_modality(self, modality: Modality) -&gt; bool:
        return self.modality == modality

    def is_audio(self):
        return self.modality == Modality.AUDIO

    def is_image(self):
        return self.modality in [Modality.IMAGE, Modality.MULTI_IMAGES]

    def is_video(self):
        return self.modality == Modality.VIDEO

    def is_valid(self) -&gt; bool:
        return self.is_image() or self.is_video() or self.is_audio()

    def validate(self):
        ...
        # TODO

    @staticmethod
    def from_dict(obj: dict):
        kwargs = dict(obj)
        modality = kwargs.pop(&#34;modality&#34;)
        if isinstance(modality, str):
            modality = Modality[modality]
        ret = MultimodalDataItem(modality=modality, **kwargs)
        ret.validate()
        return ret

    def merge(self, other):
        self.feature += other.feature
        self.offsets += other.offsets
        self.hash = hash((self.hash, other.hash))
        self.set_pad_value()</code></pre>
</details>
<div class="desc"><p>One MultimodalDataItem contains all inputs for one modality.
For example, if there are 3 images and 1 audio inputs, there will be 2 MultimodalDataItem.
One for images and one for audio.</p>
<p>We put the common fields first and the model-specific fields in model_specific_data.</p></div>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.managers.schedule_batch.MultimodalDataItem.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>obj: dict)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_dict(obj: dict):
    kwargs = dict(obj)
    modality = kwargs.pop(&#34;modality&#34;)
    if isinstance(modality, str):
        modality = Modality[modality]
    ret = MultimodalDataItem(modality=modality, **kwargs)
    ret.validate()
    return ret</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalDataItem.is_empty_list"><code class="name flex">
<span>def <span class="ident">is_empty_list</span></span>(<span>l)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def is_empty_list(l):
    if l is None:
        return True
    return len([item for item in flatten_nested_list(l) if item is not None]) == 0</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.managers.schedule_batch.MultimodalDataItem.feature"><code class="name">var <span class="ident">feature</span> : torch.Tensor | numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalDataItem.hash"><code class="name">var <span class="ident">hash</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalDataItem.modality"><code class="name">var <span class="ident">modality</span> : <a title="sglang.srt.managers.schedule_batch.Modality" href="#sglang.srt.managers.schedule_batch.Modality">Modality</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalDataItem.model_specific_data"><code class="name">var <span class="ident">model_specific_data</span> : dict[str, typing.Any]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalDataItem.offsets"><code class="name">var <span class="ident">offsets</span> : list | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalDataItem.pad_value"><code class="name">var <span class="ident">pad_value</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalDataItem.precomputed_embeddings"><code class="name">var <span class="ident">precomputed_embeddings</span> : torch.Tensor | numpy.ndarray | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.managers.schedule_batch.MultimodalDataItem.is_audio"><code class="name flex">
<span>def <span class="ident">is_audio</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_audio(self):
    return self.modality == Modality.AUDIO</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalDataItem.is_image"><code class="name flex">
<span>def <span class="ident">is_image</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_image(self):
    return self.modality in [Modality.IMAGE, Modality.MULTI_IMAGES]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalDataItem.is_modality"><code class="name flex">
<span>def <span class="ident">is_modality</span></span>(<span>self,<br>modality: <a title="sglang.srt.managers.schedule_batch.Modality" href="#sglang.srt.managers.schedule_batch.Modality">Modality</a>) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_modality(self, modality: Modality) -&gt; bool:
    return self.modality == modality</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalDataItem.is_valid"><code class="name flex">
<span>def <span class="ident">is_valid</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_valid(self) -&gt; bool:
    return self.is_image() or self.is_video() or self.is_audio()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalDataItem.is_video"><code class="name flex">
<span>def <span class="ident">is_video</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_video(self):
    return self.modality == Modality.VIDEO</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalDataItem.merge"><code class="name flex">
<span>def <span class="ident">merge</span></span>(<span>self, other)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge(self, other):
    self.feature += other.feature
    self.offsets += other.offsets
    self.hash = hash((self.hash, other.hash))
    self.set_pad_value()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalDataItem.set"><code class="name flex">
<span>def <span class="ident">set</span></span>(<span>self, key: str, value: Any)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set(self, key: str, value: Any):
    self.__setitem__(key, value)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalDataItem.set_pad_value"><code class="name flex">
<span>def <span class="ident">set_pad_value</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_pad_value(self):
    &#34;&#34;&#34;
    Set the pad value after first hashing the data
    &#34;&#34;&#34;
    from sglang.srt.managers.mm_utils import hash_feature

    if self.hash is None:
        if self.feature is not None:
            hashed_feature = self.feature
        else:
            hashed_feature = self.precomputed_embeddings
        self.hash = hash_feature(hashed_feature)
    assert self.hash is not None
    self.pad_value = self.hash % (1 &lt;&lt; 30)</code></pre>
</details>
<div class="desc"><p>Set the pad value after first hashing the data</p></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalDataItem.validate"><code class="name flex">
<span>def <span class="ident">validate</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate(self):
    ...
    # TODO</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs"><code class="flex name class">
<span>class <span class="ident">MultimodalInputs</span></span>
<span>(</span><span>mm_items: List[<a title="sglang.srt.managers.schedule_batch.MultimodalDataItem" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem">MultimodalDataItem</a>],<br>image_pad_len: Optional[list] = None,<br>num_image_tokens: Optional[int] = None,<br>im_token_id: Optional[int] = None,<br>im_start_id: Optional[int] = None,<br>im_end_id: Optional[int] = None,<br>slice_start_id: Optional[int] = None,<br>slice_end_id: Optional[int] = None,<br>video_token_id: Optional[int] = None,<br>audio_token_id: Optional[int] = None,<br>audio_start_id: Optional[int] = None,<br>audio_end_id: Optional[int] = None,<br>mrope_positions: Optional[torch.Tensor] = None,<br>mrope_position_delta: Optional[torch.Tensor] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclasses.dataclass
class MultimodalInputs:
    &#34;&#34;&#34;The multimodal data related inputs.&#34;&#34;&#34;

    # items of data
    mm_items: List[MultimodalDataItem]
    image_pad_len: Optional[list] = None
    num_image_tokens: Optional[int] = None

    # image
    im_token_id: Optional[int] = None
    im_start_id: Optional[int] = None
    im_end_id: Optional[int] = None
    slice_start_id: Optional[int] = None
    slice_end_id: Optional[int] = None

    # video
    video_token_id: Optional[int] = None

    # audio
    audio_token_id: Optional[int] = None
    audio_start_id: Optional[int] = None
    audio_end_id: Optional[int] = None

    # QWen2-VL related
    mrope_positions: Optional[torch.Tensor] = None
    mrope_position_delta: Optional[torch.Tensor] = None

    @staticmethod
    def from_dict(obj: dict):
        ret = MultimodalInputs(
            mm_items=obj[&#34;mm_items&#34;],
        )

        assert isinstance(ret.mm_items, list)
        ret.mm_items = [item for item in ret.mm_items if item.is_valid()]
        for item in ret.mm_items:
            item.set_pad_value()

        optional_args = [
            &#34;mrope_positions&#34;,
            &#34;mrope_position_delta&#34;,
            &#34;im_token_id&#34;,
            &#34;im_start_id&#34;,
            &#34;im_end_id&#34;,
            &#34;video_token_id&#34;,
            &#34;slice_start_id&#34;,
            &#34;slice_end_id&#34;,
            &#34;audio_start_id&#34;,
            &#34;audio_end_id&#34;,
            &#34;audio_token_id&#34;,
        ]
        for arg in optional_args:
            if arg in obj:
                setattr(ret, arg, obj[arg])

        return ret

    def contains_image_inputs(self) -&gt; bool:
        return any(item.is_image() for item in self.mm_items)

    def contains_video_inputs(self) -&gt; bool:
        return any(item.is_video() for item in self.mm_items)

    def contains_audio_inputs(self) -&gt; bool:
        return any(item.is_audio() for item in self.mm_items)

    def contains_mm_input(self) -&gt; bool:
        return any(True for item in self.mm_items if item.is_valid())

    def merge(self, other: MultimodalInputs):
        &#34;&#34;&#34;
        merge image inputs when requests are being merged
        &#34;&#34;&#34;

        # args needed to be merged
        optional_args = [
            &#34;mm_items&#34;,
            &#34;image_pad_len&#34;,
        ]
        for arg in optional_args:
            self_arg = getattr(self, arg, None)
            if self_arg is not None:
                setattr(self, arg, self_arg + getattr(other, arg))

        mrope_positions = self.mrope_positions
        if mrope_positions is not None:
            if other.mrope_positions is None:
                self.mrope_positions = mrope_positions
            else:
                self.mrope_positions = torch.cat(
                    [self.mrope_positions, other.mrope_positions], dim=1
                )

        mrope_position_delta = self.mrope_position_delta
        if mrope_position_delta is not None:
            if other.mrope_position_delta is None:
                self.mrope_position_delta = mrope_position_delta
            else:
                self.mrope_position_delta = torch.cat(
                    [self.mrope_position_delta, other.mrope_position_delta], dim=0
                )

        for key, val in other.__dict__.items():
            if &#34;_id&#34; in key:
                # set token_ids
                if getattr(self, key, None) is None:
                    setattr(self, key, getattr(other, key, None))
        # other args would be kept intact</code></pre>
</details>
<div class="desc"><p>The multimodal data related inputs.</p></div>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>obj: dict)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_dict(obj: dict):
    ret = MultimodalInputs(
        mm_items=obj[&#34;mm_items&#34;],
    )

    assert isinstance(ret.mm_items, list)
    ret.mm_items = [item for item in ret.mm_items if item.is_valid()]
    for item in ret.mm_items:
        item.set_pad_value()

    optional_args = [
        &#34;mrope_positions&#34;,
        &#34;mrope_position_delta&#34;,
        &#34;im_token_id&#34;,
        &#34;im_start_id&#34;,
        &#34;im_end_id&#34;,
        &#34;video_token_id&#34;,
        &#34;slice_start_id&#34;,
        &#34;slice_end_id&#34;,
        &#34;audio_start_id&#34;,
        &#34;audio_end_id&#34;,
        &#34;audio_token_id&#34;,
    ]
    for arg in optional_args:
        if arg in obj:
            setattr(ret, arg, obj[arg])

    return ret</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.audio_end_id"><code class="name">var <span class="ident">audio_end_id</span> : int | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.audio_start_id"><code class="name">var <span class="ident">audio_start_id</span> : int | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.audio_token_id"><code class="name">var <span class="ident">audio_token_id</span> : int | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.im_end_id"><code class="name">var <span class="ident">im_end_id</span> : int | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.im_start_id"><code class="name">var <span class="ident">im_start_id</span> : int | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.im_token_id"><code class="name">var <span class="ident">im_token_id</span> : int | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.image_pad_len"><code class="name">var <span class="ident">image_pad_len</span> : list | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.mm_items"><code class="name">var <span class="ident">mm_items</span> : List[<a title="sglang.srt.managers.schedule_batch.MultimodalDataItem" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem">MultimodalDataItem</a>]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.mrope_position_delta"><code class="name">var <span class="ident">mrope_position_delta</span> : torch.Tensor | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.mrope_positions"><code class="name">var <span class="ident">mrope_positions</span> : torch.Tensor | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.num_image_tokens"><code class="name">var <span class="ident">num_image_tokens</span> : int | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.slice_end_id"><code class="name">var <span class="ident">slice_end_id</span> : int | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.slice_start_id"><code class="name">var <span class="ident">slice_start_id</span> : int | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.video_token_id"><code class="name">var <span class="ident">video_token_id</span> : int | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.contains_audio_inputs"><code class="name flex">
<span>def <span class="ident">contains_audio_inputs</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def contains_audio_inputs(self) -&gt; bool:
    return any(item.is_audio() for item in self.mm_items)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.contains_image_inputs"><code class="name flex">
<span>def <span class="ident">contains_image_inputs</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def contains_image_inputs(self) -&gt; bool:
    return any(item.is_image() for item in self.mm_items)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.contains_mm_input"><code class="name flex">
<span>def <span class="ident">contains_mm_input</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def contains_mm_input(self) -&gt; bool:
    return any(True for item in self.mm_items if item.is_valid())</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.contains_video_inputs"><code class="name flex">
<span>def <span class="ident">contains_video_inputs</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def contains_video_inputs(self) -&gt; bool:
    return any(item.is_video() for item in self.mm_items)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.MultimodalInputs.merge"><code class="name flex">
<span>def <span class="ident">merge</span></span>(<span>self,<br>other: <a title="sglang.srt.managers.schedule_batch.MultimodalInputs" href="#sglang.srt.managers.schedule_batch.MultimodalInputs">MultimodalInputs</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge(self, other: MultimodalInputs):
    &#34;&#34;&#34;
    merge image inputs when requests are being merged
    &#34;&#34;&#34;

    # args needed to be merged
    optional_args = [
        &#34;mm_items&#34;,
        &#34;image_pad_len&#34;,
    ]
    for arg in optional_args:
        self_arg = getattr(self, arg, None)
        if self_arg is not None:
            setattr(self, arg, self_arg + getattr(other, arg))

    mrope_positions = self.mrope_positions
    if mrope_positions is not None:
        if other.mrope_positions is None:
            self.mrope_positions = mrope_positions
        else:
            self.mrope_positions = torch.cat(
                [self.mrope_positions, other.mrope_positions], dim=1
            )

    mrope_position_delta = self.mrope_position_delta
    if mrope_position_delta is not None:
        if other.mrope_position_delta is None:
            self.mrope_position_delta = mrope_position_delta
        else:
            self.mrope_position_delta = torch.cat(
                [self.mrope_position_delta, other.mrope_position_delta], dim=0
            )

    for key, val in other.__dict__.items():
        if &#34;_id&#34; in key:
            # set token_ids
            if getattr(self, key, None) is None:
                setattr(self, key, getattr(other, key, None))
    # other args would be kept intact</code></pre>
</details>
<div class="desc"><p>merge image inputs when requests are being merged</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.managers.schedule_batch.Req"><code class="flex name class">
<span>class <span class="ident">Req</span></span>
<span>(</span><span>rid: str,<br>origin_input_text: str,<br>origin_input_ids: List[int],<br>sampling_params: SamplingParams,<br>return_logprob: bool = False,<br>top_logprobs_num: int = 0,<br>token_ids_logprob: List[int] = None,<br>stream: bool = False,<br>origin_input_ids_unpadded: Optional[Tuple[int]] = None,<br>lora_id: Optional[str] = None,<br>input_embeds: Optional[List[List[float]]] = None,<br>token_type_ids: List[int] = None,<br>session_id: Optional[str] = None,<br>custom_logit_processor: Optional[str] = None,<br>return_hidden_states: bool = False,<br>eos_token_ids: Optional[Set[int]] = None,<br>bootstrap_host: Optional[str] = None,<br>bootstrap_port: Optional[int] = None,<br>bootstrap_room: Optional[int] = None,<br>data_parallel_rank: Optional[int] = None,<br>vocab_size: Optional[int] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Req:
    &#34;&#34;&#34;The input and output status of a request.&#34;&#34;&#34;

    def __init__(
        self,
        rid: str,
        origin_input_text: str,
        origin_input_ids: List[int],
        sampling_params: SamplingParams,
        return_logprob: bool = False,
        top_logprobs_num: int = 0,
        token_ids_logprob: List[int] = None,
        stream: bool = False,
        origin_input_ids_unpadded: Optional[Tuple[int]] = None,
        lora_id: Optional[str] = None,
        input_embeds: Optional[List[List[float]]] = None,
        token_type_ids: List[int] = None,
        session_id: Optional[str] = None,
        custom_logit_processor: Optional[str] = None,
        return_hidden_states: bool = False,
        eos_token_ids: Optional[Set[int]] = None,
        bootstrap_host: Optional[str] = None,
        bootstrap_port: Optional[int] = None,
        bootstrap_room: Optional[int] = None,
        data_parallel_rank: Optional[int] = None,
        vocab_size: Optional[int] = None,
    ):
        # Input and output info
        self.rid = rid
        self.origin_input_text = origin_input_text
        self.origin_input_ids_unpadded = (
            origin_input_ids_unpadded
            if origin_input_ids_unpadded
            else origin_input_ids  # Before image padding
        )
        self.origin_input_ids = origin_input_ids
        # Each decode stage&#39;s output ids
        self.output_ids = []
        # fill_ids = origin_input_ids + output_ids. Updated if chunked.
        self.fill_ids = []
        self.session_id = session_id
        self.input_embeds = input_embeds

        # for corss-endoder model
        self.token_type_ids = token_type_ids

        # The length of KV that have been removed in local attention chunked prefill
        self.evicted_seqlen_local = 0

        # Sampling info
        if isinstance(sampling_params.custom_params, dict):
            sampling_params = copy.copy(sampling_params)
            sampling_params.custom_params = sampling_params.custom_params | {
                &#34;__req__&#34;: self
            }
        self.sampling_params = sampling_params
        self.custom_logit_processor = custom_logit_processor
        self.return_hidden_states = return_hidden_states
        self.lora_id = lora_id

        # Memory pool info
        self.req_pool_idx: Optional[int] = None

        # Check finish
        self.tokenizer = None
        self.finished_reason = None
        # Whether this request has finished output
        self.finished_output = None
        # If we want to abort the request in the middle of the event loop, set this to true
        # Note: We should never set finished_reason in the middle, the req will get filtered and never respond
        self.to_abort = False
        # This carries the error message for `.to_abort` and will be attached to the finished_reason at the end of the event loop
        self.to_abort_message: str = None
        self.stream = stream
        self.eos_token_ids = eos_token_ids
        self.vocab_size = vocab_size

        # For incremental decoding
        # ----- | --------- read_ids -------|
        # ----- |   surr_ids  |
        # xxxxx | xxxxxxxxxxx | xxxxxxxxxxx |
        # ----- ^ ----------- ^ ----------- ^
        # ----- 1 ----------- 2 ----------- 3
        # 1: surr_offset
        # 2: read_offset
        # 3: last token
        self.surr_offset = None  # Surrounding offset to defeat the cleanup algorithm
        self.read_offset = None
        self.decoded_text = &#34;&#34;

        # For multimodal inputs
        self.multimodal_inputs: Optional[MultimodalInputs] = None

        # Prefix info
        # The indices to kv cache for the shared prefix.
        self.prefix_indices: torch.Tensor = []
        # Number of tokens to run prefill.
        self.extend_input_len = 0
        # The relative logprob_start_len in an extend batch
        self.extend_logprob_start_len = 0
        self.last_node: Any = None
        self.last_host_node: Any = None
        self.host_hit_length = 0
        # The node to lock until for swa radix tree lock ref
        self.swa_uuid_for_lock: Optional[int] = None

        # Whether or not if it is chunked. It increments whenever
        # it is chunked, and decrement whenever chunked request is
        # processed.
        self.is_chunked = 0

        # For retraction
        self.is_retracted = False

        # Incremental streamining
        self.send_token_offset: int = 0
        self.send_decode_id_offset: int = 0
        # TODO (Byron): send_output_token_logprobs_offset and send_decode_id_offset can be different in disaggregation mode
        # because the decode server does not have the first output token logprobs
        self.send_output_token_logprobs_offset: int = 0

        # Logprobs (arguments)
        self.return_logprob = return_logprob
        # Start index to compute logprob from.
        self.logprob_start_len = 0
        self.top_logprobs_num = top_logprobs_num
        self.token_ids_logprob = token_ids_logprob
        self.temp_scaled_logprobs = False
        self.top_p_normalized_logprobs = False

        # Logprobs (return values)
        # True means the input logprob has been already sent to detokenizer.
        self.input_logprob_sent: bool = False
        self.input_token_logprobs_val: Optional[List[float]] = None
        self.input_token_logprobs_idx: Optional[List[int]] = None
        self.input_top_logprobs_val: Optional[List[float]] = None
        self.input_top_logprobs_idx: Optional[List[int]] = None
        self.input_token_ids_logprobs_val: Optional[List[float]] = None
        self.input_token_ids_logprobs_idx: Optional[List[int]] = None
        # Temporary holder to store input_token_logprobs.
        self.input_token_logprobs: Optional[List[Tuple[int]]] = None
        self.temp_input_top_logprobs_val: Optional[List[torch.Tensor]] = None
        self.temp_input_top_logprobs_idx: Optional[List[int]] = None
        self.temp_input_token_ids_logprobs_val: Optional[List[float]] = None
        self.temp_input_token_ids_logprobs_idx: Optional[List[int]] = None

        if return_logprob:
            # shape: (bs, 1)
            self.output_token_logprobs_val = []
            self.output_token_logprobs_idx = []
            # shape: (bs, k)
            self.output_top_logprobs_val = []
            self.output_top_logprobs_idx = []
            self.output_token_ids_logprobs_val = []
            self.output_token_ids_logprobs_idx = []
        else:
            self.output_token_logprobs_val = self.output_token_logprobs_idx = (
                self.output_top_logprobs_val
            ) = self.output_top_logprobs_idx = self.output_token_ids_logprobs_val = (
                self.output_token_ids_logprobs_idx
            ) = None
        self.hidden_states: List[List[float]] = []
        self.hidden_states_tensor = None  # Note: use tensor instead of list to transfer hidden_states when PD + MTP

        # Embedding (return values)
        self.embedding = None

        # Constrained decoding
        self.grammar: Optional[BaseGrammarObject] = None
        self.grammar_wait_ct = 0

        # The number of cached tokens that were already cached in the KV cache
        self.cached_tokens = 0
        self.already_computed = 0

        # The number of verification forward passes in the speculative decoding.
        # This is used to compute the average acceptance length per request.
        self.spec_verify_ct = 0

        # For metrics
        self.time_stats: TimeStats = TimeStats()
        self.has_log_time_stats: bool = False
        self.queue_time_start = None
        self.queue_time_end = None

        # For disaggregation
        self.bootstrap_host: str = bootstrap_host
        self.bootstrap_port: Optional[int] = bootstrap_port
        self.bootstrap_room: Optional[int] = bootstrap_room
        self.disagg_kv_sender: Optional[BaseKVSender] = None

        # For data parallel rank routing
        self.data_parallel_rank: Optional[int] = data_parallel_rank

        # the start index of the sent kv cache
        # We want to send it chunk by chunk for chunked prefill.
        # After every chunk forward, we do the following:
        # kv_send(req.input_ids[req.start_send_idx:len(req.fill_ids)])
        # start_send_idx = len(req.fill_ids)
        self.start_send_idx: int = 0

        # For overlap schedule, we delay the kv transfer until `process_batch_result_disagg_prefill` rather than `process_prefill_chunk` in non-overlap
        # This is because kv is not ready in `process_prefill_chunk`.
        # We use `tmp_end_idx` to store the end index of the kv cache to send.
        self.tmp_end_idx: int = -1
        self.metadata_buffer_index: int = -1

    @property
    def seqlen(self):
        return len(self.origin_input_ids) + len(self.output_ids)

    def extend_image_inputs(self, image_inputs):
        if self.multimodal_inputs is None:
            self.multimodal_inputs = image_inputs
        else:
            self.multimodal_inputs.merge(image_inputs)

    def finished(self) -&gt; bool:
        # Whether request reached finished condition
        return self.finished_reason is not None

    def init_next_round_input(
        self,
        tree_cache: Optional[BasePrefixCache] = None,
    ):
        self.fill_ids = self.origin_input_ids + self.output_ids
        if tree_cache is not None:
            if isinstance(tree_cache, LoRARadixCache):
                (
                    self.prefix_indices,
                    self.last_node,
                    self.last_host_node,
                    self.host_hit_length,
                ) = tree_cache.match_prefix_with_lora_id(
                    key=LoRAKey(
                        lora_id=self.lora_id, token_ids=self.adjust_max_prefix_ids()
                    ),
                )
            else:
                (
                    self.prefix_indices,
                    self.last_node,
                    self.last_host_node,
                    self.host_hit_length,
                ) = tree_cache.match_prefix(
                    key=self.adjust_max_prefix_ids(),
                )
        self.extend_input_len = len(self.fill_ids) - len(self.prefix_indices)

    def adjust_max_prefix_ids(self):
        self.fill_ids = self.origin_input_ids + self.output_ids
        input_len = len(self.fill_ids)

        # FIXME: To work around some bugs in logprob computation, we need to ensure each
        # request has at least one token. Later, we can relax this requirement and use `input_len`.
        max_prefix_len = input_len - 1

        if self.sampling_params.max_new_tokens &gt; 0:
            # Need at least one token to compute logits
            max_prefix_len = min(max_prefix_len, input_len - 1)

        if self.return_logprob:
            max_prefix_len = min(max_prefix_len, self.logprob_start_len)

        max_prefix_len = max(max_prefix_len, 0)
        return self.fill_ids[:max_prefix_len]

    # Based on https://github.com/vllm-project/vllm/blob/7a64d24aad69e4d2548aa0bf528d9fe63428ab01/vllm/transformers_utils/detokenizer.py#L194-L313
    def init_incremental_detokenize(self):
        first_iter = self.surr_offset is None or self.read_offset is None

        if first_iter:
            self.read_offset = len(self.origin_input_ids_unpadded)
            self.surr_offset = max(
                self.read_offset - INIT_INCREMENTAL_DETOKENIZATION_OFFSET, 0
            )

        all_ids = self.origin_input_ids_unpadded + self.output_ids
        return all_ids[self.surr_offset :], self.read_offset - self.surr_offset

    def check_finished(self):
        if self.finished():
            return

        if self.to_abort:
            self.finished_reason = FINISH_ABORT(
                message=self.to_abort_message,
            )
            return

        if len(self.output_ids) &gt;= self.sampling_params.max_new_tokens:
            self.finished_reason = FINISH_LENGTH(
                length=self.sampling_params.max_new_tokens
            )
            return

        if self.grammar is not None:
            if self.grammar.is_terminated():
                self.finished_reason = FINISH_MATCHED_TOKEN(matched=self.output_ids[-1])
                return

        last_token_id = self.output_ids[-1]

        if not self.sampling_params.ignore_eos:
            matched_eos = False

            # Check stop token ids
            if self.sampling_params.stop_token_ids:
                matched_eos = last_token_id in self.sampling_params.stop_token_ids
            if self.eos_token_ids:
                matched_eos |= last_token_id in self.eos_token_ids
            if self.tokenizer is not None:
                matched_eos |= last_token_id == self.tokenizer.eos_token_id
                if self.tokenizer.additional_stop_token_ids:
                    matched_eos |= (
                        last_token_id in self.tokenizer.additional_stop_token_ids
                    )
            if matched_eos:
                self.finished_reason = FINISH_MATCHED_TOKEN(matched=last_token_id)
                return

        if last_token_id &gt; self.vocab_size or last_token_id &lt; 0:
            if self.sampling_params.stop_token_ids:
                self.output_ids[-1] = next(iter(self.sampling_params.stop_token_ids))
            if self.eos_token_ids:
                self.output_ids[-1] = next(iter(self.eos_token_ids))
            self.finished_reason = FINISH_MATCHED_STR(matched=&#34;NaN happened&#34;)
            return

        # Check stop strings
        if len(self.sampling_params.stop_strs) &gt; 0:
            tail_str = self.tokenizer.decode(
                self.output_ids[-(self.sampling_params.stop_str_max_len + 1) :]
            )

            for stop_str in self.sampling_params.stop_strs:
                if stop_str in tail_str or stop_str in self.decoded_text:
                    self.finished_reason = FINISH_MATCHED_STR(matched=stop_str)
                    return

    def reset_for_retract(self):
        self.prefix_indices = []
        self.last_node = None
        self.swa_uuid_for_lock = None
        self.extend_input_len = 0
        self.is_retracted = True
        self.input_token_logprobs = None
        self.temp_input_top_logprobs_val = None
        self.temp_input_top_logprobs_idx = None
        self.extend_logprob_start_len = 0
        self.is_chunked = 0
        self.req_pool_idx = None
        self.already_computed = 0

    def offload_kv_cache(self, req_to_token_pool, token_to_kv_pool_allocator):
        token_indices = req_to_token_pool.req_to_token[
            self.req_pool_idx, : self.seqlen - 1
        ]
        self.kv_cache_cpu = token_to_kv_pool_allocator.get_cpu_copy(token_indices)

    def load_kv_cache(self, req_to_token_pool, token_to_kv_pool_allocator):
        token_indices = req_to_token_pool.req_to_token[
            self.req_pool_idx, : self.seqlen - 1
        ]
        token_to_kv_pool_allocator.load_cpu_copy(self.kv_cache_cpu, token_indices)
        del self.kv_cache_cpu

    def log_time_stats(self):
        # If overlap schedule, we schedule one decode batch ahead so this gets called twice.
        if self.has_log_time_stats is True:
            return

        if self.bootstrap_room is not None:
            prefix = f&#34;Req Time Stats(rid={self.rid}, bootstrap_room={self.bootstrap_room}, input len={len(self.origin_input_ids)}, output len={len(self.output_ids)}, type={self.time_stats.get_type().value})&#34;
        else:
            prefix = f&#34;Req Time Stats(rid={self.rid}, input len={len(self.origin_input_ids)}, output len={len(self.output_ids)}, type={self.time_stats.get_type().value})&#34;
        logger.info(f&#34;{prefix}: {self.time_stats}&#34;)
        self.has_log_time_stats = True

    def set_finish_with_abort(self, error_msg: str):
        if get_tensor_model_parallel_rank() == 0:
            logger.error(f&#34;{error_msg}, {self.rid=}&#34;)
        self.multimodal_inputs = None
        self.grammar = None
        self.origin_input_ids = [0]  # set it to one token to skip the long prefill
        self.return_logprob = False
        self.finished_reason = FINISH_ABORT(
            error_msg, HTTPStatus.BAD_REQUEST, &#34;BadRequestError&#34;
        )

    def __repr__(self):
        return (
            f&#34;Req(rid={self.rid}, &#34;
            f&#34;input_ids={self.origin_input_ids}, output_ids={self.output_ids}, &#34;
            f&#34;{self.grammar=}, &#34;
            f&#34;{self.sampling_params=})&#34;
        )</code></pre>
</details>
<div class="desc"><p>The input and output status of a request.</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.managers.schedule_batch.Req.seqlen"><code class="name">prop <span class="ident">seqlen</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def seqlen(self):
    return len(self.origin_input_ids) + len(self.output_ids)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.managers.schedule_batch.Req.adjust_max_prefix_ids"><code class="name flex">
<span>def <span class="ident">adjust_max_prefix_ids</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def adjust_max_prefix_ids(self):
    self.fill_ids = self.origin_input_ids + self.output_ids
    input_len = len(self.fill_ids)

    # FIXME: To work around some bugs in logprob computation, we need to ensure each
    # request has at least one token. Later, we can relax this requirement and use `input_len`.
    max_prefix_len = input_len - 1

    if self.sampling_params.max_new_tokens &gt; 0:
        # Need at least one token to compute logits
        max_prefix_len = min(max_prefix_len, input_len - 1)

    if self.return_logprob:
        max_prefix_len = min(max_prefix_len, self.logprob_start_len)

    max_prefix_len = max(max_prefix_len, 0)
    return self.fill_ids[:max_prefix_len]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.Req.check_finished"><code class="name flex">
<span>def <span class="ident">check_finished</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_finished(self):
    if self.finished():
        return

    if self.to_abort:
        self.finished_reason = FINISH_ABORT(
            message=self.to_abort_message,
        )
        return

    if len(self.output_ids) &gt;= self.sampling_params.max_new_tokens:
        self.finished_reason = FINISH_LENGTH(
            length=self.sampling_params.max_new_tokens
        )
        return

    if self.grammar is not None:
        if self.grammar.is_terminated():
            self.finished_reason = FINISH_MATCHED_TOKEN(matched=self.output_ids[-1])
            return

    last_token_id = self.output_ids[-1]

    if not self.sampling_params.ignore_eos:
        matched_eos = False

        # Check stop token ids
        if self.sampling_params.stop_token_ids:
            matched_eos = last_token_id in self.sampling_params.stop_token_ids
        if self.eos_token_ids:
            matched_eos |= last_token_id in self.eos_token_ids
        if self.tokenizer is not None:
            matched_eos |= last_token_id == self.tokenizer.eos_token_id
            if self.tokenizer.additional_stop_token_ids:
                matched_eos |= (
                    last_token_id in self.tokenizer.additional_stop_token_ids
                )
        if matched_eos:
            self.finished_reason = FINISH_MATCHED_TOKEN(matched=last_token_id)
            return

    if last_token_id &gt; self.vocab_size or last_token_id &lt; 0:
        if self.sampling_params.stop_token_ids:
            self.output_ids[-1] = next(iter(self.sampling_params.stop_token_ids))
        if self.eos_token_ids:
            self.output_ids[-1] = next(iter(self.eos_token_ids))
        self.finished_reason = FINISH_MATCHED_STR(matched=&#34;NaN happened&#34;)
        return

    # Check stop strings
    if len(self.sampling_params.stop_strs) &gt; 0:
        tail_str = self.tokenizer.decode(
            self.output_ids[-(self.sampling_params.stop_str_max_len + 1) :]
        )

        for stop_str in self.sampling_params.stop_strs:
            if stop_str in tail_str or stop_str in self.decoded_text:
                self.finished_reason = FINISH_MATCHED_STR(matched=stop_str)
                return</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.Req.extend_image_inputs"><code class="name flex">
<span>def <span class="ident">extend_image_inputs</span></span>(<span>self, image_inputs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extend_image_inputs(self, image_inputs):
    if self.multimodal_inputs is None:
        self.multimodal_inputs = image_inputs
    else:
        self.multimodal_inputs.merge(image_inputs)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.Req.finished"><code class="name flex">
<span>def <span class="ident">finished</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def finished(self) -&gt; bool:
    # Whether request reached finished condition
    return self.finished_reason is not None</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.Req.init_incremental_detokenize"><code class="name flex">
<span>def <span class="ident">init_incremental_detokenize</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_incremental_detokenize(self):
    first_iter = self.surr_offset is None or self.read_offset is None

    if first_iter:
        self.read_offset = len(self.origin_input_ids_unpadded)
        self.surr_offset = max(
            self.read_offset - INIT_INCREMENTAL_DETOKENIZATION_OFFSET, 0
        )

    all_ids = self.origin_input_ids_unpadded + self.output_ids
    return all_ids[self.surr_offset :], self.read_offset - self.surr_offset</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.Req.init_next_round_input"><code class="name flex">
<span>def <span class="ident">init_next_round_input</span></span>(<span>self, tree_cache: Optional[BasePrefixCache] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_next_round_input(
    self,
    tree_cache: Optional[BasePrefixCache] = None,
):
    self.fill_ids = self.origin_input_ids + self.output_ids
    if tree_cache is not None:
        if isinstance(tree_cache, LoRARadixCache):
            (
                self.prefix_indices,
                self.last_node,
                self.last_host_node,
                self.host_hit_length,
            ) = tree_cache.match_prefix_with_lora_id(
                key=LoRAKey(
                    lora_id=self.lora_id, token_ids=self.adjust_max_prefix_ids()
                ),
            )
        else:
            (
                self.prefix_indices,
                self.last_node,
                self.last_host_node,
                self.host_hit_length,
            ) = tree_cache.match_prefix(
                key=self.adjust_max_prefix_ids(),
            )
    self.extend_input_len = len(self.fill_ids) - len(self.prefix_indices)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.Req.load_kv_cache"><code class="name flex">
<span>def <span class="ident">load_kv_cache</span></span>(<span>self, req_to_token_pool, token_to_kv_pool_allocator)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_kv_cache(self, req_to_token_pool, token_to_kv_pool_allocator):
    token_indices = req_to_token_pool.req_to_token[
        self.req_pool_idx, : self.seqlen - 1
    ]
    token_to_kv_pool_allocator.load_cpu_copy(self.kv_cache_cpu, token_indices)
    del self.kv_cache_cpu</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.Req.log_time_stats"><code class="name flex">
<span>def <span class="ident">log_time_stats</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_time_stats(self):
    # If overlap schedule, we schedule one decode batch ahead so this gets called twice.
    if self.has_log_time_stats is True:
        return

    if self.bootstrap_room is not None:
        prefix = f&#34;Req Time Stats(rid={self.rid}, bootstrap_room={self.bootstrap_room}, input len={len(self.origin_input_ids)}, output len={len(self.output_ids)}, type={self.time_stats.get_type().value})&#34;
    else:
        prefix = f&#34;Req Time Stats(rid={self.rid}, input len={len(self.origin_input_ids)}, output len={len(self.output_ids)}, type={self.time_stats.get_type().value})&#34;
    logger.info(f&#34;{prefix}: {self.time_stats}&#34;)
    self.has_log_time_stats = True</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.Req.offload_kv_cache"><code class="name flex">
<span>def <span class="ident">offload_kv_cache</span></span>(<span>self, req_to_token_pool, token_to_kv_pool_allocator)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def offload_kv_cache(self, req_to_token_pool, token_to_kv_pool_allocator):
    token_indices = req_to_token_pool.req_to_token[
        self.req_pool_idx, : self.seqlen - 1
    ]
    self.kv_cache_cpu = token_to_kv_pool_allocator.get_cpu_copy(token_indices)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.Req.reset_for_retract"><code class="name flex">
<span>def <span class="ident">reset_for_retract</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_for_retract(self):
    self.prefix_indices = []
    self.last_node = None
    self.swa_uuid_for_lock = None
    self.extend_input_len = 0
    self.is_retracted = True
    self.input_token_logprobs = None
    self.temp_input_top_logprobs_val = None
    self.temp_input_top_logprobs_idx = None
    self.extend_logprob_start_len = 0
    self.is_chunked = 0
    self.req_pool_idx = None
    self.already_computed = 0</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.Req.set_finish_with_abort"><code class="name flex">
<span>def <span class="ident">set_finish_with_abort</span></span>(<span>self, error_msg: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_finish_with_abort(self, error_msg: str):
    if get_tensor_model_parallel_rank() == 0:
        logger.error(f&#34;{error_msg}, {self.rid=}&#34;)
    self.multimodal_inputs = None
    self.grammar = None
    self.origin_input_ids = [0]  # set it to one token to skip the long prefill
    self.return_logprob = False
    self.finished_reason = FINISH_ABORT(
        error_msg, HTTPStatus.BAD_REQUEST, &#34;BadRequestError&#34;
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch"><code class="flex name class">
<span>class <span class="ident">ScheduleBatch</span></span>
<span>(</span><span>reqs: List[<a title="sglang.srt.managers.schedule_batch.Req" href="#sglang.srt.managers.schedule_batch.Req">Req</a>],<br>req_to_token_pool: ReqToTokenPool = None,<br>token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator = None,<br>tree_cache: BasePrefixCache = None,<br>is_hybrid: bool = False,<br>model_config: ModelConfig = None,<br>forward_mode: ForwardMode = None,<br>enable_overlap: bool = False,<br>batch_is_full: bool = False,<br>launch_done: Optional[threading.Event] = None,<br>chunked_req: Optional[<a title="sglang.srt.managers.schedule_batch.Req" href="#sglang.srt.managers.schedule_batch.Req">Req</a>] = None,<br>sampling_info: SamplingBatchInfo = None,<br>next_batch_sampling_info: SamplingBatchInfo = None,<br>input_ids: torch.Tensor = None,<br>input_embeds: torch.Tensor = None,<br>token_type_ids: torch.Tensor = None,<br>req_pool_indices: torch.Tensor = None,<br>seq_lens: torch.Tensor = None,<br>out_cache_loc: torch.Tensor = None,<br>output_ids: torch.Tensor = None,<br>multimodal_inputs: Optional[List] = None,<br>seq_lens_sum: int = None,<br>orig_seq_lens: torch.Tensor = None,<br>global_num_tokens: Optional[List[int]] = None,<br>global_num_tokens_for_logprob: Optional[List[int]] = None,<br>is_extend_in_batch: bool = False,<br>can_run_dp_cuda_graph: bool = False,<br>tbo_split_seq_index: Optional[int] = None,<br>global_forward_mode: Optional[ForwardMode] = None,<br>return_logprob: bool = False,<br>top_logprobs_nums: Optional[List[int]] = None,<br>token_ids_logprobs: Optional[List[List[int]]] = None,<br>temp_scaled_logprobs: bool = False,<br>top_p_normalized_logprobs: bool = False,<br>prefix_lens: List[int] = None,<br>extend_lens: List[int] = None,<br>extend_num_tokens: Optional[int] = None,<br>decoding_reqs: List[<a title="sglang.srt.managers.schedule_batch.Req" href="#sglang.srt.managers.schedule_batch.Req">Req</a>] = None,<br>extend_logprob_start_lens: List[int] = None,<br>extend_input_logprob_token_ids: Optional[torch.Tensor] = None,<br>encoder_cached: Optional[List[bool]] = None,<br>encoder_lens: Optional[torch.Tensor] = None,<br>encoder_lens_cpu: Optional[List[int]] = None,<br>encoder_out_cache_loc: Optional[torch.Tensor] = None,<br>has_stream: bool = False,<br>has_grammar: bool = False,<br>device: str = 'cuda',<br>spec_algorithm: SpeculativeAlgorithm = None,<br>spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]] = None,<br>return_hidden_states: bool = False,<br>is_prefill_only: bool = False,<br>hicache_consumer_index: int = 0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclasses.dataclass
class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):
    &#34;&#34;&#34;Store all information of a batch on the scheduler.&#34;&#34;&#34;

    # Request, memory pool, and cache
    reqs: List[Req]
    req_to_token_pool: ReqToTokenPool = None
    token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator = None
    tree_cache: BasePrefixCache = None
    is_hybrid: bool = False

    # Batch configs
    model_config: ModelConfig = None
    forward_mode: ForwardMode = None
    enable_overlap: bool = False
    # Tell whether the current running batch is full so that we can skip
    # the check of whether to prefill new requests.
    # This is an optimization to reduce the overhead of the prefill check.
    batch_is_full: bool = False

    # Events
    launch_done: Optional[threading.Event] = None

    # For chunked prefill in PP
    chunked_req: Optional[Req] = None

    # Sampling info
    sampling_info: SamplingBatchInfo = None
    next_batch_sampling_info: SamplingBatchInfo = None

    # Batched arguments to model runner
    input_ids: torch.Tensor = None  # shape: [b], int64
    input_embeds: torch.Tensor = None  # shape: [b, hidden_size], float32
    token_type_ids: torch.Tensor = None  # shape: [b], int64
    req_pool_indices: torch.Tensor = None  # shape: [b], int64
    seq_lens: torch.Tensor = None  # shape: [b], int64
    # The output locations of the KV cache
    out_cache_loc: torch.Tensor = None  # shape: [b], int64
    output_ids: torch.Tensor = None  # shape: [b], int64

    # For multimodal inputs
    multimodal_inputs: Optional[List] = None

    # The sum of all sequence lengths
    seq_lens_sum: int = None
    # The original sequence lengths, Qwen-1M related
    orig_seq_lens: torch.Tensor = None  # shape: [b], int32

    # For DP attention
    global_num_tokens: Optional[List[int]] = None
    global_num_tokens_for_logprob: Optional[List[int]] = None
    is_extend_in_batch: bool = False
    can_run_dp_cuda_graph: bool = False
    tbo_split_seq_index: Optional[int] = None
    global_forward_mode: Optional[ForwardMode] = None

    # For processing logprobs
    return_logprob: bool = False
    top_logprobs_nums: Optional[List[int]] = None
    token_ids_logprobs: Optional[List[List[int]]] = None

    # For logits and logprob post processing
    temp_scaled_logprobs: bool = False
    top_p_normalized_logprobs: bool = False

    # For extend and mixed chunekd prefill
    prefix_lens: List[int] = None
    extend_lens: List[int] = None
    extend_num_tokens: Optional[int] = None
    decoding_reqs: List[Req] = None
    extend_logprob_start_lens: List[int] = None
    # It comes empty list if logprob is not required.
    extend_input_logprob_token_ids: Optional[torch.Tensor] = None

    # For encoder-decoder architectures
    encoder_cached: Optional[List[bool]] = None
    encoder_lens: Optional[torch.Tensor] = None
    encoder_lens_cpu: Optional[List[int]] = None
    encoder_out_cache_loc: Optional[torch.Tensor] = None

    # Stream
    has_stream: bool = False

    # Has grammar
    has_grammar: bool = False

    # Device
    device: str = &#34;cuda&#34;

    # Speculative decoding
    spec_algorithm: SpeculativeAlgorithm = None
    spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]] = None

    # Whether to return hidden states
    return_hidden_states: bool = False

    # Whether this batch is prefill-only (no token generation needed)
    is_prefill_only: bool = False

    # hicache pointer for synchronizing data loading from CPU to GPU
    hicache_consumer_index: int = 0

    @classmethod
    def init_new(
        cls,
        reqs: List[Req],
        req_to_token_pool: ReqToTokenPool,
        token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,
        tree_cache: BasePrefixCache,
        model_config: ModelConfig,
        enable_overlap: bool,
        spec_algorithm: SpeculativeAlgorithm,
        chunked_req: Optional[Req] = None,
    ):
        return_logprob = any(req.return_logprob for req in reqs)

        is_hybrid = False
        if isinstance(token_to_kv_pool_allocator, SWATokenToKVPoolAllocator):
            assert (
                tree_cache is None
                or isinstance(tree_cache, SWARadixCache)
                or isinstance(tree_cache, SWAChunkCache)
            ), &#34;SWARadixCache or SWAChunkCache is required for SWATokenToKVPoolAllocator&#34;
            is_hybrid = True

        return cls(
            reqs=reqs,
            req_to_token_pool=req_to_token_pool,
            token_to_kv_pool_allocator=token_to_kv_pool_allocator,
            tree_cache=tree_cache,
            is_hybrid=is_hybrid,
            model_config=model_config,
            enable_overlap=enable_overlap,
            return_logprob=return_logprob,
            has_stream=any(req.stream for req in reqs),
            has_grammar=any(req.grammar for req in reqs),
            device=req_to_token_pool.device,
            spec_algorithm=spec_algorithm,
            return_hidden_states=any(req.return_hidden_states for req in reqs),
            is_prefill_only=all(
                req.sampling_params.max_new_tokens == 0 for req in reqs
            ),
            chunked_req=chunked_req,
        )

    def batch_size(self):
        return len(self.reqs)

    def is_empty(self):
        return len(self.reqs) == 0

    def alloc_req_slots(self, num_reqs: int):
        req_pool_indices = self.req_to_token_pool.alloc(num_reqs)
        if req_pool_indices is None:
            raise RuntimeError(
                &#34;alloc_req_slots runs out of memory. &#34;
                &#34;Please set a smaller number for `--max-running-requests`. &#34;
                f&#34;{self.req_to_token_pool.available_size()=}, &#34;
                f&#34;{num_reqs=}, &#34;
            )
        return req_pool_indices

    def alloc_token_slots(self, num_tokens: int, backup_state: bool = False):
        self._evict_tree_cache_if_needed(num_tokens)

        if backup_state:
            state = self.token_to_kv_pool_allocator.backup_state()

        out_cache_loc = self.token_to_kv_pool_allocator.alloc(num_tokens)
        if out_cache_loc is None:
            phase_str = &#34;Prefill&#34; if self.forward_mode.is_extend() else &#34;Decode&#34;
            error_msg = (
                f&#34;{phase_str} out of memory. Try to lower your batch size.\n&#34;
                f&#34;Try to allocate {num_tokens} tokens.\n&#34;
                f&#34;{self._available_and_evictable_str()}&#34;
            )
            logger.error(error_msg)
            if self.tree_cache is not None:
                self.tree_cache.pretty_print()
            raise RuntimeError(error_msg)

        if backup_state:
            return out_cache_loc, state
        else:
            return out_cache_loc

    def alloc_paged_token_slots_extend(
        self,
        prefix_lens: torch.Tensor,
        seq_lens: torch.Tensor,
        last_loc: torch.Tensor,
        extend_num_tokens: int,
        backup_state: bool = False,
    ):
        # Over estimate the number of tokens: assume each request needs a new page.
        num_tokens = (
            extend_num_tokens
            + len(seq_lens) * self.token_to_kv_pool_allocator.page_size
        )
        self._evict_tree_cache_if_needed(num_tokens)

        if backup_state:
            state = self.token_to_kv_pool_allocator.backup_state()

        out_cache_loc = self.token_to_kv_pool_allocator.alloc_extend(
            prefix_lens, seq_lens, last_loc, extend_num_tokens
        )
        if out_cache_loc is None:
            error_msg = (
                f&#34;Prefill out of memory. Try to lower your batch size.\n&#34;
                f&#34;Try to allocate {extend_num_tokens} tokens.\n&#34;
                f&#34;{self._available_and_evictable_str()}&#34;
            )
            logger.error(error_msg)
            raise RuntimeError(error_msg)

        if backup_state:
            return out_cache_loc, state
        else:
            return out_cache_loc

    def alloc_paged_token_slots_decode(
        self,
        seq_lens: torch.Tensor,
        last_loc: torch.Tensor,
        backup_state: bool = False,
    ):
        # Over estimate the number of tokens: assume each request needs a new page.
        num_tokens = len(seq_lens) * self.token_to_kv_pool_allocator.page_size
        self._evict_tree_cache_if_needed(num_tokens)

        if backup_state:
            state = self.token_to_kv_pool_allocator.backup_state()

        out_cache_loc = self.token_to_kv_pool_allocator.alloc_decode(seq_lens, last_loc)
        if out_cache_loc is None:
            error_msg = (
                f&#34;Decode out of memory. Try to lower your batch size.\n&#34;
                f&#34;Try to allocate {len(seq_lens)} tokens.\n&#34;
                f&#34;{self._available_and_evictable_str()}&#34;
            )
            logger.error(error_msg)
            raise RuntimeError(error_msg)

        if backup_state:
            return out_cache_loc, state
        else:
            return out_cache_loc

    def prepare_encoder_info_extend(self, input_ids: List[int], seq_lens: List[int]):
        self.encoder_lens_cpu = []
        self.encoder_cached = []

        for req in self.reqs:
            im = req.multimodal_inputs
            if im is None or im.num_image_tokens is None:
                # No image input
                self.encoder_lens_cpu.append(0)
                self.encoder_cached.append(True)
            else:
                self.encoder_lens_cpu.append(im.num_image_tokens)
                self.encoder_cached.append(
                    self.forward_mode.is_decode()
                    or len(req.prefix_indices) &gt;= im.num_image_tokens
                )

        self.encoder_lens = torch.tensor(self.encoder_lens_cpu, dtype=torch.int64).to(
            self.device, non_blocking=True
        )

        # Strip encoder infos
        pt = 0
        decoder_out_cache_loc = []
        encoder_out_cache_loc = []
        for i, req in enumerate(self.reqs):
            encoder_len = self.encoder_lens_cpu[i]
            seq_lens[i] -= encoder_len

            if len(req.prefix_indices) &lt; encoder_len:
                # NOTE: the encoder part should be considered as a whole
                assert len(req.prefix_indices) == 0
                input_ids[i] = input_ids[i][encoder_len:]
                encoder_out_cache_loc.append(self.out_cache_loc[pt : pt + encoder_len])
                decoder_out_cache_loc.append(
                    self.out_cache_loc[pt + encoder_len : pt + req.extend_input_len]
                )
                self.extend_lens[i] -= encoder_len
                self.extend_num_tokens -= encoder_len
            else:
                decoder_out_cache_loc.append(
                    self.out_cache_loc[pt : pt + req.extend_input_len]
                )
                self.prefix_lens[i] -= encoder_len

            pt += req.extend_input_len

        # Reassign
        self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int64).to(
            self.device, non_blocking=True
        )
        self.seq_lens = torch.tensor(seq_lens, dtype=torch.int64).to(
            self.device, non_blocking=True
        )

        if not decoder_out_cache_loc:
            self.out_cache_loc = torch.zeros(0, dtype=torch.int64).to(
                self.device, non_blocking=True
            )
        else:
            self.out_cache_loc = torch.cat(decoder_out_cache_loc)

        if not encoder_out_cache_loc:
            self.encoder_out_cache_loc = torch.zeros(0, dtype=torch.int64).to(
                self.device, non_blocking=True
            )
        else:
            self.encoder_out_cache_loc = torch.cat(encoder_out_cache_loc)

        assert (
            len(self.out_cache_loc) == self.extend_num_tokens
        ), f&#34;Expected {len(self.out_cache_loc)}, got {self.extend_num_tokens}&#34;

    def prepare_for_extend(self):
        self.forward_mode = ForwardMode.EXTEND

        # Allocate req slots
        bs = len(self.reqs)
        req_pool_indices = self.alloc_req_slots(bs)

        # Init tensors
        reqs = self.reqs
        input_ids = [r.fill_ids[len(r.prefix_indices) :] for r in reqs]
        extend_num_tokens = sum(len(ids) for ids in input_ids)
        seq_lens = [len(r.fill_ids) for r in reqs]
        orig_seq_lens = [max(len(r.fill_ids), len(r.origin_input_ids)) for r in reqs]
        prefix_lens = [len(r.prefix_indices) for r in reqs]
        extend_lens = [r.extend_input_len for r in reqs]

        token_type_ids = [
            r.token_type_ids for r in reqs if r.token_type_ids is not None
        ]

        req_pool_indices_tensor = torch.tensor(req_pool_indices, dtype=torch.int64).to(
            self.device, non_blocking=True
        )
        input_ids_tensor = torch.tensor(
            list(chain.from_iterable(input_ids)), dtype=torch.int64
        ).to(self.device, non_blocking=True)
        seq_lens_tensor = torch.tensor(seq_lens, dtype=torch.int64).to(
            self.device, non_blocking=True
        )
        orig_seq_lens_tensor = torch.tensor(orig_seq_lens, dtype=torch.int32).to(
            self.device, non_blocking=True
        )
        prefix_lens_tensor = torch.tensor(
            prefix_lens, dtype=torch.int64, device=self.device
        )

        token_type_ids_tensor = None
        if len(token_type_ids) &gt; 0:
            token_type_ids_tensor = torch.tensor(
                sum(token_type_ids, []), dtype=torch.int64
            ).to(self.device, non_blocking=True)

        extend_lens_tensor = seq_lens_tensor - prefix_lens_tensor

        # Copy prefix and do some basic check
        input_embeds = []
        extend_input_logprob_token_ids = []
        multimodal_inputs = []

        for i, (req, seq_len, pre_len) in enumerate(zip(reqs, seq_lens, prefix_lens)):
            req.req_pool_idx = req_pool_indices[i]
            assert seq_len - pre_len == req.extend_input_len

            if pre_len &gt; 0:
                self.req_to_token_pool.write(
                    (req.req_pool_idx, slice(0, pre_len)), req.prefix_indices
                )
                if isinstance(self.tree_cache, SWAChunkCache):
                    self.tree_cache.evict_swa(
                        req, pre_len, self.model_config.attention_chunk_size
                    )

            # If input_embeds are available, store them
            if req.input_embeds is not None:
                # If req.input_embeds is already a list, append its content directly
                input_embeds.extend(req.input_embeds)  # Use extend to avoid nesting

            multimodal_inputs.append(req.multimodal_inputs)

            req.cached_tokens += pre_len - req.already_computed
            req.already_computed = seq_len
            req.is_retracted = False

            # Compute the relative logprob_start_len in an extend batch
            if req.logprob_start_len &gt;= pre_len:
                req.extend_logprob_start_len = min(
                    req.logprob_start_len - pre_len,
                    req.extend_input_len,
                    req.seqlen - 1,
                )
            else:
                req.extend_logprob_start_len = 0

            if self.return_logprob:
                # Find input logprob token ids.
                # First, find a global index within origin_input_ids and slide it by 1
                # to compute input logprobs. It is because you need the next token
                # to compute input logprobs. E.g., (chunk size 2)
                #
                # input_logprobs = [1, 2, 3, 4]
                # fill_ids = [1, 2]
                # extend_input_logprob_token_id = [2, 3]
                #
                # Note that it can also overflow. In this case, we pad it with 0.
                # input_logprobs = [1, 2, 3, 4]
                # fill_ids = [3, 4]
                # extend_input_logprob_token_id = [4, 0]
                global_start_idx, global_end_idx = (
                    len(req.prefix_indices),
                    len(req.fill_ids),
                )
                # Apply logprob_start_len
                if global_start_idx &lt; req.logprob_start_len:
                    global_start_idx = req.logprob_start_len

                logprob_token_ids = req.origin_input_ids[
                    global_start_idx + 1 : global_end_idx + 1
                ]
                extend_input_logprob_token_ids.extend(logprob_token_ids)

                # We will need req.extend_input_len - req.extend_logprob_start_len number of
                # tokens, and logprob_token_ids is for input logprob, so pad the rest of them by 0.
                extend_input_logprob_token_ids.extend(
                    [0]
                    * (
                        req.extend_input_len
                        - req.extend_logprob_start_len
                        - len(logprob_token_ids)
                    )
                )

        if self.return_logprob:
            extend_input_logprob_token_ids = torch.tensor(
                extend_input_logprob_token_ids
            )
        else:
            extend_input_logprob_token_ids = None

        # Allocate memory
        if self.token_to_kv_pool_allocator.page_size == 1:
            out_cache_loc = self.alloc_token_slots(extend_num_tokens)
        else:
            last_loc = get_last_loc(
                self.req_to_token_pool.req_to_token,
                req_pool_indices_tensor,
                prefix_lens_tensor,
            )
            out_cache_loc = self.alloc_paged_token_slots_extend(
                prefix_lens_tensor, seq_lens_tensor, last_loc, extend_num_tokens
            )

        # Set fields
        self.input_ids = input_ids_tensor
        self.req_pool_indices = req_pool_indices_tensor
        self.seq_lens = seq_lens_tensor
        self.orig_seq_lens = orig_seq_lens_tensor
        self.out_cache_loc = out_cache_loc
        self.input_embeds = (
            torch.tensor(input_embeds).to(self.device, non_blocking=True)
            if input_embeds
            else None
        )
        for mm_input in multimodal_inputs:
            if mm_input is None:
                continue
            for mm_item in mm_input.mm_items:
                pixel_values = getattr(mm_item, &#34;feature&#34;, None)
                if isinstance(pixel_values, torch.Tensor):
                    mm_item.feature = pixel_values.to(self.device, non_blocking=True)
        self.multimodal_inputs = multimodal_inputs
        self.token_type_ids = token_type_ids_tensor
        self.seq_lens_sum = sum(seq_lens)

        if self.return_logprob:
            self.top_logprobs_nums = [r.top_logprobs_num for r in reqs]
            self.token_ids_logprobs = [r.token_ids_logprob for r in reqs]

        self.extend_logprob_start_lens = [r.extend_logprob_start_len for r in reqs]
        self.extend_num_tokens = extend_num_tokens
        self.prefix_lens = prefix_lens
        self.extend_lens = extend_lens
        self.extend_input_logprob_token_ids = extend_input_logprob_token_ids

        # Write to req_to_token_pool
        if support_triton(global_server_args_dict.get(&#34;attention_backend&#34;)):
            # TODO: some tensors can be reused for ForwardBatchInfo (e.g., extend_lens, cumsum_start)

            write_req_to_token_pool_triton[(bs,)](
                self.req_to_token_pool.req_to_token,
                req_pool_indices_tensor,
                prefix_lens_tensor,
                seq_lens_tensor,
                extend_lens_tensor,
                out_cache_loc,
                self.req_to_token_pool.req_to_token.shape[1],
            )
        else:
            pt = 0
            for i in range(bs):
                self.req_to_token_pool.write(
                    (req_pool_indices[i], slice(prefix_lens[i], seq_lens[i])),
                    out_cache_loc[pt : pt + extend_lens[i]],
                )
                pt += extend_lens[i]

        if self.model_config.is_encoder_decoder:
            self.prepare_encoder_info_extend(input_ids, seq_lens)

        # Build sampling info
        self.sampling_info = SamplingBatchInfo.from_schedule_batch(
            self,
            self.model_config.vocab_size,
        )

    def prepare_for_split_prefill(self):
        self.prepare_for_extend()
        # For split prefill, we need to set the forward mode to SPLIT_PREFILL
        self.forward_mode = ForwardMode.SPLIT_PREFILL

    def mix_with_running(self, running_batch: &#34;ScheduleBatch&#34;):
        self.forward_mode = ForwardMode.MIXED
        running_bs = running_batch.batch_size()

        for req in running_batch.reqs:
            req.fill_ids = req.origin_input_ids + req.output_ids
            req.extend_input_len = 1

        input_ids = torch.cat([self.input_ids, running_batch.input_ids])
        out_cache_loc = torch.cat([self.out_cache_loc, running_batch.out_cache_loc])

        self.merge_batch(running_batch)
        self.input_ids = input_ids
        self.out_cache_loc = out_cache_loc

        # For overlap scheduler, the output_ids has one step delay
        delta = 0 if self.enable_overlap else -1

        # NOTE: prefix_indices is what has been cached, but we don&#39;t cache each decode step
        self.prefix_lens.extend(
            [
                len(r.origin_input_ids) + len(r.output_ids) + delta
                for r in running_batch.reqs
            ]
        )
        self.extend_lens.extend([1] * running_bs)
        self.extend_num_tokens += running_bs
        # TODO (lianmin): Revisit this. It should be seq_len - 1
        self.extend_logprob_start_lens.extend([0] * running_bs)

    def new_page_count_next_decode(self):
        page_size = self.token_to_kv_pool_allocator.page_size
        if page_size == 1:
            return len(self.reqs)
        # In the decoding phase, the length of a request&#39;s KV cache should be
        # the total length of the request minus 1
        return (
            sum(1 for req in self.reqs if req.seqlen % page_size == 0)
            if self.enable_overlap
            else sum(1 for req in self.reqs if (req.seqlen - 1) % page_size == 0)
        )

    def check_decode_mem(self, buf_multiplier=1):
        num_tokens = (
            self.new_page_count_next_decode()
            * buf_multiplier
            * self.token_to_kv_pool_allocator.page_size
        )

        self._evict_tree_cache_if_needed(num_tokens)
        return self._is_available_size_sufficient(num_tokens)

    def retract_decode(self, server_args: ServerArgs):
        &#34;&#34;&#34;Retract the decoding requests when there is not enough memory.&#34;&#34;&#34;
        sorted_indices = list(range(len(self.reqs)))

        # TODO(lsyin): improve retraction policy for radix cache
        # For spec decoding, filter_batch API can only filter
        # requests from the back, so we can only retract from the back.
        # TODO(sang): Clean up finish path and support better retract
        # policy.
        if not server_args.speculative_algorithm:
            sorted_indices.sort(
                key=lambda i: (
                    len(self.reqs[i].output_ids),
                    -len(self.reqs[i].origin_input_ids),
                ),
                reverse=True,
            )

        def get_required_tokens(num_reqs: int):
            headroom_for_spec_decode = 0
            if server_args.speculative_algorithm:
                headroom_for_spec_decode += (
                    num_reqs
                    * server_args.speculative_eagle_topk
                    * server_args.speculative_num_steps
                    + num_reqs * server_args.speculative_num_draft_tokens
                )
            return (
                num_reqs * global_config.retract_decode_steps + headroom_for_spec_decode
            )

        def _get_available_size():
            if self.is_hybrid:
                return min(
                    self.token_to_kv_pool_allocator.full_available_size(),
                    self.token_to_kv_pool_allocator.swa_available_size(),
                )
            else:
                return self.token_to_kv_pool_allocator.available_size()

        retracted_reqs = []
        seq_lens_cpu = self.seq_lens.cpu().numpy()
        first_iter = True
        while (
            _get_available_size() &lt; get_required_tokens(len(sorted_indices))
            or first_iter
        ):
            if len(sorted_indices) == 1:
                # Corner case: only one request left
                if self.is_hybrid:
                    full_available_size = (
                        self.token_to_kv_pool_allocator.full_available_size()
                    )
                    swa_available_size = (
                        self.token_to_kv_pool_allocator.swa_available_size()
                    )
                    assert (
                        full_available_size &gt; 0 and swa_available_size &gt; 0
                    ), f&#34;No space left for only one request in SWA mode {full_available_size=}, {swa_available_size=}&#34;
                else:
                    assert (
                        self.token_to_kv_pool_allocator.available_size() &gt; 0
                    ), f&#34;No space left for only one request, {self.token_to_kv_pool_allocator.available_size()=}&#34;
                break

            first_iter = False
            idx = sorted_indices.pop()
            req = self.reqs[idx]
            retracted_reqs.append(req)

            if server_args.disaggregation_mode == &#34;decode&#34;:
                req.offload_kv_cache(
                    self.req_to_token_pool, self.token_to_kv_pool_allocator
                )

            if isinstance(self.tree_cache, ChunkCache):
                # ChunkCache does not have eviction
                token_indices = self.req_to_token_pool.req_to_token[
                    req.req_pool_idx, : seq_lens_cpu[idx]
                ]
                self.token_to_kv_pool_allocator.free(token_indices)
                self.req_to_token_pool.free(req.req_pool_idx)
            else:
                # TODO: apply more fine-grained retraction
                last_uncached_pos = (
                    len(req.prefix_indices) // server_args.page_size
                ) * server_args.page_size
                token_indices = self.req_to_token_pool.req_to_token[
                    req.req_pool_idx, last_uncached_pos : seq_lens_cpu[idx]
                ]
                self.token_to_kv_pool_allocator.free(token_indices)
                self.req_to_token_pool.free(req.req_pool_idx)

                # release the last node
                if self.is_hybrid:
                    self.tree_cache.dec_lock_ref(req.last_node, req.swa_uuid_for_lock)
                else:
                    self.tree_cache.dec_lock_ref(req.last_node)

                # NOTE(lsyin): we should use the newly evictable memory instantly.
                num_tokens = len(sorted_indices) * global_config.retract_decode_steps
                self._evict_tree_cache_if_needed(num_tokens)

            req.reset_for_retract()

            if len(retracted_reqs) == 0:
                # Corner case: only one request left
                raise ValueError(
                    &#34;Failed to retract any request. No space left for only one request.&#34;
                )

        self.filter_batch(keep_indices=sorted_indices)

        # Reqs in batch are filtered
        total_decoded_tokens = sum(len(r.output_ids) for r in self.reqs)
        total_max_new_tokens = sum(r.sampling_params.max_new_tokens for r in self.reqs)

        new_estimate_ratio = (
            total_decoded_tokens + global_config.retract_decode_steps * len(self.reqs)
        ) / total_max_new_tokens
        new_estimate_ratio = min(1.0, new_estimate_ratio)

        return retracted_reqs, new_estimate_ratio

    def prepare_encoder_info_decode(self):
        # Reset the encoder cached status
        self.encoder_cached = [True] * len(self.reqs)

    def prepare_for_idle(self):
        self.forward_mode = ForwardMode.IDLE
        self.input_ids = torch.empty(0, dtype=torch.int64, device=self.device)
        self.seq_lens = torch.empty(0, dtype=torch.int64, device=self.device)
        self.orig_seq_lens = torch.empty(0, dtype=torch.int32, device=self.device)
        self.out_cache_loc = torch.empty(0, dtype=torch.int64, device=self.device)
        self.req_pool_indices = torch.empty(0, dtype=torch.int32, device=self.device)
        self.seq_lens_sum = 0
        self.extend_num_tokens = 0
        self.sampling_info = SamplingBatchInfo.from_schedule_batch(
            self,
            self.model_config.vocab_size,
        )

    def prepare_for_decode(self):
        self.forward_mode = ForwardMode.DECODE
        bs = len(self.reqs)

        if self.spec_algorithm.is_eagle():
            # if spec decoding is used, the decode batch is prepared inside
            # `forward_batch_speculative_generation` after running draft models.
            return

        if self.sampling_info.penalizer_orchestrator.is_required:
            if self.enable_overlap:
                # TODO: this can be slow, optimize this.
                delayed_output_ids = torch.tensor(
                    [
                        (
                            req.output_ids[-1]
                            if len(req.output_ids)
                            else req.origin_input_ids[-1]
                        )
                        for req in self.reqs
                    ],
                    dtype=torch.int64,
                    device=self.device,
                )
                self.sampling_info.penalizer_orchestrator.cumulate_output_tokens(
                    delayed_output_ids
                )
            else:
                self.sampling_info.penalizer_orchestrator.cumulate_output_tokens(
                    self.output_ids.to(torch.int64)
                )

        # Update fields
        self.input_ids = self.output_ids
        self.output_ids = None

        if self.model_config.is_encoder_decoder:
            locs = self.encoder_lens + self.seq_lens
            self.prepare_encoder_info_decode()
        else:
            locs = self.seq_lens.clone()

        if self.enable_overlap:
            # Do not use in-place operations in the overlap mode
            self.seq_lens = self.seq_lens + 1
            self.orig_seq_lens = self.orig_seq_lens + 1
        else:
            # A faster in-place version
            self.seq_lens.add_(1)
            self.orig_seq_lens.add_(1)
        self.seq_lens_sum += bs

        # free memory
        if isinstance(self.tree_cache, SWAChunkCache):
            for req in self.reqs:
                self.tree_cache.evict_swa(
                    req, req.seqlen - 1, self.model_config.attention_chunk_size
                )

        # Allocate memory
        if self.token_to_kv_pool_allocator.page_size == 1:
            self.out_cache_loc = self.alloc_token_slots(bs)
        else:
            last_loc = self.req_to_token_pool.req_to_token[
                self.req_pool_indices, self.seq_lens - 2
            ]
            self.out_cache_loc = self.alloc_paged_token_slots_decode(
                self.seq_lens, last_loc
            )

        self.req_to_token_pool.write(
            (self.req_pool_indices, locs), self.out_cache_loc.to(torch.int32)
        )

    def filter_batch(
        self,
        chunked_req_to_exclude: Optional[Union[Req, List[Req]]] = None,
        keep_indices: Optional[List[int]] = None,
    ):
        if keep_indices is None:
            if isinstance(chunked_req_to_exclude, Req):
                chunked_req_to_exclude = [chunked_req_to_exclude]
            elif chunked_req_to_exclude is None:
                chunked_req_to_exclude = []
            keep_indices = [
                i
                for i in range(len(self.reqs))
                if not self.reqs[i].finished()
                and self.reqs[i] not in chunked_req_to_exclude
            ]

        if keep_indices is None or len(keep_indices) == 0:
            # Filter out all requests
            self.reqs = []
            return

        if len(keep_indices) == len(self.reqs):
            # No need to filter
            return

        keep_indices_device = torch.tensor(keep_indices, dtype=torch.int64).to(
            self.device, non_blocking=True
        )

        if self.model_config.is_encoder_decoder:
            self.encoder_lens = self.encoder_lens[keep_indices_device]
            self.encoder_lens_cpu = [self.encoder_lens_cpu[i] for i in keep_indices]

        self.reqs = [self.reqs[i] for i in keep_indices]
        if self.multimodal_inputs is not None:
            self.multimodal_inputs = [self.multimodal_inputs[i] for i in keep_indices]
        self.req_pool_indices = self.req_pool_indices[keep_indices_device]
        self.seq_lens = self.seq_lens[keep_indices_device]
        self.orig_seq_lens = self.orig_seq_lens[keep_indices_device]
        self.out_cache_loc = None
        self.seq_lens_sum = self.seq_lens.sum().item()
        self.output_ids = self.output_ids[keep_indices_device]
        self.return_logprob = any(req.return_logprob for req in self.reqs)
        if self.return_logprob:
            self.top_logprobs_nums = [self.top_logprobs_nums[i] for i in keep_indices]
            self.token_ids_logprobs = [self.token_ids_logprobs[i] for i in keep_indices]
        else:
            self.top_logprobs_nums = None
            self.token_ids_logprobs = None

        self.has_stream = any(req.stream for req in self.reqs)
        self.has_grammar = any(req.grammar for req in self.reqs)

        self.sampling_info.filter_batch(keep_indices, keep_indices_device)
        if self.spec_info:
            self.spec_info.filter_batch(keep_indices_device)

    def merge_batch(self, other: &#34;ScheduleBatch&#34;):
        # Penalizer orchestrator must be merged before Batch.reqs is merged. This is because
        # orchestrator.merge() depends on Batch.reqs during preparation of each penalizers, so it
        # needs to be called with pre-merged Batch.reqs.
        self.sampling_info.merge_batch(other.sampling_info)

        # Encoder-decoder infos
        if self.model_config.is_encoder_decoder:
            self.encoder_lens = torch.cat([self.encoder_lens, other.encoder_lens])
            self.encoder_lens_cpu.extend(other.encoder_lens_cpu)
        self.req_pool_indices = torch.cat(
            [self.req_pool_indices, other.req_pool_indices]
        )
        self.seq_lens = torch.cat([self.seq_lens, other.seq_lens])
        self.orig_seq_lens = torch.cat([self.orig_seq_lens, other.orig_seq_lens])
        self.out_cache_loc = None
        self.seq_lens_sum += other.seq_lens_sum
        if self.output_ids is not None:
            self.output_ids = torch.cat([self.output_ids, other.output_ids])
        if self.return_logprob and other.return_logprob:
            self.top_logprobs_nums.extend(other.top_logprobs_nums)
            self.token_ids_logprobs.extend(other.token_ids_logprobs)
        elif self.return_logprob:
            self.top_logprobs_nums.extend([0] * len(other.reqs))
            self.token_ids_logprobs.extend([None] * len(other.reqs))
        elif other.return_logprob:
            self.top_logprobs_nums = [0] * len(self.reqs) + other.top_logprobs_nums
            self.token_ids_logprobs = [None] * len(self.reqs) + other.token_ids_logprobs
        self.reqs.extend(other.reqs)
        if self.multimodal_inputs is not None:
            self.multimodal_inputs.extend(other.multimodal_inputs)

        self.return_logprob |= other.return_logprob
        self.has_stream |= other.has_stream
        self.has_grammar |= other.has_grammar
        self.return_hidden_states |= other.return_hidden_states

        if self.spec_info:
            self.spec_info.merge_batch(other.spec_info)

    def get_model_worker_batch(
        self, seq_lens_cpu_cache: Optional[torch.Tensor] = None
    ) -&gt; ModelWorkerBatch:
        if self.forward_mode.is_decode_or_idle():
            extend_seq_lens = extend_prefix_lens = extend_logprob_start_lens = None
        else:
            extend_seq_lens = self.extend_lens
            extend_prefix_lens = self.prefix_lens
            extend_logprob_start_lens = self.extend_logprob_start_lens

        if self.sampling_info:
            if self.has_grammar:
                self.sampling_info.grammars = [req.grammar for req in self.reqs]
            else:
                self.sampling_info.grammars = None

        seq_lens_cpu = (
            seq_lens_cpu_cache
            if seq_lens_cpu_cache is not None
            else self.seq_lens.cpu()
        )

        global bid
        bid += 1
        return ModelWorkerBatch(
            bid=bid,
            forward_mode=self.forward_mode,
            input_ids=self.input_ids,
            req_pool_indices=self.req_pool_indices,
            seq_lens=self.seq_lens,
            orig_seq_lens=self.orig_seq_lens,
            out_cache_loc=self.out_cache_loc,
            seq_lens_cpu=seq_lens_cpu,
            seq_lens_sum=self.seq_lens_sum,
            return_logprob=self.return_logprob,
            top_logprobs_nums=self.top_logprobs_nums,
            token_ids_logprobs=self.token_ids_logprobs,
            global_num_tokens=self.global_num_tokens,
            global_num_tokens_for_logprob=self.global_num_tokens_for_logprob,
            is_extend_in_batch=self.is_extend_in_batch,
            can_run_dp_cuda_graph=self.can_run_dp_cuda_graph,
            tbo_split_seq_index=self.tbo_split_seq_index,
            global_forward_mode=self.global_forward_mode,
            extend_num_tokens=self.extend_num_tokens,
            extend_seq_lens=extend_seq_lens,
            extend_prefix_lens=extend_prefix_lens,
            extend_logprob_start_lens=extend_logprob_start_lens,
            multimodal_inputs=self.multimodal_inputs,
            encoder_cached=self.encoder_cached,
            encoder_lens=self.encoder_lens,
            encoder_lens_cpu=self.encoder_lens_cpu,
            encoder_out_cache_loc=self.encoder_out_cache_loc,
            lora_ids=[req.lora_id for req in self.reqs],
            sampling_info=self.sampling_info,
            input_embeds=self.input_embeds,
            token_type_ids=self.token_type_ids,
            spec_algorithm=self.spec_algorithm,
            spec_info=self.spec_info,
            hicache_consumer_index=self.hicache_consumer_index,
            capture_hidden_mode=(
                CaptureHiddenMode.FULL
                if self.return_hidden_states
                else (
                    getattr(
                        self.spec_info, &#34;capture_hidden_mode&#34;, CaptureHiddenMode.NULL
                    )
                    if self.spec_info
                    else CaptureHiddenMode.NULL
                )
            ),
            extend_input_logprob_token_ids=self.extend_input_logprob_token_ids,
            launch_done=self.launch_done,
        )

    def copy(self):
        # Only contain fields that will be used by process_batch_result
        return ScheduleBatch(
            reqs=self.reqs,
            model_config=self.model_config,
            forward_mode=self.forward_mode,
            out_cache_loc=self.out_cache_loc,
            return_logprob=self.return_logprob,
            decoding_reqs=self.decoding_reqs,
            spec_algorithm=self.spec_algorithm,
            global_num_tokens=self.global_num_tokens,
            global_num_tokens_for_logprob=self.global_num_tokens_for_logprob,
            can_run_dp_cuda_graph=self.can_run_dp_cuda_graph,
            is_extend_in_batch=self.is_extend_in_batch,
            is_prefill_only=self.is_prefill_only,
        )

    def _evict_tree_cache_if_needed(self, num_tokens: int):
        if isinstance(self.tree_cache, (SWAChunkCache, ChunkCache)):
            return

        if self.is_hybrid:
            full_available_size = self.token_to_kv_pool_allocator.full_available_size()
            swa_available_size = self.token_to_kv_pool_allocator.swa_available_size()

            if full_available_size &lt; num_tokens or swa_available_size &lt; num_tokens:
                if self.tree_cache is not None:
                    full_num_tokens = max(0, num_tokens - full_available_size)
                    swa_num_tokens = max(0, num_tokens - swa_available_size)
                    self.tree_cache.evict(full_num_tokens, swa_num_tokens)
        else:
            if self.token_to_kv_pool_allocator.available_size() &lt; num_tokens:
                if self.tree_cache is not None:
                    self.tree_cache.evict(num_tokens)

    def _is_available_size_sufficient(self, num_tokens: int) -&gt; bool:
        if self.is_hybrid:
            return (
                self.token_to_kv_pool_allocator.full_available_size() &gt;= num_tokens
                and self.token_to_kv_pool_allocator.swa_available_size() &gt;= num_tokens
            )
        else:
            return self.token_to_kv_pool_allocator.available_size() &gt;= num_tokens

    def _available_and_evictable_str(self) -&gt; str:
        if self.is_hybrid:
            full_available_size = self.token_to_kv_pool_allocator.full_available_size()
            swa_available_size = self.token_to_kv_pool_allocator.swa_available_size()
            full_evictable_size = self.tree_cache.full_evictable_size()
            swa_evictable_size = self.tree_cache.swa_evictable_size()
            return (
                f&#34;Available full tokens: {full_available_size + full_evictable_size} ({full_available_size=} + {full_evictable_size=})\n&#34;
                f&#34;Available swa tokens: {swa_available_size + swa_evictable_size} ({swa_available_size=} + {swa_evictable_size=})\n&#34;
                f&#34;Full LRU list evictable size: {self.tree_cache.full_lru_list_evictable_size()}\n&#34;
                f&#34;SWA LRU list evictable size: {self.tree_cache.swa_lru_list_evictable_size()}\n&#34;
            )
        else:
            available_size = self.token_to_kv_pool_allocator.available_size()
            evictable_size = self.tree_cache.evictable_size()
            return f&#34;Available tokens: {available_size + evictable_size} ({available_size=} + {evictable_size=})\n&#34;

    def __str__(self):
        return (
            f&#34;ScheduleBatch(forward_mode={self.forward_mode.name if self.forward_mode else &#39;None&#39;}, &#34;
            f&#34;#req={(len(self.reqs))})&#34;
        )</code></pre>
</details>
<div class="desc"><p>Store all information of a batch on the scheduler.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.disaggregation.decode_schedule_batch_mixin.ScheduleBatchDisaggregationDecodeMixin" href="../disaggregation/decode_schedule_batch_mixin.html#sglang.srt.disaggregation.decode_schedule_batch_mixin.ScheduleBatchDisaggregationDecodeMixin">ScheduleBatchDisaggregationDecodeMixin</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.init_new"><code class="name flex">
<span>def <span class="ident">init_new</span></span>(<span>reqs: List[<a title="sglang.srt.managers.schedule_batch.Req" href="#sglang.srt.managers.schedule_batch.Req">Req</a>],<br>req_to_token_pool: ReqToTokenPool,<br>token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,<br>tree_cache: BasePrefixCache,<br>model_config: ModelConfig,<br>enable_overlap: bool,<br>spec_algorithm: SpeculativeAlgorithm,<br>chunked_req: Optional[<a title="sglang.srt.managers.schedule_batch.Req" href="#sglang.srt.managers.schedule_batch.Req">Req</a>] = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.batch_is_full"><code class="name">var <span class="ident">batch_is_full</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.can_run_dp_cuda_graph"><code class="name">var <span class="ident">can_run_dp_cuda_graph</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.chunked_req"><code class="name">var <span class="ident">chunked_req</span> : Optional[<a title="sglang.srt.managers.schedule_batch.Req" href="#sglang.srt.managers.schedule_batch.Req">Req</a>]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.decoding_reqs"><code class="name">var <span class="ident">decoding_reqs</span> : List[<a title="sglang.srt.managers.schedule_batch.Req" href="#sglang.srt.managers.schedule_batch.Req">Req</a>]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.device"><code class="name">var <span class="ident">device</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.enable_overlap"><code class="name">var <span class="ident">enable_overlap</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.encoder_cached"><code class="name">var <span class="ident">encoder_cached</span> : Optional[List[bool]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.encoder_lens"><code class="name">var <span class="ident">encoder_lens</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.encoder_lens_cpu"><code class="name">var <span class="ident">encoder_lens_cpu</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.encoder_out_cache_loc"><code class="name">var <span class="ident">encoder_out_cache_loc</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.extend_input_logprob_token_ids"><code class="name">var <span class="ident">extend_input_logprob_token_ids</span> : Optional[torch.Tensor]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.extend_lens"><code class="name">var <span class="ident">extend_lens</span> : List[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.extend_logprob_start_lens"><code class="name">var <span class="ident">extend_logprob_start_lens</span> : List[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.extend_num_tokens"><code class="name">var <span class="ident">extend_num_tokens</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.forward_mode"><code class="name">var <span class="ident">forward_mode</span> : ForwardMode</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.global_forward_mode"><code class="name">var <span class="ident">global_forward_mode</span> : Optional[ForwardMode]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.global_num_tokens"><code class="name">var <span class="ident">global_num_tokens</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.global_num_tokens_for_logprob"><code class="name">var <span class="ident">global_num_tokens_for_logprob</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.has_grammar"><code class="name">var <span class="ident">has_grammar</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.has_stream"><code class="name">var <span class="ident">has_stream</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.hicache_consumer_index"><code class="name">var <span class="ident">hicache_consumer_index</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.input_embeds"><code class="name">var <span class="ident">input_embeds</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.input_ids"><code class="name">var <span class="ident">input_ids</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.is_extend_in_batch"><code class="name">var <span class="ident">is_extend_in_batch</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.is_hybrid"><code class="name">var <span class="ident">is_hybrid</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.is_prefill_only"><code class="name">var <span class="ident">is_prefill_only</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.launch_done"><code class="name">var <span class="ident">launch_done</span> : Optional[threading.Event]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.model_config"><code class="name">var <span class="ident">model_config</span> : ModelConfig</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.multimodal_inputs"><code class="name">var <span class="ident">multimodal_inputs</span> : Optional[List]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.next_batch_sampling_info"><code class="name">var <span class="ident">next_batch_sampling_info</span> : SamplingBatchInfo</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.orig_seq_lens"><code class="name">var <span class="ident">orig_seq_lens</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.out_cache_loc"><code class="name">var <span class="ident">out_cache_loc</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.output_ids"><code class="name">var <span class="ident">output_ids</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.prefix_lens"><code class="name">var <span class="ident">prefix_lens</span> : List[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.req_pool_indices"><code class="name">var <span class="ident">req_pool_indices</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.req_to_token_pool"><code class="name">var <span class="ident">req_to_token_pool</span> : ReqToTokenPool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.reqs"><code class="name">var <span class="ident">reqs</span> : List[<a title="sglang.srt.managers.schedule_batch.Req" href="#sglang.srt.managers.schedule_batch.Req">Req</a>]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.return_hidden_states"><code class="name">var <span class="ident">return_hidden_states</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.return_logprob"><code class="name">var <span class="ident">return_logprob</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.sampling_info"><code class="name">var <span class="ident">sampling_info</span> : SamplingBatchInfo</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.seq_lens"><code class="name">var <span class="ident">seq_lens</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.seq_lens_sum"><code class="name">var <span class="ident">seq_lens_sum</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.spec_algorithm"><code class="name">var <span class="ident">spec_algorithm</span> : SpeculativeAlgorithm</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.spec_info"><code class="name">var <span class="ident">spec_info</span> : Optional[Union[EagleDraftInput, EagleVerifyInput]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.tbo_split_seq_index"><code class="name">var <span class="ident">tbo_split_seq_index</span> : Optional[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.temp_scaled_logprobs"><code class="name">var <span class="ident">temp_scaled_logprobs</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.token_ids_logprobs"><code class="name">var <span class="ident">token_ids_logprobs</span> : Optional[List[List[int]]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.token_to_kv_pool_allocator"><code class="name">var <span class="ident">token_to_kv_pool_allocator</span> : BaseTokenToKVPoolAllocator</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.token_type_ids"><code class="name">var <span class="ident">token_type_ids</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.top_logprobs_nums"><code class="name">var <span class="ident">top_logprobs_nums</span> : Optional[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.top_p_normalized_logprobs"><code class="name">var <span class="ident">top_p_normalized_logprobs</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.tree_cache"><code class="name">var <span class="ident">tree_cache</span> : BasePrefixCache</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.alloc_paged_token_slots_decode"><code class="name flex">
<span>def <span class="ident">alloc_paged_token_slots_decode</span></span>(<span>self, seq_lens: torch.Tensor, last_loc: torch.Tensor, backup_state: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def alloc_paged_token_slots_decode(
    self,
    seq_lens: torch.Tensor,
    last_loc: torch.Tensor,
    backup_state: bool = False,
):
    # Over estimate the number of tokens: assume each request needs a new page.
    num_tokens = len(seq_lens) * self.token_to_kv_pool_allocator.page_size
    self._evict_tree_cache_if_needed(num_tokens)

    if backup_state:
        state = self.token_to_kv_pool_allocator.backup_state()

    out_cache_loc = self.token_to_kv_pool_allocator.alloc_decode(seq_lens, last_loc)
    if out_cache_loc is None:
        error_msg = (
            f&#34;Decode out of memory. Try to lower your batch size.\n&#34;
            f&#34;Try to allocate {len(seq_lens)} tokens.\n&#34;
            f&#34;{self._available_and_evictable_str()}&#34;
        )
        logger.error(error_msg)
        raise RuntimeError(error_msg)

    if backup_state:
        return out_cache_loc, state
    else:
        return out_cache_loc</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.alloc_paged_token_slots_extend"><code class="name flex">
<span>def <span class="ident">alloc_paged_token_slots_extend</span></span>(<span>self,<br>prefix_lens: torch.Tensor,<br>seq_lens: torch.Tensor,<br>last_loc: torch.Tensor,<br>extend_num_tokens: int,<br>backup_state: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def alloc_paged_token_slots_extend(
    self,
    prefix_lens: torch.Tensor,
    seq_lens: torch.Tensor,
    last_loc: torch.Tensor,
    extend_num_tokens: int,
    backup_state: bool = False,
):
    # Over estimate the number of tokens: assume each request needs a new page.
    num_tokens = (
        extend_num_tokens
        + len(seq_lens) * self.token_to_kv_pool_allocator.page_size
    )
    self._evict_tree_cache_if_needed(num_tokens)

    if backup_state:
        state = self.token_to_kv_pool_allocator.backup_state()

    out_cache_loc = self.token_to_kv_pool_allocator.alloc_extend(
        prefix_lens, seq_lens, last_loc, extend_num_tokens
    )
    if out_cache_loc is None:
        error_msg = (
            f&#34;Prefill out of memory. Try to lower your batch size.\n&#34;
            f&#34;Try to allocate {extend_num_tokens} tokens.\n&#34;
            f&#34;{self._available_and_evictable_str()}&#34;
        )
        logger.error(error_msg)
        raise RuntimeError(error_msg)

    if backup_state:
        return out_cache_loc, state
    else:
        return out_cache_loc</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.alloc_req_slots"><code class="name flex">
<span>def <span class="ident">alloc_req_slots</span></span>(<span>self, num_reqs: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def alloc_req_slots(self, num_reqs: int):
    req_pool_indices = self.req_to_token_pool.alloc(num_reqs)
    if req_pool_indices is None:
        raise RuntimeError(
            &#34;alloc_req_slots runs out of memory. &#34;
            &#34;Please set a smaller number for `--max-running-requests`. &#34;
            f&#34;{self.req_to_token_pool.available_size()=}, &#34;
            f&#34;{num_reqs=}, &#34;
        )
    return req_pool_indices</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.alloc_token_slots"><code class="name flex">
<span>def <span class="ident">alloc_token_slots</span></span>(<span>self, num_tokens: int, backup_state: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def alloc_token_slots(self, num_tokens: int, backup_state: bool = False):
    self._evict_tree_cache_if_needed(num_tokens)

    if backup_state:
        state = self.token_to_kv_pool_allocator.backup_state()

    out_cache_loc = self.token_to_kv_pool_allocator.alloc(num_tokens)
    if out_cache_loc is None:
        phase_str = &#34;Prefill&#34; if self.forward_mode.is_extend() else &#34;Decode&#34;
        error_msg = (
            f&#34;{phase_str} out of memory. Try to lower your batch size.\n&#34;
            f&#34;Try to allocate {num_tokens} tokens.\n&#34;
            f&#34;{self._available_and_evictable_str()}&#34;
        )
        logger.error(error_msg)
        if self.tree_cache is not None:
            self.tree_cache.pretty_print()
        raise RuntimeError(error_msg)

    if backup_state:
        return out_cache_loc, state
    else:
        return out_cache_loc</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.batch_size"><code class="name flex">
<span>def <span class="ident">batch_size</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch_size(self):
    return len(self.reqs)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.check_decode_mem"><code class="name flex">
<span>def <span class="ident">check_decode_mem</span></span>(<span>self, buf_multiplier=1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_decode_mem(self, buf_multiplier=1):
    num_tokens = (
        self.new_page_count_next_decode()
        * buf_multiplier
        * self.token_to_kv_pool_allocator.page_size
    )

    self._evict_tree_cache_if_needed(num_tokens)
    return self._is_available_size_sufficient(num_tokens)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.copy"><code class="name flex">
<span>def <span class="ident">copy</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy(self):
    # Only contain fields that will be used by process_batch_result
    return ScheduleBatch(
        reqs=self.reqs,
        model_config=self.model_config,
        forward_mode=self.forward_mode,
        out_cache_loc=self.out_cache_loc,
        return_logprob=self.return_logprob,
        decoding_reqs=self.decoding_reqs,
        spec_algorithm=self.spec_algorithm,
        global_num_tokens=self.global_num_tokens,
        global_num_tokens_for_logprob=self.global_num_tokens_for_logprob,
        can_run_dp_cuda_graph=self.can_run_dp_cuda_graph,
        is_extend_in_batch=self.is_extend_in_batch,
        is_prefill_only=self.is_prefill_only,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.filter_batch"><code class="name flex">
<span>def <span class="ident">filter_batch</span></span>(<span>self,<br>chunked_req_to_exclude: Optional[Union[<a title="sglang.srt.managers.schedule_batch.Req" href="#sglang.srt.managers.schedule_batch.Req">Req</a>, List[<a title="sglang.srt.managers.schedule_batch.Req" href="#sglang.srt.managers.schedule_batch.Req">Req</a>]]] = None,<br>keep_indices: Optional[List[int]] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_batch(
    self,
    chunked_req_to_exclude: Optional[Union[Req, List[Req]]] = None,
    keep_indices: Optional[List[int]] = None,
):
    if keep_indices is None:
        if isinstance(chunked_req_to_exclude, Req):
            chunked_req_to_exclude = [chunked_req_to_exclude]
        elif chunked_req_to_exclude is None:
            chunked_req_to_exclude = []
        keep_indices = [
            i
            for i in range(len(self.reqs))
            if not self.reqs[i].finished()
            and self.reqs[i] not in chunked_req_to_exclude
        ]

    if keep_indices is None or len(keep_indices) == 0:
        # Filter out all requests
        self.reqs = []
        return

    if len(keep_indices) == len(self.reqs):
        # No need to filter
        return

    keep_indices_device = torch.tensor(keep_indices, dtype=torch.int64).to(
        self.device, non_blocking=True
    )

    if self.model_config.is_encoder_decoder:
        self.encoder_lens = self.encoder_lens[keep_indices_device]
        self.encoder_lens_cpu = [self.encoder_lens_cpu[i] for i in keep_indices]

    self.reqs = [self.reqs[i] for i in keep_indices]
    if self.multimodal_inputs is not None:
        self.multimodal_inputs = [self.multimodal_inputs[i] for i in keep_indices]
    self.req_pool_indices = self.req_pool_indices[keep_indices_device]
    self.seq_lens = self.seq_lens[keep_indices_device]
    self.orig_seq_lens = self.orig_seq_lens[keep_indices_device]
    self.out_cache_loc = None
    self.seq_lens_sum = self.seq_lens.sum().item()
    self.output_ids = self.output_ids[keep_indices_device]
    self.return_logprob = any(req.return_logprob for req in self.reqs)
    if self.return_logprob:
        self.top_logprobs_nums = [self.top_logprobs_nums[i] for i in keep_indices]
        self.token_ids_logprobs = [self.token_ids_logprobs[i] for i in keep_indices]
    else:
        self.top_logprobs_nums = None
        self.token_ids_logprobs = None

    self.has_stream = any(req.stream for req in self.reqs)
    self.has_grammar = any(req.grammar for req in self.reqs)

    self.sampling_info.filter_batch(keep_indices, keep_indices_device)
    if self.spec_info:
        self.spec_info.filter_batch(keep_indices_device)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.get_model_worker_batch"><code class="name flex">
<span>def <span class="ident">get_model_worker_batch</span></span>(<span>self, seq_lens_cpu_cache: Optional[torch.Tensor] = None) ‑> <a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch">ModelWorkerBatch</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_model_worker_batch(
    self, seq_lens_cpu_cache: Optional[torch.Tensor] = None
) -&gt; ModelWorkerBatch:
    if self.forward_mode.is_decode_or_idle():
        extend_seq_lens = extend_prefix_lens = extend_logprob_start_lens = None
    else:
        extend_seq_lens = self.extend_lens
        extend_prefix_lens = self.prefix_lens
        extend_logprob_start_lens = self.extend_logprob_start_lens

    if self.sampling_info:
        if self.has_grammar:
            self.sampling_info.grammars = [req.grammar for req in self.reqs]
        else:
            self.sampling_info.grammars = None

    seq_lens_cpu = (
        seq_lens_cpu_cache
        if seq_lens_cpu_cache is not None
        else self.seq_lens.cpu()
    )

    global bid
    bid += 1
    return ModelWorkerBatch(
        bid=bid,
        forward_mode=self.forward_mode,
        input_ids=self.input_ids,
        req_pool_indices=self.req_pool_indices,
        seq_lens=self.seq_lens,
        orig_seq_lens=self.orig_seq_lens,
        out_cache_loc=self.out_cache_loc,
        seq_lens_cpu=seq_lens_cpu,
        seq_lens_sum=self.seq_lens_sum,
        return_logprob=self.return_logprob,
        top_logprobs_nums=self.top_logprobs_nums,
        token_ids_logprobs=self.token_ids_logprobs,
        global_num_tokens=self.global_num_tokens,
        global_num_tokens_for_logprob=self.global_num_tokens_for_logprob,
        is_extend_in_batch=self.is_extend_in_batch,
        can_run_dp_cuda_graph=self.can_run_dp_cuda_graph,
        tbo_split_seq_index=self.tbo_split_seq_index,
        global_forward_mode=self.global_forward_mode,
        extend_num_tokens=self.extend_num_tokens,
        extend_seq_lens=extend_seq_lens,
        extend_prefix_lens=extend_prefix_lens,
        extend_logprob_start_lens=extend_logprob_start_lens,
        multimodal_inputs=self.multimodal_inputs,
        encoder_cached=self.encoder_cached,
        encoder_lens=self.encoder_lens,
        encoder_lens_cpu=self.encoder_lens_cpu,
        encoder_out_cache_loc=self.encoder_out_cache_loc,
        lora_ids=[req.lora_id for req in self.reqs],
        sampling_info=self.sampling_info,
        input_embeds=self.input_embeds,
        token_type_ids=self.token_type_ids,
        spec_algorithm=self.spec_algorithm,
        spec_info=self.spec_info,
        hicache_consumer_index=self.hicache_consumer_index,
        capture_hidden_mode=(
            CaptureHiddenMode.FULL
            if self.return_hidden_states
            else (
                getattr(
                    self.spec_info, &#34;capture_hidden_mode&#34;, CaptureHiddenMode.NULL
                )
                if self.spec_info
                else CaptureHiddenMode.NULL
            )
        ),
        extend_input_logprob_token_ids=self.extend_input_logprob_token_ids,
        launch_done=self.launch_done,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.is_empty"><code class="name flex">
<span>def <span class="ident">is_empty</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_empty(self):
    return len(self.reqs) == 0</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.merge_batch"><code class="name flex">
<span>def <span class="ident">merge_batch</span></span>(<span>self,<br>other: "'<a title="sglang.srt.managers.schedule_batch.ScheduleBatch" href="#sglang.srt.managers.schedule_batch.ScheduleBatch">ScheduleBatch</a>'")</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_batch(self, other: &#34;ScheduleBatch&#34;):
    # Penalizer orchestrator must be merged before Batch.reqs is merged. This is because
    # orchestrator.merge() depends on Batch.reqs during preparation of each penalizers, so it
    # needs to be called with pre-merged Batch.reqs.
    self.sampling_info.merge_batch(other.sampling_info)

    # Encoder-decoder infos
    if self.model_config.is_encoder_decoder:
        self.encoder_lens = torch.cat([self.encoder_lens, other.encoder_lens])
        self.encoder_lens_cpu.extend(other.encoder_lens_cpu)
    self.req_pool_indices = torch.cat(
        [self.req_pool_indices, other.req_pool_indices]
    )
    self.seq_lens = torch.cat([self.seq_lens, other.seq_lens])
    self.orig_seq_lens = torch.cat([self.orig_seq_lens, other.orig_seq_lens])
    self.out_cache_loc = None
    self.seq_lens_sum += other.seq_lens_sum
    if self.output_ids is not None:
        self.output_ids = torch.cat([self.output_ids, other.output_ids])
    if self.return_logprob and other.return_logprob:
        self.top_logprobs_nums.extend(other.top_logprobs_nums)
        self.token_ids_logprobs.extend(other.token_ids_logprobs)
    elif self.return_logprob:
        self.top_logprobs_nums.extend([0] * len(other.reqs))
        self.token_ids_logprobs.extend([None] * len(other.reqs))
    elif other.return_logprob:
        self.top_logprobs_nums = [0] * len(self.reqs) + other.top_logprobs_nums
        self.token_ids_logprobs = [None] * len(self.reqs) + other.token_ids_logprobs
    self.reqs.extend(other.reqs)
    if self.multimodal_inputs is not None:
        self.multimodal_inputs.extend(other.multimodal_inputs)

    self.return_logprob |= other.return_logprob
    self.has_stream |= other.has_stream
    self.has_grammar |= other.has_grammar
    self.return_hidden_states |= other.return_hidden_states

    if self.spec_info:
        self.spec_info.merge_batch(other.spec_info)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.mix_with_running"><code class="name flex">
<span>def <span class="ident">mix_with_running</span></span>(<span>self,<br>running_batch: "'<a title="sglang.srt.managers.schedule_batch.ScheduleBatch" href="#sglang.srt.managers.schedule_batch.ScheduleBatch">ScheduleBatch</a>'")</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mix_with_running(self, running_batch: &#34;ScheduleBatch&#34;):
    self.forward_mode = ForwardMode.MIXED
    running_bs = running_batch.batch_size()

    for req in running_batch.reqs:
        req.fill_ids = req.origin_input_ids + req.output_ids
        req.extend_input_len = 1

    input_ids = torch.cat([self.input_ids, running_batch.input_ids])
    out_cache_loc = torch.cat([self.out_cache_loc, running_batch.out_cache_loc])

    self.merge_batch(running_batch)
    self.input_ids = input_ids
    self.out_cache_loc = out_cache_loc

    # For overlap scheduler, the output_ids has one step delay
    delta = 0 if self.enable_overlap else -1

    # NOTE: prefix_indices is what has been cached, but we don&#39;t cache each decode step
    self.prefix_lens.extend(
        [
            len(r.origin_input_ids) + len(r.output_ids) + delta
            for r in running_batch.reqs
        ]
    )
    self.extend_lens.extend([1] * running_bs)
    self.extend_num_tokens += running_bs
    # TODO (lianmin): Revisit this. It should be seq_len - 1
    self.extend_logprob_start_lens.extend([0] * running_bs)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.new_page_count_next_decode"><code class="name flex">
<span>def <span class="ident">new_page_count_next_decode</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def new_page_count_next_decode(self):
    page_size = self.token_to_kv_pool_allocator.page_size
    if page_size == 1:
        return len(self.reqs)
    # In the decoding phase, the length of a request&#39;s KV cache should be
    # the total length of the request minus 1
    return (
        sum(1 for req in self.reqs if req.seqlen % page_size == 0)
        if self.enable_overlap
        else sum(1 for req in self.reqs if (req.seqlen - 1) % page_size == 0)
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.prepare_encoder_info_decode"><code class="name flex">
<span>def <span class="ident">prepare_encoder_info_decode</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_encoder_info_decode(self):
    # Reset the encoder cached status
    self.encoder_cached = [True] * len(self.reqs)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.prepare_encoder_info_extend"><code class="name flex">
<span>def <span class="ident">prepare_encoder_info_extend</span></span>(<span>self, input_ids: List[int], seq_lens: List[int])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_encoder_info_extend(self, input_ids: List[int], seq_lens: List[int]):
    self.encoder_lens_cpu = []
    self.encoder_cached = []

    for req in self.reqs:
        im = req.multimodal_inputs
        if im is None or im.num_image_tokens is None:
            # No image input
            self.encoder_lens_cpu.append(0)
            self.encoder_cached.append(True)
        else:
            self.encoder_lens_cpu.append(im.num_image_tokens)
            self.encoder_cached.append(
                self.forward_mode.is_decode()
                or len(req.prefix_indices) &gt;= im.num_image_tokens
            )

    self.encoder_lens = torch.tensor(self.encoder_lens_cpu, dtype=torch.int64).to(
        self.device, non_blocking=True
    )

    # Strip encoder infos
    pt = 0
    decoder_out_cache_loc = []
    encoder_out_cache_loc = []
    for i, req in enumerate(self.reqs):
        encoder_len = self.encoder_lens_cpu[i]
        seq_lens[i] -= encoder_len

        if len(req.prefix_indices) &lt; encoder_len:
            # NOTE: the encoder part should be considered as a whole
            assert len(req.prefix_indices) == 0
            input_ids[i] = input_ids[i][encoder_len:]
            encoder_out_cache_loc.append(self.out_cache_loc[pt : pt + encoder_len])
            decoder_out_cache_loc.append(
                self.out_cache_loc[pt + encoder_len : pt + req.extend_input_len]
            )
            self.extend_lens[i] -= encoder_len
            self.extend_num_tokens -= encoder_len
        else:
            decoder_out_cache_loc.append(
                self.out_cache_loc[pt : pt + req.extend_input_len]
            )
            self.prefix_lens[i] -= encoder_len

        pt += req.extend_input_len

    # Reassign
    self.input_ids = torch.tensor(sum(input_ids, []), dtype=torch.int64).to(
        self.device, non_blocking=True
    )
    self.seq_lens = torch.tensor(seq_lens, dtype=torch.int64).to(
        self.device, non_blocking=True
    )

    if not decoder_out_cache_loc:
        self.out_cache_loc = torch.zeros(0, dtype=torch.int64).to(
            self.device, non_blocking=True
        )
    else:
        self.out_cache_loc = torch.cat(decoder_out_cache_loc)

    if not encoder_out_cache_loc:
        self.encoder_out_cache_loc = torch.zeros(0, dtype=torch.int64).to(
            self.device, non_blocking=True
        )
    else:
        self.encoder_out_cache_loc = torch.cat(encoder_out_cache_loc)

    assert (
        len(self.out_cache_loc) == self.extend_num_tokens
    ), f&#34;Expected {len(self.out_cache_loc)}, got {self.extend_num_tokens}&#34;</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.prepare_for_decode"><code class="name flex">
<span>def <span class="ident">prepare_for_decode</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_for_decode(self):
    self.forward_mode = ForwardMode.DECODE
    bs = len(self.reqs)

    if self.spec_algorithm.is_eagle():
        # if spec decoding is used, the decode batch is prepared inside
        # `forward_batch_speculative_generation` after running draft models.
        return

    if self.sampling_info.penalizer_orchestrator.is_required:
        if self.enable_overlap:
            # TODO: this can be slow, optimize this.
            delayed_output_ids = torch.tensor(
                [
                    (
                        req.output_ids[-1]
                        if len(req.output_ids)
                        else req.origin_input_ids[-1]
                    )
                    for req in self.reqs
                ],
                dtype=torch.int64,
                device=self.device,
            )
            self.sampling_info.penalizer_orchestrator.cumulate_output_tokens(
                delayed_output_ids
            )
        else:
            self.sampling_info.penalizer_orchestrator.cumulate_output_tokens(
                self.output_ids.to(torch.int64)
            )

    # Update fields
    self.input_ids = self.output_ids
    self.output_ids = None

    if self.model_config.is_encoder_decoder:
        locs = self.encoder_lens + self.seq_lens
        self.prepare_encoder_info_decode()
    else:
        locs = self.seq_lens.clone()

    if self.enable_overlap:
        # Do not use in-place operations in the overlap mode
        self.seq_lens = self.seq_lens + 1
        self.orig_seq_lens = self.orig_seq_lens + 1
    else:
        # A faster in-place version
        self.seq_lens.add_(1)
        self.orig_seq_lens.add_(1)
    self.seq_lens_sum += bs

    # free memory
    if isinstance(self.tree_cache, SWAChunkCache):
        for req in self.reqs:
            self.tree_cache.evict_swa(
                req, req.seqlen - 1, self.model_config.attention_chunk_size
            )

    # Allocate memory
    if self.token_to_kv_pool_allocator.page_size == 1:
        self.out_cache_loc = self.alloc_token_slots(bs)
    else:
        last_loc = self.req_to_token_pool.req_to_token[
            self.req_pool_indices, self.seq_lens - 2
        ]
        self.out_cache_loc = self.alloc_paged_token_slots_decode(
            self.seq_lens, last_loc
        )

    self.req_to_token_pool.write(
        (self.req_pool_indices, locs), self.out_cache_loc.to(torch.int32)
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.prepare_for_extend"><code class="name flex">
<span>def <span class="ident">prepare_for_extend</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_for_extend(self):
    self.forward_mode = ForwardMode.EXTEND

    # Allocate req slots
    bs = len(self.reqs)
    req_pool_indices = self.alloc_req_slots(bs)

    # Init tensors
    reqs = self.reqs
    input_ids = [r.fill_ids[len(r.prefix_indices) :] for r in reqs]
    extend_num_tokens = sum(len(ids) for ids in input_ids)
    seq_lens = [len(r.fill_ids) for r in reqs]
    orig_seq_lens = [max(len(r.fill_ids), len(r.origin_input_ids)) for r in reqs]
    prefix_lens = [len(r.prefix_indices) for r in reqs]
    extend_lens = [r.extend_input_len for r in reqs]

    token_type_ids = [
        r.token_type_ids for r in reqs if r.token_type_ids is not None
    ]

    req_pool_indices_tensor = torch.tensor(req_pool_indices, dtype=torch.int64).to(
        self.device, non_blocking=True
    )
    input_ids_tensor = torch.tensor(
        list(chain.from_iterable(input_ids)), dtype=torch.int64
    ).to(self.device, non_blocking=True)
    seq_lens_tensor = torch.tensor(seq_lens, dtype=torch.int64).to(
        self.device, non_blocking=True
    )
    orig_seq_lens_tensor = torch.tensor(orig_seq_lens, dtype=torch.int32).to(
        self.device, non_blocking=True
    )
    prefix_lens_tensor = torch.tensor(
        prefix_lens, dtype=torch.int64, device=self.device
    )

    token_type_ids_tensor = None
    if len(token_type_ids) &gt; 0:
        token_type_ids_tensor = torch.tensor(
            sum(token_type_ids, []), dtype=torch.int64
        ).to(self.device, non_blocking=True)

    extend_lens_tensor = seq_lens_tensor - prefix_lens_tensor

    # Copy prefix and do some basic check
    input_embeds = []
    extend_input_logprob_token_ids = []
    multimodal_inputs = []

    for i, (req, seq_len, pre_len) in enumerate(zip(reqs, seq_lens, prefix_lens)):
        req.req_pool_idx = req_pool_indices[i]
        assert seq_len - pre_len == req.extend_input_len

        if pre_len &gt; 0:
            self.req_to_token_pool.write(
                (req.req_pool_idx, slice(0, pre_len)), req.prefix_indices
            )
            if isinstance(self.tree_cache, SWAChunkCache):
                self.tree_cache.evict_swa(
                    req, pre_len, self.model_config.attention_chunk_size
                )

        # If input_embeds are available, store them
        if req.input_embeds is not None:
            # If req.input_embeds is already a list, append its content directly
            input_embeds.extend(req.input_embeds)  # Use extend to avoid nesting

        multimodal_inputs.append(req.multimodal_inputs)

        req.cached_tokens += pre_len - req.already_computed
        req.already_computed = seq_len
        req.is_retracted = False

        # Compute the relative logprob_start_len in an extend batch
        if req.logprob_start_len &gt;= pre_len:
            req.extend_logprob_start_len = min(
                req.logprob_start_len - pre_len,
                req.extend_input_len,
                req.seqlen - 1,
            )
        else:
            req.extend_logprob_start_len = 0

        if self.return_logprob:
            # Find input logprob token ids.
            # First, find a global index within origin_input_ids and slide it by 1
            # to compute input logprobs. It is because you need the next token
            # to compute input logprobs. E.g., (chunk size 2)
            #
            # input_logprobs = [1, 2, 3, 4]
            # fill_ids = [1, 2]
            # extend_input_logprob_token_id = [2, 3]
            #
            # Note that it can also overflow. In this case, we pad it with 0.
            # input_logprobs = [1, 2, 3, 4]
            # fill_ids = [3, 4]
            # extend_input_logprob_token_id = [4, 0]
            global_start_idx, global_end_idx = (
                len(req.prefix_indices),
                len(req.fill_ids),
            )
            # Apply logprob_start_len
            if global_start_idx &lt; req.logprob_start_len:
                global_start_idx = req.logprob_start_len

            logprob_token_ids = req.origin_input_ids[
                global_start_idx + 1 : global_end_idx + 1
            ]
            extend_input_logprob_token_ids.extend(logprob_token_ids)

            # We will need req.extend_input_len - req.extend_logprob_start_len number of
            # tokens, and logprob_token_ids is for input logprob, so pad the rest of them by 0.
            extend_input_logprob_token_ids.extend(
                [0]
                * (
                    req.extend_input_len
                    - req.extend_logprob_start_len
                    - len(logprob_token_ids)
                )
            )

    if self.return_logprob:
        extend_input_logprob_token_ids = torch.tensor(
            extend_input_logprob_token_ids
        )
    else:
        extend_input_logprob_token_ids = None

    # Allocate memory
    if self.token_to_kv_pool_allocator.page_size == 1:
        out_cache_loc = self.alloc_token_slots(extend_num_tokens)
    else:
        last_loc = get_last_loc(
            self.req_to_token_pool.req_to_token,
            req_pool_indices_tensor,
            prefix_lens_tensor,
        )
        out_cache_loc = self.alloc_paged_token_slots_extend(
            prefix_lens_tensor, seq_lens_tensor, last_loc, extend_num_tokens
        )

    # Set fields
    self.input_ids = input_ids_tensor
    self.req_pool_indices = req_pool_indices_tensor
    self.seq_lens = seq_lens_tensor
    self.orig_seq_lens = orig_seq_lens_tensor
    self.out_cache_loc = out_cache_loc
    self.input_embeds = (
        torch.tensor(input_embeds).to(self.device, non_blocking=True)
        if input_embeds
        else None
    )
    for mm_input in multimodal_inputs:
        if mm_input is None:
            continue
        for mm_item in mm_input.mm_items:
            pixel_values = getattr(mm_item, &#34;feature&#34;, None)
            if isinstance(pixel_values, torch.Tensor):
                mm_item.feature = pixel_values.to(self.device, non_blocking=True)
    self.multimodal_inputs = multimodal_inputs
    self.token_type_ids = token_type_ids_tensor
    self.seq_lens_sum = sum(seq_lens)

    if self.return_logprob:
        self.top_logprobs_nums = [r.top_logprobs_num for r in reqs]
        self.token_ids_logprobs = [r.token_ids_logprob for r in reqs]

    self.extend_logprob_start_lens = [r.extend_logprob_start_len for r in reqs]
    self.extend_num_tokens = extend_num_tokens
    self.prefix_lens = prefix_lens
    self.extend_lens = extend_lens
    self.extend_input_logprob_token_ids = extend_input_logprob_token_ids

    # Write to req_to_token_pool
    if support_triton(global_server_args_dict.get(&#34;attention_backend&#34;)):
        # TODO: some tensors can be reused for ForwardBatchInfo (e.g., extend_lens, cumsum_start)

        write_req_to_token_pool_triton[(bs,)](
            self.req_to_token_pool.req_to_token,
            req_pool_indices_tensor,
            prefix_lens_tensor,
            seq_lens_tensor,
            extend_lens_tensor,
            out_cache_loc,
            self.req_to_token_pool.req_to_token.shape[1],
        )
    else:
        pt = 0
        for i in range(bs):
            self.req_to_token_pool.write(
                (req_pool_indices[i], slice(prefix_lens[i], seq_lens[i])),
                out_cache_loc[pt : pt + extend_lens[i]],
            )
            pt += extend_lens[i]

    if self.model_config.is_encoder_decoder:
        self.prepare_encoder_info_extend(input_ids, seq_lens)

    # Build sampling info
    self.sampling_info = SamplingBatchInfo.from_schedule_batch(
        self,
        self.model_config.vocab_size,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.prepare_for_idle"><code class="name flex">
<span>def <span class="ident">prepare_for_idle</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_for_idle(self):
    self.forward_mode = ForwardMode.IDLE
    self.input_ids = torch.empty(0, dtype=torch.int64, device=self.device)
    self.seq_lens = torch.empty(0, dtype=torch.int64, device=self.device)
    self.orig_seq_lens = torch.empty(0, dtype=torch.int32, device=self.device)
    self.out_cache_loc = torch.empty(0, dtype=torch.int64, device=self.device)
    self.req_pool_indices = torch.empty(0, dtype=torch.int32, device=self.device)
    self.seq_lens_sum = 0
    self.extend_num_tokens = 0
    self.sampling_info = SamplingBatchInfo.from_schedule_batch(
        self,
        self.model_config.vocab_size,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.prepare_for_split_prefill"><code class="name flex">
<span>def <span class="ident">prepare_for_split_prefill</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_for_split_prefill(self):
    self.prepare_for_extend()
    # For split prefill, we need to set the forward mode to SPLIT_PREFILL
    self.forward_mode = ForwardMode.SPLIT_PREFILL</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.schedule_batch.ScheduleBatch.retract_decode"><code class="name flex">
<span>def <span class="ident">retract_decode</span></span>(<span>self, server_args: ServerArgs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def retract_decode(self, server_args: ServerArgs):
    &#34;&#34;&#34;Retract the decoding requests when there is not enough memory.&#34;&#34;&#34;
    sorted_indices = list(range(len(self.reqs)))

    # TODO(lsyin): improve retraction policy for radix cache
    # For spec decoding, filter_batch API can only filter
    # requests from the back, so we can only retract from the back.
    # TODO(sang): Clean up finish path and support better retract
    # policy.
    if not server_args.speculative_algorithm:
        sorted_indices.sort(
            key=lambda i: (
                len(self.reqs[i].output_ids),
                -len(self.reqs[i].origin_input_ids),
            ),
            reverse=True,
        )

    def get_required_tokens(num_reqs: int):
        headroom_for_spec_decode = 0
        if server_args.speculative_algorithm:
            headroom_for_spec_decode += (
                num_reqs
                * server_args.speculative_eagle_topk
                * server_args.speculative_num_steps
                + num_reqs * server_args.speculative_num_draft_tokens
            )
        return (
            num_reqs * global_config.retract_decode_steps + headroom_for_spec_decode
        )

    def _get_available_size():
        if self.is_hybrid:
            return min(
                self.token_to_kv_pool_allocator.full_available_size(),
                self.token_to_kv_pool_allocator.swa_available_size(),
            )
        else:
            return self.token_to_kv_pool_allocator.available_size()

    retracted_reqs = []
    seq_lens_cpu = self.seq_lens.cpu().numpy()
    first_iter = True
    while (
        _get_available_size() &lt; get_required_tokens(len(sorted_indices))
        or first_iter
    ):
        if len(sorted_indices) == 1:
            # Corner case: only one request left
            if self.is_hybrid:
                full_available_size = (
                    self.token_to_kv_pool_allocator.full_available_size()
                )
                swa_available_size = (
                    self.token_to_kv_pool_allocator.swa_available_size()
                )
                assert (
                    full_available_size &gt; 0 and swa_available_size &gt; 0
                ), f&#34;No space left for only one request in SWA mode {full_available_size=}, {swa_available_size=}&#34;
            else:
                assert (
                    self.token_to_kv_pool_allocator.available_size() &gt; 0
                ), f&#34;No space left for only one request, {self.token_to_kv_pool_allocator.available_size()=}&#34;
            break

        first_iter = False
        idx = sorted_indices.pop()
        req = self.reqs[idx]
        retracted_reqs.append(req)

        if server_args.disaggregation_mode == &#34;decode&#34;:
            req.offload_kv_cache(
                self.req_to_token_pool, self.token_to_kv_pool_allocator
            )

        if isinstance(self.tree_cache, ChunkCache):
            # ChunkCache does not have eviction
            token_indices = self.req_to_token_pool.req_to_token[
                req.req_pool_idx, : seq_lens_cpu[idx]
            ]
            self.token_to_kv_pool_allocator.free(token_indices)
            self.req_to_token_pool.free(req.req_pool_idx)
        else:
            # TODO: apply more fine-grained retraction
            last_uncached_pos = (
                len(req.prefix_indices) // server_args.page_size
            ) * server_args.page_size
            token_indices = self.req_to_token_pool.req_to_token[
                req.req_pool_idx, last_uncached_pos : seq_lens_cpu[idx]
            ]
            self.token_to_kv_pool_allocator.free(token_indices)
            self.req_to_token_pool.free(req.req_pool_idx)

            # release the last node
            if self.is_hybrid:
                self.tree_cache.dec_lock_ref(req.last_node, req.swa_uuid_for_lock)
            else:
                self.tree_cache.dec_lock_ref(req.last_node)

            # NOTE(lsyin): we should use the newly evictable memory instantly.
            num_tokens = len(sorted_indices) * global_config.retract_decode_steps
            self._evict_tree_cache_if_needed(num_tokens)

        req.reset_for_retract()

        if len(retracted_reqs) == 0:
            # Corner case: only one request left
            raise ValueError(
                &#34;Failed to retract any request. No space left for only one request.&#34;
            )

    self.filter_batch(keep_indices=sorted_indices)

    # Reqs in batch are filtered
    total_decoded_tokens = sum(len(r.output_ids) for r in self.reqs)
    total_max_new_tokens = sum(r.sampling_params.max_new_tokens for r in self.reqs)

    new_estimate_ratio = (
        total_decoded_tokens + global_config.retract_decode_steps * len(self.reqs)
    ) / total_max_new_tokens
    new_estimate_ratio = min(1.0, new_estimate_ratio)

    return retracted_reqs, new_estimate_ratio</code></pre>
</details>
<div class="desc"><p>Retract the decoding requests when there is not enough memory.</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.disaggregation.decode_schedule_batch_mixin.ScheduleBatchDisaggregationDecodeMixin" href="../disaggregation/decode_schedule_batch_mixin.html#sglang.srt.disaggregation.decode_schedule_batch_mixin.ScheduleBatchDisaggregationDecodeMixin">ScheduleBatchDisaggregationDecodeMixin</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.disaggregation.decode_schedule_batch_mixin.ScheduleBatchDisaggregationDecodeMixin.prepare_for_prebuilt_extend" href="../disaggregation/decode_schedule_batch_mixin.html#sglang.srt.disaggregation.decode_schedule_batch_mixin.ScheduleBatchDisaggregationDecodeMixin.prepare_for_prebuilt_extend">prepare_for_prebuilt_extend</a></code></li>
<li><code><a title="sglang.srt.disaggregation.decode_schedule_batch_mixin.ScheduleBatchDisaggregationDecodeMixin.process_prebuilt_extend" href="../disaggregation/decode_schedule_batch_mixin.html#sglang.srt.disaggregation.decode_schedule_batch_mixin.ScheduleBatchDisaggregationDecodeMixin.process_prebuilt_extend">process_prebuilt_extend</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.managers" href="index.html">sglang.srt.managers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.managers.schedule_batch.get_last_loc" href="#sglang.srt.managers.schedule_batch.get_last_loc">get_last_loc</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.get_last_loc_torch" href="#sglang.srt.managers.schedule_batch.get_last_loc_torch">get_last_loc_torch</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.get_last_loc_triton" href="#sglang.srt.managers.schedule_batch.get_last_loc_triton">get_last_loc_triton</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.managers.schedule_batch.BaseFinishReason" href="#sglang.srt.managers.schedule_batch.BaseFinishReason">BaseFinishReason</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.schedule_batch.BaseFinishReason.to_json" href="#sglang.srt.managers.schedule_batch.BaseFinishReason.to_json">to_json</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.managers.schedule_batch.FINISH_ABORT" href="#sglang.srt.managers.schedule_batch.FINISH_ABORT">FINISH_ABORT</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.schedule_batch.FINISH_ABORT.to_json" href="#sglang.srt.managers.schedule_batch.FINISH_ABORT.to_json">to_json</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.managers.schedule_batch.FINISH_LENGTH" href="#sglang.srt.managers.schedule_batch.FINISH_LENGTH">FINISH_LENGTH</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.schedule_batch.FINISH_LENGTH.to_json" href="#sglang.srt.managers.schedule_batch.FINISH_LENGTH.to_json">to_json</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.managers.schedule_batch.FINISH_MATCHED_STR" href="#sglang.srt.managers.schedule_batch.FINISH_MATCHED_STR">FINISH_MATCHED_STR</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.schedule_batch.FINISH_MATCHED_STR.to_json" href="#sglang.srt.managers.schedule_batch.FINISH_MATCHED_STR.to_json">to_json</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.managers.schedule_batch.FINISH_MATCHED_TOKEN" href="#sglang.srt.managers.schedule_batch.FINISH_MATCHED_TOKEN">FINISH_MATCHED_TOKEN</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.schedule_batch.FINISH_MATCHED_TOKEN.to_json" href="#sglang.srt.managers.schedule_batch.FINISH_MATCHED_TOKEN.to_json">to_json</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.managers.schedule_batch.Modality" href="#sglang.srt.managers.schedule_batch.Modality">Modality</a></code></h4>
<ul class="two-column">
<li><code><a title="sglang.srt.managers.schedule_batch.Modality.AUDIO" href="#sglang.srt.managers.schedule_batch.Modality.AUDIO">AUDIO</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.Modality.IMAGE" href="#sglang.srt.managers.schedule_batch.Modality.IMAGE">IMAGE</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.Modality.MULTI_IMAGES" href="#sglang.srt.managers.schedule_batch.Modality.MULTI_IMAGES">MULTI_IMAGES</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.Modality.VIDEO" href="#sglang.srt.managers.schedule_batch.Modality.VIDEO">VIDEO</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.Modality.all" href="#sglang.srt.managers.schedule_batch.Modality.all">all</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.Modality.from_str" href="#sglang.srt.managers.schedule_batch.Modality.from_str">from_str</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch">ModelWorkerBatch</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.bid" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.bid">bid</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.can_run_dp_cuda_graph" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.can_run_dp_cuda_graph">can_run_dp_cuda_graph</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.capture_hidden_mode" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.capture_hidden_mode">capture_hidden_mode</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.encoder_cached" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.encoder_cached">encoder_cached</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.encoder_lens" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.encoder_lens">encoder_lens</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.encoder_lens_cpu" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.encoder_lens_cpu">encoder_lens_cpu</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.encoder_out_cache_loc" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.encoder_out_cache_loc">encoder_out_cache_loc</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.extend_input_logprob_token_ids" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.extend_input_logprob_token_ids">extend_input_logprob_token_ids</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.extend_logprob_start_lens" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.extend_logprob_start_lens">extend_logprob_start_lens</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.extend_num_tokens" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.extend_num_tokens">extend_num_tokens</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.extend_prefix_lens" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.extend_prefix_lens">extend_prefix_lens</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.extend_seq_lens" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.extend_seq_lens">extend_seq_lens</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.forward_mode" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.forward_mode">forward_mode</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.global_forward_mode" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.global_forward_mode">global_forward_mode</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.global_num_tokens" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.global_num_tokens">global_num_tokens</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.global_num_tokens_for_logprob" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.global_num_tokens_for_logprob">global_num_tokens_for_logprob</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.hicache_consumer_index" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.hicache_consumer_index">hicache_consumer_index</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.input_embeds" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.input_embeds">input_embeds</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.input_ids" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.input_ids">input_ids</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.is_extend_in_batch" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.is_extend_in_batch">is_extend_in_batch</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.launch_done" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.launch_done">launch_done</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.lora_ids" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.lora_ids">lora_ids</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.multimodal_inputs" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.multimodal_inputs">multimodal_inputs</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.orig_seq_lens" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.orig_seq_lens">orig_seq_lens</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.out_cache_loc" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.out_cache_loc">out_cache_loc</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.req_pool_indices" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.req_pool_indices">req_pool_indices</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.return_logprob" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.return_logprob">return_logprob</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.sampling_info" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.sampling_info">sampling_info</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.seq_lens" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.seq_lens">seq_lens</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.seq_lens_cpu" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.seq_lens_cpu">seq_lens_cpu</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.seq_lens_sum" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.seq_lens_sum">seq_lens_sum</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.spec_algorithm" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.spec_algorithm">spec_algorithm</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.spec_info" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.spec_info">spec_info</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.tbo_split_seq_index" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.tbo_split_seq_index">tbo_split_seq_index</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.token_ids_logprobs" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.token_ids_logprobs">token_ids_logprobs</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.token_type_ids" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.token_type_ids">token_type_ids</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ModelWorkerBatch.top_logprobs_nums" href="#sglang.srt.managers.schedule_batch.ModelWorkerBatch.top_logprobs_nums">top_logprobs_nums</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.managers.schedule_batch.MultimodalDataItem" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem">MultimodalDataItem</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalDataItem.feature" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem.feature">feature</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalDataItem.from_dict" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem.from_dict">from_dict</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalDataItem.hash" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem.hash">hash</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalDataItem.is_audio" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem.is_audio">is_audio</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalDataItem.is_empty_list" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem.is_empty_list">is_empty_list</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalDataItem.is_image" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem.is_image">is_image</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalDataItem.is_modality" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem.is_modality">is_modality</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalDataItem.is_valid" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem.is_valid">is_valid</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalDataItem.is_video" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem.is_video">is_video</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalDataItem.merge" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem.merge">merge</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalDataItem.modality" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem.modality">modality</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalDataItem.model_specific_data" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem.model_specific_data">model_specific_data</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalDataItem.offsets" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem.offsets">offsets</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalDataItem.pad_value" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem.pad_value">pad_value</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalDataItem.precomputed_embeddings" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem.precomputed_embeddings">precomputed_embeddings</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalDataItem.set" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem.set">set</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalDataItem.set_pad_value" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem.set_pad_value">set_pad_value</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalDataItem.validate" href="#sglang.srt.managers.schedule_batch.MultimodalDataItem.validate">validate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs" href="#sglang.srt.managers.schedule_batch.MultimodalInputs">MultimodalInputs</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.audio_end_id" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.audio_end_id">audio_end_id</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.audio_start_id" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.audio_start_id">audio_start_id</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.audio_token_id" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.audio_token_id">audio_token_id</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.contains_audio_inputs" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.contains_audio_inputs">contains_audio_inputs</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.contains_image_inputs" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.contains_image_inputs">contains_image_inputs</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.contains_mm_input" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.contains_mm_input">contains_mm_input</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.contains_video_inputs" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.contains_video_inputs">contains_video_inputs</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.from_dict" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.from_dict">from_dict</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.im_end_id" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.im_end_id">im_end_id</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.im_start_id" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.im_start_id">im_start_id</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.im_token_id" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.im_token_id">im_token_id</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.image_pad_len" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.image_pad_len">image_pad_len</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.merge" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.merge">merge</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.mm_items" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.mm_items">mm_items</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.mrope_position_delta" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.mrope_position_delta">mrope_position_delta</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.mrope_positions" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.mrope_positions">mrope_positions</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.num_image_tokens" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.num_image_tokens">num_image_tokens</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.slice_end_id" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.slice_end_id">slice_end_id</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.slice_start_id" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.slice_start_id">slice_start_id</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.MultimodalInputs.video_token_id" href="#sglang.srt.managers.schedule_batch.MultimodalInputs.video_token_id">video_token_id</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.managers.schedule_batch.Req" href="#sglang.srt.managers.schedule_batch.Req">Req</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.schedule_batch.Req.adjust_max_prefix_ids" href="#sglang.srt.managers.schedule_batch.Req.adjust_max_prefix_ids">adjust_max_prefix_ids</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.Req.check_finished" href="#sglang.srt.managers.schedule_batch.Req.check_finished">check_finished</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.Req.extend_image_inputs" href="#sglang.srt.managers.schedule_batch.Req.extend_image_inputs">extend_image_inputs</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.Req.finished" href="#sglang.srt.managers.schedule_batch.Req.finished">finished</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.Req.init_incremental_detokenize" href="#sglang.srt.managers.schedule_batch.Req.init_incremental_detokenize">init_incremental_detokenize</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.Req.init_next_round_input" href="#sglang.srt.managers.schedule_batch.Req.init_next_round_input">init_next_round_input</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.Req.load_kv_cache" href="#sglang.srt.managers.schedule_batch.Req.load_kv_cache">load_kv_cache</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.Req.log_time_stats" href="#sglang.srt.managers.schedule_batch.Req.log_time_stats">log_time_stats</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.Req.offload_kv_cache" href="#sglang.srt.managers.schedule_batch.Req.offload_kv_cache">offload_kv_cache</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.Req.reset_for_retract" href="#sglang.srt.managers.schedule_batch.Req.reset_for_retract">reset_for_retract</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.Req.seqlen" href="#sglang.srt.managers.schedule_batch.Req.seqlen">seqlen</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.Req.set_finish_with_abort" href="#sglang.srt.managers.schedule_batch.Req.set_finish_with_abort">set_finish_with_abort</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch" href="#sglang.srt.managers.schedule_batch.ScheduleBatch">ScheduleBatch</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.alloc_paged_token_slots_decode" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.alloc_paged_token_slots_decode">alloc_paged_token_slots_decode</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.alloc_paged_token_slots_extend" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.alloc_paged_token_slots_extend">alloc_paged_token_slots_extend</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.alloc_req_slots" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.alloc_req_slots">alloc_req_slots</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.alloc_token_slots" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.alloc_token_slots">alloc_token_slots</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.batch_is_full" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.batch_is_full">batch_is_full</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.batch_size" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.batch_size">batch_size</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.can_run_dp_cuda_graph" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.can_run_dp_cuda_graph">can_run_dp_cuda_graph</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.check_decode_mem" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.check_decode_mem">check_decode_mem</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.chunked_req" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.chunked_req">chunked_req</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.copy" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.copy">copy</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.decoding_reqs" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.decoding_reqs">decoding_reqs</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.device" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.device">device</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.enable_overlap" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.enable_overlap">enable_overlap</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.encoder_cached" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.encoder_cached">encoder_cached</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.encoder_lens" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.encoder_lens">encoder_lens</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.encoder_lens_cpu" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.encoder_lens_cpu">encoder_lens_cpu</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.encoder_out_cache_loc" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.encoder_out_cache_loc">encoder_out_cache_loc</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.extend_input_logprob_token_ids" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.extend_input_logprob_token_ids">extend_input_logprob_token_ids</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.extend_lens" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.extend_lens">extend_lens</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.extend_logprob_start_lens" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.extend_logprob_start_lens">extend_logprob_start_lens</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.extend_num_tokens" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.extend_num_tokens">extend_num_tokens</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.filter_batch" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.filter_batch">filter_batch</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.forward_mode" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.forward_mode">forward_mode</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.get_model_worker_batch" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.get_model_worker_batch">get_model_worker_batch</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.global_forward_mode" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.global_forward_mode">global_forward_mode</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.global_num_tokens" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.global_num_tokens">global_num_tokens</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.global_num_tokens_for_logprob" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.global_num_tokens_for_logprob">global_num_tokens_for_logprob</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.has_grammar" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.has_grammar">has_grammar</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.has_stream" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.has_stream">has_stream</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.hicache_consumer_index" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.hicache_consumer_index">hicache_consumer_index</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.init_new" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.init_new">init_new</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.input_embeds" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.input_embeds">input_embeds</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.input_ids" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.input_ids">input_ids</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.is_empty" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.is_empty">is_empty</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.is_extend_in_batch" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.is_extend_in_batch">is_extend_in_batch</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.is_hybrid" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.is_hybrid">is_hybrid</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.is_prefill_only" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.is_prefill_only">is_prefill_only</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.launch_done" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.launch_done">launch_done</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.merge_batch" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.merge_batch">merge_batch</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.mix_with_running" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.mix_with_running">mix_with_running</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.model_config" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.model_config">model_config</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.multimodal_inputs" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.multimodal_inputs">multimodal_inputs</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.new_page_count_next_decode" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.new_page_count_next_decode">new_page_count_next_decode</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.next_batch_sampling_info" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.next_batch_sampling_info">next_batch_sampling_info</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.orig_seq_lens" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.orig_seq_lens">orig_seq_lens</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.out_cache_loc" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.out_cache_loc">out_cache_loc</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.output_ids" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.output_ids">output_ids</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.prefix_lens" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.prefix_lens">prefix_lens</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.prepare_encoder_info_decode" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.prepare_encoder_info_decode">prepare_encoder_info_decode</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.prepare_encoder_info_extend" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.prepare_encoder_info_extend">prepare_encoder_info_extend</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.prepare_for_decode" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.prepare_for_decode">prepare_for_decode</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.prepare_for_extend" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.prepare_for_extend">prepare_for_extend</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.prepare_for_idle" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.prepare_for_idle">prepare_for_idle</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.prepare_for_split_prefill" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.prepare_for_split_prefill">prepare_for_split_prefill</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.req_pool_indices" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.req_pool_indices">req_pool_indices</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.req_to_token_pool" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.req_to_token_pool">req_to_token_pool</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.reqs" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.reqs">reqs</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.retract_decode" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.retract_decode">retract_decode</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.return_hidden_states" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.return_hidden_states">return_hidden_states</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.return_logprob" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.return_logprob">return_logprob</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.sampling_info" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.sampling_info">sampling_info</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.seq_lens" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.seq_lens">seq_lens</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.seq_lens_sum" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.seq_lens_sum">seq_lens_sum</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.spec_algorithm" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.spec_algorithm">spec_algorithm</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.spec_info" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.spec_info">spec_info</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.tbo_split_seq_index" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.tbo_split_seq_index">tbo_split_seq_index</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.temp_scaled_logprobs" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.temp_scaled_logprobs">temp_scaled_logprobs</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.token_ids_logprobs" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.token_ids_logprobs">token_ids_logprobs</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.token_to_kv_pool_allocator" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.token_to_kv_pool_allocator">token_to_kv_pool_allocator</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.token_type_ids" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.token_type_ids">token_type_ids</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.top_logprobs_nums" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.top_logprobs_nums">top_logprobs_nums</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.top_p_normalized_logprobs" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.top_p_normalized_logprobs">top_p_normalized_logprobs</a></code></li>
<li><code><a title="sglang.srt.managers.schedule_batch.ScheduleBatch.tree_cache" href="#sglang.srt.managers.schedule_batch.ScheduleBatch.tree_cache">tree_cache</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
