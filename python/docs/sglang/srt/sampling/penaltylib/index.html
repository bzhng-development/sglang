<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.sampling.penaltylib API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.sampling.penaltylib</code></h1>
</header>
<section id="section-intro">
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="sglang.srt.sampling.penaltylib.frequency_penalty" href="frequency_penalty.html">sglang.srt.sampling.penaltylib.frequency_penalty</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="sglang.srt.sampling.penaltylib.min_new_tokens" href="min_new_tokens.html">sglang.srt.sampling.penaltylib.min_new_tokens</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="sglang.srt.sampling.penaltylib.orchestrator" href="orchestrator.html">sglang.srt.sampling.penaltylib.orchestrator</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="sglang.srt.sampling.penaltylib.presence_penalty" href="presence_penalty.html">sglang.srt.sampling.penaltylib.presence_penalty</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.sampling.penaltylib.BatchedFrequencyPenalizer"><code class="flex name class">
<span>class <span class="ident">BatchedFrequencyPenalizer</span></span>
<span>(</span><span>orchestrator:Â <a title="sglang.srt.sampling.penaltylib.orchestrator.BatchedPenalizerOrchestrator" href="orchestrator.html#sglang.srt.sampling.penaltylib.orchestrator.BatchedPenalizerOrchestrator">BatchedPenalizerOrchestrator</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BatchedFrequencyPenalizer(_BatchedPenalizer):
    &#34;&#34;&#34;
    Frequency penalizer penalizes tokens based on their frequency in the output.
    &#34;&#34;&#34;

    def __init__(self, orchestrator: BatchedPenalizerOrchestrator):
        self.orchestrator = orchestrator
        self._is_prepared = False

    def _is_required(self) -&gt; bool:
        return any(
            req.sampling_params.frequency_penalty != 0.0
            for req in self.orchestrator.reqs()
        )

    def _prepare(self):
        self.cumulated_frequency_penalties = torch.zeros(
            (len(self.orchestrator.reqs()), self.orchestrator.vocab_size),
            dtype=torch.float32,
            device=self.orchestrator.device,
        )

        self.frequency_penalties = (
            torch.tensor(
                data=[
                    req.sampling_params.frequency_penalty
                    for req in self.orchestrator.reqs()
                ],
                dtype=torch.float32,
                device=self.orchestrator.device,
            )
        ).unsqueeze_(1)

    def _cumulate_output_tokens(self, output_ids: torch.Tensor):
        self.cumulated_frequency_penalties.scatter_add_(
            dim=1,
            index=output_ids.unsqueeze(1),
            src=self.frequency_penalties,
        )

    def _apply(self, logits: torch.Tensor) -&gt; torch.Tensor:
        logits.sub_(self.cumulated_frequency_penalties)

    def _filter(self, keep_indices: torch.Tensor):
        self.frequency_penalties = self.frequency_penalties[keep_indices]
        self.cumulated_frequency_penalties = self.cumulated_frequency_penalties[
            keep_indices
        ]

    def _merge(self, their: &#34;BatchedFrequencyPenalizer&#34;):
        self.frequency_penalties = torch.cat(
            [self.frequency_penalties, their.frequency_penalties], dim=0
        )
        self.cumulated_frequency_penalties = torch.cat(
            [self.cumulated_frequency_penalties, their.cumulated_frequency_penalties],
            dim=0,
        )</code></pre>
</details>
<div class="desc"><p>Frequency penalizer penalizes tokens based on their frequency in the output.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sglang.srt.sampling.penaltylib.orchestrator._BatchedPenalizer</li>
<li>abc.ABC</li>
</ul>
</dd>
<dt id="sglang.srt.sampling.penaltylib.BatchedMinNewTokensPenalizer"><code class="flex name class">
<span>class <span class="ident">BatchedMinNewTokensPenalizer</span></span>
<span>(</span><span>orchestrator:Â <a title="sglang.srt.sampling.penaltylib.orchestrator.BatchedPenalizerOrchestrator" href="orchestrator.html#sglang.srt.sampling.penaltylib.orchestrator.BatchedPenalizerOrchestrator">BatchedPenalizerOrchestrator</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BatchedMinNewTokensPenalizer(_BatchedPenalizer):
    &#34;&#34;&#34;
    Min new tokens penalizer penalizes tokens based on the length of the output.
    &#34;&#34;&#34;

    def __init__(self, orchestrator: BatchedPenalizerOrchestrator):
        self.orchestrator = orchestrator
        self._is_prepared = False

    def _is_required(self) -&gt; bool:
        return any(
            req.sampling_params.min_new_tokens &gt; 0 for req in self.orchestrator.reqs()
        )

    def _prepare(self):
        self.min_new_tokens = torch.tensor(
            data=[
                req.sampling_params.min_new_tokens for req in self.orchestrator.reqs()
            ],
            dtype=torch.int32,
            device=self.orchestrator.device,
        ).unsqueeze_(1)

        padded_stop_token_ids = torch.nn.utils.rnn.pad_sequence(
            sequences=[
                torch.tensor(
                    data=(
                        list(
                            (req.sampling_params.stop_token_ids or set())
                            | (req.tokenizer.additional_stop_token_ids or set())
                            | {req.tokenizer.eos_token_id}
                        )
                    ),
                    dtype=torch.int64,
                    device=self.orchestrator.device,
                )
                for req in self.orchestrator.reqs()
            ],
            batch_first=True,
            padding_value=self.orchestrator.vocab_size,
        )
        self.stop_token_penalties = torch.zeros(
            size=(len(self.orchestrator.reqs()), self.orchestrator.vocab_size + 1),
            dtype=torch.float32,
            device=self.orchestrator.device,
        ).scatter_add_(
            dim=1,
            index=padded_stop_token_ids,
            src=torch.full_like(
                input=padded_stop_token_ids,
                dtype=torch.float32,
                fill_value=float(&#34;-inf&#34;),
                device=self.orchestrator.device,
            ),
        )[
            :, : self.orchestrator.vocab_size
        ]

        self.len_output_tokens = torch.zeros(
            size=(len(self.orchestrator.reqs()), 1),
            dtype=torch.int32,
            device=self.orchestrator.device,
        )

    def _cumulate_output_tokens(self, output_ids: torch.Tensor):
        self.len_output_tokens += 1

    def _apply(self, logits: torch.Tensor):
        mask = (self.len_output_tokens &lt; self.min_new_tokens).expand_as(logits)
        logits[mask] += self.stop_token_penalties[mask]

    def _filter(self, keep_indices: torch.Tensor):
        self.min_new_tokens = self.min_new_tokens[keep_indices]
        self.stop_token_penalties = self.stop_token_penalties[keep_indices]
        self.len_output_tokens = self.len_output_tokens[keep_indices]

    def _merge(self, their: &#34;BatchedMinNewTokensPenalizer&#34;):
        self.min_new_tokens = torch.cat(
            [self.min_new_tokens, their.min_new_tokens], dim=0
        )
        self.stop_token_penalties = torch.cat(
            [self.stop_token_penalties, their.stop_token_penalties], dim=0
        )
        self.len_output_tokens = torch.cat(
            [self.len_output_tokens, their.len_output_tokens], dim=0
        )</code></pre>
</details>
<div class="desc"><p>Min new tokens penalizer penalizes tokens based on the length of the output.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sglang.srt.sampling.penaltylib.orchestrator._BatchedPenalizer</li>
<li>abc.ABC</li>
</ul>
</dd>
<dt id="sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator"><code class="flex name class">
<span>class <span class="ident">BatchedPenalizerOrchestrator</span></span>
<span>(</span><span>vocab_size:Â int,<br>batch:Â ScheduleBatch,<br>penalizers:Â "Set[Type['_BatchedPenalizer']]")</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BatchedPenalizerOrchestrator:
    def __init__(
        self,
        vocab_size: int,
        batch: ScheduleBatch,
        penalizers: Set[Type[&#34;_BatchedPenalizer&#34;]],
    ):
        self.vocab_size = vocab_size
        self._batch_ref = weakref.ref(batch)
        self.device = batch.device
        self.penalizers = {Penalizer: Penalizer(self) for Penalizer in penalizers}

        is_required = False
        for penalizer in self.penalizers.values():
            pen_is_required = penalizer.prepare_if_required()
            is_required |= pen_is_required
        self.is_required = is_required

    @property
    def batch(self) -&gt; ScheduleBatch | None:
        return self._batch_ref()

    @batch.setter
    def batch(self, value: Optional[ScheduleBatch]):
        if value is None:
            self._batch_ref = lambda: None
        else:
            self._batch_ref = weakref.ref(value)

    def reqs(self):
        return self.batch.reqs

    def cumulate_output_tokens(self, output_ids: torch.Tensor):
        &#34;&#34;&#34;
        Feed the output tokens to the penalizers.

        Args:
            output_ids (torch.Tensor): The output tokens.
        &#34;&#34;&#34;
        for penalizer in self.penalizers.values():
            penalizer.cumulate_output_tokens(output_ids=output_ids)

    def apply(self, logits: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Apply the penalizers to the logits.
        Note that it may apply the penalizers in-place.

        Args:
            logits (torch.Tensor): The logits to apply the penalizers to.

        Returns:
            torch.Tensor: The logits after applying the penalizers.
        &#34;&#34;&#34;
        for penalizer in self.penalizers.values():
            penalizer.apply(logits)

    def filter(self, keep_indices: torch.Tensor):
        &#34;&#34;&#34;
        Filter the penalizers based on the indices to keep in the batch.

        Args:
            keep_indices (torch.Tensor): Tensor of indices to keep in the batch.
        &#34;&#34;&#34;
        if not self.is_required:
            return

        if len(keep_indices) == 0:
            self.is_required = False
            for penalizer in self.penalizers.values():
                penalizer.teardown()
            return

        is_required = False
        for penalizer in self.penalizers.values():
            tmp_is_required = penalizer.is_required()
            is_required |= tmp_is_required
            if tmp_is_required:
                penalizer.filter(keep_indices=keep_indices)
            else:
                penalizer.teardown()
        self.is_required = is_required

    def merge(self, their: &#34;BatchedPenalizerOrchestrator&#34;):
        &#34;&#34;&#34;
        Merge the penalizers of another orchestrator into this one.

        Note that this function **must** be called _before_ self.batch.reqs is updated (filtered).
        Each unprepared penalizers would have to be prepared (creating tensors, etc.) first before merging.
        This step requires the original batch.reqs, before it gets merged with other batch.reqs.

        Args:
            their (BatchedPenalizerOrchestrator): The orchestrator to merge into this one.
        &#34;&#34;&#34;
        if not self.is_required and not their.is_required:
            return

        self.is_required = True
        for penalizer, their_penalizer in their.penalizers.items():
            self.penalizers[penalizer].merge(their_penalizer)</code></pre>
</details>
<div class="desc"></div>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator.batch"><code class="name">prop <span class="ident">batch</span> :Â ScheduleBatchÂ |Â None</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def batch(self) -&gt; ScheduleBatch | None:
    return self._batch_ref()</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator.apply"><code class="name flex">
<span>def <span class="ident">apply</span></span>(<span>self, logits:Â torch.Tensor) â>Â torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply(self, logits: torch.Tensor) -&gt; torch.Tensor:
    &#34;&#34;&#34;
    Apply the penalizers to the logits.
    Note that it may apply the penalizers in-place.

    Args:
        logits (torch.Tensor): The logits to apply the penalizers to.

    Returns:
        torch.Tensor: The logits after applying the penalizers.
    &#34;&#34;&#34;
    for penalizer in self.penalizers.values():
        penalizer.apply(logits)</code></pre>
</details>
<div class="desc"><p>Apply the penalizers to the logits.
Note that it may apply the penalizers in-place.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>logits</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The logits to apply the penalizers to.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>The logits after applying the penalizers.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator.cumulate_output_tokens"><code class="name flex">
<span>def <span class="ident">cumulate_output_tokens</span></span>(<span>self, output_ids:Â torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cumulate_output_tokens(self, output_ids: torch.Tensor):
    &#34;&#34;&#34;
    Feed the output tokens to the penalizers.

    Args:
        output_ids (torch.Tensor): The output tokens.
    &#34;&#34;&#34;
    for penalizer in self.penalizers.values():
        penalizer.cumulate_output_tokens(output_ids=output_ids)</code></pre>
</details>
<div class="desc"><p>Feed the output tokens to the penalizers.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>output_ids</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The output tokens.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator.filter"><code class="name flex">
<span>def <span class="ident">filter</span></span>(<span>self, keep_indices:Â torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter(self, keep_indices: torch.Tensor):
    &#34;&#34;&#34;
    Filter the penalizers based on the indices to keep in the batch.

    Args:
        keep_indices (torch.Tensor): Tensor of indices to keep in the batch.
    &#34;&#34;&#34;
    if not self.is_required:
        return

    if len(keep_indices) == 0:
        self.is_required = False
        for penalizer in self.penalizers.values():
            penalizer.teardown()
        return

    is_required = False
    for penalizer in self.penalizers.values():
        tmp_is_required = penalizer.is_required()
        is_required |= tmp_is_required
        if tmp_is_required:
            penalizer.filter(keep_indices=keep_indices)
        else:
            penalizer.teardown()
    self.is_required = is_required</code></pre>
</details>
<div class="desc"><p>Filter the penalizers based on the indices to keep in the batch.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>keep_indices</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Tensor of indices to keep in the batch.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator.merge"><code class="name flex">
<span>def <span class="ident">merge</span></span>(<span>self,<br>their:Â "'<a title="sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator" href="#sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator">BatchedPenalizerOrchestrator</a>'")</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge(self, their: &#34;BatchedPenalizerOrchestrator&#34;):
    &#34;&#34;&#34;
    Merge the penalizers of another orchestrator into this one.

    Note that this function **must** be called _before_ self.batch.reqs is updated (filtered).
    Each unprepared penalizers would have to be prepared (creating tensors, etc.) first before merging.
    This step requires the original batch.reqs, before it gets merged with other batch.reqs.

    Args:
        their (BatchedPenalizerOrchestrator): The orchestrator to merge into this one.
    &#34;&#34;&#34;
    if not self.is_required and not their.is_required:
        return

    self.is_required = True
    for penalizer, their_penalizer in their.penalizers.items():
        self.penalizers[penalizer].merge(their_penalizer)</code></pre>
</details>
<div class="desc"><p>Merge the penalizers of another orchestrator into this one.</p>
<p>Note that this function <strong>must</strong> be called <em>before</em> self.batch.reqs is updated (filtered).
Each unprepared penalizers would have to be prepared (creating tensors, etc.) first before merging.
This step requires the original batch.reqs, before it gets merged with other batch.reqs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>their</code></strong> :&ensp;<code><a title="sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator" href="#sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator">BatchedPenalizerOrchestrator</a></code></dt>
<dd>The orchestrator to merge into this one.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator.reqs"><code class="name flex">
<span>def <span class="ident">reqs</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reqs(self):
    return self.batch.reqs</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.sampling.penaltylib.BatchedPresencePenalizer"><code class="flex name class">
<span>class <span class="ident">BatchedPresencePenalizer</span></span>
<span>(</span><span>orchestrator:Â <a title="sglang.srt.sampling.penaltylib.orchestrator.BatchedPenalizerOrchestrator" href="orchestrator.html#sglang.srt.sampling.penaltylib.orchestrator.BatchedPenalizerOrchestrator">BatchedPenalizerOrchestrator</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BatchedPresencePenalizer(_BatchedPenalizer):
    &#34;&#34;&#34;
    Presence penalizer penalizes tokens based on their presence in the output.
    &#34;&#34;&#34;

    def __init__(self, orchestrator: BatchedPenalizerOrchestrator):
        self.orchestrator = orchestrator
        self._is_prepared = False

    def _is_required(self) -&gt; bool:
        return any(
            req.sampling_params.presence_penalty != 0.0
            for req in self.orchestrator.reqs()
        )

    def _prepare(self):
        self.cumulated_presence_penalties = torch.zeros(
            (len(self.orchestrator.reqs()), self.orchestrator.vocab_size),
            dtype=torch.float32,
            device=self.orchestrator.device,
        )

        self.presence_penalties = (
            torch.tensor(
                data=[
                    req.sampling_params.presence_penalty
                    for req in self.orchestrator.reqs()
                ],
                dtype=torch.float32,
                device=self.orchestrator.device,
            )
        ).unsqueeze_(1)

    def _cumulate_output_tokens(self, output_ids: torch.Tensor):
        self.cumulated_presence_penalties.scatter_(
            dim=1,
            index=output_ids.unsqueeze(1),
            src=self.presence_penalties,
        )

    def _apply(self, logits: torch.Tensor) -&gt; torch.Tensor:
        logits.sub_(self.cumulated_presence_penalties)

    def _filter(self, keep_indices: torch.Tensor):
        self.presence_penalties = self.presence_penalties[keep_indices]
        self.cumulated_presence_penalties = self.cumulated_presence_penalties[
            keep_indices
        ]

    def _merge(self, their: &#34;BatchedPresencePenalizer&#34;):
        self.presence_penalties = torch.cat(
            [self.presence_penalties, their.presence_penalties], dim=0
        )
        self.cumulated_presence_penalties = torch.cat(
            [self.cumulated_presence_penalties, their.cumulated_presence_penalties],
            dim=0,
        )</code></pre>
</details>
<div class="desc"><p>Presence penalizer penalizes tokens based on their presence in the output.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sglang.srt.sampling.penaltylib.orchestrator._BatchedPenalizer</li>
<li>abc.ABC</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.sampling" href="../index.html">sglang.srt.sampling</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="sglang.srt.sampling.penaltylib.frequency_penalty" href="frequency_penalty.html">sglang.srt.sampling.penaltylib.frequency_penalty</a></code></li>
<li><code><a title="sglang.srt.sampling.penaltylib.min_new_tokens" href="min_new_tokens.html">sglang.srt.sampling.penaltylib.min_new_tokens</a></code></li>
<li><code><a title="sglang.srt.sampling.penaltylib.orchestrator" href="orchestrator.html">sglang.srt.sampling.penaltylib.orchestrator</a></code></li>
<li><code><a title="sglang.srt.sampling.penaltylib.presence_penalty" href="presence_penalty.html">sglang.srt.sampling.penaltylib.presence_penalty</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.sampling.penaltylib.BatchedFrequencyPenalizer" href="#sglang.srt.sampling.penaltylib.BatchedFrequencyPenalizer">BatchedFrequencyPenalizer</a></code></h4>
</li>
<li>
<h4><code><a title="sglang.srt.sampling.penaltylib.BatchedMinNewTokensPenalizer" href="#sglang.srt.sampling.penaltylib.BatchedMinNewTokensPenalizer">BatchedMinNewTokensPenalizer</a></code></h4>
</li>
<li>
<h4><code><a title="sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator" href="#sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator">BatchedPenalizerOrchestrator</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator.apply" href="#sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator.apply">apply</a></code></li>
<li><code><a title="sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator.batch" href="#sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator.batch">batch</a></code></li>
<li><code><a title="sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator.cumulate_output_tokens" href="#sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator.cumulate_output_tokens">cumulate_output_tokens</a></code></li>
<li><code><a title="sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator.filter" href="#sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator.filter">filter</a></code></li>
<li><code><a title="sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator.merge" href="#sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator.merge">merge</a></code></li>
<li><code><a title="sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator.reqs" href="#sglang.srt.sampling.penaltylib.BatchedPenalizerOrchestrator.reqs">reqs</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.sampling.penaltylib.BatchedPresencePenalizer" href="#sglang.srt.sampling.penaltylib.BatchedPresencePenalizer">BatchedPresencePenalizer</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
