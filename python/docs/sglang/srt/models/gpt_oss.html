<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.models.gpt_oss API documentation</title>
<meta name="description" content="Inference-only GptOss model compatible with HuggingFace weights.">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.models.gpt_oss</code></h1>
</header>
<section id="section-intro">
<p>Inference-only GptOss model compatible with HuggingFace weights.</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.models.gpt_oss.get_attention_sliding_window_size"><code class="name flex">
<span>def <span class="ident">get_attention_sliding_window_size</span></span>(<span>config)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_attention_sliding_window_size(config):
    return config.sliding_window - 1</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.models.gpt_oss.GptOssAttention"><code class="flex name class">
<span>class <span class="ident">GptOssAttention</span></span>
<span>(</span><span>hidden_size: int,<br>num_heads: int,<br>num_kv_heads: int,<br>layer_id: int = 0,<br>rope_theta: float = 10000,<br>rope_scaling: Dict[str, Any] | None = None,<br>max_position_embeddings: int = 8192,<br>head_dim: int | None = None,<br>rms_norm_eps: float = 1e-06,<br>attention_bias: bool = False,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '',<br>sliding_window_size: int = -1,<br>layer_type: str = '',<br>params_dtype: torch.dtype = torch.bfloat16)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GptOssAttention(nn.Module):
    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        num_kv_heads: int,
        layer_id: int = 0,
        rope_theta: float = 10000,
        rope_scaling: Optional[Dict[str, Any]] = None,
        max_position_embeddings: int = 8192,
        head_dim: Optional[int] = None,
        rms_norm_eps: float = 1e-06,
        attention_bias: bool = False,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
        sliding_window_size: int = -1,  # if -1, normal attention, else, window attention.
        layer_type: str = &#34;&#34;,
        params_dtype: torch.dtype = torch.bfloat16,
    ) -&gt; None:
        super().__init__()
        self.hidden_size = hidden_size
        self.sliding_window_size = sliding_window_size

        attn_tp_rank = get_attention_tp_rank()
        attn_tp_size = get_attention_tp_size()

        self.total_num_heads = num_heads
        assert self.total_num_heads % attn_tp_size == 0
        self.num_heads = self.total_num_heads // attn_tp_size
        self.total_num_kv_heads = num_kv_heads
        if self.total_num_kv_heads &gt;= attn_tp_size:
            # Number of KV heads is greater than TP size, so we partition
            # the KV heads across multiple tensor parallel GPUs.
            assert self.total_num_kv_heads % attn_tp_size == 0
        else:
            # Number of KV heads is less than TP size, so we replicate
            # the KV heads across multiple tensor parallel GPUs.
            assert attn_tp_size % self.total_num_kv_heads == 0
        self.num_kv_heads = max(1, self.total_num_kv_heads // attn_tp_size)
        self.head_dim = head_dim or hidden_size // self.total_num_heads
        self.q_size = self.num_heads * self.head_dim
        self.kv_size = self.num_kv_heads * self.head_dim
        self.scaling = self.head_dim**-0.5
        self.rope_theta = rope_theta
        self.max_position_embeddings = max_position_embeddings
        self.tp_rank = get_tensor_model_parallel_rank()

        self.qkv_proj = QKVParallelLinear(
            hidden_size,
            self.head_dim,
            self.total_num_heads,
            self.total_num_kv_heads,
            bias=attention_bias,
            params_dtype=params_dtype,
            quant_config=quant_config,
            tp_rank=attn_tp_rank,
            tp_size=attn_tp_size,
            prefix=add_prefix(&#34;qkv_proj&#34;, prefix),
        )

        # Choose dtype of sinks based on attention backend: trtllm_mha requires float32,
        # others can use bfloat16
        attn_backend = global_server_args_dict.get(&#34;attention_backend&#34;)
        sinks_dtype = torch.float32 if attn_backend == &#34;trtllm_mha&#34; else torch.bfloat16
        self.sinks = nn.Parameter(
            torch.empty(self.num_heads, dtype=sinks_dtype), requires_grad=False
        )

        self.o_proj = RowParallelLinear(
            self.total_num_heads * self.head_dim,
            hidden_size,
            bias=attention_bias,
            quant_config=quant_config,
            tp_rank=attn_tp_rank,
            tp_size=attn_tp_size,
            reduce_results=False,
            params_dtype=params_dtype,
            prefix=add_prefix(&#34;o_proj&#34;, prefix),
        )

        self.rotary_emb = get_rope(
            self.head_dim,
            rotary_dim=self.head_dim,
            max_position=max_position_embeddings,
            base=rope_theta,
            rope_scaling=rope_scaling,
        )

        assert layer_type in {&#34;sliding_attention&#34;, &#34;full_attention&#34;}
        use_sliding_window = layer_type == &#34;sliding_attention&#34;
        self.attn = RadixAttention(
            self.num_heads,
            self.head_dim,
            self.scaling,
            num_kv_heads=self.num_kv_heads,
            layer_id=layer_id,
            prefix=add_prefix(&#34;attn&#34;, prefix),
            sliding_window_size=(sliding_window_size if use_sliding_window else -1),
        )
        self.layer_id = layer_id

    def forward_prepare(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        forward_batch: ForwardBatch,
    ):
        if hidden_states.shape[0] == 0:
            return hidden_states, forward_batch, None
        qkv, _ = self.qkv_proj(hidden_states)
        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)

        q, k = self.rotary_emb(
            positions,
            q,
            k,
            fused_set_kv_buffer_arg=(
                _create_fused_set_kv_buffer_arg(
                    value=v,
                    layer=self.attn,
                    forward_batch=forward_batch,
                )
                if _enable_fused_set_kv_buffer()
                else None
            ),
        )
        inner_state = q, k, v, forward_batch
        return None, forward_batch, inner_state

    def forward_core(self, intermediate_state):
        hidden_states, forward_batch, inner_state = intermediate_state
        if inner_state is None:
            return hidden_states
        attn_output = self.attn(
            *inner_state,
            sinks=self.sinks,
            save_kv_cache=not _enable_fused_set_kv_buffer(),
        )
        output, _ = self.o_proj(attn_output)
        return output

    def forward(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        forward_batch: ForwardBatch,
    ) -&gt; torch.Tensor:
        s = self.forward_prepare(
            positions=positions,
            hidden_states=hidden_states,
            forward_batch=forward_batch,
        )
        return self.forward_core(s)</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gpt_oss.GptOssAttention.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>positions: torch.Tensor,<br>hidden_states: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    positions: torch.Tensor,
    hidden_states: torch.Tensor,
    forward_batch: ForwardBatch,
) -&gt; torch.Tensor:
    s = self.forward_prepare(
        positions=positions,
        hidden_states=hidden_states,
        forward_batch=forward_batch,
    )
    return self.forward_core(s)</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssAttention.forward_core"><code class="name flex">
<span>def <span class="ident">forward_core</span></span>(<span>self, intermediate_state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_core(self, intermediate_state):
    hidden_states, forward_batch, inner_state = intermediate_state
    if inner_state is None:
        return hidden_states
    attn_output = self.attn(
        *inner_state,
        sinks=self.sinks,
        save_kv_cache=not _enable_fused_set_kv_buffer(),
    )
    output, _ = self.o_proj(attn_output)
    return output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssAttention.forward_prepare"><code class="name flex">
<span>def <span class="ident">forward_prepare</span></span>(<span>self,<br>positions: torch.Tensor,<br>hidden_states: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_prepare(
    self,
    positions: torch.Tensor,
    hidden_states: torch.Tensor,
    forward_batch: ForwardBatch,
):
    if hidden_states.shape[0] == 0:
        return hidden_states, forward_batch, None
    qkv, _ = self.qkv_proj(hidden_states)
    q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)

    q, k = self.rotary_emb(
        positions,
        q,
        k,
        fused_set_kv_buffer_arg=(
            _create_fused_set_kv_buffer_arg(
                value=v,
                layer=self.attn,
                forward_batch=forward_batch,
            )
            if _enable_fused_set_kv_buffer()
            else None
        ),
    )
    inner_state = q, k, v, forward_batch
    return None, forward_batch, inner_state</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssConfig"><code class="flex name class">
<span>class <span class="ident">GptOssConfig</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GptOssConfig(PretrainedConfig):
    model_type = &#34;gpt_oss&#34;

    def __init__(self, **kwargs):
        super().__init__(**kwargs)</code></pre>
</details>
<div class="desc"><p>Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as
methods for loading/downloading/saving configurations.</p>
<p><Tip></p>
<p>A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to
initialize a model does <strong>not</strong> load the model weights. It only affects the model's configuration.</p>
<p></Tip></p>
<p>Class attributes (overridden by derived classes):</p>
<ul>
<li><strong>model_type</strong> (<code>str</code>) &ndash; An identifier for the model type, serialized into the JSON file, and used to recreate
the correct object in [<code>~transformers.AutoConfig</code>].</li>
<li><strong>has_no_defaults_at_init</strong> (<code>bool</code>) &ndash; Whether the config class can be initialized without providing input arguments.
Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,
(but not necessarily) such as [<code>~transformers.EncoderDecoderConfig</code>] or [<code>~RagConfig</code>]. They have to be initialized from
two or more configs of type [<code>~transformers.PretrainedConfig</code>].</li>
<li><strong>keys_to_ignore_at_inference</strong> (<code>list[str]</code>) &ndash; A list of keys to ignore by default when looking at dictionary
outputs of the model during inference.</li>
<li><strong>attribute_map</strong> (<code>dict[str, str]</code>) &ndash; A dict that maps model specific attribute names to the standardized
naming of attributes.</li>
<li><strong>base_model_tp_plan</strong> (<code>dict[str, Any]</code>) &ndash; A dict that maps sub-modules FQNs of a base model to a tensor
parallel plan applied to the sub-module when <code>model.tensor_parallel</code> is called.</li>
<li><strong>base_model_pp_plan</strong> (<code>dict[str, tuple[list[str]]]</code>) &ndash; A dict that maps child-modules of a base model to a
pipeline parallel plan that enables users to place the child-module on the appropriate device.</li>
</ul>
<p>Common attributes (present in all subclasses):</p>
<ul>
<li><strong>vocab_size</strong> (<code>int</code>) &ndash; The number of tokens in the vocabulary, which is also the first dimension of the
embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).</li>
<li><strong>hidden_size</strong> (<code>int</code>) &ndash; The hidden size of the model.</li>
<li><strong>num_attention_heads</strong> (<code>int</code>) &ndash; The number of attention heads used in the multi-head attention layers of the
model.</li>
<li><strong>num_hidden_layers</strong> (<code>int</code>) &ndash; The number of blocks in the model.</li>
</ul>
<p><Tip warning={true}></p>
<p>Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading
some of them will still be possible, but attempting to overwrite them will throw an exception &ndash; you should set
them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more
information about the individual parameters.</p>
<p></Tip></p>
<h2 id="arg">Arg</h2>
<p>name_or_path (<code>str</code>, <em>optional</em>, defaults to <code>""</code>):
Store the string that was passed to [<code>PreTrainedModel.from_pretrained</code>] or
[<code>TFPreTrainedModel.from_pretrained</code>] as <code>pretrained_model_name_or_path</code> if the configuration was created
with such a method.
output_hidden_states (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should return all hidden-states.
output_attentions (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should returns all attentions.
return_dict (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should return a [<code>~transformers.utils.ModelOutput</code>] instead of a plain tuple.
is_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether the model is used as an encoder/decoder or not.
is_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether to only use the decoder in an encoder-decoder architecture, otherwise it has no effect on
decoder-only or encoder-only architectures.
cross_attention_hidden_size (<code>bool</code>, <em>optional</em>):
The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder
setting and the cross-attention hidden dimension differs from <code>self.config.hidden_size</code>.
add_cross_attention (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether cross-attention layers should be added to the model. Note, this option is only relevant for models
that can be used as decoder models within the [<code>EncoderDecoderModel</code>] class, which consists of all models
in <code>AUTO_MODELS_FOR_CAUSAL_LM</code>.
tie_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder
and decoder model to have the exact same parameter names.
prune_heads (<code>dict[int, list[int]]</code>, <em>optional</em>, defaults to <code>{}</code>):
Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of
heads to prune in said layer.</p>
<pre><code>For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.
</code></pre>
<p>chunk_size_feed_forward (<code>int</code>, <em>optional</em>, defaults to <code>0</code>):
The chunk size of all feed forward layers in the residual attention blocks. A chunk size of <code>0</code> means that
the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes <code>n</code> &lt;
sequence_length embeddings at a time. For more information on feed forward chunking, see <a href="../glossary.html#feed-forward-chunking">How does Feed
Forward Chunking work?</a>.</p>
<blockquote>
<p>Parameters for fine-tuning tasks</p>
</blockquote>
<p>architectures (<code>list[str]</code>, <em>optional</em>):
Model architectures that can be used with the model pretrained weights.
finetuning_task (<code>str</code>, <em>optional</em>):
Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow
or PyTorch) checkpoint.
id2label (<code>dict[int, str]</code>, <em>optional</em>):
A map from index (for instance prediction index, or target index) to label.
label2id (<code>dict[str, int]</code>, <em>optional</em>):
A map from label to index for the model.
num_labels (<code>int</code>, <em>optional</em>):
Number of labels to use in the last layer added to the model, typically for a classification task.
task_specific_params (<code>dict[str, Any]</code>, <em>optional</em>):
Additional keyword arguments to store for the current task.
problem_type (<code>str</code>, <em>optional</em>):
Problem type for <code>XxxForSequenceClassification</code> models. Can be one of <code>"regression"</code>,
<code>"single_label_classification"</code> or <code>"multi_label_classification"</code>.</p>
<blockquote>
<p>Parameters linked to the tokenizer</p>
</blockquote>
<p>tokenizer_class (<code>str</code>, <em>optional</em>):
The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the
model by default).
prefix (<code>str</code>, <em>optional</em>):
A specific prompt that should be added at the beginning of each text before calling the model.
bos_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>beginning-of-stream</em> token.
pad_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>padding</em> token.
eos_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>end-of-stream</em> token.
decoder_start_token_id (<code>int</code>, <em>optional</em>):
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.
sep_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>separation</em> token.</p>
<blockquote>
<p>PyTorch specific parameters</p>
</blockquote>
<p>torchscript (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should be used with Torchscript.
tie_word_embeddings (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the
model has a output word embedding layer.
torch_dtype (<code>str</code>, <em>optional</em>):
The <code>dtype</code> of the weights. This attribute can be used to initialize the model to a non-default <code>dtype</code>
(which is normally <code>float32</code>) and thus allow for optimal storage allocation. For example, if the saved
model is <code>float16</code>, ideally we want to load it back using the minimal amount of memory needed to load
<code>float16</code> weights. Since the config object is stored in plain text, this attribute contains just the
floating type string without the <code>torch.</code> prefix. For example, for <code>torch.float16</code> <code><code>torch\_dtype&lt;/code&gt; is the</code>"float16"` string.</p>
<pre><code>This attribute is currently not being used during model loading time, but this may change in the future
versions. But we can already start preparing for the future by saving the dtype with save_pretrained.
</code></pre></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.configuration_utils.PretrainedConfig</li>
<li>transformers.utils.hub.PushToHubMixin</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.gpt_oss.GptOssConfig.model_type"><code class="name">var <span class="ident">model_type</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssDecoderLayer"><code class="flex name class">
<span>class <span class="ident">GptOssDecoderLayer</span></span>
<span>(</span><span>config: <a title="sglang.srt.models.gpt_oss.GptOssConfig" href="#sglang.srt.models.gpt_oss.GptOssConfig">GptOssConfig</a>,<br>layer_id: int,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '',<br>sliding_window_size: int | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GptOssDecoderLayer(nn.Module):
    def __init__(
        self,
        config: GptOssConfig,
        layer_id: int,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
        sliding_window_size: int | None = None,
    ) -&gt; None:
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        rope_theta = getattr(config, &#34;rope_theta&#34;, 10000)
        rope_scaling = getattr(config, &#34;rope_scaling&#34;, None)
        max_position_embeddings = getattr(config, &#34;max_position_embeddings&#34;, 8192)
        head_dim = getattr(
            config, &#34;head_dim&#34;, config.hidden_size // config.num_attention_heads
        )
        rms_norm_eps = config.rms_norm_eps
        attention_bias = config.attention_bias

        if sliding_window_size is None:
            self.sliding_window_size = get_attention_sliding_window_size(self.config)
        else:
            self.sliding_window_size = sliding_window_size

        self.self_attn = GptOssAttention(
            hidden_size=self.hidden_size,
            num_heads=config.num_attention_heads,
            num_kv_heads=config.num_key_value_heads,
            layer_id=layer_id,
            rope_theta=rope_theta,
            rope_scaling=rope_scaling,
            max_position_embeddings=max_position_embeddings,
            head_dim=head_dim,
            rms_norm_eps=rms_norm_eps,
            attention_bias=attention_bias,
            prefix=add_prefix(&#34;self_attn&#34;, prefix),
            sliding_window_size=self.sliding_window_size,
            layer_type=config.layer_types[layer_id],
            params_dtype=config.torch_dtype,
        )

        self.layer_id = layer_id

        self.attn_tp_size = get_attention_tp_size()
        self.attn_tp_rank = get_attention_tp_rank()

        # GptOss all layers are sparse and have no nextn now
        self.is_layer_sparse = True
        self.is_nextn = False
        is_previous_layer_sparse = True

        self.layer_scatter_modes = LayerScatterModes.init_new(
            layer_id=layer_id,
            num_layers=config.num_hidden_layers,
            is_layer_sparse=self.is_layer_sparse,
            is_previous_layer_sparse=is_previous_layer_sparse,
        )

        if self.is_layer_sparse:
            self.mlp = GptOssSparseMoeBlock(
                layer_id=self.layer_id,
                config=config,
                quant_config=quant_config,
                prefix=add_prefix(&#34;mlp&#34;, prefix),
            )
        else:
            raise NotImplementedError(
                &#34;Dense MLP is not implemented for GptOssDecoderLayer. &#34;
                &#34;Please use GptOssSparseMoeBlock instead.&#34;
            )
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = RMSNorm(
            config.hidden_size, eps=config.rms_norm_eps
        )

        self.layer_communicator = LayerCommunicator(
            layer_scatter_modes=self.layer_scatter_modes,
            input_layernorm=self.input_layernorm,
            post_attention_layernorm=self.post_attention_layernorm,
            is_last_layer=(
                self.is_nextn or (self.layer_id == self.config.num_hidden_layers - 1)
            ),
        )

    def forward(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        forward_batch: ForwardBatch,
        residual: Optional[torch.Tensor],
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        hidden_states, residual = self.layer_communicator.prepare_attn(
            hidden_states, residual, forward_batch
        )

        if hidden_states.shape[0] != 0:
            hidden_states = self.self_attn(
                positions=positions,
                hidden_states=hidden_states,
                forward_batch=forward_batch,
            )

        hidden_states, residual = self.layer_communicator.prepare_mlp(
            hidden_states, residual, forward_batch
        )

        should_allreduce_fusion = (
            self.layer_communicator.should_fuse_mlp_allreduce_with_next_layer(
                forward_batch
            )
        )

        hidden_states = self.mlp(hidden_states, forward_batch, should_allreduce_fusion)

        if should_allreduce_fusion:
            hidden_states._sglang_needs_allreduce_fusion = True

        if not should_allreduce_fusion:
            hidden_states, residual = self.layer_communicator.postprocess_layer(
                hidden_states, residual, forward_batch
            )

        return hidden_states, residual</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gpt_oss.GptOssDecoderLayer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>positions: torch.Tensor,<br>hidden_states: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>residual: torch.Tensor | None) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    positions: torch.Tensor,
    hidden_states: torch.Tensor,
    forward_batch: ForwardBatch,
    residual: Optional[torch.Tensor],
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    hidden_states, residual = self.layer_communicator.prepare_attn(
        hidden_states, residual, forward_batch
    )

    if hidden_states.shape[0] != 0:
        hidden_states = self.self_attn(
            positions=positions,
            hidden_states=hidden_states,
            forward_batch=forward_batch,
        )

    hidden_states, residual = self.layer_communicator.prepare_mlp(
        hidden_states, residual, forward_batch
    )

    should_allreduce_fusion = (
        self.layer_communicator.should_fuse_mlp_allreduce_with_next_layer(
            forward_batch
        )
    )

    hidden_states = self.mlp(hidden_states, forward_batch, should_allreduce_fusion)

    if should_allreduce_fusion:
        hidden_states._sglang_needs_allreduce_fusion = True

    if not should_allreduce_fusion:
        hidden_states, residual = self.layer_communicator.postprocess_layer(
            hidden_states, residual, forward_batch
        )

    return hidden_states, residual</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM"><code class="flex name class">
<span>class <span class="ident">GptOssForCausalLM</span></span>
<span>(</span><span>config: <a title="sglang.srt.models.gpt_oss.GptOssConfig" href="#sglang.srt.models.gpt_oss.GptOssConfig">GptOssConfig</a>,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GptOssForCausalLM(nn.Module):
    fall_back_to_pt_during_load = False

    def __init__(
        self,
        config: GptOssConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; None:
        super().__init__()
        self.pp_group = get_pp_group()
        self.config = config
        self.quant_config = quant_config
        self.model = GptOssModel(
            config, quant_config, prefix=add_prefix(&#34;model&#34;, prefix)
        )
        self.lm_head = ParallelLMHead(
            config.vocab_size,
            config.hidden_size,
            # quant_config=quant_config,
            prefix=add_prefix(&#34;lm_head&#34;, prefix),
            use_attn_tp_group=global_server_args_dict[&#34;enable_dp_lm_head&#34;],
        )
        self.logits_processor = LogitsProcessor(config)
        self.capture_aux_hidden_states = False

        self._routed_experts_weights_of_layer = LazyValue(
            lambda: {
                layer_id: self.model.layers[layer_id].mlp.get_moe_weights()
                for layer_id in range(self.start_layer, self.end_layer)
                if isinstance(self.model.layers[layer_id].mlp, GptOssSparseMoeBlock)
            }
        )

    @property
    def routed_experts_weights_of_layer(self):
        return self._routed_experts_weights_of_layer.value

    @torch.no_grad()
    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        forward_batch: ForwardBatch,
        input_embeds: torch.Tensor = None,
        pp_proxy_tensors: Optional[PPProxyTensors] = None,
    ) -&gt; torch.Tensor:
        hidden_states = self.model(
            input_ids,
            positions,
            forward_batch,
            input_embeds,
            pp_proxy_tensors=pp_proxy_tensors,
        )

        aux_hidden_states = None
        if self.capture_aux_hidden_states:
            hidden_states, aux_hidden_states = hidden_states

        if self.pp_group.is_last_rank:
            return self.logits_processor(
                input_ids,
                hidden_states,
                self.lm_head,
                forward_batch,
                aux_hidden_states,
            )
        else:
            return hidden_states

    @property
    def start_layer(self):
        return self.model.start_layer

    @property
    def end_layer(self):
        return self.model.end_layer

    def _get_default_weight_mapping(self):
        &#34;&#34;&#34;Generate default weight name mapping for GptOss safetensors.&#34;&#34;&#34;
        weight_mapping = {}

        # Map router weights to gate
        weight_mapping[&#34;embedding.weight&#34;] = &#34;model.embed_tokens.weight&#34;
        weight_mapping[&#34;unembedding.weight&#34;] = &#34;lm_head.weight&#34;
        weight_mapping[&#34;norm.scale&#34;] = &#34;model.norm.weight&#34;
        for layer_id in range(self.config.num_hidden_layers):
            weight_mapping[f&#34;block.{layer_id}.attn.q_proj.weight&#34;] = (
                f&#34;model.layers.{layer_id}.self_attn.q_proj.weight&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.attn.q_proj.bias&#34;] = (
                f&#34;model.layers.{layer_id}.self_attn.q_proj.bias&#34;
            )

            weight_mapping[f&#34;block.{layer_id}.attn.k_proj.weight&#34;] = (
                f&#34;model.layers.{layer_id}.self_attn.k_proj.weight&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.attn.k_proj.bias&#34;] = (
                f&#34;model.layers.{layer_id}.self_attn.k_proj.bias&#34;
            )

            weight_mapping[f&#34;block.{layer_id}.attn.v_proj.weight&#34;] = (
                f&#34;model.layers.{layer_id}.self_attn.v_proj.weight&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.attn.v_proj.bias&#34;] = (
                f&#34;model.layers.{layer_id}.self_attn.v_proj.bias&#34;
            )

            weight_mapping[f&#34;block.{layer_id}.attn.out.weight&#34;] = (
                f&#34;model.layers.{layer_id}.self_attn.o_proj.weight&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.attn.out.bias&#34;] = (
                f&#34;model.layers.{layer_id}.self_attn.o_proj.bias&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.attn.sinks&#34;] = (
                f&#34;model.layers.{layer_id}.self_attn.sinks&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.attn.norm.scale&#34;] = (
                f&#34;model.layers.{layer_id}.input_layernorm.weight&#34;
            )

            weight_mapping[f&#34;block.{layer_id}.mlp.gate.weight&#34;] = (
                f&#34;model.layers.{layer_id}.mlp.router.weight&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.mlp.gate.bias&#34;] = (
                f&#34;model.layers.{layer_id}.mlp.router.bias&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.mlp.norm.scale&#34;] = (
                f&#34;model.layers.{layer_id}.post_attention_layernorm.weight&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.mlp.experts.gate_up_proj&#34;] = (
                f&#34;model.layers.{layer_id}.mlp.experts.gate_up_proj&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.mlp.gate_up_proj_bias&#34;] = (
                f&#34;model.layers.{layer_id}.mlp.experts.gate_up_proj_bias&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.mlp.down_proj&#34;] = (
                f&#34;model.layers.{layer_id}.mlp.experts.mlp2_weight&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.mlp.down_proj_bias&#34;] = (
                f&#34;model.layers.{layer_id}.mlp.experts.mlp2_bias&#34;
            )

        return weight_mapping

    # TODO beautify code
    def load_weights(
        self,
        weights: Iterable[Tuple[str, torch.Tensor]],
        is_nextn: bool = False,
        weight_name_mapping: dict = None,
    ):
        quant_config_name = (
            self.quant_config.get_name() if self.quant_config is not None else None
        )
        if quant_config_name != &#34;mxfp4&#34;:
            self._load_normal_weights(
                weights, is_nextn=is_nextn, weight_name_mapping=weight_name_mapping
            )
        else:
            self._load_weights_mxfp4(
                weights, is_nextn=is_nextn, weight_name_mapping=weight_name_mapping
            )

    def _load_weights_mxfp4(self, weights, is_nextn, weight_name_mapping):
        mxfp4_weights = []
        normal_weights = []

        for name, weight in weights:
            if (
                &#34;.experts&#34; in name
                and self.quant_config is not None
                and self.quant_config.get_name() == &#34;mxfp4&#34;
            ):
                mxfp4_weights.append((name, weight))
            else:
                normal_weights.append((name, weight))

        mxfp4_loaded_params = self._load_mxfp4_experts_weights(mxfp4_weights)
        self._load_normal_weights(
            normal_weights,
            is_nextn=is_nextn,
            weight_name_mapping=weight_name_mapping,
            other_loaded_param_names=mxfp4_loaded_params,
        )

    def _load_mxfp4_experts_weights(self, weights):

        params_dict = dict(self.named_parameters())
        loaded_params: set[str] = set()
        mxfp4_block = 32

        moe_tp_rank = get_moe_tensor_parallel_rank()
        moe_tp_size = get_moe_tensor_parallel_world_size()
        moe_ep_rank = get_moe_expert_parallel_rank()
        moe_ep_size = get_moe_expert_parallel_world_size()

        intermediate_size = self.config.intermediate_size
        assert (
            intermediate_size % mxfp4_block == 0
        ), f&#34;{intermediate_size=} must be divisible by {mxfp4_block=}&#34;
        intermediate_size_block = intermediate_size // mxfp4_block

        per_rank_intermediate_size_block = math.ceil(
            intermediate_size_block / moe_tp_size
        )

        per_rank_intermediate_size = per_rank_intermediate_size_block * mxfp4_block

        # Calculate common slicing bounds for current rank
        assert self.config.num_local_experts % moe_ep_size == 0
        moe_num_global_experts = self.config.num_local_experts
        moe_num_local_experts = self.config.num_local_experts // moe_ep_size

        moe_tp_rank_start = moe_tp_rank * per_rank_intermediate_size
        moe_tp_rank_end = min(
            (moe_tp_rank + 1) * per_rank_intermediate_size, intermediate_size
        )

        moe_ep_rank_start = moe_ep_rank * moe_num_local_experts
        moe_ep_rank_end = (moe_ep_rank + 1) * moe_num_local_experts

        for name, weight in weights:
            weight = weight.cuda()

            if &#34;gate_up_proj_blocks&#34; in name:
                # Handle MLP gate and up projection weights
                new_name = name.replace(&#34;gate_up_proj_blocks&#34;, &#34;w13_weight&#34;)

                # flat weight from (E, 2 * N, block_size, entry_per_block)
                # to (E, 2 * N, -1), shouldn&#39;t trigger copy for contiguous
                weight = weight.view(
                    moe_num_global_experts, 2 * intermediate_size, -1
                ).contiguous()

                narrow_weight = weight[
                    moe_ep_rank_start:moe_ep_rank_end,
                    2 * moe_tp_rank_start : 2 * moe_tp_rank_end,
                    ...,
                ]

                param = params_dict[new_name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(
                    param,
                    narrow_weight,
                    weight_name=new_name,
                    shard_id=None,
                    expert_id=None,
                )
                loaded_params.add(new_name)

            elif &#34;down_proj_blocks&#34; in name:
                # Handle MLP down projection weights
                new_name = name.replace(&#34;down_proj_blocks&#34;, &#34;w2_weight&#34;)
                # same flatten here, but since 2 mx4 value are packed in 1
                # uint8, divide by 2
                weight = weight.view(
                    moe_num_global_experts, -1, intermediate_size // 2
                ).contiguous()
                narrow_weight = weight[
                    moe_ep_rank_start:moe_ep_rank_end,
                    ...,
                    moe_tp_rank_start // 2 : moe_tp_rank_end // 2,
                ]

                param = params_dict[new_name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(
                    param,
                    narrow_weight,
                    weight_name=new_name,
                    shard_id=None,
                    expert_id=None,
                )
                loaded_params.add(new_name)

            elif &#34;gate_up_proj_scales&#34; in name:
                # Handle MLP gate and up projection weights scale
                new_name = name.replace(&#34;gate_up_proj_scales&#34;, &#34;w13_weight_scale&#34;)
                narrow_weight = weight[
                    moe_ep_rank_start:moe_ep_rank_end,
                    2 * moe_tp_rank_start : 2 * moe_tp_rank_end,
                    ...,
                ]

                param = params_dict[new_name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(
                    param,
                    narrow_weight,
                    weight_name=new_name,
                    shard_id=None,
                    expert_id=None,
                )
                loaded_params.add(new_name)

            elif &#34;down_proj_scales&#34; in name:
                # Handle MLP down projection weights
                new_name = name.replace(&#34;down_proj_scales&#34;, &#34;w2_weight_scale&#34;)
                narrow_weight = weight[
                    moe_ep_rank_start:moe_ep_rank_end,
                    ...,
                    moe_tp_rank_start // mxfp4_block : moe_tp_rank_end // mxfp4_block,
                ]

                param = params_dict[new_name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(
                    param,
                    narrow_weight,
                    weight_name=new_name,
                    shard_id=None,
                    expert_id=None,
                )
                loaded_params.add(new_name)
            elif &#34;gate_up_proj_bias&#34; in name:
                # Handle MLP gate and up projection biases
                new_name = name.replace(&#34;gate_up_proj_bias&#34;, &#34;w13_weight_bias&#34;)

                narrow_weight = weight[
                    moe_ep_rank_start:moe_ep_rank_end,
                    2 * moe_tp_rank_start : 2 * moe_tp_rank_end,
                ]

                param = params_dict[new_name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(
                    param,
                    narrow_weight,
                    weight_name=new_name,
                    shard_id=None,
                    expert_id=None,
                )
                loaded_params.add(new_name)

            elif &#34;down_proj_bias&#34; in name:
                narrow_weight = weight[moe_ep_rank_start:moe_ep_rank_end, ...]
                if moe_tp_rank != 0:
                    narrow_weight = torch.zeros_like(narrow_weight)

                # Handle MLP down projection bias
                new_name = name.replace(&#34;down_proj_bias&#34;, &#34;w2_weight_bias&#34;)
                param = params_dict[new_name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(
                    param,
                    narrow_weight,
                    weight_name=new_name,
                    shard_id=None,
                    expert_id=None,
                )
                loaded_params.add(new_name)

        return loaded_params

    def _load_normal_weights(
        self,
        weights,
        is_nextn: bool,
        weight_name_mapping: dict,
        other_loaded_param_names=[],
    ):
        tp_rank = get_tensor_model_parallel_rank()
        if is_nextn:
            logging.warning(
                &#34;Loading weights for nextn is currently not supported in GptOssForCausalLM. &#34;
            )
            return
        weights = _canonicalize_weights(self.config, weights)
        weights = sorted(weights, key=lambda x: x[0])  # Sort by name for consistency

        new_weights = []
        for name, p in weights:
            if &#34;qkv.weight&#34; in name:
                q_proj, k_proj, v_proj = p.split(
                    [
                        self.config.num_attention_heads * self.config.head_dim,
                        self.config.num_key_value_heads * self.config.head_dim,
                        self.config.num_key_value_heads * self.config.head_dim,
                    ],
                    dim=0,
                )
                new_weights.append(
                    (f&#34;{name.replace(&#39;qkv.weight&#39;, &#39;q_proj.weight&#39;)}&#34;, q_proj)
                )
                new_weights.append(
                    (f&#34;{name.replace(&#39;qkv.weight&#39;, &#39;k_proj.weight&#39;)}&#34;, k_proj)
                )
                new_weights.append(
                    (f&#34;{name.replace(&#39;qkv.weight&#39;, &#39;v_proj.weight&#39;)}&#34;, v_proj)
                )
            elif &#34;qkv.bias&#34; in name:
                q_bias, k_bias, v_bias = p.split(
                    [
                        self.config.num_attention_heads * self.config.head_dim,
                        self.config.num_key_value_heads * self.config.head_dim,
                        self.config.num_key_value_heads * self.config.head_dim,
                    ],
                    dim=0,
                )
                new_weights.append(
                    (f&#34;{name.replace(&#39;qkv.bias&#39;, &#39;q_proj.bias&#39;)}&#34;, q_bias)
                )
                new_weights.append(
                    (f&#34;{name.replace(&#39;qkv.bias&#39;, &#39;k_proj.bias&#39;)}&#34;, k_bias)
                )
                new_weights.append(
                    (f&#34;{name.replace(&#39;qkv.bias&#39;, &#39;v_proj.bias&#39;)}&#34;, v_bias)
                )
            else:
                new_weights.append((name, p))
        weights = new_weights

        # Use provided weight name mapping if available, otherwise use default
        if weight_name_mapping is None:
            weight_name_mapping = self._get_default_weight_mapping()
        else:
            # Merge with default mapping
            default_mapping = self._get_default_weight_mapping()
            default_mapping.update(weight_name_mapping)
            weight_name_mapping = default_mapping

        stacked_params_mapping = [
            # (param_name, shard_name, shard_id)
            (&#34;qkv_proj&#34;, &#34;q_proj&#34;, &#34;q&#34;),
            (&#34;qkv_proj&#34;, &#34;k_proj&#34;, &#34;k&#34;),
            (&#34;qkv_proj&#34;, &#34;v_proj&#34;, &#34;v&#34;),
        ]
        expert_params_mapping = FusedMoE.make_expert_params_mapping_fused(
            ckpt_gate_up_proj_name=&#34;gate_up_proj&#34;,
            ckpt_down_proj_name=&#34;down_proj&#34;,
            ckpt_gate_up_proj_bias_name=&#34;gate_up_proj_bias&#34;,
            ckpt_down_proj_bias_name=&#34;down_proj_bias&#34;,
        )

        params_dict = dict(self.named_parameters())

        for name, loaded_weight in weights:
            loaded_weight = _WeightCreator.maybe_materialize(loaded_weight)

            # Apply weight name mapping if provided
            if weight_name_mapping and name in weight_name_mapping:
                name = weight_name_mapping[name]

            layer_id = get_layer_id(name)
            if (
                layer_id is not None
                and hasattr(self.model, &#34;start_layer&#34;)
                and (
                    layer_id &lt; self.model.start_layer
                    or layer_id &gt;= self.model.end_layer
                )
            ):
                continue

            if &#34;rotary_emb.inv_freq&#34; in name:
                continue
            for param_name, weight_name, shard_id in stacked_params_mapping:
                if weight_name not in name:
                    continue
                if &#34;mlp.experts&#34; in name:
                    continue

                name = name.replace(weight_name, param_name)
                if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                    continue
                if name not in params_dict:
                    continue

                param = params_dict[name]
                weight_loader = param.weight_loader
                weight_loader(param, loaded_weight, shard_id)
                break
            else:
                for mapping in expert_params_mapping:
                    param_name, weight_name, shard_id = mapping
                    if weight_name not in name:
                        continue
                    name = name.replace(weight_name, param_name)
                    if name not in params_dict:
                        continue
                    param = params_dict[name]
                    weight_loader = param.weight_loader
                    if &#34;bias&#34; not in name:
                        loaded_weight = loaded_weight.transpose(-2, -1)
                    if &#34;w2_weight_bias&#34; in name and get_moe_tensor_parallel_rank() != 0:
                        loaded_weight = loaded_weight.zero_()

                    weight_loader(
                        param,
                        loaded_weight,
                        name,
                        shard_id=shard_id,
                    )
                    break
                else:
                    if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                        continue
                    if name not in params_dict:
                        continue
                    if name in params_dict.keys():
                        param = params_dict[name]
                        if &#34;sinks&#34; in name:
                            start = get_attention_tp_rank() * param.numel()
                            param.data.copy_(
                                loaded_weight[start : start + param.numel()]
                            )
                        else:
                            weight_loader = getattr(
                                param, &#34;weight_loader&#34;, default_weight_loader
                            )
                            weight_loader(param, loaded_weight)
                    else:
                        logger.warning(f&#34;Parameter {name} not found in params_dict&#34;)

    def get_embed_and_head(self):
        return self.model.embed_tokens.weight, self.lm_head.weight

    def set_embed_and_head(self, embed, head):
        del self.model.embed_tokens.weight
        del self.lm_head.weight
        self.model.embed_tokens.weight = embed
        self.lm_head.weight = head
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

    def set_eagle3_layers_to_capture(self, layer_ids: Optional[List[int]] = None):
        if not self.pp_group.is_last_rank:
            return

        if layer_ids is None:
            self.capture_aux_hidden_states = True
            num_layers = self.config.num_hidden_layers
            self.model.layers_to_capture = [2, num_layers // 2, num_layers - 3]
        else:
            self.capture_aux_hidden_states = True
            # we plus 1 here because in sglang, for the ith layer, it takes the output
            # of the (i-1)th layer as aux hidden state
            self.model.layers_to_capture = [val + 1 for val in layer_ids]

    @classmethod
    def get_model_config_for_expert_location(cls, config):
        return ModelConfigForExpertLocation(
            num_layers=config.num_hidden_layers,
            num_logical_experts=config.num_local_experts,
            num_groups=None,
        )

    def get_attention_sliding_window_size(self):
        return get_attention_sliding_window_size(self.config)</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.fall_back_to_pt_during_load"><code class="name">var <span class="ident">fall_back_to_pt_during_load</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.get_model_config_for_expert_location"><code class="name flex">
<span>def <span class="ident">get_model_config_for_expert_location</span></span>(<span>config)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.end_layer"><code class="name">prop <span class="ident">end_layer</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def end_layer(self):
    return self.model.end_layer</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.routed_experts_weights_of_layer"><code class="name">prop <span class="ident">routed_experts_weights_of_layer</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def routed_experts_weights_of_layer(self):
    return self._routed_experts_weights_of_layer.value</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.start_layer"><code class="name">prop <span class="ident">start_layer</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def start_layer(self):
    return self.model.start_layer</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>input_ids: torch.Tensor,<br>positions: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>input_embeds: torch.Tensor = None,<br>pp_proxy_tensors: <a title="sglang.srt.model_executor.forward_batch_info.PPProxyTensors" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.PPProxyTensors">PPProxyTensors</a> | None = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def forward(
    self,
    input_ids: torch.Tensor,
    positions: torch.Tensor,
    forward_batch: ForwardBatch,
    input_embeds: torch.Tensor = None,
    pp_proxy_tensors: Optional[PPProxyTensors] = None,
) -&gt; torch.Tensor:
    hidden_states = self.model(
        input_ids,
        positions,
        forward_batch,
        input_embeds,
        pp_proxy_tensors=pp_proxy_tensors,
    )

    aux_hidden_states = None
    if self.capture_aux_hidden_states:
        hidden_states, aux_hidden_states = hidden_states

    if self.pp_group.is_last_rank:
        return self.logits_processor(
            input_ids,
            hidden_states,
            self.lm_head,
            forward_batch,
            aux_hidden_states,
        )
    else:
        return hidden_states</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.get_attention_sliding_window_size"><code class="name flex">
<span>def <span class="ident">get_attention_sliding_window_size</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_attention_sliding_window_size(self):
    return get_attention_sliding_window_size(self.config)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.get_embed_and_head"><code class="name flex">
<span>def <span class="ident">get_embed_and_head</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_embed_and_head(self):
    return self.model.embed_tokens.weight, self.lm_head.weight</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.load_weights"><code class="name flex">
<span>def <span class="ident">load_weights</span></span>(<span>self,<br>weights: Iterable[typing.Tuple[str, torch.Tensor]],<br>is_nextn: bool = False,<br>weight_name_mapping: dict = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_weights(
    self,
    weights: Iterable[Tuple[str, torch.Tensor]],
    is_nextn: bool = False,
    weight_name_mapping: dict = None,
):
    quant_config_name = (
        self.quant_config.get_name() if self.quant_config is not None else None
    )
    if quant_config_name != &#34;mxfp4&#34;:
        self._load_normal_weights(
            weights, is_nextn=is_nextn, weight_name_mapping=weight_name_mapping
        )
    else:
        self._load_weights_mxfp4(
            weights, is_nextn=is_nextn, weight_name_mapping=weight_name_mapping
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.set_eagle3_layers_to_capture"><code class="name flex">
<span>def <span class="ident">set_eagle3_layers_to_capture</span></span>(<span>self, layer_ids: List[int] | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_eagle3_layers_to_capture(self, layer_ids: Optional[List[int]] = None):
    if not self.pp_group.is_last_rank:
        return

    if layer_ids is None:
        self.capture_aux_hidden_states = True
        num_layers = self.config.num_hidden_layers
        self.model.layers_to_capture = [2, num_layers // 2, num_layers - 3]
    else:
        self.capture_aux_hidden_states = True
        # we plus 1 here because in sglang, for the ith layer, it takes the output
        # of the (i-1)th layer as aux hidden state
        self.model.layers_to_capture = [val + 1 for val in layer_ids]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.set_embed_and_head"><code class="name flex">
<span>def <span class="ident">set_embed_and_head</span></span>(<span>self, embed, head)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_embed_and_head(self, embed, head):
    del self.model.embed_tokens.weight
    del self.lm_head.weight
    self.model.embed_tokens.weight = embed
    self.lm_head.weight = head
    torch.cuda.empty_cache()
    torch.cuda.synchronize()</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM"><code class="flex name class">
<span>class <span class="ident">EntryClass</span></span>
<span>(</span><span>config: <a title="sglang.srt.models.gpt_oss.GptOssConfig" href="#sglang.srt.models.gpt_oss.GptOssConfig">GptOssConfig</a>,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GptOssForCausalLM(nn.Module):
    fall_back_to_pt_during_load = False

    def __init__(
        self,
        config: GptOssConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; None:
        super().__init__()
        self.pp_group = get_pp_group()
        self.config = config
        self.quant_config = quant_config
        self.model = GptOssModel(
            config, quant_config, prefix=add_prefix(&#34;model&#34;, prefix)
        )
        self.lm_head = ParallelLMHead(
            config.vocab_size,
            config.hidden_size,
            # quant_config=quant_config,
            prefix=add_prefix(&#34;lm_head&#34;, prefix),
            use_attn_tp_group=global_server_args_dict[&#34;enable_dp_lm_head&#34;],
        )
        self.logits_processor = LogitsProcessor(config)
        self.capture_aux_hidden_states = False

        self._routed_experts_weights_of_layer = LazyValue(
            lambda: {
                layer_id: self.model.layers[layer_id].mlp.get_moe_weights()
                for layer_id in range(self.start_layer, self.end_layer)
                if isinstance(self.model.layers[layer_id].mlp, GptOssSparseMoeBlock)
            }
        )

    @property
    def routed_experts_weights_of_layer(self):
        return self._routed_experts_weights_of_layer.value

    @torch.no_grad()
    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        forward_batch: ForwardBatch,
        input_embeds: torch.Tensor = None,
        pp_proxy_tensors: Optional[PPProxyTensors] = None,
    ) -&gt; torch.Tensor:
        hidden_states = self.model(
            input_ids,
            positions,
            forward_batch,
            input_embeds,
            pp_proxy_tensors=pp_proxy_tensors,
        )

        aux_hidden_states = None
        if self.capture_aux_hidden_states:
            hidden_states, aux_hidden_states = hidden_states

        if self.pp_group.is_last_rank:
            return self.logits_processor(
                input_ids,
                hidden_states,
                self.lm_head,
                forward_batch,
                aux_hidden_states,
            )
        else:
            return hidden_states

    @property
    def start_layer(self):
        return self.model.start_layer

    @property
    def end_layer(self):
        return self.model.end_layer

    def _get_default_weight_mapping(self):
        &#34;&#34;&#34;Generate default weight name mapping for GptOss safetensors.&#34;&#34;&#34;
        weight_mapping = {}

        # Map router weights to gate
        weight_mapping[&#34;embedding.weight&#34;] = &#34;model.embed_tokens.weight&#34;
        weight_mapping[&#34;unembedding.weight&#34;] = &#34;lm_head.weight&#34;
        weight_mapping[&#34;norm.scale&#34;] = &#34;model.norm.weight&#34;
        for layer_id in range(self.config.num_hidden_layers):
            weight_mapping[f&#34;block.{layer_id}.attn.q_proj.weight&#34;] = (
                f&#34;model.layers.{layer_id}.self_attn.q_proj.weight&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.attn.q_proj.bias&#34;] = (
                f&#34;model.layers.{layer_id}.self_attn.q_proj.bias&#34;
            )

            weight_mapping[f&#34;block.{layer_id}.attn.k_proj.weight&#34;] = (
                f&#34;model.layers.{layer_id}.self_attn.k_proj.weight&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.attn.k_proj.bias&#34;] = (
                f&#34;model.layers.{layer_id}.self_attn.k_proj.bias&#34;
            )

            weight_mapping[f&#34;block.{layer_id}.attn.v_proj.weight&#34;] = (
                f&#34;model.layers.{layer_id}.self_attn.v_proj.weight&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.attn.v_proj.bias&#34;] = (
                f&#34;model.layers.{layer_id}.self_attn.v_proj.bias&#34;
            )

            weight_mapping[f&#34;block.{layer_id}.attn.out.weight&#34;] = (
                f&#34;model.layers.{layer_id}.self_attn.o_proj.weight&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.attn.out.bias&#34;] = (
                f&#34;model.layers.{layer_id}.self_attn.o_proj.bias&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.attn.sinks&#34;] = (
                f&#34;model.layers.{layer_id}.self_attn.sinks&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.attn.norm.scale&#34;] = (
                f&#34;model.layers.{layer_id}.input_layernorm.weight&#34;
            )

            weight_mapping[f&#34;block.{layer_id}.mlp.gate.weight&#34;] = (
                f&#34;model.layers.{layer_id}.mlp.router.weight&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.mlp.gate.bias&#34;] = (
                f&#34;model.layers.{layer_id}.mlp.router.bias&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.mlp.norm.scale&#34;] = (
                f&#34;model.layers.{layer_id}.post_attention_layernorm.weight&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.mlp.experts.gate_up_proj&#34;] = (
                f&#34;model.layers.{layer_id}.mlp.experts.gate_up_proj&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.mlp.gate_up_proj_bias&#34;] = (
                f&#34;model.layers.{layer_id}.mlp.experts.gate_up_proj_bias&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.mlp.down_proj&#34;] = (
                f&#34;model.layers.{layer_id}.mlp.experts.mlp2_weight&#34;
            )
            weight_mapping[f&#34;block.{layer_id}.mlp.down_proj_bias&#34;] = (
                f&#34;model.layers.{layer_id}.mlp.experts.mlp2_bias&#34;
            )

        return weight_mapping

    # TODO beautify code
    def load_weights(
        self,
        weights: Iterable[Tuple[str, torch.Tensor]],
        is_nextn: bool = False,
        weight_name_mapping: dict = None,
    ):
        quant_config_name = (
            self.quant_config.get_name() if self.quant_config is not None else None
        )
        if quant_config_name != &#34;mxfp4&#34;:
            self._load_normal_weights(
                weights, is_nextn=is_nextn, weight_name_mapping=weight_name_mapping
            )
        else:
            self._load_weights_mxfp4(
                weights, is_nextn=is_nextn, weight_name_mapping=weight_name_mapping
            )

    def _load_weights_mxfp4(self, weights, is_nextn, weight_name_mapping):
        mxfp4_weights = []
        normal_weights = []

        for name, weight in weights:
            if (
                &#34;.experts&#34; in name
                and self.quant_config is not None
                and self.quant_config.get_name() == &#34;mxfp4&#34;
            ):
                mxfp4_weights.append((name, weight))
            else:
                normal_weights.append((name, weight))

        mxfp4_loaded_params = self._load_mxfp4_experts_weights(mxfp4_weights)
        self._load_normal_weights(
            normal_weights,
            is_nextn=is_nextn,
            weight_name_mapping=weight_name_mapping,
            other_loaded_param_names=mxfp4_loaded_params,
        )

    def _load_mxfp4_experts_weights(self, weights):

        params_dict = dict(self.named_parameters())
        loaded_params: set[str] = set()
        mxfp4_block = 32

        moe_tp_rank = get_moe_tensor_parallel_rank()
        moe_tp_size = get_moe_tensor_parallel_world_size()
        moe_ep_rank = get_moe_expert_parallel_rank()
        moe_ep_size = get_moe_expert_parallel_world_size()

        intermediate_size = self.config.intermediate_size
        assert (
            intermediate_size % mxfp4_block == 0
        ), f&#34;{intermediate_size=} must be divisible by {mxfp4_block=}&#34;
        intermediate_size_block = intermediate_size // mxfp4_block

        per_rank_intermediate_size_block = math.ceil(
            intermediate_size_block / moe_tp_size
        )

        per_rank_intermediate_size = per_rank_intermediate_size_block * mxfp4_block

        # Calculate common slicing bounds for current rank
        assert self.config.num_local_experts % moe_ep_size == 0
        moe_num_global_experts = self.config.num_local_experts
        moe_num_local_experts = self.config.num_local_experts // moe_ep_size

        moe_tp_rank_start = moe_tp_rank * per_rank_intermediate_size
        moe_tp_rank_end = min(
            (moe_tp_rank + 1) * per_rank_intermediate_size, intermediate_size
        )

        moe_ep_rank_start = moe_ep_rank * moe_num_local_experts
        moe_ep_rank_end = (moe_ep_rank + 1) * moe_num_local_experts

        for name, weight in weights:
            weight = weight.cuda()

            if &#34;gate_up_proj_blocks&#34; in name:
                # Handle MLP gate and up projection weights
                new_name = name.replace(&#34;gate_up_proj_blocks&#34;, &#34;w13_weight&#34;)

                # flat weight from (E, 2 * N, block_size, entry_per_block)
                # to (E, 2 * N, -1), shouldn&#39;t trigger copy for contiguous
                weight = weight.view(
                    moe_num_global_experts, 2 * intermediate_size, -1
                ).contiguous()

                narrow_weight = weight[
                    moe_ep_rank_start:moe_ep_rank_end,
                    2 * moe_tp_rank_start : 2 * moe_tp_rank_end,
                    ...,
                ]

                param = params_dict[new_name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(
                    param,
                    narrow_weight,
                    weight_name=new_name,
                    shard_id=None,
                    expert_id=None,
                )
                loaded_params.add(new_name)

            elif &#34;down_proj_blocks&#34; in name:
                # Handle MLP down projection weights
                new_name = name.replace(&#34;down_proj_blocks&#34;, &#34;w2_weight&#34;)
                # same flatten here, but since 2 mx4 value are packed in 1
                # uint8, divide by 2
                weight = weight.view(
                    moe_num_global_experts, -1, intermediate_size // 2
                ).contiguous()
                narrow_weight = weight[
                    moe_ep_rank_start:moe_ep_rank_end,
                    ...,
                    moe_tp_rank_start // 2 : moe_tp_rank_end // 2,
                ]

                param = params_dict[new_name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(
                    param,
                    narrow_weight,
                    weight_name=new_name,
                    shard_id=None,
                    expert_id=None,
                )
                loaded_params.add(new_name)

            elif &#34;gate_up_proj_scales&#34; in name:
                # Handle MLP gate and up projection weights scale
                new_name = name.replace(&#34;gate_up_proj_scales&#34;, &#34;w13_weight_scale&#34;)
                narrow_weight = weight[
                    moe_ep_rank_start:moe_ep_rank_end,
                    2 * moe_tp_rank_start : 2 * moe_tp_rank_end,
                    ...,
                ]

                param = params_dict[new_name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(
                    param,
                    narrow_weight,
                    weight_name=new_name,
                    shard_id=None,
                    expert_id=None,
                )
                loaded_params.add(new_name)

            elif &#34;down_proj_scales&#34; in name:
                # Handle MLP down projection weights
                new_name = name.replace(&#34;down_proj_scales&#34;, &#34;w2_weight_scale&#34;)
                narrow_weight = weight[
                    moe_ep_rank_start:moe_ep_rank_end,
                    ...,
                    moe_tp_rank_start // mxfp4_block : moe_tp_rank_end // mxfp4_block,
                ]

                param = params_dict[new_name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(
                    param,
                    narrow_weight,
                    weight_name=new_name,
                    shard_id=None,
                    expert_id=None,
                )
                loaded_params.add(new_name)
            elif &#34;gate_up_proj_bias&#34; in name:
                # Handle MLP gate and up projection biases
                new_name = name.replace(&#34;gate_up_proj_bias&#34;, &#34;w13_weight_bias&#34;)

                narrow_weight = weight[
                    moe_ep_rank_start:moe_ep_rank_end,
                    2 * moe_tp_rank_start : 2 * moe_tp_rank_end,
                ]

                param = params_dict[new_name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(
                    param,
                    narrow_weight,
                    weight_name=new_name,
                    shard_id=None,
                    expert_id=None,
                )
                loaded_params.add(new_name)

            elif &#34;down_proj_bias&#34; in name:
                narrow_weight = weight[moe_ep_rank_start:moe_ep_rank_end, ...]
                if moe_tp_rank != 0:
                    narrow_weight = torch.zeros_like(narrow_weight)

                # Handle MLP down projection bias
                new_name = name.replace(&#34;down_proj_bias&#34;, &#34;w2_weight_bias&#34;)
                param = params_dict[new_name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(
                    param,
                    narrow_weight,
                    weight_name=new_name,
                    shard_id=None,
                    expert_id=None,
                )
                loaded_params.add(new_name)

        return loaded_params

    def _load_normal_weights(
        self,
        weights,
        is_nextn: bool,
        weight_name_mapping: dict,
        other_loaded_param_names=[],
    ):
        tp_rank = get_tensor_model_parallel_rank()
        if is_nextn:
            logging.warning(
                &#34;Loading weights for nextn is currently not supported in GptOssForCausalLM. &#34;
            )
            return
        weights = _canonicalize_weights(self.config, weights)
        weights = sorted(weights, key=lambda x: x[0])  # Sort by name for consistency

        new_weights = []
        for name, p in weights:
            if &#34;qkv.weight&#34; in name:
                q_proj, k_proj, v_proj = p.split(
                    [
                        self.config.num_attention_heads * self.config.head_dim,
                        self.config.num_key_value_heads * self.config.head_dim,
                        self.config.num_key_value_heads * self.config.head_dim,
                    ],
                    dim=0,
                )
                new_weights.append(
                    (f&#34;{name.replace(&#39;qkv.weight&#39;, &#39;q_proj.weight&#39;)}&#34;, q_proj)
                )
                new_weights.append(
                    (f&#34;{name.replace(&#39;qkv.weight&#39;, &#39;k_proj.weight&#39;)}&#34;, k_proj)
                )
                new_weights.append(
                    (f&#34;{name.replace(&#39;qkv.weight&#39;, &#39;v_proj.weight&#39;)}&#34;, v_proj)
                )
            elif &#34;qkv.bias&#34; in name:
                q_bias, k_bias, v_bias = p.split(
                    [
                        self.config.num_attention_heads * self.config.head_dim,
                        self.config.num_key_value_heads * self.config.head_dim,
                        self.config.num_key_value_heads * self.config.head_dim,
                    ],
                    dim=0,
                )
                new_weights.append(
                    (f&#34;{name.replace(&#39;qkv.bias&#39;, &#39;q_proj.bias&#39;)}&#34;, q_bias)
                )
                new_weights.append(
                    (f&#34;{name.replace(&#39;qkv.bias&#39;, &#39;k_proj.bias&#39;)}&#34;, k_bias)
                )
                new_weights.append(
                    (f&#34;{name.replace(&#39;qkv.bias&#39;, &#39;v_proj.bias&#39;)}&#34;, v_bias)
                )
            else:
                new_weights.append((name, p))
        weights = new_weights

        # Use provided weight name mapping if available, otherwise use default
        if weight_name_mapping is None:
            weight_name_mapping = self._get_default_weight_mapping()
        else:
            # Merge with default mapping
            default_mapping = self._get_default_weight_mapping()
            default_mapping.update(weight_name_mapping)
            weight_name_mapping = default_mapping

        stacked_params_mapping = [
            # (param_name, shard_name, shard_id)
            (&#34;qkv_proj&#34;, &#34;q_proj&#34;, &#34;q&#34;),
            (&#34;qkv_proj&#34;, &#34;k_proj&#34;, &#34;k&#34;),
            (&#34;qkv_proj&#34;, &#34;v_proj&#34;, &#34;v&#34;),
        ]
        expert_params_mapping = FusedMoE.make_expert_params_mapping_fused(
            ckpt_gate_up_proj_name=&#34;gate_up_proj&#34;,
            ckpt_down_proj_name=&#34;down_proj&#34;,
            ckpt_gate_up_proj_bias_name=&#34;gate_up_proj_bias&#34;,
            ckpt_down_proj_bias_name=&#34;down_proj_bias&#34;,
        )

        params_dict = dict(self.named_parameters())

        for name, loaded_weight in weights:
            loaded_weight = _WeightCreator.maybe_materialize(loaded_weight)

            # Apply weight name mapping if provided
            if weight_name_mapping and name in weight_name_mapping:
                name = weight_name_mapping[name]

            layer_id = get_layer_id(name)
            if (
                layer_id is not None
                and hasattr(self.model, &#34;start_layer&#34;)
                and (
                    layer_id &lt; self.model.start_layer
                    or layer_id &gt;= self.model.end_layer
                )
            ):
                continue

            if &#34;rotary_emb.inv_freq&#34; in name:
                continue
            for param_name, weight_name, shard_id in stacked_params_mapping:
                if weight_name not in name:
                    continue
                if &#34;mlp.experts&#34; in name:
                    continue

                name = name.replace(weight_name, param_name)
                if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                    continue
                if name not in params_dict:
                    continue

                param = params_dict[name]
                weight_loader = param.weight_loader
                weight_loader(param, loaded_weight, shard_id)
                break
            else:
                for mapping in expert_params_mapping:
                    param_name, weight_name, shard_id = mapping
                    if weight_name not in name:
                        continue
                    name = name.replace(weight_name, param_name)
                    if name not in params_dict:
                        continue
                    param = params_dict[name]
                    weight_loader = param.weight_loader
                    if &#34;bias&#34; not in name:
                        loaded_weight = loaded_weight.transpose(-2, -1)
                    if &#34;w2_weight_bias&#34; in name and get_moe_tensor_parallel_rank() != 0:
                        loaded_weight = loaded_weight.zero_()

                    weight_loader(
                        param,
                        loaded_weight,
                        name,
                        shard_id=shard_id,
                    )
                    break
                else:
                    if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                        continue
                    if name not in params_dict:
                        continue
                    if name in params_dict.keys():
                        param = params_dict[name]
                        if &#34;sinks&#34; in name:
                            start = get_attention_tp_rank() * param.numel()
                            param.data.copy_(
                                loaded_weight[start : start + param.numel()]
                            )
                        else:
                            weight_loader = getattr(
                                param, &#34;weight_loader&#34;, default_weight_loader
                            )
                            weight_loader(param, loaded_weight)
                    else:
                        logger.warning(f&#34;Parameter {name} not found in params_dict&#34;)

    def get_embed_and_head(self):
        return self.model.embed_tokens.weight, self.lm_head.weight

    def set_embed_and_head(self, embed, head):
        del self.model.embed_tokens.weight
        del self.lm_head.weight
        self.model.embed_tokens.weight = embed
        self.lm_head.weight = head
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

    def set_eagle3_layers_to_capture(self, layer_ids: Optional[List[int]] = None):
        if not self.pp_group.is_last_rank:
            return

        if layer_ids is None:
            self.capture_aux_hidden_states = True
            num_layers = self.config.num_hidden_layers
            self.model.layers_to_capture = [2, num_layers // 2, num_layers - 3]
        else:
            self.capture_aux_hidden_states = True
            # we plus 1 here because in sglang, for the ith layer, it takes the output
            # of the (i-1)th layer as aux hidden state
            self.model.layers_to_capture = [val + 1 for val in layer_ids]

    @classmethod
    def get_model_config_for_expert_location(cls, config):
        return ModelConfigForExpertLocation(
            num_layers=config.num_hidden_layers,
            num_logical_experts=config.num_local_experts,
            num_groups=None,
        )

    def get_attention_sliding_window_size(self):
        return get_attention_sliding_window_size(self.config)</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.fall_back_to_pt_during_load"><code class="name">var <span class="ident">fall_back_to_pt_during_load</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.get_model_config_for_expert_location"><code class="name flex">
<span>def <span class="ident">get_model_config_for_expert_location</span></span>(<span>config)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.end_layer"><code class="name">prop <span class="ident">end_layer</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def end_layer(self):
    return self.model.end_layer</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.routed_experts_weights_of_layer"><code class="name">prop <span class="ident">routed_experts_weights_of_layer</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def routed_experts_weights_of_layer(self):
    return self._routed_experts_weights_of_layer.value</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.start_layer"><code class="name">prop <span class="ident">start_layer</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def start_layer(self):
    return self.model.start_layer</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>input_ids: torch.Tensor,<br>positions: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>input_embeds: torch.Tensor = None,<br>pp_proxy_tensors: <a title="sglang.srt.model_executor.forward_batch_info.PPProxyTensors" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.PPProxyTensors">PPProxyTensors</a> | None = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def forward(
    self,
    input_ids: torch.Tensor,
    positions: torch.Tensor,
    forward_batch: ForwardBatch,
    input_embeds: torch.Tensor = None,
    pp_proxy_tensors: Optional[PPProxyTensors] = None,
) -&gt; torch.Tensor:
    hidden_states = self.model(
        input_ids,
        positions,
        forward_batch,
        input_embeds,
        pp_proxy_tensors=pp_proxy_tensors,
    )

    aux_hidden_states = None
    if self.capture_aux_hidden_states:
        hidden_states, aux_hidden_states = hidden_states

    if self.pp_group.is_last_rank:
        return self.logits_processor(
            input_ids,
            hidden_states,
            self.lm_head,
            forward_batch,
            aux_hidden_states,
        )
    else:
        return hidden_states</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.get_attention_sliding_window_size"><code class="name flex">
<span>def <span class="ident">get_attention_sliding_window_size</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_attention_sliding_window_size(self):
    return get_attention_sliding_window_size(self.config)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.get_embed_and_head"><code class="name flex">
<span>def <span class="ident">get_embed_and_head</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_embed_and_head(self):
    return self.model.embed_tokens.weight, self.lm_head.weight</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.load_weights"><code class="name flex">
<span>def <span class="ident">load_weights</span></span>(<span>self,<br>weights: Iterable[typing.Tuple[str, torch.Tensor]],<br>is_nextn: bool = False,<br>weight_name_mapping: dict = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_weights(
    self,
    weights: Iterable[Tuple[str, torch.Tensor]],
    is_nextn: bool = False,
    weight_name_mapping: dict = None,
):
    quant_config_name = (
        self.quant_config.get_name() if self.quant_config is not None else None
    )
    if quant_config_name != &#34;mxfp4&#34;:
        self._load_normal_weights(
            weights, is_nextn=is_nextn, weight_name_mapping=weight_name_mapping
        )
    else:
        self._load_weights_mxfp4(
            weights, is_nextn=is_nextn, weight_name_mapping=weight_name_mapping
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.set_eagle3_layers_to_capture"><code class="name flex">
<span>def <span class="ident">set_eagle3_layers_to_capture</span></span>(<span>self, layer_ids: List[int] | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_eagle3_layers_to_capture(self, layer_ids: Optional[List[int]] = None):
    if not self.pp_group.is_last_rank:
        return

    if layer_ids is None:
        self.capture_aux_hidden_states = True
        num_layers = self.config.num_hidden_layers
        self.model.layers_to_capture = [2, num_layers // 2, num_layers - 3]
    else:
        self.capture_aux_hidden_states = True
        # we plus 1 here because in sglang, for the ith layer, it takes the output
        # of the (i-1)th layer as aux hidden state
        self.model.layers_to_capture = [val + 1 for val in layer_ids]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssForCausalLM.set_embed_and_head"><code class="name flex">
<span>def <span class="ident">set_embed_and_head</span></span>(<span>self, embed, head)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_embed_and_head(self, embed, head):
    del self.model.embed_tokens.weight
    del self.lm_head.weight
    self.model.embed_tokens.weight = embed
    self.lm_head.weight = head
    torch.cuda.empty_cache()
    torch.cuda.synchronize()</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssModel"><code class="flex name class">
<span>class <span class="ident">GptOssModel</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '',<br>decoder_layer_type: type[torch.nn.modules.module.Module] = sglang.srt.models.gpt_oss.GptOssDecoderLayer)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GptOssModel(nn.Module):
    def __init__(
        self,
        config: PretrainedConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
        decoder_layer_type: type[nn.Module] = GptOssDecoderLayer,
    ) -&gt; None:
        super().__init__()
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size
        self.pp_group = get_pp_group()

        if self.pp_group.is_first_rank:
            self.embed_tokens = VocabParallelEmbedding(
                config.vocab_size,
                config.hidden_size,
                enable_tp=not is_dp_attention_enabled(),
                prefix=add_prefix(&#34;embed_tokens&#34;, prefix),
            )
        else:
            self.embed_tokens = PPMissingLayer()

        # Use the provided decoder layer type or default to GptOssDecoderLayer
        decoder_layer_type = decoder_layer_type or GptOssDecoderLayer
        self.layers, self.start_layer, self.end_layer = make_layers(
            config.num_hidden_layers,
            lambda idx, prefix: decoder_layer_type(
                layer_id=idx,
                config=config,
                quant_config=quant_config,
                prefix=prefix,
            ),
            pp_rank=self.pp_group.rank_in_group,
            pp_size=self.pp_group.world_size,
            prefix=add_prefix(&#34;layers&#34;, prefix),
        )
        if self.pp_group.is_last_rank:
            self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        else:
            self.norm = PPMissingLayer(return_tuple=True)

        self.layers_to_capture = []

    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        forward_batch: ForwardBatch,
        input_embeds: torch.Tensor = None,
        pp_proxy_tensors: Optional[PPProxyTensors] = None,
    ) -&gt; Union[torch.Tensor, PPProxyTensors]:
        if self.pp_group.is_first_rank:
            if input_embeds is None:
                hidden_states = self.embed_tokens(input_ids)
            else:
                hidden_states = input_embeds
            residual = None
        else:
            assert pp_proxy_tensors is not None
            hidden_states = pp_proxy_tensors[&#34;hidden_states&#34;]
            residual = pp_proxy_tensors[&#34;residual&#34;]

        aux_hidden_states = []
        for i in range(self.start_layer, self.end_layer):
            with get_global_expert_distribution_recorder().with_current_layer(i):
                if i in self.layers_to_capture:
                    aux_hidden_states.append(hidden_states + residual)
                layer = self.layers[i]
                hidden_states, residual = layer(
                    positions, hidden_states, forward_batch, residual
                )
        if not self.pp_group.is_last_rank:
            return PPProxyTensors(
                {
                    &#34;hidden_states&#34;: hidden_states,
                    &#34;residual&#34;: residual,
                }
            )
        else:
            if hidden_states.shape[0] != 0:
                if residual is None:
                    hidden_states = self.norm(hidden_states)
                else:
                    hidden_states, _ = self.norm(hidden_states, residual)
        if len(aux_hidden_states) == 0:
            return hidden_states

        return hidden_states, aux_hidden_states</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gpt_oss.GptOssModel.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>input_ids: torch.Tensor,<br>positions: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>input_embeds: torch.Tensor = None,<br>pp_proxy_tensors: <a title="sglang.srt.model_executor.forward_batch_info.PPProxyTensors" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.PPProxyTensors">PPProxyTensors</a> | None = None) ‑> torch.Tensor | <a title="sglang.srt.model_executor.forward_batch_info.PPProxyTensors" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.PPProxyTensors">PPProxyTensors</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    input_ids: torch.Tensor,
    positions: torch.Tensor,
    forward_batch: ForwardBatch,
    input_embeds: torch.Tensor = None,
    pp_proxy_tensors: Optional[PPProxyTensors] = None,
) -&gt; Union[torch.Tensor, PPProxyTensors]:
    if self.pp_group.is_first_rank:
        if input_embeds is None:
            hidden_states = self.embed_tokens(input_ids)
        else:
            hidden_states = input_embeds
        residual = None
    else:
        assert pp_proxy_tensors is not None
        hidden_states = pp_proxy_tensors[&#34;hidden_states&#34;]
        residual = pp_proxy_tensors[&#34;residual&#34;]

    aux_hidden_states = []
    for i in range(self.start_layer, self.end_layer):
        with get_global_expert_distribution_recorder().with_current_layer(i):
            if i in self.layers_to_capture:
                aux_hidden_states.append(hidden_states + residual)
            layer = self.layers[i]
            hidden_states, residual = layer(
                positions, hidden_states, forward_batch, residual
            )
    if not self.pp_group.is_last_rank:
        return PPProxyTensors(
            {
                &#34;hidden_states&#34;: hidden_states,
                &#34;residual&#34;: residual,
            }
        )
    else:
        if hidden_states.shape[0] != 0:
            if residual is None:
                hidden_states = self.norm(hidden_states)
            else:
                hidden_states, _ = self.norm(hidden_states, residual)
    if len(aux_hidden_states) == 0:
        return hidden_states

    return hidden_states, aux_hidden_states</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssSparseMoeBlock"><code class="flex name class">
<span>class <span class="ident">GptOssSparseMoeBlock</span></span>
<span>(</span><span>layer_id: int,<br>config: <a title="sglang.srt.models.gpt_oss.GptOssConfig" href="#sglang.srt.models.gpt_oss.GptOssConfig">GptOssConfig</a>,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GptOssSparseMoeBlock(nn.Module):
    def __init__(
        self,
        layer_id: int,
        config: GptOssConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ):
        super().__init__()
        self.tp_size = get_tensor_model_parallel_world_size()
        self.layer_id = layer_id
        self.activation = config.hidden_act
        self.gemm1_alpha = getattr(config, &#34;hidden_act_alpha&#34;, 1.702)
        self.gemm1_clamp_limit = config.swiglu_limit

        self.topk = TopK(
            top_k=config.num_experts_per_tok,
            renormalize=True,
        )

        self.top_k = config.num_experts_per_tok
        experts_type = get_moe_impl_class()
        extra_kwargs = {}
        if experts_type.__name__ == &#34;FusedMoE&#34;:
            quant_config_name = (
                quant_config.get_name() if quant_config is not None else None
            )
            extra_kwargs = {
                # for moe gate_up_proj and down_proj and their bias loading
                &#34;use_weight_loader_fused&#34;: quant_config_name
                != &#34;mxfp4&#34;
            }
        self.experts = experts_type(
            num_experts=config.num_local_experts
            + global_server_args_dict[&#34;ep_num_redundant_experts&#34;],
            top_k=config.num_experts_per_tok,
            layer_id=layer_id,
            hidden_size=config.hidden_size,
            intermediate_size=config.intermediate_size,
            quant_config=quant_config,
            activation=self.activation,
            gemm1_alpha=self.gemm1_alpha,
            gemm1_clamp_limit=self.gemm1_clamp_limit,
            with_bias=True,
            prefix=add_prefix(&#34;experts&#34;, prefix),
            **extra_kwargs,
        )

        self.router = ReplicatedLinear(
            config.hidden_size,
            config.num_local_experts,
            bias=True,
            quant_config=None,
            prefix=add_prefix(&#34;gate&#34;, prefix),
            params_dtype=config.torch_dtype,
        )

    def forward(
        self,
        hidden_states: torch.Tensor,
        forward_batch: Optional[ForwardBatch] = None,
        should_allreduce_fusion: bool = False,
    ) -&gt; torch.Tensor:
        if not get_moe_a2a_backend().is_deepep():
            return self.forward_normal(hidden_states, should_allreduce_fusion)
        else:
            raise Exception(&#34;forward_deepep branch not implemented yet&#34;)

    def get_moe_weights(self):
        return [
            x.data
            for name, x in self.experts.named_parameters()
            if name not in [&#34;correction_bias&#34;]
        ]

    def forward_normal(
        self,
        hidden_states: torch.Tensor,
        should_allreduce_fusion: bool = False,
    ) -&gt; torch.Tensor:
        num_tokens, hidden_dim = hidden_states.shape

        router_logits, _ = self.router(hidden_states)
        topk_output = self.topk(hidden_states, router_logits)
        final_hidden_states = self.experts(hidden_states, topk_output)

        if self.tp_size &gt; 1 and not should_allreduce_fusion:
            final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)

        ans = final_hidden_states.view(num_tokens, hidden_dim)
        return ans</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gpt_oss.GptOssSparseMoeBlock.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>hidden_states: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a> | None = None,<br>should_allreduce_fusion: bool = False) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    hidden_states: torch.Tensor,
    forward_batch: Optional[ForwardBatch] = None,
    should_allreduce_fusion: bool = False,
) -&gt; torch.Tensor:
    if not get_moe_a2a_backend().is_deepep():
        return self.forward_normal(hidden_states, should_allreduce_fusion)
    else:
        raise Exception(&#34;forward_deepep branch not implemented yet&#34;)</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssSparseMoeBlock.forward_normal"><code class="name flex">
<span>def <span class="ident">forward_normal</span></span>(<span>self, hidden_states: torch.Tensor, should_allreduce_fusion: bool = False) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_normal(
    self,
    hidden_states: torch.Tensor,
    should_allreduce_fusion: bool = False,
) -&gt; torch.Tensor:
    num_tokens, hidden_dim = hidden_states.shape

    router_logits, _ = self.router(hidden_states)
    topk_output = self.topk(hidden_states, router_logits)
    final_hidden_states = self.experts(hidden_states, topk_output)

    if self.tp_size &gt; 1 and not should_allreduce_fusion:
        final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)

    ans = final_hidden_states.view(num_tokens, hidden_dim)
    return ans</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gpt_oss.GptOssSparseMoeBlock.get_moe_weights"><code class="name flex">
<span>def <span class="ident">get_moe_weights</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_moe_weights(self):
    return [
        x.data
        for name, x in self.experts.named_parameters()
        if name not in [&#34;correction_bias&#34;]
    ]</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.models" href="index.html">sglang.srt.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.models.gpt_oss.get_attention_sliding_window_size" href="#sglang.srt.models.gpt_oss.get_attention_sliding_window_size">get_attention_sliding_window_size</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.models.gpt_oss.GptOssAttention" href="#sglang.srt.models.gpt_oss.GptOssAttention">GptOssAttention</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gpt_oss.GptOssAttention.forward" href="#sglang.srt.models.gpt_oss.GptOssAttention.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssAttention.forward_core" href="#sglang.srt.models.gpt_oss.GptOssAttention.forward_core">forward_core</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssAttention.forward_prepare" href="#sglang.srt.models.gpt_oss.GptOssAttention.forward_prepare">forward_prepare</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gpt_oss.GptOssConfig" href="#sglang.srt.models.gpt_oss.GptOssConfig">GptOssConfig</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gpt_oss.GptOssConfig.model_type" href="#sglang.srt.models.gpt_oss.GptOssConfig.model_type">model_type</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gpt_oss.GptOssDecoderLayer" href="#sglang.srt.models.gpt_oss.GptOssDecoderLayer">GptOssDecoderLayer</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gpt_oss.GptOssDecoderLayer.forward" href="#sglang.srt.models.gpt_oss.GptOssDecoderLayer.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM">GptOssForCausalLM</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.end_layer" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.end_layer">end_layer</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.fall_back_to_pt_during_load" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.fall_back_to_pt_during_load">fall_back_to_pt_during_load</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.forward" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.get_attention_sliding_window_size" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.get_attention_sliding_window_size">get_attention_sliding_window_size</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.get_embed_and_head" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.get_embed_and_head">get_embed_and_head</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.get_model_config_for_expert_location" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.get_model_config_for_expert_location">get_model_config_for_expert_location</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.load_weights" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.load_weights">load_weights</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.routed_experts_weights_of_layer" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.routed_experts_weights_of_layer">routed_experts_weights_of_layer</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.set_eagle3_layers_to_capture" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.set_eagle3_layers_to_capture">set_eagle3_layers_to_capture</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.set_embed_and_head" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.set_embed_and_head">set_embed_and_head</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.start_layer" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.start_layer">start_layer</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM">GptOssForCausalLM</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.end_layer" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.end_layer">end_layer</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.fall_back_to_pt_during_load" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.fall_back_to_pt_during_load">fall_back_to_pt_during_load</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.forward" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.get_attention_sliding_window_size" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.get_attention_sliding_window_size">get_attention_sliding_window_size</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.get_embed_and_head" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.get_embed_and_head">get_embed_and_head</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.get_model_config_for_expert_location" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.get_model_config_for_expert_location">get_model_config_for_expert_location</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.load_weights" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.load_weights">load_weights</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.routed_experts_weights_of_layer" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.routed_experts_weights_of_layer">routed_experts_weights_of_layer</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.set_eagle3_layers_to_capture" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.set_eagle3_layers_to_capture">set_eagle3_layers_to_capture</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.set_embed_and_head" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.set_embed_and_head">set_embed_and_head</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssForCausalLM.start_layer" href="#sglang.srt.models.gpt_oss.GptOssForCausalLM.start_layer">start_layer</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gpt_oss.GptOssModel" href="#sglang.srt.models.gpt_oss.GptOssModel">GptOssModel</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gpt_oss.GptOssModel.forward" href="#sglang.srt.models.gpt_oss.GptOssModel.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gpt_oss.GptOssSparseMoeBlock" href="#sglang.srt.models.gpt_oss.GptOssSparseMoeBlock">GptOssSparseMoeBlock</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gpt_oss.GptOssSparseMoeBlock.forward" href="#sglang.srt.models.gpt_oss.GptOssSparseMoeBlock.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssSparseMoeBlock.forward_normal" href="#sglang.srt.models.gpt_oss.GptOssSparseMoeBlock.forward_normal">forward_normal</a></code></li>
<li><code><a title="sglang.srt.models.gpt_oss.GptOssSparseMoeBlock.get_moe_weights" href="#sglang.srt.models.gpt_oss.GptOssSparseMoeBlock.get_moe_weights">get_moe_weights</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
