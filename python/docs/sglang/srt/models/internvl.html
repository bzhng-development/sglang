<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.models.internvl API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.models.internvl</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.models.internvl.InternAttention"><code class="flex name class">
<span>class <span class="ident">InternAttention</span></span>
<span>(</span><span>config,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InternAttention(nn.Module):
    def __init__(
        self,
        config,
        quant_config: QuantizationConfig = None,
    ):
        super().__init__()
        self.config = config
        self.embed_dim = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.embed_dim // self.num_heads
        self.scale = self.head_dim**-0.5

        self.attn = VisionAttention(
            qkv_backend=&#34;fa3&#34;,
            embed_dim=self.embed_dim,
            num_heads=self.num_heads,
            projection_size=self.embed_dim,
            use_qkv_parallel=True,
            quant_config=quant_config,
            dropout=getattr(config, &#34;dropout&#34;, 0.0),
            qkv_bias=getattr(config, &#34;qkv_bias&#34;, False)
            or getattr(config, &#34;attention_bias&#34;, False),
            num_dummy_heads=getattr(config, &#34;num_dummy_heads&#34;, 0),
            qk_normalization=getattr(config, &#34;qk_normalization&#34;, False)
            or getattr(config, &#34;use_qk_norm&#34;, False),
            flatten_batch=False,
        )

        self.proj_drop = nn.Dropout(config.dropout)

    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
    ) -&gt; torch.Tensor:
        out = self.attn(hidden_states, cu_seqlens=cu_seqlens)
        outs = self.proj_drop(out)
        return outs</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.internvl.InternAttention.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    hidden_states: torch.Tensor,
    cu_seqlens: torch.Tensor,
) -&gt; torch.Tensor:
    out = self.attn(hidden_states, cu_seqlens=cu_seqlens)
    outs = self.proj_drop(out)
    return outs</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.internvl.InternMLP"><code class="flex name class">
<span>class <span class="ident">InternMLP</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InternMLP(nn.Module):
    def __init__(self, config: PretrainedConfig):
        super().__init__()
        self.config = config
        self.act = ACT2FN[config.hidden_act]
        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)
        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)

    def forward(self, hidden_states: torch.Tensor) -&gt; torch.Tensor:
        hidden_states = self.fc1(hidden_states)
        hidden_states = self.act(hidden_states)
        hidden_states = self.fc2(hidden_states)
        return hidden_states</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.internvl.InternMLP.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, hidden_states: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, hidden_states: torch.Tensor) -&gt; torch.Tensor:
    hidden_states = self.fc1(hidden_states)
    hidden_states = self.act(hidden_states)
    hidden_states = self.fc2(hidden_states)
    return hidden_states</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.internvl.InternRMSNorm"><code class="flex name class">
<span>class <span class="ident">InternRMSNorm</span></span>
<span>(</span><span>hidden_size, eps=1e-06)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InternRMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.internvl.InternRMSNorm.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, hidden_states) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, hidden_states):
    input_dtype = hidden_states.dtype
    hidden_states = hidden_states.to(torch.float32)
    variance = hidden_states.pow(2).mean(-1, keepdim=True)
    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
    return self.weight * hidden_states.to(input_dtype)</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.internvl.InternVLChatModel"><code class="flex name class">
<span>class <span class="ident">InternVLChatModel</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>use_flash_attn=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InternVLChatModel(nn.Module):
    def __init__(
        self,
        config: PretrainedConfig,
        quant_config: Optional[QuantizationConfig] = None,
        use_flash_attn=True,
    ) -&gt; None:
        super().__init__()
        self.config = config
        self.quant_config = quant_config
        vision_utils.update_vit_attn_dummy_heads_config(self.config)
        image_size = config.force_image_size or config.vision_config.image_size
        patch_size = config.vision_config.patch_size
        self.patch_size = patch_size
        self.select_layer = config.select_layer
        self.template = config.template
        self.num_image_token = int(
            (image_size // patch_size) ** 2 * (config.downsample_ratio**2)
        )
        self.downsample_ratio = config.downsample_ratio
        self.ps_version = config.ps_version

        config.vision_config.use_flash_attn = True if use_flash_attn else False
        config.llm_config._attn_implementation = (
            &#34;flash_attention_2&#34; if use_flash_attn else &#34;eager&#34;
        )

        logger.info(f&#34;num_image_token: {self.num_image_token}&#34;)
        logger.info(f&#34;ps_version: {self.ps_version}&#34;)

        self.vision_model = InternVisionModel(config.vision_config)
        if config.llm_config.architectures[0] == &#34;Qwen2ForCausalLM&#34;:
            self.language_model = Qwen2ForCausalLM(
                config=config.llm_config, quant_config=quant_config
            )
        elif config.llm_config.architectures[0] == &#34;InternLM2ForCausalLM&#34;:
            self.language_model = InternLM2ForCausalLM(
                config=config.llm_config, quant_config=quant_config
            )
        elif config.llm_config.architectures[0] == &#34;Qwen3MoeForCausalLM&#34;:
            self.language_model = Qwen3MoeForCausalLM(
                config=config.llm_config, quant_config=quant_config
            )
        else:
            raise NotImplementedError(
                f&#34;{config.llm_config.architectures[0]} is not implemented.&#34;
            )

        vit_hidden_size = config.vision_config.hidden_size
        llm_hidden_size = config.llm_config.hidden_size

        self.mlp1 = nn.Sequential(
            nn.LayerNorm(vit_hidden_size * int(1 / self.downsample_ratio) ** 2),
            nn.Linear(
                vit_hidden_size * int(1 / self.downsample_ratio) ** 2, llm_hidden_size
            ),
            nn.GELU(),
            nn.Linear(llm_hidden_size, llm_hidden_size),
        )

    def pixel_shuffle(self, x, scale_factor=0.5):
        n, w, h, c = x.size()
        # N, W, H, C --&gt; N, W, H * scale, C // scale
        x = x.view(n, w, int(h * scale_factor), int(c / scale_factor))
        # N, W, H * scale, C // scale --&gt; N, H * scale, W, C // scale
        x = x.permute(0, 2, 1, 3).contiguous()
        # N, H * scale, W, C // scale --&gt; N, H * scale, W * scale, C // (scale ** 2)
        x = x.view(
            n,
            int(h * scale_factor),
            int(w * scale_factor),
            int(c / (scale_factor * scale_factor)),
        )
        if self.ps_version == &#34;v1&#34;:
            logger.warn(
                &#34;In ps_version &#39;v1&#39;, the height and width have not been swapped back, &#34;
                &#34;which results in a transposed image.&#34;
            )
        else:
            x = x.permute(0, 2, 1, 3).contiguous()
        return x

    def extract_feature(self, pixel_values):
        if self.select_layer == -1:
            vit_embeds = self.vision_model(
                pixel_values=pixel_values, output_hidden_states=False, return_dict=True
            ).last_hidden_state
        else:
            vit_embeds = self.vision_model(
                pixel_values=pixel_values, output_hidden_states=True, return_dict=True
            ).hidden_states[self.select_layer]
        vit_embeds = vit_embeds[:, 1:, :]

        h = w = int(vit_embeds.shape[1] ** 0.5)
        vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], h, w, -1)
        vit_embeds = self.pixel_shuffle(vit_embeds, scale_factor=self.downsample_ratio)
        vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], -1, vit_embeds.shape[-1])
        vit_embeds = self.mlp1(vit_embeds)
        return vit_embeds

    def get_image_feature(self, items: List[MultimodalDataItem]):
        &#34;&#34;&#34;
        Projects the last hidden state from the vision model into language model space.

        Returns:
            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).
        &#34;&#34;&#34;
        pixel_values = torch.cat([item.feature for item in items])
        image_features = self.extract_feature(pixel_values)
        return image_features

    @torch.no_grad()
    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        forward_batch: ForwardBatch,
        input_embeds: torch.Tensor = None,
    ) -&gt; torch.Tensor:

        hs = general_mm_embed_routine(
            input_ids=input_ids,
            forward_batch=forward_batch,
            language_model=self.language_model,
            data_embedding_funcs={
                Modality.IMAGE: self.get_image_feature,
            },
            positions=positions,
        )

        return hs

    def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs):
        # Get all special token IDs
        im_start_id: int = mm_inputs.im_start_id
        im_end_id: int = mm_inputs.im_end_id

        media_token_pairs = [(im_start_id, im_end_id)]
        helper = MultiModalityDataPaddingPatternTokenPairs(media_token_pairs)

        return helper.pad_input_tokens(input_ids, mm_inputs)

    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
        expert_params_mapping = []
        if &#34;InternLM2ForCausalLM&#34; in self.config.llm_config.architectures:
            stacked_params_mapping = [
                # (param_name, shard_name, shard_id)
                (&#34;gate_up_proj&#34;, &#34;w1&#34;, 0),
                (&#34;gate_up_proj&#34;, &#34;w3&#34;, 1),
            ]
        elif &#34;Qwen2ForCausalLM&#34; in self.config.llm_config.architectures:
            stacked_params_mapping = [
                # (param_name, shard_name, shard_id)
                (&#34;qkv_proj&#34;, &#34;q_proj&#34;, &#34;q&#34;),
                (&#34;qkv_proj&#34;, &#34;k_proj&#34;, &#34;k&#34;),
                (&#34;qkv_proj&#34;, &#34;v_proj&#34;, &#34;v&#34;),
                (&#34;gate_up_proj&#34;, &#34;gate_proj&#34;, 0),
                (&#34;gate_up_proj&#34;, &#34;up_proj&#34;, 1),
            ]
        elif &#34;Qwen3MoeForCausalLM&#34; in self.config.llm_config.architectures:
            stacked_params_mapping = [
                # (param_name, shard_name, shard_id)
                (&#34;qkv_proj&#34;, &#34;q_proj&#34;, &#34;q&#34;),
                (&#34;qkv_proj&#34;, &#34;k_proj&#34;, &#34;k&#34;),
                (&#34;qkv_proj&#34;, &#34;v_proj&#34;, &#34;v&#34;),
                (&#34;gate_up_proj&#34;, &#34;gate_proj&#34;, 0),
                (&#34;gate_up_proj&#34;, &#34;up_proj&#34;, 1),
            ]

            expert_params_mapping = FusedMoE.make_expert_params_mapping(
                ckpt_gate_proj_name=&#34;gate_proj&#34;,
                ckpt_down_proj_name=&#34;down_proj&#34;,
                ckpt_up_proj_name=&#34;up_proj&#34;,
                num_experts=self.config.num_experts,
            )

        params_dict = dict(self.named_parameters())
        loaded_params: Set[str] = set()

        for name, loaded_weight in weights:
            if &#34;rotary_emb.inv_freq&#34; in name:
                continue

            for param_name, weight_name, shard_id in stacked_params_mapping:
                if weight_name not in name:
                    continue
                # We have mlp.experts[0].gate_proj in the checkpoint.
                # Since we handle the experts below in expert_params_mapping,
                # we need to skip here BEFORE we update the name, otherwise
                # name will be updated to mlp.experts[0].gate_up_proj, which
                # will then be updated below in expert_params_mapping
                # for mlp.experts[0].gate_gate_up_proj, which breaks load.
                if &#34;mlp.experts&#34; in name:
                    continue
                name = name.replace(weight_name, param_name)
                # Skip loading extra bias for GPTQ models.
                if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                    continue
                param = params_dict[name]
                weight_loader = param.weight_loader
                weight_loader(param, loaded_weight, shard_id)
                break
            else:
                if &#34;vision_model&#34; in name:
                    # adapt to VisionAttention
                    name = name.replace(r&#34;attn.&#34;, r&#34;attn.attn.&#34;)
                    name = name.replace(r&#34;qkv.&#34;, r&#34;qkv_proj.&#34;)

                for mapping in expert_params_mapping:
                    param_name, weight_name, expert_id, shard_id = mapping
                    if weight_name not in name:
                        continue
                    name = name.replace(weight_name, param_name)
                    param = params_dict[name]
                    weight_loader = param.weight_loader
                    weight_loader(
                        param,
                        loaded_weight,
                        name,
                        shard_id=shard_id,
                        expert_id=expert_id,
                    )
                    break
                else:
                    # Skip loading extra bias for GPTQ models.
                    if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                        continue
                    param = params_dict[name]
                    if &#34;wqkv&#34; in name:
                        config = self.config
                        kv_groups = (
                            config.num_attention_heads // config.num_key_value_heads
                        )
                        head_dim = config.hidden_size // config.num_attention_heads
                        loaded_weight = loaded_weight.view(
                            -1, 2 + kv_groups, head_dim, loaded_weight.shape[-1]
                        )
                        wq, wk, wv = torch.split(
                            loaded_weight, [kv_groups, 1, 1], dim=1
                        )
                        wq = wq.reshape(-1, wq.shape[-1])
                        wk = wk.reshape(-1, wk.shape[-1])
                        wv = wv.reshape(-1, wv.shape[-1])
                        weight_loader = param.weight_loader
                        weight_loader(param, wq, &#34;q&#34;)
                        weight_loader(param, wk, &#34;k&#34;)
                        weight_loader(param, wv, &#34;v&#34;)
                    else:
                        weight_loader = getattr(
                            param, &#34;weight_loader&#34;, default_weight_loader
                        )
                        if &#34;vision_model&#34; in name:
                            loaded_weight = vision_utils.pad_vit_attn_dummy_heads(
                                self.config, name, loaded_weight
                            )
                        weight_loader(param, loaded_weight)

            loaded_params.add(name)
        unloaded_params = params_dict.keys() - loaded_params
        if unloaded_params:
            raise RuntimeError(
                f&#34;Some weights are not initialized from checkpoints: {unloaded_params}&#34;
            )
        return loaded_params</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.internvl.InternVLChatModel.extract_feature"><code class="name flex">
<span>def <span class="ident">extract_feature</span></span>(<span>self, pixel_values)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_feature(self, pixel_values):
    if self.select_layer == -1:
        vit_embeds = self.vision_model(
            pixel_values=pixel_values, output_hidden_states=False, return_dict=True
        ).last_hidden_state
    else:
        vit_embeds = self.vision_model(
            pixel_values=pixel_values, output_hidden_states=True, return_dict=True
        ).hidden_states[self.select_layer]
    vit_embeds = vit_embeds[:, 1:, :]

    h = w = int(vit_embeds.shape[1] ** 0.5)
    vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], h, w, -1)
    vit_embeds = self.pixel_shuffle(vit_embeds, scale_factor=self.downsample_ratio)
    vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], -1, vit_embeds.shape[-1])
    vit_embeds = self.mlp1(vit_embeds)
    return vit_embeds</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.internvl.InternVLChatModel.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>input_ids: torch.Tensor,<br>positions: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>input_embeds: torch.Tensor = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def forward(
    self,
    input_ids: torch.Tensor,
    positions: torch.Tensor,
    forward_batch: ForwardBatch,
    input_embeds: torch.Tensor = None,
) -&gt; torch.Tensor:

    hs = general_mm_embed_routine(
        input_ids=input_ids,
        forward_batch=forward_batch,
        language_model=self.language_model,
        data_embedding_funcs={
            Modality.IMAGE: self.get_image_feature,
        },
        positions=positions,
    )

    return hs</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.internvl.InternVLChatModel.get_image_feature"><code class="name flex">
<span>def <span class="ident">get_image_feature</span></span>(<span>self,<br>items: List[<a title="sglang.srt.managers.schedule_batch.MultimodalDataItem" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.MultimodalDataItem">MultimodalDataItem</a>])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_image_feature(self, items: List[MultimodalDataItem]):
    &#34;&#34;&#34;
    Projects the last hidden state from the vision model into language model space.

    Returns:
        image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).
    &#34;&#34;&#34;
    pixel_values = torch.cat([item.feature for item in items])
    image_features = self.extract_feature(pixel_values)
    return image_features</code></pre>
</details>
<div class="desc"><p>Projects the last hidden state from the vision model into language model space.</p>
<h2 id="returns">Returns</h2>
<p>image_features (<code>torch.Tensor</code>): Image feature tensor of shape <code>(num_images, image_length, embed_dim)</code>).</p></div>
</dd>
<dt id="sglang.srt.models.internvl.InternVLChatModel.load_weights"><code class="name flex">
<span>def <span class="ident">load_weights</span></span>(<span>self, weights: Iterable[Tuple[str, torch.Tensor]])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
    expert_params_mapping = []
    if &#34;InternLM2ForCausalLM&#34; in self.config.llm_config.architectures:
        stacked_params_mapping = [
            # (param_name, shard_name, shard_id)
            (&#34;gate_up_proj&#34;, &#34;w1&#34;, 0),
            (&#34;gate_up_proj&#34;, &#34;w3&#34;, 1),
        ]
    elif &#34;Qwen2ForCausalLM&#34; in self.config.llm_config.architectures:
        stacked_params_mapping = [
            # (param_name, shard_name, shard_id)
            (&#34;qkv_proj&#34;, &#34;q_proj&#34;, &#34;q&#34;),
            (&#34;qkv_proj&#34;, &#34;k_proj&#34;, &#34;k&#34;),
            (&#34;qkv_proj&#34;, &#34;v_proj&#34;, &#34;v&#34;),
            (&#34;gate_up_proj&#34;, &#34;gate_proj&#34;, 0),
            (&#34;gate_up_proj&#34;, &#34;up_proj&#34;, 1),
        ]
    elif &#34;Qwen3MoeForCausalLM&#34; in self.config.llm_config.architectures:
        stacked_params_mapping = [
            # (param_name, shard_name, shard_id)
            (&#34;qkv_proj&#34;, &#34;q_proj&#34;, &#34;q&#34;),
            (&#34;qkv_proj&#34;, &#34;k_proj&#34;, &#34;k&#34;),
            (&#34;qkv_proj&#34;, &#34;v_proj&#34;, &#34;v&#34;),
            (&#34;gate_up_proj&#34;, &#34;gate_proj&#34;, 0),
            (&#34;gate_up_proj&#34;, &#34;up_proj&#34;, 1),
        ]

        expert_params_mapping = FusedMoE.make_expert_params_mapping(
            ckpt_gate_proj_name=&#34;gate_proj&#34;,
            ckpt_down_proj_name=&#34;down_proj&#34;,
            ckpt_up_proj_name=&#34;up_proj&#34;,
            num_experts=self.config.num_experts,
        )

    params_dict = dict(self.named_parameters())
    loaded_params: Set[str] = set()

    for name, loaded_weight in weights:
        if &#34;rotary_emb.inv_freq&#34; in name:
            continue

        for param_name, weight_name, shard_id in stacked_params_mapping:
            if weight_name not in name:
                continue
            # We have mlp.experts[0].gate_proj in the checkpoint.
            # Since we handle the experts below in expert_params_mapping,
            # we need to skip here BEFORE we update the name, otherwise
            # name will be updated to mlp.experts[0].gate_up_proj, which
            # will then be updated below in expert_params_mapping
            # for mlp.experts[0].gate_gate_up_proj, which breaks load.
            if &#34;mlp.experts&#34; in name:
                continue
            name = name.replace(weight_name, param_name)
            # Skip loading extra bias for GPTQ models.
            if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                continue
            param = params_dict[name]
            weight_loader = param.weight_loader
            weight_loader(param, loaded_weight, shard_id)
            break
        else:
            if &#34;vision_model&#34; in name:
                # adapt to VisionAttention
                name = name.replace(r&#34;attn.&#34;, r&#34;attn.attn.&#34;)
                name = name.replace(r&#34;qkv.&#34;, r&#34;qkv_proj.&#34;)

            for mapping in expert_params_mapping:
                param_name, weight_name, expert_id, shard_id = mapping
                if weight_name not in name:
                    continue
                name = name.replace(weight_name, param_name)
                param = params_dict[name]
                weight_loader = param.weight_loader
                weight_loader(
                    param,
                    loaded_weight,
                    name,
                    shard_id=shard_id,
                    expert_id=expert_id,
                )
                break
            else:
                # Skip loading extra bias for GPTQ models.
                if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                    continue
                param = params_dict[name]
                if &#34;wqkv&#34; in name:
                    config = self.config
                    kv_groups = (
                        config.num_attention_heads // config.num_key_value_heads
                    )
                    head_dim = config.hidden_size // config.num_attention_heads
                    loaded_weight = loaded_weight.view(
                        -1, 2 + kv_groups, head_dim, loaded_weight.shape[-1]
                    )
                    wq, wk, wv = torch.split(
                        loaded_weight, [kv_groups, 1, 1], dim=1
                    )
                    wq = wq.reshape(-1, wq.shape[-1])
                    wk = wk.reshape(-1, wk.shape[-1])
                    wv = wv.reshape(-1, wv.shape[-1])
                    weight_loader = param.weight_loader
                    weight_loader(param, wq, &#34;q&#34;)
                    weight_loader(param, wk, &#34;k&#34;)
                    weight_loader(param, wv, &#34;v&#34;)
                else:
                    weight_loader = getattr(
                        param, &#34;weight_loader&#34;, default_weight_loader
                    )
                    if &#34;vision_model&#34; in name:
                        loaded_weight = vision_utils.pad_vit_attn_dummy_heads(
                            self.config, name, loaded_weight
                        )
                    weight_loader(param, loaded_weight)

        loaded_params.add(name)
    unloaded_params = params_dict.keys() - loaded_params
    if unloaded_params:
        raise RuntimeError(
            f&#34;Some weights are not initialized from checkpoints: {unloaded_params}&#34;
        )
    return loaded_params</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.internvl.InternVLChatModel.pad_input_ids"><code class="name flex">
<span>def <span class="ident">pad_input_ids</span></span>(<span>self,<br>input_ids: List[int],<br>mm_inputs: <a title="sglang.srt.managers.schedule_batch.MultimodalInputs" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.MultimodalInputs">MultimodalInputs</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs):
    # Get all special token IDs
    im_start_id: int = mm_inputs.im_start_id
    im_end_id: int = mm_inputs.im_end_id

    media_token_pairs = [(im_start_id, im_end_id)]
    helper = MultiModalityDataPaddingPatternTokenPairs(media_token_pairs)

    return helper.pad_input_tokens(input_ids, mm_inputs)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.internvl.InternVLChatModel.pixel_shuffle"><code class="name flex">
<span>def <span class="ident">pixel_shuffle</span></span>(<span>self, x, scale_factor=0.5)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pixel_shuffle(self, x, scale_factor=0.5):
    n, w, h, c = x.size()
    # N, W, H, C --&gt; N, W, H * scale, C // scale
    x = x.view(n, w, int(h * scale_factor), int(c / scale_factor))
    # N, W, H * scale, C // scale --&gt; N, H * scale, W, C // scale
    x = x.permute(0, 2, 1, 3).contiguous()
    # N, H * scale, W, C // scale --&gt; N, H * scale, W * scale, C // (scale ** 2)
    x = x.view(
        n,
        int(h * scale_factor),
        int(w * scale_factor),
        int(c / (scale_factor * scale_factor)),
    )
    if self.ps_version == &#34;v1&#34;:
        logger.warn(
            &#34;In ps_version &#39;v1&#39;, the height and width have not been swapped back, &#34;
            &#34;which results in a transposed image.&#34;
        )
    else:
        x = x.permute(0, 2, 1, 3).contiguous()
    return x</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.internvl.InternVLChatModel"><code class="flex name class">
<span>class <span class="ident">EntryClass</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>use_flash_attn=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InternVLChatModel(nn.Module):
    def __init__(
        self,
        config: PretrainedConfig,
        quant_config: Optional[QuantizationConfig] = None,
        use_flash_attn=True,
    ) -&gt; None:
        super().__init__()
        self.config = config
        self.quant_config = quant_config
        vision_utils.update_vit_attn_dummy_heads_config(self.config)
        image_size = config.force_image_size or config.vision_config.image_size
        patch_size = config.vision_config.patch_size
        self.patch_size = patch_size
        self.select_layer = config.select_layer
        self.template = config.template
        self.num_image_token = int(
            (image_size // patch_size) ** 2 * (config.downsample_ratio**2)
        )
        self.downsample_ratio = config.downsample_ratio
        self.ps_version = config.ps_version

        config.vision_config.use_flash_attn = True if use_flash_attn else False
        config.llm_config._attn_implementation = (
            &#34;flash_attention_2&#34; if use_flash_attn else &#34;eager&#34;
        )

        logger.info(f&#34;num_image_token: {self.num_image_token}&#34;)
        logger.info(f&#34;ps_version: {self.ps_version}&#34;)

        self.vision_model = InternVisionModel(config.vision_config)
        if config.llm_config.architectures[0] == &#34;Qwen2ForCausalLM&#34;:
            self.language_model = Qwen2ForCausalLM(
                config=config.llm_config, quant_config=quant_config
            )
        elif config.llm_config.architectures[0] == &#34;InternLM2ForCausalLM&#34;:
            self.language_model = InternLM2ForCausalLM(
                config=config.llm_config, quant_config=quant_config
            )
        elif config.llm_config.architectures[0] == &#34;Qwen3MoeForCausalLM&#34;:
            self.language_model = Qwen3MoeForCausalLM(
                config=config.llm_config, quant_config=quant_config
            )
        else:
            raise NotImplementedError(
                f&#34;{config.llm_config.architectures[0]} is not implemented.&#34;
            )

        vit_hidden_size = config.vision_config.hidden_size
        llm_hidden_size = config.llm_config.hidden_size

        self.mlp1 = nn.Sequential(
            nn.LayerNorm(vit_hidden_size * int(1 / self.downsample_ratio) ** 2),
            nn.Linear(
                vit_hidden_size * int(1 / self.downsample_ratio) ** 2, llm_hidden_size
            ),
            nn.GELU(),
            nn.Linear(llm_hidden_size, llm_hidden_size),
        )

    def pixel_shuffle(self, x, scale_factor=0.5):
        n, w, h, c = x.size()
        # N, W, H, C --&gt; N, W, H * scale, C // scale
        x = x.view(n, w, int(h * scale_factor), int(c / scale_factor))
        # N, W, H * scale, C // scale --&gt; N, H * scale, W, C // scale
        x = x.permute(0, 2, 1, 3).contiguous()
        # N, H * scale, W, C // scale --&gt; N, H * scale, W * scale, C // (scale ** 2)
        x = x.view(
            n,
            int(h * scale_factor),
            int(w * scale_factor),
            int(c / (scale_factor * scale_factor)),
        )
        if self.ps_version == &#34;v1&#34;:
            logger.warn(
                &#34;In ps_version &#39;v1&#39;, the height and width have not been swapped back, &#34;
                &#34;which results in a transposed image.&#34;
            )
        else:
            x = x.permute(0, 2, 1, 3).contiguous()
        return x

    def extract_feature(self, pixel_values):
        if self.select_layer == -1:
            vit_embeds = self.vision_model(
                pixel_values=pixel_values, output_hidden_states=False, return_dict=True
            ).last_hidden_state
        else:
            vit_embeds = self.vision_model(
                pixel_values=pixel_values, output_hidden_states=True, return_dict=True
            ).hidden_states[self.select_layer]
        vit_embeds = vit_embeds[:, 1:, :]

        h = w = int(vit_embeds.shape[1] ** 0.5)
        vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], h, w, -1)
        vit_embeds = self.pixel_shuffle(vit_embeds, scale_factor=self.downsample_ratio)
        vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], -1, vit_embeds.shape[-1])
        vit_embeds = self.mlp1(vit_embeds)
        return vit_embeds

    def get_image_feature(self, items: List[MultimodalDataItem]):
        &#34;&#34;&#34;
        Projects the last hidden state from the vision model into language model space.

        Returns:
            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).
        &#34;&#34;&#34;
        pixel_values = torch.cat([item.feature for item in items])
        image_features = self.extract_feature(pixel_values)
        return image_features

    @torch.no_grad()
    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        forward_batch: ForwardBatch,
        input_embeds: torch.Tensor = None,
    ) -&gt; torch.Tensor:

        hs = general_mm_embed_routine(
            input_ids=input_ids,
            forward_batch=forward_batch,
            language_model=self.language_model,
            data_embedding_funcs={
                Modality.IMAGE: self.get_image_feature,
            },
            positions=positions,
        )

        return hs

    def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs):
        # Get all special token IDs
        im_start_id: int = mm_inputs.im_start_id
        im_end_id: int = mm_inputs.im_end_id

        media_token_pairs = [(im_start_id, im_end_id)]
        helper = MultiModalityDataPaddingPatternTokenPairs(media_token_pairs)

        return helper.pad_input_tokens(input_ids, mm_inputs)

    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
        expert_params_mapping = []
        if &#34;InternLM2ForCausalLM&#34; in self.config.llm_config.architectures:
            stacked_params_mapping = [
                # (param_name, shard_name, shard_id)
                (&#34;gate_up_proj&#34;, &#34;w1&#34;, 0),
                (&#34;gate_up_proj&#34;, &#34;w3&#34;, 1),
            ]
        elif &#34;Qwen2ForCausalLM&#34; in self.config.llm_config.architectures:
            stacked_params_mapping = [
                # (param_name, shard_name, shard_id)
                (&#34;qkv_proj&#34;, &#34;q_proj&#34;, &#34;q&#34;),
                (&#34;qkv_proj&#34;, &#34;k_proj&#34;, &#34;k&#34;),
                (&#34;qkv_proj&#34;, &#34;v_proj&#34;, &#34;v&#34;),
                (&#34;gate_up_proj&#34;, &#34;gate_proj&#34;, 0),
                (&#34;gate_up_proj&#34;, &#34;up_proj&#34;, 1),
            ]
        elif &#34;Qwen3MoeForCausalLM&#34; in self.config.llm_config.architectures:
            stacked_params_mapping = [
                # (param_name, shard_name, shard_id)
                (&#34;qkv_proj&#34;, &#34;q_proj&#34;, &#34;q&#34;),
                (&#34;qkv_proj&#34;, &#34;k_proj&#34;, &#34;k&#34;),
                (&#34;qkv_proj&#34;, &#34;v_proj&#34;, &#34;v&#34;),
                (&#34;gate_up_proj&#34;, &#34;gate_proj&#34;, 0),
                (&#34;gate_up_proj&#34;, &#34;up_proj&#34;, 1),
            ]

            expert_params_mapping = FusedMoE.make_expert_params_mapping(
                ckpt_gate_proj_name=&#34;gate_proj&#34;,
                ckpt_down_proj_name=&#34;down_proj&#34;,
                ckpt_up_proj_name=&#34;up_proj&#34;,
                num_experts=self.config.num_experts,
            )

        params_dict = dict(self.named_parameters())
        loaded_params: Set[str] = set()

        for name, loaded_weight in weights:
            if &#34;rotary_emb.inv_freq&#34; in name:
                continue

            for param_name, weight_name, shard_id in stacked_params_mapping:
                if weight_name not in name:
                    continue
                # We have mlp.experts[0].gate_proj in the checkpoint.
                # Since we handle the experts below in expert_params_mapping,
                # we need to skip here BEFORE we update the name, otherwise
                # name will be updated to mlp.experts[0].gate_up_proj, which
                # will then be updated below in expert_params_mapping
                # for mlp.experts[0].gate_gate_up_proj, which breaks load.
                if &#34;mlp.experts&#34; in name:
                    continue
                name = name.replace(weight_name, param_name)
                # Skip loading extra bias for GPTQ models.
                if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                    continue
                param = params_dict[name]
                weight_loader = param.weight_loader
                weight_loader(param, loaded_weight, shard_id)
                break
            else:
                if &#34;vision_model&#34; in name:
                    # adapt to VisionAttention
                    name = name.replace(r&#34;attn.&#34;, r&#34;attn.attn.&#34;)
                    name = name.replace(r&#34;qkv.&#34;, r&#34;qkv_proj.&#34;)

                for mapping in expert_params_mapping:
                    param_name, weight_name, expert_id, shard_id = mapping
                    if weight_name not in name:
                        continue
                    name = name.replace(weight_name, param_name)
                    param = params_dict[name]
                    weight_loader = param.weight_loader
                    weight_loader(
                        param,
                        loaded_weight,
                        name,
                        shard_id=shard_id,
                        expert_id=expert_id,
                    )
                    break
                else:
                    # Skip loading extra bias for GPTQ models.
                    if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                        continue
                    param = params_dict[name]
                    if &#34;wqkv&#34; in name:
                        config = self.config
                        kv_groups = (
                            config.num_attention_heads // config.num_key_value_heads
                        )
                        head_dim = config.hidden_size // config.num_attention_heads
                        loaded_weight = loaded_weight.view(
                            -1, 2 + kv_groups, head_dim, loaded_weight.shape[-1]
                        )
                        wq, wk, wv = torch.split(
                            loaded_weight, [kv_groups, 1, 1], dim=1
                        )
                        wq = wq.reshape(-1, wq.shape[-1])
                        wk = wk.reshape(-1, wk.shape[-1])
                        wv = wv.reshape(-1, wv.shape[-1])
                        weight_loader = param.weight_loader
                        weight_loader(param, wq, &#34;q&#34;)
                        weight_loader(param, wk, &#34;k&#34;)
                        weight_loader(param, wv, &#34;v&#34;)
                    else:
                        weight_loader = getattr(
                            param, &#34;weight_loader&#34;, default_weight_loader
                        )
                        if &#34;vision_model&#34; in name:
                            loaded_weight = vision_utils.pad_vit_attn_dummy_heads(
                                self.config, name, loaded_weight
                            )
                        weight_loader(param, loaded_weight)

            loaded_params.add(name)
        unloaded_params = params_dict.keys() - loaded_params
        if unloaded_params:
            raise RuntimeError(
                f&#34;Some weights are not initialized from checkpoints: {unloaded_params}&#34;
            )
        return loaded_params</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.internvl.InternVLChatModel.extract_feature"><code class="name flex">
<span>def <span class="ident">extract_feature</span></span>(<span>self, pixel_values)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_feature(self, pixel_values):
    if self.select_layer == -1:
        vit_embeds = self.vision_model(
            pixel_values=pixel_values, output_hidden_states=False, return_dict=True
        ).last_hidden_state
    else:
        vit_embeds = self.vision_model(
            pixel_values=pixel_values, output_hidden_states=True, return_dict=True
        ).hidden_states[self.select_layer]
    vit_embeds = vit_embeds[:, 1:, :]

    h = w = int(vit_embeds.shape[1] ** 0.5)
    vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], h, w, -1)
    vit_embeds = self.pixel_shuffle(vit_embeds, scale_factor=self.downsample_ratio)
    vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], -1, vit_embeds.shape[-1])
    vit_embeds = self.mlp1(vit_embeds)
    return vit_embeds</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.internvl.InternVLChatModel.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>input_ids: torch.Tensor,<br>positions: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>input_embeds: torch.Tensor = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def forward(
    self,
    input_ids: torch.Tensor,
    positions: torch.Tensor,
    forward_batch: ForwardBatch,
    input_embeds: torch.Tensor = None,
) -&gt; torch.Tensor:

    hs = general_mm_embed_routine(
        input_ids=input_ids,
        forward_batch=forward_batch,
        language_model=self.language_model,
        data_embedding_funcs={
            Modality.IMAGE: self.get_image_feature,
        },
        positions=positions,
    )

    return hs</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.internvl.InternVLChatModel.get_image_feature"><code class="name flex">
<span>def <span class="ident">get_image_feature</span></span>(<span>self,<br>items: List[<a title="sglang.srt.managers.schedule_batch.MultimodalDataItem" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.MultimodalDataItem">MultimodalDataItem</a>])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_image_feature(self, items: List[MultimodalDataItem]):
    &#34;&#34;&#34;
    Projects the last hidden state from the vision model into language model space.

    Returns:
        image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).
    &#34;&#34;&#34;
    pixel_values = torch.cat([item.feature for item in items])
    image_features = self.extract_feature(pixel_values)
    return image_features</code></pre>
</details>
<div class="desc"><p>Projects the last hidden state from the vision model into language model space.</p>
<h2 id="returns">Returns</h2>
<p>image_features (<code>torch.Tensor</code>): Image feature tensor of shape <code>(num_images, image_length, embed_dim)</code>).</p></div>
</dd>
<dt id="sglang.srt.models.internvl.InternVLChatModel.load_weights"><code class="name flex">
<span>def <span class="ident">load_weights</span></span>(<span>self, weights: Iterable[Tuple[str, torch.Tensor]])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
    expert_params_mapping = []
    if &#34;InternLM2ForCausalLM&#34; in self.config.llm_config.architectures:
        stacked_params_mapping = [
            # (param_name, shard_name, shard_id)
            (&#34;gate_up_proj&#34;, &#34;w1&#34;, 0),
            (&#34;gate_up_proj&#34;, &#34;w3&#34;, 1),
        ]
    elif &#34;Qwen2ForCausalLM&#34; in self.config.llm_config.architectures:
        stacked_params_mapping = [
            # (param_name, shard_name, shard_id)
            (&#34;qkv_proj&#34;, &#34;q_proj&#34;, &#34;q&#34;),
            (&#34;qkv_proj&#34;, &#34;k_proj&#34;, &#34;k&#34;),
            (&#34;qkv_proj&#34;, &#34;v_proj&#34;, &#34;v&#34;),
            (&#34;gate_up_proj&#34;, &#34;gate_proj&#34;, 0),
            (&#34;gate_up_proj&#34;, &#34;up_proj&#34;, 1),
        ]
    elif &#34;Qwen3MoeForCausalLM&#34; in self.config.llm_config.architectures:
        stacked_params_mapping = [
            # (param_name, shard_name, shard_id)
            (&#34;qkv_proj&#34;, &#34;q_proj&#34;, &#34;q&#34;),
            (&#34;qkv_proj&#34;, &#34;k_proj&#34;, &#34;k&#34;),
            (&#34;qkv_proj&#34;, &#34;v_proj&#34;, &#34;v&#34;),
            (&#34;gate_up_proj&#34;, &#34;gate_proj&#34;, 0),
            (&#34;gate_up_proj&#34;, &#34;up_proj&#34;, 1),
        ]

        expert_params_mapping = FusedMoE.make_expert_params_mapping(
            ckpt_gate_proj_name=&#34;gate_proj&#34;,
            ckpt_down_proj_name=&#34;down_proj&#34;,
            ckpt_up_proj_name=&#34;up_proj&#34;,
            num_experts=self.config.num_experts,
        )

    params_dict = dict(self.named_parameters())
    loaded_params: Set[str] = set()

    for name, loaded_weight in weights:
        if &#34;rotary_emb.inv_freq&#34; in name:
            continue

        for param_name, weight_name, shard_id in stacked_params_mapping:
            if weight_name not in name:
                continue
            # We have mlp.experts[0].gate_proj in the checkpoint.
            # Since we handle the experts below in expert_params_mapping,
            # we need to skip here BEFORE we update the name, otherwise
            # name will be updated to mlp.experts[0].gate_up_proj, which
            # will then be updated below in expert_params_mapping
            # for mlp.experts[0].gate_gate_up_proj, which breaks load.
            if &#34;mlp.experts&#34; in name:
                continue
            name = name.replace(weight_name, param_name)
            # Skip loading extra bias for GPTQ models.
            if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                continue
            param = params_dict[name]
            weight_loader = param.weight_loader
            weight_loader(param, loaded_weight, shard_id)
            break
        else:
            if &#34;vision_model&#34; in name:
                # adapt to VisionAttention
                name = name.replace(r&#34;attn.&#34;, r&#34;attn.attn.&#34;)
                name = name.replace(r&#34;qkv.&#34;, r&#34;qkv_proj.&#34;)

            for mapping in expert_params_mapping:
                param_name, weight_name, expert_id, shard_id = mapping
                if weight_name not in name:
                    continue
                name = name.replace(weight_name, param_name)
                param = params_dict[name]
                weight_loader = param.weight_loader
                weight_loader(
                    param,
                    loaded_weight,
                    name,
                    shard_id=shard_id,
                    expert_id=expert_id,
                )
                break
            else:
                # Skip loading extra bias for GPTQ models.
                if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                    continue
                param = params_dict[name]
                if &#34;wqkv&#34; in name:
                    config = self.config
                    kv_groups = (
                        config.num_attention_heads // config.num_key_value_heads
                    )
                    head_dim = config.hidden_size // config.num_attention_heads
                    loaded_weight = loaded_weight.view(
                        -1, 2 + kv_groups, head_dim, loaded_weight.shape[-1]
                    )
                    wq, wk, wv = torch.split(
                        loaded_weight, [kv_groups, 1, 1], dim=1
                    )
                    wq = wq.reshape(-1, wq.shape[-1])
                    wk = wk.reshape(-1, wk.shape[-1])
                    wv = wv.reshape(-1, wv.shape[-1])
                    weight_loader = param.weight_loader
                    weight_loader(param, wq, &#34;q&#34;)
                    weight_loader(param, wk, &#34;k&#34;)
                    weight_loader(param, wv, &#34;v&#34;)
                else:
                    weight_loader = getattr(
                        param, &#34;weight_loader&#34;, default_weight_loader
                    )
                    if &#34;vision_model&#34; in name:
                        loaded_weight = vision_utils.pad_vit_attn_dummy_heads(
                            self.config, name, loaded_weight
                        )
                    weight_loader(param, loaded_weight)

        loaded_params.add(name)
    unloaded_params = params_dict.keys() - loaded_params
    if unloaded_params:
        raise RuntimeError(
            f&#34;Some weights are not initialized from checkpoints: {unloaded_params}&#34;
        )
    return loaded_params</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.internvl.InternVLChatModel.pad_input_ids"><code class="name flex">
<span>def <span class="ident">pad_input_ids</span></span>(<span>self,<br>input_ids: List[int],<br>mm_inputs: <a title="sglang.srt.managers.schedule_batch.MultimodalInputs" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.MultimodalInputs">MultimodalInputs</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs):
    # Get all special token IDs
    im_start_id: int = mm_inputs.im_start_id
    im_end_id: int = mm_inputs.im_end_id

    media_token_pairs = [(im_start_id, im_end_id)]
    helper = MultiModalityDataPaddingPatternTokenPairs(media_token_pairs)

    return helper.pad_input_tokens(input_ids, mm_inputs)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.internvl.InternVLChatModel.pixel_shuffle"><code class="name flex">
<span>def <span class="ident">pixel_shuffle</span></span>(<span>self, x, scale_factor=0.5)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pixel_shuffle(self, x, scale_factor=0.5):
    n, w, h, c = x.size()
    # N, W, H, C --&gt; N, W, H * scale, C // scale
    x = x.view(n, w, int(h * scale_factor), int(c / scale_factor))
    # N, W, H * scale, C // scale --&gt; N, H * scale, W, C // scale
    x = x.permute(0, 2, 1, 3).contiguous()
    # N, H * scale, W, C // scale --&gt; N, H * scale, W * scale, C // (scale ** 2)
    x = x.view(
        n,
        int(h * scale_factor),
        int(w * scale_factor),
        int(c / (scale_factor * scale_factor)),
    )
    if self.ps_version == &#34;v1&#34;:
        logger.warn(
            &#34;In ps_version &#39;v1&#39;, the height and width have not been swapped back, &#34;
            &#34;which results in a transposed image.&#34;
        )
    else:
        x = x.permute(0, 2, 1, 3).contiguous()
    return x</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.internvl.InternVisionEmbeddings"><code class="flex name class">
<span>class <span class="ident">InternVisionEmbeddings</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InternVisionEmbeddings(nn.Module):
    def __init__(self, config: PretrainedConfig):
        super().__init__()
        self.config = config
        self.embed_dim = config.hidden_size
        self.image_size = (
            config.image_size
            if isinstance(config.image_size, int)
            else config.image_size[0]
        )
        self.patch_size = (
            config.patch_size
            if isinstance(config.patch_size, int)
            else config.patch_size[0]
        )

        self.class_embedding = nn.Parameter(
            torch.randn(1, 1, self.embed_dim),
        )

        self.patch_embedding = nn.Conv2d(
            in_channels=3,
            out_channels=self.embed_dim,
            kernel_size=self.patch_size,
            stride=self.patch_size,
        )

        self.num_patches = (self.image_size // self.patch_size) ** 2
        self.num_positions = self.num_patches + 1

        self.position_embedding = nn.Parameter(
            torch.randn(1, self.num_positions, self.embed_dim)
        )

    def _get_pos_embed(self, pos_embed, H, W):
        target_dtype = pos_embed.dtype
        pos_embed = (
            pos_embed.float()
            .reshape(
                1,
                self.image_size // self.patch_size,
                self.image_size // self.patch_size,
                -1,
            )
            .permute(0, 3, 1, 2)
        )
        pos_embed = (
            F.interpolate(pos_embed, size=(H, W), mode=&#34;bicubic&#34;, align_corners=False)
            .reshape(1, -1, H * W)
            .permute(0, 2, 1)
            .to(target_dtype)
        )
        return pos_embed

    def forward(self, pixel_values: torch.FloatTensor) -&gt; torch.Tensor:
        target_dtype = self.patch_embedding.weight.dtype
        patch_embeds = self.patch_embedding(
            pixel_values
        )  # shape = [*, channel, width, height]
        batch_size, _, height, width = patch_embeds.shape
        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)
        class_embeds = self.class_embedding.expand(batch_size, 1, -1).to(target_dtype)
        embeddings = torch.cat([class_embeds, patch_embeds], dim=1)
        position_embedding = torch.cat(
            [
                self.position_embedding[:, :1, :],
                self._get_pos_embed(self.position_embedding[:, 1:, :], height, width),
            ],
            dim=1,
        )
        embeddings = embeddings + position_embedding.to(target_dtype)
        return embeddings</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.internvl.InternVisionEmbeddings.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, pixel_values: torch.FloatTensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, pixel_values: torch.FloatTensor) -&gt; torch.Tensor:
    target_dtype = self.patch_embedding.weight.dtype
    patch_embeds = self.patch_embedding(
        pixel_values
    )  # shape = [*, channel, width, height]
    batch_size, _, height, width = patch_embeds.shape
    patch_embeds = patch_embeds.flatten(2).transpose(1, 2)
    class_embeds = self.class_embedding.expand(batch_size, 1, -1).to(target_dtype)
    embeddings = torch.cat([class_embeds, patch_embeds], dim=1)
    position_embedding = torch.cat(
        [
            self.position_embedding[:, :1, :],
            self._get_pos_embed(self.position_embedding[:, 1:, :], height, width),
        ],
        dim=1,
    )
    embeddings = embeddings + position_embedding.to(target_dtype)
    return embeddings</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.internvl.InternVisionEncoder"><code class="flex name class">
<span>class <span class="ident">InternVisionEncoder</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InternVisionEncoder(nn.Module):
    &#34;&#34;&#34;
    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a
    [`InternEncoderLayer`].

    Args:
        config (`InternConfig`):
            The corresponding vision configuration for the `InternEncoder`.
    &#34;&#34;&#34;

    def __init__(
        self,
        config: PretrainedConfig,
        quant_config: Optional[QuantizationConfig] = None,
    ):
        super().__init__()
        self.config = config
        # stochastic depth decay rule
        dpr = [
            x.item()
            for x in torch.linspace(0, config.drop_path_rate, config.num_hidden_layers)
        ]
        self.layers = nn.ModuleList(
            [
                InternVisionEncoderLayer(config, dpr[idx], quant_config)
                for idx in range(config.num_hidden_layers)
            ]
        )

    def forward(
        self,
        inputs_embeds,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -&gt; Union[Tuple, BaseModelOutput]:
        r&#34;&#34;&#34;
        Args:
            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
                Embedded representation of the inputs. Should be float, not int tokens.
            output_hidden_states (`bool`, *optional*):
                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
                for more detail.
            return_dict (`bool`, *optional*):
                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
        &#34;&#34;&#34;
        output_hidden_states = (
            output_hidden_states
            if output_hidden_states is not None
            else self.config.output_hidden_states
        )
        return_dict = (
            return_dict if return_dict is not None else self.config.use_return_dict
        )

        encoder_states = () if output_hidden_states else None
        hidden_states = inputs_embeds

        cu_seqlens = SingletonCache()

        for idx, encoder_layer in enumerate(self.layers):
            if output_hidden_states:
                encoder_states = encoder_states + (hidden_states,)
            layer_outputs = encoder_layer(hidden_states, cu_seqlens=cu_seqlens)
            hidden_states = layer_outputs

        if output_hidden_states:
            encoder_states = encoder_states + (hidden_states,)

        if not return_dict:
            return tuple(v for v in [hidden_states, encoder_states] if v is not None)
        return BaseModelOutput(
            last_hidden_state=hidden_states, hidden_states=encoder_states
        )</code></pre>
</details>
<div class="desc"><p>Transformer encoder consisting of <code>config.num_hidden_layers</code> self attention layers. Each layer is a
[<code>InternEncoderLayer</code>].</p>
<h2 id="args">Args</h2>
<p>config (<code>InternConfig</code>):
The corresponding vision configuration for the <code>InternEncoder</code>.
Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.internvl.InternVisionEncoder.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>inputs_embeds,<br>output_hidden_states: bool | None = None,<br>return_dict: bool | None = None) ‑> Tuple | transformers.modeling_outputs.BaseModelOutput</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    inputs_embeds,
    output_hidden_states: Optional[bool] = None,
    return_dict: Optional[bool] = None,
) -&gt; Union[Tuple, BaseModelOutput]:
    r&#34;&#34;&#34;
    Args:
        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
            Embedded representation of the inputs. Should be float, not int tokens.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
            for more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
    &#34;&#34;&#34;
    output_hidden_states = (
        output_hidden_states
        if output_hidden_states is not None
        else self.config.output_hidden_states
    )
    return_dict = (
        return_dict if return_dict is not None else self.config.use_return_dict
    )

    encoder_states = () if output_hidden_states else None
    hidden_states = inputs_embeds

    cu_seqlens = SingletonCache()

    for idx, encoder_layer in enumerate(self.layers):
        if output_hidden_states:
            encoder_states = encoder_states + (hidden_states,)
        layer_outputs = encoder_layer(hidden_states, cu_seqlens=cu_seqlens)
        hidden_states = layer_outputs

    if output_hidden_states:
        encoder_states = encoder_states + (hidden_states,)

    if not return_dict:
        return tuple(v for v in [hidden_states, encoder_states] if v is not None)
    return BaseModelOutput(
        last_hidden_state=hidden_states, hidden_states=encoder_states
    )</code></pre>
</details>
<div class="desc"><h2 id="args">Args</h2>
<p>inputs_embeds (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>):
Embedded representation of the inputs. Should be float, not int tokens.
output_hidden_states (<code>bool</code>, <em>optional</em>):
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more detail.
return_dict (<code>bool</code>, <em>optional</em>):
Whether or not to return a [<code>~utils.ModelOutput</code>] instead of a plain tuple.</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.internvl.InternVisionEncoderLayer"><code class="flex name class">
<span>class <span class="ident">InternVisionEncoderLayer</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig,<br>drop_path_rate: float,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InternVisionEncoderLayer(nn.Module):

    def __init__(
        self,
        config: PretrainedConfig,
        drop_path_rate: float,
        quant_config: QuantizationConfig = None,
    ):
        super().__init__()
        self.embed_dim = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.norm_type = config.norm_type
        self.attn = InternAttention(config=config, quant_config=quant_config)
        self.mlp = InternMLP(config)
        self.norm1 = NORM2FN[self.norm_type](self.embed_dim, eps=config.layer_norm_eps)
        self.norm2 = NORM2FN[self.norm_type](self.embed_dim, eps=config.layer_norm_eps)

        self.ls1 = nn.Parameter(config.initializer_factor * torch.ones(self.embed_dim))
        self.ls2 = nn.Parameter(config.initializer_factor * torch.ones(self.embed_dim))
        self.drop_path1 = (
            DropPath(drop_path_rate) if drop_path_rate &gt; 0.0 else nn.Identity()
        )
        self.drop_path2 = (
            DropPath(drop_path_rate) if drop_path_rate &gt; 0.0 else nn.Identity()
        )

    def forward(
        self,
        hidden_states: torch.Tensor,
        cu_seqlens: torch.Tensor,
    ) -&gt; Tuple[
        torch.FloatTensor,
        Optional[torch.FloatTensor],
        Optional[Tuple[torch.FloatTensor]],
    ]:
        &#34;&#34;&#34;
        Args:
            hidden_states (`Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]`): input to the layer of shape `(batch, seq_len, embed_dim)`
        &#34;&#34;&#34;

        hidden_states = hidden_states + self.drop_path1(
            self.attn(
                self.norm1(hidden_states).to(hidden_states.dtype), cu_seqlens=cu_seqlens
            )
            * self.ls1
        )

        hidden_states = hidden_states + self.drop_path2(
            self.mlp(self.norm2(hidden_states).to(hidden_states.dtype)) * self.ls2
        )

        return hidden_states</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.internvl.InternVisionEncoderLayer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor) ‑> Tuple[torch.FloatTensor, torch.FloatTensor | None, Tuple[torch.FloatTensor] | None]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    hidden_states: torch.Tensor,
    cu_seqlens: torch.Tensor,
) -&gt; Tuple[
    torch.FloatTensor,
    Optional[torch.FloatTensor],
    Optional[Tuple[torch.FloatTensor]],
]:
    &#34;&#34;&#34;
    Args:
        hidden_states (`Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]`): input to the layer of shape `(batch, seq_len, embed_dim)`
    &#34;&#34;&#34;

    hidden_states = hidden_states + self.drop_path1(
        self.attn(
            self.norm1(hidden_states).to(hidden_states.dtype), cu_seqlens=cu_seqlens
        )
        * self.ls1
    )

    hidden_states = hidden_states + self.drop_path2(
        self.mlp(self.norm2(hidden_states).to(hidden_states.dtype)) * self.ls2
    )

    return hidden_states</code></pre>
</details>
<div class="desc"><h2 id="args">Args</h2>
<p>hidden_states (<code>Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]</code>): input to the layer of shape <code>(batch, seq_len, embed_dim)</code></p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.internvl.InternVisionModel"><code class="flex name class">
<span>class <span class="ident">InternVisionModel</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InternVisionModel(PreTrainedModel):
    main_input_name = &#34;pixel_values&#34;
    _supports_flash_attn_2 = True
    config_class = PretrainedConfig
    _no_split_modules = [&#34;InternVisionEncoderLayer&#34;]

    def __init__(
        self,
        config: PretrainedConfig,
        quant_config: Optional[QuantizationConfig] = None,
    ):
        super().__init__(config)
        self.config = config

        self.embeddings = InternVisionEmbeddings(
            config,
        )
        self.encoder = InternVisionEncoder(config, quant_config)

    def resize_pos_embeddings(self, old_size, new_size, patch_size):
        pos_emb = self.embeddings.position_embedding
        _, num_positions, embed_dim = pos_emb.shape
        cls_emb = pos_emb[:, :1, :]
        pos_emb = (
            pos_emb[:, 1:, :]
            .reshape(1, old_size // patch_size, old_size // patch_size, -1)
            .permute(0, 3, 1, 2)
        )
        pos_emb = F.interpolate(
            pos_emb.float(),
            size=new_size // patch_size,
            mode=&#34;bicubic&#34;,
            align_corners=False,
        )
        pos_emb = pos_emb.to(cls_emb.dtype).reshape(1, embed_dim, -1).permute(0, 2, 1)
        pos_emb = torch.cat([cls_emb, pos_emb], dim=1)
        self.embeddings.position_embedding = nn.Parameter(pos_emb)
        self.embeddings.image_size = new_size
        logger.info(
            &#34;Resized position embeddings from {} to {}&#34;.format(old_size, new_size)
        )

    def get_input_embeddings(self):
        return self.embeddings

    def forward(
        self,
        pixel_values: Optional[torch.FloatTensor] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        pixel_embeds: Optional[torch.FloatTensor] = None,
    ) -&gt; Union[Tuple, BaseModelOutputWithPooling]:
        pixel_values = pixel_values.to(device=self.device, dtype=self.dtype)
        output_hidden_states = (
            output_hidden_states
            if output_hidden_states is not None
            else self.config.output_hidden_states
        )
        return_dict = (
            return_dict if return_dict is not None else self.config.use_return_dict
        )

        if pixel_values is None and pixel_embeds is None:
            raise ValueError(&#34;You have to specify pixel_values or pixel_embeds&#34;)

        if pixel_embeds is not None:
            hidden_states = pixel_embeds
        else:
            if len(pixel_values.shape) == 4:
                hidden_states = self.embeddings(pixel_values)
            else:
                raise ValueError(f&#34;wrong pixel_values size: {pixel_values.shape}&#34;)
        encoder_outputs = self.encoder(
            inputs_embeds=hidden_states,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        last_hidden_state = encoder_outputs.last_hidden_state
        pooled_output = last_hidden_state[:, 0, :]

        if not return_dict:
            return (last_hidden_state, pooled_output) + encoder_outputs[1:]

        return BaseModelOutputWithPooling(
            last_hidden_state=last_hidden_state,
            pooler_output=pooled_output,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
        )</code></pre>
</details>
<div class="desc"><p>Base class for all models.</p>
<p>[<code>PreTrainedModel</code>] takes care of storing the configuration of the models and handles methods for loading,
downloading and saving models as well as a few methods common to all models to:</p>
<pre><code>- resize the input embeddings,
- prune heads in the self-attention heads.
</code></pre>
<p>Class attributes (overridden by derived classes):</p>
<pre><code>- **config_class** ([`PretrainedConfig`]) -- A subclass of [`PretrainedConfig`] to use as configuration class
  for this model architecture.
- **load_tf_weights** (&lt;code&gt;Callable&lt;/code&gt;) -- A python *method* for loading a TensorFlow checkpoint in a PyTorch model,
  taking as arguments:

    - **model** ([`PreTrainedModel`]) -- An instance of the model on which to load the TensorFlow checkpoint.
    - **config** ([`PreTrainedConfig`]) -- An instance of the configuration associated to the model.
    - **path** (&lt;code&gt;str&lt;/code&gt;) -- A path to the TensorFlow checkpoint.

- **base_model_prefix** (&lt;code&gt;str&lt;/code&gt;) -- A string indicating the attribute associated to the base model in derived
  classes of the same architecture adding modules on top of the base model.
- **is_parallelizable** (&lt;code&gt;bool&lt;/code&gt;) -- A flag indicating whether this model supports model parallelization.
- **main_input_name** (&lt;code&gt;str&lt;/code&gt;) -- The name of the principal input to the model (often &lt;code&gt;input\_ids&lt;/code&gt; for NLP
  models, &lt;code&gt;pixel\_values&lt;/code&gt; for vision models and &lt;code&gt;input\_values&lt;/code&gt; for speech models).
- **can_record_outputs** (dict):
</code></pre>
<p>Args:
config ([<code>PretrainedConfig</code>]):
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
[<code>~PreTrainedModel.from_pretrained</code>] method to load the model weights.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.modeling_utils.PreTrainedModel</li>
<li>torch.nn.modules.module.Module</li>
<li>transformers.modeling_utils.EmbeddingAccessMixin</li>
<li>transformers.modeling_utils.ModuleUtilsMixin</li>
<li>transformers.utils.hub.PushToHubMixin</li>
<li>transformers.integrations.peft.PeftAdapterMixin</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.internvl.InternVisionModel.config_class"><code class="name">var <span class="ident">config_class</span></code></dt>
<dd>
<div class="desc"><p>Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as
methods for loading/downloading/saving configurations.</p>
<p><Tip></p>
<p>A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to
initialize a model does <strong>not</strong> load the model weights. It only affects the model's configuration.</p>
<p></Tip></p>
<p>Class attributes (overridden by derived classes):</p>
<ul>
<li><strong>model_type</strong> (<code>str</code>) &ndash; An identifier for the model type, serialized into the JSON file, and used to recreate
the correct object in [<code>~transformers.AutoConfig</code>].</li>
<li><strong>has_no_defaults_at_init</strong> (<code>bool</code>) &ndash; Whether the config class can be initialized without providing input arguments.
Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,
(but not necessarily) such as [<code>~transformers.EncoderDecoderConfig</code>] or [<code>~RagConfig</code>]. They have to be initialized from
two or more configs of type [<code>~transformers.PretrainedConfig</code>].</li>
<li><strong>keys_to_ignore_at_inference</strong> (<code>list[str]</code>) &ndash; A list of keys to ignore by default when looking at dictionary
outputs of the model during inference.</li>
<li><strong>attribute_map</strong> (<code>dict[str, str]</code>) &ndash; A dict that maps model specific attribute names to the standardized
naming of attributes.</li>
<li><strong>base_model_tp_plan</strong> (<code>dict[str, Any]</code>) &ndash; A dict that maps sub-modules FQNs of a base model to a tensor
parallel plan applied to the sub-module when <code>model.tensor_parallel</code> is called.</li>
<li><strong>base_model_pp_plan</strong> (<code>dict[str, tuple[list[str]]]</code>) &ndash; A dict that maps child-modules of a base model to a
pipeline parallel plan that enables users to place the child-module on the appropriate device.</li>
</ul>
<p>Common attributes (present in all subclasses):</p>
<ul>
<li><strong>vocab_size</strong> (<code>int</code>) &ndash; The number of tokens in the vocabulary, which is also the first dimension of the
embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).</li>
<li><strong>hidden_size</strong> (<code>int</code>) &ndash; The hidden size of the model.</li>
<li><strong>num_attention_heads</strong> (<code>int</code>) &ndash; The number of attention heads used in the multi-head attention layers of the
model.</li>
<li><strong>num_hidden_layers</strong> (<code>int</code>) &ndash; The number of blocks in the model.</li>
</ul>
<p><Tip warning={true}></p>
<p>Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading
some of them will still be possible, but attempting to overwrite them will throw an exception &ndash; you should set
them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more
information about the individual parameters.</p>
<p></Tip></p>
<h2 id="arg">Arg</h2>
<p>name_or_path (<code>str</code>, <em>optional</em>, defaults to <code>""</code>):
Store the string that was passed to [<code>PreTrainedModel.from_pretrained</code>] or
[<code>TFPreTrainedModel.from_pretrained</code>] as <code>pretrained_model_name_or_path</code> if the configuration was created
with such a method.
output_hidden_states (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should return all hidden-states.
output_attentions (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should returns all attentions.
return_dict (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should return a [<code>~transformers.utils.ModelOutput</code>] instead of a plain tuple.
is_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether the model is used as an encoder/decoder or not.
is_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether to only use the decoder in an encoder-decoder architecture, otherwise it has no effect on
decoder-only or encoder-only architectures.
cross_attention_hidden_size (<code>bool</code>, <em>optional</em>):
The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder
setting and the cross-attention hidden dimension differs from <code>self.config.hidden_size</code>.
add_cross_attention (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether cross-attention layers should be added to the model. Note, this option is only relevant for models
that can be used as decoder models within the [<code>EncoderDecoderModel</code>] class, which consists of all models
in <code>AUTO_MODELS_FOR_CAUSAL_LM</code>.
tie_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder
and decoder model to have the exact same parameter names.
prune_heads (<code>dict[int, list[int]]</code>, <em>optional</em>, defaults to <code>{}</code>):
Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of
heads to prune in said layer.</p>
<pre><code>For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.
</code></pre>
<p>chunk_size_feed_forward (<code>int</code>, <em>optional</em>, defaults to <code>0</code>):
The chunk size of all feed forward layers in the residual attention blocks. A chunk size of <code>0</code> means that
the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes <code>n</code> &lt;
sequence_length embeddings at a time. For more information on feed forward chunking, see <a href="../glossary.html#feed-forward-chunking">How does Feed
Forward Chunking work?</a>.</p>
<blockquote>
<p>Parameters for fine-tuning tasks</p>
</blockquote>
<p>architectures (<code>list[str]</code>, <em>optional</em>):
Model architectures that can be used with the model pretrained weights.
finetuning_task (<code>str</code>, <em>optional</em>):
Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow
or PyTorch) checkpoint.
id2label (<code>dict[int, str]</code>, <em>optional</em>):
A map from index (for instance prediction index, or target index) to label.
label2id (<code>dict[str, int]</code>, <em>optional</em>):
A map from label to index for the model.
num_labels (<code>int</code>, <em>optional</em>):
Number of labels to use in the last layer added to the model, typically for a classification task.
task_specific_params (<code>dict[str, Any]</code>, <em>optional</em>):
Additional keyword arguments to store for the current task.
problem_type (<code>str</code>, <em>optional</em>):
Problem type for <code>XxxForSequenceClassification</code> models. Can be one of <code>"regression"</code>,
<code>"single_label_classification"</code> or <code>"multi_label_classification"</code>.</p>
<blockquote>
<p>Parameters linked to the tokenizer</p>
</blockquote>
<p>tokenizer_class (<code>str</code>, <em>optional</em>):
The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the
model by default).
prefix (<code>str</code>, <em>optional</em>):
A specific prompt that should be added at the beginning of each text before calling the model.
bos_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>beginning-of-stream</em> token.
pad_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>padding</em> token.
eos_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>end-of-stream</em> token.
decoder_start_token_id (<code>int</code>, <em>optional</em>):
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.
sep_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>separation</em> token.</p>
<blockquote>
<p>PyTorch specific parameters</p>
</blockquote>
<p>torchscript (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should be used with Torchscript.
tie_word_embeddings (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the
model has a output word embedding layer.
torch_dtype (<code>str</code>, <em>optional</em>):
The <code>dtype</code> of the weights. This attribute can be used to initialize the model to a non-default <code>dtype</code>
(which is normally <code>float32</code>) and thus allow for optimal storage allocation. For example, if the saved
model is <code>float16</code>, ideally we want to load it back using the minimal amount of memory needed to load
<code>float16</code> weights. Since the config object is stored in plain text, this attribute contains just the
floating type string without the <code>torch.</code> prefix. For example, for <code>torch.float16</code> <code><code>torch\_dtype&lt;/code&gt; is the</code>"float16"` string.</p>
<pre><code>This attribute is currently not being used during model loading time, but this may change in the future
versions. But we can already start preparing for the future by saving the dtype with save_pretrained.
</code></pre></div>
</dd>
<dt id="sglang.srt.models.internvl.InternVisionModel.main_input_name"><code class="name">var <span class="ident">main_input_name</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.internvl.InternVisionModel.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>pixel_values: torch.FloatTensor | None = None,<br>output_hidden_states: bool | None = None,<br>return_dict: bool | None = None,<br>pixel_embeds: torch.FloatTensor | None = None) ‑> Tuple | transformers.modeling_outputs.BaseModelOutputWithPooling</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    pixel_values: Optional[torch.FloatTensor] = None,
    output_hidden_states: Optional[bool] = None,
    return_dict: Optional[bool] = None,
    pixel_embeds: Optional[torch.FloatTensor] = None,
) -&gt; Union[Tuple, BaseModelOutputWithPooling]:
    pixel_values = pixel_values.to(device=self.device, dtype=self.dtype)
    output_hidden_states = (
        output_hidden_states
        if output_hidden_states is not None
        else self.config.output_hidden_states
    )
    return_dict = (
        return_dict if return_dict is not None else self.config.use_return_dict
    )

    if pixel_values is None and pixel_embeds is None:
        raise ValueError(&#34;You have to specify pixel_values or pixel_embeds&#34;)

    if pixel_embeds is not None:
        hidden_states = pixel_embeds
    else:
        if len(pixel_values.shape) == 4:
            hidden_states = self.embeddings(pixel_values)
        else:
            raise ValueError(f&#34;wrong pixel_values size: {pixel_values.shape}&#34;)
    encoder_outputs = self.encoder(
        inputs_embeds=hidden_states,
        output_hidden_states=output_hidden_states,
        return_dict=return_dict,
    )
    last_hidden_state = encoder_outputs.last_hidden_state
    pooled_output = last_hidden_state[:, 0, :]

    if not return_dict:
        return (last_hidden_state, pooled_output) + encoder_outputs[1:]

    return BaseModelOutputWithPooling(
        last_hidden_state=last_hidden_state,
        pooler_output=pooled_output,
        hidden_states=encoder_outputs.hidden_states,
        attentions=encoder_outputs.attentions,
    )</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
<dt id="sglang.srt.models.internvl.InternVisionModel.get_input_embeddings"><code class="name flex">
<span>def <span class="ident">get_input_embeddings</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_input_embeddings(self):
    return self.embeddings</code></pre>
</details>
<div class="desc"><p>Returns the model's input embeddings.</p>
<h2 id="returns">Returns</h2>
<p><code>nn.Module</code>: A torch module mapping vocabulary to hidden states.</p></div>
</dd>
<dt id="sglang.srt.models.internvl.InternVisionModel.resize_pos_embeddings"><code class="name flex">
<span>def <span class="ident">resize_pos_embeddings</span></span>(<span>self, old_size, new_size, patch_size)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resize_pos_embeddings(self, old_size, new_size, patch_size):
    pos_emb = self.embeddings.position_embedding
    _, num_positions, embed_dim = pos_emb.shape
    cls_emb = pos_emb[:, :1, :]
    pos_emb = (
        pos_emb[:, 1:, :]
        .reshape(1, old_size // patch_size, old_size // patch_size, -1)
        .permute(0, 3, 1, 2)
    )
    pos_emb = F.interpolate(
        pos_emb.float(),
        size=new_size // patch_size,
        mode=&#34;bicubic&#34;,
        align_corners=False,
    )
    pos_emb = pos_emb.to(cls_emb.dtype).reshape(1, embed_dim, -1).permute(0, 2, 1)
    pos_emb = torch.cat([cls_emb, pos_emb], dim=1)
    self.embeddings.position_embedding = nn.Parameter(pos_emb)
    self.embeddings.image_size = new_size
    logger.info(
        &#34;Resized position embeddings from {} to {}&#34;.format(old_size, new_size)
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.models" href="index.html">sglang.srt.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.models.internvl.InternAttention" href="#sglang.srt.models.internvl.InternAttention">InternAttention</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.internvl.InternAttention.forward" href="#sglang.srt.models.internvl.InternAttention.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.internvl.InternMLP" href="#sglang.srt.models.internvl.InternMLP">InternMLP</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.internvl.InternMLP.forward" href="#sglang.srt.models.internvl.InternMLP.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.internvl.InternRMSNorm" href="#sglang.srt.models.internvl.InternRMSNorm">InternRMSNorm</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.internvl.InternRMSNorm.forward" href="#sglang.srt.models.internvl.InternRMSNorm.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.internvl.InternVLChatModel" href="#sglang.srt.models.internvl.InternVLChatModel">InternVLChatModel</a></code></h4>
<ul class="two-column">
<li><code><a title="sglang.srt.models.internvl.InternVLChatModel.extract_feature" href="#sglang.srt.models.internvl.InternVLChatModel.extract_feature">extract_feature</a></code></li>
<li><code><a title="sglang.srt.models.internvl.InternVLChatModel.forward" href="#sglang.srt.models.internvl.InternVLChatModel.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.internvl.InternVLChatModel.get_image_feature" href="#sglang.srt.models.internvl.InternVLChatModel.get_image_feature">get_image_feature</a></code></li>
<li><code><a title="sglang.srt.models.internvl.InternVLChatModel.load_weights" href="#sglang.srt.models.internvl.InternVLChatModel.load_weights">load_weights</a></code></li>
<li><code><a title="sglang.srt.models.internvl.InternVLChatModel.pad_input_ids" href="#sglang.srt.models.internvl.InternVLChatModel.pad_input_ids">pad_input_ids</a></code></li>
<li><code><a title="sglang.srt.models.internvl.InternVLChatModel.pixel_shuffle" href="#sglang.srt.models.internvl.InternVLChatModel.pixel_shuffle">pixel_shuffle</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.internvl.InternVLChatModel" href="#sglang.srt.models.internvl.InternVLChatModel">InternVLChatModel</a></code></h4>
<ul class="two-column">
<li><code><a title="sglang.srt.models.internvl.InternVLChatModel.extract_feature" href="#sglang.srt.models.internvl.InternVLChatModel.extract_feature">extract_feature</a></code></li>
<li><code><a title="sglang.srt.models.internvl.InternVLChatModel.forward" href="#sglang.srt.models.internvl.InternVLChatModel.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.internvl.InternVLChatModel.get_image_feature" href="#sglang.srt.models.internvl.InternVLChatModel.get_image_feature">get_image_feature</a></code></li>
<li><code><a title="sglang.srt.models.internvl.InternVLChatModel.load_weights" href="#sglang.srt.models.internvl.InternVLChatModel.load_weights">load_weights</a></code></li>
<li><code><a title="sglang.srt.models.internvl.InternVLChatModel.pad_input_ids" href="#sglang.srt.models.internvl.InternVLChatModel.pad_input_ids">pad_input_ids</a></code></li>
<li><code><a title="sglang.srt.models.internvl.InternVLChatModel.pixel_shuffle" href="#sglang.srt.models.internvl.InternVLChatModel.pixel_shuffle">pixel_shuffle</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.internvl.InternVisionEmbeddings" href="#sglang.srt.models.internvl.InternVisionEmbeddings">InternVisionEmbeddings</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.internvl.InternVisionEmbeddings.forward" href="#sglang.srt.models.internvl.InternVisionEmbeddings.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.internvl.InternVisionEncoder" href="#sglang.srt.models.internvl.InternVisionEncoder">InternVisionEncoder</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.internvl.InternVisionEncoder.forward" href="#sglang.srt.models.internvl.InternVisionEncoder.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.internvl.InternVisionEncoderLayer" href="#sglang.srt.models.internvl.InternVisionEncoderLayer">InternVisionEncoderLayer</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.internvl.InternVisionEncoderLayer.forward" href="#sglang.srt.models.internvl.InternVisionEncoderLayer.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.internvl.InternVisionModel" href="#sglang.srt.models.internvl.InternVisionModel">InternVisionModel</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.internvl.InternVisionModel.config_class" href="#sglang.srt.models.internvl.InternVisionModel.config_class">config_class</a></code></li>
<li><code><a title="sglang.srt.models.internvl.InternVisionModel.forward" href="#sglang.srt.models.internvl.InternVisionModel.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.internvl.InternVisionModel.get_input_embeddings" href="#sglang.srt.models.internvl.InternVisionModel.get_input_embeddings">get_input_embeddings</a></code></li>
<li><code><a title="sglang.srt.models.internvl.InternVisionModel.main_input_name" href="#sglang.srt.models.internvl.InternVisionModel.main_input_name">main_input_name</a></code></li>
<li><code><a title="sglang.srt.models.internvl.InternVisionModel.resize_pos_embeddings" href="#sglang.srt.models.internvl.InternVisionModel.resize_pos_embeddings">resize_pos_embeddings</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
