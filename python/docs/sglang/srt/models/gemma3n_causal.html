<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.models.gemma3n_causal API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.models.gemma3n_causal</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.models.gemma3n_causal.get_attention_sliding_window_size"><code class="name flex">
<span>def <span class="ident">get_attention_sliding_window_size</span></span>(<span>config)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_attention_sliding_window_size(config):
    return config.sliding_window - 1</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nAltUp"><code class="flex name class">
<span>class <span class="ident">Gemma3nAltUp</span></span>
<span>(</span><span>config: transformers.models.gemma3n.configuration_gemma3n.Gemma3nTextConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nAltUp(nn.Module):
    &#34;&#34;&#34;Alternating Updates (AltUp)&#34;&#34;&#34;

    def __init__(
        self,
        config: Gemma3nTextConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ):
        super().__init__()
        self.config = config

        self.correct_output_scale = nn.Parameter(
            torch.zeros(config.hidden_size, dtype=torch.float32)
        )
        self.correction_coefs = ColumnParallelLinear(
            config.altup_num_inputs,
            config.altup_num_inputs,
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;correction_coefs&#34;, prefix),
        )
        self.prediction_coefs = ColumnParallelLinear(
            config.altup_num_inputs,
            config.altup_num_inputs**2,
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;prediction_coefs&#34;, prefix),
        )
        self.modality_router = ColumnParallelLinear(
            config.hidden_size,
            config.altup_num_inputs,
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;modality_router&#34;, prefix),
        )

        self.router_norm = Gemma3nRMSNorm(
            dim=config.hidden_size,
            eps=config.rms_norm_eps,
        )

        self.register_buffer(
            &#34;router_input_scale&#34;,
            torch.tensor(config.hidden_size**-1.0),
            persistent=False,
        )

    def compute_router_modalities(self, x: torch.Tensor) -&gt; torch.Tensor:
        # x  : [num_tokens, hidden_size]
        router_inputs = self.router_norm(x) * self.router_input_scale.to(
            self.router_norm.weight.dtype
        )
        # router_inputs : [num_tokens, hidden_size]
        routed, _ = self.modality_router(router_inputs)

        # routed : [num_tokens, altup_num_inputs]
        return torch.tanh(routed.float()).type_as(routed)

    def predict(self, hidden_states: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;Predicts the output of a layer using a trainable map.
        hidden_states: [num_altup_inputs, num_tokens, hidden_size]
        &#34;&#34;&#34;
        modalities = self.compute_router_modalities(
            hidden_states[self.config.altup_active_idx]
        )  # (n_tokens, altup_num_inputs)
        # TODO: CHECK DO WE NEED THIS: self.prediction_coefs.float()  # Force computation in float32, in-place operation

        if self.config.altup_coef_clip is not None:
            self.prediction_coefs.weight.data.clamp_(
                -self.config.altup_coef_clip, self.config.altup_coef_clip
            )

        all_coefs, _ = self.prediction_coefs(
            modalities
        )  # (n_tokens, altup_num_inputs) -&gt; (n_tokens, altup_num_inputs**2)

        all_coefs = all_coefs.reshape(
            *modalities.shape[:-1],
            self.config.altup_num_inputs,
            self.config.altup_num_inputs,
        ).permute(0, 2, 1)

        # permute hidden_states from [num_altup_inputs, num_tokens, hidden_size] to [num_tokens, hidden_size, altup_num_inputs]
        predictions = torch.matmul(hidden_states.permute(1, 2, 0), all_coefs)
        predictions = predictions.permute(2, 0, 1)  # undo the permute
        predictions += hidden_states  # add the original input
        return predictions.contiguous().type_as(
            hidden_states
        )  # [num_altup_inputs, num_tokens, hidden_size]

    def correct(
        self, predictions: torch.Tensor, activated: torch.Tensor
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;Corrects the predictions relative to the activated inputs.&#34;&#34;&#34;
        # prediction : [num_altup_inputs, num_tokens, hidden_size]
        # activated  : [num_tokens, hidden_size]
        modalities = self.compute_router_modalities(
            activated
        )  # [num_tokens, altup_num_inputs]
        innovation = (
            activated - predictions[self.config.altup_active_idx]
        )  # [num_tokens, hidden_size]
        innovation = innovation.repeat(
            self.config.altup_num_inputs, 1, 1
        )  # (self.config.altup_num_inputs, num_tokens, hidden_size)

        if self.config.altup_coef_clip is not None:
            self.correction_coefs.weight.data.clamp_(
                -self.config.altup_coef_clip, self.config.altup_coef_clip
            )

        all_coefs, _ = self.correction_coefs(
            modalities
        )  # [num_tokens, altup_num_inputs]
        all_coefs = (all_coefs + 1.0).permute(1, 0).unsqueeze(-1)
        # # [num_tokens, altup_num_inputs, 1]

        corrected = torch.mul(innovation, all_coefs)
        corrected += predictions
        return corrected.contiguous().type_as(activated)

    def scale_corrected_output(self, corrected: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;Scales the provided 3D tensor.&#34;&#34;&#34;
        return corrected * self.correct_output_scale.to(corrected.dtype)

    def forward(
        self, hidden_states: torch.Tensor, activated: torch.Tensor
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;Predicts, correct, and optionally scales the output of a layer using trainable maps.

        hidden_states: [num_altup_inputs, num_tokens, hidden_size]
        &#34;&#34;&#34;

        predictions = self.predict(hidden_states)
        corrected = self.correct(predictions=predictions, activated=activated)
        output = corrected[self.config.altup_active_idx]
        if self.config.altup_correct_scale:
            output = self.scale_corrected_output(output)
        return corrected, output</code></pre>
</details>
<div class="desc"><p>Alternating Updates (AltUp)</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nAltUp.compute_router_modalities"><code class="name flex">
<span>def <span class="ident">compute_router_modalities</span></span>(<span>self, x: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_router_modalities(self, x: torch.Tensor) -&gt; torch.Tensor:
    # x  : [num_tokens, hidden_size]
    router_inputs = self.router_norm(x) * self.router_input_scale.to(
        self.router_norm.weight.dtype
    )
    # router_inputs : [num_tokens, hidden_size]
    routed, _ = self.modality_router(router_inputs)

    # routed : [num_tokens, altup_num_inputs]
    return torch.tanh(routed.float()).type_as(routed)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nAltUp.correct"><code class="name flex">
<span>def <span class="ident">correct</span></span>(<span>self, predictions: torch.Tensor, activated: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def correct(
    self, predictions: torch.Tensor, activated: torch.Tensor
) -&gt; torch.Tensor:
    &#34;&#34;&#34;Corrects the predictions relative to the activated inputs.&#34;&#34;&#34;
    # prediction : [num_altup_inputs, num_tokens, hidden_size]
    # activated  : [num_tokens, hidden_size]
    modalities = self.compute_router_modalities(
        activated
    )  # [num_tokens, altup_num_inputs]
    innovation = (
        activated - predictions[self.config.altup_active_idx]
    )  # [num_tokens, hidden_size]
    innovation = innovation.repeat(
        self.config.altup_num_inputs, 1, 1
    )  # (self.config.altup_num_inputs, num_tokens, hidden_size)

    if self.config.altup_coef_clip is not None:
        self.correction_coefs.weight.data.clamp_(
            -self.config.altup_coef_clip, self.config.altup_coef_clip
        )

    all_coefs, _ = self.correction_coefs(
        modalities
    )  # [num_tokens, altup_num_inputs]
    all_coefs = (all_coefs + 1.0).permute(1, 0).unsqueeze(-1)
    # # [num_tokens, altup_num_inputs, 1]

    corrected = torch.mul(innovation, all_coefs)
    corrected += predictions
    return corrected.contiguous().type_as(activated)</code></pre>
</details>
<div class="desc"><p>Corrects the predictions relative to the activated inputs.</p></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nAltUp.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, hidden_states: torch.Tensor, activated: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, hidden_states: torch.Tensor, activated: torch.Tensor
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;Predicts, correct, and optionally scales the output of a layer using trainable maps.

    hidden_states: [num_altup_inputs, num_tokens, hidden_size]
    &#34;&#34;&#34;

    predictions = self.predict(hidden_states)
    corrected = self.correct(predictions=predictions, activated=activated)
    output = corrected[self.config.altup_active_idx]
    if self.config.altup_correct_scale:
        output = self.scale_corrected_output(output)
    return corrected, output</code></pre>
</details>
<div class="desc"><p>Predicts, correct, and optionally scales the output of a layer using trainable maps.</p>
<p>hidden_states: [num_altup_inputs, num_tokens, hidden_size]</p></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nAltUp.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, hidden_states: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, hidden_states: torch.Tensor) -&gt; torch.Tensor:
    &#34;&#34;&#34;Predicts the output of a layer using a trainable map.
    hidden_states: [num_altup_inputs, num_tokens, hidden_size]
    &#34;&#34;&#34;
    modalities = self.compute_router_modalities(
        hidden_states[self.config.altup_active_idx]
    )  # (n_tokens, altup_num_inputs)
    # TODO: CHECK DO WE NEED THIS: self.prediction_coefs.float()  # Force computation in float32, in-place operation

    if self.config.altup_coef_clip is not None:
        self.prediction_coefs.weight.data.clamp_(
            -self.config.altup_coef_clip, self.config.altup_coef_clip
        )

    all_coefs, _ = self.prediction_coefs(
        modalities
    )  # (n_tokens, altup_num_inputs) -&gt; (n_tokens, altup_num_inputs**2)

    all_coefs = all_coefs.reshape(
        *modalities.shape[:-1],
        self.config.altup_num_inputs,
        self.config.altup_num_inputs,
    ).permute(0, 2, 1)

    # permute hidden_states from [num_altup_inputs, num_tokens, hidden_size] to [num_tokens, hidden_size, altup_num_inputs]
    predictions = torch.matmul(hidden_states.permute(1, 2, 0), all_coefs)
    predictions = predictions.permute(2, 0, 1)  # undo the permute
    predictions += hidden_states  # add the original input
    return predictions.contiguous().type_as(
        hidden_states
    )  # [num_altup_inputs, num_tokens, hidden_size]</code></pre>
</details>
<div class="desc"><p>Predicts the output of a layer using a trainable map.
hidden_states: [num_altup_inputs, num_tokens, hidden_size]</p></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nAltUp.scale_corrected_output"><code class="name flex">
<span>def <span class="ident">scale_corrected_output</span></span>(<span>self, corrected: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scale_corrected_output(self, corrected: torch.Tensor) -&gt; torch.Tensor:
    &#34;&#34;&#34;Scales the provided 3D tensor.&#34;&#34;&#34;
    return corrected * self.correct_output_scale.to(corrected.dtype)</code></pre>
</details>
<div class="desc"><p>Scales the provided 3D tensor.</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nAttention"><code class="flex name class">
<span>class <span class="ident">Gemma3nAttention</span></span>
<span>(</span><span>layer_id: int,<br>config: transformers.models.gemma3n.configuration_gemma3n.Gemma3nTextConfig,<br>max_position_embeddings: int,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nAttention(nn.Module):
    &#34;&#34;&#34;Multi-headed attention from &#39;Attention Is All You Need&#39; paper&#34;&#34;&#34;

    def __init__(
        self,
        layer_id: int,
        config: Gemma3nTextConfig,
        max_position_embeddings: int,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; None:
        super().__init__()
        self.layer_id = layer_id
        self.config = config
        tp_size = get_tensor_model_parallel_world_size()

        self.total_num_heads = config.num_attention_heads
        assert self.total_num_heads % tp_size == 0
        self.num_heads = self.total_num_heads // tp_size
        self.total_num_kv_heads = config.num_key_value_heads

        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)

        if self.total_num_kv_heads &gt;= tp_size:
            assert self.total_num_kv_heads % tp_size == 0
        else:
            assert tp_size % self.total_num_kv_heads == 0

        hidden_size = config.hidden_size
        head_dim = getattr(
            config, &#34;head_dim&#34;, hidden_size // config.num_attention_heads
        )
        self.head_dim = head_dim

        self.q_size = self.num_heads * self.head_dim
        self.kv_size = self.num_kv_heads * self.head_dim
        # self.scaling = config.query_rescale_scalar / config.query_pre_attn_scalar
        self.scaling = 1.0

        self.qkv_proj = QKVParallelLinear(
            hidden_size,
            self.head_dim,
            self.total_num_heads,
            self.total_num_kv_heads,
            bias=config.attention_bias,
            quant_config=quant_config,
            prefix=add_prefix(&#34;qkv_proj&#34;, prefix),
        )
        self.o_proj = RowParallelLinear(
            self.total_num_heads * self.head_dim,
            hidden_size,
            bias=config.attention_bias,
            quant_config=quant_config,
            prefix=add_prefix(&#34;o_proj&#34;, prefix),
        )

        # Determine if layer uses sliding window based on pattern
        self.is_sliding = config.layer_types[layer_id] == &#34;sliding_attention&#34;

        # Check if this is a KV shared layer
        first_kv_shared_layer_idx = (
            config.num_hidden_layers - config.num_kv_shared_layers
        )
        self.is_kv_shared_layer = layer_id &gt;= first_kv_shared_layer_idx

        # Compute the layer index from which shared KV cache values will be retrieved
        if not self.is_kv_shared_layer:
            self.kv_shared_layer_index = None
        elif self.is_sliding:
            self.kv_shared_layer_index = first_kv_shared_layer_idx - 2
        else:
            self.kv_shared_layer_index = first_kv_shared_layer_idx - 1

        if self.is_sliding:
            self.rotary_emb = get_rope(
                self.head_dim,
                rotary_dim=self.head_dim,
                max_position=config.max_position_embeddings,
                base=config.rope_local_base_freq,
                rope_scaling={&#34;rope_type&#34;: &#34;default&#34;},
            )
        else:
            self.rotary_emb = get_rope(
                self.head_dim,
                rotary_dim=self.head_dim,
                max_position=config.max_position_embeddings,
                base=config.rope_theta,
                rope_scaling=config.rope_scaling,
            )

        self.sliding_window = config.sliding_window if self.is_sliding else None

        self.attn = RadixAttention(
            self.num_heads,
            self.head_dim,
            self.scaling,
            num_kv_heads=self.num_kv_heads,
            layer_id=(
                layer_id if not self.is_kv_shared_layer else self.kv_shared_layer_index
            ),
            logit_cap=0.0,
            sliding_window_size=self.sliding_window,
            quant_config=quant_config,
            prefix=add_prefix(&#34;attn&#34;, prefix),
        )

        # Gemma3n adds normalization for q, k, v
        self.q_norm = Gemma3nRMSNorm(
            dim=config.head_dim,
            eps=config.rms_norm_eps,
        )
        self.k_norm = Gemma3nRMSNorm(
            dim=config.head_dim,
            eps=config.rms_norm_eps,
        )
        self.v_norm = Gemma3nRMSNorm(
            dim=config.head_dim,
            eps=config.rms_norm_eps,
            with_scale=False,
        )

    def forward(
        self,
        hidden_states: torch.Tensor,
        positions: Tuple[torch.Tensor, torch.Tensor],
        forward_batch: ForwardBatch,
        **kwargs,
    ) -&gt; torch.Tensor:

        qkv, _ = self.qkv_proj(hidden_states)
        # TODO: for first 20 layers, we use QKVParallelLinear
        #       for others, we only calc Q.
        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)

        # Apply normalization to q, k, v
        q = q.unflatten(-1, (self.num_heads, self.head_dim))
        q = self.q_norm(q)

        # Check if we should use shared KV cache
        if self.is_kv_shared_layer and self.kv_shared_layer_index is not None:
            # For KV shared layers, we skip K/V computation and normalization
            # The RadixAttention will handle retrieving shared KV from cache
            k = None
            v = None
        else:
            k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
            k = self.k_norm(k)

            v = v.unflatten(-1, (self.num_kv_heads, self.head_dim))
            v = self.v_norm(v)

        # Flatten back for rotary embedding
        q = q.flatten(-2, -1)

        # Apply rotary embedding
        if k is not None:
            k = k.flatten(-2, -1)
            q, k = self.rotary_emb(positions, q, k)
            # Reshape k back to head format for attention
            k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
        else:
            # For shared KV layers, create a dummy key for rotary embedding and discard it
            dummy_k = torch.zeros_like(
                q[:, : self.kv_size]
            )  # Create dummy key with same shape as needed
            q, _ = self.rotary_emb(positions, q, dummy_k)

        # Reshape q back to head format for attention
        q = q.unflatten(-1, (self.num_heads, self.head_dim))

        attn_output = self.attn(
            q,
            k,
            v,
            forward_batch=forward_batch,
            save_kv_cache=not self.is_kv_shared_layer,
        )

        output, _ = self.o_proj(attn_output)
        return output</code></pre>
</details>
<div class="desc"><p>Multi-headed attention from 'Attention Is All You Need' paper</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nAttention.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>hidden_states: torch.Tensor,<br>positions: Tuple[torch.Tensor, torch.Tensor],<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>**kwargs) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    hidden_states: torch.Tensor,
    positions: Tuple[torch.Tensor, torch.Tensor],
    forward_batch: ForwardBatch,
    **kwargs,
) -&gt; torch.Tensor:

    qkv, _ = self.qkv_proj(hidden_states)
    # TODO: for first 20 layers, we use QKVParallelLinear
    #       for others, we only calc Q.
    q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)

    # Apply normalization to q, k, v
    q = q.unflatten(-1, (self.num_heads, self.head_dim))
    q = self.q_norm(q)

    # Check if we should use shared KV cache
    if self.is_kv_shared_layer and self.kv_shared_layer_index is not None:
        # For KV shared layers, we skip K/V computation and normalization
        # The RadixAttention will handle retrieving shared KV from cache
        k = None
        v = None
    else:
        k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
        k = self.k_norm(k)

        v = v.unflatten(-1, (self.num_kv_heads, self.head_dim))
        v = self.v_norm(v)

    # Flatten back for rotary embedding
    q = q.flatten(-2, -1)

    # Apply rotary embedding
    if k is not None:
        k = k.flatten(-2, -1)
        q, k = self.rotary_emb(positions, q, k)
        # Reshape k back to head format for attention
        k = k.unflatten(-1, (self.num_kv_heads, self.head_dim))
    else:
        # For shared KV layers, create a dummy key for rotary embedding and discard it
        dummy_k = torch.zeros_like(
            q[:, : self.kv_size]
        )  # Create dummy key with same shape as needed
        q, _ = self.rotary_emb(positions, q, dummy_k)

    # Reshape q back to head format for attention
    q = q.unflatten(-1, (self.num_heads, self.head_dim))

    attn_output = self.attn(
        q,
        k,
        v,
        forward_batch=forward_batch,
        save_kv_cache=not self.is_kv_shared_layer,
    )

    output, _ = self.o_proj(attn_output)
    return output</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nDecoderLayer"><code class="flex name class">
<span>class <span class="ident">Gemma3nDecoderLayer</span></span>
<span>(</span><span>layer_id: int,<br>config: transformers.configuration_utils.PretrainedConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nDecoderLayer(nn.Module):
    def __init__(
        self,
        layer_id: int,
        config: PretrainedConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; None:
        super().__init__()
        self.hidden_size = config.hidden_size
        self.layer_id = layer_id
        self.attention_type = config.layer_types[layer_id]
        self.config = config

        self.self_attn = Gemma3nAttention(
            layer_id=layer_id,
            config=config,
            max_position_embeddings=config.max_position_embeddings,
            quant_config=quant_config,
            prefix=add_prefix(&#34;self_attn&#34;, prefix),
        )

        intermediate_size = config.intermediate_size[layer_id]
        activation_sparsity = config.activation_sparsity_pattern[layer_id]
        self.mlp = Gemma3nTextMLP(
            hidden_size=self.hidden_size,
            intermediate_size=intermediate_size,
            hidden_activation=config.hidden_activation,
            activation_sparsity=activation_sparsity,
            quant_config=quant_config,
            prefix=add_prefix(&#34;mlp&#34;, prefix),
        )

        self.input_layernorm = Gemma3nRMSNorm(self.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Gemma3nRMSNorm(
            self.hidden_size, eps=config.rms_norm_eps
        )
        self.pre_feedforward_layernorm = Gemma3nRMSNorm(
            self.hidden_size, eps=config.rms_norm_eps
        )
        self.post_feedforward_layernorm = Gemma3nRMSNorm(
            self.hidden_size, eps=config.rms_norm_eps
        )

        self.hidden_size_per_layer_input = config.hidden_size_per_layer_input

        self.altup = Gemma3nAltUp(
            config, quant_config, prefix=add_prefix(&#34;altup&#34;, prefix)
        )
        self.laurel = Gemma3nLaurelBlock(
            config, quant_config, prefix=add_prefix(&#34;laurel&#34;, prefix)
        )

        self.per_layer_input_gate = ColumnParallelLinear(
            self.hidden_size,
            self.hidden_size_per_layer_input,
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;per_layer_input_gate&#34;, prefix),
        )
        self.per_layer_projection = RowParallelLinear(
            self.hidden_size_per_layer_input,
            self.hidden_size,
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;per_layer_projection&#34;, prefix),
        )
        self.post_per_layer_input_norm = Gemma3nRMSNorm(
            self.hidden_size, eps=config.rms_norm_eps
        )
        self.is_sliding = self.self_attn.is_sliding

    def forward(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        per_layer_input: torch.Tensor,
        forward_batch: ForwardBatch,
        **kwargs,
    ) -&gt; torch.Tensor:
        predictions = self.altup.predict(
            hidden_states
        )  # [num_altup_inputs, num_tokens, hidden_size]
        active_prediction = predictions[self.config.altup_active_idx]

        active_prediction_normed = self.input_layernorm(active_prediction)
        laurel_output = self.laurel(
            active_prediction_normed
        )  # laurel_output: [num_tokens, hidden_size]
        # active_prediction: [num_tokens, hidden_size]

        attn = self.self_attn(
            positions=positions,
            hidden_states=active_prediction_normed,
            forward_batch=forward_batch,
            **kwargs,
        )
        attn = self.post_attention_layernorm(attn)  # [num_tokens, hidden_size]

        attn_gated = active_prediction + attn  # [num_tokens, hidden_size]
        attn_laurel = (attn_gated + laurel_output) / torch.sqrt(torch.tensor(2.0))

        attn_norm = self.pre_feedforward_layernorm(
            attn_laurel
        )  # [num_tokens, hidden_size]
        attn_ffw = self.mlp(attn_norm)  # [num_tokens, hidden_size]
        attn_ffw_norm = self.post_feedforward_layernorm(
            attn_ffw
        )  # [num_tokens, hidden_size]
        attn_ffw_laurel_gated = attn_laurel + attn_ffw_norm  # [num_tokens, hidden_size]
        corrected_predictions = self.altup.correct(
            predictions, attn_ffw_laurel_gated
        )  # prediction : [num_altup_inputs, num_tokens, hidden_size]
        # attn_ffw_laurel_gated: [num_tokens, hidden_size]
        first_prediction = corrected_predictions[self.config.altup_active_idx]

        if self.config.altup_correct_scale:
            first_prediction = self.altup.scale_corrected_output(first_prediction)

        # per_layer_input_gate
        first_prediction = first_prediction.to(self.per_layer_input_gate.weight.dtype)
        first_prediction, _ = self.per_layer_input_gate(first_prediction)
        first_prediction = F.gelu(first_prediction, approximate=&#34;tanh&#34;)
        first_prediction = torch.multiply(first_prediction, per_layer_input)

        # per_layer_projection
        first_prediction, _ = self.per_layer_projection(first_prediction)
        first_prediction = self.post_per_layer_input_norm(first_prediction)
        corrected_predictions[1:] += first_prediction

        return corrected_predictions</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nDecoderLayer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>positions: torch.Tensor,<br>hidden_states: torch.Tensor,<br>per_layer_input: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>**kwargs) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    positions: torch.Tensor,
    hidden_states: torch.Tensor,
    per_layer_input: torch.Tensor,
    forward_batch: ForwardBatch,
    **kwargs,
) -&gt; torch.Tensor:
    predictions = self.altup.predict(
        hidden_states
    )  # [num_altup_inputs, num_tokens, hidden_size]
    active_prediction = predictions[self.config.altup_active_idx]

    active_prediction_normed = self.input_layernorm(active_prediction)
    laurel_output = self.laurel(
        active_prediction_normed
    )  # laurel_output: [num_tokens, hidden_size]
    # active_prediction: [num_tokens, hidden_size]

    attn = self.self_attn(
        positions=positions,
        hidden_states=active_prediction_normed,
        forward_batch=forward_batch,
        **kwargs,
    )
    attn = self.post_attention_layernorm(attn)  # [num_tokens, hidden_size]

    attn_gated = active_prediction + attn  # [num_tokens, hidden_size]
    attn_laurel = (attn_gated + laurel_output) / torch.sqrt(torch.tensor(2.0))

    attn_norm = self.pre_feedforward_layernorm(
        attn_laurel
    )  # [num_tokens, hidden_size]
    attn_ffw = self.mlp(attn_norm)  # [num_tokens, hidden_size]
    attn_ffw_norm = self.post_feedforward_layernorm(
        attn_ffw
    )  # [num_tokens, hidden_size]
    attn_ffw_laurel_gated = attn_laurel + attn_ffw_norm  # [num_tokens, hidden_size]
    corrected_predictions = self.altup.correct(
        predictions, attn_ffw_laurel_gated
    )  # prediction : [num_altup_inputs, num_tokens, hidden_size]
    # attn_ffw_laurel_gated: [num_tokens, hidden_size]
    first_prediction = corrected_predictions[self.config.altup_active_idx]

    if self.config.altup_correct_scale:
        first_prediction = self.altup.scale_corrected_output(first_prediction)

    # per_layer_input_gate
    first_prediction = first_prediction.to(self.per_layer_input_gate.weight.dtype)
    first_prediction, _ = self.per_layer_input_gate(first_prediction)
    first_prediction = F.gelu(first_prediction, approximate=&#34;tanh&#34;)
    first_prediction = torch.multiply(first_prediction, per_layer_input)

    # per_layer_projection
    first_prediction, _ = self.per_layer_projection(first_prediction)
    first_prediction = self.post_per_layer_input_norm(first_prediction)
    corrected_predictions[1:] += first_prediction

    return corrected_predictions</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM"><code class="flex name class">
<span>class <span class="ident">Gemma3nForCausalLM</span></span>
<span>(</span><span>config: transformers.models.gemma3n.configuration_gemma3n.Gemma3nTextConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nForCausalLM(PreTrainedModel):
    config_class = Gemma3nTextConfig

    _tied_weights_keys = [&#34;lm_head.weight&#34;]
    _tp_plan = {&#34;lm_head&#34;: &#34;colwise_rep&#34;}
    _pp_plan = {&#34;lm_head&#34;: ([&#34;hidden_states&#34;], [&#34;logits&#34;])}
    config_class = Gemma3nTextConfig
    base_model_prefix = &#34;language_model&#34;

    # BitandBytes specific attributes
    default_bitsandbytes_target_modules = [
        &#34;.gate_proj.&#34;,
        &#34;.down_proj.&#34;,
        &#34;.up_proj.&#34;,
        &#34;.q_proj.&#34;,
        &#34;.k_proj.&#34;,
        &#34;.v_proj.&#34;,
        &#34;.o_proj.&#34;,
    ]
    bitsandbytes_stacked_params_mapping = {
        &#34;.q_proj&#34;: (&#34;.qkv_proj&#34;, 0),
        &#34;.k_proj&#34;: (&#34;.qkv_proj&#34;, 1),
        &#34;.v_proj&#34;: (&#34;.qkv_proj&#34;, 2),
        &#34;.gate_proj&#34;: (&#34;.gate_up_proj&#34;, 0),
        &#34;.up_proj&#34;: (&#34;.gate_up_proj&#34;, 1),
    }

    packed_modules_mapping = {
        &#34;.qkv_proj&#34;: [
            &#34;.q_proj&#34;,
            &#34;.k_proj&#34;,
            &#34;.v_proj&#34;,
        ],
        &#34;.gate_up_proj&#34;: [
            &#34;.gate_proj&#34;,
            &#34;.up_proj&#34;,
        ],
    }

    # LoRA specific attributes
    supported_lora_modules = [
        &#34;.qkv_proj&#34;,
        &#34;.o_proj&#34;,
        &#34;.gate_up_proj&#34;,
        &#34;.down_proj&#34;,
    ]
    # Gemma does not apply LoRA to the embedding layer
    embedding_modules = {}
    embedding_padding_modules = []
    supports_lora = True

    def __init__(
        self,
        config: Gemma3nTextConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; None:
        super().__init__(config=config)
        self.config = config
        self.quant_config = quant_config
        self.model = Gemma3nTextModel(
            config=config,
            quant_config=quant_config,
            prefix=add_prefix(&#34;model&#34;, prefix),
        )
        self.logits_processor = LogitsProcessor(config)

        if self.config.tie_word_embeddings:
            self.lm_head = self.model.embed_tokens
        else:
            self.lm_head = ParallelLMHead(
                config.vocab_size,
                config.hidden_size,
                quant_config=quant_config,
                prefix=add_prefix(&#34;lm_head&#34;, prefix),
            )
        self.post_init()

    def get_input_embeddings(self) -&gt; nn.Embedding:
        return self.model.embed_tokens

    def get_attention_sliding_window_size(self):
        return get_attention_sliding_window_size(self.config)

    def dtype(self) -&gt; torch.dtype:
        return next(self.parameters()).dtype

    @torch.no_grad()
    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        forward_batch: ForwardBatch,
        input_embeds: torch.Tensor = None,
        per_layer_inputs: Optional[torch.Tensor] = None,
        **kwargs,
    ) -&gt; LogitsProcessor:
        hidden_states = self.model(
            input_ids,
            positions,
            forward_batch,
            input_embeds,
            per_layer_inputs,
            **kwargs,
        )

        return self.logits_processor(
            input_ids, hidden_states, self.model.embed_tokens, forward_batch
        )

    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
        stacked_params_mapping = [
            # (param_name, shard_name, shard_id)
            (&#34;.qkv_proj&#34;, &#34;.q_proj&#34;, &#34;q&#34;),
            (&#34;.qkv_proj&#34;, &#34;.k_proj&#34;, &#34;k&#34;),
            (&#34;.qkv_proj&#34;, &#34;.v_proj&#34;, &#34;v&#34;),
            (&#34;.gate_up_proj&#34;, &#34;.gate_proj&#34;, 0),
            (&#34;.gate_up_proj&#34;, &#34;.up_proj&#34;, 1),
        ]
        params_dict = dict(self.named_parameters())
        loaded_params: Set[str] = set()

        for name, loaded_weight in weights:
            name = name.replace(&#34;model.language_model.&#34;, &#34;model.&#34;)
            for param_name, shard_name, shard_id in stacked_params_mapping:
                if shard_name not in name:
                    continue
                name = name.replace(shard_name, param_name)
                # Skip loading extra bias for GPTQ models
                if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                    continue
                if name not in params_dict:
                    # Skip loading weights that are not in the model
                    continue
                param = params_dict[name]
                weight_loader = param.weight_loader
                weight_loader(param, loaded_weight, shard_id)
                break
            else:
                # lm_head is not used in vllm as it is tied with embed_token
                if &#34;lm_head.weight&#34; in name:
                    continue
                # Skip loading extra bias for GPTQ models
                if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                    continue
                # Remapping the name of FP8 kv-scale
                name = maybe_remap_kv_scale_name(name, params_dict)
                if name is None:
                    continue
                if name not in params_dict:
                    # Skip loading weights that are not in the model
                    continue

                param = params_dict[name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(param, loaded_weight)
            loaded_params.add(name)
        return loaded_params</code></pre>
</details>
<div class="desc"><p>Base class for all models.</p>
<p>[<code>PreTrainedModel</code>] takes care of storing the configuration of the models and handles methods for loading,
downloading and saving models as well as a few methods common to all models to:</p>
<pre><code>- resize the input embeddings,
- prune heads in the self-attention heads.
</code></pre>
<p>Class attributes (overridden by derived classes):</p>
<pre><code>- **config_class** ([`PretrainedConfig`]) -- A subclass of [`PretrainedConfig`] to use as configuration class
  for this model architecture.
- **load_tf_weights** (&lt;code&gt;Callable&lt;/code&gt;) -- A python *method* for loading a TensorFlow checkpoint in a PyTorch model,
  taking as arguments:

    - **model** ([`PreTrainedModel`]) -- An instance of the model on which to load the TensorFlow checkpoint.
    - **config** ([`PreTrainedConfig`]) -- An instance of the configuration associated to the model.
    - **path** (&lt;code&gt;str&lt;/code&gt;) -- A path to the TensorFlow checkpoint.

- **base_model_prefix** (&lt;code&gt;str&lt;/code&gt;) -- A string indicating the attribute associated to the base model in derived
  classes of the same architecture adding modules on top of the base model.
- **is_parallelizable** (&lt;code&gt;bool&lt;/code&gt;) -- A flag indicating whether this model supports model parallelization.
- **main_input_name** (&lt;code&gt;str&lt;/code&gt;) -- The name of the principal input to the model (often &lt;code&gt;input\_ids&lt;/code&gt; for NLP
  models, &lt;code&gt;pixel\_values&lt;/code&gt; for vision models and &lt;code&gt;input\_values&lt;/code&gt; for speech models).
- **can_record_outputs** (dict):
</code></pre>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.modeling_utils.PreTrainedModel</li>
<li>torch.nn.modules.module.Module</li>
<li>transformers.modeling_utils.EmbeddingAccessMixin</li>
<li>transformers.modeling_utils.ModuleUtilsMixin</li>
<li>transformers.utils.hub.PushToHubMixin</li>
<li>transformers.integrations.peft.PeftAdapterMixin</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.base_model_prefix"><code class="name">var <span class="ident">base_model_prefix</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.bitsandbytes_stacked_params_mapping"><code class="name">var <span class="ident">bitsandbytes_stacked_params_mapping</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.config_class"><code class="name">var <span class="ident">config_class</span></code></dt>
<dd>
<div class="desc"><p>This is the configuration class to store the configuration of a [<code>Gemma3nTextModel</code>]. It is used to instantiate an
Gemma3nTextModel model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the Gemma 3n E4B, e.g.
<a href="https://huggingface.co/google/gemma-3n-E4B">google/gemma-3n-E4B</a>.</p>
<p>Configuration objects that inherit from [<code>Gemma3nTextConfig</code>] and can be used to control the model outputs. Read
the documentation from [<code>Gemma3nTextConfig</code>] for more information.</p>
<h2 id="args">Args</h2>
<p>vocab_size (<code>int</code>, <em>optional</em>, defaults to 262400):
Vocabulary size of the Gemma3nText model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling [<code>Gemma3nTextModel</code>]
vocab_size_per_layer_input (<code>int</code>, <em>optional</em>, defaults to 262144):
Vocabulary size of the per-layer text embeddings that augment the standard embeddings.
hidden_size (<code>int</code>, <em>optional</em>, defaults to 2048):
Dimension of the hidden representations.
hidden_size_per_layer_input (<code>int</code>, <em>optional</em>, defaults to 256):
Dimension of the hidden representations for per-layer emebeddings.
intermediate_size (<code>int</code> or <code>Sequence[int]</code>, <em>optional</em>, defaults to 16384):
Dimension of the MLP representations. MatFormer configurations may wish to provide a sequence of integers
to account for vairable intermediate_size values across layers. In such cases,
<code>len(intermediate_size) == num_hidden_layers</code>.
num_hidden_layers (<code>int</code>, <em>optional</em>, defaults to 35):
Number of hidden layers in the Transformer decoder.
num_attention_heads (<code>int</code>, <em>optional</em>, defaults to 8):
Number of attention heads for each attention layer in the Transformer decoder.
num_key_value_heads (<code>int</code>, <em>optional</em>, defaults to 2):
This is the number of key_value heads that should be used to implement Grouped Query Attention. If
<code>num_key_value_heads=num_attention_heads</code>, the model will use Multi Head Attention (MHA), if
<code>num_key_value_heads=1</code> the model will use Multi Query Attention (MQA) otherwise GQA is used. When
converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
by meanpooling all the original heads within that group. For more details checkout this
<a href="https://arxiv.org/pdf/2305.13245.pdf">paper</a>. If not specified, will default to <code>num_attention_heads</code>.
head_dim (<code>int</code>, <em>optional</em>, defaults to 256):
The attention head dimension.
hidden_activation (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>"gelu_pytorch_tanh"</code>):
The non-linear activation function (function or string) in the decoder. Will default to
<code>"gelu_pytorch_tanh"</code> if not specified. <code>"gelu_pytorch_tanh"</code> uses an approximation of the <code>"gelu"</code>
activation function.
max_position_embeddings (<code>int</code>, <em>optional</em>, defaults to 32768):
The maximum sequence length that this model might ever be used with.
initializer_range (<code>float</code>, <em>optional</em>, defaults to 0.02):
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
rms_norm_eps (<code>float</code>, <em>optional</em>, defaults to 1e-06):
The epsilon used by the rms normalization layers.
use_cache (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.
pad_token_id (<code>int</code>, <em>optional</em>, defaults to 0):
Padding token id.
eos_token_id (<code>int</code>, <em>optional</em>, defaults to 1):
End of stream token id.
bos_token_id (<code>int</code>, <em>optional</em>, defaults to 2):
Beginning of stream token id.
rope_theta (<code>float</code>, <em>optional</em>, defaults to 1000000.0):
The base period of the RoPE embeddings.
rope_scaling (<code>Dict</code>, <em>optional</em>):
Dictionary containing the scaling configuration for the RoPE embeddings used in gloabl attention.
NOTE: if you apply new rope type and you expect the model to work on longer <code>max_position_embeddings</code>, we
recommend you to update this value accordingly.
Expected contents:
<code>rope_type</code> (<code>str</code>):
The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',
'llama3'], with 'default' being the original RoPE implementation.
<code>factor</code> (<code>float</code>, <em>optional</em>):
Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In
most scaling types, a <code>factor</code> of x will enable the model to handle sequences of length x *
original maximum pre-trained length.
<code>original_max_position_embeddings</code> (<code>int</code>, <em>optional</em>):
Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during
pretraining.
<code>attention_factor</code> (<code>float</code>, <em>optional</em>):
Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention
computation. If unspecified, it defaults to value recommended by the implementation, using the
<code>factor</code> field to infer the suggested value.
<code>beta_fast</code> (<code>float</code>, <em>optional</em>):
Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear
ramp function. If unspecified, it defaults to 32.
<code>beta_slow</code> (<code>float</code>, <em>optional</em>):
Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear
ramp function. If unspecified, it defaults to 1.
<code>short_factor</code> (<code>List[float]</code>, <em>optional</em>):
Only used with 'longrope'. The scaling factor to be applied to short contexts (&lt;
<code>original_max_position_embeddings</code>). Must be a list of numbers with the same length as the hidden
size divided by the number of attention heads divided by 2
<code>long_factor</code> (<code>List[float]</code>, <em>optional</em>):
Only used with 'longrope'. The scaling factor to be applied to long contexts (&lt;
<code>original_max_position_embeddings</code>). Must be a list of numbers with the same length as the hidden
size divided by the number of attention heads divided by 2
<code>low_freq_factor</code> (<code>float</code>, <em>optional</em>):
Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE
<code>high_freq_factor</code> (<code>float</code>, <em>optional</em>):
Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE
rope_local_base_freq (float, <em>optional</em>, defaults to 10000.0):
The base period of the RoPE embeddings for local attention.
attention_bias (<code>bool</code>, defaults to <code>False</code>, <em>optional</em>, defaults to <code>False</code>):
Whether to use a bias in the query, key, value and output projection layers during self-attention.
attention_dropout (<code>float</code>, <em>optional</em>, defaults to 0.0):
The dropout ratio for the attention probabilities.
sliding_window (<code>int</code>, <em>optional</em>, defaults to 512):
This is the size of the sliding window used by local attention layers.
layer_types (<code>Optional</code>, <em>optional</em>):
A sequence of strings defining the attention type for that layer as either "sliding_attention" or
"full_attention". If not provided, <code>layer_types</code> will de inferred from <code>num_hidden_layers</code> using a pattern
of four "sliding_attention" layers followed one "full_attention". The last layer in the model should always
be a "full_attention" layer.
final_logit_softcapping (<code>float</code>, <em>optional</em>, defaults to 30.0):
Scaling factor when applying tanh softcapping on the logits.
altup_active_idx (<code>int</code>, <em>optional</em>, defaults to 0):
The index of the prediction from which AltUp will compute additional predictions or correct
altup_coef_clip (<code>float</code>, <em>optional</em>, defaults to 120.0):
The maximum amplitude of an AltUp prediction or correction coeficient weight.
altup_correct_scale (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
If True, apply the <code>AltUp.correct_output_scale</code> to the corrected prediction at <code>altup_active_idx</code>.
altup_num_inputs (<code>int</code>, <em>optional</em>, defaults to 4):
The number of predictions that AltUp should be make given the input sequence.
num_kv_shared_layers (<code>int</code>, <em>optional</em>, defaults to 15):
The number of layer that share KV cache values. During the forward pass, the last <code>num_kv_shared_layers</code>
layers in the model "share" the KV values in that each local and global layer in this range uses the KV
cache values computed for the last local or global layer, respectively, before entering this range. The
value should be <code>num_kv_shared_layers</code> should be a scalar of <code>sliding_window_pattern</code>.
laurel_rank (int, <em>optional</em>, defaults to 64):
The intermediate size for the linear projections in the Learned Augmented Residual Layer.
activation_sparsity_pattern (Sequence[float], <em>optional</em>, defaults to <code>(0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</code>):
The sparsity factor used to extract the top-k activations for a given layer. The provided Sequence must
explicitly provide a sparsity value for each layer in the model.</p>
<pre><code class="language-python">&gt;&gt;&gt; from transformers import Gemma3nTextModel, Gemma3nTextConfig

&gt;&gt;&gt; # Initializing a Gemma3nText gemma3n_text-E4B style configuration
&gt;&gt;&gt; configuration = Gemma3nTextConfig()

&gt;&gt;&gt; # Initializing a model from the gemma3n_text-E4B style configuration
&gt;&gt;&gt; model = Gemma3nTextModel(configuration)

&gt;&gt;&gt; # Accessing the model configuration
&gt;&gt;&gt; configuration = model.config
</code></pre></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.default_bitsandbytes_target_modules"><code class="name">var <span class="ident">default_bitsandbytes_target_modules</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.embedding_modules"><code class="name">var <span class="ident">embedding_modules</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.embedding_padding_modules"><code class="name">var <span class="ident">embedding_padding_modules</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.packed_modules_mapping"><code class="name">var <span class="ident">packed_modules_mapping</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.supported_lora_modules"><code class="name">var <span class="ident">supported_lora_modules</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.supports_lora"><code class="name">var <span class="ident">supports_lora</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.dtype"><code class="name flex">
<span>def <span class="ident">dtype</span></span>(<span>self) ‑> torch.dtype</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dtype(self) -&gt; torch.dtype:
    return next(self.parameters()).dtype</code></pre>
</details>
<div class="desc"><p><code>torch.dtype</code>: The dtype of the module (assuming that all the module parameters have the same dtype).</p></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>input_ids: torch.Tensor,<br>positions: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>input_embeds: torch.Tensor = None,<br>per_layer_inputs: torch.Tensor | None = None,<br>**kwargs) ‑> <a title="sglang.srt.layers.logits_processor.LogitsProcessor" href="../layers/logits_processor.html#sglang.srt.layers.logits_processor.LogitsProcessor">LogitsProcessor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def forward(
    self,
    input_ids: torch.Tensor,
    positions: torch.Tensor,
    forward_batch: ForwardBatch,
    input_embeds: torch.Tensor = None,
    per_layer_inputs: Optional[torch.Tensor] = None,
    **kwargs,
) -&gt; LogitsProcessor:
    hidden_states = self.model(
        input_ids,
        positions,
        forward_batch,
        input_embeds,
        per_layer_inputs,
        **kwargs,
    )

    return self.logits_processor(
        input_ids, hidden_states, self.model.embed_tokens, forward_batch
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.get_attention_sliding_window_size"><code class="name flex">
<span>def <span class="ident">get_attention_sliding_window_size</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_attention_sliding_window_size(self):
    return get_attention_sliding_window_size(self.config)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.get_input_embeddings"><code class="name flex">
<span>def <span class="ident">get_input_embeddings</span></span>(<span>self) ‑> torch.nn.modules.sparse.Embedding</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_input_embeddings(self) -&gt; nn.Embedding:
    return self.model.embed_tokens</code></pre>
</details>
<div class="desc"><p>Returns the model's input embeddings.</p>
<h2 id="returns">Returns</h2>
<p><code>nn.Module</code>: A torch module mapping vocabulary to hidden states.</p></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.load_weights"><code class="name flex">
<span>def <span class="ident">load_weights</span></span>(<span>self, weights: Iterable[Tuple[str, torch.Tensor]])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
    stacked_params_mapping = [
        # (param_name, shard_name, shard_id)
        (&#34;.qkv_proj&#34;, &#34;.q_proj&#34;, &#34;q&#34;),
        (&#34;.qkv_proj&#34;, &#34;.k_proj&#34;, &#34;k&#34;),
        (&#34;.qkv_proj&#34;, &#34;.v_proj&#34;, &#34;v&#34;),
        (&#34;.gate_up_proj&#34;, &#34;.gate_proj&#34;, 0),
        (&#34;.gate_up_proj&#34;, &#34;.up_proj&#34;, 1),
    ]
    params_dict = dict(self.named_parameters())
    loaded_params: Set[str] = set()

    for name, loaded_weight in weights:
        name = name.replace(&#34;model.language_model.&#34;, &#34;model.&#34;)
        for param_name, shard_name, shard_id in stacked_params_mapping:
            if shard_name not in name:
                continue
            name = name.replace(shard_name, param_name)
            # Skip loading extra bias for GPTQ models
            if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                continue
            if name not in params_dict:
                # Skip loading weights that are not in the model
                continue
            param = params_dict[name]
            weight_loader = param.weight_loader
            weight_loader(param, loaded_weight, shard_id)
            break
        else:
            # lm_head is not used in vllm as it is tied with embed_token
            if &#34;lm_head.weight&#34; in name:
                continue
            # Skip loading extra bias for GPTQ models
            if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                continue
            # Remapping the name of FP8 kv-scale
            name = maybe_remap_kv_scale_name(name, params_dict)
            if name is None:
                continue
            if name not in params_dict:
                # Skip loading weights that are not in the model
                continue

            param = params_dict[name]
            weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
            weight_loader(param, loaded_weight)
        loaded_params.add(name)
    return loaded_params</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM"><code class="flex name class">
<span>class <span class="ident">EntryClass</span></span>
<span>(</span><span>config: transformers.models.gemma3n.configuration_gemma3n.Gemma3nTextConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nForCausalLM(PreTrainedModel):
    config_class = Gemma3nTextConfig

    _tied_weights_keys = [&#34;lm_head.weight&#34;]
    _tp_plan = {&#34;lm_head&#34;: &#34;colwise_rep&#34;}
    _pp_plan = {&#34;lm_head&#34;: ([&#34;hidden_states&#34;], [&#34;logits&#34;])}
    config_class = Gemma3nTextConfig
    base_model_prefix = &#34;language_model&#34;

    # BitandBytes specific attributes
    default_bitsandbytes_target_modules = [
        &#34;.gate_proj.&#34;,
        &#34;.down_proj.&#34;,
        &#34;.up_proj.&#34;,
        &#34;.q_proj.&#34;,
        &#34;.k_proj.&#34;,
        &#34;.v_proj.&#34;,
        &#34;.o_proj.&#34;,
    ]
    bitsandbytes_stacked_params_mapping = {
        &#34;.q_proj&#34;: (&#34;.qkv_proj&#34;, 0),
        &#34;.k_proj&#34;: (&#34;.qkv_proj&#34;, 1),
        &#34;.v_proj&#34;: (&#34;.qkv_proj&#34;, 2),
        &#34;.gate_proj&#34;: (&#34;.gate_up_proj&#34;, 0),
        &#34;.up_proj&#34;: (&#34;.gate_up_proj&#34;, 1),
    }

    packed_modules_mapping = {
        &#34;.qkv_proj&#34;: [
            &#34;.q_proj&#34;,
            &#34;.k_proj&#34;,
            &#34;.v_proj&#34;,
        ],
        &#34;.gate_up_proj&#34;: [
            &#34;.gate_proj&#34;,
            &#34;.up_proj&#34;,
        ],
    }

    # LoRA specific attributes
    supported_lora_modules = [
        &#34;.qkv_proj&#34;,
        &#34;.o_proj&#34;,
        &#34;.gate_up_proj&#34;,
        &#34;.down_proj&#34;,
    ]
    # Gemma does not apply LoRA to the embedding layer
    embedding_modules = {}
    embedding_padding_modules = []
    supports_lora = True

    def __init__(
        self,
        config: Gemma3nTextConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; None:
        super().__init__(config=config)
        self.config = config
        self.quant_config = quant_config
        self.model = Gemma3nTextModel(
            config=config,
            quant_config=quant_config,
            prefix=add_prefix(&#34;model&#34;, prefix),
        )
        self.logits_processor = LogitsProcessor(config)

        if self.config.tie_word_embeddings:
            self.lm_head = self.model.embed_tokens
        else:
            self.lm_head = ParallelLMHead(
                config.vocab_size,
                config.hidden_size,
                quant_config=quant_config,
                prefix=add_prefix(&#34;lm_head&#34;, prefix),
            )
        self.post_init()

    def get_input_embeddings(self) -&gt; nn.Embedding:
        return self.model.embed_tokens

    def get_attention_sliding_window_size(self):
        return get_attention_sliding_window_size(self.config)

    def dtype(self) -&gt; torch.dtype:
        return next(self.parameters()).dtype

    @torch.no_grad()
    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        forward_batch: ForwardBatch,
        input_embeds: torch.Tensor = None,
        per_layer_inputs: Optional[torch.Tensor] = None,
        **kwargs,
    ) -&gt; LogitsProcessor:
        hidden_states = self.model(
            input_ids,
            positions,
            forward_batch,
            input_embeds,
            per_layer_inputs,
            **kwargs,
        )

        return self.logits_processor(
            input_ids, hidden_states, self.model.embed_tokens, forward_batch
        )

    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
        stacked_params_mapping = [
            # (param_name, shard_name, shard_id)
            (&#34;.qkv_proj&#34;, &#34;.q_proj&#34;, &#34;q&#34;),
            (&#34;.qkv_proj&#34;, &#34;.k_proj&#34;, &#34;k&#34;),
            (&#34;.qkv_proj&#34;, &#34;.v_proj&#34;, &#34;v&#34;),
            (&#34;.gate_up_proj&#34;, &#34;.gate_proj&#34;, 0),
            (&#34;.gate_up_proj&#34;, &#34;.up_proj&#34;, 1),
        ]
        params_dict = dict(self.named_parameters())
        loaded_params: Set[str] = set()

        for name, loaded_weight in weights:
            name = name.replace(&#34;model.language_model.&#34;, &#34;model.&#34;)
            for param_name, shard_name, shard_id in stacked_params_mapping:
                if shard_name not in name:
                    continue
                name = name.replace(shard_name, param_name)
                # Skip loading extra bias for GPTQ models
                if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                    continue
                if name not in params_dict:
                    # Skip loading weights that are not in the model
                    continue
                param = params_dict[name]
                weight_loader = param.weight_loader
                weight_loader(param, loaded_weight, shard_id)
                break
            else:
                # lm_head is not used in vllm as it is tied with embed_token
                if &#34;lm_head.weight&#34; in name:
                    continue
                # Skip loading extra bias for GPTQ models
                if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                    continue
                # Remapping the name of FP8 kv-scale
                name = maybe_remap_kv_scale_name(name, params_dict)
                if name is None:
                    continue
                if name not in params_dict:
                    # Skip loading weights that are not in the model
                    continue

                param = params_dict[name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(param, loaded_weight)
            loaded_params.add(name)
        return loaded_params</code></pre>
</details>
<div class="desc"><p>Base class for all models.</p>
<p>[<code>PreTrainedModel</code>] takes care of storing the configuration of the models and handles methods for loading,
downloading and saving models as well as a few methods common to all models to:</p>
<pre><code>- resize the input embeddings,
- prune heads in the self-attention heads.
</code></pre>
<p>Class attributes (overridden by derived classes):</p>
<pre><code>- **config_class** ([`PretrainedConfig`]) -- A subclass of [`PretrainedConfig`] to use as configuration class
  for this model architecture.
- **load_tf_weights** (&lt;code&gt;Callable&lt;/code&gt;) -- A python *method* for loading a TensorFlow checkpoint in a PyTorch model,
  taking as arguments:

    - **model** ([`PreTrainedModel`]) -- An instance of the model on which to load the TensorFlow checkpoint.
    - **config** ([`PreTrainedConfig`]) -- An instance of the configuration associated to the model.
    - **path** (&lt;code&gt;str&lt;/code&gt;) -- A path to the TensorFlow checkpoint.

- **base_model_prefix** (&lt;code&gt;str&lt;/code&gt;) -- A string indicating the attribute associated to the base model in derived
  classes of the same architecture adding modules on top of the base model.
- **is_parallelizable** (&lt;code&gt;bool&lt;/code&gt;) -- A flag indicating whether this model supports model parallelization.
- **main_input_name** (&lt;code&gt;str&lt;/code&gt;) -- The name of the principal input to the model (often &lt;code&gt;input\_ids&lt;/code&gt; for NLP
  models, &lt;code&gt;pixel\_values&lt;/code&gt; for vision models and &lt;code&gt;input\_values&lt;/code&gt; for speech models).
- **can_record_outputs** (dict):
</code></pre>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.modeling_utils.PreTrainedModel</li>
<li>torch.nn.modules.module.Module</li>
<li>transformers.modeling_utils.EmbeddingAccessMixin</li>
<li>transformers.modeling_utils.ModuleUtilsMixin</li>
<li>transformers.utils.hub.PushToHubMixin</li>
<li>transformers.integrations.peft.PeftAdapterMixin</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.base_model_prefix"><code class="name">var <span class="ident">base_model_prefix</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.bitsandbytes_stacked_params_mapping"><code class="name">var <span class="ident">bitsandbytes_stacked_params_mapping</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.config_class"><code class="name">var <span class="ident">config_class</span></code></dt>
<dd>
<div class="desc"><p>This is the configuration class to store the configuration of a [<code>Gemma3nTextModel</code>]. It is used to instantiate an
Gemma3nTextModel model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the Gemma 3n E4B, e.g.
<a href="https://huggingface.co/google/gemma-3n-E4B">google/gemma-3n-E4B</a>.</p>
<p>Configuration objects that inherit from [<code>Gemma3nTextConfig</code>] and can be used to control the model outputs. Read
the documentation from [<code>Gemma3nTextConfig</code>] for more information.</p>
<h2 id="args">Args</h2>
<p>vocab_size (<code>int</code>, <em>optional</em>, defaults to 262400):
Vocabulary size of the Gemma3nText model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling [<code>Gemma3nTextModel</code>]
vocab_size_per_layer_input (<code>int</code>, <em>optional</em>, defaults to 262144):
Vocabulary size of the per-layer text embeddings that augment the standard embeddings.
hidden_size (<code>int</code>, <em>optional</em>, defaults to 2048):
Dimension of the hidden representations.
hidden_size_per_layer_input (<code>int</code>, <em>optional</em>, defaults to 256):
Dimension of the hidden representations for per-layer emebeddings.
intermediate_size (<code>int</code> or <code>Sequence[int]</code>, <em>optional</em>, defaults to 16384):
Dimension of the MLP representations. MatFormer configurations may wish to provide a sequence of integers
to account for vairable intermediate_size values across layers. In such cases,
<code>len(intermediate_size) == num_hidden_layers</code>.
num_hidden_layers (<code>int</code>, <em>optional</em>, defaults to 35):
Number of hidden layers in the Transformer decoder.
num_attention_heads (<code>int</code>, <em>optional</em>, defaults to 8):
Number of attention heads for each attention layer in the Transformer decoder.
num_key_value_heads (<code>int</code>, <em>optional</em>, defaults to 2):
This is the number of key_value heads that should be used to implement Grouped Query Attention. If
<code>num_key_value_heads=num_attention_heads</code>, the model will use Multi Head Attention (MHA), if
<code>num_key_value_heads=1</code> the model will use Multi Query Attention (MQA) otherwise GQA is used. When
converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
by meanpooling all the original heads within that group. For more details checkout this
<a href="https://arxiv.org/pdf/2305.13245.pdf">paper</a>. If not specified, will default to <code>num_attention_heads</code>.
head_dim (<code>int</code>, <em>optional</em>, defaults to 256):
The attention head dimension.
hidden_activation (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>"gelu_pytorch_tanh"</code>):
The non-linear activation function (function or string) in the decoder. Will default to
<code>"gelu_pytorch_tanh"</code> if not specified. <code>"gelu_pytorch_tanh"</code> uses an approximation of the <code>"gelu"</code>
activation function.
max_position_embeddings (<code>int</code>, <em>optional</em>, defaults to 32768):
The maximum sequence length that this model might ever be used with.
initializer_range (<code>float</code>, <em>optional</em>, defaults to 0.02):
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
rms_norm_eps (<code>float</code>, <em>optional</em>, defaults to 1e-06):
The epsilon used by the rms normalization layers.
use_cache (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.
pad_token_id (<code>int</code>, <em>optional</em>, defaults to 0):
Padding token id.
eos_token_id (<code>int</code>, <em>optional</em>, defaults to 1):
End of stream token id.
bos_token_id (<code>int</code>, <em>optional</em>, defaults to 2):
Beginning of stream token id.
rope_theta (<code>float</code>, <em>optional</em>, defaults to 1000000.0):
The base period of the RoPE embeddings.
rope_scaling (<code>Dict</code>, <em>optional</em>):
Dictionary containing the scaling configuration for the RoPE embeddings used in gloabl attention.
NOTE: if you apply new rope type and you expect the model to work on longer <code>max_position_embeddings</code>, we
recommend you to update this value accordingly.
Expected contents:
<code>rope_type</code> (<code>str</code>):
The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',
'llama3'], with 'default' being the original RoPE implementation.
<code>factor</code> (<code>float</code>, <em>optional</em>):
Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In
most scaling types, a <code>factor</code> of x will enable the model to handle sequences of length x *
original maximum pre-trained length.
<code>original_max_position_embeddings</code> (<code>int</code>, <em>optional</em>):
Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during
pretraining.
<code>attention_factor</code> (<code>float</code>, <em>optional</em>):
Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention
computation. If unspecified, it defaults to value recommended by the implementation, using the
<code>factor</code> field to infer the suggested value.
<code>beta_fast</code> (<code>float</code>, <em>optional</em>):
Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear
ramp function. If unspecified, it defaults to 32.
<code>beta_slow</code> (<code>float</code>, <em>optional</em>):
Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear
ramp function. If unspecified, it defaults to 1.
<code>short_factor</code> (<code>List[float]</code>, <em>optional</em>):
Only used with 'longrope'. The scaling factor to be applied to short contexts (&lt;
<code>original_max_position_embeddings</code>). Must be a list of numbers with the same length as the hidden
size divided by the number of attention heads divided by 2
<code>long_factor</code> (<code>List[float]</code>, <em>optional</em>):
Only used with 'longrope'. The scaling factor to be applied to long contexts (&lt;
<code>original_max_position_embeddings</code>). Must be a list of numbers with the same length as the hidden
size divided by the number of attention heads divided by 2
<code>low_freq_factor</code> (<code>float</code>, <em>optional</em>):
Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE
<code>high_freq_factor</code> (<code>float</code>, <em>optional</em>):
Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE
rope_local_base_freq (float, <em>optional</em>, defaults to 10000.0):
The base period of the RoPE embeddings for local attention.
attention_bias (<code>bool</code>, defaults to <code>False</code>, <em>optional</em>, defaults to <code>False</code>):
Whether to use a bias in the query, key, value and output projection layers during self-attention.
attention_dropout (<code>float</code>, <em>optional</em>, defaults to 0.0):
The dropout ratio for the attention probabilities.
sliding_window (<code>int</code>, <em>optional</em>, defaults to 512):
This is the size of the sliding window used by local attention layers.
layer_types (<code>Optional</code>, <em>optional</em>):
A sequence of strings defining the attention type for that layer as either "sliding_attention" or
"full_attention". If not provided, <code>layer_types</code> will de inferred from <code>num_hidden_layers</code> using a pattern
of four "sliding_attention" layers followed one "full_attention". The last layer in the model should always
be a "full_attention" layer.
final_logit_softcapping (<code>float</code>, <em>optional</em>, defaults to 30.0):
Scaling factor when applying tanh softcapping on the logits.
altup_active_idx (<code>int</code>, <em>optional</em>, defaults to 0):
The index of the prediction from which AltUp will compute additional predictions or correct
altup_coef_clip (<code>float</code>, <em>optional</em>, defaults to 120.0):
The maximum amplitude of an AltUp prediction or correction coeficient weight.
altup_correct_scale (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
If True, apply the <code>AltUp.correct_output_scale</code> to the corrected prediction at <code>altup_active_idx</code>.
altup_num_inputs (<code>int</code>, <em>optional</em>, defaults to 4):
The number of predictions that AltUp should be make given the input sequence.
num_kv_shared_layers (<code>int</code>, <em>optional</em>, defaults to 15):
The number of layer that share KV cache values. During the forward pass, the last <code>num_kv_shared_layers</code>
layers in the model "share" the KV values in that each local and global layer in this range uses the KV
cache values computed for the last local or global layer, respectively, before entering this range. The
value should be <code>num_kv_shared_layers</code> should be a scalar of <code>sliding_window_pattern</code>.
laurel_rank (int, <em>optional</em>, defaults to 64):
The intermediate size for the linear projections in the Learned Augmented Residual Layer.
activation_sparsity_pattern (Sequence[float], <em>optional</em>, defaults to <code>(0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)</code>):
The sparsity factor used to extract the top-k activations for a given layer. The provided Sequence must
explicitly provide a sparsity value for each layer in the model.</p>
<pre><code class="language-python">&gt;&gt;&gt; from transformers import Gemma3nTextModel, Gemma3nTextConfig

&gt;&gt;&gt; # Initializing a Gemma3nText gemma3n_text-E4B style configuration
&gt;&gt;&gt; configuration = Gemma3nTextConfig()

&gt;&gt;&gt; # Initializing a model from the gemma3n_text-E4B style configuration
&gt;&gt;&gt; model = Gemma3nTextModel(configuration)

&gt;&gt;&gt; # Accessing the model configuration
&gt;&gt;&gt; configuration = model.config
</code></pre></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.default_bitsandbytes_target_modules"><code class="name">var <span class="ident">default_bitsandbytes_target_modules</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.embedding_modules"><code class="name">var <span class="ident">embedding_modules</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.embedding_padding_modules"><code class="name">var <span class="ident">embedding_padding_modules</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.packed_modules_mapping"><code class="name">var <span class="ident">packed_modules_mapping</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.supported_lora_modules"><code class="name">var <span class="ident">supported_lora_modules</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.supports_lora"><code class="name">var <span class="ident">supports_lora</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.dtype"><code class="name flex">
<span>def <span class="ident">dtype</span></span>(<span>self) ‑> torch.dtype</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dtype(self) -&gt; torch.dtype:
    return next(self.parameters()).dtype</code></pre>
</details>
<div class="desc"><p><code>torch.dtype</code>: The dtype of the module (assuming that all the module parameters have the same dtype).</p></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>input_ids: torch.Tensor,<br>positions: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>input_embeds: torch.Tensor = None,<br>per_layer_inputs: torch.Tensor | None = None,<br>**kwargs) ‑> <a title="sglang.srt.layers.logits_processor.LogitsProcessor" href="../layers/logits_processor.html#sglang.srt.layers.logits_processor.LogitsProcessor">LogitsProcessor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def forward(
    self,
    input_ids: torch.Tensor,
    positions: torch.Tensor,
    forward_batch: ForwardBatch,
    input_embeds: torch.Tensor = None,
    per_layer_inputs: Optional[torch.Tensor] = None,
    **kwargs,
) -&gt; LogitsProcessor:
    hidden_states = self.model(
        input_ids,
        positions,
        forward_batch,
        input_embeds,
        per_layer_inputs,
        **kwargs,
    )

    return self.logits_processor(
        input_ids, hidden_states, self.model.embed_tokens, forward_batch
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.get_attention_sliding_window_size"><code class="name flex">
<span>def <span class="ident">get_attention_sliding_window_size</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_attention_sliding_window_size(self):
    return get_attention_sliding_window_size(self.config)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.get_input_embeddings"><code class="name flex">
<span>def <span class="ident">get_input_embeddings</span></span>(<span>self) ‑> torch.nn.modules.sparse.Embedding</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_input_embeddings(self) -&gt; nn.Embedding:
    return self.model.embed_tokens</code></pre>
</details>
<div class="desc"><p>Returns the model's input embeddings.</p>
<h2 id="returns">Returns</h2>
<p><code>nn.Module</code>: A torch module mapping vocabulary to hidden states.</p></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.load_weights"><code class="name flex">
<span>def <span class="ident">load_weights</span></span>(<span>self, weights: Iterable[Tuple[str, torch.Tensor]])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
    stacked_params_mapping = [
        # (param_name, shard_name, shard_id)
        (&#34;.qkv_proj&#34;, &#34;.q_proj&#34;, &#34;q&#34;),
        (&#34;.qkv_proj&#34;, &#34;.k_proj&#34;, &#34;k&#34;),
        (&#34;.qkv_proj&#34;, &#34;.v_proj&#34;, &#34;v&#34;),
        (&#34;.gate_up_proj&#34;, &#34;.gate_proj&#34;, 0),
        (&#34;.gate_up_proj&#34;, &#34;.up_proj&#34;, 1),
    ]
    params_dict = dict(self.named_parameters())
    loaded_params: Set[str] = set()

    for name, loaded_weight in weights:
        name = name.replace(&#34;model.language_model.&#34;, &#34;model.&#34;)
        for param_name, shard_name, shard_id in stacked_params_mapping:
            if shard_name not in name:
                continue
            name = name.replace(shard_name, param_name)
            # Skip loading extra bias for GPTQ models
            if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                continue
            if name not in params_dict:
                # Skip loading weights that are not in the model
                continue
            param = params_dict[name]
            weight_loader = param.weight_loader
            weight_loader(param, loaded_weight, shard_id)
            break
        else:
            # lm_head is not used in vllm as it is tied with embed_token
            if &#34;lm_head.weight&#34; in name:
                continue
            # Skip loading extra bias for GPTQ models
            if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                continue
            # Remapping the name of FP8 kv-scale
            name = maybe_remap_kv_scale_name(name, params_dict)
            if name is None:
                continue
            if name not in params_dict:
                # Skip loading weights that are not in the model
                continue

            param = params_dict[name]
            weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
            weight_loader(param, loaded_weight)
        loaded_params.add(name)
    return loaded_params</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nLaurelBlock"><code class="flex name class">
<span>class <span class="ident">Gemma3nLaurelBlock</span></span>
<span>(</span><span>config: transformers.models.gemma3n.configuration_gemma3n.Gemma3nTextConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nLaurelBlock(nn.Module):
    &#34;&#34;&#34;Learned Augmented Residual Layer&#34;&#34;&#34;

    def __init__(
        self,
        config: Gemma3nTextConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ):
        super().__init__()
        self.config = config

        self.linear_left = ColumnParallelLinear(
            config.hidden_size,
            config.laurel_rank,
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;linear_left&#34;, prefix),
        )
        self.linear_right = RowParallelLinear(
            config.laurel_rank,
            config.hidden_size,
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;linear_right&#34;, prefix),
        )
        self.post_laurel_norm = Gemma3nRMSNorm(
            dim=config.hidden_size,
            eps=config.rms_norm_eps,
        )

    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        # [num_tokens, hidden_size]
        laurel_x, _ = self.linear_left(x)
        laurel_x, _ = self.linear_right(laurel_x)
        normed_laurel_x = self.post_laurel_norm(laurel_x)
        return x + normed_laurel_x</code></pre>
</details>
<div class="desc"><p>Learned Augmented Residual Layer</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nLaurelBlock.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
    # [num_tokens, hidden_size]
    laurel_x, _ = self.linear_left(x)
    laurel_x, _ = self.linear_right(laurel_x)
    normed_laurel_x = self.post_laurel_norm(laurel_x)
    return x + normed_laurel_x</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nRMSNorm"><code class="flex name class">
<span>class <span class="ident">Gemma3nRMSNorm</span></span>
<span>(</span><span>dim: int, eps: float = 1e-06, with_scale: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nRMSNorm(RMSNorm):
    def __init__(
        self,
        dim: int,
        eps: float = 1e-6,
        with_scale: bool = True,
    ) -&gt; None:
        super().__init__(dim, eps=eps)
        if not with_scale:
            del self.weight
            self.register_buffer(
                &#34;weight&#34;,
                torch.ones(dim, dtype=torch.get_default_dtype()),
                persistent=False,
            )

    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        original_shape = x.shape
        x_2d = x.contiguous().reshape(-1, original_shape[-1])
        x_2d = super().forward(x_2d)
        x = x_2d.reshape(original_shape)
        return x</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.layernorm.RMSNorm" href="../layers/layernorm.html#sglang.srt.layers.layernorm.RMSNorm">RMSNorm</a></li>
<li><a title="sglang.srt.custom_op.CustomOp" href="../custom_op.html#sglang.srt.custom_op.CustomOp">CustomOp</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.layers.layernorm.RMSNorm" href="../layers/layernorm.html#sglang.srt.layers.layernorm.RMSNorm">RMSNorm</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.layers.layernorm.RMSNorm.forward" href="../custom_op.html#sglang.srt.custom_op.CustomOp.forward">forward</a></code></li>
<li><code><a title="sglang.srt.layers.layernorm.RMSNorm.forward_with_allreduce_fusion" href="../layers/layernorm.html#sglang.srt.layers.layernorm.RMSNorm.forward_with_allreduce_fusion">forward_with_allreduce_fusion</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nTextMLP"><code class="flex name class">
<span>class <span class="ident">Gemma3nTextMLP</span></span>
<span>(</span><span>hidden_size: int,<br>intermediate_size: int,<br>hidden_activation: str,<br>activation_sparsity: float = 0.0,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nTextMLP(nn.Module):
    def __init__(
        self,
        hidden_size: int,
        intermediate_size: int,
        hidden_activation: str,
        activation_sparsity: float = 0.0,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; None:
        super().__init__()
        self.gate_up_proj = MergedColumnParallelLinear(
            hidden_size,
            [intermediate_size] * 2,
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;gate_up_proj&#34;, prefix),
        )
        self.down_proj = RowParallelLinear(
            intermediate_size,
            hidden_size,
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;down_proj&#34;, prefix),
        )
        if hidden_activation != &#34;gelu_pytorch_tanh&#34;:
            raise ValueError(
                &#34;Gemma3n uses `gelu_pytorch_tanh` as the hidden activation &#34;
                &#34;function. Please set `hidden_activation` to &#34;
                &#34;`gelu_pytorch_tanh`.&#34;
            )
        # Use proper GELU with tanh approximation as specified
        self.act_fn = GeluAndMul()
        self.activation_sparsity = activation_sparsity
        self.register_buffer(
            &#34;target_sparsity_tensor&#34;,
            torch.tensor(self.activation_sparsity, dtype=torch.float32),
            persistent=False,
        )  # moved from _gaussian_topk for cuda graph

    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        gate_up, _ = self.gate_up_proj(x)

        # Split gate and up projections
        gate_proj, up_proj = gate_up.chunk(2, dim=-1)

        # Apply activation sparsity if needed
        if self.activation_sparsity &gt; 0.0:
            gate_proj = self._gaussian_topk(gate_proj)

        gate_up = torch.cat([gate_proj, up_proj], dim=-1)

        # Apply GELU activation to gate projection and multiply with up projection
        x = self.act_fn(gate_up)
        x, _ = self.down_proj(x)
        return x

    def _gaussian_topk(self, inputs: torch.Tensor) -&gt; torch.Tensor:
        normal_dist = torch.distributions.normal.Normal(0, 1)
        std_multiplier = normal_dist.icdf(self.target_sparsity_tensor)
        std_multiplier = std_multiplier.type(inputs.dtype)
        inputs_mean = torch.mean(inputs, dim=-1, keepdim=True)
        inputs_std = torch.std(inputs, dim=-1, keepdim=True, unbiased=False)
        cutoff_x = inputs_mean + inputs_std * std_multiplier
        return F.relu(inputs - cutoff_x)</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nTextMLP.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
    gate_up, _ = self.gate_up_proj(x)

    # Split gate and up projections
    gate_proj, up_proj = gate_up.chunk(2, dim=-1)

    # Apply activation sparsity if needed
    if self.activation_sparsity &gt; 0.0:
        gate_proj = self._gaussian_topk(gate_proj)

    gate_up = torch.cat([gate_proj, up_proj], dim=-1)

    # Apply GELU activation to gate projection and multiply with up projection
    x = self.act_fn(gate_up)
    x, _ = self.down_proj(x)
    return x</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nTextModel"><code class="flex name class">
<span>class <span class="ident">Gemma3nTextModel</span></span>
<span>(</span><span>config: transformers.models.gemma3n.configuration_gemma3n.Gemma3nTextConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nTextModel(PreTrainedModel):
    def __init__(
        self,
        config: Gemma3nTextConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; None:
        super().__init__(config=config)
        self.config = config
        self.quant_config = quant_config
        self.vocab_size = config.vocab_size
        self.padding_idx = config.pad_token_id

        # Gemma3n downcasts the below to float16, causing sqrt(3072)=55.4256 to become 55.5
        self.embed_tokens = Gemma3nTextScaledWordEmbedding(
            config.vocab_size,
            config.hidden_size,
            self.padding_idx,
            embed_scale=self.config.hidden_size**0.5,
        )

        self.norm = Gemma3nRMSNorm(
            config.hidden_size,
            eps=config.rms_norm_eps,
        )

        self.layers = make_layers(
            config.num_hidden_layers,
            lambda idx, prefix: Gemma3nDecoderLayer(
                layer_id=idx,
                config=config,
                quant_config=quant_config,
                prefix=prefix,
            ),
            prefix=add_prefix(&#34;layers&#34;, prefix),
        )

        # Per-layer input embeddings
        self.hidden_size = config.hidden_size
        self.hidden_size_per_layer_input = config.hidden_size_per_layer_input

        self.embed_tokens_per_layer = Gemma3nTextScaledWordEmbedding(
            config.vocab_size_per_layer_input,
            config.num_hidden_layers * config.hidden_size_per_layer_input,
            self.padding_idx,
            embed_scale=self.config.hidden_size_per_layer_input**0.5,
        )

        self.per_layer_model_projection = ColumnParallelLinear(
            self.hidden_size,
            config.num_hidden_layers * config.hidden_size_per_layer_input,
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;per_layer_model_projection&#34;, prefix),
        )

        self.per_layer_projection_norm = Gemma3nRMSNorm(
            dim=config.hidden_size_per_layer_input,
            eps=config.rms_norm_eps,
        )

        self.altup_projections = make_layers(
            self.config.altup_num_inputs - 1,
            lambda idx, prefix: ColumnParallelLinear(
                self.hidden_size,
                self.hidden_size,
                bias=False,
                quant_config=quant_config,
                prefix=prefix,
            ),
            prefix=add_prefix(&#34;altup_projections&#34;, prefix),
        )

        self.altup_unembed_projections = make_layers(
            self.config.altup_num_inputs - 1,
            lambda idx, prefix: ColumnParallelLinear(
                self.hidden_size,
                self.hidden_size,
                bias=False,
                quant_config=quant_config,
                prefix=prefix,
            ),
            prefix=add_prefix(&#34;altup_unembed_projections&#34;, prefix),
        )

        self.register_buffer(
            &#34;per_layer_projection_scale&#34;,
            torch.tensor(self.hidden_size**-0.5),
            persistent=False,
        )
        self.register_buffer(
            &#34;per_layer_input_scale&#34;, torch.rsqrt(torch.tensor(2.0)), persistent=False
        )

        self.post_init()

    def get_input_embeddings(self) -&gt; nn.Embedding:
        return self.embed_tokens

    def dtype(self) -&gt; torch.dtype:
        return next(self.parameters()).dtype

    def get_per_layer_inputs(self, input_ids: torch.LongTensor) -&gt; torch.Tensor:
        embeddings = self.embed_tokens_per_layer(input_ids)
        return embeddings.reshape(
            *input_ids.shape,
            self.config.num_hidden_layers,
            self.hidden_size_per_layer_input,
        )

    def project_per_layer_inputs(
        self,
        inputs_embeds: torch.Tensor,
        per_layer_inputs: Optional[torch.Tensor] = None,
    ) -&gt; torch.Tensor:
        per_layer_projection, _ = self.per_layer_model_projection(inputs_embeds)
        per_layer_projection *= self.per_layer_projection_scale.type(
            inputs_embeds.dtype
        )
        per_layer_projection = per_layer_projection.reshape(
            *inputs_embeds.shape[:-1],
            self.config.num_hidden_layers,
            self.hidden_size_per_layer_input,
        )
        per_layer_projection = self.per_layer_projection_norm(per_layer_projection)

        if per_layer_inputs is None:
            return per_layer_projection

        if per_layer_projection.shape != per_layer_inputs.shape:
            # per-layer inputs are sometimes padded with zeros, slice the relevant embeddings
            per_layer_inputs = per_layer_inputs[..., : self.config.num_hidden_layers, :]

        return (
            per_layer_projection + per_layer_inputs
        ) * self.per_layer_input_scale.type(inputs_embeds.dtype)

    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        forward_batch: ForwardBatch,
        input_embeds: torch.Tensor = None,
        per_layer_inputs: Optional[torch.Tensor] = None,
        **kwargs,
    ) -&gt; torch.Tensor:
        if (input_ids is None) ^ (input_embeds is not None):
            raise ValueError(
                &#34;You must specify exactly one of input_ids or inputs_embeds&#34;
            )

        if input_ids is not None:
            input_embeds = self.embed_tokens(input_ids)
            per_layer_inputs = self.get_per_layer_inputs(input_ids)

        per_layer_inputs = self.project_per_layer_inputs(input_embeds, per_layer_inputs)

        if positions.dim() == 1:
            positions = positions.unsqueeze(0)

        # Expand hidden_states to support per-layer inputs
        target_magnitude = torch.mean(input_embeds**2, dim=-1, keepdim=True) ** 0.5
        epsilon_tensor = torch.tensor(torch.finfo(input_embeds.dtype).min)

        # embed positions
        hidden_states_0 = input_embeds
        temp_hidden_states = [hidden_states_0]

        for i in range(1, self.config.altup_num_inputs):
            altup_proj, _ = self.altup_projections[i - 1](hidden_states_0)
            current_hidden_state = altup_proj.type(hidden_states_0.dtype)
            new_magnitude = (
                torch.mean(current_hidden_state**2, dim=-1, keepdim=True) ** 0.5
            )
            current_hidden_state = current_hidden_state * (
                target_magnitude / torch.maximum(new_magnitude, epsilon_tensor)
            )
            temp_hidden_states.append(current_hidden_state)

        hidden_states = torch.stack(
            temp_hidden_states, dim=0
        )  # [num_altup_inputs, n_tokens, hidden_size]

        for layer_idx, layer in enumerate(self.layers):
            per_layer_input = per_layer_inputs[:, layer_idx, :]
            hidden_states = layer(
                positions=positions,
                per_layer_input=per_layer_input,
                hidden_states=hidden_states,
                forward_batch=forward_batch,
                **kwargs,
            )

        # Per-layer inputs to single output
        target_magnitude = (
            torch.mean(hidden_states[0] ** 2, dim=-1, keepdim=True) ** 0.5
        )

        temp_hidden_states = [hidden_states[0]]

        for i in range(1, self.config.altup_num_inputs):
            # altup_unembed_projections adapted from jax.numpy.einsum(&#34;btp,pd-&gt;btd&#34;, ...)
            altup_unemb_proj, _ = self.altup_unembed_projections[i - 1](
                hidden_states[i]
            )
            current_hidden_state = altup_unemb_proj.type(hidden_states_0.dtype)
            new_magnitude = (
                torch.mean(current_hidden_state**2, dim=-1, keepdim=True) ** 0.5
            )
            current_hidden_state = current_hidden_state * (
                target_magnitude / torch.maximum(new_magnitude, epsilon_tensor)
            )
            temp_hidden_states.append(current_hidden_state)

        hidden_states = torch.stack(temp_hidden_states)
        hidden_states = torch.mean(hidden_states, dim=0)
        hidden_states = self.norm(hidden_states)

        return hidden_states</code></pre>
</details>
<div class="desc"><p>Base class for all models.</p>
<p>[<code>PreTrainedModel</code>] takes care of storing the configuration of the models and handles methods for loading,
downloading and saving models as well as a few methods common to all models to:</p>
<pre><code>- resize the input embeddings,
- prune heads in the self-attention heads.
</code></pre>
<p>Class attributes (overridden by derived classes):</p>
<pre><code>- **config_class** ([`PretrainedConfig`]) -- A subclass of [`PretrainedConfig`] to use as configuration class
  for this model architecture.
- **load_tf_weights** (&lt;code&gt;Callable&lt;/code&gt;) -- A python *method* for loading a TensorFlow checkpoint in a PyTorch model,
  taking as arguments:

    - **model** ([`PreTrainedModel`]) -- An instance of the model on which to load the TensorFlow checkpoint.
    - **config** ([`PreTrainedConfig`]) -- An instance of the configuration associated to the model.
    - **path** (&lt;code&gt;str&lt;/code&gt;) -- A path to the TensorFlow checkpoint.

- **base_model_prefix** (&lt;code&gt;str&lt;/code&gt;) -- A string indicating the attribute associated to the base model in derived
  classes of the same architecture adding modules on top of the base model.
- **is_parallelizable** (&lt;code&gt;bool&lt;/code&gt;) -- A flag indicating whether this model supports model parallelization.
- **main_input_name** (&lt;code&gt;str&lt;/code&gt;) -- The name of the principal input to the model (often &lt;code&gt;input\_ids&lt;/code&gt; for NLP
  models, &lt;code&gt;pixel\_values&lt;/code&gt; for vision models and &lt;code&gt;input\_values&lt;/code&gt; for speech models).
- **can_record_outputs** (dict):
</code></pre>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.modeling_utils.PreTrainedModel</li>
<li>torch.nn.modules.module.Module</li>
<li>transformers.modeling_utils.EmbeddingAccessMixin</li>
<li>transformers.modeling_utils.ModuleUtilsMixin</li>
<li>transformers.utils.hub.PushToHubMixin</li>
<li>transformers.integrations.peft.PeftAdapterMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nTextModel.dtype"><code class="name flex">
<span>def <span class="ident">dtype</span></span>(<span>self) ‑> torch.dtype</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dtype(self) -&gt; torch.dtype:
    return next(self.parameters()).dtype</code></pre>
</details>
<div class="desc"><p><code>torch.dtype</code>: The dtype of the module (assuming that all the module parameters have the same dtype).</p></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nTextModel.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>input_ids: torch.Tensor,<br>positions: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>input_embeds: torch.Tensor = None,<br>per_layer_inputs: torch.Tensor | None = None,<br>**kwargs) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    input_ids: torch.Tensor,
    positions: torch.Tensor,
    forward_batch: ForwardBatch,
    input_embeds: torch.Tensor = None,
    per_layer_inputs: Optional[torch.Tensor] = None,
    **kwargs,
) -&gt; torch.Tensor:
    if (input_ids is None) ^ (input_embeds is not None):
        raise ValueError(
            &#34;You must specify exactly one of input_ids or inputs_embeds&#34;
        )

    if input_ids is not None:
        input_embeds = self.embed_tokens(input_ids)
        per_layer_inputs = self.get_per_layer_inputs(input_ids)

    per_layer_inputs = self.project_per_layer_inputs(input_embeds, per_layer_inputs)

    if positions.dim() == 1:
        positions = positions.unsqueeze(0)

    # Expand hidden_states to support per-layer inputs
    target_magnitude = torch.mean(input_embeds**2, dim=-1, keepdim=True) ** 0.5
    epsilon_tensor = torch.tensor(torch.finfo(input_embeds.dtype).min)

    # embed positions
    hidden_states_0 = input_embeds
    temp_hidden_states = [hidden_states_0]

    for i in range(1, self.config.altup_num_inputs):
        altup_proj, _ = self.altup_projections[i - 1](hidden_states_0)
        current_hidden_state = altup_proj.type(hidden_states_0.dtype)
        new_magnitude = (
            torch.mean(current_hidden_state**2, dim=-1, keepdim=True) ** 0.5
        )
        current_hidden_state = current_hidden_state * (
            target_magnitude / torch.maximum(new_magnitude, epsilon_tensor)
        )
        temp_hidden_states.append(current_hidden_state)

    hidden_states = torch.stack(
        temp_hidden_states, dim=0
    )  # [num_altup_inputs, n_tokens, hidden_size]

    for layer_idx, layer in enumerate(self.layers):
        per_layer_input = per_layer_inputs[:, layer_idx, :]
        hidden_states = layer(
            positions=positions,
            per_layer_input=per_layer_input,
            hidden_states=hidden_states,
            forward_batch=forward_batch,
            **kwargs,
        )

    # Per-layer inputs to single output
    target_magnitude = (
        torch.mean(hidden_states[0] ** 2, dim=-1, keepdim=True) ** 0.5
    )

    temp_hidden_states = [hidden_states[0]]

    for i in range(1, self.config.altup_num_inputs):
        # altup_unembed_projections adapted from jax.numpy.einsum(&#34;btp,pd-&gt;btd&#34;, ...)
        altup_unemb_proj, _ = self.altup_unembed_projections[i - 1](
            hidden_states[i]
        )
        current_hidden_state = altup_unemb_proj.type(hidden_states_0.dtype)
        new_magnitude = (
            torch.mean(current_hidden_state**2, dim=-1, keepdim=True) ** 0.5
        )
        current_hidden_state = current_hidden_state * (
            target_magnitude / torch.maximum(new_magnitude, epsilon_tensor)
        )
        temp_hidden_states.append(current_hidden_state)

    hidden_states = torch.stack(temp_hidden_states)
    hidden_states = torch.mean(hidden_states, dim=0)
    hidden_states = self.norm(hidden_states)

    return hidden_states</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nTextModel.get_input_embeddings"><code class="name flex">
<span>def <span class="ident">get_input_embeddings</span></span>(<span>self) ‑> torch.nn.modules.sparse.Embedding</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_input_embeddings(self) -&gt; nn.Embedding:
    return self.embed_tokens</code></pre>
</details>
<div class="desc"><p>Returns the model's input embeddings.</p>
<h2 id="returns">Returns</h2>
<p><code>nn.Module</code>: A torch module mapping vocabulary to hidden states.</p></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nTextModel.get_per_layer_inputs"><code class="name flex">
<span>def <span class="ident">get_per_layer_inputs</span></span>(<span>self, input_ids: torch.LongTensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_per_layer_inputs(self, input_ids: torch.LongTensor) -&gt; torch.Tensor:
    embeddings = self.embed_tokens_per_layer(input_ids)
    return embeddings.reshape(
        *input_ids.shape,
        self.config.num_hidden_layers,
        self.hidden_size_per_layer_input,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nTextModel.project_per_layer_inputs"><code class="name flex">
<span>def <span class="ident">project_per_layer_inputs</span></span>(<span>self, inputs_embeds: torch.Tensor, per_layer_inputs: torch.Tensor | None = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def project_per_layer_inputs(
    self,
    inputs_embeds: torch.Tensor,
    per_layer_inputs: Optional[torch.Tensor] = None,
) -&gt; torch.Tensor:
    per_layer_projection, _ = self.per_layer_model_projection(inputs_embeds)
    per_layer_projection *= self.per_layer_projection_scale.type(
        inputs_embeds.dtype
    )
    per_layer_projection = per_layer_projection.reshape(
        *inputs_embeds.shape[:-1],
        self.config.num_hidden_layers,
        self.hidden_size_per_layer_input,
    )
    per_layer_projection = self.per_layer_projection_norm(per_layer_projection)

    if per_layer_inputs is None:
        return per_layer_projection

    if per_layer_projection.shape != per_layer_inputs.shape:
        # per-layer inputs are sometimes padded with zeros, slice the relevant embeddings
        per_layer_inputs = per_layer_inputs[..., : self.config.num_hidden_layers, :]

    return (
        per_layer_projection + per_layer_inputs
    ) * self.per_layer_input_scale.type(inputs_embeds.dtype)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gemma3n_causal.Gemma3nTextScaledWordEmbedding"><code class="flex name class">
<span>class <span class="ident">Gemma3nTextScaledWordEmbedding</span></span>
<span>(</span><span>num_embeddings: int,<br>embedding_dim: int,<br>padding_idx: int,<br>embed_scale: float | None = 1.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nTextScaledWordEmbedding(Gemma3TextScaledWordEmbedding):
    pass</code></pre>
</details>
<div class="desc"><p>This module overrides nn.Embeddings' forward by multiplying with embeddings scale.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.models.gemma3_causal.Gemma3TextScaledWordEmbedding" href="gemma3_causal.html#sglang.srt.models.gemma3_causal.Gemma3TextScaledWordEmbedding">Gemma3TextScaledWordEmbedding</a></li>
<li>torch.nn.modules.sparse.Embedding</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.models.gemma3_causal.Gemma3TextScaledWordEmbedding" href="gemma3_causal.html#sglang.srt.models.gemma3_causal.Gemma3TextScaledWordEmbedding">Gemma3TextScaledWordEmbedding</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.models.gemma3_causal.Gemma3TextScaledWordEmbedding.forward" href="gemma3_causal.html#sglang.srt.models.gemma3_causal.Gemma3TextScaledWordEmbedding.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.models" href="index.html">sglang.srt.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.models.gemma3n_causal.get_attention_sliding_window_size" href="#sglang.srt.models.gemma3n_causal.get_attention_sliding_window_size">get_attention_sliding_window_size</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nAltUp" href="#sglang.srt.models.gemma3n_causal.Gemma3nAltUp">Gemma3nAltUp</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nAltUp.compute_router_modalities" href="#sglang.srt.models.gemma3n_causal.Gemma3nAltUp.compute_router_modalities">compute_router_modalities</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nAltUp.correct" href="#sglang.srt.models.gemma3n_causal.Gemma3nAltUp.correct">correct</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nAltUp.forward" href="#sglang.srt.models.gemma3n_causal.Gemma3nAltUp.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nAltUp.predict" href="#sglang.srt.models.gemma3n_causal.Gemma3nAltUp.predict">predict</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nAltUp.scale_corrected_output" href="#sglang.srt.models.gemma3n_causal.Gemma3nAltUp.scale_corrected_output">scale_corrected_output</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nAttention" href="#sglang.srt.models.gemma3n_causal.Gemma3nAttention">Gemma3nAttention</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nAttention.forward" href="#sglang.srt.models.gemma3n_causal.Gemma3nAttention.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nDecoderLayer" href="#sglang.srt.models.gemma3n_causal.Gemma3nDecoderLayer">Gemma3nDecoderLayer</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nDecoderLayer.forward" href="#sglang.srt.models.gemma3n_causal.Gemma3nDecoderLayer.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM">Gemma3nForCausalLM</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.base_model_prefix" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.base_model_prefix">base_model_prefix</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.bitsandbytes_stacked_params_mapping" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.bitsandbytes_stacked_params_mapping">bitsandbytes_stacked_params_mapping</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.config_class" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.config_class">config_class</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.default_bitsandbytes_target_modules" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.default_bitsandbytes_target_modules">default_bitsandbytes_target_modules</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.dtype" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.dtype">dtype</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.embedding_modules" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.embedding_modules">embedding_modules</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.embedding_padding_modules" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.embedding_padding_modules">embedding_padding_modules</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.forward" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.get_attention_sliding_window_size" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.get_attention_sliding_window_size">get_attention_sliding_window_size</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.get_input_embeddings" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.get_input_embeddings">get_input_embeddings</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.load_weights" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.load_weights">load_weights</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.packed_modules_mapping" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.packed_modules_mapping">packed_modules_mapping</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.supported_lora_modules" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.supported_lora_modules">supported_lora_modules</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.supports_lora" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.supports_lora">supports_lora</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM">Gemma3nForCausalLM</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.base_model_prefix" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.base_model_prefix">base_model_prefix</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.bitsandbytes_stacked_params_mapping" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.bitsandbytes_stacked_params_mapping">bitsandbytes_stacked_params_mapping</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.config_class" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.config_class">config_class</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.default_bitsandbytes_target_modules" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.default_bitsandbytes_target_modules">default_bitsandbytes_target_modules</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.dtype" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.dtype">dtype</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.embedding_modules" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.embedding_modules">embedding_modules</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.embedding_padding_modules" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.embedding_padding_modules">embedding_padding_modules</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.forward" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.get_attention_sliding_window_size" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.get_attention_sliding_window_size">get_attention_sliding_window_size</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.get_input_embeddings" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.get_input_embeddings">get_input_embeddings</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.load_weights" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.load_weights">load_weights</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.packed_modules_mapping" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.packed_modules_mapping">packed_modules_mapping</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.supported_lora_modules" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.supported_lora_modules">supported_lora_modules</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.supports_lora" href="#sglang.srt.models.gemma3n_causal.Gemma3nForCausalLM.supports_lora">supports_lora</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nLaurelBlock" href="#sglang.srt.models.gemma3n_causal.Gemma3nLaurelBlock">Gemma3nLaurelBlock</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nLaurelBlock.forward" href="#sglang.srt.models.gemma3n_causal.Gemma3nLaurelBlock.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nRMSNorm" href="#sglang.srt.models.gemma3n_causal.Gemma3nRMSNorm">Gemma3nRMSNorm</a></code></h4>
</li>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nTextMLP" href="#sglang.srt.models.gemma3n_causal.Gemma3nTextMLP">Gemma3nTextMLP</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nTextMLP.forward" href="#sglang.srt.models.gemma3n_causal.Gemma3nTextMLP.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nTextModel" href="#sglang.srt.models.gemma3n_causal.Gemma3nTextModel">Gemma3nTextModel</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nTextModel.dtype" href="#sglang.srt.models.gemma3n_causal.Gemma3nTextModel.dtype">dtype</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nTextModel.forward" href="#sglang.srt.models.gemma3n_causal.Gemma3nTextModel.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nTextModel.get_input_embeddings" href="#sglang.srt.models.gemma3n_causal.Gemma3nTextModel.get_input_embeddings">get_input_embeddings</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nTextModel.get_per_layer_inputs" href="#sglang.srt.models.gemma3n_causal.Gemma3nTextModel.get_per_layer_inputs">get_per_layer_inputs</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nTextModel.project_per_layer_inputs" href="#sglang.srt.models.gemma3n_causal.Gemma3nTextModel.project_per_layer_inputs">project_per_layer_inputs</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_causal.Gemma3nTextScaledWordEmbedding" href="#sglang.srt.models.gemma3n_causal.Gemma3nTextScaledWordEmbedding">Gemma3nTextScaledWordEmbedding</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
