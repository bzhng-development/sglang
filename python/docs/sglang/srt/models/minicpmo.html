<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.models.minicpmo API documentation</title>
<meta name="description" content="Inference-only MiniCPM-o model compatible with HuggingFace weights.">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.models.minicpmo</code></h1>
</header>
<section id="section-intro">
<p>Inference-only MiniCPM-o model compatible with HuggingFace weights.</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.models.minicpmo.apply_spk_emb"><code class="name flex">
<span>def <span class="ident">apply_spk_emb</span></span>(<span>input_ids: torch.Tensor = None,<br>spk_emb: torch.Tensor = None,<br>input_embeds: torch.Tensor = None,<br>spk_emb_token_id: int = 0,<br>num_spk_embs: int = 1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_spk_emb(
    input_ids: torch.Tensor = None,
    spk_emb: torch.Tensor = None,
    input_embeds: torch.Tensor = None,
    spk_emb_token_id: int = 0,
    num_spk_embs: int = 1,
):
    &#34;&#34;&#34;
    Replace consecutive `num_spk_embs` speaker embedding placeholders in input_embeds with pre-prepared speaker embeddings. This is an in-place replacement, no new tensor is created, so no value is returned.

    Args:
        input_ids (torch.Tensor): Input ID tensor, shape [batch_size, seq_len_max]
        spk_emb (torch.Tensor): Speaker embedding tensor, shape [batch_size, num_spk_emb, hidden_dim]
        input_embeds (torch.Tensor): Input embedding tensor, shape [batch_size, seq_len_max, hidden_dim]
        spk_emb_token_id (int): ID of the speaker embedding token
        num_spk_embs (int): Number of speaker embeddings

    Returns:
        None
    &#34;&#34;&#34;

    batch_size = input_ids.shape[0]

    for idx in range(batch_size):
        input_ids_ = input_ids[idx]  # [seq_len_max]
        spk_emb_ = spk_emb[idx]  # [num_spk_emb]
        mask_ = input_ids_ == spk_emb_token_id  # [batch_size, seq_len_max]
        nonzero_position_idx = mask_.nonzero(as_tuple=False)  # [num_spk_emb, 1]
        assert nonzero_position_idx.shape[0] == num_spk_embs
        begin_idx = nonzero_position_idx.min()
        end_idx = nonzero_position_idx.max()
        input_embeds[idx, begin_idx : end_idx + 1, :] = spk_emb_

    return</code></pre>
</details>
<div class="desc"><p>Replace consecutive <code>num_spk_embs</code> speaker embedding placeholders in input_embeds with pre-prepared speaker embeddings. This is an in-place replacement, no new tensor is created, so no value is returned.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_ids</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Input ID tensor, shape [batch_size, seq_len_max]</dd>
<dt><strong><code>spk_emb</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Speaker embedding tensor, shape [batch_size, num_spk_emb, hidden_dim]</dd>
<dt><strong><code>input_embeds</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Input embedding tensor, shape [batch_size, seq_len_max, hidden_dim]</dd>
<dt><strong><code>spk_emb_token_id</code></strong> :&ensp;<code>int</code></dt>
<dd>ID of the speaker embedding token</dd>
<dt><strong><code>num_spk_embs</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of speaker embeddings</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None</p></div>
</dd>
<dt id="sglang.srt.models.minicpmo.make_streaming_chunk_mask_generation"><code class="name flex">
<span>def <span class="ident">make_streaming_chunk_mask_generation</span></span>(<span>inputs_embeds: torch.Tensor,<br>past_seen_tokens: int,<br>streaming_tts_text_mask: torch.Tensor,<br>streaming_reserved_length: int = 300,<br>streaming_audio_chunk_size: int = 50,<br>streaming_text_chunk_size: int = 10,<br>num_spk_emb: int = 1,<br>use_spk_emb: bool = True) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_streaming_chunk_mask_generation(
    inputs_embeds: torch.Tensor,
    past_seen_tokens: int,
    streaming_tts_text_mask: torch.Tensor,
    streaming_reserved_length: int = 300,
    streaming_audio_chunk_size: int = 50,
    streaming_text_chunk_size: int = 10,
    num_spk_emb: int = 1,
    use_spk_emb: bool = True,
) -&gt; torch.Tensor:
    &#34;&#34;&#34;
    In streaming audio generation, determine which `text` positions the TTS model can attend to when generating each chunk of `audio` tokens.

    This function creates a mask that allows the model to attend to a specific chunk of text
    tokens when generating each chunk of audio tokens, enabling streaming TTS generation.

    Args:
        inputs_embeds (torch.Tensor): Input embeddings tensor.
        past_seen_tokens (int): Number of tokens already seen by the model.
        streaming_tts_text_mask (torch.Tensor): Mask for the text tokens.
        streaming_reserved_length (int, optional): Number of reserved tokens for streaming. Defaults to 300.
        streaming_text_chunk_size (int, optional): Size of each text chunk. Defaults to 7.

    Returns:
        torch.Tensor: Causal mask for streaming TTS generation, shape is [batch_size=1, 1, seq_len=1, past_seen_tokens+1]

    Raises:
        AssertionError: If the batch size is not 1 (only supports batch size of 1 for inference).
    &#34;&#34;&#34;
    assert inputs_embeds.shape[0] == 1

    dtype = inputs_embeds.dtype
    device = inputs_embeds.device
    min_dtype = torch.finfo(dtype).min

    # Add `1` to the past seen tokens to account for new `tokens` during `generate`
    causal_mask = torch.full(
        (1, past_seen_tokens + inputs_embeds.shape[1]),
        fill_value=0,
        dtype=dtype,
        device=device,
    )

    # Calculate the start of invisible text tokens
    invisible_text_tokens_start = (
        min(
            math.ceil(
                (past_seen_tokens - streaming_reserved_length)
                / streaming_audio_chunk_size
            )
            * streaming_text_chunk_size,
            streaming_reserved_length,
        )
        + 1
        + num_spk_emb * use_spk_emb
    )  # Add 1 for [Stts] and N for [spk_emb] tokens if `use_spk_emb` is True

    invisible_text_tokens_end = (
        streaming_reserved_length + 1 + num_spk_emb * use_spk_emb + 1
    )  # Add 1 for [Ptts] (aka `audio_bos_token_id`)

    # Set invisible text tokens to min_dtype (effectively -inf)
    causal_mask[0, invisible_text_tokens_start:invisible_text_tokens_end] = min_dtype

    # Mask padding positions in the text mask
    causal_mask[
        0, 0 : 1 + num_spk_emb * use_spk_emb + streaming_reserved_length + 1
    ].masked_fill_(streaming_tts_text_mask == 0, min_dtype)

    # Add extra dimensions for batch and heads
    causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)

    return causal_mask</code></pre>
</details>
<div class="desc"><p>In streaming audio generation, determine which <code>text</code> positions the TTS model can attend to when generating each chunk of <code>audio</code> tokens.</p>
<p>This function creates a mask that allows the model to attend to a specific chunk of text
tokens when generating each chunk of audio tokens, enabling streaming TTS generation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs_embeds</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Input embeddings tensor.</dd>
<dt><strong><code>past_seen_tokens</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of tokens already seen by the model.</dd>
<dt><strong><code>streaming_tts_text_mask</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Mask for the text tokens.</dd>
<dt><strong><code>streaming_reserved_length</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of reserved tokens for streaming. Defaults to 300.</dd>
<dt><strong><code>streaming_text_chunk_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Size of each text chunk. Defaults to 7.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Causal mask for streaming TTS generation, shape is [batch_size=1, 1, seq_len=1, past_seen_tokens+1]</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>AssertionError</code></dt>
<dd>If the batch size is not 1 (only supports batch size of 1 for inference).</dd>
</dl></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.models.minicpmo.ConditionalChatTTS"><code class="flex name class">
<span>class <span class="ident">ConditionalChatTTS</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConditionalChatTTS(PreTrainedModel):
    &#34;&#34;&#34;A conditional text-to-speech model that can generate speech from text with speaker conditioning.

    This model extends PreTrainedModel to provide text-to-speech capabilities with:
    - LLM hidden state conditioning
    - Streaming generation

    The model uses a transformer architecture with LLM hidden states and can operate in both
    streaming and non-streaming modes for flexible deployment.

    The model process sequence in the following format:
    | text bos token | LLM embedding projected to tts embedding space | text tokens (fixed length, reserved for future tokens) | audio bos token | audio tokens (audio token length is not fixed)| audio eos token |

    The format is designed to support LLM-conditioned streaming audio generation.

    Usage:
    To support streaming generation, two global variables should be maintained outside of the model.
        1. `audio_input_ids`: stores *discrete* audio codes. It is a tensor with shape [1, sequence length+1, num_vq].
        2. `past_key_values`: stores the KV cache for both text tokens and audio codes. It is a list of tuples, each tuple contains two tensors with shape [1, num_attention_heads, sequence length, hidden_size // num_attention_heads]

    where `num_vq` is the number of audio codebooks, in default setting, it is `4`.

    1. Create an empty `past_key_values` with
    ```python
    initial_kv_cache_length = 1 + model.num_spk_embs + model.streaming_text_reserved_len # where `1` denotes the `bos` token
    dtype = model.emb_text.weight.dtype
    device = model.emb_text.weight.device
    past_key_values = [
        (
            torch.zeros(1, model.config.num_attention_heads, initial_kv_cache_length, model.config.hidden_size // model.config.num_attention_heads, dtype=dtype, device=device),
            torch.zeros(1, model.config.num_attention_heads, initial_kv_cache_length, model.config.hidden_size // model.config.num_attention_heads, dtype=dtype, device=device)
        )
        for _ in range(model.config.num_hidden_layers)
    ]

    2. At the same time, create an empty `audio_input_ids` with shape [1, sequence length, num_vq], `num_vq` denotes multiple layer audio codebooks. But here we also include text tokens in the sequence, but they will be zeros, and will not be used, just a placeholder.

    ```python
    initial_audio_input_ids_length = 1 + model.num_spk_embs + model.streaming_text_reserved_len + 1
    # [bos token, speaker embeddings, text tokens, audio bos token]
    audio_input_ids = torch.zeros(batch_size=1, initial_audio_input_ids_length, model.num_vq)
    ```

    2. Prefill some text tokens to TTS model (for example, 10 tokens) using `prefill_text` method.

    ```python
    outputs = llm.generate(**kwargs)
    llm_tokens = some_function_to_extract_llm_tokens(outputs)
    lm_spk_emb_last_hidden_states = some_function_to_extract_lm_spk_emb_last_hidden_states(outputs)
    tts_text_input_ids = tts_tokenizer.encode(llm_tokenizer.decode(llm_tokens))
    # here assume we are prefilling text token 0 to text token 9 (included), totally 10 tokens.
    begin = 0
    end = 9+1
    position_ids = torch.arange(begin, end, dtype=torch.long, device=device)

    past_key_values = model.prefill_text(
        input_ids=tts_text_input_ids,
        position_ids=position_ids,
        past_key_values=past_key_values,
        lm_spk_emb_last_hidden_states=lm_spk_emb_last_hidden_states,
    )
    ```

    3. Make a `streaming_tts_text_mask` to denote which position contains valid text tokens, similar to `attention_mask` in standard causal attention.

    ```python
    streaming_tts_text_mask = torch.zeros(model.streaming_reserved_length)
    streaming_tts_text_mask[0:end] = 1 # denotes these post
    ```

    3. Generate audio codes using `generate` method.

    ```python
    outputs = model.generate(
        input_ids=audio_input_ids,
        past_key_values=past_key_values,
        streaming_tts_text_mask=streaming_tts_text_mask,
        max_new_token=50,
    )

    # update past_key_values and input_ids
    past_key_values = outputs.past_key_values
    audio_input_ids = outputs.input_ids
    ```

    The `past_key_values` is extended by `max_new_token=50`, and `audio_input_ids` is also extended by `max_new_token=50` after `generate` calling.

    4. Notice that after prefilling `10` text tokens, the model can generate up to `50` audio tokens, if you want to generate more audio tokens, you need to prefill next `10` text tokens. And it is okay to only generate `25` audio tokens for faster initial response.

    5. Repeat steps `2,3,4` as needed in your streaming audio generation cases, but ensure usage complies with the following guidelines discussed above.
    &#34;&#34;&#34;

    config_class = PretrainedConfig
    _no_split_modules = []

    def __init__(self, config: PretrainedConfig):
        super().__init__(config)

        self.use_speaker_embedding = config.use_speaker_embedding
        self.use_llm_hidden_state = config.use_llm_hidden_state
        self.num_spk_embs = config.num_spk_embs
        self.spk_emb_token_id = config.spk_emb_token_id

        self.use_text = config.use_text
        self.streaming = config.streaming
        self.streaming_text_chunk_size = config.streaming_text_chunk_size
        self.streaming_audio_chunk_size = config.streaming_audio_chunk_size
        self.streaming_text_reserved_len = config.streaming_text_reserved_len
        self.audio_bos_token_id = config.audio_bos_token_id
        self.num_mel_bins = config.num_mel_bins
        self.num_vq = config.num_vq
        self.num_audio_tokens = config.num_audio_tokens

        self.top_p = config.top_p
        self.top_k = config.top_k
        self.repetition_penalty = config.repetition_penalty

        if self.config.use_mlp:
            self.projector = MultiModalProjector(config.llm_dim, config.hidden_size)
        else:
            self.projector = nn.Linear(config.llm_dim, config.hidden_size, bias=False)
        self.emb_code = nn.ModuleList(
            [
                nn.Embedding(config.num_audio_tokens, config.hidden_size)
                for _ in range(config.num_vq)
            ]
        )
        self.emb_text = nn.Embedding(config.num_text_tokens, config.hidden_size)
        self.head_code = nn.ModuleList(
            [
                parametrizations.weight_norm(
                    nn.Linear(config.hidden_size, config.num_audio_tokens, bias=False),
                    name=&#34;weight&#34;,
                )
                for _ in range(config.num_vq)
            ]
        )

        dvae = DVAE()
        self.dvae = dvae

        model_config = LlamaConfig(
            hidden_size=config.hidden_size,
            intermediate_size=config.intermediate_size,
            num_attention_heads=config.num_attention_heads,
            num_hidden_layers=config.num_hidden_layers,
            max_position_embeddings=config.max_position_embeddings,
            attn_implementation=config.attn_implementation,
        )

        model = LlamaModel(model_config)
        self.model = model

    @torch.inference_mode()
    def merge_inputs_embeds(
        self,
        input_ids: torch.Tensor,
        lm_spk_emb_last_hidden_states: Optional[torch.Tensor] = None,
    ):
        &#34;&#34;&#34;Merge `input_ids` and `lm_spk_emb_last_hidden_states` to `inputs_embeds`.

        Args:
            input_ids (torch.Tensor): Input token IDs.
            lm_spk_emb_last_hidden_states (Optional[torch.Tensor], optional): Last hidden states of speaker embeddings from the language model. Defaults to None.

        Raises:
            NotImplementedError: If speaker embedding is not used and language model hidden states are not implemented.

        Returns:
            torch.Tensor: Prepared input embeddings for the model.
        &#34;&#34;&#34;
        assert input_ids.shape[0] == 1

        # Embed input_ids to input_embeds
        inputs_embeds = self.emb_text(input_ids)

        # Inject speaker embedding to input_embeds if it exists
        if self.use_speaker_embedding:
            spk_emb_mask = input_ids == self.spk_emb_token_id
            if spk_emb_mask.any():
                assert lm_spk_emb_last_hidden_states is not None
                # Project spk emb to tts hidden size first, [batch_size, num_spk_emb, llm_dim] -&gt; [batch_size, num_spk_emb, self.hidden_size]
                lm_spk_emb_last_hidden_states = lm_spk_emb_last_hidden_states.to(
                    self.projector.linear1.weight.dtype
                )
                projected_spk_emb = self.projector(lm_spk_emb_last_hidden_states)
                projected_spk_emb = F.normalize(projected_spk_emb, p=2, dim=-1)
                apply_spk_emb(
                    input_ids=input_ids,
                    spk_emb=projected_spk_emb,
                    input_embeds=inputs_embeds,
                    spk_emb_token_id=self.spk_emb_token_id,
                    num_spk_embs=self.num_spk_embs,
                )
        else:
            raise NotImplementedError

        return inputs_embeds

    @torch.inference_mode()
    def prefill_text(
        self,
        input_ids: torch.Tensor,
        position_ids: torch.LongTensor,
        past_key_values: List[Tuple[torch.Tensor, torch.Tensor]],
        lm_spk_emb_last_hidden_states: Optional[torch.Tensor] = None,
    ):
        &#34;&#34;&#34;Prefill a chunk of new text tokens in streaming setting.
        Specifically speaking, update `past_key_values` using new text tokens, then the model will read the new text tokens.

        Args:
            input_ids (Tensor): Tensor of shape [batch_size, seq_len]
            position_ids (LongTensor): Tensor of shape [batch_size, seq_len]
            past_key_values (List[Tuple[Tensor]]): KV Cache of all layers, each layer is a tuple (Tensor, Tensor) denoting keys and values. Each tensor is of seq_len = `self.streaming_text_reserved_len`. `past_key_values` will be updated.
            lm_spk_emb_last_hidden_states (Tensor, optional): Tensor of shape [batch_size, num_spk_emb, llm_dim]. Defaults to None.

        Note that all `batch_size` should be `1`.
        &#34;&#34;&#34;
        assert input_ids.shape[0] == 1
        assert past_key_values is not None

        # Merge text and LLM embeddings
        inputs_embeds = self.merge_inputs_embeds(
            input_ids=input_ids,
            lm_spk_emb_last_hidden_states=lm_spk_emb_last_hidden_states,
        )

        # Clone KV Cache
        past_key_values_for_prefill = []
        for i in range(len(past_key_values)):
            past_key_values_for_prefill.append(
                (
                    past_key_values[i][0][:, :, : position_ids[:, 0], :].clone(),
                    past_key_values[i][1][:, :, : position_ids[:, 0], :].clone(),
                )
            )

        # ModelMiniCPMVBaseModel
        outputs_prefill: BaseModelOutputWithPast = self.model(
            attention_mask=None,  # because for text, it is standard causal attention mask, do nothing
            position_ids=position_ids,  # position_ids denotes the position of new text tokens in the sequence
            past_key_values=past_key_values_for_prefill,  # `past_key_values` will be updated by the model
            inputs_embeds=inputs_embeds,  # contains text and language model embedding
            use_cache=True,
            output_attentions=False,
            cache_position=position_ids,  # which new positions will use this cache, basically the same as position_ids
        )

        # Get model updated KV Cache
        past_key_values_for_prefill_updated = outputs_prefill.past_key_values

        # Update generated KV Cache to input `past_key_values`
        for layer_idx in range(len(past_key_values)):
            # Update keys
            past_key_values[layer_idx][0][
                :, :, position_ids[:, 0] : position_ids[:, -1] + 1, :
            ] = past_key_values_for_prefill_updated[layer_idx][0][
                :, :, position_ids[:, 0] : position_ids[:, -1] + 1
            ].clone()
            # Update values
            past_key_values[layer_idx][1][
                :, :, position_ids[:, 0] : position_ids[:, -1] + 1, :
            ] = past_key_values_for_prefill_updated[layer_idx][1][
                :, :, position_ids[:, 0] : position_ids[:, -1] + 1
            ].clone()

        # TODO: del past_key_values_for_prefill_updated recursively
        # TODO: del outputs_prefill recursively

        return past_key_values

    @torch.inference_mode()
    def prefill_audio_ids(
        self,
        input_ids: torch.Tensor,
        past_key_values: List[Tuple[torch.Tensor, torch.Tensor]],
        streaming_tts_text_mask=None,
        add_audio_bos: bool = True,
    ):
        &#34;&#34;&#34;Prefill a chunk of audio ids to the model. Used in sliding-window long audio generation.
        Specifically, prefill many audio ids (typically from last window) to the model in the new window.

        Args:
            input_ids (torch.Tensor): (1, seq_len, num_vq) Audio input token ids.
            past_key_values (List[Tuple[torch.Tensor, torch.Tensor]]): Past key values for attention mechanism.
        &#34;&#34;&#34;
        assert input_ids.shape[0] == 1
        assert past_key_values is not None

        code_emb = [self.emb_code[i](input_ids[:, :, i]) for i in range(self.num_vq)]
        inputs_embeds = torch.stack(code_emb, 3).sum(3)  # [1,seq_len,768]
        input_len = input_ids.shape[1]

        if add_audio_bos:
            narrowed_input_ids = torch.tensor(
                [[self.audio_bos_token_id]], dtype=torch.long, device=self.device
            )
            bos_inputs_embeds = self.emb_text(narrowed_input_ids)
            inputs_embeds = torch.cat([bos_inputs_embeds, inputs_embeds], dim=1)
            input_len += 1

        past_key_values_length = past_key_values[0][0].shape[2]
        position_ids = torch.arange(
            past_key_values_length,
            past_key_values_length + input_len,
            dtype=torch.long,
            device=self.device,
        ).unsqueeze(0)

        cache_position = position_ids.clone()
        causal_mask = make_streaming_chunk_mask_generation(
            inputs_embeds=inputs_embeds,
            past_seen_tokens=past_key_values[0][0].shape[2],
            streaming_tts_text_mask=streaming_tts_text_mask,
            streaming_reserved_length=self.streaming_text_reserved_len,
            streaming_text_chunk_size=self.streaming_text_chunk_size,
        )  # [1, 1, 1, past_key_values_length + input_len]

        # Model forward
        outputs: BaseModelOutputWithPast = self.model(
            attention_mask=causal_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=True,
            output_attentions=False,
            cache_position=cache_position,
        )
        past_key_values = outputs.past_key_values
        return past_key_values

    @torch.inference_mode()
    def generate(
        self,
        input_ids: torch.Tensor,
        past_key_values: List[Tuple[torch.Tensor, torch.Tensor]],
        temperature: torch.Tensor,
        eos_token: Union[int, torch.Tensor],
        streaming_tts_text_mask=None,
        force_no_stop=False,
        min_new_token=10,
        max_new_token=50,
        logits_warpers: List[LogitsWarper] = [],
        logits_processors: List[CustomRepetitionPenaltyLogitsProcessorRepeat] = [],
        show_tqdm=False,
    ):
        &#34;&#34;&#34;Generate audio codes in streaming setting or non-streaming setting.
        Specifically speaking, generate audio codes when not all text tokens are prefilled.

        Always pass a valid `past_key_values` to the method. The method does not do `prefill` by itself. It relies on `prefill_text` method to provide valid `past_key_values`. Please refer to docstring of this class for more details.

        In this method, we borrowed a lot of codes from `https://github.com/2noise/ChatTTS/blob/main/ChatTTS/model/gpt.py`.

        Args:
            input_ids (torch.Tensor): Input token ids.
            past_key_values (List[Tuple[torch.Tensor, torch.Tensor]]): Past key values for attention mechanism.
            temperature (torch.Tensor): Temperature for sampling.
            eos_token (Union[int, torch.Tensor]): End of sequence token.
            streaming_tts_text_mask (Optional[torch.Tensor], optional): Mask for streaming TTS text. Defaults to None.
            max_new_token (int, optional): Maximum number of new tokens to generate. Defaults to 50.
            logits_warpers (List[LogitsWarper], optional): List of logits warpers. Defaults to [].
            logits_processors (List[CustomRepetitionPenaltyLogitsProcessorRepeat], optional): List of logits processors. Defaults to [].
            show_tqdm (bool, optional): Whether to show progress bar. Defaults to True.

        Returns:
            GenerationOutputs: Generation outputs.
        &#34;&#34;&#34;

        # We only support batch size `1` for now
        assert input_ids.shape[0] == 1
        assert past_key_values is not None

        # fix: this should not be `input_ids.shape[1]`
        # start_idx = input_ids.shape[1]
        start_idx = (
            1
            + self.num_spk_embs * self.use_speaker_embedding
            + self.streaming_text_reserved_len
            + 1
        )

        finish = torch.zeros(input_ids.shape[0], device=input_ids.device).bool()

        temperature = (
            temperature.unsqueeze(0)
            .expand(input_ids.shape[0], -1)
            .contiguous()
            .view(-1, 1)
        )

        progress = input_ids.shape[1]

        # Pre-allocate input_ids, shape is [batch_size=1, max_possible_seq_len, self.num_vqs]
        input_ids_buf = torch.zeros(
            input_ids.shape[0],  # batch_size
            progress
            + max_new_token,  # max_possible_seq_len = input_ids.shape[1] + max_new_token
            input_ids.shape[2],  # self.num_vqs
            dtype=input_ids.dtype,
            device=input_ids.device,
        )

        # Copy existing `input_ids` to `input_ids_buf`
        input_ids_buf.narrow(1, 0, progress).copy_(input_ids)

        del input_ids
        input_ids = input_ids_buf.narrow(1, 0, progress)

        pbar: Optional[tqdm] = None
        if show_tqdm:
            pbar = tqdm(
                total=max_new_token,
                desc=&#34;code&#34;,
                bar_format=&#34;{l_bar}{bar}| {n_fmt}/{total_fmt}(max) [{elapsed}, {rate_fmt}{postfix}]&#34;,
            )

        condition_length = (
            1
            + self.num_spk_embs * self.use_speaker_embedding
            + self.streaming_text_reserved_len
            + 1
        )

        for i in range(max_new_token):
            # Prepare generation inputs
            audio_bos = False

            # If this is the first audio token, the case is SPECIAL
            if progress == condition_length:
                audio_bos = True

            assert progress == (
                past_key_values[0][0].shape[2] + 1
            )  # If you are using according to the guidelines, this should be passed.

            if audio_bos:
                # Generate the first token, activate the model with `self.audio_bos_token_id`, the model will predict
                # a new audio token. This is a special case because without the `audio bos token`, it is impossible
                # to generate the first audio token in our streaming setting.
                narrowed_input_ids = torch.tensor(
                    [[self.audio_bos_token_id]], dtype=torch.long, device=self.device
                )
                inputs_embeds = self.emb_text(narrowed_input_ids)
                del narrowed_input_ids
            else:
                # Generate the following audio tokens, it is applicable to all other cases, including second and the
                # following calling of `generate`.
                narrowed_input_ids = input_ids.narrow(
                    dim=1, start=input_ids.shape[1] - 1, length=1
                )
                code_emb = [
                    self.emb_code[i](narrowed_input_ids[:, :, i])
                    for i in range(self.num_vq)
                ]
                inputs_embeds = torch.stack(code_emb, 3).sum(3)

            position_ids = torch.tensor(
                [past_key_values[0][0].shape[2]], dtype=torch.long, device=self.device
            ).unsqueeze(0)

            cache_position = position_ids.clone()

            # Make causal mask
            causal_mask = make_streaming_chunk_mask_generation(
                inputs_embeds=inputs_embeds,
                past_seen_tokens=past_key_values[0][0].shape[2],
                streaming_tts_text_mask=streaming_tts_text_mask,
                streaming_reserved_length=self.streaming_text_reserved_len,
                streaming_text_chunk_size=self.streaming_text_chunk_size,
            )

            # Model forward
            outputs: BaseModelOutputWithPast = self.model(
                attention_mask=causal_mask,
                position_ids=position_ids,
                past_key_values=past_key_values,
                inputs_embeds=inputs_embeds,
                use_cache=True,
                output_attentions=False,
                cache_position=cache_position,
            )

            del position_ids
            del inputs_embeds
            del cache_position
            del causal_mask

            hidden_states = outputs.last_hidden_state
            past_key_values = outputs.past_key_values

            with P.cached():
                logits = torch.empty(
                    hidden_states.size(0),
                    hidden_states.size(1),
                    self.num_audio_tokens,
                    self.num_vq,
                    dtype=torch.float,
                    device=self.device,
                )
                for num_vq_iter in range(self.num_vq):
                    x: torch.Tensor = self.head_code[num_vq_iter](hidden_states)
                    logits[..., num_vq_iter] = x
                    del x

            del hidden_states

            # logits = logits[:, -1].float()
            logits = logits.narrow(1, -1, 1).squeeze_(1).float()

            # logits = rearrange(logits, &#34;b c n -&gt; (b n) c&#34;)
            logits = logits.permute(0, 2, 1)
            logits = logits.reshape(-1, logits.size(2))
            # logits_token = rearrange(input_ids[:, start_idx:], &#34;b c n -&gt; (b n) c&#34;)
            input_ids_sliced = input_ids.narrow(
                1,
                start_idx,
                input_ids.size(1) - start_idx,
            ).permute(0, 2, 1)
            logits_token = input_ids_sliced.reshape(
                input_ids_sliced.size(0) * input_ids_sliced.size(1),
                -1,
            ).to(self.device)
            del input_ids_sliced

            logits /= temperature

            if not audio_bos:
                for logitsProcessors in logits_processors:
                    logits = logitsProcessors(logits_token, logits)
            if not audio_bos:
                for logitsWarpers in logits_warpers:
                    logits = logitsWarpers(logits_token, logits)

            del logits_token

            if i &lt; min_new_token:
                logits[:, eos_token] = -torch.inf

            if force_no_stop:
                logits[:, eos_token] = -torch.inf

            scores = F.softmax(logits, dim=-1)

            del logits
            idx_next = torch.multinomial(scores, num_samples=1)  # .to(finish.device)

            del scores

            # idx_next = rearrange(idx_next, &#34;(b n) 1 -&gt; b n&#34;, n=self.num_vq)
            idx_next = idx_next.view(-1, self.num_vq)
            finish_or = idx_next.eq(eos_token).any(1)
            finish.logical_or_(finish_or)

            del finish_or
            # Store new `token` into `input_ids_buf`
            input_ids_buf.narrow(1, progress, 1).copy_(idx_next.unsqueeze_(1))

            if i == 0 and finish.any():
                # raise Exception
                break

            del idx_next
            progress += 1
            input_ids = input_ids_buf.narrow(1, 0, progress)

            if finish.all():
                break

            if pbar is not None:
                pbar.update(1)

        if pbar is not None:
            pbar.close()

        if not finish.all():
            if show_tqdm:
                logger.info(f&#34;incomplete result. hit max_new_token: {max_new_token}&#34;)

        del input_ids_buf

        if finish.all():
            # the last may contains eos token
            genrated_input_ids = input_ids[:, condition_length:-1, :]
        else:
            # there is no eos token
            genrated_input_ids = input_ids[:, condition_length:, :]

        return ConditionalChatTTSGenerationOutput(
            new_ids=genrated_input_ids,
            audio_input_ids=input_ids,  # for update purpose
            past_key_values=past_key_values,  # for update purpose
            finished=finish.all(),
        )

    @torch.inference_mode()
    def decode_to_mel_specs(
        self,
        result_list: List[torch.Tensor],
    ):
        &#34;&#34;&#34;Decode discrete audio codes to mel spectrograms.

        Borrowed from `https://github.com/2noise/ChatTTS/blob/main/ChatTTS/core.py`

        Args:
            result_list (List[torch.Tensor]): Audio codes output from `generate`.

        Returns:
            torch.Tensor: Mel spectrograms.
        &#34;&#34;&#34;

        decoder = self.dvae
        max_x_len = -1
        if len(result_list) == 0:
            return np.array([], dtype=np.float32)
        for result in result_list:
            if result.size(0) &gt; max_x_len:
                max_x_len = result.size(0)
        batch_result = torch.zeros(
            (len(result_list), result_list[0].size(1), max_x_len),
            dtype=result_list[0].dtype,
            device=result_list[0].device,
        )
        for i in range(len(result_list)):
            src = result_list[i]
            batch_result[i].narrow(1, 0, src.size(0)).copy_(src.permute(1, 0))
            del src

        mel_specs = decoder(batch_result)
        del batch_result
        return mel_specs</code></pre>
</details>
<div class="desc"><p>A conditional text-to-speech model that can generate speech from text with speaker conditioning.</p>
<p>This model extends PreTrainedModel to provide text-to-speech capabilities with:
- LLM hidden state conditioning
- Streaming generation</p>
<p>The model uses a transformer architecture with LLM hidden states and can operate in both
streaming and non-streaming modes for flexible deployment.</p>
<p>The model process sequence in the following format:
| text bos token | LLM embedding projected to tts embedding space | text tokens (fixed length, reserved for future tokens) | audio bos token | audio tokens (audio token length is not fixed)| audio eos token |</p>
<p>The format is designed to support LLM-conditioned streaming audio generation.</p>
<p>Usage:
To support streaming generation, two global variables should be maintained outside of the model.
1. <code>audio_input_ids</code>: stores <em>discrete</em> audio codes. It is a tensor with shape [1, sequence length+1, num_vq].
2. <code>past_key_values</code>: stores the KV cache for both text tokens and audio codes. It is a list of tuples, each tuple contains two tensors with shape [1, num_attention_heads, sequence length, hidden_size // num_attention_heads]</p>
<p>where <code>num_vq</code> is the number of audio codebooks, in default setting, it is <code>4</code>.</p>
<ol>
<li>Create an empty <code>past_key_values</code> with</li>
</ol>
<pre><code class="language-python">initial_kv_cache_length = 1 + model.num_spk_embs + model.streaming_text_reserved_len # where `1` denotes the `bos` token
dtype = model.emb_text.weight.dtype
device = model.emb_text.weight.device
past_key_values = [
    (
        torch.zeros(1, model.config.num_attention_heads, initial_kv_cache_length, model.config.hidden_size // model.config.num_attention_heads, dtype=dtype, device=device),
        torch.zeros(1, model.config.num_attention_heads, initial_kv_cache_length, model.config.hidden_size // model.config.num_attention_heads, dtype=dtype, device=device)
    )
    for _ in range(model.config.num_hidden_layers)
]

2. At the same time, create an empty `audio_input_ids` with shape [1, sequence length, num_vq], `num_vq` denotes multiple layer audio codebooks. But here we also include text tokens in the sequence, but they will be zeros, and will not be used, just a placeholder.

```python
initial_audio_input_ids_length = 1 + model.num_spk_embs + model.streaming_text_reserved_len + 1
# [bos token, speaker embeddings, text tokens, audio bos token]
audio_input_ids = torch.zeros(batch_size=1, initial_audio_input_ids_length, model.num_vq)
</code></pre>
<ol>
<li>Prefill some text tokens to TTS model (for example, 10 tokens) using <code>prefill_text</code> method.</li>
</ol>
<pre><code class="language-python">outputs = llm.generate(**kwargs)
llm_tokens = some_function_to_extract_llm_tokens(outputs)
lm_spk_emb_last_hidden_states = some_function_to_extract_lm_spk_emb_last_hidden_states(outputs)
tts_text_input_ids = tts_tokenizer.encode(llm_tokenizer.decode(llm_tokens))
# here assume we are prefilling text token 0 to text token 9 (included), totally 10 tokens.
begin = 0
end = 9+1
position_ids = torch.arange(begin, end, dtype=torch.long, device=device)

past_key_values = model.prefill_text(
    input_ids=tts_text_input_ids,
    position_ids=position_ids,
    past_key_values=past_key_values,
    lm_spk_emb_last_hidden_states=lm_spk_emb_last_hidden_states,
)
</code></pre>
<ol>
<li>Make a <code>streaming_tts_text_mask</code> to denote which position contains valid text tokens, similar to <code>attention_mask</code> in standard causal attention.</li>
</ol>
<pre><code class="language-python">streaming_tts_text_mask = torch.zeros(model.streaming_reserved_length)
streaming_tts_text_mask[0:end] = 1 # denotes these post
</code></pre>
<ol>
<li>Generate audio codes using <code>generate</code> method.</li>
</ol>
<pre><code class="language-python">outputs = model.generate(
    input_ids=audio_input_ids,
    past_key_values=past_key_values,
    streaming_tts_text_mask=streaming_tts_text_mask,
    max_new_token=50,
)

# update past_key_values and input_ids
past_key_values = outputs.past_key_values
audio_input_ids = outputs.input_ids
</code></pre>
<p>The <code>past_key_values</code> is extended by <code>max_new_token=50</code>, and <code>audio_input_ids</code> is also extended by <code>max_new_token=50</code> after <code>generate</code> calling.</p>
<ol>
<li>
<p>Notice that after prefilling <code>10</code> text tokens, the model can generate up to <code>50</code> audio tokens, if you want to generate more audio tokens, you need to prefill next <code>10</code> text tokens. And it is okay to only generate <code>25</code> audio tokens for faster initial response.</p>
</li>
<li>
<p>Repeat steps <code>2,3,4</code> as needed in your streaming audio generation cases, but ensure usage complies with the following guidelines discussed above.</p>
</li>
</ol>
<p>Args:
config ([<code>PretrainedConfig</code>]):
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
[<code>~PreTrainedModel.from_pretrained</code>] method to load the model weights.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.modeling_utils.PreTrainedModel</li>
<li>torch.nn.modules.module.Module</li>
<li>transformers.modeling_utils.EmbeddingAccessMixin</li>
<li>transformers.modeling_utils.ModuleUtilsMixin</li>
<li>transformers.utils.hub.PushToHubMixin</li>
<li>transformers.integrations.peft.PeftAdapterMixin</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.minicpmo.ConditionalChatTTS.config_class"><code class="name">var <span class="ident">config_class</span></code></dt>
<dd>
<div class="desc"><p>Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as
methods for loading/downloading/saving configurations.</p>
<p><Tip></p>
<p>A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to
initialize a model does <strong>not</strong> load the model weights. It only affects the model's configuration.</p>
<p></Tip></p>
<p>Class attributes (overridden by derived classes):</p>
<ul>
<li><strong>model_type</strong> (<code>str</code>) &ndash; An identifier for the model type, serialized into the JSON file, and used to recreate
the correct object in [<code>~transformers.AutoConfig</code>].</li>
<li><strong>has_no_defaults_at_init</strong> (<code>bool</code>) &ndash; Whether the config class can be initialized without providing input arguments.
Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,
(but not necessarily) such as [<code>~transformers.EncoderDecoderConfig</code>] or [<code>~RagConfig</code>]. They have to be initialized from
two or more configs of type [<code>~transformers.PretrainedConfig</code>].</li>
<li><strong>keys_to_ignore_at_inference</strong> (<code>list[str]</code>) &ndash; A list of keys to ignore by default when looking at dictionary
outputs of the model during inference.</li>
<li><strong>attribute_map</strong> (<code>dict[str, str]</code>) &ndash; A dict that maps model specific attribute names to the standardized
naming of attributes.</li>
<li><strong>base_model_tp_plan</strong> (<code>dict[str, Any]</code>) &ndash; A dict that maps sub-modules FQNs of a base model to a tensor
parallel plan applied to the sub-module when <code>model.tensor_parallel</code> is called.</li>
<li><strong>base_model_pp_plan</strong> (<code>dict[str, tuple[list[str]]]</code>) &ndash; A dict that maps child-modules of a base model to a
pipeline parallel plan that enables users to place the child-module on the appropriate device.</li>
</ul>
<p>Common attributes (present in all subclasses):</p>
<ul>
<li><strong>vocab_size</strong> (<code>int</code>) &ndash; The number of tokens in the vocabulary, which is also the first dimension of the
embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).</li>
<li><strong>hidden_size</strong> (<code>int</code>) &ndash; The hidden size of the model.</li>
<li><strong>num_attention_heads</strong> (<code>int</code>) &ndash; The number of attention heads used in the multi-head attention layers of the
model.</li>
<li><strong>num_hidden_layers</strong> (<code>int</code>) &ndash; The number of blocks in the model.</li>
</ul>
<p><Tip warning={true}></p>
<p>Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading
some of them will still be possible, but attempting to overwrite them will throw an exception &ndash; you should set
them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more
information about the individual parameters.</p>
<p></Tip></p>
<h2 id="arg">Arg</h2>
<p>name_or_path (<code>str</code>, <em>optional</em>, defaults to <code>""</code>):
Store the string that was passed to [<code>PreTrainedModel.from_pretrained</code>] or
[<code>TFPreTrainedModel.from_pretrained</code>] as <code>pretrained_model_name_or_path</code> if the configuration was created
with such a method.
output_hidden_states (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should return all hidden-states.
output_attentions (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should returns all attentions.
return_dict (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should return a [<code>~transformers.utils.ModelOutput</code>] instead of a plain tuple.
is_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether the model is used as an encoder/decoder or not.
is_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether to only use the decoder in an encoder-decoder architecture, otherwise it has no effect on
decoder-only or encoder-only architectures.
cross_attention_hidden_size (<code>bool</code>, <em>optional</em>):
The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder
setting and the cross-attention hidden dimension differs from <code>self.config.hidden_size</code>.
add_cross_attention (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether cross-attention layers should be added to the model. Note, this option is only relevant for models
that can be used as decoder models within the [<code>EncoderDecoderModel</code>] class, which consists of all models
in <code>AUTO_MODELS_FOR_CAUSAL_LM</code>.
tie_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder
and decoder model to have the exact same parameter names.
prune_heads (<code>dict[int, list[int]]</code>, <em>optional</em>, defaults to <code>{}</code>):
Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of
heads to prune in said layer.</p>
<pre><code>For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.
</code></pre>
<p>chunk_size_feed_forward (<code>int</code>, <em>optional</em>, defaults to <code>0</code>):
The chunk size of all feed forward layers in the residual attention blocks. A chunk size of <code>0</code> means that
the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes <code>n</code> &lt;
sequence_length embeddings at a time. For more information on feed forward chunking, see <a href="../glossary.html#feed-forward-chunking">How does Feed
Forward Chunking work?</a>.</p>
<blockquote>
<p>Parameters for fine-tuning tasks</p>
</blockquote>
<p>architectures (<code>list[str]</code>, <em>optional</em>):
Model architectures that can be used with the model pretrained weights.
finetuning_task (<code>str</code>, <em>optional</em>):
Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow
or PyTorch) checkpoint.
id2label (<code>dict[int, str]</code>, <em>optional</em>):
A map from index (for instance prediction index, or target index) to label.
label2id (<code>dict[str, int]</code>, <em>optional</em>):
A map from label to index for the model.
num_labels (<code>int</code>, <em>optional</em>):
Number of labels to use in the last layer added to the model, typically for a classification task.
task_specific_params (<code>dict[str, Any]</code>, <em>optional</em>):
Additional keyword arguments to store for the current task.
problem_type (<code>str</code>, <em>optional</em>):
Problem type for <code>XxxForSequenceClassification</code> models. Can be one of <code>"regression"</code>,
<code>"single_label_classification"</code> or <code>"multi_label_classification"</code>.</p>
<blockquote>
<p>Parameters linked to the tokenizer</p>
</blockquote>
<p>tokenizer_class (<code>str</code>, <em>optional</em>):
The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the
model by default).
prefix (<code>str</code>, <em>optional</em>):
A specific prompt that should be added at the beginning of each text before calling the model.
bos_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>beginning-of-stream</em> token.
pad_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>padding</em> token.
eos_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>end-of-stream</em> token.
decoder_start_token_id (<code>int</code>, <em>optional</em>):
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.
sep_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>separation</em> token.</p>
<blockquote>
<p>PyTorch specific parameters</p>
</blockquote>
<p>torchscript (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should be used with Torchscript.
tie_word_embeddings (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the
model has a output word embedding layer.
torch_dtype (<code>str</code>, <em>optional</em>):
The <code>dtype</code> of the weights. This attribute can be used to initialize the model to a non-default <code>dtype</code>
(which is normally <code>float32</code>) and thus allow for optimal storage allocation. For example, if the saved
model is <code>float16</code>, ideally we want to load it back using the minimal amount of memory needed to load
<code>float16</code> weights. Since the config object is stored in plain text, this attribute contains just the
floating type string without the <code>torch.</code> prefix. For example, for <code>torch.float16</code> <code><code>torch\_dtype&lt;/code&gt; is the</code>"float16"` string.</p>
<pre><code>This attribute is currently not being used during model loading time, but this may change in the future
versions. But we can already start preparing for the future by saving the dtype with save_pretrained.
</code></pre></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.minicpmo.ConditionalChatTTS.decode_to_mel_specs"><code class="name flex">
<span>def <span class="ident">decode_to_mel_specs</span></span>(<span>self, result_list: List[torch.Tensor])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.inference_mode()
def decode_to_mel_specs(
    self,
    result_list: List[torch.Tensor],
):
    &#34;&#34;&#34;Decode discrete audio codes to mel spectrograms.

    Borrowed from `https://github.com/2noise/ChatTTS/blob/main/ChatTTS/core.py`

    Args:
        result_list (List[torch.Tensor]): Audio codes output from `generate`.

    Returns:
        torch.Tensor: Mel spectrograms.
    &#34;&#34;&#34;

    decoder = self.dvae
    max_x_len = -1
    if len(result_list) == 0:
        return np.array([], dtype=np.float32)
    for result in result_list:
        if result.size(0) &gt; max_x_len:
            max_x_len = result.size(0)
    batch_result = torch.zeros(
        (len(result_list), result_list[0].size(1), max_x_len),
        dtype=result_list[0].dtype,
        device=result_list[0].device,
    )
    for i in range(len(result_list)):
        src = result_list[i]
        batch_result[i].narrow(1, 0, src.size(0)).copy_(src.permute(1, 0))
        del src

    mel_specs = decoder(batch_result)
    del batch_result
    return mel_specs</code></pre>
</details>
<div class="desc"><p>Decode discrete audio codes to mel spectrograms.</p>
<p>Borrowed from <code>https://github.com/2noise/ChatTTS/blob/main/ChatTTS/core.py</code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>result_list</code></strong> :&ensp;<code>List[torch.Tensor]</code></dt>
<dd>Audio codes output from <code>generate</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Mel spectrograms.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.models.minicpmo.ConditionalChatTTS.generate"><code class="name flex">
<span>def <span class="ident">generate</span></span>(<span>self,<br>input_ids: torch.Tensor,<br>past_key_values: List[Tuple[torch.Tensor, torch.Tensor]],<br>temperature: torch.Tensor,<br>eos_token: int | torch.Tensor,<br>streaming_tts_text_mask=None,<br>force_no_stop=False,<br>min_new_token=10,<br>max_new_token=50,<br>logits_warpers: List[None] = [],<br>logits_processors: List[<a title="sglang.srt.models.minicpmo.CustomRepetitionPenaltyLogitsProcessorRepeat" href="#sglang.srt.models.minicpmo.CustomRepetitionPenaltyLogitsProcessorRepeat">CustomRepetitionPenaltyLogitsProcessorRepeat</a>] = [],<br>show_tqdm=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.inference_mode()
def generate(
    self,
    input_ids: torch.Tensor,
    past_key_values: List[Tuple[torch.Tensor, torch.Tensor]],
    temperature: torch.Tensor,
    eos_token: Union[int, torch.Tensor],
    streaming_tts_text_mask=None,
    force_no_stop=False,
    min_new_token=10,
    max_new_token=50,
    logits_warpers: List[LogitsWarper] = [],
    logits_processors: List[CustomRepetitionPenaltyLogitsProcessorRepeat] = [],
    show_tqdm=False,
):
    &#34;&#34;&#34;Generate audio codes in streaming setting or non-streaming setting.
    Specifically speaking, generate audio codes when not all text tokens are prefilled.

    Always pass a valid `past_key_values` to the method. The method does not do `prefill` by itself. It relies on `prefill_text` method to provide valid `past_key_values`. Please refer to docstring of this class for more details.

    In this method, we borrowed a lot of codes from `https://github.com/2noise/ChatTTS/blob/main/ChatTTS/model/gpt.py`.

    Args:
        input_ids (torch.Tensor): Input token ids.
        past_key_values (List[Tuple[torch.Tensor, torch.Tensor]]): Past key values for attention mechanism.
        temperature (torch.Tensor): Temperature for sampling.
        eos_token (Union[int, torch.Tensor]): End of sequence token.
        streaming_tts_text_mask (Optional[torch.Tensor], optional): Mask for streaming TTS text. Defaults to None.
        max_new_token (int, optional): Maximum number of new tokens to generate. Defaults to 50.
        logits_warpers (List[LogitsWarper], optional): List of logits warpers. Defaults to [].
        logits_processors (List[CustomRepetitionPenaltyLogitsProcessorRepeat], optional): List of logits processors. Defaults to [].
        show_tqdm (bool, optional): Whether to show progress bar. Defaults to True.

    Returns:
        GenerationOutputs: Generation outputs.
    &#34;&#34;&#34;

    # We only support batch size `1` for now
    assert input_ids.shape[0] == 1
    assert past_key_values is not None

    # fix: this should not be `input_ids.shape[1]`
    # start_idx = input_ids.shape[1]
    start_idx = (
        1
        + self.num_spk_embs * self.use_speaker_embedding
        + self.streaming_text_reserved_len
        + 1
    )

    finish = torch.zeros(input_ids.shape[0], device=input_ids.device).bool()

    temperature = (
        temperature.unsqueeze(0)
        .expand(input_ids.shape[0], -1)
        .contiguous()
        .view(-1, 1)
    )

    progress = input_ids.shape[1]

    # Pre-allocate input_ids, shape is [batch_size=1, max_possible_seq_len, self.num_vqs]
    input_ids_buf = torch.zeros(
        input_ids.shape[0],  # batch_size
        progress
        + max_new_token,  # max_possible_seq_len = input_ids.shape[1] + max_new_token
        input_ids.shape[2],  # self.num_vqs
        dtype=input_ids.dtype,
        device=input_ids.device,
    )

    # Copy existing `input_ids` to `input_ids_buf`
    input_ids_buf.narrow(1, 0, progress).copy_(input_ids)

    del input_ids
    input_ids = input_ids_buf.narrow(1, 0, progress)

    pbar: Optional[tqdm] = None
    if show_tqdm:
        pbar = tqdm(
            total=max_new_token,
            desc=&#34;code&#34;,
            bar_format=&#34;{l_bar}{bar}| {n_fmt}/{total_fmt}(max) [{elapsed}, {rate_fmt}{postfix}]&#34;,
        )

    condition_length = (
        1
        + self.num_spk_embs * self.use_speaker_embedding
        + self.streaming_text_reserved_len
        + 1
    )

    for i in range(max_new_token):
        # Prepare generation inputs
        audio_bos = False

        # If this is the first audio token, the case is SPECIAL
        if progress == condition_length:
            audio_bos = True

        assert progress == (
            past_key_values[0][0].shape[2] + 1
        )  # If you are using according to the guidelines, this should be passed.

        if audio_bos:
            # Generate the first token, activate the model with `self.audio_bos_token_id`, the model will predict
            # a new audio token. This is a special case because without the `audio bos token`, it is impossible
            # to generate the first audio token in our streaming setting.
            narrowed_input_ids = torch.tensor(
                [[self.audio_bos_token_id]], dtype=torch.long, device=self.device
            )
            inputs_embeds = self.emb_text(narrowed_input_ids)
            del narrowed_input_ids
        else:
            # Generate the following audio tokens, it is applicable to all other cases, including second and the
            # following calling of `generate`.
            narrowed_input_ids = input_ids.narrow(
                dim=1, start=input_ids.shape[1] - 1, length=1
            )
            code_emb = [
                self.emb_code[i](narrowed_input_ids[:, :, i])
                for i in range(self.num_vq)
            ]
            inputs_embeds = torch.stack(code_emb, 3).sum(3)

        position_ids = torch.tensor(
            [past_key_values[0][0].shape[2]], dtype=torch.long, device=self.device
        ).unsqueeze(0)

        cache_position = position_ids.clone()

        # Make causal mask
        causal_mask = make_streaming_chunk_mask_generation(
            inputs_embeds=inputs_embeds,
            past_seen_tokens=past_key_values[0][0].shape[2],
            streaming_tts_text_mask=streaming_tts_text_mask,
            streaming_reserved_length=self.streaming_text_reserved_len,
            streaming_text_chunk_size=self.streaming_text_chunk_size,
        )

        # Model forward
        outputs: BaseModelOutputWithPast = self.model(
            attention_mask=causal_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=True,
            output_attentions=False,
            cache_position=cache_position,
        )

        del position_ids
        del inputs_embeds
        del cache_position
        del causal_mask

        hidden_states = outputs.last_hidden_state
        past_key_values = outputs.past_key_values

        with P.cached():
            logits = torch.empty(
                hidden_states.size(0),
                hidden_states.size(1),
                self.num_audio_tokens,
                self.num_vq,
                dtype=torch.float,
                device=self.device,
            )
            for num_vq_iter in range(self.num_vq):
                x: torch.Tensor = self.head_code[num_vq_iter](hidden_states)
                logits[..., num_vq_iter] = x
                del x

        del hidden_states

        # logits = logits[:, -1].float()
        logits = logits.narrow(1, -1, 1).squeeze_(1).float()

        # logits = rearrange(logits, &#34;b c n -&gt; (b n) c&#34;)
        logits = logits.permute(0, 2, 1)
        logits = logits.reshape(-1, logits.size(2))
        # logits_token = rearrange(input_ids[:, start_idx:], &#34;b c n -&gt; (b n) c&#34;)
        input_ids_sliced = input_ids.narrow(
            1,
            start_idx,
            input_ids.size(1) - start_idx,
        ).permute(0, 2, 1)
        logits_token = input_ids_sliced.reshape(
            input_ids_sliced.size(0) * input_ids_sliced.size(1),
            -1,
        ).to(self.device)
        del input_ids_sliced

        logits /= temperature

        if not audio_bos:
            for logitsProcessors in logits_processors:
                logits = logitsProcessors(logits_token, logits)
        if not audio_bos:
            for logitsWarpers in logits_warpers:
                logits = logitsWarpers(logits_token, logits)

        del logits_token

        if i &lt; min_new_token:
            logits[:, eos_token] = -torch.inf

        if force_no_stop:
            logits[:, eos_token] = -torch.inf

        scores = F.softmax(logits, dim=-1)

        del logits
        idx_next = torch.multinomial(scores, num_samples=1)  # .to(finish.device)

        del scores

        # idx_next = rearrange(idx_next, &#34;(b n) 1 -&gt; b n&#34;, n=self.num_vq)
        idx_next = idx_next.view(-1, self.num_vq)
        finish_or = idx_next.eq(eos_token).any(1)
        finish.logical_or_(finish_or)

        del finish_or
        # Store new `token` into `input_ids_buf`
        input_ids_buf.narrow(1, progress, 1).copy_(idx_next.unsqueeze_(1))

        if i == 0 and finish.any():
            # raise Exception
            break

        del idx_next
        progress += 1
        input_ids = input_ids_buf.narrow(1, 0, progress)

        if finish.all():
            break

        if pbar is not None:
            pbar.update(1)

    if pbar is not None:
        pbar.close()

    if not finish.all():
        if show_tqdm:
            logger.info(f&#34;incomplete result. hit max_new_token: {max_new_token}&#34;)

    del input_ids_buf

    if finish.all():
        # the last may contains eos token
        genrated_input_ids = input_ids[:, condition_length:-1, :]
    else:
        # there is no eos token
        genrated_input_ids = input_ids[:, condition_length:, :]

    return ConditionalChatTTSGenerationOutput(
        new_ids=genrated_input_ids,
        audio_input_ids=input_ids,  # for update purpose
        past_key_values=past_key_values,  # for update purpose
        finished=finish.all(),
    )</code></pre>
</details>
<div class="desc"><p>Generate audio codes in streaming setting or non-streaming setting.
Specifically speaking, generate audio codes when not all text tokens are prefilled.</p>
<p>Always pass a valid <code>past_key_values</code> to the method. The method does not do <code>prefill</code> by itself. It relies on <code>prefill_text</code> method to provide valid <code>past_key_values</code>. Please refer to docstring of this class for more details.</p>
<p>In this method, we borrowed a lot of codes from <code>https://github.com/2noise/ChatTTS/blob/main/ChatTTS/model/gpt.py</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_ids</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Input token ids.</dd>
<dt><strong><code>past_key_values</code></strong> :&ensp;<code>List[Tuple[torch.Tensor, torch.Tensor]]</code></dt>
<dd>Past key values for attention mechanism.</dd>
<dt><strong><code>temperature</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Temperature for sampling.</dd>
<dt><strong><code>eos_token</code></strong> :&ensp;<code>Union[int, torch.Tensor]</code></dt>
<dd>End of sequence token.</dd>
<dt><strong><code>streaming_tts_text_mask</code></strong> :&ensp;<code>Optional[torch.Tensor]</code>, optional</dt>
<dd>Mask for streaming TTS text. Defaults to None.</dd>
<dt><strong><code>max_new_token</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Maximum number of new tokens to generate. Defaults to 50.</dd>
<dt><strong><code>logits_warpers</code></strong> :&ensp;<code>List[LogitsWarper]</code>, optional</dt>
<dd>List of logits warpers. Defaults to [].</dd>
<dt><strong><code>logits_processors</code></strong> :&ensp;<code>List[<a title="sglang.srt.models.minicpmo.CustomRepetitionPenaltyLogitsProcessorRepeat" href="#sglang.srt.models.minicpmo.CustomRepetitionPenaltyLogitsProcessorRepeat">CustomRepetitionPenaltyLogitsProcessorRepeat</a>]</code>, optional</dt>
<dd>List of logits processors. Defaults to [].</dd>
<dt><strong><code>show_tqdm</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to show progress bar. Defaults to True.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>GenerationOutputs</code></dt>
<dd>Generation outputs.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.models.minicpmo.ConditionalChatTTS.merge_inputs_embeds"><code class="name flex">
<span>def <span class="ident">merge_inputs_embeds</span></span>(<span>self,<br>input_ids: torch.Tensor,<br>lm_spk_emb_last_hidden_states: torch.Tensor | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.inference_mode()
def merge_inputs_embeds(
    self,
    input_ids: torch.Tensor,
    lm_spk_emb_last_hidden_states: Optional[torch.Tensor] = None,
):
    &#34;&#34;&#34;Merge `input_ids` and `lm_spk_emb_last_hidden_states` to `inputs_embeds`.

    Args:
        input_ids (torch.Tensor): Input token IDs.
        lm_spk_emb_last_hidden_states (Optional[torch.Tensor], optional): Last hidden states of speaker embeddings from the language model. Defaults to None.

    Raises:
        NotImplementedError: If speaker embedding is not used and language model hidden states are not implemented.

    Returns:
        torch.Tensor: Prepared input embeddings for the model.
    &#34;&#34;&#34;
    assert input_ids.shape[0] == 1

    # Embed input_ids to input_embeds
    inputs_embeds = self.emb_text(input_ids)

    # Inject speaker embedding to input_embeds if it exists
    if self.use_speaker_embedding:
        spk_emb_mask = input_ids == self.spk_emb_token_id
        if spk_emb_mask.any():
            assert lm_spk_emb_last_hidden_states is not None
            # Project spk emb to tts hidden size first, [batch_size, num_spk_emb, llm_dim] -&gt; [batch_size, num_spk_emb, self.hidden_size]
            lm_spk_emb_last_hidden_states = lm_spk_emb_last_hidden_states.to(
                self.projector.linear1.weight.dtype
            )
            projected_spk_emb = self.projector(lm_spk_emb_last_hidden_states)
            projected_spk_emb = F.normalize(projected_spk_emb, p=2, dim=-1)
            apply_spk_emb(
                input_ids=input_ids,
                spk_emb=projected_spk_emb,
                input_embeds=inputs_embeds,
                spk_emb_token_id=self.spk_emb_token_id,
                num_spk_embs=self.num_spk_embs,
            )
    else:
        raise NotImplementedError

    return inputs_embeds</code></pre>
</details>
<div class="desc"><p>Merge <code>input_ids</code> and <code>lm_spk_emb_last_hidden_states</code> to <code>inputs_embeds</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_ids</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Input token IDs.</dd>
<dt><strong><code>lm_spk_emb_last_hidden_states</code></strong> :&ensp;<code>Optional[torch.Tensor]</code>, optional</dt>
<dd>Last hidden states of speaker embeddings from the language model. Defaults to None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>NotImplementedError</code></dt>
<dd>If speaker embedding is not used and language model hidden states are not implemented.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Prepared input embeddings for the model.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.models.minicpmo.ConditionalChatTTS.prefill_audio_ids"><code class="name flex">
<span>def <span class="ident">prefill_audio_ids</span></span>(<span>self,<br>input_ids: torch.Tensor,<br>past_key_values: List[Tuple[torch.Tensor, torch.Tensor]],<br>streaming_tts_text_mask=None,<br>add_audio_bos: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.inference_mode()
def prefill_audio_ids(
    self,
    input_ids: torch.Tensor,
    past_key_values: List[Tuple[torch.Tensor, torch.Tensor]],
    streaming_tts_text_mask=None,
    add_audio_bos: bool = True,
):
    &#34;&#34;&#34;Prefill a chunk of audio ids to the model. Used in sliding-window long audio generation.
    Specifically, prefill many audio ids (typically from last window) to the model in the new window.

    Args:
        input_ids (torch.Tensor): (1, seq_len, num_vq) Audio input token ids.
        past_key_values (List[Tuple[torch.Tensor, torch.Tensor]]): Past key values for attention mechanism.
    &#34;&#34;&#34;
    assert input_ids.shape[0] == 1
    assert past_key_values is not None

    code_emb = [self.emb_code[i](input_ids[:, :, i]) for i in range(self.num_vq)]
    inputs_embeds = torch.stack(code_emb, 3).sum(3)  # [1,seq_len,768]
    input_len = input_ids.shape[1]

    if add_audio_bos:
        narrowed_input_ids = torch.tensor(
            [[self.audio_bos_token_id]], dtype=torch.long, device=self.device
        )
        bos_inputs_embeds = self.emb_text(narrowed_input_ids)
        inputs_embeds = torch.cat([bos_inputs_embeds, inputs_embeds], dim=1)
        input_len += 1

    past_key_values_length = past_key_values[0][0].shape[2]
    position_ids = torch.arange(
        past_key_values_length,
        past_key_values_length + input_len,
        dtype=torch.long,
        device=self.device,
    ).unsqueeze(0)

    cache_position = position_ids.clone()
    causal_mask = make_streaming_chunk_mask_generation(
        inputs_embeds=inputs_embeds,
        past_seen_tokens=past_key_values[0][0].shape[2],
        streaming_tts_text_mask=streaming_tts_text_mask,
        streaming_reserved_length=self.streaming_text_reserved_len,
        streaming_text_chunk_size=self.streaming_text_chunk_size,
    )  # [1, 1, 1, past_key_values_length + input_len]

    # Model forward
    outputs: BaseModelOutputWithPast = self.model(
        attention_mask=causal_mask,
        position_ids=position_ids,
        past_key_values=past_key_values,
        inputs_embeds=inputs_embeds,
        use_cache=True,
        output_attentions=False,
        cache_position=cache_position,
    )
    past_key_values = outputs.past_key_values
    return past_key_values</code></pre>
</details>
<div class="desc"><p>Prefill a chunk of audio ids to the model. Used in sliding-window long audio generation.
Specifically, prefill many audio ids (typically from last window) to the model in the new window.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_ids</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>(1, seq_len, num_vq) Audio input token ids.</dd>
<dt><strong><code>past_key_values</code></strong> :&ensp;<code>List[Tuple[torch.Tensor, torch.Tensor]]</code></dt>
<dd>Past key values for attention mechanism.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.models.minicpmo.ConditionalChatTTS.prefill_text"><code class="name flex">
<span>def <span class="ident">prefill_text</span></span>(<span>self,<br>input_ids: torch.Tensor,<br>position_ids: torch.LongTensor,<br>past_key_values: List[Tuple[torch.Tensor, torch.Tensor]],<br>lm_spk_emb_last_hidden_states: torch.Tensor | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.inference_mode()
def prefill_text(
    self,
    input_ids: torch.Tensor,
    position_ids: torch.LongTensor,
    past_key_values: List[Tuple[torch.Tensor, torch.Tensor]],
    lm_spk_emb_last_hidden_states: Optional[torch.Tensor] = None,
):
    &#34;&#34;&#34;Prefill a chunk of new text tokens in streaming setting.
    Specifically speaking, update `past_key_values` using new text tokens, then the model will read the new text tokens.

    Args:
        input_ids (Tensor): Tensor of shape [batch_size, seq_len]
        position_ids (LongTensor): Tensor of shape [batch_size, seq_len]
        past_key_values (List[Tuple[Tensor]]): KV Cache of all layers, each layer is a tuple (Tensor, Tensor) denoting keys and values. Each tensor is of seq_len = `self.streaming_text_reserved_len`. `past_key_values` will be updated.
        lm_spk_emb_last_hidden_states (Tensor, optional): Tensor of shape [batch_size, num_spk_emb, llm_dim]. Defaults to None.

    Note that all `batch_size` should be `1`.
    &#34;&#34;&#34;
    assert input_ids.shape[0] == 1
    assert past_key_values is not None

    # Merge text and LLM embeddings
    inputs_embeds = self.merge_inputs_embeds(
        input_ids=input_ids,
        lm_spk_emb_last_hidden_states=lm_spk_emb_last_hidden_states,
    )

    # Clone KV Cache
    past_key_values_for_prefill = []
    for i in range(len(past_key_values)):
        past_key_values_for_prefill.append(
            (
                past_key_values[i][0][:, :, : position_ids[:, 0], :].clone(),
                past_key_values[i][1][:, :, : position_ids[:, 0], :].clone(),
            )
        )

    # ModelMiniCPMVBaseModel
    outputs_prefill: BaseModelOutputWithPast = self.model(
        attention_mask=None,  # because for text, it is standard causal attention mask, do nothing
        position_ids=position_ids,  # position_ids denotes the position of new text tokens in the sequence
        past_key_values=past_key_values_for_prefill,  # `past_key_values` will be updated by the model
        inputs_embeds=inputs_embeds,  # contains text and language model embedding
        use_cache=True,
        output_attentions=False,
        cache_position=position_ids,  # which new positions will use this cache, basically the same as position_ids
    )

    # Get model updated KV Cache
    past_key_values_for_prefill_updated = outputs_prefill.past_key_values

    # Update generated KV Cache to input `past_key_values`
    for layer_idx in range(len(past_key_values)):
        # Update keys
        past_key_values[layer_idx][0][
            :, :, position_ids[:, 0] : position_ids[:, -1] + 1, :
        ] = past_key_values_for_prefill_updated[layer_idx][0][
            :, :, position_ids[:, 0] : position_ids[:, -1] + 1
        ].clone()
        # Update values
        past_key_values[layer_idx][1][
            :, :, position_ids[:, 0] : position_ids[:, -1] + 1, :
        ] = past_key_values_for_prefill_updated[layer_idx][1][
            :, :, position_ids[:, 0] : position_ids[:, -1] + 1
        ].clone()

    # TODO: del past_key_values_for_prefill_updated recursively
    # TODO: del outputs_prefill recursively

    return past_key_values</code></pre>
</details>
<div class="desc"><p>Prefill a chunk of new text tokens in streaming setting.
Specifically speaking, update <code>past_key_values</code> using new text tokens, then the model will read the new text tokens.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_ids</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>Tensor of shape [batch_size, seq_len]</dd>
<dt><strong><code>position_ids</code></strong> :&ensp;<code>LongTensor</code></dt>
<dd>Tensor of shape [batch_size, seq_len]</dd>
<dt><strong><code>past_key_values</code></strong> :&ensp;<code>List[Tuple[Tensor]]</code></dt>
<dd>KV Cache of all layers, each layer is a tuple (Tensor, Tensor) denoting keys and values. Each tensor is of seq_len = <code>self.streaming_text_reserved_len</code>. <code>past_key_values</code> will be updated.</dd>
<dt><strong><code>lm_spk_emb_last_hidden_states</code></strong> :&ensp;<code>Tensor</code>, optional</dt>
<dd>Tensor of shape [batch_size, num_spk_emb, llm_dim]. Defaults to None.</dd>
</dl>
<p>Note that all <code>batch_size</code> should be <code>1</code>.</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.minicpmo.ConditionalChatTTSGenerationOutput"><code class="flex name class">
<span>class <span class="ident">ConditionalChatTTSGenerationOutput</span></span>
<span>(</span><span>new_ids: torch.LongTensor = None,<br>audio_input_ids: torch.LongTensor = None,<br>past_key_values: Tuple[Tuple[torch.FloatTensor]] | None = None,<br>finished: bool = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class ConditionalChatTTSGenerationOutput(ModelOutput):
    &#34;&#34;&#34;
    Output class for ConditionalChatTTS generation.

    Args:
        new_ids (torch.LongTensor): Newly generated audio code sequence, shape (batch_size, sequence_length, num_vq).
        audio_input_ids (torch.LongTensor): Updated input IDs including condition and generated audio codes, shape (batch_size, full_sequence_length, num_vq).
        past_key_values (Tuple[Tuple[torch.FloatTensor]]): Tuple containing pre-computed keys and values used for attention mechanism. Each element has shape (batch_size, num_heads, sequence_length, embed_size_per_head).
        finished (bool): Boolean indicating whether generation is complete.

    &#34;&#34;&#34;

    new_ids: torch.LongTensor = None
    audio_input_ids: torch.LongTensor = None
    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
    finished: bool = None</code></pre>
</details>
<div class="desc"><p>Output class for ConditionalChatTTS generation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>new_ids</code></strong> :&ensp;<code>torch.LongTensor</code></dt>
<dd>Newly generated audio code sequence, shape (batch_size, sequence_length, num_vq).</dd>
<dt><strong><code>audio_input_ids</code></strong> :&ensp;<code>torch.LongTensor</code></dt>
<dd>Updated input IDs including condition and generated audio codes, shape (batch_size, full_sequence_length, num_vq).</dd>
<dt><strong><code>past_key_values</code></strong> :&ensp;<code>Tuple[Tuple[torch.FloatTensor]]</code></dt>
<dd>Tuple containing pre-computed keys and values used for attention mechanism. Each element has shape (batch_size, num_heads, sequence_length, embed_size_per_head).</dd>
<dt><strong><code>finished</code></strong> :&ensp;<code>bool</code></dt>
<dd>Boolean indicating whether generation is complete.</dd>
</dl>
<p>Args:</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.utils.generic.ModelOutput</li>
<li>collections.OrderedDict</li>
<li>builtins.dict</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.models.minicpmo.ConditionalChatTTSGenerationOutput.audio_input_ids"><code class="name">var <span class="ident">audio_input_ids</span> : torch.LongTensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmo.ConditionalChatTTSGenerationOutput.finished"><code class="name">var <span class="ident">finished</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmo.ConditionalChatTTSGenerationOutput.new_ids"><code class="name">var <span class="ident">new_ids</span> : torch.LongTensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmo.ConditionalChatTTSGenerationOutput.past_key_values"><code class="name">var <span class="ident">past_key_values</span> : Tuple[Tuple[torch.FloatTensor]] | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.minicpmo.ConvNeXtBlock"><code class="flex name class">
<span>class <span class="ident">ConvNeXtBlock</span></span>
<span>(</span><span>dim: int,<br>intermediate_dim: int,<br>kernel: int,<br>dilation: int,<br>layer_scale_init_value: float = 1e-06)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConvNeXtBlock(nn.Module):
    def __init__(
        self,
        dim: int,
        intermediate_dim: int,
        kernel: int,
        dilation: int,
        layer_scale_init_value: float = 1e-6,
    ):
        # ConvNeXt Block copied from Vocos.
        super().__init__()
        self.dwconv = nn.Conv1d(
            dim,
            dim,
            kernel_size=kernel,
            padding=dilation * (kernel // 2),
            dilation=dilation,
            groups=dim,
        )

        self.norm = nn.LayerNorm(dim, eps=1e-6)
        self.pwconv1 = nn.Linear(dim, intermediate_dim)
        self.act = nn.GELU()
        self.pwconv2 = nn.Linear(intermediate_dim, dim)
        self.coef = (
            nn.Parameter(layer_scale_init_value * torch.ones(dim), requires_grad=True)
            if layer_scale_init_value &gt; 0
            else None
        )

    def forward(self, x: torch.Tensor, cond=None) -&gt; torch.Tensor:
        residual = x

        y = self.dwconv(x)
        y.transpose_(1, 2)  # (B, C, T) -&gt; (B, T, C)
        x = self.norm(y)
        del y
        y = self.pwconv1(x)
        del x
        x = self.act(y)
        del y
        y = self.pwconv2(x)
        del x
        if self.coef is not None:
            y *= self.coef
        y.transpose_(1, 2)  # (B, T, C) -&gt; (B, C, T)

        x = y + residual
        del y

        return x</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.minicpmo.ConvNeXtBlock.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: torch.Tensor, cond=None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x: torch.Tensor, cond=None) -&gt; torch.Tensor:
    residual = x

    y = self.dwconv(x)
    y.transpose_(1, 2)  # (B, C, T) -&gt; (B, T, C)
    x = self.norm(y)
    del y
    y = self.pwconv1(x)
    del x
    x = self.act(y)
    del y
    y = self.pwconv2(x)
    del x
    if self.coef is not None:
        y *= self.coef
    y.transpose_(1, 2)  # (B, T, C) -&gt; (B, C, T)

    x = y + residual
    del y

    return x</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.minicpmo.CustomRepetitionPenaltyLogitsProcessorRepeat"><code class="flex name class">
<span>class <span class="ident">CustomRepetitionPenaltyLogitsProcessorRepeat</span></span>
<span>(</span><span>penalty: float, max_input_ids: int, past_window: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CustomRepetitionPenaltyLogitsProcessorRepeat:
    def __init__(self, penalty: float, max_input_ids: int, past_window: int):
        if not isinstance(penalty, float) or not (penalty &gt; 0):
            raise ValueError(
                f&#34;`penalty` has to be a strictly positive float, but is {penalty}&#34;
            )

        self.penalty = penalty
        self.max_input_ids = max_input_ids
        self.past_window = past_window

    def __call__(
        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
    ) -&gt; torch.FloatTensor:
        if input_ids.size(1) &gt; self.past_window:
            input_ids = input_ids.narrow(1, -self.past_window, self.past_window)
        freq = F.one_hot(input_ids, scores.size(1)).sum(1)
        if freq.size(0) &gt; self.max_input_ids:
            freq.narrow(
                0, self.max_input_ids, freq.size(0) - self.max_input_ids
            ).zero_()
        alpha = torch.pow(self.penalty, freq)
        scores = scores.contiguous()
        inp = scores.multiply(alpha)
        oth = scores.divide(alpha)
        con = scores &lt; 0
        out = torch.where(con, inp, oth)
        del inp, oth, scores, con, alpha
        return out</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmo.DVAE"><code class="flex name class">
<span>class <span class="ident">DVAE</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DVAE(nn.Module):
    def __init__(
        self,
    ):
        super().__init__()

        coef = torch.rand(100)
        self.coef = nn.Parameter(coef.unsqueeze(0).unsqueeze_(2))

        self.downsample_conv = nn.Sequential(
            nn.Conv1d(100, 512, 3, 1, 1),
            nn.GELU(),
            nn.Conv1d(512, 512, 4, 2, 1),
            nn.GELU(),
        )

        self.encoder = DVAEDecoder(
            idim=512,
            odim=1024,
            hidden=256,
            n_layer=12,
            bn_dim=128,
        )

        self.decoder = DVAEDecoder(
            idim=512,
            odim=512,
            hidden=256,
            n_layer=12,
            bn_dim=128,
        )

        self.out_conv = nn.Conv1d(512, 100, 3, 1, 1, bias=False)

        self.vq_layer = GFSQ(
            dim=1024,
            levels=(5, 5, 5, 5),
            G=2,
            R=2,
        )

    @torch.inference_mode()
    def forward(
        self, inp: torch.Tensor, mode: Literal[&#34;encode&#34;, &#34;decode&#34;] = &#34;decode&#34;
    ) -&gt; torch.Tensor:
        if mode == &#34;encode&#34; and hasattr(self, &#34;encoder&#34;) and self.vq_layer is not None:
            mel = inp.clone()
            x: torch.Tensor = self.downsample_conv(
                torch.div(mel, self.coef.view(100, 1).expand(mel.shape), out=mel),
            ).unsqueeze_(0)
            del mel
            x = self.encoder(x)
            ind = self.vq_layer(x)
            del x
            return ind

        if self.vq_layer is not None:
            vq_feats = self.vq_layer._embed(inp)
        else:
            vq_feats = inp

        vq_feats = (
            vq_feats.view(
                (vq_feats.size(0), 2, vq_feats.size(1) // 2, vq_feats.size(2)),
            )
            .permute(0, 2, 3, 1)
            .flatten(2)
        )

        dec_out = self.out_conv(
            self.decoder(
                x=vq_feats,
            ),
        )

        del vq_feats

        return torch.mul(dec_out, self.coef, out=dec_out)</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.minicpmo.DVAE.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, inp: torch.Tensor, mode: Literal['encode', 'decode'] = 'decode') ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.inference_mode()
def forward(
    self, inp: torch.Tensor, mode: Literal[&#34;encode&#34;, &#34;decode&#34;] = &#34;decode&#34;
) -&gt; torch.Tensor:
    if mode == &#34;encode&#34; and hasattr(self, &#34;encoder&#34;) and self.vq_layer is not None:
        mel = inp.clone()
        x: torch.Tensor = self.downsample_conv(
            torch.div(mel, self.coef.view(100, 1).expand(mel.shape), out=mel),
        ).unsqueeze_(0)
        del mel
        x = self.encoder(x)
        ind = self.vq_layer(x)
        del x
        return ind

    if self.vq_layer is not None:
        vq_feats = self.vq_layer._embed(inp)
    else:
        vq_feats = inp

    vq_feats = (
        vq_feats.view(
            (vq_feats.size(0), 2, vq_feats.size(1) // 2, vq_feats.size(2)),
        )
        .permute(0, 2, 3, 1)
        .flatten(2)
    )

    dec_out = self.out_conv(
        self.decoder(
            x=vq_feats,
        ),
    )

    del vq_feats

    return torch.mul(dec_out, self.coef, out=dec_out)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.minicpmo.DVAEDecoder"><code class="flex name class">
<span>class <span class="ident">DVAEDecoder</span></span>
<span>(</span><span>idim: int, odim: int, n_layer=12, bn_dim=64, hidden=256, kernel=7, dilation=2, up=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DVAEDecoder(nn.Module):
    def __init__(
        self,
        idim: int,
        odim: int,
        n_layer=12,
        bn_dim=64,
        hidden=256,
        kernel=7,
        dilation=2,
        up=False,
    ):
        super().__init__()
        self.up = up
        self.conv_in = nn.Sequential(
            nn.Conv1d(idim, bn_dim, 3, 1, 1),
            nn.GELU(),
            nn.Conv1d(bn_dim, hidden, 3, 1, 1),
        )
        self.decoder_block = nn.ModuleList(
            [
                ConvNeXtBlock(
                    hidden,
                    hidden * 4,
                    kernel,
                    dilation,
                )
                for _ in range(n_layer)
            ]
        )
        self.conv_out = nn.Conv1d(hidden, odim, kernel_size=1, bias=False)

    def forward(self, x: torch.Tensor, conditioning=None) -&gt; torch.Tensor:
        # B, C, T
        y = self.conv_in(x)
        del x
        for f in self.decoder_block:
            y = f(y, conditioning)

        x = self.conv_out(y)
        del y
        return x</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.minicpmo.DVAEDecoder.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: torch.Tensor, conditioning=None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x: torch.Tensor, conditioning=None) -&gt; torch.Tensor:
    # B, C, T
    y = self.conv_in(x)
    del x
    for f in self.decoder_block:
        y = f(y, conditioning)

    x = self.conv_out(y)
    del y
    return x</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.minicpmo.GFSQ"><code class="flex name class">
<span>class <span class="ident">GFSQ</span></span>
<span>(</span><span>dim: int, levels: List[int], G: int, R: int, eps=1e-05, transpose=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GFSQ(nn.Module):
    def __init__(
        self,
        dim: int,
        levels: List[int],
        G: int,
        R: int,
        eps=1e-5,
        transpose=True,
    ):
        super(GFSQ, self).__init__()
        self.quantizer = GroupedResidualFSQ(
            dim=dim,
            levels=list(levels),
            num_quantizers=R,
            groups=G,
        )
        self.n_ind = math.prod(levels)
        self.eps = eps
        self.transpose = transpose
        self.G = G
        self.R = R

    def _embed(self, x: torch.Tensor):
        if self.transpose:
            x = x.transpose(1, 2)
        x = x.view(x.size(0), x.size(1), self.G, self.R).permute(2, 0, 1, 3)
        feat = self.quantizer.get_output_from_indices(x)
        return feat.transpose_(1, 2) if self.transpose else feat

    def __call__(self, x: torch.Tensor) -&gt; torch.Tensor:
        return super().__call__(x)

    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        if self.transpose:
            x.transpose_(1, 2)
        _, ind = self.quantizer(x)
        ind = ind.permute(1, 2, 0, 3).contiguous()
        ind = ind.view(ind.size(0), ind.size(1), -1)
        return ind.transpose_(1, 2) if self.transpose else ind</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.minicpmo.GFSQ.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
    if self.transpose:
        x.transpose_(1, 2)
    _, ind = self.quantizer(x)
    ind = ind.permute(1, 2, 0, 3).contiguous()
    ind = ind.view(ind.size(0), ind.size(1), -1)
    return ind.transpose_(1, 2) if self.transpose else ind</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.minicpmo.MiniCPMO"><code class="flex name class">
<span>class <span class="ident">MiniCPMO</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MiniCPMO(MiniCPMBaseModel):
    def __init__(
        self,
        config: PretrainedConfig,
        quant_config: Optional[QuantizationConfig] = None,
    ) -&gt; None:
        super().__init__(config=config, quant_config=quant_config)

        self.llm = self.init_llm(config=config, quant_config=quant_config)

        self.embed_dim = self.llm.config.hidden_size

        # init vision module
        if self.config.init_vision:
            # print(&#34;vision-understanding enabled&#34;)
            self.vpm = self.init_vision_module(config=config, quant_config=quant_config)
            self.vision_dim = self.vpm.embed_dim
            self.resampler = self.init_resampler(self.embed_dim, self.vision_dim)

        # init audio module
        self.config.init_audio = True
        if self.config.init_audio:
            # print(&#34;audio-understanding enabled&#34;)
            self.apm = self.init_audio_module()
            audio_output_dim = int(self.apm.config.encoder_ffn_dim // 4)
            self.audio_avg_pooler = nn.AvgPool1d(
                self.config.audio_pool_step, stride=self.config.audio_pool_step
            )
            self.audio_projection_layer = MultiModalProjector(
                in_dim=audio_output_dim, out_dim=self.embed_dim
            )
            self.audio_encoder_layer = -1

        # init tts module
        self.config.init_tts = False
        logger.info(&#34;TTS is disabled for now&#34;)
        if self.config.init_tts:
            # print(&#34;tts enabled&#34;)
            assert (
                _tts_deps
            ), &#34;please make sure vector_quantize_pytorch and vocos are installed.&#34;
            self.tts = self.init_tts_module()

    def init_tts_module(self):
        model = ConditionalChatTTS(self.config.tts_config)
        return model

    def init_audio_module(self):
        model = MiniCPMWhisperEncoder(self.config.audio_config)
        return model

    def init_llm(
        self,
        config: PretrainedConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; nn.Module:
        return Qwen2ForCausalLM(config=config, quant_config=quant_config, prefix=prefix)

    def init_vision_module(
        self,
        config: PretrainedConfig,
        quant_config: Optional[QuantizationConfig],
        prefix: str = &#34;&#34;,
    ):
        if self.config._attn_implementation == &#34;flash_attention_2&#34;:
            self.config.vision_config._attn_implementation = &#34;flash_attention_2&#34;
        else:
            self.config.vision_config._attn_implementation = &#34;eager&#34;
        model = Idefics2VisionTransformer(
            config=config.vision_config, quant_config=quant_config, prefix=prefix
        )
        if self.config.drop_vision_last_layer:
            model.encoder.layers = model.encoder.layers[:-1]

        setattr(model, &#34;embed_dim&#34;, model.embeddings.embed_dim)
        setattr(model, &#34;patch_size&#34;, model.embeddings.patch_size)

        return model

    def init_resampler(
        self,
        embed_dim: int,
        vision_dim: int,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; nn.Module:
        with set_default_torch_dtype(torch.float16):
            # The resampler in 2.6 remains consistent with the one in 2.5.
            resampler = Resampler2_5(
                num_queries=self.config.query_num,
                embed_dim=embed_dim,
                num_heads=embed_dim // 128,
                kv_dim=vision_dim,
                quant_config=quant_config,
                prefix=prefix,
            )

        return resampler.to(device=&#34;cuda&#34;, dtype=torch.get_default_dtype())

    def pad_input_ids(self, input_ids: List[int], mm_input: MultimodalInputs):
        # Get all special token IDs
        im_start_id: int = mm_input.im_start_id
        im_end_id: int = mm_input.im_end_id
        slice_start_id: int = mm_input.slice_start_id
        slice_end_id: int = mm_input.slice_end_id

        data_token_pairs = [
            (im_start_id, im_end_id),
            (slice_start_id, slice_end_id),
            (mm_input.audio_start_id, mm_input.audio_end_id),
        ]
        data_start_token_ids = [im_start_id, mm_input.audio_start_id]
        pattern = MultiModalityDataPaddingPatternTokenPairs(
            data_token_pairs=data_token_pairs, data_start_token_ids=data_start_token_ids
        )

        return pattern.pad_input_tokens(input_ids, mm_input)

    def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):
        &#34;&#34;&#34;
        Computes the output length of the convolutional layers and the output length of the audio encoder
        &#34;&#34;&#34;
        input_lengths_after_cnn = (input_lengths - 1) // 2 + 1
        input_lengths_after_pooling = (
            input_lengths_after_cnn - self.config.audio_pool_step
        ) // self.config.audio_pool_step + 1
        input_lengths_after_pooling = input_lengths_after_pooling.to(dtype=torch.int32)

        return input_lengths_after_cnn, input_lengths_after_pooling

    def get_audio_embedding_streaming(self, items: List[MultimodalDataItem]):
        r&#34;&#34;&#34;
        Extract audio embeddings in a streaming manner using cached key-value pairs.

        This method processes incoming audio features incrementally and stores/updates `past_key_values`
        for faster inference on subsequent audio frames. It only supports batch_size=1 and is intended
        for streaming scenarios.

        Returns:
            List[List[torch.Tensor]]: audio embeddings
        &#34;&#34;&#34;
        wavforms = flatten_nested_list([item.feature for item in items if item.feature])
        # list, [[x1, x2], [y1], [z1]]
        audio_feature_lens_raw = flatten_nested_list(
            [item.audio_feature_lens for item in items if item.audio_feature_lens]
        )

        # exist audio
        if len(wavforms) &gt; 0:
            audio_feature_lens = torch.hstack(audio_feature_lens_raw)
            batch_size, _, max_mel_seq_len = wavforms.shape
            assert batch_size == 1
            max_seq_len = (max_mel_seq_len - 1) // 2 + 1

            if self.audio_past_key_values is not None:
                cache_length = self.audio_past_key_values[0][0].shape[2]
                apm_max_len = self.apm.embed_positions.weight.shape[0]
                if cache_length + max_seq_len &gt;= apm_max_len:
                    logger.warning(
                        f&#34;audio_past_key_values length {cache_length + max_seq_len} exceed {apm_max_len}, reset.&#34;
                    )
                    self.audio_past_key_values = None

            audio_outputs = self.apm(
                wavforms, past_key_values=self.audio_past_key_values, use_cache=True
            )
            audio_states = (
                audio_outputs.last_hidden_state
            )  # [:, :audio_feat_lengths, :]
            self.audio_past_key_values = audio_outputs.past_key_values

            audio_embeds = self.audio_projection_layer(audio_states)

            audio_embeds = audio_embeds.transpose(1, 2)
            audio_embeds = self.audio_avg_pooler(audio_embeds)
            audio_embeds = audio_embeds.transpose(1, 2)

            _, feature_lens_after_pooling = self._get_feat_extract_output_lengths(
                audio_feature_lens
            )

            num_audio_tokens = feature_lens_after_pooling

            final_audio_embeds = []
            idx = 0
            for i in range(len(audio_feature_lens_raw)):
                target_audio_embeds = []
                for _ in range(len(audio_feature_lens_raw[i])):
                    target_audio_embeds.append(
                        audio_embeds[idx, : num_audio_tokens[idx], :]
                    )
                    idx += 1
                final_audio_embeds.append(target_audio_embeds)
            return final_audio_embeds
        else:
            return []

    def subsequent_chunk_mask(
        self,
        size: int,
        chunk_size: int,
        num_left_chunks: int = -1,
        device: torch.device = torch.device(&#34;cpu&#34;),
        num_lookhead: int = 0,
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;Create mask for subsequent steps (size, size) with chunk size,
        this is for streaming encoder

        Args:
            size (int): size of mask
            chunk_size (int): size of chunk
            num_left_chunks (int): number of left chunks
                &lt;0: use full chunk
                &gt;=0: use num_left_chunks
            device (torch.device): &#34;cpu&#34; or &#34;cuda&#34; or torch.Tensor.device

        Returns:
            torch.Tensor: mask

        &#34;&#34;&#34;
        ret = torch.zeros(size, size, device=device, dtype=torch.bool)
        for i in range(size):
            if num_left_chunks &lt; 0:
                start = 0
            else:
                start = max((i // chunk_size - num_left_chunks) * chunk_size, 0)
            ending = min((i // chunk_size + 1) * chunk_size + num_lookhead, size)
            ret[i, start:ending] = True
        return ret

    def get_audio_embedding(self, items: List[MultimodalDataItem], chunk_length=-1):
        r&#34;&#34;&#34;
        Extract full audio embeddings with optional chunk-based attention.

        This method computes embeddings for all audio frames at once, either using full attention (when
        `chunk_length` is -1) or chunk-based attention (when `chunk_length` is a positive number). It does
        not use key-value caching and is suitable for non-streaming inference.

        Args:
            chunk_length (int, optional): Determines whether to use full attention (-1) or chunk-based
                attention (&gt;0) during embedding computation.

        Returns:
            List[List[torch.Tensor]]: audio embeddings
        &#34;&#34;&#34;
        # (bs, 80, frames) or [], multi audios need filled in advance
        wavforms = flatten_nested_list([item.feature for item in items if item.feature])
        # list, [[x1, x2], [y1], [z1]]
        audio_feature_lens_raw = flatten_nested_list(
            [item.audio_feature_lens for item in items if item.audio_feature_lens]
        )

        final_audio_embeds = []

        assert isinstance(wavforms, list)
        assert isinstance(wavforms[0], torch.Tensor)
        # exist audio
        for wavform in wavforms:
            if len(wavform) &gt; 0:
                audio_feature_lens = torch.hstack(audio_feature_lens_raw)
                batch_size, _, max_mel_seq_len = wavform.shape
                max_seq_len = (max_mel_seq_len - 1) // 2 + 1

                # Create a sequence tensor of shape (batch_size, max_seq_len)
                seq_range = (
                    torch.arange(
                        0,
                        max_seq_len,
                        dtype=audio_feature_lens.dtype,
                        device=audio_feature_lens.device,
                    )
                    .unsqueeze(0)
                    .expand(batch_size, max_seq_len)
                )
                lengths_expand = audio_feature_lens.unsqueeze(1).expand(
                    batch_size, max_seq_len
                )
                # Create mask
                padding_mask = seq_range &gt;= lengths_expand  # 1 for padded values

                audio_attention_mask_ = padding_mask.view(
                    batch_size, 1, 1, max_seq_len
                ).expand(batch_size, 1, max_seq_len, max_seq_len)
                audio_attention_mask = audio_attention_mask_.to(
                    dtype=self.apm.conv1.weight.dtype,
                    device=self.apm.conv1.weight.device,
                )

                if chunk_length &gt; 0:
                    chunk_num_frame = int(chunk_length * 50)
                    chunk_mask = self.subsequent_chunk_mask(
                        size=max_seq_len,
                        chunk_size=chunk_num_frame,
                        num_left_chunks=-1,
                        device=audio_attention_mask_.device,
                    )
                    audio_attention_mask_ = torch.logical_or(
                        audio_attention_mask_, torch.logical_not(chunk_mask)
                    )

                audio_attention_mask[audio_attention_mask_] = float(&#34;-inf&#34;)
                audio_states = self.apm(
                    wavform,
                    output_hidden_states=True,
                    attention_mask=audio_attention_mask,
                ).hidden_states[self.audio_encoder_layer]
                audio_embeds = self.audio_projection_layer(audio_states)

                audio_embeds = audio_embeds.transpose(1, 2)
                audio_embeds = self.audio_avg_pooler(audio_embeds)
                audio_embeds = audio_embeds.transpose(1, 2)

                _, feature_lens_after_pooling = self._get_feat_extract_output_lengths(
                    audio_feature_lens
                )

                num_audio_tokens = feature_lens_after_pooling

                idx = 0
                for i in range(len(audio_feature_lens_raw)):
                    target_audio_embeds = []
                    for _ in range(len(audio_feature_lens_raw[i])):
                        target_audio_embeds.append(
                            audio_embeds[idx, : num_audio_tokens[idx], :]
                        )
                        idx += 1
                    final_audio_embeds.append(target_audio_embeds)
            return final_audio_embeds

    def get_audio_feature(self, items: List[MultimodalDataItem]) -&gt; torch.Tensor:
        embedding = self.get_omni_embedding(
            items=items,
            chunk_length=self.config.audio_chunk_length,
            stream_input=False,
        )
        return embedding

    def get_omni_embedding(
        self,
        items: List[MultimodalDataItem],
        chunk_length=-1,
        stream_input=False,
    ):
        &#34;&#34;&#34;
        Args:
            chunk_length: whisper use full attention or chunk attention
            stream_input: use streaming audio embedding
        Returns:
            final embeddings with audio feature
        &#34;&#34;&#34;

        if stream_input:
            audio_embeddings = self.get_audio_embedding_streaming(items)
        else:
            audio_embeddings = self.get_audio_embedding(items, chunk_length)
        bs = len(audio_embeddings)
        # batch size
        audio_embs = torch.cat(flatten_nested_list(audio_embeddings), dim=0)

        return audio_embs

    def get_image_feature(self, items: List[MultimodalDataItem]) -&gt; torch.Tensor:
        # list of tensors
        pixel_values = flatten_nested_list([item.feature for item in items])
        tgt_sizes = torch.stack(
            flatten_nested_list([item.tgt_size for item in items]), dim=0
        )
        assert len(pixel_values) == tgt_sizes.shape[0]

        device = self.vpm.embeddings.position_embedding.weight.device
        dtype = self.vpm.embeddings.position_embedding.weight.dtype
        all_pixel_values_lst = [
            i.flatten(end_dim=1).permute(1, 0) for i in pixel_values
        ]

        max_patches = (tgt_sizes[:, 0] * tgt_sizes[:, 1]).max().item()
        assert isinstance(max_patches, int)
        all_pixel_values = torch.nn.utils.rnn.pad_sequence(
            all_pixel_values_lst, batch_first=True, padding_value=0.0
        )

        B, L, _ = all_pixel_values.shape
        all_pixel_values = all_pixel_values.permute(0, 2, 1).reshape(B, 3, -1, L)
        patch_attn_mask = torch.zeros(
            (B, 1, max_patches), dtype=torch.bool, device=device
        )

        tgt_sizes_tensor = tgt_sizes.clone().to(device=patch_attn_mask.device)
        mask_shapes = tgt_sizes_tensor[:, 0] * tgt_sizes_tensor[:, 1]
        patch_attn_mask[:, 0, :] = torch.arange(
            patch_attn_mask.size(2), device=patch_attn_mask.device
        ).unsqueeze(0) &lt; mask_shapes.unsqueeze(1)

        vision_embedding = self.vpm(
            all_pixel_values.type(dtype),
            patch_attention_mask=patch_attn_mask,
            tgt_sizes=tgt_sizes,
        )
        return self.resampler(vision_embedding, tgt_sizes)

    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        forward_batch: ForwardBatch,
        **kwargs: Any,
    ) -&gt; torch.Tensor:

        hidden_states = general_mm_embed_routine(
            input_ids=input_ids,
            forward_batch=forward_batch,
            language_model=self.llm,
            multimodal_model=self,
            positions=positions,
        )
        return hidden_states

    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
        stacked_params_mapping = [
            # (param_name, shard_name, shard_id)
            (&#34;qkv_proj&#34;, &#34;q_proj&#34;, &#34;q&#34;),
            (&#34;qkv_proj&#34;, &#34;k_proj&#34;, &#34;k&#34;),
            (&#34;qkv_proj&#34;, &#34;v_proj&#34;, &#34;v&#34;),
            (&#34;gate_up_proj&#34;, &#34;gate_proj&#34;, 0),
            (&#34;gate_up_proj&#34;, &#34;up_proj&#34;, 1),
        ]

        params_dict = dict(self.named_parameters())
        for name, loaded_weight in weights:

            if &#34;rotary_emb.inv_freq~&#34; in name or &#34;projector&#34; in name:
                continue
            if &#34;rotary_emb.cos_cached&#34; in name or &#34;rotary_emb.sin_cached&#34; in name:
                # Models trained using ColossalAI may include these tensors in
                # the checkpoint. Skip them.
                continue

            # For weight_norm parametrization, handle both old and new formats
            if self.config.init_tts and &#34;tts&#34; in name:
                # Handle loading from older checkpoints with weight_g/weight_v format
                if &#34;.weight_g&#34; in name or &#34;.weight_v&#34; in name:
                    name = name.replace(
                        &#34;.weight_g&#34;, &#34;.parametrizations.weight.original0&#34;
                    )
                    name = name.replace(
                        &#34;.weight_v&#34;, &#34;.parametrizations.weight.original1&#34;
                    )
                elif &#34;.weight&#34; in name and name not in params_dict:
                    param_name = name.replace(
                        &#34;.weight&#34;, &#34;.parametrizations.weight.original0&#34;
                    )
                    if param_name in params_dict:
                        name = param_name

            # adapt to VisionAttention
            if &#34;vpm&#34; in name:
                name = name.replace(r&#34;self_attn.out_proj&#34;, r&#34;self_attn.proj&#34;)

            if not self.config.init_tts and &#34;tts&#34; in name:
                continue
            if not self.config.init_audio and (&#34;apm&#34; in name or &#34;audio&#34; in name):
                continue
            if not self.config.init_vision and &#34;vpm&#34; in name:
                continue

            if (
                &#34;sampler&#34; in name
                or &#34;apm&#34; in name
                or (&#34;tts&#34; in name and &#34;self_attn&#34; in name)
                or (&#34;tts.model.layers&#34; in name and &#34;.mlp&#34; in name)
            ):
                param = params_dict[name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(param, loaded_weight)
                continue

            for param_name, weight_name, shard_id in stacked_params_mapping:
                # replace the name and load with customized loader
                if weight_name not in name:
                    continue
                name = name.replace(weight_name, param_name)
                # # Skip loading extra bias for GPTQ models.
                if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                    continue
                param = params_dict[name]
                weight_loader = param.weight_loader
                weight_loader(param, loaded_weight, shard_id)
                break
            else:
                # Skip loading extra bias for GPTQ models.
                if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                    continue
                param = params_dict[name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(param, loaded_weight)</code></pre>
</details>
<div class="desc"><p>The abstract class of MiniCPMV can only be inherited, but cannot be
instantiated.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.models.minicpmv.MiniCPMBaseModel" href="minicpmv.html#sglang.srt.models.minicpmv.MiniCPMBaseModel">MiniCPMBaseModel</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.minicpmo.MiniCPMO.get_audio_embedding"><code class="name flex">
<span>def <span class="ident">get_audio_embedding</span></span>(<span>self,<br>items: List[<a title="sglang.srt.managers.schedule_batch.MultimodalDataItem" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.MultimodalDataItem">MultimodalDataItem</a>],<br>chunk_length=-1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_audio_embedding(self, items: List[MultimodalDataItem], chunk_length=-1):
    r&#34;&#34;&#34;
    Extract full audio embeddings with optional chunk-based attention.

    This method computes embeddings for all audio frames at once, either using full attention (when
    `chunk_length` is -1) or chunk-based attention (when `chunk_length` is a positive number). It does
    not use key-value caching and is suitable for non-streaming inference.

    Args:
        chunk_length (int, optional): Determines whether to use full attention (-1) or chunk-based
            attention (&gt;0) during embedding computation.

    Returns:
        List[List[torch.Tensor]]: audio embeddings
    &#34;&#34;&#34;
    # (bs, 80, frames) or [], multi audios need filled in advance
    wavforms = flatten_nested_list([item.feature for item in items if item.feature])
    # list, [[x1, x2], [y1], [z1]]
    audio_feature_lens_raw = flatten_nested_list(
        [item.audio_feature_lens for item in items if item.audio_feature_lens]
    )

    final_audio_embeds = []

    assert isinstance(wavforms, list)
    assert isinstance(wavforms[0], torch.Tensor)
    # exist audio
    for wavform in wavforms:
        if len(wavform) &gt; 0:
            audio_feature_lens = torch.hstack(audio_feature_lens_raw)
            batch_size, _, max_mel_seq_len = wavform.shape
            max_seq_len = (max_mel_seq_len - 1) // 2 + 1

            # Create a sequence tensor of shape (batch_size, max_seq_len)
            seq_range = (
                torch.arange(
                    0,
                    max_seq_len,
                    dtype=audio_feature_lens.dtype,
                    device=audio_feature_lens.device,
                )
                .unsqueeze(0)
                .expand(batch_size, max_seq_len)
            )
            lengths_expand = audio_feature_lens.unsqueeze(1).expand(
                batch_size, max_seq_len
            )
            # Create mask
            padding_mask = seq_range &gt;= lengths_expand  # 1 for padded values

            audio_attention_mask_ = padding_mask.view(
                batch_size, 1, 1, max_seq_len
            ).expand(batch_size, 1, max_seq_len, max_seq_len)
            audio_attention_mask = audio_attention_mask_.to(
                dtype=self.apm.conv1.weight.dtype,
                device=self.apm.conv1.weight.device,
            )

            if chunk_length &gt; 0:
                chunk_num_frame = int(chunk_length * 50)
                chunk_mask = self.subsequent_chunk_mask(
                    size=max_seq_len,
                    chunk_size=chunk_num_frame,
                    num_left_chunks=-1,
                    device=audio_attention_mask_.device,
                )
                audio_attention_mask_ = torch.logical_or(
                    audio_attention_mask_, torch.logical_not(chunk_mask)
                )

            audio_attention_mask[audio_attention_mask_] = float(&#34;-inf&#34;)
            audio_states = self.apm(
                wavform,
                output_hidden_states=True,
                attention_mask=audio_attention_mask,
            ).hidden_states[self.audio_encoder_layer]
            audio_embeds = self.audio_projection_layer(audio_states)

            audio_embeds = audio_embeds.transpose(1, 2)
            audio_embeds = self.audio_avg_pooler(audio_embeds)
            audio_embeds = audio_embeds.transpose(1, 2)

            _, feature_lens_after_pooling = self._get_feat_extract_output_lengths(
                audio_feature_lens
            )

            num_audio_tokens = feature_lens_after_pooling

            idx = 0
            for i in range(len(audio_feature_lens_raw)):
                target_audio_embeds = []
                for _ in range(len(audio_feature_lens_raw[i])):
                    target_audio_embeds.append(
                        audio_embeds[idx, : num_audio_tokens[idx], :]
                    )
                    idx += 1
                final_audio_embeds.append(target_audio_embeds)
        return final_audio_embeds</code></pre>
</details>
<div class="desc"><p>Extract full audio embeddings with optional chunk-based attention.</p>
<p>This method computes embeddings for all audio frames at once, either using full attention (when
<code>chunk_length</code> is -1) or chunk-based attention (when <code>chunk_length</code> is a positive number). It does
not use key-value caching and is suitable for non-streaming inference.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>chunk_length</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Determines whether to use full attention (-1) or chunk-based
attention (&gt;0) during embedding computation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[List[torch.Tensor]]</code></dt>
<dd>audio embeddings</dd>
</dl></div>
</dd>
<dt id="sglang.srt.models.minicpmo.MiniCPMO.get_audio_embedding_streaming"><code class="name flex">
<span>def <span class="ident">get_audio_embedding_streaming</span></span>(<span>self,<br>items: List[<a title="sglang.srt.managers.schedule_batch.MultimodalDataItem" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.MultimodalDataItem">MultimodalDataItem</a>])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_audio_embedding_streaming(self, items: List[MultimodalDataItem]):
    r&#34;&#34;&#34;
    Extract audio embeddings in a streaming manner using cached key-value pairs.

    This method processes incoming audio features incrementally and stores/updates `past_key_values`
    for faster inference on subsequent audio frames. It only supports batch_size=1 and is intended
    for streaming scenarios.

    Returns:
        List[List[torch.Tensor]]: audio embeddings
    &#34;&#34;&#34;
    wavforms = flatten_nested_list([item.feature for item in items if item.feature])
    # list, [[x1, x2], [y1], [z1]]
    audio_feature_lens_raw = flatten_nested_list(
        [item.audio_feature_lens for item in items if item.audio_feature_lens]
    )

    # exist audio
    if len(wavforms) &gt; 0:
        audio_feature_lens = torch.hstack(audio_feature_lens_raw)
        batch_size, _, max_mel_seq_len = wavforms.shape
        assert batch_size == 1
        max_seq_len = (max_mel_seq_len - 1) // 2 + 1

        if self.audio_past_key_values is not None:
            cache_length = self.audio_past_key_values[0][0].shape[2]
            apm_max_len = self.apm.embed_positions.weight.shape[0]
            if cache_length + max_seq_len &gt;= apm_max_len:
                logger.warning(
                    f&#34;audio_past_key_values length {cache_length + max_seq_len} exceed {apm_max_len}, reset.&#34;
                )
                self.audio_past_key_values = None

        audio_outputs = self.apm(
            wavforms, past_key_values=self.audio_past_key_values, use_cache=True
        )
        audio_states = (
            audio_outputs.last_hidden_state
        )  # [:, :audio_feat_lengths, :]
        self.audio_past_key_values = audio_outputs.past_key_values

        audio_embeds = self.audio_projection_layer(audio_states)

        audio_embeds = audio_embeds.transpose(1, 2)
        audio_embeds = self.audio_avg_pooler(audio_embeds)
        audio_embeds = audio_embeds.transpose(1, 2)

        _, feature_lens_after_pooling = self._get_feat_extract_output_lengths(
            audio_feature_lens
        )

        num_audio_tokens = feature_lens_after_pooling

        final_audio_embeds = []
        idx = 0
        for i in range(len(audio_feature_lens_raw)):
            target_audio_embeds = []
            for _ in range(len(audio_feature_lens_raw[i])):
                target_audio_embeds.append(
                    audio_embeds[idx, : num_audio_tokens[idx], :]
                )
                idx += 1
            final_audio_embeds.append(target_audio_embeds)
        return final_audio_embeds
    else:
        return []</code></pre>
</details>
<div class="desc"><p>Extract audio embeddings in a streaming manner using cached key-value pairs.</p>
<p>This method processes incoming audio features incrementally and stores/updates <code>past_key_values</code>
for faster inference on subsequent audio frames. It only supports batch_size=1 and is intended
for streaming scenarios.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[List[torch.Tensor]]</code></dt>
<dd>audio embeddings</dd>
</dl></div>
</dd>
<dt id="sglang.srt.models.minicpmo.MiniCPMO.get_audio_feature"><code class="name flex">
<span>def <span class="ident">get_audio_feature</span></span>(<span>self,<br>items: List[<a title="sglang.srt.managers.schedule_batch.MultimodalDataItem" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.MultimodalDataItem">MultimodalDataItem</a>]) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_audio_feature(self, items: List[MultimodalDataItem]) -&gt; torch.Tensor:
    embedding = self.get_omni_embedding(
        items=items,
        chunk_length=self.config.audio_chunk_length,
        stream_input=False,
    )
    return embedding</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmo.MiniCPMO.get_image_feature"><code class="name flex">
<span>def <span class="ident">get_image_feature</span></span>(<span>self,<br>items: List[<a title="sglang.srt.managers.schedule_batch.MultimodalDataItem" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.MultimodalDataItem">MultimodalDataItem</a>]) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_image_feature(self, items: List[MultimodalDataItem]) -&gt; torch.Tensor:
    # list of tensors
    pixel_values = flatten_nested_list([item.feature for item in items])
    tgt_sizes = torch.stack(
        flatten_nested_list([item.tgt_size for item in items]), dim=0
    )
    assert len(pixel_values) == tgt_sizes.shape[0]

    device = self.vpm.embeddings.position_embedding.weight.device
    dtype = self.vpm.embeddings.position_embedding.weight.dtype
    all_pixel_values_lst = [
        i.flatten(end_dim=1).permute(1, 0) for i in pixel_values
    ]

    max_patches = (tgt_sizes[:, 0] * tgt_sizes[:, 1]).max().item()
    assert isinstance(max_patches, int)
    all_pixel_values = torch.nn.utils.rnn.pad_sequence(
        all_pixel_values_lst, batch_first=True, padding_value=0.0
    )

    B, L, _ = all_pixel_values.shape
    all_pixel_values = all_pixel_values.permute(0, 2, 1).reshape(B, 3, -1, L)
    patch_attn_mask = torch.zeros(
        (B, 1, max_patches), dtype=torch.bool, device=device
    )

    tgt_sizes_tensor = tgt_sizes.clone().to(device=patch_attn_mask.device)
    mask_shapes = tgt_sizes_tensor[:, 0] * tgt_sizes_tensor[:, 1]
    patch_attn_mask[:, 0, :] = torch.arange(
        patch_attn_mask.size(2), device=patch_attn_mask.device
    ).unsqueeze(0) &lt; mask_shapes.unsqueeze(1)

    vision_embedding = self.vpm(
        all_pixel_values.type(dtype),
        patch_attention_mask=patch_attn_mask,
        tgt_sizes=tgt_sizes,
    )
    return self.resampler(vision_embedding, tgt_sizes)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmo.MiniCPMO.get_omni_embedding"><code class="name flex">
<span>def <span class="ident">get_omni_embedding</span></span>(<span>self,<br>items: List[<a title="sglang.srt.managers.schedule_batch.MultimodalDataItem" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.MultimodalDataItem">MultimodalDataItem</a>],<br>chunk_length=-1,<br>stream_input=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_omni_embedding(
    self,
    items: List[MultimodalDataItem],
    chunk_length=-1,
    stream_input=False,
):
    &#34;&#34;&#34;
    Args:
        chunk_length: whisper use full attention or chunk attention
        stream_input: use streaming audio embedding
    Returns:
        final embeddings with audio feature
    &#34;&#34;&#34;

    if stream_input:
        audio_embeddings = self.get_audio_embedding_streaming(items)
    else:
        audio_embeddings = self.get_audio_embedding(items, chunk_length)
    bs = len(audio_embeddings)
    # batch size
    audio_embs = torch.cat(flatten_nested_list(audio_embeddings), dim=0)

    return audio_embs</code></pre>
</details>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>chunk_length</code></strong></dt>
<dd>whisper use full attention or chunk attention</dd>
<dt><strong><code>stream_input</code></strong></dt>
<dd>use streaming audio embedding</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>final embeddings with audio feature</p></div>
</dd>
<dt id="sglang.srt.models.minicpmo.MiniCPMO.init_audio_module"><code class="name flex">
<span>def <span class="ident">init_audio_module</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_audio_module(self):
    model = MiniCPMWhisperEncoder(self.config.audio_config)
    return model</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmo.MiniCPMO.init_llm"><code class="name flex">
<span>def <span class="ident">init_llm</span></span>(<span>self,<br>config: transformers.configuration_utils.PretrainedConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '') ‑> torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_llm(
    self,
    config: PretrainedConfig,
    quant_config: Optional[QuantizationConfig] = None,
    prefix: str = &#34;&#34;,
) -&gt; nn.Module:
    return Qwen2ForCausalLM(config=config, quant_config=quant_config, prefix=prefix)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmo.MiniCPMO.init_resampler"><code class="name flex">
<span>def <span class="ident">init_resampler</span></span>(<span>self,<br>embed_dim: int,<br>vision_dim: int,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '') ‑> torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_resampler(
    self,
    embed_dim: int,
    vision_dim: int,
    quant_config: Optional[QuantizationConfig] = None,
    prefix: str = &#34;&#34;,
) -&gt; nn.Module:
    with set_default_torch_dtype(torch.float16):
        # The resampler in 2.6 remains consistent with the one in 2.5.
        resampler = Resampler2_5(
            num_queries=self.config.query_num,
            embed_dim=embed_dim,
            num_heads=embed_dim // 128,
            kv_dim=vision_dim,
            quant_config=quant_config,
            prefix=prefix,
        )

    return resampler.to(device=&#34;cuda&#34;, dtype=torch.get_default_dtype())</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmo.MiniCPMO.init_tts_module"><code class="name flex">
<span>def <span class="ident">init_tts_module</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_tts_module(self):
    model = ConditionalChatTTS(self.config.tts_config)
    return model</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmo.MiniCPMO.init_vision_module"><code class="name flex">
<span>def <span class="ident">init_vision_module</span></span>(<span>self,<br>config: transformers.configuration_utils.PretrainedConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_vision_module(
    self,
    config: PretrainedConfig,
    quant_config: Optional[QuantizationConfig],
    prefix: str = &#34;&#34;,
):
    if self.config._attn_implementation == &#34;flash_attention_2&#34;:
        self.config.vision_config._attn_implementation = &#34;flash_attention_2&#34;
    else:
        self.config.vision_config._attn_implementation = &#34;eager&#34;
    model = Idefics2VisionTransformer(
        config=config.vision_config, quant_config=quant_config, prefix=prefix
    )
    if self.config.drop_vision_last_layer:
        model.encoder.layers = model.encoder.layers[:-1]

    setattr(model, &#34;embed_dim&#34;, model.embeddings.embed_dim)
    setattr(model, &#34;patch_size&#34;, model.embeddings.patch_size)

    return model</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmo.MiniCPMO.load_weights"><code class="name flex">
<span>def <span class="ident">load_weights</span></span>(<span>self, weights: Iterable[Tuple[str, torch.Tensor]])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
    stacked_params_mapping = [
        # (param_name, shard_name, shard_id)
        (&#34;qkv_proj&#34;, &#34;q_proj&#34;, &#34;q&#34;),
        (&#34;qkv_proj&#34;, &#34;k_proj&#34;, &#34;k&#34;),
        (&#34;qkv_proj&#34;, &#34;v_proj&#34;, &#34;v&#34;),
        (&#34;gate_up_proj&#34;, &#34;gate_proj&#34;, 0),
        (&#34;gate_up_proj&#34;, &#34;up_proj&#34;, 1),
    ]

    params_dict = dict(self.named_parameters())
    for name, loaded_weight in weights:

        if &#34;rotary_emb.inv_freq~&#34; in name or &#34;projector&#34; in name:
            continue
        if &#34;rotary_emb.cos_cached&#34; in name or &#34;rotary_emb.sin_cached&#34; in name:
            # Models trained using ColossalAI may include these tensors in
            # the checkpoint. Skip them.
            continue

        # For weight_norm parametrization, handle both old and new formats
        if self.config.init_tts and &#34;tts&#34; in name:
            # Handle loading from older checkpoints with weight_g/weight_v format
            if &#34;.weight_g&#34; in name or &#34;.weight_v&#34; in name:
                name = name.replace(
                    &#34;.weight_g&#34;, &#34;.parametrizations.weight.original0&#34;
                )
                name = name.replace(
                    &#34;.weight_v&#34;, &#34;.parametrizations.weight.original1&#34;
                )
            elif &#34;.weight&#34; in name and name not in params_dict:
                param_name = name.replace(
                    &#34;.weight&#34;, &#34;.parametrizations.weight.original0&#34;
                )
                if param_name in params_dict:
                    name = param_name

        # adapt to VisionAttention
        if &#34;vpm&#34; in name:
            name = name.replace(r&#34;self_attn.out_proj&#34;, r&#34;self_attn.proj&#34;)

        if not self.config.init_tts and &#34;tts&#34; in name:
            continue
        if not self.config.init_audio and (&#34;apm&#34; in name or &#34;audio&#34; in name):
            continue
        if not self.config.init_vision and &#34;vpm&#34; in name:
            continue

        if (
            &#34;sampler&#34; in name
            or &#34;apm&#34; in name
            or (&#34;tts&#34; in name and &#34;self_attn&#34; in name)
            or (&#34;tts.model.layers&#34; in name and &#34;.mlp&#34; in name)
        ):
            param = params_dict[name]
            weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
            weight_loader(param, loaded_weight)
            continue

        for param_name, weight_name, shard_id in stacked_params_mapping:
            # replace the name and load with customized loader
            if weight_name not in name:
                continue
            name = name.replace(weight_name, param_name)
            # # Skip loading extra bias for GPTQ models.
            if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                continue
            param = params_dict[name]
            weight_loader = param.weight_loader
            weight_loader(param, loaded_weight, shard_id)
            break
        else:
            # Skip loading extra bias for GPTQ models.
            if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                continue
            param = params_dict[name]
            weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
            weight_loader(param, loaded_weight)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmo.MiniCPMO.pad_input_ids"><code class="name flex">
<span>def <span class="ident">pad_input_ids</span></span>(<span>self,<br>input_ids: List[int],<br>mm_input: <a title="sglang.srt.managers.schedule_batch.MultimodalInputs" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.MultimodalInputs">MultimodalInputs</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad_input_ids(self, input_ids: List[int], mm_input: MultimodalInputs):
    # Get all special token IDs
    im_start_id: int = mm_input.im_start_id
    im_end_id: int = mm_input.im_end_id
    slice_start_id: int = mm_input.slice_start_id
    slice_end_id: int = mm_input.slice_end_id

    data_token_pairs = [
        (im_start_id, im_end_id),
        (slice_start_id, slice_end_id),
        (mm_input.audio_start_id, mm_input.audio_end_id),
    ]
    data_start_token_ids = [im_start_id, mm_input.audio_start_id]
    pattern = MultiModalityDataPaddingPatternTokenPairs(
        data_token_pairs=data_token_pairs, data_start_token_ids=data_start_token_ids
    )

    return pattern.pad_input_tokens(input_ids, mm_input)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmo.MiniCPMO.subsequent_chunk_mask"><code class="name flex">
<span>def <span class="ident">subsequent_chunk_mask</span></span>(<span>self,<br>size: int,<br>chunk_size: int,<br>num_left_chunks: int = -1,<br>device: torch.device = device(type='cpu'),<br>num_lookhead: int = 0) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def subsequent_chunk_mask(
    self,
    size: int,
    chunk_size: int,
    num_left_chunks: int = -1,
    device: torch.device = torch.device(&#34;cpu&#34;),
    num_lookhead: int = 0,
) -&gt; torch.Tensor:
    &#34;&#34;&#34;Create mask for subsequent steps (size, size) with chunk size,
    this is for streaming encoder

    Args:
        size (int): size of mask
        chunk_size (int): size of chunk
        num_left_chunks (int): number of left chunks
            &lt;0: use full chunk
            &gt;=0: use num_left_chunks
        device (torch.device): &#34;cpu&#34; or &#34;cuda&#34; or torch.Tensor.device

    Returns:
        torch.Tensor: mask

    &#34;&#34;&#34;
    ret = torch.zeros(size, size, device=device, dtype=torch.bool)
    for i in range(size):
        if num_left_chunks &lt; 0:
            start = 0
        else:
            start = max((i // chunk_size - num_left_chunks) * chunk_size, 0)
        ending = min((i // chunk_size + 1) * chunk_size + num_lookhead, size)
        ret[i, start:ending] = True
    return ret</code></pre>
</details>
<div class="desc"><p>Create mask for subsequent steps (size, size) with chunk size,
this is for streaming encoder</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code></dt>
<dd>size of mask</dd>
<dt><strong><code>chunk_size</code></strong> :&ensp;<code>int</code></dt>
<dd>size of chunk</dd>
<dt><strong><code>num_left_chunks</code></strong> :&ensp;<code>int</code></dt>
<dd>number of left chunks
&lt;0: use full chunk<blockquote>
<p>=0: use num_left_chunks</p>
</blockquote>
</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code></dt>
<dd>"cpu" or "cuda" or torch.Tensor.device</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>mask</dd>
</dl></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.models.minicpmv.MiniCPMBaseModel" href="minicpmv.html#sglang.srt.models.minicpmv.MiniCPMBaseModel">MiniCPMBaseModel</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMBaseModel.forward" href="minicpmv.html#sglang.srt.models.minicpmv.MiniCPMBaseModel.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.models.minicpmo.MiniCPMWhisperEncoder"><code class="flex name class">
<span>class <span class="ident">MiniCPMWhisperEncoder</span></span>
<span>(</span><span>config: transformers.models.whisper.configuration_whisper.WhisperConfig)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MiniCPMWhisperEncoder(WhisperEncoder):

    def __init__(self, config: WhisperConfig):
        super().__init__(config)
        self.layers = nn.ModuleList(
            [
                MiniCPMWhisperEncoderLayer(config, layer_idx=i)
                for i in range(config.encoder_layers)
            ]
        )

    def forward(
        self,
        input_features,
        attention_mask=None,
        head_mask=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        past_key_values: Optional[EncoderDecoderCache] = None,
        use_cache: Optional[bool] = None,
    ):
        r&#34;&#34;&#34;
        Forward pass of the Whisper encoder.

        Args:
            input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, sequence_length)`):
                Float values of log-mel features extracted from the raw audio waveform. Typically generated
                by a feature extractor (e.g., `WhisperFeatureExtractor`) that processes `.flac` or `.wav`
                files into padded 2D mel spectrogram frames. These features are projected via convolution layers
                (`conv1` and `conv2`) and then transformed into embeddings for the encoder.

            attention_mask (`torch.Tensor`, *optional*):
                Not used by Whisper for masking `input_features`, but included for API compatibility with
                other models. If provided, it is simply ignored within the model. By default, Whisper
                effectively ignores silence in the input log-mel spectrogram.

            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
                Mask to nullify selected attention heads. The elements should be either 1 or 0, where:
                - 1 indicates the head is **not masked**,
                - 0 indicates the head is **masked** (i.e., the attention head is dropped).

            output_attentions (`bool`, *optional*):
                Whether or not to return the attention tensors of all encoder layers. If set to `True`, the
                returned tuple (or `BaseModelOutputWithPast`) will contain an additional element with
                attention weights for each encoder layer.

            output_hidden_states (`bool`, *optional*):
                Whether or not to return the hidden states of all layers. If set to `True`, the returned
                tuple (or `BaseModelOutputWithPast`) will contain a tuple of hidden states, including the
                initial embedding output as well as the outputs of each layer.

            return_dict (`bool`, *optional*):
                Whether or not to return a `BaseModelOutputWithPast` (a subclass of `ModelOutput`) instead
                of a plain tuple. If set to `True`, the output will be a `BaseModelOutputWithPast` object,
                otherwise it will be a tuple.

            past_key_values (`EncoderDecoderCache`, *optional*):
                When using caching for faster inference, this is an object that stores the key-value pairs
                for attention states. If provided, the model will append new states to the existing cache
                and return the updated cache. This speeds up sequential decoding or chunked inference.

                - If `past_key_values` is `None`, no past states are used or returned.
                - If `past_key_values` is not `None` and `use_cache=True`, the model will use the provided
                cache and return the updated cache (as `next_encoder_cache`).

            use_cache (`bool`, *optional*):
                Whether or not the model should use caching (`past_key_values`) to speed up processing
                during inference. When set to `True`, the model will:
                - Inspect and use `past_key_values` if provided.
                - Return updated `past_key_values` (under the name `next_encoder_cache` in
                    `BaseModelOutputWithPast`).

        Returns:
            `BaseModelOutputWithPast` or `tuple` (depending on `return_dict`):
                If `return_dict=True`, a `BaseModelOutputWithPast` is returned, which contains:
                - **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
                The output of the final encoder layer.
                - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned if `output_hidden_states=True`):
                Hidden states of the model at each layer (including the initial projection).
                - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned if `output_attentions=True`):
                Attention weights from each encoder layer.
                - **past_key_values** (an object of type `EncoderDecoderCache` or `None`, *optional*):
                Updated cache of key-value pairs if `use_cache=True`.

                If `return_dict=False`, a tuple is returned, where the format is:
                `(last_hidden_state, hidden_states, attentions)`, with `hidden_states` and `attentions`
                only present if their respective `output_*` arguments are set to `True`.

        &#34;&#34;&#34;
        output_attentions = (
            output_attentions
            if output_attentions is not None
            else self.config.output_attentions
        )
        output_hidden_states = (
            output_hidden_states
            if output_hidden_states is not None
            else self.config.output_hidden_states
        )
        return_dict = (
            return_dict if return_dict is not None else self.config.use_return_dict
        )

        # Ignore copy
        input_features = input_features.to(
            dtype=self.conv1.weight.dtype, device=self.conv1.weight.device
        )

        inputs_embeds = nn.functional.gelu(self.conv1(input_features))
        inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))

        inputs_embeds = inputs_embeds.permute(0, 2, 1)

        embed_pos = self.embed_positions.weight
        past_key_values_length = 0
        if use_cache:
            if past_key_values is None:
                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())
            elif isinstance(past_key_values, list):
                past_key_values = EncoderDecoderCache(
                    DynamicCache.from_legacy_cache(past_key_values), DynamicCache()
                )
            elif isinstance(past_key_values, DynamicCache):
                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())
            else:
                pass
            past_key_values_length = (
                past_key_values.self_attention_cache.get_usable_length(
                    inputs_embeds.shape[1]
                )
            )
            if inputs_embeds.shape[1] + past_key_values_length &gt; embed_pos.shape[0]:
                logger.warning(
                    &#34;seems the audio is longer than 30s. repeating the last part of the audio&#34;
                )
                embed_pos_front = embed_pos[past_key_values_length:, :]
                embed_pos = torch.cat(
                    (
                        embed_pos_front,
                        torch.repeat_interleave(
                            embed_pos[-1, :].unsqueeze(0),
                            inputs_embeds.shape[1]
                            - embed_pos.shape[0]
                            + past_key_values_length,
                            dim=0,
                        ),
                    )
                )
            else:
                embed_pos = embed_pos[
                    past_key_values_length : inputs_embeds.shape[1]
                    + past_key_values_length,
                    :,
                ]
        else:
            embed_pos = embed_pos[: inputs_embeds.shape[1], :]

        hidden_states = inputs_embeds + embed_pos
        hidden_states = nn.functional.dropout(
            hidden_states, p=self.dropout, training=False
        )

        encoder_states = () if output_hidden_states else None
        all_attentions = () if output_attentions else None

        # check if head_mask has a correct number of layers specified if desired
        if head_mask is not None:
            assert head_mask.size()[0] == (
                len(self.layers)
            ), f&#34;The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.&#34;

        for idx, encoder_layer in enumerate(self.layers):
            if output_hidden_states:
                encoder_states = encoder_states + (hidden_states,)
            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
            to_drop = False

            # Ignore copy
            if to_drop:
                layer_outputs = (None, None)
            else:
                layer_outputs = encoder_layer(
                    hidden_states,
                    attention_mask,
                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),
                    output_attentions=output_attentions,
                    past_key_values=past_key_values,
                    use_cache=use_cache,
                )

                hidden_states = layer_outputs[0]

            if use_cache:
                next_encoder_cache = layer_outputs[2 if output_attentions else 1]
            else:
                next_encoder_cache = None

            if output_attentions:
                all_attentions = all_attentions + (layer_outputs[1],)

        hidden_states = self.layer_norm(hidden_states)
        if output_hidden_states:
            encoder_states = encoder_states + (hidden_states,)

        if not return_dict:
            return tuple(
                v
                for v in [hidden_states, encoder_states, all_attentions]
                if v is not None
            )
        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            hidden_states=encoder_states,
            attentions=all_attentions,
            past_key_values=next_encoder_cache,
        )</code></pre>
</details>
<div class="desc"><p>Transformer encoder consisting of <em>config.encoder_layers</em> self attention layers. Each layer is a
[<code>WhisperEncoderLayer</code>].</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>config</code></strong></dt>
<dd>WhisperConfig</dd>
</dl>
<p>Args:
config ([<code>PretrainedConfig</code>]):
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
[<code>~PreTrainedModel.from_pretrained</code>] method to load the model weights.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.models.whisper.modeling_whisper.WhisperEncoder</li>
<li>transformers.models.whisper.modeling_whisper.WhisperPreTrainedModel</li>
<li>transformers.modeling_utils.PreTrainedModel</li>
<li>torch.nn.modules.module.Module</li>
<li>transformers.modeling_utils.EmbeddingAccessMixin</li>
<li>transformers.modeling_utils.ModuleUtilsMixin</li>
<li>transformers.utils.hub.PushToHubMixin</li>
<li>transformers.integrations.peft.PeftAdapterMixin</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.minicpmo.MiniCPMWhisperEncoder.config_class"><code class="name">var <span class="ident">config_class</span></code></dt>
<dd>
<div class="desc"><p>This is the configuration class to store the configuration of a [<code>WhisperModel</code>]. It is used to instantiate a
Whisper model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the Whisper
<a href="https://huggingface.co/openai/whisper-tiny">openai/whisper-tiny</a> architecture.</p>
<p>Configuration objects inherit from [<code>PretrainedConfig</code>] and can be used to control the model outputs. Read the
documentation from [<code>PretrainedConfig</code>] for more information.</p>
<h2 id="args">Args</h2>
<p>vocab_size (<code>int</code>, <em>optional</em>, defaults to 51865):
Vocabulary size of the Whisper model. Defines the number of different tokens that can be represented by the
<code>decoder_input_ids</code> passed when calling [<code>WhisperModel</code>]
num_mel_bins (<code>int</code>, <em>optional</em>, defaults to 80):
Number of mel features used per input features. Should correspond to the value used in the
<code>WhisperProcessor</code> class.
encoder_layers (<code>int</code>, <em>optional</em>, defaults to 4):
Number of encoder layers.
decoder_layers (<code>int</code>, <em>optional</em>, defaults to 4):
Number of decoder layers.
encoder_attention_heads (<code>int</code>, <em>optional</em>, defaults to 6):
Number of attention heads for each attention layer in the Transformer encoder.
decoder_attention_heads (<code>int</code>, <em>optional</em>, defaults to 6):
Number of attention heads for each attention layer in the Transformer decoder.
encoder_ffn_dim (<code>int</code>, <em>optional</em>, defaults to 1536):
Dimensionality of the "intermediate" (often named feed-forward) layer in encoder.
decoder_ffn_dim (<code>int</code>, <em>optional</em>, defaults to 1536):
Dimensionality of the "intermediate" (often named feed-forward) layer in decoder.
encoder_layerdrop (<code>float</code>, <em>optional</em>, defaults to 0.0):
The LayerDrop probability for the encoder. See the <a href="see https://huggingface.co/papers/1909.11556">LayerDrop paper</a>
for more details.
decoder_layerdrop (<code>float</code>, <em>optional</em>, defaults to 0.0):
The LayerDrop probability for the decoder. See the <a href="see https://huggingface.co/papers/1909.11556">LayerDrop paper</a>
for more details.
decoder_start_token_id (<code>int</code>, <em>optional</em>, defaults to 50257):
Corresponds to the "&lt;|startoftranscript|&gt;" token, which is automatically used when no <code>decoder_input_ids</code>
are provided to the <code>generate</code> function. It is used to guide the model<code>s generation process depending on
the task.
use_cache (&lt;code&gt;bool&lt;/code&gt;, *optional*, defaults to &lt;code&gt;True&lt;/code&gt;):
Whether or not the model should return the last key/values attentions (not used by all models).
is_encoder_decoder (&lt;code&gt;bool&lt;/code&gt;, *optional*, defaults to &lt;code&gt;True&lt;/code&gt;):
Whether the model is used as an encoder/decoder or not.
activation_function (&lt;code&gt;str&lt;/code&gt;, *optional*, defaults to</code>"gelu"<code>):
The non-linear activation function (function or string) in the encoder and pooler. If string,</code>"gelu"<code>,</code>"relu"<code>,</code>"silu"<code>and</code>"gelu_new"<code>are supported.
d_model (&lt;code&gt;int&lt;/code&gt;, *optional*, defaults to 384):
Dimensionality of the layers.
dropout (&lt;code&gt;float&lt;/code&gt;, *optional*, defaults to 0.1):
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.
attention_dropout (&lt;code&gt;float&lt;/code&gt;, *optional*, defaults to 0.0):
The dropout ratio for the attention probabilities.
activation_dropout (&lt;code&gt;float&lt;/code&gt;, *optional*, defaults to 0.0):
The dropout ratio for activations inside the fully connected layer.
init_std (&lt;code&gt;float&lt;/code&gt;, *optional*, defaults to 0.02):
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
scale_embedding (&lt;code&gt;bool&lt;/code&gt;, *optional*, defaults to False):
Scale embeddings by diving by sqrt(d_model).
max_source_positions (&lt;code&gt;int&lt;/code&gt;, *optional*, defaults to 1500):
The maximum sequence length of log-mel filter-bank features that this model might ever be used with.
max_target_positions (&lt;code&gt;int&lt;/code&gt;, *optional*, defaults to 448):
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).
pad_token_id (&lt;code&gt;int&lt;/code&gt;, *optional*, defaults to 50256):
Padding token id.
bos_token_id (&lt;code&gt;int&lt;/code&gt;, *optional*, defaults to 50256):
Begin of stream token id.
eos_token_id (&lt;code&gt;int&lt;/code&gt;, *optional*, defaults to 50256):
End of stream token id.
suppress_tokens (&lt;code&gt;list\[int]&lt;/code&gt;, *optional*):
A list containing the non-speech tokens that will be used by the logit processor in the &lt;code&gt;generate&lt;/code&gt;
function. NON_SPEECH_TOKENS and NON_SPEECH_TOKENS_MULTI each correspond to the</code>english-only<code>and the
&lt;code&gt;multilingual&lt;/code&gt; model.
begin_suppress_tokens (&lt;code&gt;list\[int]&lt;/code&gt;, *optional*, defaults to &lt;code&gt;\[220,50256]&lt;/code&gt;):
A list containing tokens that will be suppressed at the beginning of the sampling process. Initialized as
the token for</code>" "<code>(&lt;code&gt;blank\_token\_id&lt;/code&gt;) and the &lt;code&gt;eos\_token\_id&lt;/code&gt;
use_weighted_layer_sum (&lt;code&gt;bool&lt;/code&gt;, *optional*, defaults to &lt;code&gt;False&lt;/code&gt;):
Whether to use a weighted average of layer outputs with learned weights. Only relevant when using an
instance of [</code>WhisperForAudioClassification<code>].
classifier_proj_size (&lt;code&gt;int&lt;/code&gt;, *optional*, defaults to 256):
Dimensionality of the projection before token mean-pooling for classification. Only relevant when using an
instance of [</code>WhisperForAudioClassification<code>].
apply_spec_augment (&lt;code&gt;bool&lt;/code&gt;, *optional*, defaults to &lt;code&gt;False&lt;/code&gt;):
Whether to apply *SpecAugment* data augmentation to the outputs of the feature encoder. For reference see
[SpecAugment: A Simple Data Augmentation Method for Automatic Speech
Recognition](&lt;https://huggingface.co/papers/1904.08779&gt;).
mask_time_prob (&lt;code&gt;float&lt;/code&gt;, *optional*, defaults to 0.05):
Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking
procedure generates</code>mask_time_prob<em>len(time_axis)/mask_time_length<code>independent masks over the axis. If
reasoning from the probability of each feature vector to be chosen as the start of the vector span to be
masked, *mask_time_prob* should be</code>prob_vector_start</em>mask_time_length<code>. Note that overlap may decrease the
actual percentage of masked vectors. This is only relevant if</code>apply_spec_augment == True<code>.
mask_time_length (&lt;code&gt;int&lt;/code&gt;, *optional*, defaults to 10):
Length of vector span along the time axis.
mask_time_min_masks (&lt;code&gt;int&lt;/code&gt;, *optional*, defaults to 2),:
The minimum number of masks of length &lt;code&gt;mask\_feature\_length&lt;/code&gt; generated along the time axis, each time step,
irrespectively of &lt;code&gt;mask\_feature\_prob&lt;/code&gt;. Only relevant if ''mask_time_prob*len(time_axis)/mask_time_length &lt;
mask_time_min_masks''
mask_feature_prob (&lt;code&gt;float&lt;/code&gt;, *optional*, defaults to 0.0):
Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The
masking procedure generates</code>mask_feature_prob<em>len(feature_axis)/mask_time_length<code>independent masks over
the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector
span to be masked, *mask_feature_prob* should be</code>prob_vector_start</em>mask_feature_length<code>. Note that overlap
may decrease the actual percentage of masked vectors. This is only relevant if</code>apply_spec_augment is
True<code>.
mask_feature_length (&lt;code&gt;int&lt;/code&gt;, *optional*, defaults to 10):
Length of vector span along the feature axis.
mask_feature_min_masks (&lt;code&gt;int&lt;/code&gt;, *optional*, defaults to 0),:
The minimum number of masks of length &lt;code&gt;mask\_feature\_length&lt;/code&gt; generated along the feature axis, each time
step, irrespectively of &lt;code&gt;mask\_feature\_prob&lt;/code&gt;. Only relevant if</code>mask_feature_prob<em>len(feature_axis)/mask_feature_length &lt; mask_feature_min_masks`.
median_filter_width (<code>int</code>, </em>optional*, defaults to 7):
Width of the median filter used to smoothen to cross-attention outputs when computing token timestamps.
Should be an odd number.
Example:</p>
<pre><code class="language-python">&gt;&gt;&gt; from transformers import WhisperConfig, WhisperModel

&gt;&gt;&gt; # Initializing a Whisper tiny style configuration
&gt;&gt;&gt; configuration = WhisperConfig()

&gt;&gt;&gt; # Initializing a model (with random weights) from the tiny style configuration
&gt;&gt;&gt; model = WhisperModel(configuration)

&gt;&gt;&gt; # Accessing the model configuration
&gt;&gt;&gt; configuration = model.config
</code></pre></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.minicpmo.MiniCPMWhisperEncoder.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>input_features,<br>attention_mask=None,<br>head_mask=None,<br>output_attentions=None,<br>output_hidden_states=None,<br>return_dict=None,<br>past_key_values: transformers.cache_utils.EncoderDecoderCache | None = None,<br>use_cache: bool | None = None) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    input_features,
    attention_mask=None,
    head_mask=None,
    output_attentions=None,
    output_hidden_states=None,
    return_dict=None,
    past_key_values: Optional[EncoderDecoderCache] = None,
    use_cache: Optional[bool] = None,
):
    r&#34;&#34;&#34;
    Forward pass of the Whisper encoder.

    Args:
        input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, sequence_length)`):
            Float values of log-mel features extracted from the raw audio waveform. Typically generated
            by a feature extractor (e.g., `WhisperFeatureExtractor`) that processes `.flac` or `.wav`
            files into padded 2D mel spectrogram frames. These features are projected via convolution layers
            (`conv1` and `conv2`) and then transformed into embeddings for the encoder.

        attention_mask (`torch.Tensor`, *optional*):
            Not used by Whisper for masking `input_features`, but included for API compatibility with
            other models. If provided, it is simply ignored within the model. By default, Whisper
            effectively ignores silence in the input log-mel spectrogram.

        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
            Mask to nullify selected attention heads. The elements should be either 1 or 0, where:
            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked** (i.e., the attention head is dropped).

        output_attentions (`bool`, *optional*):
            Whether or not to return the attention tensors of all encoder layers. If set to `True`, the
            returned tuple (or `BaseModelOutputWithPast`) will contain an additional element with
            attention weights for each encoder layer.

        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. If set to `True`, the returned
            tuple (or `BaseModelOutputWithPast`) will contain a tuple of hidden states, including the
            initial embedding output as well as the outputs of each layer.

        return_dict (`bool`, *optional*):
            Whether or not to return a `BaseModelOutputWithPast` (a subclass of `ModelOutput`) instead
            of a plain tuple. If set to `True`, the output will be a `BaseModelOutputWithPast` object,
            otherwise it will be a tuple.

        past_key_values (`EncoderDecoderCache`, *optional*):
            When using caching for faster inference, this is an object that stores the key-value pairs
            for attention states. If provided, the model will append new states to the existing cache
            and return the updated cache. This speeds up sequential decoding or chunked inference.

            - If `past_key_values` is `None`, no past states are used or returned.
            - If `past_key_values` is not `None` and `use_cache=True`, the model will use the provided
            cache and return the updated cache (as `next_encoder_cache`).

        use_cache (`bool`, *optional*):
            Whether or not the model should use caching (`past_key_values`) to speed up processing
            during inference. When set to `True`, the model will:
            - Inspect and use `past_key_values` if provided.
            - Return updated `past_key_values` (under the name `next_encoder_cache` in
                `BaseModelOutputWithPast`).

    Returns:
        `BaseModelOutputWithPast` or `tuple` (depending on `return_dict`):
            If `return_dict=True`, a `BaseModelOutputWithPast` is returned, which contains:
            - **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
            The output of the final encoder layer.
            - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned if `output_hidden_states=True`):
            Hidden states of the model at each layer (including the initial projection).
            - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned if `output_attentions=True`):
            Attention weights from each encoder layer.
            - **past_key_values** (an object of type `EncoderDecoderCache` or `None`, *optional*):
            Updated cache of key-value pairs if `use_cache=True`.

            If `return_dict=False`, a tuple is returned, where the format is:
            `(last_hidden_state, hidden_states, attentions)`, with `hidden_states` and `attentions`
            only present if their respective `output_*` arguments are set to `True`.

    &#34;&#34;&#34;
    output_attentions = (
        output_attentions
        if output_attentions is not None
        else self.config.output_attentions
    )
    output_hidden_states = (
        output_hidden_states
        if output_hidden_states is not None
        else self.config.output_hidden_states
    )
    return_dict = (
        return_dict if return_dict is not None else self.config.use_return_dict
    )

    # Ignore copy
    input_features = input_features.to(
        dtype=self.conv1.weight.dtype, device=self.conv1.weight.device
    )

    inputs_embeds = nn.functional.gelu(self.conv1(input_features))
    inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))

    inputs_embeds = inputs_embeds.permute(0, 2, 1)

    embed_pos = self.embed_positions.weight
    past_key_values_length = 0
    if use_cache:
        if past_key_values is None:
            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())
        elif isinstance(past_key_values, list):
            past_key_values = EncoderDecoderCache(
                DynamicCache.from_legacy_cache(past_key_values), DynamicCache()
            )
        elif isinstance(past_key_values, DynamicCache):
            past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())
        else:
            pass
        past_key_values_length = (
            past_key_values.self_attention_cache.get_usable_length(
                inputs_embeds.shape[1]
            )
        )
        if inputs_embeds.shape[1] + past_key_values_length &gt; embed_pos.shape[0]:
            logger.warning(
                &#34;seems the audio is longer than 30s. repeating the last part of the audio&#34;
            )
            embed_pos_front = embed_pos[past_key_values_length:, :]
            embed_pos = torch.cat(
                (
                    embed_pos_front,
                    torch.repeat_interleave(
                        embed_pos[-1, :].unsqueeze(0),
                        inputs_embeds.shape[1]
                        - embed_pos.shape[0]
                        + past_key_values_length,
                        dim=0,
                    ),
                )
            )
        else:
            embed_pos = embed_pos[
                past_key_values_length : inputs_embeds.shape[1]
                + past_key_values_length,
                :,
            ]
    else:
        embed_pos = embed_pos[: inputs_embeds.shape[1], :]

    hidden_states = inputs_embeds + embed_pos
    hidden_states = nn.functional.dropout(
        hidden_states, p=self.dropout, training=False
    )

    encoder_states = () if output_hidden_states else None
    all_attentions = () if output_attentions else None

    # check if head_mask has a correct number of layers specified if desired
    if head_mask is not None:
        assert head_mask.size()[0] == (
            len(self.layers)
        ), f&#34;The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.&#34;

    for idx, encoder_layer in enumerate(self.layers):
        if output_hidden_states:
            encoder_states = encoder_states + (hidden_states,)
        # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
        to_drop = False

        # Ignore copy
        if to_drop:
            layer_outputs = (None, None)
        else:
            layer_outputs = encoder_layer(
                hidden_states,
                attention_mask,
                layer_head_mask=(head_mask[idx] if head_mask is not None else None),
                output_attentions=output_attentions,
                past_key_values=past_key_values,
                use_cache=use_cache,
            )

            hidden_states = layer_outputs[0]

        if use_cache:
            next_encoder_cache = layer_outputs[2 if output_attentions else 1]
        else:
            next_encoder_cache = None

        if output_attentions:
            all_attentions = all_attentions + (layer_outputs[1],)

    hidden_states = self.layer_norm(hidden_states)
    if output_hidden_states:
        encoder_states = encoder_states + (hidden_states,)

    if not return_dict:
        return tuple(
            v
            for v in [hidden_states, encoder_states, all_attentions]
            if v is not None
        )
    return BaseModelOutputWithPast(
        last_hidden_state=hidden_states,
        hidden_states=encoder_states,
        attentions=all_attentions,
        past_key_values=next_encoder_cache,
    )</code></pre>
</details>
<div class="desc"><p>Forward pass of the Whisper encoder.</p>
<h2 id="args">Args</h2>
<p>input_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, feature_size, sequence_length)</code>):
Float values of log-mel features extracted from the raw audio waveform. Typically generated
by a feature extractor (e.g., <code>WhisperFeatureExtractor</code>) that processes <code>.flac</code> or <code>.wav</code>
files into padded 2D mel spectrogram frames. These features are projected via convolution layers
(<code>conv1</code> and <code>conv2</code>) and then transformed into embeddings for the encoder.</p>
<p>attention_mask (<code>torch.Tensor</code>, <em>optional</em>):
Not used by Whisper for masking <code>input_features</code>, but included for API compatibility with
other models. If provided, it is simply ignored within the model. By default, Whisper
effectively ignores silence in the input log-mel spectrogram.</p>
<p>head_mask (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>):
Mask to nullify selected attention heads. The elements should be either 1 or 0, where:
- 1 indicates the head is <strong>not masked</strong>,
- 0 indicates the head is <strong>masked</strong> (i.e., the attention head is dropped).</p>
<p>output_attentions (<code>bool</code>, <em>optional</em>):
Whether or not to return the attention tensors of all encoder layers. If set to <code>True</code>, the
returned tuple (or <code>BaseModelOutputWithPast</code>) will contain an additional element with
attention weights for each encoder layer.</p>
<p>output_hidden_states (<code>bool</code>, <em>optional</em>):
Whether or not to return the hidden states of all layers. If set to <code>True</code>, the returned
tuple (or <code>BaseModelOutputWithPast</code>) will contain a tuple of hidden states, including the
initial embedding output as well as the outputs of each layer.</p>
<p>return_dict (<code>bool</code>, <em>optional</em>):
Whether or not to return a <code>BaseModelOutputWithPast</code> (a subclass of <code>ModelOutput</code>) instead
of a plain tuple. If set to <code>True</code>, the output will be a <code>BaseModelOutputWithPast</code> object,
otherwise it will be a tuple.</p>
<p>past_key_values (<code>EncoderDecoderCache</code>, <em>optional</em>):
When using caching for faster inference, this is an object that stores the key-value pairs
for attention states. If provided, the model will append new states to the existing cache
and return the updated cache. This speeds up sequential decoding or chunked inference.</p>
<pre><code>- If &lt;code&gt;past\_key\_values&lt;/code&gt; is &lt;code&gt;None&lt;/code&gt;, no past states are used or returned.
- If &lt;code&gt;past\_key\_values&lt;/code&gt; is not &lt;code&gt;None&lt;/code&gt; and `use_cache=True`, the model will use the provided
cache and return the updated cache (as &lt;code&gt;next\_encoder\_cache&lt;/code&gt;).
</code></pre>
<p>use_cache (<code>bool</code>, <em>optional</em>):
Whether or not the model should use caching (<code>past_key_values</code>) to speed up processing
during inference. When set to <code>True</code>, the model will:
- Inspect and use <code>past_key_values</code> if provided.
- Return updated <code>past_key_values</code> (under the name <code>next_encoder_cache</code> in
<code>BaseModelOutputWithPast</code>).</p>
<h2 id="returns">Returns</h2>
<p><code>BaseModelOutputWithPast</code> or <code>tuple</code> (depending on <code>return_dict</code>):
If <code>return_dict=True</code>, a <code>BaseModelOutputWithPast</code> is returned, which contains:
- <strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>):
The output of the final encoder layer.
- <strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned if <code>output_hidden_states=True</code>):
Hidden states of the model at each layer (including the initial projection).
- <strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned if <code>output_attentions=True</code>):
Attention weights from each encoder layer.
- <strong>past_key_values</strong> (an object of type <code>EncoderDecoderCache</code> or <code>None</code>, <em>optional</em>):
Updated cache of key-value pairs if <code>use_cache=True</code>.</p>
<pre><code>If `return_dict=False`, a tuple is returned, where the format is:
&lt;code&gt;(last\_hidden\_state, hidden\_states, attentions)&lt;/code&gt;, with &lt;code&gt;hidden\_states&lt;/code&gt; and &lt;code&gt;attentions&lt;/code&gt;
only present if their respective `output_*` arguments are set to &lt;code&gt;True&lt;/code&gt;.
</code></pre></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.minicpmo.MiniCPMWhisperEncoderLayer"><code class="flex name class">
<span>class <span class="ident">MiniCPMWhisperEncoderLayer</span></span>
<span>(</span><span>config: transformers.models.whisper.configuration_whisper.WhisperConfig,<br>layer_idx: int = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MiniCPMWhisperEncoderLayer(nn.Module):
    def __init__(self, config: WhisperConfig, layer_idx: int = None):
        super().__init__()
        self.embed_dim = config.d_model
        self.self_attn = WhisperAttention(
            embed_dim=self.embed_dim,
            num_heads=config.encoder_attention_heads,
            dropout=config.attention_dropout,
            config=config,
            layer_idx=layer_idx,
        )
        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)
        self.dropout = config.dropout
        self.activation_fn = ACT2FN[config.activation_function]
        self.activation_dropout = config.activation_dropout
        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)
        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)
        self.final_layer_norm = nn.LayerNorm(self.embed_dim)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: torch.Tensor,
        layer_head_mask: torch.Tensor,
        output_attentions: bool = False,
        past_key_values: Optional[EncoderDecoderCache] = None,
        use_cache: Optional[bool] = False,
    ) -&gt; torch.Tensor:
        r&#34;&#34;&#34;
        Args:
            hidden_states (`torch.FloatTensor` of shape `(batch_size, seq_len, embed_dim)`):
                Hidden states to be fed into the encoder layer.
            attention_mask (`torch.FloatTensor` of shape `(batch_size, 1, tgt_len, src_len)`):
                Attention mask where padding elements are indicated by large negative values.
            layer_head_mask (`torch.FloatTensor` of shape `(encoder_attention_heads,)`):
                Mask to nullify selected heads of the attention modules.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attention weights.
            past_key_values (`EncoderDecoderCache`, *optional*):
                Past key-value pairs used for incremental decoding.
            use_cache (`bool`, *optional*):
                Whether or not to return updated `past_key_values` for caching.

        Returns:
            A tuple of shape `(hidden_states, optional(attn_weights), optional(past_key_values))`.
        &#34;&#34;&#34;
        residual = hidden_states
        hidden_states = self.self_attn_layer_norm(hidden_states)
        # TODO (lifuhuang): confirmed with Mick that the logic for past_key_values is copied from minicpmo official code,
        # currently we are not using past_key_values at all. We need to redesign the caching logic when we support streaming
        # in the future.
        hidden_states, attn_weights = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            layer_head_mask=layer_head_mask,
            output_attentions=output_attentions,
            past_key_value=past_key_values,
        )
        hidden_states = nn.functional.dropout(
            hidden_states, p=self.dropout, training=False
        )
        hidden_states = residual + hidden_states

        residual = hidden_states
        hidden_states = self.final_layer_norm(hidden_states)
        hidden_states = self.activation_fn(self.fc1(hidden_states))
        hidden_states = nn.functional.dropout(
            hidden_states, p=self.activation_dropout, training=False
        )
        hidden_states = self.fc2(hidden_states)
        hidden_states = nn.functional.dropout(
            hidden_states, p=self.dropout, training=False
        )
        hidden_states = residual + hidden_states

        if hidden_states.dtype == torch.float16 and (
            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()
        ):
            clamp_value = torch.finfo(hidden_states.dtype).max - 1000
            hidden_states = torch.clamp(
                hidden_states, min=-clamp_value, max=clamp_value
            )

        outputs = (hidden_states,)

        if output_attentions:
            outputs += (attn_weights,)

        if use_cache:
            outputs += (past_key_values,)

        return outputs</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.minicpmo.MiniCPMWhisperEncoderLayer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>hidden_states: torch.Tensor,<br>attention_mask: torch.Tensor,<br>layer_head_mask: torch.Tensor,<br>output_attentions: bool = False,<br>past_key_values: transformers.cache_utils.EncoderDecoderCache | None = None,<br>use_cache: bool | None = False) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    hidden_states: torch.Tensor,
    attention_mask: torch.Tensor,
    layer_head_mask: torch.Tensor,
    output_attentions: bool = False,
    past_key_values: Optional[EncoderDecoderCache] = None,
    use_cache: Optional[bool] = False,
) -&gt; torch.Tensor:
    r&#34;&#34;&#34;
    Args:
        hidden_states (`torch.FloatTensor` of shape `(batch_size, seq_len, embed_dim)`):
            Hidden states to be fed into the encoder layer.
        attention_mask (`torch.FloatTensor` of shape `(batch_size, 1, tgt_len, src_len)`):
            Attention mask where padding elements are indicated by large negative values.
        layer_head_mask (`torch.FloatTensor` of shape `(encoder_attention_heads,)`):
            Mask to nullify selected heads of the attention modules.
        output_attentions (`bool`, *optional*):
            Whether or not to return the attention weights.
        past_key_values (`EncoderDecoderCache`, *optional*):
            Past key-value pairs used for incremental decoding.
        use_cache (`bool`, *optional*):
            Whether or not to return updated `past_key_values` for caching.

    Returns:
        A tuple of shape `(hidden_states, optional(attn_weights), optional(past_key_values))`.
    &#34;&#34;&#34;
    residual = hidden_states
    hidden_states = self.self_attn_layer_norm(hidden_states)
    # TODO (lifuhuang): confirmed with Mick that the logic for past_key_values is copied from minicpmo official code,
    # currently we are not using past_key_values at all. We need to redesign the caching logic when we support streaming
    # in the future.
    hidden_states, attn_weights = self.self_attn(
        hidden_states=hidden_states,
        attention_mask=attention_mask,
        layer_head_mask=layer_head_mask,
        output_attentions=output_attentions,
        past_key_value=past_key_values,
    )
    hidden_states = nn.functional.dropout(
        hidden_states, p=self.dropout, training=False
    )
    hidden_states = residual + hidden_states

    residual = hidden_states
    hidden_states = self.final_layer_norm(hidden_states)
    hidden_states = self.activation_fn(self.fc1(hidden_states))
    hidden_states = nn.functional.dropout(
        hidden_states, p=self.activation_dropout, training=False
    )
    hidden_states = self.fc2(hidden_states)
    hidden_states = nn.functional.dropout(
        hidden_states, p=self.dropout, training=False
    )
    hidden_states = residual + hidden_states

    if hidden_states.dtype == torch.float16 and (
        torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()
    ):
        clamp_value = torch.finfo(hidden_states.dtype).max - 1000
        hidden_states = torch.clamp(
            hidden_states, min=-clamp_value, max=clamp_value
        )

    outputs = (hidden_states,)

    if output_attentions:
        outputs += (attn_weights,)

    if use_cache:
        outputs += (past_key_values,)

    return outputs</code></pre>
</details>
<div class="desc"><h2 id="args">Args</h2>
<p>hidden_states (<code>torch.FloatTensor</code> of shape <code>(batch_size, seq_len, embed_dim)</code>):
Hidden states to be fed into the encoder layer.
attention_mask (<code>torch.FloatTensor</code> of shape <code>(batch_size, 1, tgt_len, src_len)</code>):
Attention mask where padding elements are indicated by large negative values.
layer_head_mask (<code>torch.FloatTensor</code> of shape <code>(encoder_attention_heads,)</code>):
Mask to nullify selected heads of the attention modules.
output_attentions (<code>bool</code>, <em>optional</em>):
Whether or not to return the attention weights.
past_key_values (<code>EncoderDecoderCache</code>, <em>optional</em>):
Past key-value pairs used for incremental decoding.
use_cache (<code>bool</code>, <em>optional</em>):
Whether or not to return updated <code>past_key_values</code> for caching.</p>
<h2 id="returns">Returns</h2>
<p>A tuple of shape <code>(hidden_states, optional(attn_weights), optional(past_key_values))</code>.</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.minicpmo.MultiModalProjector"><code class="flex name class">
<span>class <span class="ident">MultiModalProjector</span></span>
<span>(</span><span>in_dim, out_dim)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiModalProjector(nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.linear1 = nn.Linear(in_features=in_dim, out_features=out_dim, bias=True)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(in_features=out_dim, out_features=out_dim, bias=True)

    def forward(self, audio_features):
        hidden_states = self.relu(self.linear1(audio_features))
        hidden_states = self.linear2(hidden_states)
        return hidden_states</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.minicpmo.MultiModalProjector.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, audio_features) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, audio_features):
    hidden_states = self.relu(self.linear1(audio_features))
    hidden_states = self.linear2(hidden_states)
    return hidden_states</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.models" href="index.html">sglang.srt.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.models.minicpmo.apply_spk_emb" href="#sglang.srt.models.minicpmo.apply_spk_emb">apply_spk_emb</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.make_streaming_chunk_mask_generation" href="#sglang.srt.models.minicpmo.make_streaming_chunk_mask_generation">make_streaming_chunk_mask_generation</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.models.minicpmo.ConditionalChatTTS" href="#sglang.srt.models.minicpmo.ConditionalChatTTS">ConditionalChatTTS</a></code></h4>
<ul class="two-column">
<li><code><a title="sglang.srt.models.minicpmo.ConditionalChatTTS.config_class" href="#sglang.srt.models.minicpmo.ConditionalChatTTS.config_class">config_class</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.ConditionalChatTTS.decode_to_mel_specs" href="#sglang.srt.models.minicpmo.ConditionalChatTTS.decode_to_mel_specs">decode_to_mel_specs</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.ConditionalChatTTS.generate" href="#sglang.srt.models.minicpmo.ConditionalChatTTS.generate">generate</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.ConditionalChatTTS.merge_inputs_embeds" href="#sglang.srt.models.minicpmo.ConditionalChatTTS.merge_inputs_embeds">merge_inputs_embeds</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.ConditionalChatTTS.prefill_audio_ids" href="#sglang.srt.models.minicpmo.ConditionalChatTTS.prefill_audio_ids">prefill_audio_ids</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.ConditionalChatTTS.prefill_text" href="#sglang.srt.models.minicpmo.ConditionalChatTTS.prefill_text">prefill_text</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.minicpmo.ConditionalChatTTSGenerationOutput" href="#sglang.srt.models.minicpmo.ConditionalChatTTSGenerationOutput">ConditionalChatTTSGenerationOutput</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.minicpmo.ConditionalChatTTSGenerationOutput.audio_input_ids" href="#sglang.srt.models.minicpmo.ConditionalChatTTSGenerationOutput.audio_input_ids">audio_input_ids</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.ConditionalChatTTSGenerationOutput.finished" href="#sglang.srt.models.minicpmo.ConditionalChatTTSGenerationOutput.finished">finished</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.ConditionalChatTTSGenerationOutput.new_ids" href="#sglang.srt.models.minicpmo.ConditionalChatTTSGenerationOutput.new_ids">new_ids</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.ConditionalChatTTSGenerationOutput.past_key_values" href="#sglang.srt.models.minicpmo.ConditionalChatTTSGenerationOutput.past_key_values">past_key_values</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.minicpmo.ConvNeXtBlock" href="#sglang.srt.models.minicpmo.ConvNeXtBlock">ConvNeXtBlock</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.minicpmo.ConvNeXtBlock.forward" href="#sglang.srt.models.minicpmo.ConvNeXtBlock.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.minicpmo.CustomRepetitionPenaltyLogitsProcessorRepeat" href="#sglang.srt.models.minicpmo.CustomRepetitionPenaltyLogitsProcessorRepeat">CustomRepetitionPenaltyLogitsProcessorRepeat</a></code></h4>
</li>
<li>
<h4><code><a title="sglang.srt.models.minicpmo.DVAE" href="#sglang.srt.models.minicpmo.DVAE">DVAE</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.minicpmo.DVAE.forward" href="#sglang.srt.models.minicpmo.DVAE.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.minicpmo.DVAEDecoder" href="#sglang.srt.models.minicpmo.DVAEDecoder">DVAEDecoder</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.minicpmo.DVAEDecoder.forward" href="#sglang.srt.models.minicpmo.DVAEDecoder.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.minicpmo.GFSQ" href="#sglang.srt.models.minicpmo.GFSQ">GFSQ</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.minicpmo.GFSQ.forward" href="#sglang.srt.models.minicpmo.GFSQ.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.minicpmo.MiniCPMO" href="#sglang.srt.models.minicpmo.MiniCPMO">MiniCPMO</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.minicpmo.MiniCPMO.get_audio_embedding" href="#sglang.srt.models.minicpmo.MiniCPMO.get_audio_embedding">get_audio_embedding</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.MiniCPMO.get_audio_embedding_streaming" href="#sglang.srt.models.minicpmo.MiniCPMO.get_audio_embedding_streaming">get_audio_embedding_streaming</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.MiniCPMO.get_audio_feature" href="#sglang.srt.models.minicpmo.MiniCPMO.get_audio_feature">get_audio_feature</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.MiniCPMO.get_image_feature" href="#sglang.srt.models.minicpmo.MiniCPMO.get_image_feature">get_image_feature</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.MiniCPMO.get_omni_embedding" href="#sglang.srt.models.minicpmo.MiniCPMO.get_omni_embedding">get_omni_embedding</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.MiniCPMO.init_audio_module" href="#sglang.srt.models.minicpmo.MiniCPMO.init_audio_module">init_audio_module</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.MiniCPMO.init_llm" href="#sglang.srt.models.minicpmo.MiniCPMO.init_llm">init_llm</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.MiniCPMO.init_resampler" href="#sglang.srt.models.minicpmo.MiniCPMO.init_resampler">init_resampler</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.MiniCPMO.init_tts_module" href="#sglang.srt.models.minicpmo.MiniCPMO.init_tts_module">init_tts_module</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.MiniCPMO.init_vision_module" href="#sglang.srt.models.minicpmo.MiniCPMO.init_vision_module">init_vision_module</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.MiniCPMO.load_weights" href="#sglang.srt.models.minicpmo.MiniCPMO.load_weights">load_weights</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.MiniCPMO.pad_input_ids" href="#sglang.srt.models.minicpmo.MiniCPMO.pad_input_ids">pad_input_ids</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.MiniCPMO.subsequent_chunk_mask" href="#sglang.srt.models.minicpmo.MiniCPMO.subsequent_chunk_mask">subsequent_chunk_mask</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.minicpmo.MiniCPMWhisperEncoder" href="#sglang.srt.models.minicpmo.MiniCPMWhisperEncoder">MiniCPMWhisperEncoder</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.minicpmo.MiniCPMWhisperEncoder.config_class" href="#sglang.srt.models.minicpmo.MiniCPMWhisperEncoder.config_class">config_class</a></code></li>
<li><code><a title="sglang.srt.models.minicpmo.MiniCPMWhisperEncoder.forward" href="#sglang.srt.models.minicpmo.MiniCPMWhisperEncoder.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.minicpmo.MiniCPMWhisperEncoderLayer" href="#sglang.srt.models.minicpmo.MiniCPMWhisperEncoderLayer">MiniCPMWhisperEncoderLayer</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.minicpmo.MiniCPMWhisperEncoderLayer.forward" href="#sglang.srt.models.minicpmo.MiniCPMWhisperEncoderLayer.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.minicpmo.MultiModalProjector" href="#sglang.srt.models.minicpmo.MultiModalProjector">MultiModalProjector</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.minicpmo.MultiModalProjector.forward" href="#sglang.srt.models.minicpmo.MultiModalProjector.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
