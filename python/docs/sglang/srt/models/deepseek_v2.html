<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.models.deepseek_v2 API documentation</title>
<meta name="description" content="Inference-only DeepseekV2 model.">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.models.deepseek_v2</code></h1>
</header>
<section id="section-intro">
<p>Inference-only DeepseekV2 model.</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.models.deepseek_v2.yarn_get_mscale"><code class="name flex">
<span>def <span class="ident">yarn_get_mscale</span></span>(<span>scale: float = 1, mscale: float = 1) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def yarn_get_mscale(scale: float = 1, mscale: float = 1) -&gt; float:
    import math

    if scale &lt;= 1:
        return 1.0
    return 0.1 * mscale * math.log(scale) + 1.0</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.models.deepseek_v2.AttnForwardMethod"><code class="flex name class">
<span>class <span class="ident">AttnForwardMethod</span></span>
<span>(</span><span>*args, **kwds)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AttnForwardMethod(IntEnum):
    # Use multi-head attention
    MHA = auto()

    # Use absorbed multi-latent attention
    MLA = auto()

    # Use multi-head attention, but with KV cache chunked.
    # This method can avoid OOM when prefix lengths are long.
    MHA_CHUNKED_KV = auto()

    # Use MLA but with fused RoPE
    MLA_FUSED_ROPE = auto()

    # Use MLA with fused RoPE kernel for CPU
    MLA_FUSED_ROPE_CPU = auto()</code></pre>
</details>
<div class="desc"><p>Enum where members are also (and must be) ints</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.IntEnum</li>
<li>builtins.int</li>
<li>enum.ReprEnum</li>
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.deepseek_v2.AttnForwardMethod.MHA"><code class="name">var <span class="ident">MHA</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.AttnForwardMethod.MHA_CHUNKED_KV"><code class="name">var <span class="ident">MHA_CHUNKED_KV</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.AttnForwardMethod.MLA"><code class="name">var <span class="ident">MLA</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.AttnForwardMethod.MLA_FUSED_ROPE"><code class="name">var <span class="ident">MLA_FUSED_ROPE</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.AttnForwardMethod.MLA_FUSED_ROPE_CPU"><code class="name">var <span class="ident">MLA_FUSED_ROPE_CPU</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA"><code class="flex name class">
<span>class <span class="ident">DeepseekV2AttentionMLA</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig,<br>hidden_size: int,<br>num_heads: int,<br>qk_nope_head_dim: int,<br>qk_rope_head_dim: int,<br>v_head_dim: int,<br>q_lora_rank: int,<br>kv_lora_rank: int,<br>rope_theta: float = 10000,<br>rope_scaling: Dict[str, Any] | None = None,<br>max_position_embeddings: int = 8192,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>reduce_results: bool = True,<br>layer_id: int = None,<br>prefix: str = '',<br>alt_stream: torch.cuda.streams.Stream | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DeepseekV2AttentionMLA(nn.Module):

    def __init__(
        self,
        config: PretrainedConfig,
        hidden_size: int,
        num_heads: int,
        qk_nope_head_dim: int,
        qk_rope_head_dim: int,
        v_head_dim: int,
        q_lora_rank: int,
        kv_lora_rank: int,
        rope_theta: float = 10000,
        rope_scaling: Optional[Dict[str, Any]] = None,
        max_position_embeddings: int = 8192,
        quant_config: Optional[QuantizationConfig] = None,
        reduce_results: bool = True,
        layer_id: int = None,
        prefix: str = &#34;&#34;,
        alt_stream: Optional[torch.cuda.Stream] = None,
    ) -&gt; None:
        super().__init__()
        self.layer_id = layer_id
        self.hidden_size = hidden_size
        self.qk_nope_head_dim = qk_nope_head_dim
        self.qk_rope_head_dim = qk_rope_head_dim
        self.qk_head_dim = qk_nope_head_dim + qk_rope_head_dim
        self.v_head_dim = v_head_dim
        self.q_lora_rank = q_lora_rank
        self.kv_lora_rank = kv_lora_rank
        attn_tp_rank = get_attention_tp_rank()
        attn_tp_size = get_attention_tp_size()

        self.num_heads = num_heads
        assert num_heads % attn_tp_size == 0
        self.num_local_heads = num_heads // attn_tp_size
        self.scaling = self.qk_head_dim**-0.5
        self.rope_theta = rope_theta
        self.max_position_embeddings = max_position_embeddings

        # For tensor parallel attention
        if self.q_lora_rank is not None:
            self.fused_qkv_a_proj_with_mqa = ReplicatedLinear(
                self.hidden_size,
                self.q_lora_rank + self.kv_lora_rank + self.qk_rope_head_dim,
                bias=False,
                quant_config=quant_config,
                prefix=add_prefix(&#34;fused_qkv_a_proj_with_mqa&#34;, prefix),
            )
            self.q_a_layernorm = RMSNorm(self.q_lora_rank, eps=config.rms_norm_eps)
            self.q_b_proj = ColumnParallelLinear(
                q_lora_rank,
                self.num_heads * self.qk_head_dim,
                bias=False,
                quant_config=quant_config,
                prefix=add_prefix(&#34;q_b_proj&#34;, prefix),
                tp_rank=attn_tp_rank,
                tp_size=attn_tp_size,
            )
        else:
            self.q_proj = ColumnParallelLinear(
                self.hidden_size,
                self.num_heads * self.qk_head_dim,
                bias=False,
                quant_config=quant_config,
                prefix=add_prefix(&#34;q_proj&#34;, prefix),
                tp_rank=attn_tp_rank,
                tp_size=attn_tp_size,
            )
            self.kv_a_proj_with_mqa = ReplicatedLinear(
                self.hidden_size,
                self.kv_lora_rank + self.qk_rope_head_dim,
                bias=False,
                quant_config=quant_config,
                prefix=add_prefix(&#34;kv_a_proj_with_mqa&#34;, prefix),
            )

        self.kv_b_proj = ColumnParallelLinear(
            self.kv_lora_rank,
            self.num_heads * (self.qk_nope_head_dim + self.v_head_dim),
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;kv_b_proj&#34;, prefix),
            tp_rank=attn_tp_rank,
            tp_size=attn_tp_size,
        )
        # O projection.
        self.o_proj = RowParallelLinear(
            self.num_heads * self.v_head_dim,
            self.hidden_size,
            bias=False,
            quant_config=quant_config,
            reduce_results=reduce_results,
            prefix=add_prefix(&#34;o_proj&#34;, prefix),
            tp_rank=attn_tp_rank,
            tp_size=attn_tp_size,
        )
        self.kv_a_layernorm = RMSNorm(self.kv_lora_rank, eps=config.rms_norm_eps)

        if rope_scaling:
            rope_scaling[&#34;rope_type&#34;] = &#34;deepseek_yarn&#34;

        self.rotary_emb = get_rope_wrapper(
            qk_rope_head_dim,
            rotary_dim=qk_rope_head_dim,
            max_position=max_position_embeddings,
            base=rope_theta,
            rope_scaling=rope_scaling,
            is_neox_style=False,
            device=global_server_args_dict[&#34;device&#34;],
        )

        if rope_scaling:
            mscale_all_dim = rope_scaling.get(&#34;mscale_all_dim&#34;, False)
            scaling_factor = rope_scaling[&#34;factor&#34;]
            mscale = yarn_get_mscale(scaling_factor, float(mscale_all_dim))
            self.scaling = self.scaling * mscale * mscale
        else:
            self.rotary_emb.forward = self.rotary_emb.forward_native

        self.attn_mqa = RadixAttention(
            self.num_local_heads,
            self.kv_lora_rank + self.qk_rope_head_dim,
            self.scaling,
            num_kv_heads=1,
            layer_id=layer_id,
            v_head_dim=self.kv_lora_rank,
            quant_config=quant_config,
            prefix=add_prefix(&#34;attn_mqa&#34;, prefix),
        )

        self.attn_mha = RadixAttention(
            self.num_local_heads,
            self.qk_nope_head_dim + self.qk_rope_head_dim,
            self.scaling,
            num_kv_heads=self.num_local_heads,
            layer_id=layer_id,
            v_head_dim=self.v_head_dim,
            quant_config=quant_config,
            prefix=add_prefix(&#34;attn_mha&#34;, prefix),
        )

        self.alt_stream = alt_stream
        self.attn_mha.kv_b_proj = None

        self.w_kc = None
        self.w_vc = None
        self.w_scale = 1.0

        self.w_scale_k = None
        self.w_scale_v = None
        self.use_deep_gemm_bmm = False

        self.flashinfer_mla_disable_ragged = global_server_args_dict[
            &#34;flashinfer_mla_disable_ragged&#34;
        ]
        self.disable_chunked_prefix_cache = global_server_args_dict[
            &#34;disable_chunked_prefix_cache&#34;
        ]

        self.current_attention_backend = (
            None  # Attention backend used by current forward batch
        )
        self.rocm_fused_decode_mla = get_bool_env_var(
            &#34;SGLANG_ROCM_FUSED_DECODE_MLA&#34;, &#34;false&#34;
        )

        # TODO: Design a finer way to determine the threshold
        self.chunked_prefix_cache_threshold = get_int_env_var(
            &#34;SGL_CHUNKED_PREFIX_CACHE_THRESHOLD&#34;, 8192
        )

        # If we have self.fused_qkv_a_proj_with_mqa and we&#39;re running on CPU, we will choose the torch.ops.sgl_kernel.qkv_proj_with_rope_fused_weight kernel
        # which requires self.w_kc and self.w_vc to be packed.
        # If not, we will use torch.bmm and weight shouldn&#39;t be packed in this case
        has_fused_proj = hasattr(self, &#34;fused_qkv_a_proj_with_mqa&#34;)
        if has_fused_proj and _is_cpu and _is_cpu_amx_available:
            self.quant_method = PackWeightMethod(
                weight_names=[&#34;w_kc&#34;, &#34;w_vc&#34;], transpose_dims=[[1, 2], [1, 2]]
            )

        is_packed_weight = (
            has_fused_proj
            and hasattr(self.fused_qkv_a_proj_with_mqa.quant_method, &#34;quant_config&#34;)
            and self.fused_qkv_a_proj_with_mqa.quant_method.quant_config.get_name()
            in {&#34;awq&#34;, &#34;awq_marlin&#34;, &#34;moe_wna16&#34;}
        )
        self.use_min_latency_fused_a_gemm = (
            has_fused_proj
            and not is_packed_weight
            and self.fused_qkv_a_proj_with_mqa.weight.dtype == torch.bfloat16
            and self.fused_qkv_a_proj_with_mqa.weight.shape[0] == 2112
            and self.fused_qkv_a_proj_with_mqa.weight.shape[1] == 7168
            and _is_cuda
            and _device_sm &gt;= 90
        )

        self.qkv_proj_with_rope_is_int8 = (
            has_fused_proj
            and not is_packed_weight
            and self.fused_qkv_a_proj_with_mqa.weight.dtype == torch.int8
        )
        self.qkv_proj_with_rope_is_fp8 = (
            has_fused_proj
            and not is_packed_weight
            and self.fused_qkv_a_proj_with_mqa.weight.dtype == torch.float8_e4m3fn
        )

        self.weight_block_size = None
        if self.qkv_proj_with_rope_is_fp8 and _is_cpu and _is_cpu_amx_available:
            assert getattr(
                self.fused_qkv_a_proj_with_mqa.quant_method, &#34;block_quant&#34;, False
            ) == getattr(self.q_b_proj.quant_method, &#34;block_quant&#34;, False)
            use_block_quant = getattr(
                self.fused_qkv_a_proj_with_mqa.quant_method, &#34;block_quant&#34;, False
            )

            if use_block_quant:
                assert (
                    self.fused_qkv_a_proj_with_mqa.quant_method.quant_config.weight_block_size
                    == self.q_b_proj.quant_method.quant_config.weight_block_size
                )
                self.weight_block_size = (
                    self.fused_qkv_a_proj_with_mqa.quant_method.quant_config.weight_block_size
                )

    def dispatch_attn_forward_method(
        self, forward_batch: ForwardBatch
    ) -&gt; AttnForwardMethod:
        def _dispatch_mla_subtype():
            if _is_hip:
                if (
                    self.rocm_fused_decode_mla
                    and forward_batch.forward_mode.is_decode()
                ):
                    return AttnForwardMethod.MLA_FUSED_ROPE
                else:
                    return AttnForwardMethod.MLA
            else:
                if hasattr(self, &#34;fused_qkv_a_proj_with_mqa&#34;) and use_intel_amx_backend(
                    self
                ):
                    return AttnForwardMethod.MLA_FUSED_ROPE_CPU
                else:
                    return AttnForwardMethod.MLA

        # Determine attention backend used by current forward batch
        if forward_batch.forward_mode.is_decode_or_idle():
            attention_backend = global_server_args_dict[&#34;decode_attention_backend&#34;]
        else:
            attention_backend = global_server_args_dict[&#34;prefill_attention_backend&#34;]
        self.current_attention_backend = attention_backend

        if attention_backend == &#34;ascend&#34;:
            if (
                forward_batch.forward_mode.is_extend()
                and not forward_batch.forward_mode.is_target_verify()
                and not forward_batch.forward_mode.is_draft_extend()
            ):
                return AttnForwardMethod.MHA
            else:
                return AttnForwardMethod.MLA
        elif (
            attention_backend == &#34;flashinfer&#34;
            or attention_backend == &#34;fa3&#34;
            or attention_backend == &#34;flashmla&#34;
            or attention_backend == &#34;trtllm_mla&#34;
            or attention_backend == &#34;cutlass_mla&#34;
        ):
            # Use MHA with chunked KV cache when prefilling on long sequences.
            sum_extend_prefix_lens = (
                sum(forward_batch.extend_prefix_lens_cpu)
                if forward_batch.extend_prefix_lens_cpu is not None
                else 0
            )
            # Flashinfer MLA: Do not absorb when enabling ragged prefill
            disable_ragged = (
                attention_backend == &#34;flashinfer&#34; or attention_backend == &#34;flashmla&#34;
            ) and self.flashinfer_mla_disable_ragged
            if (
                not disable_ragged
                and forward_batch.forward_mode.is_extend()
                and not forward_batch.forward_mode.is_target_verify()
                and not forward_batch.forward_mode.is_draft_extend()
                and (
                    (
                        sum_extend_prefix_lens &gt;= self.chunked_prefix_cache_threshold
                        and not self.disable_chunked_prefix_cache
                    )
                    or sum_extend_prefix_lens == 0
                )
            ):
                return AttnForwardMethod.MHA_CHUNKED_KV
            else:
                return _dispatch_mla_subtype()
        elif attention_backend == &#34;aiter&#34;:
            if (
                forward_batch.forward_mode.is_extend()
                and not forward_batch.forward_mode.is_target_verify()
                and not forward_batch.forward_mode.is_draft_extend()
            ):
                return AttnForwardMethod.MHA
            else:
                return AttnForwardMethod.MLA
        else:
            # Triton: Use normal computation for prefill and use weight absorption for extend/decode
            if (
                forward_batch.forward_mode.is_extend()
                and not forward_batch.forward_mode.is_target_verify()
                and not forward_batch.forward_mode.is_draft_extend()
                and sum(forward_batch.extend_prefix_lens_cpu) == 0
            ):
                return AttnForwardMethod.MHA
            else:
                return _dispatch_mla_subtype()

    def op_prepare(self, state):
        state.attn_intermediate_state = self.forward_prepare(
            positions=state.positions,
            hidden_states=state.pop(&#34;hidden_states_after_comm_pre_attn&#34;),
            forward_batch=state.forward_batch,
            zero_allocator=state.zero_allocator,
        )

    def op_core(self, state):
        state.hidden_states_after_attn = self.forward_core(
            state.pop(&#34;attn_intermediate_state&#34;)
        )

    def forward(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        forward_batch: ForwardBatch,
        zero_allocator: BumpAllocator,
    ):
        s = self.forward_prepare(
            positions=positions,
            hidden_states=hidden_states,
            forward_batch=forward_batch,
            zero_allocator=zero_allocator,
        )
        return self.forward_core(s)

    def forward_prepare(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        forward_batch: ForwardBatch,
        zero_allocator: BumpAllocator,
    ):
        if self.attn_mha.kv_b_proj is None:
            self.attn_mha.kv_b_proj = self.kv_b_proj

        if hidden_states.shape[0] == 0:
            assert (
                not self.o_proj.reduce_results
            ), &#34;short-circuiting allreduce will lead to hangs&#34;
            return hidden_states, None, forward_batch, None

        attn_forward_method = self.dispatch_attn_forward_method(forward_batch)

        if attn_forward_method == AttnForwardMethod.MHA:
            inner_state = self.forward_normal_prepare(
                positions, hidden_states, forward_batch, zero_allocator
            )
        elif attn_forward_method == AttnForwardMethod.MHA_CHUNKED_KV:
            inner_state = self.forward_normal_chunked_kv_prepare(
                positions, hidden_states, forward_batch, zero_allocator
            )
        elif attn_forward_method == AttnForwardMethod.MLA:
            inner_state = self.forward_absorb_prepare(
                positions, hidden_states, forward_batch, zero_allocator
            )
        elif attn_forward_method == AttnForwardMethod.MLA_FUSED_ROPE:
            inner_state = self.forward_absorb_fused_mla_rope_prepare(
                positions, hidden_states, forward_batch, zero_allocator
            )
        elif attn_forward_method == AttnForwardMethod.MLA_FUSED_ROPE_CPU:
            inner_state = self.forward_absorb_fused_mla_rope_cpu_prepare(
                positions, hidden_states, forward_batch, zero_allocator
            )
        else:
            raise NotImplementedError
        return None, attn_forward_method, forward_batch, inner_state

    def forward_core(self, intermediate_state):
        hidden_states, attn_forward_method, forward_batch, inner_state = (
            intermediate_state
        )
        if inner_state is None:
            return hidden_states

        if attn_forward_method == AttnForwardMethod.MHA:
            return self.forward_normal_core(*inner_state)
        elif attn_forward_method == AttnForwardMethod.MHA_CHUNKED_KV:
            return self.forward_normal_chunked_kv_core(*inner_state)
        elif attn_forward_method == AttnForwardMethod.MLA:
            return self.forward_absorb_core(*inner_state)
        elif attn_forward_method == AttnForwardMethod.MLA_FUSED_ROPE:
            return self.forward_absorb_fused_mla_rope_core(*inner_state)
        elif attn_forward_method == AttnForwardMethod.MLA_FUSED_ROPE_CPU:
            return self.forward_absorb_fused_mla_rope_cpu_core(*inner_state)
        else:
            raise NotImplementedError

    def forward_normal_prepare(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        forward_batch: ForwardBatch,
        zero_allocator: BumpAllocator,
    ):
        if self.q_lora_rank is not None:
            q, latent_cache = self.fused_qkv_a_proj_with_mqa(hidden_states)[0].split(
                [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim], dim=-1
            )
            q = self.q_a_layernorm(q)
            q = self.q_b_proj(q)[0].view(-1, self.num_local_heads, self.qk_head_dim)
        else:
            q = self.q_proj(hidden_states)[0].view(
                -1, self.num_local_heads, self.qk_head_dim
            )
            latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]

        _, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)
        kv_a, _ = latent_cache.split([self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)
        latent_cache = latent_cache.unsqueeze(1)
        kv_a = self.kv_a_layernorm(kv_a)
        kv = self.kv_b_proj(kv_a)[0]
        kv = kv.view(-1, self.num_local_heads, self.qk_nope_head_dim + self.v_head_dim)
        k_nope = kv[..., : self.qk_nope_head_dim]
        v = kv[..., self.qk_nope_head_dim :]
        k_pe = latent_cache[:, :, self.kv_lora_rank :]
        q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe)
        q[..., self.qk_nope_head_dim :] = q_pe
        k = torch.empty_like(q)
        k[..., : self.qk_nope_head_dim] = k_nope
        k[..., self.qk_nope_head_dim :] = k_pe

        if not _is_npu:
            latent_cache[:, :, : self.kv_lora_rank] = kv_a.unsqueeze(1)
            latent_cache[:, :, self.kv_lora_rank :] = k_pe

            # Save latent cache
            forward_batch.token_to_kv_pool.set_kv_buffer(
                self.attn_mha, forward_batch.out_cache_loc, latent_cache, None
            )
        else:
            # To reduce a time-costing split operation
            forward_batch.token_to_kv_pool.set_kv_buffer(
                self.attn_mha, forward_batch.out_cache_loc, kv_a.unsqueeze(1), k_pe
            )

        return q, k, v, forward_batch

    def forward_normal_core(self, q, k, v, forward_batch):
        attn_output = self.attn_mha(q, k, v, forward_batch, save_kv_cache=False)
        attn_output = attn_output.reshape(-1, self.num_local_heads * self.v_head_dim)
        output, _ = self.o_proj(attn_output)
        return output

    def _fuse_rope_for_trtllm_mla(self, forward_batch: ForwardBatch) -&gt; bool:
        &#34;&#34;&#34;
        Check if we should skip rope and do fused rope+quantize for TRTLLM MLA decode in fp8_e4m3 path.
        &#34;&#34;&#34;
        return (
            self.current_attention_backend == &#34;trtllm_mla&#34;
            and forward_batch.forward_mode.is_decode_or_idle()
            and forward_batch.attn_backend.data_type == torch.float8_e4m3fn
        )

    def forward_absorb_prepare(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        forward_batch: ForwardBatch,
        zero_allocator: BumpAllocator,
    ):
        from sglang.srt.model_executor.cuda_graph_runner import get_is_capture_mode

        if self.q_lora_rank is not None:
            if hidden_states.shape[0] &lt;= 16 and self.use_min_latency_fused_a_gemm:
                fused_qkv_a_proj_out = dsv3_fused_a_gemm(
                    hidden_states, self.fused_qkv_a_proj_with_mqa.weight.T
                )
            else:
                fused_qkv_a_proj_out = self.fused_qkv_a_proj_with_mqa(hidden_states)[0]
            q, latent_cache = fused_qkv_a_proj_out.split(
                [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim], dim=-1
            )
            k_nope = latent_cache[..., : self.kv_lora_rank]

            # overlap qk norm
            if self.alt_stream is not None and get_is_capture_mode():
                current_stream = torch.cuda.current_stream()
                self.alt_stream.wait_stream(current_stream)
                q = self.q_a_layernorm(q)
                with torch.cuda.stream(self.alt_stream):
                    k_nope = self.kv_a_layernorm(k_nope)
                current_stream.wait_stream(self.alt_stream)
            else:
                q = self.q_a_layernorm(q)
                k_nope = self.kv_a_layernorm(k_nope)

            k_nope = k_nope.unsqueeze(1)
            q = self.q_b_proj(q)[0].view(-1, self.num_local_heads, self.qk_head_dim)
        else:
            q = self.q_proj(hidden_states)[0].view(
                -1, self.num_local_heads, self.qk_head_dim
            )
            latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]
            k_nope = latent_cache[..., : self.kv_lora_rank]
            k_nope = self.kv_a_layernorm(k_nope).unsqueeze(1)

        q_nope, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)
        k_pe = latent_cache[..., self.kv_lora_rank :].unsqueeze(1)

        if self.use_deep_gemm_bmm:
            q_nope_val, q_nope_scale, masked_m, expected_m, aligned_m = (
                per_token_group_quant_mla_deep_gemm_masked_fp8(q_nope.transpose(0, 1))
            )
            q_nope_out = q_nope.new_empty(
                (self.num_local_heads, aligned_m, self.kv_lora_rank)
            )
            deep_gemm_wrapper.grouped_gemm_nt_f8f8bf16_masked(
                (q_nope_val, q_nope_scale),
                (self.w_kc, self.w_scale_k),
                q_nope_out,
                masked_m,
                expected_m,
            )
            q_nope_out = q_nope_out[:, :expected_m, :]
        elif _is_hip:
            # TODO(haishaw): add bmm_fp8 to ROCm
            q_nope_out = torch.bmm(
                q_nope.to(torch.bfloat16).transpose(0, 1),
                self.w_kc.to(torch.bfloat16) * self.w_scale,
            )
        elif self.w_kc.dtype == torch.float8_e4m3fn:
            q_nope_val, q_nope_scale = per_tensor_quant_mla_fp8(
                q_nope.transpose(0, 1),
                zero_allocator.allocate(1),
            )
            q_nope_out = bmm_fp8(
                q_nope_val, self.w_kc, q_nope_scale, self.w_scale, torch.bfloat16
            )
        else:
            q_nope_out = torch.bmm(q_nope.transpose(0, 1), self.w_kc)

        q_nope_out = q_nope_out.transpose(0, 1)

        if not self._fuse_rope_for_trtllm_mla(forward_batch):
            q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe)

        return q_pe, k_pe, q_nope_out, k_nope, forward_batch, zero_allocator

    def forward_absorb_core(
        self, q_pe, k_pe, q_nope_out, k_nope, forward_batch, zero_allocator
    ):
        if (
            self.current_attention_backend == &#34;fa3&#34;
            or self.current_attention_backend == &#34;flashinfer&#34;
            or self.current_attention_backend == &#34;cutlass_mla&#34;
            or self.current_attention_backend == &#34;trtllm_mla&#34;
            or self.current_attention_backend == &#34;ascend&#34;
        ):
            extra_args = {}
            if self._fuse_rope_for_trtllm_mla(forward_batch):
                extra_args = {
                    &#34;cos_sin_cache&#34;: self.rotary_emb.cos_sin_cache,
                    &#34;is_neox&#34;: self.rotary_emb.is_neox_style,
                }
            attn_output = self.attn_mqa(
                q_nope_out,
                k_nope,
                k_nope,
                forward_batch,
                q_rope=q_pe,
                k_rope=k_pe,
                **extra_args,
            )
        else:
            q = torch.cat([q_nope_out, q_pe], dim=-1)
            k = torch.cat([k_nope, k_pe], dim=-1)
            attn_output = self.attn_mqa(q, k, k_nope, forward_batch)
        attn_output = attn_output.view(-1, self.num_local_heads, self.kv_lora_rank)

        if self.use_deep_gemm_bmm:
            attn_output_val, attn_output_scale, masked_m, expected_m, aligned_m = (
                per_token_group_quant_mla_deep_gemm_masked_fp8(
                    attn_output.transpose(0, 1)
                )
            )
            attn_bmm_output = attn_output.new_empty(
                (self.num_local_heads, aligned_m, self.v_head_dim)
            )
            deep_gemm_wrapper.grouped_gemm_nt_f8f8bf16_masked(
                (attn_output_val, attn_output_scale),
                (self.w_vc, self.w_scale_v),
                attn_bmm_output,
                masked_m,
                expected_m,
            )
            attn_bmm_output = (
                attn_bmm_output[:, :expected_m, :].transpose(0, 1).flatten(1, 2)
            )
        elif _is_hip:
            # TODO(haishaw): add bmm_fp8 to ROCm
            attn_bmm_output = torch.bmm(
                attn_output.to(torch.bfloat16).transpose(0, 1),
                self.w_vc.to(torch.bfloat16) * self.w_scale,
            )
            attn_bmm_output = attn_bmm_output.transpose(0, 1).flatten(1, 2)
        elif self.w_vc.dtype == torch.float8_e4m3fn:
            attn_output_val, attn_output_scale = per_tensor_quant_mla_fp8(
                attn_output.transpose(0, 1),
                zero_allocator.allocate(1),
            )
            attn_bmm_output = bmm_fp8(
                attn_output_val,
                self.w_vc,
                attn_output_scale,
                self.w_scale,
                torch.bfloat16,
            )
            attn_bmm_output = attn_bmm_output.transpose(0, 1).flatten(1, 2)
        else:
            attn_bmm_output = torch.empty(
                (attn_output.shape[0], self.num_local_heads * self.v_head_dim),
                dtype=attn_output.dtype,
                device=attn_output.device,
            )
            torch.bmm(
                attn_output.transpose(0, 1),
                self.w_vc,
                out=attn_bmm_output.view(
                    -1, self.num_local_heads, self.v_head_dim
                ).transpose(0, 1),
            )
        output, _ = self.o_proj(attn_bmm_output)

        return output

    def forward_absorb_fused_mla_rope_prepare(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        forward_batch: ForwardBatch,
        zero_allocator: BumpAllocator,
    ):
        enable_rope_fusion = (
            os.getenv(&#34;SGLANG_FUSED_MLA_ENABLE_ROPE_FUSION&#34;, &#34;1&#34;) == &#34;1&#34;
        )
        q_len = hidden_states.shape[0]
        q_input = hidden_states.new_empty(
            q_len, self.num_local_heads, self.kv_lora_rank + self.qk_rope_head_dim
        )
        if self.q_lora_rank is not None:
            q, latent_cache = self.fused_qkv_a_proj_with_mqa(hidden_states)[0].split(
                [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim], dim=-1
            )
            q = self.q_a_layernorm(q)
            q = self.q_b_proj(q)[0].view(-1, self.num_local_heads, self.qk_head_dim)
        else:
            q = self.q_proj(hidden_states)[0].view(
                -1, self.num_local_heads, self.qk_head_dim
            )
            latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]
        q_nope, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)

        if _is_hip:
            # TODO(haishaw): add bmm_fp8 to ROCm
            q_nope_out = torch.bmm(
                q_nope.to(torch.bfloat16).transpose(0, 1),
                self.w_kc.to(torch.bfloat16) * self.w_scale,
            )
        elif self.w_kc.dtype == torch.float8_e4m3fn:
            q_nope_val, q_nope_scale = per_tensor_quant_mla_fp8(
                q_nope.transpose(0, 1),
                zero_allocator.allocate(1),
                dtype=torch.float8_e4m3fn,
            )
            q_nope_out = bmm_fp8(
                q_nope_val, self.w_kc, q_nope_scale, self.w_scale, torch.bfloat16
            )
        else:
            q_nope_out = torch.bmm(q_nope.transpose(0, 1), self.w_kc)
        q_input[..., : self.kv_lora_rank] = q_nope_out.transpose(0, 1)
        v_input = latent_cache[..., : self.kv_lora_rank]
        v_input = self.kv_a_layernorm(v_input.contiguous()).unsqueeze(1)
        k_input = latent_cache.unsqueeze(1)
        k_input[..., : self.kv_lora_rank] = v_input

        if not enable_rope_fusion:
            k_pe = k_input[..., self.kv_lora_rank :]
            q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe)
            q_input[..., self.kv_lora_rank :] = q_pe
            k_input[..., self.kv_lora_rank :] = k_pe
            k_pe_output = None
        else:
            k_pe_output = torch.empty_like(k_input[..., self.kv_lora_rank :])

        q_input[..., self.kv_lora_rank :] = q_pe

        # attn_output = self.attn_mqa(q_input, k_input, v_input, forward_batch)
        # Use Fused ROPE with use_rope=OFF.
        attn_output = torch.empty(
            (q_len, self.num_local_heads, self.kv_lora_rank),
            dtype=q.dtype,
            device=q.device,
        )
        attn_logits, _, kv_indptr, kv_indices, _, _, _ = (
            forward_batch.attn_backend.forward_metadata
        )
        cos_sin_cache = self.rotary_emb.cos_sin_cache
        num_kv_split = forward_batch.attn_backend.num_kv_splits
        sm_scale = self.attn_mqa.scaling
        if attn_logits is None:
            attn_logits = torch.empty(
                (
                    forward_batch.batch_size,
                    self.num_local_heads,
                    num_kv_split,
                    self.kv_lora_rank + 1,
                ),
                dtype=torch.float32,
                device=q.device,
            )

        # save current latent cache.
        forward_batch.token_to_kv_pool.set_kv_buffer(
            self.attn_mqa, forward_batch.out_cache_loc, k_input, None
        )
        key_cache_buf = forward_batch.token_to_kv_pool.get_key_buffer(
            self.attn_mqa.layer_id
        )
        val_cache_buf = key_cache_buf[..., : self.kv_lora_rank]

        return (
            q_input,
            key_cache_buf,
            val_cache_buf,
            attn_output,
            kv_indptr,
            kv_indices,
            k_pe_output,
            cos_sin_cache,
            positions,
            attn_logits,
            num_kv_split,
            sm_scale,
            enable_rope_fusion,
            k_input,
            forward_batch,
            zero_allocator,
        )

    def forward_absorb_fused_mla_rope_cpu_prepare(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        forward_batch: ForwardBatch,
        zero_allocator: BumpAllocator,
    ):
        assert self.q_lora_rank is not None and use_intel_amx_backend(
            self
        ), &#34;forward_absorb_fused_mla_rope_cpu_prepare requires q_lora_rank is not None and use_intel_amx_backend&#34;

        q_input, k_input, v_input = (
            torch.ops.sgl_kernel.qkv_proj_with_rope_fused_weight(
                hidden_states,
                self.fused_qkv_a_proj_with_mqa.weight,
                self.q_b_proj.weight,
                self.w_kc,
                self.q_a_layernorm.weight,
                self.kv_a_layernorm.weight,
                positions,
                self.rotary_emb.cos_sin_cache,
                self.kv_a_layernorm.variance_epsilon,
                self.qkv_proj_with_rope_is_int8,
                self.qkv_proj_with_rope_is_fp8,
                (
                    self.fused_qkv_a_proj_with_mqa.weight_scale
                    if self.qkv_proj_with_rope_is_int8
                    else (
                        self.fused_qkv_a_proj_with_mqa.weight_scale_inv
                        if self.qkv_proj_with_rope_is_fp8
                        else None
                    )
                ),
                (
                    self.q_b_proj.weight_scale
                    if self.qkv_proj_with_rope_is_int8
                    else (
                        self.q_b_proj.weight_scale_inv
                        if self.qkv_proj_with_rope_is_fp8
                        else None
                    )
                ),
                True,  # is_vnni
                self.weight_block_size,
                self.q_lora_rank,
                self.kv_lora_rank,
                self.qk_rope_head_dim,
            )
        )
        return (q_input, k_input, v_input, forward_batch, zero_allocator)

    def forward_absorb_fused_mla_rope_core(
        self,
        q_input,
        key_cache_buf,
        val_cache_buf,
        attn_output,
        kv_indptr,
        kv_indices,
        k_pe_output,
        cos_sin_cache,
        positions,
        attn_logits,
        num_kv_split,
        sm_scale,
        enable_rope_fusion,
        k_input,
        forward_batch,
        zero_allocator,
    ):
        decode_attention_fwd_grouped_rope(
            q_input,
            key_cache_buf,
            val_cache_buf,
            attn_output,
            kv_indptr,
            kv_indices,
            k_pe_output,
            self.kv_lora_rank,
            self.rotary_emb.rotary_dim,
            cos_sin_cache,
            positions,
            attn_logits,
            num_kv_split,
            sm_scale,
            logit_cap=self.attn_mqa.logit_cap,
            use_rope=enable_rope_fusion,
            is_neox_style=self.rotary_emb.is_neox_style,
        )

        if enable_rope_fusion:
            k_input[..., self.kv_lora_rank :] = k_pe_output
            forward_batch.token_to_kv_pool.set_kv_buffer(
                self.attn_mqa, forward_batch.out_cache_loc, k_input, None
            )

        attn_output = attn_output.view(-1, self.num_local_heads, self.kv_lora_rank)

        if _is_hip:
            # TODO(haishaw): add bmm_fp8 to ROCm
            attn_bmm_output = torch.bmm(
                attn_output.to(torch.bfloat16).transpose(0, 1),
                self.w_vc.to(torch.bfloat16) * self.w_scale,
            )
        elif self.w_vc.dtype == torch.float8_e4m3fn:
            attn_output_val, attn_output_scale = per_tensor_quant_mla_fp8(
                attn_output.transpose(0, 1),
                zero_allocator.allocate(1),
                dtype=torch.float8_e4m3fn,
            )
            attn_bmm_output = bmm_fp8(
                attn_output_val,
                self.w_vc,
                attn_output_scale,
                self.w_scale,
                torch.bfloat16,
            )
        else:
            attn_bmm_output = torch.bmm(attn_output.transpose(0, 1), self.w_vc)
        attn_output = attn_bmm_output.transpose(0, 1).flatten(1, 2)
        output, _ = self.o_proj(attn_output)

        return output

    def forward_absorb_fused_mla_rope_cpu_core(
        self, q_input, k_input, v_input, forward_batch, zero_allocator
    ):
        assert self.q_lora_rank is not None and use_intel_amx_backend(
            self
        ), &#34;forward_absorb_fused_mla_rope_cpu_core requires q_lora_rank is not None and use_intel_amx_backend&#34;

        attn_output = self.attn_mqa(q_input, k_input, v_input, forward_batch)
        attn_output = attn_output.view(-1, self.num_local_heads, self.kv_lora_rank)

        # [Note] Align shapes of bmm inputs.
        # Shapes of inputs:
        #   q_nope: [M, B, K]
        #   original self.w_kc: [B, K, N]
        #   current self.w_kc (which has been converted in PackWeightMethod): [B, N, K]

        # Shapes of inputs to sgl_kernel.cpu.bmm:
        #   out: [B, M, N]
        #   mat1: [B, M, K]
        #   mat2: [B, N, K]
        B = self.w_vc.size(0)
        N = self.w_vc.size(1)
        M = attn_output.size(0)
        output = torch.empty([M, int(B * N)], dtype=attn_output.dtype)
        attn_bmm_output = output.view([M, B, N]).transpose_(0, 1)
        torch.ops.sgl_kernel.bmm_cpu(
            attn_bmm_output,
            attn_output.transpose(0, 1),
            self.w_vc,
            True,  # is_vnni
            None,  # scale
        )
        attn_output = output
        output, _ = self.o_proj(attn_output)

        return output

    def _chunked_prefix_attn_mha(
        self,
        q: torch.Tensor,
        accum_output: torch.Tensor,
        accum_lse: torch.Tensor,
        forward_batch: ForwardBatch,
    ) -&gt; torch.Tensor:

        assert forward_batch.num_prefix_chunks is not None
        for i in range(forward_batch.num_prefix_chunks):
            forward_batch.set_prefix_chunk_idx(i)

            # Fetch latent cache from memory pool with precomputed chunked kv indices
            latent_cache_buf = forward_batch.token_to_kv_pool.get_key_buffer(
                self.attn_mha.layer_id
            )
            latent_cache = latent_cache_buf[
                forward_batch.prefix_chunk_kv_indices[i]
            ].contiguous()

            kv_a_normed, k_pe = latent_cache.split(
                [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1
            )
            kv_a_normed = kv_a_normed.squeeze(1).contiguous()
            kv = self.kv_b_proj(kv_a_normed)[0]
            kv = kv.view(
                -1, self.num_local_heads, self.qk_nope_head_dim + self.v_head_dim
            )
            v = kv[..., self.qk_nope_head_dim :]
            k_nope = kv[..., : self.qk_nope_head_dim]

            k = torch.empty(
                (
                    k_nope.shape[0],
                    self.num_local_heads,
                    self.qk_nope_head_dim + self.qk_rope_head_dim,
                ),
                dtype=v.dtype,
                device=v.device,
            )
            k[..., : self.qk_nope_head_dim] = k_nope
            k[..., self.qk_nope_head_dim :] = k_pe

            output, lse = self.attn_mha(q, k, v, forward_batch, save_kv_cache=False)
            tmp_output = torch.empty_like(accum_output)
            tmp_lse = torch.empty_like(accum_lse)
            merge_state_v2(output, lse, accum_output, accum_lse, tmp_output, tmp_lse)
            accum_output, accum_lse = tmp_output, tmp_lse

        return accum_output

    def forward_normal_chunked_kv_prepare(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        forward_batch: ForwardBatch,
        zero_allocator: BumpAllocator,
    ):
        # In normal mha, the k and v tensors will become overly large when the prefix length is long.
        # To avoid this, we split the kv cache into chunks and process them one after another.
        # Since mha is compute friendly, the for loop induced here will not introduce significant overhead.
        # The top comments in https://github.com/vllm-project/vllm/blob/main/vllm/v1/attention/backends/mla/common.py
        # will be helpful for understanding the purpose of this function.

        # First do normal mha forward to get output for extended part
        return self.forward_normal_prepare(
            positions, hidden_states, forward_batch, zero_allocator
        )

    def forward_normal_chunked_kv_core(self, q, k, v, forward_batch):
        has_extend_prefix = any(forward_batch.extend_prefix_lens_cpu)
        # Only initialize the info once
        if has_extend_prefix and forward_batch.num_prefix_chunks is None:
            forward_batch.prepare_chunked_prefix_cache_info(q.device)
            if hasattr(forward_batch.attn_backend, &#34;init_mha_chunk_metadata&#34;):
                forward_batch.attn_backend.init_mha_chunk_metadata(forward_batch)

        forward_batch.mha_return_lse = has_extend_prefix
        # Do mha for extended part without prefix
        forward_batch.set_attn_attend_prefix_cache(False)
        attn_output = self.attn_mha(q, k, v, forward_batch, save_kv_cache=False)

        # Do mha attention with chunked prefix cache if there are any sequence with prefix
        if has_extend_prefix:
            attn_output, lse = attn_output
            forward_batch.set_attn_attend_prefix_cache(True)
            attn_output = self._chunked_prefix_attn_mha(
                q=q,
                accum_output=attn_output,
                accum_lse=lse,
                forward_batch=forward_batch,
            )

        attn_output = attn_output.reshape(-1, self.num_local_heads * self.v_head_dim)
        output, _ = self.o_proj(attn_output)
        return output</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.dispatch_attn_forward_method"><code class="name flex">
<span>def <span class="ident">dispatch_attn_forward_method</span></span>(<span>self,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>) ‑> <a title="sglang.srt.models.deepseek_v2.AttnForwardMethod" href="#sglang.srt.models.deepseek_v2.AttnForwardMethod">AttnForwardMethod</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dispatch_attn_forward_method(
    self, forward_batch: ForwardBatch
) -&gt; AttnForwardMethod:
    def _dispatch_mla_subtype():
        if _is_hip:
            if (
                self.rocm_fused_decode_mla
                and forward_batch.forward_mode.is_decode()
            ):
                return AttnForwardMethod.MLA_FUSED_ROPE
            else:
                return AttnForwardMethod.MLA
        else:
            if hasattr(self, &#34;fused_qkv_a_proj_with_mqa&#34;) and use_intel_amx_backend(
                self
            ):
                return AttnForwardMethod.MLA_FUSED_ROPE_CPU
            else:
                return AttnForwardMethod.MLA

    # Determine attention backend used by current forward batch
    if forward_batch.forward_mode.is_decode_or_idle():
        attention_backend = global_server_args_dict[&#34;decode_attention_backend&#34;]
    else:
        attention_backend = global_server_args_dict[&#34;prefill_attention_backend&#34;]
    self.current_attention_backend = attention_backend

    if attention_backend == &#34;ascend&#34;:
        if (
            forward_batch.forward_mode.is_extend()
            and not forward_batch.forward_mode.is_target_verify()
            and not forward_batch.forward_mode.is_draft_extend()
        ):
            return AttnForwardMethod.MHA
        else:
            return AttnForwardMethod.MLA
    elif (
        attention_backend == &#34;flashinfer&#34;
        or attention_backend == &#34;fa3&#34;
        or attention_backend == &#34;flashmla&#34;
        or attention_backend == &#34;trtllm_mla&#34;
        or attention_backend == &#34;cutlass_mla&#34;
    ):
        # Use MHA with chunked KV cache when prefilling on long sequences.
        sum_extend_prefix_lens = (
            sum(forward_batch.extend_prefix_lens_cpu)
            if forward_batch.extend_prefix_lens_cpu is not None
            else 0
        )
        # Flashinfer MLA: Do not absorb when enabling ragged prefill
        disable_ragged = (
            attention_backend == &#34;flashinfer&#34; or attention_backend == &#34;flashmla&#34;
        ) and self.flashinfer_mla_disable_ragged
        if (
            not disable_ragged
            and forward_batch.forward_mode.is_extend()
            and not forward_batch.forward_mode.is_target_verify()
            and not forward_batch.forward_mode.is_draft_extend()
            and (
                (
                    sum_extend_prefix_lens &gt;= self.chunked_prefix_cache_threshold
                    and not self.disable_chunked_prefix_cache
                )
                or sum_extend_prefix_lens == 0
            )
        ):
            return AttnForwardMethod.MHA_CHUNKED_KV
        else:
            return _dispatch_mla_subtype()
    elif attention_backend == &#34;aiter&#34;:
        if (
            forward_batch.forward_mode.is_extend()
            and not forward_batch.forward_mode.is_target_verify()
            and not forward_batch.forward_mode.is_draft_extend()
        ):
            return AttnForwardMethod.MHA
        else:
            return AttnForwardMethod.MLA
    else:
        # Triton: Use normal computation for prefill and use weight absorption for extend/decode
        if (
            forward_batch.forward_mode.is_extend()
            and not forward_batch.forward_mode.is_target_verify()
            and not forward_batch.forward_mode.is_draft_extend()
            and sum(forward_batch.extend_prefix_lens_cpu) == 0
        ):
            return AttnForwardMethod.MHA
        else:
            return _dispatch_mla_subtype()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>positions: torch.Tensor,<br>hidden_states: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>zero_allocator: <a title="sglang.srt.utils.BumpAllocator" href="../utils.html#sglang.srt.utils.BumpAllocator">BumpAllocator</a>) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    positions: torch.Tensor,
    hidden_states: torch.Tensor,
    forward_batch: ForwardBatch,
    zero_allocator: BumpAllocator,
):
    s = self.forward_prepare(
        positions=positions,
        hidden_states=hidden_states,
        forward_batch=forward_batch,
        zero_allocator=zero_allocator,
    )
    return self.forward_core(s)</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_absorb_core"><code class="name flex">
<span>def <span class="ident">forward_absorb_core</span></span>(<span>self, q_pe, k_pe, q_nope_out, k_nope, forward_batch, zero_allocator)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_absorb_core(
    self, q_pe, k_pe, q_nope_out, k_nope, forward_batch, zero_allocator
):
    if (
        self.current_attention_backend == &#34;fa3&#34;
        or self.current_attention_backend == &#34;flashinfer&#34;
        or self.current_attention_backend == &#34;cutlass_mla&#34;
        or self.current_attention_backend == &#34;trtllm_mla&#34;
        or self.current_attention_backend == &#34;ascend&#34;
    ):
        extra_args = {}
        if self._fuse_rope_for_trtllm_mla(forward_batch):
            extra_args = {
                &#34;cos_sin_cache&#34;: self.rotary_emb.cos_sin_cache,
                &#34;is_neox&#34;: self.rotary_emb.is_neox_style,
            }
        attn_output = self.attn_mqa(
            q_nope_out,
            k_nope,
            k_nope,
            forward_batch,
            q_rope=q_pe,
            k_rope=k_pe,
            **extra_args,
        )
    else:
        q = torch.cat([q_nope_out, q_pe], dim=-1)
        k = torch.cat([k_nope, k_pe], dim=-1)
        attn_output = self.attn_mqa(q, k, k_nope, forward_batch)
    attn_output = attn_output.view(-1, self.num_local_heads, self.kv_lora_rank)

    if self.use_deep_gemm_bmm:
        attn_output_val, attn_output_scale, masked_m, expected_m, aligned_m = (
            per_token_group_quant_mla_deep_gemm_masked_fp8(
                attn_output.transpose(0, 1)
            )
        )
        attn_bmm_output = attn_output.new_empty(
            (self.num_local_heads, aligned_m, self.v_head_dim)
        )
        deep_gemm_wrapper.grouped_gemm_nt_f8f8bf16_masked(
            (attn_output_val, attn_output_scale),
            (self.w_vc, self.w_scale_v),
            attn_bmm_output,
            masked_m,
            expected_m,
        )
        attn_bmm_output = (
            attn_bmm_output[:, :expected_m, :].transpose(0, 1).flatten(1, 2)
        )
    elif _is_hip:
        # TODO(haishaw): add bmm_fp8 to ROCm
        attn_bmm_output = torch.bmm(
            attn_output.to(torch.bfloat16).transpose(0, 1),
            self.w_vc.to(torch.bfloat16) * self.w_scale,
        )
        attn_bmm_output = attn_bmm_output.transpose(0, 1).flatten(1, 2)
    elif self.w_vc.dtype == torch.float8_e4m3fn:
        attn_output_val, attn_output_scale = per_tensor_quant_mla_fp8(
            attn_output.transpose(0, 1),
            zero_allocator.allocate(1),
        )
        attn_bmm_output = bmm_fp8(
            attn_output_val,
            self.w_vc,
            attn_output_scale,
            self.w_scale,
            torch.bfloat16,
        )
        attn_bmm_output = attn_bmm_output.transpose(0, 1).flatten(1, 2)
    else:
        attn_bmm_output = torch.empty(
            (attn_output.shape[0], self.num_local_heads * self.v_head_dim),
            dtype=attn_output.dtype,
            device=attn_output.device,
        )
        torch.bmm(
            attn_output.transpose(0, 1),
            self.w_vc,
            out=attn_bmm_output.view(
                -1, self.num_local_heads, self.v_head_dim
            ).transpose(0, 1),
        )
    output, _ = self.o_proj(attn_bmm_output)

    return output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_core"><code class="name flex">
<span>def <span class="ident">forward_absorb_fused_mla_rope_core</span></span>(<span>self,<br>q_input,<br>key_cache_buf,<br>val_cache_buf,<br>attn_output,<br>kv_indptr,<br>kv_indices,<br>k_pe_output,<br>cos_sin_cache,<br>positions,<br>attn_logits,<br>num_kv_split,<br>sm_scale,<br>enable_rope_fusion,<br>k_input,<br>forward_batch,<br>zero_allocator)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_absorb_fused_mla_rope_core(
    self,
    q_input,
    key_cache_buf,
    val_cache_buf,
    attn_output,
    kv_indptr,
    kv_indices,
    k_pe_output,
    cos_sin_cache,
    positions,
    attn_logits,
    num_kv_split,
    sm_scale,
    enable_rope_fusion,
    k_input,
    forward_batch,
    zero_allocator,
):
    decode_attention_fwd_grouped_rope(
        q_input,
        key_cache_buf,
        val_cache_buf,
        attn_output,
        kv_indptr,
        kv_indices,
        k_pe_output,
        self.kv_lora_rank,
        self.rotary_emb.rotary_dim,
        cos_sin_cache,
        positions,
        attn_logits,
        num_kv_split,
        sm_scale,
        logit_cap=self.attn_mqa.logit_cap,
        use_rope=enable_rope_fusion,
        is_neox_style=self.rotary_emb.is_neox_style,
    )

    if enable_rope_fusion:
        k_input[..., self.kv_lora_rank :] = k_pe_output
        forward_batch.token_to_kv_pool.set_kv_buffer(
            self.attn_mqa, forward_batch.out_cache_loc, k_input, None
        )

    attn_output = attn_output.view(-1, self.num_local_heads, self.kv_lora_rank)

    if _is_hip:
        # TODO(haishaw): add bmm_fp8 to ROCm
        attn_bmm_output = torch.bmm(
            attn_output.to(torch.bfloat16).transpose(0, 1),
            self.w_vc.to(torch.bfloat16) * self.w_scale,
        )
    elif self.w_vc.dtype == torch.float8_e4m3fn:
        attn_output_val, attn_output_scale = per_tensor_quant_mla_fp8(
            attn_output.transpose(0, 1),
            zero_allocator.allocate(1),
            dtype=torch.float8_e4m3fn,
        )
        attn_bmm_output = bmm_fp8(
            attn_output_val,
            self.w_vc,
            attn_output_scale,
            self.w_scale,
            torch.bfloat16,
        )
    else:
        attn_bmm_output = torch.bmm(attn_output.transpose(0, 1), self.w_vc)
    attn_output = attn_bmm_output.transpose(0, 1).flatten(1, 2)
    output, _ = self.o_proj(attn_output)

    return output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_cpu_core"><code class="name flex">
<span>def <span class="ident">forward_absorb_fused_mla_rope_cpu_core</span></span>(<span>self, q_input, k_input, v_input, forward_batch, zero_allocator)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_absorb_fused_mla_rope_cpu_core(
    self, q_input, k_input, v_input, forward_batch, zero_allocator
):
    assert self.q_lora_rank is not None and use_intel_amx_backend(
        self
    ), &#34;forward_absorb_fused_mla_rope_cpu_core requires q_lora_rank is not None and use_intel_amx_backend&#34;

    attn_output = self.attn_mqa(q_input, k_input, v_input, forward_batch)
    attn_output = attn_output.view(-1, self.num_local_heads, self.kv_lora_rank)

    # [Note] Align shapes of bmm inputs.
    # Shapes of inputs:
    #   q_nope: [M, B, K]
    #   original self.w_kc: [B, K, N]
    #   current self.w_kc (which has been converted in PackWeightMethod): [B, N, K]

    # Shapes of inputs to sgl_kernel.cpu.bmm:
    #   out: [B, M, N]
    #   mat1: [B, M, K]
    #   mat2: [B, N, K]
    B = self.w_vc.size(0)
    N = self.w_vc.size(1)
    M = attn_output.size(0)
    output = torch.empty([M, int(B * N)], dtype=attn_output.dtype)
    attn_bmm_output = output.view([M, B, N]).transpose_(0, 1)
    torch.ops.sgl_kernel.bmm_cpu(
        attn_bmm_output,
        attn_output.transpose(0, 1),
        self.w_vc,
        True,  # is_vnni
        None,  # scale
    )
    attn_output = output
    output, _ = self.o_proj(attn_output)

    return output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_cpu_prepare"><code class="name flex">
<span>def <span class="ident">forward_absorb_fused_mla_rope_cpu_prepare</span></span>(<span>self,<br>positions: torch.Tensor,<br>hidden_states: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>zero_allocator: <a title="sglang.srt.utils.BumpAllocator" href="../utils.html#sglang.srt.utils.BumpAllocator">BumpAllocator</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_absorb_fused_mla_rope_cpu_prepare(
    self,
    positions: torch.Tensor,
    hidden_states: torch.Tensor,
    forward_batch: ForwardBatch,
    zero_allocator: BumpAllocator,
):
    assert self.q_lora_rank is not None and use_intel_amx_backend(
        self
    ), &#34;forward_absorb_fused_mla_rope_cpu_prepare requires q_lora_rank is not None and use_intel_amx_backend&#34;

    q_input, k_input, v_input = (
        torch.ops.sgl_kernel.qkv_proj_with_rope_fused_weight(
            hidden_states,
            self.fused_qkv_a_proj_with_mqa.weight,
            self.q_b_proj.weight,
            self.w_kc,
            self.q_a_layernorm.weight,
            self.kv_a_layernorm.weight,
            positions,
            self.rotary_emb.cos_sin_cache,
            self.kv_a_layernorm.variance_epsilon,
            self.qkv_proj_with_rope_is_int8,
            self.qkv_proj_with_rope_is_fp8,
            (
                self.fused_qkv_a_proj_with_mqa.weight_scale
                if self.qkv_proj_with_rope_is_int8
                else (
                    self.fused_qkv_a_proj_with_mqa.weight_scale_inv
                    if self.qkv_proj_with_rope_is_fp8
                    else None
                )
            ),
            (
                self.q_b_proj.weight_scale
                if self.qkv_proj_with_rope_is_int8
                else (
                    self.q_b_proj.weight_scale_inv
                    if self.qkv_proj_with_rope_is_fp8
                    else None
                )
            ),
            True,  # is_vnni
            self.weight_block_size,
            self.q_lora_rank,
            self.kv_lora_rank,
            self.qk_rope_head_dim,
        )
    )
    return (q_input, k_input, v_input, forward_batch, zero_allocator)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_prepare"><code class="name flex">
<span>def <span class="ident">forward_absorb_fused_mla_rope_prepare</span></span>(<span>self,<br>positions: torch.Tensor,<br>hidden_states: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>zero_allocator: <a title="sglang.srt.utils.BumpAllocator" href="../utils.html#sglang.srt.utils.BumpAllocator">BumpAllocator</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_absorb_fused_mla_rope_prepare(
    self,
    positions: torch.Tensor,
    hidden_states: torch.Tensor,
    forward_batch: ForwardBatch,
    zero_allocator: BumpAllocator,
):
    enable_rope_fusion = (
        os.getenv(&#34;SGLANG_FUSED_MLA_ENABLE_ROPE_FUSION&#34;, &#34;1&#34;) == &#34;1&#34;
    )
    q_len = hidden_states.shape[0]
    q_input = hidden_states.new_empty(
        q_len, self.num_local_heads, self.kv_lora_rank + self.qk_rope_head_dim
    )
    if self.q_lora_rank is not None:
        q, latent_cache = self.fused_qkv_a_proj_with_mqa(hidden_states)[0].split(
            [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim], dim=-1
        )
        q = self.q_a_layernorm(q)
        q = self.q_b_proj(q)[0].view(-1, self.num_local_heads, self.qk_head_dim)
    else:
        q = self.q_proj(hidden_states)[0].view(
            -1, self.num_local_heads, self.qk_head_dim
        )
        latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]
    q_nope, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)

    if _is_hip:
        # TODO(haishaw): add bmm_fp8 to ROCm
        q_nope_out = torch.bmm(
            q_nope.to(torch.bfloat16).transpose(0, 1),
            self.w_kc.to(torch.bfloat16) * self.w_scale,
        )
    elif self.w_kc.dtype == torch.float8_e4m3fn:
        q_nope_val, q_nope_scale = per_tensor_quant_mla_fp8(
            q_nope.transpose(0, 1),
            zero_allocator.allocate(1),
            dtype=torch.float8_e4m3fn,
        )
        q_nope_out = bmm_fp8(
            q_nope_val, self.w_kc, q_nope_scale, self.w_scale, torch.bfloat16
        )
    else:
        q_nope_out = torch.bmm(q_nope.transpose(0, 1), self.w_kc)
    q_input[..., : self.kv_lora_rank] = q_nope_out.transpose(0, 1)
    v_input = latent_cache[..., : self.kv_lora_rank]
    v_input = self.kv_a_layernorm(v_input.contiguous()).unsqueeze(1)
    k_input = latent_cache.unsqueeze(1)
    k_input[..., : self.kv_lora_rank] = v_input

    if not enable_rope_fusion:
        k_pe = k_input[..., self.kv_lora_rank :]
        q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe)
        q_input[..., self.kv_lora_rank :] = q_pe
        k_input[..., self.kv_lora_rank :] = k_pe
        k_pe_output = None
    else:
        k_pe_output = torch.empty_like(k_input[..., self.kv_lora_rank :])

    q_input[..., self.kv_lora_rank :] = q_pe

    # attn_output = self.attn_mqa(q_input, k_input, v_input, forward_batch)
    # Use Fused ROPE with use_rope=OFF.
    attn_output = torch.empty(
        (q_len, self.num_local_heads, self.kv_lora_rank),
        dtype=q.dtype,
        device=q.device,
    )
    attn_logits, _, kv_indptr, kv_indices, _, _, _ = (
        forward_batch.attn_backend.forward_metadata
    )
    cos_sin_cache = self.rotary_emb.cos_sin_cache
    num_kv_split = forward_batch.attn_backend.num_kv_splits
    sm_scale = self.attn_mqa.scaling
    if attn_logits is None:
        attn_logits = torch.empty(
            (
                forward_batch.batch_size,
                self.num_local_heads,
                num_kv_split,
                self.kv_lora_rank + 1,
            ),
            dtype=torch.float32,
            device=q.device,
        )

    # save current latent cache.
    forward_batch.token_to_kv_pool.set_kv_buffer(
        self.attn_mqa, forward_batch.out_cache_loc, k_input, None
    )
    key_cache_buf = forward_batch.token_to_kv_pool.get_key_buffer(
        self.attn_mqa.layer_id
    )
    val_cache_buf = key_cache_buf[..., : self.kv_lora_rank]

    return (
        q_input,
        key_cache_buf,
        val_cache_buf,
        attn_output,
        kv_indptr,
        kv_indices,
        k_pe_output,
        cos_sin_cache,
        positions,
        attn_logits,
        num_kv_split,
        sm_scale,
        enable_rope_fusion,
        k_input,
        forward_batch,
        zero_allocator,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_absorb_prepare"><code class="name flex">
<span>def <span class="ident">forward_absorb_prepare</span></span>(<span>self,<br>positions: torch.Tensor,<br>hidden_states: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>zero_allocator: <a title="sglang.srt.utils.BumpAllocator" href="../utils.html#sglang.srt.utils.BumpAllocator">BumpAllocator</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_absorb_prepare(
    self,
    positions: torch.Tensor,
    hidden_states: torch.Tensor,
    forward_batch: ForwardBatch,
    zero_allocator: BumpAllocator,
):
    from sglang.srt.model_executor.cuda_graph_runner import get_is_capture_mode

    if self.q_lora_rank is not None:
        if hidden_states.shape[0] &lt;= 16 and self.use_min_latency_fused_a_gemm:
            fused_qkv_a_proj_out = dsv3_fused_a_gemm(
                hidden_states, self.fused_qkv_a_proj_with_mqa.weight.T
            )
        else:
            fused_qkv_a_proj_out = self.fused_qkv_a_proj_with_mqa(hidden_states)[0]
        q, latent_cache = fused_qkv_a_proj_out.split(
            [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim], dim=-1
        )
        k_nope = latent_cache[..., : self.kv_lora_rank]

        # overlap qk norm
        if self.alt_stream is not None and get_is_capture_mode():
            current_stream = torch.cuda.current_stream()
            self.alt_stream.wait_stream(current_stream)
            q = self.q_a_layernorm(q)
            with torch.cuda.stream(self.alt_stream):
                k_nope = self.kv_a_layernorm(k_nope)
            current_stream.wait_stream(self.alt_stream)
        else:
            q = self.q_a_layernorm(q)
            k_nope = self.kv_a_layernorm(k_nope)

        k_nope = k_nope.unsqueeze(1)
        q = self.q_b_proj(q)[0].view(-1, self.num_local_heads, self.qk_head_dim)
    else:
        q = self.q_proj(hidden_states)[0].view(
            -1, self.num_local_heads, self.qk_head_dim
        )
        latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]
        k_nope = latent_cache[..., : self.kv_lora_rank]
        k_nope = self.kv_a_layernorm(k_nope).unsqueeze(1)

    q_nope, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)
    k_pe = latent_cache[..., self.kv_lora_rank :].unsqueeze(1)

    if self.use_deep_gemm_bmm:
        q_nope_val, q_nope_scale, masked_m, expected_m, aligned_m = (
            per_token_group_quant_mla_deep_gemm_masked_fp8(q_nope.transpose(0, 1))
        )
        q_nope_out = q_nope.new_empty(
            (self.num_local_heads, aligned_m, self.kv_lora_rank)
        )
        deep_gemm_wrapper.grouped_gemm_nt_f8f8bf16_masked(
            (q_nope_val, q_nope_scale),
            (self.w_kc, self.w_scale_k),
            q_nope_out,
            masked_m,
            expected_m,
        )
        q_nope_out = q_nope_out[:, :expected_m, :]
    elif _is_hip:
        # TODO(haishaw): add bmm_fp8 to ROCm
        q_nope_out = torch.bmm(
            q_nope.to(torch.bfloat16).transpose(0, 1),
            self.w_kc.to(torch.bfloat16) * self.w_scale,
        )
    elif self.w_kc.dtype == torch.float8_e4m3fn:
        q_nope_val, q_nope_scale = per_tensor_quant_mla_fp8(
            q_nope.transpose(0, 1),
            zero_allocator.allocate(1),
        )
        q_nope_out = bmm_fp8(
            q_nope_val, self.w_kc, q_nope_scale, self.w_scale, torch.bfloat16
        )
    else:
        q_nope_out = torch.bmm(q_nope.transpose(0, 1), self.w_kc)

    q_nope_out = q_nope_out.transpose(0, 1)

    if not self._fuse_rope_for_trtllm_mla(forward_batch):
        q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe)

    return q_pe, k_pe, q_nope_out, k_nope, forward_batch, zero_allocator</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_core"><code class="name flex">
<span>def <span class="ident">forward_core</span></span>(<span>self, intermediate_state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_core(self, intermediate_state):
    hidden_states, attn_forward_method, forward_batch, inner_state = (
        intermediate_state
    )
    if inner_state is None:
        return hidden_states

    if attn_forward_method == AttnForwardMethod.MHA:
        return self.forward_normal_core(*inner_state)
    elif attn_forward_method == AttnForwardMethod.MHA_CHUNKED_KV:
        return self.forward_normal_chunked_kv_core(*inner_state)
    elif attn_forward_method == AttnForwardMethod.MLA:
        return self.forward_absorb_core(*inner_state)
    elif attn_forward_method == AttnForwardMethod.MLA_FUSED_ROPE:
        return self.forward_absorb_fused_mla_rope_core(*inner_state)
    elif attn_forward_method == AttnForwardMethod.MLA_FUSED_ROPE_CPU:
        return self.forward_absorb_fused_mla_rope_cpu_core(*inner_state)
    else:
        raise NotImplementedError</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_normal_chunked_kv_core"><code class="name flex">
<span>def <span class="ident">forward_normal_chunked_kv_core</span></span>(<span>self, q, k, v, forward_batch)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_normal_chunked_kv_core(self, q, k, v, forward_batch):
    has_extend_prefix = any(forward_batch.extend_prefix_lens_cpu)
    # Only initialize the info once
    if has_extend_prefix and forward_batch.num_prefix_chunks is None:
        forward_batch.prepare_chunked_prefix_cache_info(q.device)
        if hasattr(forward_batch.attn_backend, &#34;init_mha_chunk_metadata&#34;):
            forward_batch.attn_backend.init_mha_chunk_metadata(forward_batch)

    forward_batch.mha_return_lse = has_extend_prefix
    # Do mha for extended part without prefix
    forward_batch.set_attn_attend_prefix_cache(False)
    attn_output = self.attn_mha(q, k, v, forward_batch, save_kv_cache=False)

    # Do mha attention with chunked prefix cache if there are any sequence with prefix
    if has_extend_prefix:
        attn_output, lse = attn_output
        forward_batch.set_attn_attend_prefix_cache(True)
        attn_output = self._chunked_prefix_attn_mha(
            q=q,
            accum_output=attn_output,
            accum_lse=lse,
            forward_batch=forward_batch,
        )

    attn_output = attn_output.reshape(-1, self.num_local_heads * self.v_head_dim)
    output, _ = self.o_proj(attn_output)
    return output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_normal_chunked_kv_prepare"><code class="name flex">
<span>def <span class="ident">forward_normal_chunked_kv_prepare</span></span>(<span>self,<br>positions: torch.Tensor,<br>hidden_states: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>zero_allocator: <a title="sglang.srt.utils.BumpAllocator" href="../utils.html#sglang.srt.utils.BumpAllocator">BumpAllocator</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_normal_chunked_kv_prepare(
    self,
    positions: torch.Tensor,
    hidden_states: torch.Tensor,
    forward_batch: ForwardBatch,
    zero_allocator: BumpAllocator,
):
    # In normal mha, the k and v tensors will become overly large when the prefix length is long.
    # To avoid this, we split the kv cache into chunks and process them one after another.
    # Since mha is compute friendly, the for loop induced here will not introduce significant overhead.
    # The top comments in https://github.com/vllm-project/vllm/blob/main/vllm/v1/attention/backends/mla/common.py
    # will be helpful for understanding the purpose of this function.

    # First do normal mha forward to get output for extended part
    return self.forward_normal_prepare(
        positions, hidden_states, forward_batch, zero_allocator
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_normal_core"><code class="name flex">
<span>def <span class="ident">forward_normal_core</span></span>(<span>self, q, k, v, forward_batch)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_normal_core(self, q, k, v, forward_batch):
    attn_output = self.attn_mha(q, k, v, forward_batch, save_kv_cache=False)
    attn_output = attn_output.reshape(-1, self.num_local_heads * self.v_head_dim)
    output, _ = self.o_proj(attn_output)
    return output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_normal_prepare"><code class="name flex">
<span>def <span class="ident">forward_normal_prepare</span></span>(<span>self,<br>positions: torch.Tensor,<br>hidden_states: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>zero_allocator: <a title="sglang.srt.utils.BumpAllocator" href="../utils.html#sglang.srt.utils.BumpAllocator">BumpAllocator</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_normal_prepare(
    self,
    positions: torch.Tensor,
    hidden_states: torch.Tensor,
    forward_batch: ForwardBatch,
    zero_allocator: BumpAllocator,
):
    if self.q_lora_rank is not None:
        q, latent_cache = self.fused_qkv_a_proj_with_mqa(hidden_states)[0].split(
            [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim], dim=-1
        )
        q = self.q_a_layernorm(q)
        q = self.q_b_proj(q)[0].view(-1, self.num_local_heads, self.qk_head_dim)
    else:
        q = self.q_proj(hidden_states)[0].view(
            -1, self.num_local_heads, self.qk_head_dim
        )
        latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]

    _, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)
    kv_a, _ = latent_cache.split([self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)
    latent_cache = latent_cache.unsqueeze(1)
    kv_a = self.kv_a_layernorm(kv_a)
    kv = self.kv_b_proj(kv_a)[0]
    kv = kv.view(-1, self.num_local_heads, self.qk_nope_head_dim + self.v_head_dim)
    k_nope = kv[..., : self.qk_nope_head_dim]
    v = kv[..., self.qk_nope_head_dim :]
    k_pe = latent_cache[:, :, self.kv_lora_rank :]
    q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe)
    q[..., self.qk_nope_head_dim :] = q_pe
    k = torch.empty_like(q)
    k[..., : self.qk_nope_head_dim] = k_nope
    k[..., self.qk_nope_head_dim :] = k_pe

    if not _is_npu:
        latent_cache[:, :, : self.kv_lora_rank] = kv_a.unsqueeze(1)
        latent_cache[:, :, self.kv_lora_rank :] = k_pe

        # Save latent cache
        forward_batch.token_to_kv_pool.set_kv_buffer(
            self.attn_mha, forward_batch.out_cache_loc, latent_cache, None
        )
    else:
        # To reduce a time-costing split operation
        forward_batch.token_to_kv_pool.set_kv_buffer(
            self.attn_mha, forward_batch.out_cache_loc, kv_a.unsqueeze(1), k_pe
        )

    return q, k, v, forward_batch</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_prepare"><code class="name flex">
<span>def <span class="ident">forward_prepare</span></span>(<span>self,<br>positions: torch.Tensor,<br>hidden_states: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>zero_allocator: <a title="sglang.srt.utils.BumpAllocator" href="../utils.html#sglang.srt.utils.BumpAllocator">BumpAllocator</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_prepare(
    self,
    positions: torch.Tensor,
    hidden_states: torch.Tensor,
    forward_batch: ForwardBatch,
    zero_allocator: BumpAllocator,
):
    if self.attn_mha.kv_b_proj is None:
        self.attn_mha.kv_b_proj = self.kv_b_proj

    if hidden_states.shape[0] == 0:
        assert (
            not self.o_proj.reduce_results
        ), &#34;short-circuiting allreduce will lead to hangs&#34;
        return hidden_states, None, forward_batch, None

    attn_forward_method = self.dispatch_attn_forward_method(forward_batch)

    if attn_forward_method == AttnForwardMethod.MHA:
        inner_state = self.forward_normal_prepare(
            positions, hidden_states, forward_batch, zero_allocator
        )
    elif attn_forward_method == AttnForwardMethod.MHA_CHUNKED_KV:
        inner_state = self.forward_normal_chunked_kv_prepare(
            positions, hidden_states, forward_batch, zero_allocator
        )
    elif attn_forward_method == AttnForwardMethod.MLA:
        inner_state = self.forward_absorb_prepare(
            positions, hidden_states, forward_batch, zero_allocator
        )
    elif attn_forward_method == AttnForwardMethod.MLA_FUSED_ROPE:
        inner_state = self.forward_absorb_fused_mla_rope_prepare(
            positions, hidden_states, forward_batch, zero_allocator
        )
    elif attn_forward_method == AttnForwardMethod.MLA_FUSED_ROPE_CPU:
        inner_state = self.forward_absorb_fused_mla_rope_cpu_prepare(
            positions, hidden_states, forward_batch, zero_allocator
        )
    else:
        raise NotImplementedError
    return None, attn_forward_method, forward_batch, inner_state</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.op_core"><code class="name flex">
<span>def <span class="ident">op_core</span></span>(<span>self, state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def op_core(self, state):
    state.hidden_states_after_attn = self.forward_core(
        state.pop(&#34;attn_intermediate_state&#34;)
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.op_prepare"><code class="name flex">
<span>def <span class="ident">op_prepare</span></span>(<span>self, state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def op_prepare(self, state):
    state.attn_intermediate_state = self.forward_prepare(
        positions=state.positions,
        hidden_states=state.pop(&#34;hidden_states_after_comm_pre_attn&#34;),
        forward_batch=state.forward_batch,
        zero_allocator=state.zero_allocator,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2DecoderLayer"><code class="flex name class">
<span>class <span class="ident">DeepseekV2DecoderLayer</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig,<br>layer_id: int,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>is_nextn: bool = False,<br>prefix: str = '',<br>alt_stream: torch.cuda.streams.Stream | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DeepseekV2DecoderLayer(nn.Module):

    def __init__(
        self,
        config: PretrainedConfig,
        layer_id: int,
        quant_config: Optional[QuantizationConfig] = None,
        is_nextn: bool = False,
        prefix: str = &#34;&#34;,
        alt_stream: Optional[torch.cuda.Stream] = None,
    ) -&gt; None:
        super().__init__()
        self.hidden_size = config.hidden_size
        self.config = config
        rope_theta = getattr(config, &#34;rope_theta&#34;, 10000)
        rope_scaling = getattr(config, &#34;rope_scaling&#34;, None)
        max_position_embeddings = getattr(config, &#34;max_position_embeddings&#34;, 8192)
        self.speculative_algorithm = global_server_args_dict[&#34;speculative_algorithm&#34;]
        self.layer_id = layer_id
        self.is_nextn = is_nextn
        self.self_attn = DeepseekV2AttentionMLA(
            config=config,
            hidden_size=self.hidden_size,
            num_heads=config.num_attention_heads,
            qk_nope_head_dim=config.qk_nope_head_dim,
            qk_rope_head_dim=config.qk_rope_head_dim,
            v_head_dim=config.v_head_dim,
            q_lora_rank=(
                config.q_lora_rank if hasattr(config, &#34;q_lora_rank&#34;) else None
            ),
            kv_lora_rank=config.kv_lora_rank,
            rope_theta=rope_theta,
            rope_scaling=rope_scaling,
            max_position_embeddings=max_position_embeddings,
            quant_config=quant_config,
            layer_id=layer_id,
            reduce_results=False,
            prefix=add_prefix(&#34;self_attn&#34;, prefix),
            alt_stream=alt_stream,
        )

        self.is_layer_sparse = self._is_layer_sparse(layer_id, is_nextn=is_nextn)
        is_previous_layer_sparse = self._is_layer_sparse(layer_id - 1, is_nextn=False)

        self.layer_scatter_modes = LayerScatterModes.init_new(
            layer_id=layer_id,
            num_layers=1 if is_nextn else config.num_hidden_layers,
            is_layer_sparse=self.is_layer_sparse,
            is_previous_layer_sparse=is_previous_layer_sparse,
        )

        if self.is_layer_sparse:
            self.mlp = DeepseekV2MoE(
                config=config,
                quant_config=quant_config,
                prefix=add_prefix(&#34;mlp&#34;, prefix),
                layer_id=self.layer_id,
                alt_stream=alt_stream,
                is_nextn=is_nextn,
            )
        else:
            if enable_moe_dense_fully_dp():
                mlp_tp_rank, mlp_tp_size = 0, 1
            else:
                mlp_tp_rank, mlp_tp_size = None, None
            self.mlp = DeepseekV2MLP(
                hidden_size=config.hidden_size,
                intermediate_size=config.intermediate_size,
                hidden_act=config.hidden_act,
                quant_config=quant_config,
                prefix=add_prefix(&#34;mlp&#34;, prefix),
                tp_rank=mlp_tp_rank,
                tp_size=mlp_tp_size,
            )

        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = RMSNorm(
            config.hidden_size, eps=config.rms_norm_eps
        )

        self.layer_communicator = LayerCommunicator(
            layer_scatter_modes=self.layer_scatter_modes,
            input_layernorm=self.input_layernorm,
            post_attention_layernorm=self.post_attention_layernorm,
            allow_reduce_scatter=True,
            is_last_layer=(
                is_nextn or (self.layer_id == self.config.num_hidden_layers - 1)
            ),
        )

    def _is_layer_sparse(self, layer_id: int, is_nextn: bool) -&gt; bool:
        return is_nextn or (
            self.config.n_routed_experts is not None
            and layer_id &gt;= self.config.first_k_dense_replace
            and layer_id % self.config.moe_layer_freq == 0
        )

    def forward(
        self,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        forward_batch: ForwardBatch,
        residual: Optional[torch.Tensor],
        zero_allocator: BumpAllocator,
    ) -&gt; torch.Tensor:

        hidden_states, residual = self.layer_communicator.prepare_attn(
            hidden_states, residual, forward_batch
        )

        hidden_states = self.self_attn(
            positions=positions,
            hidden_states=hidden_states,
            forward_batch=forward_batch,
            zero_allocator=zero_allocator,
        )

        hidden_states, residual = self.layer_communicator.prepare_mlp(
            hidden_states, residual, forward_batch
        )

        should_allreduce_fusion = (
            self.layer_communicator.should_fuse_mlp_allreduce_with_next_layer(
                forward_batch
            )
        )

        # For DP with padding, reduce scatter can be used instead of all-reduce.
        use_reduce_scatter = self.layer_communicator.should_use_reduce_scatter(
            forward_batch
        )
        hidden_states = self.mlp(
            hidden_states, forward_batch, should_allreduce_fusion, use_reduce_scatter
        )

        if should_allreduce_fusion:
            hidden_states._sglang_needs_allreduce_fusion = True

        if not should_allreduce_fusion:
            hidden_states, residual = self.layer_communicator.postprocess_layer(
                hidden_states, residual, forward_batch
            )

        return hidden_states, residual

    def op_comm_prepare_attn(
        self,
        state,
        positions: torch.Tensor,
        hidden_states: torch.Tensor,
        forward_batch: ForwardBatch,
        residual: Optional[torch.Tensor],
        zero_allocator: BumpAllocator,
        tbo_subbatch_index: Optional[int] = None,
    ):
        state.hidden_states_after_comm_pre_attn, state.residual_after_input_ln = (
            self.layer_communicator.prepare_attn(hidden_states, residual, forward_batch)
        )
        state.update(
            dict(
                forward_batch=forward_batch,
                positions=positions,
                zero_allocator=zero_allocator,
                tbo_subbatch_index=tbo_subbatch_index,
            )
        )

    def op_comm_prepare_mlp(self, state):
        state.hidden_states_mlp_input, state.residual_after_comm_pre_mlp = (
            self.layer_communicator.prepare_mlp(
                state.pop(&#34;hidden_states_after_attn&#34;),
                state.pop(&#34;residual_after_input_ln&#34;),
                state.forward_batch,
            )
        )

    def op_mlp(self, state):
        hidden_states = state.pop(&#34;hidden_states_mlp_input&#34;)
        if not (
            enable_moe_dense_fully_dp()
            and (not self.is_layer_sparse)
            and hidden_states.shape[0] == 0
        ):
            state.hidden_states_mlp_output = self.mlp(
                hidden_states, state.forward_batch
            )
        else:
            state.hidden_states_mlp_output = hidden_states

    def op_comm_postprocess_layer(self, state):
        hidden_states, residual = self.layer_communicator.postprocess_layer(
            state.pop(&#34;hidden_states_mlp_output&#34;),
            state.pop(&#34;residual_after_comm_pre_mlp&#34;),
            state.forward_batch,
        )

        output = dict(
            positions=state.positions,
            hidden_states=hidden_states,
            residual=residual,
            forward_batch=state.forward_batch,
            zero_allocator=state.zero_allocator,
            tbo_subbatch_index=state.tbo_subbatch_index,
        )

        state.clear(
            expect_keys={
                &#34;positions&#34;,
                &#34;forward_batch&#34;,
                &#34;zero_allocator&#34;,
                &#34;tbo_subbatch_index&#34;,
            }
        )
        return output</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.models.glm4_moe.Glm4MoeDecoderLayer" href="glm4_moe.html#sglang.srt.models.glm4_moe.Glm4MoeDecoderLayer">Glm4MoeDecoderLayer</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2DecoderLayer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>positions: torch.Tensor,<br>hidden_states: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>residual: torch.Tensor | None,<br>zero_allocator: <a title="sglang.srt.utils.BumpAllocator" href="../utils.html#sglang.srt.utils.BumpAllocator">BumpAllocator</a>) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    positions: torch.Tensor,
    hidden_states: torch.Tensor,
    forward_batch: ForwardBatch,
    residual: Optional[torch.Tensor],
    zero_allocator: BumpAllocator,
) -&gt; torch.Tensor:

    hidden_states, residual = self.layer_communicator.prepare_attn(
        hidden_states, residual, forward_batch
    )

    hidden_states = self.self_attn(
        positions=positions,
        hidden_states=hidden_states,
        forward_batch=forward_batch,
        zero_allocator=zero_allocator,
    )

    hidden_states, residual = self.layer_communicator.prepare_mlp(
        hidden_states, residual, forward_batch
    )

    should_allreduce_fusion = (
        self.layer_communicator.should_fuse_mlp_allreduce_with_next_layer(
            forward_batch
        )
    )

    # For DP with padding, reduce scatter can be used instead of all-reduce.
    use_reduce_scatter = self.layer_communicator.should_use_reduce_scatter(
        forward_batch
    )
    hidden_states = self.mlp(
        hidden_states, forward_batch, should_allreduce_fusion, use_reduce_scatter
    )

    if should_allreduce_fusion:
        hidden_states._sglang_needs_allreduce_fusion = True

    if not should_allreduce_fusion:
        hidden_states, residual = self.layer_communicator.postprocess_layer(
            hidden_states, residual, forward_batch
        )

    return hidden_states, residual</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2DecoderLayer.op_comm_postprocess_layer"><code class="name flex">
<span>def <span class="ident">op_comm_postprocess_layer</span></span>(<span>self, state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def op_comm_postprocess_layer(self, state):
    hidden_states, residual = self.layer_communicator.postprocess_layer(
        state.pop(&#34;hidden_states_mlp_output&#34;),
        state.pop(&#34;residual_after_comm_pre_mlp&#34;),
        state.forward_batch,
    )

    output = dict(
        positions=state.positions,
        hidden_states=hidden_states,
        residual=residual,
        forward_batch=state.forward_batch,
        zero_allocator=state.zero_allocator,
        tbo_subbatch_index=state.tbo_subbatch_index,
    )

    state.clear(
        expect_keys={
            &#34;positions&#34;,
            &#34;forward_batch&#34;,
            &#34;zero_allocator&#34;,
            &#34;tbo_subbatch_index&#34;,
        }
    )
    return output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2DecoderLayer.op_comm_prepare_attn"><code class="name flex">
<span>def <span class="ident">op_comm_prepare_attn</span></span>(<span>self,<br>state,<br>positions: torch.Tensor,<br>hidden_states: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>residual: torch.Tensor | None,<br>zero_allocator: <a title="sglang.srt.utils.BumpAllocator" href="../utils.html#sglang.srt.utils.BumpAllocator">BumpAllocator</a>,<br>tbo_subbatch_index: int | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def op_comm_prepare_attn(
    self,
    state,
    positions: torch.Tensor,
    hidden_states: torch.Tensor,
    forward_batch: ForwardBatch,
    residual: Optional[torch.Tensor],
    zero_allocator: BumpAllocator,
    tbo_subbatch_index: Optional[int] = None,
):
    state.hidden_states_after_comm_pre_attn, state.residual_after_input_ln = (
        self.layer_communicator.prepare_attn(hidden_states, residual, forward_batch)
    )
    state.update(
        dict(
            forward_batch=forward_batch,
            positions=positions,
            zero_allocator=zero_allocator,
            tbo_subbatch_index=tbo_subbatch_index,
        )
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2DecoderLayer.op_comm_prepare_mlp"><code class="name flex">
<span>def <span class="ident">op_comm_prepare_mlp</span></span>(<span>self, state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def op_comm_prepare_mlp(self, state):
    state.hidden_states_mlp_input, state.residual_after_comm_pre_mlp = (
        self.layer_communicator.prepare_mlp(
            state.pop(&#34;hidden_states_after_attn&#34;),
            state.pop(&#34;residual_after_input_ln&#34;),
            state.forward_batch,
        )
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2DecoderLayer.op_mlp"><code class="name flex">
<span>def <span class="ident">op_mlp</span></span>(<span>self, state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def op_mlp(self, state):
    hidden_states = state.pop(&#34;hidden_states_mlp_input&#34;)
    if not (
        enable_moe_dense_fully_dp()
        and (not self.is_layer_sparse)
        and hidden_states.shape[0] == 0
    ):
        state.hidden_states_mlp_output = self.mlp(
            hidden_states, state.forward_batch
        )
    else:
        state.hidden_states_mlp_output = hidden_states</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM"><code class="flex name class">
<span>class <span class="ident">DeepseekV2ForCausalLM</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DeepseekV2ForCausalLM(nn.Module):
    # for quark model load
    packed_modules_mapping = {}

    def __init__(
        self,
        config: PretrainedConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; None:
        super().__init__()

        # for quark model load
        # Fuse q_a_proj and kv_a_proj_with_mqa along output dimension when q_lora_rank is not None
        self.fuse_qkv_a_proj = (
            hasattr(config, &#34;q_lora_rank&#34;) and config.q_lora_rank is not None
        )
        if self.fuse_qkv_a_proj:
            self.packed_modules_mapping[&#34;fused_qkv_a_proj_with_mqa&#34;] = [
                &#34;q_a_proj&#34;,
                &#34;kv_a_proj_with_mqa&#34;,
            ]

        self.pp_group = get_pp_group()
        self.config = config
        self.tp_size = get_tensor_model_parallel_world_size()
        self.quant_config = quant_config
        self.determine_num_fused_shared_experts()
        self.model = DeepseekV2Model(
            config, quant_config, prefix=add_prefix(&#34;model&#34;, prefix)
        )
        self.lm_head = ParallelLMHead(
            config.vocab_size,
            config.hidden_size,
            quant_config=quant_config,
            prefix=add_prefix(&#34;lm_head&#34;, prefix),
            use_attn_tp_group=global_server_args_dict[&#34;enable_dp_lm_head&#34;],
        )
        self.logits_processor = LogitsProcessor(config)

        self._routed_experts_weights_of_layer = LazyValue(
            lambda: {
                layer_id: layer.mlp.get_moe_weights()
                for layer_id, layer in enumerate(self.model.layers)
                if isinstance(layer.mlp, DeepseekV2MoE)
            }
        )

    @property
    def routed_experts_weights_of_layer(self):
        return self._routed_experts_weights_of_layer.value

    def determine_num_fused_shared_experts(
        self, architecture: str = &#34;DeepseekV3ForCausalLM&#34;
    ):
        self.num_fused_shared_experts = 0
        if global_server_args_dict[&#34;disable_shared_experts_fusion&#34;]:
            return

        # Only Deepseek V3/R1 can use shared experts fusion optimization now.
        disable_reason = None
        if (
            not _is_cuda
            or torch.cuda.get_device_capability(&#34;cuda&#34;) &lt; (8, 0)
            or self.config.architectures[0] != architecture
            or self.config.n_routed_experts != 256
            or self.config.n_shared_experts != 1
        ):
            disable_reason = &#34;Only Deepseek V3/R1 on NV-platform with capability &gt;= 80 can use shared experts fusion optimization.&#34;
        elif get_moe_expert_parallel_world_size() &gt; 1:
            disable_reason = &#34;Deepseek V3/R1 can not use shared experts fusion optimization under expert parallelism.&#34;

        if disable_reason is not None:
            global_server_args_dict[&#34;disable_shared_experts_fusion&#34;] = True
            self.num_fused_shared_experts = 0
            log_info_on_rank0(
                logger,
                f&#34;{disable_reason} Shared experts fusion optimization is disabled.&#34;,
            )
            return

        self.num_fused_shared_experts = self.config.n_shared_experts

    def get_input_embeddings(self) -&gt; nn.Embedding:
        return self.model.embed_tokens

    @torch.no_grad()
    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        forward_batch: ForwardBatch,
        input_embeds: torch.Tensor = None,
        pp_proxy_tensors: Optional[PPProxyTensors] = None,
    ) -&gt; torch.Tensor:
        hidden_states = self.model(
            input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors
        )

        if self.pp_group.is_last_rank:
            return self.logits_processor(
                input_ids, hidden_states, self.lm_head, forward_batch
            )
        else:
            return hidden_states

    @property
    def start_layer(self):
        return self.model.start_layer

    @property
    def end_layer(self):
        return self.model.end_layer

    def post_load_weights(self, is_nextn=False, weight_names=None):

        # Perform post-processing after loading weights
        if is_nextn:
            layer_ids = [self.config.num_hidden_layers]
        else:
            if weight_names is None:
                layer_ids = range(self.model.start_layer, self.model.end_layer)
            else:
                layer_ids = set()
                for name in weight_names:
                    if &#34;kv_b_proj&#34; in name:
                        layer_id = int(name.split(&#34;.&#34;)[2])
                        if layer_id &lt; self.config.num_hidden_layers:
                            layer_ids.add(layer_id)

        for layer_id in layer_ids:
            self_attn = (
                self.model.layers[layer_id].self_attn
                if not is_nextn
                else self.model.decoder.self_attn
            )
            if hasattr(self_attn.kv_b_proj, &#34;qweight&#34;):
                # AWQ compatible
                if _is_cuda or _is_hip:
                    w = awq_dequantize(
                        self_attn.kv_b_proj.qweight,
                        self_attn.kv_b_proj.scales,
                        self_attn.kv_b_proj.qzeros,
                    ).T
                else:
                    w = awq_dequantize(
                        self_attn.kv_b_proj.qweight,
                        self_attn.kv_b_proj.scales,
                        self_attn.kv_b_proj.qzeros,
                        0,
                        0,
                        0,
                    ).T
            else:
                w = self_attn.kv_b_proj.weight
            # NOTE(HandH1998): Since `bmm_fp8` only supports per-tensor scale, we have to requantize `self_attn.kv_b_proj`.
            # This may affect the accuracy of fp8 model.
            # Fix deepseek v3 blockwise bmm by using deep_gemm
            use_deep_gemm_bmm = False

            if w.dtype in (
                torch.float8_e4m3fn,
                torch.float8_e4m3fnuz,
            ):
                if (
                    hasattr(self.quant_config, &#34;weight_block_size&#34;)
                    and self.quant_config.weight_block_size is not None
                ):
                    weight_block_size = self.quant_config.weight_block_size
                    assert hasattr(self_attn.kv_b_proj, &#34;weight_scale_inv&#34;)
                    if _is_fp8_fnuz:
                        weight, weight_scale, _ = normalize_e4m3fn_to_e4m3fnuz(
                            weight=w,
                            weight_scale=self_attn.kv_b_proj.weight_scale_inv,
                            input_scale=None,
                        )
                    else:
                        weight = w
                        weight_scale = self_attn.kv_b_proj.weight_scale_inv

                    if (
                        _is_cuda
                        and weight_block_size[0] == 128
                        and weight_block_size[1] == 128
                    ):
                        if (
                            deep_gemm_wrapper.ENABLE_JIT_DEEPGEMM
                            and not deep_gemm_wrapper.DEEPGEMM_BLACKWELL
                            and get_bool_env_var(&#34;SGL_USE_DEEPGEMM_BMM&#34;, &#34;false&#34;)
                        ):
                            block_scale = weight_scale
                            use_deep_gemm_bmm = True
                        else:
                            w = block_quant_dequant(
                                weight,
                                weight_scale,
                                weight_block_size,
                                torch.bfloat16,
                            )
                    else:
                        w, scale = block_quant_to_tensor_quant(
                            weight, weight_scale, weight_block_size
                        )
                        self_attn.w_scale = scale
                else:
                    if _is_fp8_fnuz:
                        weight, weight_scale, _ = normalize_e4m3fn_to_e4m3fnuz(
                            weight=w,
                            weight_scale=self_attn.kv_b_proj.weight_scale,
                            input_scale=None,
                        )
                    else:
                        weight = w
                        weight_scale = self_attn.kv_b_proj.weight_scale

                    w, scale = channel_quant_to_tensor_quant(weight, weight_scale)
                    self_attn.w_scale = scale

            if w.dtype == torch.int8:
                if hasattr(self.quant_config, &#34;weight_block_size&#34;):
                    # block-wise int8 need it
                    weight_block_size = self.quant_config.weight_block_size
                    if weight_block_size is not None:
                        assert hasattr(self_attn.kv_b_proj, &#34;weight_scale_inv&#34;)
                        weight = w
                        weight_scale = self_attn.kv_b_proj.weight_scale_inv
                        w = int8_block_dequant(
                            weight, weight_scale, weight_block_size
                        ).to(torch.bfloat16)
                else:
                    # channel-wise int8 need it
                    w = w.to(torch.bfloat16) * self_attn.kv_b_proj.weight_scale.to(
                        torch.bfloat16
                    )

            w_kc, w_vc = w.unflatten(
                0, (-1, self_attn.qk_nope_head_dim + self_attn.v_head_dim)
            ).split([self_attn.qk_nope_head_dim, self_attn.v_head_dim], dim=1)
            if not use_deep_gemm_bmm:
                self_attn.w_kc = bind_or_assign(
                    self_attn.w_kc, w_kc.transpose(1, 2).contiguous().transpose(1, 2)
                )
                self_attn.w_vc = bind_or_assign(
                    self_attn.w_vc, w_vc.contiguous().transpose(1, 2)
                )
                if (
                    hasattr(self_attn.kv_b_proj, &#34;weight_scale&#34;)
                    and self_attn.w_scale is None
                ):
                    self_attn.w_scale = bind_or_assign(
                        self_attn.w_scale, self_attn.kv_b_proj.weight_scale
                    )
                    if _is_hip:
                        self_attn.w_scale *= 2.0
                # TODO: remove this after adding FP8 support in bmm cpu kernel
                if _is_cpu and _is_cpu_amx_available and w.dtype == torch.float8_e4m3fn:
                    self_attn.w_kc = (
                        self_attn.w_kc.to(torch.bfloat16) * self_attn.w_scale
                    )
                    self_attn.w_vc = (
                        self_attn.w_vc.to(torch.bfloat16) * self_attn.w_scale
                    )
            else:
                num_tiles_k = self_attn.qk_nope_head_dim // weight_block_size[1]
                num_tiles_n = self_attn.v_head_dim // weight_block_size[0]
                ws_kc, ws_vc = block_scale.unflatten(
                    0, (-1, (num_tiles_k + num_tiles_n))
                ).split([num_tiles_k, num_tiles_n], dim=1)
                self_attn.w_scale_k = bind_or_assign(
                    self_attn.w_scale_k, ws_kc.transpose(1, 2).contiguous()
                )
                self_attn.w_scale_v = bind_or_assign(
                    self_attn.w_scale_v, ws_vc.contiguous()
                )
                self_attn.w_kc = bind_or_assign(
                    self_attn.w_kc, w_kc.transpose(1, 2).contiguous()
                )
                self_attn.w_vc = bind_or_assign(self_attn.w_vc, w_vc.contiguous())
                self_attn.use_deep_gemm_bmm = True

        if (
            deep_gemm_wrapper.ENABLE_JIT_DEEPGEMM
            and deep_gemm_wrapper.DEEPGEMM_SCALE_UE8M0
            and hasattr(self.quant_config, &#34;weight_block_size&#34;)
            and self.quant_config.weight_block_size is not None
        ):
            self._weight_requant_ue8m0(is_nextn)

    def _weight_requant_ue8m0(self, is_nextn=False):
        weight_block_size = self.quant_config.weight_block_size

        moe_layers = list(
            range(
                self.config.first_k_dense_replace,
                self.config.num_hidden_layers,
                self.config.moe_layer_freq,
            )
        )

        num_hidden_layers = 1 if is_nextn else self.config.num_hidden_layers

        for layer_id in range(num_hidden_layers):
            if is_nextn:
                layer = self.model.decoder
            else:
                layer = self.model.layers[layer_id]

            module_list = [
                layer.self_attn.kv_b_proj,
                layer.self_attn.o_proj,
            ]

            if self.config.q_lora_rank is not None:
                module_list.append(layer.self_attn.fused_qkv_a_proj_with_mqa)
                module_list.append(layer.self_attn.q_b_proj)
            else:
                module_list.append(layer.self_attn.kv_a_proj_with_mqa)
                module_list.append(layer.self_attn.q_proj)

            for module in module_list:
                requant_weight_ue8m0_inplace(
                    module.weight, module.weight_scale_inv, weight_block_size
                )

            if layer_id in moe_layers or is_nextn:
                shared_experts = getattr(layer.mlp, &#34;shared_experts&#34;, None)
                if shared_experts is not None:
                    for module in [
                        shared_experts.gate_up_proj,
                        shared_experts.down_proj,
                    ]:
                        requant_weight_ue8m0_inplace(
                            module.weight, module.weight_scale_inv, weight_block_size
                        )

                experts = layer.mlp.experts
                if isinstance(experts, DeepEPMoE):
                    for w in [
                        experts.w13_weight_fp8,
                        experts.w2_weight_fp8,
                    ]:
                        requant_weight_ue8m0_inplace(w[0], w[1], weight_block_size)
            else:
                mlp = layer.mlp
                assert isinstance(mlp, DeepseekV2MLP)
                for module in [
                    mlp.gate_up_proj,
                    mlp.down_proj,
                ]:
                    requant_weight_ue8m0_inplace(
                        module.weight, module.weight_scale_inv, weight_block_size
                    )

    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn=False):

        if is_nextn:
            if hasattr(self.config, &#34;num_nextn_predict_layers&#34;):
                num_nextn_layers = self.config.num_nextn_predict_layers
                assert num_nextn_layers == 1, &#34;Only 1 nextn layer is supported&#34;
                # compatible with old design
                nextn_layer_id = (
                    0
                    if self.config.num_hidden_layers == 1
                    else self.config.num_hidden_layers
                )
            else:
                raise ValueError(&#34;num_nextn_predict_layers is not in the config&#34;)

        stacked_params_mapping = [
            # (param_name, shard_name, shard_id)
            (&#34;gate_up_proj&#34;, &#34;gate_proj&#34;, 0),
            (&#34;gate_up_proj&#34;, &#34;up_proj&#34;, 1),
        ]

        # Params for weights, fp8 weight scales, fp8 activation scales
        # (param_name, weight_name, expert_id, shard_id)
        expert_params_mapping = FusedMoE.make_expert_params_mapping(
            ckpt_gate_proj_name=&#34;gate_proj&#34;,
            ckpt_down_proj_name=&#34;down_proj&#34;,
            ckpt_up_proj_name=&#34;up_proj&#34;,
            num_experts=self.config.n_routed_experts + self.num_fused_shared_experts,
        )
        if self.quant_config and self.quant_config.get_name() == &#34;w4afp8&#34;:
            expert_params_mapping += FusedMoE.make_expert_input_scale_params_mapping(
                num_experts=self.config.n_routed_experts
            )

        # Fuse q_a_proj and kv_a_proj_with_mqa along output dimension when q_lora_rank is not None
        fuse_qkv_a_proj = hasattr(self.config, &#34;q_lora_rank&#34;) and (
            self.config.q_lora_rank is not None
        )
        cached_a_proj = {} if fuse_qkv_a_proj else None

        if is_nextn:
            nextn_layer_prefix = f&#34;model.layers.{nextn_layer_id}&#34;
            nextn_spec_weight_names = [
                &#34;shared_head.norm&#34;,
                &#34;eh_proj&#34;,
                &#34;enorm&#34;,
                &#34;hnorm&#34;,
            ]

        if self.num_fused_shared_experts &gt; 0:
            assert self.num_fused_shared_experts == 1
            log_info_on_rank0(logger, &#34;Shared experts fusion optimization enabled.&#34;)

        with concurrent.futures.ThreadPoolExecutor() as executor:
            futures = []
            params_dict = dict(self.named_parameters())
            weight_names = []
            for name, loaded_weight in weights:
                layer_id = get_layer_id(name)
                if (
                    layer_id is not None
                    and hasattr(self.model, &#34;start_layer&#34;)
                    and (
                        layer_id &lt; self.model.start_layer
                        or layer_id &gt;= self.model.end_layer
                    )
                ):
                    continue
                if self.num_fused_shared_experts &gt; 0 and &#34;mlp.shared_experts&#34; in name:
                    name = name.replace(
                        &#34;mlp.shared_experts&#34;,
                        f&#34;mlp.experts.{self.config.n_routed_experts}&#34;,
                    )

                weight_names.append(name)

                if not is_nextn:
                    if hasattr(self.config, &#34;num_nextn_predict_layers&#34;):
                        num_nextn_layers = self.config.num_nextn_predict_layers
                        if num_nextn_layers &gt; 0 and name.startswith(&#34;model.layers&#34;):
                            name_list = name.split(&#34;.&#34;)
                            if (
                                len(name_list) &gt;= 3
                                and int(name_list[2]) &gt;= self.config.num_hidden_layers
                            ):
                                continue
                else:
                    if not name.startswith(nextn_layer_prefix):
                        continue

                    # Use shared head and embed weights from target model
                    if &#34;shared_head.head&#34; in name or &#34;embed_tokens&#34; in name:
                        continue

                    is_decoder = True
                    # For nextn specific weights
                    for weight_name in nextn_spec_weight_names:
                        if weight_name in name:
                            name = name.replace(nextn_layer_prefix, &#34;model&#34;)
                            is_decoder = False
                            break
                    # For decoder layer weights
                    if is_decoder:
                        name = name.replace(nextn_layer_prefix, &#34;model.decoder&#34;)

                if &#34;rotary_emb.inv_freq&#34; in name:
                    continue
                for param_name, weight_name, shard_id in stacked_params_mapping:
                    # Skip non-stacked layers and experts (experts handled below).
                    if weight_name not in name:
                        continue
                    # We have mlp.experts[0].gate_proj in the checkpoint.
                    # Since we handle the experts below in expert_params_mapping,
                    # we need to skip here BEFORE we update the name, otherwise
                    # name will be updated to mlp.experts[0].gate_up_proj, which
                    # will then be updated below in expert_params_mapping
                    # for mlp.experts[0].gate_gate_up_proj, which breaks load.
                    if (&#34;mlp.experts.&#34; in name) and name not in params_dict:
                        continue
                    name = name.replace(weight_name, param_name)
                    # Skip loading extra bias for GPTQ models.
                    if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                        continue
                    param = params_dict[name]
                    weight_loader = param.weight_loader
                    futures.append(
                        executor.submit(weight_loader, param, loaded_weight, shard_id)
                    )
                    break
                else:
                    for mapping in expert_params_mapping:
                        param_name, weight_name, expert_id, shard_id = mapping
                        if weight_name not in name:
                            continue
                        name = name.replace(weight_name, param_name)
                        param = params_dict[name]
                        weight_loader = param.weight_loader
                        futures.append(
                            executor.submit(
                                weight_loader,
                                param,
                                loaded_weight,
                                name,
                                shard_id=shard_id,
                                expert_id=expert_id,
                            )
                        )
                        break
                    else:
                        # Skip loading extra bias for GPTQ models.
                        if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                            continue
                        # Skip loading embed_tokens if not first rank in pipeline parallelism
                        if &#34;.embed_tokens.&#34; in name and not self.pp_group.is_first_rank:
                            continue
                        # Skip loading norm if not last rank in pipeline parallelism
                        if &#34;.norm.&#34; in name and not self.pp_group.is_last_rank:
                            continue
                        if fuse_qkv_a_proj and (
                            &#34;q_a_proj&#34; in name or &#34;kv_a_proj_with_mqa&#34; in name
                        ):
                            cached_a_proj[name] = loaded_weight
                            q_a_proj_name = (
                                name
                                if &#34;q_a_proj&#34; in name
                                else name.replace(&#34;kv_a_proj_with_mqa&#34;, &#34;q_a_proj&#34;)
                            )
                            kv_a_proj_name = (
                                name
                                if &#34;kv_a_proj_with_mqa&#34; in name
                                else name.replace(&#34;q_a_proj&#34;, &#34;kv_a_proj_with_mqa&#34;)
                            )

                            # When both q_a_proj and kv_a_proj_with_mqa has been cached, load the fused weight to parameter
                            if (
                                q_a_proj_name in cached_a_proj
                                and kv_a_proj_name in cached_a_proj
                            ):
                                q_a_proj_weight = cached_a_proj[q_a_proj_name]
                                kv_a_proj_weight = cached_a_proj[kv_a_proj_name]
                                cat_dim = 0
                                if self.quant_config is not None and (
                                    self.quant_config.get_name() == &#34;awq&#34;
                                    or self.quant_config.get_name() == &#34;awq_marlin&#34;
                                    or self.quant_config.get_name() == &#34;moe_wna16&#34;
                                ):
                                    cat_dim = 1
                                fused_weight = torch.cat(
                                    [q_a_proj_weight, kv_a_proj_weight], dim=cat_dim
                                )
                                param_name = (
                                    name.replace(
                                        &#34;q_a_proj&#34;, &#34;fused_qkv_a_proj_with_mqa&#34;
                                    )
                                    if &#34;q_a_proj&#34; in name
                                    else name.replace(
                                        &#34;kv_a_proj_with_mqa&#34;,
                                        &#34;fused_qkv_a_proj_with_mqa&#34;,
                                    )
                                )
                                param = params_dict[param_name]

                                weight_loader = getattr(
                                    param, &#34;weight_loader&#34;, default_weight_loader
                                )
                                futures.append(
                                    executor.submit(weight_loader, param, fused_weight)
                                )
                                cached_a_proj.pop(q_a_proj_name)
                                cached_a_proj.pop(kv_a_proj_name)
                        else:
                            if (
                                &#34;k_scale&#34; in name or &#34;v_scale&#34; in name
                            ) and name not in params_dict:
                                # modelopt attn kv scale is named differently
                                for scale in [&#34;k_scale&#34;, &#34;v_scale&#34;]:
                                    if scale in name:
                                        name = name.replace(
                                            f&#34;{scale[0]}_proj&#34;, &#34;attn_mqa&#34;
                                        )
                                        break
                            if name not in params_dict:
                                # modelopt ckpt contains not needed weights for MTP module:
                                # model.decoder.self_attn.attn_mqa.v_scale and
                                # model.decoder.self_attn.attn_mqa.k_scale
                                logger.warning(f&#34;{name} not found in params_dict.&#34;)
                                continue
                            param = params_dict[name]
                            weight_loader = getattr(
                                param, &#34;weight_loader&#34;, default_weight_loader
                            )
                            futures.append(
                                executor.submit(weight_loader, param, loaded_weight)
                            )

            # Wait for all tasks to complete and raise any exceptions.
            for future in concurrent.futures.as_completed(futures):
                future.result()

        self.post_load_weights(is_nextn=is_nextn, weight_names=weight_names)

    def get_embed_and_head(self):
        return self.model.embed_tokens.weight, self.lm_head.weight

    def set_embed_and_head(self, embed, head):
        del self.model.embed_tokens.weight
        del self.lm_head.weight
        self.model.embed_tokens.weight = embed
        self.lm_head.weight = head
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

    @classmethod
    def get_model_config_for_expert_location(cls, config):
        return ModelConfigForExpertLocation(
            num_layers=config.num_hidden_layers,
            num_logical_experts=config.n_routed_experts,
            num_groups=config.n_group,
        )</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.models.deepseek_v2.DeepseekV3ForCausalLM" href="#sglang.srt.models.deepseek_v2.DeepseekV3ForCausalLM">DeepseekV3ForCausalLM</a></li>
<li><a title="sglang.srt.models.glm4_moe.Glm4MoeForCausalLM" href="glm4_moe.html#sglang.srt.models.glm4_moe.Glm4MoeForCausalLM">Glm4MoeForCausalLM</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.packed_modules_mapping"><code class="name">var <span class="ident">packed_modules_mapping</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.get_model_config_for_expert_location"><code class="name flex">
<span>def <span class="ident">get_model_config_for_expert_location</span></span>(<span>config)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.end_layer"><code class="name">prop <span class="ident">end_layer</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def end_layer(self):
    return self.model.end_layer</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.routed_experts_weights_of_layer"><code class="name">prop <span class="ident">routed_experts_weights_of_layer</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def routed_experts_weights_of_layer(self):
    return self._routed_experts_weights_of_layer.value</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.start_layer"><code class="name">prop <span class="ident">start_layer</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def start_layer(self):
    return self.model.start_layer</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.determine_num_fused_shared_experts"><code class="name flex">
<span>def <span class="ident">determine_num_fused_shared_experts</span></span>(<span>self, architecture: str = 'DeepseekV3ForCausalLM')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def determine_num_fused_shared_experts(
    self, architecture: str = &#34;DeepseekV3ForCausalLM&#34;
):
    self.num_fused_shared_experts = 0
    if global_server_args_dict[&#34;disable_shared_experts_fusion&#34;]:
        return

    # Only Deepseek V3/R1 can use shared experts fusion optimization now.
    disable_reason = None
    if (
        not _is_cuda
        or torch.cuda.get_device_capability(&#34;cuda&#34;) &lt; (8, 0)
        or self.config.architectures[0] != architecture
        or self.config.n_routed_experts != 256
        or self.config.n_shared_experts != 1
    ):
        disable_reason = &#34;Only Deepseek V3/R1 on NV-platform with capability &gt;= 80 can use shared experts fusion optimization.&#34;
    elif get_moe_expert_parallel_world_size() &gt; 1:
        disable_reason = &#34;Deepseek V3/R1 can not use shared experts fusion optimization under expert parallelism.&#34;

    if disable_reason is not None:
        global_server_args_dict[&#34;disable_shared_experts_fusion&#34;] = True
        self.num_fused_shared_experts = 0
        log_info_on_rank0(
            logger,
            f&#34;{disable_reason} Shared experts fusion optimization is disabled.&#34;,
        )
        return

    self.num_fused_shared_experts = self.config.n_shared_experts</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>input_ids: torch.Tensor,<br>positions: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>input_embeds: torch.Tensor = None,<br>pp_proxy_tensors: <a title="sglang.srt.model_executor.forward_batch_info.PPProxyTensors" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.PPProxyTensors">PPProxyTensors</a> | None = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def forward(
    self,
    input_ids: torch.Tensor,
    positions: torch.Tensor,
    forward_batch: ForwardBatch,
    input_embeds: torch.Tensor = None,
    pp_proxy_tensors: Optional[PPProxyTensors] = None,
) -&gt; torch.Tensor:
    hidden_states = self.model(
        input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors
    )

    if self.pp_group.is_last_rank:
        return self.logits_processor(
            input_ids, hidden_states, self.lm_head, forward_batch
        )
    else:
        return hidden_states</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.get_embed_and_head"><code class="name flex">
<span>def <span class="ident">get_embed_and_head</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_embed_and_head(self):
    return self.model.embed_tokens.weight, self.lm_head.weight</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.get_input_embeddings"><code class="name flex">
<span>def <span class="ident">get_input_embeddings</span></span>(<span>self) ‑> torch.nn.modules.sparse.Embedding</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_input_embeddings(self) -&gt; nn.Embedding:
    return self.model.embed_tokens</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.load_weights"><code class="name flex">
<span>def <span class="ident">load_weights</span></span>(<span>self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn=False):

    if is_nextn:
        if hasattr(self.config, &#34;num_nextn_predict_layers&#34;):
            num_nextn_layers = self.config.num_nextn_predict_layers
            assert num_nextn_layers == 1, &#34;Only 1 nextn layer is supported&#34;
            # compatible with old design
            nextn_layer_id = (
                0
                if self.config.num_hidden_layers == 1
                else self.config.num_hidden_layers
            )
        else:
            raise ValueError(&#34;num_nextn_predict_layers is not in the config&#34;)

    stacked_params_mapping = [
        # (param_name, shard_name, shard_id)
        (&#34;gate_up_proj&#34;, &#34;gate_proj&#34;, 0),
        (&#34;gate_up_proj&#34;, &#34;up_proj&#34;, 1),
    ]

    # Params for weights, fp8 weight scales, fp8 activation scales
    # (param_name, weight_name, expert_id, shard_id)
    expert_params_mapping = FusedMoE.make_expert_params_mapping(
        ckpt_gate_proj_name=&#34;gate_proj&#34;,
        ckpt_down_proj_name=&#34;down_proj&#34;,
        ckpt_up_proj_name=&#34;up_proj&#34;,
        num_experts=self.config.n_routed_experts + self.num_fused_shared_experts,
    )
    if self.quant_config and self.quant_config.get_name() == &#34;w4afp8&#34;:
        expert_params_mapping += FusedMoE.make_expert_input_scale_params_mapping(
            num_experts=self.config.n_routed_experts
        )

    # Fuse q_a_proj and kv_a_proj_with_mqa along output dimension when q_lora_rank is not None
    fuse_qkv_a_proj = hasattr(self.config, &#34;q_lora_rank&#34;) and (
        self.config.q_lora_rank is not None
    )
    cached_a_proj = {} if fuse_qkv_a_proj else None

    if is_nextn:
        nextn_layer_prefix = f&#34;model.layers.{nextn_layer_id}&#34;
        nextn_spec_weight_names = [
            &#34;shared_head.norm&#34;,
            &#34;eh_proj&#34;,
            &#34;enorm&#34;,
            &#34;hnorm&#34;,
        ]

    if self.num_fused_shared_experts &gt; 0:
        assert self.num_fused_shared_experts == 1
        log_info_on_rank0(logger, &#34;Shared experts fusion optimization enabled.&#34;)

    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = []
        params_dict = dict(self.named_parameters())
        weight_names = []
        for name, loaded_weight in weights:
            layer_id = get_layer_id(name)
            if (
                layer_id is not None
                and hasattr(self.model, &#34;start_layer&#34;)
                and (
                    layer_id &lt; self.model.start_layer
                    or layer_id &gt;= self.model.end_layer
                )
            ):
                continue
            if self.num_fused_shared_experts &gt; 0 and &#34;mlp.shared_experts&#34; in name:
                name = name.replace(
                    &#34;mlp.shared_experts&#34;,
                    f&#34;mlp.experts.{self.config.n_routed_experts}&#34;,
                )

            weight_names.append(name)

            if not is_nextn:
                if hasattr(self.config, &#34;num_nextn_predict_layers&#34;):
                    num_nextn_layers = self.config.num_nextn_predict_layers
                    if num_nextn_layers &gt; 0 and name.startswith(&#34;model.layers&#34;):
                        name_list = name.split(&#34;.&#34;)
                        if (
                            len(name_list) &gt;= 3
                            and int(name_list[2]) &gt;= self.config.num_hidden_layers
                        ):
                            continue
            else:
                if not name.startswith(nextn_layer_prefix):
                    continue

                # Use shared head and embed weights from target model
                if &#34;shared_head.head&#34; in name or &#34;embed_tokens&#34; in name:
                    continue

                is_decoder = True
                # For nextn specific weights
                for weight_name in nextn_spec_weight_names:
                    if weight_name in name:
                        name = name.replace(nextn_layer_prefix, &#34;model&#34;)
                        is_decoder = False
                        break
                # For decoder layer weights
                if is_decoder:
                    name = name.replace(nextn_layer_prefix, &#34;model.decoder&#34;)

            if &#34;rotary_emb.inv_freq&#34; in name:
                continue
            for param_name, weight_name, shard_id in stacked_params_mapping:
                # Skip non-stacked layers and experts (experts handled below).
                if weight_name not in name:
                    continue
                # We have mlp.experts[0].gate_proj in the checkpoint.
                # Since we handle the experts below in expert_params_mapping,
                # we need to skip here BEFORE we update the name, otherwise
                # name will be updated to mlp.experts[0].gate_up_proj, which
                # will then be updated below in expert_params_mapping
                # for mlp.experts[0].gate_gate_up_proj, which breaks load.
                if (&#34;mlp.experts.&#34; in name) and name not in params_dict:
                    continue
                name = name.replace(weight_name, param_name)
                # Skip loading extra bias for GPTQ models.
                if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                    continue
                param = params_dict[name]
                weight_loader = param.weight_loader
                futures.append(
                    executor.submit(weight_loader, param, loaded_weight, shard_id)
                )
                break
            else:
                for mapping in expert_params_mapping:
                    param_name, weight_name, expert_id, shard_id = mapping
                    if weight_name not in name:
                        continue
                    name = name.replace(weight_name, param_name)
                    param = params_dict[name]
                    weight_loader = param.weight_loader
                    futures.append(
                        executor.submit(
                            weight_loader,
                            param,
                            loaded_weight,
                            name,
                            shard_id=shard_id,
                            expert_id=expert_id,
                        )
                    )
                    break
                else:
                    # Skip loading extra bias for GPTQ models.
                    if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                        continue
                    # Skip loading embed_tokens if not first rank in pipeline parallelism
                    if &#34;.embed_tokens.&#34; in name and not self.pp_group.is_first_rank:
                        continue
                    # Skip loading norm if not last rank in pipeline parallelism
                    if &#34;.norm.&#34; in name and not self.pp_group.is_last_rank:
                        continue
                    if fuse_qkv_a_proj and (
                        &#34;q_a_proj&#34; in name or &#34;kv_a_proj_with_mqa&#34; in name
                    ):
                        cached_a_proj[name] = loaded_weight
                        q_a_proj_name = (
                            name
                            if &#34;q_a_proj&#34; in name
                            else name.replace(&#34;kv_a_proj_with_mqa&#34;, &#34;q_a_proj&#34;)
                        )
                        kv_a_proj_name = (
                            name
                            if &#34;kv_a_proj_with_mqa&#34; in name
                            else name.replace(&#34;q_a_proj&#34;, &#34;kv_a_proj_with_mqa&#34;)
                        )

                        # When both q_a_proj and kv_a_proj_with_mqa has been cached, load the fused weight to parameter
                        if (
                            q_a_proj_name in cached_a_proj
                            and kv_a_proj_name in cached_a_proj
                        ):
                            q_a_proj_weight = cached_a_proj[q_a_proj_name]
                            kv_a_proj_weight = cached_a_proj[kv_a_proj_name]
                            cat_dim = 0
                            if self.quant_config is not None and (
                                self.quant_config.get_name() == &#34;awq&#34;
                                or self.quant_config.get_name() == &#34;awq_marlin&#34;
                                or self.quant_config.get_name() == &#34;moe_wna16&#34;
                            ):
                                cat_dim = 1
                            fused_weight = torch.cat(
                                [q_a_proj_weight, kv_a_proj_weight], dim=cat_dim
                            )
                            param_name = (
                                name.replace(
                                    &#34;q_a_proj&#34;, &#34;fused_qkv_a_proj_with_mqa&#34;
                                )
                                if &#34;q_a_proj&#34; in name
                                else name.replace(
                                    &#34;kv_a_proj_with_mqa&#34;,
                                    &#34;fused_qkv_a_proj_with_mqa&#34;,
                                )
                            )
                            param = params_dict[param_name]

                            weight_loader = getattr(
                                param, &#34;weight_loader&#34;, default_weight_loader
                            )
                            futures.append(
                                executor.submit(weight_loader, param, fused_weight)
                            )
                            cached_a_proj.pop(q_a_proj_name)
                            cached_a_proj.pop(kv_a_proj_name)
                    else:
                        if (
                            &#34;k_scale&#34; in name or &#34;v_scale&#34; in name
                        ) and name not in params_dict:
                            # modelopt attn kv scale is named differently
                            for scale in [&#34;k_scale&#34;, &#34;v_scale&#34;]:
                                if scale in name:
                                    name = name.replace(
                                        f&#34;{scale[0]}_proj&#34;, &#34;attn_mqa&#34;
                                    )
                                    break
                        if name not in params_dict:
                            # modelopt ckpt contains not needed weights for MTP module:
                            # model.decoder.self_attn.attn_mqa.v_scale and
                            # model.decoder.self_attn.attn_mqa.k_scale
                            logger.warning(f&#34;{name} not found in params_dict.&#34;)
                            continue
                        param = params_dict[name]
                        weight_loader = getattr(
                            param, &#34;weight_loader&#34;, default_weight_loader
                        )
                        futures.append(
                            executor.submit(weight_loader, param, loaded_weight)
                        )

        # Wait for all tasks to complete and raise any exceptions.
        for future in concurrent.futures.as_completed(futures):
            future.result()

    self.post_load_weights(is_nextn=is_nextn, weight_names=weight_names)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.post_load_weights"><code class="name flex">
<span>def <span class="ident">post_load_weights</span></span>(<span>self, is_nextn=False, weight_names=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def post_load_weights(self, is_nextn=False, weight_names=None):

    # Perform post-processing after loading weights
    if is_nextn:
        layer_ids = [self.config.num_hidden_layers]
    else:
        if weight_names is None:
            layer_ids = range(self.model.start_layer, self.model.end_layer)
        else:
            layer_ids = set()
            for name in weight_names:
                if &#34;kv_b_proj&#34; in name:
                    layer_id = int(name.split(&#34;.&#34;)[2])
                    if layer_id &lt; self.config.num_hidden_layers:
                        layer_ids.add(layer_id)

    for layer_id in layer_ids:
        self_attn = (
            self.model.layers[layer_id].self_attn
            if not is_nextn
            else self.model.decoder.self_attn
        )
        if hasattr(self_attn.kv_b_proj, &#34;qweight&#34;):
            # AWQ compatible
            if _is_cuda or _is_hip:
                w = awq_dequantize(
                    self_attn.kv_b_proj.qweight,
                    self_attn.kv_b_proj.scales,
                    self_attn.kv_b_proj.qzeros,
                ).T
            else:
                w = awq_dequantize(
                    self_attn.kv_b_proj.qweight,
                    self_attn.kv_b_proj.scales,
                    self_attn.kv_b_proj.qzeros,
                    0,
                    0,
                    0,
                ).T
        else:
            w = self_attn.kv_b_proj.weight
        # NOTE(HandH1998): Since `bmm_fp8` only supports per-tensor scale, we have to requantize `self_attn.kv_b_proj`.
        # This may affect the accuracy of fp8 model.
        # Fix deepseek v3 blockwise bmm by using deep_gemm
        use_deep_gemm_bmm = False

        if w.dtype in (
            torch.float8_e4m3fn,
            torch.float8_e4m3fnuz,
        ):
            if (
                hasattr(self.quant_config, &#34;weight_block_size&#34;)
                and self.quant_config.weight_block_size is not None
            ):
                weight_block_size = self.quant_config.weight_block_size
                assert hasattr(self_attn.kv_b_proj, &#34;weight_scale_inv&#34;)
                if _is_fp8_fnuz:
                    weight, weight_scale, _ = normalize_e4m3fn_to_e4m3fnuz(
                        weight=w,
                        weight_scale=self_attn.kv_b_proj.weight_scale_inv,
                        input_scale=None,
                    )
                else:
                    weight = w
                    weight_scale = self_attn.kv_b_proj.weight_scale_inv

                if (
                    _is_cuda
                    and weight_block_size[0] == 128
                    and weight_block_size[1] == 128
                ):
                    if (
                        deep_gemm_wrapper.ENABLE_JIT_DEEPGEMM
                        and not deep_gemm_wrapper.DEEPGEMM_BLACKWELL
                        and get_bool_env_var(&#34;SGL_USE_DEEPGEMM_BMM&#34;, &#34;false&#34;)
                    ):
                        block_scale = weight_scale
                        use_deep_gemm_bmm = True
                    else:
                        w = block_quant_dequant(
                            weight,
                            weight_scale,
                            weight_block_size,
                            torch.bfloat16,
                        )
                else:
                    w, scale = block_quant_to_tensor_quant(
                        weight, weight_scale, weight_block_size
                    )
                    self_attn.w_scale = scale
            else:
                if _is_fp8_fnuz:
                    weight, weight_scale, _ = normalize_e4m3fn_to_e4m3fnuz(
                        weight=w,
                        weight_scale=self_attn.kv_b_proj.weight_scale,
                        input_scale=None,
                    )
                else:
                    weight = w
                    weight_scale = self_attn.kv_b_proj.weight_scale

                w, scale = channel_quant_to_tensor_quant(weight, weight_scale)
                self_attn.w_scale = scale

        if w.dtype == torch.int8:
            if hasattr(self.quant_config, &#34;weight_block_size&#34;):
                # block-wise int8 need it
                weight_block_size = self.quant_config.weight_block_size
                if weight_block_size is not None:
                    assert hasattr(self_attn.kv_b_proj, &#34;weight_scale_inv&#34;)
                    weight = w
                    weight_scale = self_attn.kv_b_proj.weight_scale_inv
                    w = int8_block_dequant(
                        weight, weight_scale, weight_block_size
                    ).to(torch.bfloat16)
            else:
                # channel-wise int8 need it
                w = w.to(torch.bfloat16) * self_attn.kv_b_proj.weight_scale.to(
                    torch.bfloat16
                )

        w_kc, w_vc = w.unflatten(
            0, (-1, self_attn.qk_nope_head_dim + self_attn.v_head_dim)
        ).split([self_attn.qk_nope_head_dim, self_attn.v_head_dim], dim=1)
        if not use_deep_gemm_bmm:
            self_attn.w_kc = bind_or_assign(
                self_attn.w_kc, w_kc.transpose(1, 2).contiguous().transpose(1, 2)
            )
            self_attn.w_vc = bind_or_assign(
                self_attn.w_vc, w_vc.contiguous().transpose(1, 2)
            )
            if (
                hasattr(self_attn.kv_b_proj, &#34;weight_scale&#34;)
                and self_attn.w_scale is None
            ):
                self_attn.w_scale = bind_or_assign(
                    self_attn.w_scale, self_attn.kv_b_proj.weight_scale
                )
                if _is_hip:
                    self_attn.w_scale *= 2.0
            # TODO: remove this after adding FP8 support in bmm cpu kernel
            if _is_cpu and _is_cpu_amx_available and w.dtype == torch.float8_e4m3fn:
                self_attn.w_kc = (
                    self_attn.w_kc.to(torch.bfloat16) * self_attn.w_scale
                )
                self_attn.w_vc = (
                    self_attn.w_vc.to(torch.bfloat16) * self_attn.w_scale
                )
        else:
            num_tiles_k = self_attn.qk_nope_head_dim // weight_block_size[1]
            num_tiles_n = self_attn.v_head_dim // weight_block_size[0]
            ws_kc, ws_vc = block_scale.unflatten(
                0, (-1, (num_tiles_k + num_tiles_n))
            ).split([num_tiles_k, num_tiles_n], dim=1)
            self_attn.w_scale_k = bind_or_assign(
                self_attn.w_scale_k, ws_kc.transpose(1, 2).contiguous()
            )
            self_attn.w_scale_v = bind_or_assign(
                self_attn.w_scale_v, ws_vc.contiguous()
            )
            self_attn.w_kc = bind_or_assign(
                self_attn.w_kc, w_kc.transpose(1, 2).contiguous()
            )
            self_attn.w_vc = bind_or_assign(self_attn.w_vc, w_vc.contiguous())
            self_attn.use_deep_gemm_bmm = True

    if (
        deep_gemm_wrapper.ENABLE_JIT_DEEPGEMM
        and deep_gemm_wrapper.DEEPGEMM_SCALE_UE8M0
        and hasattr(self.quant_config, &#34;weight_block_size&#34;)
        and self.quant_config.weight_block_size is not None
    ):
        self._weight_requant_ue8m0(is_nextn)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.set_embed_and_head"><code class="name flex">
<span>def <span class="ident">set_embed_and_head</span></span>(<span>self, embed, head)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_embed_and_head(self, embed, head):
    del self.model.embed_tokens.weight
    del self.lm_head.weight
    self.model.embed_tokens.weight = embed
    self.lm_head.weight = head
    torch.cuda.empty_cache()
    torch.cuda.synchronize()</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2MLP"><code class="flex name class">
<span>class <span class="ident">DeepseekV2MLP</span></span>
<span>(</span><span>hidden_size: int,<br>intermediate_size: int,<br>hidden_act: str,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>reduce_results: bool = True,<br>prefix: str = '',<br>tp_rank: int | None = None,<br>tp_size: int | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DeepseekV2MLP(nn.Module):
    def __init__(
        self,
        hidden_size: int,
        intermediate_size: int,
        hidden_act: str,
        quant_config: Optional[QuantizationConfig] = None,
        reduce_results: bool = True,
        prefix: str = &#34;&#34;,
        tp_rank: Optional[int] = None,
        tp_size: Optional[int] = None,
    ) -&gt; None:
        super().__init__()
        self.tp_size = tp_size

        self.gate_up_proj = MergedColumnParallelLinear(
            hidden_size,
            [intermediate_size] * 2,
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;gate_up_proj&#34;, prefix),
            tp_rank=tp_rank,
            tp_size=tp_size,
        )
        self.down_proj = RowParallelLinear(
            intermediate_size,
            hidden_size,
            bias=False,
            quant_config=quant_config,
            reduce_results=reduce_results,
            prefix=add_prefix(&#34;down_proj&#34;, prefix),
            tp_rank=tp_rank,
            tp_size=tp_size,
        )
        if hidden_act != &#34;silu&#34;:
            raise ValueError(
                f&#34;Unsupported activation: {hidden_act}. &#34;
                &#34;Only silu is supported for now.&#34;
            )
        self.act_fn = SiluAndMul()

    def forward(
        self,
        x,
        forward_batch=None,
        should_allreduce_fusion: bool = False,
        use_reduce_scatter: bool = False,
    ):
        if (self.tp_size == 1) and x.shape[0] == 0:
            return x

        gate_up, _ = self.gate_up_proj(x)
        x = self.act_fn(gate_up)
        x, _ = self.down_proj(
            x, skip_all_reduce=should_allreduce_fusion or use_reduce_scatter
        )
        return x</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2MLP.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>x,<br>forward_batch=None,<br>should_allreduce_fusion: bool = False,<br>use_reduce_scatter: bool = False) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    x,
    forward_batch=None,
    should_allreduce_fusion: bool = False,
    use_reduce_scatter: bool = False,
):
    if (self.tp_size == 1) and x.shape[0] == 0:
        return x

    gate_up, _ = self.gate_up_proj(x)
    x = self.act_fn(gate_up)
    x, _ = self.down_proj(
        x, skip_all_reduce=should_allreduce_fusion or use_reduce_scatter
    )
    return x</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2MoE"><code class="flex name class">
<span>class <span class="ident">DeepseekV2MoE</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig,<br>layer_id: int,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '',<br>alt_stream: torch.cuda.streams.Stream | None = None,<br>is_nextn: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DeepseekV2MoE(nn.Module):

    def __init__(
        self,
        config: PretrainedConfig,
        layer_id: int,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
        alt_stream: Optional[torch.cuda.Stream] = None,
        is_nextn: bool = False,
    ):
        super().__init__()
        self.tp_size = get_tensor_model_parallel_world_size()
        self.routed_scaling_factor = config.routed_scaling_factor
        self.n_shared_experts = config.n_shared_experts
        self.num_fused_shared_experts = (
            0
            if global_server_args_dict[&#34;disable_shared_experts_fusion&#34;]
            else config.n_shared_experts
        )
        self.config = config
        self.layer_id = layer_id
        self.alt_stream = alt_stream

        if self.tp_size &gt; config.n_routed_experts:
            raise ValueError(
                f&#34;Tensor parallel size {self.tp_size} is greater than &#34;
                f&#34;the number of experts {config.n_routed_experts}.&#34;
            )

        if config.hidden_act != &#34;silu&#34;:
            raise ValueError(
                f&#34;Unsupported activation: {config.hidden_act}. &#34;
                &#34;Only silu is supported for now.&#34;
            )

        self.gate = MoEGate(
            config=config, prefix=add_prefix(&#34;gate&#34;, prefix), is_nextn=is_nextn
        )

        self.experts = get_moe_impl_class(quant_config)(
            num_experts=config.n_routed_experts
            + self.num_fused_shared_experts
            + global_server_args_dict[&#34;ep_num_redundant_experts&#34;],
            num_fused_shared_experts=self.num_fused_shared_experts,
            top_k=config.num_experts_per_tok + self.num_fused_shared_experts,
            hidden_size=config.hidden_size,
            intermediate_size=config.moe_intermediate_size,
            layer_id=self.layer_id,
            quant_config=quant_config,
            routed_scaling_factor=self.routed_scaling_factor,
            prefix=add_prefix(&#34;experts&#34;, prefix),
        )

        self.topk = TopK(
            top_k=config.num_experts_per_tok + self.num_fused_shared_experts,
            renormalize=config.norm_topk_prob,
            use_grouped_topk=True,
            num_expert_group=config.n_group,
            num_fused_shared_experts=self.num_fused_shared_experts,
            topk_group=config.topk_group,
            correction_bias=self.gate.e_score_correction_bias,
            routed_scaling_factor=self.routed_scaling_factor,
            apply_routed_scaling_factor_on_output=self.experts.should_fuse_routed_scaling_factor_in_topk(),
            force_topk=quant_config is None,
        )

        self.shared_experts_is_int8 = False
        self.shared_experts_is_fp8 = False
        self.shared_experts_weight_block_size = None
        if config.n_shared_experts is not None and self.num_fused_shared_experts == 0:
            intermediate_size = config.moe_intermediate_size * config.n_shared_experts
            # disable tp for shared experts when enable deepep moe, or with fp4 allgather
            self.shared_experts = DeepseekV2MLP(
                hidden_size=config.hidden_size,
                intermediate_size=intermediate_size,
                hidden_act=config.hidden_act,
                quant_config=quant_config,
                reduce_results=False,
                prefix=add_prefix(&#34;shared_experts&#34;, prefix),
                **(
                    dict(tp_rank=0, tp_size=1)
                    if get_moe_a2a_backend().is_deepep()
                    or should_use_flashinfer_cutlass_moe_fp4_allgather()
                    else {}
                ),
            )
            is_packed_weight = hasattr(
                self.shared_experts.gate_up_proj.quant_method, &#34;quant_config&#34;
            ) and self.shared_experts.gate_up_proj.quant_method.quant_config.get_name() in {
                &#34;awq&#34;,
                &#34;awq_marlin&#34;,
                &#34;moe_wna16&#34;,
            }
            self.shared_experts_is_int8 = (
                not is_packed_weight
                and self.shared_experts.gate_up_proj.weight.dtype == torch.int8
            )
            self.shared_experts_is_fp8 = (
                not is_packed_weight
                and self.shared_experts.gate_up_proj.weight.dtype == torch.float8_e4m3fn
            )
            if self.shared_experts_is_fp8:
                assert (
                    self.shared_experts.gate_up_proj.quant_method.quant_config.weight_block_size
                    == self.shared_experts.down_proj.quant_method.quant_config.weight_block_size
                )
                self.shared_experts_weight_block_size = (
                    self.shared_experts.gate_up_proj.quant_method.quant_config.weight_block_size
                )

        self.top_k = config.num_experts_per_tok

        if get_moe_a2a_backend().is_deepep():
            # TODO: we will support tp &lt; ep in the future
            self.ep_size = get_moe_expert_parallel_world_size()
            self.num_experts = (
                config.n_routed_experts
                + global_server_args_dict[&#34;ep_num_redundant_experts&#34;]
            )
            self.renormalize = config.norm_topk_prob
            self.topk_group = config.topk_group
            self.num_expert_group = config.n_group
            self.correction_bias = (
                self.gate.e_score_correction_bias.data
                if self.gate.e_score_correction_bias is not None
                else None
            )

            self.deepep_dispatcher = MaybeTboDeepEPDispatcher(
                group=parallel_state.get_tp_group().device_group,
                router_topk=self.top_k,
                permute_fusion=True,
                num_experts=self.num_experts,
                num_local_experts=config.n_routed_experts // self.tp_size,
                hidden_size=config.hidden_size,
                params_dtype=config.torch_dtype,
                deepep_mode=get_deepep_mode(),
                async_finish=True,
                return_recv_hook=True,
            )

        self._enable_deepep_moe = get_moe_a2a_backend().is_deepep()

    def get_moe_weights(self):
        return [
            x.data
            for name, x in self.experts.named_parameters()
            if name not in [&#34;correction_bias&#34;]
        ]

    def forward(
        self,
        hidden_states: torch.Tensor,
        forward_batch: Optional[ForwardBatch] = None,
        should_allreduce_fusion: bool = False,
        use_reduce_scatter: bool = False,
    ) -&gt; torch.Tensor:
        if not self._enable_deepep_moe:
            DUAL_STREAM_TOKEN_THRESHOLD = 1024
            if (
                self.alt_stream is not None
                and self.num_fused_shared_experts == 0
                and hidden_states.shape[0] &gt; 0
                and hidden_states.shape[0] &lt;= DUAL_STREAM_TOKEN_THRESHOLD
            ):
                return self.forward_normal_dual_stream(
                    hidden_states,
                    should_allreduce_fusion,
                    use_reduce_scatter,
                )
            else:
                return self.forward_normal(
                    hidden_states,
                    should_allreduce_fusion,
                    use_reduce_scatter,
                )
        else:
            return self.forward_deepep(hidden_states, forward_batch)

    def forward_normal_dual_stream(
        self,
        hidden_states: torch.Tensor,
        should_allreduce_fusion: bool = False,
        use_reduce_scatter: bool = False,
    ) -&gt; torch.Tensor:

        current_stream = torch.cuda.current_stream()
        self.alt_stream.wait_stream(current_stream)
        shared_output = self._forward_shared_experts(hidden_states)

        with torch.cuda.stream(self.alt_stream):
            # router_logits: (num_tokens, n_experts)
            router_logits = self.gate(hidden_states)
            topk_output = self.topk(hidden_states, router_logits)
            final_hidden_states = self.experts(hidden_states, topk_output)
            if not _is_cuda:
                final_hidden_states *= self.routed_scaling_factor

        current_stream.wait_stream(self.alt_stream)
        with use_symmetric_memory(parallel_state.get_tp_group()) as sm:
            final_hidden_states_out = torch.empty_like(final_hidden_states)

        torch.add(final_hidden_states, shared_output, out=final_hidden_states_out)
        final_hidden_states = final_hidden_states_out
        sm.tag(final_hidden_states)
        if (
            self.tp_size &gt; 1
            and not should_allreduce_fusion
            and not use_reduce_scatter
            and not should_use_flashinfer_cutlass_moe_fp4_allgather()
        ):
            final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)
        return final_hidden_states

    def forward_normal(
        self,
        hidden_states: torch.Tensor,
        should_allreduce_fusion: bool = False,
        use_reduce_scatter: bool = False,
    ) -&gt; torch.Tensor:
        if hasattr(self, &#34;shared_experts&#34;) and use_intel_amx_backend(
            self.shared_experts.gate_up_proj
        ):
            return self.forward_cpu(hidden_states, should_allreduce_fusion)

        if hidden_states.shape[0] &gt; 0:
            shared_output = self._forward_shared_experts(hidden_states)
            # router_logits: (num_tokens, n_experts)
            router_logits = self.gate(hidden_states)
            topk_output = self.topk(hidden_states, router_logits)
        else:
            shared_output = None
            topk_output = self.topk.empty_topk_output(hidden_states.device)

        final_hidden_states = self.experts(hidden_states, topk_output)
        if not _is_cuda and not _use_aiter:
            # fused in biased_grouped_topk so we can skip here
            final_hidden_states *= self.routed_scaling_factor
        if shared_output is not None:
            with use_symmetric_memory(parallel_state.get_tp_group()) as sm:
                final_hidden_states_out = torch.empty_like(final_hidden_states)
            torch.add(final_hidden_states, shared_output, out=final_hidden_states_out)
            final_hidden_states = final_hidden_states_out
            sm.tag(final_hidden_states)
        if (
            self.tp_size &gt; 1
            and not should_allreduce_fusion
            and not use_reduce_scatter
            and not should_use_flashinfer_cutlass_moe_fp4_allgather()
        ):
            final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)
        return final_hidden_states

    def forward_cpu(
        self,
        hidden_states: torch.Tensor,
        should_allreduce_fusion: bool = False,
    ) -&gt; torch.Tensor:
        # router_logits: (num_tokens, n_experts)
        router_logits = self.gate(hidden_states)
        topk_output = self.topk(hidden_states, router_logits)
        fused_experts_out = self.experts(
            hidden_states=hidden_states, topk_output=topk_output
        )

        assert use_intel_amx_backend(
            self.shared_experts.gate_up_proj
        ) == use_intel_amx_backend(self.shared_experts.down_proj)
        # [Note] inplace should be False in fused_experts.
        # If inplace is True in fused_experts (self.experts), hidden_states will be changed after fused_experts
        # While hidden_states is still needed in shared_expert.
        final_hidden_states = torch.ops.sgl_kernel.shared_expert_cpu(
            hidden_states,
            self.shared_experts.gate_up_proj.weight,
            self.shared_experts.down_proj.weight,
            fused_experts_out,
            self.routed_scaling_factor,
            True,  # inplace
            self.shared_experts_is_int8,  # use_int8_w8a8
            self.shared_experts_is_fp8,  # use_fp8_w8a16
            (
                self.shared_experts.gate_up_proj.weight_scale
                if self.shared_experts_is_int8
                else (
                    self.shared_experts.gate_up_proj.weight_scale_inv
                    if self.shared_experts_is_fp8
                    else None
                )
            ),  # w1_scale
            (
                self.shared_experts.down_proj.weight_scale
                if self.shared_experts_is_int8
                else (
                    self.shared_experts.down_proj.weight_scale_inv
                    if self.shared_experts_is_fp8
                    else None
                )
            ),  # w2_scale
            (
                self.shared_experts_weight_block_size
                if self.shared_experts_is_fp8
                else None
            ),  # block_size
            None,  # a1_scale
            None,  # a2_scale
            True,  # is_vnni
        )
        if self.tp_size &gt; 1 and not should_allreduce_fusion:
            final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)
        return final_hidden_states

    def forward_deepep(
        self, hidden_states: torch.Tensor, forward_batch: ForwardBatch
    ) -&gt; torch.Tensor:
        shared_output = None
        if hidden_states.shape[0] &gt; 0:
            # router_logits: (num_tokens, n_experts)
            router_logits = self.gate(hidden_states)
            shared_output = self._forward_shared_experts(hidden_states)
            topk_weights, topk_idx, _ = self.topk(
                hidden_states,
                router_logits,
                num_token_non_padded=forward_batch.num_token_non_padded,
                expert_location_dispatch_info=ExpertLocationDispatchInfo.init_new(
                    layer_id=self.layer_id,
                ),
            )
        else:
            topk_weights, topk_idx, _ = self.topk.empty_topk_output(
                hidden_states.device
            )

        final_hidden_states = self.experts(
            hidden_states=hidden_states,
            topk_idx=topk_idx,
            topk_weights=topk_weights,
            forward_batch=forward_batch,
        )

        if shared_output is not None:
            x = shared_output
            x.add_(final_hidden_states, alpha=self.routed_scaling_factor)
            final_hidden_states = x
        else:
            final_hidden_states *= self.routed_scaling_factor

        return final_hidden_states

    def _forward_shared_experts(self, hidden_states):
        if self.num_fused_shared_experts == 0:
            return self.shared_experts(hidden_states)
        else:
            return None

    def op_gate(self, state):
        if is_non_idle_and_non_empty(
            state.forward_batch.forward_mode, state.hidden_states_mlp_input
        ):
            # router_logits: (num_tokens, n_experts)
            state.router_logits = self.gate(state.hidden_states_mlp_input)
        else:
            state.router_logits = None

    def op_shared_experts(self, state):
        hidden_states_mlp_input = state.pop(&#34;hidden_states_mlp_input&#34;)
        if (self.num_fused_shared_experts == 0) and is_non_idle_and_non_empty(
            state.forward_batch.forward_mode, hidden_states_mlp_input
        ):
            state.shared_output = self.shared_experts(hidden_states_mlp_input)
        else:
            state.shared_output = None

    def op_select_experts(self, state):
        router_logits = state.pop(&#34;router_logits&#34;)
        hidden_states = state.hidden_states_mlp_input

        if router_logits is not None:
            with get_global_expert_distribution_recorder().with_current_layer(
                self.layer_id
            ):
                state.topk_weights_local, state.topk_idx_local, _ = self.topk(
                    hidden_states=hidden_states,
                    router_logits=router_logits,
                    num_token_non_padded=state.forward_batch.num_token_non_padded,
                    expert_location_dispatch_info=ExpertLocationDispatchInfo.init_new(
                        layer_id=self.layer_id,
                    ),
                )
        else:
            state.topk_idx_local = torch.full(
                (0, self.top_k), -1, dtype=torch.int, device=hidden_states.device
            )
            state.topk_weights_local = torch.empty(
                (0, self.top_k), dtype=torch.float32, device=hidden_states.device
            )

    def op_dispatch_a(self, state):
        if self.ep_size &gt; 1:
            self.experts.deepep_dispatcher.dispatch_a(
                hidden_states=state.hidden_states_mlp_input,
                topk_idx=state.pop(&#34;topk_idx_local&#34;),
                topk_weights=state.pop(&#34;topk_weights_local&#34;),
                forward_batch=state.forward_batch,
                tbo_subbatch_index=state.get(&#34;tbo_subbatch_index&#34;),
            )

    def op_dispatch_b(self, state):
        if self.ep_size &gt; 1:
            with get_global_expert_distribution_recorder().with_current_layer(
                self.layer_id
            ):
                state.dispatch_output = self.experts.deepep_dispatcher.dispatch_b(
                    tbo_subbatch_index=state.get(&#34;tbo_subbatch_index&#34;),
                )

    def op_experts(self, state):
        state.hidden_states_experts_output = self.experts.moe_impl(
            dispatch_output=state.dispatch_output,
        )

    def op_combine_a(self, state):
        if self.ep_size &gt; 1:
            self.experts.deepep_dispatcher.combine_a(
                hidden_states=state.pop(&#34;hidden_states_experts_output&#34;),
                topk_idx=state.dispatch_output.topk_idx,
                topk_weights=state.dispatch_output.topk_weights,
                forward_batch=state.forward_batch,
                tbo_subbatch_index=state.get(&#34;tbo_subbatch_index&#34;),
            )
            state.pop(&#34;dispatch_output&#34;)

    def op_combine_b(self, state):
        if self.ep_size &gt; 1:
            state.hidden_states_after_combine = (
                self.experts.deepep_dispatcher.combine_b(
                    tbo_subbatch_index=state.get(&#34;tbo_subbatch_index&#34;),
                )
            )

    def op_output(self, state):
        final_hidden_states = state.pop(&#34;hidden_states_after_combine&#34;)

        if (shared_output := state.pop(&#34;shared_output&#34;)) is not None:
            x = shared_output
            x.add_(final_hidden_states, alpha=self.routed_scaling_factor)
            final_hidden_states = x
        else:
            final_hidden_states *= self.routed_scaling_factor

        state.hidden_states_mlp_output = final_hidden_states</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.models.glm4_moe.Glm4MoeSparseMoeBlock" href="glm4_moe.html#sglang.srt.models.glm4_moe.Glm4MoeSparseMoeBlock">Glm4MoeSparseMoeBlock</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2MoE.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>hidden_states: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a> | None = None,<br>should_allreduce_fusion: bool = False,<br>use_reduce_scatter: bool = False) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    hidden_states: torch.Tensor,
    forward_batch: Optional[ForwardBatch] = None,
    should_allreduce_fusion: bool = False,
    use_reduce_scatter: bool = False,
) -&gt; torch.Tensor:
    if not self._enable_deepep_moe:
        DUAL_STREAM_TOKEN_THRESHOLD = 1024
        if (
            self.alt_stream is not None
            and self.num_fused_shared_experts == 0
            and hidden_states.shape[0] &gt; 0
            and hidden_states.shape[0] &lt;= DUAL_STREAM_TOKEN_THRESHOLD
        ):
            return self.forward_normal_dual_stream(
                hidden_states,
                should_allreduce_fusion,
                use_reduce_scatter,
            )
        else:
            return self.forward_normal(
                hidden_states,
                should_allreduce_fusion,
                use_reduce_scatter,
            )
    else:
        return self.forward_deepep(hidden_states, forward_batch)</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2MoE.forward_cpu"><code class="name flex">
<span>def <span class="ident">forward_cpu</span></span>(<span>self, hidden_states: torch.Tensor, should_allreduce_fusion: bool = False) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_cpu(
    self,
    hidden_states: torch.Tensor,
    should_allreduce_fusion: bool = False,
) -&gt; torch.Tensor:
    # router_logits: (num_tokens, n_experts)
    router_logits = self.gate(hidden_states)
    topk_output = self.topk(hidden_states, router_logits)
    fused_experts_out = self.experts(
        hidden_states=hidden_states, topk_output=topk_output
    )

    assert use_intel_amx_backend(
        self.shared_experts.gate_up_proj
    ) == use_intel_amx_backend(self.shared_experts.down_proj)
    # [Note] inplace should be False in fused_experts.
    # If inplace is True in fused_experts (self.experts), hidden_states will be changed after fused_experts
    # While hidden_states is still needed in shared_expert.
    final_hidden_states = torch.ops.sgl_kernel.shared_expert_cpu(
        hidden_states,
        self.shared_experts.gate_up_proj.weight,
        self.shared_experts.down_proj.weight,
        fused_experts_out,
        self.routed_scaling_factor,
        True,  # inplace
        self.shared_experts_is_int8,  # use_int8_w8a8
        self.shared_experts_is_fp8,  # use_fp8_w8a16
        (
            self.shared_experts.gate_up_proj.weight_scale
            if self.shared_experts_is_int8
            else (
                self.shared_experts.gate_up_proj.weight_scale_inv
                if self.shared_experts_is_fp8
                else None
            )
        ),  # w1_scale
        (
            self.shared_experts.down_proj.weight_scale
            if self.shared_experts_is_int8
            else (
                self.shared_experts.down_proj.weight_scale_inv
                if self.shared_experts_is_fp8
                else None
            )
        ),  # w2_scale
        (
            self.shared_experts_weight_block_size
            if self.shared_experts_is_fp8
            else None
        ),  # block_size
        None,  # a1_scale
        None,  # a2_scale
        True,  # is_vnni
    )
    if self.tp_size &gt; 1 and not should_allreduce_fusion:
        final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)
    return final_hidden_states</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2MoE.forward_deepep"><code class="name flex">
<span>def <span class="ident">forward_deepep</span></span>(<span>self,<br>hidden_states: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_deepep(
    self, hidden_states: torch.Tensor, forward_batch: ForwardBatch
) -&gt; torch.Tensor:
    shared_output = None
    if hidden_states.shape[0] &gt; 0:
        # router_logits: (num_tokens, n_experts)
        router_logits = self.gate(hidden_states)
        shared_output = self._forward_shared_experts(hidden_states)
        topk_weights, topk_idx, _ = self.topk(
            hidden_states,
            router_logits,
            num_token_non_padded=forward_batch.num_token_non_padded,
            expert_location_dispatch_info=ExpertLocationDispatchInfo.init_new(
                layer_id=self.layer_id,
            ),
        )
    else:
        topk_weights, topk_idx, _ = self.topk.empty_topk_output(
            hidden_states.device
        )

    final_hidden_states = self.experts(
        hidden_states=hidden_states,
        topk_idx=topk_idx,
        topk_weights=topk_weights,
        forward_batch=forward_batch,
    )

    if shared_output is not None:
        x = shared_output
        x.add_(final_hidden_states, alpha=self.routed_scaling_factor)
        final_hidden_states = x
    else:
        final_hidden_states *= self.routed_scaling_factor

    return final_hidden_states</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2MoE.forward_normal"><code class="name flex">
<span>def <span class="ident">forward_normal</span></span>(<span>self,<br>hidden_states: torch.Tensor,<br>should_allreduce_fusion: bool = False,<br>use_reduce_scatter: bool = False) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_normal(
    self,
    hidden_states: torch.Tensor,
    should_allreduce_fusion: bool = False,
    use_reduce_scatter: bool = False,
) -&gt; torch.Tensor:
    if hasattr(self, &#34;shared_experts&#34;) and use_intel_amx_backend(
        self.shared_experts.gate_up_proj
    ):
        return self.forward_cpu(hidden_states, should_allreduce_fusion)

    if hidden_states.shape[0] &gt; 0:
        shared_output = self._forward_shared_experts(hidden_states)
        # router_logits: (num_tokens, n_experts)
        router_logits = self.gate(hidden_states)
        topk_output = self.topk(hidden_states, router_logits)
    else:
        shared_output = None
        topk_output = self.topk.empty_topk_output(hidden_states.device)

    final_hidden_states = self.experts(hidden_states, topk_output)
    if not _is_cuda and not _use_aiter:
        # fused in biased_grouped_topk so we can skip here
        final_hidden_states *= self.routed_scaling_factor
    if shared_output is not None:
        with use_symmetric_memory(parallel_state.get_tp_group()) as sm:
            final_hidden_states_out = torch.empty_like(final_hidden_states)
        torch.add(final_hidden_states, shared_output, out=final_hidden_states_out)
        final_hidden_states = final_hidden_states_out
        sm.tag(final_hidden_states)
    if (
        self.tp_size &gt; 1
        and not should_allreduce_fusion
        and not use_reduce_scatter
        and not should_use_flashinfer_cutlass_moe_fp4_allgather()
    ):
        final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)
    return final_hidden_states</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2MoE.forward_normal_dual_stream"><code class="name flex">
<span>def <span class="ident">forward_normal_dual_stream</span></span>(<span>self,<br>hidden_states: torch.Tensor,<br>should_allreduce_fusion: bool = False,<br>use_reduce_scatter: bool = False) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_normal_dual_stream(
    self,
    hidden_states: torch.Tensor,
    should_allreduce_fusion: bool = False,
    use_reduce_scatter: bool = False,
) -&gt; torch.Tensor:

    current_stream = torch.cuda.current_stream()
    self.alt_stream.wait_stream(current_stream)
    shared_output = self._forward_shared_experts(hidden_states)

    with torch.cuda.stream(self.alt_stream):
        # router_logits: (num_tokens, n_experts)
        router_logits = self.gate(hidden_states)
        topk_output = self.topk(hidden_states, router_logits)
        final_hidden_states = self.experts(hidden_states, topk_output)
        if not _is_cuda:
            final_hidden_states *= self.routed_scaling_factor

    current_stream.wait_stream(self.alt_stream)
    with use_symmetric_memory(parallel_state.get_tp_group()) as sm:
        final_hidden_states_out = torch.empty_like(final_hidden_states)

    torch.add(final_hidden_states, shared_output, out=final_hidden_states_out)
    final_hidden_states = final_hidden_states_out
    sm.tag(final_hidden_states)
    if (
        self.tp_size &gt; 1
        and not should_allreduce_fusion
        and not use_reduce_scatter
        and not should_use_flashinfer_cutlass_moe_fp4_allgather()
    ):
        final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)
    return final_hidden_states</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2MoE.get_moe_weights"><code class="name flex">
<span>def <span class="ident">get_moe_weights</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_moe_weights(self):
    return [
        x.data
        for name, x in self.experts.named_parameters()
        if name not in [&#34;correction_bias&#34;]
    ]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_combine_a"><code class="name flex">
<span>def <span class="ident">op_combine_a</span></span>(<span>self, state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def op_combine_a(self, state):
    if self.ep_size &gt; 1:
        self.experts.deepep_dispatcher.combine_a(
            hidden_states=state.pop(&#34;hidden_states_experts_output&#34;),
            topk_idx=state.dispatch_output.topk_idx,
            topk_weights=state.dispatch_output.topk_weights,
            forward_batch=state.forward_batch,
            tbo_subbatch_index=state.get(&#34;tbo_subbatch_index&#34;),
        )
        state.pop(&#34;dispatch_output&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_combine_b"><code class="name flex">
<span>def <span class="ident">op_combine_b</span></span>(<span>self, state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def op_combine_b(self, state):
    if self.ep_size &gt; 1:
        state.hidden_states_after_combine = (
            self.experts.deepep_dispatcher.combine_b(
                tbo_subbatch_index=state.get(&#34;tbo_subbatch_index&#34;),
            )
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_dispatch_a"><code class="name flex">
<span>def <span class="ident">op_dispatch_a</span></span>(<span>self, state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def op_dispatch_a(self, state):
    if self.ep_size &gt; 1:
        self.experts.deepep_dispatcher.dispatch_a(
            hidden_states=state.hidden_states_mlp_input,
            topk_idx=state.pop(&#34;topk_idx_local&#34;),
            topk_weights=state.pop(&#34;topk_weights_local&#34;),
            forward_batch=state.forward_batch,
            tbo_subbatch_index=state.get(&#34;tbo_subbatch_index&#34;),
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_dispatch_b"><code class="name flex">
<span>def <span class="ident">op_dispatch_b</span></span>(<span>self, state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def op_dispatch_b(self, state):
    if self.ep_size &gt; 1:
        with get_global_expert_distribution_recorder().with_current_layer(
            self.layer_id
        ):
            state.dispatch_output = self.experts.deepep_dispatcher.dispatch_b(
                tbo_subbatch_index=state.get(&#34;tbo_subbatch_index&#34;),
            )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_experts"><code class="name flex">
<span>def <span class="ident">op_experts</span></span>(<span>self, state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def op_experts(self, state):
    state.hidden_states_experts_output = self.experts.moe_impl(
        dispatch_output=state.dispatch_output,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_gate"><code class="name flex">
<span>def <span class="ident">op_gate</span></span>(<span>self, state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def op_gate(self, state):
    if is_non_idle_and_non_empty(
        state.forward_batch.forward_mode, state.hidden_states_mlp_input
    ):
        # router_logits: (num_tokens, n_experts)
        state.router_logits = self.gate(state.hidden_states_mlp_input)
    else:
        state.router_logits = None</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_output"><code class="name flex">
<span>def <span class="ident">op_output</span></span>(<span>self, state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def op_output(self, state):
    final_hidden_states = state.pop(&#34;hidden_states_after_combine&#34;)

    if (shared_output := state.pop(&#34;shared_output&#34;)) is not None:
        x = shared_output
        x.add_(final_hidden_states, alpha=self.routed_scaling_factor)
        final_hidden_states = x
    else:
        final_hidden_states *= self.routed_scaling_factor

    state.hidden_states_mlp_output = final_hidden_states</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_select_experts"><code class="name flex">
<span>def <span class="ident">op_select_experts</span></span>(<span>self, state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def op_select_experts(self, state):
    router_logits = state.pop(&#34;router_logits&#34;)
    hidden_states = state.hidden_states_mlp_input

    if router_logits is not None:
        with get_global_expert_distribution_recorder().with_current_layer(
            self.layer_id
        ):
            state.topk_weights_local, state.topk_idx_local, _ = self.topk(
                hidden_states=hidden_states,
                router_logits=router_logits,
                num_token_non_padded=state.forward_batch.num_token_non_padded,
                expert_location_dispatch_info=ExpertLocationDispatchInfo.init_new(
                    layer_id=self.layer_id,
                ),
            )
    else:
        state.topk_idx_local = torch.full(
            (0, self.top_k), -1, dtype=torch.int, device=hidden_states.device
        )
        state.topk_weights_local = torch.empty(
            (0, self.top_k), dtype=torch.float32, device=hidden_states.device
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_shared_experts"><code class="name flex">
<span>def <span class="ident">op_shared_experts</span></span>(<span>self, state)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def op_shared_experts(self, state):
    hidden_states_mlp_input = state.pop(&#34;hidden_states_mlp_input&#34;)
    if (self.num_fused_shared_experts == 0) and is_non_idle_and_non_empty(
        state.forward_batch.forward_mode, hidden_states_mlp_input
    ):
        state.shared_output = self.shared_experts(hidden_states_mlp_input)
    else:
        state.shared_output = None</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2Model"><code class="flex name class">
<span>class <span class="ident">DeepseekV2Model</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DeepseekV2Model(nn.Module):
    fall_back_to_pt_during_load = False

    def __init__(
        self,
        config: PretrainedConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; None:
        super().__init__()
        self.padding_id = config.pad_token_id
        self.vocab_size = config.vocab_size
        self.first_k_dense_replace = config.first_k_dense_replace
        self.pp_group = get_pp_group()

        if self.pp_group.is_first_rank:
            self.embed_tokens = VocabParallelEmbedding(
                config.vocab_size,
                config.hidden_size,
                enable_tp=not is_dp_attention_enabled(),
            )
        else:
            self.embed_tokens = PPMissingLayer()

        self.alt_stream = torch.cuda.Stream() if _is_cuda else None
        self.layers, self.start_layer, self.end_layer = make_layers(
            config.num_hidden_layers,
            lambda idx, prefix: DeepseekV2DecoderLayer(
                config=config,
                layer_id=idx,
                quant_config=quant_config,
                prefix=prefix,
                alt_stream=self.alt_stream,
            ),
            pp_rank=self.pp_group.rank_in_group,
            pp_size=self.pp_group.world_size,
            prefix=add_prefix(&#34;layers&#34;, prefix),
            offloader_kwargs=dict(
                submodule_accessor=lambda layer: (
                    layer.mlp.experts
                    if isinstance(layer.mlp, DeepseekV2MoE)
                    else layer.mlp
                ),
                whitelist_param_names_creator=lambda module: (
                    [
                        &#34;w13_weight&#34;,
                        &#34;w2_weight&#34;,
                        &#34;w13_blockscale_swizzled&#34;,
                        &#34;w2_blockscale_swizzled&#34;,
                    ]
                    if isinstance(module, FusedMoE)
                    else []
                ),
            ),
        )
        if self.pp_group.is_last_rank:
            self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        else:
            self.norm = PPMissingLayer(return_tuple=True)

    def get_input_embeddings(self) -&gt; torch.Tensor:
        return self.embed_tokens

    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        forward_batch: ForwardBatch,
        input_embeds: torch.Tensor = None,
        pp_proxy_tensors: Optional[PPProxyTensors] = None,
    ) -&gt; Union[torch.Tensor, PPProxyTensors]:
        total_num_layers = self.end_layer - self.start_layer
        device = input_embeds.device if input_embeds is not None else input_ids.device
        zero_allocator = BumpAllocator(
            buffer_size=total_num_layers * 2 * (2 if forward_batch.can_run_tbo else 1),
            dtype=torch.float32,
            device=device,
        )

        if self.pp_group.is_first_rank:
            if input_embeds is None:
                hidden_states = self.embed_tokens(input_ids)
            else:
                hidden_states = input_embeds
            residual = None
        else:
            assert pp_proxy_tensors is not None
            hidden_states = pp_proxy_tensors[&#34;hidden_states&#34;]
            residual = pp_proxy_tensors[&#34;residual&#34;]

        normal_start_layer = self.start_layer
        normal_end_layer = self.end_layer
        if forward_batch.can_run_tbo:
            if (
                self.first_k_dense_replace &gt; normal_start_layer
                and self.first_k_dense_replace &lt; normal_end_layer
            ):
                normal_end_layer = self.first_k_dense_replace
            elif self.first_k_dense_replace &lt; normal_start_layer:
                normal_end_layer = normal_start_layer = 0

        for i in range(normal_start_layer, normal_end_layer):
            with get_global_expert_distribution_recorder().with_current_layer(i):
                layer = self.layers[i]
                hidden_states, residual = layer(
                    positions, hidden_states, forward_batch, residual, zero_allocator
                )

        if normal_end_layer != self.end_layer:
            hidden_states, residual = model_forward_maybe_tbo(
                layers=self.layers[normal_end_layer : self.end_layer],
                enable_tbo=True,
                positions=positions,
                forward_batch=forward_batch,
                hidden_states=hidden_states,
                residual=residual,
                input_data_scatter_mode=self.layers[
                    normal_end_layer - 1
                ].layer_scatter_modes.layer_output_mode,
                zero_allocator=zero_allocator,
            )

        if not self.pp_group.is_last_rank:
            return PPProxyTensors(
                {
                    &#34;hidden_states&#34;: hidden_states,
                    &#34;residual&#34;: residual,
                }
            )
        else:
            if not forward_batch.forward_mode.is_idle():
                if residual is None:
                    hidden_states = self.norm(hidden_states)
                else:
                    hidden_states, _ = self.norm(hidden_states, residual)
        return hidden_states</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.models.glm4_moe.Glm4MoeModel" href="glm4_moe.html#sglang.srt.models.glm4_moe.Glm4MoeModel">Glm4MoeModel</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2Model.fall_back_to_pt_during_load"><code class="name">var <span class="ident">fall_back_to_pt_during_load</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2Model.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>input_ids: torch.Tensor,<br>positions: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>input_embeds: torch.Tensor = None,<br>pp_proxy_tensors: <a title="sglang.srt.model_executor.forward_batch_info.PPProxyTensors" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.PPProxyTensors">PPProxyTensors</a> | None = None) ‑> torch.Tensor | <a title="sglang.srt.model_executor.forward_batch_info.PPProxyTensors" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.PPProxyTensors">PPProxyTensors</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    input_ids: torch.Tensor,
    positions: torch.Tensor,
    forward_batch: ForwardBatch,
    input_embeds: torch.Tensor = None,
    pp_proxy_tensors: Optional[PPProxyTensors] = None,
) -&gt; Union[torch.Tensor, PPProxyTensors]:
    total_num_layers = self.end_layer - self.start_layer
    device = input_embeds.device if input_embeds is not None else input_ids.device
    zero_allocator = BumpAllocator(
        buffer_size=total_num_layers * 2 * (2 if forward_batch.can_run_tbo else 1),
        dtype=torch.float32,
        device=device,
    )

    if self.pp_group.is_first_rank:
        if input_embeds is None:
            hidden_states = self.embed_tokens(input_ids)
        else:
            hidden_states = input_embeds
        residual = None
    else:
        assert pp_proxy_tensors is not None
        hidden_states = pp_proxy_tensors[&#34;hidden_states&#34;]
        residual = pp_proxy_tensors[&#34;residual&#34;]

    normal_start_layer = self.start_layer
    normal_end_layer = self.end_layer
    if forward_batch.can_run_tbo:
        if (
            self.first_k_dense_replace &gt; normal_start_layer
            and self.first_k_dense_replace &lt; normal_end_layer
        ):
            normal_end_layer = self.first_k_dense_replace
        elif self.first_k_dense_replace &lt; normal_start_layer:
            normal_end_layer = normal_start_layer = 0

    for i in range(normal_start_layer, normal_end_layer):
        with get_global_expert_distribution_recorder().with_current_layer(i):
            layer = self.layers[i]
            hidden_states, residual = layer(
                positions, hidden_states, forward_batch, residual, zero_allocator
            )

    if normal_end_layer != self.end_layer:
        hidden_states, residual = model_forward_maybe_tbo(
            layers=self.layers[normal_end_layer : self.end_layer],
            enable_tbo=True,
            positions=positions,
            forward_batch=forward_batch,
            hidden_states=hidden_states,
            residual=residual,
            input_data_scatter_mode=self.layers[
                normal_end_layer - 1
            ].layer_scatter_modes.layer_output_mode,
            zero_allocator=zero_allocator,
        )

    if not self.pp_group.is_last_rank:
        return PPProxyTensors(
            {
                &#34;hidden_states&#34;: hidden_states,
                &#34;residual&#34;: residual,
            }
        )
    else:
        if not forward_batch.forward_mode.is_idle():
            if residual is None:
                hidden_states = self.norm(hidden_states)
            else:
                hidden_states, _ = self.norm(hidden_states, residual)
    return hidden_states</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV2Model.get_input_embeddings"><code class="name flex">
<span>def <span class="ident">get_input_embeddings</span></span>(<span>self) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_input_embeddings(self) -&gt; torch.Tensor:
    return self.embed_tokens</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_v2.DeepseekV3ForCausalLM"><code class="flex name class">
<span>class <span class="ident">DeepseekV3ForCausalLM</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DeepseekV3ForCausalLM(DeepseekV2ForCausalLM):
    pass</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM" href="#sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM">DeepseekV2ForCausalLM</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.models.deepseek_nextn.DeepseekV3ForCausalLMNextN" href="deepseek_nextn.html#sglang.srt.models.deepseek_nextn.DeepseekV3ForCausalLMNextN">DeepseekV3ForCausalLMNextN</a></li>
</ul>
</dd>
<dt id="sglang.srt.models.deepseek_v2.MoEGate"><code class="flex name class">
<span>class <span class="ident">MoEGate</span></span>
<span>(</span><span>config, prefix: str = '', is_nextn: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MoEGate(nn.Module):
    def __init__(
        self,
        config,
        prefix: str = &#34;&#34;,
        is_nextn: bool = False,
    ):
        super().__init__()
        self.is_nextn = is_nextn
        self.weight = nn.Parameter(
            torch.empty((config.n_routed_experts, config.hidden_size))
        )
        if config.topk_method == &#34;noaux_tc&#34;:
            self.e_score_correction_bias = nn.Parameter(
                torch.empty((config.n_routed_experts), dtype=torch.float32)
            )
        else:
            self.e_score_correction_bias = None
        if _is_cpu and _is_cpu_amx_available:
            self.quant_method = PackWeightMethod(weight_names=[&#34;weight&#34;])

    def forward(self, hidden_states):
        if use_intel_amx_backend(self):
            return torch.ops.sgl_kernel.weight_packed_linear(
                hidden_states,
                self.weight,
                None,  # bias
                True,  # is_vnni
            )

        # NOTE: For some unknown reason, router_gemm seems degrade accept length.
        if (
            _is_cuda
            and hidden_states.shape[0] &lt;= 16
            and hidden_states.shape[1] == 7168
            and self.weight.shape[0] == 256
            and _device_sm &gt;= 90
        ):
            # router gemm output float32
            logits = dsv3_router_gemm(hidden_states, self.weight)
        else:
            logits = F.linear(hidden_states, self.weight, None)

        return logits</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_v2.MoEGate.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, hidden_states) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, hidden_states):
    if use_intel_amx_backend(self):
        return torch.ops.sgl_kernel.weight_packed_linear(
            hidden_states,
            self.weight,
            None,  # bias
            True,  # is_vnni
        )

    # NOTE: For some unknown reason, router_gemm seems degrade accept length.
    if (
        _is_cuda
        and hidden_states.shape[0] &lt;= 16
        and hidden_states.shape[1] == 7168
        and self.weight.shape[0] == 256
        and _device_sm &gt;= 90
    ):
        # router gemm output float32
        logits = dsv3_router_gemm(hidden_states, self.weight)
    else:
        logits = F.linear(hidden_states, self.weight, None)

    return logits</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.models" href="index.html">sglang.srt.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_v2.yarn_get_mscale" href="#sglang.srt.models.deepseek_v2.yarn_get_mscale">yarn_get_mscale</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.models.deepseek_v2.AttnForwardMethod" href="#sglang.srt.models.deepseek_v2.AttnForwardMethod">AttnForwardMethod</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_v2.AttnForwardMethod.MHA" href="#sglang.srt.models.deepseek_v2.AttnForwardMethod.MHA">MHA</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.AttnForwardMethod.MHA_CHUNKED_KV" href="#sglang.srt.models.deepseek_v2.AttnForwardMethod.MHA_CHUNKED_KV">MHA_CHUNKED_KV</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.AttnForwardMethod.MLA" href="#sglang.srt.models.deepseek_v2.AttnForwardMethod.MLA">MLA</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.AttnForwardMethod.MLA_FUSED_ROPE" href="#sglang.srt.models.deepseek_v2.AttnForwardMethod.MLA_FUSED_ROPE">MLA_FUSED_ROPE</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.AttnForwardMethod.MLA_FUSED_ROPE_CPU" href="#sglang.srt.models.deepseek_v2.AttnForwardMethod.MLA_FUSED_ROPE_CPU">MLA_FUSED_ROPE_CPU</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA" href="#sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA">DeepseekV2AttentionMLA</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.dispatch_attn_forward_method" href="#sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.dispatch_attn_forward_method">dispatch_attn_forward_method</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward" href="#sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_absorb_core" href="#sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_absorb_core">forward_absorb_core</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_core" href="#sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_core">forward_absorb_fused_mla_rope_core</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_cpu_core" href="#sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_cpu_core">forward_absorb_fused_mla_rope_cpu_core</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_cpu_prepare" href="#sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_cpu_prepare">forward_absorb_fused_mla_rope_cpu_prepare</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_prepare" href="#sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_prepare">forward_absorb_fused_mla_rope_prepare</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_absorb_prepare" href="#sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_absorb_prepare">forward_absorb_prepare</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_core" href="#sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_core">forward_core</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_normal_chunked_kv_core" href="#sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_normal_chunked_kv_core">forward_normal_chunked_kv_core</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_normal_chunked_kv_prepare" href="#sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_normal_chunked_kv_prepare">forward_normal_chunked_kv_prepare</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_normal_core" href="#sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_normal_core">forward_normal_core</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_normal_prepare" href="#sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_normal_prepare">forward_normal_prepare</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_prepare" href="#sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.forward_prepare">forward_prepare</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.op_core" href="#sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.op_core">op_core</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.op_prepare" href="#sglang.srt.models.deepseek_v2.DeepseekV2AttentionMLA.op_prepare">op_prepare</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2DecoderLayer" href="#sglang.srt.models.deepseek_v2.DeepseekV2DecoderLayer">DeepseekV2DecoderLayer</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2DecoderLayer.forward" href="#sglang.srt.models.deepseek_v2.DeepseekV2DecoderLayer.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2DecoderLayer.op_comm_postprocess_layer" href="#sglang.srt.models.deepseek_v2.DeepseekV2DecoderLayer.op_comm_postprocess_layer">op_comm_postprocess_layer</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2DecoderLayer.op_comm_prepare_attn" href="#sglang.srt.models.deepseek_v2.DeepseekV2DecoderLayer.op_comm_prepare_attn">op_comm_prepare_attn</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2DecoderLayer.op_comm_prepare_mlp" href="#sglang.srt.models.deepseek_v2.DeepseekV2DecoderLayer.op_comm_prepare_mlp">op_comm_prepare_mlp</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2DecoderLayer.op_mlp" href="#sglang.srt.models.deepseek_v2.DeepseekV2DecoderLayer.op_mlp">op_mlp</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM" href="#sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM">DeepseekV2ForCausalLM</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.determine_num_fused_shared_experts" href="#sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.determine_num_fused_shared_experts">determine_num_fused_shared_experts</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.end_layer" href="#sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.end_layer">end_layer</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.forward" href="#sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.get_embed_and_head" href="#sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.get_embed_and_head">get_embed_and_head</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.get_input_embeddings" href="#sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.get_input_embeddings">get_input_embeddings</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.get_model_config_for_expert_location" href="#sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.get_model_config_for_expert_location">get_model_config_for_expert_location</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.load_weights" href="#sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.load_weights">load_weights</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.packed_modules_mapping" href="#sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.packed_modules_mapping">packed_modules_mapping</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.post_load_weights" href="#sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.post_load_weights">post_load_weights</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.routed_experts_weights_of_layer" href="#sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.routed_experts_weights_of_layer">routed_experts_weights_of_layer</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.set_embed_and_head" href="#sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.set_embed_and_head">set_embed_and_head</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.start_layer" href="#sglang.srt.models.deepseek_v2.DeepseekV2ForCausalLM.start_layer">start_layer</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2MLP" href="#sglang.srt.models.deepseek_v2.DeepseekV2MLP">DeepseekV2MLP</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2MLP.forward" href="#sglang.srt.models.deepseek_v2.DeepseekV2MLP.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2MoE" href="#sglang.srt.models.deepseek_v2.DeepseekV2MoE">DeepseekV2MoE</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2MoE.forward" href="#sglang.srt.models.deepseek_v2.DeepseekV2MoE.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2MoE.forward_cpu" href="#sglang.srt.models.deepseek_v2.DeepseekV2MoE.forward_cpu">forward_cpu</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2MoE.forward_deepep" href="#sglang.srt.models.deepseek_v2.DeepseekV2MoE.forward_deepep">forward_deepep</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2MoE.forward_normal" href="#sglang.srt.models.deepseek_v2.DeepseekV2MoE.forward_normal">forward_normal</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2MoE.forward_normal_dual_stream" href="#sglang.srt.models.deepseek_v2.DeepseekV2MoE.forward_normal_dual_stream">forward_normal_dual_stream</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2MoE.get_moe_weights" href="#sglang.srt.models.deepseek_v2.DeepseekV2MoE.get_moe_weights">get_moe_weights</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_combine_a" href="#sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_combine_a">op_combine_a</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_combine_b" href="#sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_combine_b">op_combine_b</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_dispatch_a" href="#sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_dispatch_a">op_dispatch_a</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_dispatch_b" href="#sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_dispatch_b">op_dispatch_b</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_experts" href="#sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_experts">op_experts</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_gate" href="#sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_gate">op_gate</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_output" href="#sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_output">op_output</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_select_experts" href="#sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_select_experts">op_select_experts</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_shared_experts" href="#sglang.srt.models.deepseek_v2.DeepseekV2MoE.op_shared_experts">op_shared_experts</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2Model" href="#sglang.srt.models.deepseek_v2.DeepseekV2Model">DeepseekV2Model</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2Model.fall_back_to_pt_during_load" href="#sglang.srt.models.deepseek_v2.DeepseekV2Model.fall_back_to_pt_during_load">fall_back_to_pt_during_load</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2Model.forward" href="#sglang.srt.models.deepseek_v2.DeepseekV2Model.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_v2.DeepseekV2Model.get_input_embeddings" href="#sglang.srt.models.deepseek_v2.DeepseekV2Model.get_input_embeddings">get_input_embeddings</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_v2.DeepseekV3ForCausalLM" href="#sglang.srt.models.deepseek_v2.DeepseekV3ForCausalLM">DeepseekV3ForCausalLM</a></code></h4>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_v2.MoEGate" href="#sglang.srt.models.deepseek_v2.MoEGate">MoEGate</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_v2.MoEGate.forward" href="#sglang.srt.models.deepseek_v2.MoEGate.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
