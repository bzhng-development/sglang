<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.models.gemma3n_audio API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.models.gemma3n_audio</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nAudioAttention"><code class="flex name class">
<span>class <span class="ident">Gemma3nAudioAttention</span></span>
<span>(</span><span>config: transformers.models.gemma3n.configuration_gemma3n.Gemma3nAudioConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nAudioAttention(nn.Module):
    &#34;&#34;&#34;Local dot product self-attention for audio.&#34;&#34;&#34;

    def __init__(
        self,
        config: Gemma3nAudioConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ):
        super().__init__()
        self.config = config

        self.num_heads = self.config.conf_num_attention_heads
        self.hidden_size = self.config.hidden_size
        self.head_dim = self.hidden_size // self.num_heads

        self.chunk_size = self.config.conf_attention_chunk_size
        self.max_future_horizon = self.config.conf_attention_context_right
        self.max_past_horizon = max(0, self.config.conf_attention_context_left - 1)
        self.attention_logits_soft_cap = self.config.conf_attention_logit_cap
        self.context_size = (
            self.chunk_size + self.max_past_horizon + self.max_future_horizon
        )

        self.relative_position_embedding = Gemma3nAudioRelativePositionEmbedding(
            config,
            quant_config,
            prefix=add_prefix(&#34;relative_position_embedding&#34;, prefix),
        )
        self.per_dim_scale = nn.Parameter(torch.zeros((self.head_dim,)))

        self.qkv_proj = QKVParallelLinear(
            self.hidden_size,
            self.head_dim,
            self.num_heads,
            self.num_heads,
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;qkv_proj&#34;, prefix),
        )

        q_scale = self.head_dim**-0.5
        r_softplus_0 = 1.0 / F.softplus(torch.tensor(0.0))
        self.register_buffer(
            &#34;q_scale&#34;, (q_scale * r_softplus_0).clone().detach(), persistent=False
        )

        # Create local causal mask
        lower_causal_mask = torch.tril(
            torch.ones((self.context_size, self.chunk_size), dtype=torch.bool),
            diagonal=0,
        ).T
        upper_causal_mask = torch.tril(
            torch.ones((self.chunk_size, self.context_size), dtype=torch.bool),
            diagonal=self.max_past_horizon + self.max_future_horizon,
        )
        local_causal_valid_mask = torch.ones(
            (self.chunk_size, self.context_size), dtype=torch.bool
        )
        local_causal_valid_mask = (
            local_causal_valid_mask * lower_causal_mask * upper_causal_mask
        )
        self.register_buffer(
            &#34;local_causal_valid_mask&#34;, local_causal_valid_mask, persistent=False
        )

        self.register_buffer(
            &#34;softcap&#34;,
            torch.tensor(self.attention_logits_soft_cap).float(),
            persistent=False,
        )

    def _pad_dim1(
        self, x: torch.Tensor, dim10_val: int, dim11_val: int
    ) -&gt; torch.Tensor:
        padding_tuple = [0] * x.ndim * 2
        dim_idx_from_end = x.ndim - 2
        start_idx_for_dim = 2 * dim_idx_from_end
        padding_tuple[start_idx_for_dim] = dim10_val
        padding_tuple[start_idx_for_dim + 1] = dim11_val
        return F.pad(x, tuple(padding_tuple))

    def _convert_to_block(self, x: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;Turns a sequence to non overlapping blocks.&#34;&#34;&#34;
        shape = x.shape
        b, t = shape[:2]
        num_blocks = (t + self.chunk_size - 1) // self.chunk_size

        if (padding_len := num_blocks * self.chunk_size - t) &gt; 0:
            x = self._pad_dim1(x, 0, padding_len)

        permute_dims = (b, num_blocks, self.chunk_size) + shape[2:]
        x = x.reshape(permute_dims).contiguous()
        return x

    def _extract_block_context(self, x: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;Extracts temporal context for every block.&#34;&#34;&#34;
        pad_left = self.max_past_horizon
        pad_right = self.max_future_horizon + self.chunk_size - 1
        x = self._pad_dim1(x, pad_left, pad_right)

        frame_len = self.context_size
        frame_step = self.chunk_size

        x_unfolded = x.unfold(dimension=1, size=frame_len, step=frame_step)

        if x.ndim &gt; 2 and x_unfolded.ndim &gt; 3:
            x_unfolded = torch.movedim(x_unfolded, source=-1, destination=2)

        return x_unfolded.contiguous()

    def forward(self, x: torch.Tensor, mask: torch.BoolTensor) -&gt; torch.Tensor:
        # Project to Q, K, V
        qkv, _ = self.qkv_proj(x)
        query_states, key_states, value_states = qkv.chunk(chunks=3, dim=-1)

        # Reshape
        query_states = query_states.reshape(
            *x.shape[:-1], self.num_heads, self.head_dim
        ).contiguous()
        key_states = key_states.reshape(
            *x.shape[:-1], self.num_heads, self.head_dim
        ).contiguous()
        value_states = value_states.reshape(
            *x.shape[:-1], self.num_heads, self.head_dim
        ).contiguous()

        # Apply per-dim scale
        per_dim_scale_sp = F.softplus(self.per_dim_scale)
        broadcast_shape = (1, 1, 1, self.head_dim)
        per_dim_scale_sp_broadcast = per_dim_scale_sp.view(broadcast_shape)
        query_states = query_states * self.q_scale * per_dim_scale_sp_broadcast

        batch_size, q_time = query_states.shape[:2]

        # Convert to blocks
        query_blocks = self._convert_to_block(query_states)
        key_blocks = self._extract_block_context(key_states)
        value_blocks = self._extract_block_context(value_states)
        num_query_blocks = query_blocks.shape[1]

        # Create mask for valid positions
        original_valid_mask = ~mask
        extracted_valid_mask_blocks = self._extract_block_context(original_valid_mask)

        if (
            extracted_valid_mask_blocks.ndim == 4
            and extracted_valid_mask_blocks.shape[0] == batch_size
            and extracted_valid_mask_blocks.shape[1] == num_query_blocks
            and extracted_valid_mask_blocks.shape[2]
            * extracted_valid_mask_blocks.shape[3]
            == self.context_size
        ):
            extracted_valid_mask_blocks = extracted_valid_mask_blocks.reshape(
                batch_size, num_query_blocks, self.context_size
            )

        condition_from_input_validity = extracted_valid_mask_blocks.unsqueeze(
            1
        ).unsqueeze(-2)
        condition_from_causality = (
            self.local_causal_valid_mask.unsqueeze(0).unsqueeze(0).unsqueeze(0)
        )

        final_condition_for_where = torch.logical_and(
            condition_from_input_validity,
            condition_from_causality.to(condition_from_input_validity.device),
        )

        # Compute attention scores
        logits = self.relative_position_embedding(query_blocks, key_blocks)

        # Apply attention logit softcap
        softcap_val = self.softcap.to(logits.device)
        logits = logits / softcap_val
        logits = torch.tanh(logits)
        logits = logits * softcap_val

        # Apply the combined mask.
        # final_condition_for_where will broadcast with logits [B,N,U,W,C]
        logits = torch.where(
            final_condition_for_where, logits, torch.finfo(logits.dtype).min
        )

        probabilities = F.softmax(logits, dim=-1, dtype=torch.float32).to(
            dtype=value_blocks.dtype
        )

        # context_vectors is adapted from jax.numpy.einsum(&#34;BNuwc,BucNH-&gt;BuwNH&#34;, ...)
        b_dim, n_dim, u_dim, w_dim, c_dim = probabilities.shape
        h_dim = value_blocks.shape[-1]
        prob_bun = probabilities.permute(0, 2, 1, 3, 4).reshape(-1, w_dim, c_dim)
        v_bun = value_blocks.permute(0, 1, 3, 2, 4).reshape(-1, c_dim, h_dim)
        result_bmm = torch.bmm(prob_bun, v_bun)
        context_vectors = result_bmm.reshape(b_dim, u_dim, n_dim, w_dim, h_dim).permute(
            0, 1, 3, 2, 4
        )
        context_vectors = context_vectors.reshape(
            (
                batch_size,
                num_query_blocks * self.chunk_size,
                self.num_heads,
                self.head_dim,
            )
        )
        context_vectors = context_vectors[:, :q_time]

        return context_vectors</code></pre>
</details>
<div class="desc"><p>Local dot product self-attention for audio.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nAudioAttention.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: torch.Tensor, mask: torch.BoolTensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x: torch.Tensor, mask: torch.BoolTensor) -&gt; torch.Tensor:
    # Project to Q, K, V
    qkv, _ = self.qkv_proj(x)
    query_states, key_states, value_states = qkv.chunk(chunks=3, dim=-1)

    # Reshape
    query_states = query_states.reshape(
        *x.shape[:-1], self.num_heads, self.head_dim
    ).contiguous()
    key_states = key_states.reshape(
        *x.shape[:-1], self.num_heads, self.head_dim
    ).contiguous()
    value_states = value_states.reshape(
        *x.shape[:-1], self.num_heads, self.head_dim
    ).contiguous()

    # Apply per-dim scale
    per_dim_scale_sp = F.softplus(self.per_dim_scale)
    broadcast_shape = (1, 1, 1, self.head_dim)
    per_dim_scale_sp_broadcast = per_dim_scale_sp.view(broadcast_shape)
    query_states = query_states * self.q_scale * per_dim_scale_sp_broadcast

    batch_size, q_time = query_states.shape[:2]

    # Convert to blocks
    query_blocks = self._convert_to_block(query_states)
    key_blocks = self._extract_block_context(key_states)
    value_blocks = self._extract_block_context(value_states)
    num_query_blocks = query_blocks.shape[1]

    # Create mask for valid positions
    original_valid_mask = ~mask
    extracted_valid_mask_blocks = self._extract_block_context(original_valid_mask)

    if (
        extracted_valid_mask_blocks.ndim == 4
        and extracted_valid_mask_blocks.shape[0] == batch_size
        and extracted_valid_mask_blocks.shape[1] == num_query_blocks
        and extracted_valid_mask_blocks.shape[2]
        * extracted_valid_mask_blocks.shape[3]
        == self.context_size
    ):
        extracted_valid_mask_blocks = extracted_valid_mask_blocks.reshape(
            batch_size, num_query_blocks, self.context_size
        )

    condition_from_input_validity = extracted_valid_mask_blocks.unsqueeze(
        1
    ).unsqueeze(-2)
    condition_from_causality = (
        self.local_causal_valid_mask.unsqueeze(0).unsqueeze(0).unsqueeze(0)
    )

    final_condition_for_where = torch.logical_and(
        condition_from_input_validity,
        condition_from_causality.to(condition_from_input_validity.device),
    )

    # Compute attention scores
    logits = self.relative_position_embedding(query_blocks, key_blocks)

    # Apply attention logit softcap
    softcap_val = self.softcap.to(logits.device)
    logits = logits / softcap_val
    logits = torch.tanh(logits)
    logits = logits * softcap_val

    # Apply the combined mask.
    # final_condition_for_where will broadcast with logits [B,N,U,W,C]
    logits = torch.where(
        final_condition_for_where, logits, torch.finfo(logits.dtype).min
    )

    probabilities = F.softmax(logits, dim=-1, dtype=torch.float32).to(
        dtype=value_blocks.dtype
    )

    # context_vectors is adapted from jax.numpy.einsum(&#34;BNuwc,BucNH-&gt;BuwNH&#34;, ...)
    b_dim, n_dim, u_dim, w_dim, c_dim = probabilities.shape
    h_dim = value_blocks.shape[-1]
    prob_bun = probabilities.permute(0, 2, 1, 3, 4).reshape(-1, w_dim, c_dim)
    v_bun = value_blocks.permute(0, 1, 3, 2, 4).reshape(-1, c_dim, h_dim)
    result_bmm = torch.bmm(prob_bun, v_bun)
    context_vectors = result_bmm.reshape(b_dim, u_dim, n_dim, w_dim, h_dim).permute(
        0, 1, 3, 2, 4
    )
    context_vectors = context_vectors.reshape(
        (
            batch_size,
            num_query_blocks * self.chunk_size,
            self.num_heads,
            self.head_dim,
        )
    )
    context_vectors = context_vectors[:, :q_time]

    return context_vectors</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerAttention"><code class="flex name class">
<span>class <span class="ident">Gemma3nAudioConformerAttention</span></span>
<span>(</span><span>config: transformers.models.gemma3n.configuration_gemma3n.Gemma3nAudioConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nAudioConformerAttention(nn.Module):
    def __init__(
        self,
        config: Gemma3nAudioConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ):
        super().__init__()
        self.config = config

        head_dim = self.config.hidden_size // self.config.conf_num_attention_heads
        self.post_in_shape = (self.config.conf_num_attention_heads, head_dim)
        self.post_in_features = self.config.hidden_size

        self.register_buffer(
            &#34;gradient_clipping&#34;,
            torch.tensor(self.config.gradient_clipping),
            persistent=False,
        )

        self.pre_attn_norm = Gemma3nRMSNorm(self.config.hidden_size)
        self.attn = Gemma3nAudioAttention(
            config, quant_config, prefix=add_prefix(&#34;attn&#34;, prefix)
        )
        self.post = RowParallelLinear(
            self.post_in_features,
            self.config.hidden_size,
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;post&#34;, prefix),
        )
        self.post_norm = Gemma3nRMSNorm(self.config.hidden_size)

    def forward(
        self, audio_encodings: torch.Tensor, audio_mel_mask: torch.BoolTensor
    ) -&gt; torch.Tensor:
        audio_encodings_input_to_attn = audio_encodings
        audio_encodings = torch.clamp(
            audio_encodings, -self.gradient_clipping, self.gradient_clipping
        )
        audio_encodings_norm = self.pre_attn_norm(audio_encodings)
        audio_encodings_attn_out = self.attn(audio_encodings_norm, audio_mel_mask)

        b, t, num_heads, head_dim = audio_encodings_attn_out.shape
        audio_encodings_reshaped = audio_encodings_attn_out.reshape(
            b, t, num_heads * head_dim
        )

        audio_encodings, _ = self.post(audio_encodings_reshaped)
        audio_encodings = torch.clamp(
            audio_encodings, -self.gradient_clipping, self.gradient_clipping
        )
        return audio_encodings_input_to_attn + self.post_norm(audio_encodings)</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerAttention.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, audio_encodings: torch.Tensor, audio_mel_mask: torch.BoolTensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, audio_encodings: torch.Tensor, audio_mel_mask: torch.BoolTensor
) -&gt; torch.Tensor:
    audio_encodings_input_to_attn = audio_encodings
    audio_encodings = torch.clamp(
        audio_encodings, -self.gradient_clipping, self.gradient_clipping
    )
    audio_encodings_norm = self.pre_attn_norm(audio_encodings)
    audio_encodings_attn_out = self.attn(audio_encodings_norm, audio_mel_mask)

    b, t, num_heads, head_dim = audio_encodings_attn_out.shape
    audio_encodings_reshaped = audio_encodings_attn_out.reshape(
        b, t, num_heads * head_dim
    )

    audio_encodings, _ = self.post(audio_encodings_reshaped)
    audio_encodings = torch.clamp(
        audio_encodings, -self.gradient_clipping, self.gradient_clipping
    )
    return audio_encodings_input_to_attn + self.post_norm(audio_encodings)</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerBlock"><code class="flex name class">
<span>class <span class="ident">Gemma3nAudioConformerBlock</span></span>
<span>(</span><span>config: transformers.models.gemma3n.configuration_gemma3n.Gemma3nAudioConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nAudioConformerBlock(nn.Module):
    def __init__(
        self,
        config: Gemma3nAudioConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ):
        super().__init__()
        self.config = config

        self.ffw_layer_start = Gemma3nAudioConformerFeedForward(
            config, quant_config, prefix=add_prefix(&#34;ffw_layer_start&#34;, prefix)
        )
        self.attention = Gemma3nAudioConformerAttention(
            config, quant_config, prefix=add_prefix(&#34;attention&#34;, prefix)
        )
        self.lconv1d = Gemma3nAudioConformerLightConv1d(
            config, quant_config, prefix=add_prefix(&#34;lconv1d&#34;, prefix)
        )
        self.ffw_layer_end = Gemma3nAudioConformerFeedForward(
            config, quant_config, prefix=add_prefix(&#34;ffw_layer_end&#34;, prefix)
        )
        self.register_buffer(
            &#34;gradient_clipping&#34;,
            torch.tensor(self.config.gradient_clipping),
            persistent=False,
        )
        self.norm = Gemma3nRMSNorm(self.config.hidden_size)

    def forward(
        self, audio_encodings: torch.Tensor, audio_mel_mask: torch.BoolTensor
    ) -&gt; torch.Tensor:
        audio_encodings = self.ffw_layer_start(audio_encodings)
        audio_encodings = self.attention(audio_encodings, audio_mel_mask)
        validity_mask_for_lconv = ~audio_mel_mask  # True for valid
        audio_encodings_for_lconv_input = (
            audio_encodings
            * validity_mask_for_lconv.unsqueeze(-1).to(audio_encodings.dtype)
        )
        audio_encodings = self.lconv1d(audio_encodings_for_lconv_input)

        audio_encodings = self.ffw_layer_end(audio_encodings)
        audio_encodings = torch.clamp(
            audio_encodings, -self.gradient_clipping, self.gradient_clipping
        )
        output = self.norm(audio_encodings)
        return output</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerBlock.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, audio_encodings: torch.Tensor, audio_mel_mask: torch.BoolTensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, audio_encodings: torch.Tensor, audio_mel_mask: torch.BoolTensor
) -&gt; torch.Tensor:
    audio_encodings = self.ffw_layer_start(audio_encodings)
    audio_encodings = self.attention(audio_encodings, audio_mel_mask)
    validity_mask_for_lconv = ~audio_mel_mask  # True for valid
    audio_encodings_for_lconv_input = (
        audio_encodings
        * validity_mask_for_lconv.unsqueeze(-1).to(audio_encodings.dtype)
    )
    audio_encodings = self.lconv1d(audio_encodings_for_lconv_input)

    audio_encodings = self.ffw_layer_end(audio_encodings)
    audio_encodings = torch.clamp(
        audio_encodings, -self.gradient_clipping, self.gradient_clipping
    )
    output = self.norm(audio_encodings)
    return output</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerFeedForward"><code class="flex name class">
<span>class <span class="ident">Gemma3nAudioConformerFeedForward</span></span>
<span>(</span><span>config: transformers.models.gemma3n.configuration_gemma3n.Gemma3nAudioConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nAudioConformerFeedForward(nn.Module):
    def __init__(
        self,
        config: Gemma3nAudioConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ):
        super().__init__()
        self.config = config

        self.register_buffer(
            &#34;gradient_clipping&#34;,
            torch.tensor(self.config.gradient_clipping),
            persistent=False,
        )

        self.pre_layer_norm = Gemma3nRMSNorm(self.config.hidden_size)
        self.ffw_layer_1 = ColumnParallelLinear(
            self.config.hidden_size,
            self.config.hidden_size * 4,
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;ffw_layer_1&#34;, prefix),
        )
        self.ffw_layer_2 = RowParallelLinear(
            self.config.hidden_size * 4,
            self.config.hidden_size,
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;ffw_layer_2&#34;, prefix),
        )
        self.post_layer_norm = Gemma3nRMSNorm(self.config.hidden_size)
        self.post_layer_scale = torch.tensor(self.config.conf_residual_weight)

    def forward(self, audio_encodings: torch.Tensor) -&gt; torch.Tensor:
        residual = audio_encodings
        audio_encodings = torch.clamp(
            audio_encodings, -self.gradient_clipping, self.gradient_clipping
        )
        audio_encodings = self.pre_layer_norm(audio_encodings)
        audio_encodings, _ = self.ffw_layer_1(audio_encodings)
        audio_encodings = F.silu(audio_encodings)
        audio_encodings, _ = self.ffw_layer_2(audio_encodings)
        audio_encodings = torch.clamp(
            audio_encodings, -self.gradient_clipping, self.gradient_clipping
        )
        audio_encodings = self.post_layer_norm(audio_encodings)
        return residual + (audio_encodings * self.post_layer_scale)</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerFeedForward.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, audio_encodings: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, audio_encodings: torch.Tensor) -&gt; torch.Tensor:
    residual = audio_encodings
    audio_encodings = torch.clamp(
        audio_encodings, -self.gradient_clipping, self.gradient_clipping
    )
    audio_encodings = self.pre_layer_norm(audio_encodings)
    audio_encodings, _ = self.ffw_layer_1(audio_encodings)
    audio_encodings = F.silu(audio_encodings)
    audio_encodings, _ = self.ffw_layer_2(audio_encodings)
    audio_encodings = torch.clamp(
        audio_encodings, -self.gradient_clipping, self.gradient_clipping
    )
    audio_encodings = self.post_layer_norm(audio_encodings)
    return residual + (audio_encodings * self.post_layer_scale)</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerLightConv1d"><code class="flex name class">
<span>class <span class="ident">Gemma3nAudioConformerLightConv1d</span></span>
<span>(</span><span>config: transformers.models.gemma3n.configuration_gemma3n.Gemma3nAudioConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nAudioConformerLightConv1d(nn.Module):
    def __init__(
        self,
        config: Gemma3nAudioConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ):
        super().__init__()
        self.config = config

        self.pre_layer_norm = Gemma3nRMSNorm(
            self.config.hidden_size, eps=self.config.rms_norm_eps
        )
        self.linear_start = ColumnParallelLinear(
            self.config.hidden_size,
            self.config.hidden_size * 2,
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;linear_start&#34;, prefix),
        )

        self.depthwise_conv1d = nn.Conv1d(
            in_channels=self.config.hidden_size,
            out_channels=self.config.hidden_size,
            kernel_size=self.config.conf_conv_kernel_size,
            stride=1,
            padding=0,  # Manual causal padding
            groups=self.config.hidden_size,  # Depthwise
            bias=False,
        )
        self.register_buffer(
            &#34;gradient_clipping&#34;,
            torch.tensor(self.config.gradient_clipping),
            persistent=False,
        )
        self.conv_norm = Gemma3nRMSNorm(
            self.config.hidden_size, eps=self.config.rms_norm_eps
        )
        self.linear_end = RowParallelLinear(
            self.config.hidden_size,
            self.config.hidden_size,
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;linear_end&#34;, prefix),
        )

        self.causal_padding = self.config.conf_conv_kernel_size - 1

    def forward(self, audio_encodings: torch.Tensor) -&gt; torch.Tensor:
        audio_encodings_residual = audio_encodings  # Save for residual connection

        audio_encodings = self.pre_layer_norm(audio_encodings)
        audio_encodings, _ = self.linear_start(audio_encodings)
        audio_encodings = F.glu(audio_encodings, dim=-1)

        # Permute for Conv1d: [B, T, D] -&gt; [B, D, T]
        audio_encodings_permuted = audio_encodings.permute(0, 2, 1)
        # Apply manual causal padding
        audio_encodings_permuted_padded = F.pad(
            audio_encodings_permuted, (self.causal_padding, 0)
        )
        audio_encodings = self.depthwise_conv1d(audio_encodings_permuted_padded)
        # Permute back: [B, D, T_out] -&gt; [B, T_out, D]
        audio_encodings = audio_encodings.permute(0, 2, 1)
        audio_encodings = torch.clamp(
            audio_encodings, -self.gradient_clipping, self.gradient_clipping
        )
        audio_encodings = self.conv_norm(audio_encodings)
        audio_encodings = F.silu(audio_encodings)
        audio_encodings, _ = self.linear_end(audio_encodings)
        output = audio_encodings + audio_encodings_residual
        return output</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerLightConv1d.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, audio_encodings: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, audio_encodings: torch.Tensor) -&gt; torch.Tensor:
    audio_encodings_residual = audio_encodings  # Save for residual connection

    audio_encodings = self.pre_layer_norm(audio_encodings)
    audio_encodings, _ = self.linear_start(audio_encodings)
    audio_encodings = F.glu(audio_encodings, dim=-1)

    # Permute for Conv1d: [B, T, D] -&gt; [B, D, T]
    audio_encodings_permuted = audio_encodings.permute(0, 2, 1)
    # Apply manual causal padding
    audio_encodings_permuted_padded = F.pad(
        audio_encodings_permuted, (self.causal_padding, 0)
    )
    audio_encodings = self.depthwise_conv1d(audio_encodings_permuted_padded)
    # Permute back: [B, D, T_out] -&gt; [B, T_out, D]
    audio_encodings = audio_encodings.permute(0, 2, 1)
    audio_encodings = torch.clamp(
        audio_encodings, -self.gradient_clipping, self.gradient_clipping
    )
    audio_encodings = self.conv_norm(audio_encodings)
    audio_encodings = F.silu(audio_encodings)
    audio_encodings, _ = self.linear_end(audio_encodings)
    output = audio_encodings + audio_encodings_residual
    return output</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nAudioEncoder"><code class="flex name class">
<span>class <span class="ident">Gemma3nAudioEncoder</span></span>
<span>(</span><span>config: transformers.models.gemma3n.configuration_gemma3n.Gemma3nAudioConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nAudioEncoder(PreTrainedModel):
    &#34;&#34;&#34;A Universal Speech Encoder -- https://arxiv.org/abs/2303.01037&#34;&#34;&#34;

    config_class = Gemma3nAudioConfig

    def __init__(
        self,
        config: Gemma3nAudioConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ):
        super().__init__(config)
        self.config = config

        self.subsample_conv_projection = Gemma3nAudioSubSampleConvProjection(
            config, quant_config, prefix=add_prefix(&#34;subsample_conv_projection&#34;, prefix)
        )
        self.conformer = make_layers(
            config.conf_num_hidden_layers,
            lambda idx, prefix: Gemma3nAudioConformerBlock(
                config=config,
                quant_config=quant_config,
                prefix=prefix,
            ),
            prefix=add_prefix(&#34;conformer&#34;, prefix),
        )

    def forward(
        self, audio_mel: torch.Tensor, audio_mel_mask: torch.BoolTensor
    ) -&gt; Tuple[torch.Tensor, torch.BoolTensor]:
        &#34;&#34;&#34;Encodes a batch of MELs.

        Args:
            audio_mel: a torch.Tensor of shape [batch, num_frames, mel_bins].
            audio_mel_mask: a torch.BoolTensor of shape [batch, num_frames].

        Returns:
            audio_encodings: a torch.Tensor of shape
                `[batch_size, reduced_time_frames, hidden_size]`
            audio_mel_mask: a torch.BoolTensor of shape [batch, reduced_time_frames].
        &#34;&#34;&#34;
        audio_encodings = self.subsample_conv_projection(
            audio_mel
        )  # audio_encodings: [B, T_sub, D]

        # Subsample the input audio_mel_mask to match the time dimension of audio_encodings (T_sub)
        t_sub = audio_encodings.shape[1]

        time_stride_product = 1
        for stride_pair_idx in range(len(self.config.sscp_conv_stride_size)):
            time_stride_product *= self.config.sscp_conv_stride_size[stride_pair_idx][0]

        # Create indices for gathering from the original mask.
        # These indices map to original time steps corresponding to the start of each
        # receptive field in the subsampled output.
        indices = (
            torch.arange(t_sub, device=audio_mel_mask.device) * time_stride_product
        )
        indices = torch.clamp(indices, max=audio_mel_mask.shape[1] - 1)

        # Expand indices for batch compatibility if B &gt; 1 and indices is 1D.
        if audio_mel_mask.ndim &gt; 1 and indices.ndim == 1:
            indices = indices.unsqueeze(0).expand(
                audio_mel_mask.shape[0], -1
            )  # [B, T_sub]
        elif (
            audio_mel_mask.ndim == indices.ndim
            and audio_mel_mask.shape[0] == 1
            and indices.shape[0] != 1
            and t_sub == indices.shape[0]
        ):
            # Handle case where B=1 but indices became [T_sub] instead of [1, T_sub]
            indices = indices.unsqueeze(0)

        current_mask = torch.gather(audio_mel_mask, 1, indices)  # [B, T_sub]

        # Fallback: Ensure mask length matches feature length after gather.
        if current_mask.shape[1] != t_sub:
            if current_mask.shape[1] &gt; t_sub:
                current_mask = current_mask[:, :t_sub]
            else:  # current_mask.shape[1] &lt; t_sub
                padding_needed = t_sub - current_mask.shape[1]
                current_mask = F.pad(
                    current_mask, (0, padding_needed), value=True
                )  # Pad with True (masked)

        for i, block in enumerate(self.conformer):
            audio_encodings = block(
                audio_encodings, current_mask
            )  # Pass the processed mask

        if self.config.conf_reduction_factor &gt; 1:
            audio_encodings = audio_encodings[:, :: self.config.conf_reduction_factor]
            # Reduce the mask as well
            current_mask = current_mask[:, :: self.config.conf_reduction_factor]

        # Final masking of audio_encodings based on the final current_mask
        # Ensure current_mask length matches the finally reduced audio_encodings length
        if current_mask.shape[1] != audio_encodings.shape[1]:
            target_len = audio_encodings.shape[1]
            mask_current_len = current_mask.shape[1]
            if target_len &gt; mask_current_len:
                padding_needed = target_len - mask_current_len
                current_mask = F.pad(current_mask, (0, padding_needed), value=True)
            elif mask_current_len &gt; target_len:  # mask is longer
                current_mask = current_mask[:, :target_len]

        audio_encodings = audio_encodings.masked_fill(current_mask.unsqueeze(-1), 0.0)
        return audio_encodings, current_mask</code></pre>
</details>
<div class="desc"><p>A Universal Speech Encoder &ndash; <a href="https://arxiv.org/abs/2303.01037">https://arxiv.org/abs/2303.01037</a></p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.modeling_utils.PreTrainedModel</li>
<li>torch.nn.modules.module.Module</li>
<li>transformers.modeling_utils.EmbeddingAccessMixin</li>
<li>transformers.modeling_utils.ModuleUtilsMixin</li>
<li>transformers.utils.hub.PushToHubMixin</li>
<li>transformers.integrations.peft.PeftAdapterMixin</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nAudioEncoder.config_class"><code class="name">var <span class="ident">config_class</span></code></dt>
<dd>
<div class="desc"><p>This is the configuration class to store the configuration of a [<code>Gemma3nAudioEncoder</code>]. It is used to instantiate
an <code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioEncoder" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioEncoder">Gemma3nAudioEncoder</a></code> model according to the specified arguments, defining the model architecture. Instantiating
a configuration with the defaults will yield a similar configuration to that of the Gemma 3n E4B, e.g.,
<a href="https://huggingface.co/google/gemma-3n-E4B">google/gemma-3n-E4B</a>.</p>
<p>Configuration objects that inherit from [<code>Gemma3nAudioConfig</code>] and can be used to control the model outputs. Read
the documentation from [<code>Gemma3nAudioConfig</code>] for more information.</p>
<h2 id="args">Args</h2>
<p>vocab_size (<code>int</code>, <em>optional</em>, defaults to 128):
Vocabulary size of the additional hard-token embeddings for audio model. These augment the embeddings
included in the <code>Gemma3nTextModel</code> to provide, e.g., the end of audio and audio soft token placeholder
tokens when converting <code>input_ids</code> to embeddings in the <code>Gemma3nForConditionalGeneration</code> model.
vocab_offset (<code>int</code>, <em>optional</em>, defaults to 262272):
Offset between the tokenizer vocab index for the token ids embedded by <code>Gemma3nMultimodalEmbedder</code> and the
0-indexed <code>Gemma3nMultimodalEmbedder.embedding</code> table.
input_feat_size (<code>int</code>, <em>optional</em>, defaults to 128):
The number of channels in each mel-spectrogram frame.
hidden_size (<code>int</code>, <em>optional</em>, defaults to 1536):
Dimension of the hidden representations.
rms_norm_eps (<code>float</code>, <em>optional</em>, defaults to 1e-06):
The epsilon used by the rms normalization layers.
gradient_clipping (<code>float</code>, <em>optional</em>, defaults to 10000000000.0):
Clipping value used to stablize extremely large gradient values.
conf_attention_chunk_size (<code>int</code>, <em>optional</em>, defaults to 12):
The sub-sequence size for local attention processing inside the Conformer ("conf") section of the
Universal Speech Model.
conf_attention_context_left (<code>int</code>, <em>optional</em>, defaults to 13):
The left context size of the local attention inside the Conformer ("conf") section of the
Universal Speech Model.
conf_attention_context_right (<code>int</code>, <em>optional</em>, defaults to 0):
The right context size of the local attention inside the Conformer ("conf") section of the
Universal Speech Model.
conf_attention_logit_cap (<code>float</code>, <em>optional</em>, defaults to 50.0):
Logit cap applied during local attention inside the Conformer ("conf") section of the
Universal Speech Model.
conf_num_attention_heads (<code>int</code>, <em>optional</em>, defaults to 8):
The number of attention heads in local attention inside the Conformer ("conf") section of the
Universal Speech Model.
conf_num_hidden_layers (<code>int</code>, <em>optional</em>, defaults to 12):
The number of layers that use local attention inside the Conformer ("conf") section of the
Universal Speech Model.
conf_conv_kernel_size (<code>int</code>, <em>optional</em>, defaults to 5):
Convolution kernel size for the conformer block inside the Conformer ("conf") section of the
Universal Speech Model.
conf_reduction_factor (<code>int</code>, <em>optional</em>, defaults to 4):
Reduction factor used in the conformer block inside the Conformer ("conf") section of the
Universal Speech Model.
conf_residual_weight (<code>float</code>, <em>optional</em>, defaults to 0.5):
Residual connection weight inside the Conformer ("conf") section of the
Universal Speech Model.
sscp_conv_channel_size (<code>tuple(int, int)</code>, <em>optional</em>, defaults to <code>(128, 32)</code>):
The channel sizes for the first and second convolutional layers in the Sub-sample Convolution Projection
("sscp") section of the Universal Speech Model.
sscp_conv_group_norm_eps (<code>float</code>, <em>optional</em>, defaults to 0.001):
Epsilon used in group normalization in the subsample convolution projection in the Sub-sample Convolution
Projection ("sscp") section of the Universal Speech Model.
sscp_conv_kernel_size (<code>tuple(tuple(int, int), tuple(int, int))</code>, <em>optional</em>, defaults to <code>((3, 3), (3, 3))</code>):
Kernel sizes of the two convolutional layers in the subsample convolution projection
in the Sub-sample
Convolution Projection ("sscp") section of the Universal Speech Model. The kernel sizes are specified as a
tuple of height and width for each layer, where the height corresponds to the time dimension and the width
corresponds to the frequency dimension.
sscp_conv_stride_size (<code>tuple(tuple(int, int), tuple(int, int))</code>, <em>optional</em>, defaults to <code>((2, 2), (2, 2))</code>):
Stride sizes of the two convolutional layers in the subsample convolution projection in the Sub-sample
Convolution Projection ("sscp") section of the Universal Speech Model. The stride sizes are specified as a
tuple of height and width for each layer, where the height corresponds to the time dimension and the width
corresponds to the frequency dimension.
Example:</p>
<pre><code class="language-python">&gt;&gt;&gt; from transformers import Gemma3nAudioConfig, Gemma3nAudioEncoder

&gt;&gt;&gt; # Initializing a Gemma3nAudioEncoder gemma3n_audio-E4B-style configuration
&gt;&gt;&gt; configuration = Gemma3nAudioConfig()

&gt;&gt;&gt; # Initializing a model from the gemma3n_audio-E4B style configuration
&gt;&gt;&gt; model = Gemma3nAudioEncoder(configuration)

&gt;&gt;&gt; # Accessing the model configuration
&gt;&gt;&gt; configuration = model.config
</code></pre></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nAudioEncoder.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, audio_mel: torch.Tensor, audio_mel_mask: torch.BoolTensor) ‑> Tuple[torch.Tensor, torch.BoolTensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, audio_mel: torch.Tensor, audio_mel_mask: torch.BoolTensor
) -&gt; Tuple[torch.Tensor, torch.BoolTensor]:
    &#34;&#34;&#34;Encodes a batch of MELs.

    Args:
        audio_mel: a torch.Tensor of shape [batch, num_frames, mel_bins].
        audio_mel_mask: a torch.BoolTensor of shape [batch, num_frames].

    Returns:
        audio_encodings: a torch.Tensor of shape
            `[batch_size, reduced_time_frames, hidden_size]`
        audio_mel_mask: a torch.BoolTensor of shape [batch, reduced_time_frames].
    &#34;&#34;&#34;
    audio_encodings = self.subsample_conv_projection(
        audio_mel
    )  # audio_encodings: [B, T_sub, D]

    # Subsample the input audio_mel_mask to match the time dimension of audio_encodings (T_sub)
    t_sub = audio_encodings.shape[1]

    time_stride_product = 1
    for stride_pair_idx in range(len(self.config.sscp_conv_stride_size)):
        time_stride_product *= self.config.sscp_conv_stride_size[stride_pair_idx][0]

    # Create indices for gathering from the original mask.
    # These indices map to original time steps corresponding to the start of each
    # receptive field in the subsampled output.
    indices = (
        torch.arange(t_sub, device=audio_mel_mask.device) * time_stride_product
    )
    indices = torch.clamp(indices, max=audio_mel_mask.shape[1] - 1)

    # Expand indices for batch compatibility if B &gt; 1 and indices is 1D.
    if audio_mel_mask.ndim &gt; 1 and indices.ndim == 1:
        indices = indices.unsqueeze(0).expand(
            audio_mel_mask.shape[0], -1
        )  # [B, T_sub]
    elif (
        audio_mel_mask.ndim == indices.ndim
        and audio_mel_mask.shape[0] == 1
        and indices.shape[0] != 1
        and t_sub == indices.shape[0]
    ):
        # Handle case where B=1 but indices became [T_sub] instead of [1, T_sub]
        indices = indices.unsqueeze(0)

    current_mask = torch.gather(audio_mel_mask, 1, indices)  # [B, T_sub]

    # Fallback: Ensure mask length matches feature length after gather.
    if current_mask.shape[1] != t_sub:
        if current_mask.shape[1] &gt; t_sub:
            current_mask = current_mask[:, :t_sub]
        else:  # current_mask.shape[1] &lt; t_sub
            padding_needed = t_sub - current_mask.shape[1]
            current_mask = F.pad(
                current_mask, (0, padding_needed), value=True
            )  # Pad with True (masked)

    for i, block in enumerate(self.conformer):
        audio_encodings = block(
            audio_encodings, current_mask
        )  # Pass the processed mask

    if self.config.conf_reduction_factor &gt; 1:
        audio_encodings = audio_encodings[:, :: self.config.conf_reduction_factor]
        # Reduce the mask as well
        current_mask = current_mask[:, :: self.config.conf_reduction_factor]

    # Final masking of audio_encodings based on the final current_mask
    # Ensure current_mask length matches the finally reduced audio_encodings length
    if current_mask.shape[1] != audio_encodings.shape[1]:
        target_len = audio_encodings.shape[1]
        mask_current_len = current_mask.shape[1]
        if target_len &gt; mask_current_len:
            padding_needed = target_len - mask_current_len
            current_mask = F.pad(current_mask, (0, padding_needed), value=True)
        elif mask_current_len &gt; target_len:  # mask is longer
            current_mask = current_mask[:, :target_len]

    audio_encodings = audio_encodings.masked_fill(current_mask.unsqueeze(-1), 0.0)
    return audio_encodings, current_mask</code></pre>
</details>
<div class="desc"><p>Encodes a batch of MELs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>audio_mel</code></strong></dt>
<dd>a torch.Tensor of shape [batch, num_frames, mel_bins].</dd>
<dt><strong><code>audio_mel_mask</code></strong></dt>
<dd>a torch.BoolTensor of shape [batch, num_frames].</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>audio_encodings</code></dt>
<dd>a torch.Tensor of shape
<code>[batch_size, reduced_time_frames, hidden_size]</code></dd>
<dt><code>audio_mel_mask</code></dt>
<dd>a torch.BoolTensor of shape [batch, reduced_time_frames].</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nAudioRelativePositionEmbedding"><code class="flex name class">
<span>class <span class="ident">Gemma3nAudioRelativePositionEmbedding</span></span>
<span>(</span><span>config: transformers.models.gemma3n.configuration_gemma3n.Gemma3nAudioConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nAudioRelativePositionEmbedding(nn.Module):
    def __init__(
        self,
        config: Gemma3nAudioConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ):
        super().__init__()
        self.config = config

        self.num_heads = self.config.conf_num_attention_heads
        self.channels = self.config.hidden_size
        self.head_dim = self.channels // self.num_heads
        self.max_backward = max(0, self.config.conf_attention_context_left - 1)
        self.max_forward = self.config.conf_attention_context_right

        self.pos_proj = ColumnParallelLinear(
            self.channels,
            self.num_heads * self.head_dim,
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;pos_proj&#34;, prefix),
        )

        min_timescale = 1.0
        max_timescale = 1.0e4
        num_timescales = self.channels // 2
        log_timescale_increment = math.log(
            float(max_timescale) / float(min_timescale)
        ) / max(num_timescales - 1, 1)
        inv_timescales = min_timescale * torch.exp(
            torch.arange(num_timescales) * -log_timescale_increment
        )
        self.register_buffer(
            &#34;inv_timescales&#34;,
            inv_timescales.float().unsqueeze(0).unsqueeze(0),
            persistent=False,
        )

    def _get_timing_signal_1d_pos(
        self, position: torch.Tensor, dtype: torch.dtype
    ) -&gt; torch.Tensor:
        assert position.ndim == 2
        position = position.float().unsqueeze(-1)
        scaled_time = position * self.inv_timescales.to(
            device=position.device, dtype=torch.float32
        )
        timing_signal = torch.cat(
            [torch.sin(scaled_time), torch.cos(scaled_time)], dim=-1
        )
        return timing_signal.type(dtype)

    def _relative_shift(
        self,
        term_bd_before_shift: torch.Tensor,
        batch_size: int,
        num_heads: int,
        num_query_blocks: int,
        query_block_size: int,
        key_context_size: int,
        max_span_plus_1: int,
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;Performs the relative shift.&#34;&#34;&#34;
        pad_amount_last_dim = (key_context_size + 1) - max_span_plus_1
        padding_tuple = (0, pad_amount_last_dim)

        term_bd_padded = F.pad(term_bd_before_shift, padding_tuple)
        term_bd_reshaped = term_bd_padded.reshape(
            (
                batch_size,
                num_heads,
                num_query_blocks,
                query_block_size * (key_context_size + 1),
            )
        )
        term_bd_sliced = term_bd_reshaped[
            :, :, :, : query_block_size * key_context_size
        ]
        term_bd_shifted = term_bd_sliced.reshape(
            (
                batch_size,
                num_heads,
                num_query_blocks,
                query_block_size,
                key_context_size,
            )
        )
        return term_bd_shifted

    def forward(self, queries: torch.Tensor, keys: torch.Tensor) -&gt; torch.Tensor:
        batch_size, num_query_blocks, query_block_size, num_heads, head_dim = (
            queries.shape
        )
        _, _, key_context_size, _, _ = keys.shape

        pos_indices = torch.arange(
            self.max_backward, -self.max_forward - 1, -1, device=queries.device
        ).unsqueeze(0)
        max_span_plus_1 = pos_indices.shape[1]

        sin_emb_timing_signal = self._get_timing_signal_1d_pos(
            pos_indices, dtype=queries.dtype
        )
        projected_sin_emb, _ = self.pos_proj(sin_emb_timing_signal)
        sin_emb = projected_sin_emb.reshape(
            1, max_span_plus_1, self.num_heads, self.head_dim
        ).squeeze(0)

        queries_p = queries.permute(0, 3, 1, 2, 4)
        keys_p_t = keys.permute(0, 3, 1, 4, 2)
        term_ac = torch.matmul(queries_p, keys_p_t)

        q_permuted = queries.permute(0, 3, 1, 2, 4)
        s_permuted = sin_emb.permute(1, 2, 0)
        q_reshaped = q_permuted.reshape(
            batch_size, num_heads, num_query_blocks * query_block_size, head_dim
        )
        term_bd_unshifed_matmul = torch.matmul(q_reshaped, s_permuted)
        term_bd_unshifed = term_bd_unshifed_matmul.reshape(
            batch_size,
            num_heads,
            num_query_blocks,
            query_block_size,
            max_span_plus_1,
        )

        term_bd_shifted = self._relative_shift(
            term_bd_unshifed,
            batch_size,
            num_heads,
            num_query_blocks,
            query_block_size,
            key_context_size,
            max_span_plus_1,
        )

        return term_ac + term_bd_shifted</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nAudioRelativePositionEmbedding.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, queries: torch.Tensor, keys: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, queries: torch.Tensor, keys: torch.Tensor) -&gt; torch.Tensor:
    batch_size, num_query_blocks, query_block_size, num_heads, head_dim = (
        queries.shape
    )
    _, _, key_context_size, _, _ = keys.shape

    pos_indices = torch.arange(
        self.max_backward, -self.max_forward - 1, -1, device=queries.device
    ).unsqueeze(0)
    max_span_plus_1 = pos_indices.shape[1]

    sin_emb_timing_signal = self._get_timing_signal_1d_pos(
        pos_indices, dtype=queries.dtype
    )
    projected_sin_emb, _ = self.pos_proj(sin_emb_timing_signal)
    sin_emb = projected_sin_emb.reshape(
        1, max_span_plus_1, self.num_heads, self.head_dim
    ).squeeze(0)

    queries_p = queries.permute(0, 3, 1, 2, 4)
    keys_p_t = keys.permute(0, 3, 1, 4, 2)
    term_ac = torch.matmul(queries_p, keys_p_t)

    q_permuted = queries.permute(0, 3, 1, 2, 4)
    s_permuted = sin_emb.permute(1, 2, 0)
    q_reshaped = q_permuted.reshape(
        batch_size, num_heads, num_query_blocks * query_block_size, head_dim
    )
    term_bd_unshifed_matmul = torch.matmul(q_reshaped, s_permuted)
    term_bd_unshifed = term_bd_unshifed_matmul.reshape(
        batch_size,
        num_heads,
        num_query_blocks,
        query_block_size,
        max_span_plus_1,
    )

    term_bd_shifted = self._relative_shift(
        term_bd_unshifed,
        batch_size,
        num_heads,
        num_query_blocks,
        query_block_size,
        key_context_size,
        max_span_plus_1,
    )

    return term_ac + term_bd_shifted</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nAudioSSCPConvBlock"><code class="flex name class">
<span>class <span class="ident">Gemma3nAudioSSCPConvBlock</span></span>
<span>(</span><span>config: transformers.models.gemma3n.configuration_gemma3n.Gemma3nAudioConfig,<br>idx: int,<br>input_freq_dim: int,<br>manual_padding: Tuple[int, int, int, int] = (0, 0, 0, 0),<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nAudioSSCPConvBlock(nn.Module):
    &#34;&#34;&#34;A single convolution block for the SubSampleConvProjection.&#34;&#34;&#34;

    def __init__(
        self,
        config: Gemma3nAudioConfig,
        idx: int,
        input_freq_dim: int,
        manual_padding: Tuple[int, int, int, int] = (0, 0, 0, 0),
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ):
        super().__init__()
        self.config = config
        self.manual_padding = manual_padding

        in_channels = 1 if idx == 0 else self.config.sscp_conv_channel_size[idx - 1]
        out_channels = self.config.sscp_conv_channel_size[idx]
        kernel_h, kernel_w = self.config.sscp_conv_kernel_size[idx]
        stride_h, stride_w = self.config.sscp_conv_stride_size[idx]

        self.conv = nn.Conv2d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=(kernel_h, kernel_w),
            stride=(stride_h, stride_w),
            padding=(0, 0),  # Manual padding is used
            bias=False,
        )

        f_in_padded = input_freq_dim + self.manual_padding[0] + self.manual_padding[1]
        f_out_conv = (f_in_padded - kernel_w) // stride_w + 1

        self.norm = Gemma3nCumulativeGroupNorm(
            num_channels=out_channels,
            feature_dims=(f_out_conv,),
            eps=self.config.sscp_conv_group_norm_eps,
        )

        self.activation = nn.ReLU()

    def forward(self, audio_encodings: torch.Tensor) -&gt; torch.Tensor:
        audio_encodings_padded = F.pad(
            audio_encodings, self.manual_padding, mode=&#34;constant&#34;, value=0.0
        )
        audio_encodings_conv = self.conv(audio_encodings_padded)
        x_for_norm = audio_encodings_conv.permute(0, 2, 3, 1).contiguous()
        x_normed = self.norm(x_for_norm)
        audio_encodings_normed = x_normed.permute(0, 3, 1, 2).contiguous()
        return self.activation(audio_encodings_normed)</code></pre>
</details>
<div class="desc"><p>A single convolution block for the SubSampleConvProjection.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nAudioSSCPConvBlock.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, audio_encodings: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, audio_encodings: torch.Tensor) -&gt; torch.Tensor:
    audio_encodings_padded = F.pad(
        audio_encodings, self.manual_padding, mode=&#34;constant&#34;, value=0.0
    )
    audio_encodings_conv = self.conv(audio_encodings_padded)
    x_for_norm = audio_encodings_conv.permute(0, 2, 3, 1).contiguous()
    x_normed = self.norm(x_for_norm)
    audio_encodings_normed = x_normed.permute(0, 3, 1, 2).contiguous()
    return self.activation(audio_encodings_normed)</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nAudioSubSampleConvProjection"><code class="flex name class">
<span>class <span class="ident">Gemma3nAudioSubSampleConvProjection</span></span>
<span>(</span><span>config: transformers.models.gemma3n.configuration_gemma3n.Gemma3nAudioConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nAudioSubSampleConvProjection(nn.Module):
    def __init__(
        self,
        config: Gemma3nAudioConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ):
        super().__init__()
        self.config = config

        current_f_for_block_input = config.input_feat_size
        calculated_block_padding = []
        calculated_f_out_dims = []

        for i in range(2):  # Assuming 2 conv layers
            kernel_h, kernel_w = config.sscp_conv_kernel_size[i]
            stride_h, stride_w = config.sscp_conv_stride_size[i]

            # Padding for Time (Height for Conv2d) - REVERSE_CAUSAL like
            pad_t_top = 0
            pad_t_bottom = kernel_h - 1

            # Frequency Padding (Width for Conv2d)
            pad_f_left = 1
            pad_f_right = 1

            manual_padding_tuple = (pad_f_left, pad_f_right, pad_t_top, pad_t_bottom)
            calculated_block_padding.append(manual_padding_tuple)

            f_in_padded = current_f_for_block_input + pad_f_left + pad_f_right
            f_out_after_conv = (f_in_padded - kernel_w) // stride_w + 1
            calculated_f_out_dims.append(f_out_after_conv)
            current_f_for_block_input = f_out_after_conv

        self.conv_0 = Gemma3nAudioSSCPConvBlock(
            idx=0,
            input_freq_dim=config.input_feat_size,
            config=config,
            manual_padding=calculated_block_padding[0],
            quant_config=quant_config,
            prefix=add_prefix(&#34;conv_0&#34;, prefix),
        )
        self.conv_1 = Gemma3nAudioSSCPConvBlock(
            idx=1,
            input_freq_dim=calculated_f_out_dims[0],
            config=config,
            manual_padding=calculated_block_padding[1],
            quant_config=quant_config,
            prefix=add_prefix(&#34;conv_1&#34;, prefix),
        )

        final_c_out = config.sscp_conv_channel_size[-1]
        final_f_out = calculated_f_out_dims[-1]
        self.input_proj_in_features = final_c_out * final_f_out

        self.input_proj_linear = RowParallelLinear(
            self.input_proj_in_features,
            self.config.hidden_size,
            bias=False,
            quant_config=quant_config,
            prefix=add_prefix(&#34;input_proj_linear&#34;, prefix),
        )

    def forward(self, audio_encodings: torch.Tensor) -&gt; torch.Tensor:
        audio_encodings_reshaped = audio_encodings.unsqueeze(1)
        x = self.conv_0(audio_encodings_reshaped)
        x = self.conv_1(x)
        b, c_out, t_out, f_out = x.shape
        x_permuted = x.permute(0, 2, 3, 1).contiguous()
        output_flattened = x_permuted.view(b, t_out, f_out * c_out)
        output, _ = self.input_proj_linear(output_flattened)
        return output</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nAudioSubSampleConvProjection.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, audio_encodings: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, audio_encodings: torch.Tensor) -&gt; torch.Tensor:
    audio_encodings_reshaped = audio_encodings.unsqueeze(1)
    x = self.conv_0(audio_encodings_reshaped)
    x = self.conv_1(x)
    b, c_out, t_out, f_out = x.shape
    x_permuted = x.permute(0, 2, 3, 1).contiguous()
    output_flattened = x_permuted.view(b, t_out, f_out * c_out)
    output, _ = self.input_proj_linear(output_flattened)
    return output</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nCumulativeGroupNorm"><code class="flex name class">
<span>class <span class="ident">Gemma3nCumulativeGroupNorm</span></span>
<span>(</span><span>num_channels: int, feature_dims: Sequence[int], eps: float = 0.001)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gemma3nCumulativeGroupNorm(nn.Module):
    &#34;&#34;&#34;Applies Group Normalization cumulatively over the time dimension.

    This layer normalizes the input by calculating the mean and variance
    cumulatively over the time dimension (dim 1). The statistics are computed
    over all feature dimensions (specified by `feature_dims` and `num_channels`)
    for elements marked as valid by the optional `mask`.

    If a `mask` is provided (True for valid, False for invalid/padded),
    invalid time steps do not contribute to the statistics calculation, and
    their corresponding output values are zeroed out.

    Scale and bias, if enabled, are applied per-channel (last dimension).
    This behavior is similar to JAX&#39;s `GroupNormalization` with `num_groups=1`
    and `cumulative=True`.
    &#34;&#34;&#34;

    def __init__(
        self,
        num_channels: int,  # Number of channels (size of the last dimension)
        feature_dims: Sequence[
            int
        ],  # Sizes of non-channel feature dimensions, e.g., (H, W) for input [B,T,H,W,C]
        eps: float = 1e-3,
    ):
        super().__init__()
        self.num_channels = num_channels
        self.feature_dims = tuple(feature_dims)
        self.eps = eps

        # Scale parameter depends only on the channel dimension
        self.weight = nn.Parameter(torch.ones(num_channels))

        # Axes for normalization: all dimensions except Batch (0) and Time (1).
        # For input [B, T, *feature_dims, C], these are dims from 2 onwards.
        self.reduction_axes = tuple(range(2, 2 + len(self.feature_dims) + 1))

    def forward(
        self, x: torch.Tensor, mask: Optional[torch.Tensor] = None
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;Applies cumulative group norm, optionally using a mask.

        Args:
          x: Input tensor, shape [B, T, *feature_dims, C].
          mask: Optional boolean mask, shape [B, T]. True indicates a valid
            (non-padded) time step. If None, all time steps are considered valid.

        Returns:
          Normalized tensor with the same shape as x.
        &#34;&#34;&#34;
        expected_input_suffix = self.feature_dims + (self.num_channels,)
        if x.shape[2:] != expected_input_suffix:
            raise ValueError(
                f&#34;Input tensor shape suffix {x.shape[2:]} does not match expected&#34;
                f&#34; suffix (feature_dims + num_channels) {expected_input_suffix}&#34;
            )

        input_dtype = x.dtype
        # Calculations are performed in float32 for numerical stability.
        calc_dtype = torch.float32
        x_calc = x.to(calc_dtype)

        # Prepare a broadcastable mask (`mask_calc`).
        # If no mask is provided, treat all elements as valid
        # (mask_calc is all ones).
        # Otherwise, expand the [B, T] mask to [B, T, 1, ..., 1] for broadcasting.
        mask_calc = torch.ones_like(x_calc, dtype=calc_dtype)

        # Cumulative Statistics Calculation
        # 1. Sum of values over reduction axes at each time step.
        sum_values_at_t = torch.sum(x_calc, dim=self.reduction_axes, keepdim=True)
        # 2. Cumulative sum of values over time.
        cum_sum_values = torch.cumsum(sum_values_at_t, dim=1)

        # 3. Count of valid elements in the normalization group at each time step.
        #    (A &#34;group&#34; here consists of all features at a given Batch, Time).
        elements_in_group_at_t = torch.sum(
            mask_calc, dim=self.reduction_axes, keepdim=True
        )
        # 4. Cumulative count of valid elements over time.
        cum_count_elements = torch.cumsum(elements_in_group_at_t, dim=1)
        # Avoid division by zero if all preceding elements were masked.
        safe_cum_count_elements = torch.clamp(cum_count_elements, min=1.0)

        # 5. Cumulative mean.
        cum_mean = cum_sum_values / safe_cum_count_elements

        # 6. Sum of squared differences from the cumulative mean.
        #    Only sum for valid elements: (x_calc - cum_mean)^2 * mask_calc.
        #    Using x_calc here for the difference, as cum_mean already accounts for masking.
        squared_diff_from_mean = (x_calc - cum_mean).pow(2)
        sum_sq_diff_at_t = torch.sum(
            squared_diff_from_mean, dim=self.reduction_axes, keepdim=True
        )

        # 7. Cumulative sum of squared differences over time.
        cum_sum_sq_diff = torch.cumsum(sum_sq_diff_at_t, dim=1)

        # 8. Cumulative variance.
        cum_variance = cum_sum_sq_diff / safe_cum_count_elements

        # Normalize the input using the calculated cumulative statistics:
        # (x - E[x]) / sqrt(Var[x] + eps)
        normalized_x = (x_calc - cum_mean) * torch.rsqrt(cum_variance + self.eps)

        # Apply affine transformation (scale and bias) if enabled.
        # Scale and bias are applied per-channel (last dimension).
        scale = self.weight.to(calc_dtype)
        # Reshape for broadcasting: [C] -&gt; [1, ..., 1, C]
        scale_view_shape = [1] * (x.dim() - 1) + [self.num_channels]
        normalized_x = normalized_x * scale.view(scale_view_shape)

        # Zero out outputs for time steps that were originally masked (where mask_calc is 0).
        # This ensures padded/invalid positions in the input result in zero output.
        final_output = normalized_x * mask_calc

        return final_output.to(input_dtype)</code></pre>
</details>
<div class="desc"><p>Applies Group Normalization cumulatively over the time dimension.</p>
<p>This layer normalizes the input by calculating the mean and variance
cumulatively over the time dimension (dim 1). The statistics are computed
over all feature dimensions (specified by <code>feature_dims</code> and <code>num_channels</code>)
for elements marked as valid by the optional <code>mask</code>.</p>
<p>If a <code>mask</code> is provided (True for valid, False for invalid/padded),
invalid time steps do not contribute to the statistics calculation, and
their corresponding output values are zeroed out.</p>
<p>Scale and bias, if enabled, are applied per-channel (last dimension).
This behavior is similar to JAX's <code>GroupNormalization</code> with <code>num_groups=1</code>
and <code>cumulative=True</code>.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.gemma3n_audio.Gemma3nCumulativeGroupNorm.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: torch.Tensor, mask: torch.Tensor | None = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, x: torch.Tensor, mask: Optional[torch.Tensor] = None
) -&gt; torch.Tensor:
    &#34;&#34;&#34;Applies cumulative group norm, optionally using a mask.

    Args:
      x: Input tensor, shape [B, T, *feature_dims, C].
      mask: Optional boolean mask, shape [B, T]. True indicates a valid
        (non-padded) time step. If None, all time steps are considered valid.

    Returns:
      Normalized tensor with the same shape as x.
    &#34;&#34;&#34;
    expected_input_suffix = self.feature_dims + (self.num_channels,)
    if x.shape[2:] != expected_input_suffix:
        raise ValueError(
            f&#34;Input tensor shape suffix {x.shape[2:]} does not match expected&#34;
            f&#34; suffix (feature_dims + num_channels) {expected_input_suffix}&#34;
        )

    input_dtype = x.dtype
    # Calculations are performed in float32 for numerical stability.
    calc_dtype = torch.float32
    x_calc = x.to(calc_dtype)

    # Prepare a broadcastable mask (`mask_calc`).
    # If no mask is provided, treat all elements as valid
    # (mask_calc is all ones).
    # Otherwise, expand the [B, T] mask to [B, T, 1, ..., 1] for broadcasting.
    mask_calc = torch.ones_like(x_calc, dtype=calc_dtype)

    # Cumulative Statistics Calculation
    # 1. Sum of values over reduction axes at each time step.
    sum_values_at_t = torch.sum(x_calc, dim=self.reduction_axes, keepdim=True)
    # 2. Cumulative sum of values over time.
    cum_sum_values = torch.cumsum(sum_values_at_t, dim=1)

    # 3. Count of valid elements in the normalization group at each time step.
    #    (A &#34;group&#34; here consists of all features at a given Batch, Time).
    elements_in_group_at_t = torch.sum(
        mask_calc, dim=self.reduction_axes, keepdim=True
    )
    # 4. Cumulative count of valid elements over time.
    cum_count_elements = torch.cumsum(elements_in_group_at_t, dim=1)
    # Avoid division by zero if all preceding elements were masked.
    safe_cum_count_elements = torch.clamp(cum_count_elements, min=1.0)

    # 5. Cumulative mean.
    cum_mean = cum_sum_values / safe_cum_count_elements

    # 6. Sum of squared differences from the cumulative mean.
    #    Only sum for valid elements: (x_calc - cum_mean)^2 * mask_calc.
    #    Using x_calc here for the difference, as cum_mean already accounts for masking.
    squared_diff_from_mean = (x_calc - cum_mean).pow(2)
    sum_sq_diff_at_t = torch.sum(
        squared_diff_from_mean, dim=self.reduction_axes, keepdim=True
    )

    # 7. Cumulative sum of squared differences over time.
    cum_sum_sq_diff = torch.cumsum(sum_sq_diff_at_t, dim=1)

    # 8. Cumulative variance.
    cum_variance = cum_sum_sq_diff / safe_cum_count_elements

    # Normalize the input using the calculated cumulative statistics:
    # (x - E[x]) / sqrt(Var[x] + eps)
    normalized_x = (x_calc - cum_mean) * torch.rsqrt(cum_variance + self.eps)

    # Apply affine transformation (scale and bias) if enabled.
    # Scale and bias are applied per-channel (last dimension).
    scale = self.weight.to(calc_dtype)
    # Reshape for broadcasting: [C] -&gt; [1, ..., 1, C]
    scale_view_shape = [1] * (x.dim() - 1) + [self.num_channels]
    normalized_x = normalized_x * scale.view(scale_view_shape)

    # Zero out outputs for time steps that were originally masked (where mask_calc is 0).
    # This ensures padded/invalid positions in the input result in zero output.
    final_output = normalized_x * mask_calc

    return final_output.to(input_dtype)</code></pre>
</details>
<div class="desc"><p>Applies cumulative group norm, optionally using a mask.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Input tensor, shape [B, T, *feature_dims, C].</dd>
<dt><strong><code>mask</code></strong></dt>
<dd>Optional boolean mask, shape [B, T]. True indicates a valid
(non-padded) time step. If None, all time steps are considered valid.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Normalized tensor with the same shape as x.</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.models" href="index.html">sglang.srt.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioAttention" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioAttention">Gemma3nAudioAttention</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioAttention.forward" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioAttention.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerAttention" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerAttention">Gemma3nAudioConformerAttention</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerAttention.forward" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerAttention.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerBlock" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerBlock">Gemma3nAudioConformerBlock</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerBlock.forward" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerBlock.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerFeedForward" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerFeedForward">Gemma3nAudioConformerFeedForward</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerFeedForward.forward" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerFeedForward.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerLightConv1d" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerLightConv1d">Gemma3nAudioConformerLightConv1d</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerLightConv1d.forward" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioConformerLightConv1d.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioEncoder" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioEncoder">Gemma3nAudioEncoder</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioEncoder.config_class" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioEncoder.config_class">config_class</a></code></li>
<li><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioEncoder.forward" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioEncoder.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioRelativePositionEmbedding" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioRelativePositionEmbedding">Gemma3nAudioRelativePositionEmbedding</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioRelativePositionEmbedding.forward" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioRelativePositionEmbedding.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioSSCPConvBlock" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioSSCPConvBlock">Gemma3nAudioSSCPConvBlock</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioSSCPConvBlock.forward" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioSSCPConvBlock.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioSubSampleConvProjection" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioSubSampleConvProjection">Gemma3nAudioSubSampleConvProjection</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nAudioSubSampleConvProjection.forward" href="#sglang.srt.models.gemma3n_audio.Gemma3nAudioSubSampleConvProjection.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nCumulativeGroupNorm" href="#sglang.srt.models.gemma3n_audio.Gemma3nCumulativeGroupNorm">Gemma3nCumulativeGroupNorm</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.gemma3n_audio.Gemma3nCumulativeGroupNorm.forward" href="#sglang.srt.models.gemma3n_audio.Gemma3nCumulativeGroupNorm.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
