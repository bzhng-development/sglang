<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.models.deepseek_janus_pro API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.models.deepseek_janus_pro</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.Normalize"><code class="name flex">
<span>def <span class="ident">Normalize</span></span>(<span>in_channels, norm_type='group')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Normalize(in_channels, norm_type=&#34;group&#34;):
    assert norm_type in [&#34;group&#34;, &#34;batch&#34;]
    if norm_type == &#34;group&#34;:
        return nn.GroupNorm(
            num_groups=32, num_channels=in_channels, eps=1e-6, affine=True
        )
    elif norm_type == &#34;batch&#34;:
        return nn.SyncBatchNorm(in_channels)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.VQ_16"><code class="name flex">
<span>def <span class="ident">VQ_16</span></span>(<span>**kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def VQ_16(**kwargs):
    return VQModel(
        ModelArgs(
            encoder_ch_mult=[1, 1, 2, 2, 4], decoder_ch_mult=[1, 1, 2, 2, 4], **kwargs
        )
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.compute_entropy_loss"><code class="name flex">
<span>def <span class="ident">compute_entropy_loss</span></span>(<span>affinity, loss_type='softmax', temperature=0.01)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_entropy_loss(affinity, loss_type=&#34;softmax&#34;, temperature=0.01):
    flat_affinity = affinity.reshape(-1, affinity.shape[-1])
    flat_affinity /= temperature
    probs = F.softmax(flat_affinity, dim=-1)
    log_probs = F.log_softmax(flat_affinity + 1e-5, dim=-1)
    if loss_type == &#34;softmax&#34;:
        target_probs = probs
    else:
        raise ValueError(&#34;Entropy loss {} not supported&#34;.format(loss_type))
    avg_probs = torch.mean(target_probs, dim=0)
    avg_entropy = -torch.sum(avg_probs * torch.log(avg_probs + 1e-5))
    sample_entropy = -torch.mean(torch.sum(target_probs * log_probs, dim=-1))
    loss = sample_entropy - avg_entropy
    return loss</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.create_siglip_vit"><code class="name flex">
<span>def <span class="ident">create_siglip_vit</span></span>(<span>model_name: str = 'siglip_so400m_patch14_384',<br>image_size: int = 384,<br>select_layer: int = -1,<br>ckpt_path: str = '',<br>**kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_siglip_vit(
    model_name: str = &#34;siglip_so400m_patch14_384&#34;,
    image_size: int = 384,
    select_layer: int = -1,
    ckpt_path: str = &#34;&#34;,
    **kwargs,
):
    assert (
        model_name in SigLIP_MODEL_CONFIG.keys()
    ), f&#34;model name should be in {SigLIP_MODEL_CONFIG.keys()}&#34;

    vision_cfg = SigLIPVisionCfg(**SigLIP_MODEL_CONFIG[model_name])

    if select_layer &lt;= 0:
        layers = min(vision_cfg.layers, vision_cfg.layers + select_layer + 1)
    else:
        layers = min(vision_cfg.layers, select_layer)

    model = VisionTransformer(
        img_size=image_size,
        patch_size=vision_cfg.patch_size,
        embed_dim=vision_cfg.width,
        depth=layers,
        num_heads=vision_cfg.heads,
        mlp_ratio=vision_cfg.mlp_ratio,
        class_token=vision_cfg.class_token,
        global_pool=vision_cfg.global_pool,
        ignore_head=kwargs.get(&#34;ignore_head&#34;, True),
        weight_init=kwargs.get(&#34;weight_init&#34;, &#34;skip&#34;),
        num_classes=0,
    )

    if ckpt_path:
        state_dict = torch.load(ckpt_path, map_location=&#34;cpu&#34;, weights_only=True)

        incompatible_keys = model.load_state_dict(state_dict, strict=False)
        print(
            f&#34;SigLIP-ViT restores from {ckpt_path},\n&#34;
            f&#34;\tincompatible_keys:&#39;, {incompatible_keys}.&#34;
        )

    return model</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.drop_path"><code class="name flex">
<span>def <span class="ident">drop_path</span></span>(<span>x, drop_prob: float = 0.0, training: bool = False, scale_by_keep: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def drop_path(
    x, drop_prob: float = 0.0, training: bool = False, scale_by_keep: bool = True
):
    &#34;&#34;&#34;Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).

    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
    the original name is misleading as &#39;Drop Connect&#39; is a different form of dropout in a separate paper...
    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I&#39;ve opted for
    changing the layer and argument names to &#39;drop path&#39; rather than mix DropConnect as a layer name and use
    &#39;survival rate&#39; as the argument.

    &#34;&#34;&#34;
    if drop_prob == 0.0 or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (
        x.ndim - 1
    )  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)
    if keep_prob &gt; 0.0 and scale_by_keep:
        random_tensor.div_(keep_prob)
    return x * random_tensor</code></pre>
</details>
<div class="desc"><p>Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).</p>
<p>This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper&hellip;
See discussion: <a href="https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956">https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956</a> &hellip; I've opted for
changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
'survival rate' as the argument.</p></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.init_weights"><code class="name flex">
<span>def <span class="ident">init_weights</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_weights(self):
    if self.pos_embed is not None:
        trunc_normal_(self.pos_embed, std=self.pos_embed.shape[1] ** -0.5)
    trunc_normal_(self.latent, std=self.latent_dim**-0.5)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.init_weights_vit_timm"><code class="name flex">
<span>def <span class="ident">init_weights_vit_timm</span></span>(<span>module: torch.nn.modules.module.Module, name: str = '') ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_weights_vit_timm(module: nn.Module, name: str = &#34;&#34;) -&gt; None:
    &#34;&#34;&#34;ViT weight initialization, original timm impl (for reproducibility)&#34;&#34;&#34;
    if isinstance(module, nn.Linear):
        trunc_normal_(module.weight, std=0.02)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif hasattr(module, &#34;init_weights&#34;):
        module.init_weights()</code></pre>
</details>
<div class="desc"><p>ViT weight initialization, original timm impl (for reproducibility)</p></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.model_name_to_cls"><code class="name flex">
<span>def <span class="ident">model_name_to_cls</span></span>(<span>cls_name)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model_name_to_cls(cls_name):
    if &#34;MlpProjector&#34; in cls_name:
        cls = MlpProjector

    elif &#34;CLIPVisionTower&#34; in cls_name:
        cls = CLIPVisionTower

    elif &#34;VQ&#34; in cls_name:

        cls = VQ_models[cls_name]
    elif &#34;vision_head&#34; in cls_name:
        cls = vision_head
    else:
        raise ValueError(f&#34;class_name {cls_name} is invalid.&#34;)

    return cls</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.named_apply"><code class="name flex">
<span>def <span class="ident">named_apply</span></span>(<span>fn: Callable,<br>module: torch.nn.modules.module.Module,<br>name='',<br>depth_first: bool = True,<br>include_root: bool = False) ‑> torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def named_apply(
    fn: Callable,
    module: nn.Module,
    name=&#34;&#34;,
    depth_first: bool = True,
    include_root: bool = False,
) -&gt; nn.Module:
    if not depth_first and include_root:
        fn(module=module, name=name)
    for child_name, child_module in module.named_children():
        child_name = &#34;.&#34;.join((name, child_name)) if name else child_name
        named_apply(
            fn=fn,
            module=child_module,
            name=child_name,
            depth_first=depth_first,
            include_root=True,
        )
    if depth_first and include_root:
        fn(module=module, name=name)
    return module</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.nchw_to"><code class="name flex">
<span>def <span class="ident">nchw_to</span></span>(<span>x: torch.Tensor,<br>fmt: <a title="sglang.srt.models.deepseek_janus_pro.Format" href="#sglang.srt.models.deepseek_janus_pro.Format">Format</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def nchw_to(x: torch.Tensor, fmt: Format):
    if fmt == Format.NHWC:
        x = x.permute(0, 2, 3, 1)
    elif fmt == Format.NLC:
        x = x.flatten(2).transpose(1, 2)
    elif fmt == Format.NCL:
        x = x.flatten(2)
    return x</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.nonlinearity"><code class="name flex">
<span>def <span class="ident">nonlinearity</span></span>(<span>x)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def nonlinearity(x):
    # swish
    return x * torch.sigmoid(x)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.resample_abs_pos_embed"><code class="name flex">
<span>def <span class="ident">resample_abs_pos_embed</span></span>(<span>posemb: torch.Tensor,<br>new_size: List[int],<br>old_size: List[int] | None = None,<br>num_prefix_tokens: int = 1,<br>interpolation: str = 'bicubic',<br>antialias: bool = True,<br>verbose: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resample_abs_pos_embed(
    posemb: torch.Tensor,
    new_size: List[int],
    old_size: Optional[List[int]] = None,
    num_prefix_tokens: int = 1,
    interpolation: str = &#34;bicubic&#34;,
    antialias: bool = True,
    verbose: bool = False,
):
    # sort out sizes, assume square if old size not provided
    num_pos_tokens = posemb.shape[1]
    num_new_tokens = new_size[0] * new_size[1] + num_prefix_tokens
    if num_new_tokens == num_pos_tokens and new_size[0] == new_size[1]:
        return posemb

    if old_size is None:
        hw = int(math.sqrt(num_pos_tokens - num_prefix_tokens))
        old_size = hw, hw

    if num_prefix_tokens:
        posemb_prefix, posemb = (
            posemb[:, :num_prefix_tokens],
            posemb[:, num_prefix_tokens:],
        )
    else:
        posemb_prefix, posemb = None, posemb

    # do the interpolation
    embed_dim = posemb.shape[-1]
    orig_dtype = posemb.dtype
    posemb = posemb.float()  # interpolate needs float32
    posemb = posemb.reshape(1, old_size[0], old_size[1], -1).permute(0, 3, 1, 2)
    posemb = F.interpolate(
        posemb, size=new_size, mode=interpolation, antialias=antialias
    )
    posemb = posemb.permute(0, 2, 3, 1).reshape(1, -1, embed_dim)
    posemb = posemb.to(orig_dtype)

    # add back extra (class, etc) prefix tokens
    if posemb_prefix is not None:
        posemb = torch.cat([posemb_prefix, posemb], dim=1)

    if not torch.jit.is_scripting() and verbose:
        logger.info(f&#34;Resized position embedding: {old_size} to {new_size}.&#34;)

    return posemb</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.resample_patch_embed"><code class="name flex">
<span>def <span class="ident">resample_patch_embed</span></span>(<span>patch_embed,<br>new_size: List[int],<br>interpolation: str = 'bicubic',<br>antialias: bool = True,<br>verbose: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resample_patch_embed(
    patch_embed,
    new_size: List[int],
    interpolation: str = &#34;bicubic&#34;,
    antialias: bool = True,
    verbose: bool = False,
):
    &#34;&#34;&#34;Resample the weights of the patch embedding kernel to target resolution.
    We resample the patch embedding kernel by approximately inverting the effect
    of patch resizing.

    Code based on:
      https://github.com/google-research/big_vision/blob/b00544b81f8694488d5f36295aeb7972f3755ffe/big_vision/models/proj/flexi/vit.py

    With this resizing, we can for example load a B/8 filter into a B/16 model
    and, on 2x larger input image, the result will match.

    Args:
        patch_embed: original parameter to be resized.
        new_size (tuple(int, int): target shape (height, width)-only.
        interpolation (str): interpolation for resize
        antialias (bool): use anti-aliasing filter in resize
        verbose (bool): log operation
    Returns:
        Resized patch embedding kernel.
    &#34;&#34;&#34;
    import numpy as np

    try:
        from torch import vmap
    except ImportError:
        from torch.func import vmap

    assert len(patch_embed.shape) == 4, &#34;Four dimensions expected&#34;
    assert len(new_size) == 2, &#34;New shape should only be hw&#34;
    old_size = patch_embed.shape[-2:]
    if tuple(old_size) == tuple(new_size):
        return patch_embed

    if verbose:
        logger.info(
            f&#34;Resize patch embedding {patch_embed.shape} to {new_size}, w/ {interpolation} interpolation.&#34;
        )

    def resize(x_np, _new_size):
        x_tf = torch.Tensor(x_np)[None, None, ...]
        x_upsampled = F.interpolate(
            x_tf, size=_new_size, mode=interpolation, antialias=antialias
        )[0, 0, ...].numpy()
        return x_upsampled

    def get_resize_mat(_old_size, _new_size):
        mat = []
        for i in range(np.prod(_old_size)):
            basis_vec = np.zeros(_old_size)
            basis_vec[np.unravel_index(i, _old_size)] = 1.0
            mat.append(resize(basis_vec, _new_size).reshape(-1))
        return np.stack(mat).T

    resize_mat = get_resize_mat(old_size, new_size)
    resize_mat_pinv = torch.tensor(
        np.linalg.pinv(resize_mat.T), device=patch_embed.device
    )

    def resample_kernel(kernel):
        resampled_kernel = resize_mat_pinv @ kernel.reshape(-1)
        return resampled_kernel.reshape(new_size)

    v_resample_kernel = vmap(vmap(resample_kernel, 0, 0), 1, 1)
    orig_dtype = patch_embed.dtype
    patch_embed = patch_embed.float()
    patch_embed = v_resample_kernel(patch_embed)
    patch_embed = patch_embed.to(orig_dtype)
    return patch_embed</code></pre>
</details>
<div class="desc"><p>Resample the weights of the patch embedding kernel to target resolution.
We resample the patch embedding kernel by approximately inverting the effect
of patch resizing.</p>
<p>Code based on:
<a href="https://github.com/google-research/big_vision/blob/b00544b81f8694488d5f36295aeb7972f3755ffe/big_vision/models/proj/flexi/vit.py">https://github.com/google-research/big_vision/blob/b00544b81f8694488d5f36295aeb7972f3755ffe/big_vision/models/proj/flexi/vit.py</a></p>
<p>With this resizing, we can for example load a B/8 filter into a B/16 model
and, on 2x larger input image, the result will match.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>patch_embed</code></strong></dt>
<dd>original parameter to be resized.</dd>
<dt>new_size (tuple(int, int): target shape (height, width)-only.</dt>
<dt><strong><code>interpolation</code></strong> :&ensp;<code>str</code></dt>
<dd>interpolation for resize</dd>
<dt><strong><code>antialias</code></strong> :&ensp;<code>bool</code></dt>
<dd>use anti-aliasing filter in resize</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>log operation</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Resized patch embedding kernel.</p></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.to_2tuple"><code class="name flex">
<span>def <span class="ident">to_2tuple</span></span>(<span>x)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse(x):
    if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):
        return tuple(x)
    return tuple(repeat(x, n))</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.trunc_normal_tf_"><code class="name flex">
<span>def <span class="ident">trunc_normal_tf_</span></span>(<span>tensor: torch.Tensor,<br>mean: float = 0.0,<br>std: float = 1.0,<br>a: float = -2.0,<br>b: float = 2.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def trunc_normal_tf_(
    tensor: torch.Tensor,
    mean: float = 0.0,
    std: float = 1.0,
    a: float = -2.0,
    b: float = 2.0,
):
    &#34;&#34;&#34;Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \\leq \text{mean} \\leq b`.
    NOTE: this &#39;tf&#39; variant behaves closer to Tensorflow / JAX impl where the
    bounds [a, b] are applied when sampling the normal distribution with mean=0, std=1.0
    and the result is subsequently scaled and shifted by the mean and std args.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    &#34;&#34;&#34;
    with torch.no_grad():
        _trunc_normal_(tensor, 0, 1.0, a, b)
        tensor.mul_(std).add_(mean)</code></pre>
</details>
<div class="desc"><p>Fills the input Tensor with values drawn from a truncated
normal distribution. The values are effectively drawn from the
normal distribution :math:<code>\mathcal{N}(
ext{mean},
ext{std}^2)</code>
with values outside :math:<code>[a, b]</code> redrawn until they are within
the bounds. The method used for generating the random values works
best when :math:<code>a \leq
ext{mean} \leq b</code>.
NOTE: this 'tf' variant behaves closer to Tensorflow / JAX impl where the
bounds [a, b] are applied when sampling the normal distribution with mean=0, std=1.0
and the result is subsequently scaled and shifted by the mean and std args.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensor</code></strong></dt>
<dd>an n-dimensional <code>torch.Tensor</code></dd>
<dt><strong><code>mean</code></strong></dt>
<dd>the mean of the normal distribution</dd>
<dt><strong><code>std</code></strong></dt>
<dd>the standard deviation of the normal distribution</dd>
<dt><strong><code>a</code></strong></dt>
<dd>the minimum cutoff value</dd>
<dt><strong><code>b</code></strong></dt>
<dd>the maximum cutoff value</dd>
</dl></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.use_fused_attn"><code class="name flex">
<span>def <span class="ident">use_fused_attn</span></span>(<span>experimental: bool = False) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def use_fused_attn(experimental: bool = False) -&gt; bool:
    # NOTE: ONNX export cannot handle F.scaled_dot_product_attention as of pytorch 2.0
    if not _HAS_FUSED_ATTN or _EXPORTABLE:
        return False
    if experimental:
        return _USE_FUSED_ATTN &gt; 1
    return _USE_FUSED_ATTN &gt; 0</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.AttentionPoolLatent"><code class="flex name class">
<span>class <span class="ident">AttentionPoolLatent</span></span>
<span>(</span><span>in_features: int,<br>out_features: int = None,<br>embed_dim: int = None,<br>num_heads: int = 8,<br>feat_size: int | None = None,<br>mlp_ratio: float = 4.0,<br>qkv_bias: bool = True,<br>qk_norm: bool = False,<br>latent_len: int = 1,<br>latent_dim: int = None,<br>pos_embed: str = '',<br>pool_type: str = 'token',<br>norm_layer: torch.nn.modules.module.Module | None = None,<br>drop: float = 0.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AttentionPoolLatent(nn.Module):
    &#34;&#34;&#34;Attention pooling w/ latent query&#34;&#34;&#34;

    fused_attn: torch.jit.Final[bool]

    def __init__(
        self,
        in_features: int,
        out_features: int = None,
        embed_dim: int = None,
        num_heads: int = 8,
        feat_size: Optional[int] = None,
        mlp_ratio: float = 4.0,
        qkv_bias: bool = True,
        qk_norm: bool = False,
        latent_len: int = 1,
        latent_dim: int = None,
        pos_embed: str = &#34;&#34;,
        pool_type: str = &#34;token&#34;,
        norm_layer: Optional[nn.Module] = None,
        drop: float = 0.0,
    ):
        super().__init__()
        embed_dim = embed_dim or in_features
        out_features = out_features or in_features
        assert embed_dim % num_heads == 0
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.feat_size = feat_size
        self.scale = self.head_dim**-0.5
        self.pool = pool_type
        self.fused_attn = use_fused_attn()

        if pos_embed == &#34;abs&#34;:
            assert feat_size is not None
            self.pos_embed = nn.Parameter(torch.zeros(feat_size, in_features))
        else:
            self.pos_embed = None

        self.latent_dim = latent_dim or embed_dim
        self.latent_len = latent_len
        self.latent = nn.Parameter(torch.zeros(1, self.latent_len, embed_dim))

        self.q = nn.Linear(embed_dim, embed_dim, bias=qkv_bias)
        self.kv = nn.Linear(embed_dim, embed_dim * 2, bias=qkv_bias)
        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()
        self.proj = nn.Linear(embed_dim, embed_dim)
        self.proj_drop = nn.Dropout(drop)

        self.norm = (
            norm_layer(out_features) if norm_layer is not None else nn.Identity()
        )
        self.mlp = Mlp(embed_dim, int(embed_dim * mlp_ratio))

        self.init_weights()

    def init_weights(self):
        if self.pos_embed is not None:
            trunc_normal_tf_(self.pos_embed, std=self.pos_embed.shape[1] ** -0.5)
        trunc_normal_tf_(self.latent, std=self.latent_dim**-0.5)

    def forward(self, x):
        B, N, C = x.shape

        if self.pos_embed is not None:
            # FIXME interpolate
            x = x + self.pos_embed.unsqueeze(0).to(x.dtype)

        q_latent = self.latent.expand(B, -1, -1)
        q = (
            self.q(q_latent)
            .reshape(B, self.latent_len, self.num_heads, self.head_dim)
            .transpose(1, 2)
        )

        kv = (
            self.kv(x)
            .reshape(B, N, 2, self.num_heads, self.head_dim)
            .permute(2, 0, 3, 1, 4)
        )
        k, v = kv.unbind(0)

        q, k = self.q_norm(q), self.k_norm(k)

        if self.fused_attn:
            x = F.scaled_dot_product_attention(q, k, v)
        else:
            q = q * self.scale
            attn = q @ k.transpose(-2, -1)
            attn = attn.softmax(dim=-1)
            x = attn @ v
        x = x.transpose(1, 2).reshape(B, self.latent_len, C)
        x = self.proj(x)
        x = self.proj_drop(x)

        x = x + self.mlp(self.norm(x))

        # optional pool if latent seq_len &gt; 1 and pooled output is desired
        if self.pool == &#34;token&#34;:
            x = x[:, 0]
        elif self.pool == &#34;avg&#34;:
            x = x.mean(1)</code></pre>
</details>
<div class="desc"><p>Attention pooling w/ latent query</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.AttentionPoolLatent.fused_attn"><code class="name">var <span class="ident">fused_attn</span> : Final[bool]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.AttentionPoolLatent.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    B, N, C = x.shape

    if self.pos_embed is not None:
        # FIXME interpolate
        x = x + self.pos_embed.unsqueeze(0).to(x.dtype)

    q_latent = self.latent.expand(B, -1, -1)
    q = (
        self.q(q_latent)
        .reshape(B, self.latent_len, self.num_heads, self.head_dim)
        .transpose(1, 2)
    )

    kv = (
        self.kv(x)
        .reshape(B, N, 2, self.num_heads, self.head_dim)
        .permute(2, 0, 3, 1, 4)
    )
    k, v = kv.unbind(0)

    q, k = self.q_norm(q), self.k_norm(k)

    if self.fused_attn:
        x = F.scaled_dot_product_attention(q, k, v)
    else:
        q = q * self.scale
        attn = q @ k.transpose(-2, -1)
        attn = attn.softmax(dim=-1)
        x = attn @ v
    x = x.transpose(1, 2).reshape(B, self.latent_len, C)
    x = self.proj(x)
    x = self.proj_drop(x)

    x = x + self.mlp(self.norm(x))

    # optional pool if latent seq_len &gt; 1 and pooled output is desired
    if self.pool == &#34;token&#34;:
        x = x[:, 0]
    elif self.pool == &#34;avg&#34;:
        x = x.mean(1)</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.AttentionPoolLatent.init_weights"><code class="name flex">
<span>def <span class="ident">init_weights</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_weights(self):
    if self.pos_embed is not None:
        trunc_normal_tf_(self.pos_embed, std=self.pos_embed.shape[1] ** -0.5)
    trunc_normal_tf_(self.latent, std=self.latent_dim**-0.5)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.AttnBlock"><code class="flex name class">
<span>class <span class="ident">AttnBlock</span></span>
<span>(</span><span>in_channels, norm_type='group')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AttnBlock(nn.Module):
    def __init__(self, in_channels, norm_type=&#34;group&#34;):
        super().__init__()
        self.norm = Normalize(in_channels, norm_type)
        self.q = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)
        self.k = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)
        self.v = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)
        self.proj_out = nn.Conv2d(
            in_channels, in_channels, kernel_size=1, stride=1, padding=0
        )

    def forward(self, x):
        h_ = x
        h_ = self.norm(h_)
        q = self.q(h_)
        k = self.k(h_)
        v = self.v(h_)

        # compute attention
        b, c, h, w = q.shape
        q = q.reshape(b, c, h * w)
        q = q.permute(0, 2, 1)  # b,hw,c
        k = k.reshape(b, c, h * w)  # b,c,hw
        w_ = torch.bmm(q, k)  # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]
        w_ = w_ * (int(c) ** (-0.5))
        w_ = F.softmax(w_, dim=2)

        # attend to values
        v = v.reshape(b, c, h * w)
        w_ = w_.permute(0, 2, 1)  # b,hw,hw (first hw of k, second of q)
        h_ = torch.bmm(v, w_)  # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]
        h_ = h_.reshape(b, c, h, w)

        h_ = self.proj_out(h_)

        return x + h_</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.AttnBlock.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    h_ = x
    h_ = self.norm(h_)
    q = self.q(h_)
    k = self.k(h_)
    v = self.v(h_)

    # compute attention
    b, c, h, w = q.shape
    q = q.reshape(b, c, h * w)
    q = q.permute(0, 2, 1)  # b,hw,c
    k = k.reshape(b, c, h * w)  # b,c,hw
    w_ = torch.bmm(q, k)  # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]
    w_ = w_ * (int(c) ** (-0.5))
    w_ = F.softmax(w_, dim=2)

    # attend to values
    v = v.reshape(b, c, h * w)
    w_ = w_.permute(0, 2, 1)  # b,hw,hw (first hw of k, second of q)
    h_ = torch.bmm(v, w_)  # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]
    h_ = h_.reshape(b, c, h, w)

    h_ = self.proj_out(h_)

    return x + h_</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.CLIPVisionTower"><code class="flex name class">
<span>class <span class="ident">CLIPVisionTower</span></span>
<span>(</span><span>model_name: str = 'siglip_large_patch16_384',<br>image_size: Tuple[int, int] | int = 336,<br>select_feature: str = 'patch',<br>select_layer: int = -2,<br>select_layers: list = None,<br>ckpt_path: str = '',<br>pixel_mean: List[float] | None = None,<br>pixel_std: List[float] | None = None,<br>**kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CLIPVisionTower(nn.Module):
    def __init__(
        self,
        model_name: str = &#34;siglip_large_patch16_384&#34;,
        image_size: Union[Tuple[int, int], int] = 336,
        select_feature: str = &#34;patch&#34;,
        select_layer: int = -2,
        select_layers: list = None,
        ckpt_path: str = &#34;&#34;,
        pixel_mean: Optional[List[float]] = None,
        pixel_std: Optional[List[float]] = None,
        **kwargs,
    ):
        super().__init__()

        self.model_name = model_name
        self.select_feature = select_feature
        self.select_layer = select_layer
        self.select_layers = select_layers

        vision_tower_params = {
            &#34;model_name&#34;: model_name,
            &#34;image_size&#34;: image_size,
            &#34;ckpt_path&#34;: ckpt_path,
            &#34;select_layer&#34;: select_layer,
        }
        vision_tower_params.update(kwargs)
        self.vision_tower, self.forward_kwargs = self.build_vision_tower(
            vision_tower_params
        )

        if pixel_mean is not None and pixel_std is not None:
            image_norm = Normalize(mean=pixel_mean, std=pixel_std)
        else:
            image_norm = None

        self.image_norm = image_norm

    @property
    def device(self) -&gt; torch.device:
        return next(self.vision_tower.parameters()).device

    @property
    def dtype(self):
        return next(self.vision_tower.parameters()).dtype

    def build_vision_tower(self, vision_tower_params):
        if self.model_name.startswith(&#34;siglip&#34;):
            self.select_feature = &#34;same&#34;
            vision_tower = create_siglip_vit(**vision_tower_params)
            forward_kwargs = dict()

        elif self.model_name.startswith(&#34;sam&#34;):
            # vision_tower = create_sam_vit(**vision_tower_params)
            forward_kwargs = dict()

        else:  # huggingface
            from transformers import CLIPVisionModel

            vision_tower = CLIPVisionModel.from_pretrained(**vision_tower_params)
            forward_kwargs = dict(output_hidden_states=True)

        return vision_tower, forward_kwargs

    def feature_select(self, image_forward_outs):
        if isinstance(image_forward_outs, torch.Tensor):
            # the output has been the self.select_layer&#34;s features
            image_features = image_forward_outs
        else:
            image_features = image_forward_outs.hidden_states[self.select_layer]

        if self.select_feature == &#34;patch&#34;:
            # if the output has cls_token
            image_features = image_features[:, 1:]
        elif self.select_feature == &#34;cls_patch&#34;:
            image_features = image_features
        elif self.select_feature == &#34;same&#34;:
            image_features = image_features

        else:
            raise ValueError(f&#34;Unexpected select feature: {self.select_feature}&#34;)
        return image_features

    def forward(self, images):
        &#34;&#34;&#34;

        Args:
            images (torch.Tensor): [b, 3, H, W]

        Returns:
            image_features (torch.Tensor): [b, n_patch, d]
        &#34;&#34;&#34;

        if self.image_norm is not None:
            images = self.image_norm(images)

        image_forward_outs = self.vision_tower(images, **self.forward_kwargs)
        image_features = self.feature_select(image_forward_outs)
        return image_features</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.CLIPVisionTower.device"><code class="name">prop <span class="ident">device</span> : torch.device</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def device(self) -&gt; torch.device:
    return next(self.vision_tower.parameters()).device</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.CLIPVisionTower.dtype"><code class="name">prop <span class="ident">dtype</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dtype(self):
    return next(self.vision_tower.parameters()).dtype</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.CLIPVisionTower.build_vision_tower"><code class="name flex">
<span>def <span class="ident">build_vision_tower</span></span>(<span>self, vision_tower_params)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_vision_tower(self, vision_tower_params):
    if self.model_name.startswith(&#34;siglip&#34;):
        self.select_feature = &#34;same&#34;
        vision_tower = create_siglip_vit(**vision_tower_params)
        forward_kwargs = dict()

    elif self.model_name.startswith(&#34;sam&#34;):
        # vision_tower = create_sam_vit(**vision_tower_params)
        forward_kwargs = dict()

    else:  # huggingface
        from transformers import CLIPVisionModel

        vision_tower = CLIPVisionModel.from_pretrained(**vision_tower_params)
        forward_kwargs = dict(output_hidden_states=True)

    return vision_tower, forward_kwargs</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.CLIPVisionTower.feature_select"><code class="name flex">
<span>def <span class="ident">feature_select</span></span>(<span>self, image_forward_outs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def feature_select(self, image_forward_outs):
    if isinstance(image_forward_outs, torch.Tensor):
        # the output has been the self.select_layer&#34;s features
        image_features = image_forward_outs
    else:
        image_features = image_forward_outs.hidden_states[self.select_layer]

    if self.select_feature == &#34;patch&#34;:
        # if the output has cls_token
        image_features = image_features[:, 1:]
    elif self.select_feature == &#34;cls_patch&#34;:
        image_features = image_features
    elif self.select_feature == &#34;same&#34;:
        image_features = image_features

    else:
        raise ValueError(f&#34;Unexpected select feature: {self.select_feature}&#34;)
    return image_features</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.CLIPVisionTower.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, images) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, images):
    &#34;&#34;&#34;

    Args:
        images (torch.Tensor): [b, 3, H, W]

    Returns:
        image_features (torch.Tensor): [b, n_patch, d]
    &#34;&#34;&#34;

    if self.image_norm is not None:
        images = self.image_norm(images)

    image_forward_outs = self.vision_tower(images, **self.forward_kwargs)
    image_features = self.feature_select(image_forward_outs)
    return image_features</code></pre>
</details>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>images</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>[b, 3, H, W]</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>image_features (torch.Tensor): [b, n_patch, d]</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.Decoder"><code class="flex name class">
<span>class <span class="ident">Decoder</span></span>
<span>(</span><span>z_channels=256,<br>ch=128,<br>ch_mult=(1, 1, 2, 2, 4),<br>num_res_blocks=2,<br>norm_type='group',<br>dropout=0.0,<br>resamp_with_conv=True,<br>out_channels=3)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Decoder(nn.Module):
    def __init__(
        self,
        z_channels=256,
        ch=128,
        ch_mult=(1, 1, 2, 2, 4),
        num_res_blocks=2,
        norm_type=&#34;group&#34;,
        dropout=0.0,
        resamp_with_conv=True,
        out_channels=3,
    ):
        super().__init__()
        self.num_resolutions = len(ch_mult)
        self.num_res_blocks = num_res_blocks

        block_in = ch * ch_mult[self.num_resolutions - 1]
        # z to block_in
        self.conv_in = nn.Conv2d(
            z_channels, block_in, kernel_size=3, stride=1, padding=1
        )

        # middle
        self.mid = nn.ModuleList()
        self.mid.append(
            ResnetBlock(block_in, block_in, dropout=dropout, norm_type=norm_type)
        )
        self.mid.append(AttnBlock(block_in, norm_type=norm_type))
        self.mid.append(
            ResnetBlock(block_in, block_in, dropout=dropout, norm_type=norm_type)
        )

        # upsampling
        self.conv_blocks = nn.ModuleList()
        for i_level in reversed(range(self.num_resolutions)):
            conv_block = nn.Module()
            # res &amp; attn
            res_block = nn.ModuleList()
            attn_block = nn.ModuleList()
            block_out = ch * ch_mult[i_level]
            for _ in range(self.num_res_blocks + 1):
                res_block.append(
                    ResnetBlock(
                        block_in, block_out, dropout=dropout, norm_type=norm_type
                    )
                )
                block_in = block_out
                if i_level == self.num_resolutions - 1:
                    attn_block.append(AttnBlock(block_in, norm_type))
            conv_block.res = res_block
            conv_block.attn = attn_block
            # downsample
            if i_level != 0:
                conv_block.upsample = Upsample(block_in, resamp_with_conv)
            self.conv_blocks.append(conv_block)

        # end
        self.norm_out = Normalize(block_in, norm_type)
        self.conv_out = nn.Conv2d(
            block_in, out_channels, kernel_size=3, stride=1, padding=1
        )

    @property
    def last_layer(self):
        return self.conv_out.weight

    def forward(self, z):
        # z to block_in
        h = self.conv_in(z)

        # middle
        for mid_block in self.mid:
            h = mid_block(h)

        # upsampling
        for i_level, block in enumerate(self.conv_blocks):
            for i_block in range(self.num_res_blocks + 1):
                h = block.res[i_block](h)
                if len(block.attn) &gt; 0:
                    h = block.attn[i_block](h)
            if i_level != self.num_resolutions - 1:
                h = block.upsample(h)

        # end
        h = self.norm_out(h)
        h = nonlinearity(h)
        h = self.conv_out(h)
        return h</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.Decoder.last_layer"><code class="name">prop <span class="ident">last_layer</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def last_layer(self):
    return self.conv_out.weight</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.Decoder.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, z) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, z):
    # z to block_in
    h = self.conv_in(z)

    # middle
    for mid_block in self.mid:
        h = mid_block(h)

    # upsampling
    for i_level, block in enumerate(self.conv_blocks):
        for i_block in range(self.num_res_blocks + 1):
            h = block.res[i_block](h)
            if len(block.attn) &gt; 0:
                h = block.attn[i_block](h)
        if i_level != self.num_resolutions - 1:
            h = block.upsample(h)

    # end
    h = self.norm_out(h)
    h = nonlinearity(h)
    h = self.conv_out(h)
    return h</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.Downsample"><code class="flex name class">
<span>class <span class="ident">Downsample</span></span>
<span>(</span><span>in_channels, with_conv)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Downsample(nn.Module):
    def __init__(self, in_channels, with_conv):
        super().__init__()
        self.with_conv = with_conv
        if self.with_conv:
            # no asymmetric padding in torch conv, must do it ourselves
            self.conv = nn.Conv2d(
                in_channels, in_channels, kernel_size=3, stride=2, padding=0
            )

    def forward(self, x):
        if self.with_conv:
            pad = (0, 1, 0, 1)
            x = F.pad(x, pad, mode=&#34;constant&#34;, value=0)
            x = self.conv(x)
        else:
            x = F.avg_pool2d(x, kernel_size=2, stride=2)
        return x</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.Downsample.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    if self.with_conv:
        pad = (0, 1, 0, 1)
        x = F.pad(x, pad, mode=&#34;constant&#34;, value=0)
        x = self.conv(x)
    else:
        x = F.avg_pool2d(x, kernel_size=2, stride=2)
    return x</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.DropPath"><code class="flex name class">
<span>class <span class="ident">DropPath</span></span>
<span>(</span><span>drop_prob: float = 0.0, scale_by_keep: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DropPath(nn.Module):
    &#34;&#34;&#34;Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).&#34;&#34;&#34;

    def __init__(self, drop_prob: float = 0.0, scale_by_keep: bool = True):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob
        self.scale_by_keep = scale_by_keep

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)

    def extra_repr(self):
        return f&#34;drop_prob={round(self.drop_prob, 3):0.3f}&#34;</code></pre>
</details>
<div class="desc"><p>Drop paths (Stochastic Depth) per sample
(when applied in main path of residual blocks).</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.DropPath.extra_repr"><code class="name flex">
<span>def <span class="ident">extra_repr</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extra_repr(self):
    return f&#34;drop_prob={round(self.drop_prob, 3):0.3f}&#34;</code></pre>
</details>
<div class="desc"><p>Return the extra representation of the module.</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.DropPath.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.Encoder"><code class="flex name class">
<span>class <span class="ident">Encoder</span></span>
<span>(</span><span>in_channels=3,<br>ch=128,<br>ch_mult=(1, 1, 2, 2, 4),<br>num_res_blocks=2,<br>norm_type='group',<br>dropout=0.0,<br>resamp_with_conv=True,<br>z_channels=256)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Encoder(nn.Module):
    def __init__(
        self,
        in_channels=3,
        ch=128,
        ch_mult=(1, 1, 2, 2, 4),
        num_res_blocks=2,
        norm_type=&#34;group&#34;,
        dropout=0.0,
        resamp_with_conv=True,
        z_channels=256,
    ):
        super().__init__()
        self.num_resolutions = len(ch_mult)
        self.num_res_blocks = num_res_blocks
        self.conv_in = nn.Conv2d(in_channels, ch, kernel_size=3, stride=1, padding=1)

        # downsampling
        in_ch_mult = (1,) + tuple(ch_mult)
        self.conv_blocks = nn.ModuleList()
        for i_level in range(self.num_resolutions):
            conv_block = nn.Module()
            # res &amp; attn
            res_block = nn.ModuleList()
            attn_block = nn.ModuleList()
            block_in = ch * in_ch_mult[i_level]
            block_out = ch * ch_mult[i_level]
            for _ in range(self.num_res_blocks):
                res_block.append(
                    ResnetBlock(
                        block_in, block_out, dropout=dropout, norm_type=norm_type
                    )
                )
                block_in = block_out
                if i_level == self.num_resolutions - 1:
                    attn_block.append(AttnBlock(block_in, norm_type))
            conv_block.res = res_block
            conv_block.attn = attn_block
            # downsample
            if i_level != self.num_resolutions - 1:
                conv_block.downsample = Downsample(block_in, resamp_with_conv)
            self.conv_blocks.append(conv_block)

        # middle
        self.mid = nn.ModuleList()
        self.mid.append(
            ResnetBlock(block_in, block_in, dropout=dropout, norm_type=norm_type)
        )
        self.mid.append(AttnBlock(block_in, norm_type=norm_type))
        self.mid.append(
            ResnetBlock(block_in, block_in, dropout=dropout, norm_type=norm_type)
        )

        # end
        self.norm_out = Normalize(block_in, norm_type)
        self.conv_out = nn.Conv2d(
            block_in, z_channels, kernel_size=3, stride=1, padding=1
        )

    def forward(self, x):
        h = self.conv_in(x)
        # downsampling
        for i_level, block in enumerate(self.conv_blocks):
            for i_block in range(self.num_res_blocks):
                h = block.res[i_block](h)
                if len(block.attn) &gt; 0:
                    h = block.attn[i_block](h)
            if i_level != self.num_resolutions - 1:
                h = block.downsample(h)

        # middle
        for mid_block in self.mid:
            h = mid_block(h)

        # end
        h = self.norm_out(h)
        h = nonlinearity(h)
        h = self.conv_out(h)
        return h</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.Encoder.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    h = self.conv_in(x)
    # downsampling
    for i_level, block in enumerate(self.conv_blocks):
        for i_block in range(self.num_res_blocks):
            h = block.res[i_block](h)
            if len(block.attn) &gt; 0:
                h = block.attn[i_block](h)
        if i_level != self.num_resolutions - 1:
            h = block.downsample(h)

    # middle
    for mid_block in self.mid:
        h = mid_block(h)

    # end
    h = self.norm_out(h)
    h = nonlinearity(h)
    h = self.conv_out(h)
    return h</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.Format"><code class="flex name class">
<span>class <span class="ident">Format</span></span>
<span>(</span><span>*args, **kwds)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Format(str, Enum):
    NCHW = &#34;NCHW&#34;
    NHWC = &#34;NHWC&#34;
    NCL = &#34;NCL&#34;
    NLC = &#34;NLC&#34;</code></pre>
</details>
<div class="desc"><p>str(object='') -&gt; str
str(bytes_or_buffer[, encoding[, errors]]) -&gt; str</p>
<p>Create a new string object from the given object. If encoding or
errors is specified, then the object must expose a data buffer
that will be decoded using the given encoding and error handler.
Otherwise, returns the result of object.<strong>str</strong>() (if defined)
or repr(object).
encoding defaults to sys.getdefaultencoding().
errors defaults to 'strict'.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.str</li>
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.Format.NCHW"><code class="name">var <span class="ident">NCHW</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.Format.NCL"><code class="name">var <span class="ident">NCL</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.Format.NHWC"><code class="name">var <span class="ident">NHWC</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.Format.NLC"><code class="name">var <span class="ident">NLC</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.LayerScale"><code class="flex name class">
<span>class <span class="ident">LayerScale</span></span>
<span>(</span><span>dim: int, init_values: float = 1e-05, inplace: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LayerScale(nn.Module):
    def __init__(
        self,
        dim: int,
        init_values: float = 1e-5,
        inplace: bool = False,
    ) -&gt; None:
        super().__init__()
        self.inplace = inplace
        self.gamma = nn.Parameter(init_values * torch.ones(dim))

    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        return x.mul_(self.gamma) if self.inplace else x * self.gamma</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.LayerScale.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
    return x.mul_(self.gamma) if self.inplace else x * self.gamma</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.Mlp"><code class="flex name class">
<span>class <span class="ident">Mlp</span></span>
<span>(</span><span>in_features,<br>hidden_features=None,<br>out_features=None,<br>act_layer=torch.nn.modules.activation.GELU,<br>norm_layer=None,<br>bias=True,<br>drop=0.0,<br>use_conv=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Mlp(nn.Module):
    &#34;&#34;&#34;MLP as used in Vision Transformer, MLP-Mixer and related networks

    NOTE: When use_conv=True, expects 2D NCHW tensors, otherwise N*C expected.
    &#34;&#34;&#34;

    def __init__(
        self,
        in_features,
        hidden_features=None,
        out_features=None,
        act_layer=nn.GELU,
        norm_layer=None,
        bias=True,
        drop=0.0,
        use_conv=False,
    ):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        bias = to_2tuple(bias)
        drop_probs = to_2tuple(drop)
        linear_layer = partial(nn.Conv2d, kernel_size=1) if use_conv else nn.Linear

        self.fc1 = linear_layer(in_features, hidden_features, bias=bias[0])
        self.act = act_layer()
        self.drop1 = nn.Dropout(drop_probs[0])
        self.norm = (
            norm_layer(hidden_features) if norm_layer is not None else nn.Identity()
        )
        self.fc2 = linear_layer(hidden_features, out_features, bias=bias[1])
        self.drop2 = nn.Dropout(drop_probs[1])

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop1(x)
        x = self.norm(x)
        x = self.fc2(x)
        x = self.drop2(x)
        return x</code></pre>
</details>
<div class="desc"><p>MLP as used in Vision Transformer, MLP-Mixer and related networks</p>
<p>NOTE: When use_conv=True, expects 2D NCHW tensors, otherwise N*C expected.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.Mlp.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    x = self.fc1(x)
    x = self.act(x)
    x = self.drop1(x)
    x = self.norm(x)
    x = self.fc2(x)
    x = self.drop2(x)
    return x</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.MlpProjector"><code class="flex name class">
<span>class <span class="ident">MlpProjector</span></span>
<span>(</span><span>cfg)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MlpProjector(nn.Module):
    def __init__(self, cfg):
        super().__init__()

        self.cfg = cfg

        if cfg[&#34;projector_type&#34;] == &#34;identity&#34;:
            modules = nn.Identity()

        elif cfg[&#34;projector_type&#34;] == &#34;linear&#34;:
            modules = nn.Linear(cfg[&#34;input_dim&#34;], cfg[&#34;n_embed&#34;])

        elif cfg[&#34;projector_type&#34;] == &#34;mlp_gelu&#34;:
            mlp_depth = cfg.get(&#34;depth&#34;, 1)
            modules = [nn.Linear(cfg[&#34;input_dim&#34;], cfg[&#34;n_embed&#34;])]
            for _ in range(1, mlp_depth):
                modules.append(nn.GELU())
                modules.append(nn.Linear(cfg[&#34;n_embed&#34;], cfg[&#34;n_embed&#34;]))
            modules = nn.Sequential(*modules)

        elif cfg[&#34;projector_type&#34;] == &#34;low_high_hybrid_split_mlp_gelu&#34;:
            mlp_depth = cfg.get(&#34;depth&#34;, 1)
            self.high_up_proj = nn.Linear(cfg[&#34;input_dim&#34;], cfg[&#34;n_embed&#34;] // 2)
            self.low_up_proj = nn.Linear(cfg[&#34;input_dim&#34;], cfg[&#34;n_embed&#34;] // 2)

            modules = []
            for _ in range(1, mlp_depth):
                modules.append(nn.GELU())
                modules.append(nn.Linear(cfg[&#34;n_embed&#34;], cfg[&#34;n_embed&#34;]))
            modules = nn.Sequential(*modules)

        else:
            raise ValueError(f&#34;Unknown projector type: {cfg[&#39;projector_type&#39;]}&#34;)

        self.layers = modules

    def forward(
        self, x_or_tuple: Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]
    ):
        &#34;&#34;&#34;

        Args:
            x_or_tuple (Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:  if it is a tuple of torch.Tensor,
                then it comes from the hybrid vision encoder, and x = high_res_x, low_res_x);
                otherwise it is the feature from the single vision encoder.

        Returns:
            x (torch.Tensor): [b, s, c]
        &#34;&#34;&#34;

        if isinstance(x_or_tuple, tuple):
            # self.cfg.projector_type == &#34;low_high_hybrid_split_mlp_gelu&#34;:
            high_x, low_x = x_or_tuple
            high_x = self.high_up_proj(high_x)
            low_x = self.low_up_proj(low_x)
            x = torch.cat([high_x, low_x], dim=-1)
        else:
            x = x_or_tuple

        return self.layers(x)</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.MlpProjector.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x_or_tuple: Tuple[torch.Tensor, torch.Tensor] | torch.Tensor) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, x_or_tuple: Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]
):
    &#34;&#34;&#34;

    Args:
        x_or_tuple (Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:  if it is a tuple of torch.Tensor,
            then it comes from the hybrid vision encoder, and x = high_res_x, low_res_x);
            otherwise it is the feature from the single vision encoder.

    Returns:
        x (torch.Tensor): [b, s, c]
    &#34;&#34;&#34;

    if isinstance(x_or_tuple, tuple):
        # self.cfg.projector_type == &#34;low_high_hybrid_split_mlp_gelu&#34;:
        high_x, low_x = x_or_tuple
        high_x = self.high_up_proj(high_x)
        low_x = self.low_up_proj(low_x)
        x = torch.cat([high_x, low_x], dim=-1)
    else:
        x = x_or_tuple

    return self.layers(x)</code></pre>
</details>
<div class="desc"><h2 id="args">Args</h2>
<p>x_or_tuple (Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:
if it is a tuple of torch.Tensor,
then it comes from the hybrid vision encoder, and x = high_res_x, low_res_x);
otherwise it is the feature from the single vision encoder.</p>
<h2 id="returns">Returns</h2>
<p>x (torch.Tensor): [b, s, c]</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.ModelArgs"><code class="flex name class">
<span>class <span class="ident">ModelArgs</span></span>
<span>(</span><span>codebook_size: int = 16384,<br>codebook_embed_dim: int = 8,<br>codebook_l2_norm: bool = True,<br>codebook_show_usage: bool = True,<br>commit_loss_beta: float = 0.25,<br>entropy_loss_ratio: float = 0.0,<br>encoder_ch_mult: List[int] = &lt;factory&gt;,<br>decoder_ch_mult: List[int] = &lt;factory&gt;,<br>z_channels: int = 256,<br>dropout_p: float = 0.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class ModelArgs:
    codebook_size: int = 16384
    codebook_embed_dim: int = 8
    codebook_l2_norm: bool = True
    codebook_show_usage: bool = True
    commit_loss_beta: float = 0.25
    entropy_loss_ratio: float = 0.0

    encoder_ch_mult: List[int] = field(default_factory=lambda: [1, 1, 2, 2, 4])
    decoder_ch_mult: List[int] = field(default_factory=lambda: [1, 1, 2, 2, 4])
    z_channels: int = 256
    dropout_p: float = 0.0</code></pre>
</details>
<div class="desc"><p>ModelArgs(codebook_size: int = 16384, codebook_embed_dim: int = 8, codebook_l2_norm: bool = True, codebook_show_usage: bool = True, commit_loss_beta: float = 0.25, entropy_loss_ratio: float = 0.0, encoder_ch_mult: List[int] = <factory>, decoder_ch_mult: List[int] = <factory>, z_channels: int = 256, dropout_p: float = 0.0)</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.ModelArgs.codebook_embed_dim"><code class="name">var <span class="ident">codebook_embed_dim</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.ModelArgs.codebook_l2_norm"><code class="name">var <span class="ident">codebook_l2_norm</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.ModelArgs.codebook_show_usage"><code class="name">var <span class="ident">codebook_show_usage</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.ModelArgs.codebook_size"><code class="name">var <span class="ident">codebook_size</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.ModelArgs.commit_loss_beta"><code class="name">var <span class="ident">commit_loss_beta</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.ModelArgs.decoder_ch_mult"><code class="name">var <span class="ident">decoder_ch_mult</span> : List[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.ModelArgs.dropout_p"><code class="name">var <span class="ident">dropout_p</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.ModelArgs.encoder_ch_mult"><code class="name">var <span class="ident">encoder_ch_mult</span> : List[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.ModelArgs.entropy_loss_ratio"><code class="name">var <span class="ident">entropy_loss_ratio</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.ModelArgs.z_channels"><code class="name">var <span class="ident">z_channels</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM"><code class="flex name class">
<span>class <span class="ident">MultiModalityCausalLM</span></span>
<span>(</span><span>config: <a title="sglang.srt.configs.janus_pro.MultiModalityConfig" href="../configs/janus_pro.html#sglang.srt.configs.janus_pro.MultiModalityConfig">MultiModalityConfig</a>,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiModalityCausalLM(MultiModalityPreTrainedModel):

    def __init__(
        self,
        config: MultiModalityConfig,
        quant_config: Optional[QuantizationConfig] = None,
    ):
        super().__init__(config)

        vision_config = config.vision_config
        vision_cls = model_name_to_cls(vision_config.cls)
        self.vision_model = vision_cls(**vision_config.params)

        aligner_config = config.aligner_config
        aligner_cls = model_name_to_cls(aligner_config.cls)
        self.aligner = aligner_cls(aligner_config.params)

        gen_vision_config = config.gen_vision_config
        gen_vision_cls = model_name_to_cls(gen_vision_config.cls)
        self.gen_vision_model = gen_vision_cls()

        gen_aligner_config = config.gen_aligner_config
        gen_aligner_cls = model_name_to_cls(gen_aligner_config.cls)
        self.gen_aligner = gen_aligner_cls(gen_aligner_config.params)

        gen_head_config = config.gen_head_config
        gen_head_cls = model_name_to_cls(gen_head_config.cls)
        self.gen_head = gen_head_cls(gen_head_config.params)

        self.gen_embed = torch.nn.Embedding(
            gen_vision_config.params[&#34;image_token_size&#34;],
            gen_vision_config.params[&#34;n_embed&#34;],
        )

        language_config = config.language_config
        self.language_model = LlamaForCausalLM(
            language_config, quant_config=quant_config
        )
        self.logits_processor = LogitsProcessor(config)

    def get_image_feature(self, items: List[MultimodalDataItem]) -&gt; torch.Tensor:
        pixel_values = torch.concat([item.feature for item in items], dim=0)
        bs, n = pixel_values.shape[0:2]
        pixel_values = pixel_values.to(
            device=self.vision_model.device, dtype=self.vision_model.dtype
        )
        images = rearrange(pixel_values, &#34;b n c h w -&gt; (b n) c h w&#34;)

        # [b x n, T2, D]
        images_embeds = self.aligner(self.vision_model(images))

        # [b x n, T2, D] -&gt; [b, n x T2, D]
        images_embeds = rearrange(images_embeds, &#34;(b n) t d -&gt; b (n t) d&#34;, b=bs, n=n)

        return images_embeds

    def get_input_embeddings(self) -&gt; nn.Embedding:
        return self.language_model.get_input_embeddings()

    @torch.no_grad()
    def forward(
        self,
        input_ids: torch.LongTensor,
        positions: torch.Tensor,
        forward_batch: ForwardBatch,
        get_embedding: bool = False,
    ) -&gt; torch.Tensor:
        hidden_states = general_mm_embed_routine(
            input_ids=input_ids,
            forward_batch=forward_batch,
            multimodal_model=self,
            language_model=self.language_model,
            positions=positions,
        )

        return hidden_states

    def prepare_gen_img_embeds(self, image_ids: torch.LongTensor):
        return self.gen_aligner(self.gen_embed(image_ids))

    def pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs):
        im_start_id = image_inputs.im_start_id
        im_end_id = image_inputs.im_end_id
        media_token_pairs = [(im_start_id, im_end_id)]

        helper = MultiModalityDataPaddingPatternTokenPairs(media_token_pairs)

        return helper.pad_input_tokens(input_ids, image_inputs)

    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
        stacked_params_mapping = [
            # (param_name, shard_name, shard_id)
            (&#34;.qkv_proj&#34;, &#34;.q_proj&#34;, &#34;q&#34;),
            (&#34;.qkv_proj&#34;, &#34;.k_proj&#34;, &#34;k&#34;),
            (&#34;.qkv_proj&#34;, &#34;.v_proj&#34;, &#34;v&#34;),
            (&#34;gate_up_proj&#34;, &#34;gate_proj&#34;, 0),
            (&#34;gate_up_proj&#34;, &#34;up_proj&#34;, 1),
        ]

        params_dict = dict(self.named_parameters())
        for name, loaded_weight in weights:
            if &#34;rotary_emb.inv_freq~&#34; in name or &#34;projector&#34; in name:
                continue
            if &#34;rotary_emb.cos_cached&#34; in name or &#34;rotary_emb.sin_cached&#34; in name:
                # Models trained using ColossalAI may include these tensors in
                # the checkpoint. Skip them.
                continue
            if name.startswith(&#34;model.vision_tower&#34;) and name not in params_dict:
                continue

            # skip generation sub model
            if &#34;gen&#34; in name:
                continue

            # adapt to VisionAttention
            name = name.replace(r&#34;self_attn.out_proj&#34;, r&#34;self_attn.proj&#34;)
            if &#34;vision_model.vision_tower&#34; in name:
                name = name.replace(&#34;attn.qkv&#34;, &#34;attn.qkv_proj&#34;)

            for param_name, weight_name, shard_id in stacked_params_mapping:
                # replace the name and load with customized loader
                if weight_name not in name:
                    continue
                name = name.replace(weight_name, param_name)

                # # Skip loading extra bias for GPTQ models.
                if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                    continue
                param = params_dict[name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, None)
                weight_loader(param, loaded_weight, shard_id)
                break
            else:
                # Skip loading extra bias for GPTQ models.
                if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                    continue

                param = params_dict[name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(param, loaded_weight)</code></pre>
</details>
<div class="desc"><p>Base class for all models.</p>
<p>[<code>PreTrainedModel</code>] takes care of storing the configuration of the models and handles methods for loading,
downloading and saving models as well as a few methods common to all models to:</p>
<pre><code>- resize the input embeddings,
- prune heads in the self-attention heads.
</code></pre>
<p>Class attributes (overridden by derived classes):</p>
<pre><code>- **config_class** ([`PretrainedConfig`]) -- A subclass of [`PretrainedConfig`] to use as configuration class
  for this model architecture.
- **load_tf_weights** (&lt;code&gt;Callable&lt;/code&gt;) -- A python *method* for loading a TensorFlow checkpoint in a PyTorch model,
  taking as arguments:

    - **model** ([`PreTrainedModel`]) -- An instance of the model on which to load the TensorFlow checkpoint.
    - **config** ([`PreTrainedConfig`]) -- An instance of the configuration associated to the model.
    - **path** (&lt;code&gt;str&lt;/code&gt;) -- A path to the TensorFlow checkpoint.

- **base_model_prefix** (&lt;code&gt;str&lt;/code&gt;) -- A string indicating the attribute associated to the base model in derived
  classes of the same architecture adding modules on top of the base model.
- **is_parallelizable** (&lt;code&gt;bool&lt;/code&gt;) -- A flag indicating whether this model supports model parallelization.
- **main_input_name** (&lt;code&gt;str&lt;/code&gt;) -- The name of the principal input to the model (often &lt;code&gt;input\_ids&lt;/code&gt; for NLP
  models, &lt;code&gt;pixel\_values&lt;/code&gt; for vision models and &lt;code&gt;input\_values&lt;/code&gt; for speech models).
- **can_record_outputs** (dict):
</code></pre>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.models.deepseek_janus_pro.MultiModalityPreTrainedModel" href="#sglang.srt.models.deepseek_janus_pro.MultiModalityPreTrainedModel">MultiModalityPreTrainedModel</a></li>
<li>transformers.modeling_utils.PreTrainedModel</li>
<li>torch.nn.modules.module.Module</li>
<li>transformers.modeling_utils.EmbeddingAccessMixin</li>
<li>transformers.modeling_utils.ModuleUtilsMixin</li>
<li>transformers.utils.hub.PushToHubMixin</li>
<li>transformers.integrations.peft.PeftAdapterMixin</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>input_ids: torch.LongTensor,<br>positions: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>get_embedding: bool = False) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.no_grad()
def forward(
    self,
    input_ids: torch.LongTensor,
    positions: torch.Tensor,
    forward_batch: ForwardBatch,
    get_embedding: bool = False,
) -&gt; torch.Tensor:
    hidden_states = general_mm_embed_routine(
        input_ids=input_ids,
        forward_batch=forward_batch,
        multimodal_model=self,
        language_model=self.language_model,
        positions=positions,
    )

    return hidden_states</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM.get_image_feature"><code class="name flex">
<span>def <span class="ident">get_image_feature</span></span>(<span>self,<br>items: List[<a title="sglang.srt.managers.schedule_batch.MultimodalDataItem" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.MultimodalDataItem">MultimodalDataItem</a>]) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_image_feature(self, items: List[MultimodalDataItem]) -&gt; torch.Tensor:
    pixel_values = torch.concat([item.feature for item in items], dim=0)
    bs, n = pixel_values.shape[0:2]
    pixel_values = pixel_values.to(
        device=self.vision_model.device, dtype=self.vision_model.dtype
    )
    images = rearrange(pixel_values, &#34;b n c h w -&gt; (b n) c h w&#34;)

    # [b x n, T2, D]
    images_embeds = self.aligner(self.vision_model(images))

    # [b x n, T2, D] -&gt; [b, n x T2, D]
    images_embeds = rearrange(images_embeds, &#34;(b n) t d -&gt; b (n t) d&#34;, b=bs, n=n)

    return images_embeds</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM.get_input_embeddings"><code class="name flex">
<span>def <span class="ident">get_input_embeddings</span></span>(<span>self) ‑> torch.nn.modules.sparse.Embedding</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_input_embeddings(self) -&gt; nn.Embedding:
    return self.language_model.get_input_embeddings()</code></pre>
</details>
<div class="desc"><p>Returns the model's input embeddings.</p>
<h2 id="returns">Returns</h2>
<p><code>nn.Module</code>: A torch module mapping vocabulary to hidden states.</p></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM.load_weights"><code class="name flex">
<span>def <span class="ident">load_weights</span></span>(<span>self, weights: Iterable[Tuple[str, torch.Tensor]])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
    stacked_params_mapping = [
        # (param_name, shard_name, shard_id)
        (&#34;.qkv_proj&#34;, &#34;.q_proj&#34;, &#34;q&#34;),
        (&#34;.qkv_proj&#34;, &#34;.k_proj&#34;, &#34;k&#34;),
        (&#34;.qkv_proj&#34;, &#34;.v_proj&#34;, &#34;v&#34;),
        (&#34;gate_up_proj&#34;, &#34;gate_proj&#34;, 0),
        (&#34;gate_up_proj&#34;, &#34;up_proj&#34;, 1),
    ]

    params_dict = dict(self.named_parameters())
    for name, loaded_weight in weights:
        if &#34;rotary_emb.inv_freq~&#34; in name or &#34;projector&#34; in name:
            continue
        if &#34;rotary_emb.cos_cached&#34; in name or &#34;rotary_emb.sin_cached&#34; in name:
            # Models trained using ColossalAI may include these tensors in
            # the checkpoint. Skip them.
            continue
        if name.startswith(&#34;model.vision_tower&#34;) and name not in params_dict:
            continue

        # skip generation sub model
        if &#34;gen&#34; in name:
            continue

        # adapt to VisionAttention
        name = name.replace(r&#34;self_attn.out_proj&#34;, r&#34;self_attn.proj&#34;)
        if &#34;vision_model.vision_tower&#34; in name:
            name = name.replace(&#34;attn.qkv&#34;, &#34;attn.qkv_proj&#34;)

        for param_name, weight_name, shard_id in stacked_params_mapping:
            # replace the name and load with customized loader
            if weight_name not in name:
                continue
            name = name.replace(weight_name, param_name)

            # # Skip loading extra bias for GPTQ models.
            if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                continue
            param = params_dict[name]
            weight_loader = getattr(param, &#34;weight_loader&#34;, None)
            weight_loader(param, loaded_weight, shard_id)
            break
        else:
            # Skip loading extra bias for GPTQ models.
            if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                continue

            param = params_dict[name]
            weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
            weight_loader(param, loaded_weight)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM.pad_input_ids"><code class="name flex">
<span>def <span class="ident">pad_input_ids</span></span>(<span>self,<br>input_ids: List[int],<br>image_inputs: <a title="sglang.srt.managers.schedule_batch.MultimodalInputs" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.MultimodalInputs">MultimodalInputs</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs):
    im_start_id = image_inputs.im_start_id
    im_end_id = image_inputs.im_end_id
    media_token_pairs = [(im_start_id, im_end_id)]

    helper = MultiModalityDataPaddingPatternTokenPairs(media_token_pairs)

    return helper.pad_input_tokens(input_ids, image_inputs)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM.prepare_gen_img_embeds"><code class="name flex">
<span>def <span class="ident">prepare_gen_img_embeds</span></span>(<span>self, image_ids: torch.LongTensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_gen_img_embeds(self, image_ids: torch.LongTensor):
    return self.gen_aligner(self.gen_embed(image_ids))</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.models.deepseek_janus_pro.MultiModalityPreTrainedModel" href="#sglang.srt.models.deepseek_janus_pro.MultiModalityPreTrainedModel">MultiModalityPreTrainedModel</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.MultiModalityPreTrainedModel.config_class" href="#sglang.srt.models.deepseek_janus_pro.MultiModalityPreTrainedModel.config_class">config_class</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.MultiModalityPreTrainedModel"><code class="flex name class">
<span>class <span class="ident">MultiModalityPreTrainedModel</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig, *inputs, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiModalityPreTrainedModel(PreTrainedModel):
    config_class = MultiModalityConfig
    base_model_prefix = &#34;multi_modality&#34;
    _no_split_modules = []
    _skip_keys_device_placement = &#34;past_key_values&#34;</code></pre>
</details>
<div class="desc"><p>Base class for all models.</p>
<p>[<code>PreTrainedModel</code>] takes care of storing the configuration of the models and handles methods for loading,
downloading and saving models as well as a few methods common to all models to:</p>
<pre><code>- resize the input embeddings,
- prune heads in the self-attention heads.
</code></pre>
<p>Class attributes (overridden by derived classes):</p>
<pre><code>- **config_class** ([`PretrainedConfig`]) -- A subclass of [`PretrainedConfig`] to use as configuration class
  for this model architecture.
- **load_tf_weights** (&lt;code&gt;Callable&lt;/code&gt;) -- A python *method* for loading a TensorFlow checkpoint in a PyTorch model,
  taking as arguments:

    - **model** ([`PreTrainedModel`]) -- An instance of the model on which to load the TensorFlow checkpoint.
    - **config** ([`PreTrainedConfig`]) -- An instance of the configuration associated to the model.
    - **path** (&lt;code&gt;str&lt;/code&gt;) -- A path to the TensorFlow checkpoint.

- **base_model_prefix** (&lt;code&gt;str&lt;/code&gt;) -- A string indicating the attribute associated to the base model in derived
  classes of the same architecture adding modules on top of the base model.
- **is_parallelizable** (&lt;code&gt;bool&lt;/code&gt;) -- A flag indicating whether this model supports model parallelization.
- **main_input_name** (&lt;code&gt;str&lt;/code&gt;) -- The name of the principal input to the model (often &lt;code&gt;input\_ids&lt;/code&gt; for NLP
  models, &lt;code&gt;pixel\_values&lt;/code&gt; for vision models and &lt;code&gt;input\_values&lt;/code&gt; for speech models).
- **can_record_outputs** (dict):
</code></pre>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.modeling_utils.PreTrainedModel</li>
<li>torch.nn.modules.module.Module</li>
<li>transformers.modeling_utils.EmbeddingAccessMixin</li>
<li>transformers.modeling_utils.ModuleUtilsMixin</li>
<li>transformers.utils.hub.PushToHubMixin</li>
<li>transformers.integrations.peft.PeftAdapterMixin</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM" href="#sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM">MultiModalityCausalLM</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.MultiModalityPreTrainedModel.base_model_prefix"><code class="name">var <span class="ident">base_model_prefix</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.MultiModalityPreTrainedModel.config_class"><code class="name">var <span class="ident">config_class</span></code></dt>
<dd>
<div class="desc"><p>Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as
methods for loading/downloading/saving configurations.</p>
<p><Tip></p>
<p>A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to
initialize a model does <strong>not</strong> load the model weights. It only affects the model's configuration.</p>
<p></Tip></p>
<p>Class attributes (overridden by derived classes):</p>
<ul>
<li><strong>model_type</strong> (<code>str</code>) &ndash; An identifier for the model type, serialized into the JSON file, and used to recreate
the correct object in [<code>~transformers.AutoConfig</code>].</li>
<li><strong>has_no_defaults_at_init</strong> (<code>bool</code>) &ndash; Whether the config class can be initialized without providing input arguments.
Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,
(but not necessarily) such as [<code>~transformers.EncoderDecoderConfig</code>] or [<code>~RagConfig</code>]. They have to be initialized from
two or more configs of type [<code>~transformers.PretrainedConfig</code>].</li>
<li><strong>keys_to_ignore_at_inference</strong> (<code>list[str]</code>) &ndash; A list of keys to ignore by default when looking at dictionary
outputs of the model during inference.</li>
<li><strong>attribute_map</strong> (<code>dict[str, str]</code>) &ndash; A dict that maps model specific attribute names to the standardized
naming of attributes.</li>
<li><strong>base_model_tp_plan</strong> (<code>dict[str, Any]</code>) &ndash; A dict that maps sub-modules FQNs of a base model to a tensor
parallel plan applied to the sub-module when <code>model.tensor_parallel</code> is called.</li>
<li><strong>base_model_pp_plan</strong> (<code>dict[str, tuple[list[str]]]</code>) &ndash; A dict that maps child-modules of a base model to a
pipeline parallel plan that enables users to place the child-module on the appropriate device.</li>
</ul>
<p>Common attributes (present in all subclasses):</p>
<ul>
<li><strong>vocab_size</strong> (<code>int</code>) &ndash; The number of tokens in the vocabulary, which is also the first dimension of the
embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).</li>
<li><strong>hidden_size</strong> (<code>int</code>) &ndash; The hidden size of the model.</li>
<li><strong>num_attention_heads</strong> (<code>int</code>) &ndash; The number of attention heads used in the multi-head attention layers of the
model.</li>
<li><strong>num_hidden_layers</strong> (<code>int</code>) &ndash; The number of blocks in the model.</li>
</ul>
<p><Tip warning={true}></p>
<p>Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading
some of them will still be possible, but attempting to overwrite them will throw an exception &ndash; you should set
them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more
information about the individual parameters.</p>
<p></Tip></p>
<h2 id="arg">Arg</h2>
<p>name_or_path (<code>str</code>, <em>optional</em>, defaults to <code>""</code>):
Store the string that was passed to [<code>PreTrainedModel.from_pretrained</code>] or
[<code>TFPreTrainedModel.from_pretrained</code>] as <code>pretrained_model_name_or_path</code> if the configuration was created
with such a method.
output_hidden_states (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should return all hidden-states.
output_attentions (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should returns all attentions.
return_dict (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should return a [<code>~transformers.utils.ModelOutput</code>] instead of a plain tuple.
is_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether the model is used as an encoder/decoder or not.
is_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether to only use the decoder in an encoder-decoder architecture, otherwise it has no effect on
decoder-only or encoder-only architectures.
cross_attention_hidden_size (<code>bool</code>, <em>optional</em>):
The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder
setting and the cross-attention hidden dimension differs from <code>self.config.hidden_size</code>.
add_cross_attention (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether cross-attention layers should be added to the model. Note, this option is only relevant for models
that can be used as decoder models within the [<code>EncoderDecoderModel</code>] class, which consists of all models
in <code>AUTO_MODELS_FOR_CAUSAL_LM</code>.
tie_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder
and decoder model to have the exact same parameter names.
prune_heads (<code>dict[int, list[int]]</code>, <em>optional</em>, defaults to <code>{}</code>):
Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of
heads to prune in said layer.</p>
<pre><code>For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.
</code></pre>
<p>chunk_size_feed_forward (<code>int</code>, <em>optional</em>, defaults to <code>0</code>):
The chunk size of all feed forward layers in the residual attention blocks. A chunk size of <code>0</code> means that
the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes <code>n</code> &lt;
sequence_length embeddings at a time. For more information on feed forward chunking, see <a href="../glossary.html#feed-forward-chunking">How does Feed
Forward Chunking work?</a>.</p>
<blockquote>
<p>Parameters for fine-tuning tasks</p>
</blockquote>
<p>architectures (<code>list[str]</code>, <em>optional</em>):
Model architectures that can be used with the model pretrained weights.
finetuning_task (<code>str</code>, <em>optional</em>):
Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow
or PyTorch) checkpoint.
id2label (<code>dict[int, str]</code>, <em>optional</em>):
A map from index (for instance prediction index, or target index) to label.
label2id (<code>dict[str, int]</code>, <em>optional</em>):
A map from label to index for the model.
num_labels (<code>int</code>, <em>optional</em>):
Number of labels to use in the last layer added to the model, typically for a classification task.
task_specific_params (<code>dict[str, Any]</code>, <em>optional</em>):
Additional keyword arguments to store for the current task.
problem_type (<code>str</code>, <em>optional</em>):
Problem type for <code>XxxForSequenceClassification</code> models. Can be one of <code>"regression"</code>,
<code>"single_label_classification"</code> or <code>"multi_label_classification"</code>.</p>
<blockquote>
<p>Parameters linked to the tokenizer</p>
</blockquote>
<p>tokenizer_class (<code>str</code>, <em>optional</em>):
The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the
model by default).
prefix (<code>str</code>, <em>optional</em>):
A specific prompt that should be added at the beginning of each text before calling the model.
bos_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>beginning-of-stream</em> token.
pad_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>padding</em> token.
eos_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>end-of-stream</em> token.
decoder_start_token_id (<code>int</code>, <em>optional</em>):
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.
sep_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>separation</em> token.</p>
<blockquote>
<p>PyTorch specific parameters</p>
</blockquote>
<p>torchscript (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should be used with Torchscript.
tie_word_embeddings (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the
model has a output word embedding layer.
torch_dtype (<code>str</code>, <em>optional</em>):
The <code>dtype</code> of the weights. This attribute can be used to initialize the model to a non-default <code>dtype</code>
(which is normally <code>float32</code>) and thus allow for optimal storage allocation. For example, if the saved
model is <code>float16</code>, ideally we want to load it back using the minimal amount of memory needed to load
<code>float16</code> weights. Since the config object is stored in plain text, this attribute contains just the
floating type string without the <code>torch.</code> prefix. For example, for <code>torch.float16</code> <code><code>torch\_dtype&lt;/code&gt; is the</code>"float16"` string.</p>
<pre><code>This attribute is currently not being used during model loading time, but this may change in the future
versions. But we can already start preparing for the future by saving the dtype with save_pretrained.
</code></pre></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.PatchDropout"><code class="flex name class">
<span>class <span class="ident">PatchDropout</span></span>
<span>(</span><span>prob: float = 0.5,<br>num_prefix_tokens: int = 1,<br>ordered: bool = False,<br>return_indices: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PatchDropout(nn.Module):
    &#34;&#34;&#34;
    https://arxiv.org/abs/2212.00794 and https://arxiv.org/pdf/2208.07220
    &#34;&#34;&#34;

    return_indices: torch.jit.Final[bool]

    def __init__(
        self,
        prob: float = 0.5,
        num_prefix_tokens: int = 1,
        ordered: bool = False,
        return_indices: bool = False,
    ):
        super().__init__()
        assert 0 &lt;= prob &lt; 1.0
        self.prob = prob
        self.num_prefix_tokens = (
            num_prefix_tokens  # exclude CLS token (or other prefix tokens)
        )
        self.ordered = ordered
        self.return_indices = return_indices

    def forward(
        self, x
    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]]]:
        if not self.training or self.prob == 0.0:
            if self.return_indices:
                return x, None
            return x

        if self.num_prefix_tokens:
            prefix_tokens, x = (
                x[:, : self.num_prefix_tokens],
                x[:, self.num_prefix_tokens :],
            )
        else:
            prefix_tokens = None

        B = x.shape[0]
        L = x.shape[1]
        num_keep = max(1, int(L * (1.0 - self.prob)))
        keep_indices = torch.argsort(torch.randn(B, L, device=x.device), dim=-1)[
            :, :num_keep
        ]
        if self.ordered:
            # NOTE does not need to maintain patch order in typical transformer use,
            # but possibly useful for debug / visualization
            keep_indices = keep_indices.sort(dim=-1)[0]
        x = x.gather(1, keep_indices.unsqueeze(-1).expand((-1, -1) + x.shape[2:]))

        if prefix_tokens is not None:
            x = torch.cat((prefix_tokens, x), dim=1)

        if self.return_indices:
            return x, keep_indices
        return x</code></pre>
</details>
<div class="desc"><p><a href="https://arxiv.org/abs/2212.00794">https://arxiv.org/abs/2212.00794</a> and <a href="https://arxiv.org/pdf/2208.07220">https://arxiv.org/pdf/2208.07220</a></p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.PatchDropout.return_indices"><code class="name">var <span class="ident">return_indices</span> : Final[bool]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.PatchDropout.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> torch.Tensor | Tuple[torch.Tensor, torch.Tensor | None]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self, x
) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]]]:
    if not self.training or self.prob == 0.0:
        if self.return_indices:
            return x, None
        return x

    if self.num_prefix_tokens:
        prefix_tokens, x = (
            x[:, : self.num_prefix_tokens],
            x[:, self.num_prefix_tokens :],
        )
    else:
        prefix_tokens = None

    B = x.shape[0]
    L = x.shape[1]
    num_keep = max(1, int(L * (1.0 - self.prob)))
    keep_indices = torch.argsort(torch.randn(B, L, device=x.device), dim=-1)[
        :, :num_keep
    ]
    if self.ordered:
        # NOTE does not need to maintain patch order in typical transformer use,
        # but possibly useful for debug / visualization
        keep_indices = keep_indices.sort(dim=-1)[0]
    x = x.gather(1, keep_indices.unsqueeze(-1).expand((-1, -1) + x.shape[2:]))

    if prefix_tokens is not None:
        x = torch.cat((prefix_tokens, x), dim=1)

    if self.return_indices:
        return x, keep_indices
    return x</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.PatchEmbed"><code class="flex name class">
<span>class <span class="ident">PatchEmbed</span></span>
<span>(</span><span>img_size: int | None = 224,<br>patch_size: int = 16,<br>in_chans: int = 3,<br>embed_dim: int = 768,<br>norm_layer: Callable | None = None,<br>flatten: bool = True,<br>output_fmt: str | None = None,<br>bias: bool = True,<br>strict_img_size: bool = True,<br>dynamic_img_pad: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PatchEmbed(nn.Module):
    &#34;&#34;&#34;2D Image to Patch Embedding&#34;&#34;&#34;

    output_fmt: Format
    dynamic_img_pad: torch.jit.Final[bool]

    def __init__(
        self,
        img_size: Optional[int] = 224,
        patch_size: int = 16,
        in_chans: int = 3,
        embed_dim: int = 768,
        norm_layer: Optional[Callable] = None,
        flatten: bool = True,
        output_fmt: Optional[str] = None,
        bias: bool = True,
        strict_img_size: bool = True,
        dynamic_img_pad: bool = False,
    ):
        super().__init__()
        self.patch_size = tuple(to_2tuple(patch_size))
        self.img_size, self.grid_size, self.num_patches = self._init_img_size(img_size)

        if output_fmt is not None:
            self.flatten = False
            self.output_fmt = Format(output_fmt)
        else:
            # flatten spatial dim and transpose to channels last, kept for bwd compat
            self.flatten = flatten
            self.output_fmt = Format.NCHW
        self.strict_img_size = strict_img_size
        self.dynamic_img_pad = dynamic_img_pad

        self.proj = nn.Conv2d(
            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias
        )
        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()

    def _init_img_size(self, img_size: Union[int, Tuple[int, int]]):
        assert self.patch_size
        if img_size is None:
            return None, None, None
        img_size = to_2tuple(img_size)
        grid_size = tuple([s // p for s, p in zip(img_size, self.patch_size)])
        num_patches = grid_size[0] * grid_size[1]
        return img_size, grid_size, num_patches

    def set_input_size(
        self,
        img_size: Optional[Union[int, Tuple[int, int]]] = None,
        patch_size: Optional[Union[int, Tuple[int, int]]] = None,
    ):
        new_patch_size = None
        if patch_size is not None:
            new_patch_size = to_2tuple(patch_size)
        if new_patch_size is not None and new_patch_size != self.patch_size:
            with torch.no_grad():
                new_proj = nn.Conv2d(
                    self.proj.in_channels,
                    self.proj.out_channels,
                    kernel_size=new_patch_size,
                    stride=new_patch_size,
                    bias=self.proj.bias is not None,
                )
                new_proj.weight.copy_(
                    resample_patch_embed(self.proj.weight, new_patch_size, verbose=True)
                )
                if self.proj.bias is not None:
                    new_proj.bias.copy_(self.proj.bias)
                self.proj = new_proj
            self.patch_size = new_patch_size
        img_size = img_size or self.img_size
        if img_size != self.img_size or new_patch_size is not None:
            self.img_size, self.grid_size, self.num_patches = self._init_img_size(
                img_size
            )

    def feat_ratio(self, as_scalar=True) -&gt; Union[Tuple[int, int], int]:
        if as_scalar:
            return max(self.patch_size)
        else:
            return self.patch_size

    def dynamic_feat_size(self, img_size: Tuple[int, int]) -&gt; Tuple[int, int]:
        &#34;&#34;&#34;Get grid (feature) size for given image size taking account of dynamic padding.
        NOTE: must be torchscript compatible so using fixed tuple indexing
        &#34;&#34;&#34;
        if self.dynamic_img_pad:
            return math.ceil(img_size[0] / self.patch_size[0]), math.ceil(
                img_size[1] / self.patch_size[1]
            )
        else:
            return img_size[0] // self.patch_size[0], img_size[1] // self.patch_size[1]

    def forward(self, x):
        B, C, H, W = x.shape
        if self.img_size is not None:
            if self.strict_img_size:
                _assert(
                    H == self.img_size[0],
                    f&#34;Input height ({H}) doesn&#39;t match model ({self.img_size[0]}).&#34;,
                )
                _assert(
                    W == self.img_size[1],
                    f&#34;Input width ({W}) doesn&#39;t match model ({self.img_size[1]}).&#34;,
                )
            elif not self.dynamic_img_pad:
                _assert(
                    H % self.patch_size[0] == 0,
                    f&#34;Input height ({H}) should be divisible by patch size ({self.patch_size[0]}).&#34;,
                )
                _assert(
                    W % self.patch_size[1] == 0,
                    f&#34;Input width ({W}) should be divisible by patch size ({self.patch_size[1]}).&#34;,
                )
        if self.dynamic_img_pad:
            pad_h = (self.patch_size[0] - H % self.patch_size[0]) % self.patch_size[0]
            pad_w = (self.patch_size[1] - W % self.patch_size[1]) % self.patch_size[1]
            x = F.pad(x, (0, pad_w, 0, pad_h))
        x = self.proj(x)
        if self.flatten:
            x = x.flatten(2).transpose(1, 2)  # NCHW -&gt; NLC
        elif self.output_fmt != Format.NCHW:
            x = nchw_to(x, self.output_fmt)
        x = self.norm(x)
        return x</code></pre>
</details>
<div class="desc"><p>2D Image to Patch Embedding</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.PatchEmbed.dynamic_img_pad"><code class="name">var <span class="ident">dynamic_img_pad</span> : Final[bool]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.PatchEmbed.output_fmt"><code class="name">var <span class="ident">output_fmt</span> : <a title="sglang.srt.models.deepseek_janus_pro.Format" href="#sglang.srt.models.deepseek_janus_pro.Format">Format</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.PatchEmbed.dynamic_feat_size"><code class="name flex">
<span>def <span class="ident">dynamic_feat_size</span></span>(<span>self, img_size: Tuple[int, int]) ‑> Tuple[int, int]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dynamic_feat_size(self, img_size: Tuple[int, int]) -&gt; Tuple[int, int]:
    &#34;&#34;&#34;Get grid (feature) size for given image size taking account of dynamic padding.
    NOTE: must be torchscript compatible so using fixed tuple indexing
    &#34;&#34;&#34;
    if self.dynamic_img_pad:
        return math.ceil(img_size[0] / self.patch_size[0]), math.ceil(
            img_size[1] / self.patch_size[1]
        )
    else:
        return img_size[0] // self.patch_size[0], img_size[1] // self.patch_size[1]</code></pre>
</details>
<div class="desc"><p>Get grid (feature) size for given image size taking account of dynamic padding.
NOTE: must be torchscript compatible so using fixed tuple indexing</p></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.PatchEmbed.feat_ratio"><code class="name flex">
<span>def <span class="ident">feat_ratio</span></span>(<span>self, as_scalar=True) ‑> Tuple[int, int] | int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def feat_ratio(self, as_scalar=True) -&gt; Union[Tuple[int, int], int]:
    if as_scalar:
        return max(self.patch_size)
    else:
        return self.patch_size</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.PatchEmbed.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    B, C, H, W = x.shape
    if self.img_size is not None:
        if self.strict_img_size:
            _assert(
                H == self.img_size[0],
                f&#34;Input height ({H}) doesn&#39;t match model ({self.img_size[0]}).&#34;,
            )
            _assert(
                W == self.img_size[1],
                f&#34;Input width ({W}) doesn&#39;t match model ({self.img_size[1]}).&#34;,
            )
        elif not self.dynamic_img_pad:
            _assert(
                H % self.patch_size[0] == 0,
                f&#34;Input height ({H}) should be divisible by patch size ({self.patch_size[0]}).&#34;,
            )
            _assert(
                W % self.patch_size[1] == 0,
                f&#34;Input width ({W}) should be divisible by patch size ({self.patch_size[1]}).&#34;,
            )
    if self.dynamic_img_pad:
        pad_h = (self.patch_size[0] - H % self.patch_size[0]) % self.patch_size[0]
        pad_w = (self.patch_size[1] - W % self.patch_size[1]) % self.patch_size[1]
        x = F.pad(x, (0, pad_w, 0, pad_h))
    x = self.proj(x)
    if self.flatten:
        x = x.flatten(2).transpose(1, 2)  # NCHW -&gt; NLC
    elif self.output_fmt != Format.NCHW:
        x = nchw_to(x, self.output_fmt)
    x = self.norm(x)
    return x</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.PatchEmbed.set_input_size"><code class="name flex">
<span>def <span class="ident">set_input_size</span></span>(<span>self,<br>img_size: int | Tuple[int, int] | None = None,<br>patch_size: int | Tuple[int, int] | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_input_size(
    self,
    img_size: Optional[Union[int, Tuple[int, int]]] = None,
    patch_size: Optional[Union[int, Tuple[int, int]]] = None,
):
    new_patch_size = None
    if patch_size is not None:
        new_patch_size = to_2tuple(patch_size)
    if new_patch_size is not None and new_patch_size != self.patch_size:
        with torch.no_grad():
            new_proj = nn.Conv2d(
                self.proj.in_channels,
                self.proj.out_channels,
                kernel_size=new_patch_size,
                stride=new_patch_size,
                bias=self.proj.bias is not None,
            )
            new_proj.weight.copy_(
                resample_patch_embed(self.proj.weight, new_patch_size, verbose=True)
            )
            if self.proj.bias is not None:
                new_proj.bias.copy_(self.proj.bias)
            self.proj = new_proj
        self.patch_size = new_patch_size
    img_size = img_size or self.img_size
    if img_size != self.img_size or new_patch_size is not None:
        self.img_size, self.grid_size, self.num_patches = self._init_img_size(
            img_size
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.ResnetBlock"><code class="flex name class">
<span>class <span class="ident">ResnetBlock</span></span>
<span>(</span><span>in_channels, out_channels=None, conv_shortcut=False, dropout=0.0, norm_type='group')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ResnetBlock(nn.Module):
    def __init__(
        self,
        in_channels,
        out_channels=None,
        conv_shortcut=False,
        dropout=0.0,
        norm_type=&#34;group&#34;,
    ):
        super().__init__()
        self.in_channels = in_channels
        out_channels = in_channels if out_channels is None else out_channels
        self.out_channels = out_channels
        self.use_conv_shortcut = conv_shortcut

        self.norm1 = Normalize(in_channels, norm_type)
        self.conv1 = nn.Conv2d(
            in_channels, out_channels, kernel_size=3, stride=1, padding=1
        )
        self.norm2 = Normalize(out_channels, norm_type)
        self.dropout = nn.Dropout(dropout)
        self.conv2 = nn.Conv2d(
            out_channels, out_channels, kernel_size=3, stride=1, padding=1
        )

        if self.in_channels != self.out_channels:
            if self.use_conv_shortcut:
                self.conv_shortcut = nn.Conv2d(
                    in_channels, out_channels, kernel_size=3, stride=1, padding=1
                )
            else:
                self.nin_shortcut = nn.Conv2d(
                    in_channels, out_channels, kernel_size=1, stride=1, padding=0
                )

    def forward(self, x):
        h = x
        h = self.norm1(h)
        h = nonlinearity(h)
        h = self.conv1(h)
        h = self.norm2(h)
        h = nonlinearity(h)
        h = self.dropout(h)
        h = self.conv2(h)

        if self.in_channels != self.out_channels:
            if self.use_conv_shortcut:
                x = self.conv_shortcut(x)
            else:
                x = self.nin_shortcut(x)
        return x + h</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.ResnetBlock.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    h = x
    h = self.norm1(h)
    h = nonlinearity(h)
    h = self.conv1(h)
    h = self.norm2(h)
    h = nonlinearity(h)
    h = self.dropout(h)
    h = self.conv2(h)

    if self.in_channels != self.out_channels:
        if self.use_conv_shortcut:
            x = self.conv_shortcut(x)
        else:
            x = self.nin_shortcut(x)
    return x + h</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.Upsample"><code class="flex name class">
<span>class <span class="ident">Upsample</span></span>
<span>(</span><span>in_channels, with_conv)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Upsample(nn.Module):
    def __init__(self, in_channels, with_conv):
        super().__init__()
        self.with_conv = with_conv
        if self.with_conv:
            self.conv = nn.Conv2d(
                in_channels, in_channels, kernel_size=3, stride=1, padding=1
            )

    def forward(self, x):
        if x.dtype != torch.float32:
            x = F.interpolate(x.to(torch.float), scale_factor=2.0, mode=&#34;nearest&#34;).to(
                torch.bfloat16
            )
        else:
            x = F.interpolate(x, scale_factor=2.0, mode=&#34;nearest&#34;)

        if self.with_conv:
            x = self.conv(x)
        return x</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.Upsample.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    if x.dtype != torch.float32:
        x = F.interpolate(x.to(torch.float), scale_factor=2.0, mode=&#34;nearest&#34;).to(
            torch.bfloat16
        )
    else:
        x = F.interpolate(x, scale_factor=2.0, mode=&#34;nearest&#34;)

    if self.with_conv:
        x = self.conv(x)
    return x</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.VQModel"><code class="flex name class">
<span>class <span class="ident">VQModel</span></span>
<span>(</span><span>config: <a title="sglang.srt.models.deepseek_janus_pro.ModelArgs" href="#sglang.srt.models.deepseek_janus_pro.ModelArgs">ModelArgs</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VQModel(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.config = config
        self.encoder = Encoder(
            ch_mult=config.encoder_ch_mult,
            z_channels=config.z_channels,
            dropout=config.dropout_p,
        )
        self.decoder = Decoder(
            ch_mult=config.decoder_ch_mult,
            z_channels=config.z_channels,
            dropout=config.dropout_p,
        )

        self.quantize = VectorQuantizer(
            config.codebook_size,
            config.codebook_embed_dim,
            config.commit_loss_beta,
            config.entropy_loss_ratio,
            config.codebook_l2_norm,
            config.codebook_show_usage,
        )
        self.quant_conv = nn.Conv2d(config.z_channels, config.codebook_embed_dim, 1)
        self.post_quant_conv = nn.Conv2d(
            config.codebook_embed_dim, config.z_channels, 1
        )

    def encode(self, x):
        h = self.encoder(x)
        h = self.quant_conv(h)
        quant, emb_loss, info = self.quantize(h)
        return quant, emb_loss, info

    def decode(self, quant):
        quant = self.post_quant_conv(quant)
        dec = self.decoder(quant)
        return dec

    def decode_code(self, code_b, shape=None, channel_first=True):
        quant_b = self.quantize.get_codebook_entry(code_b, shape, channel_first)
        dec = self.decode(quant_b)
        return dec

    def forward(self, input):
        quant, diff, _ = self.encode(input)
        dec = self.decode(quant)
        return dec, diff</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.VQModel.decode"><code class="name flex">
<span>def <span class="ident">decode</span></span>(<span>self, quant)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decode(self, quant):
    quant = self.post_quant_conv(quant)
    dec = self.decoder(quant)
    return dec</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.VQModel.decode_code"><code class="name flex">
<span>def <span class="ident">decode_code</span></span>(<span>self, code_b, shape=None, channel_first=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decode_code(self, code_b, shape=None, channel_first=True):
    quant_b = self.quantize.get_codebook_entry(code_b, shape, channel_first)
    dec = self.decode(quant_b)
    return dec</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.VQModel.encode"><code class="name flex">
<span>def <span class="ident">encode</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def encode(self, x):
    h = self.encoder(x)
    h = self.quant_conv(h)
    quant, emb_loss, info = self.quantize(h)
    return quant, emb_loss, info</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.VQModel.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, input) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, input):
    quant, diff, _ = self.encode(input)
    dec = self.decode(quant)
    return dec, diff</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.VectorQuantizer"><code class="flex name class">
<span>class <span class="ident">VectorQuantizer</span></span>
<span>(</span><span>n_e, e_dim, beta, entropy_loss_ratio, l2_norm, show_usage)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VectorQuantizer(nn.Module):
    def __init__(self, n_e, e_dim, beta, entropy_loss_ratio, l2_norm, show_usage):
        super().__init__()
        self.n_e = n_e
        self.e_dim = e_dim
        self.beta = beta
        self.entropy_loss_ratio = entropy_loss_ratio
        self.l2_norm = l2_norm
        self.show_usage = show_usage

        self.embedding = nn.Embedding(self.n_e, self.e_dim)
        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)
        if self.l2_norm:
            self.embedding.weight.data = F.normalize(
                self.embedding.weight.data, p=2, dim=-1
            )
        if self.show_usage:
            # self.register_buffer(&#34;codebook_used&#34;, nn.Parameter(torch.zeros(65536)))
            self.codebook_used = nn.Parameter(torch.zeros(65536))

    def forward(self, z):
        # reshape z -&gt; (batch, height, width, channel) and flatten
        z = torch.einsum(&#34;b c h w -&gt; b h w c&#34;, z).contiguous()
        z_flattened = z.view(-1, self.e_dim)
        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z

        if self.l2_norm:
            z = F.normalize(z, p=2, dim=-1)
            z_flattened = F.normalize(z_flattened, p=2, dim=-1)
            embedding = F.normalize(self.embedding.weight, p=2, dim=-1)
        else:
            embedding = self.embedding.weight

        d = (
            torch.sum(z_flattened**2, dim=1, keepdim=True)
            + torch.sum(embedding**2, dim=1)
            - 2
            * torch.einsum(
                &#34;bd,dn-&gt;bn&#34;, z_flattened, torch.einsum(&#34;n d -&gt; d n&#34;, embedding)
            )
        )

        min_encoding_indices = torch.argmin(d, dim=1)
        z_q = embedding[min_encoding_indices].view(z.shape)
        perplexity = None
        min_encodings = None
        vq_loss = None
        commit_loss = None
        entropy_loss = None

        # compute loss for embedding
        if self.training:
            vq_loss = torch.mean((z_q - z.detach()) ** 2)
            commit_loss = self.beta * torch.mean((z_q.detach() - z) ** 2)
            entropy_loss = self.entropy_loss_ratio * compute_entropy_loss(-d)

        # preserve gradients
        z_q = z + (z_q - z).detach()

        # reshape back to match original input shape
        z_q = torch.einsum(&#34;b h w c -&gt; b c h w&#34;, z_q)

        return (
            z_q,
            (vq_loss, commit_loss, entropy_loss),
            (perplexity, min_encodings, min_encoding_indices),
        )

    def get_codebook_entry(self, indices, shape=None, channel_first=True):
        # shape = (batch, channel, height, width) if channel_first else (batch, height, width, channel)
        if self.l2_norm:
            embedding = F.normalize(self.embedding.weight, p=2, dim=-1)
        else:
            embedding = self.embedding.weight
        z_q = embedding[indices]  # (b*h*w, c)

        if shape is not None:
            if channel_first:
                z_q = z_q.reshape(shape[0], shape[2], shape[3], shape[1])
                # reshape back to match original input shape
                z_q = z_q.permute(0, 3, 1, 2).contiguous()
            else:
                z_q = z_q.view(shape)
        return z_q</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.VectorQuantizer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, z) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, z):
    # reshape z -&gt; (batch, height, width, channel) and flatten
    z = torch.einsum(&#34;b c h w -&gt; b h w c&#34;, z).contiguous()
    z_flattened = z.view(-1, self.e_dim)
    # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z

    if self.l2_norm:
        z = F.normalize(z, p=2, dim=-1)
        z_flattened = F.normalize(z_flattened, p=2, dim=-1)
        embedding = F.normalize(self.embedding.weight, p=2, dim=-1)
    else:
        embedding = self.embedding.weight

    d = (
        torch.sum(z_flattened**2, dim=1, keepdim=True)
        + torch.sum(embedding**2, dim=1)
        - 2
        * torch.einsum(
            &#34;bd,dn-&gt;bn&#34;, z_flattened, torch.einsum(&#34;n d -&gt; d n&#34;, embedding)
        )
    )

    min_encoding_indices = torch.argmin(d, dim=1)
    z_q = embedding[min_encoding_indices].view(z.shape)
    perplexity = None
    min_encodings = None
    vq_loss = None
    commit_loss = None
    entropy_loss = None

    # compute loss for embedding
    if self.training:
        vq_loss = torch.mean((z_q - z.detach()) ** 2)
        commit_loss = self.beta * torch.mean((z_q.detach() - z) ** 2)
        entropy_loss = self.entropy_loss_ratio * compute_entropy_loss(-d)

    # preserve gradients
    z_q = z + (z_q - z).detach()

    # reshape back to match original input shape
    z_q = torch.einsum(&#34;b h w c -&gt; b c h w&#34;, z_q)

    return (
        z_q,
        (vq_loss, commit_loss, entropy_loss),
        (perplexity, min_encodings, min_encoding_indices),
    )</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.VectorQuantizer.get_codebook_entry"><code class="name flex">
<span>def <span class="ident">get_codebook_entry</span></span>(<span>self, indices, shape=None, channel_first=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_codebook_entry(self, indices, shape=None, channel_first=True):
    # shape = (batch, channel, height, width) if channel_first else (batch, height, width, channel)
    if self.l2_norm:
        embedding = F.normalize(self.embedding.weight, p=2, dim=-1)
    else:
        embedding = self.embedding.weight
    z_q = embedding[indices]  # (b*h*w, c)

    if shape is not None:
        if channel_first:
            z_q = z_q.reshape(shape[0], shape[2], shape[3], shape[1])
            # reshape back to match original input shape
            z_q = z_q.permute(0, 3, 1, 2).contiguous()
        else:
            z_q = z_q.view(shape)
    return z_q</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.VisionTransformer"><code class="flex name class">
<span>class <span class="ident">VisionTransformer</span></span>
<span>(</span><span>img_size: Tuple[int, int] | int = 224,<br>patch_size: Tuple[int, int] | int = 16,<br>in_chans: int = 3,<br>num_classes: int = 1000,<br>global_pool: Literal['', 'avg', 'token', 'map'] = 'token',<br>embed_dim: int = 768,<br>depth: int = 12,<br>num_heads: int = 12,<br>mlp_ratio: float = 4.0,<br>qkv_bias: bool = True,<br>qk_norm: bool = False,<br>init_values: float | None = None,<br>class_token: bool = True,<br>no_embed_class: bool = False,<br>reg_tokens: int = 0,<br>pre_norm: bool = False,<br>fc_norm: bool | None = None,<br>dynamic_img_size: bool = False,<br>dynamic_img_pad: bool = False,<br>drop_rate: float = 0.0,<br>pos_drop_rate: float = 0.0,<br>patch_drop_rate: float = 0.0,<br>proj_drop_rate: float = 0.0,<br>attn_drop_rate: float = 0.0,<br>drop_path_rate: float = 0.0,<br>weight_init: Literal['skip', 'jax', 'jax_nlhb', 'moco', ''] = '',<br>embed_layer: Callable = sglang.srt.models.deepseek_janus_pro.PatchEmbed,<br>block_fn: Type[torch.nn.modules.module.Module] = sglang.srt.models.deepseek_janus_pro.VisionTransformerBlock,<br>mlp_layer: Type[torch.nn.modules.module.Module] = sglang.srt.models.deepseek_janus_pro.Mlp,<br>ignore_head: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VisionTransformer(nn.Module):
    &#34;&#34;&#34;Vision Transformer

    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`
        - https://arxiv.org/abs/2010.11929
    &#34;&#34;&#34;

    dynamic_img_size: Final[bool]

    def __init__(
        self,
        img_size: Union[int, Tuple[int, int]] = 224,
        patch_size: Union[int, Tuple[int, int]] = 16,
        in_chans: int = 3,
        num_classes: int = 1000,
        global_pool: Literal[&#34;&#34;, &#34;avg&#34;, &#34;token&#34;, &#34;map&#34;] = &#34;token&#34;,
        embed_dim: int = 768,
        depth: int = 12,
        num_heads: int = 12,
        mlp_ratio: float = 4.0,
        qkv_bias: bool = True,
        qk_norm: bool = False,
        init_values: Optional[float] = None,
        class_token: bool = True,
        no_embed_class: bool = False,
        reg_tokens: int = 0,
        pre_norm: bool = False,
        fc_norm: Optional[bool] = None,
        dynamic_img_size: bool = False,
        dynamic_img_pad: bool = False,
        drop_rate: float = 0.0,
        pos_drop_rate: float = 0.0,
        patch_drop_rate: float = 0.0,
        proj_drop_rate: float = 0.0,
        attn_drop_rate: float = 0.0,
        drop_path_rate: float = 0.0,
        weight_init: Literal[&#34;skip&#34;, &#34;jax&#34;, &#34;jax_nlhb&#34;, &#34;moco&#34;, &#34;&#34;] = &#34;&#34;,
        embed_layer: Callable = PatchEmbed,
        _norm_layer: Optional[LayerType] = None,
        _act_layer: Optional[LayerType] = None,
        block_fn: Type[nn.Module] = VisionTransformerBlock,
        mlp_layer: Type[nn.Module] = Mlp,
        ignore_head: bool = False,
    ) -&gt; None:
        &#34;&#34;&#34;
        Args:
            img_size: Input image size.
            patch_size: Patch size.
            in_chans: Number of image input channels.
            num_classes: Number of classes for classification head.
            global_pool: Type of global pooling for final sequence (default: &#39;token&#39;).
            embed_dim: Transformer embedding dimension.
            depth: Depth of transformer.
            num_heads: Number of attention heads.
            mlp_ratio: Ratio of mlp hidden dim to embedding dim.
            qkv_bias: Enable bias for qkv projections if True.
            init_values: Layer-scale init values (layer-scale enabled if not None).
            class_token: Use class token.
            no_embed_class: Don&#39;t include position embeddings for class (or reg) tokens.
            reg_tokens: Number of register tokens.
            fc_norm: Pre head norm after pool (instead of before), if None, enabled when global_pool == &#39;avg&#39;.
            drop_rate: Head dropout rate.
            pos_drop_rate: Position embedding dropout rate.
            attn_drop_rate: Attention dropout rate.
            drop_path_rate: Stochastic depth rate.
            weight_init: Weight initialization scheme.
            embed_layer: Patch embedding layer.
            _norm_layer: Normalization layer.
            _act_layer: MLP activation layer.
            block_fn: Transformer block layer.
        &#34;&#34;&#34;
        super().__init__()
        assert global_pool in (&#34;&#34;, &#34;avg&#34;, &#34;token&#34;, &#34;map&#34;)
        assert class_token or global_pool != &#34;token&#34;
        use_fc_norm = global_pool == &#34;avg&#34; if fc_norm is None else fc_norm
        # norm_layer = get_norm_layer(norm_layer) or partial(nn.LayerNorm, eps=1e-6)
        # act_layer = get_act_layer(act_layer) or nn.GELU
        norm_layer = partial(nn.LayerNorm, eps=1e-6)
        act_layer = nn.GELU

        self.num_classes = num_classes
        self.global_pool = global_pool
        self.num_features = self.embed_dim = (
            embed_dim  # num_features for consistency with other models
        )
        self.num_prefix_tokens = 1 if class_token else 0
        self.num_prefix_tokens += reg_tokens
        self.num_reg_tokens = reg_tokens
        self.has_class_token = class_token
        self.no_embed_class = (
            no_embed_class  # don&#39;t embed prefix positions (includes reg)
        )
        self.dynamic_img_size = dynamic_img_size
        self.grad_checkpointing = False
        self.ignore_head = ignore_head

        embed_args = {}
        if dynamic_img_size:
            # flatten deferred until after pos embed
            embed_args.update(dict(strict_img_size=False, output_fmt=&#34;NHWC&#34;))
        self.patch_embed = embed_layer(
            img_size=img_size,
            patch_size=patch_size,
            in_chans=in_chans,
            embed_dim=embed_dim,
            bias=not pre_norm,  # disable bias if pre-norm is used (e.g. CLIP)
            dynamic_img_pad=dynamic_img_pad,
            **embed_args,
        )
        num_patches = self.patch_embed.num_patches

        self.cls_token = (
            nn.Parameter(torch.zeros(1, 1, embed_dim)) if class_token else None
        )
        self.reg_token = (
            nn.Parameter(torch.zeros(1, reg_tokens, embed_dim)) if reg_tokens else None
        )
        embed_len = (
            num_patches if no_embed_class else num_patches + self.num_prefix_tokens
        )
        self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * 0.02)
        self.pos_drop = nn.Dropout(p=pos_drop_rate)
        if patch_drop_rate &gt; 0:
            self.patch_drop = PatchDropout(
                patch_drop_rate,
                num_prefix_tokens=self.num_prefix_tokens,
            )
        else:
            self.patch_drop = nn.Identity()
        self.norm_pre = norm_layer(embed_dim) if pre_norm else nn.Identity()

        dpr = [
            x.item() for x in torch.linspace(0, drop_path_rate, depth)
        ]  # stochastic depth decay rule
        self.blocks = nn.Sequential(
            *[
                block_fn(
                    dim=embed_dim,
                    num_heads=num_heads,
                    mlp_ratio=mlp_ratio,
                    qkv_bias=qkv_bias,
                    qk_norm=qk_norm,
                    init_values=init_values,
                    proj_drop=proj_drop_rate,
                    attn_drop=attn_drop_rate,
                    drop_path=dpr[i],
                    norm_layer=norm_layer,
                    act_layer=act_layer,
                    mlp_layer=mlp_layer,
                )
                for i in range(depth)
            ]
        )
        self.norm = norm_layer(embed_dim) if not use_fc_norm else nn.Identity()

        # Classifier Head
        if global_pool == &#34;map&#34;:
            AttentionPoolLatent.init_weights = init_weights
            self.attn_pool = AttentionPoolLatent(
                self.embed_dim,
                num_heads=num_heads,
                mlp_ratio=mlp_ratio,
                norm_layer=norm_layer,
            )
        else:
            self.attn_pool = None
        self.fc_norm = norm_layer(embed_dim) if use_fc_norm else nn.Identity()
        self.head_drop = nn.Dropout(drop_rate)
        self.head = (
            nn.Linear(self.embed_dim, num_classes) if num_classes &gt; 0 else nn.Identity()
        )

        if weight_init != &#34;skip&#34;:
            self.init_weights(weight_init)

    def init_weights(self, mode: Literal[&#34;jax&#34;, &#34;jax_nlhb&#34;, &#34;moco&#34;, &#34;&#34;] = &#34;&#34;) -&gt; None:
        assert mode in (&#34;jax&#34;, &#34;jax_nlhb&#34;, &#34;moco&#34;, &#34;&#34;)
        # head_bias = -math.log(self.num_classes) if &#34;nlhb&#34; in mode else 0.0
        trunc_normal_(self.pos_embed, std=0.02)
        if self.cls_token is not None:
            nn.init.normal_(self.cls_token, std=1e-6)
        named_apply(init_weights_vit_timm, self)

    @torch.jit.ignore
    def no_weight_decay(self) -&gt; Set:
        return {&#34;pos_embed&#34;, &#34;cls_token&#34;, &#34;dist_token&#34;}

    @torch.jit.ignore
    def group_matcher(self, coarse: bool = False) -&gt; Dict:
        return dict(
            stem=r&#34;^cls_token|pos_embed|patch_embed&#34;,  # stem and embed
            blocks=[(r&#34;^blocks\.(\d+)&#34;, None), (r&#34;^norm&#34;, (99999,))],
        )

    @torch.jit.ignore
    def get_classifier(self) -&gt; nn.Module:
        return self.head

    def reset_classifier(self, num_classes: int, global_pool=None) -&gt; None:
        self.num_classes = num_classes
        if global_pool is not None:
            assert global_pool in (&#34;&#34;, &#34;avg&#34;, &#34;token&#34;, &#34;map&#34;)
            if global_pool == &#34;map&#34; and self.attn_pool is None:
                assert (
                    False
                ), &#34;Cannot currently add attention pooling in reset_classifier().&#34;
            elif global_pool != &#34;map &#34; and self.attn_pool is not None:
                self.attn_pool = None  # remove attention pooling
            self.global_pool = global_pool
        self.head = (
            nn.Linear(self.embed_dim, num_classes) if num_classes &gt; 0 else nn.Identity()
        )

    def _pos_embed(self, x: torch.Tensor) -&gt; torch.Tensor:
        if self.dynamic_img_size:
            B, H, W, C = x.shape
            pos_embed = resample_abs_pos_embed(
                self.pos_embed,
                [H, W],
                num_prefix_tokens=0 if self.no_embed_class else self.num_prefix_tokens,
            )
            x = x.view(B, -1, C)
        else:
            pos_embed = self.pos_embed

        to_cat = []
        if self.cls_token is not None:
            to_cat.append(self.cls_token.expand(x.shape[0], -1, -1))
        if self.reg_token is not None:
            to_cat.append(self.reg_token.expand(x.shape[0], -1, -1))

        if self.no_embed_class:
            # deit-3, updated JAX (big vision)
            # position embedding does not overlap with class token, add then concat
            x = x + pos_embed
            if to_cat:
                x = torch.cat(to_cat + [x], dim=1)
        else:
            # original timm, JAX, and deit vit impl
            # pos_embed has entry for class token, concat then add
            if to_cat:
                x = torch.cat(to_cat + [x], dim=1)
            x = x + pos_embed

        return self.pos_drop(x)

    def _intermediate_layers(
        self,
        x: torch.Tensor,
        n: Union[int, Sequence] = 1,
    ) -&gt; List[torch.Tensor]:
        outputs, num_blocks = [], len(self.blocks)
        take_indices = set(
            range(num_blocks - n, num_blocks) if isinstance(n, int) else n
        )

        # forward pass
        x = self.patch_embed(x)
        x = self._pos_embed(x)
        x = self.patch_drop(x)
        x = self.norm_pre(x)
        for i, blk in enumerate(self.blocks):
            x = blk(x)
            if i in take_indices:
                outputs.append(x)

        return outputs

    def forward_features(self, x: torch.Tensor) -&gt; torch.Tensor:
        x = self.patch_embed(x)
        x = self._pos_embed(x)
        x = self.patch_drop(x)
        x = self.norm_pre(x)
        x = self.blocks(x)
        x = self.norm(x)
        return x

    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -&gt; torch.Tensor:
        if self.attn_pool is not None:
            x = self.attn_pool(x)
        elif self.global_pool == &#34;avg&#34;:
            x = x[:, self.num_prefix_tokens :].mean(dim=1)
        elif self.global_pool:
            x = x[:, 0]  # class token
        x = self.fc_norm(x)
        x = self.head_drop(x)
        return x if pre_logits else self.head(x)

    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        x = self.forward_features(x)
        if not self.ignore_head:
            x = self.forward_head(x)
        return x</code></pre>
</details>
<div class="desc"><p>Vision Transformer</p>
<p>A PyTorch impl of : <code>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</code>
- <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>img_size</code></strong></dt>
<dd>Input image size.</dd>
<dt><strong><code>patch_size</code></strong></dt>
<dd>Patch size.</dd>
<dt><strong><code>in_chans</code></strong></dt>
<dd>Number of image input channels.</dd>
<dt><strong><code>num_classes</code></strong></dt>
<dd>Number of classes for classification head.</dd>
<dt><strong><code>global_pool</code></strong></dt>
<dd>Type of global pooling for final sequence (default: 'token').</dd>
<dt><strong><code>embed_dim</code></strong></dt>
<dd>Transformer embedding dimension.</dd>
<dt><strong><code>depth</code></strong></dt>
<dd>Depth of transformer.</dd>
<dt><strong><code>num_heads</code></strong></dt>
<dd>Number of attention heads.</dd>
<dt><strong><code>mlp_ratio</code></strong></dt>
<dd>Ratio of mlp hidden dim to embedding dim.</dd>
<dt><strong><code>qkv_bias</code></strong></dt>
<dd>Enable bias for qkv projections if True.</dd>
<dt><strong><code>init_values</code></strong></dt>
<dd>Layer-scale init values (layer-scale enabled if not None).</dd>
<dt><strong><code>class_token</code></strong></dt>
<dd>Use class token.</dd>
<dt><strong><code>no_embed_class</code></strong></dt>
<dd>Don't include position embeddings for class (or reg) tokens.</dd>
<dt><strong><code>reg_tokens</code></strong></dt>
<dd>Number of register tokens.</dd>
<dt><strong><code>fc_norm</code></strong></dt>
<dd>Pre head norm after pool (instead of before), if None, enabled when global_pool == 'avg'.</dd>
<dt><strong><code>drop_rate</code></strong></dt>
<dd>Head dropout rate.</dd>
<dt><strong><code>pos_drop_rate</code></strong></dt>
<dd>Position embedding dropout rate.</dd>
<dt><strong><code>attn_drop_rate</code></strong></dt>
<dd>Attention dropout rate.</dd>
<dt><strong><code>drop_path_rate</code></strong></dt>
<dd>Stochastic depth rate.</dd>
<dt><strong><code>weight_init</code></strong></dt>
<dd>Weight initialization scheme.</dd>
<dt><strong><code>embed_layer</code></strong></dt>
<dd>Patch embedding layer.</dd>
<dt><strong><code>_norm_layer</code></strong></dt>
<dd>Normalization layer.</dd>
<dt><strong><code>_act_layer</code></strong></dt>
<dd>MLP activation layer.</dd>
<dt><strong><code>block_fn</code></strong></dt>
<dd>Transformer block layer.</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.VisionTransformer.dynamic_img_size"><code class="name">var <span class="ident">dynamic_img_size</span> : Final[bool]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.VisionTransformer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
    x = self.forward_features(x)
    if not self.ignore_head:
        x = self.forward_head(x)
    return x</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.VisionTransformer.forward_features"><code class="name flex">
<span>def <span class="ident">forward_features</span></span>(<span>self, x: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_features(self, x: torch.Tensor) -&gt; torch.Tensor:
    x = self.patch_embed(x)
    x = self._pos_embed(x)
    x = self.patch_drop(x)
    x = self.norm_pre(x)
    x = self.blocks(x)
    x = self.norm(x)
    return x</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.VisionTransformer.forward_head"><code class="name flex">
<span>def <span class="ident">forward_head</span></span>(<span>self, x: torch.Tensor, pre_logits: bool = False) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -&gt; torch.Tensor:
    if self.attn_pool is not None:
        x = self.attn_pool(x)
    elif self.global_pool == &#34;avg&#34;:
        x = x[:, self.num_prefix_tokens :].mean(dim=1)
    elif self.global_pool:
        x = x[:, 0]  # class token
    x = self.fc_norm(x)
    x = self.head_drop(x)
    return x if pre_logits else self.head(x)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.VisionTransformer.get_classifier"><code class="name flex">
<span>def <span class="ident">get_classifier</span></span>(<span>self) ‑> torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.jit.ignore
def get_classifier(self) -&gt; nn.Module:
    return self.head</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.VisionTransformer.group_matcher"><code class="name flex">
<span>def <span class="ident">group_matcher</span></span>(<span>self, coarse: bool = False) ‑> Dict</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.jit.ignore
def group_matcher(self, coarse: bool = False) -&gt; Dict:
    return dict(
        stem=r&#34;^cls_token|pos_embed|patch_embed&#34;,  # stem and embed
        blocks=[(r&#34;^blocks\.(\d+)&#34;, None), (r&#34;^norm&#34;, (99999,))],
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.VisionTransformer.init_weights"><code class="name flex">
<span>def <span class="ident">init_weights</span></span>(<span>self, mode: Literal['jax', 'jax_nlhb', 'moco', ''] = '') ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_weights(self, mode: Literal[&#34;jax&#34;, &#34;jax_nlhb&#34;, &#34;moco&#34;, &#34;&#34;] = &#34;&#34;) -&gt; None:
    assert mode in (&#34;jax&#34;, &#34;jax_nlhb&#34;, &#34;moco&#34;, &#34;&#34;)
    # head_bias = -math.log(self.num_classes) if &#34;nlhb&#34; in mode else 0.0
    trunc_normal_(self.pos_embed, std=0.02)
    if self.cls_token is not None:
        nn.init.normal_(self.cls_token, std=1e-6)
    named_apply(init_weights_vit_timm, self)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.VisionTransformer.no_weight_decay"><code class="name flex">
<span>def <span class="ident">no_weight_decay</span></span>(<span>self) ‑> Set</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.jit.ignore
def no_weight_decay(self) -&gt; Set:
    return {&#34;pos_embed&#34;, &#34;cls_token&#34;, &#34;dist_token&#34;}</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.VisionTransformer.reset_classifier"><code class="name flex">
<span>def <span class="ident">reset_classifier</span></span>(<span>self, num_classes: int, global_pool=None) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_classifier(self, num_classes: int, global_pool=None) -&gt; None:
    self.num_classes = num_classes
    if global_pool is not None:
        assert global_pool in (&#34;&#34;, &#34;avg&#34;, &#34;token&#34;, &#34;map&#34;)
        if global_pool == &#34;map&#34; and self.attn_pool is None:
            assert (
                False
            ), &#34;Cannot currently add attention pooling in reset_classifier().&#34;
        elif global_pool != &#34;map &#34; and self.attn_pool is not None:
            self.attn_pool = None  # remove attention pooling
        self.global_pool = global_pool
    self.head = (
        nn.Linear(self.embed_dim, num_classes) if num_classes &gt; 0 else nn.Identity()
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.VisionTransformerBlock"><code class="flex name class">
<span>class <span class="ident">VisionTransformerBlock</span></span>
<span>(</span><span>dim: int,<br>num_heads: int,<br>mlp_ratio: float = 4.0,<br>qkv_bias: bool = False,<br>qk_norm: bool = False,<br>proj_drop: float = 0.0,<br>attn_drop: float = 0.0,<br>init_values: float | None = None,<br>drop_path: float = 0.0,<br>act_layer: torch.nn.modules.module.Module = torch.nn.modules.activation.GELU,<br>norm_layer: torch.nn.modules.module.Module = torch.nn.modules.normalization.LayerNorm,<br>mlp_layer: torch.nn.modules.module.Module = sglang.srt.models.deepseek_janus_pro.Mlp)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VisionTransformerBlock(nn.Module):
    def __init__(
        self,
        dim: int,
        num_heads: int,
        mlp_ratio: float = 4.0,
        qkv_bias: bool = False,
        qk_norm: bool = False,
        proj_drop: float = 0.0,
        attn_drop: float = 0.0,
        init_values: Optional[float] = None,
        drop_path: float = 0.0,
        act_layer: nn.Module = nn.GELU,
        norm_layer: nn.Module = nn.LayerNorm,
        mlp_layer: nn.Module = Mlp,
    ) -&gt; None:
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = VisionAttention(
            embed_dim=dim,
            num_heads=num_heads,
            projection_size=dim,
            use_qkv_parallel=True,
            qkv_backend=&#34;sdpa&#34;,
            softmax_in_single_precision=False,
            dropout=attn_drop,
        )

        self.ls1 = (
            LayerScale(dim, init_values=init_values) if init_values else nn.Identity()
        )
        self.drop_path1 = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity()

        self.norm2 = norm_layer(dim)
        self.mlp = mlp_layer(
            in_features=dim,
            hidden_features=int(dim * mlp_ratio),
            act_layer=act_layer,
            drop=proj_drop,
        )
        self.ls2 = (
            LayerScale(dim, init_values=init_values) if init_values else nn.Identity()
        )
        self.drop_path2 = DropPath(drop_path) if drop_path &gt; 0.0 else nn.Identity()

    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))
        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
        return x</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.VisionTransformerBlock.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))
    x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
    return x</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.deepseek_janus_pro.vision_head"><code class="flex name class">
<span>class <span class="ident">vision_head</span></span>
<span>(</span><span>params)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class vision_head(torch.nn.Module):
    def __init__(self, params):
        super().__init__()
        self.output_mlp_projector = torch.nn.Linear(
            params[&#34;n_embed&#34;], params[&#34;image_token_embed&#34;]
        )
        self.vision_activation = torch.nn.GELU()
        self.vision_head = torch.nn.Linear(
            params[&#34;image_token_embed&#34;], params[&#34;image_token_size&#34;]
        )

    def forward(self, x):
        x = self.output_mlp_projector(x)
        x = self.vision_activation(x)
        x = self.vision_head(x)
        return x</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.deepseek_janus_pro.vision_head.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    x = self.output_mlp_projector(x)
    x = self.vision_activation(x)
    x = self.vision_head(x)
    return x</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.models" href="index.html">sglang.srt.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.Normalize" href="#sglang.srt.models.deepseek_janus_pro.Normalize">Normalize</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.VQ_16" href="#sglang.srt.models.deepseek_janus_pro.VQ_16">VQ_16</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.compute_entropy_loss" href="#sglang.srt.models.deepseek_janus_pro.compute_entropy_loss">compute_entropy_loss</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.create_siglip_vit" href="#sglang.srt.models.deepseek_janus_pro.create_siglip_vit">create_siglip_vit</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.drop_path" href="#sglang.srt.models.deepseek_janus_pro.drop_path">drop_path</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.init_weights" href="#sglang.srt.models.deepseek_janus_pro.init_weights">init_weights</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.init_weights_vit_timm" href="#sglang.srt.models.deepseek_janus_pro.init_weights_vit_timm">init_weights_vit_timm</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.model_name_to_cls" href="#sglang.srt.models.deepseek_janus_pro.model_name_to_cls">model_name_to_cls</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.named_apply" href="#sglang.srt.models.deepseek_janus_pro.named_apply">named_apply</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.nchw_to" href="#sglang.srt.models.deepseek_janus_pro.nchw_to">nchw_to</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.nonlinearity" href="#sglang.srt.models.deepseek_janus_pro.nonlinearity">nonlinearity</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.resample_abs_pos_embed" href="#sglang.srt.models.deepseek_janus_pro.resample_abs_pos_embed">resample_abs_pos_embed</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.resample_patch_embed" href="#sglang.srt.models.deepseek_janus_pro.resample_patch_embed">resample_patch_embed</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.to_2tuple" href="#sglang.srt.models.deepseek_janus_pro.to_2tuple">to_2tuple</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.trunc_normal_tf_" href="#sglang.srt.models.deepseek_janus_pro.trunc_normal_tf_">trunc_normal_tf_</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.use_fused_attn" href="#sglang.srt.models.deepseek_janus_pro.use_fused_attn">use_fused_attn</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.AttentionPoolLatent" href="#sglang.srt.models.deepseek_janus_pro.AttentionPoolLatent">AttentionPoolLatent</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.AttentionPoolLatent.forward" href="#sglang.srt.models.deepseek_janus_pro.AttentionPoolLatent.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.AttentionPoolLatent.fused_attn" href="#sglang.srt.models.deepseek_janus_pro.AttentionPoolLatent.fused_attn">fused_attn</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.AttentionPoolLatent.init_weights" href="#sglang.srt.models.deepseek_janus_pro.AttentionPoolLatent.init_weights">init_weights</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.AttnBlock" href="#sglang.srt.models.deepseek_janus_pro.AttnBlock">AttnBlock</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.AttnBlock.forward" href="#sglang.srt.models.deepseek_janus_pro.AttnBlock.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.CLIPVisionTower" href="#sglang.srt.models.deepseek_janus_pro.CLIPVisionTower">CLIPVisionTower</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.CLIPVisionTower.build_vision_tower" href="#sglang.srt.models.deepseek_janus_pro.CLIPVisionTower.build_vision_tower">build_vision_tower</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.CLIPVisionTower.device" href="#sglang.srt.models.deepseek_janus_pro.CLIPVisionTower.device">device</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.CLIPVisionTower.dtype" href="#sglang.srt.models.deepseek_janus_pro.CLIPVisionTower.dtype">dtype</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.CLIPVisionTower.feature_select" href="#sglang.srt.models.deepseek_janus_pro.CLIPVisionTower.feature_select">feature_select</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.CLIPVisionTower.forward" href="#sglang.srt.models.deepseek_janus_pro.CLIPVisionTower.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.Decoder" href="#sglang.srt.models.deepseek_janus_pro.Decoder">Decoder</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.Decoder.forward" href="#sglang.srt.models.deepseek_janus_pro.Decoder.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.Decoder.last_layer" href="#sglang.srt.models.deepseek_janus_pro.Decoder.last_layer">last_layer</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.Downsample" href="#sglang.srt.models.deepseek_janus_pro.Downsample">Downsample</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.Downsample.forward" href="#sglang.srt.models.deepseek_janus_pro.Downsample.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.DropPath" href="#sglang.srt.models.deepseek_janus_pro.DropPath">DropPath</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.DropPath.extra_repr" href="#sglang.srt.models.deepseek_janus_pro.DropPath.extra_repr">extra_repr</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.DropPath.forward" href="#sglang.srt.models.deepseek_janus_pro.DropPath.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.Encoder" href="#sglang.srt.models.deepseek_janus_pro.Encoder">Encoder</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.Encoder.forward" href="#sglang.srt.models.deepseek_janus_pro.Encoder.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.Format" href="#sglang.srt.models.deepseek_janus_pro.Format">Format</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.Format.NCHW" href="#sglang.srt.models.deepseek_janus_pro.Format.NCHW">NCHW</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.Format.NCL" href="#sglang.srt.models.deepseek_janus_pro.Format.NCL">NCL</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.Format.NHWC" href="#sglang.srt.models.deepseek_janus_pro.Format.NHWC">NHWC</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.Format.NLC" href="#sglang.srt.models.deepseek_janus_pro.Format.NLC">NLC</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.LayerScale" href="#sglang.srt.models.deepseek_janus_pro.LayerScale">LayerScale</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.LayerScale.forward" href="#sglang.srt.models.deepseek_janus_pro.LayerScale.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.Mlp" href="#sglang.srt.models.deepseek_janus_pro.Mlp">Mlp</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.Mlp.forward" href="#sglang.srt.models.deepseek_janus_pro.Mlp.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.MlpProjector" href="#sglang.srt.models.deepseek_janus_pro.MlpProjector">MlpProjector</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.MlpProjector.forward" href="#sglang.srt.models.deepseek_janus_pro.MlpProjector.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.ModelArgs" href="#sglang.srt.models.deepseek_janus_pro.ModelArgs">ModelArgs</a></code></h4>
<ul class="two-column">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.ModelArgs.codebook_embed_dim" href="#sglang.srt.models.deepseek_janus_pro.ModelArgs.codebook_embed_dim">codebook_embed_dim</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.ModelArgs.codebook_l2_norm" href="#sglang.srt.models.deepseek_janus_pro.ModelArgs.codebook_l2_norm">codebook_l2_norm</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.ModelArgs.codebook_show_usage" href="#sglang.srt.models.deepseek_janus_pro.ModelArgs.codebook_show_usage">codebook_show_usage</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.ModelArgs.codebook_size" href="#sglang.srt.models.deepseek_janus_pro.ModelArgs.codebook_size">codebook_size</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.ModelArgs.commit_loss_beta" href="#sglang.srt.models.deepseek_janus_pro.ModelArgs.commit_loss_beta">commit_loss_beta</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.ModelArgs.decoder_ch_mult" href="#sglang.srt.models.deepseek_janus_pro.ModelArgs.decoder_ch_mult">decoder_ch_mult</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.ModelArgs.dropout_p" href="#sglang.srt.models.deepseek_janus_pro.ModelArgs.dropout_p">dropout_p</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.ModelArgs.encoder_ch_mult" href="#sglang.srt.models.deepseek_janus_pro.ModelArgs.encoder_ch_mult">encoder_ch_mult</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.ModelArgs.entropy_loss_ratio" href="#sglang.srt.models.deepseek_janus_pro.ModelArgs.entropy_loss_ratio">entropy_loss_ratio</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.ModelArgs.z_channels" href="#sglang.srt.models.deepseek_janus_pro.ModelArgs.z_channels">z_channels</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM" href="#sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM">MultiModalityCausalLM</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM.forward" href="#sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM.get_image_feature" href="#sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM.get_image_feature">get_image_feature</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM.get_input_embeddings" href="#sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM.get_input_embeddings">get_input_embeddings</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM.load_weights" href="#sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM.load_weights">load_weights</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM.pad_input_ids" href="#sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM.pad_input_ids">pad_input_ids</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM.prepare_gen_img_embeds" href="#sglang.srt.models.deepseek_janus_pro.MultiModalityCausalLM.prepare_gen_img_embeds">prepare_gen_img_embeds</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.MultiModalityPreTrainedModel" href="#sglang.srt.models.deepseek_janus_pro.MultiModalityPreTrainedModel">MultiModalityPreTrainedModel</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.MultiModalityPreTrainedModel.base_model_prefix" href="#sglang.srt.models.deepseek_janus_pro.MultiModalityPreTrainedModel.base_model_prefix">base_model_prefix</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.MultiModalityPreTrainedModel.config_class" href="#sglang.srt.models.deepseek_janus_pro.MultiModalityPreTrainedModel.config_class">config_class</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.PatchDropout" href="#sglang.srt.models.deepseek_janus_pro.PatchDropout">PatchDropout</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.PatchDropout.forward" href="#sglang.srt.models.deepseek_janus_pro.PatchDropout.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.PatchDropout.return_indices" href="#sglang.srt.models.deepseek_janus_pro.PatchDropout.return_indices">return_indices</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.PatchEmbed" href="#sglang.srt.models.deepseek_janus_pro.PatchEmbed">PatchEmbed</a></code></h4>
<ul class="two-column">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.PatchEmbed.dynamic_feat_size" href="#sglang.srt.models.deepseek_janus_pro.PatchEmbed.dynamic_feat_size">dynamic_feat_size</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.PatchEmbed.dynamic_img_pad" href="#sglang.srt.models.deepseek_janus_pro.PatchEmbed.dynamic_img_pad">dynamic_img_pad</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.PatchEmbed.feat_ratio" href="#sglang.srt.models.deepseek_janus_pro.PatchEmbed.feat_ratio">feat_ratio</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.PatchEmbed.forward" href="#sglang.srt.models.deepseek_janus_pro.PatchEmbed.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.PatchEmbed.output_fmt" href="#sglang.srt.models.deepseek_janus_pro.PatchEmbed.output_fmt">output_fmt</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.PatchEmbed.set_input_size" href="#sglang.srt.models.deepseek_janus_pro.PatchEmbed.set_input_size">set_input_size</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.ResnetBlock" href="#sglang.srt.models.deepseek_janus_pro.ResnetBlock">ResnetBlock</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.ResnetBlock.forward" href="#sglang.srt.models.deepseek_janus_pro.ResnetBlock.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.Upsample" href="#sglang.srt.models.deepseek_janus_pro.Upsample">Upsample</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.Upsample.forward" href="#sglang.srt.models.deepseek_janus_pro.Upsample.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.VQModel" href="#sglang.srt.models.deepseek_janus_pro.VQModel">VQModel</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.VQModel.decode" href="#sglang.srt.models.deepseek_janus_pro.VQModel.decode">decode</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.VQModel.decode_code" href="#sglang.srt.models.deepseek_janus_pro.VQModel.decode_code">decode_code</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.VQModel.encode" href="#sglang.srt.models.deepseek_janus_pro.VQModel.encode">encode</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.VQModel.forward" href="#sglang.srt.models.deepseek_janus_pro.VQModel.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.VectorQuantizer" href="#sglang.srt.models.deepseek_janus_pro.VectorQuantizer">VectorQuantizer</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.VectorQuantizer.forward" href="#sglang.srt.models.deepseek_janus_pro.VectorQuantizer.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.VectorQuantizer.get_codebook_entry" href="#sglang.srt.models.deepseek_janus_pro.VectorQuantizer.get_codebook_entry">get_codebook_entry</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.VisionTransformer" href="#sglang.srt.models.deepseek_janus_pro.VisionTransformer">VisionTransformer</a></code></h4>
<ul class="two-column">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.VisionTransformer.dynamic_img_size" href="#sglang.srt.models.deepseek_janus_pro.VisionTransformer.dynamic_img_size">dynamic_img_size</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.VisionTransformer.forward" href="#sglang.srt.models.deepseek_janus_pro.VisionTransformer.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.VisionTransformer.forward_features" href="#sglang.srt.models.deepseek_janus_pro.VisionTransformer.forward_features">forward_features</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.VisionTransformer.forward_head" href="#sglang.srt.models.deepseek_janus_pro.VisionTransformer.forward_head">forward_head</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.VisionTransformer.get_classifier" href="#sglang.srt.models.deepseek_janus_pro.VisionTransformer.get_classifier">get_classifier</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.VisionTransformer.group_matcher" href="#sglang.srt.models.deepseek_janus_pro.VisionTransformer.group_matcher">group_matcher</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.VisionTransformer.init_weights" href="#sglang.srt.models.deepseek_janus_pro.VisionTransformer.init_weights">init_weights</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.VisionTransformer.no_weight_decay" href="#sglang.srt.models.deepseek_janus_pro.VisionTransformer.no_weight_decay">no_weight_decay</a></code></li>
<li><code><a title="sglang.srt.models.deepseek_janus_pro.VisionTransformer.reset_classifier" href="#sglang.srt.models.deepseek_janus_pro.VisionTransformer.reset_classifier">reset_classifier</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.VisionTransformerBlock" href="#sglang.srt.models.deepseek_janus_pro.VisionTransformerBlock">VisionTransformerBlock</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.VisionTransformerBlock.forward" href="#sglang.srt.models.deepseek_janus_pro.VisionTransformerBlock.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.deepseek_janus_pro.vision_head" href="#sglang.srt.models.deepseek_janus_pro.vision_head">vision_head</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.deepseek_janus_pro.vision_head.forward" href="#sglang.srt.models.deepseek_janus_pro.vision_head.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
