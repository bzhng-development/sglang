<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.models.minicpmv API documentation</title>
<meta name="description" content="Inference-only MiniCPM-V model compatible with HuggingFace weights.">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.models.minicpmv</code></h1>
</header>
<section id="section-intro">
<p>Inference-only MiniCPM-V model compatible with HuggingFace weights.</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.models.minicpmv.get_1d_sincos_pos_embed_from_grid"><code class="name flex">
<span>def <span class="ident">get_1d_sincos_pos_embed_from_grid</span></span>(<span>embed_dim: int, pos: numpy.ndarray, version: Tuple[int, int] = (2, 0)) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_1d_sincos_pos_embed_from_grid(
    embed_dim: int, pos: np.ndarray, version: Tuple[int, int] = (2, 0)
) -&gt; torch.Tensor:
    &#34;&#34;&#34;
    embed_dim: output dimension for each position
    pos: a list of positions to be encoded: size (M,) / (H, W)
    out: (M, D) / (H, W, D)
    &#34;&#34;&#34;
    assert embed_dim % 2 == 0
    omega = np.arange(embed_dim // 2, dtype=np.float32)
    omega /= embed_dim / 2.0
    omega = 1.0 / 10000**omega  # (D/2,)

    if version == (2, 0):
        pos = pos.reshape(-1)  # (M,)
        out = np.einsum(&#34;m,d-&gt;md&#34;, pos, omega)  # (M, D/2), outer product
        emb_sin = np.sin(out)  # (M, D/2)
        emb_cos = np.cos(out)  # (M, D/2)
        emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)
    else:
        out = np.einsum(&#34;hw,d-&gt;hwd&#34;, pos, omega)  # (H, W, D/2), outer product
        emb_sin = np.sin(out)  # (H, W, D/2)
        emb_cos = np.cos(out)  # (H, W, D/2)
        emb = np.concatenate([emb_sin, emb_cos], axis=-1)  # (H, W, D)
    return emb</code></pre>
</details>
<div class="desc"><p>embed_dim: output dimension for each position
pos: a list of positions to be encoded: size (M,) / (H, W)
out: (M, D) / (H, W, D)</p></div>
</dd>
<dt id="sglang.srt.models.minicpmv.get_2d_sincos_pos_embed"><code class="name flex">
<span>def <span class="ident">get_2d_sincos_pos_embed</span></span>(<span>embed_dim: int,<br>grid_size: Tuple[int, int] | int,<br>cls_token: bool = False,<br>version: Tuple[int, int] = (2, 0)) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_2d_sincos_pos_embed(
    embed_dim: int,
    grid_size: Union[int, Tuple[int, int]],
    cls_token: bool = False,
    version: Tuple[int, int] = (2, 0),
) -&gt; torch.Tensor:
    &#34;&#34;&#34;
    grid_size: int of the grid height and width
    return:
    pos_embed: [grid_size*grid_size, embed_dim] or
                [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)
    &#34;&#34;&#34;
    if isinstance(grid_size, int):
        grid_h_size, grid_w_size = grid_size, grid_size
    else:
        grid_h_size, grid_w_size = grid_size[0], grid_size[1]

    grid_h = np.arange(grid_h_size, dtype=np.float32)
    grid_w = np.arange(grid_w_size, dtype=np.float32)
    grid = np.meshgrid(grid_w, grid_h)  # here w goes first
    grid = np.stack(grid, axis=0)
    assert isinstance(grid, np.ndarray) and grid.shape == (2, grid_h_size, grid_w_size)

    if version == (2, 0):
        grid = grid.reshape([2, 1, grid_h_size, grid_w_size])
        pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid, version)
        if cls_token:
            pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    else:
        pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid, version)
    return pos_embed</code></pre>
</details>
<div class="desc"><p>grid_size: int of the grid height and width
return:
pos_embed: [grid_size<em>grid_size, embed_dim] or
[1+grid_size</em>grid_size, embed_dim] (w/ or w/o cls_token)</p></div>
</dd>
<dt id="sglang.srt.models.minicpmv.get_2d_sincos_pos_embed_from_grid"><code class="name flex">
<span>def <span class="ident">get_2d_sincos_pos_embed_from_grid</span></span>(<span>embed_dim: int, grid: numpy.ndarray, version: Tuple[int, int] = (2, 0)) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_2d_sincos_pos_embed_from_grid(
    embed_dim: int, grid: np.ndarray, version: Tuple[int, int] = (2, 0)
) -&gt; torch.Tensor:
    assert embed_dim % 2 == 0

    # use half of dimensions to encode grid_h
    emb_h = get_1d_sincos_pos_embed_from_grid(
        embed_dim // 2, grid[0], version
    )  # (H*W, D/2) or (H, W, D/2)
    emb_w = get_1d_sincos_pos_embed_from_grid(
        embed_dim // 2, grid[1], version
    )  # (H*W, D/2) or (H, W, D/2)

    if version == (2, 0):
        emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)
    else:
        emb = np.concatenate([emb_h, emb_w], axis=-1)  # (H, W, D)
    return emb</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.get_version_by_config"><code class="name flex">
<span>def <span class="ident">get_version_by_config</span></span>(<span>config: transformers.configuration_utils.PretrainedConfig) ‑> Tuple[int, ...]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_version_by_config(config: PretrainedConfig) -&gt; Tuple[int, ...]:
    version_float = getattr(config, &#34;version&#34;, None)

    # The old configs do not include version number
    # TODO: Remove this after the HF repos are updated
    if version_float is None:
        if config.hidden_size == 2304 and config.query_num == 64:
            return 2, 0
        return 2, 5

    version_str = str(version_float)
    return tuple(int(x) for x in version_str.split(&#34;.&#34;))</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.models.minicpmv.BaseResampler"><code class="flex name class">
<span>class <span class="ident">BaseResampler</span></span>
<span>(</span><span>num_queries: int,<br>embed_dim: int,<br>num_heads: int,<br>kv_dim: int | None = None,<br>norm_layer: Callable[[int], torch.nn.modules.normalization.LayerNorm] = functools.partial(&lt;class &#x27;torch.nn.modules.normalization.LayerNorm&#x27;&gt;, eps=1e-06),<br>do_post_projection: bool = True,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseResampler(nn.Module):
    &#34;&#34;&#34;
    A 2D perceiver-resampler network with one cross attention layers by
        (grid_size**2) learnable queries and 2d sincos pos_emb.
    Outputs:
        A tensor with the shape of (grid_size**2, embed_dim)
    &#34;&#34;&#34;

    def __init__(
        self,
        num_queries: int,
        embed_dim: int,
        num_heads: int,
        kv_dim: Optional[int] = None,
        norm_layer: Callable[[int], nn.LayerNorm] = DEFAULT_LN,
        do_post_projection: bool = True,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; None:
        super().__init__()

        self.num_queries = num_queries
        self.embed_dim = embed_dim
        self.num_heads = num_heads

        self.query = nn.Parameter(torch.zeros(self.num_queries, embed_dim))
        trunc_normal_(self.query, std=0.02)
        if kv_dim is not None and kv_dim != embed_dim:
            self.kv_proj = ReplicatedLinear(
                kv_dim,
                embed_dim,
                bias=False,
                quant_config=quant_config,
                prefix=add_prefix(&#34;kv_proj&#34;, prefix),
            )
        else:
            # Maintain the same return value with ReplicatedLinear.forward
            self.kv_proj = lambda *args, **kwargs: (  # type: ignore # noqa
                nn.Identity()(*args, **kwargs),
                None,
            )
        self.attn = nn.MultiheadAttention(embed_dim, num_heads)
        self.ln_q = norm_layer(embed_dim)
        self.ln_kv = norm_layer(embed_dim)
        self.do_post_projection = do_post_projection
        self.ln_post = norm_layer(embed_dim) if do_post_projection else None
        self.proj = (
            nn.Parameter((embed_dim**-0.5) * torch.randn(embed_dim, embed_dim))
            if do_post_projection
            else None
        )

    def _init_weights(self, m: nn.Module) -&gt; None:
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=0.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def _repeat(self, query, N: int):
        return query.unsqueeze(1).repeat(1, N, 1)</code></pre>
</details>
<div class="desc"><p>A 2D perceiver-resampler network with one cross attention layers by
(grid_size**2) learnable queries and 2d sincos pos_emb.</p>
<h2 id="outputs">Outputs</h2>
<p>A tensor with the shape of (grid_size**2, embed_dim)</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.models.minicpmv.Resampler2_5" href="#sglang.srt.models.minicpmv.Resampler2_5">Resampler2_5</a></li>
</ul>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMBaseModel"><code class="flex name class">
<span>class <span class="ident">MiniCPMBaseModel</span></span>
<span>(</span><span>*,<br>config: transformers.configuration_utils.PretrainedConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MiniCPMBaseModel(nn.Module):
    &#34;&#34;&#34;
    The abstract class of MiniCPMV can only be inherited, but cannot be
    instantiated.
    &#34;&#34;&#34;

    def __init__(
        self,
        *,
        config: PretrainedConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ):
        super().__init__()
        # All MiniCPM-V models disable `tie_word_embeddings` but
        # `PretrainedConfig.tie_word_embeddings` defaults to True; we cannot
        # check `tie_word_embeddings` until SGLang integrate MiniCPM-V model
        # and config class
        self.config = config

        self.version = get_version_by_config(self.config)
        self.llm = self.init_llm(
            config=config, quant_config=quant_config, prefix=add_prefix(&#34;llm&#34;, prefix)
        )
        self.vpm = self.init_vision_module(
            config, quant_config, add_prefix(&#34;vpm&#34;, prefix)
        )
        self.vision_dim = (
            self.vpm.embed_dim
            if self.version == (2, 0)
            else self.vpm.embeddings.embed_dim
        )
        self.embed_dim = self.config.hidden_size

        self.resampler = self.init_resampler(
            self.embed_dim,
            self.vision_dim,
            quant_config=quant_config,
            prefix=add_prefix(&#34;resampler&#34;, prefix),
        )

        self.logits_processor = LogitsProcessor(config)

    def _get_image_bounds(
        self,
        input_ids: torch.Tensor,
        pad_values: List[int],
        im_start_id: int,
        im_end_id: int,
        slice_start_id: Optional[int] = None,
        slice_end_id: Optional[int] = None,
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Returns a tensor indicating the bounds (start and end token ids) of the images
        &#34;&#34;&#34;
        # All the images in the batch should share the same special image
        # bound token ids.
        start_cond = input_ids == im_start_id
        end_cond = input_ids == im_end_id
        if slice_start_id is not None:
            start_cond |= input_ids == slice_start_id
            end_cond |= input_ids == slice_end_id

        (image_start_tokens,) = torch.where(start_cond)
        image_start_tokens += 1
        (image_end_tokens,) = torch.where(end_cond)

        # the im_start_id sometimes can be cached as prefix, but it is needed for the embedding of the images
        if len(image_start_tokens) != len(image_end_tokens):
            if (
                len(image_start_tokens) + 1 == len(image_end_tokens)
                and input_ids[0] in pad_values
                and len(image_start_tokens) != 0
                and len(image_end_tokens) != 0
                and image_end_tokens[0] &lt; image_start_tokens[0]
            ):
                image_start_tokens = torch.cat(
                    [
                        torch.tensor([0], device=image_start_tokens.device),
                        image_start_tokens,
                    ]
                )
        valid_image_nums = min(len(image_start_tokens), len(image_end_tokens))

        if valid_image_nums == 0:
            return torch.zeros((0, 2), device=input_ids.device)

        # Filter out pairs where start_token &gt;= end_token
        valid_pairs = []
        for i in range(valid_image_nums):
            start_token = image_start_tokens[i]
            end_token = image_end_tokens[i]
            if start_token &lt; end_token:
                valid_pairs.append((start_token, end_token))

        if not valid_pairs:
            return torch.zeros((0, 2), device=input_ids.device)

        # Convert valid pairs to tensor
        valid_pairs_tensor = torch.tensor(valid_pairs, device=input_ids.device)
        return valid_pairs_tensor

    def _parse_and_validate_inputs(
        self,
        input_ids: torch.Tensor,
        **kwargs: object,
    ) -&gt; Optional[MiniCPMVImageInputs]:
        pixel_values = kwargs.pop(&#34;pixel_values&#34;, [])
        tgt_sizes = kwargs.pop(&#34;tgt_sizes&#34;, [])
        im_start_id = kwargs.pop(&#34;im_start_id&#34;, None)
        im_end_id = kwargs.pop(&#34;im_end_id&#34;, None)
        slice_start_id = kwargs.pop(&#34;slice_start_id&#34;, None)
        slice_end_id = kwargs.pop(&#34;slice_end_id&#34;, None)
        image_embeds = kwargs.pop(&#34;image_embeds&#34;, None)
        pad_values = kwargs.pop(&#34;pad_values&#34;, None)

        if image_embeds is not None:
            image_bounds = self._get_image_bounds(
                input_ids=input_ids,
                pad_values=pad_values,
                im_start_id=im_start_id,
                im_end_id=im_end_id,
                slice_start_id=slice_start_id,
                slice_end_id=slice_end_id,
            )
            if not isinstance(image_embeds, (torch.Tensor, list)):
                raise ValueError(
                    f&#34;Incorrect type of image embeds. &#34;
                    f&#34;Got type: {type(image_embeds)}&#34;
                )

            if isinstance(image_embeds, list):
                image_embeds = torch.cat(image_embeds)

            return MiniCPMVImageEmbeddingInputs(
                image_bounds=image_bounds,
                data=image_embeds,
                type=&#34;image_embeds&#34;,
            )

        image_bounds = self._get_image_bounds(
            input_ids=input_ids,
            pad_values=pad_values,
            im_start_id=im_start_id,
            im_end_id=im_end_id,
            slice_start_id=slice_start_id,
            slice_end_id=slice_end_id,
        )
        return MiniCPMVImagePixelInputs(
            image_bounds=image_bounds.to(device=input_ids.device),
            data=pixel_values,
            tgt_sizes=tgt_sizes,
            type=&#34;pixel_values&#34;,
        )

    def get_embedding(
        self,
        input_ids: torch.Tensor,
        image_inputs: Optional[MiniCPMVImageInputs],
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        vlm_embedding: torch.Tensor = self.llm.get_input_embeddings(input_ids)

        if image_inputs is None:  # No image
            vision_hidden_states = torch.tensor([], device=input_ids.device)
        else:
            if image_inputs[&#34;type&#34;] == &#34;image_embeds&#34;:
                vision_hidden_states = (
                    image_inputs[&#34;data&#34;]
                    .type(vlm_embedding.dtype)
                    .to(vlm_embedding.device)
                )
            else:
                vision_hidden_states = self.get_vision_hidden_states(image_inputs)
            # See NOTE in _parse_and_validate_inputs
            image_bounds = image_inputs[&#34;image_bounds&#34;]
            if len(image_bounds) &gt; 0:
                image_indices = torch.stack(
                    [
                        torch.arange(start, end, dtype=torch.long)
                        for start, end in image_bounds.tolist()
                    ]
                ).to(vlm_embedding.device)

                vlm_embedding.scatter_(
                    0,
                    image_indices.view(-1, 1).repeat(1, vlm_embedding.shape[-1]),
                    vision_hidden_states.view(-1, vision_hidden_states.shape[-1]),
                )

        return vlm_embedding, vision_hidden_states

    def get_input_embeddings(self) -&gt; nn.Embedding:
        return self.llm.get_input_embeddings()

    def forward(
        self,
        input_ids: torch.Tensor,
        positions: torch.Tensor,
        forward_batch: ForwardBatch,
        **kwargs: Any,
    ) -&gt; torch.Tensor:
        hidden_states = general_mm_embed_routine(
            input_ids=input_ids,
            forward_batch=forward_batch,
            multimodal_model=self,
            language_model=self.llm,
            positions=positions,
        )
        return hidden_states

    def init_llm(
        self,
        config: Qwen2Config,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; nn.Module:
        raise NotImplementedError

    def init_vision_module(
        self,
        config: PretrainedConfig,
        quant_config: Optional[QuantizationConfig],
        prefix: str = &#34;&#34;,
    ) -&gt; nn.Module:
        raise NotImplementedError

    def init_resampler(
        self,
        embed_dim: int,
        vision_dim: int,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; nn.Module:
        raise NotImplementedError

    def get_vision_embedding(
        self,
        pixel_values: List[torch.Tensor],
        patch_attn_mask: Optional[torch.Tensor] = None,
        tgt_sizes: Optional[torch.Tensor] = None,
    ) -&gt; torch.Tensor:
        raise NotImplementedError

    def get_image_feature(self, items: List[MultimodalDataItem]) -&gt; torch.Tensor:
        raise NotImplementedError</code></pre>
</details>
<div class="desc"><p>The abstract class of MiniCPMV can only be inherited, but cannot be
instantiated.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.models.minicpmo.MiniCPMO" href="minicpmo.html#sglang.srt.models.minicpmo.MiniCPMO">MiniCPMO</a></li>
<li><a title="sglang.srt.models.minicpmv.MiniCPMV2_6" href="#sglang.srt.models.minicpmv.MiniCPMV2_6">MiniCPMV2_6</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.minicpmv.MiniCPMBaseModel.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>input_ids: torch.Tensor,<br>positions: torch.Tensor,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>,<br>**kwargs: Any) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    input_ids: torch.Tensor,
    positions: torch.Tensor,
    forward_batch: ForwardBatch,
    **kwargs: Any,
) -&gt; torch.Tensor:
    hidden_states = general_mm_embed_routine(
        input_ids=input_ids,
        forward_batch=forward_batch,
        multimodal_model=self,
        language_model=self.llm,
        positions=positions,
    )
    return hidden_states</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMBaseModel.get_embedding"><code class="name flex">
<span>def <span class="ident">get_embedding</span></span>(<span>self,<br>input_ids: torch.Tensor,<br>image_inputs: <a title="sglang.srt.models.minicpmv.MiniCPMVImagePixelInputs" href="#sglang.srt.models.minicpmv.MiniCPMVImagePixelInputs">MiniCPMVImagePixelInputs</a> | <a title="sglang.srt.models.minicpmv.MiniCPMVImageEmbeddingInputs" href="#sglang.srt.models.minicpmv.MiniCPMVImageEmbeddingInputs">MiniCPMVImageEmbeddingInputs</a> | None) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_embedding(
    self,
    input_ids: torch.Tensor,
    image_inputs: Optional[MiniCPMVImageInputs],
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    vlm_embedding: torch.Tensor = self.llm.get_input_embeddings(input_ids)

    if image_inputs is None:  # No image
        vision_hidden_states = torch.tensor([], device=input_ids.device)
    else:
        if image_inputs[&#34;type&#34;] == &#34;image_embeds&#34;:
            vision_hidden_states = (
                image_inputs[&#34;data&#34;]
                .type(vlm_embedding.dtype)
                .to(vlm_embedding.device)
            )
        else:
            vision_hidden_states = self.get_vision_hidden_states(image_inputs)
        # See NOTE in _parse_and_validate_inputs
        image_bounds = image_inputs[&#34;image_bounds&#34;]
        if len(image_bounds) &gt; 0:
            image_indices = torch.stack(
                [
                    torch.arange(start, end, dtype=torch.long)
                    for start, end in image_bounds.tolist()
                ]
            ).to(vlm_embedding.device)

            vlm_embedding.scatter_(
                0,
                image_indices.view(-1, 1).repeat(1, vlm_embedding.shape[-1]),
                vision_hidden_states.view(-1, vision_hidden_states.shape[-1]),
            )

    return vlm_embedding, vision_hidden_states</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMBaseModel.get_image_feature"><code class="name flex">
<span>def <span class="ident">get_image_feature</span></span>(<span>self,<br>items: List[<a title="sglang.srt.managers.schedule_batch.MultimodalDataItem" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.MultimodalDataItem">MultimodalDataItem</a>]) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_image_feature(self, items: List[MultimodalDataItem]) -&gt; torch.Tensor:
    raise NotImplementedError</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMBaseModel.get_input_embeddings"><code class="name flex">
<span>def <span class="ident">get_input_embeddings</span></span>(<span>self) ‑> torch.nn.modules.sparse.Embedding</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_input_embeddings(self) -&gt; nn.Embedding:
    return self.llm.get_input_embeddings()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMBaseModel.get_vision_embedding"><code class="name flex">
<span>def <span class="ident">get_vision_embedding</span></span>(<span>self,<br>pixel_values: List[torch.Tensor],<br>patch_attn_mask: torch.Tensor | None = None,<br>tgt_sizes: torch.Tensor | None = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_vision_embedding(
    self,
    pixel_values: List[torch.Tensor],
    patch_attn_mask: Optional[torch.Tensor] = None,
    tgt_sizes: Optional[torch.Tensor] = None,
) -&gt; torch.Tensor:
    raise NotImplementedError</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMBaseModel.init_llm"><code class="name flex">
<span>def <span class="ident">init_llm</span></span>(<span>self,<br>config: None,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '') ‑> torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_llm(
    self,
    config: Qwen2Config,
    quant_config: Optional[QuantizationConfig] = None,
    prefix: str = &#34;&#34;,
) -&gt; nn.Module:
    raise NotImplementedError</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMBaseModel.init_resampler"><code class="name flex">
<span>def <span class="ident">init_resampler</span></span>(<span>self,<br>embed_dim: int,<br>vision_dim: int,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '') ‑> torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_resampler(
    self,
    embed_dim: int,
    vision_dim: int,
    quant_config: Optional[QuantizationConfig] = None,
    prefix: str = &#34;&#34;,
) -&gt; nn.Module:
    raise NotImplementedError</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMBaseModel.init_vision_module"><code class="name flex">
<span>def <span class="ident">init_vision_module</span></span>(<span>self,<br>config: transformers.configuration_utils.PretrainedConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None,<br>prefix: str = '') ‑> torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_vision_module(
    self,
    config: PretrainedConfig,
    quant_config: Optional[QuantizationConfig],
    prefix: str = &#34;&#34;,
) -&gt; nn.Module:
    raise NotImplementedError</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV"><code class="flex name class">
<span>class <span class="ident">MiniCPMV</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MiniCPMV:
    &#34;&#34;&#34;
    Different versions of MiniCPMV use different visual encoders and LLMs,
    which is not conducive to the current integration logic of LoRA and
    bitsandbytes in SGLang. Therefore, it is necessary to separate them.
    &#34;&#34;&#34;

    # Ensure that the LoRA support check passes when the class is not
    # initialized, but set all these attributes to empty.
    packed_modules_mapping = {}
    supported_lora_modules = []
    embedding_modules = {}
    embedding_padding_modules = []

    minicpmv: nn.Module

    def __init__(
        self,
        config: PretrainedConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; None:
        super().__init__()

        if not hasattr(config, &#34;version&#34;):
            version = (2, 6)
        else:
            version = str(config.version).split(&#34;.&#34;)
            version = tuple([int(x) for x in version])
        # Dispatch class based on version
        instance_class = _SUPPORT_VERSION.get(version)
        if instance_class is None:
            raise ValueError(&#34;Currently, MiniCPMV only supports versions 2.6&#34;)

        try:
            minicpmv = instance_class(
                config=config, quant_config=quant_config, prefix=prefix
            )
            self.minicpmv = minicpmv
        except Exception as e:
            print(f&#34;Failed to instantiate MiniCPMV: {e}&#34;)
            raise e
        self.config = config

    def __getattr__(self, name):
        if name == &#34;minicpmv&#34;:
            return None
        return getattr(self.minicpmv, name)

    def __call__(self, *args, **kwargs):
        return self.minicpmv(*args, **kwargs)

    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
        stacked_params_mapping = [
            # (param_name, shard_name, shard_id)
            (&#34;qkv_proj&#34;, &#34;q_proj&#34;, &#34;q&#34;),
            (&#34;qkv_proj&#34;, &#34;k_proj&#34;, &#34;k&#34;),
            (&#34;qkv_proj&#34;, &#34;v_proj&#34;, &#34;v&#34;),
            (&#34;gate_up_proj&#34;, &#34;gate_proj&#34;, 0),
            (&#34;gate_up_proj&#34;, &#34;up_proj&#34;, 1),
        ]

        params_dict = dict(self.minicpmv.named_parameters())
        for name, loaded_weight in weights:
            if &#34;rotary_emb.inv_freq~&#34; in name or &#34;projector&#34; in name:
                continue
            if &#34;rotary_emb.cos_cached&#34; in name or &#34;rotary_emb.sin_cached&#34; in name:
                # Models trained using ColossalAI may include these tensors in
                # the checkpoint. Skip them.
                continue
            if name.startswith(&#34;model.vision_tower&#34;) and name not in params_dict:
                continue

            # adapt to VisionAttention
            name = name.replace(r&#34;self_attn.out_proj&#34;, r&#34;self_attn.proj&#34;)

            if &#34;sampler&#34; in name:
                param = params_dict[name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(param, loaded_weight)
                continue

            for param_name, weight_name, shard_id in stacked_params_mapping:
                # replace the name and load with customized loader
                if weight_name not in name:
                    continue
                name = name.replace(weight_name, param_name)
                # # Skip loading extra bias for GPTQ models.
                if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                    continue
                param = params_dict[name]
                weight_loader = param.weight_loader
                weight_loader(param, loaded_weight, shard_id)
                break
            else:
                # Skip loading extra bias for GPTQ models.
                if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                    continue

                param = params_dict[name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(param, loaded_weight)</code></pre>
</details>
<div class="desc"><p>Different versions of MiniCPMV use different visual encoders and LLMs,
which is not conducive to the current integration logic of LoRA and
bitsandbytes in SGLang. Therefore, it is necessary to separate them.</p></div>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.minicpmv.MiniCPMV.embedding_modules"><code class="name">var <span class="ident">embedding_modules</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV.embedding_padding_modules"><code class="name">var <span class="ident">embedding_padding_modules</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV.minicpmv"><code class="name">var <span class="ident">minicpmv</span> : torch.nn.modules.module.Module</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV.packed_modules_mapping"><code class="name">var <span class="ident">packed_modules_mapping</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV.supported_lora_modules"><code class="name">var <span class="ident">supported_lora_modules</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.minicpmv.MiniCPMV.load_weights"><code class="name flex">
<span>def <span class="ident">load_weights</span></span>(<span>self, weights: Iterable[Tuple[str, torch.Tensor]])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
    stacked_params_mapping = [
        # (param_name, shard_name, shard_id)
        (&#34;qkv_proj&#34;, &#34;q_proj&#34;, &#34;q&#34;),
        (&#34;qkv_proj&#34;, &#34;k_proj&#34;, &#34;k&#34;),
        (&#34;qkv_proj&#34;, &#34;v_proj&#34;, &#34;v&#34;),
        (&#34;gate_up_proj&#34;, &#34;gate_proj&#34;, 0),
        (&#34;gate_up_proj&#34;, &#34;up_proj&#34;, 1),
    ]

    params_dict = dict(self.minicpmv.named_parameters())
    for name, loaded_weight in weights:
        if &#34;rotary_emb.inv_freq~&#34; in name or &#34;projector&#34; in name:
            continue
        if &#34;rotary_emb.cos_cached&#34; in name or &#34;rotary_emb.sin_cached&#34; in name:
            # Models trained using ColossalAI may include these tensors in
            # the checkpoint. Skip them.
            continue
        if name.startswith(&#34;model.vision_tower&#34;) and name not in params_dict:
            continue

        # adapt to VisionAttention
        name = name.replace(r&#34;self_attn.out_proj&#34;, r&#34;self_attn.proj&#34;)

        if &#34;sampler&#34; in name:
            param = params_dict[name]
            weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
            weight_loader(param, loaded_weight)
            continue

        for param_name, weight_name, shard_id in stacked_params_mapping:
            # replace the name and load with customized loader
            if weight_name not in name:
                continue
            name = name.replace(weight_name, param_name)
            # # Skip loading extra bias for GPTQ models.
            if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                continue
            param = params_dict[name]
            weight_loader = param.weight_loader
            weight_loader(param, loaded_weight, shard_id)
            break
        else:
            # Skip loading extra bias for GPTQ models.
            if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                continue

            param = params_dict[name]
            weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
            weight_loader(param, loaded_weight)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV"><code class="flex name class">
<span>class <span class="ident">EntryClass</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MiniCPMV:
    &#34;&#34;&#34;
    Different versions of MiniCPMV use different visual encoders and LLMs,
    which is not conducive to the current integration logic of LoRA and
    bitsandbytes in SGLang. Therefore, it is necessary to separate them.
    &#34;&#34;&#34;

    # Ensure that the LoRA support check passes when the class is not
    # initialized, but set all these attributes to empty.
    packed_modules_mapping = {}
    supported_lora_modules = []
    embedding_modules = {}
    embedding_padding_modules = []

    minicpmv: nn.Module

    def __init__(
        self,
        config: PretrainedConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; None:
        super().__init__()

        if not hasattr(config, &#34;version&#34;):
            version = (2, 6)
        else:
            version = str(config.version).split(&#34;.&#34;)
            version = tuple([int(x) for x in version])
        # Dispatch class based on version
        instance_class = _SUPPORT_VERSION.get(version)
        if instance_class is None:
            raise ValueError(&#34;Currently, MiniCPMV only supports versions 2.6&#34;)

        try:
            minicpmv = instance_class(
                config=config, quant_config=quant_config, prefix=prefix
            )
            self.minicpmv = minicpmv
        except Exception as e:
            print(f&#34;Failed to instantiate MiniCPMV: {e}&#34;)
            raise e
        self.config = config

    def __getattr__(self, name):
        if name == &#34;minicpmv&#34;:
            return None
        return getattr(self.minicpmv, name)

    def __call__(self, *args, **kwargs):
        return self.minicpmv(*args, **kwargs)

    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
        stacked_params_mapping = [
            # (param_name, shard_name, shard_id)
            (&#34;qkv_proj&#34;, &#34;q_proj&#34;, &#34;q&#34;),
            (&#34;qkv_proj&#34;, &#34;k_proj&#34;, &#34;k&#34;),
            (&#34;qkv_proj&#34;, &#34;v_proj&#34;, &#34;v&#34;),
            (&#34;gate_up_proj&#34;, &#34;gate_proj&#34;, 0),
            (&#34;gate_up_proj&#34;, &#34;up_proj&#34;, 1),
        ]

        params_dict = dict(self.minicpmv.named_parameters())
        for name, loaded_weight in weights:
            if &#34;rotary_emb.inv_freq~&#34; in name or &#34;projector&#34; in name:
                continue
            if &#34;rotary_emb.cos_cached&#34; in name or &#34;rotary_emb.sin_cached&#34; in name:
                # Models trained using ColossalAI may include these tensors in
                # the checkpoint. Skip them.
                continue
            if name.startswith(&#34;model.vision_tower&#34;) and name not in params_dict:
                continue

            # adapt to VisionAttention
            name = name.replace(r&#34;self_attn.out_proj&#34;, r&#34;self_attn.proj&#34;)

            if &#34;sampler&#34; in name:
                param = params_dict[name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(param, loaded_weight)
                continue

            for param_name, weight_name, shard_id in stacked_params_mapping:
                # replace the name and load with customized loader
                if weight_name not in name:
                    continue
                name = name.replace(weight_name, param_name)
                # # Skip loading extra bias for GPTQ models.
                if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                    continue
                param = params_dict[name]
                weight_loader = param.weight_loader
                weight_loader(param, loaded_weight, shard_id)
                break
            else:
                # Skip loading extra bias for GPTQ models.
                if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                    continue

                param = params_dict[name]
                weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
                weight_loader(param, loaded_weight)</code></pre>
</details>
<div class="desc"><p>Different versions of MiniCPMV use different visual encoders and LLMs,
which is not conducive to the current integration logic of LoRA and
bitsandbytes in SGLang. Therefore, it is necessary to separate them.</p></div>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.minicpmv.MiniCPMV.embedding_modules"><code class="name">var <span class="ident">embedding_modules</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV.embedding_padding_modules"><code class="name">var <span class="ident">embedding_padding_modules</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV.minicpmv"><code class="name">var <span class="ident">minicpmv</span> : torch.nn.modules.module.Module</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV.packed_modules_mapping"><code class="name">var <span class="ident">packed_modules_mapping</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV.supported_lora_modules"><code class="name">var <span class="ident">supported_lora_modules</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.minicpmv.MiniCPMV.load_weights"><code class="name flex">
<span>def <span class="ident">load_weights</span></span>(<span>self, weights: Iterable[Tuple[str, torch.Tensor]])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
    stacked_params_mapping = [
        # (param_name, shard_name, shard_id)
        (&#34;qkv_proj&#34;, &#34;q_proj&#34;, &#34;q&#34;),
        (&#34;qkv_proj&#34;, &#34;k_proj&#34;, &#34;k&#34;),
        (&#34;qkv_proj&#34;, &#34;v_proj&#34;, &#34;v&#34;),
        (&#34;gate_up_proj&#34;, &#34;gate_proj&#34;, 0),
        (&#34;gate_up_proj&#34;, &#34;up_proj&#34;, 1),
    ]

    params_dict = dict(self.minicpmv.named_parameters())
    for name, loaded_weight in weights:
        if &#34;rotary_emb.inv_freq~&#34; in name or &#34;projector&#34; in name:
            continue
        if &#34;rotary_emb.cos_cached&#34; in name or &#34;rotary_emb.sin_cached&#34; in name:
            # Models trained using ColossalAI may include these tensors in
            # the checkpoint. Skip them.
            continue
        if name.startswith(&#34;model.vision_tower&#34;) and name not in params_dict:
            continue

        # adapt to VisionAttention
        name = name.replace(r&#34;self_attn.out_proj&#34;, r&#34;self_attn.proj&#34;)

        if &#34;sampler&#34; in name:
            param = params_dict[name]
            weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
            weight_loader(param, loaded_weight)
            continue

        for param_name, weight_name, shard_id in stacked_params_mapping:
            # replace the name and load with customized loader
            if weight_name not in name:
                continue
            name = name.replace(weight_name, param_name)
            # # Skip loading extra bias for GPTQ models.
            if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                continue
            param = params_dict[name]
            weight_loader = param.weight_loader
            weight_loader(param, loaded_weight, shard_id)
            break
        else:
            # Skip loading extra bias for GPTQ models.
            if name.endswith(&#34;.bias&#34;) and name not in params_dict:
                continue

            param = params_dict[name]
            weight_loader = getattr(param, &#34;weight_loader&#34;, default_weight_loader)
            weight_loader(param, loaded_weight)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV2_6"><code class="flex name class">
<span>class <span class="ident">MiniCPMV2_6</span></span>
<span>(</span><span>config: transformers.configuration_utils.PretrainedConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MiniCPMV2_6(MiniCPMBaseModel):
    packed_modules_mapping = {
        &#34;qkv_proj&#34;: [
            &#34;q_proj&#34;,
            &#34;k_proj&#34;,
            &#34;v_proj&#34;,
        ],
        &#34;gate_up_proj&#34;: [
            &#34;gate_proj&#34;,
            &#34;up_proj&#34;,
        ],
    }
    # LoRA specific attributes
    supported_lora_modules = [
        # vision encoder
        &#34;fc1&#34;,
        &#34;fc2&#34;,
        &#34;out_proj&#34;,
        # language model
        &#34;qkv_proj&#34;,  # same name with vision encoder
        &#34;o_proj&#34;,
        &#34;gate_up_proj&#34;,
        &#34;down_proj&#34;,
        # resampler
        &#34;kv_proj&#34;,
    ]

    # BitandBytes specific attributes
    bitsandbytes_stacked_params_mapping = {
        # shard_name, weight_name, index
        &#34;q_proj&#34;: (&#34;qkv_proj&#34;, 0),
        &#34;k_proj&#34;: (&#34;qkv_proj&#34;, 1),
        &#34;v_proj&#34;: (&#34;qkv_proj&#34;, 2),
        &#34;gate_proj&#34;: (&#34;gate_up_proj&#34;, 0),
        &#34;up_proj&#34;: (&#34;gate_up_proj&#34;, 1),
    }

    embedding_modules = {}
    embedding_padding_modules = []

    def __init__(
        self,
        config: PretrainedConfig,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ):
        super().__init__(config=config, quant_config=quant_config, prefix=prefix)
        assert self.version == (2, 6)

    def init_llm(
        self,
        config: Qwen2Config,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; nn.Module:
        return Qwen2ForCausalLM(config=config, quant_config=quant_config, prefix=prefix)

    def init_vision_module(
        self,
        config: PretrainedConfig,
        quant_config: Optional[QuantizationConfig],
        prefix: str = &#34;&#34;,
    ) -&gt; nn.Module:
        model = Idefics2VisionTransformer(
            config=config.vision_config, quant_config=quant_config, prefix=prefix
        )
        if self.config.drop_vision_last_layer:
            model.encoder.layers = model.encoder.layers[:-1]

        setattr(model, &#34;embed_dim&#34;, model.embeddings.embed_dim)
        setattr(model, &#34;patch_size&#34;, model.embeddings.patch_size)
        return model

    def init_resampler(
        self,
        embed_dim: int,
        vision_dim: int,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; nn.Module:
        with set_default_torch_dtype(torch.float16):
            # The resampler in 2.6 remains consistent with the one in 2.5.
            resampler = Resampler2_5(
                num_queries=self.config.query_num,
                embed_dim=embed_dim,
                num_heads=embed_dim // 128,
                kv_dim=vision_dim,
                quant_config=quant_config,
                prefix=prefix,
            )

        return resampler.to(device=&#34;cuda&#34;, dtype=torch.get_default_dtype())

    def get_vision_embedding(
        self,
        pixel_values: List[torch.Tensor],
        patch_attn_mask: Optional[torch.Tensor] = None,
        tgt_sizes: Optional[torch.Tensor] = None,
    ) -&gt; torch.Tensor:
        vision_embedding = self.vpm(
            pixel_values,
            patch_attention_mask=patch_attn_mask,
            tgt_sizes=tgt_sizes,
        )
        return vision_embedding

    def get_image_feature(self, items: List[MultimodalDataItem]) -&gt; torch.Tensor:
        # list of tensors
        pixel_values = flatten_nested_list([item.feature for item in items])
        tgt_sizes = torch.stack(
            flatten_nested_list([item.tgt_size for item in items]), dim=0
        )
        assert len(pixel_values) == tgt_sizes.shape[0]

        device = self.vpm.embeddings.position_embedding.weight.device
        dtype = self.vpm.embeddings.position_embedding.weight.dtype
        all_pixel_values_lst = [
            i.flatten(end_dim=1).permute(1, 0) for i in pixel_values
        ]

        max_patches = (tgt_sizes[:, 0] * tgt_sizes[:, 1]).max().item()
        assert isinstance(max_patches, int)
        all_pixel_values = torch.nn.utils.rnn.pad_sequence(
            all_pixel_values_lst, batch_first=True, padding_value=0.0
        )

        B, L, _ = all_pixel_values.shape
        all_pixel_values = all_pixel_values.permute(0, 2, 1).reshape(B, 3, -1, L)
        patch_attn_mask = torch.zeros(
            (B, 1, max_patches), dtype=torch.bool, device=device
        )

        tgt_sizes_tensor = tgt_sizes.clone().to(device=patch_attn_mask.device)
        mask_shapes = tgt_sizes_tensor[:, 0] * tgt_sizes_tensor[:, 1]
        patch_attn_mask[:, 0, :] = torch.arange(
            patch_attn_mask.size(2), device=patch_attn_mask.device
        ).unsqueeze(0) &lt; mask_shapes.unsqueeze(1)

        vision_embedding = self.vpm(
            all_pixel_values.type(dtype),
            patch_attention_mask=patch_attn_mask,
            tgt_sizes=tgt_sizes,
        )
        return self.resampler(vision_embedding, tgt_sizes)

    def pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs):
        # Get all special token IDs
        im_start_id: int = image_inputs.im_start_id
        im_end_id: int = image_inputs.im_end_id
        slice_start_id: int = image_inputs.slice_start_id
        slice_end_id: int = image_inputs.slice_end_id

        media_token_pairs = [(im_start_id, im_end_id), (slice_start_id, slice_end_id)]
        pattern = MultiModalityDataPaddingPatternTokenPairs(media_token_pairs)

        return pattern.pad_input_tokens(input_ids, image_inputs)</code></pre>
</details>
<div class="desc"><p>The abstract class of MiniCPMV can only be inherited, but cannot be
instantiated.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.models.minicpmv.MiniCPMBaseModel" href="#sglang.srt.models.minicpmv.MiniCPMBaseModel">MiniCPMBaseModel</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.minicpmv.MiniCPMV2_6.bitsandbytes_stacked_params_mapping"><code class="name">var <span class="ident">bitsandbytes_stacked_params_mapping</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV2_6.embedding_modules"><code class="name">var <span class="ident">embedding_modules</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV2_6.embedding_padding_modules"><code class="name">var <span class="ident">embedding_padding_modules</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV2_6.packed_modules_mapping"><code class="name">var <span class="ident">packed_modules_mapping</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV2_6.supported_lora_modules"><code class="name">var <span class="ident">supported_lora_modules</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.minicpmv.MiniCPMV2_6.get_image_feature"><code class="name flex">
<span>def <span class="ident">get_image_feature</span></span>(<span>self,<br>items: List[<a title="sglang.srt.managers.schedule_batch.MultimodalDataItem" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.MultimodalDataItem">MultimodalDataItem</a>]) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_image_feature(self, items: List[MultimodalDataItem]) -&gt; torch.Tensor:
    # list of tensors
    pixel_values = flatten_nested_list([item.feature for item in items])
    tgt_sizes = torch.stack(
        flatten_nested_list([item.tgt_size for item in items]), dim=0
    )
    assert len(pixel_values) == tgt_sizes.shape[0]

    device = self.vpm.embeddings.position_embedding.weight.device
    dtype = self.vpm.embeddings.position_embedding.weight.dtype
    all_pixel_values_lst = [
        i.flatten(end_dim=1).permute(1, 0) for i in pixel_values
    ]

    max_patches = (tgt_sizes[:, 0] * tgt_sizes[:, 1]).max().item()
    assert isinstance(max_patches, int)
    all_pixel_values = torch.nn.utils.rnn.pad_sequence(
        all_pixel_values_lst, batch_first=True, padding_value=0.0
    )

    B, L, _ = all_pixel_values.shape
    all_pixel_values = all_pixel_values.permute(0, 2, 1).reshape(B, 3, -1, L)
    patch_attn_mask = torch.zeros(
        (B, 1, max_patches), dtype=torch.bool, device=device
    )

    tgt_sizes_tensor = tgt_sizes.clone().to(device=patch_attn_mask.device)
    mask_shapes = tgt_sizes_tensor[:, 0] * tgt_sizes_tensor[:, 1]
    patch_attn_mask[:, 0, :] = torch.arange(
        patch_attn_mask.size(2), device=patch_attn_mask.device
    ).unsqueeze(0) &lt; mask_shapes.unsqueeze(1)

    vision_embedding = self.vpm(
        all_pixel_values.type(dtype),
        patch_attention_mask=patch_attn_mask,
        tgt_sizes=tgt_sizes,
    )
    return self.resampler(vision_embedding, tgt_sizes)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV2_6.get_vision_embedding"><code class="name flex">
<span>def <span class="ident">get_vision_embedding</span></span>(<span>self,<br>pixel_values: List[torch.Tensor],<br>patch_attn_mask: torch.Tensor | None = None,<br>tgt_sizes: torch.Tensor | None = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_vision_embedding(
    self,
    pixel_values: List[torch.Tensor],
    patch_attn_mask: Optional[torch.Tensor] = None,
    tgt_sizes: Optional[torch.Tensor] = None,
) -&gt; torch.Tensor:
    vision_embedding = self.vpm(
        pixel_values,
        patch_attention_mask=patch_attn_mask,
        tgt_sizes=tgt_sizes,
    )
    return vision_embedding</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV2_6.init_llm"><code class="name flex">
<span>def <span class="ident">init_llm</span></span>(<span>self,<br>config: None,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '') ‑> torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_llm(
    self,
    config: Qwen2Config,
    quant_config: Optional[QuantizationConfig] = None,
    prefix: str = &#34;&#34;,
) -&gt; nn.Module:
    return Qwen2ForCausalLM(config=config, quant_config=quant_config, prefix=prefix)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV2_6.init_resampler"><code class="name flex">
<span>def <span class="ident">init_resampler</span></span>(<span>self,<br>embed_dim: int,<br>vision_dim: int,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '') ‑> torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_resampler(
    self,
    embed_dim: int,
    vision_dim: int,
    quant_config: Optional[QuantizationConfig] = None,
    prefix: str = &#34;&#34;,
) -&gt; nn.Module:
    with set_default_torch_dtype(torch.float16):
        # The resampler in 2.6 remains consistent with the one in 2.5.
        resampler = Resampler2_5(
            num_queries=self.config.query_num,
            embed_dim=embed_dim,
            num_heads=embed_dim // 128,
            kv_dim=vision_dim,
            quant_config=quant_config,
            prefix=prefix,
        )

    return resampler.to(device=&#34;cuda&#34;, dtype=torch.get_default_dtype())</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV2_6.init_vision_module"><code class="name flex">
<span>def <span class="ident">init_vision_module</span></span>(<span>self,<br>config: transformers.configuration_utils.PretrainedConfig,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None,<br>prefix: str = '') ‑> torch.nn.modules.module.Module</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_vision_module(
    self,
    config: PretrainedConfig,
    quant_config: Optional[QuantizationConfig],
    prefix: str = &#34;&#34;,
) -&gt; nn.Module:
    model = Idefics2VisionTransformer(
        config=config.vision_config, quant_config=quant_config, prefix=prefix
    )
    if self.config.drop_vision_last_layer:
        model.encoder.layers = model.encoder.layers[:-1]

    setattr(model, &#34;embed_dim&#34;, model.embeddings.embed_dim)
    setattr(model, &#34;patch_size&#34;, model.embeddings.patch_size)
    return model</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMV2_6.pad_input_ids"><code class="name flex">
<span>def <span class="ident">pad_input_ids</span></span>(<span>self,<br>input_ids: List[int],<br>image_inputs: <a title="sglang.srt.managers.schedule_batch.MultimodalInputs" href="../managers/schedule_batch.html#sglang.srt.managers.schedule_batch.MultimodalInputs">MultimodalInputs</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs):
    # Get all special token IDs
    im_start_id: int = image_inputs.im_start_id
    im_end_id: int = image_inputs.im_end_id
    slice_start_id: int = image_inputs.slice_start_id
    slice_end_id: int = image_inputs.slice_end_id

    media_token_pairs = [(im_start_id, im_end_id), (slice_start_id, slice_end_id)]
    pattern = MultiModalityDataPaddingPatternTokenPairs(media_token_pairs)

    return pattern.pad_input_tokens(input_ids, image_inputs)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.models.minicpmv.MiniCPMBaseModel" href="#sglang.srt.models.minicpmv.MiniCPMBaseModel">MiniCPMBaseModel</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMBaseModel.forward" href="#sglang.srt.models.minicpmv.MiniCPMBaseModel.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMVImageEmbeddingInputs"><code class="flex name class">
<span>class <span class="ident">MiniCPMVImageEmbeddingInputs</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MiniCPMVImageEmbeddingInputs(TypedDict):
    type: Literal[&#34;image_embeds&#34;]
    data: torch.Tensor
    &#34;&#34;&#34;
    Shape: `(batch_size * num_images, image_feature_size, hidden_size)`

    `hidden_size` must match the hidden size of language model backbone.
    instead of a batched tensor.
    &#34;&#34;&#34;

    image_bounds: torch.Tensor
    &#34;&#34;&#34;
    Shape: `(batch_size * num_images, 2)`

    This should be in `(start, stop)` format.
    &#34;&#34;&#34;</code></pre>
</details>
<div class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.dict</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.minicpmv.MiniCPMVImageEmbeddingInputs.data"><code class="name">var <span class="ident">data</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"><p>Shape: <code>(batch_size * num_images, image_feature_size, hidden_size)</code></p>
<p><code>hidden_size</code> must match the hidden size of language model backbone.
instead of a batched tensor.</p></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMVImageEmbeddingInputs.image_bounds"><code class="name">var <span class="ident">image_bounds</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"><p>Shape: <code>(batch_size * num_images, 2)</code></p>
<p>This should be in <code>(start, stop)</code> format.</p></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMVImageEmbeddingInputs.type"><code class="name">var <span class="ident">type</span> : Literal['image_embeds']</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMVImagePixelInputs"><code class="flex name class">
<span>class <span class="ident">MiniCPMVImagePixelInputs</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MiniCPMVImagePixelInputs(TypedDict):
    type: Literal[&#34;pixel_values&#34;]
    data: List[torch.Tensor]
    &#34;&#34;&#34;
    Shape: `(batch_size * num_images, num_channels, height, width)`

    Note that the image size may vary, so we pass it as a list
    instead of a batched tensor.
    &#34;&#34;&#34;

    image_bounds: torch.Tensor
    &#34;&#34;&#34;
    Shape: `(batch_size * num_images, 2)`

    This should be in `(start, stop)` format.
    &#34;&#34;&#34;

    tgt_sizes: torch.Tensor
    &#34;&#34;&#34;
    Shape: `(batch_size * num_images, 2)`

    This should be in `(height, width)` format.
    &#34;&#34;&#34;</code></pre>
</details>
<div class="desc"><p>dict() -&gt; new empty dictionary
dict(mapping) -&gt; new dictionary initialized from a mapping object's
(key, value) pairs
dict(iterable) -&gt; new dictionary initialized as if via:
d = {}
for k, v in iterable:
d[k] = v
dict(**kwargs) -&gt; new dictionary initialized with the name=value pairs
in the keyword argument list.
For example:
dict(one=1, two=2)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.dict</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.minicpmv.MiniCPMVImagePixelInputs.data"><code class="name">var <span class="ident">data</span> : List[torch.Tensor]</code></dt>
<dd>
<div class="desc"><p>Shape: <code>(batch_size * num_images, num_channels, height, width)</code></p>
<p>Note that the image size may vary, so we pass it as a list
instead of a batched tensor.</p></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMVImagePixelInputs.image_bounds"><code class="name">var <span class="ident">image_bounds</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"><p>Shape: <code>(batch_size * num_images, 2)</code></p>
<p>This should be in <code>(start, stop)</code> format.</p></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMVImagePixelInputs.tgt_sizes"><code class="name">var <span class="ident">tgt_sizes</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"><p>Shape: <code>(batch_size * num_images, 2)</code></p>
<p>This should be in <code>(height, width)</code> format.</p></div>
</dd>
<dt id="sglang.srt.models.minicpmv.MiniCPMVImagePixelInputs.type"><code class="name">var <span class="ident">type</span> : Literal['pixel_values']</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.minicpmv.Resampler2_5"><code class="flex name class">
<span>class <span class="ident">Resampler2_5</span></span>
<span>(</span><span>num_queries: int,<br>embed_dim: int,<br>num_heads: int,<br>kv_dim: int | None = None,<br>norm_layer: Callable[[int], torch.nn.modules.normalization.LayerNorm] = functools.partial(&lt;class &#x27;torch.nn.modules.normalization.LayerNorm&#x27;&gt;, eps=1e-06),<br>max_size: Tuple[int, int] = (70, 70),<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../layers/quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Resampler2_5(BaseResampler):

    def __init__(
        self,
        num_queries: int,
        embed_dim: int,
        num_heads: int,
        kv_dim: Optional[int] = None,
        norm_layer: Callable[[int], nn.LayerNorm] = DEFAULT_LN,
        max_size: Tuple[int, int] = (70, 70),
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
    ) -&gt; None:
        super().__init__(
            num_queries,
            embed_dim,
            num_heads,
            kv_dim,
            norm_layer,
            quant_config=quant_config,
            prefix=prefix,
        )

        self.max_size = max_size
        self._set_2d_pos_cache(self.max_size)

        self.apply(self._init_weights)

    def _set_2d_pos_cache(
        self, max_size: Tuple[int, int], device: torch.types.Device = &#34;cpu&#34;
    ) -&gt; None:
        pos_embed_arr = get_2d_sincos_pos_embed(
            self.embed_dim, max_size, version=(2, 5)
        )
        pos_embed = torch.from_numpy(pos_embed_arr).float().to(device)
        self.register_buffer(&#34;pos_embed&#34;, pos_embed, persistent=False)

    def _adjust_pos_cache(
        self, tgt_sizes: torch.Tensor, device: torch.types.Device
    ) -&gt; None:
        max_h = tgt_sizes[:, 0].max().item()
        max_w = tgt_sizes[:, 1].max().item()
        assert isinstance(max_h, int) and isinstance(max_w, int)

        if max_h &gt; self.max_size[0] or max_w &gt; self.max_size[1]:
            self.max_size = (
                max(max_h, self.max_size[0]),
                max(max_w, self.max_size[1]),
            )
            self._set_2d_pos_cache(self.max_size, device)

    def forward(self, x: torch.Tensor, tgt_sizes: torch.Tensor) -&gt; torch.Tensor:
        assert x.shape[0] == tgt_sizes.shape[0]
        bs = x.shape[0]

        device = x.device
        dtype = x.dtype

        patch_len = tgt_sizes[:, 0] * tgt_sizes[:, 1]

        self._adjust_pos_cache(tgt_sizes, device=device)

        max_patch_len = patch_len.max().item()
        assert isinstance(max_patch_len, int)

        key_padding_mask = torch.zeros(
            (bs, max_patch_len), dtype=torch.bool, device=device
        )

        pos_embed = []
        for i in range(bs):
            tgt_h, tgt_w = tgt_sizes[i].tolist()
            pos_embed.append(
                self.pos_embed[:tgt_h, :tgt_w, :].reshape((tgt_h * tgt_w, -1)).to(dtype)
            )  # patches * D
            key_padding_mask[i, patch_len[i] :] = True
        pos_embed = torch.nn.utils.rnn.pad_sequence(
            pos_embed, batch_first=True, padding_value=0.0
        ).permute(
            1, 0, 2
        )  # BLD =&gt; L * B * D
        x, _ = self.kv_proj(x)  # B * L * D
        x = self.ln_kv(x).permute(1, 0, 2)  # L * B * D

        q = self.ln_q(self.query)  # Q * D

        out = self.attn(
            self._repeat(q, bs),  # Q * B * D
            x + pos_embed,  # L * B * D +  L * B * D
            x,
            key_padding_mask=key_padding_mask,
        )[0]
        #  out: Q * B * D
        x = out.permute(1, 0, 2)  # B * Q * D

        x = self.ln_post(x)
        x = x @ self.proj
        return x</code></pre>
</details>
<div class="desc"><p>A 2D perceiver-resampler network with one cross attention layers by
(grid_size**2) learnable queries and 2d sincos pos_emb.</p>
<h2 id="outputs">Outputs</h2>
<p>A tensor with the shape of (grid_size**2, embed_dim)</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.models.minicpmv.BaseResampler" href="#sglang.srt.models.minicpmv.BaseResampler">BaseResampler</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.minicpmv.Resampler2_5.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: torch.Tensor, tgt_sizes: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x: torch.Tensor, tgt_sizes: torch.Tensor) -&gt; torch.Tensor:
    assert x.shape[0] == tgt_sizes.shape[0]
    bs = x.shape[0]

    device = x.device
    dtype = x.dtype

    patch_len = tgt_sizes[:, 0] * tgt_sizes[:, 1]

    self._adjust_pos_cache(tgt_sizes, device=device)

    max_patch_len = patch_len.max().item()
    assert isinstance(max_patch_len, int)

    key_padding_mask = torch.zeros(
        (bs, max_patch_len), dtype=torch.bool, device=device
    )

    pos_embed = []
    for i in range(bs):
        tgt_h, tgt_w = tgt_sizes[i].tolist()
        pos_embed.append(
            self.pos_embed[:tgt_h, :tgt_w, :].reshape((tgt_h * tgt_w, -1)).to(dtype)
        )  # patches * D
        key_padding_mask[i, patch_len[i] :] = True
    pos_embed = torch.nn.utils.rnn.pad_sequence(
        pos_embed, batch_first=True, padding_value=0.0
    ).permute(
        1, 0, 2
    )  # BLD =&gt; L * B * D
    x, _ = self.kv_proj(x)  # B * L * D
    x = self.ln_kv(x).permute(1, 0, 2)  # L * B * D

    q = self.ln_q(self.query)  # Q * D

    out = self.attn(
        self._repeat(q, bs),  # Q * B * D
        x + pos_embed,  # L * B * D +  L * B * D
        x,
        key_padding_mask=key_padding_mask,
    )[0]
    #  out: Q * B * D
    x = out.permute(1, 0, 2)  # B * Q * D

    x = self.ln_post(x)
    x = x @ self.proj
    return x</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.models" href="index.html">sglang.srt.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.models.minicpmv.get_1d_sincos_pos_embed_from_grid" href="#sglang.srt.models.minicpmv.get_1d_sincos_pos_embed_from_grid">get_1d_sincos_pos_embed_from_grid</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.get_2d_sincos_pos_embed" href="#sglang.srt.models.minicpmv.get_2d_sincos_pos_embed">get_2d_sincos_pos_embed</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.get_2d_sincos_pos_embed_from_grid" href="#sglang.srt.models.minicpmv.get_2d_sincos_pos_embed_from_grid">get_2d_sincos_pos_embed_from_grid</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.get_version_by_config" href="#sglang.srt.models.minicpmv.get_version_by_config">get_version_by_config</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.models.minicpmv.BaseResampler" href="#sglang.srt.models.minicpmv.BaseResampler">BaseResampler</a></code></h4>
</li>
<li>
<h4><code><a title="sglang.srt.models.minicpmv.MiniCPMBaseModel" href="#sglang.srt.models.minicpmv.MiniCPMBaseModel">MiniCPMBaseModel</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMBaseModel.forward" href="#sglang.srt.models.minicpmv.MiniCPMBaseModel.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMBaseModel.get_embedding" href="#sglang.srt.models.minicpmv.MiniCPMBaseModel.get_embedding">get_embedding</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMBaseModel.get_image_feature" href="#sglang.srt.models.minicpmv.MiniCPMBaseModel.get_image_feature">get_image_feature</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMBaseModel.get_input_embeddings" href="#sglang.srt.models.minicpmv.MiniCPMBaseModel.get_input_embeddings">get_input_embeddings</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMBaseModel.get_vision_embedding" href="#sglang.srt.models.minicpmv.MiniCPMBaseModel.get_vision_embedding">get_vision_embedding</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMBaseModel.init_llm" href="#sglang.srt.models.minicpmv.MiniCPMBaseModel.init_llm">init_llm</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMBaseModel.init_resampler" href="#sglang.srt.models.minicpmv.MiniCPMBaseModel.init_resampler">init_resampler</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMBaseModel.init_vision_module" href="#sglang.srt.models.minicpmv.MiniCPMBaseModel.init_vision_module">init_vision_module</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.minicpmv.MiniCPMV" href="#sglang.srt.models.minicpmv.MiniCPMV">MiniCPMV</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV.embedding_modules" href="#sglang.srt.models.minicpmv.MiniCPMV.embedding_modules">embedding_modules</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV.embedding_padding_modules" href="#sglang.srt.models.minicpmv.MiniCPMV.embedding_padding_modules">embedding_padding_modules</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV.load_weights" href="#sglang.srt.models.minicpmv.MiniCPMV.load_weights">load_weights</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV.minicpmv" href="#sglang.srt.models.minicpmv.MiniCPMV.minicpmv">minicpmv</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV.packed_modules_mapping" href="#sglang.srt.models.minicpmv.MiniCPMV.packed_modules_mapping">packed_modules_mapping</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV.supported_lora_modules" href="#sglang.srt.models.minicpmv.MiniCPMV.supported_lora_modules">supported_lora_modules</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.minicpmv.MiniCPMV" href="#sglang.srt.models.minicpmv.MiniCPMV">MiniCPMV</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV.embedding_modules" href="#sglang.srt.models.minicpmv.MiniCPMV.embedding_modules">embedding_modules</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV.embedding_padding_modules" href="#sglang.srt.models.minicpmv.MiniCPMV.embedding_padding_modules">embedding_padding_modules</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV.load_weights" href="#sglang.srt.models.minicpmv.MiniCPMV.load_weights">load_weights</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV.minicpmv" href="#sglang.srt.models.minicpmv.MiniCPMV.minicpmv">minicpmv</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV.packed_modules_mapping" href="#sglang.srt.models.minicpmv.MiniCPMV.packed_modules_mapping">packed_modules_mapping</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV.supported_lora_modules" href="#sglang.srt.models.minicpmv.MiniCPMV.supported_lora_modules">supported_lora_modules</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.minicpmv.MiniCPMV2_6" href="#sglang.srt.models.minicpmv.MiniCPMV2_6">MiniCPMV2_6</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV2_6.bitsandbytes_stacked_params_mapping" href="#sglang.srt.models.minicpmv.MiniCPMV2_6.bitsandbytes_stacked_params_mapping">bitsandbytes_stacked_params_mapping</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV2_6.embedding_modules" href="#sglang.srt.models.minicpmv.MiniCPMV2_6.embedding_modules">embedding_modules</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV2_6.embedding_padding_modules" href="#sglang.srt.models.minicpmv.MiniCPMV2_6.embedding_padding_modules">embedding_padding_modules</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV2_6.get_image_feature" href="#sglang.srt.models.minicpmv.MiniCPMV2_6.get_image_feature">get_image_feature</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV2_6.get_vision_embedding" href="#sglang.srt.models.minicpmv.MiniCPMV2_6.get_vision_embedding">get_vision_embedding</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV2_6.init_llm" href="#sglang.srt.models.minicpmv.MiniCPMV2_6.init_llm">init_llm</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV2_6.init_resampler" href="#sglang.srt.models.minicpmv.MiniCPMV2_6.init_resampler">init_resampler</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV2_6.init_vision_module" href="#sglang.srt.models.minicpmv.MiniCPMV2_6.init_vision_module">init_vision_module</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV2_6.packed_modules_mapping" href="#sglang.srt.models.minicpmv.MiniCPMV2_6.packed_modules_mapping">packed_modules_mapping</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV2_6.pad_input_ids" href="#sglang.srt.models.minicpmv.MiniCPMV2_6.pad_input_ids">pad_input_ids</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMV2_6.supported_lora_modules" href="#sglang.srt.models.minicpmv.MiniCPMV2_6.supported_lora_modules">supported_lora_modules</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.minicpmv.MiniCPMVImageEmbeddingInputs" href="#sglang.srt.models.minicpmv.MiniCPMVImageEmbeddingInputs">MiniCPMVImageEmbeddingInputs</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMVImageEmbeddingInputs.data" href="#sglang.srt.models.minicpmv.MiniCPMVImageEmbeddingInputs.data">data</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMVImageEmbeddingInputs.image_bounds" href="#sglang.srt.models.minicpmv.MiniCPMVImageEmbeddingInputs.image_bounds">image_bounds</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMVImageEmbeddingInputs.type" href="#sglang.srt.models.minicpmv.MiniCPMVImageEmbeddingInputs.type">type</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.minicpmv.MiniCPMVImagePixelInputs" href="#sglang.srt.models.minicpmv.MiniCPMVImagePixelInputs">MiniCPMVImagePixelInputs</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMVImagePixelInputs.data" href="#sglang.srt.models.minicpmv.MiniCPMVImagePixelInputs.data">data</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMVImagePixelInputs.image_bounds" href="#sglang.srt.models.minicpmv.MiniCPMVImagePixelInputs.image_bounds">image_bounds</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMVImagePixelInputs.tgt_sizes" href="#sglang.srt.models.minicpmv.MiniCPMVImagePixelInputs.tgt_sizes">tgt_sizes</a></code></li>
<li><code><a title="sglang.srt.models.minicpmv.MiniCPMVImagePixelInputs.type" href="#sglang.srt.models.minicpmv.MiniCPMVImagePixelInputs.type">type</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.minicpmv.Resampler2_5" href="#sglang.srt.models.minicpmv.Resampler2_5">Resampler2_5</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.minicpmv.Resampler2_5.forward" href="#sglang.srt.models.minicpmv.Resampler2_5.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
