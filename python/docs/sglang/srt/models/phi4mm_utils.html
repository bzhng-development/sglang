<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.models.phi4mm_utils API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.models.phi4mm_utils</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.adaptive_enc_mask"><code class="name flex">
<span>def <span class="ident">adaptive_enc_mask</span></span>(<span>x_len, chunk_start_idx, left_window=0, right_window=0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def adaptive_enc_mask(x_len, chunk_start_idx, left_window=0, right_window=0):
    &#34;&#34;&#34;
    The function is very important for Transformer Transducer Streaming mode
    Args:
        xs_len (int): sequence length
        chunk_start_idx (list): first idx of each chunk, such as [0,18,36,48].
        It also supports adaptive chunk size [0,10,15,45]
        left_window (int): how many left chunks can be seen
        right_window (int): how many right chunks can be seen. It is used for
        chunk overlap model.
        Returns:
            mask (torch.Tensor): a mask tensor for streaming model
            Torch 1.0.1
            tensor([[1., 1., 0., 0.],
                    [0., 1., 1., 0.],
                    [0., 0., 1., 1.]])
            Torch 1.4.1
            tensor([[True., True., False., False.],
                    [False., True., True., False.],
                    [False., False., True., True.]])
    &#34;&#34;&#34;
    chunk_start_idx = torch.Tensor(
        chunk_start_idx
    ).long()  # first idx of each chunk, such as [0,18,36,48].
    start_pad = torch.nn.functional.pad(
        chunk_start_idx, (1, 0)
    )  # append 0 to the beginning, so it becomes [0, 0, 18, 36, 48]
    end_pad = torch.nn.functional.pad(
        chunk_start_idx, (0, 1), value=x_len
    )  # append x_len to the end, so it becomes [0,18,36,48, x_len]
    seq_range = torch.arange(0, x_len).unsqueeze(-1)  # seq_range size: [x_len, 1]
    idx = ((seq_range &lt; end_pad) &amp; (seq_range &gt;= start_pad)).nonzero()[
        :, 1
    ]  # idx size: [x_len]
    # boundary = end_pad[idx]  # boundary size: [x_len]
    seq_range_expand = (
        torch.arange(0, x_len).unsqueeze(0).expand(x_len, -1)
    )  # seq_range_expand size [x_len, x_len]
    idx_left = idx - left_window
    idx_left[idx_left &lt; 0] = 0
    boundary_left = start_pad[idx_left]
    mask_left = seq_range_expand &gt;= boundary_left.unsqueeze(-1)
    idx_right = idx + right_window
    idx_right[idx_right &gt; len(chunk_start_idx)] = len(chunk_start_idx)
    boundary_right = end_pad[idx_right]
    mask_right = seq_range_expand &lt; boundary_right.unsqueeze(-1)
    return mask_left &amp; mask_right</code></pre>
</details>
<div class="desc"><p>The function is very important for Transformer Transducer Streaming mode</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>xs_len</code></strong> :&ensp;<code>int</code></dt>
<dd>sequence length</dd>
<dt><strong><code>chunk_start_idx</code></strong> :&ensp;<code>list</code></dt>
<dd>first idx of each chunk, such as [0,18,36,48].</dd>
<dt>It also supports adaptive chunk size [0,10,15,45]</dt>
<dt><strong><code>left_window</code></strong> :&ensp;<code>int</code></dt>
<dd>how many left chunks can be seen</dd>
<dt><strong><code>right_window</code></strong> :&ensp;<code>int</code></dt>
<dd>how many right chunks can be seen. It is used for</dd>
</dl>
<p>chunk overlap model.
Returns:
mask (torch.Tensor): a mask tensor for streaming model
Torch 1.0.1
tensor([[1., 1., 0., 0.],
[0., 1., 1., 0.],
[0., 0., 1., 1.]])
Torch 1.4.1
tensor([[True., True., False., False.],
[False., True., True., False.],
[False., False., True., True.]])</p></div>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.calc_length"><code class="name flex">
<span>def <span class="ident">calc_length</span></span>(<span>lengths, all_paddings, kernel_size, stride, ceil_mode, repeat_num=1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_length(lengths, all_paddings, kernel_size, stride, ceil_mode, repeat_num=1):
    &#34;&#34;&#34;Calculates the output length of a Tensor passed through a convolution or
    max pooling layer&#34;&#34;&#34;
    add_pad: float = all_paddings - kernel_size
    one: float = 1.0
    for i in range(repeat_num):
        lengths = torch.div(lengths.to(dtype=torch.float) + add_pad, stride) + one
        lengths = torch.ceil(lengths) if ceil_mode else torch.floor(lengths)
    return lengths.to(dtype=torch.int)</code></pre>
</details>
<div class="desc"><p>Calculates the output length of a Tensor passed through a convolution or
max pooling layer</p></div>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.get_activation"><code class="name flex">
<span>def <span class="ident">get_activation</span></span>(<span>name='relu')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_activation(name=&#34;relu&#34;):
    &#34;&#34;&#34;Select an activation function by name

    Args:
        name: str
            activation function name,
            one of [&#34;relu&#34;, &#34;gelu&#34;, &#34;swish&#34;, &#34;sigmoid&#34;],
            default &#34;relu&#34;.
    &#34;&#34;&#34;
    name = name.lower()
    if name == &#34;relu&#34;:
        return nn.ReLU(inplace=True)
    if name == &#34;gelu&#34;:
        return nn.GELU()
    if name == &#34;swish&#34;:
        return Swish()
    if name == &#34;sigmoid&#34;:
        return torch.nn.Sigmoid()
    return nn.Identity()</code></pre>
</details>
<div class="desc"><p>Select an activation function by name</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>str
activation function name,
one of ["relu", "gelu", "swish", "sigmoid"],
default "relu".</dd>
</dl></div>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.get_offset"><code class="name flex">
<span>def <span class="ident">get_offset</span></span>(<span>input_layer: str, time_reduction: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_offset(input_layer: str, time_reduction: int):
    &#34;&#34;&#34;Get an offset. We will use the offset for determining #frames of a
    subsampled feature.

    Args:
        input_layer (str): Type of an input layer
        time_reduction (int): time reduction factor for downsampling a feature
    Returns:
        int: offset
    &#34;&#34;&#34;
    if input_layer in (&#34;conv2d&#34;, &#34;nemo_conv&#34;) and time_reduction == 4:
        return 3
    if input_layer in (&#34;conv2d&#34;,) and time_reduction == 6:
        return 1
    if input_layer in (&#34;conv2d&#34;, &#34;nemo_conv&#34;) and time_reduction == 8:
        return 7
    return 0</code></pre>
</details>
<div class="desc"><p>Get an offset. We will use the offset for determining #frames of a
subsampled feature.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_layer</code></strong> :&ensp;<code>str</code></dt>
<dd>Type of an input layer</dd>
<dt><strong><code>time_reduction</code></strong> :&ensp;<code>int</code></dt>
<dd>time reduction factor for downsampling a feature</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>offset</dd>
</dl></div>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.masked_softmax"><code class="name flex">
<span>def <span class="ident">masked_softmax</span></span>(<span>scores, mask: torch.Tensor | None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def masked_softmax(
    scores,
    mask: Optional[Tensor],
):
    if mask is not None:
        mask = mask.unsqueeze(1).eq(0)  # (batch, 1, time1, time2)
        scores = scores.masked_fill(mask, -torch.inf)
        attn = torch.softmax(scores, dim=-1).masked_fill(
            mask, 0.0
        )  # (batch, head, time1, time2)
    else:
        attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)
    return attn</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.unfold_tensor"><code class="name flex">
<span>def <span class="ident">unfold_tensor</span></span>(<span>xs_pad, max_seq_len)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unfold_tensor(xs_pad, max_seq_len):
    &#34;&#34;&#34;
    For a given tensor with shape of (N, T, D), if sequence length T is
    longer than max_seq_len, this function unfold it to a
    (NT&#39;, max_seq_len, D) where T&#39; is T // max_seq_len.
    Args:
        xs_pad: N, T, D
    &#34;&#34;&#34;
    _, _, D = xs_pad.shape
    xs_pad = xs_pad.transpose(-1, -2)  # convert to N, D, T
    # N x D x 1 x T =&gt; N x (D x max_seq_len) x T&#39;
    xs_pad = F.unfold(
        xs_pad[..., None, :],
        kernel_size=(1, max_seq_len),
        stride=(1, max_seq_len),
    )
    new_bsz, _, slen = xs_pad.shape
    # N x D x max_seq_len x T&#39;
    xs_pad = xs_pad.view(new_bsz, -1, max_seq_len, slen)
    # N x T&#39; x max_seq_len x D
    xs_pad = xs_pad.permute(0, 3, 2, 1).contiguous()
    # NT&#39; x max_seq_len x D
    xs_pad = xs_pad.view(-1, max_seq_len, D)
    return xs_pad</code></pre>
</details>
<div class="desc"><p>For a given tensor with shape of (N, T, D), if sequence length T is
longer than max_seq_len, this function unfold it to a
(NT', max_seq_len, D) where T' is T // max_seq_len.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>xs_pad</code></strong></dt>
<dd>N, T, D</dd>
</dl></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.AbsolutePositionalEncoding"><code class="flex name class">
<span>class <span class="ident">AbsolutePositionalEncoding</span></span>
<span>(</span><span>d_model, dropout_rate, max_len=5000)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AbsolutePositionalEncoding(nn.Module):
    &#34;&#34;&#34;Absolute Positional encoding module.
    This module implement Absolute sinusoidal positional encoding
    from: https://arxiv.org/pdf/1706.03762.pdf

    Args:
        d_model: int
            Input embedding size.
        dropout_rate: float
            dropout rate
        max_len: int, optional
            Maximum input length sequence, Default 5000

    &#34;&#34;&#34;

    def __init__(self, d_model, dropout_rate, max_len=5000):
        &#34;&#34;&#34;Construct an PositionalEncoding object.&#34;&#34;&#34;
        super().__init__()
        self.d_model = d_model
        self.xscale = math.sqrt(self.d_model)
        self.dropout = torch.nn.Dropout(p=dropout_rate)
        self.pe = None
        self.extend_pe(torch.tensor(0.0).expand(1, max_len))
        self._register_load_state_dict_pre_hook(_pre_hook)

    def extend_pe(self, x):
        &#34;&#34;&#34;Reset the positional encodings.

        Args:
            x: torch.Tensor
        &#34;&#34;&#34;
        if self.pe is not None and self.pe.size(1) &gt;= x.size(1):
            if self.pe.dtype != x.dtype or self.pe.device != x.device:
                self.pe = self.pe.to(dtype=x.dtype, device=x.device)
            return
        pe = torch.zeros(x.size(1), self.d_model)
        position = torch.arange(0, x.size(1), dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, self.d_model, 2, dtype=torch.float32)
            * -(math.log(10000.0) / self.d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.pe = pe.to(device=x.device, dtype=x.dtype)

    def forward(self, x: torch.Tensor):
        &#34;&#34;&#34;Add positional encoding.

        Args:
            x: torch.Tensor
                Input tensor. shape is (batch, time, ...)

        Returns:
            torch.Tensor: Encoded tensor. Its shape is (batch, time, ...)

        &#34;&#34;&#34;
        self.extend_pe(x)
        x = x * self.xscale + self.pe[:, : x.size(1)]
        return self.dropout(x)</code></pre>
</details>
<div class="desc"><p>Absolute Positional encoding module.
This module implement Absolute sinusoidal positional encoding
from: <a href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>d_model</code></strong></dt>
<dd>int
Input embedding size.</dd>
<dt><strong><code>dropout_rate</code></strong></dt>
<dd>float
dropout rate</dd>
<dt><strong><code>max_len</code></strong></dt>
<dd>int, optional
Maximum input length sequence, Default 5000</dd>
</dl>
<p>Construct an PositionalEncoding object.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.AbsolutePositionalEncoding.extend_pe"><code class="name flex">
<span>def <span class="ident">extend_pe</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extend_pe(self, x):
    &#34;&#34;&#34;Reset the positional encodings.

    Args:
        x: torch.Tensor
    &#34;&#34;&#34;
    if self.pe is not None and self.pe.size(1) &gt;= x.size(1):
        if self.pe.dtype != x.dtype or self.pe.device != x.device:
            self.pe = self.pe.to(dtype=x.dtype, device=x.device)
        return
    pe = torch.zeros(x.size(1), self.d_model)
    position = torch.arange(0, x.size(1), dtype=torch.float32).unsqueeze(1)
    div_term = torch.exp(
        torch.arange(0, self.d_model, 2, dtype=torch.float32)
        * -(math.log(10000.0) / self.d_model)
    )
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    pe = pe.unsqueeze(0)
    self.pe = pe.to(device=x.device, dtype=x.dtype)</code></pre>
</details>
<div class="desc"><p>Reset the positional encodings.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>torch.Tensor</dd>
</dl></div>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.AbsolutePositionalEncoding.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: torch.Tensor) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x: torch.Tensor):
    &#34;&#34;&#34;Add positional encoding.

    Args:
        x: torch.Tensor
            Input tensor. shape is (batch, time, ...)

    Returns:
        torch.Tensor: Encoded tensor. Its shape is (batch, time, ...)

    &#34;&#34;&#34;
    self.extend_pe(x)
    x = x * self.xscale + self.pe[:, : x.size(1)]
    return self.dropout(x)</code></pre>
</details>
<div class="desc"><p>Add positional encoding.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>torch.Tensor
Input tensor. shape is (batch, time, &hellip;)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Encoded tensor. Its shape is (batch, time, &hellip;)</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.AttBlock"><code class="flex name class">
<span>class <span class="ident">AttBlock</span></span>
<span>(</span><span>input_size, output_size)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AttBlock(BlockBase, AttModule):
    &#34;&#34;&#34;Attention Block module to support both Attention and Block module.&#34;&#34;&#34;

    def memory_dims(self, max_len=False):
        &#34;&#34;&#34;memory dimensions&#34;&#34;&#34;
        return (1, self.input_size)</code></pre>
</details>
<div class="desc"><p>Attention Block module to support both Attention and Block module.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.models.phi4mm_utils.BlockBase" href="#sglang.srt.models.phi4mm_utils.BlockBase">BlockBase</a></li>
<li><a title="sglang.srt.models.phi4mm_utils.AttModule" href="#sglang.srt.models.phi4mm_utils.AttModule">AttModule</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.AttBlock.memory_dims"><code class="name flex">
<span>def <span class="ident">memory_dims</span></span>(<span>self, max_len=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def memory_dims(self, max_len=False):
    &#34;&#34;&#34;memory dimensions&#34;&#34;&#34;
    return (1, self.input_size)</code></pre>
</details>
<div class="desc"><p>memory dimensions</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.models.phi4mm_utils.AttModule" href="#sglang.srt.models.phi4mm_utils.AttModule">AttModule</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.models.phi4mm_utils.AttModule.forward" href="#sglang.srt.models.phi4mm_utils.AttModule.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.AttModule.set_export" href="#sglang.srt.models.phi4mm_utils.AttModule.set_export">set_export</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.AttModule"><code class="flex name class">
<span>class <span class="ident">AttModule</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AttModule(nn.Module):
    &#34;&#34;&#34;Attention abstraction module&#34;&#34;&#34;

    def __init__(self):
        super().__init__()
        self.export_mode = False

    def set_export(self, mode=True):
        &#34;&#34;&#34;set the export mode&#34;&#34;&#34;
        self.export_mode = mode

    def forward(
        self,
        x: Tensor,
        memory: Optional[Tensor] = None,
        pos_emb: Optional[Tensor] = None,
        att_mask: Optional[Tensor] = None,
    ) -&gt; tuple[Tensor, Tensor, Optional[Tensor], Optional[Tensor]]:
        &#34;&#34;&#34;AttModule forward

        Args:
            x: torch.Tensor
                input tensor.
            memory: torch.Tensor, optional
                memory tensor.
            pos_emb: torch.Tensor, optional
                positional encoder embedding.
            att_mask: torch.Tensor, optional
                attention mask tensor.
        &#34;&#34;&#34;
        return x, memory, pos_emb, att_mask</code></pre>
</details>
<div class="desc"><p>Attention abstraction module</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.models.phi4mm_utils.AttBlock" href="#sglang.srt.models.phi4mm_utils.AttBlock">AttBlock</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.AttModule.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>x: torch.Tensor,<br>memory: torch.Tensor | None = None,<br>pos_emb: torch.Tensor | None = None,<br>att_mask: torch.Tensor | None = None) ‑> tuple[torch.Tensor, torch.Tensor, torch.Tensor | None, torch.Tensor | None]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    x: Tensor,
    memory: Optional[Tensor] = None,
    pos_emb: Optional[Tensor] = None,
    att_mask: Optional[Tensor] = None,
) -&gt; tuple[Tensor, Tensor, Optional[Tensor], Optional[Tensor]]:
    &#34;&#34;&#34;AttModule forward

    Args:
        x: torch.Tensor
            input tensor.
        memory: torch.Tensor, optional
            memory tensor.
        pos_emb: torch.Tensor, optional
            positional encoder embedding.
        att_mask: torch.Tensor, optional
            attention mask tensor.
    &#34;&#34;&#34;
    return x, memory, pos_emb, att_mask</code></pre>
</details>
<div class="desc"><p>AttModule forward</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>torch.Tensor
input tensor.</dd>
<dt><strong><code>memory</code></strong></dt>
<dd>torch.Tensor, optional
memory tensor.</dd>
<dt><strong><code>pos_emb</code></strong></dt>
<dd>torch.Tensor, optional
positional encoder embedding.</dd>
<dt><strong><code>att_mask</code></strong></dt>
<dd>torch.Tensor, optional
attention mask tensor.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.AttModule.set_export"><code class="name flex">
<span>def <span class="ident">set_export</span></span>(<span>self, mode=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_export(self, mode=True):
    &#34;&#34;&#34;set the export mode&#34;&#34;&#34;
    self.export_mode = mode</code></pre>
</details>
<div class="desc"><p>set the export mode</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.BlockBase"><code class="flex name class">
<span>class <span class="ident">BlockBase</span></span>
<span>(</span><span>input_size, output_size)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BlockBase(nn.Module):
    &#34;&#34;&#34;Block abstract module&#34;&#34;&#34;

    def __init__(self, input_size, output_size):
        super().__init__()
        self.input_size = input_size
        self.output_size = output_size</code></pre>
</details>
<div class="desc"><p>Block abstract module</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.models.phi4mm_utils.AttBlock" href="#sglang.srt.models.phi4mm_utils.AttBlock">AttBlock</a></li>
</ul>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.CausalConv1D"><code class="flex name class">
<span>class <span class="ident">CausalConv1D</span></span>
<span>(</span><span>in_channels: int,<br>out_channels: int,<br>kernel_size: int,<br>stride: int = 1,<br>padding: str | int = 0,<br>dilation: int = 1,<br>groups: int = 1,<br>bias: bool = True,<br>padding_mode: str = 'zeros',<br>device=None,<br>dtype=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CausalConv1D(nn.Conv1d):
    &#34;&#34;&#34;
    A causal version of nn.Conv1d where each step would have limited access to
    locations on its right or left
    All arguments are the same as nn.Conv1d except padding.

    If padding is set None, then paddings are set automatically to make it a
    causal convolution where each location would not see any steps on its right.

    If padding is set as a list (size of 2), then padding[0] would be used as
    left padding and padding[1] as right padding.
    It would make it possible to control the number of steps to be accessible
    on the right and left.
    This mode is not supported when stride &gt; 1. padding[0]+padding[1] should
    be equal to (kernel_size - 1).
    &#34;&#34;&#34;

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        padding: Union[str, int] = 0,
        dilation: int = 1,
        groups: int = 1,
        bias: bool = True,
        padding_mode: str = &#34;zeros&#34;,
        device=None,
        dtype=None,
    ) -&gt; None:
        self.cache_drop_size = None
        if padding is None:
            self._left_padding = kernel_size - 1
            self._right_padding = stride - 1
        else:
            if stride != 1 and padding != kernel_size - 1:
                raise ValueError(&#34;No striding allowed for non-symmetric convolutions!&#34;)
            if isinstance(padding, int):
                self._left_padding = padding
                self._right_padding = padding
            elif (
                isinstance(padding, list)
                and len(padding) == 2
                and padding[0] + padding[1] == kernel_size - 1
            ):
                self._left_padding = padding[0]
                self._right_padding = padding[1]
            else:
                raise ValueError(f&#34;Invalid padding param: {padding}!&#34;)

        self._max_cache_len = self._left_padding

        super().__init__(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=0,
            dilation=dilation,
            groups=groups,
            bias=bias,
            padding_mode=padding_mode,
            device=device,
            dtype=dtype,
        )

    def update_cache(self, x, cache=None):
        if cache is None:
            new_x = F.pad(x, pad=(self._left_padding, self._right_padding))
            next_cache = cache
        else:
            new_x = F.pad(x, pad=(0, self._right_padding))
            new_x = torch.cat([cache, new_x], dim=-1)
            if self.cache_drop_size &gt; 0:
                next_cache = new_x[:, :, : -self.cache_drop_size]
            else:
                next_cache = new_x
            next_cache = next_cache[:, :, -cache.size(-1) :]
        return new_x, next_cache

    def forward(self, x, cache=None):
        x, cache = self.update_cache(x, cache=cache)
        x = super().forward(x)
        if cache is None:
            return x
        else:
            return x, cache</code></pre>
</details>
<div class="desc"><p>A causal version of nn.Conv1d where each step would have limited access to
locations on its right or left
All arguments are the same as nn.Conv1d except padding.</p>
<p>If padding is set None, then paddings are set automatically to make it a
causal convolution where each location would not see any steps on its right.</p>
<p>If padding is set as a list (size of 2), then padding[0] would be used as
left padding and padding[1] as right padding.
It would make it possible to control the number of steps to be accessible
on the right and left.
This mode is not supported when stride &gt; 1. padding[0]+padding[1] should
be equal to (kernel_size - 1).</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.conv.Conv1d</li>
<li>torch.nn.modules.conv._ConvNd</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.CausalConv1D.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x, cache=None) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x, cache=None):
    x, cache = self.update_cache(x, cache=cache)
    x = super().forward(x)
    if cache is None:
        return x
    else:
        return x, cache</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.CausalConv1D.update_cache"><code class="name flex">
<span>def <span class="ident">update_cache</span></span>(<span>self, x, cache=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_cache(self, x, cache=None):
    if cache is None:
        new_x = F.pad(x, pad=(self._left_padding, self._right_padding))
        next_cache = cache
    else:
        new_x = F.pad(x, pad=(0, self._right_padding))
        new_x = torch.cat([cache, new_x], dim=-1)
        if self.cache_drop_size &gt; 0:
            next_cache = new_x[:, :, : -self.cache_drop_size]
        else:
            next_cache = new_x
        next_cache = next_cache[:, :, -cache.size(-1) :]
    return new_x, next_cache</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.CausalConv2D"><code class="flex name class">
<span>class <span class="ident">CausalConv2D</span></span>
<span>(</span><span>in_channels: int,<br>out_channels: int,<br>kernel_size: int,<br>stride: int = 1,<br>padding: str | int = 0,<br>dilation: int = 1,<br>groups: int = 1,<br>bias: bool = True,<br>padding_mode: str = 'zeros',<br>device=None,<br>dtype=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CausalConv2D(nn.Conv2d):
    &#34;&#34;&#34;
    A causal version of nn.Conv2d where each location in the 2D matrix would
    have no access to locations on its right or down
    All arguments are the same as nn.Conv2d except padding which should be
    set as None
    &#34;&#34;&#34;

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int = 1,
        padding: Union[str, int] = 0,
        dilation: int = 1,
        groups: int = 1,
        bias: bool = True,
        padding_mode: str = &#34;zeros&#34;,
        device=None,
        dtype=None,
    ) -&gt; None:
        if padding is not None:
            raise ValueError(&#34;Argument padding should be set to None for CausalConv2D.&#34;)
        self._left_padding = kernel_size - 1
        self._right_padding = stride - 1

        padding = 0
        super().__init__(
            in_channels,
            out_channels,
            kernel_size,
            stride,
            padding,
            dilation,
            groups,
            bias,
            padding_mode,
            device,
            dtype,
        )

    def forward(
        self,
        x,
    ):
        x = F.pad(
            x,
            pad=(self._left_padding, self._right_padding, 0, 0),
        )
        x = super().forward(x)
        return x</code></pre>
</details>
<div class="desc"><p>A causal version of nn.Conv2d where each location in the 2D matrix would
have no access to locations on its right or down
All arguments are the same as nn.Conv2d except padding which should be
set as None</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.conv.Conv2d</li>
<li>torch.nn.modules.conv._ConvNd</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.CausalConv2D.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    x,
):
    x = F.pad(
        x,
        pad=(self._left_padding, self._right_padding, 0, 0),
    )
    x = super().forward(x)
    return x</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.ConvModule"><code class="flex name class">
<span>class <span class="ident">ConvModule</span></span>
<span>(</span><span>input_dim,<br>ext_pw_out_channel,<br>depthwise_seperable_out_channel,<br>ext_pw_kernel_size,<br>kernel_size,<br>depthwise_multiplier,<br>dropout_rate,<br>causal=False,<br>batch_norm=False,<br>chunk_se=0,<br>chunk_size=18,<br>activation='relu',<br>glu_type='sigmoid',<br>bias_in_glu=True,<br>linear_glu_in_convm=False,<br>export=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConvModule(nn.Module):
    &#34;&#34;&#34;ConvModule Module for the conformer block.
    for more details see:
    https://arxiv.org/pdf/2005.08100v1.pdf

    Args:
        input_dim: int
            input channel size.
        ext_pw_out_channel: int
            if &gt; 0, ext_pw_out_channel is a dim channel size
             for the last pointwise conv after swish activation.
        depthwise_seperable_out_channel: int
            if set different to 0, the number of
             depthwise_seperable_out_channel
             will be used as a channel_out of the second conv1d layer.
             otherwise, it equal to 0, the second conv1d layer is skipped.
        ext_pw_kernel_size: int
            kernel size of the conv pointwise of the conformer.
        kernel_size: int
            kernel size.
        depthwise_multiplier: int
            number of input_dim channels duplication. this value
             will be used to compute the hidden channels of the Conv1D.
        dropout_rate: float
            dropout rate.
        causal: bool, optional
            if set to True, convolution have no access
             to future frames. default False.
        batch_norm: bool, optional
            if set to True, apply batchnorm before activation.
            default False
        chunk_se: int, optional
            0 for offline SE.
            1 for streaming SE, where mean is computed
             by accumulated history until current chunk_se.
            2 for streaming SE, where mean is computed
             by only the current chunk.
        chunk_size: int, optional
            chunk size for cnn. default 18
        activation: str, optional
            activation function used in ConvModule,
            default: &#34;relu&#34;.
        glu_type: str, optional
            activation function used for the glu,
            default: &#34;sigmoid&#34;.
        bias_in_glu: bool, optional
            if set to True, use additive bias in the weight module
             before GLU.
        linear_glu_in_convm: bool, optional
            if set to True, use GLULinear module,
             otherwise, used GLUPointWiseConv module.
              default to False.
        export: bool, optional,
            if set to True, padding is equal to 0.  This is for inference,
             or onnx export.  Typically this is set by the export program or
             the decoder program, and it isn&#39;t present in your config file.
             default False
    &#34;&#34;&#34;

    def __init__(
        self,
        input_dim,
        ext_pw_out_channel,
        depthwise_seperable_out_channel,
        ext_pw_kernel_size,
        kernel_size,
        depthwise_multiplier,
        dropout_rate,
        causal=False,
        batch_norm=False,
        chunk_se=0,
        chunk_size=18,
        activation=&#34;relu&#34;,
        glu_type=&#34;sigmoid&#34;,
        bias_in_glu=True,
        linear_glu_in_convm=False,
        export=False,
    ):
        super().__init__()
        self.layer_norm = nn.LayerNorm(input_dim)
        self.input_dim = input_dim
        self.ext_pw_out_channel = ext_pw_out_channel
        self.ext_pw_kernel_size = ext_pw_kernel_size
        self.depthwise_seperable_out_channel = depthwise_seperable_out_channel
        self.glu_type = glu_type
        self.bias_in_glu = bias_in_glu
        self.linear_glu_in_convm = linear_glu_in_convm
        self.causal = causal

        self._add_ext_pw_layer()

        self.batch_norm = batch_norm
        self.kernel_size = kernel_size

        if batch_norm:
            self.bn_layer = nn.BatchNorm1d(input_dim)

        self.act = get_activation(activation)
        self.dropout = nn.Dropout(dropout_rate)
        self.export = export

        if causal:
            padding = 0 if export else kernel_size - 1
        else:
            padding = (kernel_size - 1) // 2

        self.dw_sep_conv_1d = DepthWiseSeperableConv1d(
            input_dim,
            depthwise_seperable_out_channel,
            kernel_size,
            depthwise_multiplier,
            padding=padding,
        )

        if depthwise_seperable_out_channel != 0:
            if input_dim != depthwise_seperable_out_channel:
                self.ln2 = nn.Linear(depthwise_seperable_out_channel, input_dim)
        else:
            if depthwise_multiplier != 1:
                self.ln2 = nn.Linear(input_dim * depthwise_multiplier, input_dim)

    def _add_ext_pw_layer(self):
        &#34;&#34;&#34;
        This function is an extension of __init__ function
        and dedicated to the convolution module creation
        of the conformer.
        &#34;&#34;&#34;
        self.ln1 = self.glu = self.bn_layer = self.ext_pw_conv_1d = (
            nn.Identity()
        )  # jit hacks.
        self.squeeze_excitation = nn.Identity()  # jit.
        self.apply_ln1 = self.fix_len1 = False  # jit.

        if self.ext_pw_out_channel != 0:
            if self.causal:
                self.ext_pw_conv_1d = nn.Conv1d(
                    self.input_dim,
                    self.ext_pw_out_channel,
                    self.ext_pw_kernel_size,
                    1,
                    padding=(self.ext_pw_kernel_size - 1),
                )
                if self.ext_pw_kernel_size &gt; 1:
                    self.fix_len1 = True
                else:
                    self.fix_len1 = False
            else:
                self.ext_pw_conv_1d = nn.Conv1d(
                    self.input_dim,
                    self.ext_pw_out_channel,
                    self.ext_pw_kernel_size,
                    1,
                    padding=(self.ext_pw_kernel_size - 1) // 2,
                )
                self.fix_len1 = False

            if self.linear_glu_in_convm:
                self.glu = GLULinear(
                    self.input_dim,
                    self.ext_pw_out_channel,
                    self.glu_type,
                    self.bias_in_glu,
                )
            else:
                self.glu = GLUPointWiseConv(
                    self.input_dim,
                    self.ext_pw_out_channel,
                    self.ext_pw_kernel_size,
                    self.glu_type,
                    self.bias_in_glu,
                    self.causal,
                )

            if self.input_dim != self.ext_pw_out_channel:
                self.apply_ln1 = True
                self.ln1 = nn.Linear(self.ext_pw_out_channel, self.input_dim)
            else:
                self.apply_ln1 = False
        else:
            self.pw_conv_simplify_w = torch.nn.Parameter(torch.ones(3))
            self.pw_conv_simplify_b = torch.nn.Parameter(torch.zeros(3))

    def forward(self, x):
        &#34;&#34;&#34;ConvModule Forward.

        Args:
            x: torch.Tensor
                input tensor.
        &#34;&#34;&#34;
        x = self.layer_norm(x)

        if self.ext_pw_out_channel != 0:
            x = self.glu(x)
            if self.causal and self.ext_pw_kernel_size &gt; 1:
                x = x[:, : -(self.ext_pw_kernel_size - 1), :]
            if self.apply_ln1:
                x = self.ln1(x)
        else:
            x_0 = x * self.pw_conv_simplify_w[0] + self.pw_conv_simplify_b[0]
            x_1 = x * self.pw_conv_simplify_w[1] + self.pw_conv_simplify_b[1]
            x = x_0 + x_1

        x = x.permute([0, 2, 1])

        x = self.dw_sep_conv_1d(x)
        if self.causal and self.kernel_size &gt; 1:
            x = x[:, :, : -(self.kernel_size - 1)]
        if hasattr(self, &#34;ln2&#34;):
            x = x.permute([0, 2, 1])
            x = self.ln2(x)
            x = x.permute([0, 2, 1])
        if self.batch_norm:
            x = self.bn_layer(x)
        x = self.act(x)

        if self.ext_pw_out_channel != 0:
            x = self.ext_pw_conv_1d(x)
            if self.fix_len1:
                x = x[:, :, : -(self.ext_pw_kernel_size - 1)]

            if self.apply_ln1:
                x = x.permute([0, 2, 1])
                x = self.ln1(x)
                x = x.permute([0, 2, 1])

            x = x.permute([0, 2, 1])
        else:
            x = x.unsqueeze(1).permute([0, 1, 3, 2])
            x = x * self.pw_conv_simplify_w[2] + self.pw_conv_simplify_b[2]
            x = x.squeeze(1)

        x = self.dropout(x)
        return x</code></pre>
</details>
<div class="desc"><p>ConvModule Module for the conformer block.
for more details see:
<a href="https://arxiv.org/pdf/2005.08100v1.pdf">https://arxiv.org/pdf/2005.08100v1.pdf</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_dim</code></strong></dt>
<dd>int
input channel size.</dd>
<dt><strong><code>ext_pw_out_channel</code></strong></dt>
<dd>int
if &gt; 0, ext_pw_out_channel is a dim channel size
for the last pointwise conv after swish activation.</dd>
<dt><strong><code>depthwise_seperable_out_channel</code></strong></dt>
<dd>int
if set different to 0, the number of
depthwise_seperable_out_channel
will be used as a channel_out of the second conv1d layer.
otherwise, it equal to 0, the second conv1d layer is skipped.</dd>
<dt><strong><code>ext_pw_kernel_size</code></strong></dt>
<dd>int
kernel size of the conv pointwise of the conformer.</dd>
<dt><strong><code>kernel_size</code></strong></dt>
<dd>int
kernel size.</dd>
<dt><strong><code>depthwise_multiplier</code></strong></dt>
<dd>int
number of input_dim channels duplication. this value
will be used to compute the hidden channels of the Conv1D.</dd>
<dt><strong><code>dropout_rate</code></strong></dt>
<dd>float
dropout rate.</dd>
<dt><strong><code>causal</code></strong></dt>
<dd>bool, optional
if set to True, convolution have no access
to future frames. default False.</dd>
<dt><strong><code>batch_norm</code></strong></dt>
<dd>bool, optional
if set to True, apply batchnorm before activation.
default False</dd>
<dt><strong><code>chunk_se</code></strong></dt>
<dd>int, optional
0 for offline SE.
1 for streaming SE, where mean is computed
by accumulated history until current chunk_se.
2 for streaming SE, where mean is computed
by only the current chunk.</dd>
<dt><strong><code>chunk_size</code></strong></dt>
<dd>int, optional
chunk size for cnn. default 18</dd>
<dt><strong><code>activation</code></strong></dt>
<dd>str, optional
activation function used in ConvModule,
default: "relu".</dd>
<dt><strong><code>glu_type</code></strong></dt>
<dd>str, optional
activation function used for the glu,
default: "sigmoid".</dd>
<dt><strong><code>bias_in_glu</code></strong></dt>
<dd>bool, optional
if set to True, use additive bias in the weight module
before GLU.</dd>
<dt><strong><code>linear_glu_in_convm</code></strong></dt>
<dd>bool, optional
if set to True, use GLULinear module,
otherwise, used GLUPointWiseConv module.
default to False.</dd>
<dt><strong><code>export</code></strong></dt>
<dd>bool, optional,
if set to True, padding is equal to 0.
This is for inference,
or onnx export.
Typically this is set by the export program or
the decoder program, and it isn't present in your config file.
default False</dd>
</dl>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.ConvModule.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    &#34;&#34;&#34;ConvModule Forward.

    Args:
        x: torch.Tensor
            input tensor.
    &#34;&#34;&#34;
    x = self.layer_norm(x)

    if self.ext_pw_out_channel != 0:
        x = self.glu(x)
        if self.causal and self.ext_pw_kernel_size &gt; 1:
            x = x[:, : -(self.ext_pw_kernel_size - 1), :]
        if self.apply_ln1:
            x = self.ln1(x)
    else:
        x_0 = x * self.pw_conv_simplify_w[0] + self.pw_conv_simplify_b[0]
        x_1 = x * self.pw_conv_simplify_w[1] + self.pw_conv_simplify_b[1]
        x = x_0 + x_1

    x = x.permute([0, 2, 1])

    x = self.dw_sep_conv_1d(x)
    if self.causal and self.kernel_size &gt; 1:
        x = x[:, :, : -(self.kernel_size - 1)]
    if hasattr(self, &#34;ln2&#34;):
        x = x.permute([0, 2, 1])
        x = self.ln2(x)
        x = x.permute([0, 2, 1])
    if self.batch_norm:
        x = self.bn_layer(x)
    x = self.act(x)

    if self.ext_pw_out_channel != 0:
        x = self.ext_pw_conv_1d(x)
        if self.fix_len1:
            x = x[:, :, : -(self.ext_pw_kernel_size - 1)]

        if self.apply_ln1:
            x = x.permute([0, 2, 1])
            x = self.ln1(x)
            x = x.permute([0, 2, 1])

        x = x.permute([0, 2, 1])
    else:
        x = x.unsqueeze(1).permute([0, 1, 3, 2])
        x = x * self.pw_conv_simplify_w[2] + self.pw_conv_simplify_b[2]
        x = x.squeeze(1)

    x = self.dropout(x)
    return x</code></pre>
</details>
<div class="desc"><p>ConvModule Forward.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>torch.Tensor
input tensor.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.DepthWiseSeperableConv1d"><code class="flex name class">
<span>class <span class="ident">DepthWiseSeperableConv1d</span></span>
<span>(</span><span>input_dim,<br>depthwise_seperable_out_channel,<br>kernel_size,<br>depthwise_multiplier,<br>padding=0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DepthWiseSeperableConv1d(nn.Module):
    &#34;&#34;&#34;DepthWiseSeperableConv1d module used in Convnet module
    for the conformer, for more details see:
    https://arxiv.org/pdf/2005.08100v1.pdf

    Args:
        input_dim: int
            input channel size.
        depthwise_seperable_out_channel: int
            if set different to 0, the number of
             depthwise_seperable_out_channel will be used as a channel_out
             of the second conv1d layer.
             otherwise, it equal to 0, the second conv1d layer is skipped.
        kernel_size: int
            kernel_size
        depthwise_multiplier: int
            number of input_dim channels duplication. this value
            will be used to compute the hidden channels of the Conv1D.
        padding: int, optional
            padding for the conv1d,
             default: 0.

    &#34;&#34;&#34;

    def __init__(
        self,
        input_dim,
        depthwise_seperable_out_channel,
        kernel_size,
        depthwise_multiplier,
        padding=0,
    ):
        super().__init__()

        self.dw_conv = nn.Conv1d(
            input_dim,
            input_dim * depthwise_multiplier,
            kernel_size,
            1,
            padding=padding,
            groups=input_dim,
        )

        if depthwise_seperable_out_channel != 0:
            self.pw_conv = nn.Conv1d(
                input_dim * depthwise_multiplier,
                depthwise_seperable_out_channel,
                1,
                1,
                0,
            )
        else:
            self.pw_conv = nn.Identity()
        self.depthwise_seperable_out_channel = depthwise_seperable_out_channel

    def forward(self, x):
        &#34;&#34;&#34;

        Args:
            x: torch.Tensor
                input tensor
        &#34;&#34;&#34;
        x = self.dw_conv(x)
        if self.depthwise_seperable_out_channel != 0:
            x = self.pw_conv(x)
        return x</code></pre>
</details>
<div class="desc"><p>DepthWiseSeperableConv1d module used in Convnet module
for the conformer, for more details see:
<a href="https://arxiv.org/pdf/2005.08100v1.pdf">https://arxiv.org/pdf/2005.08100v1.pdf</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_dim</code></strong></dt>
<dd>int
input channel size.</dd>
<dt><strong><code>depthwise_seperable_out_channel</code></strong></dt>
<dd>int
if set different to 0, the number of
depthwise_seperable_out_channel will be used as a channel_out
of the second conv1d layer.
otherwise, it equal to 0, the second conv1d layer is skipped.</dd>
<dt><strong><code>kernel_size</code></strong></dt>
<dd>int
kernel_size</dd>
<dt><strong><code>depthwise_multiplier</code></strong></dt>
<dd>int
number of input_dim channels duplication. this value
will be used to compute the hidden channels of the Conv1D.</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>int, optional
padding for the conv1d,
default: 0.</dd>
</dl>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.DepthWiseSeperableConv1d.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    &#34;&#34;&#34;

    Args:
        x: torch.Tensor
            input tensor
    &#34;&#34;&#34;
    x = self.dw_conv(x)
    if self.depthwise_seperable_out_channel != 0:
        x = self.pw_conv(x)
    return x</code></pre>
</details>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>torch.Tensor
input tensor</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.FeedForward"><code class="flex name class">
<span>class <span class="ident">FeedForward</span></span>
<span>(</span><span>d_model, d_inner, dropout_rate, activation='sigmoid', bias_in_glu=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FeedForward(nn.Module):
    &#34;&#34;&#34;FeedForward Module.
    For more details see Conformer paper:
        https://arxiv.org/pdf/2005.08100.pdf

    Args:
        d_model: int
            input size.
        d_inner: int
            output size.
        dropout_rate: float,
            dropout rate.
        activation: str,
            activation function name,
            one of [&#34;relu&#34;, &#34;swish&#34;, &#34;sigmoid&#34;],
            sigmoid activation is only used with &#34;glu_in_fnn=True&#34;,
            default &#34;sigmoid&#34;.
        bias_in_glu: bool, optional
    &#34;&#34;&#34;

    def __init__(
        self,
        d_model,
        d_inner,
        dropout_rate,
        activation=&#34;sigmoid&#34;,
        bias_in_glu=True,
    ):
        super().__init__()
        self.d_model = d_model
        self.d_inner = d_inner

        self.layer_norm = nn.LayerNorm(d_model)
        module = GLULinear(d_model, d_inner, activation, bias_in_glu)
        self.net = nn.Sequential(
            module,
            nn.Dropout(dropout_rate),
            nn.Linear(d_inner, d_model),
            nn.Dropout(dropout_rate),
        )

    def forward(self, x):
        &#34;&#34;&#34;FeedForward forward function.

        Args:
            x: torch.Tensor
                input tensor.
        &#34;&#34;&#34;
        out = self.net(self.layer_norm(x))

        return out</code></pre>
</details>
<div class="desc"><p>FeedForward Module.
For more details see Conformer paper:
<a href="https://arxiv.org/pdf/2005.08100.pdf">https://arxiv.org/pdf/2005.08100.pdf</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>d_model</code></strong></dt>
<dd>int
input size.</dd>
<dt><strong><code>d_inner</code></strong></dt>
<dd>int
output size.</dd>
<dt><strong><code>dropout_rate</code></strong></dt>
<dd>float,
dropout rate.</dd>
<dt><strong><code>activation</code></strong></dt>
<dd>str,
activation function name,
one of ["relu", "swish", "sigmoid"],
sigmoid activation is only used with "glu_in_fnn=True",
default "sigmoid".</dd>
<dt><strong><code>bias_in_glu</code></strong></dt>
<dd>bool, optional</dd>
</dl>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.FeedForward.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    &#34;&#34;&#34;FeedForward forward function.

    Args:
        x: torch.Tensor
            input tensor.
    &#34;&#34;&#34;
    out = self.net(self.layer_norm(x))

    return out</code></pre>
</details>
<div class="desc"><p>FeedForward forward function.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>torch.Tensor
input tensor.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.GLU"><code class="flex name class">
<span>class <span class="ident">GLU</span></span>
<span>(</span><span>dim: int = -1, act_name: str = 'sigmoid')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GLU(nn.Module):
    &#34;&#34;&#34;Implement Gated Linear Unit (GLU) module&#34;&#34;&#34;

    def __init__(self, dim: int = -1, act_name: str = &#34;sigmoid&#34;) -&gt; None:
        super().__init__()
        self.dim = dim
        self.act_name = act_name.lower()

        if self.act_name == &#34;relu&#34;:
            self.act_fn = nn.ReLU(inplace=True)
        elif self.act_name == &#34;gelu&#34;:
            self.act_fn = nn.GELU()
        elif self.act_name == &#34;swish&#34;:
            self.act_fn = Swish()
        elif self.act_name == &#34;sigmoid&#34;:
            self.act_fn = nn.Sigmoid()
        else:
            self.act_fn = nn.Identity()

    def forward(self, x: Tensor) -&gt; Tensor:
        &#34;&#34;&#34;GLU forward
        Apply Swish function on the first half of input matrices
        with sigmoid of the second half.

        Args:
            x: torch.Tensor
                Input.

        &#34;&#34;&#34;
        half_x, gate = x.chunk(2, dim=self.dim)
        return half_x * self.act_fn(gate)</code></pre>
</details>
<div class="desc"><p>Implement Gated Linear Unit (GLU) module</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.GLU.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;GLU forward
    Apply Swish function on the first half of input matrices
    with sigmoid of the second half.

    Args:
        x: torch.Tensor
            Input.

    &#34;&#34;&#34;
    half_x, gate = x.chunk(2, dim=self.dim)
    return half_x * self.act_fn(gate)</code></pre>
</details>
<div class="desc"><p>GLU forward
Apply Swish function on the first half of input matrices
with sigmoid of the second half.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>torch.Tensor
Input.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.GLULinear"><code class="flex name class">
<span>class <span class="ident">GLULinear</span></span>
<span>(</span><span>input_dim, output_dim, glu_type='sigmoid', bias_in_glu=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GLULinear(nn.Module):
    &#34;&#34;&#34;Linear + GLU module

    Args:
        input_dim: int
            input size
        output_dim: int
            output size.
        glu_type:
            activation function name used in glu module.
            default &#34;sigmoid&#34; (swish function).
        bias_in_glu: bool, optional
            If True, the addtive bias is added. Default False.
    &#34;&#34;&#34;

    def __init__(
        self,
        input_dim,
        output_dim,
        glu_type=&#34;sigmoid&#34;,
        bias_in_glu=True,
    ):
        super().__init__()
        self.linear = nn.Linear(input_dim, output_dim * 2, bias_in_glu)
        self.glu_act = GLU(-1, glu_type)

    def forward(self, x):
        &#34;&#34;&#34;GLULinear forward

        Args:
            x: torch.Tensor
                inpute tensor.
        &#34;&#34;&#34;
        x = self.linear(x)
        return self.glu_act(x)</code></pre>
</details>
<div class="desc"><p>Linear + GLU module</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_dim</code></strong></dt>
<dd>int
input size</dd>
<dt><strong><code>output_dim</code></strong></dt>
<dd>int
output size.</dd>
<dt>glu_type:</dt>
<dt>activation function name used in glu module.</dt>
<dt>default "sigmoid" (swish function).</dt>
<dt><strong><code>bias_in_glu</code></strong></dt>
<dd>bool, optional
If True, the addtive bias is added. Default False.</dd>
</dl>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.GLULinear.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    &#34;&#34;&#34;GLULinear forward

    Args:
        x: torch.Tensor
            inpute tensor.
    &#34;&#34;&#34;
    x = self.linear(x)
    return self.glu_act(x)</code></pre>
</details>
<div class="desc"><p>GLULinear forward</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>torch.Tensor
inpute tensor.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.GLUPointWiseConv"><code class="flex name class">
<span>class <span class="ident">GLUPointWiseConv</span></span>
<span>(</span><span>input_dim,<br>output_dim,<br>kernel_size,<br>glu_type='sigmoid',<br>bias_in_glu=True,<br>causal=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GLUPointWiseConv(nn.Module):
    &#34;&#34;&#34;GLUPointWiseConv module
    used for conformer architecture,
    for more details see:
    https://arxiv.org/pdf/2005.08100v1.pdf

    Args:
        input_dim: int
            input channel size.
        output_dim: int
            output channel size.
        kernel_size: int
            kernel size
        glu_type: str, optional
            activation function one of
             [&#34;sigmoid&#34;, &#34;relu&#34;, &#34;gelu&#34;]
              default &#34;sigmoid&#34;.
        bias_in_glu: bool, optional
            use addtive bias in glu
        causal: bool, optional
            if set to True, padding is set to the half of
             kernel size, ie, convolution can&#39;t see future frames.
              default False.

    &#34;&#34;&#34;

    def __init__(
        self,
        input_dim,
        output_dim,
        kernel_size,
        glu_type=&#34;sigmoid&#34;,
        bias_in_glu=True,
        causal=False,
    ):
        super().__init__()

        self.glu_type = glu_type
        self.output_dim = output_dim
        self.bias_in_glu = bias_in_glu
        if causal:
            self.ext_pw_conv_1d = nn.Conv1d(
                input_dim,
                output_dim * 2,
                kernel_size,
                1,
                padding=(kernel_size - 1),
            )
        else:
            self.ext_pw_conv_1d = nn.Conv1d(
                input_dim,
                output_dim * 2,
                kernel_size,
                1,
                padding=(kernel_size - 1) // 2,
            )

        if glu_type == &#34;sigmoid&#34;:
            self.glu_act = nn.Sigmoid()
        elif glu_type == &#34;relu&#34;:
            self.glu_act = nn.ReLU()
        elif glu_type == &#34;gelu&#34;:
            self.glu_act = nn.GELU()
        elif glu_type == &#34;swish&#34;:
            self.glu_act = Swish()
        else:
            raise ValueError(f&#34;Unsupported activation type {self.glu_act}&#34;)

        if bias_in_glu:
            self.b1 = nn.Parameter(torch.zeros(1, output_dim, 1))
            self.b2 = nn.Parameter(torch.zeros(1, output_dim, 1))

    def forward(self, x):
        &#34;&#34;&#34;
        Args:
            x: torch.Tensor
                input tensor
        &#34;&#34;&#34;
        # to be consistent with GLULinear, we assume the input always has the
        # #channel (#dim) in the last dimension of the tensor, so need to
        # switch the dimension first for 1D-Conv case
        x = x.permute([0, 2, 1])
        x = self.ext_pw_conv_1d(x)
        if self.glu_type == &#34;bilinear&#34;:
            if self.bias_in_glu:
                x = (x[:, 0 : self.output_dim, :] + self.b1) * (
                    x[:, self.output_dim : self.output_dim * 2, :] + self.b2
                )
            else:
                x = (x[:, 0 : self.output_dim, :]) * (
                    x[:, self.output_dim : self.output_dim * 2, :]
                )
        else:
            if self.bias_in_glu:
                x = (x[:, 0 : self.output_dim, :] + self.b1) * self.glu_act(
                    x[:, self.output_dim : self.output_dim * 2, :] + self.b2
                )
            else:
                x = (x[:, 0 : self.output_dim, :]) * self.glu_act(
                    x[:, self.output_dim : self.output_dim * 2, :]
                )

        x = x.permute([0, 2, 1])
        return x</code></pre>
</details>
<div class="desc"><p>GLUPointWiseConv module
used for conformer architecture,
for more details see:
<a href="https://arxiv.org/pdf/2005.08100v1.pdf">https://arxiv.org/pdf/2005.08100v1.pdf</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_dim</code></strong></dt>
<dd>int
input channel size.</dd>
<dt><strong><code>output_dim</code></strong></dt>
<dd>int
output channel size.</dd>
<dt><strong><code>kernel_size</code></strong></dt>
<dd>int
kernel size</dd>
<dt><strong><code>glu_type</code></strong></dt>
<dd>str, optional
activation function one of
["sigmoid", "relu", "gelu"]
default "sigmoid".</dd>
<dt><strong><code>bias_in_glu</code></strong></dt>
<dd>bool, optional
use addtive bias in glu</dd>
<dt><strong><code>causal</code></strong></dt>
<dd>bool, optional
if set to True, padding is set to the half of
kernel size, ie, convolution can't see future frames.
default False.</dd>
</dl>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.GLUPointWiseConv.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    &#34;&#34;&#34;
    Args:
        x: torch.Tensor
            input tensor
    &#34;&#34;&#34;
    # to be consistent with GLULinear, we assume the input always has the
    # #channel (#dim) in the last dimension of the tensor, so need to
    # switch the dimension first for 1D-Conv case
    x = x.permute([0, 2, 1])
    x = self.ext_pw_conv_1d(x)
    if self.glu_type == &#34;bilinear&#34;:
        if self.bias_in_glu:
            x = (x[:, 0 : self.output_dim, :] + self.b1) * (
                x[:, self.output_dim : self.output_dim * 2, :] + self.b2
            )
        else:
            x = (x[:, 0 : self.output_dim, :]) * (
                x[:, self.output_dim : self.output_dim * 2, :]
            )
    else:
        if self.bias_in_glu:
            x = (x[:, 0 : self.output_dim, :] + self.b1) * self.glu_act(
                x[:, self.output_dim : self.output_dim * 2, :] + self.b2
            )
        else:
            x = (x[:, 0 : self.output_dim, :]) * self.glu_act(
                x[:, self.output_dim : self.output_dim * 2, :]
            )

    x = x.permute([0, 2, 1])
    return x</code></pre>
</details>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>torch.Tensor
input tensor</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.MeanVarianceNormLayer"><code class="flex name class">
<span>class <span class="ident">MeanVarianceNormLayer</span></span>
<span>(</span><span>input_size)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MeanVarianceNormLayer(nn.Module):
    &#34;&#34;&#34;Mean/variance normalization layer.

    Will subtract mean and multiply input by inverted standard deviation.
    Typically used as a very first layer in a model.

    Args:
        input_size: int
            layer input size.
    &#34;&#34;&#34;

    def __init__(self, input_size):
        super().__init__()
        self.input_size = input_size
        self.global_mean = nn.Parameter(torch.zeros(input_size))
        self.global_invstd = nn.Parameter(torch.ones(input_size))

    def forward(self, input_: Tensor) -&gt; Tensor:
        &#34;&#34;&#34;MeanVarianceNormLayer Forward

        Args:
            input_: torch.Tensor
                input tensor.
        &#34;&#34;&#34;
        return (input_ - self.global_mean) * self.global_invstd</code></pre>
</details>
<div class="desc"><p>Mean/variance normalization layer.</p>
<p>Will subtract mean and multiply input by inverted standard deviation.
Typically used as a very first layer in a model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_size</code></strong></dt>
<dd>int
layer input size.</dd>
</dl>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.MeanVarianceNormLayer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, input_: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, input_: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;MeanVarianceNormLayer Forward

    Args:
        input_: torch.Tensor
            input tensor.
    &#34;&#34;&#34;
    return (input_ - self.global_mean) * self.global_invstd</code></pre>
</details>
<div class="desc"><p>MeanVarianceNormLayer Forward</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_</code></strong></dt>
<dd>torch.Tensor
input tensor.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.MultiHeadedAttention"><code class="flex name class">
<span>class <span class="ident">MultiHeadedAttention</span></span>
<span>(</span><span>n_head,<br>n_feat,<br>dropout_rate,<br>attention_inner_dim=-1,<br>glu_type='swish',<br>bias_in_glu=True,<br>use_pt_scaled_dot_product_attention=False,<br>n_value=-1,<br>group_size: int = 1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiHeadedAttention(nn.Module):
    &#34;&#34;&#34;Multi-Head Attention layer with optional relative position embedding
    and GLU.

    Args:
        n_head: int
            the number of heads.
        n_feat: int
            input size features.
        dropout_rate: float
            dropout rate.
        use_LN: bool
            apply layer norm or not
        dropout_at_output: bool
            whether to apply dropout at output
        attention_inner_dim: int, optional
            the attention dimension used in the class,
            it can be different from the input dimension n_feat.
            default: -1 (equal to n_feat).
        use_pt_scaled_dot_product_attention: bool, optional
            if set True, use pytorch scaled dot product attention in training.
            NOTE: this will NOT be used in ONNX decoding due to a lack of
            support.  In that case, we use the original attention
            implementation, which shows no regression.
            default: False.
        n_value: int, optional
            if set to values other than -1, use a different dimension for
            value. With the default value (i.e. -1), it is backward compatible.
        group_size: int, optional. must divide `n_head`
            if group_size &gt; 1:       GQA
            if group_size = 1:       MHA
            if group_size = n_head:  MQA
    &#34;&#34;&#34;

    inv_sqrt_d_k: torch.jit.Final[float]
    h: torch.jit.Final[int]
    h_k: torch.jit.Final[int]
    g: torch.jit.Final[int]

    def __init__(
        self,
        n_head,
        n_feat,
        dropout_rate,
        attention_inner_dim=-1,
        glu_type=&#34;swish&#34;,
        bias_in_glu=True,
        use_pt_scaled_dot_product_attention=False,
        n_value=-1,
        group_size: int = 1,
    ):
        super().__init__()
        if n_value == -1:
            n_value = n_feat
        if attention_inner_dim == -1:
            attention_inner_dim = n_feat
        assert attention_inner_dim % n_head == 0

        # We assume d_v always equals d_k
        self.d_k = attention_inner_dim // n_head
        self.inv_sqrt_d_k = 1.0 / math.sqrt(self.d_k)
        self.h = n_head
        assert n_head % group_size == 0, &#34;group_size must divide n_head&#34;
        self.g = group_size
        self.h_k = n_head // group_size

        self.linear_q = nn.Linear(n_feat, attention_inner_dim)
        self.linear_k = nn.Linear(n_feat, attention_inner_dim // group_size)
        self.linear_v = nn.Linear(n_value, attention_inner_dim // group_size)
        self.linear_out = nn.Linear(attention_inner_dim // group_size, n_value)

        self.attn = torch.jit.Attribute(None, Optional[Tensor])
        self.dropout = nn.Dropout(p=dropout_rate)
        self.dropout_rate = dropout_rate
        self.use_pt_scaled_dot_product_attention = use_pt_scaled_dot_product_attention

        if use_pt_scaled_dot_product_attention and group_size &gt; 1:
            raise ValueError(&#34;Cannot use PT Scaled Attention with GQA&#34;)

        # Torchscript eager quantization.  Note that these functions below are
        # NOOPs and have very little impact on performance unless quantization
        # is enabled.
        self.quant_q = torch.ao.quantization.QuantStub()
        self.quant_x = torch.ao.quantization.QuantStub()
        self.dequant = torch.ao.quantization.DeQuantStub()
        self.ffunc = torch.ao.nn.quantized.FloatFunctional()

    def forward(
        self,
        query: Tensor,
        key: Tensor,
        value: Tensor,
        pos_k: Tensor,
        pos_v: Tensor,
        mask: Optional[Tensor],
        relative_attention_bias: Optional[Tensor] = None,
    ):
        &#34;&#34;&#34;Compute &#39;Scaled Dot Product Attention&#39;.

        Args:
            query: torch.Tensor
                query tensor (batch, time1, size)
            key: torch.Tensor
                key tensor (batch, time2, size)
            value: torch.Tensor
                value tensor (batch, time1, size)
            pos_k: torch.Tensor
                key tensor used for relative positional embedding.
            pos_v: torch.Tensor
                value tensor used for relative positional embedding.
            mask: torch.Tensor
                mask tensor (batch, time1, time2)
            relative_attention_bias: torch.Tensor
                bias added to attention logits w.r.t. relative positions
                (1, n_head, time1, time2)
        &#34;&#34;&#34;
        n_batch = query.size(0)

        q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)  # (b, t, d)
        k = self.linear_k(key).view(n_batch, -1, self.h_k, self.d_k)  # (b, t, d)
        v = self.linear_v(value).view(n_batch, -1, self.h_k, self.d_k)
        q = (
            q.transpose(1, 2)
            if self.use_pt_scaled_dot_product_attention and not torch.jit.is_scripting()
            else q.transpose(1, 2) * self.inv_sqrt_d_k
        )
        k = k.transpose(1, 2)  # (batch, head_k, time2, d_k)
        v = v.transpose(1, 2)  # (batch, head_k, time2, d_k)

        if self.use_pt_scaled_dot_product_attention and not torch.jit.is_scripting():
            attn_mask = None
            if mask is not None:
                mask = mask.unsqueeze(1)
                if relative_attention_bias is not None:
                    attn_mask = mask + relative_attention_bias
                else:
                    attn_mask = mask
                if mask.dtype != q.dtype:
                    attn_mask = attn_mask.to(q.dtype)

            with torch.nn.attention.sdpa_kernel(
                [
                    torch.nn.attention.SDPBackend.FLASH_ATTENTION,
                    torch.nn.attention.SDPBackend.EFFICIENT_ATTENTION,
                    torch.nn.attention.SDPBackend.MATH,
                    torch.nn.attention.SDPBackend.CUDNN_ATTENTION,
                ]
            ):
                x = torch.nn.functional.scaled_dot_product_attention(
                    q,
                    k,
                    v,
                    attn_mask=attn_mask,
                    dropout_p=self.dropout_rate,
                )
        else:
            if self.h != self.h_k:
                q = q.reshape(n_batch, self.g, self.h_k, -1, self.d_k)
                A = torch.einsum(&#34;b g h t d, b h s d -&gt; b h t s&#34;, q, k)
            else:
                A = torch.matmul(q, k.transpose(-2, -1))
            if pos_k is not None:
                if self.h != self.h_k:
                    B = torch.einsum(&#34;b g h t d, t s d -&gt; b h t s&#34;, q, pos_k)
                else:
                    reshape_q = (
                        q.contiguous()
                        .view(n_batch * self.h, -1, self.d_k)
                        .transpose(0, 1)
                    )  # (t1,nh,dk)
                    B = torch.matmul(
                        reshape_q, pos_k.transpose(-2, -1)
                    )  # pos_k: (t1,dk,t2)
                    B = B.transpose(0, 1).view(
                        n_batch, self.h, pos_k.size(0), pos_k.size(1)
                    )
                scores = A + B
            else:
                scores = A

            if relative_attention_bias is not None:
                scores = scores + relative_attention_bias

            attn = masked_softmax(scores, mask)  # (batch, head, time1, time2)

            self.attn = attn

            p_attn = self.dropout(attn)
            x = torch.matmul(p_attn.to(v.dtype), v)  # (batch, head, time1, d_k)
            if pos_v is not None:
                reshape_attn = (
                    p_attn.contiguous()
                    .view(n_batch * self.h, pos_v.size(0), pos_v.size(1))
                    .transpose(0, 1)
                )  # (t1, bh, t2)

                attn_v = (
                    torch.matmul(reshape_attn, pos_v)
                    .transpose(0, 1)
                    .contiguous()
                    .view(n_batch, self.h, pos_v.size(0), self.d_k)
                )
                x = x + attn_v
        x = (
            x.transpose(1, 2).contiguous().view(n_batch, -1, self.h_k * self.d_k)
        )  # (batch, time1, d_model)

        return self.linear_out(x)  # (batch, time1, d_model)</code></pre>
</details>
<div class="desc"><p>Multi-Head Attention layer with optional relative position embedding
and GLU.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_head</code></strong></dt>
<dd>int
the number of heads.</dd>
<dt><strong><code>n_feat</code></strong></dt>
<dd>int
input size features.</dd>
<dt><strong><code>dropout_rate</code></strong></dt>
<dd>float
dropout rate.</dd>
<dt><strong><code>use_LN</code></strong></dt>
<dd>bool
apply layer norm or not</dd>
<dt><strong><code>dropout_at_output</code></strong></dt>
<dd>bool
whether to apply dropout at output</dd>
<dt><strong><code>attention_inner_dim</code></strong></dt>
<dd>int, optional
the attention dimension used in the class,
it can be different from the input dimension n_feat.
default: -1 (equal to n_feat).</dd>
<dt><strong><code>use_pt_scaled_dot_product_attention</code></strong></dt>
<dd>bool, optional
if set True, use pytorch scaled dot product attention in training.
NOTE: this will NOT be used in ONNX decoding due to a lack of
support.
In that case, we use the original attention
implementation, which shows no regression.
default: False.</dd>
<dt><strong><code>n_value</code></strong></dt>
<dd>int, optional
if set to values other than -1, use a different dimension for
value. With the default value (i.e. -1), it is backward compatible.</dd>
<dt><strong><code>group_size</code></strong></dt>
<dd>int, optional. must divide <code>n_head</code>
if group_size &gt; 1:
GQA
if group_size = 1:
MHA
if group_size = n_head:
MQA</dd>
</dl>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.MultiHeadedAttention.g"><code class="name">var <span class="ident">g</span> : Final[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.MultiHeadedAttention.h"><code class="name">var <span class="ident">h</span> : Final[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.MultiHeadedAttention.h_k"><code class="name">var <span class="ident">h_k</span> : Final[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.MultiHeadedAttention.inv_sqrt_d_k"><code class="name">var <span class="ident">inv_sqrt_d_k</span> : Final[float]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.MultiHeadedAttention.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>query: torch.Tensor,<br>key: torch.Tensor,<br>value: torch.Tensor,<br>pos_k: torch.Tensor,<br>pos_v: torch.Tensor,<br>mask: torch.Tensor | None,<br>relative_attention_bias: torch.Tensor | None = None) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    query: Tensor,
    key: Tensor,
    value: Tensor,
    pos_k: Tensor,
    pos_v: Tensor,
    mask: Optional[Tensor],
    relative_attention_bias: Optional[Tensor] = None,
):
    &#34;&#34;&#34;Compute &#39;Scaled Dot Product Attention&#39;.

    Args:
        query: torch.Tensor
            query tensor (batch, time1, size)
        key: torch.Tensor
            key tensor (batch, time2, size)
        value: torch.Tensor
            value tensor (batch, time1, size)
        pos_k: torch.Tensor
            key tensor used for relative positional embedding.
        pos_v: torch.Tensor
            value tensor used for relative positional embedding.
        mask: torch.Tensor
            mask tensor (batch, time1, time2)
        relative_attention_bias: torch.Tensor
            bias added to attention logits w.r.t. relative positions
            (1, n_head, time1, time2)
    &#34;&#34;&#34;
    n_batch = query.size(0)

    q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)  # (b, t, d)
    k = self.linear_k(key).view(n_batch, -1, self.h_k, self.d_k)  # (b, t, d)
    v = self.linear_v(value).view(n_batch, -1, self.h_k, self.d_k)
    q = (
        q.transpose(1, 2)
        if self.use_pt_scaled_dot_product_attention and not torch.jit.is_scripting()
        else q.transpose(1, 2) * self.inv_sqrt_d_k
    )
    k = k.transpose(1, 2)  # (batch, head_k, time2, d_k)
    v = v.transpose(1, 2)  # (batch, head_k, time2, d_k)

    if self.use_pt_scaled_dot_product_attention and not torch.jit.is_scripting():
        attn_mask = None
        if mask is not None:
            mask = mask.unsqueeze(1)
            if relative_attention_bias is not None:
                attn_mask = mask + relative_attention_bias
            else:
                attn_mask = mask
            if mask.dtype != q.dtype:
                attn_mask = attn_mask.to(q.dtype)

        with torch.nn.attention.sdpa_kernel(
            [
                torch.nn.attention.SDPBackend.FLASH_ATTENTION,
                torch.nn.attention.SDPBackend.EFFICIENT_ATTENTION,
                torch.nn.attention.SDPBackend.MATH,
                torch.nn.attention.SDPBackend.CUDNN_ATTENTION,
            ]
        ):
            x = torch.nn.functional.scaled_dot_product_attention(
                q,
                k,
                v,
                attn_mask=attn_mask,
                dropout_p=self.dropout_rate,
            )
    else:
        if self.h != self.h_k:
            q = q.reshape(n_batch, self.g, self.h_k, -1, self.d_k)
            A = torch.einsum(&#34;b g h t d, b h s d -&gt; b h t s&#34;, q, k)
        else:
            A = torch.matmul(q, k.transpose(-2, -1))
        if pos_k is not None:
            if self.h != self.h_k:
                B = torch.einsum(&#34;b g h t d, t s d -&gt; b h t s&#34;, q, pos_k)
            else:
                reshape_q = (
                    q.contiguous()
                    .view(n_batch * self.h, -1, self.d_k)
                    .transpose(0, 1)
                )  # (t1,nh,dk)
                B = torch.matmul(
                    reshape_q, pos_k.transpose(-2, -1)
                )  # pos_k: (t1,dk,t2)
                B = B.transpose(0, 1).view(
                    n_batch, self.h, pos_k.size(0), pos_k.size(1)
                )
            scores = A + B
        else:
            scores = A

        if relative_attention_bias is not None:
            scores = scores + relative_attention_bias

        attn = masked_softmax(scores, mask)  # (batch, head, time1, time2)

        self.attn = attn

        p_attn = self.dropout(attn)
        x = torch.matmul(p_attn.to(v.dtype), v)  # (batch, head, time1, d_k)
        if pos_v is not None:
            reshape_attn = (
                p_attn.contiguous()
                .view(n_batch * self.h, pos_v.size(0), pos_v.size(1))
                .transpose(0, 1)
            )  # (t1, bh, t2)

            attn_v = (
                torch.matmul(reshape_attn, pos_v)
                .transpose(0, 1)
                .contiguous()
                .view(n_batch, self.h, pos_v.size(0), self.d_k)
            )
            x = x + attn_v
    x = (
        x.transpose(1, 2).contiguous().view(n_batch, -1, self.h_k * self.d_k)
    )  # (batch, time1, d_model)

    return self.linear_out(x)  # (batch, time1, d_model)</code></pre>
</details>
<div class="desc"><p>Compute 'Scaled Dot Product Attention'.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>query</code></strong></dt>
<dd>torch.Tensor
query tensor (batch, time1, size)</dd>
<dt><strong><code>key</code></strong></dt>
<dd>torch.Tensor
key tensor (batch, time2, size)</dd>
<dt><strong><code>value</code></strong></dt>
<dd>torch.Tensor
value tensor (batch, time1, size)</dd>
<dt><strong><code>pos_k</code></strong></dt>
<dd>torch.Tensor
key tensor used for relative positional embedding.</dd>
<dt><strong><code>pos_v</code></strong></dt>
<dd>torch.Tensor
value tensor used for relative positional embedding.</dd>
<dt><strong><code>mask</code></strong></dt>
<dd>torch.Tensor
mask tensor (batch, time1, time2)</dd>
<dt><strong><code>relative_attention_bias</code></strong></dt>
<dd>torch.Tensor
bias added to attention logits w.r.t. relative positions
(1, n_head, time1, time2)</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.MultiSequential"><code class="flex name class">
<span>class <span class="ident">MultiSequential</span></span>
<span>(</span><span>*args)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiSequential(torch.nn.Sequential):
    &#34;&#34;&#34;Multi-input multi-output torch.nn.Sequential&#34;&#34;&#34;

    @torch.jit.ignore
    def forward(self, *args):
        &#34;&#34;&#34;Forward method implementation.&#34;&#34;&#34;
        for m in self:
            args = m(*args)
        return args</code></pre>
</details>
<div class="desc"><p>Multi-input multi-output torch.nn.Sequential</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.container.Sequential</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.MultiSequential.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, *args) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.jit.ignore
def forward(self, *args):
    &#34;&#34;&#34;Forward method implementation.&#34;&#34;&#34;
    for m in self:
        args = m(*args)
    return args</code></pre>
</details>
<div class="desc"><p>Forward method implementation.</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.NemoConvSubsampling"><code class="flex name class">
<span>class <span class="ident">NemoConvSubsampling</span></span>
<span>(</span><span>feat_in,<br>feat_out,<br>subsampling_factor=4,<br>subsampling='dw_striding',<br>conv_channels=256,<br>subsampling_conv_chunking_factor=1,<br>activation=ReLU(),<br>is_causal=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NemoConvSubsampling(torch.nn.Module):
    &#34;&#34;&#34;Convlutional subsampling module, taken from NeMo ASR
    (https://github.com/NVIDIA/NeMo/blob/b367413645d5c72db3c2c96e46e95a
    34501479cf/nemo/collections/asr/parts/submodules/subsampling.py)

    Striding Subsampling: &#34;Speech-Transformer: A No-Recurrence
    Sequence-to-Sequence Model for Speech Recognition&#34; by Linhao Dong
    et al. (https://ieeexplore.ieee.org/document/8462506)


    Compared with the EncoderConv2D (`input_layer: custom`), this is a
    much simplified approach, and uses no LayerNorm and far fewer Conv2Ds.
    Moreover, depthwise convolutions are used to reduce FLOPs, but the first
      layer is kept as a regular convolution so as not to degrade accuracy.

    `Striding` and `dw_striding` are the same except that the latter uses
    depthwise convolutions after the first layer, whereas the former does not.

    Args:
        subsampling_factor (int): Time reduction factor
        feat_in (int): size of the input features
        feat_out (int): size of the output features
        subsampling (str): The subsampling technique, choose from
            {&#34;striding&#34;, &#34;dw-striding&#34;, &#34;striding_conv1d&#34;,
            &#34;dw_striding_conv1d&#34;}
        conv_channels (int): Number of channels for the convolution layers,
                            default is 256.
        subsampling_conv_chunking_factor (int): Input chunking factor which
            can be -1 (no chunking) 1 (auto) or a power of 2. Default is 1
        activation (Module): activation function, default is nn.ReLU()
        is_causal (bool): whether to use causal Conv1/2D, where each step will
            have limited access to locations on its right or left
    &#34;&#34;&#34;

    def __init__(
        self,
        feat_in,
        feat_out,
        subsampling_factor=4,
        subsampling=&#34;dw_striding&#34;,
        conv_channels=256,
        subsampling_conv_chunking_factor=1,
        activation=nn.ReLU(),  # noqa: B008
        is_causal=False,
    ):
        super().__init__()
        self._subsampling = subsampling
        self._conv_channels = conv_channels
        self._feat_in = feat_in
        self._feat_out = feat_out

        if subsampling_factor % 2 != 0:
            raise ValueError(&#34;Sampling factor should be a multiply of 2!&#34;)
        self._sampling_num = int(math.log(subsampling_factor, 2))
        self.subsampling_factor = subsampling_factor
        self.is_causal = is_causal
        self.subsampling_causal_cond = subsampling in (
            &#34;dw_striding&#34;,
            &#34;striding&#34;,
            &#34;striding_conv1d&#34;,
        )

        if (
            subsampling_conv_chunking_factor != -1
            and subsampling_conv_chunking_factor != 1
            and subsampling_conv_chunking_factor % 2 != 0
        ):
            raise ValueError(
                &#34;subsampling_conv_chunking_factor should be -1, 1, or a &#34; &#34;power of 2&#34;
            )
        self.subsampling_conv_chunking_factor = subsampling_conv_chunking_factor

        in_channels = 1
        layers = []

        if subsampling == &#34;dw_striding&#34;:
            self._stride = 2
            self._kernel_size = 3
            self._ceil_mode = False

            if self.is_causal:
                self._left_padding = self._kernel_size - 1
                self._right_padding = self._stride - 1
                self._max_cache_len = subsampling_factor + 1
            else:
                self._left_padding = (self._kernel_size - 1) // 2
                self._right_padding = (self._kernel_size - 1) // 2
                self._max_cache_len = 0

            # Layer 1
            if self.is_causal:
                layers.append(
                    CausalConv2D(
                        in_channels=in_channels,
                        out_channels=conv_channels,
                        kernel_size=self._kernel_size,
                        stride=self._stride,
                        padding=None,
                    )
                )
            else:
                layers.append(
                    torch.nn.Conv2d(
                        in_channels=in_channels,
                        out_channels=conv_channels,
                        kernel_size=self._kernel_size,
                        stride=self._stride,
                        padding=self._left_padding,
                    )
                )
            in_channels = conv_channels
            layers.append(activation)

            for i in range(self._sampling_num - 1):
                if self.is_causal:
                    layers.append(
                        CausalConv2D(
                            in_channels=in_channels,
                            out_channels=in_channels,
                            kernel_size=self._kernel_size,
                            stride=self._stride,
                            padding=None,
                            groups=in_channels,
                        )
                    )
                else:
                    layers.append(
                        torch.nn.Conv2d(
                            in_channels=in_channels,
                            out_channels=in_channels,
                            kernel_size=self._kernel_size,
                            stride=self._stride,
                            padding=self._left_padding,
                            groups=in_channels,
                        )
                    )

                layers.append(
                    torch.nn.Conv2d(
                        in_channels=in_channels,
                        out_channels=conv_channels,
                        kernel_size=1,
                        stride=1,
                        padding=0,
                        groups=1,
                    )
                )
                layers.append(activation)
                in_channels = conv_channels

        elif subsampling == &#34;striding&#34;:
            self._stride = 2
            self._kernel_size = 3
            self._ceil_mode = False

            if self.is_causal:
                self._left_padding = self._kernel_size - 1
                self._right_padding = self._stride - 1
                self._max_cache_len = subsampling_factor + 1
            else:
                self._left_padding = (self._kernel_size - 1) // 2
                self._right_padding = (self._kernel_size - 1) // 2
                self._max_cache_len = 0

            for i in range(self._sampling_num):
                if self.is_causal:
                    layers.append(
                        CausalConv2D(
                            in_channels=in_channels,
                            out_channels=conv_channels,
                            kernel_size=self._kernel_size,
                            stride=self._stride,
                            padding=None,
                        )
                    )
                else:
                    layers.append(
                        torch.nn.Conv2d(
                            in_channels=in_channels,
                            out_channels=conv_channels,
                            kernel_size=self._kernel_size,
                            stride=self._stride,
                            padding=self._left_padding,
                        )
                    )
                layers.append(activation)
                in_channels = conv_channels

        elif subsampling == &#34;striding_conv1d&#34;:
            in_channels = feat_in

            self._stride = 2
            self._kernel_size = 5
            self._ceil_mode = False

            if self.is_causal:
                self._left_padding = self._kernel_size - 1
                self._right_padding = self._stride - 1
                self._max_cache_len = subsampling_factor + 1
            else:
                self._left_padding = (self._kernel_size - 1) // 2
                self._right_padding = (self._kernel_size - 1) // 2
                self._max_cache_len = 0

            for i in range(self._sampling_num):
                if self.is_causal:
                    layers.append(
                        CausalConv1D(
                            in_channels=in_channels,
                            out_channels=(
                                feat_out
                                if self._sampling_num == i + 1
                                else conv_channels
                            ),
                            kernel_size=self._kernel_size,
                            stride=self._stride,
                            padding=None,
                        )
                    )
                else:
                    layers.append(
                        torch.nn.Conv1d(
                            in_channels=in_channels,
                            out_channels=(
                                feat_out
                                if self._sampling_num == i + 1
                                else conv_channels
                            ),
                            kernel_size=self._kernel_size,
                            stride=self._stride,
                            padding=self._left_padding,
                        )
                    )
                layers.append(activation)
                in_channels = conv_channels

        elif subsampling == &#34;dw_striding_conv1d&#34;:
            in_channels = feat_in

            self._stride = 2
            self._kernel_size = 5
            self._ceil_mode = False

            self._left_padding = (self._kernel_size - 1) // 2
            self._right_padding = (self._kernel_size - 1) // 2

            # Layer 1
            layers.extend(
                [
                    torch.nn.Conv1d(
                        in_channels=in_channels,
                        out_channels=in_channels,
                        kernel_size=self._kernel_size,
                        stride=self._stride,
                        padding=self._left_padding,
                        groups=in_channels,
                    ),
                    torch.nn.Conv1d(
                        in_channels=in_channels,
                        out_channels=(
                            feat_out if self._sampling_num == 1 else conv_channels
                        ),
                        kernel_size=1,
                        stride=1,
                        padding=0,
                        groups=1,
                    ),
                ]
            )
            in_channels = conv_channels
            layers.append(activation)

            for i in range(self._sampling_num - 1):
                layers.extend(
                    [
                        torch.nn.Conv1d(
                            in_channels=in_channels,
                            out_channels=in_channels,
                            kernel_size=self._kernel_size,
                            stride=self._stride,
                            padding=self._left_padding,
                            groups=in_channels,
                        ),
                        torch.nn.Conv1d(
                            in_channels=in_channels,
                            out_channels=(
                                feat_out
                                if self._sampling_num == i + 2
                                else conv_channels
                            ),
                            kernel_size=1,
                            stride=1,
                            padding=0,
                            groups=1,
                        ),
                    ]
                )
                layers.append(activation)
                in_channels = conv_channels

        else:
            raise ValueError(f&#34;Not valid sub-sampling: {subsampling}!&#34;)

        if subsampling in [&#34;dw_striding&#34;, &#34;striding&#34;]:
            in_length = torch.tensor(feat_in, dtype=torch.float)
            out_length = calc_length(
                lengths=in_length,
                all_paddings=self._left_padding + self._right_padding,
                kernel_size=self._kernel_size,
                stride=self._stride,
                ceil_mode=self._ceil_mode,
                repeat_num=self._sampling_num,
            )
            self.out = torch.nn.Linear(conv_channels * int(out_length), feat_out)
            self.conv2d_subsampling = True
        elif subsampling in [&#34;striding_conv1d&#34;, &#34;dw_striding_conv1d&#34;]:
            self.out = None
            self.conv2d_subsampling = False
        else:
            raise ValueError(f&#34;Not valid sub-sampling: {subsampling}!&#34;)

        self.conv = torch.nn.Sequential(*layers)

    def get_sampling_frames(self):
        return [1, self.subsampling_factor]

    def get_streaming_cache_size(self):
        return [0, self.subsampling_factor + 1]

    def forward(self, x, mask):
        &#34;&#34;&#34;
        Forward method for NeMo subsampling.

        Args:
            x[Batch, Time, Filters]: torch.Tensor
                input tensor
            x_mask: torch.Tensor
                input mask

        Returns:
            x: torch.Tensor
                Resulting tensor from subsampling (B, T //
                time_reduction_factor, feat_out)
            pad_mask: torch.Tensor
                tensor of padded hidden state sequences (B, 1, T //
                time_reduction_factor)
        &#34;&#34;&#34;
        x = x.unsqueeze(1) if self.conv2d_subsampling else x.transpose(1, 2)

        # split inputs if chunking_factor is set
        if self.subsampling_conv_chunking_factor != -1 and self.conv2d_subsampling:
            if self.subsampling_conv_chunking_factor == 1:
                # if subsampling_conv_chunking_factor is 1, we split only
                # if needed.
                # avoiding a bug / feature limiting indexing of tensors
                # to 2**31.
                # see https://github.com/pytorch/pytorch/issues/80020
                x_ceil = 2**31 / self._conv_channels * self._stride * self._stride
                need_to_split = torch.numel(x) &gt; x_ceil
            else:
                # if subsampling_conv_chunking_factor &gt; 1 we always split
                need_to_split = True

            if need_to_split:
                x, success = self.conv_split_by_batch(x)
                if not success:  # if unable to split by batch, try by channel
                    if self._subsampling == &#34;dw_striding&#34;:
                        x = self.conv_split_by_channel(x)
                    else:
                        x = self.conv(x)  # try anyway
            else:
                x = self.conv(x)
        else:
            x = self.conv(x)

        # Flatten Channel and Frequency Axes
        if self.conv2d_subsampling:
            b, c, t, f = x.size()
            x = self.out(x.transpose(1, 2).reshape(b, t, -1))
        # Transpose to Channel Last mode
        else:
            x = x.transpose(1, 2)

        if mask is None:
            return x, None

        max_audio_length = x.shape[1]
        feature_lens = mask.sum(1)
        padding_length = torch.ceil(feature_lens / self.subsampling_factor)
        if self.is_causal and self.subsampling_causal_cond:
            feature_lens_remainder = feature_lens % self.subsampling_factor
            padding_length[feature_lens_remainder != 1] += 1
        pad_mask = torch.arange(0, max_audio_length, device=x.device).expand(
            padding_length.size(0), -1
        ) &lt; padding_length.unsqueeze(1)
        return x, pad_mask.unsqueeze(1)

    def reset_parameters(self):
        # initialize weights
        if self._subsampling == &#34;dw_striding&#34;:
            with torch.no_grad():
                # init conv
                scale = 1.0 / self._kernel_size
                dw_max = (self._kernel_size**2) ** -0.5
                pw_max = self._conv_channels**-0.5

                torch.nn.init.uniform_(self.conv[0].weight, -scale, scale)
                torch.nn.init.uniform_(self.conv[0].bias, -scale, scale)

                for idx in range(2, len(self.conv), 3):
                    torch.nn.init.uniform_(self.conv[idx].weight, -dw_max, dw_max)
                    torch.nn.init.uniform_(self.conv[idx].bias, -dw_max, dw_max)
                    torch.nn.init.uniform_(self.conv[idx + 1].weight, -pw_max, pw_max)
                    torch.nn.init.uniform_(self.conv[idx + 1].bias, -pw_max, pw_max)

                # init fc (80 * 64 = 5120 from https://github.com/kssteven418/
                # Squeezeformer/blob/13c97d6cf92f2844d2cb3142b4c5bfa9ad1a8951/
                # src/models/conformer_encoder.py#L487
                fc_scale = (self._feat_out * self._feat_in / self._sampling_num) ** -0.5
                torch.nn.init.uniform_(self.out.weight, -fc_scale, fc_scale)
                torch.nn.init.uniform_(self.out.bias, -fc_scale, fc_scale)

    def conv_split_by_batch(self, x):
        &#34;&#34;&#34;Tries to split input by batch, run conv and concat results&#34;&#34;&#34;
        b, _, _, _ = x.size()
        if b == 1:  # can&#39;t split if batch size is 1
            return x, False

        if self.subsampling_conv_chunking_factor &gt; 1:
            cf = self.subsampling_conv_chunking_factor
        else:
            # avoiding a bug / feature limiting indexing of tensors to 2**31
            # see https://github.com/pytorch/pytorch/issues/80020
            x_ceil = 2**31 / self._conv_channels * self._stride * self._stride
            p = math.ceil(math.log(torch.numel(x) / x_ceil, 2))
            cf = 2**p

        new_batch_size = b // cf
        if new_batch_size == 0:  # input is too big
            return x, False

        return (
            torch.cat(
                [self.conv(chunk) for chunk in torch.split(x, new_batch_size, 0)]
            ),
            True,
        )

    def conv_split_by_channel(self, x):
        &#34;&#34;&#34;For dw convs, tries to split input by time, run conv and concat
        results&#34;&#34;&#34;
        x = self.conv[0](x)  # full conv2D
        x = self.conv[1](x)  # activation

        for i in range(self._sampling_num - 1):
            _, c, t, _ = x.size()

            if self.subsampling_conv_chunking_factor &gt; 1:
                cf = self.subsampling_conv_chunking_factor
            else:
                # avoiding a bug / feature limiting indexing of tensors
                # to 2**31
                # see https://github.com/pytorch/pytorch/issues/80020
                p = math.ceil(math.log(torch.numel(x) / 2**31, 2))
                cf = 2**p

            new_c = int(c // cf)
            if new_c == 0:
                new_c = 1

            new_t = int(t // cf)
            if new_t == 0:
                new_t = 1

            x = self.channel_chunked_conv(
                self.conv[i * 3 + 2], new_c, x
            )  # conv2D, depthwise

            # splitting pointwise convs by time
            x = torch.cat(
                [self.conv[i * 3 + 3](chunk) for chunk in torch.split(x, new_t, 2)],
                2,
            )  # conv2D, pointwise
            x = self.conv[i * 3 + 4](x)  # activation
        return x

    def channel_chunked_conv(self, conv, chunk_size, x):
        &#34;&#34;&#34;Performs channel chunked convolution&#34;&#34;&#34;

        ind = 0
        out_chunks = []
        for chunk in torch.split(x, chunk_size, 1):
            step = chunk.size()[1]

            if self.is_causal:
                chunk = nn.functional.pad(
                    chunk,
                    pad=(
                        self._kernel_size - 1,
                        self._stride - 1,
                        self._kernel_size - 1,
                        self._stride - 1,
                    ),
                )
                ch_out = nn.functional.conv2d(
                    chunk,
                    conv.weight[ind : ind + step, :, :, :],
                    bias=conv.bias[ind : ind + step],
                    stride=self._stride,
                    padding=0,
                    groups=step,
                )
            else:
                ch_out = nn.functional.conv2d(
                    chunk,
                    conv.weight[ind : ind + step, :, :, :],
                    bias=conv.bias[ind : ind + step],
                    stride=self._stride,
                    padding=self._left_padding,
                    groups=step,
                )
            out_chunks.append(ch_out)
            ind += step

        return torch.cat(out_chunks, 1)

    def change_subsampling_conv_chunking_factor(
        self, subsampling_conv_chunking_factor: int
    ):
        if (
            subsampling_conv_chunking_factor != -1
            and subsampling_conv_chunking_factor != 1
            and subsampling_conv_chunking_factor % 2 != 0
        ):
            raise ValueError(
                &#34;subsampling_conv_chunking_factor should be -1, 1, or a &#34; &#34;power of 2&#34;
            )
        self.subsampling_conv_chunking_factor = subsampling_conv_chunking_factor</code></pre>
</details>
<div class="desc"><p>Convlutional subsampling module, taken from NeMo ASR
(<a href="https://github.com/NVIDIA/NeMo/blob/b367413645d5c72db3c2c96e46e95a">https://github.com/NVIDIA/NeMo/blob/b367413645d5c72db3c2c96e46e95a</a>
34501479cf/nemo/collections/asr/parts/submodules/subsampling.py)</p>
<p>Striding Subsampling: "Speech-Transformer: A No-Recurrence
Sequence-to-Sequence Model for Speech Recognition" by Linhao Dong
et al. (<a href="https://ieeexplore.ieee.org/document/8462506">https://ieeexplore.ieee.org/document/8462506</a>)</p>
<p>Compared with the EncoderConv2D (<code>input_layer: custom</code>), this is a
much simplified approach, and uses no LayerNorm and far fewer Conv2Ds.
Moreover, depthwise convolutions are used to reduce FLOPs, but the first
layer is kept as a regular convolution so as not to degrade accuracy.</p>
<p><code>Striding</code> and <code>dw_striding</code> are the same except that the latter uses
depthwise convolutions after the first layer, whereas the former does not.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>subsampling_factor</code></strong> :&ensp;<code>int</code></dt>
<dd>Time reduction factor</dd>
<dt><strong><code>feat_in</code></strong> :&ensp;<code>int</code></dt>
<dd>size of the input features</dd>
<dt><strong><code>feat_out</code></strong> :&ensp;<code>int</code></dt>
<dd>size of the output features</dd>
<dt><strong><code>subsampling</code></strong> :&ensp;<code>str</code></dt>
<dd>The subsampling technique, choose from
{"striding", "dw-striding", "striding_conv1d",
"dw_striding_conv1d"}</dd>
<dt><strong><code>conv_channels</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of channels for the convolution layers,
default is 256.</dd>
<dt><strong><code>subsampling_conv_chunking_factor</code></strong> :&ensp;<code>int</code></dt>
<dd>Input chunking factor which
can be -1 (no chunking) 1 (auto) or a power of 2. Default is 1</dd>
<dt><strong><code>activation</code></strong> :&ensp;<code>Module</code></dt>
<dd>activation function, default is nn.ReLU()</dd>
<dt><strong><code>is_causal</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to use causal Conv1/2D, where each step will
have limited access to locations on its right or left</dd>
</dl>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.NemoConvSubsampling.change_subsampling_conv_chunking_factor"><code class="name flex">
<span>def <span class="ident">change_subsampling_conv_chunking_factor</span></span>(<span>self, subsampling_conv_chunking_factor: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def change_subsampling_conv_chunking_factor(
    self, subsampling_conv_chunking_factor: int
):
    if (
        subsampling_conv_chunking_factor != -1
        and subsampling_conv_chunking_factor != 1
        and subsampling_conv_chunking_factor % 2 != 0
    ):
        raise ValueError(
            &#34;subsampling_conv_chunking_factor should be -1, 1, or a &#34; &#34;power of 2&#34;
        )
    self.subsampling_conv_chunking_factor = subsampling_conv_chunking_factor</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.NemoConvSubsampling.channel_chunked_conv"><code class="name flex">
<span>def <span class="ident">channel_chunked_conv</span></span>(<span>self, conv, chunk_size, x)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def channel_chunked_conv(self, conv, chunk_size, x):
    &#34;&#34;&#34;Performs channel chunked convolution&#34;&#34;&#34;

    ind = 0
    out_chunks = []
    for chunk in torch.split(x, chunk_size, 1):
        step = chunk.size()[1]

        if self.is_causal:
            chunk = nn.functional.pad(
                chunk,
                pad=(
                    self._kernel_size - 1,
                    self._stride - 1,
                    self._kernel_size - 1,
                    self._stride - 1,
                ),
            )
            ch_out = nn.functional.conv2d(
                chunk,
                conv.weight[ind : ind + step, :, :, :],
                bias=conv.bias[ind : ind + step],
                stride=self._stride,
                padding=0,
                groups=step,
            )
        else:
            ch_out = nn.functional.conv2d(
                chunk,
                conv.weight[ind : ind + step, :, :, :],
                bias=conv.bias[ind : ind + step],
                stride=self._stride,
                padding=self._left_padding,
                groups=step,
            )
        out_chunks.append(ch_out)
        ind += step

    return torch.cat(out_chunks, 1)</code></pre>
</details>
<div class="desc"><p>Performs channel chunked convolution</p></div>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.NemoConvSubsampling.conv_split_by_batch"><code class="name flex">
<span>def <span class="ident">conv_split_by_batch</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conv_split_by_batch(self, x):
    &#34;&#34;&#34;Tries to split input by batch, run conv and concat results&#34;&#34;&#34;
    b, _, _, _ = x.size()
    if b == 1:  # can&#39;t split if batch size is 1
        return x, False

    if self.subsampling_conv_chunking_factor &gt; 1:
        cf = self.subsampling_conv_chunking_factor
    else:
        # avoiding a bug / feature limiting indexing of tensors to 2**31
        # see https://github.com/pytorch/pytorch/issues/80020
        x_ceil = 2**31 / self._conv_channels * self._stride * self._stride
        p = math.ceil(math.log(torch.numel(x) / x_ceil, 2))
        cf = 2**p

    new_batch_size = b // cf
    if new_batch_size == 0:  # input is too big
        return x, False

    return (
        torch.cat(
            [self.conv(chunk) for chunk in torch.split(x, new_batch_size, 0)]
        ),
        True,
    )</code></pre>
</details>
<div class="desc"><p>Tries to split input by batch, run conv and concat results</p></div>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.NemoConvSubsampling.conv_split_by_channel"><code class="name flex">
<span>def <span class="ident">conv_split_by_channel</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conv_split_by_channel(self, x):
    &#34;&#34;&#34;For dw convs, tries to split input by time, run conv and concat
    results&#34;&#34;&#34;
    x = self.conv[0](x)  # full conv2D
    x = self.conv[1](x)  # activation

    for i in range(self._sampling_num - 1):
        _, c, t, _ = x.size()

        if self.subsampling_conv_chunking_factor &gt; 1:
            cf = self.subsampling_conv_chunking_factor
        else:
            # avoiding a bug / feature limiting indexing of tensors
            # to 2**31
            # see https://github.com/pytorch/pytorch/issues/80020
            p = math.ceil(math.log(torch.numel(x) / 2**31, 2))
            cf = 2**p

        new_c = int(c // cf)
        if new_c == 0:
            new_c = 1

        new_t = int(t // cf)
        if new_t == 0:
            new_t = 1

        x = self.channel_chunked_conv(
            self.conv[i * 3 + 2], new_c, x
        )  # conv2D, depthwise

        # splitting pointwise convs by time
        x = torch.cat(
            [self.conv[i * 3 + 3](chunk) for chunk in torch.split(x, new_t, 2)],
            2,
        )  # conv2D, pointwise
        x = self.conv[i * 3 + 4](x)  # activation
    return x</code></pre>
</details>
<div class="desc"><p>For dw convs, tries to split input by time, run conv and concat
results</p></div>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.NemoConvSubsampling.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x, mask) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x, mask):
    &#34;&#34;&#34;
    Forward method for NeMo subsampling.

    Args:
        x[Batch, Time, Filters]: torch.Tensor
            input tensor
        x_mask: torch.Tensor
            input mask

    Returns:
        x: torch.Tensor
            Resulting tensor from subsampling (B, T //
            time_reduction_factor, feat_out)
        pad_mask: torch.Tensor
            tensor of padded hidden state sequences (B, 1, T //
            time_reduction_factor)
    &#34;&#34;&#34;
    x = x.unsqueeze(1) if self.conv2d_subsampling else x.transpose(1, 2)

    # split inputs if chunking_factor is set
    if self.subsampling_conv_chunking_factor != -1 and self.conv2d_subsampling:
        if self.subsampling_conv_chunking_factor == 1:
            # if subsampling_conv_chunking_factor is 1, we split only
            # if needed.
            # avoiding a bug / feature limiting indexing of tensors
            # to 2**31.
            # see https://github.com/pytorch/pytorch/issues/80020
            x_ceil = 2**31 / self._conv_channels * self._stride * self._stride
            need_to_split = torch.numel(x) &gt; x_ceil
        else:
            # if subsampling_conv_chunking_factor &gt; 1 we always split
            need_to_split = True

        if need_to_split:
            x, success = self.conv_split_by_batch(x)
            if not success:  # if unable to split by batch, try by channel
                if self._subsampling == &#34;dw_striding&#34;:
                    x = self.conv_split_by_channel(x)
                else:
                    x = self.conv(x)  # try anyway
        else:
            x = self.conv(x)
    else:
        x = self.conv(x)

    # Flatten Channel and Frequency Axes
    if self.conv2d_subsampling:
        b, c, t, f = x.size()
        x = self.out(x.transpose(1, 2).reshape(b, t, -1))
    # Transpose to Channel Last mode
    else:
        x = x.transpose(1, 2)

    if mask is None:
        return x, None

    max_audio_length = x.shape[1]
    feature_lens = mask.sum(1)
    padding_length = torch.ceil(feature_lens / self.subsampling_factor)
    if self.is_causal and self.subsampling_causal_cond:
        feature_lens_remainder = feature_lens % self.subsampling_factor
        padding_length[feature_lens_remainder != 1] += 1
    pad_mask = torch.arange(0, max_audio_length, device=x.device).expand(
        padding_length.size(0), -1
    ) &lt; padding_length.unsqueeze(1)
    return x, pad_mask.unsqueeze(1)</code></pre>
</details>
<div class="desc"><p>Forward method for NeMo subsampling.</p>
<h2 id="args">Args</h2>
<dl>
<dt>x[Batch, Time, Filters]: torch.Tensor</dt>
<dt>input tensor</dt>
<dt><strong><code>x_mask</code></strong></dt>
<dd>torch.Tensor
input mask</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>x</code></dt>
<dd>torch.Tensor
Resulting tensor from subsampling (B, T //
time_reduction_factor, feat_out)</dd>
<dt><code>pad_mask</code></dt>
<dd>torch.Tensor
tensor of padded hidden state sequences (B, 1, T //
time_reduction_factor)</dd>
</dl></div>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.NemoConvSubsampling.get_sampling_frames"><code class="name flex">
<span>def <span class="ident">get_sampling_frames</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_sampling_frames(self):
    return [1, self.subsampling_factor]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.NemoConvSubsampling.get_streaming_cache_size"><code class="name flex">
<span>def <span class="ident">get_streaming_cache_size</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_streaming_cache_size(self):
    return [0, self.subsampling_factor + 1]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.NemoConvSubsampling.reset_parameters"><code class="name flex">
<span>def <span class="ident">reset_parameters</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_parameters(self):
    # initialize weights
    if self._subsampling == &#34;dw_striding&#34;:
        with torch.no_grad():
            # init conv
            scale = 1.0 / self._kernel_size
            dw_max = (self._kernel_size**2) ** -0.5
            pw_max = self._conv_channels**-0.5

            torch.nn.init.uniform_(self.conv[0].weight, -scale, scale)
            torch.nn.init.uniform_(self.conv[0].bias, -scale, scale)

            for idx in range(2, len(self.conv), 3):
                torch.nn.init.uniform_(self.conv[idx].weight, -dw_max, dw_max)
                torch.nn.init.uniform_(self.conv[idx].bias, -dw_max, dw_max)
                torch.nn.init.uniform_(self.conv[idx + 1].weight, -pw_max, pw_max)
                torch.nn.init.uniform_(self.conv[idx + 1].bias, -pw_max, pw_max)

            # init fc (80 * 64 = 5120 from https://github.com/kssteven418/
            # Squeezeformer/blob/13c97d6cf92f2844d2cb3142b4c5bfa9ad1a8951/
            # src/models/conformer_encoder.py#L487
            fc_scale = (self._feat_out * self._feat_in / self._sampling_num) ** -0.5
            torch.nn.init.uniform_(self.out.weight, -fc_scale, fc_scale)
            torch.nn.init.uniform_(self.out.bias, -fc_scale, fc_scale)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.Swish"><code class="flex name class">
<span>class <span class="ident">Swish</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Swish(nn.Module):
    &#34;&#34;&#34;Implement Swish activation module.
    From https://arxiv.org/pdf/2005.03191.pdf

    &#34;&#34;&#34;

    def __init__(self) -&gt; None:
        super().__init__()
        self.act_fn = nn.Sigmoid()

    def forward(self, x: Tensor) -&gt; Tensor:
        &#34;&#34;&#34;Apply Swish function

        Args:
            x: torch.Tensor
                Input.
        &#34;&#34;&#34;
        return x * self.act_fn(x)</code></pre>
</details>
<div class="desc"><p>Implement Swish activation module.
From <a href="https://arxiv.org/pdf/2005.03191.pdf">https://arxiv.org/pdf/2005.03191.pdf</a></p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.Swish.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;Apply Swish function

    Args:
        x: torch.Tensor
            Input.
    &#34;&#34;&#34;
    return x * self.act_fn(x)</code></pre>
</details>
<div class="desc"><p>Apply Swish function</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>torch.Tensor
Input.</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.phi4mm_utils.T5RelativeAttentionLogitBias"><code class="flex name class">
<span>class <span class="ident">T5RelativeAttentionLogitBias</span></span>
<span>(</span><span>num_heads, num_buckets=-1, max_distance=1000, symmetric=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class T5RelativeAttentionLogitBias(nn.Module):
    &#34;&#34;&#34;
    This module implements the relative position bias described in Section
    2.1 of the T5 paper: https://arxiv.org/pdf/1910.10683.pdf

    The Huggingface implementation is used as a reference
    https://github.com/huggingface/transformers/blob/v4.30.0/src/
    transformers/models/t5/modeling_t5.py#L435

    Modifies attention as Q*K^T + B, where B is a learned scalar bias based
    on relative position of the query and key. It is HxNxN, where H is the
    number of heads, N is the sequence length.

    I&#39;ve made these modifications to the original T5 bias:
    - Skipping of the bucketing step. Original T5 bias converted rel
      position distances into logarithmically increasing buckets. This is
      supposed to help with length generalization.
    - I just directly use rel position index as bias values, as we don&#39;t
      need length generalization (40s max is good enough for ASR encoder),
      and it keeps ONNX export simple.
    - I&#39;ve also extended it so that biases can be asymmetric, the default
      implementation treats L-&gt;R and R-&gt;L the same. Asymmetric was found to
      yield better results in my experiments.

    Args:
        num_heads: int
            Number of attention heads
        num_buckets: int
            Number of buckets to use for relative attention bias. This is the
            size of the learnable bias parameter. Bucketing is not yet
            supported, so this defaults to -1 which means no bucketing is
            used (max_distance determines size of bias param).
        max_distance: int
            Maximum distance to use for relative attention bias. With
            num_buckets=-1, this directly controls the max size of the bias
            parameter. When num_buckets &gt; 0 is supported, this will control
            the maximum distance for logarithmic bucketing after which all
            positions are in the same bucket.
        symmetric: bool
            Whether to use symmetric or asymmetric biases. symmetric=False uses
            2x number of bias params to distinguish L-&gt;R from R-&gt;L. This was
            found to be better for the encoder.
    &#34;&#34;&#34;

    def __init__(self, num_heads, num_buckets=-1, max_distance=1000, symmetric=False):
        super().__init__()
        self.num_heads = num_heads
        self.num_buckets = num_buckets
        self.max_distance = max_distance
        self.symmetric = symmetric
        self._skip_bucketing = self.num_buckets &lt; 0
        if self._skip_bucketing:
            self.num_buckets = max_distance
        else:
            raise NotImplementedError(
                &#34;T5 attention bias with bucketed positions is not yet tested&#34;
            )
        if not self.symmetric:
            self.num_buckets *= 2
        self.bias_values = nn.Embedding(self.num_buckets, self.num_heads)

    def forward(self, x):
        # instantiate bias compatible with shape of x
        maxpos = x.size(1)
        context_position = torch.arange(maxpos, device=x.device, dtype=torch.long)[
            :, None
        ]
        memory_position = torch.arange(maxpos, device=x.device, dtype=torch.long)[
            None, :
        ]
        relative_position = memory_position - context_position
        # clipping to a maximum distance using ops that play well with ONNX
        # export
        relative_position = relative_position.masked_fill(
            relative_position &lt; -self.max_distance, -self.max_distance
        )
        relative_position = relative_position.masked_fill(
            relative_position &gt; self.max_distance - 1, self.max_distance - 1
        )

        # mapping from relative position to index in the bias parameter
        if self._skip_bucketing:
            bias_idx = relative_position
        else:
            bias_idx = self._bucket_relative_position(relative_position)
        if self.symmetric:
            bias_idx = bias_idx.abs()
        else:
            bias_idx += self.num_buckets // 2

        t5_rel_att_bias = self.bias_values(bias_idx)  # [L, L, H]
        t5_rel_att_bias = t5_rel_att_bias.permute(2, 0, 1).unsqueeze(0)  # [1, H, L, L]

        return t5_rel_att_bias

    def _bucket_relative_position(self, relative_position):
        # this is a placeholder (isn&#39;t tested, likely buggy) using HuggingFace
        # implem as a reference this also needs to be extended to support
        # asymmetric +/- ve positions
        relative_buckets = 0
        if not self.causal:
            self.num_buckets //= 2
            relative_buckets += (relative_position &gt; 0).to(
                torch.long
            ) * self.num_buckets
            relative_position = torch.abs(relative_position)
        else:
            relative_position = -torch.min(
                relative_position, torch.zeros_like(relative_position)
            )
        # now relative_position is in the range [0, inf)

        # half of the buckets are for exact increments in positions
        max_exact = self.num_buckets // 2
        is_small = relative_position &lt; max_exact

        # The other half of the buckets are for logarithmically bigger bins in
        # positions up to max_distance
        relative_position_if_large = max_exact + (
            torch.log(relative_position.float() / max_exact)
            / math.log(self.max_distance / max_exact)
            * (self.num_buckets - max_exact)
        ).to(torch.long)
        relative_position_if_large = torch.min(
            relative_position_if_large,
            torch.full_like(relative_position_if_large, self.num_buckets - 1),
        )

        relative_buckets += torch.where(
            is_small, relative_position, relative_position_if_large
        )
        return relative_buckets</code></pre>
</details>
<div class="desc"><p>This module implements the relative position bias described in Section
2.1 of the T5 paper: <a href="https://arxiv.org/pdf/1910.10683.pdf">https://arxiv.org/pdf/1910.10683.pdf</a></p>
<p>The Huggingface implementation is used as a reference
<a href="https://github.com/huggingface/transformers/blob/v4.30.0/src/">https://github.com/huggingface/transformers/blob/v4.30.0/src/</a>
transformers/models/t5/modeling_t5.py#L435</p>
<p>Modifies attention as Q*K^T + B, where B is a learned scalar bias based
on relative position of the query and key. It is HxNxN, where H is the
number of heads, N is the sequence length.</p>
<p>I've made these modifications to the original T5 bias:
- Skipping of the bucketing step. Original T5 bias converted rel
position distances into logarithmically increasing buckets. This is
supposed to help with length generalization.
- I just directly use rel position index as bias values, as we don't
need length generalization (40s max is good enough for ASR encoder),
and it keeps ONNX export simple.
- I've also extended it so that biases can be asymmetric, the default
implementation treats L-&gt;R and R-&gt;L the same. Asymmetric was found to
yield better results in my experiments.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_heads</code></strong></dt>
<dd>int
Number of attention heads</dd>
<dt><strong><code>num_buckets</code></strong></dt>
<dd>int
Number of buckets to use for relative attention bias. This is the
size of the learnable bias parameter. Bucketing is not yet
supported, so this defaults to -1 which means no bucketing is
used (max_distance determines size of bias param).</dd>
<dt><strong><code>max_distance</code></strong></dt>
<dd>int
Maximum distance to use for relative attention bias. With
num_buckets=-1, this directly controls the max size of the bias
parameter. When num_buckets &gt; 0 is supported, this will control
the maximum distance for logarithmic bucketing after which all
positions are in the same bucket.</dd>
<dt><strong><code>symmetric</code></strong></dt>
<dd>bool
Whether to use symmetric or asymmetric biases. symmetric=False uses
2x number of bias params to distinguish L-&gt;R from R-&gt;L. This was
found to be better for the encoder.</dd>
</dl>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_utils.T5RelativeAttentionLogitBias.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    # instantiate bias compatible with shape of x
    maxpos = x.size(1)
    context_position = torch.arange(maxpos, device=x.device, dtype=torch.long)[
        :, None
    ]
    memory_position = torch.arange(maxpos, device=x.device, dtype=torch.long)[
        None, :
    ]
    relative_position = memory_position - context_position
    # clipping to a maximum distance using ops that play well with ONNX
    # export
    relative_position = relative_position.masked_fill(
        relative_position &lt; -self.max_distance, -self.max_distance
    )
    relative_position = relative_position.masked_fill(
        relative_position &gt; self.max_distance - 1, self.max_distance - 1
    )

    # mapping from relative position to index in the bias parameter
    if self._skip_bucketing:
        bias_idx = relative_position
    else:
        bias_idx = self._bucket_relative_position(relative_position)
    if self.symmetric:
        bias_idx = bias_idx.abs()
    else:
        bias_idx += self.num_buckets // 2

    t5_rel_att_bias = self.bias_values(bias_idx)  # [L, L, H]
    t5_rel_att_bias = t5_rel_att_bias.permute(2, 0, 1).unsqueeze(0)  # [1, H, L, L]

    return t5_rel_att_bias</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.models" href="index.html">sglang.srt.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="sglang.srt.models.phi4mm_utils.adaptive_enc_mask" href="#sglang.srt.models.phi4mm_utils.adaptive_enc_mask">adaptive_enc_mask</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.calc_length" href="#sglang.srt.models.phi4mm_utils.calc_length">calc_length</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.get_activation" href="#sglang.srt.models.phi4mm_utils.get_activation">get_activation</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.get_offset" href="#sglang.srt.models.phi4mm_utils.get_offset">get_offset</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.masked_softmax" href="#sglang.srt.models.phi4mm_utils.masked_softmax">masked_softmax</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.unfold_tensor" href="#sglang.srt.models.phi4mm_utils.unfold_tensor">unfold_tensor</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_utils.AbsolutePositionalEncoding" href="#sglang.srt.models.phi4mm_utils.AbsolutePositionalEncoding">AbsolutePositionalEncoding</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_utils.AbsolutePositionalEncoding.extend_pe" href="#sglang.srt.models.phi4mm_utils.AbsolutePositionalEncoding.extend_pe">extend_pe</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.AbsolutePositionalEncoding.forward" href="#sglang.srt.models.phi4mm_utils.AbsolutePositionalEncoding.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_utils.AttBlock" href="#sglang.srt.models.phi4mm_utils.AttBlock">AttBlock</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_utils.AttBlock.memory_dims" href="#sglang.srt.models.phi4mm_utils.AttBlock.memory_dims">memory_dims</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_utils.AttModule" href="#sglang.srt.models.phi4mm_utils.AttModule">AttModule</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_utils.AttModule.forward" href="#sglang.srt.models.phi4mm_utils.AttModule.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.AttModule.set_export" href="#sglang.srt.models.phi4mm_utils.AttModule.set_export">set_export</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_utils.BlockBase" href="#sglang.srt.models.phi4mm_utils.BlockBase">BlockBase</a></code></h4>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_utils.CausalConv1D" href="#sglang.srt.models.phi4mm_utils.CausalConv1D">CausalConv1D</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_utils.CausalConv1D.forward" href="#sglang.srt.models.phi4mm_utils.CausalConv1D.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.CausalConv1D.update_cache" href="#sglang.srt.models.phi4mm_utils.CausalConv1D.update_cache">update_cache</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_utils.CausalConv2D" href="#sglang.srt.models.phi4mm_utils.CausalConv2D">CausalConv2D</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_utils.CausalConv2D.forward" href="#sglang.srt.models.phi4mm_utils.CausalConv2D.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_utils.ConvModule" href="#sglang.srt.models.phi4mm_utils.ConvModule">ConvModule</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_utils.ConvModule.forward" href="#sglang.srt.models.phi4mm_utils.ConvModule.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_utils.DepthWiseSeperableConv1d" href="#sglang.srt.models.phi4mm_utils.DepthWiseSeperableConv1d">DepthWiseSeperableConv1d</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_utils.DepthWiseSeperableConv1d.forward" href="#sglang.srt.models.phi4mm_utils.DepthWiseSeperableConv1d.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_utils.FeedForward" href="#sglang.srt.models.phi4mm_utils.FeedForward">FeedForward</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_utils.FeedForward.forward" href="#sglang.srt.models.phi4mm_utils.FeedForward.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_utils.GLU" href="#sglang.srt.models.phi4mm_utils.GLU">GLU</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_utils.GLU.forward" href="#sglang.srt.models.phi4mm_utils.GLU.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_utils.GLULinear" href="#sglang.srt.models.phi4mm_utils.GLULinear">GLULinear</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_utils.GLULinear.forward" href="#sglang.srt.models.phi4mm_utils.GLULinear.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_utils.GLUPointWiseConv" href="#sglang.srt.models.phi4mm_utils.GLUPointWiseConv">GLUPointWiseConv</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_utils.GLUPointWiseConv.forward" href="#sglang.srt.models.phi4mm_utils.GLUPointWiseConv.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_utils.MeanVarianceNormLayer" href="#sglang.srt.models.phi4mm_utils.MeanVarianceNormLayer">MeanVarianceNormLayer</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_utils.MeanVarianceNormLayer.forward" href="#sglang.srt.models.phi4mm_utils.MeanVarianceNormLayer.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_utils.MultiHeadedAttention" href="#sglang.srt.models.phi4mm_utils.MultiHeadedAttention">MultiHeadedAttention</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_utils.MultiHeadedAttention.forward" href="#sglang.srt.models.phi4mm_utils.MultiHeadedAttention.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.MultiHeadedAttention.g" href="#sglang.srt.models.phi4mm_utils.MultiHeadedAttention.g">g</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.MultiHeadedAttention.h" href="#sglang.srt.models.phi4mm_utils.MultiHeadedAttention.h">h</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.MultiHeadedAttention.h_k" href="#sglang.srt.models.phi4mm_utils.MultiHeadedAttention.h_k">h_k</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.MultiHeadedAttention.inv_sqrt_d_k" href="#sglang.srt.models.phi4mm_utils.MultiHeadedAttention.inv_sqrt_d_k">inv_sqrt_d_k</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_utils.MultiSequential" href="#sglang.srt.models.phi4mm_utils.MultiSequential">MultiSequential</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_utils.MultiSequential.forward" href="#sglang.srt.models.phi4mm_utils.MultiSequential.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_utils.NemoConvSubsampling" href="#sglang.srt.models.phi4mm_utils.NemoConvSubsampling">NemoConvSubsampling</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_utils.NemoConvSubsampling.change_subsampling_conv_chunking_factor" href="#sglang.srt.models.phi4mm_utils.NemoConvSubsampling.change_subsampling_conv_chunking_factor">change_subsampling_conv_chunking_factor</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.NemoConvSubsampling.channel_chunked_conv" href="#sglang.srt.models.phi4mm_utils.NemoConvSubsampling.channel_chunked_conv">channel_chunked_conv</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.NemoConvSubsampling.conv_split_by_batch" href="#sglang.srt.models.phi4mm_utils.NemoConvSubsampling.conv_split_by_batch">conv_split_by_batch</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.NemoConvSubsampling.conv_split_by_channel" href="#sglang.srt.models.phi4mm_utils.NemoConvSubsampling.conv_split_by_channel">conv_split_by_channel</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.NemoConvSubsampling.forward" href="#sglang.srt.models.phi4mm_utils.NemoConvSubsampling.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.NemoConvSubsampling.get_sampling_frames" href="#sglang.srt.models.phi4mm_utils.NemoConvSubsampling.get_sampling_frames">get_sampling_frames</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.NemoConvSubsampling.get_streaming_cache_size" href="#sglang.srt.models.phi4mm_utils.NemoConvSubsampling.get_streaming_cache_size">get_streaming_cache_size</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_utils.NemoConvSubsampling.reset_parameters" href="#sglang.srt.models.phi4mm_utils.NemoConvSubsampling.reset_parameters">reset_parameters</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_utils.Swish" href="#sglang.srt.models.phi4mm_utils.Swish">Swish</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_utils.Swish.forward" href="#sglang.srt.models.phi4mm_utils.Swish.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_utils.T5RelativeAttentionLogitBias" href="#sglang.srt.models.phi4mm_utils.T5RelativeAttentionLogitBias">T5RelativeAttentionLogitBias</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_utils.T5RelativeAttentionLogitBias.forward" href="#sglang.srt.models.phi4mm_utils.T5RelativeAttentionLogitBias.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
