<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.lora.lora_manager API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.lora.lora_manager</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.lora.lora_manager.LoRAManager"><code class="flex name class">
<span>class <span class="ident">LoRAManager</span></span>
<span>(</span><span>base_model: torch.nn.modules.module.Module,<br>base_hf_config: transformers.models.auto.configuration_auto.AutoConfig,<br>max_loras_per_batch: int,<br>load_config: <a title="sglang.srt.configs.load_config.LoadConfig" href="../configs/load_config.html#sglang.srt.configs.load_config.LoadConfig">LoadConfig</a>,<br>dtype: torch.dtype,<br>lora_backend: str = 'triton',<br>tp_size: int = 1,<br>tp_rank: int = 0,<br>max_lora_rank: int | None = None,<br>target_modules: Iterable[str] | None = None,<br>lora_paths: List[<a title="sglang.srt.lora.lora_registry.LoRARef" href="lora_registry.html#sglang.srt.lora.lora_registry.LoRARef">LoRARef</a>] | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LoRAManager:
    def __init__(
        self,
        base_model: torch.nn.Module,
        base_hf_config: AutoConfig,
        max_loras_per_batch: int,
        load_config: LoadConfig,
        dtype: torch.dtype,
        lora_backend: str = &#34;triton&#34;,
        tp_size: int = 1,
        tp_rank: int = 0,
        max_lora_rank: Optional[int] = None,
        target_modules: Optional[Iterable[str]] = None,
        lora_paths: Optional[List[LoRARef]] = None,
    ):
        self.base_model: torch.nn.Module = base_model
        self.base_hf_config: AutoConfig = base_hf_config
        self.max_loras_per_batch: int = max_loras_per_batch
        self.load_config: LoadConfig = load_config
        self.dtype: torch.dtype = dtype
        self.device: torch.device = next(self.base_model.parameters()).device
        self.tp_size: int = tp_size
        self.tp_rank: int = tp_rank

        # LoRA backend for running sgemm kernels
        logger.info(f&#34;Using {lora_backend} as backend of LoRA kernels.&#34;)
        backend_type = get_backend_from_name(lora_backend)
        self.lora_backend: BaseLoRABackend = backend_type(lora_backend)

        # Initialize mutable internal state of the LoRAManager.
        self.init_state(
            max_lora_rank=max_lora_rank,
            target_modules=target_modules,
            lora_paths=lora_paths,
        )

    def init_cuda_graph_batch_info(self, max_bs_in_cuda_graph: int):
        self.max_bs_in_cuda_graph = max_bs_in_cuda_graph
        with torch.device(&#34;cuda&#34;):
            self.cuda_graph_batch_info = LoRABatchInfo(
                bs=self.max_bs_in_cuda_graph,
                seg_lens=torch.zeros(self.max_bs_in_cuda_graph, dtype=torch.int32),
                seg_indptr=torch.zeros(
                    self.max_bs_in_cuda_graph + 1, dtype=torch.int32
                ),
                max_len=1,
                weight_indices=torch.zeros(
                    self.max_bs_in_cuda_graph, dtype=torch.int32
                ),
                lora_ranks=torch.zeros(self.max_loras_per_batch, dtype=torch.int32),
                scalings=torch.zeros(self.max_loras_per_batch, dtype=torch.float),
            )

            # Initialize seg_lens and seg_indptr for CUDA graph as they remain constant
            # across batches.
            self.cuda_graph_batch_info.seg_lens[: self.max_bs_in_cuda_graph].fill_(1)
            torch.cumsum(
                self.cuda_graph_batch_info.seg_lens[: self.max_bs_in_cuda_graph],
                dim=0,
                out=self.cuda_graph_batch_info.seg_indptr[
                    1 : self.max_bs_in_cuda_graph + 1
                ],
            )

    def create_lora_update_result(
        self, success: bool, error_message: str = &#34;&#34;
    ) -&gt; LoRAUpdateResult:
        return LoRAUpdateResult(
            success=success,
            error_message=error_message,
            loaded_adapters={
                lora_ref.lora_name: lora_ref.lora_path
                for lora_ref in self.lora_refs.values()
            },
        )

    def load_lora_adapter(self, lora_ref: LoRARef) -&gt; LoRAUpdateResult:
        &#34;&#34;&#34;
        Load a single LoRA adapter from the specified path.

        Args:
            lora_ref (LoRARef): The LoRARef object containing the LoRA name, path, and ID.
        &#34;&#34;&#34;
        assert (
            lora_ref.lora_name is not None and lora_ref.lora_path is not None
        ), &#34;LoRARef must have both lora_name and lora_path set for loading.&#34;
        assert (
            lora_ref.lora_id not in self.loras
        ), f&#34;LoRA adapter with ID {lora_ref.lora_id} is already loaded. This should have been verified before request is sent to the backend.&#34;

        try:
            # load configs
            new_adapter = LoRAConfig(lora_ref.lora_path)
            self.validate_new_adapter(new_adapter, lora_ref)
            self.configs[lora_ref.lora_id] = new_adapter

            # load weights
            self.load_lora_weights(lora_ref)

            # keep metadata for displayed messages
            self.lora_refs[lora_ref.lora_id] = lora_ref
            self.num_pinned_loras += int(lora_ref.pinned)
        except Exception as e:
            return self.create_lora_update_result(
                success=False,
                error_message=str(e),
            )

        return self.create_lora_update_result(success=True)

    def validate_new_adapter(self, lora_config: LoRAConfig, lora_ref: LoRARef):
        &#34;&#34;&#34;
        Validate if an adapter can be loaded into the current LoRA memory pool and generate error if it is incompatible.
        &#34;&#34;&#34;

        # Check if the LoRA adapter shape is compatible with the current LoRA memory pool configuration.
        memory_pool = getattr(self, &#34;memory_pool&#34;, None)
        incompatible = memory_pool and not memory_pool.can_support(lora_config)
        if incompatible:
            raise ValueError(
                f&#34;LoRA adapter {lora_ref.lora_name} with rank {lora_config.r} is incompatible with the current &#34;
                &#34;LoRA memory pool configuration. Please ensure that the LoRA adapter&#39;s rank is within the configured &#34;
                &#34;`--max-lora-rank` and that the target modules are included in `--lora-target-modules`.&#34;
            )

        # Ensure pinned LoRA adapters does not exceed maximal limit or cause starvation.
        if lora_ref.pinned and self.num_pinned_loras &gt;= self.max_loras_per_batch - 1:
            raise ValueError(
                f&#34;Failed to load LoRA adapter {lora_ref.lora_name} as a pinned adapter. It is not allowed to pin all slots &#34;
                &#34;in the LoRA memory pool to avoid starvation for unpinned adapters and base models. Please increase your &#34;
                &#34;`--max-loras-per-batch` or load it as unpinned LoRA adapters.&#34;
            )

    def unload_lora_adapter(self, lora_ref: LoRARef) -&gt; LoRAUpdateResult:
        &#34;&#34;&#34;
        Unload LoRA adapters by their names. This will remove the adapters from the memory pool and
        delete the corresponding LoRA modules.
        &#34;&#34;&#34;

        adapter = self.configs.get(lora_ref.lora_id)
        lora_ref = self.lora_refs.get(lora_ref.lora_id)
        assert (
            adapter is not None and lora_ref is not None
        ), f&#34;LoRA adapter with ID {lora_ref.lora_id} is not loaded. This should have been verified before request is sent to the backend.&#34;

        try:
            del self.configs[lora_ref.lora_id]
            del self.loras[lora_ref.lora_id]
            del self.lora_refs[lora_ref.lora_id]
            self.num_pinned_loras -= int(lora_ref.pinned)
        except Exception as e:
            return self.create_lora_update_result(
                success=False,
                error_message=str(e),
            )

        return self.create_lora_update_result(success=True)

    def validate_lora_batch(self, lora_ids: set[str]) -&gt; bool:
        &#34;&#34;&#34;
        Validate if the LoRA IDs in the batch can be loaded into the current LoRA memory pool.
        &#34;&#34;&#34;
        if len(lora_ids) &gt; self.max_loras_per_batch:
            return False

        # skip pinned LoRA check if no pinned LoRA adapters are loaded.
        if self.num_pinned_loras == 0:
            return True

        # counting the number of pinned LoRA adapters in the batch.
        pinned_loras_in_batch = 0
        for lora_id in lora_ids:
            if lora_id is not None:
                lora_ref = self.lora_refs.get(lora_id)
                assert (
                    lora_ref is not None
                ), f&#34;LoRA ID {lora_id} not found in lora_refs.&#34;
                pinned_loras_in_batch += int(lora_ref.pinned)

        assert pinned_loras_in_batch &lt;= self.num_pinned_loras, (
            f&#34;Number of pinned LoRA adapters in the batch ({pinned_loras_in_batch}) exceeds the total number of pinned adapters &#34;
            f&#34;({self.num_pinned_loras}). This indicates a bug in the LoRA loading logic.&#34;
        )

        required_slots = len(lora_ids) - pinned_loras_in_batch
        mem_pool_vacancy = self.memory_pool.max_loras_per_batch - self.num_pinned_loras

        return required_slots &lt;= mem_pool_vacancy

    def prepare_lora_batch(self, forward_batch: ForwardBatch):

        # Load active loras into lora memory pool
        cur_uids = set(forward_batch.lora_ids)

        assert len(cur_uids) &lt;= self.max_loras_per_batch
        self.memory_pool.prepare_lora_batch(
            cur_uids=cur_uids,
            lora_adapters=self.loras,
            lora_modules=self.lora_modules,
            lora_refs=self.lora_refs.copy(),  # copy snapshot of current lora_refs to avoid mutation during the batch preparation.
        )

        # set up batch info shared by all lora modules
        bs = forward_batch.batch_size

        def transfer_adapter_info(
            weight_indices_out: torch.Tensor,
            lora_ranks_out: torch.Tensor,
            scalings_out: torch.Tensor,
        ):
            &#34;&#34;&#34;
            Transfer adapter metadata (weight indices, LoRA rank, scalings) from host
            to device (CUDA) asynchronously.
            &#34;&#34;&#34;
            weight_indices = [0] * len(forward_batch.lora_ids)
            lora_ranks = [0] * self.max_loras_per_batch
            scalings = [0] * self.max_loras_per_batch
            for i, uid in enumerate(forward_batch.lora_ids):
                weight_indices[i] = self.memory_pool.get_buffer_id(uid)
                if uid is not None:
                    lora = self.loras[uid]
                    lora_ranks[weight_indices[i]] = lora.config.r
                    scalings[weight_indices[i]] = lora.scaling

            # Use pinned memory to avoid synchronizations during host-to-device transfer
            weight_indices_tensor = torch.tensor(
                weight_indices, dtype=torch.int32, pin_memory=True, device=&#34;cpu&#34;
            )
            lora_ranks_tensor = torch.tensor(
                lora_ranks, dtype=torch.int32, pin_memory=True, device=&#34;cpu&#34;
            )
            scalings_tensor = torch.tensor(
                scalings, dtype=torch.float, pin_memory=True, device=&#34;cpu&#34;
            )

            # Copy to device tensors asynchronously
            weight_indices_out[:bs].copy_(weight_indices_tensor, non_blocking=True)
            lora_ranks_out[: self.max_loras_per_batch].copy_(
                lora_ranks_tensor, non_blocking=True
            )
            scalings_out[: self.max_loras_per_batch].copy_(
                scalings_tensor, non_blocking=True
            )

        if (
            hasattr(self, &#34;max_bs_in_cuda_graph&#34;)
            and bs &lt;= self.max_bs_in_cuda_graph
            and forward_batch.forward_mode.is_cuda_graph()
        ):
            # Do in-place updates when CUDA graph is enabled and the batch forward mode
            # could use CUDA graph.

            transfer_adapter_info(
                self.cuda_graph_batch_info.weight_indices,
                self.cuda_graph_batch_info.lora_ranks,
                self.cuda_graph_batch_info.scalings,
            )

            self.cuda_graph_batch_info.bs = bs
            self.cuda_graph_batch_info.max_len = 1
            batch_info = self.cuda_graph_batch_info
        else:
            weight_indices = torch.empty((bs,), dtype=torch.int32, device=self.device)
            lora_ranks = torch.zeros(
                (self.max_loras_per_batch,), dtype=torch.int64, device=self.device
            )
            scalings = torch.zeros(
                (self.max_loras_per_batch,), dtype=torch.float, device=self.device
            )
            transfer_adapter_info(
                weight_indices,
                lora_ranks,
                scalings,
            )

            seg_lens = (
                forward_batch.extend_seq_lens
                if forward_batch.forward_mode.is_extend()
                else torch.ones(bs, device=self.device)
            )

            max_len = (
                # Calculate max_len from the CPU copy to avoid D2H transfer.
                max(forward_batch.extend_seq_lens_cpu)
                if forward_batch.forward_mode.is_extend()
                else 1
            )

            seg_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device=self.device)
            seg_indptr[1:] = torch.cumsum(seg_lens, dim=0)

            batch_info = LoRABatchInfo(
                bs=bs,
                seg_lens=seg_lens,
                seg_indptr=seg_indptr,
                max_len=max_len,
                weight_indices=weight_indices,
                lora_ranks=lora_ranks,
                scalings=scalings,
            )
        self.lora_backend.set_batch_info(batch_info)

    def update_lora_info(self):
        &#34;&#34;&#34;
        Update all LoRA modules to associate them with the latest memory buffer.
        &#34;&#34;&#34;
        for layer_id, layer_modules in enumerate(self.lora_modules):
            for module_name, module in layer_modules.items():
                target_module = get_target_module_name(
                    module_name, self.memory_pool.target_modules
                )
                module.set_lora_info(
                    self.memory_pool.get_tensor(
                        target_module=target_module,
                        layer_id=layer_id,
                        lora_type=LoRAType.LORA_A,
                    ),
                    self.memory_pool.get_tensor(
                        target_module=target_module,
                        layer_id=layer_id,
                        lora_type=LoRAType.LORA_B,
                    ),
                )

    def init_state(
        self,
        max_lora_rank: Optional[int] = None,
        target_modules: Optional[Iterable[str]] = None,
        lora_paths: Optional[List[LoRARef]] = None,
    ):
        &#34;&#34;&#34;
        Initialize the internal (mutable) state of the LoRAManager.

        When `lora_paths` is provided and not empty, it might be used for inferring LoRA shape info such as
        the target modules and max_lora_rank.
        &#34;&#34;&#34;

        assert lora_paths or (
            max_lora_rank is not None and target_modules is not None
        ), &#34;When no initial --lora-paths is provided, you need to specify both --max-lora-rank and --lora-target-modules for LoRA initialization.&#34;

        self.init_lora_adapters(lora_paths)
        self.init_lora_shapes(
            max_lora_rank=max_lora_rank,
            target_modules=target_modules,
        )
        self.init_lora_modules()
        self.init_memory_pool()
        self.update_lora_info()

    def init_lora_adapters(self, lora_paths: Optional[List[LoRARef]] = None):
        # Configs of all active LoRA adapters, indexed by LoRA ID.
        self.configs: Dict[str, LoRAConfig] = {}

        # LoRA adapter weights cached in CPU memory, indexed by LoRA ID.
        self.loras: Dict[str, LoRAAdapter] = {}

        # Mapping from LoRA ID to LoRARef object.
        self.lora_refs: Dict[str, LoRARef] = {}

        # Count of pinned LoRA adapters.
        self.num_pinned_loras: int = 0

        if lora_paths:
            for lora_ref in lora_paths:
                result = self.load_lora_adapter(lora_ref)
                if not result.success:
                    raise RuntimeError(
                        f&#34;Failed to load LoRA adapter {lora_ref.lora_name}: {result.error_message}&#34;
                    )

    def init_lora_shapes(
        self,
        max_lora_rank: Optional[int] = None,
        target_modules: Optional[Iterable[str]] = None,
    ):
        &#34;&#34;&#34;Infer LoRA target modules and max_lora_rank from loaded adapters if not provided.&#34;&#34;&#34;

        self.target_modules = (
            get_normalized_target_modules(target_modules) if target_modules else set()
        )

        for lora_id, config in self.configs.items():
            if not isinstance(config.target_modules, list):
                raise ValueError(
                    f&#34;SGLang currently only supports inferring LoRA target modules when a list of &#34;
                    &#34;suffixes is provided in `target_modules` field of PEFT config. Please explicitly &#34;
                    &#34;specify `--lora-target-modules` during server startup. You can specify `all` to &#34;
                    &#34;enable all support modules types. &#34;
                )

            adapter_target_modules = get_normalized_target_modules(
                config.target_modules
            )

            if target_modules is not None:
                # When `--lora-target-modules` is provided, validate adapter target modules is a subset of the specified target modules.
                if not adapter_target_modules.issubset(self.target_modules):
                    unsupported_modules = adapter_target_modules - self.target_modules
                    lora_name = self.lora_refs[lora_id].lora_name
                    raise ValueError(
                        f&#34;LoRA adapter &#39;{lora_name}&#39; contains target modules {sorted(unsupported_modules)} &#34;
                        f&#34;that are not included in the specified --lora-target-modules {sorted(self.target_modules)}. &#34;
                        f&#34;Please update --lora-target-modules to include all required modules: &#34;
                        f&#34;{sorted(self.target_modules | adapter_target_modules)}, or use &#39;all&#39; to enable all supported modules.&#34;
                    )
            else:
                # Otherwise, infer target_modules from adapter configs.
                self.target_modules.update(adapter_target_modules)

        if max_lora_rank is not None:
            self.max_lora_rank = max_lora_rank
        else:
            self.max_lora_rank = max(
                [x.r for x in self.configs.values()],
                default=0,
            )

    def load_lora_weights(self, lora_ref: LoRARef):
        &#34;&#34;&#34;
        Load the weights of a LoRA adapter to CPU memory and conducts post-loading validation.
        &#34;&#34;&#34;
        lora_adapter = LoRAAdapter(
            lora_ref.lora_id,
            self.configs[lora_ref.lora_id],
            self.base_hf_config,
            self.load_config,
            self.lora_backend,
        )
        lora_adapter.initialize_weights()
        self.loras[lora_ref.lora_id] = lora_adapter

    def init_memory_pool(self):
        &#34;&#34;&#34;(Re)initialize the LoRA memory pool based on the current configurations.&#34;&#34;&#34;
        self.memory_pool = LoRAMemoryPool(
            base_hf_config=self.base_hf_config,
            max_loras_per_batch=self.max_loras_per_batch,
            dtype=self.dtype,
            tp_size=self.tp_size,
            tp_rank=self.tp_rank,
            max_lora_rank=self.max_lora_rank,
            target_modules=self.target_modules,
            base_model=self.base_model,
        )

    def set_lora_module(self, module_name, module):
        lora_module = get_lora_layer(module, self.lora_backend)
        replace_submodule(self.base_model, module_name, lora_module)
        return lora_module

    def init_lora_modules(self):
        # Look-up table that essentially maps (layer_index, module_name) to the corresponding LoRA module.
        self.lora_modules: List[Dict[str, BaseLayerWithLoRA]] = [
            {} for _ in range(self.base_hf_config.num_hidden_layers)
        ]

        for module_name, module in self.base_model.named_modules():
            # TODO (lifuhuang): in the future, we should consider generalizing the
            # should_apply_lora function to support mapping by full module name instead
            # of just the last part (e.g., &#34;qkv_proj&#34;) to support scenarios with multiple
            # attention stacks (e.g., multimodal models).
            # See: https://github.com/sgl-project/sglang/issues/6608
            if getattr(
                self.base_model, &#34;should_apply_lora&#34;, None
            ) and not self.base_model.should_apply_lora(module_name):
                continue

            # The module should be converted if it is included in target_names
            if module_name.split(&#34;.&#34;)[-1] in self.target_modules:
                layer_id = get_layer_id(module_name)
                self.lora_modules[layer_id][module_name] = self.set_lora_module(
                    module_name, module
                )</code></pre>
</details>
<div class="desc"></div>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.lora.lora_manager.LoRAManager.create_lora_update_result"><code class="name flex">
<span>def <span class="ident">create_lora_update_result</span></span>(<span>self, success: bool, error_message: str = '') ‑> <a title="sglang.srt.managers.io_struct.LoRAUpdateResult" href="../managers/io_struct.html#sglang.srt.managers.io_struct.LoRAUpdateResult">LoRAUpdateResult</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_lora_update_result(
    self, success: bool, error_message: str = &#34;&#34;
) -&gt; LoRAUpdateResult:
    return LoRAUpdateResult(
        success=success,
        error_message=error_message,
        loaded_adapters={
            lora_ref.lora_name: lora_ref.lora_path
            for lora_ref in self.lora_refs.values()
        },
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.lora_manager.LoRAManager.init_cuda_graph_batch_info"><code class="name flex">
<span>def <span class="ident">init_cuda_graph_batch_info</span></span>(<span>self, max_bs_in_cuda_graph: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_cuda_graph_batch_info(self, max_bs_in_cuda_graph: int):
    self.max_bs_in_cuda_graph = max_bs_in_cuda_graph
    with torch.device(&#34;cuda&#34;):
        self.cuda_graph_batch_info = LoRABatchInfo(
            bs=self.max_bs_in_cuda_graph,
            seg_lens=torch.zeros(self.max_bs_in_cuda_graph, dtype=torch.int32),
            seg_indptr=torch.zeros(
                self.max_bs_in_cuda_graph + 1, dtype=torch.int32
            ),
            max_len=1,
            weight_indices=torch.zeros(
                self.max_bs_in_cuda_graph, dtype=torch.int32
            ),
            lora_ranks=torch.zeros(self.max_loras_per_batch, dtype=torch.int32),
            scalings=torch.zeros(self.max_loras_per_batch, dtype=torch.float),
        )

        # Initialize seg_lens and seg_indptr for CUDA graph as they remain constant
        # across batches.
        self.cuda_graph_batch_info.seg_lens[: self.max_bs_in_cuda_graph].fill_(1)
        torch.cumsum(
            self.cuda_graph_batch_info.seg_lens[: self.max_bs_in_cuda_graph],
            dim=0,
            out=self.cuda_graph_batch_info.seg_indptr[
                1 : self.max_bs_in_cuda_graph + 1
            ],
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.lora_manager.LoRAManager.init_lora_adapters"><code class="name flex">
<span>def <span class="ident">init_lora_adapters</span></span>(<span>self,<br>lora_paths: List[<a title="sglang.srt.lora.lora_registry.LoRARef" href="lora_registry.html#sglang.srt.lora.lora_registry.LoRARef">LoRARef</a>] | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_lora_adapters(self, lora_paths: Optional[List[LoRARef]] = None):
    # Configs of all active LoRA adapters, indexed by LoRA ID.
    self.configs: Dict[str, LoRAConfig] = {}

    # LoRA adapter weights cached in CPU memory, indexed by LoRA ID.
    self.loras: Dict[str, LoRAAdapter] = {}

    # Mapping from LoRA ID to LoRARef object.
    self.lora_refs: Dict[str, LoRARef] = {}

    # Count of pinned LoRA adapters.
    self.num_pinned_loras: int = 0

    if lora_paths:
        for lora_ref in lora_paths:
            result = self.load_lora_adapter(lora_ref)
            if not result.success:
                raise RuntimeError(
                    f&#34;Failed to load LoRA adapter {lora_ref.lora_name}: {result.error_message}&#34;
                )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.lora_manager.LoRAManager.init_lora_modules"><code class="name flex">
<span>def <span class="ident">init_lora_modules</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_lora_modules(self):
    # Look-up table that essentially maps (layer_index, module_name) to the corresponding LoRA module.
    self.lora_modules: List[Dict[str, BaseLayerWithLoRA]] = [
        {} for _ in range(self.base_hf_config.num_hidden_layers)
    ]

    for module_name, module in self.base_model.named_modules():
        # TODO (lifuhuang): in the future, we should consider generalizing the
        # should_apply_lora function to support mapping by full module name instead
        # of just the last part (e.g., &#34;qkv_proj&#34;) to support scenarios with multiple
        # attention stacks (e.g., multimodal models).
        # See: https://github.com/sgl-project/sglang/issues/6608
        if getattr(
            self.base_model, &#34;should_apply_lora&#34;, None
        ) and not self.base_model.should_apply_lora(module_name):
            continue

        # The module should be converted if it is included in target_names
        if module_name.split(&#34;.&#34;)[-1] in self.target_modules:
            layer_id = get_layer_id(module_name)
            self.lora_modules[layer_id][module_name] = self.set_lora_module(
                module_name, module
            )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.lora_manager.LoRAManager.init_lora_shapes"><code class="name flex">
<span>def <span class="ident">init_lora_shapes</span></span>(<span>self,<br>max_lora_rank: int | None = None,<br>target_modules: Iterable[str] | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_lora_shapes(
    self,
    max_lora_rank: Optional[int] = None,
    target_modules: Optional[Iterable[str]] = None,
):
    &#34;&#34;&#34;Infer LoRA target modules and max_lora_rank from loaded adapters if not provided.&#34;&#34;&#34;

    self.target_modules = (
        get_normalized_target_modules(target_modules) if target_modules else set()
    )

    for lora_id, config in self.configs.items():
        if not isinstance(config.target_modules, list):
            raise ValueError(
                f&#34;SGLang currently only supports inferring LoRA target modules when a list of &#34;
                &#34;suffixes is provided in `target_modules` field of PEFT config. Please explicitly &#34;
                &#34;specify `--lora-target-modules` during server startup. You can specify `all` to &#34;
                &#34;enable all support modules types. &#34;
            )

        adapter_target_modules = get_normalized_target_modules(
            config.target_modules
        )

        if target_modules is not None:
            # When `--lora-target-modules` is provided, validate adapter target modules is a subset of the specified target modules.
            if not adapter_target_modules.issubset(self.target_modules):
                unsupported_modules = adapter_target_modules - self.target_modules
                lora_name = self.lora_refs[lora_id].lora_name
                raise ValueError(
                    f&#34;LoRA adapter &#39;{lora_name}&#39; contains target modules {sorted(unsupported_modules)} &#34;
                    f&#34;that are not included in the specified --lora-target-modules {sorted(self.target_modules)}. &#34;
                    f&#34;Please update --lora-target-modules to include all required modules: &#34;
                    f&#34;{sorted(self.target_modules | adapter_target_modules)}, or use &#39;all&#39; to enable all supported modules.&#34;
                )
        else:
            # Otherwise, infer target_modules from adapter configs.
            self.target_modules.update(adapter_target_modules)

    if max_lora_rank is not None:
        self.max_lora_rank = max_lora_rank
    else:
        self.max_lora_rank = max(
            [x.r for x in self.configs.values()],
            default=0,
        )</code></pre>
</details>
<div class="desc"><p>Infer LoRA target modules and max_lora_rank from loaded adapters if not provided.</p></div>
</dd>
<dt id="sglang.srt.lora.lora_manager.LoRAManager.init_memory_pool"><code class="name flex">
<span>def <span class="ident">init_memory_pool</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_memory_pool(self):
    &#34;&#34;&#34;(Re)initialize the LoRA memory pool based on the current configurations.&#34;&#34;&#34;
    self.memory_pool = LoRAMemoryPool(
        base_hf_config=self.base_hf_config,
        max_loras_per_batch=self.max_loras_per_batch,
        dtype=self.dtype,
        tp_size=self.tp_size,
        tp_rank=self.tp_rank,
        max_lora_rank=self.max_lora_rank,
        target_modules=self.target_modules,
        base_model=self.base_model,
    )</code></pre>
</details>
<div class="desc"><p>(Re)initialize the LoRA memory pool based on the current configurations.</p></div>
</dd>
<dt id="sglang.srt.lora.lora_manager.LoRAManager.init_state"><code class="name flex">
<span>def <span class="ident">init_state</span></span>(<span>self,<br>max_lora_rank: int | None = None,<br>target_modules: Iterable[str] | None = None,<br>lora_paths: List[<a title="sglang.srt.lora.lora_registry.LoRARef" href="lora_registry.html#sglang.srt.lora.lora_registry.LoRARef">LoRARef</a>] | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_state(
    self,
    max_lora_rank: Optional[int] = None,
    target_modules: Optional[Iterable[str]] = None,
    lora_paths: Optional[List[LoRARef]] = None,
):
    &#34;&#34;&#34;
    Initialize the internal (mutable) state of the LoRAManager.

    When `lora_paths` is provided and not empty, it might be used for inferring LoRA shape info such as
    the target modules and max_lora_rank.
    &#34;&#34;&#34;

    assert lora_paths or (
        max_lora_rank is not None and target_modules is not None
    ), &#34;When no initial --lora-paths is provided, you need to specify both --max-lora-rank and --lora-target-modules for LoRA initialization.&#34;

    self.init_lora_adapters(lora_paths)
    self.init_lora_shapes(
        max_lora_rank=max_lora_rank,
        target_modules=target_modules,
    )
    self.init_lora_modules()
    self.init_memory_pool()
    self.update_lora_info()</code></pre>
</details>
<div class="desc"><p>Initialize the internal (mutable) state of the LoRAManager.</p>
<p>When <code>lora_paths</code> is provided and not empty, it might be used for inferring LoRA shape info such as
the target modules and max_lora_rank.</p></div>
</dd>
<dt id="sglang.srt.lora.lora_manager.LoRAManager.load_lora_adapter"><code class="name flex">
<span>def <span class="ident">load_lora_adapter</span></span>(<span>self,<br>lora_ref: <a title="sglang.srt.lora.lora_registry.LoRARef" href="lora_registry.html#sglang.srt.lora.lora_registry.LoRARef">LoRARef</a>) ‑> <a title="sglang.srt.managers.io_struct.LoRAUpdateResult" href="../managers/io_struct.html#sglang.srt.managers.io_struct.LoRAUpdateResult">LoRAUpdateResult</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_lora_adapter(self, lora_ref: LoRARef) -&gt; LoRAUpdateResult:
    &#34;&#34;&#34;
    Load a single LoRA adapter from the specified path.

    Args:
        lora_ref (LoRARef): The LoRARef object containing the LoRA name, path, and ID.
    &#34;&#34;&#34;
    assert (
        lora_ref.lora_name is not None and lora_ref.lora_path is not None
    ), &#34;LoRARef must have both lora_name and lora_path set for loading.&#34;
    assert (
        lora_ref.lora_id not in self.loras
    ), f&#34;LoRA adapter with ID {lora_ref.lora_id} is already loaded. This should have been verified before request is sent to the backend.&#34;

    try:
        # load configs
        new_adapter = LoRAConfig(lora_ref.lora_path)
        self.validate_new_adapter(new_adapter, lora_ref)
        self.configs[lora_ref.lora_id] = new_adapter

        # load weights
        self.load_lora_weights(lora_ref)

        # keep metadata for displayed messages
        self.lora_refs[lora_ref.lora_id] = lora_ref
        self.num_pinned_loras += int(lora_ref.pinned)
    except Exception as e:
        return self.create_lora_update_result(
            success=False,
            error_message=str(e),
        )

    return self.create_lora_update_result(success=True)</code></pre>
</details>
<div class="desc"><p>Load a single LoRA adapter from the specified path.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>lora_ref</code></strong> :&ensp;<code>LoRARef</code></dt>
<dd>The LoRARef object containing the LoRA name, path, and ID.</dd>
</dl></div>
</dd>
<dt id="sglang.srt.lora.lora_manager.LoRAManager.load_lora_weights"><code class="name flex">
<span>def <span class="ident">load_lora_weights</span></span>(<span>self,<br>lora_ref: <a title="sglang.srt.lora.lora_registry.LoRARef" href="lora_registry.html#sglang.srt.lora.lora_registry.LoRARef">LoRARef</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_lora_weights(self, lora_ref: LoRARef):
    &#34;&#34;&#34;
    Load the weights of a LoRA adapter to CPU memory and conducts post-loading validation.
    &#34;&#34;&#34;
    lora_adapter = LoRAAdapter(
        lora_ref.lora_id,
        self.configs[lora_ref.lora_id],
        self.base_hf_config,
        self.load_config,
        self.lora_backend,
    )
    lora_adapter.initialize_weights()
    self.loras[lora_ref.lora_id] = lora_adapter</code></pre>
</details>
<div class="desc"><p>Load the weights of a LoRA adapter to CPU memory and conducts post-loading validation.</p></div>
</dd>
<dt id="sglang.srt.lora.lora_manager.LoRAManager.prepare_lora_batch"><code class="name flex">
<span>def <span class="ident">prepare_lora_batch</span></span>(<span>self,<br>forward_batch: <a title="sglang.srt.model_executor.forward_batch_info.ForwardBatch" href="../model_executor/forward_batch_info.html#sglang.srt.model_executor.forward_batch_info.ForwardBatch">ForwardBatch</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_lora_batch(self, forward_batch: ForwardBatch):

    # Load active loras into lora memory pool
    cur_uids = set(forward_batch.lora_ids)

    assert len(cur_uids) &lt;= self.max_loras_per_batch
    self.memory_pool.prepare_lora_batch(
        cur_uids=cur_uids,
        lora_adapters=self.loras,
        lora_modules=self.lora_modules,
        lora_refs=self.lora_refs.copy(),  # copy snapshot of current lora_refs to avoid mutation during the batch preparation.
    )

    # set up batch info shared by all lora modules
    bs = forward_batch.batch_size

    def transfer_adapter_info(
        weight_indices_out: torch.Tensor,
        lora_ranks_out: torch.Tensor,
        scalings_out: torch.Tensor,
    ):
        &#34;&#34;&#34;
        Transfer adapter metadata (weight indices, LoRA rank, scalings) from host
        to device (CUDA) asynchronously.
        &#34;&#34;&#34;
        weight_indices = [0] * len(forward_batch.lora_ids)
        lora_ranks = [0] * self.max_loras_per_batch
        scalings = [0] * self.max_loras_per_batch
        for i, uid in enumerate(forward_batch.lora_ids):
            weight_indices[i] = self.memory_pool.get_buffer_id(uid)
            if uid is not None:
                lora = self.loras[uid]
                lora_ranks[weight_indices[i]] = lora.config.r
                scalings[weight_indices[i]] = lora.scaling

        # Use pinned memory to avoid synchronizations during host-to-device transfer
        weight_indices_tensor = torch.tensor(
            weight_indices, dtype=torch.int32, pin_memory=True, device=&#34;cpu&#34;
        )
        lora_ranks_tensor = torch.tensor(
            lora_ranks, dtype=torch.int32, pin_memory=True, device=&#34;cpu&#34;
        )
        scalings_tensor = torch.tensor(
            scalings, dtype=torch.float, pin_memory=True, device=&#34;cpu&#34;
        )

        # Copy to device tensors asynchronously
        weight_indices_out[:bs].copy_(weight_indices_tensor, non_blocking=True)
        lora_ranks_out[: self.max_loras_per_batch].copy_(
            lora_ranks_tensor, non_blocking=True
        )
        scalings_out[: self.max_loras_per_batch].copy_(
            scalings_tensor, non_blocking=True
        )

    if (
        hasattr(self, &#34;max_bs_in_cuda_graph&#34;)
        and bs &lt;= self.max_bs_in_cuda_graph
        and forward_batch.forward_mode.is_cuda_graph()
    ):
        # Do in-place updates when CUDA graph is enabled and the batch forward mode
        # could use CUDA graph.

        transfer_adapter_info(
            self.cuda_graph_batch_info.weight_indices,
            self.cuda_graph_batch_info.lora_ranks,
            self.cuda_graph_batch_info.scalings,
        )

        self.cuda_graph_batch_info.bs = bs
        self.cuda_graph_batch_info.max_len = 1
        batch_info = self.cuda_graph_batch_info
    else:
        weight_indices = torch.empty((bs,), dtype=torch.int32, device=self.device)
        lora_ranks = torch.zeros(
            (self.max_loras_per_batch,), dtype=torch.int64, device=self.device
        )
        scalings = torch.zeros(
            (self.max_loras_per_batch,), dtype=torch.float, device=self.device
        )
        transfer_adapter_info(
            weight_indices,
            lora_ranks,
            scalings,
        )

        seg_lens = (
            forward_batch.extend_seq_lens
            if forward_batch.forward_mode.is_extend()
            else torch.ones(bs, device=self.device)
        )

        max_len = (
            # Calculate max_len from the CPU copy to avoid D2H transfer.
            max(forward_batch.extend_seq_lens_cpu)
            if forward_batch.forward_mode.is_extend()
            else 1
        )

        seg_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device=self.device)
        seg_indptr[1:] = torch.cumsum(seg_lens, dim=0)

        batch_info = LoRABatchInfo(
            bs=bs,
            seg_lens=seg_lens,
            seg_indptr=seg_indptr,
            max_len=max_len,
            weight_indices=weight_indices,
            lora_ranks=lora_ranks,
            scalings=scalings,
        )
    self.lora_backend.set_batch_info(batch_info)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.lora_manager.LoRAManager.set_lora_module"><code class="name flex">
<span>def <span class="ident">set_lora_module</span></span>(<span>self, module_name, module)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_lora_module(self, module_name, module):
    lora_module = get_lora_layer(module, self.lora_backend)
    replace_submodule(self.base_model, module_name, lora_module)
    return lora_module</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.lora_manager.LoRAManager.unload_lora_adapter"><code class="name flex">
<span>def <span class="ident">unload_lora_adapter</span></span>(<span>self,<br>lora_ref: <a title="sglang.srt.lora.lora_registry.LoRARef" href="lora_registry.html#sglang.srt.lora.lora_registry.LoRARef">LoRARef</a>) ‑> <a title="sglang.srt.managers.io_struct.LoRAUpdateResult" href="../managers/io_struct.html#sglang.srt.managers.io_struct.LoRAUpdateResult">LoRAUpdateResult</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unload_lora_adapter(self, lora_ref: LoRARef) -&gt; LoRAUpdateResult:
    &#34;&#34;&#34;
    Unload LoRA adapters by their names. This will remove the adapters from the memory pool and
    delete the corresponding LoRA modules.
    &#34;&#34;&#34;

    adapter = self.configs.get(lora_ref.lora_id)
    lora_ref = self.lora_refs.get(lora_ref.lora_id)
    assert (
        adapter is not None and lora_ref is not None
    ), f&#34;LoRA adapter with ID {lora_ref.lora_id} is not loaded. This should have been verified before request is sent to the backend.&#34;

    try:
        del self.configs[lora_ref.lora_id]
        del self.loras[lora_ref.lora_id]
        del self.lora_refs[lora_ref.lora_id]
        self.num_pinned_loras -= int(lora_ref.pinned)
    except Exception as e:
        return self.create_lora_update_result(
            success=False,
            error_message=str(e),
        )

    return self.create_lora_update_result(success=True)</code></pre>
</details>
<div class="desc"><p>Unload LoRA adapters by their names. This will remove the adapters from the memory pool and
delete the corresponding LoRA modules.</p></div>
</dd>
<dt id="sglang.srt.lora.lora_manager.LoRAManager.update_lora_info"><code class="name flex">
<span>def <span class="ident">update_lora_info</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_lora_info(self):
    &#34;&#34;&#34;
    Update all LoRA modules to associate them with the latest memory buffer.
    &#34;&#34;&#34;
    for layer_id, layer_modules in enumerate(self.lora_modules):
        for module_name, module in layer_modules.items():
            target_module = get_target_module_name(
                module_name, self.memory_pool.target_modules
            )
            module.set_lora_info(
                self.memory_pool.get_tensor(
                    target_module=target_module,
                    layer_id=layer_id,
                    lora_type=LoRAType.LORA_A,
                ),
                self.memory_pool.get_tensor(
                    target_module=target_module,
                    layer_id=layer_id,
                    lora_type=LoRAType.LORA_B,
                ),
            )</code></pre>
</details>
<div class="desc"><p>Update all LoRA modules to associate them with the latest memory buffer.</p></div>
</dd>
<dt id="sglang.srt.lora.lora_manager.LoRAManager.validate_lora_batch"><code class="name flex">
<span>def <span class="ident">validate_lora_batch</span></span>(<span>self, lora_ids: set[str]) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate_lora_batch(self, lora_ids: set[str]) -&gt; bool:
    &#34;&#34;&#34;
    Validate if the LoRA IDs in the batch can be loaded into the current LoRA memory pool.
    &#34;&#34;&#34;
    if len(lora_ids) &gt; self.max_loras_per_batch:
        return False

    # skip pinned LoRA check if no pinned LoRA adapters are loaded.
    if self.num_pinned_loras == 0:
        return True

    # counting the number of pinned LoRA adapters in the batch.
    pinned_loras_in_batch = 0
    for lora_id in lora_ids:
        if lora_id is not None:
            lora_ref = self.lora_refs.get(lora_id)
            assert (
                lora_ref is not None
            ), f&#34;LoRA ID {lora_id} not found in lora_refs.&#34;
            pinned_loras_in_batch += int(lora_ref.pinned)

    assert pinned_loras_in_batch &lt;= self.num_pinned_loras, (
        f&#34;Number of pinned LoRA adapters in the batch ({pinned_loras_in_batch}) exceeds the total number of pinned adapters &#34;
        f&#34;({self.num_pinned_loras}). This indicates a bug in the LoRA loading logic.&#34;
    )

    required_slots = len(lora_ids) - pinned_loras_in_batch
    mem_pool_vacancy = self.memory_pool.max_loras_per_batch - self.num_pinned_loras

    return required_slots &lt;= mem_pool_vacancy</code></pre>
</details>
<div class="desc"><p>Validate if the LoRA IDs in the batch can be loaded into the current LoRA memory pool.</p></div>
</dd>
<dt id="sglang.srt.lora.lora_manager.LoRAManager.validate_new_adapter"><code class="name flex">
<span>def <span class="ident">validate_new_adapter</span></span>(<span>self,<br>lora_config: <a title="sglang.srt.lora.lora_config.LoRAConfig" href="lora_config.html#sglang.srt.lora.lora_config.LoRAConfig">LoRAConfig</a>,<br>lora_ref: <a title="sglang.srt.lora.lora_registry.LoRARef" href="lora_registry.html#sglang.srt.lora.lora_registry.LoRARef">LoRARef</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate_new_adapter(self, lora_config: LoRAConfig, lora_ref: LoRARef):
    &#34;&#34;&#34;
    Validate if an adapter can be loaded into the current LoRA memory pool and generate error if it is incompatible.
    &#34;&#34;&#34;

    # Check if the LoRA adapter shape is compatible with the current LoRA memory pool configuration.
    memory_pool = getattr(self, &#34;memory_pool&#34;, None)
    incompatible = memory_pool and not memory_pool.can_support(lora_config)
    if incompatible:
        raise ValueError(
            f&#34;LoRA adapter {lora_ref.lora_name} with rank {lora_config.r} is incompatible with the current &#34;
            &#34;LoRA memory pool configuration. Please ensure that the LoRA adapter&#39;s rank is within the configured &#34;
            &#34;`--max-lora-rank` and that the target modules are included in `--lora-target-modules`.&#34;
        )

    # Ensure pinned LoRA adapters does not exceed maximal limit or cause starvation.
    if lora_ref.pinned and self.num_pinned_loras &gt;= self.max_loras_per_batch - 1:
        raise ValueError(
            f&#34;Failed to load LoRA adapter {lora_ref.lora_name} as a pinned adapter. It is not allowed to pin all slots &#34;
            &#34;in the LoRA memory pool to avoid starvation for unpinned adapters and base models. Please increase your &#34;
            &#34;`--max-loras-per-batch` or load it as unpinned LoRA adapters.&#34;
        )</code></pre>
</details>
<div class="desc"><p>Validate if an adapter can be loaded into the current LoRA memory pool and generate error if it is incompatible.</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.lora" href="index.html">sglang.srt.lora</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.lora.lora_manager.LoRAManager" href="#sglang.srt.lora.lora_manager.LoRAManager">LoRAManager</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.lora.lora_manager.LoRAManager.create_lora_update_result" href="#sglang.srt.lora.lora_manager.LoRAManager.create_lora_update_result">create_lora_update_result</a></code></li>
<li><code><a title="sglang.srt.lora.lora_manager.LoRAManager.init_cuda_graph_batch_info" href="#sglang.srt.lora.lora_manager.LoRAManager.init_cuda_graph_batch_info">init_cuda_graph_batch_info</a></code></li>
<li><code><a title="sglang.srt.lora.lora_manager.LoRAManager.init_lora_adapters" href="#sglang.srt.lora.lora_manager.LoRAManager.init_lora_adapters">init_lora_adapters</a></code></li>
<li><code><a title="sglang.srt.lora.lora_manager.LoRAManager.init_lora_modules" href="#sglang.srt.lora.lora_manager.LoRAManager.init_lora_modules">init_lora_modules</a></code></li>
<li><code><a title="sglang.srt.lora.lora_manager.LoRAManager.init_lora_shapes" href="#sglang.srt.lora.lora_manager.LoRAManager.init_lora_shapes">init_lora_shapes</a></code></li>
<li><code><a title="sglang.srt.lora.lora_manager.LoRAManager.init_memory_pool" href="#sglang.srt.lora.lora_manager.LoRAManager.init_memory_pool">init_memory_pool</a></code></li>
<li><code><a title="sglang.srt.lora.lora_manager.LoRAManager.init_state" href="#sglang.srt.lora.lora_manager.LoRAManager.init_state">init_state</a></code></li>
<li><code><a title="sglang.srt.lora.lora_manager.LoRAManager.load_lora_adapter" href="#sglang.srt.lora.lora_manager.LoRAManager.load_lora_adapter">load_lora_adapter</a></code></li>
<li><code><a title="sglang.srt.lora.lora_manager.LoRAManager.load_lora_weights" href="#sglang.srt.lora.lora_manager.LoRAManager.load_lora_weights">load_lora_weights</a></code></li>
<li><code><a title="sglang.srt.lora.lora_manager.LoRAManager.prepare_lora_batch" href="#sglang.srt.lora.lora_manager.LoRAManager.prepare_lora_batch">prepare_lora_batch</a></code></li>
<li><code><a title="sglang.srt.lora.lora_manager.LoRAManager.set_lora_module" href="#sglang.srt.lora.lora_manager.LoRAManager.set_lora_module">set_lora_module</a></code></li>
<li><code><a title="sglang.srt.lora.lora_manager.LoRAManager.unload_lora_adapter" href="#sglang.srt.lora.lora_manager.LoRAManager.unload_lora_adapter">unload_lora_adapter</a></code></li>
<li><code><a title="sglang.srt.lora.lora_manager.LoRAManager.update_lora_info" href="#sglang.srt.lora.lora_manager.LoRAManager.update_lora_info">update_lora_info</a></code></li>
<li><code><a title="sglang.srt.lora.lora_manager.LoRAManager.validate_lora_batch" href="#sglang.srt.lora.lora_manager.LoRAManager.validate_lora_batch">validate_lora_batch</a></code></li>
<li><code><a title="sglang.srt.lora.lora_manager.LoRAManager.validate_new_adapter" href="#sglang.srt.lora.lora_manager.LoRAManager.validate_new_adapter">validate_new_adapter</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
