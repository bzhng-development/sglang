<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.lora.layers API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.lora.layers</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.lora.layers.get_lora_layer"><code class="name flex">
<span>def <span class="ident">get_lora_layer</span></span>(<span>layer: torch.nn.modules.module.Module,<br>lora_backend: <a title="sglang.srt.lora.backend.base_backend.BaseLoRABackend" href="backend/base_backend.html#sglang.srt.lora.backend.base_backend.BaseLoRABackend">BaseLoRABackend</a>) ‑> <a title="sglang.srt.lora.layers.BaseLayerWithLoRA" href="#sglang.srt.lora.layers.BaseLayerWithLoRA">BaseLayerWithLoRA</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_lora_layer(
    layer: nn.Module, lora_backend: BaseLoRABackend
) -&gt; BaseLayerWithLoRA:
    supported_layer_types = {
        # the order matters
        VocabParallelEmbedding: VocabParallelEmbeddingWithLoRA,
        QKVParallelLinear: QKVParallelLinearWithLoRA,
        MergedColumnParallelLinear: MergedColumnParallelLinearWithLoRA,
        ColumnParallelLinear: ColumnParallelLinearWithLoRA,
        RowParallelLinear: RowParallelLinearWithLoRA,
    }
    for src_layer_type, lora_layer_type in supported_layer_types.items():
        if isinstance(layer, src_layer_type):  # pylint: disable=unidiomatic-typecheck
            ret = lora_layer_type(layer, lora_backend)
            return ret
    raise Exception(f&#34;No corresponding LoRA layer supported for {type(layer)}.&#34;)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.lora.layers.BaseLayerWithLoRA"><code class="flex name class">
<span>class <span class="ident">BaseLayerWithLoRA</span></span>
<span>(</span><span>base_layer: torch.nn.modules.module.Module,<br>lora_backend: <a title="sglang.srt.lora.backend.base_backend.BaseLoRABackend" href="backend/base_backend.html#sglang.srt.lora.backend.base_backend.BaseLoRABackend">BaseLoRABackend</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseLayerWithLoRA(nn.Module):
    def __init__(
        self,
        base_layer: nn.Module,
        lora_backend: BaseLoRABackend,
    ):
        super().__init__()
        self.base_layer: nn.Module = base_layer
        self.set_lora: bool = False
        self.lora_backend: BaseLoRABackend = lora_backend

    def forward(self, x: torch.Tensor):
        return self.base_layer.forward(x)

    def set_lora_info(self, *args):
        pass

    def slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int):
        pass

    def slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int):
        pass</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.lora.layers.ColumnParallelLinearWithLoRA" href="#sglang.srt.lora.layers.ColumnParallelLinearWithLoRA">ColumnParallelLinearWithLoRA</a></li>
<li><a title="sglang.srt.lora.layers.RowParallelLinearWithLoRA" href="#sglang.srt.lora.layers.RowParallelLinearWithLoRA">RowParallelLinearWithLoRA</a></li>
<li><a title="sglang.srt.lora.layers.VocabParallelEmbeddingWithLoRA" href="#sglang.srt.lora.layers.VocabParallelEmbeddingWithLoRA">VocabParallelEmbeddingWithLoRA</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.lora.layers.BaseLayerWithLoRA.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x: torch.Tensor) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x: torch.Tensor):
    return self.base_layer.forward(x)</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
<dt id="sglang.srt.lora.layers.BaseLayerWithLoRA.set_lora_info"><code class="name flex">
<span>def <span class="ident">set_lora_info</span></span>(<span>self, *args)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_lora_info(self, *args):
    pass</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.layers.BaseLayerWithLoRA.slice_lora_a_weights"><code class="name flex">
<span>def <span class="ident">slice_lora_a_weights</span></span>(<span>self, A: torch.Tensor, tp_rank: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int):
    pass</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.layers.BaseLayerWithLoRA.slice_lora_b_weights"><code class="name flex">
<span>def <span class="ident">slice_lora_b_weights</span></span>(<span>self, B: torch.Tensor, tp_rank: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int):
    pass</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.lora.layers.ColumnParallelLinearWithLoRA"><code class="flex name class">
<span>class <span class="ident">ColumnParallelLinearWithLoRA</span></span>
<span>(</span><span>base_layer: <a title="sglang.srt.layers.linear.ColumnParallelLinear" href="../layers/linear.html#sglang.srt.layers.linear.ColumnParallelLinear">ColumnParallelLinear</a>,<br>lora_backend: <a title="sglang.srt.lora.backend.base_backend.BaseLoRABackend" href="backend/base_backend.html#sglang.srt.lora.backend.base_backend.BaseLoRABackend">BaseLoRABackend</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ColumnParallelLinearWithLoRA(BaseLayerWithLoRA):
    def __init__(
        self,
        base_layer: ColumnParallelLinear,
        lora_backend: BaseLoRABackend,
    ) -&gt; None:
        super().__init__(base_layer, lora_backend)

    def set_lora_info(
        self,
        A_buffer: torch.Tensor,
        B_buffer: torch.Tensor,
    ):
        self.set_lora = True
        self.A_buffer = A_buffer
        self.B_buffer = B_buffer

    def apply_lora(self, base_output: torch.Tensor, x: torch.Tensor) -&gt; torch.Tensor:
        lora_a_output = self.lora_backend.run_lora_a_sgemm(x, self.A_buffer)
        lora_output = self.lora_backend.run_lora_b_sgemm(
            x=lora_a_output,
            weights=self.B_buffer,
            base_output=base_output,
        )
        return lora_output

    def forward(self, input_: torch.Tensor):
        # duplicate the logic in ColumnParallelLinear
        bias = self.base_layer.bias if not self.base_layer.skip_bias_add else None
        output_parallel = self.base_layer.quant_method.apply(
            self.base_layer, input_, bias
        )

        if self.set_lora:
            output_parallel = self.apply_lora(output_parallel, input_)

        if self.base_layer.gather_output:
            output = tensor_model_parallel_all_gather(output_parallel)
        else:
            output = output_parallel
        output_bias = self.base_layer.bias if self.base_layer.skip_bias_add else None
        return output, output_bias

    def slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int):
        return A

    def slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int):
        shard_size = self.base_layer.output_partition_sizes[0]
        start_idx = tp_rank * shard_size
        end_idx = (tp_rank + 1) * shard_size
        B = B[start_idx:end_idx, :]
        return B</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.lora.layers.BaseLayerWithLoRA" href="#sglang.srt.lora.layers.BaseLayerWithLoRA">BaseLayerWithLoRA</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.lora.layers.MergedColumnParallelLinearWithLoRA" href="#sglang.srt.lora.layers.MergedColumnParallelLinearWithLoRA">MergedColumnParallelLinearWithLoRA</a></li>
<li><a title="sglang.srt.lora.layers.QKVParallelLinearWithLoRA" href="#sglang.srt.lora.layers.QKVParallelLinearWithLoRA">QKVParallelLinearWithLoRA</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.lora.layers.ColumnParallelLinearWithLoRA.apply_lora"><code class="name flex">
<span>def <span class="ident">apply_lora</span></span>(<span>self, base_output: torch.Tensor, x: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_lora(self, base_output: torch.Tensor, x: torch.Tensor) -&gt; torch.Tensor:
    lora_a_output = self.lora_backend.run_lora_a_sgemm(x, self.A_buffer)
    lora_output = self.lora_backend.run_lora_b_sgemm(
        x=lora_a_output,
        weights=self.B_buffer,
        base_output=base_output,
    )
    return lora_output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.layers.ColumnParallelLinearWithLoRA.set_lora_info"><code class="name flex">
<span>def <span class="ident">set_lora_info</span></span>(<span>self, A_buffer: torch.Tensor, B_buffer: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_lora_info(
    self,
    A_buffer: torch.Tensor,
    B_buffer: torch.Tensor,
):
    self.set_lora = True
    self.A_buffer = A_buffer
    self.B_buffer = B_buffer</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.layers.ColumnParallelLinearWithLoRA.slice_lora_a_weights"><code class="name flex">
<span>def <span class="ident">slice_lora_a_weights</span></span>(<span>self, A: torch.Tensor, tp_rank: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int):
    return A</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.layers.ColumnParallelLinearWithLoRA.slice_lora_b_weights"><code class="name flex">
<span>def <span class="ident">slice_lora_b_weights</span></span>(<span>self, B: torch.Tensor, tp_rank: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int):
    shard_size = self.base_layer.output_partition_sizes[0]
    start_idx = tp_rank * shard_size
    end_idx = (tp_rank + 1) * shard_size
    B = B[start_idx:end_idx, :]
    return B</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.lora.layers.BaseLayerWithLoRA" href="#sglang.srt.lora.layers.BaseLayerWithLoRA">BaseLayerWithLoRA</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.lora.layers.BaseLayerWithLoRA.forward" href="#sglang.srt.lora.layers.BaseLayerWithLoRA.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.lora.layers.MergedColumnParallelLinearWithLoRA"><code class="flex name class">
<span>class <span class="ident">MergedColumnParallelLinearWithLoRA</span></span>
<span>(</span><span>base_layer: <a title="sglang.srt.layers.linear.MergedColumnParallelLinear" href="../layers/linear.html#sglang.srt.layers.linear.MergedColumnParallelLinear">MergedColumnParallelLinear</a>,<br>lora_backend: <a title="sglang.srt.lora.backend.base_backend.BaseLoRABackend" href="backend/base_backend.html#sglang.srt.lora.backend.base_backend.BaseLoRABackend">BaseLoRABackend</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MergedColumnParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):
    def __init__(
        self,
        base_layer: MergedColumnParallelLinear,
        lora_backend: BaseLoRABackend,
    ) -&gt; None:
        super().__init__(base_layer, lora_backend)

    def set_lora_info(
        self,
        A_buffer: torch.Tensor,
        B_buffer: torch.Tensor,
    ):
        self.set_lora = True
        self.A_buffer_gate_up = A_buffer
        self.B_buffer_gate_up = B_buffer

    def apply_lora(self, base_output: torch.Tensor, x: torch.Tensor) -&gt; torch.Tensor:
        lora_output = self.lora_backend.run_gate_up_lora(
            x=x,
            gate_up_lora_a=self.A_buffer_gate_up,
            gate_up_lora_b=self.B_buffer_gate_up,
            base_output=base_output,
        )
        return lora_output

    def slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int):
        return A

    def slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int):
        # Since the outputs for both gate and up are identical, we use a random one.
        shard_size = self.base_layer.output_partition_sizes[0]
        gate_size = self.base_layer.output_sizes[0]
        start_idx = tp_rank * shard_size
        end_idx = (tp_rank + 1) * shard_size
        return torch.concat(
            (
                B[start_idx:end_idx, :],
                B[gate_size + start_idx : gate_size + end_idx],
            ),
            dim=0,
        )</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.lora.layers.ColumnParallelLinearWithLoRA" href="#sglang.srt.lora.layers.ColumnParallelLinearWithLoRA">ColumnParallelLinearWithLoRA</a></li>
<li><a title="sglang.srt.lora.layers.BaseLayerWithLoRA" href="#sglang.srt.lora.layers.BaseLayerWithLoRA">BaseLayerWithLoRA</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.lora.layers.MergedColumnParallelLinearWithLoRA.apply_lora"><code class="name flex">
<span>def <span class="ident">apply_lora</span></span>(<span>self, base_output: torch.Tensor, x: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_lora(self, base_output: torch.Tensor, x: torch.Tensor) -&gt; torch.Tensor:
    lora_output = self.lora_backend.run_gate_up_lora(
        x=x,
        gate_up_lora_a=self.A_buffer_gate_up,
        gate_up_lora_b=self.B_buffer_gate_up,
        base_output=base_output,
    )
    return lora_output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.layers.MergedColumnParallelLinearWithLoRA.set_lora_info"><code class="name flex">
<span>def <span class="ident">set_lora_info</span></span>(<span>self, A_buffer: torch.Tensor, B_buffer: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_lora_info(
    self,
    A_buffer: torch.Tensor,
    B_buffer: torch.Tensor,
):
    self.set_lora = True
    self.A_buffer_gate_up = A_buffer
    self.B_buffer_gate_up = B_buffer</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.layers.MergedColumnParallelLinearWithLoRA.slice_lora_a_weights"><code class="name flex">
<span>def <span class="ident">slice_lora_a_weights</span></span>(<span>self, A: torch.Tensor, tp_rank: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int):
    return A</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.layers.MergedColumnParallelLinearWithLoRA.slice_lora_b_weights"><code class="name flex">
<span>def <span class="ident">slice_lora_b_weights</span></span>(<span>self, B: torch.Tensor, tp_rank: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int):
    # Since the outputs for both gate and up are identical, we use a random one.
    shard_size = self.base_layer.output_partition_sizes[0]
    gate_size = self.base_layer.output_sizes[0]
    start_idx = tp_rank * shard_size
    end_idx = (tp_rank + 1) * shard_size
    return torch.concat(
        (
            B[start_idx:end_idx, :],
            B[gate_size + start_idx : gate_size + end_idx],
        ),
        dim=0,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.lora.layers.ColumnParallelLinearWithLoRA" href="#sglang.srt.lora.layers.ColumnParallelLinearWithLoRA">ColumnParallelLinearWithLoRA</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.lora.layers.ColumnParallelLinearWithLoRA.forward" href="#sglang.srt.lora.layers.BaseLayerWithLoRA.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.lora.layers.QKVParallelLinearWithLoRA"><code class="flex name class">
<span>class <span class="ident">QKVParallelLinearWithLoRA</span></span>
<span>(</span><span>base_layer: <a title="sglang.srt.layers.linear.QKVParallelLinear" href="../layers/linear.html#sglang.srt.layers.linear.QKVParallelLinear">QKVParallelLinear</a>,<br>lora_backend: <a title="sglang.srt.lora.backend.base_backend.BaseLoRABackend" href="backend/base_backend.html#sglang.srt.lora.backend.base_backend.BaseLoRABackend">BaseLoRABackend</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class QKVParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):
    def __init__(
        self,
        base_layer: QKVParallelLinear,
        lora_backend: BaseLoRABackend,
    ) -&gt; None:
        super().__init__(base_layer, lora_backend)
        q_proj_shard_size = self.base_layer.q_proj_shard_size
        kv_proj_shard_size = self.base_layer.kv_proj_shard_size
        self.output_offset = torch.tensor(
            [
                0,
                q_proj_shard_size,
                q_proj_shard_size + kv_proj_shard_size,
                q_proj_shard_size + 2 * kv_proj_shard_size,
            ],
            dtype=torch.int32,
            device=next(self.base_layer.parameters()).device,
        )

        # For computing number of launched blocks
        self.max_qkv_out_dim = max(q_proj_shard_size, kv_proj_shard_size)

    def set_lora_info(
        self,
        A_buffer_qkv: torch.Tensor,
        B_buffer_qkv: torch.Tensor,
    ):
        self.set_lora = True
        self.A_buffer_qkv = A_buffer_qkv
        self.B_buffer_qkv = B_buffer_qkv

    def apply_lora(self, base_output: torch.Tensor, x: torch.Tensor) -&gt; torch.Tensor:
        lora_output = self.lora_backend.run_qkv_lora(
            x=x,
            qkv_lora_a=self.A_buffer_qkv,
            qkv_lora_b=self.B_buffer_qkv,
            base_output=base_output,
            output_offset=self.output_offset,
            max_qkv_out_dim=self.max_qkv_out_dim,
        )
        return lora_output

    def slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int):
        return A

    def slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int) -&gt; torch.Tensor:
        base_layer = self.base_layer
        q_proj_shard_size = base_layer.q_proj_shard_size
        kv_proj_shard_size = base_layer.kv_proj_shard_size
        num_kv_head_replicas = base_layer.num_kv_head_replicas

        q_start_idx = q_proj_shard_size * tp_rank
        q_end_idx = q_start_idx + q_proj_shard_size

        kv_shard_id = tp_rank // num_kv_head_replicas
        kv_start_idx = kv_proj_shard_size * kv_shard_id
        kv_end_idx = kv_start_idx + kv_proj_shard_size

        q_size, k_size, _ = base_layer.output_sizes
        B_q_shard = B[q_start_idx:q_end_idx, :]
        B_k_shard = B[q_size + kv_start_idx : q_size + kv_end_idx, :]
        B_v_shard = B[q_size + k_size + kv_start_idx : q_size + k_size + kv_end_idx, :]

        return torch.concat(
            (
                B_q_shard,
                B_k_shard,
                B_v_shard,
            ),
            dim=0,
        )</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.lora.layers.ColumnParallelLinearWithLoRA" href="#sglang.srt.lora.layers.ColumnParallelLinearWithLoRA">ColumnParallelLinearWithLoRA</a></li>
<li><a title="sglang.srt.lora.layers.BaseLayerWithLoRA" href="#sglang.srt.lora.layers.BaseLayerWithLoRA">BaseLayerWithLoRA</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.lora.layers.QKVParallelLinearWithLoRA.apply_lora"><code class="name flex">
<span>def <span class="ident">apply_lora</span></span>(<span>self, base_output: torch.Tensor, x: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_lora(self, base_output: torch.Tensor, x: torch.Tensor) -&gt; torch.Tensor:
    lora_output = self.lora_backend.run_qkv_lora(
        x=x,
        qkv_lora_a=self.A_buffer_qkv,
        qkv_lora_b=self.B_buffer_qkv,
        base_output=base_output,
        output_offset=self.output_offset,
        max_qkv_out_dim=self.max_qkv_out_dim,
    )
    return lora_output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.layers.QKVParallelLinearWithLoRA.set_lora_info"><code class="name flex">
<span>def <span class="ident">set_lora_info</span></span>(<span>self, A_buffer_qkv: torch.Tensor, B_buffer_qkv: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_lora_info(
    self,
    A_buffer_qkv: torch.Tensor,
    B_buffer_qkv: torch.Tensor,
):
    self.set_lora = True
    self.A_buffer_qkv = A_buffer_qkv
    self.B_buffer_qkv = B_buffer_qkv</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.layers.QKVParallelLinearWithLoRA.slice_lora_a_weights"><code class="name flex">
<span>def <span class="ident">slice_lora_a_weights</span></span>(<span>self, A: torch.Tensor, tp_rank: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int):
    return A</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.layers.QKVParallelLinearWithLoRA.slice_lora_b_weights"><code class="name flex">
<span>def <span class="ident">slice_lora_b_weights</span></span>(<span>self, B: torch.Tensor, tp_rank: int) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int) -&gt; torch.Tensor:
    base_layer = self.base_layer
    q_proj_shard_size = base_layer.q_proj_shard_size
    kv_proj_shard_size = base_layer.kv_proj_shard_size
    num_kv_head_replicas = base_layer.num_kv_head_replicas

    q_start_idx = q_proj_shard_size * tp_rank
    q_end_idx = q_start_idx + q_proj_shard_size

    kv_shard_id = tp_rank // num_kv_head_replicas
    kv_start_idx = kv_proj_shard_size * kv_shard_id
    kv_end_idx = kv_start_idx + kv_proj_shard_size

    q_size, k_size, _ = base_layer.output_sizes
    B_q_shard = B[q_start_idx:q_end_idx, :]
    B_k_shard = B[q_size + kv_start_idx : q_size + kv_end_idx, :]
    B_v_shard = B[q_size + k_size + kv_start_idx : q_size + k_size + kv_end_idx, :]

    return torch.concat(
        (
            B_q_shard,
            B_k_shard,
            B_v_shard,
        ),
        dim=0,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.lora.layers.ColumnParallelLinearWithLoRA" href="#sglang.srt.lora.layers.ColumnParallelLinearWithLoRA">ColumnParallelLinearWithLoRA</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.lora.layers.ColumnParallelLinearWithLoRA.forward" href="#sglang.srt.lora.layers.BaseLayerWithLoRA.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.lora.layers.RowParallelLinearWithLoRA"><code class="flex name class">
<span>class <span class="ident">RowParallelLinearWithLoRA</span></span>
<span>(</span><span>base_layer: <a title="sglang.srt.layers.linear.RowParallelLinear" href="../layers/linear.html#sglang.srt.layers.linear.RowParallelLinear">RowParallelLinear</a>,<br>lora_backend: <a title="sglang.srt.lora.backend.base_backend.BaseLoRABackend" href="backend/base_backend.html#sglang.srt.lora.backend.base_backend.BaseLoRABackend">BaseLoRABackend</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RowParallelLinearWithLoRA(BaseLayerWithLoRA):
    def __init__(
        self,
        base_layer: RowParallelLinear,
        lora_backend: BaseLoRABackend,
    ) -&gt; None:
        super().__init__(base_layer, lora_backend)

    def set_lora_info(self, A_buffer: torch.Tensor, B_buffer: torch.Tensor):
        self.set_lora = True
        self.A_buffer = A_buffer
        self.B_buffer = B_buffer

    def apply_lora(self, base_output: torch.Tensor, x: torch.Tensor) -&gt; torch.Tensor:
        lora_a_output = self.lora_backend.run_lora_a_sgemm(x, self.A_buffer)
        lora_output = self.lora_backend.run_lora_b_sgemm(
            x=lora_a_output,
            weights=self.B_buffer,
            base_output=base_output,
        )
        return lora_output

    def forward(self, input_: torch.Tensor, skip_all_reduce=False):
        # duplicate the logic in RowParallelLinear
        if self.base_layer.input_is_parallel:
            input_parallel = input_
        else:
            tp_rank = get_tensor_model_parallel_rank()
            splitted_input = split_tensor_along_last_dim(
                input_, num_partitions=self.base_layer.tp_size
            )
            input_parallel = splitted_input[tp_rank].contiguous()
        output_parallel = self.base_layer.quant_method.apply(
            self.base_layer, input_parallel
        )

        if self.set_lora:
            output_parallel = self.apply_lora(output_parallel, input_parallel)

        if (
            self.base_layer.reduce_results
            and self.base_layer.tp_size &gt; 1
            and not skip_all_reduce
        ):
            output_ = tensor_model_parallel_all_reduce(output_parallel)
        else:
            output_ = output_parallel

        if not self.base_layer.skip_bias_add:
            output = (
                output_ + self.base_layer.bias
                if self.base_layer.bias is not None
                else output_
            )
            output_bias = None
        else:
            output = output_
            output_bias = self.base_layer.bias
        return output, output_bias

    def slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int):
        shard_size = self.base_layer.input_size_per_partition
        start_idx = tp_rank * shard_size
        end_idx = (tp_rank + 1) * shard_size
        A = A[:, start_idx:end_idx].contiguous()
        return A

    def slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int):
        return B</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.lora.layers.BaseLayerWithLoRA" href="#sglang.srt.lora.layers.BaseLayerWithLoRA">BaseLayerWithLoRA</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.lora.layers.RowParallelLinearWithLoRA.apply_lora"><code class="name flex">
<span>def <span class="ident">apply_lora</span></span>(<span>self, base_output: torch.Tensor, x: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_lora(self, base_output: torch.Tensor, x: torch.Tensor) -&gt; torch.Tensor:
    lora_a_output = self.lora_backend.run_lora_a_sgemm(x, self.A_buffer)
    lora_output = self.lora_backend.run_lora_b_sgemm(
        x=lora_a_output,
        weights=self.B_buffer,
        base_output=base_output,
    )
    return lora_output</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.layers.RowParallelLinearWithLoRA.set_lora_info"><code class="name flex">
<span>def <span class="ident">set_lora_info</span></span>(<span>self, A_buffer: torch.Tensor, B_buffer: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_lora_info(self, A_buffer: torch.Tensor, B_buffer: torch.Tensor):
    self.set_lora = True
    self.A_buffer = A_buffer
    self.B_buffer = B_buffer</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.layers.RowParallelLinearWithLoRA.slice_lora_a_weights"><code class="name flex">
<span>def <span class="ident">slice_lora_a_weights</span></span>(<span>self, A: torch.Tensor, tp_rank: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int):
    shard_size = self.base_layer.input_size_per_partition
    start_idx = tp_rank * shard_size
    end_idx = (tp_rank + 1) * shard_size
    A = A[:, start_idx:end_idx].contiguous()
    return A</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.lora.layers.RowParallelLinearWithLoRA.slice_lora_b_weights"><code class="name flex">
<span>def <span class="ident">slice_lora_b_weights</span></span>(<span>self, B: torch.Tensor, tp_rank: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int):
    return B</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.lora.layers.BaseLayerWithLoRA" href="#sglang.srt.lora.layers.BaseLayerWithLoRA">BaseLayerWithLoRA</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.lora.layers.BaseLayerWithLoRA.forward" href="#sglang.srt.lora.layers.BaseLayerWithLoRA.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.lora.layers.VocabParallelEmbeddingWithLoRA"><code class="flex name class">
<span>class <span class="ident">VocabParallelEmbeddingWithLoRA</span></span>
<span>(</span><span>base_layer: <a title="sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding" href="../layers/vocab_parallel_embedding.html#sglang.srt.layers.vocab_parallel_embedding.VocabParallelEmbedding">VocabParallelEmbedding</a>,<br>lora_backend: <a title="sglang.srt.lora.backend.base_backend.BaseLoRABackend" href="backend/base_backend.html#sglang.srt.lora.backend.base_backend.BaseLoRABackend">BaseLoRABackend</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VocabParallelEmbeddingWithLoRA(BaseLayerWithLoRA):
    &#34;&#34;&#34;
    Vocab parallel embedding layer with support for LoRA (Low-Rank Adaptation).

    Note: The current version does not yet implement the LoRA functionality.
    This class behaves exactly the same as the base VocabParallelEmbedding.
    Future versions will integrate LoRA functionality to support efficient parameter fine-tuning.
    &#34;&#34;&#34;

    def __init__(
        self,
        base_layer: VocabParallelEmbedding,
        lora_backend: BaseLoRABackend,
    ) -&gt; None:
        super().__init__(base_layer, lora_backend)
        self.weight = base_layer.weight</code></pre>
</details>
<div class="desc"><p>Vocab parallel embedding layer with support for LoRA (Low-Rank Adaptation).</p>
<p>Note: The current version does not yet implement the LoRA functionality.
This class behaves exactly the same as the base VocabParallelEmbedding.
Future versions will integrate LoRA functionality to support efficient parameter fine-tuning.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.lora.layers.BaseLayerWithLoRA" href="#sglang.srt.lora.layers.BaseLayerWithLoRA">BaseLayerWithLoRA</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.lora.layers.BaseLayerWithLoRA" href="#sglang.srt.lora.layers.BaseLayerWithLoRA">BaseLayerWithLoRA</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.lora.layers.BaseLayerWithLoRA.forward" href="#sglang.srt.lora.layers.BaseLayerWithLoRA.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.lora" href="index.html">sglang.srt.lora</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.lora.layers.get_lora_layer" href="#sglang.srt.lora.layers.get_lora_layer">get_lora_layer</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.lora.layers.BaseLayerWithLoRA" href="#sglang.srt.lora.layers.BaseLayerWithLoRA">BaseLayerWithLoRA</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.lora.layers.BaseLayerWithLoRA.forward" href="#sglang.srt.lora.layers.BaseLayerWithLoRA.forward">forward</a></code></li>
<li><code><a title="sglang.srt.lora.layers.BaseLayerWithLoRA.set_lora_info" href="#sglang.srt.lora.layers.BaseLayerWithLoRA.set_lora_info">set_lora_info</a></code></li>
<li><code><a title="sglang.srt.lora.layers.BaseLayerWithLoRA.slice_lora_a_weights" href="#sglang.srt.lora.layers.BaseLayerWithLoRA.slice_lora_a_weights">slice_lora_a_weights</a></code></li>
<li><code><a title="sglang.srt.lora.layers.BaseLayerWithLoRA.slice_lora_b_weights" href="#sglang.srt.lora.layers.BaseLayerWithLoRA.slice_lora_b_weights">slice_lora_b_weights</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.lora.layers.ColumnParallelLinearWithLoRA" href="#sglang.srt.lora.layers.ColumnParallelLinearWithLoRA">ColumnParallelLinearWithLoRA</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.lora.layers.ColumnParallelLinearWithLoRA.apply_lora" href="#sglang.srt.lora.layers.ColumnParallelLinearWithLoRA.apply_lora">apply_lora</a></code></li>
<li><code><a title="sglang.srt.lora.layers.ColumnParallelLinearWithLoRA.set_lora_info" href="#sglang.srt.lora.layers.ColumnParallelLinearWithLoRA.set_lora_info">set_lora_info</a></code></li>
<li><code><a title="sglang.srt.lora.layers.ColumnParallelLinearWithLoRA.slice_lora_a_weights" href="#sglang.srt.lora.layers.ColumnParallelLinearWithLoRA.slice_lora_a_weights">slice_lora_a_weights</a></code></li>
<li><code><a title="sglang.srt.lora.layers.ColumnParallelLinearWithLoRA.slice_lora_b_weights" href="#sglang.srt.lora.layers.ColumnParallelLinearWithLoRA.slice_lora_b_weights">slice_lora_b_weights</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.lora.layers.MergedColumnParallelLinearWithLoRA" href="#sglang.srt.lora.layers.MergedColumnParallelLinearWithLoRA">MergedColumnParallelLinearWithLoRA</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.lora.layers.MergedColumnParallelLinearWithLoRA.apply_lora" href="#sglang.srt.lora.layers.MergedColumnParallelLinearWithLoRA.apply_lora">apply_lora</a></code></li>
<li><code><a title="sglang.srt.lora.layers.MergedColumnParallelLinearWithLoRA.set_lora_info" href="#sglang.srt.lora.layers.MergedColumnParallelLinearWithLoRA.set_lora_info">set_lora_info</a></code></li>
<li><code><a title="sglang.srt.lora.layers.MergedColumnParallelLinearWithLoRA.slice_lora_a_weights" href="#sglang.srt.lora.layers.MergedColumnParallelLinearWithLoRA.slice_lora_a_weights">slice_lora_a_weights</a></code></li>
<li><code><a title="sglang.srt.lora.layers.MergedColumnParallelLinearWithLoRA.slice_lora_b_weights" href="#sglang.srt.lora.layers.MergedColumnParallelLinearWithLoRA.slice_lora_b_weights">slice_lora_b_weights</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.lora.layers.QKVParallelLinearWithLoRA" href="#sglang.srt.lora.layers.QKVParallelLinearWithLoRA">QKVParallelLinearWithLoRA</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.lora.layers.QKVParallelLinearWithLoRA.apply_lora" href="#sglang.srt.lora.layers.QKVParallelLinearWithLoRA.apply_lora">apply_lora</a></code></li>
<li><code><a title="sglang.srt.lora.layers.QKVParallelLinearWithLoRA.set_lora_info" href="#sglang.srt.lora.layers.QKVParallelLinearWithLoRA.set_lora_info">set_lora_info</a></code></li>
<li><code><a title="sglang.srt.lora.layers.QKVParallelLinearWithLoRA.slice_lora_a_weights" href="#sglang.srt.lora.layers.QKVParallelLinearWithLoRA.slice_lora_a_weights">slice_lora_a_weights</a></code></li>
<li><code><a title="sglang.srt.lora.layers.QKVParallelLinearWithLoRA.slice_lora_b_weights" href="#sglang.srt.lora.layers.QKVParallelLinearWithLoRA.slice_lora_b_weights">slice_lora_b_weights</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.lora.layers.RowParallelLinearWithLoRA" href="#sglang.srt.lora.layers.RowParallelLinearWithLoRA">RowParallelLinearWithLoRA</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.lora.layers.RowParallelLinearWithLoRA.apply_lora" href="#sglang.srt.lora.layers.RowParallelLinearWithLoRA.apply_lora">apply_lora</a></code></li>
<li><code><a title="sglang.srt.lora.layers.RowParallelLinearWithLoRA.set_lora_info" href="#sglang.srt.lora.layers.RowParallelLinearWithLoRA.set_lora_info">set_lora_info</a></code></li>
<li><code><a title="sglang.srt.lora.layers.RowParallelLinearWithLoRA.slice_lora_a_weights" href="#sglang.srt.lora.layers.RowParallelLinearWithLoRA.slice_lora_a_weights">slice_lora_a_weights</a></code></li>
<li><code><a title="sglang.srt.lora.layers.RowParallelLinearWithLoRA.slice_lora_b_weights" href="#sglang.srt.lora.layers.RowParallelLinearWithLoRA.slice_lora_b_weights">slice_lora_b_weights</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.lora.layers.VocabParallelEmbeddingWithLoRA" href="#sglang.srt.lora.layers.VocabParallelEmbeddingWithLoRA">VocabParallelEmbeddingWithLoRA</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
