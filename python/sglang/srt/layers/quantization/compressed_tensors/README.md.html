<h1 id="quantization-compressed_tensors-module">quantization compressed_tensors module</h1>
<p>To support compressed_tensors format quantization models, we adapted https://github.com/vllm-project/vllm/tree/main/vllm/model_executor/layers/quantization/compressed_tensors into SGLang.</p>
<p>For practical purposes, we have only applied the compressed_tensors format of <code>w8a8_fp8</code>. If you have requirements for other formats, you can submit an issue through this <a href="https://github.com/sgl-project/sglang/issues">link</a>.</p>
