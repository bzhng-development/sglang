"cat" /tmp/srt_functions_index.txt
AST Function Index
Root: /Users/vincentzed/Documents/Github/open_source/sglang/python/sglang/srt
Excluded directories: configs, connector, constrained, debug_utils, entrypoints, function_call, lora, mem_cache, metrics, model_loader, models

File: _custom_ops.py
  - name: init_custom_ar
    signature: (ipc_tensors: List[torch.Tensor], rank_data: torch.Tensor, rank: int, full_nvlink: bool)
    return: int
  - name: all_reduce
    signature: (fa: int, inp: torch.Tensor, out: torch.Tensor, reg_buffer: int, reg_buffer_sz_bytes: int)
    return: None
  - name: dispose
    signature: (fa: int)
    return: None
  - name: meta_size
    signature: ()
    return: int
  - name: register_buffer
    signature: (fa: int, ipc_tensors: List[int])
    return: None
  - name: get_graph_buffer_ipc_meta
    signature: (fa: int)
    return: Tuple[List[int], List[int]]
  - name: register_graph_buffers
    signature: (fa: int, handles: List[List[int]], offsets: List[List[int]])
    return: None
  - name: init_custom_ar
    signature: (meta: torch.Tensor, rank_data: torch.Tensor, handles: List[str], offsets: List[int], rank: int, full_nvlink: bool)
    return: int
  - name: all_reduce_reg
    signature: (fa: int, inp: torch.Tensor, out: torch.Tensor)
    return: None
  - name: all_reduce_unreg
    signature: (fa: int, inp: torch.Tensor, reg_buffer: torch.Tensor, out: torch.Tensor)
    return: None
  - name: dispose
    signature: (fa: int)
    return: None
  - name: meta_size
    signature: ()
    return: int
  - name: register_buffer
    signature: (fa: int, t: torch.Tensor, handles: List[str], offsets: List[int])
    return: None
  - name: get_graph_buffer_ipc_meta
    signature: (fa: int)
    return: Tuple[torch.Tensor, List[int]]
  - name: register_graph_buffers
    signature: (fa: int, handles: List[str], offsets: List[List[int]])
    return: None
  - name: allocate_meta_buffer
    signature: (size: int)
    return: torch.Tensor
  - name: get_meta_buffer_ipc_handle
    signature: (inp: torch.Tensor)
    return: torch.Tensor
  - name: init_custom_qr
    signature: (rank: int, world_size: int, qr_max_size: Optional[int] = None)
    return: int
  - name: qr_get_handle
    signature: (fa: int)
    return: torch.Tensor
  - name: qr_open_handles
    signature: (fa: int, handles: list[torch.Tensor])
    return: None
  - name: qr_all_reduce
    signature: (fa: int, inp: torch.Tensor, out: torch.Tensor, quant_level: int, cast_bf2half: bool)
    return: None
  - name: qr_destroy
    signature: (fa: int)
    return: None
  - name: qr_max_size
    signature: ()
    return: int
  - name: mscclpp_generate_unique_id
    signature: ()
    return: bytes
  - name: mscclpp_init_context
    signature: (unique_id: bytes, rank: int, world_size: int, scratch: torch.Tensor, put_buffer: torch.Tensor, nranks_per_node: int, rank_to_node: List[int], rank_to_ib: List[int], context_selection: int)
    return: int
  - name: mscclpp_allreduce
    signature: (context: int, inp: torch.Tensor, out: torch.Tensor, nthreads: int, nblocks: int)
    return: None

File: aio_rwlock.py
  - name: __init__
    signature: (self)
    class: RWLock
  - name: reader_lock
    signature: (self)
    class: RWLock
    doc: A context manager for acquiring a shared (reader) lock.
  - name: writer_lock
    signature: (self)
    class: RWLock
    doc: A context manager for acquiring an exclusive (writer) lock.
  - name: acquire_reader
    signature: (self)
    class: RWLock
  - name: release_reader
    signature: (self)
    class: RWLock
  - name: acquire_writer
    signature: (self)
    class: RWLock
  - name: release_writer
    signature: (self)
    class: RWLock
  - name: __init__
    signature: (self, rwlock: RWLock)
    class: _ReaderLock
  - name: __aenter__
    signature: (self)
    class: _ReaderLock
  - name: __aexit__
    signature: (self, exc_type, exc_val, exc_tb)
    class: _ReaderLock
  - name: __init__
    signature: (self, rwlock: RWLock)
    class: _WriterLock
  - name: __aenter__
    signature: (self)
    class: _WriterLock
  - name: __aexit__
    signature: (self, exc_type, exc_val, exc_tb)
    class: _WriterLock

File: bench_utils.py
  - name: __enter__
    signature: (self)
    class: suppress_stdout_stderr
  - name: __exit__
    signature: (self, *_)
    class: suppress_stdout_stderr
  - name: bench_kineto
    signature: (fn, kernel_names, num_tests: int = 30, suppress_kineto_output: bool = False, trace_path: str = None, flush_l2: bool = True, with_multiple_kernels: bool = False)

File: code_completion_parser.py
  - name: register_completion_template
    signature: (template: CompletionTemplate, override: bool = False)
    doc: Register a new completion template.
  - name: completion_template_exists
    signature: (template_name: str)
    return: bool
  - name: is_completion_template_defined
    signature: ()
    return: bool
  - name: generate_completion_prompt_from_request
    signature: (request: CompletionRequest)
    return: str
  - name: generate_completion_prompt
    signature: (prompt: str, suffix: str, template_name: str)
    return: str

File: constants.py
  (no function definitions found)
File: conversation.py
  - name: get_prompt
    signature: (self)
    return: str
    class: Conversation
    doc: Get the prompt for generation.
  - name: set_system_message
    signature: (self, system_message: str)
    class: Conversation
    doc: Set the system message.
  - name: append_message
    signature: (self, role: str, message: str)
    class: Conversation
    doc: Append a new message.
  - name: append_image
    signature: (self, image: str, detail: Literal['auto', 'low', 'high'])
    class: Conversation
    doc: Append a new image.
  - name: append_video
    signature: (self, video: str)
    class: Conversation
    doc: Append a new video.
  - name: append_audio
    signature: (self, audio: str)
    class: Conversation
    doc: Append a new audio.
  - name: update_last_message
    signature: (self, message: str)
    class: Conversation
    doc: Update the last output.
  - name: to_gradio_chatbot
    signature: (self)
    class: Conversation
    doc: Convert the conversation to gradio chatbot format.
  - name: to_openai_api_messages
    signature: (self)
    class: Conversation
    doc: Convert the conversation to OpenAI chat completion format.
  - name: copy
    signature: (self)
    class: Conversation
  - name: dict
    signature: (self)
    class: Conversation
  - name: register_conv_template
    signature: (template: Conversation, override: bool = False)
    doc: Register a new conversation template.
  - name: register_conv_template_matching_function
    signature: (func)
  - name: get_conv_template_by_model_path
    signature: (model_path)
  - name: chat_template_exists
    signature: (template_name: str)
    return: bool
  - name: generate_embedding_convs
    signature: (texts: List[str], images: List[str], template_name: str)
    return: List[Conversation]
  - name: _get_full_multimodal_text_prompt
    signature: (modality_token: str, modality_count: int, text_prompt: str)
    return: str
    doc: Combine multimodal prompts for a multimodal language model.
  - name: generate_chat_conv
    signature: (request: ChatCompletionRequest, template_name: str)
    return: Conversation
  - name: get_model_type
    signature: (model_path: str)
    return: Optional[str]
  - name: match_internvl
    signature: (model_path: str)
  - name: match_deepseek_janus_pro
    signature: (model_path: str)
  - name: match_vicuna
    signature: (model_path: str)
  - name: match_deepseek_vl
    signature: (model_path: str)
  - name: match_qwen_chat_ml
    signature: (model_path: str)
  - name: match_minicpm
    signature: (model_path: str)
  - name: match_phi_4_mm
    signature: (model_path: str)

File: custom_op.py
  - name: __init__
    signature: (self)
    class: CustomOp
  - name: enter_torch_compile
    signature: (self, num_tokens: int)
    class: CustomOp
  - name: leave_torch_compile
    signature: (self)
    class: CustomOp
  - name: forward
    signature: (self, *args, **kwargs)
    class: CustomOp
  - name: forward_native
    signature: (self, *args, **kwargs)
    class: CustomOp
  - name: forward_cuda
    signature: (self, *args, **kwargs)
    class: CustomOp
  - name: forward_npu
    signature: (self, *args, **kwargs)
    class: CustomOp
  - name: forward_hip
    signature: (self, *args, **kwargs)
    class: CustomOp
  - name: forward_xpu
    signature: (self, *args, **kwargs)
    class: CustomOp
  - name: forward_hpu
    signature: (self, *args, **kwargs)
    class: CustomOp
  - name: forward_cpu
    signature: (self, *args, **kwargs)
    class: CustomOp
  - name: dispatch_forward
    signature: (self)
    class: CustomOp

File: disaggregation/ascend/__init__.py
  (no function definitions found)
File: disaggregation/ascend/conn.py
  - name: init_engine
    signature: (self)
    class: AscendKVManager
  - name: register_buffer_to_engine
    signature: (self)
    class: AscendKVManager
  - name: send_kvcache
    signature: (self, mooncake_session_id: str, prefill_kv_indices: npt.NDArray[np.int32], dst_kv_ptrs: list[int], dst_kv_indices: npt.NDArray[np.int32], executor: concurrent.futures.ThreadPoolExecutor)
    class: AscendKVManager
  - name: set_transfer_blocks
    signature: (src_ptr: int, dst_ptr: int, item_len: int)
    return: List[Tuple[int, int, int]]
    class: AscendKVManager
  - name: process_layer
    signature: (src_ptr: int, dst_ptr: int, item_len: int)
    return: int
    class: AscendKVManager
  - name: process_layers
    signature: (layers_params: List[Tuple[int, int, int]])
    return: int
    class: AscendKVManager

File: disaggregation/ascend/transfer_engine.py
  - name: __init__
    signature: (self, hostname: str, npu_id: int, disaggregation_mode: DisaggregationMode)
    class: AscendTransferEngine
  - name: initialize
    signature: (self)
    return: None
    class: AscendTransferEngine
    doc: Initialize the ascend transfer instance.
  - name: batch_register
    signature: (self, ptrs: List[int], lengths: List[int])
    class: AscendTransferEngine

File: disaggregation/base/__init__.py
  (no function definitions found)
File: disaggregation/base/conn.py
  - name: __init__
    signature: (self, args: KVArgs, disaggregation_mode: DisaggregationMode, server_args: ServerArgs, is_mla_backend: Optional[bool] = False)
    class: BaseKVManager
  - name: __init__
    signature: (self, mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: int, dest_tp_ranks: List[int], pp_rank: int)
    class: BaseKVSender
  - name: init
    signature: (self, num_kv_indices: int, aux_index: Optional[int] = None)
    class: BaseKVSender
    doc: Notify the decoder server about the kv indices length and aux index
  - name: send
    signature: (self, kv_indices: npt.NDArray[np.int32])
    class: BaseKVSender
    doc: Send the kv cache at the given kv indices to the decoder server
  - name: poll
    signature: (self)
    return: KVPoll
    class: BaseKVSender
    doc: Check the status of the kv cache transfer
  - name: failure_exception
    signature: (self)
    class: BaseKVSender
    doc: Raise an exception if the kv cache transfer fails
  - name: __init__
    signature: (self, mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: Optional[int] = None)
    class: BaseKVReceiver
  - name: init
    signature: (self, kv_indices: npt.NDArray[np.int32], aux_index: Optional[int] = None)
    class: BaseKVReceiver
    doc: Notify the prefill server about the kv indices and aux index
  - name: poll
    signature: (self)
    return: KVPoll
    class: BaseKVReceiver
    doc: Check the status of the kv cache transfer
  - name: failure_exception
    signature: (self)
    class: BaseKVReceiver
    doc: Raise an exception if the kv cache transfer fails
  - name: __init__
    signature: (self, port: int)
    class: BaseKVBootstrapServer

File: disaggregation/common/__init__.py
  (no function definitions found)
File: disaggregation/common/conn.py
  - name: __init__
    signature: (self, args: KVArgs, disaggregation_mode: DisaggregationMode, server_args: ServerArgs, is_mla_backend: Optional[bool] = False)
    class: CommonKVManager
  - name: _register_to_bootstrap
    signature: (self)
    class: CommonKVManager
    doc: Register KVSender to bootstrap server via HTTP POST.
  - name: _connect
    signature: (self, endpoint: str, is_ipv6: bool = False)
    class: CommonKVManager
  - name: __init__
    signature: (self, mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: Optional[int] = None, data_parallel_rank: Optional[int] = None)
    class: CommonKVReceiver
  - name: _get_bootstrap_info_from_server
    signature: (self, engine_rank, target_dp_group)
    class: CommonKVReceiver
    doc: Fetch the bootstrap info from the bootstrap server.
  - name: _get_prefill_dp_size_from_server
    signature: (self)
    return: int
    class: CommonKVReceiver
    doc: Fetch the prefill parallel info from the bootstrap server.
  - name: _connect
    signature: (cls, endpoint: str, is_ipv6: bool = False)
    class: CommonKVReceiver
  - name: _connect_to_bootstrap_server
    signature: (cls, bootstrap_info: dict)
    class: CommonKVReceiver
  - name: _register_kv_args
    signature: (self)
    class: CommonKVReceiver
  - name: failure_exception
    signature: (self)
    class: CommonKVReceiver
  - name: __init__
    signature: (self, port: int)
    class: CommonKVBootstrapServer
  - name: run
    signature: (self)
    class: CommonKVBootstrapServer
  - name: _setup_routes
    signature: (self)
    class: CommonKVBootstrapServer
  - name: _handle_route
    signature: (self, request: web.Request)
    class: CommonKVBootstrapServer
  - name: _handle_route_put
    signature: (self, request: web.Request)
    class: CommonKVBootstrapServer
  - name: _handle_route_get
    signature: (self, request: web.Request)
    class: CommonKVBootstrapServer
  - name: _run_server
    signature: (self)
    class: CommonKVBootstrapServer
  - name: close
    signature: (self)
    class: CommonKVBootstrapServer
    doc: Shutdown
  - name: poll
    signature: (self)
    return: KVPoll
    class: CommonKVBootstrapServer

File: disaggregation/common/utils.py
  - name: __init__
    signature: (self)
    class: FastQueue
  - name: put
    signature: (self, item)
    class: FastQueue
  - name: get
    signature: (self)
    class: FastQueue
  - name: group_concurrent_contiguous
    signature: (src_indices: npt.NDArray[np.int32], dst_indices: npt.NDArray[np.int32])
    return: Tuple[List[npt.NDArray[np.int32]], List[npt.NDArray[np.int32]]]
    doc: Vectorised NumPy implementation.

File: disaggregation/decode.py
  - name: __init__
    signature: (self, size: int, max_context_len: int, device: str, enable_memory_saver: bool, pre_alloc_size: int)
    class: DecodeReqToTokenPool
  - name: write
    signature: (self, indices, values)
    class: DecodeReqToTokenPool
  - name: available_size
    signature: (self)
    class: DecodeReqToTokenPool
  - name: alloc
    signature: (self, need_size: int)
    return: List[int]
    class: DecodeReqToTokenPool
  - name: free
    signature: (self, free_index: Union[int, List[int]])
    class: DecodeReqToTokenPool
  - name: clear
    signature: (self)
    class: DecodeReqToTokenPool
  - name: __init__
    signature: (self, req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, draft_token_to_kv_pool: Optional[KVCache], req_to_metadata_buffer_idx_allocator: ReqToMetadataIdxAllocator, metadata_buffers: MetadataBuffers, scheduler: Scheduler, transfer_queue: DecodeTransferQueue, tree_cache: BasePrefixCache, gloo_group: ProcessGroup, tp_rank: int, tp_size: int, dp_size: int, gpu_id: int, bootstrap_port: int, max_total_num_tokens: int, prefill_pp_size: int, num_reserved_decode_tokens: int, transfer_backend: TransferBackend)
    class: DecodePreallocQueue
  - name: _init_kv_manager
    signature: (self)
    return: BaseKVManager
    class: DecodePreallocQueue
  - name: add
    signature: (self, req: Req, is_retracted: bool = False)
    return: None
    class: DecodePreallocQueue
    doc: Add a request to the pending queue.
  - name: _check_if_req_exceed_kv_capacity
    signature: (self, req: Req)
    return: bool
    class: DecodePreallocQueue
  - name: extend
    signature: (self, reqs: List[Req], is_retracted: bool = False)
    return: None
    class: DecodePreallocQueue
    doc: Add a request to the pending queue.
  - name: resume_retracted_reqs
    signature: (self)
    return: List[Req]
    class: DecodePreallocQueue
  - name: _update_handshake_waiters
    signature: (self)
    return: None
    class: DecodePreallocQueue
  - name: pop_preallocated
    signature: (self)
    return: List[DecodeRequest]
    class: DecodePreallocQueue
    doc: Pop the preallocated requests from the pending queue (FIFO).
  - name: num_tokens_pre_allocated
    signature: (self)
    class: DecodePreallocQueue
  - name: _allocatable_tokens
    signature: (self, retractable_tokens: Optional[int] = None, count_retracted: bool = True)
    return: int
    class: DecodePreallocQueue
  - name: _pre_alloc
    signature: (self, req: Req)
    return: torch.Tensor
    class: DecodePreallocQueue
    doc: Pre-allocate the memory for req_to_token and token_kv_pool
  - name: __init__
    signature: (self, gloo_group: ProcessGroup, req_to_metadata_buffer_idx_allocator: ReqToMetadataIdxAllocator, tp_rank: int, metadata_buffers: MetadataBuffers, scheduler: Scheduler, tree_cache: BasePrefixCache)
    class: DecodeTransferQueue
  - name: add
    signature: (self, decode_req: DecodeRequest)
    return: None
    class: DecodeTransferQueue
  - name: extend
    signature: (self, decode_reqs: List[DecodeRequest])
    return: None
    class: DecodeTransferQueue
  - name: pop_transferred
    signature: (self)
    return: List[Req]
    class: DecodeTransferQueue
  - name: event_loop_normal_disagg_decode
    signature: (self: Scheduler)
    class: SchedulerDisaggregationDecodeMixin
    doc: A normal scheduler loop for decode worker in disaggregation mode.
  - name: event_loop_overlap_disagg_decode
    signature: (self: Scheduler)
    class: SchedulerDisaggregationDecodeMixin
  - name: _prepare_idle_batch_and_run
    signature: (self: Scheduler, batch, delay_process = False)
    class: SchedulerDisaggregationDecodeMixin
  - name: get_next_disagg_decode_batch_to_run
    signature: (self: Scheduler)
    return: Optional[Tuple[ScheduleBatch, bool]]
    class: SchedulerDisaggregationDecodeMixin
    doc: Create fake completed prefill if possible and merge with running batch
  - name: get_new_prebuilt_batch
    signature: (self: Scheduler)
    return: Optional[ScheduleBatch]
    class: SchedulerDisaggregationDecodeMixin
    doc: Create a schedulebatch for fake completed prefill
  - name: process_decode_queue
    signature: (self: Scheduler)
    class: SchedulerDisaggregationDecodeMixin

File: disaggregation/decode_schedule_batch_mixin.py
  - name: prepare_for_prebuilt_extend
    signature: (self: ScheduleBatch)
    class: ScheduleBatchDisaggregationDecodeMixin
    doc: Prepare a prebuilt extend by populate metadata
  - name: process_prebuilt_extend
    signature: (self: ScheduleBatch, server_args: ServerArgs, model_config: ModelConfig)
    class: ScheduleBatchDisaggregationDecodeMixin
    doc: Assign the buffered last input id to schedule batch

File: disaggregation/fake/__init__.py
  (no function definitions found)
File: disaggregation/fake/conn.py
  - name: __init__
    signature: (self, mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: int, dest_tp_ranks: List[int], pp_rank: int)
    class: FakeKVSender
  - name: poll
    signature: (self)
    return: KVPoll
    class: FakeKVSender
  - name: init
    signature: (self, kv_indices: list[int], aux_index: Optional[int] = None)
    class: FakeKVSender
  - name: send
    signature: (self, kv_indices: npt.NDArray[np.int32])
    class: FakeKVSender
  - name: failure_exception
    signature: (self)
    class: FakeKVSender
  - name: __init__
    signature: (self, mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: Optional[int] = None, data_parallel_rank: Optional[int] = None)
    class: FakeKVReceiver
  - name: poll
    signature: (self)
    return: KVPoll
    class: FakeKVReceiver
  - name: init
    signature: (self, kv_indices: list[int], aux_index: Optional[int] = None)
    class: FakeKVReceiver
  - name: failure_exception
    signature: (self)
    class: FakeKVReceiver

File: disaggregation/kv_events.py
  - name: __init__
    signature: (self, attn_dp_rank: int = 0)
    class: EventPublisher
  - name: publish
    signature: (self, events: EventBatch)
    return: None
    class: EventPublisher
    doc: Emit events in order.
  - name: shutdown
    signature: (self)
    return: None
    class: EventPublisher
    doc: Shutdown the publisher.
  - name: publish
    signature: (self, events)
    return: None
    class: NullEventPublisher
  - name: shutdown
    signature: (self)
    return: None
    class: NullEventPublisher
  - name: __init__
    signature: (self, attn_dp_rank: int, endpoint: str = 'tcp://*:5557', replay_endpoint: Optional[str] = None, buffer_steps: int = 10000, hwm: int = 100000, max_queue_size: int = 100000, topic: str = '')
    return: None
    class: ZmqEventPublisher
  - name: publish
    signature: (self, events: EventBatch)
    return: None
    class: ZmqEventPublisher
  - name: shutdown
    signature: (self)
    return: None
    class: ZmqEventPublisher
    doc: Stop the publisher thread and clean up resources.
  - name: _socket_setup
    signature: (self)
    return: None
    class: ZmqEventPublisher
    doc: Initialize sockets
  - name: _publisher_thread
    signature: (self)
    return: None
    class: ZmqEventPublisher
    doc: Background thread that processes the event queue.
  - name: _service_replay
    signature: (self)
    return: None
    class: ZmqEventPublisher
    doc: If a replay request is waiting, send buffered batches.
  - name: offset_endpoint_port
    signature: (endpoint: Optional[str], data_parallel_rank: int)
    return: Optional[str]
    class: ZmqEventPublisher
    doc: Helper function to offset the port in an endpoint by
  - name: from_cli
    signature: (cls, cli_value: str)
    return: 'KVEventsConfig'
    class: KVEventsConfig
    doc: Parse the CLI value for the event publisher config.
  - name: register_publisher
    signature: (cls, name: str, ctor: Callable[..., EventPublisher])
    return: None
    class: EventPublisherFactory
  - name: create
    signature: (cls, config: Optional[str], attn_dp_rank: int = 0)
    return: EventPublisher
    class: EventPublisherFactory
    doc: Create publisher from a config mapping.

File: disaggregation/launch_lb.py
  - name: add_cli_args
    signature: (parser: argparse.ArgumentParser)
    class: LBArgs
  - name: from_cli_args
    signature: (cls, args: argparse.Namespace)
    return: 'LBArgs'
    class: LBArgs
  - name: main
    signature: ()

File: disaggregation/mini_lb.py
  - name: setup_logger
    signature: ()
  - name: __init__
    signature: (self, prefill_configs: List[PrefillConfig], decode_servers: List[str], timeout: int)
    class: MiniLoadBalancer
  - name: add_prefill_server
    signature: (self, new_prefill_config: PrefillConfig)
    class: MiniLoadBalancer
  - name: add_decode_server
    signature: (self, new_decode_server: str)
    class: MiniLoadBalancer
  - name: select_pair
    signature: (self)
    class: MiniLoadBalancer
  - name: generate
    signature: (self, modified_request, prefill_server, decode_server, endpoint)
    return: ORJSONResponse
    class: MiniLoadBalancer
  - name: generate_stream
    signature: (self, modified_request, prefill_server, decode_server, endpoint = 'generate')
    class: MiniLoadBalancer
  - name: stream_results
    signature: ()
    class: MiniLoadBalancer
  - name: health_check
    signature: ()
  - name: health_check
    signature: ()
  - name: flush_cache
    signature: ()
  - name: get_server_info
    signature: ()
  - name: get_model_info
    signature: ()
  - name: handle_generate_request
    signature: (request_data: dict)
  - name: _forward_to_backend
    signature: (request_data: dict, endpoint_name: str)
  - name: handle_chat_completion_request
    signature: (request_data: dict)
  - name: handle_completion_request
    signature: (request_data: dict)
  - name: _generate_bootstrap_room
    signature: ()
  - name: _get_request_batch_size
    signature: (request)
  - name: get_models
    signature: ()
  - name: register
    signature: (obj: PDRegistryRequest)
  - name: run
    signature: (prefill_configs, decode_addrs, host, port, timeout)

File: disaggregation/mooncake/__init__.py
  (no function definitions found)
File: disaggregation/mooncake/conn.py
  - name: __init__
    signature: (self, bootstrap_room: int, failure_reason: str)
    class: KVTransferError
  - name: __str__
    signature: (self)
    class: KVTransferError
  - name: from_zmq
    signature: (cls, msg: List[bytes])
    class: TransferInfo
  - name: from_zmq
    signature: (cls, msg: List[bytes])
    class: KVArgsRegisterInfo
  - name: serialize_data_from_buffer
    signature: (src_addr, data_length)
    class: AuxDataCodec
    doc: Serialize data from memory buffer to bytes
  - name: deserialize_data_to_buffer
    signature: (kv_args, buffer_index, aux_index, data)
    class: AuxDataCodec
    doc: Deserialize bytes into target memory buffer
  - name: __init__
    signature: (self, args: KVArgs, disaggregation_mode: DisaggregationMode, server_args: ServerArgs, is_mla_backend: Optional[bool] = False)
    class: MooncakeKVManager
  - name: init_engine
    signature: (self)
    class: MooncakeKVManager
  - name: register_buffer_to_engine
    signature: (self)
    class: MooncakeKVManager
  - name: _connect
    signature: (self, endpoint: str, is_ipv6: bool = False)
    class: MooncakeKVManager
  - name: _transfer_data
    signature: (self, mooncake_session_id, transfer_blocks)
    class: MooncakeKVManager
  - name: send_kvcache
    signature: (self, mooncake_session_id: str, prefill_kv_indices: npt.NDArray[np.int32], dst_kv_ptrs: list[int], dst_kv_indices: npt.NDArray[np.int32], executor: concurrent.futures.ThreadPoolExecutor)
    class: MooncakeKVManager
  - name: set_transfer_blocks
    signature: (src_ptr: int, dst_ptr: int, item_len: int)
    return: List[Tuple[int, int, int]]
    class: MooncakeKVManager
  - name: process_layer
    signature: (src_ptr: int, dst_ptr: int, item_len: int)
    return: int
    class: MooncakeKVManager
  - name: process_layers
    signature: (layers_params: List[Tuple[int, int, int]])
    return: int
    class: MooncakeKVManager
  - name: send_kvcache_slice
    signature: (self, mooncake_session_id: str, prefill_kv_indices: npt.NDArray[np.int64], dst_kv_ptrs: list[int], dst_kv_indices: npt.NDArray[np.int64], dst_tp_rank: int, dst_attn_tp_size: int, dst_kv_item_len: int, executor: concurrent.futures.ThreadPoolExecutor)
    class: MooncakeKVManager
    doc: Sends KV cache slices from this Prefill rank to a target Decode rank,
  - name: process_layer_tp_aware
    signature: (layer_params)
    class: MooncakeKVManager
  - name: send_aux
    signature: (self, req: TransferInfo, prefill_aux_index: int, dst_aux_ptrs: list[int])
    class: MooncakeKVManager
  - name: send_aux_tcp
    signature: (self, req: TransferInfo, prefill_aux_index: int, dst_aux_ptrs: list[int])
    class: MooncakeKVManager
  - name: send_aux_data_to_endpoint
    signature: (self, remote: str, dst_port: int, room: int, buffer_index: int, aux_index: int, data: bytes)
    class: MooncakeKVManager
  - name: sync_status_to_decode_endpoint
    signature: (self, remote: str, dst_port: int, room: int, status: int, prefill_rank: int)
    class: MooncakeKVManager
  - name: transfer_worker
    signature: (self, queue: FastQueue, executor: concurrent.futures.ThreadPoolExecutor)
    class: MooncakeKVManager
  - name: _bind_server_socket
    signature: (self)
    class: MooncakeKVManager
  - name: start_prefill_thread
    signature: (self)
    class: MooncakeKVManager
  - name: bootstrap_thread
    signature: ()
    class: MooncakeKVManager
    doc: This thread recvs pre-alloc notification from the decode engine
  - name: _handle_aux_data
    signature: (self, msg: List[bytes])
    class: MooncakeKVManager
    doc: Handle AUX_DATA messages received by the decode thread.
  - name: start_decode_thread
    signature: (self)
    class: MooncakeKVManager
  - name: decode_thread
    signature: ()
    class: MooncakeKVManager
  - name: heartbeat_checker
    signature: ()
    class: MooncakeKVManager
  - name: add_transfer_request
    signature: (self, bootstrap_room: int, kv_indices: npt.NDArray[np.int32], index_slice: slice, is_last: bool, aux_index: Optional[int] = None)
    class: MooncakeKVManager
  - name: check_status
    signature: (self, bootstrap_room: int)
    class: MooncakeKVManager
  - name: update_status
    signature: (self, bootstrap_room: int, status: KVPoll)
    class: MooncakeKVManager
  - name: record_failure
    signature: (self, bootstrap_room: int, failure_reason: str)
    class: MooncakeKVManager
  - name: get_session_id
    signature: (self)
    class: MooncakeKVManager
  - name: _register_to_bootstrap
    signature: (self)
    class: MooncakeKVManager
    doc: Register KVSender to bootstrap server via HTTP POST.
  - name: _handle_node_failure
    signature: (self, failed_bootstrap_addr)
    class: MooncakeKVManager
  - name: __init__
    signature: (self, mgr: MooncakeKVManager, bootstrap_addr: str, bootstrap_room: int, dest_tp_ranks: List[int], pp_rank: int)
    class: MooncakeKVSender
  - name: init
    signature: (self, num_kv_indices: int, aux_index: Optional[int] = None)
    class: MooncakeKVSender
  - name: send
    signature: (self, kv_indices: npt.NDArray[np.int32])
    class: MooncakeKVSender
  - name: poll
    signature: (self)
    return: KVPoll
    class: MooncakeKVSender
  - name: clear
    signature: (self)
    return: None
    class: MooncakeKVSender
  - name: failure_exception
    signature: (self)
    class: MooncakeKVSender
  - name: abort
    signature: (self)
    class: MooncakeKVSender
  - name: __init__
    signature: (self, mgr: MooncakeKVManager, bootstrap_addr: str, bootstrap_room: Optional[int] = None, data_parallel_rank: Optional[int] = None)
    class: MooncakeKVReceiver
  - name: _get_bootstrap_info_from_server
    signature: (self, engine_rank, target_dp_group, target_pp_rank)
    class: MooncakeKVReceiver
    doc: Fetch the bootstrap info from the bootstrap server.
  - name: _get_prefill_parallel_info_from_server
    signature: (self)
    return: Tuple[Optional[int], Optional[int], Optional[int]]
    class: MooncakeKVReceiver
    doc: Fetch the prefill parallel info from the bootstrap server.
  - name: _register_kv_args
    signature: (self)
    class: MooncakeKVReceiver
  - name: _connect
    signature: (cls, endpoint: str, is_ipv6: bool = False)
    class: MooncakeKVReceiver
  - name: _connect_to_bootstrap_server
    signature: (cls, bootstrap_info: dict)
    class: MooncakeKVReceiver
  - name: init
    signature: (self, kv_indices: npt.NDArray[np.int32], aux_index: Optional[int] = None)
    class: MooncakeKVReceiver
  - name: poll
    signature: (self)
    return: KVPoll
    class: MooncakeKVReceiver
  - name: clear
    signature: (self)
    return: None
    class: MooncakeKVReceiver
  - name: failure_exception
    signature: (self)
    class: MooncakeKVReceiver
  - name: abort
    signature: (self)
    class: MooncakeKVReceiver
  - name: __init__
    signature: (self, port: int)
    class: MooncakeKVBootstrapServer
  - name: run
    signature: (self)
    class: MooncakeKVBootstrapServer
  - name: _setup_routes
    signature: (self)
    class: MooncakeKVBootstrapServer
  - name: _handle_health_check
    signature: (self, request)
    class: MooncakeKVBootstrapServer
  - name: _handle_route
    signature: (self, request: web.Request)
    class: MooncakeKVBootstrapServer
  - name: _handle_route_put
    signature: (self, request: web.Request)
    class: MooncakeKVBootstrapServer
  - name: _handle_route_get
    signature: (self, request: web.Request)
    class: MooncakeKVBootstrapServer
  - name: _run_server
    signature: (self)
    class: MooncakeKVBootstrapServer
  - name: close
    signature: (self)
    class: MooncakeKVBootstrapServer
    doc: Shutdown
  - name: poll
    signature: (self)
    return: KVPoll
    class: MooncakeKVBootstrapServer

File: disaggregation/mooncake/transfer_engine.py
  - name: __init__
    signature: (self, hostname: str, gpu_id: int, ib_device: Optional[str] = None)
    class: MooncakeTransferEngine
  - name: register
    signature: (self, ptr, length)
    class: MooncakeTransferEngine
  - name: deregister
    signature: (self, ptr)
    class: MooncakeTransferEngine
  - name: batch_register
    signature: (self, ptrs: List[int], lengths: List[int])
    return: int
    class: MooncakeTransferEngine
    doc: Batch register multiple memory regions.
  - name: batch_deregister
    signature: (self, ptrs: List[int])
    return: int
    class: MooncakeTransferEngine
    doc: Batch deregister multiple memory regions.
  - name: initialize
    signature: (self, hostname: str, device_name: Optional[str])
    return: None
    class: MooncakeTransferEngine
    doc: Initialize the mooncake instance.
  - name: transfer_sync
    signature: (self, session_id: str, buffer: int, peer_buffer_address: int, length: int)
    return: int
    class: MooncakeTransferEngine
    doc: Synchronously transfer data to the specified address.
  - name: batch_transfer_sync
    signature: (self, session_id: str, buffers: List[int], peer_buffer_addresses: List[int], lengths: List[int])
    return: int
    class: MooncakeTransferEngine
    doc: Synchronously transfer data to the specified addresses in batches.
  - name: get_session_id
    signature: (self)
    class: MooncakeTransferEngine

File: disaggregation/nixl/__init__.py
  (no function definitions found)
File: disaggregation/nixl/conn.py
  - name: is_dummy
    signature: (self)
    class: TransferInfo
  - name: from_zmq
    signature: (cls, msg: List[bytes])
    class: TransferInfo
  - name: from_zmq
    signature: (cls, msg: List[bytes])
    class: KVArgsRegisterInfo
  - name: is_done
    signature: (self)
    class: TransferStatus
  - name: __init__
    signature: (self, args: KVArgs, disaggregation_mode: DisaggregationMode, server_args: ServerArgs, is_mla_backend: Optional[bool] = False)
    class: NixlKVManager
  - name: check_status
    signature: (self, bootstrap_room: int)
    class: NixlKVManager
  - name: update_status
    signature: (self, bootstrap_room: int, status: KVPoll)
    class: NixlKVManager
  - name: register_buffer_to_engine
    signature: (self)
    class: NixlKVManager
  - name: _add_remote_peer
    signature: (self, decode_kv_args: KVArgsRegisterInfo)
    class: NixlKVManager
  - name: send_kvcache
    signature: (self, peer_name: str, prefill_kv_indices: npt.NDArray[np.int32], dst_kv_ptrs: list[int], dst_kv_indices: npt.NDArray[np.int32], dst_gpu_id: int, notif: str)
    class: NixlKVManager
  - name: send_aux
    signature: (self, peer_name: str, prefill_aux_index: int, dst_aux_ptrs: list[int], dst_aux_index: int, notif: str)
    class: NixlKVManager
  - name: add_transfer_request
    signature: (self, bootstrap_room: int, kv_indices: npt.NDArray[np.int32], index_slice: slice, is_last: bool, chunk_id: int, aux_index: Optional[int] = None)
    class: NixlKVManager
  - name: update_transfer_status
    signature: (self)
    class: NixlKVManager
  - name: check_transfer_done
    signature: (self, room: int)
    class: NixlKVManager
  - name: _bind_server_socket
    signature: (self)
    class: NixlKVManager
  - name: _start_bootstrap_thread
    signature: (self)
    class: NixlKVManager
  - name: bootstrap_thread
    signature: ()
    class: NixlKVManager
    doc: This thread recvs transfer info from the decode engine
  - name: __init__
    signature: (self, mgr: NixlKVManager, bootstrap_addr: str, bootstrap_room: int, dest_tp_ranks: List[int], pp_rank: int)
    class: NixlKVSender
  - name: init
    signature: (self, num_kv_indices: int, aux_index: Optional[int] = None)
    class: NixlKVSender
  - name: send
    signature: (self, kv_indices: npt.NDArray[np.int32])
    class: NixlKVSender
  - name: poll
    signature: (self)
    return: KVPoll
    class: NixlKVSender
  - name: failure_exception
    signature: (self)
    class: NixlKVSender
  - name: __init__
    signature: (self, mgr: NixlKVManager, bootstrap_addr: str, bootstrap_room: Optional[int] = None, data_parallel_rank: Optional[int] = None)
    class: NixlKVReceiver
  - name: init
    signature: (self, kv_indices: npt.NDArray[np.int32], aux_index: Optional[int] = None)
    class: NixlKVReceiver
  - name: poll
    signature: (self)
    return: KVPoll
    class: NixlKVReceiver
  - name: _register_kv_args
    signature: (self)
    class: NixlKVReceiver
  - name: failure_exception
    signature: (self)
    class: NixlKVReceiver

File: disaggregation/prefill.py
  - name: __init__
    signature: (self, token_to_kv_pool: KVCache, draft_token_to_kv_pool: Optional[KVCache], req_to_metadata_buffer_idx_allocator: ReqToMetadataIdxAllocator, metadata_buffers: MetadataBuffers, tp_rank: int, tp_size: int, gpu_id: int, bootstrap_port: int, gloo_group: ProcessGroup, max_total_num_tokens: int, decode_tp_size: int, decode_dp_size: int, scheduler: Scheduler, pp_rank: int, pp_size: int, transfer_backend: TransferBackend)
    class: PrefillBootstrapQueue
  - name: _init_kv_manager
    signature: (self)
    return: BaseKVManager
    class: PrefillBootstrapQueue
  - name: add
    signature: (self, req: Req, num_kv_heads: int)
    return: None
    class: PrefillBootstrapQueue
  - name: extend
    signature: (self, reqs: List[Req], num_kv_heads: int)
    return: None
    class: PrefillBootstrapQueue
  - name: _check_if_req_exceed_kv_capacity
    signature: (self, req: Req)
    return: bool
    class: PrefillBootstrapQueue
  - name: _process_req
    signature: (self, req: Req)
    return: None
    class: PrefillBootstrapQueue
    doc: Set max_new_tokens = 1, so PrefillAdder memory estimation is accurate
  - name: pop_bootstrapped
    signature: (self, return_failed_reqs: bool = False, rids_to_check: Optional[List[str]] = None)
    return: List[Req]
    class: PrefillBootstrapQueue
    doc: pop the reqs which has finished bootstrapping
  - name: event_loop_normal_disagg_prefill
    signature: (self: Scheduler)
    return: None
    class: SchedulerDisaggregationPrefillMixin
    doc: A normal scheduler loop for prefill worker in disaggregation mode.
  - name: event_loop_overlap_disagg_prefill
    signature: (self: Scheduler)
    return: None
    class: SchedulerDisaggregationPrefillMixin
  - name: process_batch_result_disagg_prefill
    signature: (self: Scheduler, batch: ScheduleBatch, result: GenerationBatchResult, launch_done: Optional[threading.Event] = None)
    return: None
    class: SchedulerDisaggregationPrefillMixin
    doc: Transfer kv for prefill completed requests and add it into disagg_prefill_inflight_queue
  - name: process_disagg_prefill_inflight_queue
    signature: (self: Scheduler, rids_to_check: Optional[List[str]] = None)
    return: List[Req]
    class: SchedulerDisaggregationPrefillMixin
    doc: Poll the requests in the middle of transfer. If done, return the request.
  - name: get_transferred_rids
    signature: (self: Scheduler)
    return: List[str]
    class: SchedulerDisaggregationPrefillMixin
    doc: Used by PP, get the transferred rids but **do not pop**
  - name: process_prefill_chunk
    signature: (self: Scheduler)
    return: None
    class: SchedulerDisaggregationPrefillMixin
  - name: send_kv_chunk
    signature: (self: Scheduler, req: Req, last_chunk: bool = False, end_idx: Optional[int] = None)
    return: None
    class: SchedulerDisaggregationPrefillMixin
    doc: Send a prefilled chunk to the decode server
  - name: event_loop_pp_disagg_prefill
    signature: (self: Scheduler)
    class: SchedulerDisaggregationPrefillMixin
    doc: An event loop for the prefill server in pipeline parallelism.
  - name: send_pyobj_to_next_stage
    signature: (self, data)
    class: SchedulerDisaggregationPrefillMixin
  - name: recv_pyobj_from_prev_stage
    signature: (self)
    class: SchedulerDisaggregationPrefillMixin

File: disaggregation/utils.py
  - name: poll_and_all_reduce
    signature: (pollers, gloo_group)
  - name: __init__
    signature: (self, size: int)
    class: ReqToMetadataIdxAllocator
  - name: available_size
    signature: (self)
    class: ReqToMetadataIdxAllocator
  - name: alloc
    signature: (self)
    return: Optional[int]
    class: ReqToMetadataIdxAllocator
  - name: free
    signature: (self, free_index: int)
    class: ReqToMetadataIdxAllocator
  - name: __init__
    signature: (self, size: int, hidden_size: int, dtype: torch.dtype, max_top_logprobs_num: int = 128, custom_mem_pool: torch.cuda.MemPool = None)
    class: MetadataBuffers
  - name: get_buf_infos
    signature: (self)
    class: MetadataBuffers
  - name: get_buf
    signature: (self, idx: int)
    class: MetadataBuffers
  - name: set_buf
    signature: (self, req: Req)
    class: MetadataBuffers
  - name: get_kv_class
    signature: (transfer_backend: TransferBackend, class_type: KVClassType)
  - name: kv_to_page_indices
    signature: (kv_indices: np.ndarray, page_size: int)
  - name: kv_to_page_num
    signature: (num_kv_indices: int, page_size: int)
  - name: __post_init__
    signature: (self)
    class: PDRegistryRequest
  - name: register_disaggregation_server
    signature: (mode: str, server_port: int, bootstrap_port: int, pdlb_url: str)
  - name: is_mla_backend
    signature: (target_kv_pool)
    return: bool
  - name: prepare_abort
    signature: (req: Req, error_message: str, status_code = None)

File: distributed/__init__.py
  (no function definitions found)
File: distributed/communication_op.py
  - name: tensor_model_parallel_all_reduce
    signature: (input_: torch.Tensor)
    return: torch.Tensor
    doc: All-reduce the input tensor across model parallel group.
  - name: tensor_model_parallel_all_gather
    signature: (input_: torch.Tensor, dim: int = -1)
    return: torch.Tensor
    doc: All-gather the input tensor across model parallel group.
  - name: tensor_model_parallel_gather
    signature: (input_: torch.Tensor, dst: int = 0, dim: int = -1)
    return: Optional[torch.Tensor]
    doc: Gather the input tensor across model parallel group.
  - name: broadcast_tensor_dict
    signature: (tensor_dict: Optional[Dict[Any, Union[torch.Tensor, Any]]] = None, src: int = 0)

File: distributed/device_communicators/cuda_wrapper.py
  - name: find_loaded_library
    signature: (lib_name)
    return: Optional[str]
    doc: According to according to https://man7.org/linux/man-pages/man5/proc_pid_maps.5.html,
  - name: __init__
    signature: (self, so_file: Optional[str] = None)
    class: CudaRTLibrary
  - name: CUDART_CHECK
    signature: (self, result: cudaError_t)
    return: None
    class: CudaRTLibrary
  - name: cudaGetErrorString
    signature: (self, error: cudaError_t)
    return: str
    class: CudaRTLibrary
  - name: cudaSetDevice
    signature: (self, device: int)
    return: None
    class: CudaRTLibrary
  - name: cudaDeviceSynchronize
    signature: (self)
    return: None
    class: CudaRTLibrary
  - name: cudaDeviceReset
    signature: (self)
    return: None
    class: CudaRTLibrary
  - name: cudaMalloc
    signature: (self, size: int)
    return: ctypes.c_void_p
    class: CudaRTLibrary
  - name: cudaFree
    signature: (self, devPtr: ctypes.c_void_p)
    return: None
    class: CudaRTLibrary
  - name: cudaMemset
    signature: (self, devPtr: ctypes.c_void_p, value: int, count: int)
    return: None
    class: CudaRTLibrary
  - name: cudaMemcpy
    signature: (self, dst: ctypes.c_void_p, src: ctypes.c_void_p, count: int)
    return: None
    class: CudaRTLibrary
  - name: cudaIpcGetMemHandle
    signature: (self, devPtr: ctypes.c_void_p)
    return: cudaIpcMemHandle_t
    class: CudaRTLibrary
  - name: cudaIpcOpenMemHandle
    signature: (self, handle: cudaIpcMemHandle_t)
    return: ctypes.c_void_p
    class: CudaRTLibrary

File: distributed/device_communicators/custom_all_reduce.py
  - name: _can_p2p
    signature: (rank: int, world_size: int)
    return: bool
  - name: __init__
    signature: (self, group: ProcessGroup, device: Union[int, str, torch.device], max_size = _MAX_CAR_SIZE)
    return: None
    class: CustomAllreduce
    doc: Args:
  - name: create_shared_buffer
    signature: (size_in_bytes: int, group: Optional[ProcessGroup] = None)
    return: List[int]
    class: CustomAllreduce
    doc: Creates a shared buffer and returns a list of pointers
  - name: free_shared_buffer
    signature: (pointers: List[int], group: Optional[ProcessGroup] = None)
    return: None
    class: CustomAllreduce
  - name: capture
    signature: (self)
    class: CustomAllreduce
    doc: The main responsibility of this context manager is the
  - name: _get_ipc_meta
    signature: (self, inp: torch.Tensor)
    class: CustomAllreduce
  - name: _gather_ipc_meta
    signature: (self, shard_data)
    class: CustomAllreduce
  - name: register_buffer
    signature: (self, inp: torch.Tensor)
    class: CustomAllreduce
  - name: register_graph_buffers
    signature: (self)
    class: CustomAllreduce
  - name: should_custom_ar
    signature: (self, inp: torch.Tensor)
    class: CustomAllreduce
  - name: all_reduce_reg
    signature: (self, inp: torch.Tensor, out: torch.Tensor = None)
    class: CustomAllreduce
  - name: all_reduce_unreg
    signature: (self, inp: torch.Tensor, out: torch.Tensor = None)
    class: CustomAllreduce
  - name: all_reduce
    signature: (self, inp: torch.Tensor, *, out: torch.Tensor = None, registered: bool = False)
    class: CustomAllreduce
    doc: Performs an out-of-place all reduce.
  - name: custom_all_reduce
    signature: (self, input: torch.Tensor)
    return: Optional[torch.Tensor]
    class: CustomAllreduce
    doc: The main allreduce API that provides support for cuda graph.
  - name: close
    signature: (self)
    class: CustomAllreduce
  - name: __del__
    signature: (self)
    class: CustomAllreduce

File: distributed/device_communicators/custom_all_reduce_utils.py
  - name: update_environment_variables
    signature: (envs: Dict[str, str])
  - name: producer
    signature: (batch_src: Sequence[int], producer_queue, consumer_queue, result_queue, cuda_visible_devices: Optional[str] = None)
  - name: consumer
    signature: (batch_tgt: Sequence[int], producer_queue, consumer_queue, result_queue, cuda_visible_devices: Optional[str] = None)
  - name: can_actually_p2p
    signature: (batch_src: Sequence[int], batch_tgt: Sequence[int])
    return: Sequence[bool]
    doc: Usually, checking if P2P access is enabled can be done by
  - name: gpu_p2p_access_check
    signature: (src: int, tgt: int)
    return: bool
    doc: Check if GPU src can access GPU tgt.
  - name: with_nvml_context
    signature: (fn: Callable[_P, _R])
    return: Callable[_P, _R]
  - name: wrapper
    signature: (*args: _P.args, **kwargs: _P.kwargs)
    return: _R
  - name: is_full_nvlink
    signature: (physical_device_ids: List[int], world_size: int)
    return: bool
  - name: is_weak_contiguous
    signature: (inp: torch.Tensor)

File: distributed/device_communicators/hpu_communicator.py
  - name: __init__
    signature: (self, group: ProcessGroup)
    class: HpuCommunicator
  - name: all_reduce
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: HpuCommunicator
  - name: all_gather
    signature: (self, x: torch.Tensor, dim: int = -1)
    return: torch.Tensor
    class: HpuCommunicator

File: distributed/device_communicators/npu_communicator.py
  - name: __init__
    signature: (self, group: ProcessGroup)
    class: NpuCommunicator
  - name: all_reduce
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: NpuCommunicator
  - name: all_gather
    signature: (self, x: torch.Tensor, dim: int = -1)
    return: torch.Tensor
    class: NpuCommunicator

File: distributed/device_communicators/pymscclpp.py
  - name: mscclpp_is_weak_contiguous
    signature: (inp: torch.Tensor)
  - name: mscclpp_convert_to_bytes
    signature: (size_str)
    doc: Converts a human-readable size string (e.g., "1MB", "2.5kb", "3 GB")
  - name: mscclpp_bench_time
    signature: (func, test_niter: int = 10, warmup_niter: int = 2)
  - name: __init__
    signature: (self, group: ProcessGroup, device: Union[int, str, torch.device], max_bytes = _MAX_BYTES)
    return: None
    class: PyMscclppCommunicator
    doc: Args:
  - name: pre_tune_config
    signature: (self, dtype = torch.bfloat16)
    return: bool
    class: PyMscclppCommunicator
  - name: should_mscclpp_allreduce
    signature: (self, inp: torch.Tensor, op: ReduceOp = ReduceOp.SUM)
    return: bool
    class: PyMscclppCommunicator
  - name: all_reduce
    signature: (self, tensor: torch.Tensor, op: ReduceOp = ReduceOp.SUM)
    class: PyMscclppCommunicator
  - name: change_state
    signature: (self, enable: Optional[bool] = None)
    class: PyMscclppCommunicator

File: distributed/device_communicators/pynccl.py
  - name: __init__
    signature: (self, group: Union[ProcessGroup, StatelessProcessGroup], device: Union[int, str, torch.device], library_path: Optional[str] = None)
    class: PyNcclCommunicator
    doc: Args:
  - name: all_reduce
    signature: (self, tensor: torch.Tensor, op: ReduceOp = ReduceOp.SUM, stream = None)
    class: PyNcclCommunicator
  - name: all_gather
    signature: (self, output_tensor: torch.Tensor, input_tensor: torch.Tensor, stream = None, sizes: Optional[list[int]] = None)
    class: PyNcclCommunicator
  - name: reduce_scatter
    signature: (self, output_tensor: torch.Tensor, input_tensor: torch.Tensor, op: ReduceOp = ReduceOp.SUM, stream = None, sizes: Optional[list[int]] = None)
    class: PyNcclCommunicator
  - name: send
    signature: (self, tensor: torch.Tensor, dst: int, stream = None)
    class: PyNcclCommunicator
  - name: recv
    signature: (self, tensor: torch.Tensor, src: int, stream = None)
    class: PyNcclCommunicator
  - name: broadcast
    signature: (self, tensor: torch.Tensor, src: int, stream = None)
    class: PyNcclCommunicator
  - name: register_comm_window_raw
    signature: (self, ptr: int, size: int)
    class: PyNcclCommunicator
  - name: deregister_comm_window
    signature: (self, window)
    class: PyNcclCommunicator
  - name: group_start
    signature: (self)
    class: PyNcclCommunicator
  - name: group_end
    signature: (self)
    class: PyNcclCommunicator
  - name: change_state
    signature: (self, enable: Optional[bool] = None, stream: Optional[torch.cuda.Stream] = None)
    class: PyNcclCommunicator
    doc: A context manager to change the state of the communicator.

File: distributed/device_communicators/pynccl_allocator.py
  - name: is_symmetric_memory_enabled
    signature: ()
  - name: set_graph_pool_id
    signature: (graph_pool_id)
  - name: get_nccl_mem_pool
    signature: ()
  - name: __init__
    signature: (self, group_coordinator: GroupCoordinator)
    class: use_symmetric_memory
  - name: __enter__
    signature: (self)
    class: use_symmetric_memory
  - name: tag
    signature: (self, tensor: torch.Tensor)
    class: use_symmetric_memory
  - name: __exit__
    signature: (self, exc_type, exc_val, exc_tb)
    class: use_symmetric_memory

File: distributed/device_communicators/pynccl_wrapper.py
  - name: find_nccl_library
    signature: ()
    return: str
    doc: We either use the library file specified by the `SGLANG_NCCL_SO_PATH`
  - name: from_torch
    signature: (cls, dtype: torch.dtype)
    return: int
    class: ncclDataTypeEnum
  - name: from_torch
    signature: (cls, op: ReduceOp)
    return: int
    class: ncclRedOpTypeEnum
  - name: __init__
    signature: (self, so_file: Optional[str] = None)
    class: NCCLLibrary
  - name: ncclGetErrorString
    signature: (self, result: ncclResult_t)
    return: str
    class: NCCLLibrary
  - name: NCCL_CHECK
    signature: (self, result: ncclResult_t)
    return: None
    class: NCCLLibrary
  - name: ncclGetRawVersion
    signature: (self)
    return: int
    class: NCCLLibrary
  - name: ncclGetVersion
    signature: (self)
    return: str
    class: NCCLLibrary
  - name: ncclGetUniqueId
    signature: (self)
    return: ncclUniqueId
    class: NCCLLibrary
  - name: ncclCommInitRank
    signature: (self, world_size: int, unique_id: ncclUniqueId, rank: int)
    return: ncclComm_t
    class: NCCLLibrary
  - name: ncclAllReduce
    signature: (self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, comm: ncclComm_t, stream: cudaStream_t)
    return: None
    class: NCCLLibrary
  - name: ncclReduce
    signature: (self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, root: int, comm: ncclComm_t, stream: cudaStream_t)
    return: None
    class: NCCLLibrary
  - name: ncclReduceScatter
    signature: (self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, comm: ncclComm_t, stream: cudaStream_t)
    return: None
    class: NCCLLibrary
  - name: ncclAllGather
    signature: (self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, comm: ncclComm_t, stream: cudaStream_t)
    return: None
    class: NCCLLibrary
  - name: ncclSend
    signature: (self, sendbuff: buffer_type, count: int, datatype: int, dest: int, comm: ncclComm_t, stream: cudaStream_t)
    return: None
    class: NCCLLibrary
  - name: ncclRecv
    signature: (self, recvbuff: buffer_type, count: int, datatype: int, src: int, comm: ncclComm_t, stream: cudaStream_t)
    return: None
    class: NCCLLibrary
  - name: ncclBroadcast
    signature: (self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, root: int, comm: ncclComm_t, stream: cudaStream_t)
    return: None
    class: NCCLLibrary
  - name: ncclCommDestroy
    signature: (self, comm: ncclComm_t)
    return: None
    class: NCCLLibrary
  - name: ncclCommWindowRegister
    signature: (self, comm: ncclComm_t, buff: buffer_type, size: int, win_flags: int)
    return: ncclWindow_t
    class: NCCLLibrary
  - name: ncclCommWindowDeregister
    signature: (self, comm: ncclComm_t, window: ncclWindow_t)
    return: None
    class: NCCLLibrary
  - name: ncclGroupStart
    signature: (self)
    return: None
    class: NCCLLibrary
  - name: ncclGroupEnd
    signature: (self)
    return: None
    class: NCCLLibrary

File: distributed/device_communicators/quick_all_reduce.py
  - name: qr_rocm_arch_available
    signature: ()
  - name: __init__
    signature: (self, group: ProcessGroup, device: Union[int, str, torch.device])
    return: None
    class: QuickAllReduce
    doc: Custom allreduce provides non-destructive acceleration and is
  - name: init_quick_all_reduce
    signature: (self)
    class: QuickAllReduce
  - name: create_shared_buffer
    signature: (self)
    class: QuickAllReduce
    doc: Creates a shared buffer for quickreduce.
  - name: should_quick_allreduce
    signature: (self, inp: torch.Tensor)
    class: QuickAllReduce
    doc: Check if quickreduce is available
  - name: quick_all_reduce
    signature: (self, inp: torch.Tensor, *, out: torch.Tensor = None)
    class: QuickAllReduce
    doc: Performs an out-of-place custom quick all reduce.
  - name: close
    signature: (self)
    class: QuickAllReduce
  - name: __del__
    signature: (self)
    class: QuickAllReduce

File: distributed/device_communicators/shm_broadcast.py
  - name: __init__
    signature: (self, n_reader: int, max_chunk_bytes: int, max_chunks: int, name: Optional[str] = None)
    class: ShmRingBuffer
    doc: A shared memory ring buffer implementation for broadcast communication.
  - name: __reduce__
    signature: (self)
    class: ShmRingBuffer
  - name: __del__
    signature: (self)
    class: ShmRingBuffer
  - name: get_data
    signature: (self, current_idx: int)
    class: ShmRingBuffer
  - name: get_metadata
    signature: (self, current_idx: int)
    class: ShmRingBuffer
  - name: __init__
    signature: (self, n_reader, n_local_reader, local_reader_ranks: Optional[List[int]] = None, max_chunk_bytes: int = 1024 * 1024 * 10, max_chunks: int = 10, connect_ip: Optional[str] = None)
    class: MessageQueue
  - name: export_handle
    signature: (self)
    return: Handle
    class: MessageQueue
  - name: create_from_handle
    signature: (handle: Handle, rank)
    return: 'MessageQueue'
    class: MessageQueue
  - name: wait_until_ready
    signature: (self)
    class: MessageQueue
    doc: This is a collective operation. All processes (including the
  - name: acquire_write
    signature: (self)
    class: MessageQueue
  - name: acquire_read
    signature: (self)
    class: MessageQueue
  - name: enqueue
    signature: (self, obj)
    class: MessageQueue
  - name: dequeue
    signature: (self)
    class: MessageQueue
  - name: broadcast_object
    signature: (self, obj = None)
    class: MessageQueue
  - name: create_from_process_group
    signature: (pg: ProcessGroup, max_chunk_bytes, max_chunks, writer_rank = 0)
    return: 'MessageQueue'
    class: MessageQueue

File: distributed/device_communicators/xpu_communicator.py
  - name: __init__
    signature: (self, group: ProcessGroup)
    class: XpuCommunicator
  - name: all_reduce
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: XpuCommunicator
  - name: gather
    signature: (self, input_: torch.Tensor, rank_in_group: int, dst: int = 0, dim: int = -1)
    class: XpuCommunicator

File: distributed/naive_distributed.py
  - name: __init__
    signature: (self, rank: int, world_size: int, rendezvous: str)
    class: NaiveDistributed
  - name: get_rank
    signature: (self)
    class: NaiveDistributed
  - name: get_world_size
    signature: (self)
    class: NaiveDistributed
  - name: scatter
    signature: (self, tensor: torch.Tensor, scatter_list: List[torch.Tensor], src: int = 0)
    class: NaiveDistributed
  - name: all_gather_object
    signature: (self, obj: Any)
    return: List[Any]
    class: NaiveDistributed
  - name: _get_path
    signature: (interesting_rank: int)
    class: NaiveDistributed
  - name: _read_one
    signature: (interesting_rank: int)
    class: NaiveDistributed
  - name: barrier
    signature: (self)
    class: NaiveDistributed
  - name: get_naive_distributed
    signature: ()
  - name: set_naive_distributed
    signature: (instance: NaiveDistributed)

File: distributed/parallel_state.py
  - name: _split_tensor_dict
    signature: (tensor_dict: Dict[str, Union[torch.Tensor, Any]])
    return: Tuple[List[Tuple[str, Any]], List[torch.Tensor]]
    doc: Split the tensor dictionary into two parts:
  - name: _get_unique_name
    signature: (name: str)
    return: str
    doc: Get a unique name for the group.
  - name: _register_group
    signature: (group: 'GroupCoordinator')
    return: None
  - name: inplace_all_reduce
    signature: (tensor: torch.Tensor, group_name: str)
    return: None
  - name: inplace_all_reduce_fake
    signature: (tensor: torch.Tensor, group_name: str)
    return: None
  - name: outplace_all_reduce
    signature: (tensor: torch.Tensor, group_name: str, outplace_all_reduce_method: str)
    return: torch.Tensor
  - name: outplace_all_reduce_fake
    signature: (tensor: torch.Tensor, group_name: str, outplace_all_reduce_method: str)
    return: torch.Tensor
  - name: reg_all_gather_into_tensor
    signature: (output: torch.Tensor, input: torch.Tensor, group_name: str)
    return: None
  - name: reg_all_gather_into_tensor_fake
    signature: (output: torch.Tensor, input: torch.Tensor, group_name: str)
    return: None
  - name: __init__
    signature: (self, group_ranks: List[List[int]], local_rank: int, torch_distributed_backend: Union[str, Backend], use_pynccl: bool, use_pymscclpp: bool, use_custom_allreduce: bool, use_hpu_communicator: bool, use_xpu_communicator: bool, use_npu_communicator: bool, use_message_queue_broadcaster: bool = False, group_name: Optional[str] = None)
    class: GroupCoordinator
  - name: __repr__
    signature: (self)
    class: GroupCoordinator
  - name: first_rank
    signature: (self)
    class: GroupCoordinator
    doc: Return the global rank of the first process in the group
  - name: last_rank
    signature: (self)
    class: GroupCoordinator
    doc: Return the global rank of the last process in the group
  - name: is_first_rank
    signature: (self)
    class: GroupCoordinator
    doc: Return whether the caller is the first process in the group
  - name: is_last_rank
    signature: (self)
    class: GroupCoordinator
    doc: Return whether the caller is the last process in the group
  - name: next_rank
    signature: (self)
    class: GroupCoordinator
    doc: Return the global rank of the process that follows the caller
  - name: prev_rank
    signature: (self)
    class: GroupCoordinator
    doc: Return the global rank of the process that precedes the caller
  - name: graph_capture
    signature: (self, graph_capture_context: Optional[GraphCaptureContext] = None)
    class: GroupCoordinator
  - name: all_reduce
    signature: (self, input_: torch.Tensor)
    return: torch.Tensor
    class: GroupCoordinator
    doc: User-facing all-reduce function before we actually call the
  - name: _all_reduce_out_place
    signature: (self, input_: torch.Tensor, outplace_all_reduce_method: str)
    return: torch.Tensor
    class: GroupCoordinator
  - name: _all_reduce_in_place
    signature: (self, input_: torch.Tensor)
    return: None
    class: GroupCoordinator
  - name: reduce_scatter_tensor
    signature: (self, output: torch.Tensor, input: torch.Tensor)
    return: None
    class: GroupCoordinator
  - name: reduce_scatter
    signature: (self, output: torch.Tensor, input_list: List[torch.Tensor])
    return: None
    class: GroupCoordinator
  - name: reduce_scatterv
    signature: (self, input_: torch.Tensor, output: Optional[torch.Tensor] = None, sizes: Optional[List[int]] = None)
    return: torch.Tensor
    class: GroupCoordinator
  - name: _all_gather_into_tensor
    signature: (self, output: torch.Tensor, input: torch.Tensor)
    class: GroupCoordinator
  - name: all_gather_into_tensor
    signature: (self, output: torch.Tensor, input: torch.Tensor)
    class: GroupCoordinator
  - name: all_gather
    signature: (self, input_: torch.Tensor, dim: int = -1, output_tensor_list: Optional[List[torch.Tensor]] = None)
    return: torch.Tensor
    class: GroupCoordinator
  - name: all_gatherv
    signature: (self, input_: Union[torch.Tensor, List[torch.Tensor]], sizes: Optional[List[int]] = None)
    return: Union[torch.Tensor, List[torch.Tensor]]
    class: GroupCoordinator
    doc: Supports varying sizes per rank and input tensor list.
  - name: _all_gather_single
    signature: (input_: torch.Tensor, sizes: Optional[List[int]] = None)
    class: GroupCoordinator
  - name: gather
    signature: (self, input_: torch.Tensor, dst: int = 0, dim: int = -1)
    return: Optional[torch.Tensor]
    class: GroupCoordinator
    doc: NOTE: We assume that the input tensor is on the same device across
  - name: broadcast
    signature: (self, input_: torch.Tensor, src: int = 0)
    class: GroupCoordinator
    doc: Broadcast the input tensor.
  - name: broadcast_object
    signature: (self, obj: Optional[Any] = None, src: int = 0)
    class: GroupCoordinator
    doc: Broadcast the input object.
  - name: broadcast_object_list
    signature: (self, obj_list: List[Any], src: int = 0, group: Optional[ProcessGroup] = None)
    class: GroupCoordinator
    doc: Broadcast the input object list.
  - name: all_gather_object
    signature: (self, obj: Any)
    return: List[Any]
    class: GroupCoordinator
  - name: send_object
    signature: (self, obj: Any, dst: int)
    return: None
    class: GroupCoordinator
    doc: Send the input object list to the destination rank.
  - name: recv_object
    signature: (self, src: int)
    return: Any
    class: GroupCoordinator
    doc: Receive the input object list from the source rank.
  - name: broadcast_tensor_dict
    signature: (self, tensor_dict: Optional[Dict[str, Union[torch.Tensor, Any]]] = None, src: int = 0, group: Optional[ProcessGroup] = None, metadata_group: Optional[ProcessGroup] = None)
    return: Optional[Dict[str, Union[torch.Tensor, Any]]]
    class: GroupCoordinator
    doc: Broadcast the input tensor dictionary.
  - name: send_tensor_dict
    signature: (self, tensor_dict: Dict[str, Union[torch.Tensor, Any]], dst: Optional[int] = None, all_gather_group: Optional['GroupCoordinator'] = None)
    return: Optional[Dict[str, Union[torch.Tensor, Any]]]
    class: GroupCoordinator
    doc: Send the input tensor dictionary.
  - name: recv_tensor_dict
    signature: (self, src: Optional[int] = None, all_gather_group: Optional['GroupCoordinator'] = None)
    return: Optional[Dict[str, Union[torch.Tensor, Any]]]
    class: GroupCoordinator
    doc: Recv the input tensor dictionary.
  - name: barrier
    signature: (self)
    class: GroupCoordinator
    doc: Barrier synchronization among the group.
  - name: send
    signature: (self, tensor: torch.Tensor, dst: Optional[int] = None)
    return: None
    class: GroupCoordinator
    doc: Sends a tensor to the destination rank in a non-blocking way
  - name: recv
    signature: (self, size: torch.Size, dtype: torch.dtype, src: Optional[int] = None)
    return: torch.Tensor
    class: GroupCoordinator
    doc: Receives a tensor from the source rank.
  - name: destroy
    signature: (self)
    class: GroupCoordinator
  - name: get_world_group
    signature: ()
    return: GroupCoordinator
  - name: init_world_group
    signature: (ranks: List[int], local_rank: int, backend: str)
    return: GroupCoordinator
  - name: init_model_parallel_group
    signature: (group_ranks: List[List[int]], local_rank: int, backend: str, use_custom_allreduce: Optional[bool] = None, use_message_queue_broadcaster: bool = False, group_name: Optional[str] = None, use_mscclpp_allreduce: Optional[bool] = None)
    return: GroupCoordinator
  - name: set_pdmux_status
    signature: (enable_prefill_multiplexing: bool)
  - name: get_tp_group
    signature: ()
    return: GroupCoordinator
  - name: get_moe_ep_group
    signature: ()
    return: GroupCoordinator
  - name: get_moe_tp_group
    signature: ()
    return: GroupCoordinator
  - name: get_pp_group
    signature: ()
    return: GroupCoordinator
  - name: graph_capture
    signature: ()
    doc: `graph_capture` is a context manager which should surround the code that
  - name: set_custom_all_reduce
    signature: (enable: bool)
  - name: set_mscclpp_all_reduce
    signature: (enable: bool)
  - name: init_distributed_environment
    signature: (world_size: int = -1, rank: int = -1, distributed_init_method: str = 'env://', local_rank: int = -1, backend: str = 'nccl', timeout: Optional[int] = None)
  - name: initialize_model_parallel
    signature: (tensor_model_parallel_size: int = 1, expert_model_parallel_size: int = 1, pipeline_model_parallel_size: int = 1, backend: Optional[str] = None, duplicate_tp_group: bool = False)
    return: None
    doc: Initialize model parallel groups.
  - name: ensure_model_parallel_initialized
    signature: (tensor_model_parallel_size: int, expert_model_parallel_size: int, pipeline_model_parallel_size: int, backend: Optional[str] = None)
    return: None
    doc: Helper to initialize model parallel groups if they are not initialized,
  - name: model_parallel_is_initialized
    signature: ()
    doc: Check if tensor and pipeline parallel groups are initialized.
  - name: patch_tensor_parallel_group
    signature: (tp_group: GroupCoordinator)
    doc: Patch the tp group temporarily until this function ends.
  - name: get_tensor_model_parallel_world_size
    signature: ()
    doc: Return world size for the tensor model parallel group.
  - name: get_tensor_model_parallel_rank
    signature: ()
    doc: Return my rank for the tensor model parallel group.
  - name: get_moe_expert_parallel_world_size
    signature: ()
    doc: Return world size for the moe expert parallel group.
  - name: get_moe_expert_parallel_rank
    signature: ()
    doc: Return my rank for the moe expert parallel group.
  - name: get_moe_tensor_parallel_world_size
    signature: ()
    doc: Return world size for the moe tensor parallel group.
  - name: get_moe_tensor_parallel_rank
    signature: ()
    doc: Return my rank for the moe tensor parallel group.
  - name: destroy_model_parallel
    signature: ()
    doc: Set the groups to none and destroy them.
  - name: destroy_distributed_environment
    signature: ()
  - name: cleanup_dist_env_and_memory
    signature: (shutdown_ray: bool = False)
  - name: in_the_same_node_as
    signature: (pg: ProcessGroup, source_rank: int = 0)
    return: List[bool]
    doc: This is a collective operation that returns if each rank is in the same node
  - name: monkey_patch_vllm_parallel_state
    signature: (reverse: bool = False)

File: distributed/utils.py
  - name: ensure_divisibility
    signature: (numerator, denominator)
    doc: Ensure that numerator is divisible by the denominator.
  - name: divide
    signature: (numerator, denominator)
    doc: Ensure that numerator is divisible by the denominator and return
  - name: split_tensor_along_last_dim
    signature: (tensor: torch.Tensor, num_partitions: int, contiguous_split_chunks: bool = False)
    return: Sequence[torch.Tensor]
    doc: Split a tensor along its last dimension.
  - name: get_pp_indices
    signature: (num_hidden_layers: int, pp_rank: int, pp_size: int)
    return: Tuple[int, int]
    doc: Try to evenly distribute layers across partitions.
  - name: __post_init__
    signature: (self)
    class: StatelessProcessGroup
  - name: send_obj
    signature: (self, obj: Any, dst: int)
    class: StatelessProcessGroup
    doc: Send an object to a destination rank.
  - name: expire_data
    signature: (self)
    class: StatelessProcessGroup
    doc: Expire data that is older than `data_expiration_seconds` seconds.
  - name: recv_obj
    signature: (self, src: int)
    return: Any
    class: StatelessProcessGroup
    doc: Receive an object from a source rank.
  - name: broadcast_obj
    signature: (self, obj: Optional[Any], src: int)
    return: Any
    class: StatelessProcessGroup
    doc: Broadcast an object from a source rank to all other ranks.
  - name: all_gather_obj
    signature: (self, obj: Any)
    return: list[Any]
    class: StatelessProcessGroup
    doc: All gather an object from all ranks.
  - name: barrier
    signature: (self)
    class: StatelessProcessGroup
    doc: A barrier to synchronize all ranks.
  - name: create
    signature: (host: str, port: int, rank: int, world_size: int, data_expiration_seconds: int = 3600)
    return: 'StatelessProcessGroup'
    class: StatelessProcessGroup
    doc: A replacement for `torch.distributed.init_process_group` that does not

File: eplb/__init__.py
  (no function definitions found)
File: eplb/eplb_algorithms/__init__.py
  - name: rebalance_experts
    signature: (tokens_per_expert: torch.Tensor, num_physical_experts: int, num_local_physical_experts: int, num_groups: Optional[int], num_nodes: int, algorithm: EplbAlgorithm)
  - name: compute_algorithm
    signature: (raw_algorithm: str, num_groups: Optional[int], num_nodes: int)
    return: EplbAlgorithm

File: eplb/eplb_algorithms/deepseek.py
  - name: balanced_packing
    signature: (weight: torch.Tensor, num_packs: int)
    return: Tuple[torch.Tensor, torch.Tensor]
    doc: Pack n weighted objects to m packs, such that each bin contains exactly n/m objects and the weights of all packs
  - name: replicate_experts
    signature: (weight: torch.Tensor, num_phy: int)
    return: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
    doc: Replicate `num_log` experts to `num_phy` replicas, such that the maximum load of all replicas is minimized.
  - name: rebalance_experts_hierarchical
    signature: (weight: torch.Tensor, num_physical_experts: int, num_groups: int, num_nodes: int, num_gpus: int)
    doc: Parameters:
  - name: inverse
    signature: (perm: torch.Tensor)
    return: torch.Tensor
  - name: rebalance_experts
    signature: (weight: torch.Tensor, num_replicas: int, num_groups: int, num_nodes: int, num_gpus: int, enable_hierarchical: bool)
    return: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
    doc: Entry point for expert-parallelism load balancer.

File: eplb/eplb_algorithms/deepseek_vec.py
  - name: pack_groups
    signature: (tokens_per_group: torch.Tensor, num_nodes: int)
    return: torch.Tensor
  - name: key_func
    signature: (rank: int)
    return: int
  - name: make_redundant_experts_chunkwise
    signature: (tokens_per_expert: torch.Tensor, num_physical_experts: int, num_local_physical_experts: int, num_physical_experts_per_chunk: int)
    return: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
  - name: decode_rebalance_experts
    signature: (tokens_per_expert: torch.Tensor, num_physical_experts: int, num_local_physical_experts: int)
  - name: prefill_rebalance_experts
    signature: (tokens_per_expert: torch.Tensor, num_physical_experts: int, num_local_physical_experts: int, num_groups: int, num_nodes: int)
  - name: rebalance_experts
    signature: (tokens_per_expert: torch.Tensor, num_physical_experts: int, num_local_physical_experts: int, num_groups: Optional[int], num_nodes: int, enable_hierarchical: bool)

File: eplb/eplb_manager.py
  - name: __init__
    signature: (self, model_runner: 'ModelRunner')
    class: EPLBManager
  - name: on_forward_pass_end
    signature: (self)
    class: EPLBManager
  - name: _entrypoint
    signature: (self)
    class: EPLBManager
  - name: rebalance
    signature: (self)
    class: EPLBManager
  - name: _check_rebalance_needed
    signature: (self, average_utilization_rate_over_window)
    class: EPLBManager
  - name: _compute_update_layer_ids_chunks
    signature: (self)
    return: List[List[int]]
    class: EPLBManager
  - name: _chunk_list
    signature: (items: List, chunk_size)

File: eplb/eplb_simulator/__init__.py
  (no function definitions found)
File: eplb/eplb_simulator/reader.py
  - name: read_mode_per_pass
    signature: (dir_data: Path)
    doc: Read data from ExpertDistributionRecorder when recorded with mode `per_pass`

File: eplb/expert_distribution.py
  - name: init_new
    signature: (server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)
    class: ExpertDistributionRecorder
  - name: with_current_layer
    signature: (self, layer_idx)
    class: ExpertDistributionRecorder
  - name: with_debug_name
    signature: (self, debug_name)
    class: ExpertDistributionRecorder
  - name: disable_this_region
    signature: (self)
    class: ExpertDistributionRecorder
  - name: with_forward_pass
    signature: (self, forward_pass_id: int, forward_batch: ForwardBatch)
    class: ExpertDistributionRecorder
  - name: on_select_experts
    signature: (self, topk_ids: torch.Tensor)
    class: ExpertDistributionRecorder
  - name: on_deepep_dispatch_normal
    signature: (self, local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)
    class: ExpertDistributionRecorder
  - name: on_deepep_dispatch_low_latency
    signature: (self, local_physical_count_of_layer: torch.Tensor)
    class: ExpertDistributionRecorder
  - name: start_record
    signature: (self)
    class: ExpertDistributionRecorder
  - name: stop_record
    signature: (self)
    class: ExpertDistributionRecorder
  - name: dump_record
    signature: (self, output_mode: _OutputMode = 'file')
    class: ExpertDistributionRecorder
  - name: recording
    signature: (self)
    class: ExpertDistributionRecorder
  - name: _on_not_implemented
    signature: (self)
    class: ExpertDistributionRecorder
  - name: __init__
    signature: (self, server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)
    class: _ExpertDistributionRecorderReal
  - name: with_current_layer
    signature: (self, layer_idx)
    class: _ExpertDistributionRecorderReal
  - name: with_debug_name
    signature: (self, debug_name)
    class: _ExpertDistributionRecorderReal
  - name: with_forward_pass
    signature: (self, forward_pass_id: int, forward_batch: ForwardBatch)
    class: _ExpertDistributionRecorderReal
  - name: disable_this_region
    signature: (self)
    class: _ExpertDistributionRecorderReal
    doc: Context manager to temporarily disable recording.
  - name: _on_forward_pass_start
    signature: (self, forward_batch: ForwardBatch)
    class: _ExpertDistributionRecorderReal
  - name: _on_forward_pass_end
    signature: (self, forward_pass_id: int)
    class: _ExpertDistributionRecorderReal
  - name: on_select_experts
    signature: (self, topk_ids: torch.Tensor)
    class: _ExpertDistributionRecorderReal
  - name: on_deepep_dispatch_normal
    signature: (self, local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)
    class: _ExpertDistributionRecorderReal
  - name: on_deepep_dispatch_low_latency
    signature: (self, local_physical_count_of_layer: torch.Tensor)
    class: _ExpertDistributionRecorderReal
  - name: _on_hook
    signature: (self, hook_name: str, **kwargs)
    class: _ExpertDistributionRecorderReal
  - name: _reset
    signature: (self)
    class: _ExpertDistributionRecorderReal
    doc: Reset the expert distribution recorder.
  - name: start_record
    signature: (self)
    class: _ExpertDistributionRecorderReal
    doc: Start recording the expert distribution.
  - name: stop_record
    signature: (self)
    class: _ExpertDistributionRecorderReal
    doc: Stop recording the expert distribution.
  - name: dump_record
    signature: (self, output_mode: _OutputMode = 'file')
    class: _ExpertDistributionRecorderReal
    doc: Dump the expert distribution record and reset the recorder after dumping.
  - name: recording
    signature: (self)
    class: _ExpertDistributionRecorderReal
  - name: get_global_expert_distribution_recorder
    signature: ()
  - name: set_global_expert_distribution_recorder
    signature: (value)
  - name: init_new
    signature: (server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)
    return: '_SinglePassGatherer'
    class: _SinglePassGatherer
  - name: __init__
    signature: (self, expert_location_metadata: 'ExpertLocationMetadata', rank: int)
    class: _SinglePassGatherer
  - name: on_forward_pass_start
    signature: (self, forward_batch: ForwardBatch)
    class: _SinglePassGatherer
  - name: on_select_experts
    signature: (self, layer_idx: int, topk_ids: torch.Tensor)
    class: _SinglePassGatherer
  - name: on_deepep_dispatch_normal
    signature: (self, layer_idx: int, local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)
    class: _SinglePassGatherer
  - name: on_deepep_dispatch_low_latency
    signature: (self, layer_idx: int, local_physical_count_of_layer: torch.Tensor)
    class: _SinglePassGatherer
  - name: reset
    signature: (self)
    class: _SinglePassGatherer
  - name: collect
    signature: (self)
    return: Dict
    class: _SinglePassGatherer
  - name: __init__
    signature: (self, server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)
    class: _DetailSinglePassGatherer
  - name: on_forward_pass_start
    signature: (self, forward_batch: ForwardBatch)
    class: _DetailSinglePassGatherer
  - name: on_select_experts
    signature: (self, layer_idx: int, topk_ids: torch.Tensor)
    class: _DetailSinglePassGatherer
  - name: on_deepep_dispatch_normal
    signature: (self, layer_idx: int, local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)
    class: _DetailSinglePassGatherer
  - name: reset
    signature: (self)
    class: _DetailSinglePassGatherer
  - name: collect
    signature: (self)
    return: Dict
    class: _DetailSinglePassGatherer
  - name: __init__
    signature: (self, *args, **kwargs)
    class: _LayerBasedCpuSinglePassGatherer
  - name: _on_layer_data
    signature: (self, layer_idx: int, objects: List[int])
    class: _LayerBasedCpuSinglePassGatherer
  - name: reset
    signature: (self)
    class: _LayerBasedCpuSinglePassGatherer
  - name: _collect_objects
    signature: (self, pad_len: int)
    return: torch.Tensor
    class: _LayerBasedCpuSinglePassGatherer
  - name: _list_sum
    signature: (a: List, b: List)
    return: List
  - name: __init__
    signature: (self, *args, enable_global_physical_experts: bool, **kwargs)
    class: _LayerBasedGpuSinglePassGatherer
  - name: reset
    signature: (self)
    class: _LayerBasedGpuSinglePassGatherer
  - name: collect
    signature: (self)
    return: Dict
    class: _LayerBasedGpuSinglePassGatherer
  - name: __init__
    signature: (self, *args, **kwargs)
    class: _SelectExpertsSinglePassGatherer
  - name: on_select_experts
    signature: (self, layer_idx: int, topk_ids: torch.Tensor)
    class: _SelectExpertsSinglePassGatherer
  - name: __init__
    signature: (self, *args, **kwargs)
    class: _DeepepNormalSinglePassGatherer
  - name: on_deepep_dispatch_normal
    signature: (self, layer_idx: int, local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)
    class: _DeepepNormalSinglePassGatherer
  - name: collect
    signature: (self)
    return: Dict
    class: _DeepepNormalSinglePassGatherer
  - name: __init__
    signature: (self, *args, **kwargs)
    class: _DeepepLowLatencySinglePassGatherer
  - name: on_deepep_dispatch_low_latency
    signature: (self, layer_idx: int, local_physical_count_of_layer: torch.Tensor)
    class: _DeepepLowLatencySinglePassGatherer
  - name: _convert_local_to_global_physical_count
    signature: (local_physical_count: torch.Tensor, rank: int, num_local_physical_experts: int, num_physical_experts: int)
    return: torch.Tensor
  - name: init_new
    signature: (server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)
    return: '_Accumulator'
    class: _Accumulator
  - name: get_class
    signature: (server_args: ServerArgs)
    return: Type['_Accumulator']
    class: _Accumulator
  - name: __init__
    signature: (self, server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)
    class: _Accumulator
  - name: get_single_pass_gatherer_keys
    signature: (self)
    class: _Accumulator
  - name: get_single_pass_gatherer_key
    signature: (self, debug_name: Optional[str])
    class: _Accumulator
  - name: append
    signature: (self, forward_pass_id: int, gatherer_key: str, single_pass_data: Dict)
    class: _Accumulator
  - name: reset
    signature: (self)
    class: _Accumulator
  - name: dump
    signature: (self, output_mode: _OutputMode)
    class: _Accumulator
  - name: __init__
    signature: (self, *args, **kwargs)
    class: _UtilizationRateAccumulatorMixin
  - name: append
    signature: (self, forward_pass_id: int, gatherer_key: str, single_pass_data: Dict)
    class: _UtilizationRateAccumulatorMixin
  - name: reset
    signature: (self)
    class: _UtilizationRateAccumulatorMixin
  - name: _append_utilization_rate
    signature: (self, forward_pass_id: int, single_pass_global_physical_count: torch.Tensor)
    class: _UtilizationRateAccumulatorMixin
  - name: __init__
    signature: (self, maxlens: List[int])
    class: _DequeCollection
  - name: append
    signature: (self, value)
    class: _DequeCollection
  - name: clear
    signature: (self)
    class: _DequeCollection
  - name: mean
    signature: (self)
    return: Dict[int, float]
    class: _DequeCollection
  - name: __init__
    signature: (self, *args, **kwargs)
    class: _DetailAccumulator
  - name: get_single_pass_gatherer_keys
    signature: (self)
    class: _DetailAccumulator
  - name: get_single_pass_gatherer_key
    signature: (self, debug_name: Optional[str])
    class: _DetailAccumulator
  - name: append
    signature: (self, forward_pass_id: int, gatherer_key: str, single_pass_data: Dict)
    class: _DetailAccumulator
  - name: _process_object
    signature: (obj)
    class: _DetailAccumulator
  - name: reset
    signature: (self)
    class: _DetailAccumulator
  - name: dump
    signature: (self, output_mode: _OutputMode)
    class: _DetailAccumulator
  - name: __init__
    signature: (self, *args, **kwargs)
    class: _StatAccumulator
  - name: append
    signature: (self, forward_pass_id: int, gatherer_key: str, single_pass_data: Dict)
    class: _StatAccumulator
  - name: reset
    signature: (self)
    class: _StatAccumulator
  - name: dump
    signature: (self, output_mode: _OutputMode)
    class: _StatAccumulator
  - name: _get_global_average_utilization_rate
    signature: (self)
    class: _StatAccumulator
  - name: _dump_to_file
    signature: (name, data)
  - name: init_new
    signature: (item_shape: Tuple, buffer_size: int, dtype, device)
    class: _Buffer
  - name: append
    signature: (self, value: torch.Tensor)
    class: _Buffer
  - name: get_all
    signature: (self)
    return: torch.Tensor
    class: _Buffer
  - name: reset
    signature: (self)
    class: _Buffer
  - name: __init__
    signature: (self, item_shape: Tuple, buffer_size: int, dtype, device)
    class: _CircularBuffer
  - name: append
    signature: (self, value: torch.Tensor)
    class: _CircularBuffer
  - name: get_all
    signature: (self)
    return: torch.Tensor
    class: _CircularBuffer
  - name: reset
    signature: (self)
    class: _CircularBuffer
  - name: __init__
    signature: (self, item_shape: Tuple, dtype, device)
    class: _InfiniteBuffer
  - name: append
    signature: (self, value: torch.Tensor)
    class: _InfiniteBuffer
  - name: get_all
    signature: (self)
    return: torch.Tensor
    class: _InfiniteBuffer
  - name: reset
    signature: (self)
    class: _InfiniteBuffer
  - name: _convert_global_physical_count_to_logical_count
    signature: (global_physical_count: torch.Tensor, num_layers: int, num_logical_experts: int, physical_to_logical_map: torch.Tensor)
  - name: compute_gpu_physical_count
    signature: (physical_count_of_whatever: torch.Tensor, num_gpu: int)
    doc: output: gpu_physical_count_of_batch (..., num_layer, num_gpu)
  - name: compute_utilization_rate
    signature: (gpu_physical_count_of_batch: torch.Tensor)
    doc: output: utilization_rate (..., num_layer)

File: eplb/expert_location.py
  - name: num_layers
    signature: (self)
    return: int
    class: ExpertLocationMetadata
  - name: num_physical_experts
    signature: (self)
    return: int
    class: ExpertLocationMetadata
  - name: num_local_physical_experts
    signature: (self)
    return: int
    class: ExpertLocationMetadata
  - name: num_logical_experts
    signature: (self)
    return: int
    class: ExpertLocationMetadata
  - name: ep_size
    signature: (self)
    class: ExpertLocationMetadata
  - name: __post_init__
    signature: (self)
    class: ExpertLocationMetadata
  - name: init_trivial
    signature: (server_args: ServerArgs, model_config: ModelConfig)
    class: ExpertLocationMetadata
    doc: Trivial location - logical expert i corresponds to physical expert i
  - name: init_by_mapping
    signature: (server_args: ServerArgs, model_config: ModelConfig, physical_to_logical_map)
    class: ExpertLocationMetadata
  - name: init_by_eplb
    signature: (server_args: ServerArgs, model_config: ModelConfig, logical_count: torch.Tensor)
    class: ExpertLocationMetadata
  - name: _init_common
    signature: (server_args: ServerArgs, model_config: ModelConfig)
    class: ExpertLocationMetadata
  - name: _init_raw
    signature: (server_args: ServerArgs, ep_size: int, physical_to_logical_map: torch.Tensor, logical_to_all_physical_map: torch.Tensor)
    class: ExpertLocationMetadata
  - name: update
    signature: (self, other: 'ExpertLocationMetadata', update_layer_ids: List[int])
    class: ExpertLocationMetadata
  - name: logical_to_all_physical
    signature: (self, layer_id: int, logical_expert_id: int)
    return: List[int]
    class: ExpertLocationMetadata
  - name: get_global_expert_location_metadata
    signature: ()
  - name: set_global_expert_location_metadata
    signature: (value)
  - name: _compute_logical_to_all_physical_map
    signature: (physical_to_logical_map: torch.Tensor, num_logical_experts: int)
  - name: _pad_nested_array
    signature: (arr, pad_value)
  - name: compute_logical_to_rank_dispatch_physical_map
    signature: (logical_to_all_physical_map: torch.Tensor, num_gpus: int, num_physical_experts: int, ep_rank: int, seed: int = 42)
  - name: _logical_to_all_physical_raw
    signature: (logical_to_all_physical_map, layer_id: int, logical_expert_id: int)
    return: List[int]
  - name: _compute_gpu_id_of_physical_expert
    signature: (physical_expert_id: int, num_local_physical_experts: int)
    return: int
  - name: _fair_choices
    signature: (arr: List, k: int, r: random.Random)
    return: List
  - name: from_model_config
    signature: (model_config: ModelConfig)
    class: ModelConfigForExpertLocation
  - name: compute_initial_expert_location_metadata
    signature: (server_args: ServerArgs, model_config: ModelConfig)
    return: Optional[ExpertLocationMetadata]

File: eplb/expert_location_dispatch.py
  - name: init_new
    signature: (cls, layer_id: int)
    class: ExpertLocationDispatchInfo
  - name: transform_select_experts_inputs
    signature: (router_logits: torch.Tensor, correction_bias: Optional[torch.Tensor], info: Optional[ExpertLocationDispatchInfo])
  - name: topk_ids_logical_to_physical
    signature: (topk_ids: torch.Tensor, info: Optional[ExpertLocationDispatchInfo])
    return: torch.Tensor
  - name: _topk_ids_logical_to_physical_static
    signature: (topk_ids: torch.Tensor, info: Optional[ExpertLocationDispatchInfo])
    return: torch.Tensor
  - name: _topk_ids_logical_to_physical_dynamic
    signature: (topk_ids: torch.Tensor, info: Optional[ExpertLocationDispatchInfo])
    return: torch.Tensor

File: eplb/expert_location_updater.py
  - name: __init__
    signature: (self)
    class: ExpertLocationUpdater
  - name: update
    signature: (self, routed_experts_weights_of_layer: Dict[int, List[torch.Tensor]], new_expert_location_metadata: ExpertLocationMetadata, update_layer_ids: List[int], nnodes: int, rank: int)
    class: ExpertLocationUpdater
  - name: _update_expert_weights
    signature: (**kwargs)
  - name: _update_expert_weights_with_canary
    signature: (routed_experts_weights_of_layer: Dict[int, List[torch.Tensor]], old_expert_location_metadata: ExpertLocationMetadata, new_expert_location_metadata: ExpertLocationMetadata, update_layer_ids: List[int], nnodes: int, rank: int)
  - name: _get_canary_value
    signature: (meta: ExpertLocationMetadata, layer_id: int)
  - name: _update_expert_weights_raw
    signature: (routed_experts_weights_of_layer: Dict[int, List[torch.Tensor]], old_expert_location_metadata: ExpertLocationMetadata, new_expert_location_metadata: ExpertLocationMetadata, update_layer_ids: List[int], nnodes: int, rank: int)
  - name: create_temp_buffers
    signature: (sample_tensors)
  - name: update_expert_weights_single_layer
    signature: (routed_experts_weights: List[torch.Tensor], temp_buffers: List[torch.Tensor], old_physical_to_logical_map: List[int], new_physical_to_logical_map: List[int], num_local_physical_experts: int, num_gpu_per_node: int, rank: int, world_size: Optional[int] = None, debug: bool = False, log_metrics: bool = False)
  - name: _entrypoint
    signature: ()
  - name: _handle_recv
    signature: (buffer2weight_copy_infos, p2p_op_infos)
  - name: _handle_recv_of_dst_expert_location
    signature: (dst_expert_location: int, buffer2weight_copy_infos, p2p_op_infos)
  - name: _create_p2p_recv_and_buffer2weight_copy
    signature: (buffer2weight_copy_infos, p2p_op_infos, *, logical_expert_id: int, src_rank: int, dst_expert_location: int)
  - name: _create_isend_ops
    signature: (p2p_op_infos)
  - name: _create_isend_ops_of_logical_expert_id
    signature: (logical_expert_id, src_expert_location, p2p_op_infos)
  - name: _compute_comm_info
    signature: (logical_expert_id: int)
  - name: _execute_p2p_ops
    signature: (p2p_op_infos)
  - name: _execute_buffer2weight_copies
    signature: (buffer2weight_copy_infos)
  - name: _get_tensor
    signature: (tensors, tensor_index: int, expert_location: int)
    return: torch.Tensor
  - name: _get_local_expert_location
    signature: (expert_location: int)
    return: int
  - name: __init__
    signature: (self, *, chunk_values: List, element_values: List)
    class: _ChunkUtils
  - name: chunk_value_from_element_value
    signature: (self, element_value)
    class: _ChunkUtils
  - name: element_values_from_chunk_value
    signature: (self, chunk_value)
    return: List
    class: _ChunkUtils
  - name: _chunk_index_from_element_index
    signature: (num_elements: int, num_chunks: int, element_index: int)
    return: int
    class: _ChunkUtils
  - name: _element_slice_from_chunk_index
    signature: (num_elements: int, num_chunks: int, chunk_index: int)
    return: slice
    class: _ChunkUtils
  - name: _deduplicate_ordered
    signature: (arr: List[int])
  - name: _log_p2p_op_metrics
    signature: (p2p_op_infos: List[Tuple[int, List[P2POp]]], num_gpu_per_node: int, world_size: int, self_node_id: int)
  - name: _get_direction_from_op
    signature: (op: P2POp)
  - name: _group_by
    signature: (items, keyfunc)

File: harmony_parser.py
  - name: prefix_hold
    signature: (text: str, tokens: List[str])
    return: Tuple[str, str]
    doc: Holds back the longest suffix of `text` that could be a prefix of any token.
  - name: iter_tokens
    signature: (text: str, start_pos: int = 0)
    return: Iterator[Token]
    doc: Iterate over structural tokens in left-to-right order.
  - name: __init__
    signature: (self)
    class: CanonicalStrategy
  - name: parse
    signature: (self, text: str)
    return: Tuple[List[Event], str]
    class: CanonicalStrategy
  - name: _parse_partial_analysis
    signature: (self, text: str, tokens: List[Token], start_pos: int)
    return: Optional[Tuple[Event, str]]
    class: CanonicalStrategy
    doc: Try to parse partial analysis content for incremental streaming.
  - name: _extract_channel_type
    signature: (self, header_text: str)
    return: Optional[str]
    class: CanonicalStrategy
    doc: Extract channel type from header, ignoring other attributes like to=... or <|constrain|>...
  - name: _parse_block
    signature: (self, text: str, tokens: List[Token], start_pos: int)
    return: Optional[Tuple[Optional[Event], int]]
    class: CanonicalStrategy
    doc: Parse a channel block. Returns (event, next_pos) or None if incomplete.
  - name: _is_commentary_filler_between_blocks
    signature: (self, text: str, tokens: List[Token], pos: int)
    return: bool
    class: CanonicalStrategy
    doc: Check if this is commentary filler text or problematic structural tokens in malformed sequences.
  - name: _is_standalone_structural_token
    signature: (self, content: str)
    return: bool
    class: CanonicalStrategy
    doc: Check if content is just a standalone structural token that should be filtered.
  - name: __init__
    signature: (self)
    class: TextStrategy
  - name: set_buffer_context
    signature: (self, buffer: str)
    class: TextStrategy
  - name: parse
    signature: (self, text: str)
    return: Tuple[List[Event], str]
    class: TextStrategy
  - name: __init__
    signature: (self)
    class: HarmonyParser
  - name: parse
    signature: (self, chunk: str)
    return: List[Event]
    class: HarmonyParser

File: hf_transformers_utils.py
  - name: download_from_hf
    signature: (model_path: str, allow_patterns: Optional[Union[str, list]] = None)
  - name: get_hf_text_config
    signature: (config: PretrainedConfig)
    doc: Get the "sub" config relevant to llm for multi modal models.
  - name: get_config
    signature: (model: str, trust_remote_code: bool, revision: Optional[str] = None, model_override_args: Optional[dict] = None, **kwargs)
  - name: get_generation_config
    signature: (model: str, trust_remote_code: bool, revision: Optional[str] = None, **kwargs)
  - name: get_sparse_attention_config
    signature: (model: str, sparse_attention_config_filename: str = 'sparse_attention_config.json')
    return: Dict[str, Any]
  - name: get_context_length
    signature: (config)
    doc: Get the context length of a model from a huggingface model configs.
  - name: get_tokenizer
    signature: (tokenizer_name: str, *args, tokenizer_mode: str = 'auto', trust_remote_code: bool = False, tokenizer_revision: Optional[str] = None, **kwargs)
    return: Union[PreTrainedTokenizer, PreTrainedTokenizerFast]
    doc: Gets a tokenizer for the given model name via Huggingface.
  - name: get_tokenizer_from_processor
    signature: (processor)
  - name: get_processor
    signature: (tokenizer_name: str, *args, tokenizer_mode: str = 'auto', trust_remote_code: bool = False, tokenizer_revision: Optional[str] = None, use_fast: Optional[bool] = True, **kwargs)
  - name: attach_additional_stop_token_ids
    signature: (tokenizer)
  - name: check_gguf_file
    signature: (model: Union[str, os.PathLike])
    return: bool
    doc: Check if the file is a GGUF model.

File: host_shared_memory.py
  - name: __init__
    signature: (self, base_name: str)
    class: HostSharedMemoryManager
  - name: malloc
    signature: (self, *, shape, dtype)
    class: HostSharedMemoryManager
  - name: _malloc_raw
    signature: (self, *, num_bytes: int)
    return: torch.Tensor
    class: HostSharedMemoryManager
  - name: get_host_shared_memory_manager
    signature: ()
  - name: set_host_shared_memory_manager
    signature: (instance: HostSharedMemoryManager)

File: jinja_template_utils.py
  - name: _is_var_access
    signature: (node: jinja2.nodes.Node, varname: str)
    return: bool
    doc: Check if node is a variable access like {{ varname }}
  - name: _is_attr_access
    signature: (node: jinja2.nodes.Node, varname: str, key: str)
    return: bool
    doc: Check if node is an attribute access like {{ varname['key'] }} or {{ varname.key }}
  - name: _is_var_or_elems_access
    signature: (node: jinja2.nodes.Node, varname: str, key: str = None)
    return: bool
    doc: Check if node accesses varname or varname[key] with filters/tests
  - name: _try_extract_ast
    signature: (chat_template: str)
    doc: Try to parse the Jinja template into an AST
  - name: detect_jinja_template_content_format
    signature: (chat_template: str)
    return: str
    doc: Detect whether a chat template expects 'string' or 'openai' content format.
  - name: process_content_for_template_format
    signature: (msg_dict: dict, content_format: str, image_data: list, video_data: list, audio_data: list, modalities: list)
    return: dict
    doc: Process message content based on detected template format.

File: layers/activation.py
  - name: forward_native
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: SiluAndMul
  - name: forward_cuda
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: SiluAndMul
  - name: forward_cpu
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: SiluAndMul
  - name: forward_npu
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: SiluAndMul
  - name: __init__
    signature: (self, approximate = 'tanh')
    class: GeluAndMul
  - name: forward_native
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: GeluAndMul
  - name: forward_cuda
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: GeluAndMul
  - name: forward_npu
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: GeluAndMul
  - name: forward_native
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: NewGELU
  - name: forward_cuda
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: NewGELU
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: ReLU2
  - name: forward_native
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: QuickGELU
  - name: forward_cuda
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: QuickGELU
  - name: forward_hip
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: QuickGELU
  - name: forward_npu
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: QuickGELU
  - name: __init__
    signature: (self, act_module: nn.Module, intermediate_size: int, input_is_parallel: bool = True, params_dtype: Optional[torch.dtype] = None)
    class: ScaledActivation
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: ScaledActivation
  - name: weight_loader
    signature: (self, param: nn.Parameter, loaded_weight: torch.Tensor)
    class: ScaledActivation
  - name: get_act_fn
    signature: (act_fn_name: str, quant_config: Optional[QuantizationConfig] = None, intermediate_size: Optional[int] = None, input_is_parallel: bool = True, params_dtype: Optional[torch.dtype] = None)
    return: nn.Module
    doc: Get an activation function by name.
  - name: get_cross_encoder_activation_function
    signature: (config: PretrainedConfig)

File: layers/amx_utils.py
  - name: amx_process_weight_after_loading
    signature: (weight)
  - name: dim_is_supported
    signature: (weight)
  - name: _amx_process_weight_after_loading
    signature: (module, weight_names, transpose_dims = None)
    return: None
  - name: __init__
    signature: (self, weight_names, transpose_dims = None)
    class: PackWeightMethod
  - name: process_weights_after_loading
    signature: (self, module)
    return: None
    class: PackWeightMethod

File: layers/attention/aiter_backend.py
  - name: __init__
    signature: (self, model_runner: ModelRunner, skip_prefill: bool = False, kv_indptr_buf: Optional[torch.Tensor] = None)
    class: AiterAttnBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: AiterAttnBackend
    doc: Init auxiliary variables for triton attention backend.
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor] = None)
    class: AiterAttnBackend
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])
    class: AiterAttnBackend
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])
    class: AiterAttnBackend
  - name: get_cuda_graph_seq_len_fill_value
    signature: (self)
    class: AiterAttnBackend
  - name: forward_extend
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache = True)
    class: AiterAttnBackend
  - name: forward_decode
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache = True)
    class: AiterAttnBackend
  - name: __init__
    signature: (self, model_runner: ModelRunner, attn_backend: AttentionBackend)
    class: AiterIndicesUpdaterPrefill
  - name: update
    signature: (self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, prefix_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], spec_info: Optional[SpecInfo])
    class: AiterIndicesUpdaterPrefill
  - name: update_single_wrapper
    signature: (self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, prefix_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], spec_info: Optional[SpecInfo])
    class: AiterIndicesUpdaterPrefill
  - name: __init__
    signature: (self, model_runner: ModelRunner, attn_backend: AttentionBackend)
    class: AiterMlaIndicesUpdaterPrefill
  - name: update
    signature: (self, req_pool_indices: torch.Tensor, kv_lens: torch.Tensor, kv_lens_sum: int, extend_lens: torch.Tensor, max_q_len: int, max_kv_len: int, spec_info: Optional[SpecInfo])
    class: AiterMlaIndicesUpdaterPrefill
  - name: update_single_wrapper
    signature: (self, req_pool_indices: torch.Tensor, kv_lens: torch.Tensor, kv_lens_sum: int, extend_lens: torch.Tensor, max_q_len: int, max_kv_len: int, spec_info: Optional[SpecInfo])
    class: AiterMlaIndicesUpdaterPrefill
  - name: __init__
    signature: (self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)
    class: AiterMultiStepDraftBackend
  - name: common_template
    signature: (self, forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: int)
    class: AiterMultiStepDraftBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: AiterMultiStepDraftBackend
  - name: call_fn
    signature: (i, forward_batch)
    class: AiterMultiStepDraftBackend
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int)
    class: AiterMultiStepDraftBackend
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, forward_batch: ForwardBatch)
    class: AiterMultiStepDraftBackend
  - name: call_fn
    signature: (i, forward_batch)
    class: AiterMultiStepDraftBackend
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, forward_batch: ForwardBatch, bs: int)
    class: AiterMultiStepDraftBackend
  - name: call_fn
    signature: (i, forward_batch)
    class: AiterMultiStepDraftBackend

File: layers/attention/ascend_backend.py
  - name: gen_attention_mask
    signature: (self, max_seq_len: int, dtype = torch.float16)
    class: AscendAttnBackend
  - name: __init__
    signature: (self, model_runner: ModelRunner)
    class: AscendAttnBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: AscendAttnBackend
    doc: Init the metadata for a forward pass.
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int)
    class: AscendAttnBackend
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
    class: AscendAttnBackend
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])
    class: AscendAttnBackend
  - name: get_cuda_graph_seq_len_fill_value
    signature: (self)
    class: AscendAttnBackend
  - name: forward_extend
    signature: (self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool = True)
    class: AscendAttnBackend
  - name: forward_decode_graph
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool = True, q_rope: Optional[torch.Tensor] = None, k_rope: Optional[torch.Tensor] = None)
    class: AscendAttnBackend
  - name: forward_decode
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool = True, q_rope: Optional[torch.Tensor] = None, k_rope: Optional[torch.Tensor] = None)
    class: AscendAttnBackend

File: layers/attention/base_attn_backend.py
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: AttentionBackend
    doc: Init the metadata for a forward pass.
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int)
    class: AttentionBackend
    doc: Init the global shared states for cuda graph.
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
    class: AttentionBackend
    doc: Init the metadata for a forward pass for capturing a cuda graph.
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])
    class: AttentionBackend
    doc: Init the metadata for a forward pass for replaying a cuda graph.
  - name: get_cuda_graph_seq_len_fill_value
    signature: (self)
    class: AttentionBackend
    doc: Get the fill value for padded seq lens. Typically, it is 0 or 1.
  - name: forward
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool = True, **kwargs)
    class: AttentionBackend
    doc: Run forward on an attention layer.
  - name: forward_decode
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool = True)
    class: AttentionBackend
    doc: Run a forward for decode.
  - name: forward_extend
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool = True)
    class: AttentionBackend
    doc: Run a forward for extend.
  - name: support_triton
    signature: (self)
    class: AttentionBackend
    doc: Check if the current backend supports triton.

File: layers/attention/cutlass_mla_backend.py
  - name: __init__
    signature: (self, workspace: Optional[torch.Tensor] = None, block_kv_indices: Optional[torch.Tensor] = None)
    class: CutlassMLADecodeMetadata
  - name: __init__
    signature: (self, model_runner: ModelRunner, skip_prefill: bool = False, kv_indptr_buf: Optional[torch.Tensor] = None, kv_last_page_len_buf: Optional[torch.Tensor] = None)
    class: CutlassMLABackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: CutlassMLABackend
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int, block_kv_indices: Optional[torch.Tensor] = None)
    class: CutlassMLABackend
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])
    class: CutlassMLABackend
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])
    class: CutlassMLABackend
  - name: get_cuda_graph_seq_len_fill_value
    signature: (self)
    class: CutlassMLABackend
  - name: forward_decode
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool = True, q_rope: Optional[torch.Tensor] = None, k_rope: Optional[torch.Tensor] = None)
    class: CutlassMLABackend

File: layers/attention/double_sparsity_backend.py
  - name: __init__
    signature: (self, model_runner: ModelRunner)
    class: DoubleSparseAttnBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: DoubleSparseAttnBackend
    doc: Init auxiliary variables for triton attention backend.
  - name: forward_extend
    signature: (self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache = True)
    class: DoubleSparseAttnBackend
  - name: forward_decode
    signature: (self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache = True)
    class: DoubleSparseAttnBackend

File: layers/attention/dual_chunk_flashattention_backend.py
  - name: __init__
    signature: (self, model_runner: 'ModelRunner')
    return: None
    class: DualChunkFlashAttentionBackend
  - name: get_sparse_attention_config
    signature: (self, layer_idx)
    return: List[Dict[str, Any]]
    class: DualChunkFlashAttentionBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: DualChunkFlashAttentionBackend
    doc: Initialize forward metadata hence all layers in the forward pass can reuse it.
  - name: forward_extend
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: 'RadixAttention', forward_batch: ForwardBatch, save_kv_cache = True)
    class: DualChunkFlashAttentionBackend
  - name: forward_decode
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: 'RadixAttention', forward_batch: ForwardBatch, save_kv_cache = True)
    return: torch.Tensor
    class: DualChunkFlashAttentionBackend
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int)
    class: DualChunkFlashAttentionBackend
    doc: Initialize CUDA graph state for the attention backend.
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[None])
    class: DualChunkFlashAttentionBackend
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[None], seq_lens_cpu: Optional[torch.Tensor], out_cache_loc: torch.Tensor = None)
    class: DualChunkFlashAttentionBackend
    doc: Initialize forward metadata for replaying CUDA graph.
  - name: get_cuda_graph_seq_len_fill_value
    signature: (self)
    class: DualChunkFlashAttentionBackend
    doc: Get the fill value for sequence length in CUDA graph.
  - name: _dual_chunk_flash_attn_prefill
    signature: (self, q, q_succ, q_inter, q_succ_critical, q_inter_critical, k, v, cu_seqlens_q, cu_seqlens_k, orig_seq_lens: List[int], scaling_factor: torch.Tensor, softmax_scale: float, causal: Optional[bool] = True, window_size: Tuple[int, int] = (-1, -1), block_table: Optional[torch.Tensor] = None, chunk_size: int = 8192, local_size: int = 1024)
    class: DualChunkFlashAttentionBackend
  - name: _dual_chunk_flash_attn_prefill_func
    signature: (self, q, q_succ, q_inter, q_succ_critical, q_inter_critical, k, v, block_table, softmax_scale: float, chunk_size: int, local_size: int, scaling_factor: float, k_length: int, sparse_attn_enabled: Optional[bool] = True, heads_vertical_size = None, heads_slash_size = None, group_size = None)
    class: DualChunkFlashAttentionBackend
  - name: _do_flash_attn
    signature: (self, query_states: torch.Tensor, key_states: torch.Tensor, value_states: torch.Tensor, softmax_scale: float, causal: bool = True, max_seqlen_k: Optional[int] = None, stage: str = 'intra', vertical_indices: Optional[torch.Tensor] = None, slash_indices: Optional[torch.Tensor] = None, vertical_indices_count: Optional[torch.Tensor] = None, slash_indices_count: Optional[torch.Tensor] = None, mergehead_softmax_scale: Optional[float] = None, sparse_attn_enabled: Optional[bool] = False)
    class: DualChunkFlashAttentionBackend
  - name: _merge_attn_outputs
    signature: (self, flash_results: List[List[Tuple[torch.Tensor, torch.Tensor]]], return_lse: Optional[bool] = False)
    return: torch.Tensor
    class: DualChunkFlashAttentionBackend
  - name: _dual_chunk_flash_attn_decoding
    signature: (self, query: torch.Tensor, query_succ: torch.Tensor, query_inter: torch.Tensor, key_cache: torch.Tensor, value_cache: torch.Tensor, block_table: torch.Tensor, cache_seqlens: torch.Tensor, softmax_scale: float, causal: bool, chunk_size: int, local_size: int, original_max_position_embeddings: int, decode_meta: DualChunkFlashAttentionMetadata)
    class: DualChunkFlashAttentionBackend
  - name: _dual_chunk_flash_attn_decoding_with_exp_sums
    signature: (self, query: torch.Tensor, key_cache: torch.Tensor, value_cache: torch.Tensor, block_table: torch.Tensor, cache_seqlens: torch.Tensor, softmax_scale: float, causal: bool)
    class: DualChunkFlashAttentionBackend
  - name: _vertical_slash_sparse_attention
    signature: (query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, v_idx: torch.Tensor, s_idx: torch.Tensor, softmax_scale: float, causal: bool = True, stage: str = 'intra', block_size_M: int = 64, block_size_N: int = 64, vertical_indices_count: torch.Tensor = None, slash_indices_count: torch.Tensor = None)
  - name: _sum_all_diagonal_matrix
    signature: (mat: torch.tensor)
  - name: _get_block
    signature: (block_table: torch.Tensor, block_size: int, begin: int, end: int)

File: layers/attention/flashattention_backend.py
  - name: make_local_attention_virtual_batches
    signature: (attn_chunk_size: int, query_start_loc_np: np.ndarray, seq_lens_np: np.ndarray, block_table: torch.Tensor, page_size: int = 0)
    return: tuple[np.ndarray, np.ndarray, np.ndarray, torch.Tensor]
    doc: Take in `query_start_loc_np` and `seq_lens_np` and break the sequences into
  - name: cdiv
    signature: (a: int, b: int)
    return: int
    doc: Ceiling division.
  - name: merge_state_v2_wrapper
    signature: (o, s_a, o_exp, s_b)
  - name: __init__
    signature: (self, model_runner: ModelRunner, skip_prefill: bool = False, speculative_step_id = 0, topk = 0, speculative_num_steps = 0)
    class: FlashAttentionBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: FlashAttentionBackend
    doc: Initialize forward metadata hence all layers in the forward pass can reuse it.
  - name: forward_extend
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache = True, q_rope: Optional[torch.Tensor] = None, k_rope: Optional[torch.Tensor] = None, sinks: Optional[torch.Tensor] = None)
    class: FlashAttentionBackend
  - name: forward_decode
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache = True, q_rope: Optional[torch.Tensor] = None, k_rope: Optional[torch.Tensor] = None, sinks: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: FlashAttentionBackend
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int)
    class: FlashAttentionBackend
    doc: Initialize CUDA graph state for the attention backend.
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
    class: FlashAttentionBackend
    doc: Initialize forward metadata for capturing CUDA graph.
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor], out_cache_loc: Optional[torch.Tensor] = None)
    class: FlashAttentionBackend
    doc: Initialize forward metadata for replaying CUDA graph.
  - name: get_cuda_graph_seq_len_fill_value
    signature: (self)
    class: FlashAttentionBackend
    doc: Get the fill value for sequence length in CUDA graph.
  - name: _init_local_attn_metadata
    signature: (self, forwardbatch: ForwardBatch, metadata: FlashAttentionMetadata, device)
    class: FlashAttentionBackend
    doc: Centralized utility to initialize local_attn_metadata if chunked attention is enabled.
  - name: _update_local_attn_metadata_for_capture
    signature: (self, metadata: FlashAttentionMetadata, bs: int)
    class: FlashAttentionBackend
    doc: Update local attention metadata during CUDA graph capture phase.
  - name: _update_local_attn_metadata_for_replay
    signature: (self, metadata: FlashAttentionMetadata, bs: int)
    class: FlashAttentionBackend
    doc: Update preallocated local attention metadata in-place before CUDA graph replay.
  - name: _init_sliding_window_attn_spec_metadata
    signature: (self, metadata: FlashAttentionMetadata, metadata_expand: FlashAttentionMetadata, metadata_swa: Optional[FlashAttentionMetadata] = None)
    class: FlashAttentionBackend
  - name: _prepare_swa_spec_page_table_kernel
    signature: (dst_ptr, src_a_ptr, src_b_ptr, seq_len_a_ptr, seq_len_b_ptr, dst_stride_m, dst_stride_n, a_stride_m, a_stride_n, b_stride_m, b_stride_n, LEN_A: tl.constexpr, LEN_B: tl.constexpr, REPEAT_STEP: tl.constexpr, BLOCK_N: tl.constexpr)
  - name: prepare_swa_spec_page_table_triton
    signature: (page_table_dst: torch.Tensor, page_table_a: torch.Tensor, page_table_b: torch.Tensor, seq_len_a: torch.Tensor, seq_len_b: torch.Tensor, speculative_num_draft_tokens: int)
  - name: __init__
    signature: (self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)
    class: FlashAttentionMultiStepBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: FlashAttentionMultiStepBackend
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int)
    class: FlashAttentionMultiStepBackend
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, forward_batch: ForwardBatch)
    class: FlashAttentionMultiStepBackend
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, forward_batch: ForwardBatch, bs: int)
    class: FlashAttentionMultiStepBackend
  - name: normal_decode_set_metadata
    signature: (cache_seqlens_int32: torch.Tensor, cu_seqlens_k: torch.Tensor, page_table: torch.Tensor, req_to_token: torch.Tensor, req_pool_indices: torch.Tensor, strided_indices: torch.Tensor, max_seq_pages: torch.Tensor, seq_lens: torch.Tensor, seq_len_delta: int, page_size: int)

File: layers/attention/flashinfer_backend.py
  - name: __init__
    signature: (self, model_runner: ModelRunner, skip_prefill: bool = False, kv_indptr_buf: Optional[torch.Tensor] = None, kv_last_page_len_buf: Optional[torch.Tensor] = None)
    class: FlashInferAttnBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: FlashInferAttnBackend
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor] = None)
    class: FlashInferAttnBackend
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
    class: FlashInferAttnBackend
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])
    class: FlashInferAttnBackend
  - name: get_cuda_graph_seq_len_fill_value
    signature: (self)
    class: FlashInferAttnBackend
  - name: forward_extend
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache = True)
    class: FlashInferAttnBackend
  - name: forward_decode
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache = True)
    class: FlashInferAttnBackend
  - name: _get_wrapper_idx
    signature: (self, layer: RadixAttention)
    class: FlashInferAttnBackend
  - name: __init__
    signature: (self, model_runner: ModelRunner, attn_backend: FlashInferAttnBackend)
    class: FlashInferIndicesUpdaterDecode
  - name: update
    signature: (self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
    class: FlashInferIndicesUpdaterDecode
  - name: update_single_wrapper
    signature: (self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
    class: FlashInferIndicesUpdaterDecode
  - name: update_sliding_window
    signature: (self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
    class: FlashInferIndicesUpdaterDecode
  - name: update_cross_attention
    signature: (self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
    class: FlashInferIndicesUpdaterDecode
  - name: call_begin_forward
    signature: (self, wrapper: BatchDecodeWithPagedKVCacheWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, kv_indptr: torch.Tensor, kv_start_idx: torch.Tensor, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor], use_sliding_window_kv_pool: bool = False)
    class: FlashInferIndicesUpdaterDecode
  - name: __init__
    signature: (self, model_runner: ModelRunner, attn_backend: FlashInferAttnBackend)
    class: FlashInferIndicesUpdaterPrefill
  - name: update
    signature: (self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
    class: FlashInferIndicesUpdaterPrefill
  - name: update_single_wrapper
    signature: (self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
    class: FlashInferIndicesUpdaterPrefill
  - name: update_sliding_window
    signature: (self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
    class: FlashInferIndicesUpdaterPrefill
  - name: update_cross_attention
    signature: (self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
    class: FlashInferIndicesUpdaterPrefill
  - name: call_begin_forward
    signature: (self, wrapper_ragged: BatchPrefillWithRaggedKVCacheWrapper, wrapper_paged: BatchPrefillWithPagedKVCacheWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, seq_lens: torch.Tensor, prefix_lens: torch.Tensor, kv_start_idx: torch.Tensor, kv_indptr: torch.Tensor, qo_indptr: torch.Tensor, use_ragged: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], use_sliding_window_kv_pool: bool = False)
    class: FlashInferIndicesUpdaterPrefill
  - name: __init__
    signature: (self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)
    class: FlashInferMultiStepDraftBackend
  - name: common_template
    signature: (self, forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: Callable)
    class: FlashInferMultiStepDraftBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: FlashInferMultiStepDraftBackend
  - name: call_fn
    signature: (i, forward_batch)
    class: FlashInferMultiStepDraftBackend
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int)
    class: FlashInferMultiStepDraftBackend
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, forward_batch: ForwardBatch)
    class: FlashInferMultiStepDraftBackend
  - name: call_fn
    signature: (i, forward_batch)
    class: FlashInferMultiStepDraftBackend
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, forward_batch: ForwardBatch, bs: int)
    class: FlashInferMultiStepDraftBackend
  - name: call_fn
    signature: (i, forward_batch)
    class: FlashInferMultiStepDraftBackend
  - name: should_use_tensor_core
    signature: (kv_cache_dtype: torch.dtype, num_attention_heads: int, num_kv_heads: int)
    return: bool
    doc: Determine whether to use tensor cores for attention computation.
  - name: fast_decode_plan
    signature: (self, indptr: torch.Tensor, indices: torch.Tensor, last_page_len: torch.Tensor, num_qo_heads: int, num_kv_heads: int, head_dim: int, page_size: int, pos_encoding_mode: str = 'NONE', window_left: int = -1, logits_soft_cap: Optional[float] = None, q_data_type: Optional[Union[str, torch.dtype]] = None, kv_data_type: Optional[Union[str, torch.dtype]] = None, data_type: Optional[Union[str, torch.dtype]] = None, sm_scale: Optional[float] = None, rope_scale: Optional[float] = None, rope_theta: Optional[float] = None, non_blocking: bool = True)
    return: None
    doc: A faster version of BatchDecodeWithPagedKVCacheWrapper::plan used for FlashInferMultiStepDraftBackend.

File: layers/attention/flashinfer_mla_backend.py
  - name: __init__
    signature: (self, model_runner: ModelRunner, attn_backend: 'FlashInferMlaAttnBackend')
    class: FlashInferMhaChunkKVRunner
  - name: update_prefix_chunks
    signature: (self, num_prefix_chunks: int)
    class: FlashInferMhaChunkKVRunner
  - name: update_wrapper
    signature: (self, forward_batch: ForwardBatch)
    class: FlashInferMhaChunkKVRunner
  - name: forward
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch)
    class: FlashInferMhaChunkKVRunner
  - name: __init__
    signature: (self, model_runner: ModelRunner, skip_prefill: bool = False, kv_indptr_buf: Optional[torch.Tensor] = None, q_indptr_decode_buf: Optional[torch.Tensor] = None)
    class: FlashInferMLAAttnBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: FlashInferMLAAttnBackend
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor] = None)
    class: FlashInferMLAAttnBackend
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])
    class: FlashInferMLAAttnBackend
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])
    class: FlashInferMLAAttnBackend
  - name: get_cuda_graph_seq_len_fill_value
    signature: (self)
    class: FlashInferMLAAttnBackend
  - name: init_mha_chunk_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: FlashInferMLAAttnBackend
    doc: Init the metadata for a forward pass.
  - name: forward_extend
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool = True, q_rope: Optional[torch.Tensor] = None, k_rope: Optional[torch.Tensor] = None)
    class: FlashInferMLAAttnBackend
  - name: forward_decode
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool = True, q_rope: Optional[torch.Tensor] = None, k_rope: Optional[torch.Tensor] = None)
    class: FlashInferMLAAttnBackend
  - name: __init__
    signature: (self, model_runner: ModelRunner, attn_backend: AttentionBackend)
    class: FlashInferMLAIndicesUpdaterDecode
  - name: update
    signature: (self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, decode_wrapper: BatchMLAPagedAttentionWrapper, init_metadata_replay: bool = False, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]] = None, **fast_decode_kwargs)
    class: FlashInferMLAIndicesUpdaterDecode
  - name: call_begin_forward
    signature: (self, wrapper: BatchMLAPagedAttentionWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, q_indptr: torch.Tensor, kv_indptr: torch.Tensor, init_metadata_replay: bool = False, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]] = None, **fast_decode_kwargs)
    class: FlashInferMLAIndicesUpdaterDecode
  - name: __init__
    signature: (self, model_runner: ModelRunner, attn_backend: AttentionBackend)
    class: FlashInferMLAIndicesUpdaterPrefill
  - name: update
    signature: (self, req_pool_indices: torch.Tnesor, seq_lens: torch.Tensor, seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrapper_paged: BatchMLAPagedAttentionWrapper, use_ragged: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]] = None)
    class: FlashInferMLAIndicesUpdaterPrefill
  - name: call_begin_forward
    signature: (self, wrapper_ragged: BatchPrefillWithRaggedKVCacheWrapper, wrapper_paged: BatchMLAPagedAttentionWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, seq_lens: torch.Tensor, prefix_lens: torch.Tensor, kv_indptr: torch.Tensor, qo_indptr: torch.Tensor, use_ragged: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]] = None)
    class: FlashInferMLAIndicesUpdaterPrefill
  - name: __init__
    signature: (self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)
    class: FlashInferMLAMultiStepDraftBackend
  - name: common_template
    signature: (self, forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: Callable)
    class: FlashInferMLAMultiStepDraftBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: FlashInferMLAMultiStepDraftBackend
  - name: call_fn
    signature: (i, forward_batch)
    class: FlashInferMLAMultiStepDraftBackend
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int)
    class: FlashInferMLAMultiStepDraftBackend
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, forward_batch: ForwardBatch)
    class: FlashInferMLAMultiStepDraftBackend
  - name: call_fn
    signature: (i, forward_batch)
    class: FlashInferMLAMultiStepDraftBackend
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, forward_batch: ForwardBatch, bs: int)
    class: FlashInferMLAMultiStepDraftBackend
  - name: call_fn
    signature: (i, forward_batch)
    class: FlashInferMLAMultiStepDraftBackend
  - name: fast_mla_decode_plan
    signature: (self, qo_indptr_cpu: torch.Tensor, kv_indptr_cpu: torch.Tensor, kv_indices: torch.Tensor, kv_len_arr_cpu: torch.Tensor, num_heads: int, head_dim_ckv: int, head_dim_kpe: int, page_size: int, causal: bool, sm_scale: float, q_data_type: torch.dtype, kv_data_type: torch.dtype)
    return: None
    doc: A faster version of BatchMLAPagedAttentionWrapper::plan,

File: layers/attention/flashmla_backend.py
  - name: __init__
    signature: (self, flashmla_metadata: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, num_splits: Optional[torch.Tensor] = None, block_kv_indices: Optional[torch.Tensor] = None)
    class: FlashMLADecodeMetadata
  - name: __init__
    signature: (self, model_runner: ModelRunner, skip_prefill: bool = False, kv_indptr_buf: Optional[torch.Tensor] = None, kv_last_page_len_buf: Optional[torch.Tensor] = None)
    class: FlashMLABackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: FlashMLABackend
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int, block_kv_indices: Optional[torch.Tensor] = None)
    class: FlashMLABackend
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])
    class: FlashMLABackend
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])
    class: FlashMLABackend
  - name: get_cuda_graph_seq_len_fill_value
    signature: (self)
    class: FlashMLABackend
  - name: forward_decode
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool = True)
    class: FlashMLABackend
  - name: forward_extend
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool = True)
    class: FlashMLABackend
  - name: __init__
    signature: (self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)
    class: FlashMLAMultiStepDraftBackend
  - name: common_template
    signature: (self, forward_batch: ForwardBatch, call_fn: Callable)
    class: FlashMLAMultiStepDraftBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: FlashMLAMultiStepDraftBackend
  - name: call_fn
    signature: (i, forward_batch)
    class: FlashMLAMultiStepDraftBackend
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int)
    class: FlashMLAMultiStepDraftBackend
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, forward_batch: ForwardBatch)
    class: FlashMLAMultiStepDraftBackend
  - name: call_fn
    signature: (i, forward_batch)
    class: FlashMLAMultiStepDraftBackend
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, forward_batch: ForwardBatch, bs: int)
    class: FlashMLAMultiStepDraftBackend
  - name: call_fn
    signature: (i, forward_batch)
    class: FlashMLAMultiStepDraftBackend

File: layers/attention/hybrid_attn_backend.py
  - name: __init__
    signature: (self, model_runner: ModelRunner, prefill_backend: AttentionBackend, decode_backend: AttentionBackend)
    class: HybridAttnBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: HybridAttnBackend
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int)
    class: HybridAttnBackend
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
    class: HybridAttnBackend
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])
    class: HybridAttnBackend
  - name: get_cuda_graph_seq_len_fill_value
    signature: (self)
    class: HybridAttnBackend
  - name: forward_decode
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool = True, **kwargs)
    class: HybridAttnBackend
  - name: forward_extend
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool = True, **kwargs)
    class: HybridAttnBackend

File: layers/attention/intel_amx_backend.py
  - name: __init__
    signature: (self, model_runner: ModelRunner)
    class: IntelAMXAttnBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: IntelAMXAttnBackend
    doc: Init the metadata for a forward pass.
  - name: forward_extend
    signature: (self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache = True)
    class: IntelAMXAttnBackend
  - name: forward_decode
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache = True)
    class: IntelAMXAttnBackend
  - name: support_triton
    signature: (self)
    class: IntelAMXAttnBackend

File: layers/attention/merge_state.py
  - name: _supported_dtypes
    signature: (o: torch.Tensor)
    return: bool
  - name: _supported_headdim
    signature: (o: torch.Tensor)
    return: bool
  - name: merge_state
    signature: (prefix_output: torch.Tensor, prefix_lse: torch.Tensor, suffix_output: torch.Tensor, suffix_lse: torch.Tensor, output: Optional[torch.Tensor] = None, output_lse: Optional[torch.Tensor] = None)
    return: Tuple[torch.Tensor, Optional[torch.Tensor]]

File: layers/attention/tbo_backend.py
  - name: __init__
    signature: (self, primary: AttentionBackend, children: List[AttentionBackend])
    class: TboAttnBackend
  - name: init_new
    signature: (cls, creator: Callable[[], AttentionBackend])
    class: TboAttnBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: 'ForwardBatch')
    class: TboAttnBackend
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int)
    class: TboAttnBackend
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: 'ForwardMode', spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
    class: TboAttnBackend
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: 'ForwardMode', spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])
    class: TboAttnBackend
  - name: _init_forward_metadata_cuda_graph_children
    signature: (self, fn_name: str, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: 'ForwardMode', spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], capture_num_tokens: int = None, replay_seq_lens_sum: int = None, replay_seq_lens_cpu: Optional[torch.Tensor] = None)
    class: TboAttnBackend
  - name: get_cuda_graph_seq_len_fill_value
    signature: (self)
    class: TboAttnBackend
  - name: forward_extend
    signature: (self, *args, **kwargs)
    class: TboAttnBackend
  - name: forward_decode
    signature: (self, *args, **kwargs)
    class: TboAttnBackend
  - name: _init_forward_metadata_cuda_graph_split
    signature: (fn_name: str, seq_slice: slice, output_bs: int, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: 'ForwardMode', spec_info: Optional[EagleVerifyInput], capture_num_tokens: int = None, replay_seq_lens_sum: int = None, replay_seq_lens_cpu: Optional[torch.Tensor] = None)

File: layers/attention/torch_native_backend.py
  - name: __init__
    signature: (self, model_runner: ModelRunner)
    class: TorchNativeAttnBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: TorchNativeAttnBackend
    doc: Init the metadata for a forward pass.
  - name: _run_sdpa_forward_extend
    signature: (self, query: torch.Tensor, output: torch.Tensor, k_cache: torch.Tensor, v_cache: torch.Tensor, req_to_token: torch.Tensor, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, extend_prefix_lens: torch.Tensor, extend_seq_lens: torch.Tensor, scaling = None, enable_gqa = False, causal = False)
    class: TorchNativeAttnBackend
    doc: Run the extend forward by using torch native sdpa op.
  - name: _run_sdpa_forward_decode
    signature: (self, query: torch.Tensor, output: torch.Tensor, k_cache: torch.Tensor, v_cache: torch.Tensor, req_to_token: torch.Tensor, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, scaling = None, enable_gqa = False, causal = False)
    class: TorchNativeAttnBackend
    doc: Run the decode forward by using torch native sdpa op.
  - name: forward_extend
    signature: (self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache = True)
    class: TorchNativeAttnBackend
  - name: forward_decode
    signature: (self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache = True)
    class: TorchNativeAttnBackend
  - name: support_triton
    signature: (self)
    class: TorchNativeAttnBackend

File: layers/attention/triton_backend.py
  - name: logit_capping_mod
    signature: (logit_capping_method, logit_cap)
  - name: __init__
    signature: (self, model_runner: ModelRunner, skip_prefill: bool = False, kv_indptr_buf: Optional[torch.Tensor] = None)
    class: TritonAttnBackend
  - name: get_num_kv_splits
    signature: (self, num_kv_splits: torch.Tensor, seq_lens: torch.Tensor)
    class: TritonAttnBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: TritonAttnBackend
    doc: Init auxiliary variables for triton attention backend.
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor] = None)
    class: TritonAttnBackend
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
    class: TritonAttnBackend
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])
    class: TritonAttnBackend
  - name: get_cuda_graph_seq_len_fill_value
    signature: (self)
    class: TritonAttnBackend
  - name: forward_extend
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache = True, sinks = None)
    class: TritonAttnBackend
  - name: forward_decode
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache = True, sinks = None)
    class: TritonAttnBackend
  - name: __init__
    signature: (self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)
    class: TritonMultiStepDraftBackend
  - name: common_template
    signature: (self, forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: int)
    class: TritonMultiStepDraftBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: TritonMultiStepDraftBackend
  - name: call_fn
    signature: (i, forward_batch)
    class: TritonMultiStepDraftBackend
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int)
    class: TritonMultiStepDraftBackend
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, forward_batch: ForwardBatch)
    class: TritonMultiStepDraftBackend
  - name: call_fn
    signature: (i, forward_batch)
    class: TritonMultiStepDraftBackend
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, forward_batch: ForwardBatch, bs: int)
    class: TritonMultiStepDraftBackend
  - name: call_fn
    signature: (i, forward_batch)
    class: TritonMultiStepDraftBackend
  - name: get_num_kv_splits_triton
    signature: (num_kv_splits_ptr, seq_lens_ptr, num_seq, num_group, num_head, num_kv_head, max_kv_splits, device_core_count, MAX_NUM_SEQ: tl.constexpr)
  - name: update_sliding_window_buffer
    signature: (window_kv_indptr, req_to_token, sliding_window_size, seq_lens, req_pool_indices, bs, device, token_to_kv_pool_allocator = None)
  - name: update_sliding_window_buffer_cuda_graph
    signature: (window_kv_indptr, window_kv_indices, req_to_token, sliding_window_size, seq_lens, req_pool_indices, bs, token_to_kv_pool_allocator = None)

File: layers/attention/triton_ops/decode_attention.py
  - name: tanh
    signature: (x)
  - name: _fwd_kernel_stage1
    signature: (Q, K_Buffer, V_Buffer, sm_scale, kv_indptr, kv_indices, Att_Out, Att_Lse, num_kv_splits, stride_qbs, stride_qh, stride_buf_kbs, stride_buf_kh, stride_buf_vbs, stride_buf_vh, stride_mid_ob, stride_mid_oh, stride_mid_os, kv_group_num: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_DV: tl.constexpr, BLOCK_N: tl.constexpr, MIN_BLOCK_KV: tl.constexpr, logit_cap: tl.constexpr, Lk: tl.constexpr, Lv: tl.constexpr, xai_temperature_len: tl.constexpr)
  - name: _decode_att_m_fwd
    signature: (q, k_buffer, v_buffer, att_out, att_lse, kv_indptr, kv_indices, num_kv_splits, max_kv_splits, sm_scale, logit_cap, xai_temperature_len = -1)
  - name: _fwd_grouped_kernel_stage1
    signature: (Q, K_Buffer, V_Buffer, sm_scale, kv_indptr, kv_indices, Att_Out, Att_Lse, num_kv_splits, stride_qbs, stride_qh, stride_buf_kbs, stride_buf_kh, stride_buf_vbs, stride_buf_vh, stride_mid_ob, stride_mid_oh, stride_mid_os, kv_group_num: tl.constexpr, q_head_num: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_DPE: tl.constexpr, BLOCK_DV: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_H: tl.constexpr, MIN_BLOCK_KV: tl.constexpr, logit_cap: tl.constexpr, xai_temperature_len: tl.constexpr, Lk: tl.constexpr, Lv: tl.constexpr)
  - name: _decode_grouped_att_m_fwd
    signature: (q, k_buffer, v_buffer, att_out, att_lse, kv_indptr, kv_indices, num_kv_splits, max_kv_splits, sm_scale, logit_cap, xai_temperature_len = -1)
  - name: _fwd_kernel_stage2
    signature: (Mid_O, Mid_O_1, O, kv_indptr, num_kv_splits, sink_ptr, stride_mid_ob, stride_mid_oh, stride_mid_os, stride_obs, stride_oh, MAX_KV_SPLITS: tl.constexpr, MIN_BLOCK_KV: tl.constexpr, BLOCK_DV: tl.constexpr, Lv: tl.constexpr, HAS_SINK: tl.constexpr)
  - name: _decode_softmax_reducev_fwd
    signature: (logits, lse, q, o, v_buffer, kv_indptr, num_kv_splits, max_kv_splits, sinks = None)
  - name: decode_attention_fwd_normal
    signature: (q, k_buffer, v_buffer, o, kv_indptr, kv_indices, attn_logits, attn_lse, num_kv_splits, max_kv_splits, sm_scale, logit_cap = 0.0, sinks = None, xai_temperature_len = -1)
  - name: decode_attention_fwd_grouped
    signature: (q, k_buffer, v_buffer, o, kv_indptr, kv_indices, attn_logits, attn_lse, num_kv_splits, max_kv_splits, sm_scale, logit_cap = 0.0, sinks = None, xai_temperature_len = -1)
  - name: decode_attention_fwd
    signature: (q, k_buffer, v_buffer, o, kv_indptr, kv_indices, attn_logits, attn_lse, num_kv_splits, max_kv_splits, sm_scale, logit_cap = 0.0, sinks = None, xai_temperature_len = -1)

File: layers/attention/triton_ops/double_sparsity_attention.py
  - name: tanh
    signature: (x)
  - name: _fwd_kernel_flash_decode_stage1
    signature: (Q, K, V, sm_scale, Req_to_tokens, B_req_idx, B_Seqlen, Mid_O, Mid_O_LogExpSum, stride_req_to_tokens_b, stride_req_to_tokens_s, stride_qbs, stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd, stride_vbs, stride_vh, stride_vd, stride_mid_ob, stride_mid_oh, stride_mid_os, stride_mid_od, stride_mid_o_eb, stride_mid_o_eh, stride_mid_o_es, gqa_group_size, BLOCK_SEQ: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr)
  - name: _fwd_kernel_flash_decode_stage2
    signature: (B_Seqlen, Mid_O, Mid_O_LogExpSum, O, stride_mid_ob, stride_mid_oh, stride_mid_os, stride_mid_od, stride_mid_o_eb, stride_mid_o_eh, stride_mid_o_es, stride_obs, stride_oh, stride_od, BLOCK_SEQ: tl.constexpr, BLOCK_DMODEL: tl.constexpr)
  - name: flash_decode_stage1
    signature: (q, k, v, Req_to_tokens, B_req_idx, B_Seqlen, max_len_in_batch, mid_out, mid_out_logsumexp, block_seq)
  - name: flash_decode_stage2
    signature: (mid_out, mid_out_logexpsum, B_Seqlen, O, block_seq)
  - name: flash_decode_attention_fwd
    signature: (q, k_buffer, v_buffer, o, req_to_token, b_req_idx, b_start_loc, b_seq_len, attn_logits, max_len_in_batch, sm_scale, logit_cap = 0.0)
  - name: _sparse_fwd_kernel_flash_decode_stage1
    signature: (Q_Label, K_Label_Buffer, sm_scale, Req_to_tokens, B_Seqlen, Att_Out, stride_req_to_tokens_b, stride_qbs, stride_qh, stride_buf_kbs, stride_buf_kh, att_stride_h, att_stride_b, kv_group_num: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr, logit_cap: tl.constexpr)
  - name: _sparse_fwd_kernel_flash_decode_stage2
    signature: (Q, K, V, sm_scale, Req_to_tokens, Topk_token_indices, Mid_O, Mid_O_LogExpSum, Heavy_token_num, stride_req_to_tokens_b, stride_topk_token_indices_h, stride_topk_token_indices_b, stride_qbs, stride_qh, stride_kbs, stride_kh, stride_vbs, stride_vh, stride_mid_ob, stride_mid_oh, stride_mid_os, stride_mid_o_eb, stride_mid_o_eh, gqa_group_size, BLOCK_SEQ: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr)
  - name: _sparse_fwd_kernel_flash_decode_stage3
    signature: (Mid_O, Mid_O_LogExpSum, O, seq_len, stride_mid_ob, stride_mid_oh, stride_mid_os, stride_mid_o_eb, stride_mid_o_eh, stride_obs, stride_oh, BLOCK_SEQ: tl.constexpr, BLOCK_DMODEL: tl.constexpr)
  - name: sparse_flash_decode_stage1
    signature: (q_label, k_label_buffer, att_out, Req_to_tokens, B_Seqlen, max_len_in_batch, sm_scale, logit_cap)
  - name: sparse_flash_decode_stage2
    signature: (q, k, v, Req_to_tokens, Topk_token_indices, heavy_token_num, mid_out, mid_out_logsumexp, block_seq, sm_scale)
  - name: sparse_flash_decode_stage3
    signature: (Seqlen, mid_out, mid_out_logexpsum, O, block_seq)
  - name: flash_decode_sparse_attention_fwd
    signature: (q, k_buffer, v_buffer, o, q_label, k_label_buffer, req_to_token, b_seq_len, max_len_in_batch, sm_scale, logit_cap, heavy_token_num = 32, att_out_approx = None, mid_out = None, mid_o_logexpsum = None, BLOCK_SEQ = 256)
  - name: _fwd_kernel
    signature: (Q_Extend, K_Extend, V_Extend, O_Extend, K_Buffer, V_Buffer, Req_to_tokens, B_req_idx, B_Seq_Len, B_Start_Loc_Extend, B_Seq_Len_Extend, sm_scale, kv_group_num, stride_qbs, stride_qh, stride_kbs, stride_kh, stride_vbs, stride_vh, stride_obs, stride_oh, stride_buf_kbs, stride_buf_kh, stride_buf_vbs, stride_buf_vh, stride_req_to_tokens_b, logit_cap: tl.constexpr, Lq: tl.constexpr, Lv: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_DPE: tl.constexpr, BLOCK_DV: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr)
  - name: extend_attention_fwd
    signature: (q_extend, k_extend, v_extend, o_extend, k_buffer, v_buffer, req_to_tokens, b_req_idx, b_seq_len, b_seq_len_extend, b_start_loc_extend, max_len_extend, sm_scale = None, logit_cap = 0.0)
    doc: q_extend, k_extend, v_extend, o_extend: contiguous tensors

File: layers/attention/triton_ops/extend_attention.py
  - name: tanh
    signature: (x)
  - name: _fwd_kernel
    signature: (Q_Extend, K_Extend, V_Extend, O_Extend, K_Buffer, V_Buffer, qo_indptr, kv_indptr, kv_indices, mask_ptr, mask_indptr, sink_ptr, window_kv_offset_ptr, sm_scale, kv_group_num, stride_qbs, stride_qh, stride_kbs, stride_kh, stride_vbs, stride_vh, stride_obs, stride_oh, stride_buf_kbs, stride_buf_kh, stride_buf_vbs, stride_buf_vh, SLIDING_WINDOW_SIZE: tl.constexpr, logit_cap: tl.constexpr, xai_temperature_len: tl.constexpr, Lq: tl.constexpr, Lv: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_DPE: tl.constexpr, BLOCK_DV: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, USE_CUSTOM_MASK: tl.constexpr, IS_CAUSAL: tl.constexpr, SKIP_PREFIX_CUSTOM_MASK: tl.constexpr, STORE_TRANSPOSE: tl.constexpr, HAS_SINK: tl.constexpr)
  - name: extend_attention_fwd
    signature: (q_extend, k_extend, v_extend, o_extend, k_buffer, v_buffer, qo_indptr, kv_indptr, kv_indices, custom_mask, is_causal, mask_indptr, max_len_extend, sm_scale = None, logit_cap = 0.0, skip_prefix_custom_mask = True, sliding_window_size = -1, sinks = None, window_kv_offsets = None, xai_temperature_len = -1)
    doc: q_extend, k_extend, v_extend, o_extend: contiguous tensors
  - name: redundant_attention
    signature: (q_extend, o_extend, k_buffer, v_buffer, b_req_idx, b_start_loc, b_seq_len, b_seq_len_prefix, max_len_in_batch)

File: layers/attention/triton_ops/merge_state.py
  - name: merge_state_kernel
    signature: (output, output_lse, prefix_output, prefix_lse, suffix_output, suffix_lse, HEAD_SIZE: tl.constexpr, PADDED_HEAD_SIZE: tl.constexpr, OUTPUT_LSE: tl.constexpr)
  - name: merge_state_triton
    signature: (prefix_output: torch.Tensor, prefix_lse: torch.Tensor, suffix_output: torch.Tensor, suffix_lse: torch.Tensor, output: Optional[torch.Tensor] = None, output_lse: Optional[torch.Tensor] = None)
    return: Tuple[torch.Tensor, Optional[torch.Tensor]]

File: layers/attention/triton_ops/prefill_attention.py
  - name: _fwd_kernel
    signature: (Q, K, V, sm_scale, B_Start_Loc, B_Seqlen, Out, stride_qbs, stride_qh, stride_kbs, stride_kh, stride_vbs, stride_vh, stride_obs, stride_oh, kv_group_num: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr, IS_CAUSAL: tl.constexpr, Lk: tl.constexpr)
  - name: context_attention_fwd
    signature: (q, k, v, o, b_start_loc, b_seq_len, max_input_len, is_causal = True)
    doc: q, k, v: [b * s, head, head_dim]

File: layers/attention/triton_ops/rocm_mla_decode_rope.py
  - name: is_hip
    signature: ()
  - name: tanh
    signature: (x)
  - name: _fwd_grouped_kernel_stage1_rope
    signature: (Q, K_Buffer, V_buffer, cos_sin_cache, positions, sm_scale, kv_indptr, kv_indices, Att_Out, k_pe_t_out, stride_qb, stride_qh, stride_buf_kbs, stride_buf_vbs, stride_mid_ob, stride_mid_oh, stride_mid_os, stride_kpe_tokens_out_b, stride_cos_sin_cache_s, stride_positions_b, rotary_dim: tl.constexpr, kv_lora_rank: tl.constexpr, qk_rope_head_dim: tl.constexpr, kv_group_num: tl.constexpr, q_head_num: tl.constexpr, BLOCK_C: tl.constexpr, BLOCK_R: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_H: tl.constexpr, NUM_KV_SPLITS: tl.constexpr, logit_cap: tl.constexpr, USE_ROPE: tl.constexpr, IS_NEOX_STYLE: tl.constexpr)
  - name: _decode_grouped_att_m_fwd_rope
    signature: (q, k_buffer, v_buffer, att_out, k_pe_tokens_out, kv_lora_rank, cos_sin_cache, positions, rotary_dim, kv_indptr, kv_indices, num_kv_splits, sm_scale, logit_cap, use_rope, is_neox_style = True)
  - name: decode_attention_fwd_grouped_rope
    signature: (q, k_buffer, v_buffer, o, kv_indptr, kv_indices, k_pe_tokens, kv_lora_rank, rotary_dim, cos_sin_cache, positions, attn_logits, num_kv_splits, sm_scale, logit_cap = 0.0, use_rope = False, is_neox_style = False)

File: layers/attention/trtllm_mha_backend.py
  - name: __init__
    signature: (self, model_runner: ModelRunner, skip_prefill: bool = False, kv_indptr_buf: Optional[torch.Tensor] = None, kv_last_page_len_buf: Optional[torch.Tensor] = None, speculative_step_id: int = 0)
    class: TRTLLMHAAttnBackend
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor] = None)
    class: TRTLLMHAAttnBackend
    doc: Initialize CUDA graph state for TRTLLM MHA.
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])
    class: TRTLLMHAAttnBackend
    doc: Initialize metadata for CUDA graph capture.
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])
    class: TRTLLMHAAttnBackend
    doc: Replay CUDA graph with new inputs.
  - name: get_cuda_graph_seq_len_fill_value
    signature: (self)
    return: int
    class: TRTLLMHAAttnBackend
    doc: Get the fill value for sequence lengths in CUDA graph.
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: TRTLLMHAAttnBackend
    doc: Initialize the metadata for a forward pass.
  - name: forward_decode
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool = True, **kwargs)
    return: torch.Tensor
    class: TRTLLMHAAttnBackend
    doc: Run forward for decode using TRTLLM MHA kernel.
  - name: forward_extend
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache = True, **kwargs)
    class: TRTLLMHAAttnBackend
  - name: __init__
    signature: (self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)
    class: TRTLLMHAAttnMultiStepDraftBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: TRTLLMHAAttnMultiStepDraftBackend
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int)
    class: TRTLLMHAAttnMultiStepDraftBackend
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, forward_batch: ForwardBatch)
    class: TRTLLMHAAttnMultiStepDraftBackend
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, forward_batch: ForwardBatch, bs: int)
    class: TRTLLMHAAttnMultiStepDraftBackend

File: layers/attention/trtllm_mla_backend.py
  - name: __init__
    signature: (self, model_runner: ModelRunner, skip_prefill: bool = False, kv_indptr_buf: Optional[torch.Tensor] = None, q_indptr_decode_buf: Optional[torch.Tensor] = None)
    class: TRTLLMMLABackend
  - name: _calc_padded_blocks
    signature: (self, max_seq_len: int)
    return: int
    class: TRTLLMMLABackend
    doc: Calculate padded block count that satisfies both TRT-LLM and Triton constraints.
  - name: _create_block_kv_indices
    signature: (self, batch_size: int, max_blocks: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, device: torch.device)
    return: torch.Tensor
    class: TRTLLMMLABackend
    doc: Create block KV indices tensor using Triton kernel.
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor] = None)
    class: TRTLLMMLABackend
    doc: Initialize CUDA graph state for TRTLLM MLA.
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])
    class: TRTLLMMLABackend
    doc: Initialize metadata for CUDA graph capture.
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])
    class: TRTLLMMLABackend
    doc: Replay CUDA graph with new inputs.
  - name: get_cuda_graph_seq_len_fill_value
    signature: (self)
    return: int
    class: TRTLLMMLABackend
    doc: Get the fill value for sequence lengths in CUDA graph.
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: TRTLLMMLABackend
    doc: Initialize the metadata for a forward pass.
  - name: quantize_and_rope_for_fp8
    signature: (self, q_nope: torch.Tensor, q_rope: torch.Tensor, k_nope: torch.Tensor, k_rope: torch.Tensor, forward_batch: ForwardBatch, cos_sin_cache: torch.Tensor, is_neox: bool)
    return: tuple[torch.Tensor, torch.Tensor, torch.Tensor]
    class: TRTLLMMLABackend
    doc: Quantize and apply RoPE for FP8 attention path.
  - name: forward_decode
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool = True, q_rope: Optional[torch.Tensor] = None, k_rope: Optional[torch.Tensor] = None, cos_sin_cache: Optional[torch.Tensor] = None, is_neox: Optional[bool] = False)
    return: torch.Tensor
    class: TRTLLMMLABackend
    doc: Run forward for decode using TRTLLM MLA kernel.
  - name: __init__
    signature: (self, model_runner: 'ModelRunner', topk: int, speculative_num_steps: int)
    class: TRTLLMMLAMultiStepDraftBackend

File: layers/attention/utils.py
  - name: create_flashinfer_kv_indices_triton
    signature: (req_to_token_ptr, req_pool_indices_ptr, page_kernel_lens_ptr, kv_indptr, kv_start_idx, kv_indices_ptr, req_to_token_ptr_stride: tl.constexpr)
  - name: create_flashmla_kv_indices_triton
    signature: (req_to_token_ptr, req_pool_indices_ptr, page_kernel_lens_ptr, kv_start_idx, kv_indices_ptr, req_to_token_ptr_stride: tl.constexpr, kv_indices_ptr_stride: tl.constexpr, NUM_PAGE_PER_BLOCK: tl.constexpr = TRITON_PAD_NUM_PAGE_PER_BLOCK, PAGED_SIZE: tl.constexpr = 64)

File: layers/attention/vision.py
  - name: set_data
    signature: (self, value: Any)
    return: None
    class: SingletonCache
  - name: get_data
    signature: (self)
    return: Optional[Any]
    class: SingletonCache
  - name: empty
    signature: (self)
    return: bool
    class: SingletonCache
  - name: _get_cu_seqlens_for_shape
    signature: (batch_size: int, seqlen: int, device)
    return: torch.Tensor
    doc: Generates cumulative sequence lengths (cu_seqlens) for a given batch_size, seqlen, and device.
  - name: __init__
    signature: (self, head_dim: int, num_heads: int, num_kv_heads: int, dropout: float = 0.0, flatten_batch: bool = False, softmax_in_single_precision: bool = False, **kwargs)
    class: VisionSdpaAttention
  - name: _generate_mask_cache
    signature: (s: int, flatten_batch: bool, cu_seqlens: tuple)
    return: torch.BoolTensor
    class: VisionSdpaAttention
    doc: Generate a boolean attention mask with caching mechanism.
  - name: generate_patch_attention_mask
    signature: (self, s: int, cu_seqlens: Optional[torch.Tensor], flatten_batch: bool = False)
    return: Optional[torch.Tensor]
    class: VisionSdpaAttention
    doc: Creates a non-causal 4D mask of shape `(b, 1, s, s)` or `(1, 1, s, s)`.
  - name: forward
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, bsz: int, cu_seqlens: Optional[torch.Tensor] = None, attention_mask: Optional[torch.Tensor] = None, **kwargs)
    return: torch.Tensor
    class: VisionSdpaAttention
    doc: Args:
  - name: __init__
    signature: (self, **kwargs)
    class: VisionTritonAttention
  - name: forward
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens: Optional[torch.Tensor], bsz: int, seq_len: int, **kwargs)
    return: torch.Tensor
    class: VisionTritonAttention
    doc: Args:
  - name: __init__
    signature: (self, **kwargs)
    class: VisionFlash3Attention
  - name: forward
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens: Optional[Union[SingletonCache, torch.Tensor]], bsz: int, seq_len: int, **kwargs)
    return: torch.Tensor
    class: VisionFlash3Attention
    doc: Args:
  - name: __init__
    signature: (self, embed_dim: int, num_heads: int, projection_size: int, use_qkv_parallel: bool, qkv_backend: Optional[str] = None, quant_config: Optional[QuantizationConfig] = None, dropout: float = 0.0, softmax_in_single_precision: bool = False, flatten_batch: bool = False, prefix: str = '', proj_bias: bool = True, num_dummy_heads: int = 0, qkv_bias: bool = True, qk_normalization: bool = False, layer_norm_eps: float = 1e-06, customized_position_embedding_applier: Callable[[torch.Tensor, torch.Tensor, Any, Any], Tuple[torch.Tensor, torch.Tensor]] = None, **kwargs)
    class: VisionAttention
  - name: _determine_attention_backend
    signature: (self, passed_backend: Optional[str])
    return: str
    class: VisionAttention
    doc: Decide the multimodal attention backend string.
  - name: _apply_qk_norm
    signature: (self, q: torch.Tensor, k: torch.Tensor)
    class: VisionAttention
    doc: apply qk norm for internvl vit attn
  - name: forward
    signature: (self, x: torch.Tensor, cu_seqlens: Optional[torch.Tensor] = None, position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, attention_mask: Optional[torch.Tensor] = None, **kwargs)
    return: torch.Tensor
    class: VisionAttention
    doc: Args:

File: layers/attention/vision_utils.py
  - name: update_vit_attn_dummy_heads_config
    signature: (config)
    doc: Update HF config to ensure vision attention num_attention_heads is divisible by tp_size
  - name: pad_vit_attn_dummy_heads
    signature: (config, name: str, loaded_weight: torch.Tensor)
    doc: Pad attention qkv weights for dummy heads

File: layers/attention/wave_backend.py
  - name: get_num_kv_splits_triton
    signature: (num_kv_splits_ptr, seq_lens_ptr, num_seq, num_group, num_head, num_kv_head, max_kv_splits, device_core_count, MAX_NUM_SEQ: tl.constexpr)
  - name: __init__
    signature: (self, model_runner: ModelRunner, skip_prefill: bool = False, kv_indptr_buf: Optional[torch.Tensor] = None)
    class: WaveAttnBackend
  - name: get_num_kv_splits
    signature: (self, num_kv_splits: torch.Tensor, seq_lens: torch.Tensor)
    class: WaveAttnBackend
  - name: init_forward_metadata
    signature: (self, forward_batch: ForwardBatch)
    class: WaveAttnBackend
    doc: Init auxiliary variables for wave attention backend.
  - name: init_cuda_graph_state
    signature: (self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor] = None)
    class: WaveAttnBackend
  - name: init_forward_metadata_capture_cuda_graph
    signature: (self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
    class: WaveAttnBackend
  - name: init_forward_metadata_replay_cuda_graph
    signature: (self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])
    class: WaveAttnBackend
  - name: get_cuda_graph_seq_len_fill_value
    signature: (self)
    class: WaveAttnBackend
  - name: forward_extend
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache = True)
    class: WaveAttnBackend
  - name: forward_decode
    signature: (self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache = True)
    class: WaveAttnBackend

File: layers/attention/wave_ops/decode_attention.py
  - name: get_wave_kernel
    signature: (shape: paged_decode_attention_shape, max_kv_splits, input_dtype, output_dtype, logit_cap)
  - name: decode_attention_intermediate_arrays_shapes
    signature: (num_seqs, head_size_kv, num_query_heads, max_kv_splits)
  - name: decode_attention_wave
    signature: (q, k_buffer, v_buffer, o, b_req_idx, req_to_token, attn_logits, attn_logits_max, num_kv_splits, max_kv_splits, sm_scale, logit_cap)
  - name: decode_attention_fwd
    signature: (q, k_buffer, v_buffer, o, b_req_idx, req_to_token, attn_logits, attn_logits_max, num_kv_splits, max_kv_splits, sm_scale, logit_cap = 0.0)

File: layers/attention/wave_ops/extend_attention.py
  - name: get_wave_kernel
    signature: (shape: AttentionShape, q_shape: tuple[int], k_shape: tuple[int], v_shape: tuple[int], k_cache_shape: tuple[int], v_cache_shape: tuple[int], o_shape: tuple[int], input_dtype: torch.dtype, output_dtype: torch.dtype, size_dtype: torch.dtype, is_causal: bool, logit_cap: float, layer_scaling: float)
  - name: extend_attention_wave
    signature: (q_extend, k_extend, v_extend, k_buffer, v_buffer, qo_indptr, kv_indptr, kv_indices, custom_mask, mask_indptr, max_seq_len, output, is_causal = True, layer_scaling = None, logit_cap = 0)

File: layers/attention/wave_ops/prefill_attention.py
  - name: prefill_attention_wave
    signature: (q, k, v, o, b_start_loc, b_seq_len, max_seq_len, is_causal = True)

File: layers/communicator.py
  - name: model_input_output
    signature: ()
    class: ScatterMode
    doc: The scatter mode for model forward pass input and output data
  - name: previous_layer
    signature: (self)
    class: _LayerModeComputationContext
  - name: init_new
    signature: (cls, **kwargs)
    class: LayerScatterModes
  - name: _compute_layer_input_mode
    signature: (cls, context: _LayerModeComputationContext)
    class: LayerScatterModes
  - name: _compute_mlp_mode
    signature: (cls, context: _LayerModeComputationContext)
    class: LayerScatterModes
  - name: _compute_middle_residual_mode
    signature: (cls, context: _LayerModeComputationContext)
    class: LayerScatterModes
  - name: _compute_layer_output_mode
    signature: (cls, context: _LayerModeComputationContext)
    class: LayerScatterModes
  - name: enable_moe_dense_fully_dp
    signature: ()
  - name: __init__
    signature: (self, layer_scatter_modes: LayerScatterModes, input_layernorm: torch.nn.Module, post_attention_layernorm: torch.nn.Module, allow_reduce_scatter: bool = False, is_last_layer: bool = False)
    class: LayerCommunicator
  - name: prepare_attn
    signature: (self, hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch)
    class: LayerCommunicator
  - name: prepare_mlp
    signature: (self, hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch)
    class: LayerCommunicator
  - name: postprocess_layer
    signature: (self, hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch)
    class: LayerCommunicator
  - name: should_use_reduce_scatter
    signature: (self, forward_batch: ForwardBatch)
    class: LayerCommunicator
  - name: should_fuse_mlp_allreduce_with_next_layer
    signature: (self, forward_batch: ForwardBatch)
    return: bool
    class: LayerCommunicator
  - name: is_same_group_size
    signature: (self, a: ScatterMode, b: ScatterMode)
    class: CommunicateContext
  - name: init_new
    signature: (cls)
    class: CommunicateContext
  - name: get_fn
    signature: (input_mode: ScatterMode, output_mode: ScatterMode, context: CommunicateContext)
    class: CommunicateSimpleFn
  - name: _trivial
    signature: (hidden_states: torch.Tensor, forward_batch: ForwardBatch, context: CommunicateContext)
    return: torch.Tensor
    class: CommunicateSimpleFn
  - name: _scattered_to_tp_attn_full
    signature: (hidden_states: torch.Tensor, forward_batch: ForwardBatch, context: CommunicateContext)
    return: torch.Tensor
    class: CommunicateSimpleFn
  - name: get_fn
    signature: (hidden_states_input_mode: ScatterMode, residual_input_mode: ScatterMode, hidden_states_output_mode: ScatterMode, residual_output_mode: ScatterMode, context: CommunicateContext)
    class: CommunicateWithAllReduceAndLayerNormFn
  - name: _simple
    signature: (hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch, layernorm: torch.nn.Module, context: CommunicateContext)
    class: CommunicateWithAllReduceAndLayerNormFn
  - name: _gather_hidden_states_and_residual
    signature: (hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch, layernorm: torch.nn.Module, context: CommunicateContext, *, residual_input_mode)
    class: CommunicateWithAllReduceAndLayerNormFn
  - name: _scatter_hidden_states_and_residual
    signature: (hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch, layernorm: torch.nn.Module, context: CommunicateContext, *, residual_input_mode)
    class: CommunicateWithAllReduceAndLayerNormFn
  - name: execute
    signature: (cls, hidden_states_input_mode, residual_input_mode, output_mode, context, **kwargs)
    class: CommunicateSummableTensorPairFn
  - name: get_fn
    signature: (hidden_states_input_mode: ScatterMode, residual_input_mode: ScatterMode, output_mode: ScatterMode, context: CommunicateContext)
    class: CommunicateSummableTensorPairFn
  - name: _trivial
    signature: (hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch, context: CommunicateContext, **kwargs)
    class: CommunicateSummableTensorPairFn
  - name: _scatter_hidden_states
    signature: (hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch, context: CommunicateContext, allow_reduce_scatter: bool = False)
    class: CommunicateSummableTensorPairFn
  - name: _gather
    signature: (hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch, context: CommunicateContext, **kwargs)
    class: CommunicateSummableTensorPairFn
  - name: _scatter
    signature: (hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch, context: CommunicateContext)
    class: CommunicateSummableTensorPairFn

File: layers/dp_attention.py
  - name: is_max_len
    signature: (self)
    class: DpPaddingMode
  - name: is_sum_len
    signature: (self)
    class: DpPaddingMode
  - name: get_dp_padding_mode
    signature: (cls, global_num_tokens: List[int])
    return: DpPaddingMode
    class: DpPaddingMode
  - name: get_default_mode_in_cuda_graph
    signature: (cls)
    return: DpPaddingMode
    class: DpPaddingMode
  - name: set_metadata
    signature: (cls, hidden_size: int, dtype: torch.dtype, device: torch.device)
    class: _DpGatheredBufferWrapper
  - name: set_dp_buffer_len
    signature: (cls, global_dp_buffer_len: int, local_dp_buffer_len: int, global_num_tokens: Optional[List[int]] = None)
    class: _DpGatheredBufferWrapper
  - name: get_global_dp_buffer
    signature: (cls)
    return: torch.Tensor
    class: _DpGatheredBufferWrapper
  - name: get_local_dp_buffer
    signature: (cls)
    return: torch.Tensor
    class: _DpGatheredBufferWrapper
  - name: get_global_dp_buffer_len
    signature: (cls)
    return: int
    class: _DpGatheredBufferWrapper
  - name: get_local_dp_buffer_len
    signature: (cls)
    return: int
    class: _DpGatheredBufferWrapper
  - name: get_dp_global_num_tokens
    signature: (cls)
    return: List[int]
    class: _DpGatheredBufferWrapper
  - name: set_dp_buffer_len
    signature: (global_dp_buffer_len: int, local_dp_buffer_len: int, global_num_tokens: Optional[List[int]] = None)
  - name: get_global_dp_buffer
    signature: ()
    return: torch.Tensor
  - name: get_local_dp_buffer
    signature: ()
    return: torch.Tensor
  - name: get_global_dp_buffer_len
    signature: ()
    return: int
  - name: get_local_dp_buffer_len
    signature: ()
    return: int
  - name: get_dp_global_num_tokens
    signature: ()
    return: List[int]
  - name: compute_dp_attention_world_info
    signature: (enable_dp_attention, tp_rank, tp_size, dp_size)
  - name: compute_dp_attention_local_info
    signature: (enable_dp_attention, tp_rank, tp_size, dp_size, moe_dense_tp_size)
  - name: initialize_dp_attention
    signature: (server_args: ServerArgs, model_config: ModelConfig)
  - name: is_dp_attention_enabled
    signature: ()
    return: bool
  - name: get_attention_tp_group
    signature: ()
    return: GroupCoordinator
  - name: get_attention_tp_rank
    signature: ()
    return: int
  - name: get_attention_tp_size
    signature: ()
    return: int
  - name: get_attention_dp_rank
    signature: ()
    return: int
  - name: get_attention_dp_size
    signature: ()
    return: int
  - name: get_local_attention_dp_rank
    signature: ()
    return: int
  - name: get_local_attention_dp_size
    signature: ()
    return: int
  - name: disable_dp_size
    signature: ()
    doc: Patch the tp group temporarily until this function ends.
  - name: get_dp_local_info
    signature: (forward_batch: ForwardBatch)
    return: Tuple[torch.Tensor, torch.Tensor]
  - name: memcpy_triton_kernel
    signature: (dst_ptr, src_ptr, offset_ptr, sz_ptr, offset_src: tl.constexpr, chunk_size, BLOCK_SIZE: tl.constexpr)
  - name: prod
    signature: (x)
  - name: memcpy_triton
    signature: (dst, src, dim, offset, sz, offset_src)
  - name: _dp_gather_via_all_reduce
    signature: (global_tokens: torch.Tensor, local_tokens: torch.Tensor, forward_batch: ForwardBatch, is_partial: bool)
  - name: _dp_gather_via_all_gather
    signature: (global_tokens: torch.Tensor, local_tokens: torch.Tensor, forward_batch: ForwardBatch, is_partial: bool)
  - name: _dp_gather
    signature: (global_tokens: torch.Tensor, local_tokens: torch.Tensor, forward_batch: ForwardBatch, is_partial: bool)
  - name: dp_gather_partial
    signature: (global_tokens: torch.Tensor, local_tokens: torch.Tensor, forward_batch: ForwardBatch)
  - name: dp_gather_replicate
    signature: (global_tokens: torch.Tensor, local_tokens: torch.Tensor, forward_batch: ForwardBatch)
  - name: dp_scatter
    signature: (local_tokens: torch.Tensor, global_tokens: torch.Tensor, forward_batch: ForwardBatch)
  - name: dp_reduce_scatter_tensor
    signature: (output: torch.Tensor, input: torch.Tensor)
  - name: attn_tp_reduce_scatter_tensor
    signature: (output: torch.Tensor, input: torch.Tensor)
  - name: attn_tp_all_gather_into_tensor
    signature: (output: torch.Tensor, input: torch.Tensor)
  - name: attn_tp_all_gather
    signature: (output_list: List[torch.Tensor], input: torch.Tensor)

File: layers/elementwise.py
  - name: fused_softcap_kernel
    signature: (output_ptr, input_ptr, n_ele, softcap_const: tl.constexpr, BLOCK_SIZE: tl.constexpr)
  - name: fused_softcap
    signature: (x, softcap_const, autotune = False)
  - name: __init__
    signature: (self, softcap_const: float)
    class: Softcap
  - name: __call__
    signature: (self, *args, **kwargs)
    class: Softcap
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: Softcap
  - name: forward_native
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: Softcap
  - name: forward_cuda
    signature: (self, x: torch.Tensor, autotune = False)
    return: torch.Tensor
    class: Softcap
  - name: fused_dual_residual_rmsnorm_kernel
    signature: (output_ptr, mid_ptr, activ_ptr, residual_ptr, weight1_ptr, weight2_ptr, eps: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)
  - name: fused_dual_residual_rmsnorm
    signature: (x, residual, weight1, weight2, eps, autotune = False)
  - name: fused_rmsnorm_kernel
    signature: (output_ptr, activ_ptr, weight_ptr, eps: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)
  - name: fused_rmsnorm
    signature: (x, weight, eps, autotune = False, inplace = False)
  - name: __init__
    signature: (self, rmsnorm1, rmsnorm2)
    return: None
    class: FusedDualResidualRMSNorm
  - name: __call__
    signature: (self, *args, **kwargs)
    class: FusedDualResidualRMSNorm
  - name: forward
    signature: (self, x: torch.Tensor, residual: torch.Tensor)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: FusedDualResidualRMSNorm
  - name: forward_cuda
    signature: (self, x: torch.Tensor, residual: torch.Tensor, autotune = False)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: FusedDualResidualRMSNorm
  - name: forward_flashinfer
    signature: (self, x: torch.Tensor, residual: torch.Tensor)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: FusedDualResidualRMSNorm
  - name: forward_native
    signature: (self, x: torch.Tensor, residual: torch.Tensor)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: FusedDualResidualRMSNorm
  - name: experts_combine_kernel
    signature: (out_hidden_states, moe_hidden_states, mlp_hidden_states, combine_k: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)
  - name: experts_combine_triton
    signature: (moe_hidden_states, mlp_hidden_states, output_buffer = None)
  - name: gelu_and_mul_kernel
    signature: (out_hidden_states_ptr, out_scales_ptr, hidden_states_ptr, quant_max: tl.constexpr, static_scale: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)
  - name: gelu_and_mul_triton
    signature: (hidden_states, scales = None, quantize = None, out = None)
  - name: silu_and_mul_kernel
    signature: (out_hidden_states_ptr, out_scales_ptr, hidden_states_ptr, quant_max: tl.constexpr, static_scale: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)
  - name: silu_and_mul_triton
    signature: (hidden_states, scales = None, quantize = None, out = None)

File: layers/flashinfer_comm_fusion.py
  - name: __init__
    signature: (self)
    class: FlashInferWorkspaceManager
  - name: initialize
    signature: (self, world_size: int, rank: int, max_token_num: int, hidden_dim: int, group = None, use_fp32_lamport: bool = False)
    class: FlashInferWorkspaceManager
    doc: Initialize workspace
  - name: cleanup
    signature: (self)
    class: FlashInferWorkspaceManager
    doc: Clean up workspace
  - name: ensure_workspace_initialized
    signature: (max_token_num: int = 2048, hidden_dim: int = 4096, use_fp32_lamport: bool = False)
    doc: Ensure workspace is initialized
  - name: flashinfer_allreduce_residual_rmsnorm
    signature: (input_tensor: torch.Tensor, residual: torch.Tensor, weight: torch.Tensor, eps: float = 1e-06, max_token_num: int = 2048, use_oneshot: Optional[bool] = None, trigger_completion_at_end: bool = False, fp32_acc: bool = False)
    return: Tuple[torch.Tensor, torch.Tensor]
    doc: Use FlashInfer's fused allreduce + residual + RMS norm operation
  - name: fake_flashinfer_allreduce_residual_rmsnorm
    signature: (input_tensor: torch.Tensor, residual: torch.Tensor, weight: torch.Tensor, eps: float = 1e-06, max_token_num: int = 2048, use_oneshot: Optional[bool] = None, trigger_completion_at_end: bool = False, fp32_acc: bool = False)
    return: Tuple[torch.Tensor, torch.Tensor]
  - name: cleanup_flashinfer_workspace
    signature: ()

File: layers/layernorm.py
  - name: __init__
    signature: (self, hidden_size: int, eps: float = 1e-06, var_hidden_size: Optional[int] = None)
    return: None
    class: RMSNorm
  - name: forward_cuda
    signature: (self, x: torch.Tensor, residual: Optional[torch.Tensor] = None)
    return: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
    class: RMSNorm
  - name: forward_npu
    signature: (self, x: torch.Tensor, residual: Optional[torch.Tensor] = None)
    return: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
    class: RMSNorm
  - name: forward_aiter
    signature: (self, x: torch.Tensor, residual: Optional[torch.Tensor] = None)
    return: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
    class: RMSNorm
  - name: forward_hip
    signature: (self, x: torch.Tensor, residual: Optional[torch.Tensor] = None)
    return: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
    class: RMSNorm
  - name: forward_native
    signature: (self, x: torch.Tensor, residual: Optional[torch.Tensor] = None)
    return: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
    class: RMSNorm
  - name: forward_cpu
    signature: (self, x: torch.Tensor, residual: Optional[torch.Tensor] = None)
    return: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
    class: RMSNorm
  - name: forward_with_allreduce_fusion
    signature: (self, x: torch.Tensor, residual: Optional[torch.Tensor] = None)
    return: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
    class: RMSNorm
    doc: Forward method with allreduce fusion, prioritizing flashinfer fused operations
  - name: __init__
    signature: (self, hidden_size: int, eps: float = 1e-06)
    return: None
    class: GemmaRMSNorm
  - name: forward_native
    signature: (self, x: torch.Tensor, residual: Optional[torch.Tensor] = None)
    return: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
    class: GemmaRMSNorm
  - name: forward_cuda
    signature: (self, x: torch.Tensor, residual: Optional[torch.Tensor] = None)
    return: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
    class: GemmaRMSNorm
  - name: forward_npu
    signature: (self, x: torch.Tensor, residual: Optional[torch.Tensor] = None)
    return: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
    class: GemmaRMSNorm
  - name: __init__
    signature: (self, dim: int, eps: float = 1e-06)
    class: Gemma3RMSNorm
  - name: _norm
    signature: (self, x)
    class: Gemma3RMSNorm
  - name: forward_native
    signature: (self, x)
    class: Gemma3RMSNorm
  - name: forward_cuda
    signature: (self, x)
    class: Gemma3RMSNorm
  - name: forward_npu
    signature: (self, x)
    class: Gemma3RMSNorm
  - name: extra_repr
    signature: (self)
    class: Gemma3RMSNorm

File: layers/linear.py
  - name: adjust_marlin_shard
    signature: (param, shard_size, shard_offset)
  - name: adjust_bitsandbytes_4bit_shard
    signature: (param: Parameter, shard_offsets: Dict[str, Tuple[int, int]], loaded_shard_id: str)
    return: Tuple[int, int]
    doc: Adjust the quantization offsets and sizes for BitsAndBytes sharding.
  - name: adjust_scalar_to_fused_array
    signature: (param, loaded_weight, shard_id)
    doc: For fused modules (QKV and MLP) we have an array of length
  - name: adjust_shard_offsets
    signature: (shard_offsets, loaded_weight, dim)
  - name: __init__
    signature: (self, input_size: int, output_size: int, skip_bias_add: bool = False, params_dtype: Optional[torch.dtype] = None, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: LinearBase
  - name: forward
    signature: (self, x: torch.Tensor)
    return: torch.Tensor
    class: LinearBase
  - name: __init__
    signature: (self, input_size: int, output_size: int, bias: bool = True, skip_bias_add: bool = False, params_dtype: Optional[torch.dtype] = None, quant_config: Optional[QuantizationConfig] = None, prefix: str = '')
    class: ReplicatedLinear
  - name: weight_loader
    signature: (self, param: Parameter, loaded_weight: torch.Tensor)
    class: ReplicatedLinear
  - name: forward
    signature: (self, x: torch.Tensor)
    return: Tuple[torch.Tensor, Optional[torch.Tensor]]
    class: ReplicatedLinear
  - name: extra_repr
    signature: (self)
    return: str
    class: ReplicatedLinear
  - name: __init__
    signature: (self, input_size: int, output_size: int, bias: bool = True, gather_output: bool = False, skip_bias_add: bool = False, params_dtype: Optional[torch.dtype] = None, quant_config: Optional[QuantizationConfig] = None, output_sizes: Optional[List[int]] = None, prefix: str = '', tp_rank: Optional[int] = None, tp_size: Optional[int] = None, use_presharded_weights: bool = False)
    class: ColumnParallelLinear
  - name: weight_loader
    signature: (self, param: Parameter, loaded_weight: torch.Tensor)
    class: ColumnParallelLinear
  - name: weight_loader_v2
    signature: (self, param: Parameter, loaded_weight: torch.Tensor)
    class: ColumnParallelLinear
  - name: forward
    signature: (self, input_)
    class: ColumnParallelLinear
  - name: extra_repr
    signature: (self)
    return: str
    class: ColumnParallelLinear
  - name: __init__
    signature: (self, input_size: int, output_sizes: List[int], bias: bool = True, gather_output: bool = False, skip_bias_add: bool = False, params_dtype: Optional[torch.dtype] = None, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', tp_rank: Optional[int] = None, tp_size: Optional[int] = None, use_presharded_weights: bool = False)
    class: MergedColumnParallelLinear
  - name: weight_loader
    signature: (self, param: Parameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[int] = None)
    class: MergedColumnParallelLinear
  - name: _load_fused_module_from_checkpoint
    signature: (self, param: BasevLLMParameter, loaded_weight: torch.Tensor)
    class: MergedColumnParallelLinear
    doc: Handle special case for models where MLP layers are already
  - name: weight_loader_v2
    signature: (self, param: BasevLLMParameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[int] = None)
    class: MergedColumnParallelLinear
  - name: __init__
    signature: (self, hidden_size: int, head_size: int, total_num_heads: int, total_num_kv_heads: Optional[int] = None, bias: bool = True, skip_bias_add: bool = False, params_dtype: Optional[torch.dtype] = None, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', tp_rank: Optional[int] = None, tp_size: Optional[int] = None, load_presharded_attn: bool = False)
    class: QKVParallelLinear
  - name: _get_shard_offset_mapping
    signature: (self, loaded_shard_id: str)
    class: QKVParallelLinear
  - name: _get_shard_size_mapping
    signature: (self, loaded_shard_id: str)
    class: QKVParallelLinear
  - name: _load_fused_module_from_checkpoint
    signature: (self, param: BasevLLMParameter, loaded_weight: torch.Tensor)
    class: QKVParallelLinear
    doc: Handle special case for models where QKV layers are already
  - name: weight_loader_v2
    signature: (self, param: BasevLLMParameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[str] = None)
    class: QKVParallelLinear
  - name: weight_loader
    signature: (self, param: Parameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[str] = None)
    class: QKVParallelLinear
  - name: __init__
    signature: (self, input_size: int, output_size: int, bias: bool = True, input_is_parallel: bool = True, skip_bias_add: bool = False, params_dtype: Optional[torch.dtype] = None, reduce_results: bool = True, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', tp_rank: Optional[int] = None, tp_size: Optional[int] = None, use_presharded_weights: bool = False)
    class: RowParallelLinear
  - name: weight_loader
    signature: (self, param: Parameter, loaded_weight: torch.Tensor)
    class: RowParallelLinear
  - name: weight_loader_v2
    signature: (self, param: BasevLLMParameter, loaded_weight: torch.Tensor)
    class: RowParallelLinear
  - name: forward
    signature: (self, input_, skip_all_reduce = False)
    class: RowParallelLinear
  - name: extra_repr
    signature: (self)
    return: str
    class: RowParallelLinear

File: layers/logits_processor.py
  - name: from_forward_batch
    signature: (cls, forward_batch: ForwardBatch)
    class: LogitsMetadata
  - name: compute_dp_attention_metadata
    signature: (self)
    class: LogitsMetadata
  - name: __init__
    signature: (self, config, skip_all_gather: bool = False, logit_scale: Optional[float] = None)
    class: LogitsProcessor
  - name: forward
    signature: (self, input_ids, hidden_states, lm_head: VocabParallelEmbedding, logits_metadata: Union[LogitsMetadata, ForwardBatch], aux_hidden_states: Optional[torch.Tensor] = None)
    return: LogitsProcessorOutput
    class: LogitsProcessor
  - name: _get_logits
    signature: (self, hidden_states: torch.Tensor, lm_head: VocabParallelEmbedding, logits_metadata: LogitsMetadata, embedding_bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: LogitsProcessor
    doc: Get logits from hidden_states.
  - name: get_top_logprobs
    signature: (all_logprobs: torch.Tensor, logits_metadata: LogitsMetadata)
    class: LogitsProcessor
  - name: get_token_ids_logprobs
    signature: (all_logprobs: torch.Tensor, logits_metadata: LogitsMetadata)
    class: LogitsProcessor
  - name: compute_temp_top_p_normalized_logprobs
    signature: (last_logits: torch.Tensor, logits_metadata: LogitsMetadata)
    return: torch.Tensor
    class: LogitsProcessor
    doc: compute logprobs for the output token from the given logits.
  - name: fused_softcap_kernel
    signature: (full_logits_ptr, softcapping_value, n_elements, BLOCK_SIZE: tl.constexpr)
  - name: fused_softcap
    signature: (full_logits, final_logit_softcapping)

File: layers/moe/__init__.py
  (no function definitions found)
File: layers/moe/cutlass_moe.py
  - name: cutlass_fused_experts_fp8
    signature: (a: torch.Tensor, w1_q: torch.Tensor, w2_q: torch.Tensor, w1_scale: torch.Tensor, w2_scale: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, a1_strides: torch.Tensor, c1_strides: torch.Tensor, a2_strides: torch.Tensor, c2_strides: torch.Tensor, workspace: torch.Tensor, a_ptrs: torch.Tensor, b_ptrs: torch.Tensor, out_ptrs: torch.Tensor, a_scales_ptrs: torch.Tensor, b_scales_ptrs: torch.Tensor, expert_offsets: torch.Tensor, problem_sizes1: torch.Tensor, problem_sizes2: torch.Tensor, use_fp8_blockscale: bool = True)
    return: torch.Tensor
    doc: Performs Fused MoE computation using CUTLASS-like kernels with FP8 weights and activations.
  - name: cutlass_moe_fp4
    signature: (a: torch.Tensor, a1_gscale: torch.Tensor, w1_fp4: torch.Tensor, w1_blockscale: torch.Tensor, w1_alphas: torch.Tensor, a2_gscale: torch.Tensor, w2_fp4: torch.Tensor, w2_blockscale: torch.Tensor, w2_alphas: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, params: CutlassMoEParams, apply_router_weight_on_input: bool = False)
    doc: MoE implementation for FP4 Inputs

File: layers/moe/cutlass_moe_params.py
  - name: __init__
    signature: (self, cutlass_moe_type: CutlassMoEType, device: torch.device, num_experts: int, intermediate_size_per_partition: int, hidden_size: int)
    class: CutlassMoEParams
  - name: to_gemm1_args
    signature: (self)
    return: dict
    class: CutlassMoEParams
  - name: to_gemm2_args
    signature: (self)
    return: dict
    class: CutlassMoEParams

File: layers/moe/cutlass_w4a8_moe.py
  - name: cutlass_w4a8_moe
    signature: (start_expert_id: int, end_expert_id: int, total_num_experts: int, a: torch.Tensor, w1_q: torch.Tensor, w2_q: torch.Tensor, w1_scale: torch.Tensor, w2_scale: torch.Tensor, topk_weights: torch.Tensor, topk_ids_: torch.Tensor, local_topk_ids: torch.Tensor, a_strides1: torch.Tensor, b_strides1: torch.Tensor, c_strides1: torch.Tensor, a_strides2: torch.Tensor, b_strides2: torch.Tensor, c_strides2: torch.Tensor, s_strides13: torch.Tensor, s_strides2: torch.Tensor, expert_offsets: torch.Tensor, problem_sizes1: torch.Tensor, problem_sizes2: torch.Tensor, a1_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, apply_router_weight_on_input: bool = False)
    return: torch.Tensor
    doc: This function computes a w4a8-quantized Mixture of Experts (MoE) layer

File: layers/moe/ep_moe/__init__.py
  (no function definitions found)
File: layers/moe/ep_moe/kernels.py
  - name: deepep_permute_triton_kernel
    signature: (input_ptr, gateup_input_ptr, src2dst_ptr, topk_ids_ptr, a1_scales_ptr, topk, hidden_size, BLOCK_SIZE: tl.constexpr)
  - name: deepep_post_reorder_triton_kernel
    signature: (down_output_ptr, output_ptr, src2dst_ptr, topk_ids_ptr, topk_weights_ptr, topk, hidden_size, BLOCK_SIZE: tl.constexpr)
  - name: compute_src2dst_triton_kernel
    signature: (reorder_ids, src2dst, num_toks, BLOCK_SIZE: tl.constexpr)
  - name: deepep_compute_src2dst_triton_kernel
    signature: (reorder_ids, src2dst, num_toks, num_minus_one, BLOCK_SIZE: tl.constexpr)
  - name: deepep_run_moe_deep_preprocess
    signature: (topk_ids: torch.Tensor, num_experts: int)
  - name: compute_seg_indptr_triton_kernel
    signature: (reorder_topk_ids, seg_indptr, num_toks)
  - name: run_moe_ep_preproess
    signature: (topk_ids: torch.Tensor, num_experts: int)
  - name: run_cutlass_moe_ep_preproess
    signature: (local_topk_ids: torch.Tensor, local_num_experts: int)
  - name: pre_reorder_triton_kernel_for_cutlass_moe
    signature: (input_ptr, gateup_input_ptr, src2dst_ptr, topk_ids_ptr, a1_scales_ptr, num_experts, topk, hidden_size, BLOCK_SIZE: tl.constexpr)
  - name: pre_reorder_triton_kernel
    signature: (input_ptr, gateup_input_ptr, src2dst_ptr, topk_ids_ptr, a1_scales_ptr, start_expert_id, end_expert_id, topk, hidden_size, BLOCK_SIZE: tl.constexpr, use_per_token_if_dynamic: tl.constexpr)
  - name: silu_and_mul_triton_kernel
    signature: (gateup_output, down_input, hidden_size, reorder_topk_ids, scales, start_expert_id, end_expert_id, BLOCK_SIZE: tl.constexpr)
  - name: _silu_and_mul_post_quant_kernel
    signature: (input_ptr, stride_input_0, stride_input_1, stride_input_2, output_ptr, stride_output_0, stride_output_1, stride_output_2, output_scale_ptr, stride_output_scale_0, stride_output_scale_1, stride_output_scale_2, masked_m_ptr, size_n, fp8_max, fp8_min, BLOCK_N: tl.constexpr, NUM_STAGE: tl.constexpr, SCALE_UE8M0: tl.constexpr)
  - name: silu_and_mul_masked_post_quant_fwd
    signature: (input: torch.Tensor, output: torch.Tensor, output_scale: torch.Tensor, quant_group_size: int, masked_m: torch.Tensor, scale_ue8m0: bool = False)
    doc: input shape [expert_num, token_num_padded, hidden_dim]
  - name: tanh
    signature: (x)
  - name: gelu_and_mul_triton_kernel
    signature: (gateup_output, down_input, hidden_size, reorder_topk_ids, scales, start_expert_id, end_expert_id, BLOCK_SIZE: tl.constexpr)
  - name: post_reorder_triton_kernel
    signature: (down_output_ptr, output_ptr, src2dst_ptr, topk_ids_ptr, topk_weights_ptr, start_expert_id, end_expert_id, topk, hidden_size, dst_start, BLOCK_SIZE: tl.constexpr)
  - name: post_reorder_triton_kernel_for_cutlass_moe
    signature: (down_output_ptr, output_ptr, src2dst_ptr, topk_ids_ptr, topk_weights_ptr, num_experts, topk, hidden_size, dst_start, BLOCK_SIZE: tl.constexpr)
  - name: compute_m_range
    signature: (pid, batch_size, seg_indptr, weight_indices, m_num_tiles_indptr, BLOCK_SIZE_M: tl.constexpr)
  - name: grouped_gemm_triton_kernel
    signature: (a, b, c, batch_size, N, K, seg_indptr, weight_indices, m_num_tiles_indptr, scale_a, scale_b, use_fp8_w8a8: tl.constexpr, group_n: tl.constexpr, group_k: tl.constexpr, a_stride_0: tl.constexpr, b_stride_0: tl.constexpr, b_stride_1: tl.constexpr, as_stride_0: tl.constexpr, as_stride_1: tl.constexpr, bs_stride_0: tl.constexpr, bs_stride_2: tl.constexpr, bs_stride_1: tl.constexpr, use_per_token_if_dynamic: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr)
  - name: compute_m_num_tiles_indptr
    signature: (m_num_tiles_indptr, seg_indptr, batch_size: tl.constexpr, BLOCK_SIZE_M: tl.constexpr)
  - name: grouped_gemm_triton
    signature: (a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, batch_size: int, weight_column_major: bool, seg_indptr: Optional[torch.Tensor] = None, weight_indices: Optional[torch.Tensor] = None, use_fp8_w8a8: bool = False, scale_a: torch.Tensor = None, scale_b: torch.Tensor = None, block_shape: Optional[List[int]] = None, c_dtype = None, use_per_token_if_dynamic: bool = True)
  - name: _fwd_kernel_ep_scatter_1
    signature: (num_recv_tokens_per_expert, expert_start_loc, m_indices, num_experts: tl.constexpr, BLOCK_E: tl.constexpr, BLOCK_EXPERT_NUM: tl.constexpr)
  - name: _fwd_kernel_ep_scatter_2
    signature: (total_token_num, expert_start_loc, recv_x, recv_x_stride0, recv_x_stride1, recv_x_scale, recv_x_scale_stride0, recv_x_scale_stride1, recv_topk, recv_topk_stride0, recv_topk_stride1, output_tensor, output_tensor_stride0, output_tensor_stride1, output_tensor_scale, output_tensor_scale_stride0, output_tensor_scale_stride1, output_index, output_index_stride0, output_index_stride1, topk_num: tl.constexpr, HIDDEN_SIZE: tl.constexpr, HIDDEN_SIZE_PAD: tl.constexpr, SCALE_HIDDEN_SIZE: tl.constexpr, SCALE_HIDDEN_SIZE_PAD: tl.constexpr)
  - name: ep_scatter
    signature: (recv_x: torch.Tensor, recv_x_scale: torch.Tensor, recv_topk: torch.Tensor, num_recv_tokens_per_expert: torch.Tensor, expert_start_loc: torch.Tensor, output_tensor: torch.Tensor, output_tensor_scale: torch.Tensor, m_indices: torch.Tensor, output_index: torch.Tensor, scale_ue8m0: bool = False)
  - name: _fwd_kernel_ep_gather
    signature: (total_token_num, input_tensor, input_tensor_stride0, input_tensor_stride1, recv_topk_ids, recv_topk_ids_stride0, recv_topk_ids_stride1, recv_topk_weight, recv_topk_weight_stride0, recv_topk_weight_stride1, input_index, input_index_stride0, input_index_stride1, output_tensor, output_tensor_stride0, output_tensor_stride1, topk_num: tl.constexpr, BLOCK_D: tl.constexpr)
  - name: ep_gather
    signature: (input_tensor: torch.Tensor, recv_topk_ids: torch.Tensor, recv_topk_weight: torch.Tensor, input_index: torch.Tensor, output_tensor: torch.Tensor)
  - name: get_tma_aligned_size
    signature: (x: int, element_size: int)
    return: int
    doc: Global memory address of TMA must be 16-byte aligned.
  - name: _tma_align_input_scale_kernel
    signature: (input_scale_ptr, output_ptr, m, k_div_block_size, input_scale_stride_m, input_scale_stride_k, output_stride_m, output_stride_k, BLOCK_SIZE_K: tl.constexpr)
  - name: tma_align_input_scale
    signature: (input_scale: torch.Tensor)
  - name: compute_masked_m_triton_kernel
    signature: (seg_indptr, masked_m)
  - name: deepgemm_compute_src2dst_triton_kernel
    signature: (topk_ids, reorder_ids, seg_indptr, src2dst, m_max, num_toks, BLOCK_SIZE: tl.constexpr)
  - name: fill_gateup_input_triton_kernel
    signature: (input_ptr, scale_ptr, gateup_input_ptr, gateup_input_scale_ptr, src2dst_ptr, topk_ids_ptr, start_expert_id, end_expert_id, topk, m_max, hidden_size, scale_size, BLOCK_SIZE: tl.constexpr)
  - name: moe_ep_deepgemm_preprocess
    signature: (topk_ids: torch.Tensor, num_experts: int, hidden_states: torch.Tensor, top_k: int, start_expert_id, end_expert_id, block_shape, output_dtype: torch.dtype = torch.float8_e4m3fn)
  - name: compute_identity_kernel
    signature: (top_k, hidden_states_ptr, expert_scales_ptr, num_tokens, output_ptr, hidden_dim, scales_stride, BLOCK_SIZE: tl.constexpr)
  - name: zero_experts_compute_triton
    signature: (expert_indices, expert_scales, num_experts, zero_expert_type, hidden_states)

File: layers/moe/ep_moe/layer.py
  - name: _cast_to_e8m0_with_rounding_up
    signature: (x: torch.Tensor)
    return: torch.Tensor
  - name: __init__
    signature: (self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, num_fused_shared_experts: int = 0, params_dtype: Optional[torch.dtype] = None, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', activation: str = 'silu', routed_scaling_factor: Optional[float] = None, gemm1_alpha: Optional[float] = None, gemm1_clamp_limit: Optional[float] = None, with_bias: bool = False)
    class: EPMoE
  - name: forward
    signature: (self, hidden_states: torch.Tensor, topk_output: TopKOutput)
    class: EPMoE
  - name: forward_deepgemm
    signature: (self, hidden_states: torch.Tensor, topk_output: TopKOutput)
    class: EPMoE
  - name: __init__
    signature: (self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, num_fused_shared_experts: int = 0, params_dtype: Optional[torch.dtype] = None, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', activation: str = 'silu', routed_scaling_factor: Optional[float] = None)
    class: DeepEPMoE
  - name: forward
    signature: (self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)
    class: DeepEPMoE
  - name: dispatch
    signature: (self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)
    class: DeepEPMoE
  - name: moe_impl
    signature: (self, dispatch_output: DispatchOutput)
    class: DeepEPMoE
  - name: combine
    signature: (self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)
    class: DeepEPMoE
  - name: forward_aiter
    signature: (self, dispatch_output: Union[DeepEPNormalOutput, DeepEPLLOutput])
    class: DeepEPMoE
  - name: forward_deepgemm_contiguous
    signature: (self, dispatch_output: DeepEPNormalOutput)
    class: DeepEPMoE
  - name: forward_deepgemm_masked
    signature: (self, dispatch_output: DeepEPLLOutput)
    class: DeepEPMoE
  - name: forward_npu
    signature: (self, dispatch_output: DeepEPLLOutput)
    class: DeepEPMoE
  - name: get_moe_impl_class
    signature: (quant_config: Optional[QuantizationConfig] = None)

File: layers/moe/fused_moe_native.py
  - name: fused_moe_forward_native
    signature: (layer: torch.nn.Module, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
  - name: moe_forward_native
    signature: (layer: torch.nn.Module, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor

File: layers/moe/fused_moe_triton/__init__.py
  - name: override_config
    signature: (config)
  - name: get_config
    signature: ()
    return: Optional[Dict[str, Any]]

File: layers/moe/fused_moe_triton/fused_moe.py
  - name: write_zeros_to_output
    signature: (c_ptr, stride_cm, stride_cn, pid_n, N, offs_token, token_mask, BLOCK_SIZE_M, BLOCK_SIZE_N, compute_type)
  - name: fused_moe_kernel_gptq_awq
    signature: (a_ptr, b_ptr, c_ptr, b_scale_ptr, b_zp_ptr, topk_weights_ptr, sorted_token_ids_ptr, expert_ids_ptr, num_tokens_post_padded_ptr, N: tl.constexpr, K: tl.constexpr, EM, num_valid_tokens, stride_am, stride_ak, stride_be, stride_bk, stride_bn, stride_cm, stride_cn, stride_bse, stride_bsk, stride_bsn, stride_bze, stride_bzk, stride_bzn, group_size: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, MUL_ROUTED_WEIGHT: tl.constexpr, top_k: tl.constexpr, compute_type: tl.constexpr, has_zp: tl.constexpr, use_int4_w4a16: tl.constexpr, use_int8_w8a16: tl.constexpr, even_Ks: tl.constexpr)
    doc: Implements the fused computation for a Mixture of Experts (MOE) using
  - name: fused_moe_kernel
    signature: (a_ptr, b_ptr, bias_ptr, c_ptr, a_scale_ptr, b_scale_ptr, topk_weights_ptr, sorted_token_ids_ptr, expert_ids_ptr, num_tokens_post_padded_ptr, N, K, EM, num_valid_tokens, stride_am, stride_ak, stride_be, stride_bk, stride_bn, stride_bias_e, stride_bias_n, stride_cm, stride_cn, stride_asm, stride_ask, stride_bse, stride_bsk, stride_bsn, group_n: tl.constexpr, group_k: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, MUL_ROUTED_WEIGHT: tl.constexpr, top_k: tl.constexpr, compute_type: tl.constexpr, use_fp8_w8a8: tl.constexpr, use_int8_w8a8: tl.constexpr, use_int8_w8a16: tl.constexpr, per_channel_quant: tl.constexpr, even_Ks: tl.constexpr)
    doc: Implements the fused computation for a Mixture of Experts (MOE) using
  - name: moe_align_block_size
    signature: (topk_ids: torch.Tensor, block_size: int, num_experts: int)
    return: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
    doc: Aligns the token distribution across experts to be compatible with block
  - name: invoke_fused_moe_kernel
    signature: (A: torch.Tensor, B: torch.Tensor, bias: Optional[torch.Tensor], C: torch.Tensor, A_scale: Optional[torch.Tensor], B_scale: Optional[torch.Tensor], B_zp: Optional[torch.Tensor], topk_weights: torch.Tensor, topk_ids: torch.Tensor, sorted_token_ids: torch.Tensor, expert_ids: torch.Tensor, num_tokens_post_padded: torch.Tensor, mul_routed_weight: bool, top_k: int, config: Dict[str, Any], compute_type: tl.dtype, use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, block_shape: Optional[List[int]] = None, no_combine: bool = False)
    return: None
  - name: get_config_file_name
    signature: (E: int, N: int, dtype: Optional[str], block_shape: Optional[int] = None)
    return: str
  - name: get_moe_configs
    signature: (E: int, N: int, dtype: Optional[str], block_n: Optional[int] = 0, block_k: Optional[int] = 0)
    return: Optional[Dict[int, Any]]
    doc: Return optimized configurations for the fused MoE kernel.
  - name: get_default_config
    signature: (M: int, E: int, N: int, K: int, topk: int, dtype: Optional[str], is_marlin: bool, block_shape: Optional[List[int]] = None)
    return: Dict[str, int]
  - name: try_get_optimal_moe_config
    signature: (w1_shape: Tuple[int, ...], w2_shape: Tuple[int, ...], top_k: int, dtype: Optional[str], M: int, is_marlin: bool = False, block_shape: Optional[List[int]] = None)
  - name: get_config_dtype_str
    signature: (dtype: torch.dtype, use_int8_w8a16: Optional[bool] = False, use_int4_w4a16: Optional[bool] = False, use_fp8_w8a8: Optional[bool] = False, use_int8_w8a8: Optional[bool] = False)
  - name: inplace_fused_experts
    signature: (hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor] = None, b2: Optional[torch.Tensor] = None, activation: str = 'silu', apply_router_weight_on_input: bool = False, use_fp8_w8a8: bool = False, use_int8_w8a8: bool = False, use_int8_w8a16: bool = False, use_int4_w4a16: bool = False, per_channel_quant: bool = False, w1_scale: Optional[torch.Tensor] = None, w2_scale: Optional[torch.Tensor] = None, w1_zp: Optional[torch.Tensor] = None, w2_zp: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_shape: Optional[List[int]] = None, routed_scaling_factor: Optional[float] = None, gemm1_alpha: Optional[float] = None, gemm1_limit: Optional[float] = None)
    return: None
  - name: inplace_fused_experts_fake
    signature: (hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor] = None, b2: Optional[torch.Tensor] = None, activation: str = 'silu', apply_router_weight_on_input: bool = False, use_fp8_w8a8: bool = False, use_int8_w8a8: bool = False, use_int8_w8a16: bool = False, use_int4_w4a16: bool = False, per_channel_quant: bool = False, w1_scale: Optional[torch.Tensor] = None, w2_scale: Optional[torch.Tensor] = None, w1_zp: Optional[torch.Tensor] = None, w2_zp: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_shape: Optional[List[int]] = None, routed_scaling_factor: Optional[float] = None, gemm1_alpha: Optional[float] = None, gemm1_limit: Optional[float] = None)
    return: None
  - name: outplace_fused_experts
    signature: (hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor] = None, b2: Optional[torch.Tensor] = None, activation: str = 'silu', apply_router_weight_on_input: bool = False, use_fp8_w8a8: bool = False, use_int8_w8a8: bool = False, use_int8_w8a16: bool = False, use_int4_w4a16: bool = False, per_channel_quant: bool = False, w1_scale: Optional[torch.Tensor] = None, w2_scale: Optional[torch.Tensor] = None, w1_zp: Optional[torch.Tensor] = None, w2_zp: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_shape: Optional[List[int]] = None, no_combine: bool = False, routed_scaling_factor: Optional[float] = None, gemm1_alpha: Optional[float] = None, gemm1_limit: Optional[float] = None)
    return: torch.Tensor
  - name: outplace_fused_experts_fake
    signature: (hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor] = None, b2: Optional[torch.Tensor] = None, activation: str = 'silu', apply_router_weight_on_input: bool = False, use_fp8_w8a8: bool = False, use_int8_w8a8: bool = False, use_int8_w8a16: bool = False, use_int4_w4a16: bool = False, per_channel_quant: bool = False, w1_scale: Optional[torch.Tensor] = None, w2_scale: Optional[torch.Tensor] = None, w1_zp: Optional[torch.Tensor] = None, w2_zp: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_shape: Optional[List[int]] = None, no_combine: bool = False, routed_scaling_factor: Optional[float] = None, gemm1_alpha: Optional[float] = None, gemm1_limit: Optional[float] = None)
    return: torch.Tensor
  - name: fused_experts
    signature: (hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig, b1: Optional[torch.Tensor] = None, b2: Optional[torch.Tensor] = None, use_fp8_w8a8: bool = False, use_int8_w8a8: bool = False, use_int8_w8a16: bool = False, use_int4_w4a16: bool = False, per_channel_quant: bool = False, w1_scale: Optional[torch.Tensor] = None, w2_scale: Optional[torch.Tensor] = None, w1_zp: Optional[torch.Tensor] = None, w2_zp: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_shape: Optional[List[int]] = None)
  - name: _moe_sum_reduce_kernel
    signature: (input_ptr, input_stride_0, input_stride_1, input_stride_2, output_ptr, output_stride_0, output_stride_1, token_num: int, topk_num: int, hidden_dim: int, routed_scaling_factor: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_DIM: tl.constexpr, NUM_STAGE: tl.constexpr)
  - name: moe_sum_reduce_triton
    signature: (input: torch.Tensor, output: torch.Tensor, routed_scaling_factor: float)
  - name: moe_sum_reduce_torch_compile
    signature: (x, out, routed_scaling_factor)
  - name: swiglu_with_alpha_and_limit
    signature: (x, gemm1_alpha, gemm1_limit)
  - name: fused_experts_impl
    signature: (hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor] = None, b2: Optional[torch.Tensor] = None, inplace: bool = False, activation: str = 'silu', apply_router_weight_on_input: bool = False, use_fp8_w8a8: bool = False, use_int8_w8a8: bool = False, use_int8_w8a16: bool = False, use_int4_w4a16: bool = False, per_channel_quant: bool = False, w1_scale: Optional[torch.Tensor] = None, w2_scale: Optional[torch.Tensor] = None, w1_zp: Optional[torch.Tensor] = None, w2_zp: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_shape: Optional[List[int]] = None, no_combine: bool = False, routed_scaling_factor: Optional[float] = None, gemm1_alpha: Optional[float] = None, gemm1_limit: Optional[float] = None)
  - name: fused_moe
    signature: (hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig = MoeRunnerConfig(), b1: Optional[torch.Tensor] = None, b2: Optional[torch.Tensor] = None, use_fp8_w8a8: bool = False, use_int8_w8a8: bool = False, use_int8_w8a16: bool = False, use_int4_w4a16: bool = False, per_channel_quant: bool = False, w1_scale: Optional[torch.Tensor] = None, w2_scale: Optional[torch.Tensor] = None, w1_zp: Optional[torch.Tensor] = None, w2_zp: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_shape: Optional[List[int]] = None)
    return: torch.Tensor
    doc: This function computes a Mixture of Experts (MoE) layer using two sets of

File: layers/moe/fused_moe_triton/layer.py
  - name: _is_fp4_quantization_enabled
    signature: ()
    doc: Check if ModelOpt FP4 quantization is enabled.
  - name: _get_tile_tokens_dim
    signature: (num_tokens, top_k, num_experts)
  - name: __init__
    signature: (self, num_experts: int, hidden_size: int, intermediate_size: int, layer_id: int, top_k: Optional[int] = None, num_fused_shared_experts: int = 0, params_dtype: Optional[torch.dtype] = None, reduce_results: bool = False, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', activation: str = 'silu', apply_router_weight_on_input: bool = False, use_presharded_weights: bool = False, inplace: bool = True, no_combine: bool = False, routed_scaling_factor: Optional[float] = None, gemm1_alpha: Optional[float] = None, gemm1_clamp_limit: Optional[float] = None, use_weight_loader_fused: bool = False, with_bias = False)
    class: FusedMoE
  - name: _load_per_tensor_weight_scale
    signature: (self, shard_id: str, param: torch.nn.Parameter, loaded_weight: torch.Tensor, expert_id: int)
    class: FusedMoE
  - name: _load_model_weight_or_group_weight_scale
    signature: (self, shard_dim: int, expert_data: torch.Tensor, shard_id: str, loaded_weight: torch.Tensor, tp_rank: int, is_bias: bool = False)
    class: FusedMoE
  - name: _load_per_channel_weight_scale
    signature: (self, expert_data: torch.Tensor, shard_dim: int, shard_id: str, loaded_weight: torch.Tensor, tp_rank: int)
    class: FusedMoE
  - name: _load_w13
    signature: (self, expert_data: torch.Tensor, shard_dim: int, shard_id: str, loaded_weight: torch.Tensor, tp_rank: int, is_bias: bool = False)
    class: FusedMoE
  - name: _load_w2
    signature: (self, expert_data: torch.Tensor, shard_dim: int, shard_id: str, loaded_weight: torch.Tensor, tp_rank: int, is_bias: bool = False)
    class: FusedMoE
    doc: Load w2 weights for down projection.
  - name: _load_single_value
    signature: (self, param: torch.nn.Parameter, loaded_weight: torch.Tensor, expert_id: int)
    class: FusedMoE
  - name: _load_g_idx
    signature: (self, shard_id: str, expert_data: torch.Tensor, shard_dim: int, loaded_weight: torch.Tensor, tp_rank: int)
    class: FusedMoE
  - name: _map_global_expert_id_to_local_expert_id
    signature: (self, expert_id: int)
    return: int
    class: FusedMoE
  - name: weight_loader
    signature: (self, param: torch.nn.Parameter, loaded_weight: torch.Tensor, weight_name: str, shard_id: str, expert_id: Optional[int])
    return: None
    class: FusedMoE
  - name: _weight_loader_physical
    signature: (self, param: torch.nn.Parameter, loaded_weight: torch.Tensor, weight_name: str, shard_id: str, expert_id: int)
    return: None
    class: FusedMoE
  - name: _weight_loader_impl
    signature: (self, param: torch.nn.Parameter, loaded_weight: torch.Tensor, weight_name: str, shard_id: str, expert_id: int)
    return: None
    class: FusedMoE
  - name: weight_loader_fused
    signature: (self, param: torch.nn.Parameter, loaded_weight: torch.Tensor, weight_name: str, shard_id: str)
    return: None
    class: FusedMoE
  - name: forward
    signature: (self, hidden_states: torch.Tensor, topk_output: TopKOutput)
    class: FusedMoE
  - name: make_expert_params_mapping
    signature: (cls, ckpt_gate_proj_name: str, ckpt_down_proj_name: str, ckpt_up_proj_name: str, num_experts: int)
    return: List[Tuple[str, str, int, str]]
    class: FusedMoE
  - name: make_expert_params_mapping_fused
    signature: (cls, ckpt_gate_up_proj_name: str, ckpt_down_proj_name: str, ckpt_gate_up_proj_bias_name: str, ckpt_down_proj_bias_name: str)
    class: FusedMoE
  - name: make_expert_params_mapping_fused_mxfp4
    signature: (cls, ckpt_gate_up_proj_name: str, ckpt_down_proj_name: str, ckpt_gate_up_proj_bias_name: str, ckpt_down_proj_bias_name: str, ckpt_gate_up_proj_scale_name: str, ckpt_down_proj_scale_name: str)
    class: FusedMoE
  - name: make_expert_input_scale_params_mapping
    signature: (cls, num_experts: int)
    return: List[Tuple[str, str, int, str]]
    class: FusedMoE
  - name: should_fuse_routed_scaling_factor_in_topk
    signature: (self)
    class: FusedMoE
  - name: __init__
    signature: (self, *args, **kwargs)
    class: FlashInferFusedMoE
  - name: forward
    signature: (self, hidden_states: torch.Tensor, topk_output: TopKOutput)
    class: FlashInferFusedMoE
  - name: __init__
    signature: (self, *args, **kwargs)
    class: FlashInferFP4MoE
  - name: _quantize_hidden_states_fp4
    signature: (self, hidden_states: torch.Tensor)
    class: FlashInferFP4MoE
    doc: Quantize hidden states using global scale factor from quantization method.
  - name: forward
    signature: (self, hidden_states: torch.Tensor, topk_output: TopKOutput)
    class: FlashInferFP4MoE
    doc: Forward pass using FP4 TRTLLM kernel.
  - name: get_fused_moe_impl_class
    signature: ()
    doc: Factory function to get the appropriate FusedMoE implementation class.

File: layers/moe/fused_moe_triton/triton_kernels_moe.py
  - name: quantize
    signature: (w, dtype, dev, **opt)
  - name: triton_kernel_moe_forward
    signature: (hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig, apply_router_weight_on_input: bool = False, use_fp8_w8a8: bool = False, per_channel_quant: bool = False, global_num_experts: int = -1, expert_map: Optional[torch.Tensor] = None, w1_scale: Optional[torch.Tensor] = None, w2_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_shape: Optional[list[int]] = None)
    return: torch.Tensor
  - name: triton_kernel_fused_experts
    signature: (hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, routing_data: RoutingData, gather_indx: GatherIndx, scatter_indx: ScatterIndx, inplace: bool = False, activation: str = 'silu', apply_router_weight_on_input: bool = False, use_fp8_w8a8: bool = False, per_channel_quant: bool = False, global_num_experts: int = -1, expert_map: Optional[torch.Tensor] = None, w1_scale: Optional[torch.Tensor] = None, w2_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_shape: Optional[list[int]] = None)
    return: torch.Tensor
  - name: triton_kernel_moe_with_bias_forward
    signature: (hidden_states: torch.Tensor, w1: torch.Tensor, w1_pcg, b1: torch.Tensor, w2: torch.Tensor, w2_pcg, b2: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig, use_fp8_w8a8: bool = False, per_channel_quant: bool = False, global_num_experts: int = -1, expert_map: Optional[torch.Tensor] = None, w1_scale: Optional[torch.Tensor] = None, w2_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_shape: Optional[list[int]] = None)
    return: torch.Tensor
  - name: triton_kernel_fused_experts_with_bias
    signature: (hidden_states: torch.Tensor, w1: torch.Tensor, w1_pcg, b1: torch.Tensor, w2: torch.Tensor, w2_pcg, b2: torch.Tensor, routing_data: RoutingData, gather_indx: GatherIndx, scatter_indx: ScatterIndx, inplace: bool = False, activation: str = 'silu', use_fp8_w8a8: bool = False, per_channel_quant: bool = False, global_num_experts: int = -1, expert_map: Optional[torch.Tensor] = None, w1_scale: Optional[torch.Tensor] = None, w2_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_shape: Optional[list[int]] = None, gemm1_alpha: Optional[float] = None, gemm1_clamp_limit: Optional[float] = None)
    return: torch.Tensor

File: layers/moe/moe_runner/__init__.py
  (no function definitions found)
File: layers/moe/moe_runner/base.py
  (no function definitions found)
File: layers/moe/rocm_moe_utils.py
  - name: rocm_aiter_asm_moe_tkw1_impl
    signature: (hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, fc1_scale: Optional[torch.Tensor] = None, fc2_scale: Optional[torch.Tensor] = None, fc1_smooth_scale: Optional[torch.Tensor] = None, fc2_smooth_scale: Optional[torch.Tensor] = None, a16: bool = False, per_tensor_quant_scale: Optional[torch.Tensor] = None, expert_mask: Optional[torch.Tensor] = None, activation_method: int = ActivationMethod.SILU.value)
    return: torch.Tensor
  - name: rocm_aiter_asm_moe_tkw1_fake
    signature: (hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, fc1_scale: Optional[torch.Tensor] = None, fc2_scale: Optional[torch.Tensor] = None, fc1_smooth_scale: Optional[torch.Tensor] = None, fc2_smooth_scale: Optional[torch.Tensor] = None, a16: bool = False, per_tensor_quant_scale: Optional[torch.Tensor] = None, expert_mask: Optional[torch.Tensor] = None, activation_method: int = ActivationMethod.SILU.value)
    return: torch.Tensor
  - name: rocm_fused_experts_tkw1
    signature: (hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, activation: str = 'silu', apply_router_weight_on_input: bool = False, use_fp8_w8a8: bool = False, per_channel_quant: bool = False, w1_scale: Optional[torch.Tensor] = None, w2_scale: Optional[torch.Tensor] = None, a1_scale: Optional[torch.Tensor] = None, a2_scale: Optional[torch.Tensor] = None, block_shape: Optional[list[int]] = None)
    return: torch.Tensor

File: layers/moe/router.py
  - name: fused_moe_router_kernel
    signature: (input_ptr, moe_router_weight_ptr, topk_weights_ptr, topk_ids_ptr, correction_bias_ptr, is_correction_bias: tl.constexpr, num_experts: tl.constexpr, topk: tl.constexpr, moe_softcapping: tl.constexpr, moe_renormalize: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)
  - name: fused_moe_router_impl
    signature: (x: torch.Tensor, router_weight: torch.Tensor, topk: int, moe_softcapping: float, correction_bias: Optional[torch.Tensor] = None)
  - name: fused_moe_router_large_bs_kernel
    signature: (a_ptr, b_ptr, topk_weights_ptr, topk_ids_ptr, bs, num_experts: tl.constexpr, topk: tl.constexpr, moe_softcapping: tl.constexpr, moe_renormalize: tl.constexpr, K: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, stride_am: tl.constexpr, stride_bn: tl.constexpr)
  - name: fused_moe_router_large_bs_impl
    signature: (x: torch.Tensor, router_weight: torch.Tensor, topk: int, moe_softcapping: float, BLOCK_SIZE_M: int, BLOCK_SIZE_N: int, BLOCK_SIZE_K: int)
  - name: fused_moe_router_shim
    signature: (moe_softcapping, hidden_states, gating_output, topk, renormalize, correction_bias: Optional[torch.Tensor] = None)
  - name: __init__
    signature: (self, router_linear, topk, moe_softcapping)
    return: None
    class: FusedMoeRouter
  - name: __call__
    signature: (self, *args, **kwargs)
    class: FusedMoeRouter
  - name: forward
    signature: (self, x: torch.Tensor, residual: torch.Tensor)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: FusedMoeRouter
  - name: forward_cuda
    signature: (self, x: torch.Tensor, autotune = False)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: FusedMoeRouter
  - name: forward_vllm
    signature: (self, x: torch.Tensor)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: FusedMoeRouter

File: layers/moe/token_dispatcher/__init__.py
  (no function definitions found)
File: layers/moe/token_dispatcher/base_dispatcher.py
  - name: format_is_standard
    signature: (dispatch_output: DispatchOutput)
    return: TypeGuard[StandardDispatchOutput]
    class: DispatchOutputChecker
  - name: format_is_deepep_normal
    signature: (dispatch_output: DispatchOutput)
    return: TypeGuard[DeepEPNormalOutput]
    class: DispatchOutputChecker
  - name: format_is_deepep_ll
    signature: (dispatch_output: DispatchOutput)
    return: TypeGuard[DeepEPLLOutput]
    class: DispatchOutputChecker
  - name: format_is_deepep
    signature: (dispatch_output: DispatchOutput)
    return: TypeGuard[Union[DeepEPNormalOutput, DeepEPLLOutput]]
    class: DispatchOutputChecker
  - name: format_is_ascent_ll
    signature: (dispatch_output: DispatchOutput)
    return: TypeGuard[AscendDeepEPLLOutput]
    class: DispatchOutputChecker
  - name: is_standard
    signature: (self)
    return: bool
    class: DispatchOutputFormat
  - name: is_deepep_normal
    signature: (self)
    return: bool
    class: DispatchOutputFormat
  - name: is_deepep_ll
    signature: (self)
    return: bool
    class: DispatchOutputFormat
  - name: is_deepep
    signature: (self)
    return: bool
    class: DispatchOutputFormat
  - name: is_ascent_ll
    signature: (self)
    return: bool
    class: DispatchOutputFormat
  - name: format
    signature: (self)
    return: DispatchOutputFormat
    class: DispatchOutput
  - name: dispatch
    signature: (self, *args, **kwargs)
    return: DispatchOutput
    class: BaseDispatcher
  - name: combine
    signature: (self, *args, **kwargs)
    return: torch.Tensor
    class: BaseDispatcher

File: layers/moe/token_dispatcher/deepep.py
  - name: format
    signature: (self)
    return: DispatchOutputFormat
    class: DeepEPNormalOutput
  - name: format
    signature: (self)
    return: DispatchOutputFormat
    class: DeepEPLLOutput
  - name: format
    signature: (self)
    return: DispatchOutputFormat
    class: AscendDeepEPLLOutput
  - name: get_deepep_buffer
    signature: (cls, group: dist.ProcessGroup, hidden_size: int, param_bytes: int, deepep_mode: DeepEPMode, num_max_dispatch_tokens_per_rank: int = -1, num_experts: int = -1)
    class: DeepEPBuffer
  - name: clean_buffer
    signature: (cls)
    class: DeepEPBuffer
  - name: set_dispatch_mode_as_normal
    signature: (cls)
    class: DeepEPBuffer
  - name: set_dispatch_mode_as_low_latency
    signature: (cls)
    class: DeepEPBuffer
  - name: __init__
    signature: (self)
    class: DeepEPConfig
  - name: get_instance
    signature: (cls)
    class: DeepEPConfig
  - name: __init__
    signature: (self, group: torch.distributed.ProcessGroup, router_topk: int, permute_fusion: bool, num_experts: int, num_local_experts: int, hidden_size: int, params_dtype: torch.dtype, deepep_mode: DeepEPMode)
    class: _DeepEPDispatcherImplBase
  - name: dispatch_a
    signature: (self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)
    class: _DeepEPDispatcherImplBase
  - name: dispatch_b
    signature: (self, *args, **kwargs)
    class: _DeepEPDispatcherImplBase
  - name: combine_a
    signature: (self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)
    class: _DeepEPDispatcherImplBase
  - name: combine_b
    signature: (self, *args, **kwargs)
    class: _DeepEPDispatcherImplBase
  - name: _get_buffer
    signature: (self)
    class: _DeepEPDispatcherImplBase
  - name: __init__
    signature: (self, async_finish: bool, **kwargs)
    class: _DeepEPDispatcherImplNormal
  - name: dispatch_a
    signature: (self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)
    class: _DeepEPDispatcherImplNormal
  - name: dispatch_b
    signature: (self, hidden_states, topk_idx, topk_weights, previous_event)
    class: _DeepEPDispatcherImplNormal
  - name: _dispatch_core
    signature: (self, x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], topk_idx: torch.Tensor, topk_weights: torch.Tensor, previous_event)
    class: _DeepEPDispatcherImplNormal
  - name: combine_a
    signature: (self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)
    class: _DeepEPDispatcherImplNormal
  - name: combine_b
    signature: (self, output, previous_event)
    class: _DeepEPDispatcherImplNormal
  - name: _combine_core
    signature: (self, x: torch.Tensor, previous_event)
    class: _DeepEPDispatcherImplNormal
  - name: _get_buffer
    signature: (self)
    class: _DeepEPDispatcherImplNormal
  - name: __init__
    signature: (self, return_recv_hook: bool, **kwargs)
    class: _DeepEPDispatcherImplLowLatency
  - name: dispatch_a
    signature: (self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)
    class: _DeepEPDispatcherImplLowLatency
  - name: dispatch_b
    signature: (self, hidden_states, topk_idx, topk_weights, masked_m, expected_m, event, hook)
    class: _DeepEPDispatcherImplLowLatency
  - name: _dispatch_core
    signature: (self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, use_fp8: bool = False)
    class: _DeepEPDispatcherImplLowLatency
  - name: combine_a
    signature: (self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)
    class: _DeepEPDispatcherImplLowLatency
  - name: combine_b
    signature: (self, hidden_states, event, hook)
    class: _DeepEPDispatcherImplLowLatency
  - name: _combine_core
    signature: (self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)
    class: _DeepEPDispatcherImplLowLatency
  - name: _get_buffer
    signature: (self)
    class: _DeepEPDispatcherImplLowLatency
  - name: __init__
    signature: (self, group: torch.distributed.ProcessGroup, router_topk: int, permute_fusion: bool = False, num_experts: int = None, num_local_experts: int = None, hidden_size: int = None, params_dtype: torch.dtype = None, deepep_mode: DeepEPMode = DeepEPMode.AUTO, async_finish: bool = False, return_recv_hook: bool = False)
    class: DeepEPDispatcher
  - name: dispatch
    signature: (self, *args, **kwargs)
    return: DispatchOutput
    class: DeepEPDispatcher
  - name: dispatch_a
    signature: (self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)
    class: DeepEPDispatcher
  - name: dispatch_b
    signature: (self)
    class: DeepEPDispatcher
  - name: combine
    signature: (self, *args, **kwargs)
    return: Tuple
    class: DeepEPDispatcher
  - name: combine_a
    signature: (self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)
    class: DeepEPDispatcher
  - name: combine_b
    signature: (self)
    class: DeepEPDispatcher
  - name: _get_impl
    signature: (self, forward_batch: ForwardBatch)
    return: _DeepEPDispatcherImplBase
    class: DeepEPDispatcher
  - name: _update_stage
    signature: (self, old_stage, new_stage)
    class: DeepEPDispatcher

File: layers/moe/token_dispatcher/standard.py
  - name: format
    signature: (self)
    return: DispatchOutputFormat
    class: StandardDispatchOutput

File: layers/moe/topk.py
  - name: format_is_standard
    signature: (topk_output: TopKOutput)
    return: TypeGuard[StandardTopKOutput]
    class: TopKOutputChecker
  - name: format_is_triton_kernel
    signature: (topk_output: TopKOutput)
    return: TypeGuard[TritonKernelTopKOutput]
    class: TopKOutputChecker
  - name: format_is_bypassed
    signature: (topk_output: TopKOutput)
    return: TypeGuard[BypassedTopKOutput]
    class: TopKOutputChecker
  - name: is_standard
    signature: (self)
    return: bool
    class: TopKOutputFormat
  - name: is_triton_kernel
    signature: (self)
    return: bool
    class: TopKOutputFormat
  - name: is_bypassed
    signature: (self)
    return: bool
    class: TopKOutputFormat
  - name: format
    signature: (self)
    return: TopKOutputFormat
    class: TopKOutput
    doc: The format of the output.
  - name: format
    signature: (self)
    return: TopKOutputFormat
    class: StandardTopKOutput
  - name: format
    signature: (self)
    return: TopKOutputFormat
    class: TritonKernelTopKOutput
  - name: format
    signature: (self)
    return: TopKOutputFormat
    class: BypassedTopKOutput
  - name: __init__
    signature: (self, top_k: int, *, use_grouped_topk: bool = False, topk_group: Optional[int] = None, num_expert_group: Optional[int] = None, renormalize: bool = True, num_fused_shared_experts: int = 0, custom_routing_function: Optional[Callable] = None, scoring_func: str = 'softmax', correction_bias: Optional[torch.Tensor] = None, routed_scaling_factor: Optional[float] = None, apply_routed_scaling_factor_on_output: Optional[bool] = False, force_topk: bool = False)
    class: TopK
  - name: forward_native
    signature: (self, hidden_states: torch.Tensor, router_logits: torch.Tensor, *, num_token_non_padded: Optional[torch.Tensor] = None, expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None)
    return: TopKOutput
    class: TopK
  - name: forward_cuda
    signature: (self, hidden_states: torch.Tensor, router_logits: torch.Tensor, *, num_token_non_padded: Optional[torch.Tensor] = None, expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None)
    return: TopKOutput
    class: TopK
  - name: forward_cpu
    signature: (self, hidden_states: torch.Tensor, router_logits: torch.Tensor, *, num_token_non_padded: Optional[torch.Tensor] = None, expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None)
    return: TopKOutput
    class: TopK
  - name: forward_npu
    signature: (self, hidden_states: torch.Tensor, router_logits: torch.Tensor, *, num_token_non_padded: Optional[torch.Tensor] = None, expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None)
    return: TopKOutput
    class: TopK
  - name: empty_topk_output
    signature: (self, device: torch.device)
    return: TopKOutput
    class: TopK
  - name: fused_topk_torch_native
    signature: (hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool, correction_bias: torch.Tensor = None)
  - name: fused_topk_cpu
    signature: (hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool, num_token_non_padded: Optional[torch.Tensor] = None, expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None, correction_bias: torch.Tensor = None)
  - name: apply_topk_weights_cpu
    signature: (need_apply, topk_weights, inputs)
  - name: fused_topk
    signature: (hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool, num_token_non_padded: Optional[torch.Tensor] = None, expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None)
  - name: grouped_topk_gpu
    signature: (hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool, num_expert_group: Optional[int] = None, topk_group: Optional[int] = None, num_fused_shared_experts: int = 0, routed_scaling_factor: Optional[float] = None, num_token_non_padded: Optional[torch.Tensor] = None, expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None, apply_routed_scaling_factor_on_output: Optional[bool] = False)
  - name: grouped_topk_cpu
    signature: (hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool, num_expert_group: Optional[int] = None, topk_group: Optional[int] = None, num_fused_shared_experts: int = 0, routed_scaling_factor: Optional[float] = None, num_token_non_padded: Optional[torch.Tensor] = None, expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None, apply_routed_scaling_factor_on_output: Optional[bool] = False)
  - name: biased_grouped_topk_impl
    signature: (hidden_states: torch.Tensor, gating_output: torch.Tensor, correction_bias: torch.Tensor, topk: int, renormalize: bool, num_expert_group: Optional[int] = None, topk_group: Optional[int] = None, num_fused_shared_experts: int = 0, routed_scaling_factor: Optional[float] = None, num_token_non_padded: Optional[torch.Tensor] = None, expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None, apply_routed_scaling_factor_on_output: Optional[bool] = False)
  - name: is_power_of_two
    signature: (n)
  - name: _mask_topk_ids_padded_region
    signature: (topk_ids: torch.Tensor, num_token_non_padded: Optional[torch.Tensor] = None)
  - name: _biased_grouped_topk_postprocess
    signature: (topk_ids, expert_location_dispatch_info, num_token_non_padded)
  - name: biased_grouped_topk_gpu
    signature: (hidden_states: torch.Tensor, gating_output: torch.Tensor, correction_bias: torch.Tensor, topk: int, renormalize: bool, num_expert_group: Optional[int] = None, topk_group: Optional[int] = None, num_fused_shared_experts: int = 0, routed_scaling_factor: Optional[float] = None, num_token_non_padded: Optional[torch.Tensor] = None, expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None, apply_routed_scaling_factor_on_output: Optional[bool] = False)
  - name: biased_grouped_topk_cpu
    signature: (hidden_states: torch.Tensor, gating_output: torch.Tensor, correction_bias: torch.Tensor, topk: int, renormalize: bool, num_expert_group: Optional[int] = None, topk_group: Optional[int] = None, compiled: bool = True, num_fused_shared_experts: int = 0, routed_scaling_factor: Optional[float] = None, num_token_non_padded: Optional[torch.Tensor] = None, expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None, apply_routed_scaling_factor_on_output: Optional[bool] = False)
  - name: select_experts
    signature: (hidden_states: torch.Tensor, router_logits: torch.Tensor, topk_config: TopKConfig, *, num_token_non_padded: Optional[torch.Tensor] = None, expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None)
    return: StandardTopKOutput

File: layers/moe/utils.py
  - name: _missing_
    signature: (cls, value)
    class: MoeA2ABackend
  - name: is_none
    signature: (self)
    class: MoeA2ABackend
  - name: is_deepep
    signature: (self)
    class: MoeA2ABackend
  - name: is_auto
    signature: (self)
    class: MoeRunnerBackend
  - name: is_triton
    signature: (self)
    class: MoeRunnerBackend
  - name: is_triton_kernel
    signature: (self)
    class: MoeRunnerBackend
  - name: is_flashinfer_trtllm
    signature: (self)
    class: MoeRunnerBackend
  - name: is_flashinfer_cutlass
    signature: (self)
    class: MoeRunnerBackend
  - name: is_flashinfer_mxfp4
    signature: (self)
    class: MoeRunnerBackend
  - name: enable_normal
    signature: (self)
    return: bool
    class: DeepEPMode
  - name: enable_low_latency
    signature: (self)
    return: bool
    class: DeepEPMode
  - name: resolve
    signature: (self, is_extend_in_batch: bool)
    return: DeepEPMode
    class: DeepEPMode
  - name: is_normal
    signature: (self)
    return: bool
    class: DeepEPMode
  - name: is_low_latency
    signature: (self)
    return: bool
    class: DeepEPMode
  - name: is_auto
    signature: (self)
    return: bool
    class: DeepEPMode
  - name: initialize_moe_config
    signature: (server_args: ServerArgs)
  - name: get_moe_a2a_backend
    signature: ()
    return: MoeA2ABackend
  - name: get_moe_runner_backend
    signature: ()
    return: MoeRunnerBackend
  - name: get_deepep_mode
    signature: ()
    return: DeepEPMode
  - name: get_deepep_config
    signature: ()
    return: str
  - name: is_tbo_enabled
    signature: ()
    return: bool
  - name: get_tbo_token_distribution_threshold
    signature: ()
    return: float
  - name: should_use_flashinfer_trtllm_moe
    signature: ()
  - name: should_use_flashinfer_cutlass_moe_fp4_allgather
    signature: ()
    doc: Perform FP4 quantize before all-gather for flashinfer cutlass moe to reduce communication cost for high-throughput serving.

File: layers/multimodal.py
  - name: _rotl32
    signature: (x, r: tl.constexpr)
  - name: _fmix32
    signature: (x, C1: tl.constexpr, C2: tl.constexpr)
  - name: hash_tiles32_kernel_blocked
    signature: (in_ptr, out_ptr, n_u32, seed1, seed2, FM_C1: tl.constexpr, FM_C2: tl.constexpr, POS_A: tl.constexpr, POS_B: tl.constexpr, TILE: tl.constexpr, BLOCK: tl.constexpr, USE_CG: tl.constexpr)
  - name: add_tree_reduce_u64_kernel
    signature: (in_ptr, out_ptr, n_elems, CHUNK: tl.constexpr)
  - name: _as_uint32_words
    signature: (t: torch.Tensor)
    return: torch.Tensor
  - name: _final_splitmix64
    signature: (x: int)
    return: int
  - name: gpu_tensor_hash
    signature: (tensor: torch.Tensor, *, seed: int = 608135816, tile_words: int = 8192, block_words: int = 256, reduce_chunk: int = 1024, num_warps: int = 4, num_stages: int = 4, use_cg: bool = True)
    return: int

File: layers/parameter.py
  - name: __new__
    signature: (cls, data: torch.Tensor, **kwargs)
    class: BasevLLMParameter
  - name: __init__
    signature: (self, data: torch.Tensor, weight_loader: Callable)
    class: BasevLLMParameter
    doc: Initialize the BasevLLMParameter
  - name: weight_loader
    signature: (self)
    class: BasevLLMParameter
  - name: _assert_and_load
    signature: (self, loaded_weight: torch.Tensor)
    class: BasevLLMParameter
  - name: load_column_parallel_weight
    signature: (self, loaded_weight: torch.Tensor)
    class: BasevLLMParameter
  - name: load_row_parallel_weight
    signature: (self, loaded_weight: torch.Tensor)
    class: BasevLLMParameter
  - name: load_merged_column_weight
    signature: (self, loaded_weight: torch.Tensor, **kwargs)
    class: BasevLLMParameter
  - name: load_qkv_weight
    signature: (self, loaded_weight: torch.Tensor, **kwargs)
    class: BasevLLMParameter
  - name: __init__
    signature: (self, output_dim: int, **kwargs)
    class: _ColumnvLLMParameter
  - name: output_dim
    signature: (self)
    class: _ColumnvLLMParameter
  - name: load_column_parallel_weight
    signature: (self, loaded_weight: torch.Tensor, tp_rank: int, use_presharded_weights: bool = False)
    class: _ColumnvLLMParameter
  - name: load_merged_column_weight
    signature: (self, loaded_weight: torch.Tensor, **kwargs)
    class: _ColumnvLLMParameter
  - name: load_qkv_weight
    signature: (self, loaded_weight: torch.Tensor, tp_rank: int, use_presharded_weights: bool = False, **kwargs)
    class: _ColumnvLLMParameter
  - name: __init__
    signature: (self, input_dim: int, **kwargs)
    class: RowvLLMParameter
  - name: input_dim
    signature: (self)
    class: RowvLLMParameter
  - name: load_row_parallel_weight
    signature: (self, loaded_weight: torch.Tensor, tp_rank: int, use_presharded_weights: bool = False)
    class: RowvLLMParameter
  - name: __init__
    signature: (self, **kwargs)
    class: PerTensorScaleParameter
  - name: _shard_id_as_int
    signature: (self, shard_id: Union[str, int])
    return: int
    class: PerTensorScaleParameter
  - name: load_row_parallel_weight
    signature: (self, *args, **kwargs)
    class: PerTensorScaleParameter
  - name: load_merged_column_weight
    signature: (self, *args, **kwargs)
    class: PerTensorScaleParameter
  - name: load_qkv_weight
    signature: (self, *args, **kwargs)
    class: PerTensorScaleParameter
  - name: load_column_parallel_weight
    signature: (self, *args, **kwargs)
    class: PerTensorScaleParameter
  - name: _load_into_shard_id
    signature: (self, loaded_weight: torch.Tensor, shard_id: Union[str, int], **kwargs)
    class: PerTensorScaleParameter
    doc: Slice the parameter data based on the shard id for
  - name: __init__
    signature: (self, packed_factor: Union[int, Fraction], packed_dim: int, marlin_tile_size: Optional[int] = None, **kwargs)
    class: PackedColumnParameter
  - name: packed_dim
    signature: (self)
    class: PackedColumnParameter
  - name: packed_factor
    signature: (self)
    class: PackedColumnParameter
  - name: marlin_tile_size
    signature: (self)
    class: PackedColumnParameter
  - name: adjust_shard_indexes_for_packing
    signature: (self, shard_size, shard_offset)
    class: PackedColumnParameter
  - name: __init__
    signature: (self, packed_factor: Union[int, Fraction], packed_dim: int, marlin_tile_size: Optional[int] = None, **kwargs)
    class: PackedvLLMParameter
  - name: packed_dim
    signature: (self)
    class: PackedvLLMParameter
  - name: packed_factor
    signature: (self)
    class: PackedvLLMParameter
  - name: marlin_tile_size
    signature: (self)
    class: PackedvLLMParameter
  - name: adjust_shard_indexes_for_packing
    signature: (self, shard_size, shard_offset)
    class: PackedvLLMParameter
  - name: permute_param_layout_
    signature: (param: BasevLLMParameter, input_dim: int, output_dim: int, **kwargs)
    return: BasevLLMParameter
    doc: Permute a parameter's layout to the specified input and output dimensions,
  - name: _adjust_shard_indexes_for_marlin
    signature: (shard_size, shard_offset, marlin_tile_size)
  - name: _adjust_shard_indexes_for_packing
    signature: (shard_size, shard_offset, packed_factor, marlin_tile_size)

File: layers/pooler.py
  - name: __init__
    signature: (self, pooling_type: PoolingType, normalize: bool)
    class: Pooler
  - name: forward
    signature: (self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: EmbeddingPoolerOutput
    class: Pooler
  - name: __init__
    signature: (self, config: PretrainedConfig, classifier: nn.Module, pooler: Optional[nn.Module] = None)
    class: CrossEncodingPooler
  - name: forward
    signature: (self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
    return: EmbeddingPoolerOutput
    class: CrossEncodingPooler
    doc: Pools sentence pair scores from the hidden_states.

File: layers/quantization/__init__.py
  - name: override_quantization_method
    signature: (self, *args, **kwargs)
    class: DummyConfig
  - name: get_quantization_config
    signature: (quantization: str)
    return: Type[QuantizationConfig]
  - name: monkey_patch_isinstance_for_vllm_base_layer
    signature: (reverse: bool = False)
    doc: Patch isinstance so that the `get_quant_method` in vllm's QuantizationConfig
  - name: patched_isinstance
    signature: (obj, classinfo)
  - name: monkey_patch_moe_apply
    signature: (class_obj: 'FusedMoEMethodBase')
    doc: Monkey patch the apply function of vllm's FusedMoEMethodBase.
  - name: new_apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, *, activation: str = 'silu', apply_router_weight_on_input: bool = False, inplace: bool = True, no_combine: bool = False, routed_scaling_factor: Optional[float] = None)
  - name: monkey_patch_quant_configs
    signature: ()
    doc: Apply all monkey patches in one place.

File: layers/quantization/awq.py
  - name: is_layer_skipped_awq
    signature: (prefix: str, modules_to_not_convert: List[str])
  - name: __init__
    signature: (self, weight_bits: int, group_size: int, zero_point: bool, modules_to_not_convert: Optional[List[str]] = None)
    return: None
    class: AWQConfig
  - name: __repr__
    signature: (self)
    return: str
    class: AWQConfig
  - name: get_scaled_act_names
    signature: (self)
    return: List[str]
    class: AWQConfig
  - name: get_name
    signature: (self)
    return: str
    class: AWQConfig
  - name: get_supported_act_dtypes
    signature: (self)
    return: List[torch.dtype]
    class: AWQConfig
  - name: get_min_capability
    signature: (cls)
    return: int
    class: AWQConfig
  - name: get_config_filenames
    signature: ()
    return: List[str]
    class: AWQConfig
  - name: from_config
    signature: (cls, config: Dict[str, Any])
    return: AWQConfig
    class: AWQConfig
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional[LinearMethodBase]
    class: AWQConfig
  - name: __init__
    signature: (self, weight_bits: int, group_size: int, zero_point: bool, lm_head_quantized: bool, modules_to_not_convert: Optional[list[str]], full_config: dict[str, Any])
    return: None
    class: AWQMarlinConfig
  - name: __repr__
    signature: (self)
    return: str
    class: AWQMarlinConfig
  - name: get_scaled_act_names
    signature: (self)
    return: List[str]
    class: AWQMarlinConfig
  - name: get_name
    signature: (cls)
    return: str
    class: AWQMarlinConfig
  - name: get_supported_act_dtypes
    signature: (cls)
    return: list[torch.dtype]
    class: AWQMarlinConfig
  - name: get_min_capability
    signature: (cls)
    return: int
    class: AWQMarlinConfig
  - name: get_config_filenames
    signature: (cls)
    return: list[str]
    class: AWQMarlinConfig
  - name: from_config
    signature: (cls, config: dict[str, Any])
    return: AWQMarlinConfig
    class: AWQMarlinConfig
  - name: override_quantization_method
    signature: (cls, hf_quant_cfg, user_quant)
    return: Optional[str]
    class: AWQMarlinConfig
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional[QuantizeMethodBase]
    class: AWQMarlinConfig
  - name: is_awq_marlin_compatible
    signature: (cls, quant_config: dict[str, Any])
    class: AWQMarlinConfig
  - name: __init__
    signature: (self, quant_config: AWQConfig)
    class: AWQLinearMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: AWQLinearMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: AWQLinearMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: AWQLinearMethod
  - name: __init__
    signature: (self, quant_config: AWQMarlinConfig)
    return: None
    class: AWQMarlinLinearMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    return: None
    class: AWQMarlinLinearMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: AWQMarlinLinearMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: AWQMarlinLinearMethod
  - name: __init__
    signature: (self, quant_config: AWQMarlinConfig)
    class: AWQMoEMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: AWQMoEMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: AWQMoEMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: AWQMoEMethod

File: layers/quantization/awq_triton.py
  - name: awq_dequantize_kernel
    signature: (qweight_ptr, scales_ptr, zeros_ptr, group_size, result_ptr, num_cols, num_rows, BLOCK_SIZE_X: tl.constexpr, BLOCK_SIZE_Y: tl.constexpr)
  - name: awq_gemm_kernel
    signature: (a_ptr, b_ptr, c_ptr, zeros_ptr, scales_ptr, M, N, K, group_size, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, SPLIT_K: tl.constexpr)
  - name: awq_dequantize_triton
    signature: (qweight: torch.Tensor, scales: torch.Tensor, zeros: torch.Tensor, block_size_x: int = 32, block_size_y: int = 32)
    return: torch.Tensor
  - name: awq_gemm_triton
    signature: (input: torch.Tensor, qweight: torch.Tensor, scales: torch.Tensor, qzeros: torch.Tensor, split_k_iters: int, block_size_m: int = 32, block_size_n: int = 32, block_size_k: int = 32)
    return: torch.Tensor

File: layers/quantization/base_config.py
  - name: create_weights
    signature: (self, layer: torch.nn.Module, *weight_args, **extra_weight_attrs)
    class: QuantizeMethodBase
    doc: Create weights for a layer.
  - name: apply
    signature: (self, layer: torch.nn.Module, *args, **kwargs)
    return: torch.Tensor
    class: QuantizeMethodBase
    doc: Apply the weights in layer to the input tensor.
  - name: process_weights_after_loading
    signature: (self, layer: nn.Module)
    return: None
    class: QuantizeMethodBase
    doc: Process the weight after loading.
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: LinearMethodBase
    doc: Create weights for a linear layer.
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: LinearMethodBase
    doc: Apply the weights in layer to the input tensor.
  - name: create_weights
    signature: (self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: FusedMoEMethodBase
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: FusedMoEMethodBase
  - name: __init__
    signature: (self)
    class: QuantizationConfig
  - name: get_name
    signature: (self)
    return: str
    class: QuantizationConfig
    doc: Name of the quantization method.
  - name: get_supported_act_dtypes
    signature: (self)
    return: List[torch.dtype]
    class: QuantizationConfig
    doc: List of supported activation dtypes.
  - name: get_min_capability
    signature: (cls)
    return: int
    class: QuantizationConfig
    doc: Minimum GPU capability to support the quantization method.
  - name: get_config_filenames
    signature: ()
    return: List[str]
    class: QuantizationConfig
    doc: List of filenames to search for in the model directory.
  - name: from_config
    signature: (cls, config: Dict[str, Any])
    return: 'QuantizationConfig'
    class: QuantizationConfig
    doc: Create a config class from the model's quantization config.
  - name: override_quantization_method
    signature: (cls, hf_quant_cfg, user_quant)
    return: Optional[str]
    class: QuantizationConfig
    doc: Detects if this quantization method can support a given checkpoint
  - name: get_from_keys
    signature: (config: Dict[str, Any], keys: List[str])
    return: Any
    class: QuantizationConfig
    doc: Get a value from the model's quantization config.
  - name: get_from_keys_or
    signature: (config: Dict[str, Any], keys: List[str], default: Any)
    return: Any
    class: QuantizationConfig
    doc: Get a optional value from the model's quantization config.
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional[QuantizeMethodBase]
    class: QuantizationConfig
    doc: Get the quantize method to use for the quantized layer.
  - name: get_scaled_act_names
    signature: (self)
    return: List[str]
    class: QuantizationConfig
    doc: Returns the activation function names that should be post-scaled.
  - name: method_has_implemented_embedding
    signature: (method_class: Type[QuantizeMethodBase])
    return: bool
    doc: Not all quant methods have embedding implemented, so we need to check that

File: layers/quantization/blockwise_int8.py
  - name: __init__
    signature: (self, is_checkpoint_int8_serialized: bool = False, activation_scheme: str = 'dynamic', ignored_layers: Optional[List[str]] = None, weight_block_size: List[int] = None)
    return: None
    class: BlockInt8Config
  - name: get_name
    signature: (cls)
    return: str
    class: BlockInt8Config
  - name: get_supported_act_dtypes
    signature: (cls)
    return: List[torch.dtype]
    class: BlockInt8Config
  - name: get_min_capability
    signature: (cls)
    return: int
    class: BlockInt8Config
  - name: get_config_filenames
    signature: (cls)
    return: List[str]
    class: BlockInt8Config
  - name: from_config
    signature: (cls, config: Dict[str, Any])
    return: BlockInt8Config
    class: BlockInt8Config
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional[QuantizeMethodBase]
    class: BlockInt8Config
  - name: get_scaled_act_names
    signature: (self)
    return: List[str]
    class: BlockInt8Config
  - name: __init__
    signature: (self, quant_config: BlockInt8Config)
    class: BlockInt8LinearMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: BlockInt8LinearMethod
  - name: process_weights_after_loading
    signature: (self, layer: Module)
    return: None
    class: BlockInt8LinearMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: BlockInt8LinearMethod
  - name: __init__
    signature: (self, quant_config: BlockInt8Config)
    class: BlockInt8MoEMethod
  - name: create_weights
    signature: (self, layer: Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: BlockInt8MoEMethod
  - name: process_weights_after_loading
    signature: (self, layer: Module)
    return: None
    class: BlockInt8MoEMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: BlockInt8MoEMethod

File: layers/quantization/compressed_tensors/__init__.py
  (no function definitions found)
File: layers/quantization/compressed_tensors/compressed_tensors.py
  - name: as_version_str
    signature: (self)
    return: str
    class: DeviceCapability
  - name: to_int
    signature: (self)
    return: int
    class: DeviceCapability
    doc: Express device capability as an integer ``<major><minor>``.
  - name: __init__
    signature: (self, target_scheme_map: Dict[str, Any], ignore: List[str], quant_format: str, sparsity_scheme_map: Dict[str, SparsityCompressionConfig], sparsity_ignore_list: List[str], kv_cache_scheme: Optional[Dict[str, Any]] = None, config: Optional[Dict[str, Any]] = None, packed_modules_mapping: Dict[str, List[str]] = {})
    class: CompressedTensorsConfig
  - name: get_linear_method
    signature: (self)
    return: CompressedTensorsLinearMethod
    class: CompressedTensorsConfig
  - name: get_supported_act_dtypes
    signature: (cls)
    return: List[torch.dtype]
    class: CompressedTensorsConfig
  - name: get_min_capability
    signature: (cls)
    return: int
    class: CompressedTensorsConfig
  - name: get_name
    signature: (self)
    return: str
    class: CompressedTensorsConfig
  - name: get_scaled_act_names
    signature: (self)
    return: List[str]
    class: CompressedTensorsConfig
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional[QuantizeMethodBase]
    class: CompressedTensorsConfig
  - name: from_config
    signature: (cls, config: Dict[str, Any])
    return: CompressedTensorsConfig
    class: CompressedTensorsConfig
  - name: _parse_sparsity_config
    signature: (cls, config: Dict[str, Any])
    return: Tuple[Dict[str, SparsityCompressionConfig], List[str]]
    class: CompressedTensorsConfig
    doc: :param config: The `quantization_config` dictionary from config.json
  - name: _quantization_scheme_map_from_config
    signature: (cls, config: Dict[str, Any])
    return: QUANTIZATION_SCHEME_MAP_TYPE
    class: CompressedTensorsConfig
    doc: :param config: The `quantization_config` dictionary from config.json
  - name: get_config_filenames
    signature: (cls)
    return: List[str]
    class: CompressedTensorsConfig
  - name: _check_scheme_supported
    signature: (self, min_capability: int, error: bool = True)
    return: bool
    class: CompressedTensorsConfig
  - name: _is_static_tensor_w8a8
    signature: (self, weight_quant: BaseModel, input_quant: BaseModel)
    return: bool
    class: CompressedTensorsConfig
  - name: _is_dynamic_token_w8a8
    signature: (self, weight_quant: BaseModel, input_quant: BaseModel)
    return: bool
    class: CompressedTensorsConfig
  - name: _is_fp8_w8a8
    signature: (self, weight_quant: BaseModel, input_quant: BaseModel)
    return: bool
    class: CompressedTensorsConfig
  - name: _is_fp8_w8a16
    signature: (self, weight_quant: BaseModel, input_quant: BaseModel)
    return: bool
    class: CompressedTensorsConfig
  - name: _is_wNa16_group_channel
    signature: (self, weight_quant: BaseModel, input_quant: BaseModel)
    return: bool
    class: CompressedTensorsConfig
  - name: _get_scheme_from_parts
    signature: (self, weight_quant: BaseModel, input_quant: BaseModel)
    return: CompressedTensorsScheme
    class: CompressedTensorsConfig
  - name: get_scheme
    signature: (self, layer: torch.nn.Module, layer_name: Optional[str] = None)
    return: Optional[CompressedTensorsScheme]
    class: CompressedTensorsConfig
    doc: compressed-tensors supports non uniform in the following way:
  - name: get_cache_scale
    signature: (self, name: str)
    return: Optional[str]
    class: CompressedTensorsConfig
    doc: Check whether the param name matches the format for k/v cache scales
  - name: supports_cutlass_24
    signature: (weight_quant: Optional[QuantizationArgs], input_quant: Optional[QuantizationArgs], sparsity_scheme: Optional[SparsityCompressionConfig] = None)
    return: bool
    class: CompressedTensorsConfig
    doc: Check if the layer is supported by the Cutlass 2:4 Kernel
  - name: __init__
    signature: (self, quantization_config: CompressedTensorsConfig)
    class: CompressedTensorsLinearMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: CompressedTensorsLinearMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: CompressedTensorsLinearMethod
    doc: Use the CompressedTensorsScheme associated with each layer to create
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    class: CompressedTensorsLinearMethod
    doc: Use the output of create_weights and the CompressedTensorsScheme

File: layers/quantization/compressed_tensors/compressed_tensors_moe.py
  - name: __new__
    signature: (cls, *args, **kwargs)
    class: CompressedTensorsMoEMethod
  - name: get_moe_method
    signature: (quant_config: CompressedTensorsConfig)
    return: 'CompressedTensorsMoEMethod'
    class: CompressedTensorsMoEMethod
  - name: __init__
    signature: (self, quant_config: CompressedTensorsConfig)
    class: CompressedTensorsW8A8Fp8MoEMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: CompressedTensorsW8A8Fp8MoEMethod
  - name: process_weights_after_loading
    signature: (self, layer: FusedMoE)
    return: None
    class: CompressedTensorsW8A8Fp8MoEMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: CompressedTensorsW8A8Fp8MoEMethod
  - name: __init__
    signature: (self, quant_config: CompressedTensorsConfig)
    class: CompressedTensorsWNA16MoEMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: CompressedTensorsWNA16MoEMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: CompressedTensorsWNA16MoEMethod
  - name: replace_tensor
    signature: (name, new_t)
    class: CompressedTensorsWNA16MoEMethod
  - name: get_scale_perms
    signature: (num_bits: int)
    class: CompressedTensorsWNA16MoEMethod
  - name: marlin_permute_scales
    signature: (s: torch.Tensor, size_k: int, size_n: int, group_size: int, num_bits: int)
    class: CompressedTensorsWNA16MoEMethod
  - name: marlin_moe_permute_scales
    signature: (s: torch.Tensor, size_k: int, size_n: int, group_size: int, num_bits: int)
    class: CompressedTensorsWNA16MoEMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: CompressedTensorsWNA16MoEMethod

File: layers/quantization/compressed_tensors/schemes/__init__.py
  (no function definitions found)
File: layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py
  - name: get_min_capability
    signature: (cls)
    return: int
    class: CompressedTensorsScheme
    doc: Get minimum device capability.
  - name: create_weights
    signature: (self, *args, **kwargs)
    class: CompressedTensorsScheme
    doc: Weight creation for the particular scheme. Inputs to this function
  - name: apply_weights
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
    class: CompressedTensorsScheme
    doc: Run the forward pass for the particular scheme. This is where
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    class: CompressedTensorsScheme
    doc: Called after weight loading is complete for any cleanup that

File: layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py
  - name: apply_fp8_marlin_linear
    signature: (*args, **kwargs)
  - name: prepare_fp8_layer_for_marlin
    signature: (*args, **kwargs)
  - name: __init__
    signature: (self, strategy: str, is_static_input_scheme: bool)
    class: CompressedTensorsW8A16Fp8
  - name: get_min_capability
    signature: (cls)
    return: int
    class: CompressedTensorsW8A16Fp8
  - name: process_weights_after_loading
    signature: (self, layer)
    return: None
    class: CompressedTensorsW8A16Fp8
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size: int, output_partition_sizes: List[int], input_size_per_partition: int, params_dtype: torch.dtype, weight_loader: Callable, **kwargs)
    class: CompressedTensorsW8A16Fp8
  - name: apply_weights
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: CompressedTensorsW8A16Fp8

File: layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
  - name: __init__
    signature: (self, strategy: str, is_static_input_scheme: bool)
    class: CompressedTensorsW8A8Fp8
  - name: get_min_capability
    signature: (cls)
    return: int
    class: CompressedTensorsW8A8Fp8
  - name: process_weights_after_loading
    signature: (self, layer)
    return: None
    class: CompressedTensorsW8A8Fp8
  - name: create_weights
    signature: (self, layer: torch.nn.Module, output_partition_sizes: List[int], input_size_per_partition: int, params_dtype: torch.dtype, weight_loader: Callable, **kwargs)
    class: CompressedTensorsW8A8Fp8
  - name: apply_weights
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: CompressedTensorsW8A8Fp8

File: layers/quantization/compressed_tensors/utils.py
  - name: is_activation_quantization_format
    signature: (format: str)
    return: bool
  - name: should_ignore_layer
    signature: (layer_name: Optional[str], ignore: Iterable[str] = tuple(), fused_mapping: Mapping[str, List[str]] = MappingProxyType({}))
    return: bool
  - name: check_equal_or_regex_match
    signature: (layer_name: str, targets: Iterable[str])
    return: bool
    doc: Checks whether a layer_name is exactly equal or a regex match for
  - name: find_matched_target
    signature: (layer_name: Optional[str], module: Module, targets: Iterable[str], fused_mapping: Mapping[str, List[str]] = MappingProxyType({}))
    return: str
    doc: Helper function to look up which "target" in the compressed-tensors
  - name: _find_first_match
    signature: (value: str, targets: Iterable[str], check_contains: bool = False)
    return: Optional[str]
    doc: Returns first element of target that matches value either
  - name: _is_equal_or_regex_match
    signature: (value: str, target: str, check_contains: bool = False)
    return: bool
    doc: Checks whether a value is exactly equal or a regex match for target
  - name: _match_fused_layer
    signature: (layer_name: str, target_layers: Iterable[str], fused_mapping: Mapping[str, List[str]])
    return: Optional[str]
    doc: Match a fused layer name to its corresponding individual layer in

File: layers/quantization/deep_gemm_wrapper/__init__.py
  (no function definitions found)
File: layers/quantization/deep_gemm_wrapper/compile_utils.py
  - name: update_deep_gemm_config
    signature: (gpu_id: int, server_args: ServerArgs)
  - name: _maybe_compile_deep_gemm_one_type_all
    signature: (kernel_type: DeepGemmKernelType, n: int, k: int, num_groups: int)
    return: None
  - name: _compile_deep_gemm_one_type_all
    signature: (kernel_type: DeepGemmKernelType, n: int, k: int, num_groups: int, m_list: List[int])
    return: None
  - name: create
    signature: (kernel_type: DeepGemmKernelType, **kwargs)
    class: _BaseWarmupExecutor
  - name: execute
    signature: (self, m)
    class: _BaseWarmupExecutor
  - name: _empty_token_fp8
    signature: (size)
  - name: _empty_block_fp8
    signature: (size)
  - name: __init__
    signature: (self, max_m: int, n: int, k: int, num_groups: int)
    class: _NormalWarmupExecutor
  - name: execute
    signature: (self, m)
    class: _NormalWarmupExecutor
  - name: __init__
    signature: (self, max_m: int, n: int, k: int, num_groups: int)
    class: _GroupedContWarmupExecutor
  - name: execute
    signature: (self, m)
    class: _GroupedContWarmupExecutor
  - name: __init__
    signature: (self, max_m: int, n: int, k: int, num_groups: int)
    class: _GroupedMaskedWarmupExecutor
  - name: execute
    signature: (self, m)
    class: _GroupedMaskedWarmupExecutor
  - name: deep_gemm_execution_hook
    signature: (m: int, n: int, k: int, num_groups: int, kernel_type: DeepGemmKernelType)

File: layers/quantization/deep_gemm_wrapper/configurer.py
  - name: _compute_enable_deep_gemm
    signature: ()
  - name: _is_blackwell_arch
    signature: ()
    return: bool

File: layers/quantization/deep_gemm_wrapper/entrypoint.py
  - name: grouped_gemm_nt_f8f8bf16_masked
    signature: (lhs: Tuple[torch.Tensor, torch.Tensor], rhs: Tuple[torch.Tensor, torch.Tensor], out: torch.Tensor, masked_m: torch.Tensor, expected_m: int)
  - name: grouped_gemm_nt_f8f8bf16_contig
    signature: (lhs: Tuple[torch.Tensor, torch.Tensor], rhs: Tuple[torch.Tensor, torch.Tensor], out: torch.Tensor, m_indices: torch.Tensor)
  - name: gemm_nt_f8f8bf16
    signature: (lhs: Tuple[torch.Tensor, torch.Tensor], rhs: Tuple[torch.Tensor, torch.Tensor], out: torch.Tensor)
  - name: update_deep_gemm_config
    signature: (gpu_id: int, server_args: ServerArgs)
  - name: configure_deep_gemm_num_sms
    signature: (num_sms)

File: layers/quantization/fp8.py
  - name: dummy_func
    signature: (*args, **kwargs)
  - name: __init__
    signature: (self, is_checkpoint_fp8_serialized: bool = False, activation_scheme: str = 'dynamic', ignored_layers: Optional[List[str]] = None, weight_block_size: List[int] = None)
    return: None
    class: Fp8Config
  - name: get_name
    signature: (cls)
    return: str
    class: Fp8Config
  - name: get_supported_act_dtypes
    signature: (cls)
    return: List[torch.dtype]
    class: Fp8Config
  - name: get_min_capability
    signature: (cls)
    return: int
    class: Fp8Config
  - name: get_config_filenames
    signature: (cls)
    return: List[str]
    class: Fp8Config
  - name: from_config
    signature: (cls, config: Dict[str, Any])
    return: Fp8Config
    class: Fp8Config
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional[QuantizeMethodBase]
    class: Fp8Config
  - name: get_scaled_act_names
    signature: (self)
    return: List[str]
    class: Fp8Config
  - name: __init__
    signature: (self, quant_config: Union[Fp8Config, W4AFp8Config])
    class: Fp8LinearMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: Fp8LinearMethod
  - name: process_weights_after_loading
    signature: (self, layer: Module)
    return: None
    class: Fp8LinearMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: Fp8LinearMethod
  - name: get_tile_tokens_dim
    signature: (num_tokens, top_k, num_experts)
  - name: __init__
    signature: (self, quant_config: Fp8Config)
    class: Fp8MoEMethod
  - name: create_weights
    signature: (self, layer: Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: Fp8MoEMethod
  - name: process_weights_after_loading
    signature: (self, layer: Module)
    return: None
    class: Fp8MoEMethod
  - name: process_weights_hip_int4
    signature: (self, layer: Module)
    class: Fp8MoEMethod
  - name: process_weights_hip_scale_padding
    signature: (self, layer: Module)
    class: Fp8MoEMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: Fp8MoEMethod
  - name: apply_with_router_logits
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: Fp8MoEMethod
  - name: maybe_apply_hip_fused_experts
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, activation: str = 'silu', no_combine: bool = False)
    return: Optional[torch.Tensor]
    class: Fp8MoEMethod
  - name: __init__
    signature: (self, quant_config: Fp8Config)
    class: Fp8KVCacheMethod

File: layers/quantization/fp8_kernel.py
  - name: is_fp8_fnuz
    signature: ()
    return: bool
  - name: deep_gemm_fp8_fp8_bf16_nt
    signature: (A: torch.Tensor, As: torch.Tensor, B: torch.Tensor, Bs: torch.Tensor, C: torch.Tensor)
    return: None
  - name: deep_gemm_fp8_fp8_bf16_nt_fake
    signature: (A: torch.Tensor, As: torch.Tensor, B: torch.Tensor, Bs: torch.Tensor, C: torch.Tensor)
    return: None
  - name: _per_token_group_quant_8bit
    signature: (y_ptr, y_q_ptr, y_s_ptr, y_stride, N, eps, bit8_min, bit8_max, BLOCK: tl.constexpr)
    doc: A Triton-accelerated function to perform per-token-group quantization on a
  - name: _per_token_group_quant_8bit_colmajor
    signature: (y_ptr, y_q_ptr, y_s_ptr, group_size, y_num_columns, y_s_col_stride, eps, bit8_min, bit8_max, BLOCK: tl.constexpr, SCALE_UE8M0: tl.constexpr)
    doc: A Triton-accelerated function to perform per-token-group
  - name: _per_token_group_quant_8bit_raw
    signature: (x: torch.Tensor, group_size: int, eps: float = 1e-10, dtype: torch.dtype = fp8_dtype, column_major_scales: bool = False, scale_tma_aligned: bool = False, scale_ue8m0: bool = False)
    return: Tuple[torch.Tensor, torch.Tensor]
    doc: Function to perform per-token-group quantization on an input tensor `x`.
  - name: _per_token_group_quant_8bit_fuse_silu_and_mul
    signature: (x: torch.Tensor, group_size: int, dst_dtype: torch.dtype, column_major_scales: bool, scale_tma_aligned: bool, scale_ue8m0: bool, masked_m: Optional[torch.Tensor])
    return: Tuple[torch.Tensor, torch.Tensor]
  - name: per_token_group_quant_8bit
    signature: (x: torch.Tensor, group_size: int, dst_dtype: torch.dtype, eps: float = 1e-10, column_major_scales: bool = False, scale_tma_aligned: bool = False, scale_ue8m0: bool = False, fuse_silu_and_mul: bool = False, masked_m: Optional[torch.Tensor] = None)
    return: Tuple[torch.Tensor, torch.Tensor]
  - name: create_per_token_group_quant_fp8_output_scale
    signature: (x_shape, device, group_size, column_major_scales: bool, scale_tma_aligned: bool, scale_ue8m0: bool)
  - name: sglang_per_token_group_quant_fp8
    signature: (x: torch.Tensor, group_size: int, eps: float = 1e-10, column_major_scales: bool = False, scale_tma_aligned: bool = False, scale_ue8m0: bool = False, fuse_silu_and_mul: bool = False, masked_m: Optional[torch.Tensor] = None)
  - name: sglang_per_token_group_quant_8bit
    signature: (x: torch.Tensor, group_size: int, dst_dtype: torch.dtype, eps: float = 1e-10, column_major_scales: bool = False, scale_tma_aligned: bool = False, scale_ue8m0: bool = False, fuse_silu_and_mul: bool = False, masked_m: Optional[torch.Tensor] = None)
  - name: sglang_per_token_quant_fp8
    signature: (x: torch.Tensor, dtype: torch.dtype = fp8_dtype)
  - name: _static_quant_fp8
    signature: (y_ptr, y_q_ptr, y_s_ptr, y_s_repeat_ptr, y_stride, N, fp8_min, fp8_max, BLOCK: tl.constexpr, REPEAT_SCALE: tl.constexpr)
    doc: A Triton-accelerated function to perform quantization using the given scale on a
  - name: static_quant_fp8
    signature: (x: torch.Tensor, x_s: torch.Tensor, repeat_scale: bool = False)
    return: Tuple[torch.Tensor, torch.Tensor]
    doc: Function to perform static quantization using the given scale on an input tensor `x`.
  - name: _w8a8_block_fp8_matmul
    signature: (A, B, C, As, Bs, M, N, K, group_n, group_k, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, stride_As_m, stride_As_k, stride_Bs_k, stride_Bs_n, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr)
    doc: Triton-accelerated function used to perform linear operations (dot
  - name: _w8a8_block_fp8_matmul_unrolledx4
    signature: (A, B, C, As, Bs, M, N, K, group_n, group_k, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, stride_As_m, stride_As_k, stride_Bs_k, stride_Bs_n, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr)
    doc: Triton-accelerated function used to perform linear operations (dot
  - name: get_w8a8_block_fp8_configs
    signature: (N: int, K: int, block_n: int, block_k: int)
    return: Optional[Dict[int, Any]]
    doc: Return optimized configurations for the w8a8 block fp8 kernel.
  - name: select_w8a8_block_fp8_matmul_kernel
    signature: (M, N, META)
  - name: use_w8a8_block_fp8_matmul_unrolledx4
    signature: (M, N, META)
  - name: select_w8a8_block_fp8_matmul_kernel
    signature: (M, N, META)
  - name: prepare_block_fp8_matmul_inputs
    signature: (A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype = torch.float16)
    return: Tuple[int, int, int]
  - name: w8a8_block_fp8_matmul_deepgemm
    signature: (A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype)
    return: torch.Tensor
  - name: w8a8_block_fp8_matmul_triton
    signature: (A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype = torch.float16)
    return: torch.Tensor
    doc: This function performs matrix multiplication with block-wise quantization.
  - name: grid
    signature: (META)
  - name: w8a8_block_fp8_matmul
    signature: (A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype = torch.float16)
    return: torch.Tensor
  - name: _per_tensor_quant_mla_fp8_stage1
    signature: (x_ptr, x_s_ptr, head_size, x_stride_h, x_stride_s, eps, fp8_max, BLOCK_SIZE: tl.constexpr)
  - name: _per_tensor_quant_mla_fp8_stage2
    signature: (x_ptr, x_s_ptr, x_q_ptr, num_seq, head_size, x_stride_h, x_stride_s, fp8_min, fp8_max, BLOCK_SIZE: tl.constexpr)
  - name: per_tensor_quant_mla_fp8
    signature: (x: torch.Tensor, x_s_out: torch.Tensor, eps: float = 1e-12)
    return: Tuple[torch.Tensor, torch.Tensor]
    doc: This function quantizes input values to float8 values with tensor-wise quantization
  - name: _per_token_group_quant_mla_deep_gemm_masked_fp8
    signature: (y_ptr, y_q_ptr, y_s_ptr, masked_m_ptr, group_size, y_stride_b, y_stride_t, y_q_stride_b, y_q_stride_t, y_s_stride_b, y_s_stride_g, eps, fp8_min, fp8_max, NUM_GROUP: tl.constexpr, BLOCK: tl.constexpr)
    doc: A Triton-accelerated function to perform per-token-group
  - name: per_token_group_quant_mla_deep_gemm_masked_fp8
    signature: (x: torch.Tensor, group_size: int = 128, eps: float = 1e-12, dtype: torch.dtype = fp8_dtype)
    return: Tuple[torch.Tensor, torch.Tensor]
    doc: This function quantizes input values to float8 values with per-token-group-quantization
  - name: scaled_fp8_quant
    signature: (input: torch.Tensor, scale: Optional[torch.Tensor] = None, num_token_padding: Optional[int] = None, use_per_token_if_dynamic: bool = False)
    return: tuple[torch.Tensor, torch.Tensor]
  - name: scaled_fp8_quant
    signature: (input: torch.Tensor, scale: Optional[torch.Tensor] = None, num_token_padding: Optional[int] = None, use_per_token_if_dynamic: bool = False)
    return: tuple[torch.Tensor, torch.Tensor]
  - name: _per_token_group_quant_fp8_hopper_moe_mn_major
    signature: (a, expert_offsets, problem_sizes, a_fp8, sfa, K: tl.constexpr, BLOCK_K: tl.constexpr, M_ALIGNMENT: tl.constexpr, BLOCK_M: tl.constexpr)
  - name: per_token_group_quant_fp8_hopper_moe_mn_major
    signature: (A: torch.Tensor, expert_offsets: torch.Tensor, problem_sizes: torch.Tensor, group_size: int, expert_tokens_alignment: int = 1)
    return: Tuple[torch.Tensor, torch.Tensor]
  - name: _per_group_transpose
    signature: (data_ptr: torch.Tensor, trans_data_ptr: torch.Tensor, expert_offsets: torch.Tensor, k: int, M_ALIGNMENT: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_K: tl.constexpr)
  - name: per_group_transpose
    signature: (a: torch.Tensor, expert_offsets: torch.Tensor, M_ALIGNMENT: int = 1)
    return: torch.Tensor
  - name: is_weak_contiguous
    signature: (x: torch.Tensor)
  - name: scaled_mm_kernel
    signature: (a_ptr, b_ptr, scale_a_ptr, scale_b_ptr, c_ptr, bias_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, ACCUMULATOR_DTYPE: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_SCALE_A: tl.constexpr, BLOCK_SIZE_SCALE_B: tl.constexpr)
  - name: triton_scaled_mm
    signature: (input: torch.Tensor, weight: torch.Tensor, scale_a: torch.Tensor, scale_b: torch.Tensor, out_dtype: type[torch.dtype], bias: Optional[torch.Tensor] = None, block_size_m: int = 32, block_size_n: int = 32, block_size_k: int = 32, use_heuristic = True)
    return: torch.Tensor

File: layers/quantization/fp8_utils.py
  - name: use_rowwise_torch_scaled_mm
    signature: ()
  - name: cutlass_fp8_supported
    signature: ()
  - name: normalize_e4m3fn_to_e4m3fnuz
    signature: (weight: torch.Tensor, weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor] = None)
    return: Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]
  - name: cutlass_block_fp8_supported
    signature: ()
    return: bool
  - name: dispatch_w8a8_block_fp8_linear
    signature: ()
    return: Callable
  - name: flashinfer_gemm_w8a8_block_fp8_linear
    signature: (input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
  - name: cutlass_w8a8_block_fp8_linear_with_fallback
    signature: (input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
  - name: deepgemm_w8a8_block_fp8_linear_with_fallback
    signature: (input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
  - name: _check_ue8m0
    signature: (name, x)
  - name: aiter_w8a8_block_fp8_linear
    signature: (input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
  - name: triton_w8a8_block_fp8_linear
    signature: (input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
  - name: dequant_mxfp4
    signature: (w_block: torch.Tensor, w_scale: torch.Tensor, out_dtype)
    return: torch.Tensor
    doc: :param w_block: (batch, n, k, 16), uint8, pack two mxfp4 into one byte
  - name: input_to_float8
    signature: (x: torch.Tensor, dtype: torch.dtype = fp8_dtype)
    return: Tuple[torch.Tensor, torch.Tensor]
    doc: This function quantizes input values to float8 values with tensor-wise quantization.
  - name: block_quant_to_tensor_quant
    signature: (x_q_block: torch.Tensor, x_s: torch.Tensor, block_size: List[int])
    return: Tuple[torch.Tensor, torch.Tensor]
    doc: This function converts block-wise quantization to tensor-wise quantization.
  - name: block_quant_dequant
    signature: (x_q_block: torch.Tensor, x_s: torch.Tensor, block_size: List[int], dtype: torch.dtype)
    return: torch.Tensor
    doc: This function converts block-wise quantization to unquantized.
  - name: requant_weight_ue8m0_inplace
    signature: (weight, weight_scale_inv, weight_block_size)
  - name: _requant_weight_ue8m0
    signature: (weight: torch.Tensor, weight_scale_inv: torch.Tensor, weight_block_size: List[int])
  - name: _transform_scale
    signature: (sf, mn: int)
  - name: per_block_cast_to_fp8
    signature: (x: torch.Tensor)
    return: Tuple[torch.Tensor, torch.Tensor]
  - name: ceil_to_ue8m0
    signature: (x: torch.Tensor)
  - name: channel_quant_to_tensor_quant
    signature: (x_q_channel: torch.Tensor, x_s: torch.Tensor)
    return: Tuple[torch.Tensor, torch.Tensor]
  - name: _process_scaled_mm_output
    signature: (output, input_2d_shape, output_shape)
  - name: _apply_fallback_scaled_mm
    signature: (qinput, weight, x_scale, weight_scale, input_2d_shape, output_shape, bias, input_dtype)
  - name: apply_fp8_linear
    signature: (input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor] = None, input_scale_ub: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, cutlass_fp8_supported: bool = cutlass_fp8_supported(), use_per_token_if_dynamic: bool = False, pad_output: Optional[bool] = None, compressed_tensor_quant: bool = False)
    return: torch.Tensor
  - name: can_auto_enable_marlin_fp8
    signature: ()
    return: bool

File: layers/quantization/fpgemm_fp8.py
  - name: __init__
    signature: (self, ignore_list: list[str], input_scale_ub: float)
    class: FBGEMMFp8Config
  - name: get_name
    signature: (cls)
    return: str
    class: FBGEMMFp8Config
  - name: get_supported_act_dtypes
    signature: (cls)
    return: list[torch.dtype]
    class: FBGEMMFp8Config
  - name: get_min_capability
    signature: (cls)
    return: int
    class: FBGEMMFp8Config
  - name: get_config_filenames
    signature: (cls)
    return: list[str]
    class: FBGEMMFp8Config
  - name: from_config
    signature: (cls, config: dict[str, Any])
    return: FBGEMMFp8Config
    class: FBGEMMFp8Config
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional[QuantizeMethodBase]
    class: FBGEMMFp8Config
  - name: get_scaled_act_names
    signature: (self)
    return: List[str]
    class: FBGEMMFp8Config
  - name: __init__
    signature: (self, quant_config: FBGEMMFp8Config)
    class: FBGEMMFp8LinearMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: FBGEMMFp8LinearMethod
  - name: process_weights_after_loading
    signature: (self, layer: Module)
    return: None
    class: FBGEMMFp8LinearMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: FBGEMMFp8LinearMethod

File: layers/quantization/gptq.py
  - name: check_marlin_format
    signature: (hf_quant_cfg: Dict[str, Any])
    return: bool
  - name: gptq_marlin_moe_repack
    signature: (b_q_weight: torch.Tensor, perm: torch.Tensor, size_k: int, size_n: int, num_bits: int)
    return: torch.Tensor
  - name: __init__
    signature: (self, weight_bits: int, group_size: int, desc_act: bool, lm_head_quantized: bool, dynamic: Dict[str, Dict[str, Union[int, bool]]])
    return: None
    class: GPTQConfig
  - name: __repr__
    signature: (self)
    return: str
    class: GPTQConfig
  - name: get_scaled_act_names
    signature: (self)
    return: List[str]
    class: GPTQConfig
    doc: Returns the activation function names that should be post-scaled.
  - name: get_name
    signature: (cls)
    return: str
    class: GPTQConfig
  - name: get_supported_act_dtypes
    signature: (cls)
    return: List[torch.dtype]
    class: GPTQConfig
  - name: get_min_capability
    signature: (cls)
    return: int
    class: GPTQConfig
  - name: get_config_filenames
    signature: (cls)
    return: List[str]
    class: GPTQConfig
  - name: from_config
    signature: (cls, config: Dict[str, Any])
    return: GPTQConfig
    class: GPTQConfig
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional[LinearMethodBase]
    class: GPTQConfig
  - name: __init__
    signature: (self, weight_bits: int, group_size: int, desc_act: bool, is_sym: bool, lm_head_quantized: bool, dynamic: Dict[str, Dict[str, Union[int, bool]]], full_config: Dict[str, Any])
    return: None
    class: GPTQMarlinConfig
  - name: __repr__
    signature: (self)
    return: str
    class: GPTQMarlinConfig
  - name: get_scaled_act_names
    signature: (self)
    return: List[str]
    class: GPTQMarlinConfig
    doc: Returns the activation function names that should be post-scaled.
  - name: get_name
    signature: (cls)
    return: str
    class: GPTQMarlinConfig
  - name: get_supported_act_dtypes
    signature: (cls)
    return: List[torch.dtype]
    class: GPTQMarlinConfig
  - name: get_min_capability
    signature: (cls)
    return: int
    class: GPTQMarlinConfig
  - name: get_config_filenames
    signature: (cls)
    return: List[str]
    class: GPTQMarlinConfig
  - name: from_config
    signature: (cls, config: Dict[str, Any])
    return: GPTQMarlinConfig
    class: GPTQMarlinConfig
  - name: override_quantization_method
    signature: (cls, hf_quant_cfg, user_quant)
    return: Optional[str]
    class: GPTQMarlinConfig
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional[QuantizeMethodBase]
    class: GPTQMarlinConfig
  - name: is_gptq_marlin_compatible
    signature: (cls, quant_config: Dict[str, Any])
    class: GPTQMarlinConfig
  - name: __init__
    signature: (self, quant_config: GPTQConfig)
    class: GPTQLinearMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: GPTQLinearMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: GPTQLinearMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: GPTQLinearMethod
  - name: __init__
    signature: (self, quant_config: GPTQMarlinConfig)
    return: None
    class: GPTQMarlinLinearMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    return: None
    class: GPTQMarlinLinearMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: GPTQMarlinLinearMethod
  - name: _transform_param
    signature: (layer: torch.nn.Module, name: Optional[str], fn: Callable)
    return: None
    class: GPTQMarlinLinearMethod
  - name: transform_w_q
    signature: (x)
    class: GPTQMarlinLinearMethod
  - name: transform_w_s
    signature: (x)
    class: GPTQMarlinLinearMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: GPTQMarlinLinearMethod
  - name: _get_weight_params
    signature: (layer: torch.nn.Module)
    return: tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]
    class: GPTQMarlinLinearMethod
  - name: __init__
    signature: (self, quant_config: GPTQMarlinConfig)
    return: None
    class: GPTQMarlinMoEMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: GPTQMarlinMoEMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: GPTQMarlinMoEMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: GPTQMarlinMoEMethod

File: layers/quantization/int8_kernel.py
  - name: _per_token_quant_int8
    signature: (x_ptr, xq_ptr, scale_ptr, x_sum_ptr, stride_x, stride_xq, N, CAL_SUM: tl.constexpr, BLOCK: tl.constexpr)
  - name: per_token_quant_int8
    signature: (x, scale_dtype = torch.float32, cal_sum = False)
  - name: _per_token_group_quant_int8
    signature: (y_ptr, y_q_ptr, y_s_ptr, y_stride, N, eps, int8_min, int8_max, BLOCK: tl.constexpr)
    doc: A Triton-accelerated function to perform per-token-group quantization on a
  - name: per_token_group_quant_int8
    signature: (x: torch.Tensor, group_size: int, eps: float = 1e-10, dtype: torch.dtype = torch.int8)
    return: Tuple[torch.Tensor, torch.Tensor]
    doc: Function to perform per-token-group quantization on an input tensor `x`.
  - name: sglang_per_token_group_quant_int8
    signature: (x: torch.Tensor, group_size: int, eps: float = 1e-10, dtype: torch.dtype = torch.int8)
  - name: _w8a8_block_int8_matmul
    signature: (A, B, C, As, Bs, M, N, K, group_n, group_k, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, stride_As_m, stride_As_k, stride_Bs_k, stride_Bs_n, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr)
    doc: Triton-accelerated function used to perform linear operations (dot
  - name: get_w8a8_block_int8_configs
    signature: (N: int, K: int, block_n: int, block_k: int)
    return: Optional[Dict[int, Any]]
    doc: Return optimized configurations for the w8a8 block fp8 kernel.
  - name: w8a8_block_int8_matmul
    signature: (A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype = torch.float16)
    return: torch.Tensor
    doc: This function performs matrix multiplication with block-wise quantization.
  - name: grid
    signature: (META)

File: layers/quantization/int8_utils.py
  - name: apply_w8a8_block_int8_linear
    signature: (input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
  - name: input_to_int8
    signature: (x: torch.Tensor, dtype: torch.dtype = torch.int8)
    return: Tuple[torch.Tensor, torch.Tensor]
    doc: This function quantizes input values to int8 values with tensor-wise quantization.
  - name: block_dequant
    signature: (x_q_block: torch.Tensor, x_s: torch.Tensor, block_size: List[int])
    return: torch.Tensor
    doc: This function conducts block-wise dequantization.

File: layers/quantization/kv_cache.py
  - name: __init__
    signature: (self, quant_config: QuantizationConfig)
    class: BaseKVCacheMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module)
    class: BaseKVCacheMethod
    doc: Create "weight" (aka k_scale and v_scale) for an attention layer.
  - name: apply
    signature: (self, layer: torch.nn.Module)
    return: torch.Tensor
    class: BaseKVCacheMethod
  - name: process_weights_after_loading
    signature: (self, layer: RadixAttention)
    return: None
    class: BaseKVCacheMethod

File: layers/quantization/marlin_utils.py
  - name: query_marlin_supported_quant_types
    signature: (has_zp: Optional[bool] = None, include_fp_type: bool = True, device_capability: Optional[int] = None)
  - name: _check_marlin_supported
    signature: (quant_type: ScalarType, group_size: Optional[int], has_zp: bool, device_capability: Optional[int] = None)
    return: tuple[bool, Optional[str]]
  - name: check_marlin_supported
    signature: (quant_type: ScalarType, group_size: int, has_zp: bool = False, device_capability: Optional[int] = None)
    return: bool
  - name: verify_marlin_supported
    signature: (quant_type: ScalarType, group_size: int, has_zp: bool = False)
    return: None
  - name: verify_marlin_supports_shape
    signature: (output_size_per_partition: int, input_size_per_partition: int, input_size: int, group_size: int)
    return: None
  - name: check_marlin_supports_shape
    signature: (output_size_per_partition: int, input_size_per_partition: int, input_size: int, group_size: int)
    return: tuple[bool, Optional[str]]
  - name: check_marlin_supports_layer
    signature: (layer: LinearBase, group_size: int)
    return: bool
  - name: check_moe_marlin_supports_layer
    signature: (layer: FusedMoE, group_size: int)
    return: bool
  - name: marlin_make_workspace
    signature: (device: torch.device, max_blocks_per_sm: int = 1)
    return: torch.Tensor
  - name: marlin_is_k_full
    signature: (act_order: bool, is_row_parallel: bool)
    return: bool
  - name: marlin_repeat_scales_on_all_ranks
    signature: (act_order: bool, group_size: int, is_row_parallel: bool)
    return: bool
  - name: marlin_make_empty_g_idx
    signature: (device: torch.device)
    return: torch.Tensor
  - name: marlin_make_empty_zp
    signature: (device: torch.device)
    return: torch.Tensor
  - name: marlin_sort_g_idx
    signature: (g_idx: torch.Tensor)
    return: tuple[torch.Tensor, torch.Tensor]
  - name: get_scale_perms
    signature: ()
  - name: marlin_permute_scales
    signature: (s: torch.Tensor, size_k: int, size_n: int, group_size: int)
    return: torch.Tensor
  - name: marlin_permute_bias
    signature: (s: torch.Tensor)
    return: torch.Tensor
  - name: marlin_moe_permute_scales
    signature: (s: torch.Tensor, size_k: int, size_n: int, group_size: int)
  - name: marlin_zero_points
    signature: (zp: torch.Tensor, size_k: int, size_n: int, num_bits: int)
    return: torch.Tensor
  - name: awq_to_marlin_zero_points
    signature: (q_zp_packed: torch.Tensor, size_k: int, size_n: int, num_bits: int)
    return: torch.Tensor
  - name: moe_awq_to_marlin_zero_points
    signature: (q_zp_packed: torch.Tensor, size_k: int, size_n: int, num_bits: int)
  - name: maybe_warn_marlin_atomic_add
    signature: (device, dtype)
  - name: maybe_warn_marlin_atomic_add_env
    signature: ()
  - name: should_use_atomic_add_reduce
    signature: (m: int, n: int, k: int, device: torch.device, dtype: torch.dtype)
    return: bool
  - name: apply_gptq_marlin_linear
    signature: (input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, weight_zp: torch.Tensor, g_idx: torch.Tensor, g_idx_sort_indices: torch.Tensor, workspace: torch.Tensor, wtype: ScalarType, output_size_per_partition: int, input_size_per_partition: int, is_k_full: bool, bias: Optional[torch.Tensor] = None, use_fp32_reduce: bool = USE_FP32_REDUCE_DEFAULT)
    return: torch.Tensor
  - name: apply_awq_marlin_linear
    signature: (input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, weight_zp: torch.Tensor, g_idx: torch.Tensor, g_idx_sort_indices: torch.Tensor, workspace: torch.Tensor, quant_type: ScalarType, output_size_per_partition: int, input_size_per_partition: int, bias: Optional[torch.Tensor] = None, use_fp32_reduce: bool = USE_FP32_REDUCE_DEFAULT)
    return: torch.Tensor
  - name: __init__
    signature: (self, group_size: int, lm_head_quantized: bool)
    return: None
    class: MarlinConfig
  - name: __repr__
    signature: (self)
    return: str
    class: MarlinConfig
  - name: get_name
    signature: (cls)
    return: str
    class: MarlinConfig
  - name: get_supported_act_dtypes
    signature: (cls)
    return: list[torch.dtype]
    class: MarlinConfig
  - name: get_min_capability
    signature: (cls)
    return: int
    class: MarlinConfig
  - name: get_config_filenames
    signature: (cls)
    return: list[str]
    class: MarlinConfig
  - name: from_config
    signature: (cls, config: dict[str, Any])
    return: 'MarlinConfig'
    class: MarlinConfig
  - name: override_quantization_method
    signature: (cls, hf_quant_cfg, user_quant)
    return: Optional[str]
    class: MarlinConfig
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional[MarlinLinearMethod]
    class: MarlinConfig
  - name: __init__
    signature: (self, quant_config: MarlinConfig)
    class: MarlinLinearMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: MarlinLinearMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: MarlinLinearMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: MarlinLinearMethod

File: layers/quantization/marlin_utils_fp8.py
  - name: fp8_fused_exponent_bias_into_scales
    signature: (scales)
  - name: apply_fp8_marlin_linear
    signature: (input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, workspace: torch.Tensor, size_n: int, size_k: int, bias: Optional[torch.Tensor], use_fp32_reduce: bool = USE_FP32_REDUCE_DEFAULT)
    return: torch.Tensor
  - name: prepare_fp8_layer_for_marlin
    signature: (layer: torch.nn.Module, size_k_first: bool = True)
    return: None
  - name: prepare_moe_fp8_layer_for_marlin
    signature: (layer: torch.nn.Module, size_k_first: bool = True)
    return: None
  - name: pack_fp8_to_int32
    signature: (fp8_tensor: torch.Tensor, size_k_first: bool = True)
    return: torch.Tensor
    doc: Repack FP8 weights to gptq format (packed int32 elements)
  - name: marlin_quant_fp8_torch
    signature: (weight, group_size)

File: layers/quantization/modelopt_quant.py
  - name: __init__
    signature: (self, is_checkpoint_fp8_serialized: bool = False, kv_cache_quant_method: Optional[str] = None, exclude_modules: Optional[List[str]] = None)
    return: None
    class: ModelOptFp8Config
    doc: Args:
  - name: get_name
    signature: (cls)
    return: str
    class: ModelOptFp8Config
  - name: get_supported_act_dtypes
    signature: (cls)
    return: List[torch.dtype]
    class: ModelOptFp8Config
  - name: get_min_capability
    signature: (cls)
    return: int
    class: ModelOptFp8Config
  - name: get_config_filenames
    signature: (cls)
    return: List[str]
    class: ModelOptFp8Config
  - name: from_config
    signature: (cls, config: Dict[str, Any])
    return: ModelOptFp8Config
    class: ModelOptFp8Config
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional[QuantizeMethodBase]
    class: ModelOptFp8Config
  - name: get_scaled_act_names
    signature: (self)
    return: List[str]
    class: ModelOptFp8Config
  - name: __init__
    signature: (self, quant_config: ModelOptFp8Config)
    class: ModelOptFp8LinearMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], params_dtype: torch.dtype, **extra_weight_attrs)
    return: None
    class: ModelOptFp8LinearMethod
    doc: Creates and registers weights, weight scales, and input scales for FP8 quantization.
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: ModelOptFp8LinearMethod
    doc: Requantizes weights after loading using the maximum scale.
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: ModelOptFp8LinearMethod
    doc: Applies FP8 linear transformation.
  - name: __init__
    signature: (self, quant_config: ModelOptFp8Config)
    class: ModelOptFp8KVCacheMethod
  - name: __init__
    signature: (self, quant_config: ModelOptFp8Config)
    class: ModelOptFp8MoEMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: ModelOptFp8MoEMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: ModelOptFp8MoEMethod
    doc: Process FP8 MoE weights after loading from serialized checkpoint.
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: ModelOptFp8MoEMethod
  - name: __init__
    signature: (self, is_checkpoint_nvfp4_serialized: bool = False, kv_cache_quant_algo: str = None, group_size: int = None, exclude_modules: List[str] = None)
    return: None
    class: ModelOptFp4Config
  - name: get_name
    signature: (cls)
    return: str
    class: ModelOptFp4Config
  - name: get_supported_act_dtypes
    signature: (cls)
    return: List[torch.dtype]
    class: ModelOptFp4Config
  - name: get_min_capability
    signature: (cls)
    return: int
    class: ModelOptFp4Config
  - name: get_config_filenames
    signature: (cls)
    return: List[str]
    class: ModelOptFp4Config
  - name: from_config
    signature: (cls, config: Dict[str, Any])
    return: ModelOptFp4Config
    class: ModelOptFp4Config
  - name: is_layer_excluded
    signature: (self, prefix: str, exclude_modules: list)
    class: ModelOptFp4Config
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional[QuantizeMethodBase]
    class: ModelOptFp4Config
  - name: get_scaled_act_names
    signature: (self)
    return: List[str]
    class: ModelOptFp4Config
  - name: __init__
    signature: (self, quant_config: ModelOptFp4Config)
    class: ModelOptFp4LinearMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: ModelOptFp4LinearMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: ModelOptFp4LinearMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: ModelOptFp4LinearMethod
  - name: __init__
    signature: (self, quant_config: ModelOptFp4Config)
    class: ModelOptNvFp4FusedMoEMethod
  - name: enable_flashinfer_cutlass_moe
    signature: (self)
    return: bool
    class: ModelOptNvFp4FusedMoEMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: ModelOptNvFp4FusedMoEMethod
  - name: swizzle_blockscale
    signature: (self, scale: torch.Tensor)
    class: ModelOptNvFp4FusedMoEMethod
  - name: prepare_static_weights_for_kernel
    signature: (self, gemm1_weights, gemm2_weights, gemm1_scales_linear_fp4_bytes, gemm2_scales_linear_fp4_bytes, hidden_size, intermediate_size, num_experts)
    class: ModelOptNvFp4FusedMoEMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: ModelOptNvFp4FusedMoEMethod
    doc: Process FP4 MoE weights after loading from serialized checkpoint.
  - name: load_up_proj_weight_first
    signature: (self)
    return: bool
    class: ModelOptNvFp4FusedMoEMethod
  - name: apply
    signature: (self, layer: FusedMoE, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: ModelOptNvFp4FusedMoEMethod

File: layers/quantization/moe_wna16.py
  - name: get_weight_perm
    signature: (num_bits: int)
  - name: __init__
    signature: (self, linear_quant_method: str, weight_bits: int, group_size: int, has_zp: bool, lm_head_quantized: bool, modules_to_not_convert: Optional[List[str]], full_config: Dict[str, Any])
    return: None
    class: MoeWNA16Config
  - name: get_name
    signature: (cls)
    return: str
    class: MoeWNA16Config
  - name: get_supported_act_dtypes
    signature: (cls)
    return: List[torch.dtype]
    class: MoeWNA16Config
  - name: get_min_capability
    signature: (cls)
    return: int
    class: MoeWNA16Config
  - name: get_config_filenames
    signature: (cls)
    return: List[str]
    class: MoeWNA16Config
  - name: get_scaled_act_names
    signature: (self)
    return: List[str]
    class: MoeWNA16Config
  - name: from_config
    signature: (cls, config: Dict[str, Any])
    return: MoeWNA16Config
    class: MoeWNA16Config
  - name: override_quantization_method
    signature: (cls, hf_quant_cfg, user_quant)
    return: Optional[str]
    class: MoeWNA16Config
  - name: is_moe_wna16_compatible
    signature: (cls, quant_config: Dict[str, Any])
    class: MoeWNA16Config
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional[QuantizeMethodBase]
    class: MoeWNA16Config
  - name: is_layer_skipped_quant
    signature: (prefix: str, modules_to_not_convert: List[str])
  - name: __init__
    signature: (self, quant_config: MoeWNA16Config)
    class: MoeWNA16Method
  - name: create_weights
    signature: (self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: MoeWNA16Method
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: MoeWNA16Method
  - name: get_weight_loader
    signature: (layer, weight_loader)
    class: MoeWNA16Method
  - name: convert_awq_tensor
    signature: (tensor, tensor_type)
    class: MoeWNA16Method
  - name: convert_gptq_int4_qzeros
    signature: (tensor)
    class: MoeWNA16Method
  - name: moe_wna16_weight_loader
    signature: (param: torch.nn.Parameter, loaded_weight: torch.Tensor, weight_name: str, shard_id: str, expert_id: int)
    class: MoeWNA16Method

File: layers/quantization/mxfp4.py
  - name: _swizzle_mxfp4
    signature: (quant_tensor, scale, num_warps)
    doc: weight swizzle for mxfp4 moe, used for OAI mxfp4 kernel
  - name: _dequant_mxfp4
    signature: (x: torch.Tensor, scale: torch.Tensor, float_dtype: torch.dtype)
    return: torch.Tensor
  - name: _dequant_mxfp4_fake
    signature: (x: torch.Tensor, scale: torch.Tensor, float_dtype: torch.dtype)
    return: torch.Tensor
  - name: _quant_dequant_mxfp4
    signature: (x: torch.Tensor, scale_calculation_mode: str = 'even')
    return: torch.Tensor
  - name: _quant_dequant_mxfp4_fake
    signature: (x: torch.Tensor, scale_calculation_mode: str = 'even')
    return: torch.Tensor
  - name: __init__
    signature: (self, ignored_layers: Optional[list[str]] = None, is_checkpoint_mxfp4_serialized: bool = False)
    class: Mxfp4Config
  - name: from_config
    signature: (cls, config)
    class: Mxfp4Config
  - name: get_min_capability
    signature: (cls)
    return: int
    class: Mxfp4Config
  - name: get_name
    signature: (cls)
    return: str
    class: Mxfp4Config
  - name: get_supported_act_dtypes
    signature: (cls)
    return: list[torch.dtype]
    class: Mxfp4Config
  - name: get_config_filenames
    signature: (cls)
    return: list[str]
    class: Mxfp4Config
  - name: is_static_cfg
    signature: (self)
    class: Mxfp4Config
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional['QuantizeMethodBase']
    class: Mxfp4Config
  - name: get_scaled_act_names
    signature: (self)
    return: List[str]
    class: Mxfp4Config
  - name: __init__
    signature: (self, prefix: str)
    class: Mxfp4MoEMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype, with_bias: bool = False, **extra_weight_attrs)
    class: Mxfp4MoEMethod
  - name: process_weights_after_loading
    signature: (self, layer)
    class: Mxfp4MoEMethod
  - name: swap_every_two_rows
    signature: (x, axis = -1)
    class: Mxfp4MoEMethod
  - name: _get_tile_tokens_dim
    signature: (self, x: torch.Tensor, top_k: int)
    class: Mxfp4MoEMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: Mxfp4MoEMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: Mxfp4DynamicQuantMoEMethod
  - name: mxfp4_quantize
    signature: (self, w)
    class: Mxfp4DynamicQuantMoEMethod
  - name: process_weights_after_loading
    signature: (self, layer: Module)
    return: None
    class: Mxfp4DynamicQuantMoEMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: Mxfp4DynamicQuantMoEMethod

File: layers/quantization/mxfp4_tensor.py
  - name: quantize
    signature: (cls, input: torch.Tensor, block_size: Optional[int])
    return: tuple
    class: MXFP4QuantizeUtil
    doc: Converting a tensor to a quantized format based on MXFP4 quantization. Only E4M3 is supported.
  - name: cast_fp4
    signature: (x)
    class: MXFP4QuantizeUtil
  - name: fuse_uint4_to_uint8
    signature: (x)
    class: MXFP4QuantizeUtil
  - name: dequantize
    signature: (cls, quantized_data, dtype: torch.dtype, scale, block_sizes)
    class: MXFP4QuantizeUtil
    doc: Dequantze MXFP4 packed tensor to a target dtype.
  - name: unfuse_uint8_to_uint4
    signature: (x)
    class: MXFP4QuantizeUtil
    doc: Unfuse uint8 values back to uint4 values.

File: layers/quantization/petit.py
  - name: __init__
    signature: (self, is_checkpoint_nvfp4_serialized: bool = False, kv_cache_quant_algo: str = None, group_size: int = None, exclude_modules: List[str] = None)
    return: None
    class: PetitNvFp4Config
  - name: get_name
    signature: (cls)
    return: str
    class: PetitNvFp4Config
  - name: get_supported_act_dtypes
    signature: (cls)
    return: List[torch.dtype]
    class: PetitNvFp4Config
  - name: get_min_capability
    signature: (cls)
    return: int
    class: PetitNvFp4Config
  - name: get_config_filenames
    signature: (cls)
    return: List[str]
    class: PetitNvFp4Config
  - name: from_config
    signature: (cls, config: Dict[str, Any])
    return: 'PetitNvFp4Config'
    class: PetitNvFp4Config
  - name: override_quantization_method
    signature: (cls, hf_quant_cfg, user_quant)
    return: Optional[str]
    class: PetitNvFp4Config
  - name: is_petit_nvfp4_compatible
    signature: (cls, quant_config: Dict[str, Any])
    return: bool
    class: PetitNvFp4Config
  - name: is_layer_excluded
    signature: (self, prefix: str, exclude_modules: list)
    class: PetitNvFp4Config
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional['QuantizeMethodBase']
    class: PetitNvFp4Config
  - name: get_scaled_act_names
    signature: (self)
    return: List[str]
    class: PetitNvFp4Config
  - name: __init__
    signature: (self, quant_config: PetitNvFp4Config)
    class: PetitNvFp4LinearMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: PetitNvFp4LinearMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: PetitNvFp4LinearMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: PetitNvFp4LinearMethod

File: layers/quantization/petit_utils.py
  - name: _check_petit_nvfp4_supported
    signature: (quant_method: str, group_size: Optional[int])
    return: tuple[bool, Optional[str]]
  - name: prepare_nvfp4_layer_for_petit
    signature: (layer: torch.nn.Module)
    return: None
  - name: apply_petit_nvfp4_linear
    signature: (input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, weight_scale_2: torch.Tensor, size_n: int, size_k: int, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
  - name: _check_petit_nvfp4_supported
    signature: (quant_method: str, group_size: Optional[int])
    return: tuple[bool, Optional[str]]
  - name: verify_petit_nvfp4_supported
    signature: (quant_method: str, group_size: Optional[int])
    return: None
  - name: prepare_nvfp4_layer_for_petit
    signature: (layer: torch.nn.Module)
    return: None
  - name: apply_petit_nvfp4_linear
    signature: (input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, weight_scale_2: torch.Tensor, size_n: int, size_k: int, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor

File: layers/quantization/qoq.py
  - name: __init__
    signature: (self, weight_bits: int, group_size: int)
    return: None
    class: QoQConfig
  - name: __repr__
    signature: (self)
    return: str
    class: QoQConfig
  - name: get_supported_act_dtypes
    signature: (cls)
    return: List[torch.dtype]
    class: QoQConfig
  - name: get_min_capability
    signature: (cls)
    return: int
    class: QoQConfig
  - name: get_name
    signature: (cls)
    return: str
    class: QoQConfig
  - name: get_config_filenames
    signature: (cls)
    return: List[str]
    class: QoQConfig
    doc: List of filenames to search for in the model directory.
  - name: from_config
    signature: (cls, config: Dict[str, Any])
    return: QoQConfig
    class: QoQConfig
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional[QuantizeMethodBase]
    class: QoQConfig
  - name: get_scaled_act_names
    signature: (self)
    return: List[str]
    class: QoQConfig
  - name: __init__
    signature: (self, quant_config: QoQConfig)
    class: QoQLinearMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: QoQLinearMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: QoQLinearMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    class: QoQLinearMethod

File: layers/quantization/quark/__init__.py
  (no function definitions found)
File: layers/quantization/quark/quark.py
  - name: __init__
    signature: (self, quant_config: dict[str, Any], kv_cache_group: Optional[list[str]] = None, kv_cache_config: Optional[dict[str, Any]] = None, pack_method: str = 'reorder')
    class: QuarkConfig
  - name: get_linear_method
    signature: (self)
    return: 'QuarkLinearMethod'
    class: QuarkConfig
  - name: get_supported_act_dtypes
    signature: (cls)
    return: list[torch.dtype]
    class: QuarkConfig
  - name: get_min_capability
    signature: (cls)
    return: int
    class: QuarkConfig
  - name: get_name
    signature: (self)
    return: str
    class: QuarkConfig
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional['QuantizeMethodBase']
    class: QuarkConfig
  - name: from_config
    signature: (cls, config: dict[str, Any])
    return: 'QuarkConfig'
    class: QuarkConfig
  - name: get_config_filenames
    signature: (cls)
    return: list[str]
    class: QuarkConfig
  - name: _check_scheme_supported
    signature: (self, min_capability: int, error: bool = True)
    return: bool
    class: QuarkConfig
  - name: _is_mx_fp4
    signature: (self, weight_quant: Optional[dict[str, Any]], input_quant: Optional[dict[str, Any]])
    return: bool
    class: QuarkConfig
  - name: _find_matched_config
    signature: (self, layer_name: str, module: torch.nn.Module)
    return: dict[str, Any]
    class: QuarkConfig
  - name: _get_scheme_from_config
    signature: (self, config: dict[str, Any])
    return: 'QuarkScheme'
    class: QuarkConfig
  - name: get_scheme
    signature: (self, layer: torch.nn.Module, layer_name: str)
    return: 'QuarkScheme'
    class: QuarkConfig
  - name: get_scaled_act_names
    signature: (self)
    return: List[str]
    class: QuarkConfig
  - name: __init__
    signature: (self, quantization_config: QuarkConfig)
    class: QuarkLinearMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: QuarkLinearMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: QuarkLinearMethod
    doc: Use the CompressedTensorsScheme associated with each layer to create
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    class: QuarkLinearMethod
    doc: Use the output of create_weights and the CompressedTensorsScheme
  - name: __init__
    signature: (self, quant_config: QuarkConfig)
    class: QuarkKVCacheMethod
  - name: validate_kv_cache_config
    signature: (kv_cache_config: Optional[dict[str, Any]])
    class: QuarkKVCacheMethod
    doc: Validator for the kv cache configuration. Useful for controlling the

File: layers/quantization/quark/quark_moe.py
  - name: __new__
    signature: (cls, *args, **kwargs)
    class: QuarkMoEMethod
  - name: get_moe_method
    signature: (quant_config: 'QuarkConfig', module: torch.nn.Module, layer_name: str)
    return: 'QuarkMoEMethod'
    class: QuarkMoEMethod
  - name: __init__
    signature: (self, weight_config: dict[str, Any], input_config: dict[str, Any])
    class: QuarkW4A4MXFp4MoEMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: QuarkW4A4MXFp4MoEMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: QuarkW4A4MXFp4MoEMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: QuarkW4A4MXFp4MoEMethod

File: layers/quantization/quark/schemes/__init__.py
  (no function definitions found)
File: layers/quantization/quark/schemes/quark_scheme.py
  - name: get_min_capability
    signature: (cls)
    return: int
    class: QuarkScheme
    doc: Get minimum device capability.
  - name: create_weights
    signature: (self, *args, **kwargs)
    class: QuarkScheme
    doc: Weight creation for the particular scheme. Inputs to this function
  - name: apply_weights
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
    class: QuarkScheme
    doc: Run the forward pass for the particular scheme. This is where
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    class: QuarkScheme
    doc: Called after weight loading is complete for any cleanup that

File: layers/quantization/quark/schemes/quark_w4a4_mxfp4.py
  - name: __init__
    signature: (self, weight_quant_spec: dict[str, Any], input_quant_spec: dict[str, Any])
    class: QuarkW4A4MXFP4
  - name: get_min_capability
    signature: (cls)
    return: int
    class: QuarkW4A4MXFP4
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: QuarkW4A4MXFP4
  - name: create_weights
    signature: (self, layer: torch.nn.Module, output_partition_sizes: list[int], input_size_per_partition: int, params_dtype: torch.dtype, weight_loader: Callable, **kwargs)
    class: QuarkW4A4MXFP4
  - name: apply_weights
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: QuarkW4A4MXFP4

File: layers/quantization/quark/utils.py
  - name: deep_compare
    signature: (dict1: Any, dict2: Any)
    return: bool
  - name: should_ignore_layer
    signature: (layer_name: Optional[str], ignore: Iterable[str], fused_mapping: Mapping[str, list[str]] = MappingProxyType({}))
    return: bool
  - name: check_equal_or_regex_match
    signature: (layer_name: str, targets: Iterable[str])
    return: bool
    doc: Checks whether a layer_name is exactly equal or a regex match for
  - name: _is_equal_or_regex_match
    signature: (value: str, target: str, check_contains: bool = False)
    return: bool
    doc: Checks whether a value is exactly equal or a regex match for target

File: layers/quantization/unquant.py
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: UnquantizedEmbeddingMethod
    doc: Create weights for embedding layer.
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: UnquantizedEmbeddingMethod
  - name: embedding
    signature: (self, layer: torch.nn.Module, input_: torch.Tensor)
    return: torch.Tensor
    class: UnquantizedEmbeddingMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: UnquantizedLinearMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: UnquantizedLinearMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: UnquantizedLinearMethod
  - name: __init__
    signature: (self, use_triton_kernels: bool = False)
    class: UnquantizedFusedMoEMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype, with_bias: bool = False, **extra_weight_attrs)
    class: UnquantizedFusedMoEMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: UnquantizedFusedMoEMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: UnquantizedFusedMoEMethod
  - name: forward_cuda
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: UnquantizedFusedMoEMethod
  - name: forward_cpu
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: UnquantizedFusedMoEMethod
  - name: forward_npu
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: UnquantizedFusedMoEMethod
  - name: forward_tpu
    signature: (self, *args, **kwargs)
    return: torch.Tensor
    class: UnquantizedFusedMoEMethod

File: layers/quantization/utils.py
  - name: get_scalar_types
    signature: ()
    doc: Returns:
  - name: __getattr__
    signature: (self, name)
    class: MockScalarTypes
  - name: is_layer_skipped
    signature: (prefix: str, ignored_layers: List[str], fused_mapping: Mapping[str, List[str]] = MappingProxyType({}))
    return: bool
  - name: per_tensor_dequantize
    signature: (tensor: torch.Tensor, inv_scale: Union[float, torch.Tensor])
    return: torch.Tensor
  - name: all_close_1d
    signature: (x: torch.Tensor)
    return: bool
  - name: convert_to_channelwise
    signature: (weight_scale: torch.Tensor, logical_widths: List[int])
    return: Tuple[torch.Tensor, torch.Tensor]
  - name: requantize_with_max_scale
    signature: (weight: torch.Tensor, weight_scale: torch.Tensor, logical_widths: List[int])
    return: Tuple[torch.Tensor, torch.Tensor]
  - name: update_tensor_inplace
    signature: (old: torch.Tensor, new: torch.Tensor)
    return: None
  - name: replace_parameter
    signature: (mod: torch.nn.Module, name: str, new: Union[torch.Tensor, torch.nn.Parameter])
    return: None
  - name: assert_fp8_all_close
    signature: (a: torch.Tensor, b: torch.Tensor)
  - name: override_config
    signature: (config: QuantizationConfig, prefix: str)
  - name: get_dynamic_override
    signature: (config: QuantizationConfig, layer_name: str, key: Optional[str] = None, default_value: Union[int, bool, None] = None)
    return: Union[Dict, int, bool, None]
  - name: get_linear_quant_method
    signature: (config: QuantizationConfig, layer: torch.nn.Module, prefix: str, linear_method_cls: type)
  - name: get_pack_factor
    signature: (num_bits)
  - name: permute_rows
    signature: (q_w: torch.Tensor, w_ref: torch.Tensor, group_size: int, test_perm: Optional[torch.Tensor] = None)
  - name: pack_cols
    signature: (q_w: torch.Tensor, num_bits: int, size_k: int, size_n: int)
  - name: pack_rows
    signature: (q_w: torch.Tensor, num_bits: int, size_k: int, size_n: int)
  - name: unpack_cols
    signature: (packed_q_w: torch.Tensor, num_bits: int, size_k: int, size_n: int)
  - name: quantize_weights
    signature: (w: torch.Tensor, quant_type: ScalarType, group_size: Optional[int], zero_points: bool = False, ref_zero_points_after_scales: bool = False)
  - name: reshape_w
    signature: (w)
  - name: gptq_quantize_weights
    signature: (w: torch.Tensor, quant_type: ScalarType, group_size: int, act_order: bool, test_perm: Optional[torch.Tensor] = None)
  - name: sort_weights
    signature: (q_w: torch.Tensor, g_idx: torch.Tensor)

File: layers/quantization/w4afp8.py
  - name: __init__
    signature: (self, is_checkpoint_fp8_serialized: bool = True, is_checkpoint_w4afp8_serialized: bool = True, linear_activation_scheme: str = 'dynamic', moe_activation_scheme: str = 'static', ignored_layers: Optional[List[str]] = None, weight_block_size: Optional[List[int]] = None, group_size: int = 128)
    return: None
    class: W4AFp8Config
  - name: get_name
    signature: (cls)
    return: str
    class: W4AFp8Config
  - name: get_supported_act_dtypes
    signature: (cls)
    return: List[torch.dtype]
    class: W4AFp8Config
  - name: get_min_capability
    signature: (cls)
    return: int
    class: W4AFp8Config
  - name: get_config_filenames
    signature: (cls)
    return: List[str]
    class: W4AFp8Config
  - name: from_config
    signature: (cls, config: Dict[str, Any])
    return: W4AFp8Config
    class: W4AFp8Config
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional[QuantizeMethodBase]
    class: W4AFp8Config
  - name: get_scaled_act_names
    signature: (self)
    return: List[str]
    class: W4AFp8Config
  - name: __init__
    signature: (self, quant_config: W4AFp8Config)
    class: W4AFp8MoEMethod
  - name: create_weights
    signature: (self, layer: EPMoE, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: W4AFp8MoEMethod
  - name: _interleave_scales
    signature: (self, scales: torch.Tensor)
    return: torch.Tensor
    class: W4AFp8MoEMethod
    doc: Interleave scales in groups of 4 similar to TRT-LLM implementation.
  - name: process_weights_after_loading
    signature: (self, layer: Module)
    return: None
    class: W4AFp8MoEMethod
  - name: apply
    signature: (self, layer: EPMoE, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: W4AFp8MoEMethod

File: layers/quantization/w8a8_fp8.py
  - name: __init__
    signature: (self, is_checkpoint_fp8_serialized: bool = False)
    class: W8A8Fp8Config
  - name: get_supported_act_dtypes
    signature: (cls)
    return: List[torch.dtype]
    class: W8A8Fp8Config
  - name: get_min_capability
    signature: (cls)
    return: int
    class: W8A8Fp8Config
  - name: get_name
    signature: (self)
    return: str
    class: W8A8Fp8Config
  - name: get_config_filenames
    signature: (cls)
    return: List[str]
    class: W8A8Fp8Config
  - name: from_config
    signature: (cls, config: Dict[str, Any])
    return: W8A8Fp8Config
    class: W8A8Fp8Config
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional[QuantizeMethodBase]
    class: W8A8Fp8Config
  - name: get_scaled_act_names
    signature: (self)
    return: List[str]
    class: W8A8Fp8Config
  - name: __init__
    signature: (self, quantization_config: W8A8Fp8Config)
    class: W8A8Fp8LinearMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: W8A8Fp8LinearMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: W8A8Fp8LinearMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    class: W8A8Fp8LinearMethod
  - name: __init__
    signature: (self, quant_config: W8A8Fp8Config)
    class: W8A8FP8MoEMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: W8A8FP8MoEMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: W8A8FP8MoEMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: W8A8FP8MoEMethod

File: layers/quantization/w8a8_int8.py
  - name: npu_wrapper_rmsnorm_init
    signature: (func)
  - name: init
    signature: (self, hidden_size: int, **extra_args)
    return: None
  - name: npu_wrapper_rmsnorm_forward
    signature: (func)
  - name: _rmsnorm_forward_oot
    signature: (self, x: torch.Tensor, residual: Optional[torch.Tensor] = None)
    return: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  - name: npu_fused_experts
    signature: (hidden_states: torch.Tensor, w13: torch.Tensor, w13_scale: torch.Tensor, w2: torch.Tensor, w2_scale: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, top_k: int)
  - name: __init__
    signature: (self, quant_config: Dict[str, Any] = {})
    class: W8A8Int8Config
  - name: get_supported_act_dtypes
    signature: (cls)
    return: List[torch.dtype]
    class: W8A8Int8Config
  - name: get_min_capability
    signature: (cls)
    return: int
    class: W8A8Int8Config
  - name: get_name
    signature: (self)
    return: str
    class: W8A8Int8Config
  - name: get_config_filenames
    signature: (cls)
    return: List[str]
    class: W8A8Int8Config
  - name: from_config
    signature: (cls, config: Dict[str, Any])
    return: W8A8Int8Config
    class: W8A8Int8Config
  - name: get_quant_method
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional[QuantizeMethodBase]
    class: W8A8Int8Config
  - name: is_layer_skipped
    signature: (self, prefix: str, fused_mapping: Mapping[str, List[str]] = MappingProxyType({}))
    class: W8A8Int8Config
  - name: get_scaled_act_names
    signature: (self)
    return: List[str]
    class: W8A8Int8Config
  - name: __init__
    signature: (self, quantization_config: W8A8Int8Config)
    class: W8A8Int8LinearMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: W8A8Int8LinearMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: W8A8Int8LinearMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    class: W8A8Int8LinearMethod
  - name: __init__
    signature: (self, quant_config: W8A8Int8Config)
    class: W8A8Int8MoEMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    class: W8A8Int8MoEMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: W8A8Int8MoEMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: W8A8Int8MoEMethod
  - name: __init__
    signature: (self)
    return: None
    class: NPU_W8A8LinearMethodImpl
  - name: get_weight
    signature: (input_size: int, output_size: int, params_dtype: torch.dtype = torch.bfloat16)
    return: Dict[str, Any]
    class: NPU_W8A8LinearMethodImpl
  - name: get_pertensor_param
    signature: (params_dtype: torch.dtype)
    return: Dict[str, Any]
    class: NPU_W8A8LinearMethodImpl
  - name: get_perchannel_param
    signature: (output_size: int, params_dtype: torch.dtype)
    return: Dict[str, Any]
    class: NPU_W8A8LinearMethodImpl
  - name: apply
    signature: (layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: NPU_W8A8LinearMethodImpl
  - name: process_weights_after_loading
    signature: (self, layer)
    class: NPU_W8A8LinearMethodImpl
  - name: __init__
    signature: (self)
    return: None
    class: NPU_W8A8LinearMethodMTImpl
  - name: get_weight
    signature: (input_size: int, output_size: int, params_dtype: torch.dtype = torch.bfloat16)
    return: Dict[str, Any]
    class: NPU_W8A8LinearMethodMTImpl
  - name: get_pertensor_param
    signature: (params_dtype: torch.dtype)
    return: Dict[str, Any]
    class: NPU_W8A8LinearMethodMTImpl
  - name: get_perchannel_param
    signature: (output_size: int, params_dtype: torch.dtype)
    return: Dict[str, Any]
    class: NPU_W8A8LinearMethodMTImpl
  - name: apply
    signature: (layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: NPU_W8A8LinearMethodMTImpl
  - name: process_weights_after_loading
    signature: (self, layer)
    class: NPU_W8A8LinearMethodMTImpl
  - name: __init__
    signature: (self, quantization_config: W8A8Int8Config)
    return: None
    class: NPU_W8A8LinearMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    return: None
    class: NPU_W8A8LinearMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: NPU_W8A8LinearMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: NPU_W8A8LinearMethod
  - name: __init__
    signature: (self)
    class: NPU_W8A8DynamicLinearMethodImpl
  - name: get_weight
    signature: (input_size: int, output_size: int, params_dtype: torch.dtype)
    return: Dict[str, Any]
    class: NPU_W8A8DynamicLinearMethodImpl
  - name: get_pertensor_param
    signature: (params_dtype: torch.dtype)
    return: Dict[str, Any]
    class: NPU_W8A8DynamicLinearMethodImpl
  - name: get_perchannel_param
    signature: (output_size: int, params_dtype: torch.dtype)
    return: Dict[str, Any]
    class: NPU_W8A8DynamicLinearMethodImpl
  - name: apply
    signature: (layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None, tp_rank: Optional[int] = 0)
    return: torch.Tensor
    class: NPU_W8A8DynamicLinearMethodImpl
  - name: process_weights_after_loading
    signature: (self, layer)
    class: NPU_W8A8DynamicLinearMethodImpl
  - name: __init__
    signature: (self, quantization_config: W8A8Int8Config)
    return: None
    class: NPU_W8A8DynamicLinearMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    return: None
    class: NPU_W8A8DynamicLinearMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: NPU_W8A8DynamicLinearMethod
  - name: apply
    signature: (self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: NPU_W8A8DynamicLinearMethod
  - name: __init__
    signature: (self, quantization_config: W8A8Int8Config)
    return: None
    class: NPU_W8A8MoEMethod
  - name: create_weights
    signature: (self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype, **extra_weight_attrs)
    return: None
    class: NPU_W8A8MoEMethod
  - name: process_weights_after_loading
    signature: (self, layer: torch.nn.Module)
    return: None
    class: NPU_W8A8MoEMethod
  - name: apply
    signature: (self, layer, x, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
    return: torch.Tensor
    class: NPU_W8A8MoEMethod

File: layers/radix_attention.py
  - name: __init__
    signature: (self, num_heads: int, head_dim: int, scaling: float, num_kv_heads: int, layer_id: int, logit_cap: float = 0.0, v_head_dim: int = -1, sliding_window_size: int = -1, is_cross_attention: bool = False, pos_encoding_mode: str = 'NONE', logit_capping_method: str = 'tanh', quant_config: Optional[QuantizationConfig] = None, attn_type: AttentionType = AttentionType.DECODER, use_irope: bool = False, prefix: str = '')
    class: RadixAttention
  - name: forward
    signature: (self, q, k, v, forward_batch: ForwardBatch, save_kv_cache: bool = True, **kwargs)
    class: RadixAttention

File: layers/rotary_embedding.py
  - name: _rotate_neox
    signature: (x: torch.Tensor)
    return: torch.Tensor
  - name: _rotate_gptj
    signature: (x: torch.Tensor)
    return: torch.Tensor
  - name: _apply_rotary_emb
    signature: (x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor, is_neox_style: bool)
    return: torch.Tensor
    doc: Args:
  - name: __init__
    signature: (self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype)
    return: None
    class: RotaryEmbedding
  - name: _compute_inv_freq
    signature: (self, base: Union[int, float])
    return: torch.Tensor
    class: RotaryEmbedding
    doc: Compute the inverse frequency.
  - name: _compute_cos_sin_cache
    signature: (self)
    return: torch.Tensor
    class: RotaryEmbedding
    doc: Compute the cos and sin cache.
  - name: forward_native
    signature: (self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor] = None)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: RotaryEmbedding
    doc: A PyTorch-native implementation of forward().
  - name: forward_npu
    signature: (self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor] = None)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: RotaryEmbedding
    doc: A PyTorch-npu implementation of forward().
  - name: forward_cpu
    signature: (self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor] = None)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: RotaryEmbedding
  - name: forward_cuda
    signature: (self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor] = None, fused_set_kv_buffer_arg = None)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: RotaryEmbedding
  - name: extra_repr
    signature: (self)
    return: str
    class: RotaryEmbedding
  - name: __init__
    signature: (self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factors: Union[List[float], float], dtype: torch.dtype)
    return: None
    class: LinearScalingRotaryEmbedding
  - name: _compute_cos_sin_cache
    signature: (self)
    return: torch.Tensor
    class: LinearScalingRotaryEmbedding
  - name: scaling_factor_to_offset
    signature: (self)
    return: Dict[float, int]
    class: LinearScalingRotaryEmbedding
  - name: __init__
    signature: (self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype)
    return: None
    class: DynamicNTKScalingRotaryEmbedding
  - name: _compute_cos_sin_cache
    signature: (self)
    return: torch.Tensor
    class: DynamicNTKScalingRotaryEmbedding
  - name: _yarn_find_correction_dim
    signature: (num_rotations: int, dim: int, base: float = 10000, max_position_embeddings: int = 2048)
    return: float
  - name: _yarn_find_correction_range
    signature: (low_rot: int, high_rot: int, dim: int, base: float = 10000, max_position_embeddings: int = 2048)
    return: Tuple[int, int]
  - name: _yarn_linear_ramp_mask
    signature: (low: float, high: float, dim: int, dtype: torch.dtype, device: torch.device = None)
    return: torch.Tensor
  - name: _yarn_get_mscale
    signature: (scale: float = 1)
    return: float
  - name: __init__
    signature: (self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype, *, extrapolation_factor: float = 1, attn_factor: float = 1, beta_fast: int = 32, beta_slow: int = 1)
    return: None
    class: YaRNScalingRotaryEmbedding
  - name: _compute_inv_freq
    signature: (self, scaling_factor: float)
    return: torch.Tensor
    class: YaRNScalingRotaryEmbedding
  - name: _compute_cos_sin_cache
    signature: (self)
    return: torch.Tensor
    class: YaRNScalingRotaryEmbedding
  - name: __init__
    signature: (self, head_size: int, rotary_dim: int, max_position_embeddings: int, original_max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, short_factor: List[float], long_factor: List[float], short_mscale: Optional[float] = None, long_mscale: Optional[float] = None)
    class: Phi3LongRoPEScaledRotaryEmbedding
  - name: _compute_inv_freq
    signature: (self, rescale_factors: List[float])
    return: torch.Tensor
    class: Phi3LongRoPEScaledRotaryEmbedding
  - name: _compute_cos_sin_cache
    signature: (self, max_position_embeddings: int, rescale_factors: List[float], mscale: float)
    return: torch.Tensor
    class: Phi3LongRoPEScaledRotaryEmbedding
  - name: forward
    signature: (self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor] = None)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: Phi3LongRoPEScaledRotaryEmbedding
  - name: yarn_get_mscale
    signature: (scale: float = 1, mscale: float = 1)
    return: float
  - name: __init__
    signature: (self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype, *, extrapolation_factor: float = 1, attn_factor: float = 1, beta_fast: int = 32, beta_slow: int = 1, mscale: float = 1, mscale_all_dim: float = 0, device: Optional[str] = 'cuda' if not _is_npu else 'npu')
    return: None
    class: DeepseekScalingRotaryEmbedding
  - name: _compute_inv_freq
    signature: (self, scaling_factor: float)
    return: torch.Tensor
    class: DeepseekScalingRotaryEmbedding
  - name: _compute_cos_sin_cache
    signature: (self)
    return: torch.Tensor
    class: DeepseekScalingRotaryEmbedding
  - name: forward_native
    signature: (self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor] = None)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: DeepseekScalingRotaryEmbedding
    doc: PyTorch-native implementation equivalent to forward().
  - name: forward_npu
    signature: (self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor] = None)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: DeepseekScalingRotaryEmbedding
  - name: forward_cpu
    signature: (self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor] = None)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: DeepseekScalingRotaryEmbedding
  - name: __init__
    signature: (self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, scaling_factor: float, low_freq_factor: float, high_freq_factor: float, orig_max_position: int)
    return: None
    class: Llama3RotaryEmbedding
  - name: _compute_inv_freq
    signature: (self, base: Union[int, float])
    return: torch.Tensor
    class: Llama3RotaryEmbedding
  - name: __init__
    signature: (self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype)
    class: Llama4VisionRotaryEmbedding
  - name: _compute_inv_freq
    signature: (self, base: Union[int, float])
    return: torch.Tensor
    class: Llama4VisionRotaryEmbedding
  - name: _compute_cos_sin_cache
    signature: (self)
    return: torch.Tensor
    class: Llama4VisionRotaryEmbedding
  - name: forward
    signature: (self, query: torch.Tensor, key: torch.Tensor)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: Llama4VisionRotaryEmbedding
  - name: __init__
    signature: (self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_alpha: float, dtype: torch.dtype)
    return: None
    class: DynamicNTKAlphaRotaryEmbedding
  - name: _compute_cos_sin_cache
    signature: (self)
    return: torch.Tensor
    class: DynamicNTKAlphaRotaryEmbedding
  - name: __init__
    signature: (self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, mrope_section: Optional[List[int]] = None)
    return: None
    class: MRotaryEmbedding
  - name: forward
    signature: (self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: MRotaryEmbedding
    doc: PyTorch-native implementation equivalent to forward().
  - name: get_rope_index
    signature: (spatial_merge_size: int, image_token_id: int, video_token_id: int, vision_start_token_id: int, model_type: str, tokens_per_second: Optional[int] = None, input_ids: Optional[torch.LongTensor] = None, image_grid_thw: Optional[torch.LongTensor] = None, video_grid_thw: Optional[torch.LongTensor] = None, second_per_grid_ts: Optional[torch.Tensor] = None, **kwargs)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: MRotaryEmbedding
  - name: get_rope_index_glm4v
    signature: (input_ids: torch.Tensor, hf_config: Any, image_grid_thw: Union[list[list[int]], torch.Tensor], video_grid_thw: Union[list[list[int]], torch.Tensor], attention_mask: torch.Tensor, **kwargs)
    return: tuple[torch.Tensor, torch.Tensor]
    class: MRotaryEmbedding
    doc: Get mrope input positions and delta value for GLM4V.
  - name: get_next_input_positions
    signature: (mrope_position_delta: int, context_len: int, seq_len: int)
    return: torch.Tensor
    class: MRotaryEmbedding
  - name: __init__
    signature: (self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, chunk_size: int, local_size: int)
    return: None
    class: DualChunkRotaryEmbedding
  - name: _compute_inv_freq
    signature: (self, base: Union[int, float])
    return: torch.Tensor
    class: DualChunkRotaryEmbedding
    doc: Compute the inverse frequency.
  - name: _compute_cos_sin_cache
    signature: (self)
    return: torch.Tensor
    class: DualChunkRotaryEmbedding
    doc: Compute the cos and sin cache.
  - name: forward
    signature: (self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor] = None)
    return: Tuple[torch.Tensor, torch.Tensor]
    class: DualChunkRotaryEmbedding
  - name: _apply_rotary_embedding
    signature: (self, cos_sin, hidden_rot, hidden_pass)
    class: DualChunkRotaryEmbedding
  - name: extra_repr
    signature: (self)
    return: str
    class: DualChunkRotaryEmbedding
  - name: get_rope
    signature: (head_size: int, rotary_dim: int, max_position: int, base: int, is_neox_style: bool = True, rope_scaling: Optional[Dict[str, Any]] = None, dtype: Optional[torch.dtype] = None, partial_rotary_factor: float = 1.0, dual_chunk_attention_config: Optional[Dict[str, Any]] = None)
    return: RotaryEmbedding
  - name: rotate_half
    signature: (x)
    doc: Rotates half the hidden dims of the input.
  - name: apply_rotary_pos_emb_native
    signature: (q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor, unsqueeze_dim = 1)
    return: Tuple[torch.Tensor, torch.Tensor]
  - name: apply_rotary_pos_emb_npu
    signature: (q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor, unsqueeze_dim = 1)
    return: Tuple[torch.Tensor, torch.Tensor]
  - name: get_rope_cpu
    signature: (head_size: int, rotary_dim: int, max_position: int, base: int, is_neox_style: bool = True, rope_scaling: Optional[Dict[str, Any]] = None, dtype: Optional[torch.dtype] = None, partial_rotary_factor: float = 1.0, device: Optional[str] = None)
    return: RotaryEmbedding
  - name: get_rope_wrapper
    signature: (head_size: int, rotary_dim: int, max_position: int, base: int, is_neox_style: bool = True, rope_scaling: Optional[Dict[str, Any]] = None, dtype: Optional[torch.dtype] = None, partial_rotary_factor: float = 1.0, device: Optional[str] = None)

File: layers/sampler.py
  - name: __init__
    signature: (self)
    class: Sampler
  - name: forward
    signature: (self, logits_output: LogitsProcessorOutput, sampling_info: SamplingBatchInfo, return_logprob: bool, top_logprobs_nums: List[int], token_ids_logprobs: List[List[int]])
    class: Sampler
    doc: Run a sampler & compute logprobs and update logits_output accordingly.
  - name: top_k_top_p_min_p_sampling_from_probs_torch
    signature: (probs: torch.Tensor, top_ks: torch.Tensor, top_ps: torch.Tensor, min_ps: torch.Tensor, need_min_p_sampling: bool)
    doc: A top-k, top-p and min-p sampling implementation with native pytorch operations.
  - name: sampling_from_probs_torch
    signature: (probs: torch.Tensor)
    doc: A sampling implementation with native pytorch operations, without
  - name: top_p_normalize_probs_torch
    signature: (probs: torch.Tensor, top_ps: torch.Tensor)
  - name: get_top_logprobs
    signature: (logprobs: torch.Tensor, top_logprobs_nums: List[int])
  - name: get_token_ids_logprobs
    signature: (logprobs: torch.Tensor, token_ids_logprobs: List[List[int]])
  - name: apply_custom_logit_processor
    signature: (logits: torch.Tensor, sampling_batch_info: SamplingBatchInfo, num_tokens_in_batch: int = 1)
    doc: Apply custom logit processors to the logits.

File: layers/torchao_utils.py
  - name: get_gemlite_cache_path
    signature: ()
    return: str
  - name: save_gemlite_cache
    signature: (print_error: bool = False)
    return: bool
  - name: proj_filter
    signature: (module: torch.nn.Module, fqn: str)
    doc: Filter function for quantizing projection layers.
  - name: apply_torchao_config_to_model
    signature: (model: torch.nn.Module, torchao_config: str, filter_fn: Optional[Callable] = proj_filter)
    doc: Quantize a modelwith torchao quantization specified by torchao_config

File: layers/utils.py
  - name: get_layer_id
    signature: (weight_name)
  - name: __init__
    signature: (self, *args, **kwargs)
    class: PPMissingLayer
  - name: forward
    signature: (self, *args, **kwargs)
    class: PPMissingLayer
    doc: Return the first arg from args or the first value from kwargs.

File: layers/vocab_parallel_embedding.py
  - name: pad_vocab_size
    signature: (vocab_size: int, pad_to: int = DEFAULT_VOCAB_PADDING_SIZE)
    return: int
    doc: Pad the vocab size to the given value.
  - name: vocab_range_from_per_partition_vocab_size
    signature: (per_partition_vocab_size: int, rank: int, offset: int = 0)
    return: Sequence[int]
  - name: vocab_range_from_global_vocab_size
    signature: (global_vocab_size: int, rank: int, world_size: int, offset: int = 0)
    return: Sequence[int]
  - name: num_org_elements
    signature: (self)
    return: int
    class: VocabParallelEmbeddingShardIndices
  - name: num_added_elements
    signature: (self)
    return: int
    class: VocabParallelEmbeddingShardIndices
  - name: num_org_elements_padded
    signature: (self)
    return: int
    class: VocabParallelEmbeddingShardIndices
  - name: num_added_elements_padded
    signature: (self)
    return: int
    class: VocabParallelEmbeddingShardIndices
  - name: num_org_vocab_padding
    signature: (self)
    return: int
    class: VocabParallelEmbeddingShardIndices
  - name: num_added_vocab_padding
    signature: (self)
    return: int
    class: VocabParallelEmbeddingShardIndices
  - name: num_elements_padded
    signature: (self)
    return: int
    class: VocabParallelEmbeddingShardIndices
  - name: __post_init__
    signature: (self)
    class: VocabParallelEmbeddingShardIndices
  - name: get_masked_input_and_mask
    signature: (input_: torch.Tensor, org_vocab_start_index: int, org_vocab_end_index: int, num_org_vocab_padding: int, added_vocab_start_index: int, added_vocab_end_index: int)
    return: Tuple[torch.Tensor, torch.Tensor]
  - name: __init__
    signature: (self, num_embeddings: int, embedding_dim: int, *, params_dtype: Optional[torch.dtype] = None, org_num_embeddings: Optional[int] = None, padding_size: int = DEFAULT_VOCAB_PADDING_SIZE, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', enable_tp: bool = True, use_attn_tp_group: bool = False, use_presharded_weights: bool = False)
    class: VocabParallelEmbedding
  - name: _get_indices
    signature: (cls, vocab_size_padded: int, org_vocab_size_padded: int, vocab_size: int, org_vocab_size: int, tp_rank: int, tp_size: int)
    return: VocabParallelEmbeddingShardIndices
    class: VocabParallelEmbedding
    doc: Get start and end indices for vocab parallel embedding, following the
  - name: get_sharded_to_full_mapping
    signature: (self)
    return: Optional[List[int]]
    class: VocabParallelEmbedding
    doc: Get a mapping that can be used to reindex the gathered
  - name: weight_loader
    signature: (self, param: Parameter, loaded_weight: torch.Tensor)
    class: VocabParallelEmbedding
  - name: forward
    signature: (self, input_)
    class: VocabParallelEmbedding
  - name: extra_repr
    signature: (self)
    return: str
    class: VocabParallelEmbedding
  - name: __init__
    signature: (self, num_embeddings: int, embedding_dim: int, *, bias: bool = False, params_dtype: Optional[torch.dtype] = None, org_num_embeddings: Optional[int] = None, padding_size: int = DEFAULT_VOCAB_PADDING_SIZE, quant_config: Optional[QuantizationConfig] = None, prefix: str = '', use_attn_tp_group: bool = False, use_presharded_weights: bool = False)
    class: ParallelLMHead
  - name: tie_weights
    signature: (self, embed_tokens: VocabParallelEmbedding)
    class: ParallelLMHead
    doc: Tie the weights with word embeddings.
  - name: forward
    signature: (self, input_)
    class: ParallelLMHead

File: managers/cache_controller.py
  - name: __init__
    signature: (self, num_layers)
    class: LayerDoneCounter
  - name: next_producer
    signature: (self)
    class: LayerDoneCounter
  - name: update_producer
    signature: (self)
    class: LayerDoneCounter
  - name: set_consumer
    signature: (self, index)
    class: LayerDoneCounter
  - name: increment
    signature: (self)
    class: LayerDoneCounter
  - name: wait_until
    signature: (self, threshold)
    class: LayerDoneCounter
  - name: reset
    signature: (self)
    class: LayerDoneCounter
  - name: __init__
    signature: (self, host_indices: torch.Tensor, device_indices: torch.Tensor, node_id: int, priority: Optional[int] = None)
    class: CacheOperation
  - name: merge
    signature: (self, other: 'CacheOperation')
    return: None
    class: CacheOperation
  - name: split
    signature: (self, factor)
    return: List['CacheOperation']
    class: CacheOperation
  - name: __lt__
    signature: (self, other: 'CacheOperation')
    class: CacheOperation
  - name: __init__
    signature: (self, stop_event, buffer_count: int = 3, max_buffer_size: int = 1024)
    return: None
    class: TransferBuffer
  - name: full
    signature: (self)
    return: bool
    class: TransferBuffer
  - name: empty
    signature: (self)
    return: bool
    class: TransferBuffer
  - name: put
    signature: (self, item, block = True, timeout = 1)
    return: None
    class: TransferBuffer
  - name: get
    signature: (self, block = True, timeout = 1)
    return: Optional[CacheOperation]
    class: TransferBuffer
  - name: clear
    signature: (self)
    class: TransferBuffer
  - name: __init__
    signature: (self, host_indices: torch.Tensor, token_ids: List[int], last_hash: Optional[str] = None, hash_value: Optional[List[str]] = None)
    class: StorageOperation
  - name: __lt__
    signature: (self, other: 'StorageOperation')
    class: StorageOperation
  - name: __init__
    signature: (self, request_id: str, host_indices: torch.Tensor, token_ids: List[int], last_hash: Optional[str] = None)
    class: PrefetchOperation
  - name: increment
    signature: (self, num_tokens: int)
    class: PrefetchOperation
  - name: mark_done
    signature: (self)
    class: PrefetchOperation
  - name: is_done
    signature: (self)
    return: bool
    class: PrefetchOperation
  - name: __init__
    signature: (self, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, mem_pool_host: HostKVCache, page_size: int, tp_group: torch.distributed.ProcessGroup, load_cache_event: threading.Event = None, write_policy: str = 'write_through_selective', io_backend: str = '', storage_backend: Optional[str] = None, prefetch_threshold: int = 256, model_name: Optional[str] = None, storage_backend_extra_config: Optional[str] = None)
    class: HiCacheController
  - name: _generate_storage_config
    signature: (self, model_name: Optional[str] = None, storage_backend_extra_config: Optional[str] = None)
    class: HiCacheController
  - name: reset
    signature: (self)
    class: HiCacheController
  - name: write
    signature: (self, device_indices: torch.Tensor, priority: Optional[int] = None, node_id: int = 0)
    return: Optional[torch.Tensor]
    class: HiCacheController
    doc: Back up KV caches from device memory to host memory.
  - name: load
    signature: (self, host_indices: torch.Tensor, priority: Optional[int] = None, node_id: int = 0)
    return: Optional[torch.Tensor]
    class: HiCacheController
    doc: Load KV caches from host memory to device memory.
  - name: move_indices
    signature: (self, host_indices, device_indices)
    class: HiCacheController
  - name: write_thread_func_direct
    signature: (self)
    class: HiCacheController
    doc: Directly write through KV caches to host memory without buffering.
  - name: load_thread_func_layer_by_layer
    signature: (self)
    class: HiCacheController
    doc: Load KV caches from host memory to device memory layer by layer.
  - name: evict_device
    signature: (self, device_indices: torch.Tensor, host_indices: torch.Tensor)
    return: int
    class: HiCacheController
  - name: evict_host
    signature: (self, host_indices: torch.Tensor, backup_only: bool = True)
    return: int
    class: HiCacheController
  - name: prefetch
    signature: (self, request_id: str, host_indices: torch.Tensor, new_input_tokens: List[int], last_hash: Optional[str] = None)
    return: PrefetchOperation
    class: HiCacheController
    doc: Prefetch KV caches from storage backend to host memory.
  - name: terminate_prefetch
    signature: (self, operation)
    class: HiCacheController
  - name: _3fs_zero_copy_page_get
    signature: (self, operation, hash_values, host_indices)
    class: HiCacheController
  - name: _mooncake_page_get
    signature: (self, operation, hash_values, host_indices)
    class: HiCacheController
  - name: _generic_page_get
    signature: (self, operation, hash_values, host_indices)
    class: HiCacheController
  - name: _page_transfer
    signature: (self, operation)
    class: HiCacheController
  - name: is_mooncake_backend
    signature: (self)
    class: HiCacheController
  - name: prefetch_io_aux_func
    signature: (self)
    class: HiCacheController
    doc: Auxiliary function conducting IO operations for prefetching.
  - name: prefetch_rate_limit_check
    signature: (self)
    return: bool
    class: HiCacheController
    doc: Rate limit the prefetching operations to avoid overwhelming the storage backend.
  - name: _generic_storage_hit_query
    signature: (self, operation)
    return: tuple[list[str], int]
    class: HiCacheController
  - name: prefetch_thread_func
    signature: (self)
    class: HiCacheController
    doc: Manage prefetching operations from storage backend to host memory.
  - name: write_storage
    signature: (self, host_indices: torch.Tensor, token_ids: List[int], hash_value: Optional[List[str]] = None)
    return: int
    class: HiCacheController
    doc: Write KV caches from host memory to storage backend.
  - name: _generic_page_set
    signature: (self, hash_values, host_indices)
    return: bool
    class: HiCacheController
  - name: _mooncake_page_set
    signature: (self, hash_values, host_indices)
    return: bool
    class: HiCacheController
  - name: _3fs_zero_copy_page_set
    signature: (self, hash_values, host_indices)
    return: bool
    class: HiCacheController
  - name: _page_backup
    signature: (self, operation)
    class: HiCacheController
  - name: backup_thread_func
    signature: (self)
    class: HiCacheController
    doc: Manage backup operations from host memory to storage backend.

File: managers/configure_logging.py
  (no function definitions found)
File: managers/data_parallel_controller.py
  - name: from_str
    signature: (cls, method: str)
    class: LoadBalanceMethod
  - name: __init__
    signature: (self, server_args: ServerArgs, port_args: PortArgs, dp_balance_meta: DPBalanceMeta)
    return: None
    class: DataParallelController
  - name: launch_dp_schedulers
    signature: (self, server_args, port_args)
    class: DataParallelController
  - name: launch_tensor_parallel_group_thread
    signature: (self, server_args: ServerArgs, port_args: PortArgs, base_gpu_id: int, dp_rank: int, ready_event: threading.Event)
    class: DataParallelController
  - name: launch_dp_attention_schedulers
    signature: (self, server_args, port_args)
    class: DataParallelController
  - name: launch_tensor_parallel_group
    signature: (self, server_args: ServerArgs, port_args: PortArgs, base_gpu_id: int, dp_rank: int)
    class: DataParallelController
  - name: round_robin_scheduler
    signature: (self, req: Req)
    class: DataParallelController
  - name: shortest_queue_scheduler
    signature: (self, input_requests)
    class: DataParallelController
  - name: minimum_tokens_scheduler
    signature: (self, req)
    class: DataParallelController
  - name: get_next_global_balance_id
    signature: ()
    return: int
    class: DataParallelController
  - name: event_loop
    signature: (self)
    class: DataParallelController
  - name: run_data_parallel_controller_process
    signature: (server_args: ServerArgs, port_args: PortArgs, pipe_writer)

File: managers/detokenizer_manager.py
  - name: __init__
    signature: (self, server_args: ServerArgs, port_args: PortArgs)
    class: DetokenizerManager
  - name: event_loop
    signature: (self)
    class: DetokenizerManager
    doc: The event loop that handles requests
  - name: trim_matched_stop
    signature: (self, output: Union[str, List[int]], finished_reason: Dict, no_stop_trim: bool)
    class: DetokenizerManager
  - name: handle_batch_embedding_out
    signature: (self, recv_obj: BatchEmbeddingOut)
    class: DetokenizerManager
  - name: handle_batch_token_id_out
    signature: (self, recv_obj: BatchTokenIDOut)
    class: DetokenizerManager
  - name: handle_multimodal_decode_req
    signature: (self, recv_obj: BatchMultimodalDecodeReq)
    class: DetokenizerManager
  - name: handle_freeze_gc_req
    signature: (self, recv_req: FreezeGCReq)
    class: DetokenizerManager
  - name: __init__
    signature: (self, capacity: int, *args, **kwargs)
    class: LimitedCapacityDict
  - name: __setitem__
    signature: (self, key, value)
    class: LimitedCapacityDict
  - name: run_detokenizer_process
    signature: (server_args: ServerArgs, port_args: PortArgs)

File: managers/io_struct.py
  - name: contains_mm_input
    signature: (self)
    return: bool
    class: GenerateReqInput
  - name: normalize_batch_and_arguments
    signature: (self)
    class: GenerateReqInput
    doc: Normalize the batch size and arguments for the request.
  - name: _validate_inputs
    signature: (self)
    class: GenerateReqInput
    doc: Validate that the input configuration is valid.
  - name: _determine_batch_size
    signature: (self)
    class: GenerateReqInput
    doc: Determine if this is a single example or a batch and the batch size.
  - name: _handle_parallel_sampling
    signature: (self)
    class: GenerateReqInput
    doc: Handle parallel sampling parameters and adjust batch size if needed.
  - name: _normalize_single_inputs
    signature: (self)
    class: GenerateReqInput
    doc: Normalize inputs for a single example.
  - name: _normalize_batch_inputs
    signature: (self)
    class: GenerateReqInput
    doc: Normalize inputs for a batch of examples, including parallel sampling expansion.
  - name: _expand_inputs
    signature: (self, num)
    class: GenerateReqInput
    doc: Expand the main inputs (text, input_ids, input_embeds) for parallel sampling.
  - name: _normalize_lora_paths
    signature: (self, num)
    class: GenerateReqInput
    doc: Normalize LoRA paths for batch processing.
  - name: _normalize_image_data
    signature: (self, num)
    class: GenerateReqInput
    doc: Normalize image data for batch processing.
  - name: _normalize_video_data
    signature: (self, num)
    class: GenerateReqInput
    doc: Normalize video data for batch processing.
  - name: _normalize_audio_data
    signature: (self, num)
    class: GenerateReqInput
    doc: Normalize audio data for batch processing.
  - name: _normalize_sampling_params
    signature: (self, num)
    class: GenerateReqInput
    doc: Normalize sampling parameters for batch processing.
  - name: _normalize_rid
    signature: (self, num)
    class: GenerateReqInput
    doc: Normalize request IDs for batch processing.
  - name: _normalize_logprob_params
    signature: (self, num)
    class: GenerateReqInput
    doc: Normalize logprob-related parameters for batch processing.
  - name: normalize_param
    signature: (param, default_value, param_name)
    class: GenerateReqInput
  - name: _normalize_custom_logit_processor
    signature: (self, num)
    class: GenerateReqInput
    doc: Normalize custom logit processor for batch processing.
  - name: _validate_session_params
    signature: (self)
    class: GenerateReqInput
    doc: Validate that session parameters are properly formatted.
  - name: regenerate_rid
    signature: (self)
    class: GenerateReqInput
    doc: Generate a new request ID and return it.
  - name: __getitem__
    signature: (self, i)
    class: GenerateReqInput
  - name: __len__
    signature: (self)
    class: BatchTokenizedGenerateReqInput
  - name: __getitem__
    signature: (self, i)
    class: BatchTokenizedGenerateReqInput
  - name: __iter__
    signature: (self)
    class: BatchTokenizedGenerateReqInput
  - name: normalize_batch_and_arguments
    signature: (self)
    class: EmbeddingReqInput
  - name: regenerate_rid
    signature: (self)
    class: EmbeddingReqInput
  - name: contains_mm_input
    signature: (self)
    return: bool
    class: EmbeddingReqInput
  - name: __getitem__
    signature: (self, i)
    class: EmbeddingReqInput
  - name: __len__
    signature: (self)
    class: BatchTokenizedEmbeddingReqInput
  - name: __getitem__
    signature: (self, i)
    class: BatchTokenizedEmbeddingReqInput
  - name: __iter__
    signature: (self)
    class: BatchTokenizedEmbeddingReqInput
  - name: to_ref
    signature: (self)
    return: LoRARef
    class: LoadLoRAAdapterReqInput
  - name: to_ref
    signature: (self)
    return: LoRARef
    class: UnloadLoRAAdapterReqInput

File: managers/mm_utils.py
  - name: __new__
    signature: (cls, data: torch.Tensor, name: Optional[str] = None, fields: Optional[Dict[str, Any]] = None, transport_mode: TensorTransportMode = 'default', *args, **kwargs)
    class: TransportProxyTensor
  - name: __getstate__
    signature: (self)
    class: TransportProxyTensor
    doc: Called during pickling. Implements the serialization logic.
  - name: __setstate__
    signature: (self, state: Dict[str, Any])
    class: TransportProxyTensor
    doc: Called during unpickling. Implements the deserialization logic.
  - name: name
    signature: (self)
    return: Optional[str]
    class: TransportProxyTensor
  - name: fields
    signature: (self)
    return: Dict[str, Any]
    class: TransportProxyTensor
  - name: transport_mode
    signature: (self)
    return: TensorTransportMode
    class: TransportProxyTensor
  - name: pad_input_tokens
    signature: (self, input_ids: List[int], mm_inputs: MultimodalInputs)
    return: List[int]
    class: MultiModalityDataPaddingPattern
    doc: Pad the input ids sequence containing data tokens, and replace them with pad_values
  - name: __init__
    signature: (self, data_token_pairs: Optional[List[Tuple[int, int]]], data_start_token_ids: Optional[List[int]] = None)
    return: None
    class: MultiModalityDataPaddingPatternTokenPairs
    doc: Args:
  - name: pad_input_tokens
    signature: (self, input_ids: List[int], mm_inputs: MultimodalInputs)
    return: List[int]
    class: MultiModalityDataPaddingPatternTokenPairs
    doc: This function will replace the data-tokens in between with pad_values accordingly
  - name: pad_input_tokens
    signature: (self, input_ids: List[int], mm_inputs: MultimodalInputs)
    return: List[int]
    class: MultiModalityDataPaddingPatternMultimodalTokens
    doc: Replaces multimodal tokens in input_ids with corresponding pad_values from mm_items.
  - name: init_embedding_cache
    signature: (max_size: int = 0)
  - name: get_embedding_hash
    signature: (embedding_items: List[MultimodalDataItem])
    return: int
  - name: get_embedding_chunk
    signature: (embedding: torch.Tensor, extend_prefix_len: int, extend_seq_len: int, items_offset: List[Tuple[int, int]])
    return: Tuple[torch.Tensor, int, int]
    doc: Extract a chunk of embeddings based on the specified prefix length, sequence length, and offset ranges.
  - name: _get_precomputed_embedding
    signature: (items: List[MultimodalDataItem])
    return: Optional[torch.Tensor]
    doc: If all items have precomputed_embeddings, return their concatenation.
  - name: _get_chunked_prefill_embedding
    signature: (data_embedding_func: Callable[[List[MultimodalDataItem]], torch.Tensor], embedding_items: List[MultimodalDataItem], items_size: List[int], prefix_length: List[int], extend_length: List[int], items_offset_list: List[List[Tuple[int, int]]])
    return: Optional[torch.Tensor]
  - name: _get_multimodal_mask
    signature: (input_ids: torch.Tensor, placeholder_tensor: torch.Tensor)
    return: torch.Tensor
  - name: _adjust_embedding_length
    signature: (embedding: torch.Tensor, mask: torch.Tensor, logger)
    return: torch.Tensor
  - name: get_embedding_and_mask
    signature: (data_embedding_func: Callable[[List[MultimodalDataItem]], torch.Tensor], embedding_items: List[MultimodalDataItem], placeholder_tensor: torch.Tensor, input_ids: torch.Tensor, items_size: List[int], prefix_length: List[int], extend_length: List[int], items_offset_list: List[List[Tuple[int, int]]])
    return: Tuple[torch.Tensor, torch.Tensor]
    doc: Generate multimodal embeddings and create a mask for identifying their positions in the input sequence.
  - name: embed_mm_inputs
    signature: (mm_inputs_list: List[MultimodalInputs], extend_prefix_lens: List[int], extend_seq_lens: List[int], input_ids: torch.Tensor, input_embedding: nn.Embedding, multimodal_model: nn.Module = None, data_embedding_func_mapping: Dict[Modality, Callable[[List[MultimodalDataItem]], torch.Tensor]] = None, placeholder_tokens: dict[Modality, List[int]] = None)
    return: Optional[torch.Tensor]
    doc: Embed multimodal inputs and integrate them with text token embeddings.
  - name: general_mm_embed_routine
    signature: (input_ids: torch.Tensor, forward_batch: ForwardBatch, language_model: nn.Module, multimodal_model: Optional[nn.Module] = None, data_embedding_funcs: Dict[Modality, Callable[[List[MultimodalDataItem]], torch.Tensor]] = None, placeholder_tokens: Optional[dict[Modality, List[int]]] = None, **kwargs)
    return: torch.Tensor
    doc: Process multimodal inputs and forward through language model.
  - name: get_multimodal_data_bounds
    signature: (input_ids: torch.Tensor, pad_values: List[int], token_pairs: List[Tuple[int, int]])
    return: torch.Tensor
    doc: Returns a tensor indicating the bounds of multimodal data (images, video, audio, etc.)
  - name: data_hash
    signature: (data)
    return: int
  - name: tensor_hash
    signature: (tensor_list)
    return: int
    doc: hash a tensor or a tensor list
  - name: hash_feature
    signature: (f)

File: managers/multimodal_processor.py
  - name: import_processors
    signature: ()
  - name: get_mm_processor
    signature: (hf_config, server_args: ServerArgs, processor, transport_mode)
    return: BaseMultimodalProcessor

File: managers/schedule_batch.py
  - name: __init__
    signature: (self, is_error: bool = False)
    class: BaseFinishReason
  - name: to_json
    signature: (self)
    class: BaseFinishReason
  - name: __init__
    signature: (self, matched: Union[int, List[int]])
    class: FINISH_MATCHED_TOKEN
  - name: to_json
    signature: (self)
    class: FINISH_MATCHED_TOKEN
  - name: __init__
    signature: (self, matched: str)
    class: FINISH_MATCHED_STR
  - name: to_json
    signature: (self)
    class: FINISH_MATCHED_STR
  - name: __init__
    signature: (self, length: int)
    class: FINISH_LENGTH
  - name: to_json
    signature: (self)
    class: FINISH_LENGTH
  - name: __init__
    signature: (self, message = None, status_code = None, err_type = None)
    class: FINISH_ABORT
  - name: to_json
    signature: (self)
    class: FINISH_ABORT
  - name: from_str
    signature: (modality_str: str)
    class: Modality
  - name: all
    signature: ()
    class: Modality
  - name: __getattr__
    signature: (self, name: str)
    class: MultimodalDataItem
  - name: __setitem__
    signature: (self, key: str, value: Any)
    class: MultimodalDataItem
  - name: set
    signature: (self, key: str, value: Any)
    class: MultimodalDataItem
  - name: is_empty_list
    signature: (l)
    class: MultimodalDataItem
  - name: set_pad_value
    signature: (self)
    class: MultimodalDataItem
    doc: Set the pad value after first hashing the data
  - name: is_modality
    signature: (self, modality: Modality)
    return: bool
    class: MultimodalDataItem
  - name: is_audio
    signature: (self)
    class: MultimodalDataItem
  - name: is_image
    signature: (self)
    class: MultimodalDataItem
  - name: is_video
    signature: (self)
    class: MultimodalDataItem
  - name: is_valid
    signature: (self)
    return: bool
    class: MultimodalDataItem
  - name: validate
    signature: (self)
    class: MultimodalDataItem
  - name: from_dict
    signature: (obj: dict)
    class: MultimodalDataItem
  - name: merge
    signature: (self, other)
    class: MultimodalDataItem
  - name: from_dict
    signature: (obj: dict)
    class: MultimodalInputs
  - name: contains_image_inputs
    signature: (self)
    return: bool
    class: MultimodalInputs
  - name: contains_video_inputs
    signature: (self)
    return: bool
    class: MultimodalInputs
  - name: contains_audio_inputs
    signature: (self)
    return: bool
    class: MultimodalInputs
  - name: contains_mm_input
    signature: (self)
    return: bool
    class: MultimodalInputs
  - name: merge
    signature: (self, other: MultimodalInputs)
    class: MultimodalInputs
    doc: merge image inputs when requests are being merged
  - name: __init__
    signature: (self, rid: str, origin_input_text: str, origin_input_ids: List[int], sampling_params: SamplingParams, return_logprob: bool = False, top_logprobs_num: int = 0, token_ids_logprob: List[int] = None, stream: bool = False, origin_input_ids_unpadded: Optional[Tuple[int]] = None, lora_id: Optional[str] = None, input_embeds: Optional[List[List[float]]] = None, token_type_ids: List[int] = None, session_id: Optional[str] = None, custom_logit_processor: Optional[str] = None, return_hidden_states: bool = False, eos_token_ids: Optional[Set[int]] = None, bootstrap_host: Optional[str] = None, bootstrap_port: Optional[int] = None, bootstrap_room: Optional[int] = None, data_parallel_rank: Optional[int] = None, vocab_size: Optional[int] = None)
    class: Req
  - name: seqlen
    signature: (self)
    class: Req
  - name: extend_image_inputs
    signature: (self, image_inputs)
    class: Req
  - name: finished
    signature: (self)
    return: bool
    class: Req
  - name: init_next_round_input
    signature: (self, tree_cache: Optional[BasePrefixCache] = None)
    class: Req
  - name: adjust_max_prefix_ids
    signature: (self)
    class: Req
  - name: init_incremental_detokenize
    signature: (self)
    class: Req
  - name: check_finished
    signature: (self)
    class: Req
  - name: reset_for_retract
    signature: (self)
    class: Req
  - name: offload_kv_cache
    signature: (self, req_to_token_pool, token_to_kv_pool_allocator)
    class: Req
  - name: load_kv_cache
    signature: (self, req_to_token_pool, token_to_kv_pool_allocator)
    class: Req
  - name: log_time_stats
    signature: (self)
    class: Req
  - name: set_finish_with_abort
    signature: (self, error_msg: str)
    class: Req
  - name: __repr__
    signature: (self)
    class: Req
  - name: init_new
    signature: (cls, reqs: List[Req], req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, tree_cache: BasePrefixCache, model_config: ModelConfig, enable_overlap: bool, spec_algorithm: SpeculativeAlgorithm, chunked_req: Optional[Req] = None)
    class: ScheduleBatch
  - name: batch_size
    signature: (self)
    class: ScheduleBatch
  - name: is_empty
    signature: (self)
    class: ScheduleBatch
  - name: alloc_req_slots
    signature: (self, num_reqs: int)
    class: ScheduleBatch
  - name: alloc_token_slots
    signature: (self, num_tokens: int, backup_state: bool = False)
    class: ScheduleBatch
  - name: alloc_paged_token_slots_extend
    signature: (self, prefix_lens: torch.Tensor, seq_lens: torch.Tensor, last_loc: torch.Tensor, extend_num_tokens: int, backup_state: bool = False)
    class: ScheduleBatch
  - name: alloc_paged_token_slots_decode
    signature: (self, seq_lens: torch.Tensor, last_loc: torch.Tensor, backup_state: bool = False)
    class: ScheduleBatch
  - name: prepare_encoder_info_extend
    signature: (self, input_ids: List[int], seq_lens: List[int])
    class: ScheduleBatch
  - name: prepare_for_extend
    signature: (self)
    class: ScheduleBatch
  - name: prepare_for_split_prefill
    signature: (self)
    class: ScheduleBatch
  - name: mix_with_running
    signature: (self, running_batch: 'ScheduleBatch')
    class: ScheduleBatch
  - name: new_page_count_next_decode
    signature: (self)
    class: ScheduleBatch
  - name: check_decode_mem
    signature: (self, buf_multiplier = 1)
    class: ScheduleBatch
  - name: retract_decode
    signature: (self, server_args: ServerArgs)
    class: ScheduleBatch
    doc: Retract the decoding requests when there is not enough memory.
  - name: get_required_tokens
    signature: (num_reqs: int)
    class: ScheduleBatch
  - name: _get_available_size
    signature: ()
    class: ScheduleBatch
  - name: prepare_encoder_info_decode
    signature: (self)
    class: ScheduleBatch
  - name: prepare_for_idle
    signature: (self)
    class: ScheduleBatch
  - name: prepare_for_decode
    signature: (self)
    class: ScheduleBatch
  - name: filter_batch
    signature: (self, chunked_req_to_exclude: Optional[Union[Req, List[Req]]] = None, keep_indices: Optional[List[int]] = None)
    class: ScheduleBatch
  - name: merge_batch
    signature: (self, other: 'ScheduleBatch')
    class: ScheduleBatch
  - name: get_model_worker_batch
    signature: (self, seq_lens_cpu_cache: Optional[torch.Tensor] = None)
    return: ModelWorkerBatch
    class: ScheduleBatch
  - name: copy
    signature: (self)
    class: ScheduleBatch
  - name: _evict_tree_cache_if_needed
    signature: (self, num_tokens: int)
    class: ScheduleBatch
  - name: _is_available_size_sufficient
    signature: (self, num_tokens: int)
    return: bool
    class: ScheduleBatch
  - name: _available_and_evictable_str
    signature: (self)
    return: str
    class: ScheduleBatch
  - name: __str__
    signature: (self)
    class: ScheduleBatch
  - name: write_req_to_token_pool_triton
    signature: (req_to_token_ptr, req_pool_indices, pre_lens, seq_lens, extend_lens, out_cache_loc, req_to_token_ptr_stride: tl.constexpr)
  - name: get_last_loc
    signature: (req_to_token: torch.Tensor, req_pool_indices_tensor: torch.Tensor, prefix_lens_tensor: torch.Tensor)
    return: torch.Tensor
  - name: get_last_loc_torch
    signature: (req_to_token: torch.Tensor, req_pool_indices_tensor: torch.Tensor, prefix_lens_tensor: torch.Tensor)
    return: torch.Tensor
  - name: get_last_loc_kernel
    signature: (req_to_token, req_pool_indices_tensor, prefix_lens_tensor, result, num_tokens, req_to_token_stride, BLOCK_SIZE: tl.constexpr)
  - name: get_last_loc_triton
    signature: (req_to_token: torch.Tensor, req_pool_indices_tensor: torch.Tensor, prefix_lens_tensor: torch.Tensor)
    return: torch.Tensor

File: managers/schedule_policy.py
  - name: __init__
    signature: (self, policy: str, tree_cache: BasePrefixCache, enable_hierarchical_cache: bool)
    class: SchedulePolicy
  - name: calc_priority
    signature: (self, waiting_queue: List[Req])
    return: bool
    class: SchedulePolicy
  - name: _determine_active_policy
    signature: (self, waiting_queue: List[Req])
    return: Policy
    class: SchedulePolicy
  - name: _validate_and_adjust_policy
    signature: (self, policy: str, tree_cache: BasePrefixCache)
    return: Policy
    class: SchedulePolicy
    doc: Validates the policy and adjusts it if necessary based on tree cache settings.
  - name: _compute_prefix_matches
    signature: (self, waiting_queue: List[Req], policy: CacheAwarePolicy)
    return: Set[int]
    class: SchedulePolicy
    doc: Computes and caches the matching prefixes for requests in the waiting queue,
  - name: _sort_by_longest_prefix
    signature: (waiting_queue: List[Req], temporary_deprioritized: Set[int])
    return: None
    class: SchedulePolicy
    doc: Sorts the waiting queue based on the longest prefix match.
  - name: _sort_by_dfs_weight
    signature: (waiting_queue: List[Req], tree_cache: BasePrefixCache)
    return: None
    class: SchedulePolicy
    doc: Sorts the waiting queue based on a depth-first search weighting.
  - name: _sort_by_longest_output
    signature: (waiting_queue: List[Req])
    return: None
    class: SchedulePolicy
    doc: Sorts the waiting queue based on the longest output (max_new_tokens).
  - name: _sort_randomly
    signature: (waiting_queue: List[Req])
    return: None
    class: SchedulePolicy
    doc: Shuffles the waiting queue randomly.
  - name: _calc_weight
    signature: (cur_node: TreeNode, node_to_weight: Dict[TreeNode, int])
    return: None
    class: SchedulePolicy
  - name: _get_dfs_priority
    signature: (cur_node: TreeNode, node_to_priority: Dict[TreeNode, int], last_node_to_reqs: Dict[TreeNode, List[Req]], q: List)
    return: None
    class: SchedulePolicy
  - name: __init__
    signature: (self, page_size: int, tree_cache: BasePrefixCache, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, running_batch: ScheduleBatch, new_token_ratio: float, rem_input_tokens: int, rem_chunk_tokens: Optional[int], mixed_with_decode_tokens: int = 0)
    class: PrefillAdder
  - name: rem_total_tokens
    signature: (self)
    class: PrefillAdder
  - name: cur_rem_tokens
    signature: (self)
    class: PrefillAdder
  - name: ceil_paged_tokens
    signature: (self, tokens: int)
    return: int
    class: PrefillAdder
  - name: budget_state
    signature: (self)
    class: PrefillAdder
  - name: _update_prefill_budget
    signature: (self, prefix_len: int, extend_input_len: int, max_new_tokens: int)
    class: PrefillAdder
  - name: add_chunked_req
    signature: (self, req: Req)
    class: PrefillAdder
  - name: _lock_node
    signature: (self, last_node: TreeNode)
    class: PrefillAdder
  - name: add_one_req_ignore_eos
    signature: (self, req: Req, has_chunked_req: bool)
    class: PrefillAdder
  - name: add_req_state
    signature: (r, insert_sort = False)
    class: PrefillAdder
  - name: add_one_req
    signature: (self, req: Req, has_chunked_req: bool)
    class: PrefillAdder

File: managers/scheduler.py
  - name: __init__
    signature: (self, server_args: ServerArgs, port_args: PortArgs, gpu_id: int, tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int], dp_balance_meta: Optional[DPBalanceMeta] = None)
    class: Scheduler
  - name: init_tokenizer
    signature: (self)
    class: Scheduler
  - name: init_memory_pool_and_cache
    signature: (self)
    class: Scheduler
  - name: init_disaggregation
    signature: (self)
    class: Scheduler
  - name: init_moe_config
    signature: (self)
    class: Scheduler
  - name: event_loop_normal
    signature: (self)
    class: Scheduler
    doc: A normal scheduler loop.
  - name: event_loop_overlap
    signature: (self)
    class: Scheduler
    doc: A scheduler loop that overlaps the CPU processing and GPU computation.
  - name: event_loop_pp
    signature: (self)
    class: Scheduler
    doc: A non-overlap scheduler loop for pipeline parallelism.
  - name: recv_requests
    signature: (self)
    return: List[Req]
    class: Scheduler
    doc: Receive results at tp_rank = 0 and broadcast it to all other TP ranks.
  - name: process_input_requests
    signature: (self, recv_reqs: List)
    class: Scheduler
  - name: handle_generate_request
    signature: (self, recv_req: TokenizedGenerateReqInput)
    class: Scheduler
  - name: handle_batch_generate_request
    signature: (self, recv_req: BatchTokenizedGenerateReqInput)
    class: Scheduler
    doc: Handle optimized batch generate request.
  - name: _add_request_to_queue
    signature: (self, req: Req)
    class: Scheduler
  - name: _prefetch_kvcache
    signature: (self, req: Req)
    class: Scheduler
  - name: _extend_requests_to_queue
    signature: (self, reqs: List[Req], is_retracted: bool = False)
    class: Scheduler
  - name: handle_embedding_request
    signature: (self, recv_req: TokenizedEmbeddingReqInput)
    class: Scheduler
  - name: handle_batch_embedding_request
    signature: (self, recv_req: BatchTokenizedEmbeddingReqInput)
    class: Scheduler
    doc: Handle optimized batch embedding request.
  - name: self_check_during_idle
    signature: (self)
    class: Scheduler
  - name: check_memory
    signature: (self)
    class: Scheduler
  - name: check_tree_cache
    signature: (self)
    class: Scheduler
  - name: _get_token_info
    signature: (self)
    class: Scheduler
  - name: _get_swa_token_info
    signature: (self)
    class: Scheduler
  - name: get_next_batch_to_run
    signature: (self)
    return: Optional[ScheduleBatch]
    class: Scheduler
  - name: get_num_allocatable_reqs
    signature: (self, running_bs)
    class: Scheduler
  - name: get_new_batch_prefill
    signature: (self)
    return: Optional[ScheduleBatch]
    class: Scheduler
  - name: update_running_batch
    signature: (self, batch: ScheduleBatch)
    return: Optional[ScheduleBatch]
    class: Scheduler
    doc: Update the current running decoding batch.
  - name: run_batch
    signature: (self, batch: ScheduleBatch)
    return: Union[GenerationBatchResult, EmbeddingBatchResult]
    class: Scheduler
    doc: Run a batch.
  - name: process_batch_result
    signature: (self, batch: ScheduleBatch, result: Union[GenerationBatchResult, EmbeddingBatchResult], launch_done: Optional[threading.Event] = None)
    class: Scheduler
  - name: maybe_send_health_check_signal
    signature: (self)
    class: Scheduler
  - name: prepare_mlp_sync_batch
    signature: (self, local_batch: ScheduleBatch)
    class: Scheduler
  - name: handle_dp_balance_data
    signature: (self, local_batch: ScheduleBatch)
    class: Scheduler
  - name: gather_dp_balance_info
    signature: (holding_tokens_list)
    return: Union[None, List[List[int]]]
    class: Scheduler
    doc: gather recv_dp_balance_id_this_term and holding tokens per worker for dp balance
  - name: write_shared_dp_balance_info
    signature: (new_recv_rid_lists, local_tokens)
    class: Scheduler
  - name: prepare_mlp_sync_batch_raw
    signature: (local_batch: ScheduleBatch, dp_size, attn_tp_size: int, tp_group, get_idle_batch, disable_cuda_graph: bool, spec_algorithm, speculative_num_draft_tokens, require_mlp_tp_gather: bool, disable_overlap_schedule: bool)
    class: Scheduler
  - name: get_idle_batch
    signature: (self)
    class: Scheduler
  - name: move_ready_grammar_requests
    signature: (self)
    class: Scheduler
    doc: Move requests whose grammar objects are ready from grammar_queue to waiting_queue.
  - name: set_next_batch_sampling_info_done
    signature: (self, batch: ScheduleBatch)
    class: Scheduler
  - name: watchdog_thread
    signature: (self)
    class: Scheduler
    doc: A watch dog thread that will try to kill the server itself if one forward batch takes too long.
  - name: flush_cache_wrapped
    signature: (self, recv_req: FlushCacheReqInput)
    class: Scheduler
  - name: clear_hicache_storage_wrapped
    signature: (self, recv_req: ClearHiCacheReqInput)
    class: Scheduler
  - name: flush_cache
    signature: (self)
    class: Scheduler
    doc: Flush the memory pool and cache.
  - name: get_load
    signature: (self)
    class: Scheduler
  - name: get_internal_state
    signature: (self, recv_req: GetInternalStateReq)
    class: Scheduler
  - name: set_internal_state
    signature: (self, recv_req: SetInternalStateReq)
    class: Scheduler
  - name: handle_rpc_request
    signature: (self, recv_req: RpcReqInput)
    class: Scheduler
  - name: abort_request
    signature: (self, recv_req: AbortReq)
    class: Scheduler
  - name: _pause_engine
    signature: (self)
    return: Tuple[List[Req], int]
    class: Scheduler
  - name: load_lora_adapter
    signature: (self, recv_req: LoadLoRAAdapterReqInput)
    return: LoadLoRAAdapterReqOutput
    class: Scheduler
    doc: In-place loading a new lora adapter from disk or huggingface.
  - name: unload_lora_adapter
    signature: (self, recv_req: UnloadLoRAAdapterReqInput)
    return: UnloadLoRAAdapterReqOutput
    class: Scheduler
    doc: Unload the lora adapter.
  - name: slow_down
    signature: (self, recv_req: SlowDownReqInput)
    class: Scheduler
  - name: expert_distribution_handle
    signature: (self, recv_req: ExpertDistributionReq)
    class: Scheduler
  - name: open_session
    signature: (self, recv_req: OpenSessionReqInput)
    class: Scheduler
  - name: close_session
    signature: (self, recv_req: CloseSessionReqInput)
    class: Scheduler
  - name: get_print_prefix
    signature: (self)
    class: Scheduler
  - name: current_scheduler_metrics_enabled
    signature: (self)
    class: Scheduler
  - name: maybe_sleep_on_idle
    signature: (self)
    class: Scheduler
  - name: handle_freeze_gc
    signature: (self, recv_req: FreezeGCReq)
    class: Scheduler
    doc: Handle freeze_gc request: freeze scheduler's GC and forward to detokenizer.
  - name: __init__
    signature: (self, sockets)
    class: IdleSleeper
  - name: maybe_sleep
    signature: (self)
    class: IdleSleeper
  - name: is_health_check_generate_req
    signature: (recv_req)
  - name: is_work_request
    signature: (recv_req)
  - name: run_scheduler_process
    signature: (server_args: ServerArgs, port_args: PortArgs, gpu_id: int, tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int], pipe_writer, balance_meta: Optional[DPBalanceMeta] = None)

File: managers/scheduler_input_blocker.py
  - name: __init__
    signature: (self, noop: bool)
    class: SchedulerInputBlocker
  - name: handle
    signature: (self, recv_reqs: Optional[List[Any]])
    class: SchedulerInputBlocker
  - name: _handle_recv_req
    signature: (self, recv_req)
    class: SchedulerInputBlocker
  - name: _execute_block_req
    signature: (self)
    class: SchedulerInputBlocker
  - name: _execute_unblock_req
    signature: (self)
    class: SchedulerInputBlocker
  - name: _handle_arrive_unblock_barrier
    signature: (self)
    class: SchedulerInputBlocker
  - name: _change_state
    signature: (self, original: '_State', target: '_State')
    class: SchedulerInputBlocker
  - name: input_blocker_guard_region
    signature: (send_to_scheduler)

File: managers/scheduler_metrics_mixin.py
  - name: __init__
    signature: (self)
    class: KvMetrics
  - name: init_metrics
    signature: (self, tp_rank: int, pp_rank: int, dp_rank: Optional[int])
    class: SchedulerMetricsMixin
  - name: init_kv_events
    signature: (self, kv_events_config: Optional[str])
    class: SchedulerMetricsMixin
  - name: log_prefill_stats
    signature: (self, adder: PrefillAdder, can_run_list: List[Req], running_bs: int)
    class: SchedulerMetricsMixin
  - name: log_decode_stats
    signature: (self, can_run_cuda_graph: bool, running_batch: ScheduleBatch = None)
    class: SchedulerMetricsMixin
  - name: _emit_kv_metrics
    signature: (self)
    class: SchedulerMetricsMixin
  - name: _publish_kv_events
    signature: (self)
    class: SchedulerMetricsMixin

File: managers/scheduler_output_processor_mixin.py
  - name: process_batch_result_prefill
    signature: (self: Scheduler, batch: ScheduleBatch, result: Union[GenerationBatchResult, EmbeddingBatchResult], launch_done: Optional[threading.Event] = None)
    class: SchedulerOutputProcessorMixin
  - name: process_batch_result_decode
    signature: (self: Scheduler, batch: ScheduleBatch, result: GenerationBatchResult, launch_done: Optional[threading.Event] = None)
    class: SchedulerOutputProcessorMixin
  - name: add_input_logprob_return_values
    signature: (self: Scheduler, i: int, req: Req, output: LogitsProcessorOutput, logprob_pt: int, num_input_logprobs: int, last_prefill_chunk: bool)
    class: SchedulerOutputProcessorMixin
    doc: Incrementally add input logprobs to `req`.
  - name: add_logprob_return_values
    signature: (self: Scheduler, i: int, req: Req, pt: int, next_token_ids: List[int], num_input_logprobs: int, output: LogitsProcessorOutput)
    class: SchedulerOutputProcessorMixin
    doc: Attach logprobs to the return values.
  - name: stream_output
    signature: (self: Scheduler, reqs: List[Req], return_logprob: bool, skip_req: Optional[Req] = None)
    class: SchedulerOutputProcessorMixin
    doc: Stream the output to detokenizer.
  - name: stream_output_generation
    signature: (self: Scheduler, reqs: List[Req], return_logprob: bool, skip_req: Optional[Req] = None)
    class: SchedulerOutputProcessorMixin
  - name: stream_output_embedding
    signature: (self: Scheduler, reqs: List[Req])
    class: SchedulerOutputProcessorMixin

File: managers/scheduler_profiler_mixin.py
  - name: init_profier
    signature: (self)
    class: SchedulerProfilerMixin
  - name: init_profile
    signature: (self, output_dir: Optional[str], start_step: Optional[int], num_steps: Optional[int], activities: Optional[List[str]], with_stack: Optional[bool], record_shapes: Optional[bool], profile_by_stage: bool, profile_id: str)
    return: ProfileReqOutput
    class: SchedulerProfilerMixin
  - name: start_profile
    signature: (self, stage: Optional[ForwardMode] = None)
    return: ProfileReqOutput | None
    class: SchedulerProfilerMixin
  - name: stop_profile
    signature: (self, stage: Optional[ForwardMode] = None)
    return: ProfileReqOutput | None
    class: SchedulerProfilerMixin
  - name: _profile_batch_predicate
    signature: (self, batch)
    class: SchedulerProfilerMixin
  - name: profile
    signature: (self, recv_req: ProfileReq)
    class: SchedulerProfilerMixin

File: managers/scheduler_recv_skipper.py
  - name: maybe_create
    signature: (server_args: ServerArgs)
    class: SchedulerRecvSkipper
  - name: __init__
    signature: (self, server_args: ServerArgs)
    class: SchedulerRecvSkipper
  - name: handle
    signature: (self, last_forward_mode: ForwardMode)
    class: SchedulerRecvSkipper

File: managers/scheduler_update_weights_mixin.py
  - name: update_weights_from_disk
    signature: (self, recv_req: UpdateWeightFromDiskReqInput)
    class: SchedulerUpdateWeightsMixin
    doc: In-place update of the weights from disk.
  - name: init_weights_update_group
    signature: (self, recv_req: InitWeightsUpdateGroupReqInput)
    class: SchedulerUpdateWeightsMixin
    doc: Initialize the online model parameter update group.
  - name: update_weights_from_distributed
    signature: (self, recv_req: UpdateWeightsFromDistributedReqInput)
    return: Tuple[bool, str]
    class: SchedulerUpdateWeightsMixin
    doc: Update the online model parameter.
  - name: update_weights_from_tensor
    signature: (self, recv_req: UpdateWeightsFromTensorReqInput)
    class: SchedulerUpdateWeightsMixin
    doc: Update the online model parameter from tensors.
  - name: get_weights_by_name
    signature: (self, recv_req: GetWeightsByNameReqInput)
    class: SchedulerUpdateWeightsMixin
  - name: release_memory_occupation
    signature: (self, recv_req: ReleaseMemoryOccupationReqInput)
    class: SchedulerUpdateWeightsMixin
  - name: resume_memory_occupation
    signature: (self, recv_req: ResumeMemoryOccupationReqInput)
    class: SchedulerUpdateWeightsMixin
  - name: save_remote_model
    signature: (self, params)
    class: SchedulerUpdateWeightsMixin
  - name: save_sharded_model
    signature: (self, params)
    class: SchedulerUpdateWeightsMixin
  - name: _export_static_state
    signature: (model)
  - name: _import_static_state
    signature: (model, static_params)

File: managers/session_controller.py
  - name: __init__
    signature: (self, req, parent = None, childs = None)
    class: SessionReqNode
  - name: clear_childs
    signature: (self, req_dict)
    class: SessionReqNode
  - name: clear
    signature: (self, req_dict)
    class: SessionReqNode
  - name: abort
    signature: (self)
    class: SessionReqNode
  - name: __str__
    signature: (self)
    class: SessionReqNode
  - name: _str_helper
    signature: (self, prefix = '')
    class: SessionReqNode
  - name: __init__
    signature: (self, capacity_of_str_len: int, session_id: Optional[str] = None)
    class: Session
  - name: create_req
    signature: (self, req: TokenizedGenerateReqInput, tokenizer)
    class: Session

File: managers/template_manager.py
  - name: __init__
    signature: (self)
    class: TemplateManager
  - name: chat_template_name
    signature: (self)
    return: Optional[str]
    class: TemplateManager
    doc: Get the current chat template name.
  - name: completion_template_name
    signature: (self)
    return: Optional[str]
    class: TemplateManager
    doc: Get the current completion template name.
  - name: jinja_template_content_format
    signature: (self)
    return: Optional[str]
    class: TemplateManager
    doc: Get the detected template content format ('string' or 'openai' or None).
  - name: force_reasoning
    signature: (self)
    return: bool
    class: TemplateManager
    doc: Check if the current chat template enforces reasoning/thinking.
  - name: _detect_reasoning_pattern
    signature: (self, template: str)
    return: bool
    class: TemplateManager
    doc: Detect if the chat template contains reasoning/thinking patterns.
  - name: load_chat_template
    signature: (self, tokenizer_manager, chat_template_arg: Optional[str], model_path: str)
    return: None
    class: TemplateManager
    doc: Load a chat template from various sources.
  - name: _load_explicit_chat_template
    signature: (self, tokenizer_manager, chat_template_arg: str)
    return: None
    class: TemplateManager
    doc: Load explicitly specified chat template.
  - name: guess_chat_template_from_model_path
    signature: (self, model_path: str)
    return: None
    class: TemplateManager
    doc: Infer chat template name from model path.
  - name: load_completion_template
    signature: (self, completion_template_arg: str)
    return: None
    class: TemplateManager
    doc: Load completion template for code completion.
  - name: initialize_templates
    signature: (self, tokenizer_manager, model_path: str, chat_template: Optional[str] = None, completion_template: Optional[str] = None)
    return: None
    class: TemplateManager
    doc: Initialize all templates based on provided configuration.
  - name: _load_jinja_template
    signature: (self, tokenizer_manager, template_path: str)
    return: None
    class: TemplateManager
    doc: Load a Jinja template file.
  - name: _load_json_chat_template
    signature: (self, template_path: str)
    return: None
    class: TemplateManager
    doc: Load a JSON chat template file.
  - name: _load_json_completion_template
    signature: (self, template_path: str)
    return: None
    class: TemplateManager
    doc: Load a JSON completion template file.
  - name: _resolve_hf_chat_template
    signature: (self, tokenizer_manager)
    return: Optional[str]
    class: TemplateManager
    doc: Resolve HuggingFace chat template.

File: managers/tokenizer_manager.py
  - name: __init__
    signature: (self, server_args: ServerArgs, port_args: PortArgs)
    class: TokenizerManager
  - name: generate_request
    signature: (self, obj: Union[GenerateReqInput, EmbeddingReqInput], request: Optional[fastapi.Request] = None)
    class: TokenizerManager
  - name: _tokenize_one_request
    signature: (self, obj: Union[GenerateReqInput, EmbeddingReqInput])
    class: TokenizerManager
    doc: Tokenize one request.
  - name: _validate_one_request
    signature: (self, obj: Union[GenerateReqInput, EmbeddingReqInput], input_ids: List[int])
    return: None
    class: TokenizerManager
    doc: Validates that the input token count and the requested token count doesn't exceed the model's context length.
  - name: _validate_input_ids_in_vocab
    signature: (self, input_ids: List[int], vocab_size: int)
    return: None
    class: TokenizerManager
  - name: _create_tokenized_object
    signature: (self, obj: Union[GenerateReqInput, EmbeddingReqInput], input_text: str, input_ids: List[int], input_embeds: Optional[Union[List[float], None]] = None, mm_inputs: Optional[Dict] = None, token_type_ids: Optional[List[int]] = None)
    return: Union[TokenizedGenerateReqInput, TokenizedEmbeddingReqInput]
    class: TokenizerManager
    doc: Create a tokenized request object from common parameters.
  - name: _batch_tokenize_and_process
    signature: (self, batch_size: int, obj: Union[GenerateReqInput, EmbeddingReqInput])
    return: List[Union[TokenizedGenerateReqInput, TokenizedEmbeddingReqInput]]
    class: TokenizerManager
    doc: Handle batch tokenization for text inputs only.
  - name: _validate_batch_tokenization_constraints
    signature: (self, batch_size: int, obj: Union[GenerateReqInput, EmbeddingReqInput])
    return: None
    class: TokenizerManager
    doc: Validate constraints for batch tokenization processing.
  - name: _send_one_request
    signature: (self, obj: Union[GenerateReqInput, EmbeddingReqInput], tokenized_obj: Union[TokenizedGenerateReqInput, TokenizedEmbeddingReqInput], created_time: Optional[float] = None)
    class: TokenizerManager
  - name: _send_batch_request
    signature: (self, obj: Union[GenerateReqInput, EmbeddingReqInput], tokenized_objs: List[Union[TokenizedGenerateReqInput, TokenizedEmbeddingReqInput]], created_time: Optional[float] = None)
    class: TokenizerManager
    doc: Send a batch of tokenized requests as a single batched request to the scheduler.
  - name: _wait_one_response
    signature: (self, obj: Union[GenerateReqInput, EmbeddingReqInput], state: ReqState, request: Optional[fastapi.Request] = None)
    class: TokenizerManager
    doc: Wait for the response of one request.
  - name: _handle_batch_request
    signature: (self, obj: Union[GenerateReqInput, EmbeddingReqInput], request: Optional[fastapi.Request] = None, created_time: Optional[float] = None)
    class: TokenizerManager
  - name: flush_cache
    signature: (self)
    return: FlushCacheReqOutput
    class: TokenizerManager
  - name: clear_hicache_storage
    signature: (self)
    return: ClearHiCacheReqOutput
    class: TokenizerManager
    doc: Clear the hierarchical cache storage.
  - name: abort_request
    signature: (self, rid: str = '', abort_all: bool = False)
    class: TokenizerManager
  - name: start_profile
    signature: (self, output_dir: Optional[str] = None, start_step: Optional[int] = None, num_steps: Optional[int] = None, activities: Optional[List[str]] = None, with_stack: Optional[bool] = None, record_shapes: Optional[bool] = None, profile_by_stage: bool = False)
    class: TokenizerManager
  - name: stop_profile
    signature: (self)
    class: TokenizerManager
  - name: _execute_profile
    signature: (self, req: ProfileReq)
    class: TokenizerManager
  - name: start_expert_distribution_record
    signature: (self)
    class: TokenizerManager
  - name: stop_expert_distribution_record
    signature: (self)
    class: TokenizerManager
  - name: dump_expert_distribution_record
    signature: (self)
    class: TokenizerManager
  - name: pause_generation
    signature: (self)
    class: TokenizerManager
  - name: continue_generation
    signature: (self)
    class: TokenizerManager
  - name: update_weights_from_disk
    signature: (self, obj: UpdateWeightFromDiskReqInput, request: Optional[fastapi.Request] = None)
    return: Tuple[bool, str]
    class: TokenizerManager
  - name: _wait_for_model_update_from_disk
    signature: (self, obj: UpdateWeightFromDiskReqInput)
    return: Tuple[bool, str]
    class: TokenizerManager
  - name: init_weights_update_group
    signature: (self, obj: InitWeightsUpdateGroupReqInput, request: Optional[fastapi.Request] = None)
    return: Tuple[bool, str]
    class: TokenizerManager
  - name: update_weights_from_distributed
    signature: (self, obj: UpdateWeightsFromDistributedReqInput, request: Optional[fastapi.Request] = None)
    return: Tuple[bool, str]
    class: TokenizerManager
  - name: update_weights_from_tensor
    signature: (self, obj: UpdateWeightsFromTensorReqInput, request: Optional[fastapi.Request] = None)
    return: Tuple[bool, str]
    class: TokenizerManager
  - name: load_lora_adapter
    signature: (self, obj: LoadLoRAAdapterReqInput, _: Optional[fastapi.Request] = None)
    return: LoadLoRAAdapterReqOutput
    class: TokenizerManager
  - name: unload_lora_adapter
    signature: (self, obj: UnloadLoRAAdapterReqInput, _: Optional[fastapi.Request] = None)
    return: UnloadLoRAAdapterReqOutput
    class: TokenizerManager
  - name: get_weights_by_name
    signature: (self, obj: GetWeightsByNameReqInput, request: Optional[fastapi.Request] = None)
    class: TokenizerManager
  - name: release_memory_occupation
    signature: (self, obj: ReleaseMemoryOccupationReqInput, request: Optional[fastapi.Request] = None)
    class: TokenizerManager
  - name: resume_memory_occupation
    signature: (self, obj: ResumeMemoryOccupationReqInput, request: Optional[fastapi.Request] = None)
    class: TokenizerManager
  - name: slow_down
    signature: (self, obj: SlowDownReqInput, request: Optional[fastapi.Request] = None)
    class: TokenizerManager
  - name: open_session
    signature: (self, obj: OpenSessionReqInput, request: Optional[fastapi.Request] = None)
    class: TokenizerManager
  - name: close_session
    signature: (self, obj: CloseSessionReqInput, request: Optional[fastapi.Request] = None)
    class: TokenizerManager
  - name: get_internal_state
    signature: (self)
    return: List[Dict[Any, Any]]
    class: TokenizerManager
  - name: set_internal_state
    signature: (self, obj: SetInternalStateReq)
    return: SetInternalStateReqOutput
    class: TokenizerManager
  - name: get_load
    signature: (self)
    return: dict
    class: TokenizerManager
  - name: get_log_request_metadata
    signature: (self)
    class: TokenizerManager
  - name: configure_logging
    signature: (self, obj: ConfigureLoggingReq)
    class: TokenizerManager
  - name: freeze_gc
    signature: (self)
    class: TokenizerManager
    doc: Send a freeze_gc message to the scheduler first, then freeze locally.
  - name: create_abort_task
    signature: (self, obj: GenerateReqInput)
    class: TokenizerManager
  - name: abort_request
    signature: ()
    class: TokenizerManager
  - name: auto_create_handle_loop
    signature: (self)
    class: TokenizerManager
  - name: dump_requests_before_crash
    signature: (self)
    class: TokenizerManager
  - name: _upload_file_to_gcs
    signature: (bucket_name, source_file_path, object_name)
    class: TokenizerManager
  - name: sigterm_watchdog
    signature: (self)
    class: TokenizerManager
  - name: handle_loop
    signature: (self)
    class: TokenizerManager
    doc: The event loop that handles requests
  - name: _handle_batch_output
    signature: (self, recv_obj: Union[BatchStrOut, BatchEmbeddingOut, BatchMultimodalOut, BatchTokenIDOut])
    class: TokenizerManager
  - name: convert_logprob_style
    signature: (self, meta_info: dict, state: ReqState, top_logprobs_num: int, token_ids_logprob: List[int], return_text_in_logprobs: bool, recv_obj: BatchStrOut, recv_obj_index: int)
    class: TokenizerManager
  - name: detokenize_logprob_tokens
    signature: (self, token_logprobs_val: List[float], token_logprobs_idx: List[int], decode_to_text: bool)
    class: TokenizerManager
  - name: detokenize_top_logprobs_tokens
    signature: (self, token_logprobs_val: List[float], token_logprobs_idx: List[int], decode_to_text: bool)
    class: TokenizerManager
  - name: collect_metrics
    signature: (self, state: ReqState, recv_obj: BatchStrOut, i: int)
    class: TokenizerManager
  - name: dump_requests
    signature: (self, state: ReqState, out_dict: dict)
    class: TokenizerManager
  - name: record_request_for_crash_dump
    signature: (self, state: ReqState, out_dict: dict)
    class: TokenizerManager
  - name: _dump_data_to_file
    signature: (self, data_list: List[Tuple], filename: str, log_message: str)
    class: TokenizerManager
  - name: background_task
    signature: ()
    class: TokenizerManager
  - name: _handle_abort_req
    signature: (self, recv_obj)
    class: TokenizerManager
  - name: _handle_open_session_req_output
    signature: (self, recv_obj)
    class: TokenizerManager
  - name: _handle_update_weights_from_disk_req_output
    signature: (self, recv_obj)
    class: TokenizerManager
  - name: score_request
    signature: (self, query: Optional[Union[str, List[int]]] = None, items: Optional[Union[str, List[str], List[List[int]]]] = None, label_token_ids: Optional[List[int]] = None, apply_softmax: bool = False, item_first: bool = False, request: Optional[Any] = None)
    return: List[List[float]]
    class: TokenizerManager
    doc: See Engine.score() for more details.
  - name: _determine_tensor_transport_mode
    signature: (server_args: ServerArgs)
    return: TensorTransportMode
  - name: print_exception_wrapper
    signature: (func)
    doc: Sometimes an asyncio function does not print exception.
  - name: __init__
    signature: (self, tokenizer_manager: TokenizerManager)
    class: SignalHandler
  - name: sigterm_handler
    signature: (self, signum = None, frame = None)
    class: SignalHandler
  - name: running_phase_sigquit_handler
    signature: (self, signum = None, frame = None)
    class: SignalHandler
  - name: __init__
    signature: (self, sender, fan_out: int)
    class: _Communicator
  - name: __call__
    signature: (self, obj)
    class: _Communicator
  - name: handle_recv
    signature: (self, recv_obj: T)
    class: _Communicator

File: managers/tp_worker.py
  - name: __init__
    signature: (self, server_args: ServerArgs, gpu_id: int, tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int], nccl_port: int, is_draft_worker: bool = False, req_to_token_pool: Optional[ReqToTokenPool] = None, token_to_kv_pool_allocator: Optional[BaseTokenToKVPoolAllocator] = None)
    class: TpModelWorker
  - name: register_hicache_layer_transfer_counter
    signature: (self, counter)
    class: TpModelWorker
  - name: set_hicache_consumer
    signature: (self, consumer_index)
    class: TpModelWorker
  - name: get_worker_info
    signature: (self)
    class: TpModelWorker
  - name: sliding_window_size
    signature: (self)
    return: Optional[int]
    class: TpModelWorker
  - name: is_hybrid
    signature: (self)
    return: bool
    class: TpModelWorker
  - name: get_tokens_per_layer_info
    signature: (self)
    class: TpModelWorker
  - name: get_pad_input_ids_func
    signature: (self)
    class: TpModelWorker
  - name: get_tp_group
    signature: (self)
    class: TpModelWorker
  - name: get_attention_tp_group
    signature: (self)
    class: TpModelWorker
  - name: get_attention_tp_cpu_group
    signature: (self)
    class: TpModelWorker
  - name: get_memory_pool
    signature: (self)
    class: TpModelWorker
  - name: forward_batch_generation
    signature: (self, model_worker_batch: ModelWorkerBatch, launch_done: Optional[threading.Event] = None, skip_sample: bool = False)
    return: Tuple[Union[LogitsProcessorOutput, torch.Tensor], Optional[torch.Tensor], bool]
    class: TpModelWorker
  - name: forward_batch_embedding
    signature: (self, model_worker_batch: ModelWorkerBatch)
    class: TpModelWorker
  - name: update_weights_from_disk
    signature: (self, recv_req: UpdateWeightFromDiskReqInput)
    class: TpModelWorker
  - name: init_weights_update_group
    signature: (self, recv_req: InitWeightsUpdateGroupReqInput)
    class: TpModelWorker
  - name: update_weights_from_distributed
    signature: (self, recv_req: UpdateWeightsFromDistributedReqInput)
    class: TpModelWorker
  - name: update_weights_from_tensor
    signature: (self, recv_req: UpdateWeightsFromTensorReqInput)
    class: TpModelWorker
  - name: get_weights_by_name
    signature: (self, recv_req: GetWeightsByNameReqInput)
    class: TpModelWorker
  - name: load_lora_adapter
    signature: (self, recv_req: LoadLoRAAdapterReqInput)
    class: TpModelWorker
  - name: unload_lora_adapter
    signature: (self, recv_req: UnloadLoRAAdapterReqInput)
    class: TpModelWorker
  - name: can_run_lora_batch
    signature: (self, lora_ids: list[str])
    return: bool
    class: TpModelWorker

File: managers/tp_worker_overlap_thread.py
  - name: resolve_future_token_ids
    signature: (input_ids, future_token_ids_map)
  - name: __init__
    signature: (self, server_args: ServerArgs, gpu_id: int, tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int], nccl_port: int)
    class: TpModelWorkerClient
  - name: register_hicache_layer_transfer_counter
    signature: (self, counter)
    class: TpModelWorkerClient
  - name: set_hicache_consumer
    signature: (self, consumer_index)
    class: TpModelWorkerClient
  - name: get_worker_info
    signature: (self)
    class: TpModelWorkerClient
  - name: get_tokens_per_layer_info
    signature: (self)
    class: TpModelWorkerClient
  - name: sliding_window_size
    signature: (self)
    return: Optional[int]
    class: TpModelWorkerClient
  - name: is_hybrid
    signature: (self)
    return: bool
    class: TpModelWorkerClient
  - name: get_pad_input_ids_func
    signature: (self)
    class: TpModelWorkerClient
  - name: get_tp_group
    signature: (self)
    class: TpModelWorkerClient
  - name: get_attention_tp_group
    signature: (self)
    class: TpModelWorkerClient
  - name: get_attention_tp_cpu_group
    signature: (self)
    class: TpModelWorkerClient
  - name: get_memory_pool
    signature: (self)
    class: TpModelWorkerClient
  - name: get_kv_cache
    signature: (self)
    class: TpModelWorkerClient
  - name: forward_thread_func
    signature: (self)
    class: TpModelWorkerClient
  - name: forward_thread_func_
    signature: (self)
    class: TpModelWorkerClient
  - name: resolve_last_batch_result
    signature: (self, launch_done: Optional[threading.Event] = None)
    class: TpModelWorkerClient
    doc: This function is called to resolve the last batch result and
  - name: forward_batch_generation
    signature: (self, model_worker_batch: ModelWorkerBatch)
    return: Tuple[None, torch.Tensor, bool]
    class: TpModelWorkerClient
  - name: update_weights_from_disk
    signature: (self, recv_req: UpdateWeightFromDiskReqInput)
    class: TpModelWorkerClient
  - name: init_weights_update_group
    signature: (self, recv_req: InitWeightsUpdateGroupReqInput)
    class: TpModelWorkerClient
  - name: update_weights_from_distributed
    signature: (self, recv_req: UpdateWeightsFromDistributedReqInput)
    class: TpModelWorkerClient
  - name: update_weights_from_tensor
    signature: (self, recv_req: UpdateWeightsFromTensorReqInput)
    class: TpModelWorkerClient
  - name: get_weights_by_name
    signature: (self, recv_req: GetWeightsByNameReqInput)
    class: TpModelWorkerClient
  - name: load_lora_adapter
    signature: (self, recv_req: LoadLoRAAdapterReqInput)
    class: TpModelWorkerClient
  - name: unload_lora_adapter
    signature: (self, recv_req: UnloadLoRAAdapterReqInput)
    class: TpModelWorkerClient
  - name: can_run_lora_batch
    signature: (self, lora_ids: list[str])
    return: bool
    class: TpModelWorkerClient
  - name: __delete__
    signature: (self)
    class: TpModelWorkerClient

File: managers/utils.py
  - name: validate_input_length
    signature: (req: Req, max_req_input_len: int, allow_auto_truncate: bool)
    return: Optional[str]
    doc: Validate and potentially truncate input length.
  - name: get_logprob_dict_from_result
    signature: (result: GenerationBatchResult)
    return: dict
  - name: get_logprob_from_pp_outputs
    signature: (next_pp_outputs: PPProxyTensors)
    return: tuple[LogitsProcessorOutput, list[int], list[int]]
  - name: __init__
    signature: (self, num_workers: int)
    class: DPBalanceMeta
  - name: destructor
    signature: (self)
    class: DPBalanceMeta
  - name: get_shared_onfly
    signature: (self)
    return: List[Dict[int, int]]
    class: DPBalanceMeta
  - name: set_shared_onfly_info
    signature: (self, data: List[Dict[int, int]])
    class: DPBalanceMeta
  - name: get_shared_local_tokens
    signature: (self)
    return: List[int]
    class: DPBalanceMeta
  - name: set_shared_local_tokens
    signature: (self, data: List[int])
    class: DPBalanceMeta
  - name: __getstate__
    signature: (self)
    class: DPBalanceMeta
  - name: __setstate__
    signature: (self, state)
    class: DPBalanceMeta

File: model_executor/cuda_graph_runner.py
  - name: get_is_capture_mode
    signature: ()
  - name: model_capture_mode
    signature: ()
  - name: freeze_gc
    signature: (enable_cudagraph_gc: bool)
    doc: Optimize garbage collection during CUDA graph capture.
  - name: _to_torch
    signature: (model: torch.nn.Module, reverse: bool, num_tokens: int)
  - name: patch_model
    signature: (model: torch.nn.Module, enable_compile: bool, num_tokens: int, tp_group: GroupCoordinator)
    doc: Patch the model to make it compatible with with torch.compile
  - name: set_torch_compile_config
    signature: ()
  - name: get_batch_sizes_to_capture
    signature: (model_runner: ModelRunner)
  - name: get_global_graph_memory_pool
    signature: ()
  - name: set_global_graph_memory_pool
    signature: (val)
  - name: __init__
    signature: (self, model_runner: ModelRunner)
    class: CudaGraphRunner
  - name: _cache_loc_dtype
    signature: (self)
    class: CudaGraphRunner
  - name: can_run
    signature: (self, forward_batch: ForwardBatch)
    class: CudaGraphRunner
  - name: capture
    signature: (self)
    return: None
    class: CudaGraphRunner
  - name: _capture_graph
    signature: (self, graph, pool, stream, run_once_fn)
    class: CudaGraphRunner
  - name: _create_device_graph
    signature: (self)
    class: CudaGraphRunner
  - name: capture_one_batch_size
    signature: (self, bs: int, forward: Callable)
    class: CudaGraphRunner
  - name: run_once
    signature: ()
    class: CudaGraphRunner
  - name: recapture_if_needed
    signature: (self, forward_batch: ForwardBatch)
    class: CudaGraphRunner
  - name: replay_prepare
    signature: (self, forward_batch: ForwardBatch, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    class: CudaGraphRunner
  - name: replay
    signature: (self, forward_batch: ForwardBatch, skip_attn_backend_init: bool = False, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: Union[LogitsProcessorOutput, PPProxyTensors]
    class: CudaGraphRunner
  - name: get_spec_info
    signature: (self, num_tokens: int)
    class: CudaGraphRunner

File: model_executor/forward_batch_info.py
  - name: is_prefill
    signature: (self)
    class: ForwardMode
  - name: is_extend
    signature: (self)
    class: ForwardMode
  - name: is_decode
    signature: (self)
    class: ForwardMode
  - name: is_mixed
    signature: (self)
    class: ForwardMode
  - name: is_idle
    signature: (self)
    class: ForwardMode
  - name: is_decode_or_idle
    signature: (self)
    class: ForwardMode
  - name: is_target_verify
    signature: (self)
    class: ForwardMode
  - name: is_draft_extend
    signature: (self)
    class: ForwardMode
  - name: is_extend_or_draft_extend_or_mixed
    signature: (self)
    class: ForwardMode
  - name: is_cuda_graph
    signature: (self)
    class: ForwardMode
  - name: is_dummy_first
    signature: (self)
    class: ForwardMode
  - name: is_split_prefill
    signature: (self)
    class: ForwardMode
  - name: need_capture
    signature: (self)
    class: CaptureHiddenMode
  - name: is_full
    signature: (self)
    class: CaptureHiddenMode
  - name: is_last
    signature: (self)
    class: CaptureHiddenMode
  - name: __lt__
    signature: (self, other)
    class: CaptureHiddenMode
  - name: init_new
    signature: (cls, batch: ModelWorkerBatch, model_runner: ModelRunner)
    class: ForwardBatch
  - name: merge_mm_inputs
    signature: (self)
    return: Optional[MultimodalInputs]
    class: ForwardBatch
    doc: Merge all multimodal inputs in the batch into a single MultiModalInputs object.
  - name: contains_image_inputs
    signature: (self)
    return: bool
    class: ForwardBatch
  - name: contains_audio_inputs
    signature: (self)
    return: bool
    class: ForwardBatch
  - name: contains_video_inputs
    signature: (self)
    return: bool
    class: ForwardBatch
  - name: contains_mm_inputs
    signature: (self)
    return: bool
    class: ForwardBatch
  - name: _compute_mrope_positions
    signature: (self, model_runner: ModelRunner, batch: ModelWorkerBatch)
    class: ForwardBatch
  - name: get_max_chunk_capacity
    signature: (self)
    class: ForwardBatch
  - name: set_prefix_chunk_idx
    signature: (self, idx: int)
    class: ForwardBatch
  - name: set_attn_attend_prefix_cache
    signature: (self, attn_attend_prefix_cache: bool)
    class: ForwardBatch
  - name: prepare_chunked_kv_indices
    signature: (self, device: torch.device)
    class: ForwardBatch
  - name: _pad_tensor_to_size
    signature: (self, tensor: torch.Tensor, size: int, *, value: int = 0)
    class: ForwardBatch
  - name: prepare_mlp_sync_batch
    signature: (self, model_runner: ModelRunner)
    class: ForwardBatch
  - name: post_forward_mlp_sync_batch
    signature: (self, logits_output: LogitsProcessorOutput)
    class: ForwardBatch
  - name: get_prefix_chunk_seq_lens
    signature: (self, prefix_lens: torch.Tensor, num_prefix_chunks: int, prefix_chunk_len: int)
    class: ForwardBatch
  - name: prepare_chunked_prefix_cache_info
    signature: (self, device: torch.device)
    class: ForwardBatch
  - name: can_run_tbo
    signature: (self)
    class: ForwardBatch
  - name: enable_num_token_non_padded
    signature: (server_args)
  - name: __init__
    signature: (self, tensors)
    class: PPProxyTensors
  - name: __getitem__
    signature: (self, key: Union[str, slice])
    class: PPProxyTensors
  - name: __setitem__
    signature: (self, key: str, value: torch.Tensor)
    class: PPProxyTensors
  - name: __len__
    signature: (self)
    class: PPProxyTensors
  - name: __eq__
    signature: (self, other: object)
    class: PPProxyTensors
  - name: __repr__
    signature: (self)
    return: str
    class: PPProxyTensors
  - name: compute_position
    signature: (attn_backend: str, extend_prefix_lens: torch.Tensor, extend_seq_lens: torch.Tensor, extend_seq_lens_sum: int)
  - name: compute_position_triton
    signature: (extend_prefix_lens: torch.Tensor, extend_seq_lens: torch.Tensor, extend_seq_lens_sum)
    doc: Compute positions. It is a fused version of `compute_position_torch`.
  - name: compute_position_kernel
    signature: (positions, extend_start_loc, extend_prefix_lens, extend_seq_lens, has_prefix: tl.constexpr)
  - name: compute_position_torch
    signature: (extend_prefix_lens: torch.Tensor, extend_seq_lens: torch.Tensor)
  - name: clamp_position
    signature: (seq_lens)
  - name: create_chunked_prefix_cache_kv_indices
    signature: (req_to_token_ptr, req_pool_indices_ptr, chunk_start_idx_ptr, chunk_seq_lens_ptr, chunk_cu_seq_lens_ptr, chunk_kv_indices_ptr, req_to_token_ptr_stride: tl.constexpr)

File: model_executor/model_runner.py
  - name: __init__
    signature: (self, is_rank_zero)
    class: RankZeroFilter
  - name: filter
    signature: (self, record)
    class: RankZeroFilter
  - name: __init__
    signature: (self, model_config: ModelConfig, mem_fraction_static: float, gpu_id: int, tp_rank: int, tp_size: int, moe_ep_rank: int, moe_ep_size: int, pp_rank: int, pp_size: int, nccl_port: int, server_args: ServerArgs, dp_rank: Optional[int] = None, is_draft_worker: bool = False, req_to_token_pool: Optional[ReqToTokenPool] = None, token_to_kv_pool_allocator: Optional[BaseTokenToKVPoolAllocator] = None)
    class: ModelRunner
  - name: initialize
    signature: (self, min_per_gpu_memory: float)
    class: ModelRunner
  - name: model_specific_adjustment
    signature: (self)
    class: ModelRunner
  - name: init_torch_distributed
    signature: (self)
    class: ModelRunner
  - name: load_model
    signature: (self)
    class: ModelRunner
  - name: update_expert_location
    signature: (self, new_expert_location_metadata: ExpertLocationMetadata, update_layer_ids: List[int])
    class: ModelRunner
  - name: update_weights_from_disk
    signature: (self, model_path: str, load_format: str)
    return: tuple[bool, str]
    class: ModelRunner
    doc: Update engine weights in-place from the disk.
  - name: get_weight_iter
    signature: (config)
    class: ModelRunner
  - name: model_load_weights
    signature: (model, iter)
    class: ModelRunner
  - name: init_weights_update_group
    signature: (self, master_address, master_port, rank_offset, world_size, group_name, backend = 'nccl')
    class: ModelRunner
    doc: Initialize the Torch process group for model parameter updates.
  - name: update_weights_from_distributed
    signature: (self, names, dtypes, shapes, group_name)
    class: ModelRunner
    doc: Update specific parameter in the model weights online
  - name: update_weights_from_tensor
    signature: (self, named_tensors: List[Tuple[str, Union[torch.Tensor, 'LocalSerializedTensor']]], load_format: Optional[str] = None)
    class: ModelRunner
  - name: _update_weights_from_flattened_bucket
    signature: (self, flattened_tensor_bucket_dict)
    class: ModelRunner
    doc: Handle flattened bucket format for weight updates
  - name: get_weights_by_name
    signature: (self, name: str, truncate_size: int = 100)
    return: Optional[torch.Tensor]
    class: ModelRunner
    doc: Get the weights of the parameter by its name. Similar to `get_parameter` in Hugging Face.
  - name: init_lora_manager
    signature: (self)
    class: ModelRunner
  - name: load_lora_adapter
    signature: (self, lora_ref: LoRARef)
    class: ModelRunner
    doc: Load a new lora adapter from disk or huggingface.
  - name: unload_lora_adapter
    signature: (self, lora_ref: LoRARef)
    class: ModelRunner
    doc: Unload a lora adapter that was previously loaded during initialization or dynamic loading.
  - name: profile_max_num_token
    signature: (self, total_gpu_memory: int)
    class: ModelRunner
  - name: set_num_token_hybrid
    signature: (self)
    class: ModelRunner
  - name: init_memory_pool
    signature: (self, total_gpu_memory: int, max_num_reqs: Optional[int] = None, max_total_tokens: Optional[int] = None)
    class: ModelRunner
  - name: init_cublas
    signature: (self)
    class: ModelRunner
    doc: We need to run a small matmul to init cublas. Otherwise, it will raise some errors later.
  - name: init_attention_backend
    signature: (self)
    class: ModelRunner
    doc: Init attention kernel backend.
  - name: _get_attention_backend
    signature: (self)
    class: ModelRunner
    doc: Init attention kernel backend.
  - name: _get_attention_backend_from_str
    signature: (self, backend_str: str)
    class: ModelRunner
  - name: init_double_sparsity_channel_config
    signature: (self, selected_channel)
    class: ModelRunner
  - name: init_device_graphs
    signature: (self)
    class: ModelRunner
    doc: Capture cuda graphs.
  - name: init_threads_binding
    signature: (self)
    class: ModelRunner
  - name: apply_torch_tp
    signature: (self)
    class: ModelRunner
  - name: forward_decode
    signature: (self, forward_batch: ForwardBatch, skip_attn_backend_init: bool = False, pp_proxy_tensors = None)
    return: LogitsProcessorOutput
    class: ModelRunner
  - name: forward_extend
    signature: (self, forward_batch: ForwardBatch, skip_attn_backend_init: bool = False, pp_proxy_tensors = None)
    return: LogitsProcessorOutput
    class: ModelRunner
  - name: forward_idle
    signature: (self, forward_batch: ForwardBatch, pp_proxy_tensors = None)
    return: LogitsProcessorOutput
    class: ModelRunner
  - name: forward_split_prefill
    signature: (self, forward_batch: ForwardBatch, reinit_attn_backend: bool = False, forward_count: int = 1)
    return: LogitsProcessorOutput
    class: ModelRunner
  - name: forward
    signature: (self, forward_batch: ForwardBatch, skip_attn_backend_init: bool = False, pp_proxy_tensors: Optional[PPProxyTensors] = None, reinit_attn_backend: bool = False, split_forward_count: int = 1)
    return: Tuple[Union[LogitsProcessorOutput, PPProxyTensors], bool]
    class: ModelRunner
  - name: _forward_raw
    signature: (self, forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors: Optional[PPProxyTensors], reinit_attn_backend: bool = False, split_forward_count: int = 1)
    return: Tuple[Union[LogitsProcessorOutput, PPProxyTensors], bool]
    class: ModelRunner
  - name: _preprocess_logits
    signature: (self, logits_output: LogitsProcessorOutput, sampling_info: SamplingBatchInfo)
    class: ModelRunner
  - name: sample
    signature: (self, logits_output: LogitsProcessorOutput, forward_batch: ForwardBatch)
    return: torch.Tensor
    class: ModelRunner
    doc: Sample and compute logprobs and update logits_output.
  - name: model_is_mrope
    signature: (self)
    return: bool
    class: ModelRunner
    doc: Detect if the model has "mrope" rope_scaling type.
  - name: save_remote_model
    signature: (self, url: str)
    class: ModelRunner
  - name: save_sharded_model
    signature: (self, path: str, pattern: Optional[str] = None, max_size: Optional[int] = None)
    class: ModelRunner
  - name: _model_load_weights_direct
    signature: (model, named_tensors: List[Tuple[str, torch.Tensor]])
  - name: _unwrap_tensor
    signature: (tensor, tp_rank, device)
  - name: get
    signature: (self, rank: int)
    class: LocalSerializedTensor

File: model_executor/npu_graph_runner.py
  - name: __init__
    signature: (self, model_runner: ModelRunner)
    class: NPUGraphRunner
  - name: _create_device_graph
    signature: (self)
    class: NPUGraphRunner
  - name: _capture_graph
    signature: (self, graph, pool, stream, run_once_fn)
    class: NPUGraphRunner
  - name: _update_inputs
    signature: (self, seq_lens)
    class: NPUGraphRunner
  - name: _cache_loc_dtype
    signature: (self)
    class: NPUGraphRunner
  - name: replay
    signature: (self, forward_batch: ForwardBatch, skip_attn_backend_init: bool = False, pp_proxy_tensors: Optional[PPProxyTensors] = None)
    return: Union[LogitsProcessorOutput, PPProxyTensors]
    class: NPUGraphRunner

File: model_parallel.py
  - name: _shard_tensor
    signature: (full_tensor: torch.Tensor, device_mesh: DeviceMesh, placements: Sequence[dt.Shard])
    return: 'dt.DTensor'
    doc: Locally shards a full tensor based on indicated sharding arrangement, and
  - name: _partition_linear_fn
    signature: (self, name, module, device_mesh)
    class: ColwiseParallelSharded
  - name: _partition_linear_fn
    signature: (self, name, module, device_mesh)
    class: RowwiseParallelMaybeWait
  - name: _prepare_output_fn
    signature: (output_layouts, use_local_output, mod, outputs, device_mesh)
    class: RowwiseParallelMaybeWait
  - name: tensor_parallel
    signature: (module: torch.nn.Module, device_mesh: Optional[DeviceMesh] = None)
    doc: Tensor parallelize the model across the given device mesh.
  - name: tplize
    signature: (mod: torch.nn.Module)
    return: None

File: multimodal/mm_utils.py
  - name: has_valid_data
    signature: (data)
    return: bool
  - name: select_best_resolution
    signature: (original_size, possible_resolutions)
    doc: Selects the best resolution from a list of possible resolutions based on the original size.
  - name: resize_and_pad_image
    signature: (image, target_resolution)
    doc: Resize and pad an image to a target resolution while maintaining aspect ratio.
  - name: divide_to_patches
    signature: (image, patch_size)
    doc: Divides an image into patches of a specified size.
  - name: get_anyres_image_grid_shape
    signature: (image_size, grid_pinpoints, patch_size)
    doc: Calculate the shape of the image patch grid after the preprocessing for images of any resolution.
  - name: process_anyres_image
    signature: (image, processor, grid_pinpoints)
    doc: Process an image with variable resolutions.
  - name: load_image_from_base64
    signature: (image)
  - name: expand2square
    signature: (pil_img, background_color)
  - name: unpad_image
    signature: (tensor, original_size)
    doc: Unpads a PyTorch tensor of a padded and resized image.
  - name: unpad_image_shape
    signature: (current_height, current_width, original_size)
    doc: Unpads a PyTorch tensor of a padded and resized image
  - name: process_images
    signature: (images, image_processor, model_cfg)

File: multimodal/processors/base_processor.py
  - name: organize_results
    signature: (self)
    return: List[Tuple[Modality, Any]]
    class: BaseMultiModalProcessorOutput
    doc: :return: a list of results, with their corresponding modalities
  - name: build
    signature: (self, processor)
    class: MultimodalSpecialTokens
  - name: convert_to_str
    signature: (self, token: Union[str, int], processor)
    return: str
    class: MultimodalSpecialTokens
  - name: convert_to_strs
    signature: (self, processor)
    class: MultimodalSpecialTokens
  - name: get_modality_of_token
    signature: (self, token: str)
    return: Optional[Modality]
    class: MultimodalSpecialTokens
    doc: :return: the modality associated with the given token, if the token is a special_token or matches with the multimodal token regex
  - name: get_token_id_by_modality
    signature: (self, modality: Modality)
    return: Optional[int]
    class: MultimodalSpecialTokens
  - name: parse_regex
    signature: (self)
    class: MultimodalSpecialTokens
  - name: get_combined_regex
    signature: (self)
    return: re.Pattern
    class: MultimodalSpecialTokens
    doc: Builds and returns a regex, used to split input str into tokens (with mm special tokens)
  - name: __init__
    signature: (self, hf_config, server_args, _processor, transport_mode, *args, **kwargs)
    class: BaseMultimodalProcessor
  - name: process_mm_data
    signature: (self, input_text, images = None, videos = None, audios = None, **kwargs)
    return: dict
    class: BaseMultimodalProcessor
    doc: process multimodal data with transformers AutoProcessor
  - name: process_mm_data_async
    signature: (self, image_data, audio_data, input_text, request_obj, **kwargs)
    return: Optional[Dict[str, Any]]
    class: BaseMultimodalProcessor
  - name: get_estimated_frames_list
    signature: (self, image_data)
    class: BaseMultimodalProcessor
    doc: estimate the total frame count from all visual input
  - name: _load_single_item
    signature: (data, modality: Modality, frame_count_limit = None, audio_sample_rate: Optional[int] = None, discard_alpha_channel = True)
    class: BaseMultimodalProcessor
    doc: Load a single multimodal data.
  - name: submit_data_loading_tasks
    signature: (self, text_parts: List[str], multimodal_tokens: MultimodalSpecialTokens, data_iterators: dict[Modality, Iterator[Any]], discard_alpha_channel: bool = True, image_estimated_frames_iter: Optional[iter] = None, image_scaling_factor: float = 1.0, max_image_frames: int = 30, audio_sample_rate: Optional[int] = None)
    return: Tuple[List, List]
    class: BaseMultimodalProcessor
    doc: load multimodal data parallelly using iterators.
  - name: load_mm_data
    signature: (self, prompt: str, multimodal_tokens: MultimodalSpecialTokens, image_data: Optional[list] = None, video_data: Optional[list] = None, audio_data: Optional[list] = None, return_text: Optional[bool] = True, discard_alpha_channel: bool = True, audio_sample_rate: Optional[int] = None)
    return: BaseMultiModalProcessorOutput
    class: BaseMultimodalProcessor
    doc: Each frame of video/image will be replaced by a single image token
  - name: get_mm_items_offset
    signature: (input_ids: torch.Tensor, mm_token_id: int)
    return: List[Tuple[int, int]]
    class: BaseMultimodalProcessor
    doc: Get a set of range for mm_items from input_ids
  - name: get_mm_items_offset_by_pair
    signature: (input_ids: torch.Tensor, mm_start_id: int, mm_end_id: int)
    return: List[Tuple[int, int]]
    class: BaseMultimodalProcessor
  - name: collect_mm_items_from_processor_output
    signature: (self, data_dict: dict)
    return: List[MultimodalDataItem]
    class: BaseMultimodalProcessor
    doc: Create mm_items directly from processor output.
  - name: _process_and_collect_mm_items
    signature: (self, input_text: str, images = None, audios = None, videos = None, **kwargs)
    return: Tuple[List[MultimodalDataItem], torch.Tensor, dict]
    class: BaseMultimodalProcessor
    doc: Helper method to process multimodal data and create mm_items in one step.
  - name: process_and_combine_mm_data
    signature: (self, base_output: BaseMultiModalProcessorOutput, mm_tokens: MultimodalSpecialTokens, **kwargs)
    return: Tuple[List[MultimodalDataItem], torch.Tensor, dict]
    class: BaseMultimodalProcessor
    doc: Process multimodal data and return the combined multimodal items and input_ids.

File: multimodal/processors/clip.py
  - name: __init__
    signature: (self, hf_config, server_args, _processor, *args, **kwargs)
    class: ClipImageProcessor
  - name: process_mm_data_async
    signature: (self, image_data: List[Union[str, bytes]], input_text, *args, **kwargs)
    class: ClipImageProcessor

File: multimodal/processors/deepseek_vl_v2.py
  - name: __init__
    signature: (self, hf_config, server_args, _processor, *args, **kwargs)
    class: DeepseekVL2ImageProcessor
  - name: process_mm_data_async
    signature: (self, image_data: List[Union[str, bytes]], input_text, request_obj, max_req_input_len, *args, **kwargs)
    class: DeepseekVL2ImageProcessor

File: multimodal/processors/gemma3.py
  - name: __init__
    signature: (self, hf_config, server_args, _processor, *args, **kwargs)
    class: Gemma3SGLangImageProcessor
  - name: process_mm_data_async
    signature: (self, image_data: List[Union[str, bytes, Dict]], input_text, request_obj, *args, **kwargs)
    class: Gemma3SGLangImageProcessor

File: multimodal/processors/gemma3n.py
  - name: __init__
    signature: (self, hf_config, server_args, _processor, *args, **kwargs)
    class: Gemma3nSGLangProcessor
  - name: process_mm_data_async
    signature: (self, image_data: Optional[List[Union[str, bytes, Dict]]] = None, audio_data: Optional[List[Union[str, bytes, Dict]]] = None, input_text: str = '', request_obj = None, *args, **kwargs)
    class: Gemma3nSGLangProcessor
    doc: Process multimodal data including images and audio.

File: multimodal/processors/glm4v.py
  - name: __init__
    signature: (self, hf_config, server_args, _processor, *args, **kwargs)
    class: Glm4vImageProcessor
  - name: preprocess_video
    signature: (self, vr: VideoReader)
    class: Glm4vImageProcessor
    doc: Preprocess video using VideoReader from Decord backend.
  - name: process_mm_data_async
    signature: (self, image_data: List[Union[str, bytes]], input_text, request_obj, *args, **kwargs)
    class: Glm4vImageProcessor

File: multimodal/processors/internvl.py
  - name: __init__
    signature: (self, hf_config, server_args, _image_processor, *args, **kwargs)
    class: InternVLImageProcessor
  - name: build_transform
    signature: (input_size)
    class: InternVLImageProcessor
  - name: resize_image
    signature: (img, size)
    class: InternVLImageProcessor
  - name: to_tensor
    signature: (img)
    class: InternVLImageProcessor
  - name: normalize
    signature: (tensor, mean, std)
    class: InternVLImageProcessor
  - name: transform
    signature: (img)
    class: InternVLImageProcessor
  - name: dynamic_preprocess
    signature: (image, min_num = 1, max_num = 12, image_size = 448, use_thumbnail = False)
    class: InternVLImageProcessor
  - name: find_closest_aspect_ratio
    signature: (aspect_ratio, target_ratios, width, height, image_size)
    class: InternVLImageProcessor
  - name: get_index
    signature: (bound, fps, max_frame, first_idx = 0, num_segments = 32)
    class: InternVLImageProcessor
  - name: load_video
    signature: (video_path, bound = None, input_size = 448, max_num = 1, num_segments = 32)
    class: InternVLImageProcessor
  - name: process_mm_data_async
    signature: (self, image_data, input_text, request_obj, **kwargs)
    class: InternVLImageProcessor
  - name: process_image_internvl
    signature: (image, input_size = 448, max_num = 12)
    class: InternVLImageProcessor

File: multimodal/processors/janus_pro.py
  - name: __init__
    signature: (self, hf_config, server_args, _processor, *args, **kwargs)
    class: JanusProImageProcessor
  - name: process_mm_data_async
    signature: (self, image_data: List[Union[str, bytes]], input_text, request_obj, **kwargs)
    class: JanusProImageProcessor

File: multimodal/processors/kimi_vl.py
  - name: __init__
    signature: (self, hf_config, server_args, _processor, *args, **kwargs)
    class: KimiVLImageProcessor
  - name: process_mm_data_async
    signature: (self, image_data: List[Union[str, bytes, Dict]], input_text, request_obj, *args, **kwargs)
    class: KimiVLImageProcessor

File: multimodal/processors/llava.py
  - name: __init__
    signature: (self, hf_config, server_args, _processor, *args, **kwargs)
    class: LlavaImageProcessor
  - name: _process_single_image_task
    signature: (image_data: Union[str, bytes, ImageData], image_aspect_ratio: Optional[str] = None, image_grid_pinpoints: Optional[str] = None, processor = None)
    class: LlavaImageProcessor
  - name: _process_single_image
    signature: (self, image_data: Union[bytes, str, ImageData], aspect_ratio: str, grid_pinpoints: str)
    class: LlavaImageProcessor
  - name: process_mm_data_async
    signature: (self, image_data: List[Union[str, bytes, ImageData]], input_text, request_obj, *args, **kwargs)
    class: LlavaImageProcessor
  - name: _get_sgl_processor_cls
    signature: (self, model_type: str)
    class: LlavaMultimodalProcessor
  - name: __init__
    signature: (self, hf_config, server_args, _processor, *args, **kwargs)
    class: LlavaMultimodalProcessor
  - name: process_mm_data_async
    signature: (self, *args, **kwargs)
    class: LlavaMultimodalProcessor

File: multimodal/processors/minicpm.py
  - name: __init__
    signature: (self, hf_config, server_args, _processor, *args, **kwargs)
    class: MiniCPMMultimodalProcessor
  - name: process_mm_data_async
    signature: (self, image_data: List[Union[str, bytes]], audio_data: List[Union[str, bytes]], input_text, request_obj, **kwargs)
    class: MiniCPMMultimodalProcessor

File: multimodal/processors/mlama.py
  - name: __init__
    signature: (self, hf_config, server_args, _processor, *args, **kwargs)
    class: MllamaImageProcessor
  - name: process_mm_data_async
    signature: (self, image_data: List[Union[str, bytes]], input_text, *args, **kwargs)
    class: MllamaImageProcessor

File: multimodal/processors/mllama4.py
  - name: __init__
    signature: (self, hf_config, server_args, _processor, *args, **kwargs)
    class: Mllama4ImageProcessor
  - name: process_mm_data_async
    signature: (self, image_data: List[Union[str, bytes]], input_text, *args, **kwargs)
    class: Mllama4ImageProcessor

File: multimodal/processors/phi4mm.py
  - name: __init__
    signature: (self, _processor)
    return: None
    class: Phi4MMProcessorAdapter
  - name: __call__
    signature: (self, **kwargs)
    class: Phi4MMProcessorAdapter
  - name: __init__
    signature: (self, hf_config, server_args, _processor, *args, **kwargs)
    class: Phi4MMMultimodalProcessor
  - name: process_mm_data_async
    signature: (self, image_data: List[Union[str, bytes]], audio_data, input_text, request_obj, **kwargs)
    class: Phi4MMMultimodalProcessor

File: multimodal/processors/pixtral.py
  - name: get_patch_grid_size
    signature: (self, *, image_width: int, image_height: int)
    return: tuple[int, int]
    class: PixtralProcessor
  - name: __init__
    signature: (self, hf_config, server_args, _processor, *args, **kwargs)
    class: PixtralProcessor
  - name: _resize
    signature: (self, image)
    class: PixtralProcessor
  - name: process_mm_data_async
    signature: (self, image_data: List[Union[str, bytes]], input_text, request_obj, *args, **kwargs)
    class: PixtralProcessor

File: multimodal/processors/qwen_audio.py
  - name: __init__
    signature: (self, hf_config, server_args, _processor, *args, **kwargs)
    class: Qwen2AudioMultimodalProcessor
  - name: process_mm_data_async
    signature: (self, audio_data, input_text, **kwargs)
    class: Qwen2AudioMultimodalProcessor

File: multimodal/processors/qwen_vl.py
  - name: smart_resize
    signature: (height: int, width: int, factor: int = IMAGE_FACTOR, min_pixels: int = MIN_PIXELS, max_pixels: int = MAX_PIXELS)
    return: tuple[int, int]
    doc: Rescales the image so that the following conditions are met:
  - name: resize_image
    signature: (image, size_factor: int = IMAGE_FACTOR)
    return: Image.Image
  - name: round_by_factor
    signature: (number: int, factor: int)
    return: int
    doc: Returns the closest integer to 'number' that is divisible by 'factor'.
  - name: ceil_by_factor
    signature: (number: int, factor: int)
    return: int
    doc: Returns the smallest integer greater than or equal to 'number' that is divisible by 'factor'.
  - name: floor_by_factor
    signature: (number: int, factor: int)
    return: int
    doc: Returns the largest integer less than or equal to 'number' that is divisible by 'factor'.
  - name: resize_image_async
    signature: (image)
  - name: smart_nframes
    signature: (ele: dict, total_frames: int, video_fps: int | float)
    return: int
    doc: calculate the number of frames for video used for model inputs.
  - name: preprocess_video
    signature: (vr, image_factor: int = IMAGE_FACTOR)
    return: torch.Tensor
  - name: __init__
    signature: (self, hf_config, server_args, _processor, *args, **kwargs)
    class: Qwen2_5VLImageProcessor
  - name: process_mm_data_async
    signature: (self, image_data: List[Union[str, bytes]], input_text, request_obj, *args, **kwargs)
    class: Qwen2_5VLImageProcessor

File: multimodal/processors/step3_vl.py
  - name: forward
    signature: (self, raw_image: Union[np.ndarray, Image.Image])
    return: torch.Tensor
    class: GPUToTensor
  - name: __init__
    signature: (self, size, interpolation_mode = 'bicubic', patch_size = None)
    class: Step3VisionProcessor
  - name: __call__
    signature: (self, image, is_patch = False)
    class: Step3VisionProcessor
  - name: determine_window_size
    signature: (self, long: int, short: int)
    return: int
    class: ImagePatcher
  - name: slide_window
    signature: (self, width: int, height: int, sizes: list[tuple[int, int]], steps: list[tuple[int, int]], img_rate_thr: float = 0.6)
    return: tuple[list[tuple[int, int, int, int]], tuple[int, int]]
    class: ImagePatcher
  - name: square_pad
    signature: (self, img: Image.Image)
    return: Image.Image
    class: ImagePatcher
  - name: get_image_size_for_padding
    signature: (self, img_width: int, img_height: int)
    return: tuple[int, int]
    class: ImagePatcher
  - name: get_image_size_for_preprocess
    signature: (self, img_width: int, img_height: int)
    return: tuple[int, int]
    class: ImagePatcher
  - name: get_image_size_for_crop
    signature: (self, img_width: int, img_height: int, window_size: int)
    class: ImagePatcher
  - name: patch_crop
    signature: (self, img: Image.Image, i: int, j: int, th: int, tw: int)
    class: ImagePatcher
  - name: get_num_patches
    signature: (self, img_width: int, img_height: int)
    return: tuple[int, int]
    class: ImagePatcher
  - name: __call__
    signature: (self, img: Image.Image)
    return: tuple[Image.Image, list[Image.Image], list[bool] | None]
    class: ImagePatcher
  - name: __init__
    signature: (self, config, tokenizer)
    return: None
    class: Step3VLProcessor
  - name: image_token_id
    signature: (self)
    return: int
    class: Step3VLProcessor
  - name: get_num_image_tokens
    signature: (self, img_width: int, img_height: int)
    return: int
    class: Step3VLProcessor
  - name: _split_images
    signature: (self, images: list[Image.Image])
    return: list[ImageWithPatches]
    class: Step3VLProcessor
  - name: _convert_images_to_pixel_values
    signature: (self, images: list[Image.Image], is_patch: bool = False)
    return: list[torch.Tensor]
    class: Step3VLProcessor
  - name: _get_patch_repl
    signature: (self, num_patches: int, patch_newline_mask: list[bool] | None)
    return: tuple[str, list[int]]
    class: Step3VLProcessor
  - name: _get_image_repl
    signature: (self, num_images: int)
    return: tuple[str, list[int]]
    class: Step3VLProcessor
  - name: _get_image_repl_features
    signature: (self, num_images: int, num_patches: int, patch_new_line_idx: Optional[list[bool]])
    return: tuple[str, list[int]]
    class: Step3VLProcessor
  - name: replace_placeholder
    signature: (self, text: str, placeholder: str, repls: list[str])
    return: str
    class: Step3VLProcessor
  - name: __call__
    signature: (self, text: Optional[Union[str, list[str]]] = None, images: Optional[Union[Image.Image, list[Image.Image]]] = None, return_tensors: Optional[Union[str, TensorType]] = None, *args, **kwargs)
    return: BatchFeature
    class: Step3VLProcessor
  - name: __init__
    signature: (self, hf_config, server_args, _processor, *args, **kwargs)
    class: Step3VLImageProcessor
  - name: preprocess
    signature: (self, image)
    class: Step3VLImageProcessor
  - name: __call__
    signature: (self, image)
    class: Step3VLImageProcessor
  - name: process_mm_data_async
    signature: (self, image_data: List[Union[str, bytes]], input_text: str | List[int], request_obj, *args, **kwargs)
    class: Step3VLImageProcessor

File: multimodal/processors/vila.py
  - name: __init__
    signature: (self, hf_config: PretrainedConfig, server_args: ServerArgs, _processor: VILAProcessor, *args, **kwargs)
    return: None
    class: VILAMultimodalProcessor
  - name: process_mm_data_async
    signature: (self, image_data: Optional[ImageDataInputItem | List[ImageDataInputItem]], input_text: str | List[int], request_obj: GenerateReqInput | EmbeddingReqInput, **kwargs)
    return: Optional[Dict[str, Any]]
    class: VILAMultimodalProcessor

File: offloader.py
  - name: wrap_modules
    signature: (self, all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor] = None, whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator] = None)
    class: BaseOffloader
  - name: post_init
    signature: (self)
    class: BaseOffloader
  - name: get_offloader
    signature: ()
  - name: set_offloader
    signature: (instance: BaseOffloader)
  - name: create_offloader_from_server_args
    signature: (server_args: ServerArgs, dp_rank: int)
  - name: __init__
    signature: (self, cpu_offload_max_bytes: int)
    class: OffloaderV1
  - name: wrap_modules
    signature: (self, all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor] = None, whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator] = None)
    class: OffloaderV1
  - name: maybe_offload_to_cpu
    signature: (self, module: torch.nn.Module)
    return: torch.nn.Module
    class: OffloaderV1
  - name: forward
    signature: (*args, **kwargs)
    class: OffloaderV1
  - name: __init__
    signature: (self, group_size: int, num_in_group: int, prefetch_step: int, mode: str, dp_rank: int, dp_size: int)
    class: OffloaderV2
  - name: wrap_modules
    signature: (self, all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor] = None, whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator] = None)
    class: OffloaderV2
  - name: post_init
    signature: (self)
    class: OffloaderV2
  - name: _hook_module_forward_for_offloader
    signature: (index, module, offloaders, prefetch_step)
  - name: _on_forward_end
    signature: ()
  - name: _hook_module_forward_raw
    signature: (module, on_forward_end, get_parameter_and_buffer_dicts)
  - name: forward
    signature: (*args, **kwargs)
  - name: __init__
    signature: (self, mode: str, module: torch.nn.Module, alt_stream: torch.cuda.Stream, whitelist_param_names: List[str])
    class: _ModuleOffloader
  - name: post_init
    signature: (self)
    class: _ModuleOffloader
  - name: start_onload
    signature: (self)
    class: _ModuleOffloader
  - name: offload
    signature: (self)
    class: _ModuleOffloader
  - name: wait_and_get_device_tensors
    signature: (self)
    class: _ModuleOffloader
  - name: _create_device_tensors
    signature: (self)
    class: _ModuleOffloader
  - name: create
    signature: (mode: str, **kwargs)
    return: '_BaseParamOffloader'
    class: _BaseParamOffloader
  - name: __init__
    signature: (self, module, param_name)
    class: _BaseParamOffloader
  - name: _param
    signature: (self)
    class: _BaseParamOffloader
  - name: post_init
    signature: (self)
    class: _BaseParamOffloader
  - name: create_device_tensor
    signature: (self)
    class: _BaseParamOffloader
  - name: __init__
    signature: (self, module, param_name)
    class: _MetaParamOffloader
  - name: create_device_tensor
    signature: (self)
    class: _MetaParamOffloader
  - name: __init__
    signature: (self, module, param_name)
    class: _CpuParamOffloader
  - name: create_device_tensor
    signature: (self)
    class: _CpuParamOffloader
  - name: __init__
    signature: (self, module, param_name)
    class: _ShmCpuParamOffloader
  - name: post_init
    signature: (self)
    class: _ShmCpuParamOffloader
  - name: create_device_tensor
    signature: (self)
    class: _ShmCpuParamOffloader
  - name: _move_param_to_cpu
    signature: (param, pin_memory: bool)
  - name: _move_param_to_meta
    signature: (module, param_name)
  - name: _empty_strided_like
    signature: (x: torch.Tensor, device, pin_memory = False)
  - name: __init__
    signature: (self, module, param_name)
    class: _ShardedGpuParamOffloader
  - name: post_init
    signature: (self)
    class: _ShardedGpuParamOffloader
  - name: create_device_tensor
    signature: (self)
    class: _ShardedGpuParamOffloader
  - name: _even_chunk
    signature: (x: torch.Tensor, chunks: int)
  - name: _create_shared_buffer_tensors
    signature: (local_tensor: torch.Tensor)
    return: List[torch.Tensor]

File: operations.py
  - name: execute_operations
    signature: (inputs, operations)
  - name: execute_overlapped_operations
    signature: (inputs_arr: Sequence, operations_arr: Sequence, delta_stages: Sequence[int])
    return: Sequence
  - name: __init__
    signature: (self, debug_name: str, stages: List[Stage], inputs: dict)
    class: _StageExecutor
  - name: next
    signature: (self)
    class: _StageExecutor
  - name: output
    signature: (self)
    class: _StageExecutor
  - name: done
    signature: (self)
    class: _StageExecutor
  - name: num_stages
    signature: (self)
    class: _StageExecutor
  - name: _annotate_region
    signature: (debug_name)
  - name: __init__
    signature: (self)
    class: _StateDict
  - name: __setattr__
    signature: (self, key, value)
    class: _StateDict
  - name: __getattr__
    signature: (self, item)
    class: _StateDict
  - name: __delattr__
    signature: (self, item)
    class: _StateDict
  - name: pop
    signature: (self, item)
    class: _StateDict
  - name: update
    signature: (self, values: Dict[str, Any])
    class: _StateDict
  - name: get
    signature: (self, item)
    class: _StateDict
  - name: clear
    signature: (self, expect_keys: Sequence[str])
    class: _StateDict
  - name: _convert_operations_to_stages
    signature: (operations: List[Operation])
    return: List[Stage]
  - name: _chunk_by_separator
    signature: (items: List[Any], is_separator: Callable[[Any], bool])
    return: Generator[List[Any], None, None]
  - name: _decorate_operations
    signature: (operations: List[Operation], debug_name_prefix: str = '')
  - name: _decorate_operation
    signature: (operation: Operation, debug_name_prefix: str)

File: operations_strategy.py
  - name: concat
    signature: (cls, items: List['OperationsStrategy'])
    return: 'OperationsStrategy'
    class: OperationsStrategy
  - name: init_new_tbo
    signature: (layers: torch.nn.ModuleList, forward_mode: ForwardMode)
    return: 'OperationsStrategy'
    class: OperationsStrategy
  - name: _assert_all_same
    signature: (items: List)
  - name: _compute_moe_deepseek_layer_operations_strategy_tbo
    signature: (layer: torch.nn.Module, forward_mode: ForwardMode)
    return: OperationsStrategy
  - name: _compute_moe_deepseek_blog_prefill
    signature: (layer)
  - name: _compute_moe_deepseek_blog_decode
    signature: (layer)
  - name: _compute_moe_qwen3_layer_operations_strategy_tbo
    signature: (layer: torch.nn.Module, forward_mode: ForwardMode)
    return: OperationsStrategy
  - name: _compute_moe_qwen3_prefill
    signature: (layer)
  - name: _compute_moe_qwen3_decode
    signature: (layer)

File: patch_torch.py
  - name: monkey_patch_torch_reductions
    signature: ()
    doc: Monkey patching before Torch https://github.com/pytorch/pytorch/pull/149248 is fixed
  - name: _reduce_tensor_modified
    signature: (*args, **kwargs)
  - name: _rebuild_cuda_tensor_modified
    signature: (*args)
  - name: _device_to_uuid
    signature: (device: int)
    return: str
  - name: _device_from_maybe_uuid
    signature: (device_maybe_uuid: Union[int, str])
    return: int
  - name: _modify_tuple
    signature: (t, index: int, modifier: Callable)
  - name: monkey_patch_torch_compile
    signature: ()

File: poll_based_barrier.py
  - name: __init__
    signature: (self, noop: bool = False)
    class: PollBasedBarrier
  - name: local_arrive
    signature: (self)
    class: PollBasedBarrier
  - name: poll_global_arrived
    signature: (self)
    return: bool
    class: PollBasedBarrier
  - name: _compute_global_arrived
    signature: (self)
    return: bool
    class: PollBasedBarrier

File: reasoning_parser.py
  - name: __init__
    signature: (self, normal_text: Optional[str] = None, reasoning_text: Optional[str] = None)
    class: StreamingParseResult
  - name: __init__
    signature: (self, think_start_token: str, think_end_token: str, force_reasoning: bool = False, stream_reasoning: bool = True)
    class: BaseReasoningFormatDetector
  - name: detect_and_parse
    signature: (self, text: str)
    return: StreamingParseResult
    class: BaseReasoningFormatDetector
    doc: One-time parsing: Detects and parses reasoning sections in the provided text.
  - name: parse_streaming_increment
    signature: (self, new_text: str)
    return: StreamingParseResult
    class: BaseReasoningFormatDetector
    doc: Streaming incremental parsing for reasoning content.
  - name: __init__
    signature: (self, stream_reasoning: bool = True, force_reasoning: bool = True)
    class: DeepSeekR1Detector
  - name: __init__
    signature: (self, stream_reasoning: bool = True, force_reasoning: bool = False)
    class: Qwen3Detector
  - name: __init__
    signature: (self, stream_reasoning: bool = True, force_reasoning: bool = False)
    class: KimiDetector
  - name: __init__
    signature: (self, stream_reasoning: bool = True, force_reasoning: bool = True)
    class: GptOssDetector
  - name: detect_and_parse
    signature: (self, text: str)
    return: StreamingParseResult
    class: GptOssDetector
  - name: parse_streaming_increment
    signature: (self, new_text: str)
    return: StreamingParseResult
    class: GptOssDetector
  - name: __init__
    signature: (self, model_type: Optional[str] = None, stream_reasoning: bool = True, force_reasoning: Optional[bool] = None)
    class: ReasoningParser
  - name: parse_non_stream
    signature: (self, full_text: str)
    return: Tuple[Optional[str], Optional[str]]
    class: ReasoningParser
    doc: Non-streaming call: one-time parsing
  - name: parse_stream_chunk
    signature: (self, chunk_text: str)
    return: Tuple[Optional[str], Optional[str]]
    class: ReasoningParser
    doc: Streaming call: incremental parsing

File: sampling/custom_logit_processor.py
  - name: _cache_from_str
    signature: (json_str: str)
    doc: Deserialize a json string to a Callable object.
  - name: __call__
    signature: (self, logits: torch.Tensor, custom_param_list: Optional[List[Dict[str, Any]]] = None)
    return: torch.Tensor
    class: CustomLogitProcessor
    doc: Define the callable behavior.
  - name: to_str
    signature: (cls)
    return: str
    class: CustomLogitProcessor
    doc: Serialize the callable function to a JSON-compatible string.
  - name: from_str
    signature: (cls, json_str: str)
    class: CustomLogitProcessor
    doc: Deserialize a callable function from a JSON string.
  - name: __call__
    signature: (self, logits: torch.Tensor, custom_param_list: Optional[List[Dict[str, Any]]] = None)
    return: torch.Tensor
    class: DisallowedTokensLogitsProcessor

File: sampling/penaltylib/__init__.py
  (no function definitions found)
File: sampling/penaltylib/frequency_penalty.py
  - name: __init__
    signature: (self, orchestrator: BatchedPenalizerOrchestrator)
    class: BatchedFrequencyPenalizer
  - name: _is_required
    signature: (self)
    return: bool
    class: BatchedFrequencyPenalizer
  - name: _prepare
    signature: (self)
    class: BatchedFrequencyPenalizer
  - name: _cumulate_output_tokens
    signature: (self, output_ids: torch.Tensor)
    class: BatchedFrequencyPenalizer
  - name: _apply
    signature: (self, logits: torch.Tensor)
    return: torch.Tensor
    class: BatchedFrequencyPenalizer
  - name: _filter
    signature: (self, keep_indices: torch.Tensor)
    class: BatchedFrequencyPenalizer
  - name: _merge
    signature: (self, their: 'BatchedFrequencyPenalizer')
    class: BatchedFrequencyPenalizer

File: sampling/penaltylib/min_new_tokens.py
  - name: __init__
    signature: (self, orchestrator: BatchedPenalizerOrchestrator)
    class: BatchedMinNewTokensPenalizer
  - name: _is_required
    signature: (self)
    return: bool
    class: BatchedMinNewTokensPenalizer
  - name: _prepare
    signature: (self)
    class: BatchedMinNewTokensPenalizer
  - name: _cumulate_output_tokens
    signature: (self, output_ids: torch.Tensor)
    class: BatchedMinNewTokensPenalizer
  - name: _apply
    signature: (self, logits: torch.Tensor)
    class: BatchedMinNewTokensPenalizer
  - name: _filter
    signature: (self, keep_indices: torch.Tensor)
    class: BatchedMinNewTokensPenalizer
  - name: _merge
    signature: (self, their: 'BatchedMinNewTokensPenalizer')
    class: BatchedMinNewTokensPenalizer

File: sampling/penaltylib/orchestrator.py
  - name: __init__
    signature: (self, vocab_size: int, batch: ScheduleBatch, penalizers: Set[Type['_BatchedPenalizer']])
    class: BatchedPenalizerOrchestrator
  - name: batch
    signature: (self)
    return: ScheduleBatch | None
    class: BatchedPenalizerOrchestrator
  - name: batch
    signature: (self, value: Optional[ScheduleBatch])
    class: BatchedPenalizerOrchestrator
  - name: reqs
    signature: (self)
    class: BatchedPenalizerOrchestrator
  - name: cumulate_output_tokens
    signature: (self, output_ids: torch.Tensor)
    class: BatchedPenalizerOrchestrator
    doc: Feed the output tokens to the penalizers.
  - name: apply
    signature: (self, logits: torch.Tensor)
    return: torch.Tensor
    class: BatchedPenalizerOrchestrator
    doc: Apply the penalizers to the logits.
  - name: filter
    signature: (self, keep_indices: torch.Tensor)
    class: BatchedPenalizerOrchestrator
    doc: Filter the penalizers based on the indices to keep in the batch.
  - name: merge
    signature: (self, their: 'BatchedPenalizerOrchestrator')
    class: BatchedPenalizerOrchestrator
    doc: Merge the penalizers of another orchestrator into this one.
  - name: is_prepared
    signature: (self)
    return: bool
    class: _BatchedPenalizer
  - name: is_required
    signature: (self)
    return: bool
    class: _BatchedPenalizer
  - name: prepare
    signature: (self)
    class: _BatchedPenalizer
  - name: prepare_if_required
    signature: (self)
    class: _BatchedPenalizer
  - name: teardown
    signature: (self)
    class: _BatchedPenalizer
  - name: cumulate_output_tokens
    signature: (self, output_ids: torch.Tensor)
    class: _BatchedPenalizer
  - name: apply
    signature: (self, logits: torch.Tensor)
    return: torch.Tensor
    class: _BatchedPenalizer
  - name: filter
    signature: (self, keep_indices: torch.Tensor)
    class: _BatchedPenalizer
  - name: merge
    signature: (self, their: '_BatchedPenalizer')
    class: _BatchedPenalizer
  - name: _is_required
    signature: (self)
    return: bool
    class: _BatchedPenalizer
    doc: Check if the penalizer is required to be prepared.
  - name: _prepare
    signature: (self)
    class: _BatchedPenalizer
    doc: Prepare the penalizer.
  - name: _cumulate_output_tokens
    signature: (self, output_ids: torch.Tensor)
    class: _BatchedPenalizer
    doc: Cumulate the output tokens.
  - name: _apply
    signature: (self, logits: torch.Tensor)
    return: torch.Tensor
    class: _BatchedPenalizer
    doc: Apply the penalizer to the logits.
  - name: _filter
    signature: (self, keep_indices: torch.Tensor)
    class: _BatchedPenalizer
    doc: Filter the penalizer (tensors or underlying data) based on the indices to keep in the batch.
  - name: _merge
    signature: (self, their: '_BatchedPenalizer')
    class: _BatchedPenalizer
    doc: Merge the penalizer with another penalizer.

File: sampling/penaltylib/presence_penalty.py
  - name: __init__
    signature: (self, orchestrator: BatchedPenalizerOrchestrator)
    class: BatchedPresencePenalizer
  - name: _is_required
    signature: (self)
    return: bool
    class: BatchedPresencePenalizer
  - name: _prepare
    signature: (self)
    class: BatchedPresencePenalizer
  - name: _cumulate_output_tokens
    signature: (self, output_ids: torch.Tensor)
    class: BatchedPresencePenalizer
  - name: _apply
    signature: (self, logits: torch.Tensor)
    return: torch.Tensor
    class: BatchedPresencePenalizer
  - name: _filter
    signature: (self, keep_indices: torch.Tensor)
    class: BatchedPresencePenalizer
  - name: _merge
    signature: (self, their: 'BatchedPresencePenalizer')
    class: BatchedPresencePenalizer

File: sampling/sampling_batch_info.py
  - name: from_schedule_batch
    signature: (cls, batch: ScheduleBatch, vocab_size: int)
    class: SamplingBatchInfo
  - name: __len__
    signature: (self)
    class: SamplingBatchInfo
  - name: update_regex_vocab_mask
    signature: (self)
    class: SamplingBatchInfo
  - name: update_penalties
    signature: (self)
    class: SamplingBatchInfo
  - name: apply_logits_bias
    signature: (self, logits: torch.Tensor)
    class: SamplingBatchInfo
  - name: filter_batch
    signature: (self, keep_indices: List[int], keep_indices_device: torch.Tensor)
    class: SamplingBatchInfo
  - name: _filter_batch_custom_logit_processor
    signature: (self, keep_indices: List[int], keep_indices_device: torch.Tensor)
    class: SamplingBatchInfo
    doc: Filter the custom logit processor and custom params
  - name: merge_custom_logit_processor
    signature: (lhs: Optional[Dict[int, Tuple[CustomLogitProcessor, torch.Tensor]]], rhs: Optional[Dict[int, Tuple[CustomLogitProcessor, torch.Tensor]]], bs1: int, bs2: int, device: str)
    class: SamplingBatchInfo
  - name: merge_batch
    signature: (self, other: 'SamplingBatchInfo')
    class: SamplingBatchInfo
  - name: merge_bias_tensor
    signature: (lhs: Optional[torch.Tensor], rhs: Optional[torch.Tensor], bs1: int, bs2: int, device: str, default: float)
    doc: Merge two bias tensors for batch merging.

File: sampling/sampling_params.py
  - name: __init__
    signature: (self, max_new_tokens: int = 128, stop: Optional[Union[str, List[str]]] = None, stop_token_ids: Optional[List[int]] = None, temperature: float = 1.0, top_p: float = 1.0, top_k: int = -1, min_p: float = 0.0, frequency_penalty: float = 0.0, presence_penalty: float = 0.0, repetition_penalty: float = 1.0, min_new_tokens: int = 0, n: int = 1, json_schema: Optional[str] = None, regex: Optional[str] = None, ebnf: Optional[str] = None, structural_tag: Optional[str] = None, ignore_eos: bool = False, skip_special_tokens: bool = True, spaces_between_special_tokens: bool = True, no_stop_trim: bool = False, custom_params: Optional[Dict[str, Any]] = None, stream_interval: Optional[int] = None, logit_bias: Optional[Dict[str, float]] = None)
    return: None
    class: SamplingParams
  - name: verify
    signature: (self, vocab_size)
    class: SamplingParams
  - name: normalize
    signature: (self, tokenizer)
    class: SamplingParams

File: server_args.py
  - name: add_load_format_choices
    signature: (choices)
  - name: add_quantization_method_choices
    signature: (choices)
  - name: add_attention_backend_choices
    signature: (choices)
  - name: add_disagg_transfer_backend_choices
    signature: (choices)
  - name: __post_init__
    signature: (self)
    class: ServerArgs
  - name: add_cli_args
    signature: (parser: argparse.ArgumentParser)
    class: ServerArgs
  - name: from_cli_args
    signature: (cls, args: argparse.Namespace)
    class: ServerArgs
  - name: url
    signature: (self)
    class: ServerArgs
  - name: get_hf_config
    signature: (self)
    class: ServerArgs
  - name: check_server_args
    signature: (self)
    class: ServerArgs
  - name: check_lora_server_args
    signature: (self)
    class: ServerArgs
  - name: validate_disagg_tp_size
    signature: (self, prefill_tp: int, decode_tp: int)
    class: ServerArgs
  - name: model_specific_adjustments
    signature: (self)
    class: ServerArgs
  - name: adjust_mem_fraction_for_vlm
    signature: (self, model_config)
    class: ServerArgs
  - name: prepare_server_args
    signature: (argv: List[str])
    return: ServerArgs
    doc: Prepare the server arguments from the command line arguments.
  - name: init_new
    signature: (server_args, dp_rank: Optional[int] = None)
    return: 'PortArgs'
    class: PortArgs
  - name: __call__
    signature: (self, parser, namespace, values, option_string = None)
    class: LoRAPathAction
  - name: __init__
    signature: (self, option_strings, dest, nargs = 0, **kwargs)
    class: DeprecatedAction
  - name: __call__
    signature: (self, parser, namespace, values, option_string = None)
    class: DeprecatedAction
  - name: print_deprecated_warning
    signature: (message: str)
  - name: auto_choose_speculative_params
    signature: (self: ServerArgs)
    doc: Automatically choose the parameters for speculative decoding.

File: speculative/build_eagle_tree.py
  - name: build_tree_kernel_efficient_preprocess
    signature: (verified_id: torch.Tensor, score_list: List[torch.Tensor], token_list: List[torch.Tensor], parents_list: List[torch.Tensor], num_verify_tokens: int)
  - name: build_tree_kernel_efficient
    signature: (verified_id: torch.Tensor, score_list: List[torch.Tensor], token_list: List[torch.Tensor], parents_list: List[torch.Tensor], seq_lens: torch.Tensor, seq_lens_sum: int, topk: int, spec_steps: int, num_verify_tokens: int, tree_mask_mode: TreeMaskMode = TreeMaskMode.FULL_MASK, tree_mask_buf: Optional[torch.Tensor] = None, position_buf: Optional[torch.Tensor] = None)
  - name: test_build_tree_kernel_efficient
    signature: ()

File: speculative/eagle_draft_cuda_graph_runner.py
  - name: __init__
    signature: (self, eagle_worker: EAGLEWorker)
    class: EAGLEDraftCudaGraphRunner
  - name: can_run
    signature: (self, forward_batch: ForwardBatch)
    class: EAGLEDraftCudaGraphRunner
  - name: capture
    signature: (self)
    class: EAGLEDraftCudaGraphRunner
  - name: capture_one_batch_size
    signature: (self, num_seqs: int, forward: Callable)
    class: EAGLEDraftCudaGraphRunner
  - name: run_once
    signature: ()
    class: EAGLEDraftCudaGraphRunner
  - name: _postprocess_output_to_raw_bs
    signature: (self, out, raw_bs)
    class: EAGLEDraftCudaGraphRunner
  - name: replay
    signature: (self, forward_batch: ForwardBatch)
    class: EAGLEDraftCudaGraphRunner

File: speculative/eagle_draft_extend_cuda_graph_runner.py
  - name: __init__
    signature: (self, eagle_worker: EAGLEWorker)
    class: EAGLEDraftExtendCudaGraphRunner
  - name: can_run
    signature: (self, forward_batch: ForwardBatch)
    class: EAGLEDraftExtendCudaGraphRunner
  - name: capture
    signature: (self)
    class: EAGLEDraftExtendCudaGraphRunner
  - name: capture_one_batch_size
    signature: (self, bs: int, forward: Callable)
    class: EAGLEDraftExtendCudaGraphRunner
  - name: run_once
    signature: ()
    class: EAGLEDraftExtendCudaGraphRunner
  - name: replay
    signature: (self, forward_batch: ForwardBatch)
    class: EAGLEDraftExtendCudaGraphRunner

File: speculative/eagle_utils.py
  - name: prepare_for_extend
    signature: (self, batch: ScheduleBatch)
    class: EagleDraftInput
  - name: create_idle_input
    signature: (cls, device: torch.device, hidden_size: int, dtype: torch.dtype, topk: int, capture_hidden_mode: CaptureHiddenMode)
    class: EagleDraftInput
  - name: prepare_extend_after_decode
    signature: (self, batch: ScheduleBatch, speculative_num_steps: int)
    class: EagleDraftInput
  - name: generate_attn_arg_prefill
    signature: (self, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, req_to_token: torch.Tensor)
    class: EagleDraftInput
  - name: filter_batch
    signature: (self, new_indices: torch.Tensor, has_been_filtered: bool = True)
    class: EagleDraftInput
  - name: merge_batch
    signature: (self, spec_info: EagleDraftInput)
    class: EagleDraftInput
  - name: create_idle_input
    signature: (cls, topk: int, spec_steps: int, num_verify_tokens: int)
    class: EagleVerifyInput
  - name: prepare_for_verify
    signature: (self, batch: ScheduleBatch, page_size: int)
    class: EagleVerifyInput
  - name: generate_attn_arg_prefill
    signature: (self, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, req_to_token: torch.Tensor)
    class: EagleVerifyInput
  - name: verify
    signature: (self, batch: ScheduleBatch, logits_output: LogitsProcessorOutput, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, page_size: int, vocab_mask: Optional[torch.Tensor] = None)
    return: torch.Tensor
    class: EagleVerifyInput
    doc: Verify and find accepted tokens based on logits output and batch
  - name: create_extend_after_decode_spec_info
    signature: (verified_id, seq_lens, accept_lens, positions, new_verified_id, bs_upper: tl.constexpr)
  - name: assign_req_to_token_pool
    signature: (req_pool_indices, req_to_token, start_offset, end_offset, out_cache_loc, pool_len: tl.constexpr, bs_upper: tl.constexpr)
  - name: assign_draft_cache_locs
    signature: (req_pool_indices, req_to_token, seq_lens, extend_lens, num_new_pages_per_topk, out_cache_loc, pool_len: tl.constexpr, topk: tl.constexpr, speculative_num_steps: tl.constexpr, page_size: tl.constexpr, bs_upper: tl.constexpr, iter_upper: tl.constexpr)
  - name: generate_draft_decode_kv_indices
    signature: (req_pool_indices, req_to_token, paged_kernel_lens, kv_indices, kv_indptr, positions, pool_len: tl.constexpr, kv_indices_stride: tl.constexpr, kv_indptr_stride: tl.constexpr, bs_upper: tl.constexpr, iter_upper: tl.constexpr, num_tokens_upper: tl.constexpr, page_size: tl.constexpr)
  - name: align_evict_mask_to_page_size
    signature: (seq_lens, evict_mask, page_size: tl.constexpr, num_draft_tokens: tl.constexpr, BLOCK_SIZE: tl.constexpr)
  - name: get_target_cache_loc
    signature: (tgt_cache_loc, to_free_slots, accept_length, to_free_num_slots, out_cache_loc, num_verify_tokens: tl.constexpr, num_verify_tokens_upper: tl.constexpr, bs_upper: tl.constexpr)
  - name: get_src_tgt_cache_loc
    signature: (seq_lens: torch.Tensor, out_cache_loc: torch.Tensor, accept_index: torch.Tensor, accept_length: torch.Tensor, draft_token_num: int, page_size: int)
  - name: filter_finished_cache_loc_kernel
    signature: (out_cache_loc, tgt_cache_loc, accept_length, accept_length_filter, bs_upper: tl.constexpr, num_verify_tokens_upper: tl.constexpr)
  - name: create_accept_length_filter
    signature: (accept_length: torch.Tensor, unfinished_index_device: torch.Tensor, seq_lens: torch.Tensor)
  - name: select_top_k_tokens
    signature: (i: int, topk_p: torch.Tensor, topk_index: torch.Tensor, hidden_states: torch.Tensor, scores: torch.Tensor, topk: int)
  - name: _generate_simulated_accept_index
    signature: (accept_index, predict, accept_length, simulate_acc_len, bs, spec_steps)
  - name: traverse_tree
    signature: (retrieve_next_token: torch.Tensor, retrieve_next_sibling: torch.Tensor, draft_tokens: torch.Tensor, grammar: BaseGrammarObject, allocate_token_bitmask: torch.Tensor)
    doc: Traverse the tree constructed by the draft model to generate the logits mask.
  - name: dfs
    signature: (curr: int, retrieve_next_token: torch.Tensor, retrieve_next_sibling: torch.Tensor, parent_pos: int)
  - name: generate_token_bitmask
    signature: (reqs: List[Req], verify_input: EagleVerifyInput, retrieve_next_token_cpu: torch.Tensor, retrieve_next_sibling_cpu: torch.Tensor, draft_tokens_cpu: torch.Tensor, vocab_size: int)
    doc: Generate the logit mask for structured output.

File: speculative/eagle_worker.py
  - name: draft_tp_context
    signature: (tp_group: GroupCoordinator)
  - name: __init__
    signature: (self, server_args: ServerArgs, gpu_id: int, tp_rank: int, dp_rank: Optional[int], moe_ep_rank: int, nccl_port: int, target_worker: TpModelWorker)
    class: EAGLEWorker
  - name: init_attention_backend
    signature: (self)
    class: EAGLEWorker
  - name: init_cuda_graphs
    signature: (self)
    class: EAGLEWorker
    doc: Capture cuda graphs.
  - name: draft_model_runner
    signature: (self)
    class: EAGLEWorker
  - name: forward_batch_speculative_generation
    signature: (self, batch: ScheduleBatch)
    return: Tuple[LogitsProcessorOutput, torch.Tensor, int, int, bool]
    class: EAGLEWorker
    doc: Run speculative decoding forward.
  - name: check_forward_draft_extend_after_decode
    signature: (self, batch: ScheduleBatch)
    class: EAGLEWorker
  - name: forward_target_extend
    signature: (self, batch: ScheduleBatch)
    return: Tuple[LogitsProcessorOutput, torch.Tensor, int, Optional[torch.Tensor]]
    class: EAGLEWorker
    doc: Run the target extend.
  - name: _draft_preprocess_decode
    signature: (self, batch: ScheduleBatch)
    class: EAGLEWorker
  - name: _draft_preprocess_idle
    signature: (self, batch: ScheduleBatch)
    class: EAGLEWorker
  - name: draft
    signature: (self, batch: ScheduleBatch)
    class: EAGLEWorker
  - name: draft_forward
    signature: (self, forward_batch: ForwardBatch)
    class: EAGLEWorker
  - name: verify
    signature: (self, batch: ScheduleBatch, spec_info: EagleVerifyInput)
    class: EAGLEWorker
  - name: add_logprob_values
    signature: (self, batch: ScheduleBatch, res: EagleVerifyOutput, logits_output: LogitsProcessorOutput)
    class: EAGLEWorker
  - name: forward_draft_extend
    signature: (self, batch: ScheduleBatch, hidden_states: torch.Tensor, next_token_ids: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor])
    class: EAGLEWorker
    doc: Run draft model extend. This API modifies the states of the batch.
  - name: forward_draft_extend_after_decode
    signature: (self, batch: ScheduleBatch)
    class: EAGLEWorker
  - name: capture_for_decode
    signature: (self, logits_output: LogitsProcessorOutput, draft_input: EagleDraftInput)
    class: EAGLEWorker
  - name: _detect_nan_if_needed
    signature: (self, logits_output: LogitsProcessorOutput)
    class: EAGLEWorker
  - name: load_token_map
    signature: (token_map_path: str)
    return: List[int]
  - name: get_last_loc_large_page_size_top_k_1
    signature: (req_to_token: torch.Tensor, req_pool_indices: torch.Tensor, seq_lens, speculative_num_steps: int)
  - name: get_last_loc_large_page_size_large_top_k
    signature: (req_to_token: torch.Tensor, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, speculative_num_steps: int, topk: int, page_size: int)

File: speculative/spec_info.py
  - name: is_none
    signature: (self)
    class: SpeculativeAlgorithm
  - name: is_eagle
    signature: (self)
    class: SpeculativeAlgorithm
  - name: is_eagle3
    signature: (self)
    class: SpeculativeAlgorithm
  - name: from_string
    signature: (name: str)
    class: SpeculativeAlgorithm

File: tokenizer/tiktoken_tokenizer.py
  - name: __init__
    signature: (self, name: str)
    class: TiktokenProcessor
  - name: image_processor
    signature: (self, image)
    class: TiktokenProcessor
  - name: __init__
    signature: (self, tokenizer_path)
    class: TiktokenTokenizer
  - name: encode_patched
    signature: (self, text: str, *, allowed_special: Union[Literal['all'], AbstractSet[str]] = set(), disallowed_special: Union[Literal['all'], Collection[str]] = 'all')
    return: List[int]
    class: TiktokenTokenizer
  - name: encode
    signature: (self, x, add_special_tokens = False)
    class: TiktokenTokenizer
  - name: decode
    signature: (self, x, *args, **kwargs)
    class: TiktokenTokenizer
  - name: batch_decode
    signature: (self, batch, skip_special_tokens = True, spaces_between_special_tokens = False)
    class: TiktokenTokenizer
  - name: apply_chat_template
    signature: (self, messages, tokenize, add_generation_prompt, tools = None, reasoning_effort = None)
    class: TiktokenTokenizer
  - name: __call__
    signature: (self, text, **kwargs)
    class: TiktokenTokenizer
  - name: init_xgrammar
    signature: (self)
    class: TiktokenTokenizer

File: torch_memory_saver_adapter.py
  - name: create
    signature: (enable: bool)
    class: TorchMemorySaverAdapter
  - name: check_validity
    signature: (self, caller_name)
    class: TorchMemorySaverAdapter
  - name: configure_subprocess
    signature: (self)
    class: TorchMemorySaverAdapter
  - name: region
    signature: (self, tag: str)
    class: TorchMemorySaverAdapter
  - name: pause
    signature: (self, tag: str)
    class: TorchMemorySaverAdapter
  - name: resume
    signature: (self, tag: str)
    class: TorchMemorySaverAdapter
  - name: enabled
    signature: (self)
    class: TorchMemorySaverAdapter
  - name: configure_subprocess
    signature: (self)
    class: _TorchMemorySaverAdapterReal
  - name: region
    signature: (self, tag: str)
    class: _TorchMemorySaverAdapterReal
  - name: pause
    signature: (self, tag: str)
    class: _TorchMemorySaverAdapterReal
  - name: resume
    signature: (self, tag: str)
    class: _TorchMemorySaverAdapterReal
  - name: enabled
    signature: (self)
    class: _TorchMemorySaverAdapterReal
  - name: configure_subprocess
    signature: (self)
    class: _TorchMemorySaverAdapterNoop
  - name: region
    signature: (self, tag: str)
    class: _TorchMemorySaverAdapterNoop
  - name: pause
    signature: (self, tag: str)
    class: _TorchMemorySaverAdapterNoop
  - name: resume
    signature: (self, tag: str)
    class: _TorchMemorySaverAdapterNoop
  - name: enabled
    signature: (self)
    class: _TorchMemorySaverAdapterNoop

File: two_batch_overlap.py
  - name: get_token_num_per_seq
    signature: (forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]] = None)
  - name: compute_split_seq_index
    signature: (forward_mode: 'ForwardMode', num_tokens: int, extend_lens: Optional[Sequence[int]], token_num_per_seq: Optional[int])
    return: Optional[int]
  - name: _is_two_chunk_split_enabled
    signature: (extend_lens: Sequence[int])
    return: bool
  - name: _split_extend_seqs
    signature: (arr: Sequence[int])
    return: int
  - name: _split_array_by_cum_less_than_half
    signature: (arr: Sequence[int])
    return: int
  - name: _split_array_by_balanced_sum
    signature: (arr: Sequence[int])
    return: int
  - name: _update_device_and_sum_field_from_cpu_field
    signature: (batch: ForwardBatch, cpu_field: str, device_field: str, sum_field: str = None)
  - name: _compute_mask_offset
    signature: (seq_index: int, spec_info: Optional[EagleVerifyInput])
    return: int
  - name: split_spec_info
    signature: (spec_info: Optional[EagleVerifyInput], start_seq_index: int, end_seq_index: int, start_token_index: int, end_token_index: int)
  - name: compute_split_token_index
    signature: (split_seq_index: int, forward_mode: 'ForwardMode', extend_seq_lens: Optional[Sequence[int]], token_num_per_seq: Optional[int])
    return: int
  - name: compute_split_indices_for_cuda_graph_replay
    signature: (forward_mode: ForwardMode, cuda_graph_num_tokens: int, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  - name: __init__
    signature: (self)
    class: TboCudaGraphRunnerPlugin
  - name: capture_one_batch_size
    signature: (self, batch: ForwardBatch, num_tokens: int)
    class: TboCudaGraphRunnerPlugin
  - name: replay_prepare
    signature: (self, forward_mode: ForwardMode, bs: int, num_token_non_padded: int, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
    class: TboCudaGraphRunnerPlugin
  - name: prepare_all_gather
    signature: (self, local_batch: ScheduleBatch)
    class: TboDPAttentionPreparer
  - name: compute_output
    signature: (self, partial_global_info)
    class: TboDPAttentionPreparer
  - name: _compute_local_forward_mode
    signature: (local_batch)
    class: TboDPAttentionPreparer
  - name: _compute_global_forward_mode
    signature: (forward_modes)
    class: TboDPAttentionPreparer
  - name: _is_all_same
    signature: (x)
    class: TboDPAttentionPreparer
  - name: prepare
    signature: (cls, batch: ForwardBatch, is_draft_worker: bool = False)
    class: TboForwardBatchPreparer
  - name: prepare_raw
    signature: (cls, batch: ForwardBatch, tbo_children_num_token_non_padded: torch.Tensor)
    class: TboForwardBatchPreparer
  - name: derive_fields_related_to_seq_len_for_two_chunk
    signature: (cls, batch: ForwardBatch, *, child_a: ForwardBatch, child_b: ForwardBatch, tbo_split_seq_index: int)
    class: TboForwardBatchPreparer
  - name: filter_batch
    signature: (cls, batch: ForwardBatch, *, start_token_index: int, end_token_index: int, start_seq_index: int, end_seq_index: int, output_attn_backend: AttentionBackend, out_num_token_non_padded: torch.Tensor)
    class: TboForwardBatchPreparer
  - name: compute_tbo_children_num_token_non_padded
    signature: (cls, batch: ForwardBatch)
    class: TboForwardBatchPreparer
  - name: compute_tbo_children_num_token_non_padded_raw
    signature: (cls, tbo_split_token_index: int, num_token_non_padded: int)
    class: TboForwardBatchPreparer
  - name: _compute_split_token_index
    signature: (cls, batch: ForwardBatch)
    class: TboForwardBatchPreparer
  - name: _compute_extend_num_tokens
    signature: (input_ids, forward_mode: ForwardMode)
  - name: model_forward_maybe_tbo
    signature: (layers, enable_tbo: bool, positions: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor, input_data_scatter_mode: ScatterMode, residual: Optional[torch.Tensor], zero_allocator: Optional[BumpAllocator] = None)
  - name: _model_forward_tbo
    signature: (inputs, operations_strategy: OperationsStrategy, input_data_scatter_mode: ScatterMode, layer_input_scatter_mode: ScatterMode)
  - name: _model_forward_non_tbo
    signature: (inputs, operations_strategy: OperationsStrategy)
  - name: _model_forward_tbo_split_inputs
    signature: (hidden_states: torch.Tensor, residual: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: Optional[BumpAllocator], input_data_scatter_mode: ScatterMode, layer_input_scatter_mode: ScatterMode)
    return: List[Dict]
  - name: _post_transform
    signature: (hidden_states, residual, forward_batch, **kwargs)
  - name: _model_forward_tbo_split_inputs_raw
    signature: (hidden_states: torch.Tensor, residual: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: Optional[BumpAllocator])
    return: List[Dict]
  - name: _model_forward_filter_inputs
    signature: (hidden_states: torch.Tensor, residual: torch.Tensor, positions: torch.Tensor, output_forward_batch: ForwardBatch, tbo_subbatch_index: int)
    return: Dict
  - name: _model_forward_tbo_merge_outputs
    signature: (output_a, output_b)
  - name: _handle_key
    signature: (name)
  - name: __init__
    signature: (self, **kwargs)
    class: MaybeTboDeepEPDispatcher
  - name: _execute
    signature: (self, name, tbo_subbatch_index: Optional[int] = None, **kwargs)
    class: MaybeTboDeepEPDispatcher
  - name: dispatch
    signature: (self, **kwargs)
    return: DispatchOutput
    class: MaybeTboDeepEPDispatcher
  - name: dispatch_a
    signature: (self, **kwargs)
    class: MaybeTboDeepEPDispatcher
  - name: dispatch_b
    signature: (self, **kwargs)
    class: MaybeTboDeepEPDispatcher
  - name: combine
    signature: (self, **kwargs)
    return: torch.Tensor
    class: MaybeTboDeepEPDispatcher
  - name: combine_a
    signature: (self, **kwargs)
    class: MaybeTboDeepEPDispatcher
  - name: combine_b
    signature: (self, **kwargs)
    class: MaybeTboDeepEPDispatcher

File: utils.py
  - name: is_hip
    signature: ()
    return: bool
  - name: is_cuda
    signature: ()
  - name: is_cuda_alike
    signature: ()
  - name: is_hpu
    signature: ()
    return: bool
  - name: is_xpu
    signature: ()
    return: bool
  - name: is_npu
    signature: ()
    return: bool
  - name: is_host_cpu_x86
    signature: ()
    return: bool
  - name: is_cpu
    signature: ()
    return: bool
  - name: get_cuda_version
    signature: ()
  - name: _check
    signature: (cc_major)
  - name: is_blackwell
    signature: ()
  - name: is_sm100_supported
    signature: (device = None)
    return: bool
  - name: is_sm90_supported
    signature: (device = None)
    return: bool
  - name: get_bool_env_var
    signature: (name: str, default: str = 'false')
    return: bool
  - name: get_int_env_var
    signature: (name: str, default: int = 0)
    return: int
  - name: support_triton
    signature: (backend: str)
    return: bool
  - name: cpu_has_amx_support
    signature: ()
  - name: use_intel_amx_backend
    signature: (layer)
  - name: is_flashinfer_available
    signature: ()
    doc: Check whether flashinfer is available.
  - name: random_uuid
    signature: ()
    return: str
  - name: set_inference_mode
    signature: (mode: bool)
    class: DynamicGradMode
  - name: __init__
    signature: (self, mode = True)
    class: DynamicGradMode
  - name: __new__
    signature: (cls, mode_or_orig_func = True if _ENABLE_TORCH_INFERENCE_MODE else None)
    class: DynamicGradMode
  - name: __enter__
    signature: (self)
    return: None
    class: DynamicGradMode
  - name: __exit__
    signature: (self, exc_type: Any, exc_value: Any, traceback: Any)
    return: None
    class: DynamicGradMode
  - name: clone
    signature: (self)
    return: 'DynamicGradMode'
    class: DynamicGradMode
    doc: Create a copy of this class
  - name: enable_show_time_cost
    signature: ()
  - name: __init__
    signature: (self, name, interval = 0.1, color = 0, indent = 0)
    class: TimeInfo
  - name: check
    signature: (self)
    class: TimeInfo
  - name: pretty_print
    signature: (self)
    class: TimeInfo
  - name: mark_start
    signature: (name, interval = 0.1, color = 0, indent = 0)
  - name: mark_end
    signature: (name)
  - name: calculate_time
    signature: (show = False, min_cost_ms = 0.0)
  - name: wrapper
    signature: (func)
  - name: inner_func
    signature: (*args, **kwargs)
  - name: get_available_gpu_memory
    signature: (device, gpu_id, distributed = False, empty_cache = True, cpu_group = None)
    doc: Get available memory for cuda:gpu_id device.
  - name: is_pin_memory_available
    signature: ()
    return: bool
  - name: __call__
    signature: (self, layer_id: int, prefix: str)
    return: torch.nn.Module
    class: LayerFn
  - name: make_layers
    signature: (num_hidden_layers: int, layer_fn: LayerFn, pp_rank: Optional[int] = None, pp_size: Optional[int] = None, prefix: str = '', return_tuple: bool = False, offloader_kwargs: Dict[str, Any] = {})
    return: Tuple[int, int, torch.nn.ModuleList]
    doc: Make a list of layers with the given layer function
  - name: set_random_seed
    signature: (seed: int)
    return: None
    doc: Set the random seed for all libraries.
  - name: find_process_using_port
    signature: (port: int)
    return: Optional[psutil.Process]
  - name: wait_port_available
    signature: (port: int, port_name: str, timeout_s: int = 30, raise_exception: bool = True)
    return: bool
  - name: is_port_available
    signature: (port)
    doc: Return whether a port is available.
  - name: get_free_port
    signature: ()
  - name: decode_video_base64
    signature: (video_base64)
  - name: load_audio
    signature: (audio_file: str, sr: Optional[int] = None, mono: bool = True)
    return: np.ndarray
  - name: load_image
    signature: (image_file: Union[Image.Image, str, ImageData, bytes])
    return: tuple[Image.Image, tuple[int, int]]
  - name: load_video
    signature: (video_file: Union[str, bytes], use_gpu: bool = True)
  - name: suppress_other_loggers
    signature: ()
  - name: assert_pkg_version
    signature: (pkg: str, min_version: str, message: str)
  - name: kill_process_tree
    signature: (parent_pid, include_parent: bool = True, skip_pid: int = None)
    doc: Kill the process and all its child processes.
  - name: monkey_patch_p2p_access_check
    signature: ()
    doc: Monkey patch the slow p2p access check.
  - name: monkey_patch_vllm_gguf_config
    signature: ()
  - name: get_quant_method_with_embedding_replaced
    signature: (self, layer: torch.nn.Module, prefix: str)
    return: Optional['QuantizeMethodBase']
  - name: set_ulimit
    signature: (target_soft_limit = 65535)
  - name: add_api_key_middleware
    signature: (app, api_key: str)
  - name: authentication
    signature: (request, call_next)
  - name: prepare_model_and_tokenizer
    signature: (model_path: str, tokenizer_path: str)
  - name: configure_logger
    signature: (server_args, prefix: str = '')
  - name: replace_submodule
    signature: (model: nn.Module, module_name: str, new_module: nn.Module)
    return: nn.Module
    doc: Replace a submodule in a model with a new module.
  - name: set_weight_attrs
    signature: (weight: torch.Tensor, weight_attrs: Optional[Dict[str, Any]])
    doc: Set attributes on a weight tensor.
  - name: broadcast_pyobj
    signature: (data: List[Any], rank: int, dist_group: Optional[torch.distributed.ProcessGroup] = None, src: int = 0, force_cpu_device: bool = True)
    doc: Broadcast inputs from src rank to all other ranks with torch.dist backend.
  - name: point_to_point_pyobj
    signature: (data: List[Any], rank: int, group: Optional[torch.distributed.ProcessGroup] = None, src: int = 0, dst: int = 1)
    doc: Send data from src to dst in group using DeviceToDevice communication.
  - name: pytorch_profile
    signature: (name, func, *args, data_size = -1)
    doc: Args:
  - name: get_zmq_socket
    signature: (context: zmq.Context, socket_type: zmq.SocketType, endpoint: str, bind: bool)
  - name: set_send_opt
    signature: ()
  - name: set_recv_opt
    signature: ()
  - name: dump_to_file
    signature: (dirpath, name, value)
  - name: is_triton_3
    signature: ()
  - name: maybe_torch_compile
    signature: (*args, **kwargs)
    doc: torch.compile does not work for triton 2.2.0, which is needed in xlm1's jax.
  - name: decorator
    signature: (func)
  - name: delete_directory
    signature: (dirpath)
  - name: set_prometheus_multiproc_dir
    signature: ()
  - name: add_prometheus_middleware
    signature: (app)
  - name: bind_port
    signature: (port)
    doc: Bind to a specific port, assuming it's available.
  - name: get_amdgpu_memory_capacity
    signature: ()
  - name: get_device_sm
    signature: ()
  - name: get_nvgpu_memory_capacity
    signature: ()
  - name: get_hpu_memory_capacity
    signature: ()
  - name: get_npu_memory_capacity
    signature: ()
  - name: get_device_memory_capacity
    signature: (device: str = None)
  - name: init_custom_process_group
    signature: (backend = None, init_method = None, timeout = None, world_size = -1, rank = -1, store = None, group_name = None, pg_options = None)
  - name: crash_on_warnings
    signature: ()
  - name: print_warning_once
    signature: (msg: str)
    return: None
  - name: print_info_once
    signature: (msg: str)
    return: None
  - name: get_device_name
    signature: (device_id: int = 0)
    return: str
  - name: is_habana_available
    signature: ()
    return: bool
  - name: get_device
    signature: (device_id: Optional[int] = None)
    return: str
  - name: get_device_count
    signature: ()
    return: int
  - name: get_device_core_count
    signature: (device_id: int = 0)
    return: int
  - name: get_device_capability
    signature: (device_id: int = 0)
    return: Tuple[int, int]
  - name: get_npu_compiler_config
    signature: ()
  - name: get_compiler_backend
    signature: ()
    return: str
  - name: supports_custom_op
    signature: ()
    return: bool
  - name: direct_register_custom_op
    signature: (op_name: str, op_func: Callable, mutates_args: List[str], fake_impl: Optional[Callable] = None, target_lib: Optional[Library] = None)
    doc: `torch.library.custom_op` can have significant overhead because it
  - name: set_gpu_proc_affinity
    signature: (tp_size: int, nnodes: int, gpu_id: int)
  - name: disable_request_logging
    signature: ()
    return: bool
  - name: dataclass_to_string_truncated
    signature: (data, max_length = 2048, skip_names: Optional[Set[str]] = None)
  - name: permute_weight
    signature: (x: torch.Tensor)
    return: torch.Tensor
  - name: serialize
    signature: (obj, output_str: bool = False)
    class: MultiprocessingSerializer
    doc: Serialize a Python object using ForkingPickler.
  - name: deserialize
    signature: (data)
    class: MultiprocessingSerializer
    doc: Deserialize a previously serialized object.
  - name: debug_timing
    signature: (func)
  - name: wrapper
    signature: (*args, **kwargs)
  - name: nullable_str
    signature: (val: str)
  - name: pyspy_dump_schedulers
    signature: ()
    doc: py-spy dump on all scheduler in a local node.
  - name: kill_itself_when_parent_died
    signature: ()
  - name: set_uvicorn_logging_configs
    signature: ()
  - name: get_ip
    signature: ()
    return: str
  - name: get_open_port
    signature: ()
    return: int
  - name: is_valid_ipv6_address
    signature: (address: str)
    return: bool
  - name: maybe_wrap_ipv6_address
    signature: (address: str)
    return: str
  - name: format_tcp_address
    signature: (ip: str, port: int)
    return: str
  - name: configure_ipv6
    signature: (dist_init_addr)
  - name: launch_dummy_health_check_server
    signature: (host, port, enable_metrics)
  - name: health
    signature: ()
    doc: Check the health of the http server.
  - name: health_generate
    signature: ()
    doc: Check the health of the http server.
  - name: create_checksum
    signature: (directory: str)
  - name: set_cuda_arch
    signature: ()
  - name: next_power_of_2
    signature: (n: int)
  - name: round_up
    signature: (x: int, y: int)
    return: int
  - name: __enter__
    signature: (self)
    class: EmptyContextManager
  - name: __exit__
    signature: (self, exc_type, exc_value, traceback)
    class: EmptyContextManager
  - name: empty_context
    signature: (*args, **kwargs)
  - name: add_prefix
    signature: (name: str, prefix: str)
    return: str
    doc: Add a weight path prefix to a module name.
  - name: is_remote_url
    signature: (url: Union[str, Path])
    return: bool
    doc: Check if the URL is a remote URL of the format:
  - name: parse_connector_type
    signature: (url: str)
    return: str
    doc: Parse the connector type from the URL of the format:
  - name: retry
    signature: (fn, max_retry: int, initial_delay: float = 2.0, max_delay: float = 60.0, should_retry: Callable[[Any], bool] = lambda e: True)
  - name: flatten_nested_list
    signature: (nested_list)
  - name: is_non_idle_and_non_empty
    signature: (forward_mode, hidden_states)
  - name: fast_topk
    signature: (values, topk, dim)
  - name: bind_or_assign
    signature: (target, source)
  - name: get_local_ip_auto
    signature: ()
    return: str
  - name: get_local_ip_by_nic
    signature: (interface: str)
    return: str
  - name: get_local_ip_by_remote
    signature: ()
    return: str
  - name: is_page_size_one
    signature: (server_args)
  - name: is_no_spec_infer_or_topk_one
    signature: (server_args)
  - name: is_fa3_default_architecture
    signature: (hf_config)
  - name: __init__
    signature: (self, buffer_size: int, dtype, device)
    class: BumpAllocator
  - name: allocate
    signature: (self, size: int)
    class: BumpAllocator
  - name: log_info_on_rank0
    signature: (logger, msg)
  - name: load_json_config
    signature: (data: str)
  - name: dispose_tensor
    signature: (x: torch.Tensor)
  - name: __init__
    signature: (self)
    class: Withable
  - name: value
    signature: (self)
    return: T
    class: Withable
  - name: with_value
    signature: (self, new_value: T)
    class: Withable
  - name: require_mlp_tp_gather
    signature: (server_args)
    doc: Check if the input of MLP is obtained by all-gather rather than all-reduce. This only happens when each MLP TP group contains multiple attention DP groups.
  - name: require_attn_tp_gather
    signature: (server_args)
    doc: Check if the input of attention is scattered.
  - name: require_gathered_buffer
    signature: (server_args)
  - name: require_mlp_sync
    signature: (server_args)
  - name: find_local_repo_dir
    signature: (repo_id: str, revision: Optional[str] = None)
    return: Optional[str]
  - name: read_system_prompt_from_file
    signature: (model_name: str)
    return: str
    doc: Read system prompt from a file in the HuggingFace cache directory.
  - name: bind_or_assign
    signature: (target, source)
  - name: prepack_weight_if_needed
    signature: (weight)
  - name: dim_is_supported
    signature: (weight)
  - name: _process_weight_after_loading
    signature: (module, weight_names, transpose_dims = None)
    return: None
  - name: __init__
    signature: (self, weight_names, transpose_dims = None)
    class: PackWeightMethod
  - name: process_weights_after_loading
    signature: (self, module)
    return: None
    class: PackWeightMethod
  - name: __init__
    signature: (self, creator: Callable)
    class: LazyValue
  - name: value
    signature: (self)
    class: LazyValue
  - name: dynamic_import
    signature: (func_path: str)
  - name: gc_object_counts
    signature: ()
  - name: configure_gc_warning
    signature: (warn_threshold_secs)
  - name: gc_callback
    signature: (phase, info)
  - name: freeze_gc
    signature: (context: str)
  - name: configure_gc_logger
    signature: ()
  - name: gc_callback
    signature: (phase, info)
  - name: align
    signature: (x: int, y: int)
    return: int
  - name: ceil_div
    signature: (x: int, y: int)
    return: int
  - name: parse_lscpu_topology
    signature: ()
  - name: get_physical_cpus_by_numa
    signature: ()
  - name: get_cpu_ids_by_node
    signature: ()
  - name: is_shm_available
    signature: (dtype, world_size, local_size)
  - name: lru_cache_frozenset
    signature: (maxsize = 128)
  - name: _to_hashable
    signature: (o)
  - name: decorator
    signature: (func)
  - name: wrapper
    signature: (*args, **kwargs)
  - name: apply_module_patch
    signature: (target_module, target_function, wrappers)
  - name: parse_module_path
    signature: (module_path, function_name, create_dummy)
  - name: create_dummy_module
    signature: (full_path, parent = None)
    doc: Create and register a placeholder module
  - name: create_placeholder_function
    signature: (func_name)
    doc: Create dummy function that raises when called
  - name: placeholder
    signature: (*args, **kwargs)
  - name: mxfp_supported
    signature: ()
    doc: Returns whether the current platform supports MX types.
  - name: __init__
    signature: (self, initial: int = 0)
    class: ConcurrentCounter
    doc: Initialize the counter with an optional initial value.
  - name: value
    signature: (self)
    return: int
    class: ConcurrentCounter
    doc: Return the current value of the counter.
  - name: __repr__
    signature: (self)
    return: str
    class: ConcurrentCounter
    doc: Return an informative string representation of the counter.
  - name: increment
    signature: (self, n: int = 1, notify_all: bool = True)
    class: ConcurrentCounter
    doc: Atomically increment the counter by a given amount and notify all waiters.
  - name: decrement
    signature: (self, n: int = 1, notify_all: bool = True)
    class: ConcurrentCounter
    doc: Atomically decrement the counter by a given amount and notify all waiters.
  - name: wait_for
    signature: (self, condition: Callable[[int], bool])
    class: ConcurrentCounter
    doc: Asynchronously wait until the counter satisfies a given condition.
  - name: wait_for_zero
    signature: (self)
    class: ConcurrentCounter
    doc: Asynchronously wait until the counter reaches zero.
  - name: is_triton_kernels_available
    signature: ()
    return: bool
  - name: check_cuda_result
    signature: (raw_output)

File: warmup.py
  - name: warmup
    signature: (name: str)
    return: callable
  - name: decorator
    signature: (fn: callable)
  - name: execute_warmups
    signature: (disaggregation_mode: str, warmup_names: List[str], tokenizer_manager: TokenizerManager)
  - name: voice_chat
    signature: (disaggregation_mode: str, tokenizer_manager: TokenizerManager)

File: weight_sync/tensor_bucket.py
  - name: __init__
    signature: (self, named_tensors: List[Tuple[str, torch.Tensor]] = None, flattened_tensor: torch.Tensor = None, metadata: List[FlattenedTensorMetadata] = None)
    class: FlattenedTensorBucket
    doc: Initialize a tensor bucket from a list of named tensors OR from pre-flattened data.
  - name: get_flattened_tensor
    signature: (self)
    return: torch.Tensor
    class: FlattenedTensorBucket
    doc: Get the flattened tensor containing all bucket tensors
  - name: get_metadata
    signature: (self)
    return: List[FlattenedTensorMetadata]
    class: FlattenedTensorBucket
    doc: Get metadata for all tensors in the bucket
  - name: reconstruct_tensors
    signature: (self)
    return: List[Tuple[str, torch.Tensor]]
    class: FlattenedTensorBucket
    doc: Reconstruct original tensors from flattened tensor with optimized performance.

File: weight_sync/utils.py
  - name: update_weights
    signature: (engine: Engine, params_batch: list[tuple[str, torch.Tensor]], device_mesh_key: str, device_mesh: DeviceMesh, load_format: Optional[str] = None)
    doc: Update weights for the inference engine.
  - name: _preprocess_tensor_for_update_weights
    signature: (tensor: torch.Tensor)
    doc: Preprocess the tensor for update weights.
