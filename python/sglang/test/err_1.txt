We have:
scheduler with memory leak (definitely)

[2025-11-15 03:53:53] token_to_kv_pool_allocator memory leak detected! self.max_total_num_tokens=1041492, available_size=1041486, evictable_size=13, protected_size=0


On the other hand what is more interesting is that if we suppress the RuntimeError we get the following result:

Where the shape mismatch only happens after several rounds of decoding:
[2025-11-15 03:53:54] Prefill batch, #new-seq: 1, #new-token: 19, #cached-token: 0, token usage: -0.00, #running-req: 0, #queue-req: 0, 
[2025-11-15 03:53:54] Decode batch, #running-req: 1, #token: 22, token usage: 0.00, accept len: 1.00, accept rate: 0.12, cuda graph: True, gen throughput (token/s): 0.78, #queue-req: 0, 
[2025-11-15 03:53:54] Decode batch, #running-req: 1, #token: 22, token usage: 0.00, accept len: 1.00, accept rate: 0.12, cuda graph: True, gen throughput (token/s): 101.24, #queue-req: 0, 
[2025-11-15 03:53:55] Decode batch, #running-req: 1, #token: 22, token usage: 0.00, accept len: 1.00, accept rate: 0.12, cuda graph: True, gen throughput (token/s): 101.21, #queue-req: 0, 
[2025-11-15 03:53:55] Decode batch, #running-req: 1, #token: 22, token usage: 0.00, accept len: 1.00, accept rate: 0.12, cuda graph: True, gen throughput (token/s): 100.73, #queue-req: 0, 
[2025-11-15 03:53:56] Decode batch, #running-req: 1, #token: 22, token usage: 0.00, accept len: 1.00, accept rate: 0.12, cuda graph: True, gen throughput (token/s): 100.71, #queue-req: 0, 
[2025-11-15 03:53:56] Decode batch, #running-req: 1, #token: 22, token usage: 0.00, accept len: 1.00, accept rate: 0.12, cuda graph: True, gen throughput (token/s): 100.63, #queue-req: 0, 
[2025-11-15 03:53:56] Decode batch, #running-req: 1, #token: 22, token usage: 0.00, accept len: 1.00, accept rate: 0.12, cuda graph: True, gen throughput (token/s): 100.38, #queue-req: 0, 
[2025-11-15 03:53:57] Decode batch, #running-req: 1, #token: 22, token usage: 0.00, accept len: 1.00, accept rate: 0.12, cuda graph: True, gen throughput (token/s): 100.68, #queue-req: 0, 
[2025-11-15 03:53:57] Decode batch, #running-req: 1, #token: 22, token usage: 0.00, accept len: 1.00, accept rate: 0.12, cuda graph: True, gen throughput (token/s): 100.47, #queue-req: 0, 
[2025-11-15 03:53:58] Decode batch, #running-req: 1, #token: 22, token usage: 0.00, accept len: 1.00, accept rate: 0.12, cuda graph: True, gen throughput (token/s): 100.56, #queue-req: 0, 
[2025-11-15 03:53:58] Decode batch, #running-req: 1, #token: 22, token usage: 0.00, accept len: 1.00, accept rate: 0.12, cuda graph: True, gen throughput (token/s): 100.51, #queue-req: 0, 
[2025-11-15 03:53:58] Decode batch, #running-req: 1, #token: 22, token usage: 0.00, accept len: 1.00, accept rate: 0.12, cuda graph: True, gen throughput (token/s): 100.45, #queue-req: 0, 
[2025-11-15 03:53:59] Scheduler hit an exception: Traceback (most recent call last):
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2727, in run_scheduler_process
    scheduler.event_loop_overlap()
  File "/sgl-workspace/sglang/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 1021, in event_loop_overlap
    batch_result = self.run_batch(batch)
                   ^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py", line 2023, in run_batch
    batch_result = self.model_worker.forward_batch_generation(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/speculative/eagle_worker_v2.py", line 557, in forward_batch_generation
    self.draft_worker._draft_extend_for_decode(model_worker_batch, batch_output)
  File "/sgl-workspace/sglang/python/sglang/srt/speculative/eagle_worker_v2.py", line 465, in _draft_extend_for_decode
    draft_logits_output = self.draft_runner.model.forward(
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/models/llama.py", line 469, in forward
    hidden_states = self.model(
                    ^^^^^^^^^^^
  File "/sgl-workspace/sglang/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/models/llama.py", line 342, in forward
    hidden_states, residual = layer(
                              ^^^^^^
  File "/sgl-workspace/sglang/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/models/llama.py", line 266, in forward
    hidden_states = self.self_attn(
                    ^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/models/llama.py", line 197, in forward
    attn_output = self.attn(q, k, v, forward_batch)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/layers/radix_attention.py", line 123, in forward
    return forward_batch.attn_backend.forward(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/layers/attention/base_attn_backend.py", line 101, in forward
    return self.forward_extend(
           ^^^^^^^^^^^^^^^^^^^^
  File "/sgl-workspace/sglang/python/sglang/srt/layers/attention/flashinfer_backend.py", line 788, in forward_extend
    forward_batch.token_to_kv_pool.set_kv_buffer(
  File "/sgl-workspace/sglang/python/sglang/srt/mem_cache/memory_pool.py", line 808, in set_kv_buffer
    self.k_buffer[layer_id - self.start_layer][loc] = cache_k
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^
RuntimeError: shape mismatch: value tensor of shape [2, 8, 128] cannot be broadcast to indexing result of shape [8, 8, 128]

[2025-11-15 03:53:59] SIGQUIT received. signum=None, frame=None. It usually means one child failed.
