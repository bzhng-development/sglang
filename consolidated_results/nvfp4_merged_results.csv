model_analyzed,command,hardware,purpose,result,status,issue_number,issue_title
nvidia/Kimi-K2-Thinking-NVFP4,python -m sglang.launch_server --model-path /models/kimi-nvpf4-modelopt --tp 8 --trust-remote-code --tool-call-parser kimi_k2 --reasoning-parser kimi_k2 --enable-metrics --enable-priority-scheduling --schedule-low-priority-values-first --log-requests --log-requests-level=1 --enable-request-time-stats-logging --priority-scheduling-preemption-threshold=1000 --port=8000 --host=0.0.0.0,8xB200,Serve a custom NVFP4 quantized version of Kimi-K2-Thinking generated via TensorRT ModelOpt.,"Broken. Encountered a fatal CUDA error: 'Check failed: t.dtype() == at::kBFloat16 (c10::Half vs. BFloat16)' during CUDA graph capture, followed by a 'Rank 0 scheduler is dead' and EOFError.",broken,https://github.com/sgl-project/sglang/issues/14677,[Bug] Cannot serve kimi k2 thinking nvfp4 checkpoint quantized with tensorrt modelopt
nvidia/Kimi-K2-Thinking-NVFP4,SGLANG_NVFP4_CKPT_FP8_GEMM_IN_ATTN=1 python -m sglang.launch_server --model-path /models/kimi-nvpf4-modelopt --tp 8 --trust-remote-code --tool-call-parser kimi_k2 --reasoning-parser kimi_k2 --enable-metrics --enable-priority-scheduling --schedule-low-priority-values-first --log-requests --log-requests-level=1 --enable-request-time-stats-logging --priority-scheduling-preemption-threshold=1000 --port=8000 --host=0.0.0.0,8xB200,Debug the dtype mismatch error by forcing FP8 GEMM in attention for the NVFP4 checkpoint.,Broken. The user reported: 'seems like the same error'.,broken,https://github.com/sgl-project/sglang/issues/14677,[Bug] Cannot serve kimi k2 thinking nvfp4 checkpoint quantized with tensorrt modelopt
nvidia/Kimi-K2-Thinking-NVFP4,python -m sglang.launch_server --model-path nvidia/Kimi-K2-Thinking-NVFP4 --tp 8 --trust-remote-code --tool-call-parser kimi_k2 --reasoning-parser kimi_k2 --enable-metrics --enable-priority-scheduling --schedule-low-priority-values-first --log-requests --log-requests-level=1 --enable-request-time-stats-logging --priority-scheduling-preemption-threshold=1000 --port=8000 --host=0.0.0.0,8xB200,Test the official NVIDIA NVFP4 checkpoint performance in SGLang.,"Working but underperforming. The user observed that model performance was 'pretty slow', even slower than the base BF16 checkpoint. Profiling showed significant time spent in Cutlass kernels compared to other checkpoints.",working,https://github.com/sgl-project/sglang/issues/14677,[Bug] Cannot serve kimi k2 thinking nvfp4 checkpoint quantized with tensorrt modelopt
nvidia/Llama-3.1-8B-Instruct-NVFP4,python3 -m sglang.bench_one_batch --model-path nvidia/Llama-3.1-8B-Instruct-NVFP4 --quantization modelopt_fp4 --batch-size 1 --input-len 1024 --output-len 1024 --kv-cache-dtype nvfp4_e2m1,"Mention of sm100 (Blackwell) requirement for certain kernels, but specific test GPU not explicitly named.",Reference testing command for the new NVFP4 KV cache implementation. Evaluates performance and functionality of fp4 quantization and kv-cache-dtype using modelopt_fp4.,Status is described as 'extremely slow' because the current implementation dequantizes from fp4 to bf16 every time. The user also reports a 'RuntimeError: [FP4 gemm Runner] Failed to initialize cutlass FP4 gemm on sm100. Error: Error Internal' related to TMA descriptor initialization.,broken,https://github.com/sgl-project/sglang/pull/15133,Add NVFP4-style KV Cache
nvidia/Llama-4-Scout-17B-16E-Instruct-NVFP4,"python -c 'import sglang as sgl; llm = sgl.Engine(model_path=""/home/scratch.omniml_data_2/HF_model_hub/Llama-4-Scout-17B-16E-Instruct-FP4"", quantization=""modelopt_fp4"", tp_size=4, context_length=1024, attention_backend=""flashinfer""); prompts = [""Hello, my name is"", ""The president of the United States is"", ""The capital of France is"", ""The future of AI is""]; sampling_params = {""temperature"": 0.7, ""top_p"": 0.9, ""max_new_tokens"": 128, ""skip_special_tokens"": True}; outputs = llm.generate(prompts, sampling_params); [print(f""Prompt: {p}\nGenerated: {o[\""text\""]}\n"") for p, o in zip(prompts, outputs)]'",Not specified (requires at least 4 GPUs for tp_size=4),"Accuracy testing for new ModelOpt NVFP4 workflow support in SGLang, specifically testing weight/scale loading logic for Llama 4 Scout. Uses flashinfer backend as required for Llama 4.","Working. Successfully loaded the model and generated coherent text outputs for various prompts (e.g., correctly identified Paris as the capital of France and generated contextually relevant responses for other prompts).",working,https://github.com/sgl-project/sglang/pull/9526,Support modelopt llama4 nvfp4 workflow and fix issues
nvidia/Llama-4-Scout-17B-16E-Instruct-NVFP4,python3 -m sglang.launch_server --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --tp=8 --trust-remote-code --mem-fraction-static 0.7 --context-length=131072 --attention-backend=fa3 --enable-multimodal --tool-call-parser=pythonic --cuda-graph-max-bs=48 --host=0.0.0.0 --port=8000,"Not explicitly stated, but includes --tp=8","Testing model deployment with FlashInfer-3 (fa3) backend, multimodal support, and tool calling.",FAILED: ValueError: Unknown quantization method: modelopt. The server failed to recognize the default 'modelopt' quantization associated with the FP8 checkpoint due to missing registration in the code at that time.,broken,https://github.com/sgl-project/sglang/pull/10154,Enable native ModelOpt quantization support (3/3)
nvidia/Llama-4-Scout-17B-16E-Instruct-NVFP4,python3 -m sglang.launch_server --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --tp=8 --trust-remote-code --mem-fraction-static 0.7 --context-length=131072 --attention-backend=triton --enable-multimodal --tool-call-parser=pythonic --cuda-graph-max-bs=48 --host=0.0.0.0 --port=8000 --quantization=modelopt_fp8,"Not explicitly stated, but includes --tp=8",Testing explicit quantization flag 'modelopt_fp8' using Triton backend.,FAILED: launch_server.py: error: argument --quantization: invalid choice: 'modelopt_fp8'. The argument was not yet accepted by the CLI parser in the current build.,broken,https://github.com/sgl-project/sglang/pull/10154,Enable native ModelOpt quantization support (3/3)
nvidia/Llama-4-Scout-17B-16E-Instruct-NVFP4,python3 -m sglang.launch_server --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --tp=8 --trust-remote-code --mem-fraction-static 0.7 --context-length=131072 --attention-backend=triton --enable-multimodal --tool-call-parser=pythonic --cuda-graph-max-bs=48 --host=0.0.0.0 --port=8000 --quantization=modelopt,"Not explicitly stated, but includes --tp=8",Testing explicit quantization flag 'modelopt' (legacy/auto-detect) using Triton backend.,FAILED: ValueError: Unknown quantization method: modelopt. The server failed to map the 'modelopt' string to the internal quantization config class.,broken,https://github.com/sgl-project/sglang/pull/10154,Enable native ModelOpt quantization support (3/3)
nvidia/NVIDIA-Nemotron-Nano-9B-v2-NVFP4,python -m sglang.test.gsm8k --model nvidia/NVIDIA-Nemotron-Nano-9B-v2-NVFP4 --max-mamba-cache-size 256,NVIDIA Blackwell (implied by requirements for NVFP4),Unit testing GSM8K benchmark accuracy for the NVFP4 quantized version of the Nemotron-Nano-9B-v2 model.,Expected accuracy: 0.855. The test is modularized to use the GSM8KMixin.,working,https://github.com/sgl-project/sglang/pull/13506,modularize gsm8k and mmmu test classes
nvidia/NVIDIA-Nemotron-Nano-9B-v2-NVFP4,python3 -m sglang.launch_server --model-path nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8--max-mamba-cache-size 256,,Benchmark GSM8K performance on the FP8 variant of Nemotron-Nano-9B-v2 using SGLang.,GSM8K score: 0.881 ± 0.008. Equivalent to vLLM performance.,working,https://github.com/sgl-project/sglang/pull/12018,Feature/nano v2 offline modelopt fp8 and nvfp4
nvidia/NVIDIA-Nemotron-Nano-9B-v2-NVFP4,vllm serve nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8 --trust-remote-code --max-num-seqs 512 --mamba_ssm_cache_dtype float32,,Benchmark GSM8K performance on the FP8 variant of Nemotron-Nano-9B-v2 using vLLM.,GSM8K score: 0.892 ± 0.008.,working,https://github.com/sgl-project/sglang/pull/12018,Feature/nano v2 offline modelopt fp8 and nvfp4
nvidia/NVIDIA-Nemotron-Nano-9B-v2-NVFP4,"python -m lm_eval --model local-chat-completions --model_args ""model=nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8,base_url=http://localhost:8000/v1/chat/completions,api_key=EMPTY,num_concurrent=256,timeout=3600"" --tasks gsm8k --apply_chat_template --system_instruction ""/no_think"" --gen_kwargs temperature=0.6,top_p=0.95,do_sample=true,seed=1 --output_path ./results/$(date +%s) --log_samples",,Run lm-eval GSM8K benchmark against the launched model server to verify accuracy.,Produced benchmark results for comparison between SGLang and vLLM implementations.,working,https://github.com/sgl-project/sglang/pull/12018,Feature/nano v2 offline modelopt fp8 and nvfp4
nvidia/NVIDIA-Nemotron-Nano-9B-v2-NVFP4,python3 -m sglang.launch_server --model-path nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8 --max-mamba-cache-size 256,Mention of Blackwell GPU runner (4-gpu setup) and 1-gpu setups in comments,Serve the FP8 quantized version of Nemotron-Nano-9B-v2 using SGLang to benchmark performance against vLLM.,"Working. Achieved a GSM8K score of 0.881 ± 0.008, which is basically equivalent to vLLM's 0.892 ± 0.008.",working,https://github.com/sgl-project/sglang/pull/11866,Support nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8/NVFP4
nvidia/NVIDIA-Nemotron-Nano-9B-v2-NVFP4,vllm serve nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8 --trust-remote-code --max-num-seqs 512 --mamba_ssm_cache_dtype float32,Mention of Blackwell GPU runner in comments,Serve the FP8 quantized version of Nemotron-Nano-9B-v2 using vLLM for comparison benchmarking.,Working. Achieved a GSM8K score of 0.892 ± 0.008.,working,https://github.com/sgl-project/sglang/pull/11866,Support nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8/NVFP4
nvidia/NVIDIA-Nemotron-Nano-9B-v2-NVFP4,"python -m lm_eval --model local-chat-completions --model_args ""model=nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8,base_url=http://localhost:8000/v1/chat/completions,api_key=EMPTY,num_concurrent=256,timeout=3600"" --tasks gsm8k --apply_chat_template --system_instruction ""/no_think"" --gen_kwargs temperature=0.6,top_p=0.95,do_sample=true,seed=1 --output_path ./results/$(date +%s) --log_samples",,Run GSM8K accuracy evaluation using lm-evaluation-harness against the served model endpoint.,SGLang (FP8) score: 0.881 ± 0.008; NVFP4 variant score: 0.899 ± 0.008.,working,https://github.com/sgl-project/sglang/pull/11866,Support nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8/NVFP4
nvidia/Qwen2.5-VL-7B-Instruct-NVFP4,python3 -m sglang.launch_server --model-path /mnt/data/models/Qwen2.5-VL-7B-Instruct-NVFP4 --host 0.0.0.0 --port 30000 --disable-radix-cache --chunked-prefill-size 1024 --max-running-requests 20 --cuda-graph-max-bs 10 --max-total-tokens 32000 --mem-fraction-static 0.75 --moe-runner-backend flashinfer_cutlass --quantization modelopt_fp4 --attention-backend triton,"Not explicitly stated (though log shows ~90GB available memory, suggesting A100/H100 class hardware)",Attempting to launch the Qwen2.5-VL-7B-Instruct-NVFP4 model with SGLang using FP4 quantization settings.,"BROKEN: Encountered an AssertionError during weight loading: `AssertionError: param_data.shape=torch.Size([1280, 640]), loaded_weight.shape=torch.Size([1280, 1280])`. The user suspects the vision module is being read incorrectly as FP8 instead of FP4, leading to a shape mismatch.",broken,https://github.com/sgl-project/sglang/issues/13821,[Feature] Does it support running VL models like Qwen3-VL with nvfp4 or other quantizations?
nvidia/Qwen3-30B-A3B-NVFP4,pytest test/registered/backends/test_qwen3_fp4_trtllm_gen_moe.py,nightly-1-gpu,Nightly CI test for nvidia/Qwen3-30B-A3B-NVFP4 using the TRT-LLM backend with FP4 MoE kernels.,Migration of the test file to a new directory structure; the test is registered in the CI suite 'nightly-1-gpu'. The PR description indicates this test covers the FP4 TRT-LLM gen MoE backend for Qwen3.,working,https://github.com/sgl-project/sglang/pull/15582,[CI] Migrate nightly tests to test/registered/
nvidia/Qwen3-30B-A3B-NVFP4,python3 -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-FP8 --tp 2 --disable-radix-cache --kv-cache-dtype fp8_e4m3,B200,Accuracy and throughput testing for Qwen3-30B-FP8 on Blackwell architecture to verify the refactored deepgemm requantization logic.,"Working. Accuracy: 0.696, Invalid: 0.001, Latency: 56.326 s, Output throughput: 6606.698 token/s. Compared to 'Main' branch which failed with 0.000 accuracy and 100% invalid results.",working,https://github.com/sgl-project/sglang/pull/13601,[1/2] Refactor DeepGeem requant for FP8 Linear on Blackwell
nvidia/Qwen3-30B-A3B-NVFP4,python3 benchmark/gsm8k/bench_sglang.py --num-questions 2000 --parallel 2000 --num-shots 8 --port 30000,,Benchmark GSM8K accuracy and throughput using the Cutlass FP4 MoE backend (Before state).,"Accuracy: 0.907, Latency: 178.658 s, Output throughput: 958.693 token/s.",working,https://github.com/sgl-project/sglang/pull/13556,[NVIDIA] Enable trtllm fp4 moe for qwen
nvidia/Qwen3-30B-A3B-NVFP4,python3 benchmark/gsm8k/bench_sglang.py --num-questions 2000 --parallel 2000 --num-shots 8 --port 30000,,Benchmark GSM8K accuracy and throughput using the FlashInfer TRT-LLM FP4 MoE backend (After state).,"Accuracy: 0.900, Latency: 162.700 s, Output throughput: 1035.904 token/s. Performance improved by ~8%.",working,https://github.com/sgl-project/sglang/pull/13556,[NVIDIA] Enable trtllm fp4 moe for qwen
nvidia/Qwen3-30B-A3B-NVFP4,python3 -m sglang.launch_server --model-path nvidia/Qwen3-30B-A3B-NVFP4 --trust-remote-code --disable-radix-cache --max-running-requests 256 --chunked-prefill-size 1024 --mem-fraction-static 0.89 --max-prefill-tokens 16384 --quantization modelopt_fp4,,Launch server with Cutlass FP4 MoE backend for testing and benchmarking.,Served as the baseline for performance comparison.,working,https://github.com/sgl-project/sglang/pull/13556,[NVIDIA] Enable trtllm fp4 moe for qwen
nvidia/Qwen3-30B-A3B-NVFP4,python3 -m sglang.launch_server --model-path nvidia/Qwen3-30B-A3B-NVFP4 --trust-remote-code --disable-radix-cache --max-running-requests 256 --chunked-prefill-size 1024 --mem-fraction-static 0.89 --max-prefill-tokens 16384 --quantization modelopt_fp4 --moe-runner-backend flashinfer_trtllm,,Launch server with the newly enabled FlashInfer TRT-LLM FP4 MoE backend.,"Functional, providing higher throughput compared to Cutlass backend.",working,https://github.com/sgl-project/sglang/pull/13556,[NVIDIA] Enable trtllm fp4 moe for qwen
nvidia/Qwen3-30B-A3B-NVFP4,SGL_ENABLE_JIT_DEEPGEMM=false /opt/conda310/bin/python -m sglang.launch_server --model-path /workspace/Qwen3-30B-A3B-NVFP4 --moe-runner-backend flashinfer_trtllm --quantization modelopt_fp4 --trust-remote-code --disable-radix-cache --port 6677 --mem-fraction-static 0.89 --max-running-requests 1024 --chunked-prefill-size 16384 --max-prefill-tokens 16384 --attention-backend trtllm_mha,GB200,Test SGLang v0.5.6 on the GB200 platform with the FlashInfer TRT-LLM MoE and MHA backend.,FAILED: The model produced corrupted/incorrect output (garbled characters/tokens like ''). User reported an e2e_latency of 0.20s for 32 tokens but wrong answers.,broken,https://github.com/sgl-project/sglang/pull/13556,[NVIDIA] Enable trtllm fp4 moe for qwen
nvidia/Qwen3-30B-A3B-NVFP4,python -m sglang.launch_server --model-path nvidia/Qwen3-30B-A3B-NVFP4 --moe-runner-backend flashinfer_trtllm --quantization modelopt_fp4 --trust-remote-code,,Verify fix for corrupted output issues on the tip of the main branch.,"Working. Successfully generated clear, coherent text (FastAPI code example) during testing with `sglang.test.send_one`.",working,https://github.com/sgl-project/sglang/pull/13556,[NVIDIA] Enable trtllm fp4 moe for qwen
nvidia/Qwen3-32B-NVFP4,"import sglang as sgl
if __name__ == '__main__':
  llm = sgl.Engine(model_path=""RedHatAI/Qwen3-32B-NVFP4A16"",trust_remote_code=True)

  prompts = [
    ""Hello, my name is"",
    ""The president of the United States is"",
    ""The capital of France is"",
    ""The future of AI is"",
  ]

  sampling_params = {""temperature"": 0.8, ""top_p"": 0.95}

  outputs = llm.generate(prompts, sampling_params)
  for prompt, output in zip(prompts, outputs):
    print(""==============================="")
    print(f""Prompt: {prompt}\nGenerated text: {output['text']}"")",Not specified (implies Blackwell/NVFP4 compatible hardware),Testing the local loading and inference of the nvfp4 (w4a16) quantized Qwen 3 model using the sglang Engine API to verify the implementation of the new quantization support.,Success. The model loaded properly and produced correct output results according to the author's local testing.,working,https://github.com/sgl-project/sglang/pull/8574,[feat] Support nvfp4 (w4a16fp4) quantization method
