model_analyzed,command,hardware,purpose,result,status,issue_number,issue_title
nvidia/Llama-3.1-405B-Instruct-FP8,python3 examples/runtime/openai_tool_choice.py,8x NVIDIA H100 80GB HBM3,Testing official sglang function-calling behavior for the Llama-3.1-405B-Instruct-FP8 model using a non-stream OpenAI-compatible client request.,"Broken. The model returns a ChatCompletion object with 'content=None' and the tool call information in 'tool_calls', but when the tool result is provided back to the model for the final response, it fails to generate the actual text response (inconsistent with OpenAI behavior).",broken,https://github.com/sgl-project/sglang/issues/4072,[Bug] Tool call with Llama3 models has inconsistent behavior with OpenAI
nvidia/Llama-3.1-405B-Instruct-FP8,"CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python3 -m sglang.launch_server --model-path /data/huggingface/Llama-3.1-405B-Instruct-FP8 --trust-remote-code --tp 8","8x Blackwell (implied by image name and label), used CUDA_VISIBLE_DEVICES 0-7",Testing the latest lmsysorg/sglang:blackwell docker image with the Llama-3.1-405B-Instruct-FP8 model.,Broken: Failed with 'Exception: Capture CUDA graph failed: CUDA error: operation failed due to a previous error during capture'. User noted that using --disable-cuda-graph resulted in 'really bad' output.,broken,https://github.com/sgl-project/sglang/issues/5338,[Tracker] Blackwell support
nvidia/Llama-3.1-405B-Instruct-FP8,GLOO_SOCKET_IFNAME=eno12399np0 python3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-405B-Instruct-FP8 --tp 16 --nccl-init-addr 172.22.224.17:20000 --nnodes 4 --node-rank 3 --disable-cuda-graph --kv-cache-dtype fp8_e5m2 --chunked-prefill-size 1024 --mem-fraction-static 0.9 --disable-disk-cache,"16xA40 (4 nodes of 4xA40, ~44GB VRAM per GPU)",Attempting to run Llama-3.1-405B FP8 in a distributed setup across 4 nodes for inference.,"BROKEN: Causes Out of Memory (OOM) issues after all safetensors checkpoints have been loaded. The user tried reducing cache and context length, but the OOM persisted even with `--mem-fraction-static 0.9`.",broken,https://github.com/sgl-project/sglang/issues/1439,[Bug] Llama 405B FP8 causes OOM on 16xA40
nvidia/Llama-3.1-405B-Instruct-FP8,python3 -m sglang.launch_server --model-path nvidia/Llama-3.1-405B-Instruct-FP8 --page-size 64 --attention-backend trtllm_mha --tp-size 8 --quantization modelopt_fp8 --kv-cache-dtype bf16,8x NVIDIA B200,Testing model performance with trtllm_mha attention backend and modelopt_fp8 quantization on Blackwell hardware.,Broken. The user explicitly stated it 'Doesn't work'. Output shown in subsequent test commands indicated garbled/nonsensical text generation.,broken,https://github.com/sgl-project/sglang/issues/12352,[Bug] `nvidia/Llama-3.1-405B-Instruct-FP8` is broken
nvidia/Llama-3.1-405B-Instruct-FP8,python3 -m sglang.launch_server --model-path nvidia/Llama-3.1-405B-Instruct-FP8 --page-size 32 --attention-backend trtllm_mha --tp-size 8 --quantization modelopt_fp8 --kv-cache-dtype bf16,8x NVIDIA B200,Debugging by reducing the page-size to 32 to see if it resolves the broken generation issues.,Broken. User stated it 'Does not' work.,broken,https://github.com/sgl-project/sglang/issues/12352,[Bug] `nvidia/Llama-3.1-405B-Instruct-FP8` is broken
nvidia/Llama-3.1-405B-Instruct-FP8,python3 -m sglang.launch_server --model-path nvidia/Llama-3.1-405B-Instruct-FP8 --attention-backend flashinfer --tp-size 8 --quantization modelopt_fp8 --kv-cache-dtype bf16,8x NVIDIA B200,Testing if switching the attention backend to flashinfer resolves the generation issues with the FP8 model.,Broken. User stated it 'Also does not work'.,broken,https://github.com/sgl-project/sglang/issues/12352,[Bug] `nvidia/Llama-3.1-405B-Instruct-FP8` is broken
nvidia/Llama-3.1-405B-Instruct-FP8,python3 -m sglang.launch_server --model-path nvidia/Llama-3.1-405B-Instruct-FP8 --tp-size 8 --quantization modelopt_fp8,8x NVIDIA B200,Follow-up test suggested by another developer (Edwardf0t1) removing specific KV cache and page size flags.,"Broken. Accuracy was 0 on GSM8K benchmark, though the user noted that 'test_send_one' seemed 'semi normal', generation was still considered failed.",broken,https://github.com/sgl-project/sglang/issues/12352,[Bug] `nvidia/Llama-3.1-405B-Instruct-FP8` is broken
nvidia/Llama-3.1-405B-Instruct-FP8,python3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-405B-Instruct-FP8 --tp 16 --nccl-init-addr <head_node_ip>:8000 --nnodes 2 --node-rank 0,"2 nodes, each with 8xH100 80GB HBM3 (Total 16xH100)",Deploying Llama 3.1 405B-fp8 on two nodes using tensor parallelism (tp=16) for multinode inference on a head node.,Broken: Failed after loading the model with a TypeError: LlamaForCausalLM.forward() missing 1 required positional argument: 'attn_metadata' during CUDA graph capture.,broken,https://github.com/sgl-project/sglang/issues/1012,[Bug] Multinode Llama 3.1 405B fp8
nvidia/Llama-3.1-405B-Instruct-FP8,python3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-405B-Instruct-FP8 --tp 16 --nccl-init-addr <head_node_ip>:8000 --nnodes 2 --node-rank 1,"2 nodes, each with 8xH100 80GB HBM3 (Total 16xH100)",Deploying Llama 3.1 405B-fp8 on two nodes using tensor parallelism (tp=16) for multinode inference on a worker node.,Broken: Failed with the same TypeError regarding 'attn_metadata' during forward pass initialization.,broken,https://github.com/sgl-project/sglang/issues/1012,[Bug] Multinode Llama 3.1 405B fp8
nvidia/Llama-3.1-405B-Instruct-FP8,python3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-405B-Instruct-FP8 --tp 8,1 node with 8xH100 80GB HBM3,Testing Llama 3.1 405B-fp8 on a single node setup to debug the multinode failure.,Broken: Encountered the same TypeError: LlamaForCausalLM.forward() missing 1 required positional argument: 'attn_metadata'. Issue was later attributed to version mismatches and building vLLM from source.,broken,https://github.com/sgl-project/sglang/issues/1012,[Bug] Multinode Llama 3.1 405B fp8
nvidia/Llama-3.1-405B-Instruct-FP8,"CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-405B-Instruct-FP8 --tp 8 --mem-fraction-static 0.8 --port 8001",8x NVIDIA A100-SXM4-80GB,Benchmark performance of a local live server using batch requests via a Python script to compare against the offline engine.,Working. Achieved approximately 279.16 to 340.79 tokens per second for a batch of 500 requests. Total time ranged from 143.36 to 176.35 seconds.,working,https://github.com/sgl-project/sglang/issues/1872,[Bug] Offline engine performance is not better than local server when running batch
nvidia/Llama-3.1-405B-Instruct-FP8,"sglang.Engine(model_path=""meta-llama/Meta-Llama-3.1-405B-Instruct-FP8"", mem_fraction_static=0.8, tp_size=8)",8x NVIDIA A100-SXM4-80GB,Benchmark performance of the SGLang offline engine (Engine.generate) for batch processing.,"Working. Achieved 281.67 tokens per second for 500 requests, with a total time of 177.41 seconds. The user observed that the offline engine was not faster than the online server in this specific test case.",working,https://github.com/sgl-project/sglang/issues/1872,[Bug] Offline engine performance is not better than local server when running batch
nvidia/Llama-3.1-405B-Instruct-FP8,python -m sglang.launch_server --model-path nvidia/Llama-3.1-405B-Instruct-FP4 --mem-fraction 0.8 --cuda-graph-max-bs 512 --tp 8 --attention-backend flashinfer --quantization modelopt_fp4 --kv-cache-dtype fp8_e4m3 --trust-remote-code --load-format dummy --enable-cudagraph-gc,Not explicitly stated (likely 8x GPU setup based on --tp 8),"Benchmarking serving throughput for Llama-3.1-405B variant with Python's Garbage Collector frozen during graph capture, using the --enable-cudagraph-gc flag.",Working well. Achieved 15.41 req/s and 1960.25 total tok/s. This configuration served as the baseline for demonstrating that freezing GC followed by a forced collect improves performance.,working,https://github.com/sgl-project/sglang/pull/10621,Garbage collector regression in the online server
nvidia/Llama-3.1-405B-Instruct-FP8,python -m sglang.launch_server --model-path nvidia/Llama-3.1-405B-Instruct-FP4 --mem-fraction 0.8 --cuda-graph-max-bs 512 --tp 8 --attention-backend flashinfer --quantization modelopt_fp4 --kv-cache-dtype fp8_e4m3 --trust-remote-code --load-format dummy,Not explicitly stated (likely 8x GPU setup based on --tp 8),Benchmarking serving throughput for Llama-3.1-405B variant without the explicit --enable-cudagraph-gc optimization to measure the impact of GC behavior on regression.,"Working but slower. Achieved 14.96 req/s and 1902.39 total tok/s, representing a ~2.95% drop in throughput compared to the optimized version.",working,https://github.com/sgl-project/sglang/pull/10621,Garbage collector regression in the online server
nvidia/Llama-3.1-405B-Instruct-FP8,python3 -m sglang.bench_serving --backend sglang --dataset-name random --random-input-len 128 --random-output-len 128 --num-prompts 1024 --max-concurrency 256 --flush-cache,,Load testing the running server with a specific concurrency (256) and prompt count (1024) to measure serving metrics like TTFT and ITL.,Provided detailed benchmark results showing metrics such as Mean TTFT (~1127-1217ms) and Mean ITL (~239-245ms).,working,https://github.com/sgl-project/sglang/pull/10621,Garbage collector regression in the online server
nvidia/Llama-3.1-405B-Instruct-FP8,"python -m sglang.launch_server --model-path /mnt/external-quantized-models/models/nvidia__Llama-3.1-405B-Instruct-FP4 --mem-fraction 0.8 --cuda-graph-max-bs 4 --tp 8 --attention-backend flashinfer --quantization modelopt_fp4 --model-loader-extra-config '{""enable_multithread_load"": true, ""num_threads"": 8}' --trust-remote-code",8x NVIDIA B200,Reproduction of accuracy issues using Flashinfer backend with FP4 quantization on Blackwell architecture.,BROKEN: Output is garbled (unreadable text). GSM8K accuracy is extremely low at 0.015. Issue traced to Flashinfer decode backend and incorrect KV cache dtype (NVFP4 instead of FP8).,broken,https://github.com/sgl-project/sglang/issues/10284,[Bug] FP4 accuracy issue with B200 + Flashinfer
nvidia/Llama-3.1-405B-Instruct-FP8,"python -m sglang.launch_server --model-path /mnt/external-quantized-models/models/nvidia__Llama-3.1-405B-Instruct-FP4 --mem-fraction 0.8 --cuda-graph-max-bs 4 --tp 8 --attention-backend triton --quantization modelopt_fp4 --model-loader-extra-config '{""enable_multithread_load"": true, ""num_threads"": 8}' --trust-remote-code",8x NVIDIA B200,Comparative test using Triton attention backend to verify if the FP4 accuracy issue is specific to Flashinfer.,working: Output is coherent and correct. GSM8K accuracy is 0.970. Confirms the issue lies with the Flashinfer backend implementation for this model/quantization.,working,https://github.com/sgl-project/sglang/issues/10284,[Bug] FP4 accuracy issue with B200 + Flashinfer
nvidia/Llama-3.1-405B-Instruct-FP8,"python -m sglang.launch_server --model-path /mnt/external-quantized-models/models/nvidia__Llama-3.1-405B-Instruct-FP4 --mem-fraction 0.8 --cuda-graph-max-bs 4 --tp 8 --prefill-attention-backend triton --decode-attention-backend flashinfer --quantization modelopt_fp4 --model-loader-extra-config '{""enable_multithread_load"": true, ""num_threads"": 8}' --trust-remote-code",8x NVIDIA B200,Hybrid backend testing to isolate whether the accuracy bug occurs during the prefill or decode phase.,broken: Output is bad. Confirmed that the issue occurs when Flashinfer is used for the decode phase.,broken,https://github.com/sgl-project/sglang/issues/10284,[Bug] FP4 accuracy issue with B200 + Flashinfer
nvidia/Llama-3.1-405B-Instruct-FP8,"python -m sglang.launch_server --model-path /mnt/external-quantized-models/models/nvidia__Llama-3.1-405B-Instruct-FP4 --mem-fraction 0.8 --cuda-graph-max-bs 4 --tp 8 --prefill-attention-backend flashinfer --decode-attention-backend triton --quantization modelopt_fp4 --model-loader-extra-config '{""enable_multithread_load"": true, ""num_threads"": 8}' --trust-remote-code",8x NVIDIA B200,Hybrid backend testing to isolate whether the accuracy bug occurs during the prefill or decode phase.,"working: Output is coherent. Confirms that Flashinfer works for prefill, but fails during decode for this specific configuration.",working,https://github.com/sgl-project/sglang/issues/10284,[Bug] FP4 accuracy issue with B200 + Flashinfer
nvidia/Llama-3.1-405B-Instruct-FP8,python3 -m sglang.launch_server --model-path neuralmagic/Meta-Llama-3.1-405B-Instruct-FP8 --tp 8 --disable-radix-cache --host 0.0.0.0 --port 9942 --log-level debug --file-storage-pth /home/dave.makhervaks/SGlang_storage --chunked-prefill-size 4096,8x NVIDIA A100-SXM4-80GB,Testing MMLU benchmark with chunked prefill and radix cache disabled.,BROKEN: Failed with RuntimeError: 'Failed to allocate memory for batch_prefill_tmp_v with size 450494464'. It seems to be caused by the combination of --disable-radix-cache and --chunked-prefill-size exceeding flashinfer_workspace_size.,broken,https://github.com/sgl-project/sglang/issues/1323,[Bug] RuntimeError in ModelTpServer
nvidia/Llama-3.1-405B-Instruct-FP8,python3 -m sglang.launch_server --model-path neuralmagic/Meta-Llama-3.1-405B-Instruct-FP8 --tp 8 --disable-radix-cache --host 0.0.0.0 --port 9942 --log-level debug --file-storage-pth /home/dave.makhervaks/SGlang_storage,8x NVIDIA A100-SXM4-80GB,Testing model with radix cache disabled to solve memory allocation issues.,BROKEN: Encountered the same RuntimeError in AlignedAllocator regarding batch_prefill_tmp_v allocation because chunked-prefill is enabled by default in sglang.,broken,https://github.com/sgl-project/sglang/issues/1323,[Bug] RuntimeError in ModelTpServer
nvidia/Llama-3.1-405B-Instruct-FP8,python3 -m sglang.launch_server --model-path neuralmagic/Meta-Llama-3.1-405B-Instruct-FP8 --tp 8 --disable-radix-cache --chunked-prefill-size -1 --host 0.0.0.0 --port 9942 --log-level debug --file-storage-pth /home/dave.makhervaks/SGlang_storage,8x NVIDIA A100-SXM4-80GB,Running with both radix cache and chunked prefill disabled to bypass memory allocation errors.,"Working, but accuracy issues identified. Achieved MMLU accuracy of ~0.86 (0-shot/5-shot) and GSM8K ~0.96 (8-shot). However, Hellaswag accuracy was very poor (~0.25-0.26), which was significantly lower than quoted results.",working,https://github.com/sgl-project/sglang/issues/1323,[Bug] RuntimeError in ModelTpServer
nvidia/Llama-3.1-405B-Instruct-FP8,python3 bench_sglang.py --host http://10.20.45.10 --port 9942 --num-shot 10 --num-questions 10042,8x NVIDIA A100-SXM4-80GB,Benchmarking Hellaswag performance on the 405B FP8 model.,Working but poor results: 'accuracy': 0.256. This was later identified as a bug in SGLang (fixed in version 0.3.1).,working,https://github.com/sgl-project/sglang/issues/1323,[Bug] RuntimeError in ModelTpServer
nvidia/Llama-3.1-70B-Instruct-FP8,python3 -m sglang.launch_server --model-path neuralmagic/Llama-3.1-Nemotron-70B-Instruct-HF-FP8-dynamic --quantization compressed-tensors --tp 8,8x NVIDIA H100 80GB HBM3,Reproduction of a bug where the 'compressed-tensors' quantization format used by llm-compressor/AutoFP8 is not supported by sglang.,"Failed with a bug indicating that the 'compressed-tensors' format is not supported directly, preventing the model from loading/running.",broken,https://github.com/sgl-project/sglang/issues/2871,[Bug] compressed-tensors format not supported
nvidia/Llama-3.1-70B-Instruct-FP8,python3 -m sglang.launch_server --model-path nvidia/Llama-3.1-70B-Instruct-FP8 --host 0.0.0.0 --port 8888 --trust-remote-code --tp 4 --cuda-graph-max-bs 32 --mem-fraction-static 0.8,4xNVIDIA B200 (Compute Capability: 10.0),Attempting to launch an SGLang server on a B200 node using the specified FP8 model.,BROKEN: Failed with a RuntimeError during CUDA graph capture. Specifically: 'CUDA error: operation failed due to a previous error during capture'. The error originated within the FlashInfer backend's decode wrapper.,broken,https://github.com/sgl-project/sglang/issues/8826,[Bug] Unable to Launch A Server on B200
nvidia/Llama-3.1-70B-Instruct-FP8,python -m sglang.launch_server --model-path NousResearch/Meta-Llama-3.1-70B-Instruct --tokenizer-path NousResearch/Meta-Llama-3.1-70B-Instruct --tokenizer-mode auto --quantization fp8 --host 0.0.0.0 --port 8000 --mem-fraction-static 0.87 --chunked-prefill-size 8192 --max-prefill-tokens 16384 --tp-size 2 --attention-backend flashinfer --sampling-backend flashinfer,"2xH100, 4xA100",Attempting to run Llama 3.1 70B FP8 inference on RunPod using the sglang engine via the TrelisResearch one-click-llms template.,"Broken. Server logs showed initialization and weight loading begin, but the user could not access the API (Connection refused/502 Bad Gateway). A contributor suggested the download might not have completed or disk space (originally 200GB) was insufficient for the weights.",broken,https://github.com/sgl-project/sglang/issues/1610,[Bug] Can't access one click llms on runpod
nvidia/Llama-3.1-70B-Instruct-FP8,python -m sglang.launch_server --model-path nvidia/Llama-3.1-70B-Instruct-FP8 --tp 2 --quantization fp8 --chunked-prefill-size 2048,Mentioned in context of A30/H100 usage,Testing 'Meta-Llama-3.1-70B-Instruct' with FP8 quantization under high concurrency/RPS to resolve sampling errors.,"Working. The user reported that 'When I add --chunked-prefill-size 2048, there is no error', effectively bypassing the 'NaN in the probability' sampling error observed at high load.",working,https://github.com/sgl-project/sglang/issues/1571,"[Bug] `Meta-Llama-3.1-8B-Instruct` triggers ""Detected errors during sampling! NaN in the probability."" under high concurrency/RPS."
nvidia/Llama-3.1-70B-Instruct-FP8,python -m sglang.launch_server --model-path nvidia/Llama-3.1-70B-Instruct-FP8 --tp 2 --quantization fp8,Mentioned in context of A30/H100 usage,Running 'Meta-Llama-3.1-70B-Instruct' with FP8 quantization under high concurrency.,Broken. Triggers 'Detected errors during sampling! NaN in the probability' under high RPS/concurrency.,broken,https://github.com/sgl-project/sglang/issues/1571,"[Bug] `Meta-Llama-3.1-8B-Instruct` triggers ""Detected errors during sampling! NaN in the probability."" under high concurrency/RPS."
nvidia/Llama-3.1-70B-Instruct-FP8,"CUDA_VISIBLE_DEVICES=2,3 python -m sglang.launch_server --model-path NousResearch/Meta-Llama-3.1-70B-Instruct --port 30003 --tp 2 --mem-fraction-static 0.93 --quantization fp8 --host 0.0.0.0 --context-length 2560 --max-num-reqs 15 --chunked-prefill-size 8192 --enable-torch-compile",2x H100 80GB (from a system with 8xH100),Testing the --enable-torch-compile flag with an FP8 quantized Llama 3.1 70B model.,"Failed with Exception: Capture cuda graph failed. Error: ""_local_scalar_dense_cuda"" not implemented for 'Float8_e4m3fn'. This occurred because torch.compile (at the time of the issue on Torch 2.4) did not support FP8.",broken,https://github.com/sgl-project/sglang/issues/1196,[Bug] enable-torch-compile error
nvidia/Llama-3.1-70B-Instruct-FP8,python3 -m sglang.launch_server --model=nvidia/Llama-3.1-70B-Instruct-FP8 --tp=8 --dp=1 --trust-remote-code --enable-torch-compile --enable-flashinfer-mla,8x NVIDIA H100 80GB HBM3,Initial attempt to run the FP8 quantized Llama 3.1 70B model with MLA and torch-compile enabled.,"BROKEN: Failed with `KeyError: 'model.layers.78.mlp.gate_up_proj.input_scale'`. Note: --enable-flashinfer-mla is intended for DeepSeek models, not Llama.",broken,https://github.com/sgl-project/sglang/issues/5095,[Bug] KeyError when running nvidia/Llama-3.1-70B-Instruct-FP8
nvidia/Llama-3.1-70B-Instruct-FP8,python3 -m sglang.launch_server --model=nvidia/Llama-3.1-70B-Instruct-FP8 --tp=8 --attention-backend=fa3 --trust-remote-code --enable-torch-compile,8x NVIDIA H100 80GB HBM3,Second attempt after removing the MLA flag and switching to the FA3 attention backend to resolve startup errors.,BROKEN: Still encountered `KeyError: 'model.layers.55.mlp.gate_up_proj.input_scale'` during weight loading.,broken,https://github.com/sgl-project/sglang/issues/5095,[Bug] KeyError when running nvidia/Llama-3.1-70B-Instruct-FP8
nvidia/Llama-3.1-70B-Instruct-FP8,python3 -m sglang.launch_server --model=nvidia/Llama-3.1-70B-Instruct-FP8 --tp=8 --attention-backend=fa3 --trust-remote-code --enable-torch-compile --quantization modelopt --context-length 8192,8x NVIDIA H100 80GB HBM3,Final recommended configuration to fix the KeyError by explicitly setting the quantization flag for the NVIDIA Modelopt format.,WORKING: Setting `--quantization modelopt` resolved the KeyError. Note that checkpoint loading is relatively slow due to the model's non-fused MLP format.,working,https://github.com/sgl-project/sglang/issues/5095,[Bug] KeyError when running nvidia/Llama-3.1-70B-Instruct-FP8
nvidia/Llama-3.1-70B-Instruct-FP8,python3 -m sglang.launch_server --model-path Meta-Llama-3.1-70B-Instruct --tp 8 --disable-radix-cache --host 0.0.0.0 --port 9942 --log-level debug --file-storage-pth /home/dave.makhervaks/SGLang_storage --chunked-prefill-size 4096,8x NVIDIA A100-SXM4-80GB,Debug a RuntimeError (Failed to allocate memory for batch_prefill_tmp_v) occurring during MMLU benchmarks.,Failure: 'RuntimeError: Failed to allocate memory for batch_prefill_tmp_v with size 450494464 and alignment 16 in AlignedAllocator'. Likely caused by the combination of --disable-radix-cache and --chunked-prefill-size exceeding flashinfer_workspace_size.,broken,https://github.com/sgl-project/sglang/issues/1323,[Bug] RuntimeError in ModelTpServer
nvidia/Llama-3.1-70B-Instruct-FP8,python3 -m sglang.launch_server --model-path Meta-Llama-3.1-70B-Instruct --tp 8 --disable-radix-cache --host 0.0.0.0 --port 9942 --log-level debug --file-storage-pth /home/dave.makhervaks/SGLang_storage,8x NVIDIA A100-SXM4-80GB,Attempt to resolve the flashinfer memory allocation error by removing the explicit chunked-prefill-size parameter.,Failure: Still encountered the same memory allocation error because SGLang enables chunked-prefill by default when radix cache is disabled.,broken,https://github.com/sgl-project/sglang/issues/1323,[Bug] RuntimeError in ModelTpServer
nvidia/Llama-3.1-70B-Instruct-FP8,python3 -m sglang.launch_server --model-path Meta-Llama-3.1-405B-Instruct-FP8 --tp 8 --disable-radix-cache --host 0.0.0.0 --port 9942 --log-level debug --file-storage-pth /home/dave.makhervaks/SGLang_storage --chunked-prefill-size -1,8x NVIDIA A100-SXM4-80GB,Avoid the flashinfer memory allocation error by explicitly disabling chunked prefill while using --disable-radix-cache.,"Working: Server started successfully. However, subsequent Hellaswag benchmarks yielded very poor accuracy (0.252-0.264), which was later identified as a bug fixed in sglang v0.3.1.",working,https://github.com/sgl-project/sglang/issues/1323,[Bug] RuntimeError in ModelTpServer
nvidia/Llama-3.1-70B-Instruct-FP8,python3 -m sglang.launch_server --model-path neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8 --port 8000 --host 0.0.0.0 --tensor-parallel-size 8 --trust-remote-code --mem-fraction-static 0.8 --max-running-requests 128 --speculative-algo EAGLE --speculative-draft lmzheng/sglang-EAGLE-LLaMA3-Instruct-8B --speculative-num-steps 2 --speculative-eagle-topk 4 --speculative-num-draft-tokens 4,8x NVIDIA H200,Testing speculative decoding using EAGLE with an FP8 target model (Meta-Llama-3.1-70B-Instruct-FP8) and a BF16 draft model (sglang-EAGLE-LLaMA3-Instruct-8B).,BROKEN: Failed with a dtype mismatch error during CUDA graph capture in the EAGLE worker. Traceback indicates an exception in `eagle_draft_cuda_graph_runner.py` at `self.model_runner.model.forward`. A maintainer confirmed that mixing an FP8 target model with a BF16 draft model is currently not supported for EAGLE.,broken,https://github.com/sgl-project/sglang/issues/3824,[Bug] Use of bf16 draft model for fp8 model speculation
nvidia/Llama-3.1-70B-Instruct-FP8,"CUDA_VISIBLE_DEVICES=2,3 python -m sglang.launch_server --model-path NousResearch/Hermes-3-Llama-3.1-70B-FP8 --port 30003 --tp 2 --mem-fraction-static 0.90 --host 0.0.0.0 --context-length 2048",2xH100 80GB (from a system with 8xH100),Testing a pre-quantized FP8 version of Llama-3.1-70B (Hermes-3) at high RPS (5.5) and high concurrent requests (#running-req: 136).,GOOD OUTPUTS. Performance was stable even at high concurrency (136 running requests) with the pre-quantized model.,working,https://github.com/sgl-project/sglang/issues/1195,[Bug] Bad outputs with fp8 quantization at high RPS
nvidia/Llama-3.1-70B-Instruct-FP8,"CUDA_VISIBLE_DEVICES=2,3 python -m sglang.launch_server --model-path NousResearch/Meta-Llama-3.1-70B-Instruct --port 30003 --tp 2 --mem-fraction-static 0.90 --host 0.0.0.0 --context-length 2048 --quantization fp8",2xH100 80GB (from a system with 8xH100),Running RPS benchmark on unquantized Llama-3.1-70B with on-the-fly FP8 quantization.,BAD OUTPUTS @ 5.5rps. Large number of concurrent requests (running-req: 137) resulted in gibberish/repetitive Chinese characters in the output.,broken,https://github.com/sgl-project/sglang/issues/1195,[Bug] Bad outputs with fp8 quantization at high RPS
nvidia/Llama-3.1-70B-Instruct-FP8,"CUDA_VISIBLE_DEVICES=2,3 python -m sglang.launch_server --model-path NousResearch/Meta-Llama-3.1-70B-Instruct --port 30003 --tp 2 --mem-fraction-static 0.90 --host 0.0.0.0 --context-length 2048 --quantization fp8 --max-num-reqs 10",2xH100 80GB (from a system with 8xH100),Debugging bad outputs by limiting the maximum number of concurrent requests to 10 using the --max-num-reqs flag.,GOOD OUTPUTS @ 5.5 RPS. Limiting concurrency resolved the output corruption issue seen at higher request counts.,working,https://github.com/sgl-project/sglang/issues/1195,[Bug] Bad outputs with fp8 quantization at high RPS
nvidia/Llama-3.1-70B-Instruct-FP8,"CUDA_VISIBLE_DEVICES=2,3 python -m sglang.launch_server --model-path NousResearch/Hermes-3-Llama-3.1-70B --port 30003 --tp 2 --mem-fraction-static 0.90 --host 0.0.0.0 --context-length 2048 --quantization fp8",2xH100 80GB (from a system with 8xH100),Testing Hermes-3 variant of Llama-3.1-70B with on-the-fly FP8 quantization at 5.5 RPS.,BAD OUTPUTS @ 5.5rps. Output corruption occurred with high concurrency (running-req: 135).,broken,https://github.com/sgl-project/sglang/issues/1195,[Bug] Bad outputs with fp8 quantization at high RPS
nvidia/Llama-3.1-70B-Instruct-FP8,"CUDA_VISIBLE_DEVICES=2,3 python -m sglang.launch_server --model-path NousResearch/Hermes-3-Llama-3.1-70B --port 30003 --tp 2 --mem-fraction-static 0.90 --host 0.0.0.0 --context-length 2048 --quantization fp8 --max-num-reqs 10",2xH100 80GB (from a system with 8xH100),Testing Hermes-3 variant of Llama-3.1-70B with limited concurrency to verify if it resolves FP8 output corruption.,GOOD OUTPUTS @ 5.5 RPS. Output was correct when concurrency was capped at 8-10 requests.,working,https://github.com/sgl-project/sglang/issues/1195,[Bug] Bad outputs with fp8 quantization at high RPS
nvidia/Llama-3.1-8B-Instruct-FP8,python3 -m sglang.bench_one_batch --model-path nvidia/Llama-3.1-8B-Instruct-NVFP4 --quantization modelopt_fp4 --batch-size 1 --input-len 1024 --output-len 1024 --kv-cache-dtype nvfp4_e2m1,Mention of sm100 (Blackwell) in error logs,Reference testing command for evaluating the new NVFP4 KV cache implementation and its impact on performance and memory.,Extremely slow due to frequent dequantization from fp4 to bf16. Encountered a 'RuntimeError: [FP4 gemm Runner] Failed to initialize cutlass FP4 gemm on sm100' during execution.,broken,https://github.com/sgl-project/sglang/pull/15133,Add NVFP4-style KV Cache
nvidia/Llama-3.1-8B-Instruct-FP8,python test/srt/quant/test_modelopt_fp8.py,B100,"Automated testing for Nvidia-ModelOPT FP8 quantization, specifically validating Llama-3.1-8B-Instruct-FP8 accuracy on GSM8K and generation throughput.",Llama-3.1-8B-Instruct-FP8 achieved a GSM8K accuracy score relative to a 69% threshold (with 200 questions tested) and a generation speed of at least 120 tokens/s.,working,https://github.com/sgl-project/sglang/pull/12478,Add the Nvidia-ModelOPT FP8 & FP4 quantization test case
nvidia/Llama-3.1-8B-Instruct-FP8,python -m sglang.launch_server --model-path nvidia/Llama-3.1-8B-Instruct-FP8 --trust-remote-code --quantization modelopt,,Verifying that ModelOpt FP8 checkpoints can still be loaded correctly with the `--quantization modelopt` flag after changes to default quantization logic.,Working. The user confirmed the command works and that the system correctly identifies the FP8 quantization from hf_quant_config.json.,working,https://github.com/sgl-project/sglang/pull/11991,[NVIDIA] Change default quant method for model_opt
nvidia/Llama-3.1-8B-Instruct-FP8,python -m sglang.launch_server --model-path nvidia/Llama-3.1-8B-Instruct-FP8 --trust-remote-code --quantization modelopt_fp8,,Testing explicit quantization flag for ModelOpt FP8 checkpoints.,Working. The system correctly handles the explicit 'modelopt_fp8' quantization method.,working,https://github.com/sgl-project/sglang/pull/11991,[NVIDIA] Change default quant method for model_opt
nvidia/Llama-3.1-8B-Instruct-FP8,python3 -m sglang.launch_server --model-path nvidia/Llama-3.1-8B-Instruct-FP4 --trust-remote-code --quantization modelopt_fp4 --kv-cache-dtype fp8_e4m3,B200,Performance comparison between the original main branch and the PR fix for Llama 3.1 FP4 with FP8 KV cache.,Before fix: 244.20 token/s. After original PR + this fix: 287.12 token/s (1.783s latency for 512 tokens). Verified working with significant speedup.,working,https://github.com/sgl-project/sglang/pull/14553,Fix FP8 KV Triton type issue and add regression test
nvidia/Llama-3.1-8B-Instruct-FP8,python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --enable-hip-attention --context-length 210000 --cuda-graph-max-bs 16 --mem-fraction-static 0.7 --tp-size 2,2x RTX 4090,Testing InfiniteHiP (sparse attention) with long-context generalization up to 210k tokens on consumer hardware. Specifically aimed at resolving OOM and illegal memory access issues reported during graph capture.,Working well according to the author. The configuration adjustments to --cuda-graph-max-bs and --mem-fraction-static allowed the model to run without typical memory errors on limited VRAM.,working,https://github.com/sgl-project/sglang/pull/3930,[Feature] Support Efficient Sparse HiP Attention (InfiniteHiP) with Long-Context Generalization and KV Offloading Capabilties
nvidia/Llama-3.1-8B-Instruct-FP8,python3 -m sglang.launch_server --model-path nvidia/Llama-3.1-8B-Instruct-FP8,H200,Benchmarking serving performance for nvidia/Llama-3.1-8B-Instruct-FP8 on H200 before and after implementing triton kernel launch config caching for static w8a8 quantization.,"Working. After optimization, TTFT (Time to First Token) was reduced by approximately 4% (from 137.11ms mean to 131.62ms mean). Total token throughput increased slightly from 12237.83 tok/s to 12263.12 tok/s.",working,https://github.com/sgl-project/sglang/pull/11157,perf: cache kernel launch of static w8a8 quant
nvidia/Llama-3.1-8B-Instruct-FP8,"python3 -m sglang.launch_server --model-path nvidia/Llama-3.1-8B-Instruct-FP4 --trust-remote-code --quantization modelopt_fp4 --kv-cache-dtype fp8_e4m3 --model-loader-extra-config ""{\""enable_multithread_load\"": true, \""num_threads\"": 8}""","Not explicitly stated, but running in a containerized environment (sgl-workspace)",Benchmark baseline performance with no_grad/inference mode optimization. Testing FP4 quantization with FP8 KV cache.,"Latency: 1.810s, Tokens: 512, Speed: 282.92 token/s. Achieved a speedup of approximately 0.68% when using the default inference settings vs explicit no_grad.",working,https://github.com/sgl-project/sglang/pull/15639,Dynamic grad instead of no_grad
nvidia/Llama-3.1-8B-Instruct-FP8,"SGLANG_ENABLE_TORCH_INFERENCE_MODE=true python3 -m sglang.launch_server --model-path nvidia/Llama-3.1-8B-Instruct-FP4 --trust-remote-code --quantization modelopt_fp4 --kv-cache-dtype fp8_e4m3 --model-loader-extra-config ""{\""enable_multithread_load\"": true, \""num_threads\"": 8}""",Not explicitly stated,Testing the newly introduced SGLANG_ENABLE_TORCH_INFERENCE_MODE environment variable to compare Inference Mode vs standard no_grad behavior.,"Latency: 1.822s, Tokens: 512, Speed: 281.00 token/s. Used to verify that the dynamic grad mode works, although in this specific test run it was slightly slower before stabilization/variance considerations.",working,https://github.com/sgl-project/sglang/pull/15639,Dynamic grad instead of no_grad
nvidia/Llama-3.1-8B-Instruct-FP8,"llm = sgl.Engine(model_path=""nvidia/Llama-3.1-8B-Instruct-FP8"", quantization=""modelopt"")",,Initial test for integrating NVIDIA ModelOpt FP8 quantized models into SGLang.,"Working initial step for FP8 LLaMA 3.1 model inference, though the author later noted potential issues with GLOO_SOCKET_IFNAME configuration in certain environments which were subsequently addressed.",working,https://github.com/sgl-project/sglang/pull/2535,Enable Nvidia's ModelOpt fp8 quantized models
nvidia/Llama-3.1-8B-Instruct-FP8,"GLOO_SOCKET_IFNAME=ens8np0 llm = sgl.Engine(model_path=""nvidia/Llama-3.1-8B-Instruct-FP8"", quantization=""modelopt"")",,Testing model inference while specifying the network interface for GLOO to resolve connection issues.,Resolved issues related to socket interface detection during engine initialization.,working,https://github.com/sgl-project/sglang/pull/2535,Enable Nvidia's ModelOpt fp8 quantized models
nvidia/Llama-3.1-8B-Instruct-FP8,python -m sglang.launch_server --model-path neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8,NVIDIA Cloud H100,Nightly evaluation of gsm8k benchmark for torch v2.5.1 update verification.,Working. Achieved a gsm8k score of 0.88 (en metric). Result was considered 'OK' by the developer despite some instability in CI environments.,working,https://github.com/sgl-project/sglang/pull/1849,chore: update torch v2.5.1
nvidia/Llama-3.1-8B-Instruct-FP8,"import sglang as sgl

def main():
    prompts = [
        ""Hello, my name is"",
        ""The president of the United States is"",
        ""The capital of France is"",
        ""The future of AI is"",
    ]
    sampling_params = {""temperature"": 0.8, ""top_p"": 0.95}
    llm = sgl.Engine(model_path=""nvidia/Llama-3.1-8B-Instruct-FP8"", quantization=""modelopt"")

    outputs = llm.generate(prompts, sampling_params)
    for prompt, output in zip(prompts, outputs):
        print(""==============================="")
        print(f""Prompt: {prompt}\nGenerated text: {output['text']}"")

if __name__ == ""__main__"":
    main()",,Testing the addition of ModelOpt FP8 KV cache support in SGLang to ensure the engine can run the quantized model with FP8 KV cache enabled.,"The PR was closed/merged, indicating the implementation of ModelOptFp8KVCacheMethod and KV cache scaler name matching for this model was successful.",working,https://github.com/sgl-project/sglang/pull/3223,Add support for nvidia modelopt fp8 kv cache
nvidia/Llama-3.1-8B-Instruct-FP8,python -m sglang.launch_server --model-path nvidia/Llama-3.1-8B-Instruct-FP8,,Server launch attempt for a modelopt-quantized model without explicitly specifying the quantization method to test the new automatic inspection logic.,"Initially failed with 'KeyError: model.layers.0.mlp.gate_up_proj.input_scale' before the PR fix because 'input_scale' parameters were not instantiated without '--quantization modelopt'. With the PR changes, the server should automatically detect the quantization method from hf_quant_config.json.",working,https://github.com/sgl-project/sglang/pull/5145,[modelopt] automatically inspect if model is ModelOpt quantized and set quantization method
nvidia/Llama-3.1-8B-Instruct-FP8,python -m sglang.launch_server --model-path nvidia/Llama-3.1-8B-Instruct-FP8 --quantization modelopt,,Manual server launch command for modelopt-quantized models required before this PR's automatic detection feature.,"Working, but required the user to explicitly pass the quantization flag.",working,https://github.com/sgl-project/sglang/pull/5145,[modelopt] automatically inspect if model is ModelOpt quantized and set quantization method
nvidia/Llama-3.1-8B-Instruct-FP8,HF_HUB_OFFLINE=1 python -m sglang.launch_server --model-path meta-llama/Llama-3.2-1B,,Testing model loading in offline mode with HF_HUB_OFFLINE=1 to verify that the quantization inspection logic doesn't crash or attempt network calls when the model is in local cache.,"Broken for local cache lookups if the model_path is a huggingface ID rather than a direct local filesystem path, as os.path.exists returns false and the code tries to call the HfApi.",broken,https://github.com/sgl-project/sglang/pull/5145,[modelopt] automatically inspect if model is ModelOpt quantized and set quantization method
nvidia/Llama-3.1-8B-Instruct-FP8,python3 -m sglang.launch_server   --model-path=nvidia/Llama-3.1-8B-Instruct-FP8   --quantization=modelopt   --tp=8   --trust-remote-code   --attention-backend=fa3   --kv-cache-dtype=bfloat16   --enable-piecewise-cuda-graph --piecewise-cuda-graph-max-tokens 32768,TP=8 GPUs (likely A100 or H100 based on FA3 and FP8 usage),Launch SGLang server to benchmark performance improvements of Piecewise CUDA Graph support for ModelOpt FP8 quantization.,Server successfully launched. Provided the base for serving benchmarks showing up to 34.4% reduction in Mean TTFT and 19.7% increase in output throughput at concurrency 1.,working,https://github.com/sgl-project/sglang/pull/13094,[Piecewise CUDA Graph] Support ModelOpt FP8
nvidia/Llama-3.1-8B-Instruct-FP8,python3 -m sglang.bench_serving --backend sglang --dataset-name random --random-input-len 1024 --random-output-len 16 --random-range-ratio 1.0 --num-prompts 8   --max-concurrency 1  --output-file res_before.jsonl,,Service benchmarking at concurrency 1 to measure baseline and optimized performance with FP8 and Piecewise CUDA Graphs.,"Input throughput: 18082.346, Output throughput: 282.537, Mean TTFT: 19.657 ms (After optimization).",working,https://github.com/sgl-project/sglang/pull/13094,[Piecewise CUDA Graph] Support ModelOpt FP8
nvidia/Llama-3.1-8B-Instruct-FP8,python3 -m sglang.bench_serving --backend sglang --dataset-name random --random-input-len 1024 --random-output-len 16 --random-range-ratio 1.0 --num-prompts 32  --max-concurrency 4  --output-file res_before.jsonl,,Service benchmarking at concurrency 4.,"Input throughput: 38431.705, Output throughput: 600.495, Mean TTFT: 48.526 ms (After optimization).",working,https://github.com/sgl-project/sglang/pull/13094,[Piecewise CUDA Graph] Support ModelOpt FP8
nvidia/Llama-3.1-8B-Instruct-FP8,python3 -m sglang.bench_serving --backend sglang --dataset-name random --random-input-len 1024 --random-output-len 16 --random-range-ratio 1.0 --num-prompts 192 --max-concurrency 16 --output-file res_before.jsonl,,Service benchmarking at concurrency 16.,"Input throughput: 99625.037, Output throughput: 1556.641, Mean TTFT: 82.231 ms (After optimization).",working,https://github.com/sgl-project/sglang/pull/13094,[Piecewise CUDA Graph] Support ModelOpt FP8
nvidia/Llama-3.1-8B-Instruct-FP8,python3 -m sglang.bench_serving --backend sglang --dataset-name random --random-input-len 1024 --random-output-len 16 --random-range-ratio 1.0 --num-prompts 256 --max-concurrency 32 --output-file res_before.jsonl,,Service benchmarking at concurrency 32.,"Input throughput: 119522.170, Output throughput: 1867.534, Mean TTFT: 138.939 ms (After optimization).",working,https://github.com/sgl-project/sglang/pull/13094,[Piecewise CUDA Graph] Support ModelOpt FP8
nvidia/Llama-3.1-8B-Instruct-FP8,python3 benchmark/gsm8k/bench_sglang.py --num-shots 8 --num-questions 1319 --parallel 500,,Evaluate model accuracy on GSM8K benchmark using FP8 and Piecewise CUDA Graphs.,"Accuracy: 0.788, Latency: 8.843 s, Output throughput: 15075.961 token/s.",working,https://github.com/sgl-project/sglang/pull/13094,[Piecewise CUDA Graph] Support ModelOpt FP8
nvidia/Llama-3.1-8B-Instruct-FP8,pytest -q test/srt/test_piecewise_cuda_graph.py::TestPiecewiseCudaGraphFP8::test_mgsm_accuracy,,Run MGSM (Multilingual GSM8K) accuracy test specifically for FP8 Piecewise CUDA Graphs.,"1 passed. Score: 0.868, MGSM Accuracy: 0.868.",working,https://github.com/sgl-project/sglang/pull/13094,[Piecewise CUDA Graph] Support ModelOpt FP8
nvidia/Llama-3.1-8B-Instruct-FP8,python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-Instruct --mem-fraction-static 0.55 --quantization fp8 --schedule-conservativeness 0.3,NVIDIA H100 80GB HBM3,Reproducing a bug where GPU memory slowly accumulates in the Radix Cache over long periods of operation (7 days) and heavy load (500k-1M requests).,Observed GPU memory increasing by 20GB after initialization when left running. Torch profiler showed small allocations in `cache_finished_req` that were not deallocated. The user noted that adding a periodic `flush_cache` operation every 5-6 hours resolved the accumulation.,broken,https://github.com/sgl-project/sglang/issues/5958,[Bug] GPU Memory slowly accumulating in Radix Cache
nvidia/Llama-3.1-8B-Instruct-FP8,python3 -m sglang.launch_server --model-path /data/huggingface/Llama-3.1-405B-Instruct-FP8 --trust-remote-code --tp 8,8x Blackwell (RTX 5090/6000 or H100 in context of Blackwell tracker),Testing Llama-3.1-405B-Instruct-FP8 on Blackwell hardware using the lmsysorg/sglang:blackwell image.,Broken. Failed with 'Exception: Capture CUDA graph failed: CUDA error: operation failed due to a previous error during capture'. User noted that using --disable-cuda-graph resulted in 'really bad' output.,broken,https://github.com/sgl-project/sglang/issues/5338,[Tracker] Blackwell support
nvidia/Llama-3.1-8B-Instruct-FP8,python -m sglang.bench_one_batch --model-path meta-llama/Meta-Llama-3.1-8B-Instruct --batch 32 --input-len 256 --output-len 32,NVIDIA A40,Benchmark a single batch of the Llama-3.1-8B-Instruct model to test performance and stability on Python 3.9.,"BROKEN: Failed with 'TypeError: dataclass() got an unexpected keyword argument 'slots'' because Python 3.9 does not support the 'slots' parameter in dataclasses, which was used in the codebase.",broken,https://github.com/sgl-project/sglang/issues/8389,[Bug] Python 3.9 compatibility broken due to `dataclass(slots=True)` usage
nvidia/Llama-3.1-8B-Instruct-FP8,python3 -m sglang.bench_offline_throughput --model-path meta-llama/Meta-Llama-3.1-8B-Instruct --num-prompts 10,NVIDIA A40,Benchmark offline throughput for the Llama-3.1-8B-Instruct model on a Python 3.9 environment.,BROKEN: Failed with 'TypeError: dataclass() got an unexpected keyword argument 'slots'' due to Python 3.9 incompatibility with the 'slots=True' decorator used in the LoRA registry.,broken,https://github.com/sgl-project/sglang/issues/8389,[Bug] Python 3.9 compatibility broken due to `dataclass(slots=True)` usage
nvidia/Llama-3.1-8B-Instruct-FP8,python3 -m sglang.launch_server --model=nvidia/Llama-3.1-8B-Instruct-FP8 --tp=1 --quantization modelopt --context-length 8192,1xRTX 4090,Resolve KeyError during weight loading by specifying the correct quantization backend for NVIDIA's model format.,"Working. The user confirmed the model loads and runs after adding the --quantization modelopt flag and adjusting context length, though checkpoint loading remains slow.",working,https://github.com/sgl-project/sglang/issues/5095,[Bug] KeyError when running nvidia/Llama-3.1-70B-Instruct-FP8
nvidia/Llama-3.1-8B-Instruct-FP8,python3 -m sglang.launch_server --model=nvidia/Llama-3.1-8B-Instruct-FP8 --tp=1,1xRTX 4090,Initial attempt to run the 8B FP8 model on a single GPU.,"Broken. Failed with 'KeyError: model.layers.0.mlp.gate_up_proj.input_scale'. This occurred because the default loader expected fused MLP weights, but the NVIDIA model uses separate gate and up keys.",broken,https://github.com/sgl-project/sglang/issues/5095,[Bug] KeyError when running nvidia/Llama-3.1-70B-Instruct-FP8
nvidia/Llama-3.1-8B-Instruct-FP8,SGL_ENABLE_JIT_DEEPGEMM=0 CUDA_VISIBLE_DEVICES=2 python -m sglang.launch_server --model-path /mnt/llms/models/RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic/ --tp 1 --context-length 2048 --port 5001 --host 0.0.0.0,NVIDIA GeForce RTX 5090,Reproduction of a bug where FP8 dynamic models from RedHatAI fail to load/run on Blackwell architecture GPUs.,"Broken. Crashes during CUDA graph capture with 'RuntimeError: Expected status == cutlass::Status::kSuccess to be true, but got false' in the fp8_scaled_mm operator.",broken,https://github.com/sgl-project/sglang/issues/7482,[Bug] Some FP8 models fail to load
nvidia/Llama-3.1-8B-Instruct-FP8,"SGL_ENABLE_JIT_DEEPGEMM=0 CUDA_VISIBLE_DEVICES=2,4 python -m sglang.launch_server --model-path /mnt/llms/models/RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic/ --tp 2 --context-length 2048 --port 5001 --host 0.0.0.0",2x NVIDIA GeForce RTX 5090,Testing if the FP8 loading issue persists with Tensor Parallelism (TP=2) after updating to sglang 0.4.8 and sgl_kernel 0.2.0.,Broken. Still fails during CUDA graph capture with the same Cutlass success status error on the RTX 5090 GPUs.,broken,https://github.com/sgl-project/sglang/issues/7482,[Bug] Some FP8 models fail to load
nvidia/Llama-3.1-8B-Instruct-FP8,CUDA_VISIBLE_DEVICES=4 python -m sglang.launch_server --model-path /mnt/llms/models/RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic/ --tp 1 --context-length 2048 --port 5001 --host 0.0.0.0 --disable-cuda-graph,NVIDIA GeForce RTX 5090,"Debugging the model crash by disabling CUDA graphs, as suggested by the error logs.","Broken. Error occurs later (during prefill batch instead of initialization), but leads to the same 'cutlass::Status::kSuccess' RuntimeError in the sgl_kernel fp8_scaled_mm operation.",broken,https://github.com/sgl-project/sglang/issues/7482,[Bug] Some FP8 models fail to load
nvidia/Llama-3.1-8B-Instruct-FP8,python -m sglang.launch_server --model-path RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic --context-length 60000 --port 8000,NVIDIA GeForce RTX 5090,Initial test of Llama-3.1-8B FP8 on Blackwell (5090) within a Docker container.,Broken. Failed with `ImportError: cannot import name '_set_default_torch_dtype' from 'vllm.model_executor.model_loader'`. This was attributed to an environment/version mismatch.,broken,https://github.com/sgl-project/sglang/issues/7227,[Roadmap] Blackwell Support and Optimizations
nvidia/Llama-3.1-8B-Instruct-FP8,python -m sglang.launch_server --model-path RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic --port 8000 --context-length 60000,NVIDIA GeForce RTX 5090,Retrying the server launch after updating components.,Broken. Failed with `ImportError: ... undefined symbol: _ZN3c104cuda9SetDeviceEa` in `sgl_kernel/common_ops.abi3.so`. Likely due to ABI incompatibility between sgl-kernel and the installed PyTorch version.,broken,https://github.com/sgl-project/sglang/issues/7227,[Roadmap] Blackwell Support and Optimizations
nvidia/Llama-3.1-8B-Instruct-FP8,docker run --gpus all -p 8000:8000 --ipc=host -e SGL_ENABLE_JIT_DEEPGEMM=0 lmsysorg/sglang:blackwell python -m sglang.launch_server --model-path Qwen/Qwen3-4B-FP8 --port 8000 --context-length 60000,NVIDIA GeForce RTX 5090,Testing Blackwell support using a different FP8 model (Qwen3-4B-FP8) and disabling DeepGEMM JIT.,"Working. After setting `SGL_ENABLE_JIT_DEEPGEMM=0`, the server launched successfully on the RTX 5090, bypassing the `KeyError: '120a'` associated with DeepGEMM not supporting the 5090's architecture code yet.",working,https://github.com/sgl-project/sglang/issues/7227,[Roadmap] Blackwell Support and Optimizations
nvidia/Llama-3.1-8B-Instruct-FP8,python3 -m sglang.launch_server --model-path neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8 --port 8000 --host 0.0.0.0 --tensor-parallel-size 8 --trust-remote-code --mem-fraction-static 0.8 --max-running-requests 128 --speculative-algo EAGLE --speculative-draft lmzheng/sglang-EAGLE-LLaMA3-Instruct-8B --speculative-num-steps 2 --speculative-eagle-topk 4 --speculative-num-draft-tokens 4,8x NVIDIA H200,Testing speculative decoding using the EAGLE algorithm with a BF16 draft model (8B) and an FP8 target model (70B) to identify potential dtype mismatch issues.,Broken. Encountered a dtype mismatch error during EAGLE CUDA graph capture: 'Scheduler hit an exception'. It appears that using a BF16 draft model with an FP8 target model for speculation is not currently supported.,broken,https://github.com/sgl-project/sglang/issues/3824,[Bug] Use of bf16 draft model for fp8 model speculation
nvidia/Llama-3.1-8B-Instruct-FP8,CUDA_VISIBLE_DEVICES=7 python -m sglang.launch_server --port 7501 --model-path meta-llama/Llama-3.1-8B-Instruct --disable-cuda-graph,1x NVIDIA A100-SXM4-80GB,Initial attempt to launch the server while trying to bypass CUDA graph capture errors related to missing JIT headers (cuda_fp8.h).,BROKEN: Fails with a RuntimeError during the 'Capture cuda graph' phase. flashinfer JIT requires cuda_fp8.h which is missing in the environment. Error: 'fatal error: cuda_fp8.h: No such file or directory'.,broken,https://github.com/sgl-project/sglang/issues/5389,[Bug] SGLang server fails during CUDA graph capture: flashinfer JIT requires cuda_fp8.h even with pre-compiled wheel and --disable-cuda-graph
nvidia/Llama-3.1-8B-Instruct-FP8,CUDA_VISIBLE_DEVICES=7 python -m sglang.launch_server --port 7501 --model-path meta-llama/Llama-3.1-8B-Instruct,1x NVIDIA A100-SXM4-80GB,Standard launch attempt for the Llama-3.1-8B-Instruct model as part of the reproduction steps.,BROKEN: Fails during JIT compilation of flashinfer kernels because the environment lacks the full CUDA Development Toolkit headers (specifically cuda_fp8.h).,broken,https://github.com/sgl-project/sglang/issues/5389,[Bug] SGLang server fails during CUDA graph capture: flashinfer JIT requires cuda_fp8.h even with pre-compiled wheel and --disable-cuda-graph
nvidia/Llama-3.1-8B-Instruct-FP8,python -m sglang.launch_server --port 7501 --model-path meta-llama/Llama-3.1-8B-Instruct,1x NVIDIA A100-SXM4-80GB,Testing model launch after following a specific installation sequence (DSPy instructions) and modifying the environment's CUDA_HOME path.,BROKEN: Failed with 'torch.OutOfMemoryError: CUDA out of memory'. The process attempted to allocate 224.00 MiB but only 174.00 MiB was free on the GPU due to other processes (Process 3110457) using 71.66 GiB.,broken,https://github.com/sgl-project/sglang/issues/5389,[Bug] SGLang server fails during CUDA graph capture: flashinfer JIT requires cuda_fp8.h even with pre-compiled wheel and --disable-cuda-graph
nvidia/Llama-3.1-8B-Instruct-FP8,"import sglang as sgl

if __name__ == '__main__':
    llm = sgl.Engine(
        model_path=""nvidia/Llama-3.1-8B-Instruct-FP8"",
        quantization=""modelopt"",
        revision=""13858565416dbdc0b4e7a4a677fadfbd5b9e5bb9"",
        log_level=""debug"",
    )",NVIDIA H100 80GB HBM3,Reproduction script to demonstrate a bug when loading a prequantized model with scalar weight scales using the modelopt quantization method.,"FAILED: Throws 'IndexError: invalid index of a 0-dim tensor' in the convert_to_channelwise function. This occurs because the code assumes weight scale tensors are 1-D, while modelopt-quantized FP8 models on H100 hardware use 0-dim (scalar) tensors for certain layers like QKVParallelLinear.",broken,https://github.com/sgl-project/sglang/issues/4594,[Bug] cannot load prequantized model with scalar weight scale
nvidia/Llama-3.1-8B-Instruct-FP8,"python -m sglang.launch_server --model-path ""meta-llama/Meta-Llama-3.1-8B-Instruct"" --dp 2 --quantization fp8 --kv-cache-dtype fp8_e5m2",2x NVIDIA H100 80GB HBM3,Reproducing a bug where the model triggers 'NaN in the probability' errors during sampling under high concurrency/RPS.,"BROKEN: Triggers 'Detected errors during sampling! NaN in the probability.' specifically when request count increases (e.g., 128 requests), though 64 requests worked.",broken,https://github.com/sgl-project/sglang/issues/1571,"[Bug] `Meta-Llama-3.1-8B-Instruct` triggers ""Detected errors during sampling! NaN in the probability."" under high concurrency/RPS."
nvidia/Llama-3.1-8B-Instruct-FP8,"python -m sglang.launch_server --model-path ""meta-llama/Meta-Llama-3.1-8B-Instruct"" --dp 2 --quantization fp8 --kv-cache-dtype fp8_e5m2 --disable-cuda-graph-padding",2x NVIDIA H100 80GB HBM3,Suggested workaround by maintainer to fix the NaN sampling error without the full performance penalty of disabling CUDA graphs entirely.,Working: Maintainer confirmed this temporarily fixes the problem until a permanent patch is applied.,working,https://github.com/sgl-project/sglang/issues/1571,"[Bug] `Meta-Llama-3.1-8B-Instruct` triggers ""Detected errors during sampling! NaN in the probability."" under high concurrency/RPS."
nvidia/Llama-3.1-8B-Instruct-FP8,"python -m sglang.launch_server --model-path ""meta-llama/Meta-Llama-3.1-8B-Instruct"" --dp 4 --quantization fp8 --kv-cache-dtype fp8_e5m2",8x NVIDIA H100 80GB HBM3,Testing model hosting under high concurrency using Data Parallelism (dp=4) and FP8 quantization/KV cache.,"Broken behavior under high concurrency (256+ requests). Model outputted 'nonsense' (corrupted text) and performance degraded. At 64 requests: 2.30 RPS, at 128 requests: 4.41 RPS. At higher loads, generated 4096 tokens of garbage text.",broken,https://github.com/sgl-project/sglang/issues/1434,[Bug] Nonsense and slow output under high concurrency
nvidia/Llama-3.1-8B-Instruct-FP8,"python -m sglang.launch_server --model-path ""meta-llama/Meta-Llama-3.1-8B-Instruct"" --tp 4 --quantization fp8 --kv-cache-dtype fp8_e5m2",8x NVIDIA H100 80GB HBM3,Testing model hosting under high concurrency using Tensor Parallelism (tp=4) and FP8 quantization/KV cache to compare against DP performance.,Reported similar issues with nonsense and slow output under high concurrency as the DP configuration.,broken,https://github.com/sgl-project/sglang/issues/1434,[Bug] Nonsense and slow output under high concurrency
nvidia/Llama-3.1-8B-Instruct-FP8,python3 -m sglang.launch_server --model-path neuralmagic/Meta-Llama-3.1-405B-Instruct-FP8 --tp 8 --disable-radix-cache --host 0.0.0.0 --port 9942 --log-level debug --file-storage-pth /home/dave.makhervaks/SGLang_storage --chunked-prefill-size 4096,8x NVIDIA A100-SXM4-80GB,Testing MMLU benchmark with chunked prefill and radix cache disabled for a 405B FP8 model.,BROKEN: Failed with RuntimeError: Failed to allocate memory for batch_prefill_tmp_v in AlignedAllocator. Likely due to the combination of --disable-radix-cache and --chunked-prefill-size.,broken,https://github.com/sgl-project/sglang/issues/1323,[Bug] RuntimeError in ModelTpServer
nvidia/Llama-3.1-8B-Instruct-FP8,python3 -m sglang.launch_server --model-path neuralmagic/Meta-Llama-3.1-405B-Instruct-FP8 --tp 8 --disable-radix-cache --host 0.0.0.0 --port 9942 --log-level debug --file-storage-pth /home/dave.makhervaks/SGLang_storage,8x NVIDIA A100-SXM4-80GB,Attempting to fix memory allocation error by removing explicit chunked-prefill-size while keeping radix cache disabled.,BROKEN: Continued to produce memory allocation errors because sglang enables chunked-prefill by default.,broken,https://github.com/sgl-project/sglang/issues/1323,[Bug] RuntimeError in ModelTpServer
nvidia/Llama-3.1-8B-Instruct-FP8,python3 -m sglang.launch_server --model-path neuralmagic/Meta-Llama-3.1-405B-Instruct-FP8 --tp 8 --disable-radix-cache --chunked-prefill-size -1 --host 0.0.0.0 --port 9942 --log-level debug --file-storage-pth /home/dave.makhervaks/SGLang_storage,8x NVIDIA A100-SXM4-80GB,Recommended configuration to fix memory allocation errors when disabling radix cache by explicitly disabling chunked prefill.,"Working for server stability, but user reported very poor accuracy on Hellaswag (0.256 vs expected higher values).",working,https://github.com/sgl-project/sglang/issues/1323,[Bug] RuntimeError in ModelTpServer
nvidia/Llama-3.3-70B-Instruct-FP8,python3 -m sglang.launch_server --model-path ~/.cache/huggingface/local/Llama-3.3-70B-Instruct-FP8 --tp-size=4,4x NVIDIA A100-SXM4-80GB,Attempting to serve a dynamic FP8 quantized Llama 3.3 70B model on hardware with CUDA compute capability < 8.9 (A100).,"BROKEN: Failed with `ImportError: vllm is not installed. To use CompressedTensorsW8A16Fp8, please install vllm`. The system incorrectly identified a need for vLLM as a fallback for the compressed tensors scheme when running on A100 GPUs.",broken,https://github.com/sgl-project/sglang/issues/11291,"[Bug] vllm is not installed. To use CompressedTensorsW8A16Fp8, please install vllm"
nvidia/Llama-3.3-70B-Instruct-FP8,python -m sglang.launch_server --model-path nvidia/Llama-3.3-70B-Instruct-FP8 --attention-backend triton,not specified,Workaround for functional issues when loading Llama 3.3 FP8 checkpoints with the default flashinfer backend.,Working fine. Using the triton attention backend avoids the issues encountered with flashinfer.,working,https://github.com/sgl-project/sglang/issues/7878,[Feature] Support Llama 3.3 FP8/NVFP4 functionally
nvidia/Llama-3.3-70B-Instruct-FP8,python -m sglang.launch_server --model-path nvidia/Llama-3.3-70B-Instruct-FP8 --attention-backend flashinfer,not specified,Loading Llama 3.3 FP8 checkpoints using the flashinfer attention backend.,Broken. Running into functional issues when using flashinfer as the attention backend for this model.,broken,https://github.com/sgl-project/sglang/issues/7878,[Feature] Support Llama 3.3 FP8/NVFP4 functionally
nvidia/Llama-3.3-70B-Instruct-FP8,python3 -m sglang.launch_server --model-path meta-llama/Llama-3.3-70B-Instruct --tp 8 --port 30000 --enable-piecewise-cuda-graph --piecewise-cuda-graph-max-tokens 8192,"Not explicitly stated, but implies 8 GPUs (tp 8)",Benchmarking Llama-3.3-70B performance with Piecewise CUDA Graph enabled for comparison against standard CUDA Graph.,Observation of performance drop in allreduce kernel during prefill. Latency increased from 370us to 461us.,working,https://github.com/sgl-project/sglang/pull/10062,Piecewise CUDA Graph Support & Torch Compile Backend
nvidia/Llama-3.3-70B-Instruct-FP8,python3 -m sglang.launch_server --model-path meta-llama/Llama-3.3-70B-Instruct --tp 8,"Not explicitly stated, but implies 8 GPUs (tp 8)",Baseline benchmarking of Llama-3.3-70B without Piecewise CUDA Graph to compare allreduce kernel performance.,Baseline prefill latency of 370us for the allreduce kernel.,working,https://github.com/sgl-project/sglang/pull/10062,Piecewise CUDA Graph Support & Torch Compile Backend
nvidia/Llama-3.3-70B-Instruct-FP8,"python -m sglang.launch_server --model-path nvidia/Llama-3.3-70B-Instruct-FP4 --mem-fraction 0.8 --cuda-graph-max-bs 128 --tp 8 --attention-backend flashinfer --quantization modelopt_fp4 --kv-cache-dtype auto --model-loader-extra-config '{""enable_multithread_load"": true, ""num_threads"": 8}'",8x NVIDIA B200,Reproduction of a performance regression in the prefill phase for FP4 MHA models on Blackwell architecture.,Broken/Regression: Encountered extremely high TTFT (Time to First Token) of ~13s on latest main compared to ~300ms on older versions. Suspected JIT compilation or issues with FP4 GEMM kernels.,broken,https://github.com/sgl-project/sglang/issues/11635,[Bug] Significant regression in server with mm_fp4 due to Flashinfer
nvidia/Llama-3.3-70B-Instruct-FP8,"SGLANG_USE_CUTLASS_BACKEND_FOR_FP4_GEMM=1 python -m sglang.launch_server --model-path nvidia/Llama-3.3-70B-Instruct-FP4 --mem-fraction 0.8 --cuda-graph-max-bs 128 --tp 8 --attention-backend flashinfer --quantization modelopt_fp4 --kv-cache-dtype auto --model-loader-extra-config '{""enable_multithread_load"": true, ""num_threads"": 8}'",8x NVIDIA B200,Testing if switching to the CUTLASS backend for FP4 GEMM resolves the significant TTFT regression.,Working: Setting the environment variable SGLANG_USE_CUTLASS_BACKEND_FOR_FP4_GEMM=1 resolved the hanging/latency issue.,working,https://github.com/sgl-project/sglang/issues/11635,[Bug] Significant regression in server with mm_fp4 due to Flashinfer
nvidia/Llama-3.3-70B-Instruct-FP8,python3 -m sglang.launch_server --model /models/meta-llama/llama-3-3-70b-instruct-fp8-dynamic/meta/llama-3-3-70b-instruct-fp8-dynamic --tp 2 --host 0.0.0.0 --enable-metrics --mem-frac 0.9 --max-total-tokens 131072 --tool-call-parser llama3 --port 8082,2x B200,Server setup for benchmarking Llama-3.3-70B FP8 on Blackwell architecture using SGLang. The user is investigating an unexpected performance anomaly regarding throughput scaling with concurrency.,"Working but showed anomalous behavior: significantly higher token throughput at concurrency 16 than at concurrency 1, despite higher TTFT (Time To First Token).",working,https://github.com/sgl-project/sglang/issues/7908,"[Bug] Unexpected Inference Speed Gain at Concurrency 16 vs 1 on Llama-3.3-70B (FP8, B200, SGLang Blackwell))"
nvidia/Llama-3.3-70B-Instruct-FP8,"benchmark --api-backend openai --api-base http://sglang-v0.4.1-llama3.3-70b-fp8-0:8082 --api-key dummy --api-model-name sglang-model --model-tokenizer /models/meta-llama/llama-3-3-70b-instruct-fp8-dynamic/meta/llama-3-3-70b-instruct-fp8-dynamic --task text-to-text --max-time-per-run 10 --max-requests-per-run 1000 --server-engine ""SGLang"" --server-gpu-type ""B200"" --server-version ""blackwell"" --server-gpu-count 2",2x B200,Benchmarking inference speed and latency of the Llama-3.3-70B FP8 model using the genai-bench 0.1.132 tool.,Detected that throughput scales non-linearly with concurrency; higher concurrency (16) outperformed lower concurrency (1) significantly.,working,https://github.com/sgl-project/sglang/issues/7908,"[Bug] Unexpected Inference Speed Gain at Concurrency 16 vs 1 on Llama-3.3-70B (FP8, B200, SGLang Blackwell))"
nvidia/Llama-3.3-70B-Instruct-FP8,python3 -m sglang.launch_server --model /Llama-3.3-70B-Instruct-FP8-Dynamic --speculative-algorithm EAGLE3 --speculative-draft-model-path /sglang-EAGLE3-LLaMA3.3-Instruct-70B --speculative-num-steps 3 --speculative-eagle-topk 2 --speculative-num-draft-tokens 4 --mem-fraction 0.6 --dtype float16 --tp-size 2 --max-running-requests 32,2x NVIDIA H100 NVL,Testing EAGLE3 speculative decoding with Llama-3.3-70B-Instruct-FP8 as the base model and a specific EAGLE3 draft model.,"BROKEN: The server fails during the capture of the 'draft extend CUDA graph'. It throws a RuntimeError: 'mat1 and mat2 shapes cannot be multiplied (128x18432 and 24576x6144)' in the Llama-EAGLE3 forward pass, likely due to a mismatch in hidden sizes introduced by the new extend CUDA graph feature.",broken,https://github.com/sgl-project/sglang/issues/7011,[Bug] Draft extend CUDA graph fails to load using llama 3.3 70b
nvidia/Llama-3.3-70B-Instruct-FP8,python3 benchmark/gsm8k/bench_sglang.py --num-shots 8 --num-questions 1319 --parallel 1319 --model-path nvidia/Llama-3.3-70B-Instruct-FP4 --quantization modelopt_fp4,,Evaluate end-to-end accuracy and throughput on the GSM8K benchmark for the FP4 quantized version of the model.,"Accuracy: 0.939, Invalid: 0.000, Latency: 65.557 s, Output throughput: 2229.165 token/s.",working,https://github.com/sgl-project/sglang/pull/3972,FP4 weight loading and inference (2/2)
nvidia/Llama-3.3-70B-Instruct-FP8,"import sglang as sgl

def main():
    prompts = [
        ""Hello, my name is"",
        ""The president of the United States is"",
        ""The capital of France is"",
        ""The future of AI is"",
    ]
    sampling_params = {""temperature"": 0.8, ""top_p"": 0.95}
    llm = sgl.Engine(model_path=""nvidia/Llama-3.3-70B-Instruct-FP4"", quantization=""modelopt_fp4"")

    outputs = llm.generate(prompts, sampling_params)
    for prompt, output in zip(prompts, outputs):
        print(""==============================="")
        print(f""Prompt: {prompt}\nGenerated text: {output['text']}"")

if __name__ == ""__main__"":
    main()",Tested successfully by author; encountered errors on 5090 device.,Quick test to verify model loading and basic text generation capabilities using the SGLang Engine API with FP4 quantization.,"Successfully generated coherent text responses describing a dog named Angelique, US government structure, Paris travel tips, and AI collaboration. However, a user on a 5090 GPU reported a 'RuntimeError: CUDA error: operation failed during capture' during CUDA graph initialization.",working,https://github.com/sgl-project/sglang/pull/3972,FP4 weight loading and inference (2/2)
nvidia/Llama-3.3-70B-Instruct-FP8,python3 -m sglang.launch_server --port=7080 --model-path=nvidia/Llama-3.3-70B-Instruct-FP8 --speculative-algorithm=EAGLE3 --speculative-draft-model-path=lmsys/sglang-EAGLE3-LLaMA3.3-Instruct-70B --speculative-num-steps=5 --speculative-eagle-topk=4 --speculative-num-draft-tokens=8 --trust-remote-code --tp=8 --dtype=bfloat16 --attention-backend=fa3 --chunked-prefill-size=16384 --mem-fraction-static=0.7 --host=0.0.0.0,8x NVIDIA H100 80GB HBM3,Testing EAGLE3 speculative decoding performance for Llama 3.3 70B across different sequence lengths.,Broken for long sequences. Performance degrades significantly when input sequence > 2k (draft model context window). At 1k input: 168.56 tok/s (1.5x speedup). At 10k input: 63.91 tok/s (worse than baseline 102.70 tok/s). Acceptance length drops from 3.2 to 1.2.,broken,https://github.com/sgl-project/sglang/issues/6783,[Bug] EAGLE3 perform worse with sequence length larger than the draft model context window
nvidia/Llama-3.3-70B-Instruct-FP8,python3 -m sglang.launch_server --port=7080 --model-path=nvidia/Llama-3.3-70B-Instruct-FP8 --speculative-algorithm=EAGLE3 --speculative-draft-model-path=lmsys/sglang-EAGLE3-LLaMA3.3-Instruct-70B --speculative-num-steps=3 --speculative-eagle-topk=2 --speculative-num-draft-tokens=4 --trust-remote-code --tp=8 --dtype=bfloat16 --attention-backend=flashinfer --chunked-prefill-size=16384 --mem-fraction-static=0.7 --host=0.0.0.0,8x NVIDIA H100 80GB HBM3,"Optimization suggestion for EAGLE3 speculative decoding, recommending flashinfer backend and reduced speculative steps/tokens.","Not explicitly benchmarked in the thread, but suggested as an improvement over the initial configuration.",experimental,https://github.com/sgl-project/sglang/issues/6783,[Bug] EAGLE3 perform worse with sequence length larger than the draft model context window
nvidia/Llama-3.3-70B-Instruct-FP8,python -m sglang.launch_server --model-path cortecs/Llama-3.3-70B-Instruct-FP8-Dynamic --tp 4 --mem-fraction-static 0.85 --context-length 65536 --max-running-requests 8,4x NVIDIA A10G (24GB),Testing W8A16 quantization compatibility in sglang version 0.4.5 for Llama-3.3-70B FP8.,BROKEN: Failed with NameError: name 'CompressedTensorsW8A16Fp8' is not defined. The user reported this configuration worked in version 0.4.4 but broke after upgrading to 0.4.5.,broken,https://github.com/sgl-project/sglang/issues/5135,[Bug] The 0.4.5 doesnt support W8A16 BUT 0.4.4 used to work fine - NameError: name 'CompressedTensorsW8A16Fp8' is not defined.
nvidia/Llama-3.3-70B-Instruct-FP8,python -m sglang.launch_server --model-path cortecs/Llama-3.3-70B-Instruct-FP8-Dynamic --tp 4 --mem-fraction-static 0.85 --context-length 65536 --enable-flashinfer-mla,4x NVIDIA A10G,Serve the Llama-3.3-70B-Instruct-FP8 model with multi-GPU tensor parallelism and optimized MLA (Multi-head Latent Attention) for processing concurrent PDF extraction requests.,"Broken: When sending multiple concurrent requests, responses get mixed/leaked between different prompts (e.g., text from Document 1 appears in the response for Document 2).",broken,https://github.com/sgl-project/sglang/issues/4593,[Bug] Cache leaking into other requests when serving multiple concurrent requests
nvidia/Llama-3.3-70B-Instruct-FP8,python3 examples/runtime/openai_tool_choice.py,8x NVIDIA H100 80GB HBM3,"Reproducing a bug where the model (Llama-3.3-70B-Instruct) fails to generate correct tool-call formatting in SGLang, leading to a NoneType error when parsing results.",FAILED: The model returned a text-based JSON object in the content field instead of populate the tool_calls field. This caused a 'TypeError: NoneType object is not subscriptable' because tool_calls was None. The author notes that the --tool-call-parser llama3 does not support Llama 3.3 yet and that the model fails to generate the <|python_tag|> token at higher temperatures.,broken,https://github.com/sgl-project/sglang/issues/4072,[Bug] Tool call with Llama3 models has inconsistent behavior with OpenAI
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,"python3 -m sglang.launch_server --model-path ""meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8""",Nvidia GB200 Blackwell (2 GPUs visible in nvidia-smi),Initial server launch to test model compatibility on the new Blackwell architecture.,FAILED: ModuleNotFoundError: No module named 'sgl_kernel'. The user encountered a dependency issue where sgl-kernel was either not installed correctly for the architecture or not found by the Python environment.,broken,https://github.com/sgl-project/sglang/issues/7504,[Bug] No module name 'sgl_kernel'
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,python -m sglang.launch_server --model-path nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8 --port 30000,8x NVIDIA H100 80GB HBM3,Serve the Llama-4-Maverick-17B model as a backend for the SGLang frontend to process multimodal (image + text) prompts.,"BROKEN: When running multimodal prompts via the SGLang frontend, it triggers a `KeyError: 'pixel_values'` in the `mllama4.py` processor. The user noted that the model fails even with simple test images despite working via OpenAI-compatible endpoints.",broken,https://github.com/sgl-project/sglang/issues/5721,[Bug] Key error on 'pixel_values' when running multi-modal prompts with SGLang frontend for Llama 4
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,python -m sglang.launch_server --model-path /root/.cache/huggingface/hub/models--nvidia--Llama-4-Maverick-17B-128E-Instruct-FP8/snapshots/e91306aa6edc0a8b551b1815027163646638d252 --tp 8 --attention-backend trtllm_mha --trust-remote-code,Mentioned as compatible with Blackwell and Hopper (8 GPUs used based on --tp 8),Enabling and benchmarking the trtllm_mha attention backend for Llama 4 Maverick to improve performance over the Triton backend.,"Working. Achieved significant performance gains: Total token throughput increased from 2468.67 to 2719.18 tok/s (+10.2%), and Mean E2E Latency decreased from 6632.70 ms to 6022.57 ms (-9.2%).",working,https://github.com/sgl-project/sglang/pull/12003,Enable Llama 4 + TRTLLM MHA
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,python3 -m sglang.launch_server --model-path meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 --speculative-algorithm EAGLE3 --speculative-draft-model-path nvidia/Llama-4-Maverick-17B-128E-Eagle3 --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4 --mem-fraction-static 0.6 --cuda-graph-max-bs 2 --tp 8 --context-length 8192 --trust-remote-code --host 0.0.0.0 --port 30000,8xH200,Performance and accuracy testing of Llama-4-Maverick-17B-128E-Instruct-FP8 with EAGLE3 speculative decoding.,Working. Throughput reached 216.697 token/s (vs 161.665 without Eagle). Accuracy on gsm8k was 0.975. Note: User reported failure due to 'hf_quant_config.json' which requires manual removal to pass quantization checks.,working,https://github.com/sgl-project/sglang/pull/6985,support llama4 eagle3
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,"python -m sglang.launch_server --model-path /mnt/external-quantized-models/models/nvidia__Llama-4-Maverick-17B-128E-Instruct-FP8 --tp 8 --attention-backend triton --model-loader-extra-config '{""enable_multithread_load"": true, ""num_threads"": 8}' --trust-remote-code",,Attempting to launch the model server using ModelOpt FP8 checkpoints on the latest main branch.,BROKEN: Failed with ValueError: 'Unsupported quantization config found for modelopt_fp8' and later 'Quantization method specified in the model config (modelopt_fp8) does not match the quantization method specified in the `quantization` argument (modelopt)'.,broken,https://github.com/sgl-project/sglang/pull/11769,Fix ModelOpt load FP8 checkpoints
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,python3 benchmark/gsm8k/bench_sglang.py --num-shots 8 --num-questions 1319 --parallel 200,,Accuracy testing after applying fixes to the ModelOpt FP8 loading logic.,"Working: Accuracy: 0.931, Invalid: 0.000, Latency: 34.958 s, Output throughput: 3927.518 token/s.",working,https://github.com/sgl-project/sglang/pull/11769,Fix ModelOpt load FP8 checkpoints
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,python -u -m sglang.launch_server --model-path /path/to/llama4-maverick --host 0.0.0.0 --port 30000 --tensor-parallel-size 8 --enable-lora --max-lora-rank 64 --lora-target-modules all --mem-fraction-static 0.85,8 X H100,"Reproduce a bug where LoRA adapter loading fails when target modules use prefixed names (e.g., feed_forward.gate_proj) despite using --lora-target-modules all.","Failure during adapter loading. The validation stage blocks the adapter because the prefixed module names in the adapter config are not recognized in the supported modules list, preventing the normalization logic from executing.",broken,https://github.com/sgl-project/sglang/issues/11034,"[Bug] LoRA adapter loading fails when target modules use prefixed names (e.g., feed_forward.gate_proj) despite proper normalization logic"
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,"python -m sglang.launch_server --model-path /mnt/external-quantized-models/models/nvidia__Llama-4-Maverick-17B-128E-Instruct-FP8 --tp 8 --attention-backend triton --model-loader-extra-config '{""enable_multithread_load"": true, ""num_threads"": 8}' --trust-remote-code --moe-runner-backend triton --quantization modelopt_fp8",SM100 (mentioned in description as the target),Baseline benchmarking of Llama 4 Maverick with the Triton MoE runner backend before optimizations.,"Working. Request throughput: 1.21 req/s, Total token throughput: 2468.67 tok/s, Mean E2E Latency: 6632.70 ms, Mean ITL: 6.34 ms.",working,https://github.com/sgl-project/sglang/pull/11928,Use Flashinfer TRT-LLM as Llama 4 compatible MoE backend
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,"python -m sglang.launch_server --model-path /mnt/external-quantized-models/models/nvidia__Llama-4-Maverick-17B-128E-Instruct-FP8 --tp 8 --attention-backend triton --model-loader-extra-config '{""enable_multithread_load"": true, ""num_threads"": 8}' --trust-remote-code --moe-runner-backend flashinfer_trtllm --quantization modelopt_fp8",SM100 (mentioned in description as the target),Benchmarking the performance improvements using the new Flashinfer TRT-LLM MoE backend for per-tensor FP8.,"Working. Significant performance gain: Request throughput increased to 1.42 req/s (+17.4%), Total throughput to 2907.91 tok/s (+17.8%), Mean E2E Latency reduced to 5630.01 ms (-15.1%).",working,https://github.com/sgl-project/sglang/pull/11928,Use Flashinfer TRT-LLM as Llama 4 compatible MoE backend
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,"python -m sglang.launch_server --model-path /mnt/external-quantized-models/models/nvidia__Llama-4-Maverick-17B-128E-Instruct-FP8 --tp 8 --attention-backend triton --model-loader-extra-config '{""enable_multithread_load"": true, ""num_threads"": 8}' --trust-remote-code",8x NVIDIA B200,Serve the model as a backend server for high-concurrency benchmarking.,"BROKEN: Crashes with a CUDA illegal memory access (IMA) error during high concurrency. Specifically, the scheduler process hits an exception in `filter_batch` while calculating `seq_lens_sum`.",broken,https://github.com/sgl-project/sglang/issues/11770,[Bug] Scheduler IMA at high concurrency
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,python3 -m sglang.launch_server --model /models/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 --port=8080 --tp-size=8 --mem-fraction-static=0.82 --context-length=524288 --tool-call-parser=pythonic --chat-template=examples/chat_template/tool_chat_template_llama4_pythonic.jinja --attention-backend=fa3 --enable-multimodal --disable-fast-image-processor --mm-attention-backend=fa3 --hybrid-kvcache-ratio=1,8x NVIDIA H100 80GB HBM3,Reproduce a server crash related to SWARadixTree sanity check errors when sending long context requests (512k context length) and multimodal inputs.,"BROKEN: Server crashes with 'Exception: SWA Radix tree sanity check failed... Incorrect LRU list, self.swa=False, x: x.id=7 != x_lru: x_lru.id=8'. This occurs when a request's input tokens exceed the SWA cache capacity (35,848 tokens in this config).",broken,https://github.com/sgl-project/sglang/issues/8931,[Bug] SWARadixTree throws sanity check error
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,cd /sgl-workspace/sglang ; /usr/bin/env /bin/python3 /root/.vscode-server/extensions/ms-python.debugpy-2025.10.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher 42543 -- -m sglang.launch_server --model /models/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 --port=8080 --tp-size=8 --mem-fraction-static=0.82 --context-length=524288 --tool-call-parser=pythonic --chat-template=examples/chat_template/tool_chat_template_llama4_pythonic.jinja --attention-backend=fa3 --enable-multimodal --disable-fast-image-processor --mm-attention-backend=fa3 --hybrid-kvcache-ratio=1,8x NVIDIA H100 80GB HBM3,Debug the Llama-4 Maverick model server using VS Code debugger to identify why the SWARadixTree throws a sanity check error.,BROKEN: Identical crash logs showing 'AssertionError: Incorrect LRU list' in `swa_radix_cache.py`. Analysis indicates it happens when input context size significantly exceeds the SWA cache size.,broken,https://github.com/sgl-project/sglang/issues/8931,[Bug] SWARadixTree throws sanity check error
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,python3 -m sglang.launch_server --model /models/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 --port=8080 --tp-size=8 --mem-fraction-static=0.85 --context-length=524288 --tool-call-parser=pythonic --chat-template=examples/chat_template/tool_chat_template_llama4_pythonic.jinja --attention-backend=fa3 --enable-multimodal,8*H100 80GB HBM3,"Serve the Llama-4-Maverick-17B-128E-Instruct-FP8 model with multimodal support and high context length, using FlashAttention-3 and pythonic tool calling.",BROKEN: The server experienced an Out of Memory (OOM) error and crashed when processing 5 large images (4096 * 4096). The error occurred both when using the GPU-based fast image processor and when falling back to CPU image processing.,broken,https://github.com/sgl-project/sglang/issues/8468,[Bug] Llama4 Maverick-FP8 OOM when sending images
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,"docker run --gpus all --shm-size 32g -p 30000:30000 -v ""$HF_HOME"":/root/.cache/huggingface --env ""HF_TOKEN=$HF_TOKEN"" --ipc=host lmsysorg/sglang:v0.5.0rc2-cu126 python3 -m sglang.launch_server --host 0.0.0.0 --port 30000 --trust-remote-code --model-path meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 --tp-size 8 --attention-backend fa3",8x NVIDIA H200 (Compute Capability 9.0),Reproducing a bug where the Llama-4-Maverick model causes an Exception in ASGI application (SystemExit/CancelledError) when launched with SGLang.,"Broken. The server encounters a SystemExit: 0 followed by an asyncio.exceptions.CancelledError in the ASGI application (uvicorn/fastapi), leading to a crash or failed request handling.",broken,https://github.com/sgl-project/sglang/issues/9280,[Bug] Llama-4-Maverick @ lmsysorg/sglang:v0.5.0rc2-cu126 causes Exception in ASGI application
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,python3 -m sglang.bench_serving --backend sglang --model meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 --dataset-name random --warmup-requests 10 --num-prompts 30000 --random-input 128 --random-output 128,8x NVIDIA H200 (Compute Capability 9.0),Benchmarking serving performance for the Llama-4-Maverick model using a random dataset to measure throughput and stability.,Broken. Part of the reproduction steps for the reported ASGI application error; the benchmarking likely triggers the crash described in the issue logs.,broken,https://github.com/sgl-project/sglang/issues/9280,[Bug] Llama-4-Maverick @ lmsysorg/sglang:v0.5.0rc2-cu126 causes Exception in ASGI application
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,python3 -m sglang.launch_server --port=8000 --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --tp=8 --trust-remote-code --mem-fraction-static 0.7 --context-length=131072 --attention-backend=fa3 --enable-multimodal --tool-call-parser=pythonic --chat-template=llama-4 --cuda-graph-max-bs=32 --host=0.0.0.0 --quantization=modelopt,8xH100,Reproducing a bug where the Llama 4 Scout FP8 checkpoint failed to load in Sglang (v0.5.0rc2) due to quantization config mismatches.,BROKEN: Failed with `ValueError: Cannot find any of ['quantization'] in the model's quantization config.`,broken,https://github.com/sgl-project/sglang/issues/9758,[Bug] Serving Quantized Llama 4 Scout with Sglang
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,python3 -m sglang.launch_server --port=8000 --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --tp=8 --trust-remote-code --mem-fraction-static 0.7 --context-length=131072 --attention-backend=fa3 --enable-multimodal --tool-call-parser=pythonic --chat-template=llama-4 --cuda-graph-max-bs=32 --host=0.0.0.0 --quantization=modelopt,8xH100,Testing the model on a newer Sglang version (v0.5.3rc0) to see if loading and image queries work.,"BROKEN: Model loaded and text queries worked, but image queries caused a server crash with `AssertionError: x is not contiguous` in `static_quant_fp8`.",broken,https://github.com/sgl-project/sglang/issues/9758,[Bug] Serving Quantized Llama 4 Scout with Sglang
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,"curl http://localhost:8000/v1/chat/completions -H ""Content-Type: application/json"" -d '{""model"":""meta/llama-4-maverick-17b-128e-instruct-maas"", ""messages"":[{""role"": ""user"", ""content"":[{""type"": ""text"", ""text"": ""What is in this picture?""},{""type"": ""image_url"", ""image_url"":{""url"": ""data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAAHElEQVQI12P4//8/w38GIAXDIBKE0DHxgljNBAAO9TXL0Y4OHwAAAABJRU5ErkJggg==""}}]}]}'",8xH100,Testing multimodal capabilities (image query) on the running Sglang server.,FAILED: curl returned `(52) Empty reply from server` because the backend model worker crashed.,broken,https://github.com/sgl-project/sglang/issues/9758,[Bug] Serving Quantized Llama 4 Scout with Sglang
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,python3 -m sglang.launch_server --port=8000 --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --tp=8 --trust-remote-code --mem-fraction-static 0.7 --context-length=131072 --attention-backend=fa3 --enable-multimodal --tool-call-parser=pythonic --chat-template=llama-4 --cuda-graph-max-bs=32 --host=0.0.0.0 --quantization=modelopt,8xH100 (implied),Testing model serving with latest version including multimodal support.,BROKEN: Failed with `OSError: Can't load image processor for 'nvidia/Llama-4-Scout-17B-16E-Instruct-FP8'`. User identified that a specific transformers git commit is required to fix this.,broken,https://github.com/sgl-project/sglang/issues/9758,[Bug] Serving Quantized Llama 4 Scout with Sglang
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,python3 -m sglang.launch_server --model /models/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 --port=8080 --tp-size=8 --mem-fraction-static=0.8 --context-length=524288 --tool-call-parser=pythonic --chat-template=examples/chat_template/tool_chat_template_llama4_pythonic.jinja --attention-backend=fa3 --enable-multimodal,8x NVIDIA H100 80GB HBM3,Reproduction of a bug involving high CUDA graph memory usage when launching Llama4-Maverick in SGLang.,Observation of high CUDA graph memory (6.19GB) compared to vLLM. The log shows: 'Capture cuda graph end. Time elapsed: 19.50 s. mem usage=6.19 GB. avail mem=5.24 GB.',broken,https://github.com/sgl-project/sglang/issues/8538,[Bug] Llama4 Large Cuda Graph Memory
nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8,"HF_TOKEN=$<your-hf-token> docker run -itd --gpus ""device=0,1,2,3,4,5,6,7"" --shm-size 10g --ulimit nofile=65535:65535 --network host --name vllm-prod-0.9.2-llama4-maverick vllm/vllm-openai:v0.9.2 --model=meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 --served-model-name=vllm-model --tensor-parallel-size=8 --max-model-len=524288 --limit-mm-per-prompt=image=10",8x NVIDIA H100 80GB HBM3,Comparison of CUDA graph memory usage between SGLang and vLLM for the same model.,vLLM v0.9.2 successfully captures CUDA graphs using significantly less memory (1.40 GiB) despite using a larger max batch size (512 vs SGLang's 160).,working,https://github.com/sgl-project/sglang/issues/8538,[Bug] Llama4 Large Cuda Graph Memory
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,"python -m sglang.launch_server --model nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --mem-fraction-static 0.8 --attention-backend triton --trust-remote-code --quantization modelopt_fp8 --kv-cache-dtype=fp8_e4m3 --context-length=131072 --trust-remote-code --quantization modelopt --model-loader-extra-config ""{\""enable_multithread_load\"": true, \""num_threads\"": 8}"" --tp 4",,Testing model execution without piecewise CUDA graphs to ensure baseline functionality remains intact after code changes for Llama 4 support.,Confirmed working by the author ('still works'). The command successfully launches the server with specific FP8 quantization (modelopt) and 4-way tensor parallelism.,working,https://github.com/sgl-project/sglang/pull/12424,wip piecewise llama 4
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,python3 -m sglang.launch_server --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --tp=8 --trust-remote-code --mem-fraction-static 0.7 --context-length=131072 --attention-backend=fa3 --enable-multimodal --tool-call-parser=pythonic --chat-template=llama-4 --cuda-graph-max-bs=48 --host=0.0.0.0 --speculative-algorithm=EAGLE3 --speculative-num-steps=3 --speculative-eagle-topk=4 --speculative-num-draft-tokens=8 --speculative-draft-model-path=/models/llama-4-scout-eagle/state_7 --port=8000,Mention of TP=8 (likely 8 GPUs),"Testing model serving with speculative decoding (EAGLE3), FlashAttention-3, and specific runtime optimizations like CUDA graphs and custom tool-call parsing.",BROKEN: Failed during model loading with a KeyError: 'language_model.model.layers.29.self_attn.qkv_proj.k_scale'. The author identified this as a regression in scale parameter remapping logic for the Llama4 architecture.,broken,https://github.com/sgl-project/sglang/pull/11282,fix: correct scale parameter remapping logic in Llama4ForConditionalGeneration
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,"python3 -m sglang.launch_server --model-path=/opt/dlami/nvme/models/Llama-4-Scout-17B-16E-Instruct/ --tp=8 --trust-remote-code --mem-fraction-static=0.7 --context-length=131072 --kv-cache-dtype=fp8_e4m3 --attention-backend=fa3 --model-loader-extra-config '{""enable_multithread_load"": true, ""num_threads"": 8}'",TP8 (likely 8 GPUs),Launching the SGLang server for Llama 4 Scout to test weightless qk_norm handling and multi-threaded loading.,Successful loading. Eliminated previous 'Some weights are not initialized' warnings related to qk_norm. Memory usage reported as 30.27 GB (avail 107.74 GB). Multi-thread loading completed in approximately 2 minutes 51 seconds. Utilized FP8 KV cache and FlashAttention 3.,working,https://github.com/sgl-project/sglang/pull/12813,add weightless qk norm to RMSNorm interface for Llama 4
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,python3 benchmark/gsm8k/bench_sglang.py --num-shots 8 --num-questions 1319 --parallel 500,TP8 (via the server launched in the previous command),Benchmarking the model accuracy and throughput on the GSM8K dataset using 8-shot prompting and high parallelism.,"Accuracy: 0.926, Output throughput: 5361.274 token/s, Latency: 25.235 s for 1319 questions.",working,https://github.com/sgl-project/sglang/pull/12813,add weightless qk norm to RMSNorm interface for Llama 4
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,python3 -m sglang.launch_server --model-path meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 --speculative-algorithm EAGLE3 --speculative-draft-model-path nvidia/Llama-4-Maverick-17B-128E-Eagle3 --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4 --mem-fraction-static 0.6 --cuda-graph-max-bs 2 --tp 8 --context-length 8192 --trust-remote-code --host 0.0.0.0 --port 30000,8xH200,Performance and accuracy testing of Llama-4-Maverick-17B-128E-Instruct-FP8 with EAGLE3 speculative decoding.,Working. Output throughput: 216.697 token/s. GSM8k accuracy: 0.975. Note: Users reported issues with 'hf_quant_config.json' and may need to remove it manually for successful loading.,working,https://github.com/sgl-project/sglang/pull/6985,support llama4 eagle3
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,python3 -m sglang.launch_server --model-path meta-llama/Llama-4-Maverick-17B-128E-Instruct --context-length 65536 --tp 8 --speculative-algorithm EAGLE3 --speculative-draft-model-path nvidia/Llama-4-Maverick-17B-128E-Eagle3 --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4 --trust-remote-code,8xH200,Testing speculative decoding performance for the non-FP8 model variant with the EAGLE3 draft model.,Working. Achieved speed of 263.30 token/s and average acceptance length of 2.47 using the /v1/chat/completions API. Performance was significantly lower using the /generate API (125.52 token/s).,working,https://github.com/sgl-project/sglang/pull/6985,support llama4 eagle3
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,"SGLANG_IMAGE_PROCESSOR_FAST_DEVICE=cpu CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 HIP_DISABLE_FLASHDECODE=1 HIP_HEAD_REDUCE=0 HIP_DEBUG_LAST_DENSE=128 HIP_DEBUG_FORCE_DENSE_DECODE=1 CUDA_LAUNCH_BLOCKING=0 HIP_DEBUG=0 python -m sglang.launch_server --model-path meta-llama/Llama-4-Scout-17B-16E-Instruct --port 20000 --tp 8 --max-total-tokens 200000 --context-length 200000 --cuda-graph-bs 1 --max-running-req 1 --chunked-prefill-size -1 --hip-attention-config '{""using_extend"": false, ""dense_layers"": [0, 1, 2, 3]}' --disable-radix-cache --attention-backend flashinfer --disable-cuda-graph --enable-multimodal --chat-template llama-4 --enable-hip-attention",8x NVIDIA GPUs (CUDA_VISIBLE_DEVICES 0-7),Testing Llama-4 Scout with HiP (Hierarchically Pruned) Attention for long-context vision/multimodal tasks up to 200k tokens.,Working/Experimental. Achieved 51.07 accuracy on LongVideoBench (first 748 samples) at 256k tokens config. Used to demonstrate HiP's ability to handle vision models and extremely long contexts with minimal accuracy degradation.,working,https://github.com/sgl-project/sglang/pull/3930,[Feature] Support Efficient Sparse HiP Attention (InfiniteHiP) with Long-Context Generalization and KV Offloading Capabilties
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,"python -c 'import os; import sglang as sgl; prompts = [""Hello, my name is"", ""The president of the United States is"", ""The capital of France is"", ""The future of AI is""]; sampling_params = {""temperature"": 0.7, ""top_p"": 0.9, ""max_new_tokens"": 128, ""skip_special_tokens"": True}; llm = sgl.Engine(model_path=""/home/scratch.omniml_data_2/HF_model_hub/Llama-4-Scout-17B-16E-Instruct-FP4"", quantization=""modelopt_fp4"", tp_size=4, context_length=1024, attention_backend=""flashinfer""); outputs = llm.generate(prompts, sampling_params); [print(f""Prompt: {p}\nGenerated: {o[\""text\""]}\n"") for p, o in zip(prompts, outputs)]'","Not explicitly specified, but requires at least 4 GPUs (tp_size=4) and at least FP8/FP4 compatible hardware (likely H100/H200 or B200 class for fp4 support).",Accuracy tests to verify the NVIDIA ModelOpt NVFP4 quantization workflow for Llama 4 models in SGLang. Specifically testing weight/scale loading logic for the FP4 checkpoint.,"Working. The model successfully generated coherent text for all four test prompts. For example, for the prompt 'The capital of France is', it correctly output 'Paris, the City of Light...'. The logs show the model successfully loaded weight scales for experts and linear layers.",working,https://github.com/sgl-project/sglang/pull/9526,Support modelopt llama4 nvfp4 workflow and fix issues
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,"python3 -m sglang.launch_server --model-path=/opt/dlami/nvme/models/Llama-4-Scout-17B-16E-Instruct/ --tp=8 --trust-remote-code --mem-fraction-static=0.7 --context-length=131072 --kv-cache-dtype=fp8_e4m3 --attention-backend=fa3 --model-loader-extra-config '{""enable_multithread_load"": true, ""num_threads"": 8}'","Mentioned ip-10-40-0-228, likely 8-GPU node due to tp=8","Launch serving for Llama 4 Scout with optimal configuration for shared experts overlap, testing the switch from torch.cuda.current_stream to get_current_device_stream_fast(). Used as the server backend for performance benchmarking.",Working. Serves as base for benchmarks. Performance results show a significant 25.4% improvement in P99 TTFT (reduced from 288.02ms to 214.91ms) compared to the old stream management implementation.,working,https://github.com/sgl-project/sglang/pull/12811,use fast stream instead of torch.cuda.current_stream in llama 4 shared experts overlap
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,python3 -m sglang.bench_serving --backend sglang --num-prompts 64 --dataset-name random --random-input-len 1024 --random-output-len 1024 --random-range-ratio 1 --max-concurrency=8 --flush-cache,,Benchmark serving performance (throughput and latency) for the Llama 4 Scout model comparing before/after optimization of device stream handling.,"Total Token Throughput (tok/s): 1823.35, Mean TTFT (ms): 198.57, P99 TTFT (ms): 214.91. Showed minor throughput gains but significant tail latency reduction.",working,https://github.com/sgl-project/sglang/pull/12811,use fast stream instead of torch.cuda.current_stream in llama 4 shared experts overlap
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,python3 benchmark/gsm8k/bench_sglang.py --num-shots 8 --num-questions 1319 --parallel 500,root@ip-10-40-0-228,Evaluate model accuracy and throughput on the GSM8K dataset using the optimized server setup.,"Accuracy: 0.920, Latency: 31.533 s, Output throughput: 4312.445 token/s. Validated that accuracy remains high and processing speed is excellent at high parallelism.",working,https://github.com/sgl-project/sglang/pull/12811,use fast stream instead of torch.cuda.current_stream in llama 4 shared experts overlap
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,"python3 -m sglang.launch_server --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --tp=8 --trust-remote-code --mem-fraction-static=0.7 --context-length=131072 --attention-backend=fa3 --enable-multimodal --tool-call-parser=pythonic --cuda-graph-max-bs=512 --kv-cache-dtype=fp8_e4m3 --model-loader-extra-config '{""enable_multithread_load"": true, ""num_threads"": 8}'","TP=8 (likely 8x GPUs, instance root@ip-10-40-9-126)",Launch a model server to benchmark performance gains from fixing the shared and routed expert overlap in Llama 4. Testing high-concurrency and long-context configurations.,"Working. Served as the backend for multiple benchmarks. Combined with the PR's optimization, it showed a 20% speed increase at BS=1 (from 141.64 to 169.46 tok/s) and improvements at higher batch sizes.",working,https://github.com/sgl-project/sglang/pull/12405,Fix the shared expert & routed expert overlap in Llama 4
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,"python3 -m sglang.launch_server --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP4 --tp=8 --quantization modelopt_fp4 --moe-runner-backend=flashinfer_cutlass --attention-backend trtllm_mha --trust-remote-code --mem-fraction-static=0.7 --context-length=131072 --kv-cache-dtype=fp8_e4m3 --model-loader-extra-config '{""enable_multithread_load"": true, ""num_threads"": 8}'","Not specified, likely 8-GPU setup based on TP=8",Testing FP4 loading and serving performance with MoE flashinfer_cutlass backend and TRT-LLM MHA attention backend.,"Server launched. GSM8K benchmark achieved 6298.596 token/s output throughput and 0.884 accuracy. However, MMLU Pro accuracy (0.7239) was lower than the target (>=0.74), suggesting potential accuracy issues in flashinfer_cutlass MoE usage.",experimental,https://github.com/sgl-project/sglang/pull/12649,fix: llama 4 fp4 loading
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,"lm_eval --model local-chat-completions --model_args model=nvidia/Llama-4-Scout-17B-16E-Instruct-FP4,base_url=http://localhost:30000/v1/chat/completions,num_concurrent=512,timeout=999999,max_gen_toks=2048 --tasks mmlu_pro --batch_size 512 --apply_chat_template --num_fewshot 0",,Evaluating the accuracy of the FP4 quantized model on the MMLU Pro benchmark.,"Accuracy was 0.7239, which was considered too low compared to the BF16 baseline of 0.749.",experimental,https://github.com/sgl-project/sglang/pull/12649,fix: llama 4 fp4 loading
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,"python3 -m sglang.launch_server --model-path=/opt/dlami/nvme/models/Llama-4-Scout-17B-16E-Instruct/ --tp=8 --trust-remote-code --mem-fraction-static=0.7 --context-length=131072 --attention-backend=fa3 --model-loader-extra-config '{""enable_multithread_load"": true, ""num_threads"": 8}'",H200 (8-GPU setup based on TP=8),Baseline performance and accuracy test using BF16 (non-quantized) configuration on H200 GPUs.,Working. GSM8K output throughput: 4169.418 token/s with 0.918 accuracy. MMLU Pro accuracy: 0.749.,working,https://github.com/sgl-project/sglang/pull/12649,fix: llama 4 fp4 loading
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,python3 benchmark/kernels/fbgemm/benchmark_fbgemm_grouped_gemm.py --model meta-llama/Llama-4-Scout-17B-16E-Instruct --tp-size 8,H100 TP8,Benchmark FBGEMM Grouped GEMM performance vs SGLang Grouped GEMM for the Llama-4-Scout-17B MoE model using BF16/FP16.,"FBGEMM showed significant performance improvements over SGLang for most batch sizes. For example, at batch_size 1024, FBGEMM latency was 0.339616ms vs SGLang's 0.693824ms.",working,https://github.com/sgl-project/sglang/pull/6924,add fbgemm moe grouped gemm kernel benchmark
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,python3 benchmark/kernels/fbgemm/benchmark_fbgemm_grouped_gemm.py --model meta-llama/Llama-4-Scout-17B-16E-Instruct --tp-size 8 --use-fp8-w8a8,H100 TP8,Benchmark FBGEMM Grouped GEMM performance vs SGLang Grouped GEMM for the Llama-4-Scout-17B MoE model using FP8 W8A8 quantization.,"FBGEMM achieved better performance than SGLang, especially at higher batch sizes. At batch_size 4096, FBGEMM latency was 0.383872ms compared to SGLang's 2.766432ms. Observed a warning regarding warp specialization being disabled on Triton 3.2.0.",working,https://github.com/sgl-project/sglang/pull/6924,add fbgemm moe grouped gemm kernel benchmark
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,"python -c 'import sglang as sgl; llm = sgl.Engine(model_path=""/home/scratch.omniml_data_2/zhiyuc/checkpoints/Llama-4-Scout-17B-16E-Instruct-fp8"", quantization=""modelopt"", tp_size=8, context_length=4096); prompts = [""Hello, my name is"", ""The president of the United States is"", ""The capital of France is"", ""The future of AI is""]; sampling_params = {""temperature"": 0.7, ""top_p"": 0.9, ""max_new_tokens"": 128, ""skip_special_tokens"": True}; outputs = llm.generate(prompts, sampling_params); [print(f""Prompt: {p}\nGenerated: {o[\""text\""]}\n"") for p, o in zip(prompts, outputs)]'","TP size 8 (likely 8x GPUs, e.g., A100 or H100)","Testing the deployment and generation of ModelOpt Llama 4 FP8 MoE checkpoint in SGLang. The script verifies that the newly introduced ModelOptFp8MoEMethod correctly handles weights, scales, and kernel invocation.",Working. The model successfully loaded with 8-way tensor parallelism and generated coherent responses for all test prompts. Example output: 'The capital of France is Paris...'. Verified both the original Llama 4 and Meta's Llama 4 FP8 models work as well.,working,https://github.com/sgl-project/sglang/pull/7129,Enable ModelOpt Llama4 fp8 checkpoint deployment in SGLang
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,"python3 -m sglang.launch_server --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP4 --tp=8 --quantization modelopt_fp4 --moe-runner-backend=flashinfer_cutlass --attention-backend trtllm_mha --trust-remote-code --mem-fraction-static=0.7 --context-length=131072 --kv-cache-dtype=fp8_e4m3 --model-loader-extra-config '{""enable_multithread_load"": true, ""num_threads"": 8}'",B200 (TP8),Reproduction of a bug where Llama 4 Scout FP4 loading/inference results in significant accuracy loss (around 4%) on GSM8K and MMLU benchmarks.,The command runs but shows accuracy regression. MMLU Pro score was 0.7279 compared to a target of ~0.75 for BF16. GSM8K accuracy was 0.88 instead of the expected 0.92. Suspected cause is the MoE runner backend usage or incorrect scaling factor handling.,broken,https://github.com/sgl-project/sglang/issues/12857,[Bug] Llama 4 + FP4 Compatibility
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,python3 benchmark/gsm8k/bench_sglang.py --num-shots 8 --num-questions 1319 --parallel 500,B200 (TP8),Benchmark GSM8K accuracy to verify model performance after loading with the server command.,"Yielded an accuracy of 0.88. The author notes it should reach 0.92 in a correct scenario, indicating a 4% accuracy loss.",experimental,https://github.com/sgl-project/sglang/issues/12857,[Bug] Llama 4 + FP4 Compatibility
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,"python3 -m sglang.launch_server --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --tp=8 --ep-size=8 --trust-remote-code --mem-fraction-static=0.7 --context-length=131072 --kv-cache-dtype=fp8_e4m3 --quantization=modelopt --attention-backend=fa3 --model-loader-extra-config '{""enable_multithread_load"": true, ""num_threads"": 8}'","Not explicitly stated, but implies an 8-GPU setup (TP8 EP8)",Reproduction of a bug report involving Index Errors when running Llama 4 with Expert Parallelism (EP) and FP8 quantization via ModelOpt using the FA3 attention backend.,BROKEN: Fails during model loading with 'IndexError: index 2 is out of bounds for dimension 0 with size 2' in the MoE weight loader implementation (mllama4.py).,broken,https://github.com/sgl-project/sglang/issues/12577,[Bug] IndexError with Llama 4 + EP
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,python3 -m sglang.launch_server --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --tp=8 --trust-remote-code --mem-fraction-static 0.7 --context-length=131072 --attention-backend=fa3 --enable-multimodal --tool-call-parser=pythonic --cuda-graph-max-bs=48 --host=0.0.0.0 --port=8000,"Not explicitly stated, but implies 8 GPUs (tp=8)",Attempting to launch the server with the native FP8 model using default quantization settings to verify integration.,Failed with `ValueError: Unknown quantization method: modelopt`. The issue was caused by the server trying to use the legacy 'modelopt' flag which wasn't correctly registered in the build at that time.,broken,https://github.com/sgl-project/sglang/pull/10154,Enable native ModelOpt quantization support (3/3)
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,python3 -m sglang.launch_server --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --tp=8 --trust-remote-code --mem-fraction-static 0.7 --context-length=131072 --attention-backend=triton --enable-multimodal --tool-call-parser=pythonic --cuda-graph-max-bs=48 --host=0.0.0.0 --port=8000 --quantization=modelopt_fp8,"Not explicitly stated, but implies 8 GPUs (tp=8)",Testing the specific 'modelopt_fp8' quantization flag for the Llama-4-Scout model.,Failed with `launch_server.py: error: argument --quantization: invalid choice: 'modelopt_fp8'`. The quantization choice was not yet accepted by the CLI argument parser in that specific PR iteration.,broken,https://github.com/sgl-project/sglang/pull/10154,Enable native ModelOpt quantization support (3/3)
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,python3 -m sglang.launch_server --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --tp=8 --trust-remote-code --mem-fraction-static 0.7 --context-length=131072 --attention-backend=triton --enable-multimodal --tool-call-parser=pythonic --cuda-graph-max-bs=48 --host=0.0.0.0 --port=8000 --quantization=modelopt,"Not explicitly stated, but implies 8 GPUs (tp=8)",Testing the 'modelopt' quantization flag to see if auto-detection would work for the model.,Failed with `ValueError: Unknown quantization method: modelopt`. The system did not recognize 'modelopt' as a valid entry in the internal `_verify_quantization` check.,broken,https://github.com/sgl-project/sglang/pull/10154,Enable native ModelOpt quantization support (3/3)
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,python3 -m sglang.launch_server --port=8000 --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --tp=8 --trust-remote-code --mem-fraction-static 0.7 --context-length=131072 --attention-backend=fa3 --enable-multimodal --tool-call-parser=pythonic --chat-template=llama-4 --cuda-graph-max-bs=32 --host=0.0.0.0 --quantization=modelopt,8xH100,Reproduction of a bug when serving the quantized Llama 4 Scout model with vision layers using sglang v0.5.0rc2-cu126.,BROKEN: Failed at startup with ValueError: Cannot find any of ['quantization'] in the model's quantization config.,broken,https://github.com/sgl-project/sglang/issues/9758,[Bug] Serving Quantized Llama 4 Scout with Sglang
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,python3 -m sglang.launch_server --port=8000 --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --tp=8 --trust-remote-code --mem-fraction-static 0.7 --context-length=131072 --attention-backend=fa3 --enable-multimodal --tool-call-parser=pythonic --chat-template=llama-4 --cuda-graph-max-bs=32 --host=0.0.0.0 --quantization=modelopt,8xH100,Testing the model with a newer version of sglang (v0.5.3rc0-cu126) to see if the quantization format is supported.,"BROKEN: Model loads and text-only queries work, but the server crashes on image queries with AssertionError: `x` is not contiguous in the fp8_kernel.",broken,https://github.com/sgl-project/sglang/issues/9758,[Bug] Serving Quantized Llama 4 Scout with Sglang
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,python3 -m sglang.launch_server --port=8000 --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --tp=8 --trust-remote-code --mem-fraction-static 0.7 --context-length=131072 --attention-backend=fa3 --enable-multimodal --tool-call-parser=pythonic --chat-template=llama-4 --cuda-graph-max-bs=32 --host=0.0.0.0 --quantization=modelopt,Mentioned 8xH100 environment earlier in thread,Testing the latest version of sglang with multimodal enabled.,BROKEN: Failed with OSError: Can't load image processor for 'nvidia/Llama-4-Scout-17B-16E-Instruct-FP8' due to missing preprocessor_config.json or transformers version issues.,broken,https://github.com/sgl-project/sglang/issues/9758,[Bug] Serving Quantized Llama 4 Scout with Sglang
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,SGLANG_ENABLE_SPEC_V2=1 python3 -m sglang.launch_server --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --tp=8 --trust-remote-code --mem-fraction-static 0.7 --context-length=131072 --attention-backend=fa3 --enable-multimodal --tool-call-parser=pythonic --cuda-graph-max-bs=48 --host=0.0.0.0 --speculative-algorithm=EAGLE3 --speculative-num-steps=3 --speculative-eagle-topk=4 --speculative-num-draft-tokens=8 --speculative-draft-model-path=<llama 4 scout eagle head> --port=8000 --schedule-low-priority-values-first --enable-priority-scheduling --log-requests --log-requests-level=1 --enable-request-time-stats-logging --priority-scheduling-preemption-threshold=1000 --max-running-requests=192 --kv-cache-dtype bfloat16,8x NVIDIA H100 80GB HBM3,Testing the Llama 4 Scout FP8 model with the overlap scheduler (SGLANG_ENABLE_SPEC_V2=1) and EAGLE3 speculative decoding.,BROKEN: The server fails during the first prefill batch with a 'RuntimeError: Triton Error [CUDA]: device-side assert triggered' related to index out of bounds in IndexKernel.cu.,broken,https://github.com/sgl-project/sglang/issues/12707,[Bug] Overlap scheduler with Llama 4 Scout FP8 cuda error
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,python3 -m sglang.launch_server --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --tp=8 --trust-remote-code --mem-fraction-static 0.7 --context-length=131072 --attention-backend=fa3 --enable-multimodal --tool-call-parser=pythonic --cuda-graph-max-bs=48 --host=0.0.0.0 --speculative-algorithm=EAGLE3 --speculative-num-steps=3 --speculative-eagle-topk=4 --speculative-num-draft-tokens=8 --speculative-draft-model-path=<llama 4 scout eagle head> --port=8000 --schedule-low-priority-values-first --enable-priority-scheduling --log-requests --log-requests-level=1 --enable-request-time-stats-logging --priority-scheduling-preemption-threshold=1000 --max-running-requests=192 --kv-cache-dtype bfloat16,8x NVIDIA H100 80GB HBM3,Running the Llama 4 Scout FP8 model with speculative decoding but with the overlap scheduler disabled (SGLANG_ENABLE_SPEC_V2 unset).,WORKING: The author notes that the server starts successfully when the overlap scheduler environment variable is not set.,working,https://github.com/sgl-project/sglang/issues/12707,[Bug] Overlap scheduler with Llama 4 Scout FP8 cuda error
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,python3 -m sglang.launch_server --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --tp=8 --trust-remote-code --mem-fraction-static 0.7 --context-length=131072 --attention-backend=fa3 --enable-multimodal --tool-call-parser=pythonic --cuda-graph-max-bs=48 --host=0.0.0.0 --port=8000,8x NVIDIA H200 (Compute Capability: 9.0),Reproduce a startup bug where cuda graph capture fails for ModelOpt FP8 quantized models on sglang v0.5.4.post1.,BROKEN: Fails with 'Exception: Capture cuda graph failed: query and key must have the same dtype' during startup. This was identified as a regression from version v0.5.3.post3.,broken,https://github.com/sgl-project/sglang/issues/12298,[Bug] modelopt query and key must have the same dtype on v0.5.4.post1
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,python3 -m sglang.launch_server --model-path=nvidia/Llama-4-Scout-17B-16E-Instruct-FP8 --quantization modelopt --tp=8 --trust-remote-code --mem-fraction-static 0.7 --context-length=131072 --attention-backend=fa3 --enable-multimodal --tool-call-parser=pythonic --cuda-graph-max-bs=48 --kv-cache-dtype=fp8_e4m3,8x NVIDIA H200,Recommended configuration for ModelOpt checkpoints to resolve the dtype mismatch error and properly load the FP8 model.,"Working/Recommended. According to maintainers, specifying '--quantization modelopt' is required for these checkpoints to work in later versions.",working,https://github.com/sgl-project/sglang/issues/12298,[Bug] modelopt query and key must have the same dtype on v0.5.4.post1
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,python3 -m sglang.launch_server --model-path=/models/llama-4-scout-fp8 --tp=8 --trust-remote-code --mem-fraction-static 0.7 --context-length=131072 --attention-backend=fa3 --enable-multimodal --tool-call-parser=pythonic --cuda-graph-max-bs=48 --host=0.0.0.0 --port=8000 --kv-cache-dtype bfloat16,8x NVIDIA H200,Attempt to bypass the Exception: Capture cuda graph failed: query and key must have the same dtype error.,Working. The user confirmed they can bypass the startup error by setting '--kv-cache-dtype bfloat16'.,working,https://github.com/sgl-project/sglang/issues/12298,[Bug] modelopt query and key must have the same dtype on v0.5.4.post1
nvidia/Llama-4-Scout-17B-16E-Instruct-FP8,"llm = Engine(model_path='meta-llama/Llama-4-Scout-17B-16E-Instruct', tokenizer_path='meta-llama/Llama-4-Scout-17B-16E-Instruct', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, dtype='auto', kv_cache_dtype='auto', quantization='fp8', quantization_param_path=None, context_length=20000, device='cuda', served_model_name='meta-llama/Llama-4-Scout-17B-16E-Instruct', chat_template='llama_4_vision', completion_template=None, is_embedding=False, enable_multimodal=True, revision=None, hybrid_kvcache_ratio=None, impl='auto', host='0.0.0.0', port=8000, mem_fraction_static=0.91, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=32768, max_prefill_tokens=32768, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=8, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=941523765, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, bucket_time_to_first_token=None, bucket_e2e_request_latency=None, bucket_inter_token_latency=None, collect_tokens_histogram=False, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, ep_size=1, enable_ep_moe=False, enable_deepep_moe=False, enable_flashinfer_moe=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=True, cuda_graph_max_bs=2000, cuda_graph_bs=[2000], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_nccl_nvls=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, disable_overlap_schedule=False, disable_overlap_cg_plan=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, enable_return_hidden_states=False, warmups=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, num_reserved_decode_tokens=512, pdlb_url=None, custom_weight_loader=[], weight_loader_disable_mmap=False)",8x NVIDIA H200 (TP 8),Reproduction script to test precomputed vision features for Llama 4 Scout 17B with FP8 quantization using SGLang Engine API.,BROKEN: Fails with `ValueError: Could not make a flat list of images`. The precomputed vision features are incorrectly passed to the HuggingFace preprocessor because the Llama 4 processor in SGLang did not support skipping the vision encoder for precomputed inputs.,broken,https://github.com/sgl-project/sglang/issues/8065,[Bug] precomputed_feature does not work with Llama 4 vision
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8,python3 -m sglang.launch_server --model-path nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16 --trust-remote-code --tp 1 --attention-backend flashinfer --cuda-graph-max-bs 16 --tool-call-parser qwen3_coder --mem-fraction-static 0.6 --reasoning-parser nano_v3,NVIDIA Jetson Thor,Attempting to run the model on a Jetson Thor edge device for serving.,BROKEN: Consistently fails with 'Rank 0 scheduler is dead' and 'Exit code: -9' (likely OOM or process kill). An EOFError is raised during the subprocess launch phase.,broken,https://github.com/sgl-project/sglang/issues/15272,NVIDIA-Nemotron-3-Nano issue: Rank 0 scheduler is dead.
nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8,python3 -m sglang.launch_server --model-path nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16 --trust-remote-code --tp 1 --attention-backend flashinfer --cuda-graph-max-bs 16 --tool-call-parser qwen3_coder --mem-fraction-static 0.6 --reasoning-parser nano_v3,DGX Spark,Running the model on a high-performance DGX Spark system for comparison.,Working successfully as reported by the author.,working,https://github.com/sgl-project/sglang/issues/15272,NVIDIA-Nemotron-3-Nano issue: Rank 0 scheduler is dead.
nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8,python3 -m sglang.launch_server --model-path nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8--max-mamba-cache-size 256,,Benchmark GSM8K performance using SGLang to compare with VLLM.,Working. Achieved GSM8K score of 0.881  0.008.,working,https://github.com/sgl-project/sglang/pull/12018,Feature/nano v2 offline modelopt fp8 and nvfp4
nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8,vllm serve nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8 --trust-remote-code --max-num-seqs 512 --mamba_ssm_cache_dtype float32,,Benchmark GSM8K performance using VLLM for comparison purposes.,Working. Achieved GSM8K score of 0.892  0.008.,working,https://github.com/sgl-project/sglang/pull/12018,Feature/nano v2 offline modelopt fp8 and nvfp4
nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8,"python -m lm_eval --model local-chat-completions --model_args ""model=nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8,base_url=http://localhost:8000/v1/chat/completions,api_key=EMPTY,num_concurrent=256,timeout=3600"" --tasks gsm8k --apply_chat_template --system_instruction ""/no_think"" --gen_kwargs temperature=0.6,top_p=0.95,do_sample=true,seed=1 --output_path ./results/$(date +%s) --log_samples",,Run accuracy evaluation (lm-eval) on the GSM8K task via a local server endpoint.,Results logged to file; used to generate comparative accuracy table in the PR description.,working,https://github.com/sgl-project/sglang/pull/12018,Feature/nano v2 offline modelopt fp8 and nvfp4
nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8,python3 -m sglang.launch_server --model-path nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8--max-mamba-cache-size 256,,Launch SGLang server to support and verify the FP8 variant of the Nemotron-Nano-9B-v2 model.,GSM8K accuracy: 0.881  0.008. Used for benchmarking against vLLM.,working,https://github.com/sgl-project/sglang/pull/11866,Support nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8/NVFP4
nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8,vllm serve nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8 --trust-remote-code --max-num-seqs 512 --mamba_ssm_cache_dtype float32,,Serve the FP8 model using vLLM for baseline comparison against SGLang.,GSM8K accuracy: 0.892  0.008.,working,https://github.com/sgl-project/sglang/pull/11866,Support nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8/NVFP4
nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8,"python -m lm_eval --model local-chat-completions --model_args ""model=nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8,base_url=http://localhost:8000/v1/chat/completions,api_key=EMPTY,num_concurrent=256,timeout=3600"" --tasks gsm8k --apply_chat_template --system_instruction ""/no_think"" --gen_kwargs temperature=0.6,top_p=0.95,do_sample=true,seed=1 --output_path ./results/$(date +%s) --log_samples",,Evaluate the accuracy of the FP8 model on the GSM8K benchmark using the local chat completions API (SGLang/vLLM server).,"Produced accuracy metrics (0.881 for SGLang, 0.892 for vLLM) used to validate the implementation of FP8 support.",working,https://github.com/sgl-project/sglang/pull/11866,Support nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8/NVFP4
nvidia/NVIDIA-Nemotron-Nano-9B-v2-FP8,python -m sglang.launch_server --model-path nvidia/NVIDIA-Nemotron-Nano-9B-v2 --tokenizer-path nvidia/NVIDIA-Nemotron-Nano-9B-v2 --tokenizer-mode auto --model-loader-extra-config '{}' --trust-remote_code True --host 0.0.0.0 --port 20002 --dtype auto --kv-cache-dtype fp8_e5m2 --mem-fraction-static 0.812 --chunked-prefill-size 2048 --max-prefill-tokens 16384 --schedule-policy fcfs --schedule-conservativeness 1.0 --page-size 1 --tp-size 2 --pp-size 1 --stream-interval 1 --stream-output False --random-seed 875238230 --watchdog-timeout 300 --log-level info --file-storage-path sglang_storage --load-balance-method round_robin --nnodes 1 --node_rank 0 --json-model-override-args '{}' --lora-backend triton --sampling-backend flashinfer --grammar-backend xgrammar --speculative-accept-threshold-single 1.0 --speculative-accept-threshold-acc 1.0 --ep_size 1 --moe_a2a_backend none --moe_runner_backend auto --flashinfer_mxfp4_moe_precision default --deepep_mode auto --ep_num_redundant_experts 0 --ep_dispatch_algorithm static --init_expert_location trivial --eplb_algorithm auto --eplb_rebalance_num_iterations 1000 --hicache_ratio 2.0 --hicache_size 0 --hicache_write_policy write_through_selective --hicache_io_backend kernel --hicache_mem_layout layer_first --offload_group_size -1 --offload_num_in_group 1 --offload_prefetch_step 1 --offload_mode cpu --cuda_graph_max_bs 8 --tbo_token_distribution_threshold 0.48 --torch_compile_max_bs 32 --torchao_config '' --triton_attention_num_kv_splits 8 --num_continuous_decode_steps 1 --scheduler_recv_interval 1 --disaggregation_mode null --disaggregation_transfer_backend mooncake --disaggregation_bootstrap_port 8998 --disaggregation_prefill_pp 1 --num_reserved_decode_tokens 512 --sm_group_num 3,2 GPUs (tp_size 2),Attempting to run the Mamba-based Nemotron-Nano model using sglang with FP8 KV cache support.,"BROKEN: Failed with `ImportError: mamba-ssm is required by the Mamba model but cannot be imported`. The logs indicate that the `mamba_ssm` dependency was missing in the `lmsysorg/sglang:b200-cu129` container, and the maintainers noted that Mamba-based models were not yet supported by sglang at that time.",broken,https://github.com/sgl-project/sglang/issues/10067,Can't run mamba based nemotron models - nvidia/NVIDIA-Nemotron-Nano-9B-v2
nvidia/Qwen2.5-VL-7B-Instruct-FP8,python3 -m sglang.launch_server --model-path Qwen/Qwen2.5-VL-7B-Instruct --host 0.0.0.0 --port 30000,NVIDIA A40 (46GB),Launch the SGLang server for Qwen2.5-VL-7B-Instruct using the OpenAI API Vision backend to serve requests.,BROKEN: Failed during CUDA graph capture with a RuntimeError. The error 'You're trying to build PyTorch with a too old version of GCC. We need GCC 9 or later' occurred during the JIT compilation of FlashInfer kernels.,broken,https://github.com/sgl-project/sglang/issues/6801,[Bug] RuntimeError: Error building extension 'batch_prefill_with_kv_cache...' when deploying Qwen2.5-VL-7B-Instruct
nvidia/Qwen2.5-VL-7B-Instruct-FP8,python3 -m sglang.launch_server --model-path /mnt/data/models/Qwen2.5-VL-7B-Instruct-NVFP4 --host 0.0.0.0 --port 30000 --disable-radix-cache --chunked-prefill-size 1024 --max-running-requests 20 --cuda-graph-max-bs 10 --max-total-tokens 32000 --mem-fraction-static 0.75 --moe-runner-backend flashinfer_cutlass --quantization modelopt_fp4 --attention-backend triton,"Not explicitly stated, but log shows approx 90.64 GB available memory",Attempting to launch a Qwen2.5-VL model with NVFP4 (ModelOpt) quantization using SGLang to test support for FP4 vision-language models.,"BROKEN: Encountered an AssertionError during weight loading: `AssertionError: param_data.shape=torch.Size([1280, 640]), loaded_weight.shape=torch.Size([1280, 1280])`. The user suspects the vision module is in FP4 format but is being read as FP8, causing a shape mismatch.",broken,https://github.com/sgl-project/sglang/issues/13821,[Feature] Does it support running VL models like Qwen3-VL with nvfp4 or other quantizations?
nvidia/Qwen2.5-VL-7B-Instruct-FP8,"CUDA_VISIBLE_DEVICES=0,1 python3 -m sglang.launch_server --model Qwen2.5-VL-7B-Instruct --port 3000 --chat-template qwen2-vl --host $HOST --tensor-parallel-size 2 --dtype half --mem-fraction-static 0.8",2x RTX 4090 (24GB each),Attempting to run Qwen2.5-VL-7B-Instruct using tensor parallelism across two GPUs to avoid OOM when processing multiple frames (14 frames).,BROKEN: Hits OOM limit at the same frame count as a single GPU. User observes that it looks like the model is being duplicated on both GPUs rather than sharded.,broken,https://github.com/sgl-project/sglang/issues/5374,Qwen2.5-VL still runs out of memory on 2 GPUs (seems to duplicate the model instead of splitting it)
nvidia/Qwen2.5-VL-7B-Instruct-FP8,"CUDA_VISIBLE_DEVICES=0,1 python3 -m sglang.launch_server --model Qwen2.5-VL-7B-Instruct --port 3000 --chat-template qwen2-vl --host $HOST --tp 2",2x RTX 4090 (24GB each),Alternative command configuration using abbreviated `--tp` flag to test tensor parallelism for Qwen2.5-VL-7B-Instruct.,"BROKEN: Same behavior as previous command; consistently runs out of memory with 14 frames. However, a commenter (yhyang201) noted that logs show weights are actually being split (~7.93 GB per GPU).",broken,https://github.com/sgl-project/sglang/issues/5374,Qwen2.5-VL still runs out of memory on 2 GPUs (seems to duplicate the model instead of splitting it)
nvidia/Qwen2.5-VL-7B-Instruct-FP8,USE_TRITON_W8A8_FP8_KERNEL=1 python -m sglang.launch_server --model-path /data/models/quant/Qwen2.5-VL-7B-Instruct-FP8-Dynamic --context-length 4096 --mem-fraction-static 0.75 --max-running-requests 64 --port 8000,RTX 5070Ti,Benchmark Triton FP8 kernel performance for Qwen2.5-VL-7B-FP8-Dynamic.,Successful. Output token throughput: 377.67 tok/s. Total token throughput: 463.20 tok/s. Mean TPOT: 88.14 ms.,working,https://github.com/sgl-project/sglang/pull/9403,[sgl-kernel] feat: Support sm120 cutlass fp8 gemm kernel
nvidia/Qwen2.5-VL-7B-Instruct-FP8,python -m sglang.launch_server --model-path /data/models/quant/Qwen2.5-VL-7B-Instruct-FP8-Dynamic --context-length 4096 --mem-fraction-static 0.75 --max-running-requests 64 --port 8000,RTX 5070Ti,Benchmark the new Cutlass FP8 kernel performance for Qwen2.5-VL-7B-FP8-Dynamic to compare against Triton.,Successful and faster than Triton. Output token throughput: 527.49 tok/s. Total token throughput: 639.70 tok/s. Mean TPOT: 52.84 ms.,working,https://github.com/sgl-project/sglang/pull/9403,[sgl-kernel] feat: Support sm120 cutlass fp8 gemm kernel
nvidia/Qwen2.5-VL-7B-Instruct-FP8,python -m sglang.launch_server --model-path /mnt/Qwen2.5-VL-7B-Instruct-FP8-Dynamic --context-length 4096 --mem-fraction-static 0.75 --max-running-requests 64 --port 8001,2x RTX 6000 Ada (referenced as 6000 PRO),Attempt to reproduce benchmarks for Cutlass FP8 kernel on different hardware (RTX 6000 Ada).,Successful but slower than Triton in this environment. Total token throughput: 1442.56 tok/s. Mean ITL: 14.65 ms.,working,https://github.com/sgl-project/sglang/pull/9403,[sgl-kernel] feat: Support sm120 cutlass fp8 gemm kernel
nvidia/Qwen2.5-VL-7B-Instruct-FP8,USE_TRITON_W8A8_FP8_KERNEL=1 python -m sglang.launch_server --model-path /mnt/Qwen2.5-VL-7B-Instruct-FP8-Dynamic --context-length 4096 --mem-fraction-static 0.75 --max-running-requests 64 --port 8001,2x RTX 6000 Ada (referenced as 6000 PRO),Baseline Triton benchmark on RTX 6000 Ada to compare with Cutlass performance.,Successful and faster than Cutlass in this configuration. Total token throughput: 1852.32 tok/s. Mean ITL: 11.37 ms.,working,https://github.com/sgl-project/sglang/pull/9403,[sgl-kernel] feat: Support sm120 cutlass fp8 gemm kernel
nvidia/Qwen2.5-VL-7B-Instruct-FP8,"VIDEO_MAX_PIXELS=89600000 SGLANG_IMAGE_PROCESSOR_FAST_DEVICE=cpu CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 HIP_DISABLE_FLASHDECODE=1 HIP_HEAD_REDUCE=1 HIP_DEBUG_LAST_DENSE=128 HIP_DEBUG_FORCE_DENSE_DECODE=1 CUDA_LAUNCH_BLOCKING=0 HIP_DEBUG=0 python -m sglang.launch_server --model-path Qwen/Qwen2.5-VL-72B-Instruct-AWQ --port 30000 --tp 8 --max-total-tokens 256000 --context-length 256000 --cuda-graph-bs 1 --max-running-req 1 --chunked-prefill-size -1 --hip-attention-config '{""using_extend"": true, ""dense_layers"": [0, 1, 2, 3, 11, 19, 27, 35, 43, 51, 59, 67, 75, 79], ""layers"": [{""sliding_window_size"": ""122880"", ""second_stage_k"": 4096, ""sa_extend_backend"": ""streaming"", ""scan_extend_backend"": ""streaming"", ""stages"": [{""stage_block_size_q"": 64, ""stage_block_stride_q"": 4, ""stage_chunk_size"": 128, ""stage_k"": null, ""stage_stride"": 1}, {""stage_block_size_q"": 64, ""stage_block_stride_q"": 4, ""stage_chunk_size"": 32, ""stage_k"": 32768, ""stage_stride"": 1}, {""stage_block_size_q"": 64, ""stage_block_stride_q"": 1, ""stage_chunk_size"": 8, ""stage_k"": 8192, ""stage_stride"": 1}]}, {""sliding_window_size"": 4096, ""second_stage_k"": 2048, ""sa_extend_backend"": ""streaming"", ""scan_extend_backend"": ""streaming"", ""stages"": [{""stage_block_size_q"": 64, ""stage_block_stride_q"": 4, ""stage_chunk_size"": 128, ""stage_k"": null, ""stage_stride"": 1}, {""stage_block_size_q"": 64, ""stage_block_stride_q"": 4, ""stage_chunk_size"": 32, ""stage_k"": 32768, ""stage_stride"": 1}, {""stage_block_size_q"": 64, ""stage_block_stride_q"": 1, ""stage_chunk_size"": 8, ""stage_k"": 8192, ""stage_stride"": 1}]}]}' --disable-radix-cache --attention-backend flashinfer --disable-cuda-graph --enable-multimodal --chat-template qwen2-vl --kv-cache-dtype fp8_e5m2 --enable-hip-attention","8 GPUs (CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7)",Testing Qwen2.5-VL-72B-Instruct-AWQ with InfiniteHiP (Sparse Attention) for long-context vision-language tasks.,Working/Experimental. Achieved 54.28 accuracy on LongVideoBench (first 748 samples) with a 128k context length. Notes that hyperparameters were copied from language models and not yet fully optimized for vision models.,experimental,https://github.com/sgl-project/sglang/pull/3930,[Feature] Support Efficient Sparse HiP Attention (InfiniteHiP) with Long-Context Generalization and KV Offloading Capabilties
nvidia/Qwen2.5-VL-7B-Instruct-FP8,python -m sglang.launch_server --model-path Qwen/Qwen3-VL-8B-Instruct-FP8 --tp-size 1 --mem-fraction-static 0.7 --context-length 8192 --max-running-requests 128 --host 0.0.0.0 --port 7860 --enable-metrics,1x GPU (44.39 GiB capacity mentioned in logs),Original bug report reproduction: Serving the model via OpenAI-compatible API to test memory stability with image requests.,"BROKEN: GPU memory steadily increases with image requests, eventually failing with 'torch.OutOfMemoryError: CUDA out of memory' during the logits processor step.",broken,https://github.com/sgl-project/sglang/issues/13940,[Bug] Qwen3-VL: GPU memory steadily increases with image requests (ends in CUDA OOM)
nvidia/Qwen2.5-VL-7B-Instruct-FP8,SGLANG_USE_CUDA_IPC_TRANSPORT=1 SGLANG_VLM_CACHE_SIZE_MB=0 python -m sglang.launch_server --model-path /home/admin/Qwen3-VL-8B-Instruct --host 0.0.0.0 --port 30000 --trust-remote-code --tp-size 8 --enable-cache-report --log-level info --max-running-requests 48 --mem-fraction-static 0.8 --chunked-prefill-size 8192 --attention-backend fa3 --mm-attention-backend fa3,8x NVIDIA H20,Maintainer attempt to reproduce the OOM issue on high-end hardware using Tensor Parallelism (TP8) and specific transport settings.,"Working/Stable: Maintainer reported CUDA usage was stable during small batch tests, but later found that rank0 overflow occurs when concurrency is increased to 100.",broken,https://github.com/sgl-project/sglang/issues/13940,[Bug] Qwen3-VL: GPU memory steadily increases with image requests (ends in CUDA OOM)
nvidia/Qwen2.5-VL-7B-Instruct-FP8,python3 -m sglang.bench_serving --backend sglang-oai-chat --base-url http://0.0.0.0:30000 --model /home/admin/Qwen3-VL-8B-Instruct/ --tokenizer /home/admin/Qwen3-VL-8B-Instruct/ --dataset-name image --num-prompts 10 --image-count 7 --image-resolution 1120x700 --image-format jpeg --random-input-len 100 --random-output-len 100 --random-range-ratio 1 --apply-chat-template --max-concurrency 1,8x NVIDIA H20,Benchmark serving performance for image understanding tasks with 7 images per request.,Working: Achieved Input token throughput of 46.87 tok/s and Total throughput of 47.72 tok/s. Memory reported as stable at this low concurrency.,working,https://github.com/sgl-project/sglang/issues/13940,[Bug] Qwen3-VL: GPU memory steadily increases with image requests (ends in CUDA OOM)
nvidia/Qwen2.5-VL-7B-Instruct-FP8,SGLANG_MM_FEATURE_CACHE_MB=8192 SGLANG_USE_CUDA_IPC_TRANSPORT=1 SGLANG_VLM_CACHE_SIZE_MB=512 python -m sglang.launch_server --model-path /home/admin/Qwen3-VL-8B-Instruct/ --host 0.0.0.0 --port 8188 --trust-remote-code --tp-size 2 --enable-cache-report --log-level info --max-running-requests 48 --mem-fraction-static 0.7 --chunked-prefill-size 8192 --attention-backend flashinfer --mm-attention-backend fa3 --log-level debug --log-requests --log-requests-level 1,TP-size 2 (likely H20 or similar based on previous logs),Testing a workaround for the CUDA IPC memory leak by enlarging the multimodal feature cache to 8GB.,Working: Resolved the OOM issue previously seen at high concurrency (100) by allocating more memory for the feature cache.,working,https://github.com/sgl-project/sglang/issues/13940,[Bug] Qwen3-VL: GPU memory steadily increases with image requests (ends in CUDA OOM)
nvidia/Qwen2.5-VL-7B-Instruct-FP8,python -m sglang.launch_server --model-path Qwen/Qwen3-VL-8B-Instruct-FP8 --tp-size 1 --mem-fraction-static 0.7 --context-length 8192 --max-running-requests 128 --host 0.0.0.0 --port 7860 --enable-metrics,1x GPU,Retesting the original issue on the latest main branch after PR #14123 was merged.,"BROKEN: Despite the PR intended to fix the issue, the reporter found it still reproduces at TP=1 with no CUDA_IPC enabled. GPU memory usage graph shows linear growth to OOM.",broken,https://github.com/sgl-project/sglang/issues/13940,[Bug] Qwen3-VL: GPU memory steadily increases with image requests (ends in CUDA OOM)
nvidia/Qwen3-14B-FP8,python -m sglang.bench_one_batch --model-path Qwen/Qwen3-14B --batch 1 --input-len 16 32 64 128 256 512 --output-len 1 --enable-piecewise-cuda-graph,H100,Prefill-only benchmark to evaluate performance of the newly introduced piecewise CUDA graph feature with different input lengths on a single batch.,Benchmarked successfully. The PR description notes that piecewise CUDA graph improves prefill performance for short length inputs by reducing kernel launch overhead.,working,https://github.com/sgl-project/sglang/pull/10062,Piecewise CUDA Graph Support & Torch Compile Backend
nvidia/Qwen3-14B-FP8,docker run -d --gpus all --name sglang-qwen14b-fp8-v2 -v /var/models:/var/models -p 8000:8000 lmsysorg/sglang:blackwell python -m sglang.launch_server --model-path /var/models/Qwen3-14B-FP8 --host 0.0.0.0 --port 8000 --trust-remote-code --context-length 32000,RTX 5090 (Blackwell architecture),Reproduction of an AttributeError and crash when running Qwen3-14B-FP8 on Blackwell architecture using the lmsysorg/sglang:blackwell Docker image.,BROKEN: The server crashed during CUDA graph capture with a RuntimeError: 'Assertion error: Unknown recipe' in deepgemm. This was followed by an AttributeError: module 'cutlass' has no attribute 'CACHE_FILE'. It appears deepgemm did not yet support the sm_120 (RTX 5090) architecture at the time.,broken,https://github.com/sgl-project/sglang/issues/10533,[Bug] Blackwell + FP8 Model: AttributeError: module 'cutlass' has no attribute 'CACHE_FILE'
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.launch_server --model Qwen/Qwen3-235B-A22B --speculative-algorithm EAGLE3 --speculative-draft-model-path nvidia/Qwen3-235B-A22B-Eagle3 --speculative-num-steps 5 --speculative-eagle-topk 8 --speculative-num-draft-tokens 32 --mem-fraction-static 0.75 --tp 8 --ep 8 --context-length 8192 --trust-remote-code --host 0.0.0.0 --port 30000 --dtype bfloat16,"Not explicitly stated, but implies 8 or 16 GPUs based on --tp 8 --ep 8",Accuracy testing to verify compatibility with NVIDIA's released Eagle3 heads (nvidia/Qwen3-235B-A22B-Eagle3) after fixing a bug where ModelOpt checkpoints defaulted to FP8.,"Working. Successfully ran GSM8K benchmark with the server. Accuracy: 0.951, Latency: 204.845 s for 1319 samples.",working,https://github.com/sgl-project/sglang/pull/11991,[NVIDIA] Change default quant method for model_opt
nvidia/Qwen3-235B-A22B-FP8,python benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py --model Qwen/Qwen3-235B-A22B-Instruct-2507 --tp-size 8 --dtype fp8_w8a8 --tune,NVIDIA H200,Tune the fused MoE Triton kernel for the Qwen3-235B FP8 model to optimize inference performance.,Approximately 8% improvement in inter-token latency. E2E benchmarking showed Mean TPOT improved from 7.50 ms to 7.10 ms.,working,https://github.com/sgl-project/sglang/pull/11730,add tuned fuse moe kernel for qwen3 235b fp8 on h200
nvidia/Qwen3-235B-A22B-FP8,python benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py --model Qwen/Qwen3-235B-A22B-Instruct-2507 --tp-size 8 --dtype fp8_w8a8,NVIDIA H200,Benchmark the fused MoE Triton kernel performance for the Qwen3-235B FP8 model.,"Provided baseline and tuned kernel performance metrics across various batch sizes. For example, at batch size 1, kernel time reduced from 31.70 us to 27.84 us.",working,https://github.com/sgl-project/sglang/pull/11730,add tuned fuse moe kernel for qwen3 235b fp8 on h200
nvidia/Qwen3-235B-A22B-FP8,model=/models/nvme0/Qwen3-VL-235B-A22B-Instruct-FP8/ python -m sglang.launch_server --model-path ${model} --host 0.0.0.0 --port 9000 --tp 8 --ep 8,NVIDIA or AMD GPUs (8 GPUs inferred from tp=8),Launching the Qwen3-VL-235B multimodal model server to debug a bench_serving error where randomly generated tokens occasionally produce conflicting image tokens.,"Server runs, but client-side benchmarking leads to an error where the code assumes multiple images exist when only one image is provided, causing a crash during test execution.",broken,https://github.com/sgl-project/sglang/pull/13254,[BugFix] fix bench_serving error when multimodal image is testing
nvidia/Qwen3-235B-A22B-FP8,"python3 -m sglang.bench_serving --backend sglang --port 9000 --model ""/models/nvme0/Qwen3-VL-235B-A22B-Instruct-FP8/"" --dataset-name image --image-count 1 --max-concurrency 64 --image-resolution 800x800 --random-input-len 1000 --random-output-len 1000 --num-prompts 640",NVIDIA or AMD GPUs,Benchmarking multimodal model serving performance with image input and random text generation.,"Probabilistic failure: IndexError: index 1 is out of bounds for dimension 0 with size 1. This occurs because random token generation occasionally produces extra image tokens, confusing the processor which expected only one image.",broken,https://github.com/sgl-project/sglang/pull/13254,[BugFix] fix bench_serving error when multimodal image is testing
nvidia/Qwen3-235B-A22B-FP8,"CUDA_VISIBLE_DEVICES=4,5,6,7 python3 -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-FP8 --tp 4 --host 0.0.0.0 --enable-piecewise-cuda-graph --piecewise-cuda-graph-compiler eager","Not explicitly stated in the first post (later comments mention 4090 24G*8 and H20, but not for this specific 235B run)",Testing piecewise CUDA graph support for the Qwen3-MoE model to improve performance and fix identified bugs like Dynamo FX node failures with fake tensors and LRU cache tracing issues.,Successful server launch followed by a benchmark achieving 5340.152 token/s output throughput and 0.923 accuracy on GSM8K with 2000 questions.,working,https://github.com/sgl-project/sglang/pull/11845,piecewise cuda graph support qwen3-moe
nvidia/Qwen3-235B-A22B-FP8,"CUDA_VISIBLE_DEVICES=4,5,6,7 python3 -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-FP8 --tp 4 --host 0.0.0.0 --enable-piecewise-cuda-graph --piecewise-cuda-graph-compiler eager",Not explicitly stated in this comment (Yansiyu550 context),User yansiyu550 re-testing the suggested command from the PR description to verify piecewise CUDA graph features on the 235B FP8 model.,"Reported as part of a thread where smaller models (4B/30B) either showed no gain or failed, though a specific result for the 235B run in this comment was not detailed beyond re-quoting the original success.",working,https://github.com/sgl-project/sglang/pull/11845,piecewise cuda graph support qwen3-moe
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.launch_server --model-path /data1/models/Qwen3-235B-A22B-FP8/ --disable-radix-cache --host 0.0.0.0 --port 8000 --tp 4 --trust-remote-code --enable-metrics --served-model-name Qwen3-235B-A22B-FP8 --context-length 4096,NVIDIA H20 (TP4),Deploy model to test new fused MoE kernel configurations for Triton 3.3 on H20 GPUs.,Working. Throughput of 1.22 req/s and 958.69 total tok/s achieved with MoE config compared to 1.21 req/s and 951.30 total tok/s without it. ITL decreased from 23.30ms to 22.99ms.,working,https://github.com/sgl-project/sglang/pull/7631,Add H20 fused MoE kernel configs for Dpsk & Qwen3
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.bench_serving --tokenizer /data1/models/Qwen3-235B-A22B-FP8/ --host 0.0.0.0 --port 8000 --backend sglang --dataset-name random --random-input 1024 --random-output 512 --max-concurrency 8 --num-prompt 200,NVIDIA H20 (TP4),Benchmark the performance of Qwen3-235B-A22B-FP8 using the sglang serving backend to evaluate the impact of new MoE kernel optimizations.,"Benchmarking successful. With MoE config: Mean E2E Latency 6394.67 ms, Median TTFT 144.90 ms, Mean ITL 22.99 ms. Total token throughput 958.69 tok/s.",working,https://github.com/sgl-project/sglang/pull/7631,Add H20 fused MoE kernel configs for Dpsk & Qwen3
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-FP8 --tp 4 --host 0.0.0.0 --enable-piecewise-cuda-graph,"Not explicitly stated, but log mentions NVIDIA B200 and 175.55 GB available memory",Testing the newly introduced Piecewise CUDA Graph support on the Qwen3-235B-A22B-FP8 model with Tensor Parallelism of 4.,BROKEN: The server fails with a fatal C++ error: 'Only a single TORCH_LIBRARY can be used to register the namespace _C'. This occurred during the loading of the 'weak_ref_tensor_ext' extension after CUDA graph capture completed.,broken,https://github.com/sgl-project/sglang/pull/10062,Piecewise CUDA Graph Support & Torch Compile Backend
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.launch_server --model-path /work/models --served-model-name qwen3 --tp 2 --port 39992 --host 0.0.0.0 --log-level info --enable-metrics --enable-p2p-check --attention-backend fa3 --enable-piecewise-cuda-graph,"2x GPUs (CUDA_VISIBLE_DEVICES=0,1)",Testing piecewise CUDA graph with Tensor Parallelism (TP=2) and FlashAttention 3 (fa3) backend on a Qwen3-235B variant (referenced as qwen3/Qwen3-30B-A3B-FP8 in context).,"BROKEN: The scheduler hit an AssertionError during the capture phase in `rotary_embedding.py`. Specifically, it failed during `torch._dynamo` graph tracing with an 'AssertionError' related to `fused_set_kv_buffer_arg`.",broken,https://github.com/sgl-project/sglang/pull/10062,Piecewise CUDA Graph Support & Torch Compile Backend
nvidia/Qwen3-235B-A22B-FP8,"SGLANG_VLM_MAX_IMAGES_PER_VIT=32 SGLANG_VLM_MAX_PATCHES_PER_VIT=10000 python -m sglang.launch_server --model-path Qwen/Qwen3-VL-235B-A22B-Instruct-FP8 --host 0.0.0.0 --port 30000 --tp 8 --ep 8 --mem-fraction-static 0.55 --max-running-requests 8 --mm-enable-dp-encoder --model-loader-extra-config '{""enable_multithread_load"": true, ""num_threads"": 2}'",8 GPUs (suggested by TP8/EP8 configuration),Launch a multimodal server to test handling of high-volume visual data (500 images) with Qwen3-VL-235B-A22B-Instruct-FP8.,"BROKEN: Before the PR, the scheduler hit a `torch.OutOfMemoryError: CUDA out of memory` when trying to allocate 4.95 GiB while processing 500 images, because all images were processed in one forward pass.",broken,https://github.com/sgl-project/sglang/pull/14907,[VLM] Support chunked vit attention
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.bench_serving --backend sglang-oai-chat --dataset-name image --num-prompts 1 --apply-chat-template --random-input-len 128 --random-output-len 32 --image-resolution 560x560 --image-format jpeg --image-count 500 --image-content random --random-range-ratio 0.1 --port 30000 --max-concurrency 1,8 GPUs (TP8/EP8),Benchmark serving performance for a request containing 500 images to verify the fix for OOM.,"WORKING: Successfully processed 500 images with 163,060 input tokens (163,000 vision) and 14 output tokens. Total throughput was 5444.20 tok/s with a TTFT of 29.7s.",working,https://github.com/sgl-project/sglang/pull/14907,[VLM] Support chunked vit attention
nvidia/Qwen3-235B-A22B-FP8,python3 -m lmms_eval --model openai_compatible --model_args model_version=Qwen/Qwen3-VL-235B-A22B-Instruct-FP8 --tasks mmmu_val --batch_size 16,Not specified,Accuracy testing using the MMMU validation task to ensure no performance degradation after implementing chunked ViT attention.,"WORKING: Achieved an overall accuracy of 0.6378 on mmmu_val tasks, demonstrating that there was no accuracy drop compared to the main branch.",working,https://github.com/sgl-project/sglang/pull/14907,[VLM] Support chunked vit attention
nvidia/Qwen3-235B-A22B-FP8,SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 SGLANG_ENABLE_SPEC_V2=1 python -m sglang.launch_server --model Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 --trust-remote-code --tp 4 --speculative-algorithm EAGLE3 --speculative-draft-model-path lmsys/Qwen3-235B-A22B-EAGLE3 --speculative-num-steps 7 --speculative-eagle-topk 1 --speculative-num-draft-tokens 8,NVIDIA B200 GPU (4x TP),Testing model on Blackwell architecture without the manual FlashInfer workaround to confirm the NaN failure. Testing speculative decoding v2 with EAGLE3.,BROKEN: Crash with 'probability tensor contains inf/nan' due to DeepGEMM's UE8M0 scale format causing numerical instability on Blackwell.,broken,https://github.com/sgl-project/sglang/pull/13788,Auto-enable FlashInfer FP8 GEMM kernel for Blackwell GPUs
nvidia/Qwen3-235B-A22B-FP8,SGLANG_ENABLE_FLASHINFER_FP8_GEMM=1 SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 SGLANG_ENABLE_SPEC_V2=1 python -m sglang.launch_server --model Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 --trust-remote-code --tp 4 --speculative-algorithm EAGLE3 --speculative-draft-model-path lmsys/Qwen3-235B-A22B-EAGLE3 --speculative-num-steps 7 --speculative-eagle-topk 1 --speculative-num-draft-tokens 8,NVIDIA B200 GPU (4x TP),Testing the manual workaround by enabling FlashInfer's FP8 GEMM kernel to resolve NaN errors on Blackwell GPUs.,Working as confirmed in #13757. Model loads and runs without device-side assertions or inf/nan in sampling probabilities.,working,https://github.com/sgl-project/sglang/pull/13788,Auto-enable FlashInfer FP8 GEMM kernel for Blackwell GPUs
nvidia/Qwen3-235B-A22B-FP8,SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 SGLANG_ENABLE_SPEC_V2=1 python -m sglang.launch_server --model Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 --trust-remote-code --tp 4 --speculative-algorithm EAGLE3 --speculative-draft-model-path lmsys/Qwen3-235B-A22B-EAGLE3 --speculative-num-steps 7 --speculative-eagle-topk 1 --speculative-num-draft-tokens 8,NVIDIA B200 GPU (4x TP),Validation of the PR's automatic detection logic which auto-enables FlashInfer FP8 GEMM on Blackwell GPUs without manual env vars.,Working. Server auto-detects Blackwell + FP8 and enables FlashInfer. Logs confirm: 'Auto-enabled FlashInfer FP8 GEMM kernel for Blackwell GPU...',working,https://github.com/sgl-project/sglang/pull/13788,Auto-enable FlashInfer FP8 GEMM kernel for Blackwell GPUs
nvidia/Qwen3-235B-A22B-FP8,python3 benchmark/gsm8k/bench_sglang.py --num-shots 8 --num-questions 1319 --parallel 1319,NVIDIA B200 GPU (4x TP),GSM8K benchmark to validate accuracy and performance after fixing the NaN issue on Blackwell.,"Working. Accuracy: 0.957, Latency: 168.655 s, Output throughput: 1138.072 token/s. Completed successfully without NaN errors.",working,https://github.com/sgl-project/sglang/pull/13788,Auto-enable FlashInfer FP8 GEMM kernel for Blackwell GPUs
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.launch_server --model /cpfs/user/siwen/HF_Model/huggingface_download/Qwen3/Qwen3-235B-A22B-FP8 --trust-remote-code --tp 8 --ep 8 --mem-fraction-static 0.9 --attention-backend fa3 --disable-radix-cache --chunked-prefill-size -1 --port 8192 --kv-cache-dtype fp8_e4m3,8x NVIDIA L20Z (Compute Capability 9.0),Testing FlashAttention3 (FA3) with FP8 KV cache to benchmark performance against BF16.,"Performance regression observed. FP8 performance is worse than BF16 FA3. Specifically, while trtllm shows a 5% improvement in TTFT and 3-4% in TPOT, sglang kernels drop 23.5% when paged attention is enabled compared to a 12.5% drop for BF16.",broken,https://github.com/sgl-project/sglang/issues/14816,"[Bug] Performance regression with fa3 fp8 attention, worse than bf16 fa3."
nvidia/Qwen3-235B-A22B-FP8,"from sglang import Engine
checkpoint_path = ""/mnt/vlm/common/models/Qwen/Qwen3-VL-235B-A22B-Thinking-FP8""
llm = Engine(
    model_path=checkpoint_path,
    enable_multimodal=True,
    mem_fraction_static=0.8,
    tp_size=8,
    ep_size=2,
    attention_backend=""fa3""
)",8x NVIDIA A800-SXM4-80GB,"Initialize the SGLang Engine for Qwen3-VL-235B-A22B-Thinking-FP8 with multimodal support, Tensor Parallelism (TP) of 8, and Expert Parallelism (EP) of 2 using the FlashAttention-3 backend.",BROKEN: Fails during CUDA graph capture with a TypeError: gptq_marlin_gemm() got an unexpected keyword argument 'b_bias'. This appears to be a bug in the FP8 Marlin quantization layer within SGLang.,broken,https://github.com/sgl-project/sglang/issues/15474,[Bug] Qwen3-VL-235B-A22B-Thinking-FP8 TypeError: gptq_marlin_gemm() got an unexpected keyword argument 'b_bias'
nvidia/Qwen3-235B-A22B-FP8,python -m sglang.launch_server --model /mnt/GLM-4.5-Air-FP8/ --tp 2 --host 0.0.0.0 --port 8001 --mem-fraction-static 0.95 --context-length 128000,2x RTX 6000 Professional,Initial performance testing of the CUTLASS FP8 GEMM kernel against the Triton kernel for a large FP8 model.,"Yielded 79 tokens/sec for a single query, which was slower than the Triton kernel's 102 tokens/sec on the same hardware.",working,https://github.com/sgl-project/sglang/pull/9403,[sgl-kernel] feat: Support sm120 cutlass fp8 gemm kernel
nvidia/Qwen3-235B-A22B-FP8,SGL_ENABLE_JIT_DEEPGEMM=0 python -m sglang.launch_server --model /mnt/GLM-4.5-Air-FP8/ --tp 2 --host 0.0.0.0 --port 8001 --mem-fraction-static 0.94 --context-length 128000 --enable-metrics --attention-backend flashinfer,2x RTX 6000 Professional,Benchmark comparison using the CUTLASS kernel (default in this PR branch) with JIT DeepGEMM disabled.,Total token throughput: 630.45 tok/s. Mean ITL: 33.43 ms. Performance was slightly lower than the Triton alternative.,working,https://github.com/sgl-project/sglang/pull/9403,[sgl-kernel] feat: Support sm120 cutlass fp8 gemm kernel
nvidia/Qwen3-235B-A22B-FP8,USE_TRITON_W8A8_FP8_KERNEL=1 SGL_ENABLE_JIT_DEEPGEMM=0 python -m sglang.launch_server --model /mnt/GLM-4.5-Air-FP8/ --tp 2 --host 0.0.0.0 --port 8001 --mem-fraction-static 0.94 --context-length 128000 --enable-metrics --attention-backend flashinfer,2x RTX 6000 Professional,Benchmark comparison forcing the Triton FP8 kernel to compare against the new CUTLASS kernel.,Total token throughput: 658.99 tok/s. Mean ITL: 31.97 ms. Triton outperformed the CUTLASS kernel in this specific multi-GPU setup.,working,https://github.com/sgl-project/sglang/pull/9403,[sgl-kernel] feat: Support sm120 cutlass fp8 gemm kernel
nvidia/Qwen3-235B-A22B-FP8,SGLANG_ENABLE_SPEC_V2=1 python -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 --speculative-draft-model-path lmsys/Qwen3-235B-A22B-EAGLE3,"Not explicitly stated, though a similar test in the thread used H100*8",Testing Speculative Decoding V2 (Overlap Scheduler) with EAGLE3 draft model for the Qwen3-235B-A22B model family to improve inference efficiency.,"Experimental. The issue discussion notes that while the configuration is valid for Spec V2, some users reported lower accuracy/acceptance rates when comparing 'eagle3+overlap' vs 'eagle3' (e.g., acceptance length dropping from 3.0 to 2.85).",experimental,https://github.com/sgl-project/sglang/issues/11762,[Feature] Overlap Spec Support
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.launch_server --model-path /shared/public/elr-models/Qwen/Qwen3-235B-A22B-Thinking-2507-FP8/f07f63f2bbd7540917118ebdf3812696ef303b03/ --attention-backend fa3 --enable-deterministic-inference --tp 4,4xH200,Testing deterministic inference for the FP8 version of Qwen3-235B-A22B-Thinking using the FA3 attention backend with tensor parallelism of 4.,"Broken/Non-deterministic. Despite --enable-deterministic-inference, the model produced unique samples across trials (e.g., 8 unique samples for Prompt 3), indicating non-deterministic behavior.",broken,https://github.com/sgl-project/sglang/issues/12232,[Bug] Qwen3-235B-A22B-Thinking is not deterministic with --enable-deterministic-inference
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.launch_server --model-path /shared/public/elr-models/Qwen/Qwen3-235B-A22B-Thinking-2507/ddee1c5fb2c5406e8315f8a6d47214c74349e15b/ --attention-backend fa3 --enable-deterministic-inference --tp 8,8xH200,Testing deterministic inference for the base (BF16/FP16) version of Qwen3-235B-A22B-Thinking using FA3 and tensor parallelism of 8 to compare with the FP8 results.,"Broken/Non-deterministic. While more stable than the TP4/FP8 setup, it still produced 2 unique samples for longer prefix lengths (2048 and 4097), failing the determinism test.",broken,https://github.com/sgl-project/sglang/issues/12232,[Bug] Qwen3-235B-A22B-Thinking is not deterministic with --enable-deterministic-inference
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.launch_server --model-path /path/to/models/Qwen3-235B-A22B-Instruct-2507-FP8 --disable-radix-cache --mem-fraction-static 0.7 --tp-size 8 --ep-size 8 --enable-dp-attention --dp-size 8 --chunked-prefill-size -1 --cuda-graph-max-bs 128 --trust-remote-code --attention-backend flashinfer --enable-dp-lm-head --moe-a2a-backend deepep,8x NVIDIA H20,Server setup for performance regression testing of Qwen3-235B inference after sgl-kernel version upgrade (0.3.5 to 0.3.8). Testing DeepGemm performance in high-concurrency serving scenarios.,"Identified performance regression in ITL (Inter-Token Latency). While E2E and TTFT improved slightly, ITL metrics regressed (e.g., Median ITL increased from 36.99ms to 39.65ms). Regression traced to 'fp8_gemm_kernel' in the decode phase.",working,https://github.com/sgl-project/sglang/issues/11997,[Bug] Performance regression in Qwen3-235B caused by DeepGemm version upgrade
nvidia/Qwen3-235B-A22B-FP8,python -m sglang.bench_serving --backend sglang --dataset-name random --dataset-path /mnt/ShareGPT_V3_unfiltered_cleaned_split.json --random-input-len 3500 --random-output-len 100 --max-concurrency 200 --num-prompts 200 --request-rate inf --random-range-ratio 1,8x NVIDIA H20,"Benchmarking serving performance (E2E latency, TTFT, ITL) for Qwen3-235B under high load (200 concurrency) to quantify regressions between kernel versions.","Measured Mean ITL of 116.45ms and P99 ITL of 61.45ms on sgl-kernel 0.3.8, compared to 114.61ms and 54.95ms respectively on version 0.3.5.",working,https://github.com/sgl-project/sglang/issues/11997,[Bug] Performance regression in Qwen3-235B caused by DeepGemm version upgrade
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.launch_server --port=8000 --model-path=Qwen/Qwen3-235B-A22B-Instruct-2507-FP8--tp=8 --attention-backend=fa3 --enable-multimodal --ep-size=8 --host=0.0.0.0,8xH100 80GB HBM3,Start server for Qwen3-235B FP8 to investigate throughput regression relative to FP16 version.,Server started successfully. Used as the backend for benchmarking which later revealed lower than expected throughput (105.98 total tok/s).,working,https://github.com/sgl-project/sglang/issues/9120,[Bug] Throughput Regression for Qwen3-235B-Instruct FP8
nvidia/Qwen3-235B-A22B-FP8,SGLANG_TORCH_PROFILER_DIR=/root/sglang/profile_log python -m sglang.bench_serving --backend sglang --model Qwen/Qwen3-235B-A22B-Instruct-2507-FP8 --num-prompts 10 --sharegpt-output-len 100 --profile --port=8000,8xH100 80GB HBM3,Benchmark throughput and latency for Qwen3-235B FP8 using the bench_serving script with profiling enabled.,Total token throughput: 105.98 tok/s. Mean E2E Latency: 3043.76 ms. Mean TTFT: 996.19 ms. Results showed a regression compared to the FP16 model (162.87 tok/s) on the same hardware.,working,https://github.com/sgl-project/sglang/issues/9120,[Bug] Throughput Regression for Qwen3-235B-Instruct FP8
nvidia/Qwen3-235B-A22B-FP8,"SGLANG_ENABLE_JIT_DEEPGEMM=1 SGLANG_DEEPEP_NUM_MAX_DISPATCH_TOKENS_PER_RANK=512 MC_TE_METRIC=true python3 -m sglang.launch_server --model-path /models/models--Qwen--Qwen3-235B-A22B-Instruct-2507-FP8/snapshots/ba82a1060073fa0ecdc70d7b1922ec071f60cf3e --dist-init-addr 10.102.207.116:5757 --enable-eplb --expert-distribution-recorder-mode per_token --enable-expert-distribution-metrics --nnodes 2 --node-rank 0 --disable-cuda-graph --disaggregation-ib-device ""mlx5_0,mlx5_1,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_7"" --disaggregation-mode decode --prefill-round-robin-balance --tp-size 16 --dp-size 16 --ep-size 16 --enable-dp-attention --decode-log-interval 1 --moe-a2a-backend deepep --host 0.0.0.0 --port 40001 --trust-remote-code --moe-dense-tp-size 1 --enable-dp-lm-head --disable-radix-cache --watchdog-timeout 1000000 --deepep-mode low_latency",H200*8 per machine (2 nodes total),Demonstrate and evaluate Expert Parallel Load Balancing (EPLB) impact on inter-GPU communication load balancing for MoE inference performance. Specifically testing 'per_token' mode for the expert distribution recorder.,"BROKEN: Throws 'AssertionError: assert output_mode == ""file""' in 'sglang/srt/eplb/expert_distribution.py' when using '--expert-distribution-recorder-mode per_token' or 'per_pass'. The user noted it only works normally when set to 'stat' mode.",broken,https://github.com/sgl-project/sglang/issues/13206,[Query & Bug] How to Demonstrate EPLB's Impact on Inter-GPU Communication Load Balancing for Improved MOE Inference Performance?
nvidia/Qwen3-235B-A22B-FP8,python -m sglang.launch_server --model-path /models/Qwen/Qwen3-VL-235B-A22B-Instruct --dp-size 1 --tp-size 4 --port 30000 --trust-remote-code --moe-a2a-backend deepep,4x NVIDIA H200,Testing if Qwen3-VL is compatible with the DeepEP MoE backend in sglang.,BROKEN: Failed with AttributeError: 'DeepEPMoE' object has no attribute 'w13_weight_scale'. The user suspected the model didn't properly inherit from Qwen3Moe or hadn't been adapted for DeepEP.,broken,https://github.com/sgl-project/sglang/issues/11088,[Usage] Qwen3-VL fails to init with DeepEP
nvidia/Qwen3-235B-A22B-FP8,python -m sglang.launch_server --model-path /models/Qwen/Qwen3-VL-235B-A22B-Instruct --dp-size 1 --tp-size 4 --port 30000 --trust-remote-code --moe-a2a-backend deepep,4x NVIDIA H200,Retesting the same configuration after applying a custom patch (vz/fix-qwen3vl-bf16-noquant).,"BROKEN: Model loaded weights successfully this time, but crashed during CUDA graph capture with TypeError: MRotaryEmbedding.forward() got an unexpected keyword argument 'fused_set_kv_buffer_arg'.",broken,https://github.com/sgl-project/sglang/issues/11088,[Usage] Qwen3-VL fails to init with DeepEP
nvidia/Qwen3-235B-A22B-FP8,/usr/bin/python3 -m sglang.launch_server --model-path Qwen/Qwen3-VL-235B-A22B-Instruct-FP8 --host 0.0.0.0 --port 30235 --tp 4 --ep 4 --attention-backend fa3 --mem-fraction-static 0.8 --cuda-graph-max-bs 16,8x NVIDIA H100 80GB HBM3,Reproduce a bug where the Vision Transformer (ViT) component fails to process text in the center of images compared to the official Qwen implementation.,Discrepancy in results: The model was unable to process text information in the center region of images. The user reported it produced different results from vLLM and the official implementation.,broken,https://github.com/sgl-project/sglang/issues/11422,[Bug] Qwen3-VL model produces different results from official website Qwen implementation
nvidia/Qwen3-235B-A22B-FP8,"curl -X POST ""http://localhost:30235/v1/chat/completions"" -H ""Content-Type: application/json"" -d '{
                 ""model"": ""Qwen/Qwen3-VL-235B-A22B-Instruct-FP8"",
                 ""messages"": [
                  {
                   ""role"": ""user"",
                   ""content"": [
                    {
                     ""type"": ""image_url"",
                     ""image_url"": {
                      ""url"": ""https://i0.wp.com/eatingintaipei.com/wp-content/uploads/2024/05/Shilin_night_market_steak_price.jpg""
                     }
                    },
                    {
                     ""type"": ""text"",
                     ""text"": ""Read all the text in the image.""
                    }
                   ]
                  }
                 ],
                 ""max_tokens"": 2048
                }'",8x NVIDIA H100 80GB HBM3,Test the OCR/Vision capabilities of the served model specifically for reading text within an image to verify the reported bug.,The user reported this failed to correctly perceive text in the center of the image prior to a fix in a subsequent sglang commit.,broken,https://github.com/sgl-project/sglang/issues/11422,[Bug] Qwen3-VL model produces different results from official website Qwen implementation
nvidia/Qwen3-235B-A22B-FP8,SGLANG_USE_CUDA_IPC_TRANSPORT=1 python -m sglang.launch_server --model Qwen/Qwen3-VL-235B-A22B-Instruct --tp 8 --port 8000,8x NVIDIA B200,Launch serving for Qwen3-VL-235B multi-modal model to investigate a prefill performance regression on Blackwell GPUs.,Resulted in very slow prefill speeds (TTFT mean ~39.6s) and low input throughput (~38.67 tok/s) when used with the OAI chat backend. Issue was later identified as a regression caused by torch 2.9 update and incorrect cuDNN version.,broken,https://github.com/sgl-project/sglang/issues/14078,[Bug] Qwen3-VL prefill perf regression (very slow)
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.bench_serving --backend sglang-oai-chat --host 127.0.0.1 --port 8000 --model Qwen/Qwen3-VL-235B-A22B-Instruct --dataset-name image --image-count 2 --image-resolution 720p --random-input-len 128 --random-output-len 1024 --num-prompts 5 --max-concurrency 1,8x NVIDIA B200,Benchmark multi-modal prefill and generation performance using the OpenAI API chat backend.,"Confirmed slow performance: Mean E2E Latency 47732.45 ms, Input token throughput 38.67 tok/s. Regression was linked to missing/incorrect cuDNN version for Torch 2.9.",broken,https://github.com/sgl-project/sglang/issues/14078,[Bug] Qwen3-VL prefill perf regression (very slow)
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.bench_serving --backend sglang --host 127.0.0.1 --port 8000 --model Qwen/Qwen3-VL-235B-A22B-Instruct --dataset-name image --image-count 2 --image-resolution 720p --random-input-len 128 --random-output-len 1024 --num-prompts 5 --max-concurrency 1,8x NVIDIA B200,Comparative benchmark using the native 'sglang' backend instead of 'sglang-oai-chat' to narrow down the performance issue.,"Showed significantly better performance (TTFT mean ~117ms, Input throughput ~225.99 tok/s) because this backend configuration bypassed multi-modal (image) processing.",working,https://github.com/sgl-project/sglang/issues/14078,[Bug] Qwen3-VL prefill perf regression (very slow)
nvidia/Qwen3-235B-A22B-FP8,SGLANG_USE_CUDA_IPC_TRANSPORT=1 python -m sglang.launch_server --model Qwen/Qwen3-VL-235B-A22B-Instruct --tp 8 --port 8000,8x NVIDIA B200,Verify fix after manually installing nvidia-cudnn-cu12==9.16.0.29 to resolve torch 2.9 regression.,Regression solved. Performance returned to expected levels (TTFT mean ~195ms at commit 64a1 compared to ~37s during regression).,working,https://github.com/sgl-project/sglang/issues/14078,[Bug] Qwen3-VL prefill perf regression (very slow)
nvidia/Qwen3-235B-A22B-FP8,python -m sglang.launch_server --model-path ./Qwen3-235B-A22B-Thinking-2507-FP8 --port 30000 --trust-remote --host 0.0.0.0 --enable-metrics --tp 8 --mem-fraction-static 0.92 --torch-compile-max-bs 8 --max-running-requests 50 --context-length 131072 --reasoning-parser qwen3,8*4090 (NVIDIA GeForce RTX 4090),Reproducing a bug where the Qwen3-235B FP8 model fails to run on an 8x4090 setup.,Failed to run. Error indicated in the issue description via screenshots (likely hardware/TP incompatibility or memory constraints). A commenter noted that this specific FP8 version does not support tp=8.,broken,https://github.com/sgl-project/sglang/issues/10324,[Bug] Qwen3-235B-A22B-Thinking-2507-FP8 with 8*4090 failed to run
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.launch_server --model-path ${model_path} --tp 4 --base-gpu-id 0 --enable-hierarchical-cache --hicache-size 120 --hicache-storage-backend file --cuda-graph-max-bs 512 --trust-remote-code --mem-fraction-static 0.7 --enable-metrics --enable-cache-report --port 7001 --enable-metrics --enable-cache-report --watchdog-timeout 1000000 --decode-log-interval 1,8x NVIDIA H20,Launch sglang server for nvidia/Qwen3-235B-A22B-FP8 with Hierarchical Cache (HiCache) enabled using 'file' as the storage backend and tensor parallelism of 4.,"Broken. When used in conjunction with the benchmark script, it leads to a server-side error related to the HiCache storage backend.",broken,https://github.com/sgl-project/sglang/issues/9779,[Bug] HiCache storage backend leads to error when running bench_serving
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.bench_serving --model /mnt/models/Qwen3-235B-A22B-FP8 --dataset-name generated-shared-prefix --gsp-num-groups 64 --gsp-prompts-per-group 16 --gsp-system-prompt-len 8192 --gsp-question-len 512 --gsp-output-len 512 --base-url http://localhost:7001,8x NVIDIA H20,Benchmark serving performance for nvidia/Qwen3-235B-A22B-FP8 using a generated shared-prefix dataset to test cache efficiency.,Triggered an error in the server backend when HiCache was set to 'file' mode. The user sought help to determine if the configuration was incorrect.,broken,https://github.com/sgl-project/sglang/issues/9779,[Bug] HiCache storage backend leads to error when running bench_serving
nvidia/Qwen3-235B-A22B-FP8,uv run python -m sglang.launch_server --model-path $MODELS/Qwen/Qwen3-235B-A22B-Thinking-2507-FP8 --tp-size 4 --ep-size 4 --dp-size 4 --pp-size 2 --enable-dp-attention --port 8000 --trust-remote-code,8x NVIDIA H100 80GB HBM3,Warmup test for a PP+EP+DP inference configuration on a single node before moving to multiple nodes.,BROKEN: The server launches but crashes immediately on the warmup request with AttributeError: 'PPProxyTensors' object has no attribute 'next_token_logits'.,broken,https://github.com/sgl-project/sglang/issues/9805,[Bug] PP+EP+DP+enable-enable-dp-attention crashes
nvidia/Qwen3-235B-A22B-FP8,python -m sglang.launch_server --model-path /mnt/nvme/Qwen/Qwen3-235B-A22B-Thinking-2507-FP8 --tp-size 4 --ep-size 4 --dp-size 4 --pp-size 2 --enable-dp-attention --port 8000 --trust-remote-code,8x NVIDIA H100 80GB HBM3,Verify a fix for the PP+DP compatibility issue during inference.,"EXPERIMENTAL: The command runs and initially serves requests, resolving the previous crash. However, the user reported that the server 'could hang some time' after running requests for a while.",experimental,https://github.com/sgl-project/sglang/issues/9805,[Bug] PP+EP+DP+enable-enable-dp-attention crashes
nvidia/Qwen3-235B-A22B-FP8,"DG_JIT_CACHE_DIR=/root/ SGLANG_DEEPEP_NUM_MAX_DISPATCH_TOKENS_PER_RANK=1024 python3 -m sglang.launch_server --model-path /models/models--Qwen--Qwen3-235B-A22B-Instruct-2507-FP8/snapshots/ba82a1060073fa0ecdc70d7b1922ec071f60cf3e --trust-remote-code --disaggregation-mode prefill --dist-init-addr <HOST_A>:8050 --disaggregation-ib-device mlx5_0,mlx5_1,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_7 --nnodes 1 --node-rank 0 --tp 8 --dp 8 --enable-dp-attention --enable-deepep-moe --ep-size 8 --disable-radix-cache --attention-backend fa3 --mem-frac 0.6 --deepep-mode low_latency --host 0.0.0.0 --moe-dense-tp-size 1 --enable-dp-lm-head --watchdog-timeout 1000000 --decode-log-interval 1000 --max-running-requests 1024 --disaggregation-bootstrap-port 8998",8H200 GPUs,Launch a prefill node in a separated Prefill/Decode disaggregation deployment to benchmark performance with high RPS (64) using a random dataset.,Broken: The system hangs at high RPS (64). Prefill node eventually throws a timeout error: 'Prefill bootstrap failed... Request timed out after 300.0s in KVPoll.Bootstrapping'. Logs stop updating.,broken,https://github.com/sgl-project/sglang/issues/10111,[Bug] Hang Issue in Prefill/Decode Separated Mode under High RPS (64) with Random Dataset
nvidia/Qwen3-235B-A22B-FP8,"DG_JIT_CACHE_DIR=/root/ SGLANG_DEEPEP_NUM_MAX_DISPATCH_TOKENS_PER_RANK=512 python3 -m sglang.launch_server --model-path /models/models--Qwen--Qwen3-235B-A22B-Instruct-2507-FP8/snapshots/ba82a1060073fa0ecdc70d7b1922ec071f60cf3e --trust-remote-code --disaggregation-mode decode --dist-init-addr <HOST_B>:8050 --disaggregation-ib-device mlx5_0,mlx5_1,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_7 --nnodes 1 --node-rank 0 --tp 8 --dp 8 --enable-dp-attention --enable-deepep-moe --ep-size 8 --disable-radix-cache --attention-backend fa3 --mem-frac 0.7 --deepep-mode low_latency --host 0.0.0.0 --moe-dense-tp-size 1 --enable-dp-lm-head --cuda-graph-max-bs 128 --watchdog-timeout 1000000 --decode-log-interval 1000 --max-running-requests 1024",8H200 GPUs,Launch a decode node in a separated Prefill/Decode disaggregation deployment to benchmark performance with high RPS (64).,"Broken: Part of a system-wide hang under high RPS (64). While the prefill node shows errors, the decode node's failure to provide KV indices leads to the overall timeout and crash of the request stream.",broken,https://github.com/sgl-project/sglang/issues/10111,[Bug] Hang Issue in Prefill/Decode Separated Mode under High RPS (64) with Random Dataset
nvidia/Qwen3-235B-A22B-FP8,"python -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-FP8 --tp 8 --context-length 131072 --json-model-override-args '{""max_position_embeddings"": 131072}' --enable-ep-moe",8x NVIDIA H100 80GB HBM3,"Reproducing a hang issue when the model receives specific requests, specifically testing the DeepGEMM implementation with EP-MoE enabled.",Broken. The engine hangs during the prefill batch stage when using DeepGEMM. Log shows it gets stuck at '#new-token: 1823' and remains at 100% GPU utilization without progressing. The issue was traced to a specific DeepGEMM commit and interaction with CUDA 12.6.,broken,https://github.com/sgl-project/sglang/issues/7869,[Bug] Qwen3 235B FP8 hangs when received some specific requests.
nvidia/Qwen3-235B-A22B-FP8,"python3 -m sglang.launch_server --model-path /nfsshare/model-checkpoint/Qwen3-235B-A22B-AWQ/ --trust-remote-code --served-model-name qwen3-235b --api-key xxx --tensor-parallel-size 4 --mem-fraction-static 0.85 --quantization awq_marlin --enable-metrics --host 0.0.0.0 --port 30000 --reasoning-parser qwen3 --tool-call-parser qwen25 --enable-ep-moe --json-model-override-args '{""rope_scaling"":{""rope_type"":""yarn"",""factor"":4.0,""original_max_position_embeddings"":32768}}'","4x GPUs (device_ids: 0, 1, 2, 3) via Docker",Deploying Qwen3-235B-A22B-AWQ using sglang with Expert Parallel (EP) MoE enabled and AWQ Marlin quantization.,BROKEN: Failed with AttributeError: 'AWQMarlinConfig' object has no attribute 'weight_block_size'. The error occurs in the EP MoE implementation when attempting to access quantization configuration attributes.,broken,https://github.com/sgl-project/sglang/issues/6234,[Bug] AttributeError: 'AWQMarlinConfig' object has no attribute 'weight_block_size' when deploying Qwen3-235B-A22B-AWQ
nvidia/Qwen3-235B-A22B-FP8,"python3 -m sglang.launch_server --model-path /nfsshare/model-checkpoint/Qwen3-235B-A22B-AWQ/ --trust-remote-code --served-model-name qwen3-235b --api-key xxx --tensor-parallel-size 4 --mem-fraction-static 0.85 --quantization awq_marlin --enable-metrics --host 0.0.0.0 --port 30000 --reasoning-parser qwen3 --tool-call-parser qwen25 --json-model-override-args '{""rope_scaling"":{""rope_type"":""yarn"",""factor"":4.0,""original_max_position_embeddings"":32768}}'",4x GPUs,Deploying the model without the problematic Expert Parallel (EP) MoE flag to resolve the AttributeError.,"Suggested as a fix by the author (""don't use --enable-ep-moe""). Implicitly allows the model to run by avoiding the code path causing the crash.",working,https://github.com/sgl-project/sglang/issues/6234,[Bug] AttributeError: 'AWQMarlinConfig' object has no attribute 'weight_block_size' when deploying Qwen3-235B-A22B-AWQ
nvidia/Qwen3-235B-A22B-FP8,"DG_JIT_CACHE_DIR=/root/ SGLANG_DEEPEP_NUM_MAX_DISPATCH_TOKENS_PER_RANK=1024 python3 -m sglang.launch_server --model-path /models/models--Qwen--Qwen3-235B-A22B-Instruct-2507-FP8/snapshots/ba82a1060073fa0ecdc70d7b1922ec071f60cf3e --trust-remote-code --disaggregation-mode prefill --dist-init-addr <HOST_A>:8050 --disaggregation-ib-device mlx5_0,mlx5_1,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_7 --nnodes 1 --node-rank 0 --tp 8 --dp 8 --enable-dp-attention --enable-deepep-moe --ep-size 8 --disable-radix-cache --attention-backend fa3 --mem-frac 0.6 --deepep-mode low_latency --host 0.0.0.0 --moe-dense-tp-size 1 --enable-dp-lm-head --watchdog-timeout 1000000 --decode-log-interval 1000 --max-running-requests 1024 --disaggregation-bootstrap-port 8998",8xH200,Launch prefill node in a disaggregated (prefill/decode separated) execution mode for Qwen 235B FP8 performance benchmarking.,"Broken. At high RPS (64), the system hangs and logs stop updating. Eventually throws 'KVTransferError' and 'Request timed out after 300.0s in KVPoll.Bootstrapping'. This indicates the prefill instance failed to receive KV indices from the decode instance.",broken,https://github.com/sgl-project/sglang/issues/10110,[Bug]
nvidia/Qwen3-235B-A22B-FP8,"DG_JIT_CACHE_DIR=/root/ SGLANG_DEEPEP_NUM_MAX_DISPATCH_TOKENS_PER_RANK=512 python3 -m sglang.launch_server --model-path /models/models--Qwen--Qwen3-235B-A22B-Instruct-2507-FP8/snapshots/ba82a1060073fa0ecdc70d7b1922ec071f60cf3e --trust-remote-code --disaggregation-mode decode --dist-init-addr <HOST_B>:8050 --disaggregation-ib-device mlx5_0,mlx5_1,mlx5_2,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_7 --nnodes 1 --node-rank 0 --tp 8 --dp 8 --enable-dp-attention --enable-deepep-moe --ep-size 8 --disable-radix-cache --attention-backend fa3 --mem-frac 0.7 --deepep-mode low_latency --host 0.0.0.0 --moe-dense-tp-size 1 --enable-dp-lm-head --cuda-graph-max-bs 128 --watchdog-timeout 1000000 --decode-log-interval 1000 --max-running-requests 1024",8xH200,Launch decode node in a disaggregated execution mode for Qwen 235B FP8 performance benchmarking.,"Broken. System hangs under high load (64 RPS), though it works normally under lower RPS (4 or 8). The decode node likely fails to communicate KV indices back to the prefill node fast enough, leading to timeouts.",broken,https://github.com/sgl-project/sglang/issues/10110,[Bug]
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.launch_server --model Qwen/Qwen3-235B-A22B-FP8 --tp 4 --reasoning-parser qwen3,Not explicitly specified in description (assumed H100/H200 based on FP8 requirement),Template command for deploying the FP8 version of Qwen 3 with reasoning capabilities.,"Considered a valid configuration; however, some users reported OOM depending on available VRAM.",working,https://github.com/sgl-project/sglang/issues/5858,[Tracker] Qwen 3 deployment
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.launch_server --model Qwen/Qwen3-235B-A22B-FP8 --tp 8  --reasoning-parser qwen3 --enable-ep-moe,8 GPUs (reported as problematic or requiring specific tuning by users),Template command for deploying the FP8 version with 8-way tensor parallelism and Expert Parallelism for MoE.,Broken/Invalid for some users: TP=8 is reported as invalid because the MoE inter-size/weight scale shape (12) and num_key_value_heads (4) are not divisible by 8. Users encountered errors during launch.,broken,https://github.com/sgl-project/sglang/issues/5858,[Tracker] Qwen 3 deployment
nvidia/Qwen3-235B-A22B-FP8,LD_LIBRARY_PATH=/opt/conda/lib/python3.11/site-packages/nvidia/cuda_nvrtc/lib:${LD_LIBRARY_PATH} python -m sglang.launch_server --model-path llm/Qwen/Qwen3-235B-A22B-FP8 --dp 2 --tp 2 --ep-size 2 --enable-ep-moe --context-length 16000 --host 0.0.0.0 --port 30001 --random-seed 1313,4x NVIDIA H200,"Attempting to run FP8 model with DP=2, TP=2, and EP=2.",Failed with `Exception: Capture cuda graph failed: Cannot find any available NVCC compiler`. User noted it was fixed by setting `SGL_ENABLE_JIT_DEEPGEMM=0`.,broken,https://github.com/sgl-project/sglang/issues/5858,[Tracker] Qwen 3 deployment
nvidia/Qwen3-235B-A22B-FP8,HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 HF_EVALUATE_OFFLINE=1 HF_HUB_OFFLINE=1 NCCL_IB_GID_INDEX=3 NCCL_DEBUG=INFO SGL_ENABLE_JIT_DEEPGEMM=0 python3 -m sglang.launch_server --model-path {path_to_your_model_hub_dir}/Qwen3-235B-A22B-FP8 --tp 4 --dp 4 --port 38106 --trust-remote-code --dist-init-addr ${MASTER_IP}:${MASTER_PORT} --node-rank ${RANK} --host 0.0.0.0 --nnodes 2 --enable-metrics --mem-fraction-static 0.8 --max-running-requests 2048 --served-model-name qwen3_235b_a22b_fp8 --enable-dp-attention --chunked-prefill-size 4096,"2 nodes, 8 GPUs total (TP 4, DP 4)",Multi-node deployment with DP-attention and JIT DeepGEMM disabled.,Working. Setting `SGL_ENABLE_JIT_DEEPGEMM=0` and using `tp 4` resolved issues seen at `tp 8`.,working,https://github.com/sgl-project/sglang/issues/5858,[Tracker] Qwen 3 deployment
nvidia/Qwen3-235B-A22B-FP8,"python3 -m sglang.launch_server --model /models/Qwen3-235B-A22B-FP8 --tp 8  --mem-fraction 0.7 --reasoning-parser qwen3 --enable-ep-moe --json-model-override-args '{""rope_scaling"":{""rope_type"":""yarn"",""factor"":4.0,""original_max_position_embeddings"":32768}}'  --host 0.0.0.0 --port 9997 --served-model-name DeepSeek-R1 --context-len 131072 --chunked-prefill-size 32768",8x 48GB GPUs,Deploying with very high context length (128k) using RoPE scaling overrides and expert parallelism.,"Working, but user noted potential quality issues (missing words in summaries) when processing large volumes of Chinese text (~100k characters).",working,https://github.com/sgl-project/sglang/issues/5858,[Tracker] Qwen 3 deployment
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-FP8 --load-format dummy --trust-remote-code --disaggregation-mode prefill --dist-init-addr 10.130.8.138:40050 --nnodes 2 --node-rank 0 --tp 16 --dp 16 --enable-dp-attention --enable-deepep-moe --ep-size 16 --disable-radix-cache --mem-frac 0.8 --deepep-mode normal --host 0.0.0.0 --moe-dense-tp-size 1 --enable-dp-lm-head --watchdog-timeout 1000000 --decode-log-interval 1 --context-length 1000000,NVIDIA L20Y,Deploying QWEN3 using PD separation (Prefill instance) with DP + EP + DeepEP method on node 0.,"Stuck during decoding when input/output is 2048/2048, though it runs normally for 1024/1024.",broken,https://github.com/sgl-project/sglang/issues/7381,"[Bug] When deploying QWEN3 using PD separation and the DP + EP + DeepEP method, the Decode instance get stuck."
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-FP8 --load-format dummy --trust-remote-code --disaggregation-mode prefill --dist-init-addr 10.130.8.138:40050 --nnodes 2 --node-rank 1 --tp 16 --dp 16 --enable-dp-attention --enable-deepep-moe --ep-size 16 --disable-radix-cache --mem-frac 0.8 --deepep-mode normal --host 0.0.0.0 --moe-dense-tp-size 1 --enable-dp-lm-head --watchdog-timeout 1000000 --decode-log-interval 1 --context-length 1000000,NVIDIA L20Y,Deploying QWEN3 using PD separation (Prefill instance) with DP + EP + DeepEP method on node 1.,Stuck during decoding when input/output is 2048/2048.,broken,https://github.com/sgl-project/sglang/issues/7381,"[Bug] When deploying QWEN3 using PD separation and the DP + EP + DeepEP method, the Decode instance get stuck."
nvidia/Qwen3-235B-A22B-FP8,SGLANG_TORCH_PROFILER_DIR=~/logs python3 -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-FP8 --load-format dummy --trust-remote-code --disaggregation-mode decode --dist-init-addr 10.130.8.143:8050 --nnodes 4 --node-rank 0 --tp 32 --dp 32 --enable-dp-attention --enable-deepep-moe --ep-size 32 --disable-radix-cache --mem-frac 0.8 --deepep-mode low_latency --host 0.0.0.0 --moe-dense-tp-size 1 --enable-dp-lm-head --cuda-graph-max-bs 128 --watchdog-timeout 1000000 --decode-log-interval 1 --context-length 1000000,NVIDIA L20Y,Deploying QWEN3 using PD separation (Decode instance) with DP + EP + DeepEP method on node 0 with low_latency mode.,The Decode instance gets stuck during high load (2048 input/output tokens).,broken,https://github.com/sgl-project/sglang/issues/7381,"[Bug] When deploying QWEN3 using PD separation and the DP + EP + DeepEP method, the Decode instance get stuck."
nvidia/Qwen3-235B-A22B-FP8,SGLANG_TORCH_PROFILER_DIR=~/logs python3 -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-FP8 --load-format dummy --trust-remote-code --disaggregation-mode decode --dist-init-addr 10.130.8.143:8050 --nnodes 4 --node-rank 1 --tp 32 --dp 32 --enable-dp-attention --enable-deepep-moe --ep-size 32 --disable-radix-cache --mem-frac 0.8 --deepep-mode low_latency --host 0.0.0.0 --moe-dense-tp-size 1 --enable-dp-lm-head --cuda-graph-max-bs 128 --watchdog-timeout 1000000 --decode-log-interval 1 --context-length 1000000,NVIDIA L20Y,Deploying QWEN3 using PD separation (Decode instance) on node 1.,Stuck behavior observed.,broken,https://github.com/sgl-project/sglang/issues/7381,"[Bug] When deploying QWEN3 using PD separation and the DP + EP + DeepEP method, the Decode instance get stuck."
nvidia/Qwen3-235B-A22B-FP8,SGLANG_TORCH_PROFILER_DIR=~/logs python3 -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-FP8 --load-format dummy --trust-remote-code --disaggregation-mode decode --dist-init-addr 10.130.8.143:8050 --nnodes 4 --node-rank 2 --tp 32 --dp 32 --enable-dp-attention --enable-deepep-moe --ep-size 32 --disable-radix-cache --mem-frac 0.8 --deepep-mode low_latency --host 0.0.0.0 --moe-dense-tp-size 1 --enable-dp-lm-head --cuda-graph-max-bs 128 --watchdog-timeout 1000000 --decode-log-interval 1 --context-length 1000000,NVIDIA L20Y,Deploying QWEN3 using PD separation (Decode instance) on node 2.,Stuck behavior observed.,broken,https://github.com/sgl-project/sglang/issues/7381,"[Bug] When deploying QWEN3 using PD separation and the DP + EP + DeepEP method, the Decode instance get stuck."
nvidia/Qwen3-235B-A22B-FP8,SGLANG_TORCH_PROFILER_DIR=~/logs python3 -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-FP8 --load-format dummy --trust-remote-code --disaggregation-mode decode --dist-init-addr 10.130.8.143:8050 --nnodes 4 --node-rank 3 --tp 32 --dp 32 --enable-dp-attention --enable-deepep-moe --ep-size 132 --disable-radix-cache --mem-frac 0.8 --deepep-mode low_latency --host 0.0.0.0 --moe-dense-tp-size 1 --enable-dp-lm-head --cuda-graph-max-bs 128 --watchdog-timeout 1000000 --decode-log-interval 1 --context-length 1000000,NVIDIA L20Y,Deploying QWEN3 using PD separation (Decode instance) on node 3.,Stuck behavior observed.,broken,https://github.com/sgl-project/sglang/issues/7381,"[Bug] When deploying QWEN3 using PD separation and the DP + EP + DeepEP method, the Decode instance get stuck."
nvidia/Qwen3-235B-A22B-FP8,python3 -m sglang.bench_serving --backend sglang --dataset-name random --num-prompt 3200 --request-rate 16 --random-input 2048 --random-output 2048 --model Qwen/Qwen3-235B-A22B-FP8 --port 8000,NVIDIA L20Y,Benchmarking serving throughput for Qwen3-235B-A22B-FP8 with large input/output pairs.,Benchmark hangs or gets stuck halfway through.,broken,https://github.com/sgl-project/sglang/issues/7381,"[Bug] When deploying QWEN3 using PD separation and the DP + EP + DeepEP method, the Decode instance get stuck."
nvidia/Qwen3-235B-A22B-FP8,SGLANG_TORCH_PROFILER_DIR=~/logs NCCL_IB_GID_INDEX=3 GLOO_SOCKET_IFNAME=eth0 SGL_ENABLE_JIT_DEEPGEMM=1 python3 -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-FP8 --load-format dummy --trust-remote-code --disaggregation-mode prefill --disaggregation-ib-device mlx5_bond_0 --dist-init-addr 10.130.8.138:8050 --nnodes 2 --node-rank 0 --tp 16 --dp 16 --ep-size 16 --enable-dp-attention --enable-deepep-moe --disable-radix-cache --mem-frac 0.8 --deepep-mode normal --host 0.0.0.0 --moe-dense-tp-size 1 --enable-dp-lm-head --watchdog-timeout 1000000 --decode-log-interval 1 --context-length 1000000,H800,Debug run with specific IB and networking environment variables to resolve intermittent sticking issues on H800 GPUs.,"Intermittent issues; successfully ran 3000 requests once, but stuck on subsequent attempts.",broken,https://github.com/sgl-project/sglang/issues/7381,"[Bug] When deploying QWEN3 using PD separation and the DP + EP + DeepEP method, the Decode instance get stuck."
nvidia/Qwen3-235B-A22B-FP8,SGLANG_TORCH_PROFILER_DIR=~/logs SGLANG_TBO_DEBUG=1 NCCL_IB_GID_INDEX=3 GLOO_SOCKET_IFNAME=eth0 SGL_ENABLE_JIT_DEEPGEMM=1 python3 -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-FP8 --load-format dummy --trust-remote-code --disaggregation-mode decode --disaggregation-ib-device mlx5_bond_0 --dist-init-addr 10.130.8.143:8050 --nnodes 4 --node-rank 0 --tp 32 --dp 32 --enable-dp-attention --enable-deepep-moe --ep-size 32 --disable-radix-cache --mem-frac 0.8 --deepep-mode low_latency --host 0.0.0.0 --moe-dense-tp-size 1 --enable-dp-lm-head --cuda-graph-max-bs 96 --watchdog-timeout 1000000 --decode-log-interval 1,H800,Debug Decode instance with TBO debug enabled and reduced cuda-graph-max-bs on H800 GPUs.,Still got stuck; py-spy stack trace shows the MainThread blocked in synchronize (torch/cuda/streams.py).,broken,https://github.com/sgl-project/sglang/issues/7381,"[Bug] When deploying QWEN3 using PD separation and the DP + EP + DeepEP method, the Decode instance get stuck."
nvidia/Qwen3-8B-FP8,SGL_ENABLE_JIT_DEEPGEMM=0 python -m sglang.launch_server --model-path nvidia/Qwen3-8B-FP4 --tp 1,4x NVIDIA RTX PRO 6000 Blackwell Server Edition (SM 12.0),Reproduction command to launch the server with the FP4 quantized model to debug quantization errors.,"BROKEN: Fails with ValueError: Unknown quantization method: modelopt. Even when trying --quantization fp4, it fails with a config mismatch error.",broken,https://github.com/sgl-project/sglang/issues/11725,[Bug] FP4 models quantized with modelopt not running in sglang
nvidia/Qwen3-8B-FP8,python -m sglang.launch_server --model-path nvidia/Qwen3-8B-FP4 --tp 1 --quantization modelopt_fp4,4x NVIDIA RTX PRO 6000 Blackwell Server Edition (SM 12.0),Attempting to use the specific modelopt_fp4 quantization flag as suggested by maintainers.,"BROKEN: Fails with 'Exception: Capture cuda graph failed: fp4_quant is only supported on sm100a/sm103a'. The user noted they are on Blackwell, which should support it, but the JIT/Kernel layer threw a compatibility error.",broken,https://github.com/sgl-project/sglang/issues/11725,[Bug] FP4 models quantized with modelopt not running in sglang
nvidia/Qwen3-8B-FP8,python3 -m sglang.test.few_shot_gsm8k --num-questions 1319,NVIDIA H20,Accuracy test on GSM8K using the default FP8 GEMM backend for the Qwen/Qwen3-8B-FP8 model.,"Accuracy: 0.904, Output throughput: 2892.650 token/s, Latency: 55.820 s.",working,https://github.com/sgl-project/sglang/pull/15763,[Perf] Add TileLang FP8 blockwise GEMM backend
nvidia/Qwen3-8B-FP8,python3 -m sglang.test.few_shot_gsm8k --num-questions 1319 --fp8-gemm-backend tilelang,NVIDIA H20,Accuracy test on GSM8K using the new TileLang FP8 GEMM backend for the Qwen/Qwen3-8B-FP8 model to ensure no regression in quality.,"Accuracy: 0.904, Output throughput: 2859.664 token/s, Latency: 55.999 s.",working,https://github.com/sgl-project/sglang/pull/15763,[Perf] Add TileLang FP8 blockwise GEMM backend
nvidia/Qwen3-8B-FP8,bash gsm8k.sh,H100,Accuracy and performance testing for Qwen3-8B model with Piecewise CUDA Graph enabled.,"Accuracy: 0.950, Latency: 7.752 s, Output throughput: 3076.427 token/s. Successfully processed 200/200 requests at 26.07 it/s.",working,https://github.com/sgl-project/sglang/pull/10062,Piecewise CUDA Graph Support & Torch Compile Backend
nvidia/Qwen3-8B-FP8,bash gsm8k.sh,H100,Baseline accuracy and performance testing for Qwen3-8B model without Piecewise CUDA Graph.,"Accuracy: 0.950, Latency: 11.384 s, Output throughput: 2091.467 token/s. Successfully processed 200/200 requests at 17.67 it/s.",working,https://github.com/sgl-project/sglang/pull/10062,Piecewise CUDA Graph Support & Torch Compile Backend
nvidia/Qwen3-8B-FP8,python -m sglang.bench_one_batch --model-path Qwen/Qwen3-14B --batch 1 --input-len 16 32 64 128 256 512 --output-len 1 --enable-piecewise-cuda-graph,H100,Prefill-only benchmark measuring latency with Piecewise CUDA Graph across different input lengths for Qwen3 models.,Benchmarked successfully. Benchmarking results showed performance improvements for short input lengths due to reduced kernel launch overhead.,working,https://github.com/sgl-project/sglang/pull/10062,Piecewise CUDA Graph Support & Torch Compile Backend
nvidia/Qwen3-8B-FP8,python3 -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B-FP8 --tp 4 --host 0.0.0.0 --enable-piecewise-cuda-graph,Tesla B200 (implied by DeepGEMM output),Testing high-parameter MoE variant (Qwen3-235B-A22B-FP8) with Piecewise CUDA Graph enabled.,BROKEN: Threw 'c10::Error: Only a single TORCH_LIBRARY can be used to register the namespace _C' and 'AssertionError' in rotary_embedding.py. The server terminated with a fatal Python error after capturing the CUDA graph.,broken,https://github.com/sgl-project/sglang/pull/10062,Piecewise CUDA Graph Support & Torch Compile Backend
nvidia/Qwen3-8B-FP8,python3 -m sglang.launch_server --model-path RedHatAI/Mistral-Small-24B-Instruct-2501-FP8-dynamic --quantization w8a8_fp8 --port 8081 --context-length 25000,NVIDIA RTX 5090 (Blackwell),Reproduce a bug where RMSNorm kernels fail on new Blackwell architecture GPUs.,BROKEN: Fails during CUDA graph capture with 'RuntimeError: RMSNorm failed with error code no kernel image is available for execution on the device'. This is due to missing Blackwell (sm_100/sm_120) support in the default sgl-kernel pip wheel.,broken,https://github.com/sgl-project/sglang/issues/7249,[Bug] RTX 5090: RMSNorm failed with error code no kernel image is available for execution on the device
nvidia/Qwen3-8B-FP8,USE_TRITON_W8A8_FP8_KERNEL=1 python3 -m sglang.launch_server --model-path zai-org/GLM-4.5-Air-FP8 --tp-size 2 --dtype fp8_w8a8,"Blackwell Workstation GPUs (e.g., RTX 5090 / Pro 6000)","Workaround for FP8 kernel issues on Blackwell by forcing the use of Triton kernels instead of Cutlass, and applying custom tuning.",Working well after manual tuning. Achieved 200 tok/s for GLM-4.5-Air-FP8. Initially reported memory issues before the user calibrated Triton and generated specific configs.,working,https://github.com/sgl-project/sglang/issues/7249,[Bug] RTX 5090: RMSNorm failed with error code no kernel image is available for execution on the device
nvidia/Qwen3-8B-FP8,python3 -m sglang.launch_server --model-path Qwen/Qwen3-8B-FP8 --dp 2 --quantization fp8 --host 0.0.0.0 --port 12000,L40S GPU (assigned device IDs 2 and 3),Normal sglang bootstrap using Docker Compose to serve the Qwen3-8B-FP8 model with data parallelism.,BROKEN: Failed with 'ModuleNotFoundError: No module named 'sentencepiece'' when starting with docker image lmsysorg/sglang:v0.4.9.post1-cu126.,broken,https://github.com/sgl-project/sglang/issues/7942,[Bug] 0.4.9.post1-cu126 reports error
nvidia/Qwen3-8B-FP8,SGL_ENABLE_JIT_DEEPGEMM=0 CUDA_VISIBLE_DEVICES=2 python -m sglang.launch_server --model-path /mnt/llms/models/Qwen/Qwen3-4B-FP8/ --tp 1 --context-length 2048 --port 5001 --host 0.0.0.0,NVIDIA GeForce RTX 5090 (Compute Capability 12.0),"Testing model loading and inference for a Qwen3 FP8 model on a single Blackwell GPU, requiring the deactivation of DeepGEMM JIT.",WORKS. Model successfully loads and runs when SGL_ENABLE_JIT_DEEPGEMM=0 is set.,working,https://github.com/sgl-project/sglang/issues/7482,[Bug] Some FP8 models fail to load
nvidia/Qwen3-8B-FP8,docker run --gpus all -p 8000:8000 --ipc=host -e SGL_ENABLE_JIT_DEEPGEMM=0 lmsysorg/sglang:blackwell python -m sglang.launch_server --model-path RedHatAI/Qwen3-32B-FP8-dynamic --port 8000 --context-length 60000,NVIDIA GeForce RTX 5090,Attempting to run a dynamic FP8 quantized version of Qwen3-32B (quantized by RedHatAI) inside a Docker container using the Blackwell-specific image.,"BROKEN. User reported that FP8-dynamic models from RedHatAI made with llm-compressor do not work with sglang, specifically failing with a CUTLASS status error during CUDA graph capture.",broken,https://github.com/sgl-project/sglang/issues/7482,[Bug] Some FP8 models fail to load
nvidia/Qwen3-8B-FP8,CUDA_VISIBLE_DEVICES=2 python -m sglang.launch_server --model-path /mnt/llms/models/Qwen/Qwen3-8B-FP8/ --tp 1 --context-length 8000 --port 5001 --host 0.0.0.0,1xRTX 5090,Testing the Qwen3-8B-FP8 model on a single Blackwell GPU setup to verify compatibility and performance.,"Broken. Encountered KeyError: '120a' in DeepGEMM dispatch, indicating that the '120a' architecture (RTX 5090) was not yet supported by the then-current DeepGEMM version during CUDA graph capture.",broken,https://github.com/sgl-project/sglang/issues/7227,[Roadmap] Blackwell Support and Optimizations
nvidia/Qwen3-8B-FP8,CUDA_VISIBLE_DEVICES=2 SGL_ENABLE_JIT_DEEPGEMM=0 python -m sglang.launch_server --model-path /mnt/llms/models/Qwen/Qwen3-8B-FP8/ --tp 1 --context-length 8000 --port 5001 --host 0.0.0.0,1xRTX 5090,Debug command to bypass DeepGEMM JIT which was failing with a KeyError on the Blackwell 5090 architecture.,Working. Disabling DeepGEMM JIT allowed the server to launch successfully on the 5090.,working,https://github.com/sgl-project/sglang/issues/7227,[Roadmap] Blackwell Support and Optimizations
nvidia/Qwen3-8B-FP8,docker run --gpus all -p 8000:8000 --ipc=host -e SGL_ENABLE_JIT_DEEPGEMM=0 lmsysorg/sglang:blackwell python -m sglang.launch_server --model-path Qwen/Qwen3-4B-FP8 --port 8000 --context-length 60000,1xRTX 5090,Experimental run using a smaller variant (4B FP8) within a specialized Blackwell docker container to verify the full stack on consumer Blackwell hardware with high context length.,Working. Successfully launched and served requests after disabling DeepGEMM JIT.,working,https://github.com/sgl-project/sglang/issues/7227,[Roadmap] Blackwell Support and Optimizations
nvidia/Qwen3-8B-FP8,CUDA_VISIBLE_DEVICES=2 python -m sglang.launch_server --model-path /mnt/llms/models/Qwen3-8B/ --tp 1 --context-length 8000 --port 5001 --host 0.0.0.0,1x RTX 5090,Testing Qwen3-8B performance with 16K context on a single 5090 GPU after manually installing torch 2.7.,Working. Achieved 83 t/s throughput for Qwen3-8B at 16K context.,working,https://github.com/sgl-project/sglang/issues/5338,[Tracker] Blackwell support
nvidia/Qwen3-8B-FP8,python -m sglang.launch_server --model-path Qwen3-8B --reasoning-parser qwen3 --dtype bfloat16,Not specified,Testing the --dtype parameter in version 0.4.6.post1 to ensure bfloat16 precision is applied correctly during server launch.,"BROKEN: Failed with AttributeError: module 'torch' has no attribute 'float8_e4m3fn'. This was caused by an outdated version of torch (2.0.1) which lacked definitions for FP8 constants required by the server's utility modules, even when attempting to run in bfloat16.",broken,https://github.com/sgl-project/sglang/issues/5869,[Bug] Issue with dtype parameter not taking effect in version 0.4.6.post1
nvidia/Qwen3-8B-FP8,python -m sglang.launch_server --model-path Qwen/Qwen3-VL-8B-Instruct-FP8 --tp-size 1 --mem-fraction-static 0.7 --context-length 8192 --max-running-requests 128 --host 0.0.0.0 --port 7860 --enable-metrics,"1x GPU (44.39 GiB capacity, likely RTX 6000 Ada or similar based on VRAM)",Original bug report: Serving Qwen3-VL-8B-Instruct-FP8 via OpenAI-compatible API with image inputs to investigate a potential memory leak.,"BROKEN: GPU memory usage grows steadily over time during concurrent image requests, eventually failing with 'torch.OutOfMemoryError: CUDA out of memory' during the logits processing step.",broken,https://github.com/sgl-project/sglang/issues/13940,[Bug] Qwen3-VL: GPU memory steadily increases with image requests (ends in CUDA OOM)
nvidia/Qwen3-8B-FP8,python3 -m sglang.launch_server --model-path /model/Qwen3-VL-32B-Instruct-FP8 --host xxx --port xxx --context-length 40960 --tp-size 4 --mem-fraction-static 0.45 --chunked-prefill-size 8192 --tool-call-parser qwen25 --attention-backend fa3 --mm-attention-backend fa3 --max-running-requests 10 --enable-cache-report --enable-metrics,"4x GPU (Environment: CUDA_VISIBLE_DEVICES=4,5,6,7)",Community member reproduction: Testing the 32B variant of the FP8 model in a Docker container with FlashAttention-3 enabled to see if memory OOM persists.,BROKEN: GPU memory usage rose from 50% to overflow; higher concurrency (--max-running-requests 20) caused faster OOM.,broken,https://github.com/sgl-project/sglang/issues/13940,[Bug] Qwen3-VL: GPU memory steadily increases with image requests (ends in CUDA OOM)
nvidia/Qwen3-8B-FP8,SGLANG_USE_CUDA_IPC_TRANSPORT=1 SGLANG_VLM_CACHE_SIZE_MB=0 python -m sglang.launch_server --model-path /home/admin/Qwen3-VL-8B-Instruct --host 0.0.0.0 --port 30000 --trust-remote-code --tp-size 8 --enable-cache-report --log-level info --max-running-requests 48 --mem-fraction-static 0.8 --chunked-prefill-size 8192 --attention-backend fa3 --mm-attention-backend fa3,8x NVIDIA H20,Maintainer attempt to reproduce the OOM using high TP and CUDA IPC transport enabled.,"Initially reported as stable usage at TP=8 on H20 GPUs, but later confirmed that rank0 overflow still occurs when concurrency is pushed to 100 with CUDA_IPC enabled.",broken,https://github.com/sgl-project/sglang/issues/13940,[Bug] Qwen3-VL: GPU memory steadily increases with image requests (ends in CUDA OOM)
nvidia/Qwen3-8B-FP8,SGLANG_MM_FEATURE_CACHE_MB=8192 SGLANG_USE_CUDA_IPC_TRANSPORT=1 SGLANG_VLM_CACHE_SIZE_MB=512 python -m sglang.launch_server --model-path /home/admin/Qwen3-VL-8B-Instruct/ --host 0.0.0.0 --port 8188 --trust-remote-code --tp-size 2 --enable-cache-report --log-level info --max-running-requests 48 --mem-fraction-static 0.7 --chunked-prefill-size 8192 --attention-backend flashinfer --mm-attention-backend fa3 --log-level debug --log-requests --log-requests-level 1,2x GPU (likely NVIDIA H20 as per previous bench contexts),Testing a workaround by significantly enlarging the multimodal feature cache to mitigate the memory leak caused by the CUDA IPC recycle mechanism.,"Working: Setting SGLANG_MM_FEATURE_CACHE_MB=8192 resolved the OOM for the specifically identified IPC leak, achieving 1.52 req/s at 100 max-concurrency.",working,https://github.com/sgl-project/sglang/issues/13940,[Bug] Qwen3-VL: GPU memory steadily increases with image requests (ends in CUDA OOM)
nvidia/Qwen3-8B-FP8,SGL_ENABLE_JIT_DEEPGEMM=0 LD_LIBRARY_PATH=/opt/conda/lib/python3.11/site-packages/nvidia/cuda_nvrtc/lib:${LD_LIBRARY_PATH} python -m sglang_router.launch_server --router-worker-startup-timeout-secs 900 --model-path llm/Qwen/Qwen3-8B-FP8 --dp 4 --context-length 32000 --host 0.0.0.0 --port 30001 --random-seed 1313,4x NVIDIA H200 (9.0 Compute Capability),"Loading a large model using sglang-router with Data Parallelism (DP) 4 and a context length of 32,000.","BROKEN: The router freezes during loading and cannot retrieve 'health' status from the workers, even with an extended startup timeout of 900 seconds.",broken,https://github.com/sgl-project/sglang/issues/5918,[Bug] sglang-router freezes loading on big models
