<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.managers.tokenizer_manager API documentation</title>
<meta name="description" content="TokenizerManager is a process that tokenizes the text.">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.managers.tokenizer_manager</code></h1>
</header>
<section id="section-intro">
<p>TokenizerManager is a process that tokenizes the text.</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.managers.tokenizer_manager.print_exception_wrapper"><code class="name flex">
<span>async def <span class="ident">print_exception_wrapper</span></span>(<span>func)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def print_exception_wrapper(func):
    &#34;&#34;&#34;
    Sometimes an asyncio function does not print exception.
    We do another wrapper to handle the exception.
    &#34;&#34;&#34;
    try:
        await func()
    except Exception:
        traceback = get_exception_traceback()
        logger.error(f&#34;TokenizerManager hit an exception: {traceback}&#34;)
        if hasattr(func, &#34;__self__&#34;) and isinstance(func.__self__, TokenizerManager):
            func.__self__.dump_requests_before_crash()
        kill_process_tree(os.getpid(), include_parent=True)
        sys.exit(1)</code></pre>
</details>
<div class="desc"><p>Sometimes an asyncio function does not print exception.
We do another wrapper to handle the exception.</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState"><code class="flex name class">
<span>class <span class="ident">ReqState</span></span>
<span>(</span><span>out_list: List[Dict[Any, Any]],<br>finished: bool,<br>event: asyncio.locks.Event,<br>obj: <a title="sglang.srt.managers.io_struct.GenerateReqInput" href="io_struct.html#sglang.srt.managers.io_struct.GenerateReqInput">GenerateReqInput</a> | <a title="sglang.srt.managers.io_struct.EmbeddingReqInput" href="io_struct.html#sglang.srt.managers.io_struct.EmbeddingReqInput">EmbeddingReqInput</a>,<br>created_time: float,<br>finished_time: float = 0.0,<br>first_token_time: float = 0.0,<br>last_time: float = 0.0,<br>last_completion_tokens: int = 1,<br>last_output_offset: int = 0,<br>text: str = '',<br>output_ids: List[int] = &lt;factory&gt;,<br>input_token_logprobs_val: List[float] = &lt;factory&gt;,<br>input_token_logprobs_idx: List[int] = &lt;factory&gt;,<br>output_token_logprobs_val: List[float] = &lt;factory&gt;,<br>output_token_logprobs_idx: List[int] = &lt;factory&gt;,<br>input_top_logprobs_val: List[List[float]] = &lt;factory&gt;,<br>input_top_logprobs_idx: List[List[int]] = &lt;factory&gt;,<br>output_top_logprobs_val: List[List[float]] = &lt;factory&gt;,<br>output_top_logprobs_idx: List[List[int]] = &lt;factory&gt;,<br>input_token_ids_logprobs_val: List = &lt;factory&gt;,<br>input_token_ids_logprobs_idx: List = &lt;factory&gt;,<br>output_token_ids_logprobs_val: List = &lt;factory&gt;,<br>output_token_ids_logprobs_idx: List = &lt;factory&gt;)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclasses.dataclass
class ReqState:
    &#34;&#34;&#34;Store the state a request.&#34;&#34;&#34;

    out_list: List[Dict[Any, Any]]
    finished: bool
    event: asyncio.Event
    obj: Union[GenerateReqInput, EmbeddingReqInput]

    # For metrics
    created_time: float
    finished_time: float = 0.0
    first_token_time: float = 0.0
    last_time: float = 0.0
    last_completion_tokens: int = 1

    # For streaming output
    last_output_offset: int = 0

    # For incremental state update.
    # TODO(lianmin): do not initialize some lists if not needed.
    text: str = &#34;&#34;
    output_ids: List[int] = dataclasses.field(default_factory=list)
    input_token_logprobs_val: List[float] = dataclasses.field(default_factory=list)
    input_token_logprobs_idx: List[int] = dataclasses.field(default_factory=list)
    output_token_logprobs_val: List[float] = dataclasses.field(default_factory=list)
    output_token_logprobs_idx: List[int] = dataclasses.field(default_factory=list)
    input_top_logprobs_val: List[List[float]] = dataclasses.field(default_factory=list)
    input_top_logprobs_idx: List[List[int]] = dataclasses.field(default_factory=list)
    output_top_logprobs_val: List[List[float]] = dataclasses.field(default_factory=list)
    output_top_logprobs_idx: List[List[int]] = dataclasses.field(default_factory=list)
    input_token_ids_logprobs_val: List = dataclasses.field(default_factory=list)
    input_token_ids_logprobs_idx: List = dataclasses.field(default_factory=list)
    output_token_ids_logprobs_val: List = dataclasses.field(default_factory=list)
    output_token_ids_logprobs_idx: List = dataclasses.field(default_factory=list)</code></pre>
</details>
<div class="desc"><p>Store the state a request.</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.created_time"><code class="name">var <span class="ident">created_time</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.event"><code class="name">var <span class="ident">event</span> : asyncio.locks.Event</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.finished"><code class="name">var <span class="ident">finished</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.finished_time"><code class="name">var <span class="ident">finished_time</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.first_token_time"><code class="name">var <span class="ident">first_token_time</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.input_token_ids_logprobs_idx"><code class="name">var <span class="ident">input_token_ids_logprobs_idx</span> : List</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.input_token_ids_logprobs_val"><code class="name">var <span class="ident">input_token_ids_logprobs_val</span> : List</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.input_token_logprobs_idx"><code class="name">var <span class="ident">input_token_logprobs_idx</span> : List[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.input_token_logprobs_val"><code class="name">var <span class="ident">input_token_logprobs_val</span> : List[float]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.input_top_logprobs_idx"><code class="name">var <span class="ident">input_top_logprobs_idx</span> : List[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.input_top_logprobs_val"><code class="name">var <span class="ident">input_top_logprobs_val</span> : List[List[float]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.last_completion_tokens"><code class="name">var <span class="ident">last_completion_tokens</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.last_output_offset"><code class="name">var <span class="ident">last_output_offset</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.last_time"><code class="name">var <span class="ident">last_time</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.obj"><code class="name">var <span class="ident">obj</span> : <a title="sglang.srt.managers.io_struct.GenerateReqInput" href="io_struct.html#sglang.srt.managers.io_struct.GenerateReqInput">GenerateReqInput</a> | <a title="sglang.srt.managers.io_struct.EmbeddingReqInput" href="io_struct.html#sglang.srt.managers.io_struct.EmbeddingReqInput">EmbeddingReqInput</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.out_list"><code class="name">var <span class="ident">out_list</span> : List[Dict[Any, Any]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.output_ids"><code class="name">var <span class="ident">output_ids</span> : List[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.output_token_ids_logprobs_idx"><code class="name">var <span class="ident">output_token_ids_logprobs_idx</span> : List</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.output_token_ids_logprobs_val"><code class="name">var <span class="ident">output_token_ids_logprobs_val</span> : List</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.output_token_logprobs_idx"><code class="name">var <span class="ident">output_token_logprobs_idx</span> : List[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.output_token_logprobs_val"><code class="name">var <span class="ident">output_token_logprobs_val</span> : List[float]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.output_top_logprobs_idx"><code class="name">var <span class="ident">output_top_logprobs_idx</span> : List[List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.output_top_logprobs_val"><code class="name">var <span class="ident">output_top_logprobs_val</span> : List[List[float]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ReqState.text"><code class="name">var <span class="ident">text</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ServerStatus"><code class="flex name class">
<span>class <span class="ident">ServerStatus</span></span>
<span>(</span><span>*args, **kwds)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ServerStatus(Enum):
    Up = &#34;Up&#34;
    Starting = &#34;Starting&#34;
    UnHealthy = &#34;UnHealthy&#34;</code></pre>
</details>
<div class="desc"><p>Create a collection of name/value pairs.</p>
<p>Example enumeration:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; class Color(Enum):
...     RED = 1
...     BLUE = 2
...     GREEN = 3
</code></pre>
<p>Access them by:</p>
<ul>
<li>attribute access:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color.RED
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<ul>
<li>value lookup:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color(1)
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<ul>
<li>name lookup:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color['RED']
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<p>Enumerations can be iterated over, and know how many members they have:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; len(Color)
3
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; list(Color)
[&lt;Color.RED: 1&gt;, &lt;Color.BLUE: 2&gt;, &lt;Color.GREEN: 3&gt;]
</code></pre>
<p>Methods can be added to enumerations, and members can have their own
attributes &ndash; see the documentation for details.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.managers.tokenizer_manager.ServerStatus.Starting"><code class="name">var <span class="ident">Starting</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ServerStatus.UnHealthy"><code class="name">var <span class="ident">UnHealthy</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.ServerStatus.Up"><code class="name">var <span class="ident">Up</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.SignalHandler"><code class="flex name class">
<span>class <span class="ident">SignalHandler</span></span>
<span>(</span><span>tokenizer_manager: <a title="sglang.srt.managers.tokenizer_manager.TokenizerManager" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager">TokenizerManager</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SignalHandler:
    def __init__(self, tokenizer_manager: TokenizerManager):
        self.tokenizer_manager = tokenizer_manager

    def sigterm_handler(self, signum=None, frame=None):
        logger.warning(
            f&#34;SIGTERM received. {signum=} {frame=}. Draining requests and shutting down...&#34;
        )
        self.tokenizer_manager.gracefully_exit = True

    def running_phase_sigquit_handler(self, signum=None, frame=None):
        logger.error(
            &#34;Received sigquit from a child process. It usually means the child failed.&#34;
        )
        self.tokenizer_manager.dump_requests_before_crash()
        kill_process_tree(os.getpid())</code></pre>
</details>
<div class="desc"></div>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.managers.tokenizer_manager.SignalHandler.running_phase_sigquit_handler"><code class="name flex">
<span>def <span class="ident">running_phase_sigquit_handler</span></span>(<span>self, signum=None, frame=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def running_phase_sigquit_handler(self, signum=None, frame=None):
    logger.error(
        &#34;Received sigquit from a child process. It usually means the child failed.&#34;
    )
    self.tokenizer_manager.dump_requests_before_crash()
    kill_process_tree(os.getpid())</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.SignalHandler.sigterm_handler"><code class="name flex">
<span>def <span class="ident">sigterm_handler</span></span>(<span>self, signum=None, frame=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sigterm_handler(self, signum=None, frame=None):
    logger.warning(
        f&#34;SIGTERM received. {signum=} {frame=}. Draining requests and shutting down...&#34;
    )
    self.tokenizer_manager.gracefully_exit = True</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager"><code class="flex name class">
<span>class <span class="ident">TokenizerManager</span></span>
<span>(</span><span>server_args: <a title="sglang.srt.server_args.ServerArgs" href="../server_args.html#sglang.srt.server_args.ServerArgs">ServerArgs</a>,<br>port_args: <a title="sglang.srt.server_args.PortArgs" href="../server_args.html#sglang.srt.server_args.PortArgs">PortArgs</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TokenizerManager:
    &#34;&#34;&#34;TokenizerManager is a process that tokenizes the text.&#34;&#34;&#34;

    def __init__(
        self,
        server_args: ServerArgs,
        port_args: PortArgs,
    ):
        # Parse args
        self.server_args = server_args
        self.enable_metrics = server_args.enable_metrics
        self.log_requests = server_args.log_requests
        self.log_requests_level = server_args.log_requests_level
        self.preferred_sampling_params = (
            json.loads(server_args.preferred_sampling_params)
            if server_args.preferred_sampling_params
            else None
        )
        self.crash_dump_folder = server_args.crash_dump_folder

        # Read model args
        self.model_path = server_args.model_path
        self.served_model_name = server_args.served_model_name
        self.model_config = ModelConfig.from_server_args(server_args)
        self.is_generation = self.model_config.is_generation
        self.is_image_gen = self.model_config.is_image_gen
        self.context_len = self.model_config.context_len
        self.image_token_id = self.model_config.image_token_id
        self.max_req_input_len = None  # Will be set later in engine.py

        if self.model_config.is_multimodal:
            import_processors()
            try:
                _processor = get_processor(
                    server_args.tokenizer_path,
                    tokenizer_mode=server_args.tokenizer_mode,
                    trust_remote_code=server_args.trust_remote_code,
                    revision=server_args.revision,
                    use_fast=not server_args.disable_fast_image_processor,
                )
            except ValueError as e:
                error_message = str(e)
                if &#34;does not have a slow version&#34; in error_message:
                    logger.info(
                        f&#34;Processor {server_args.tokenizer_path} does not have a slow version. Automatically use fast version&#34;
                    )
                    _processor = get_processor(
                        server_args.tokenizer_path,
                        tokenizer_mode=server_args.tokenizer_mode,
                        trust_remote_code=server_args.trust_remote_code,
                        revision=server_args.revision,
                        use_fast=True,
                    )
                else:
                    raise e
            transport_mode = _determine_tensor_transport_mode(self.server_args)

            # We want to parallelize the image pre-processing so we create an executor for it
            # We create mm_processor for any skip_tokenizer_init to make sure we still encode
            # images even with skip_tokenizer_init=False.
            self.mm_processor = get_mm_processor(
                self.model_config.hf_config, server_args, _processor, transport_mode
            )

            if server_args.skip_tokenizer_init:
                self.tokenizer = self.processor = None
            else:
                self.processor = _processor
                self.tokenizer = get_tokenizer_from_processor(self.processor)
                os.environ[&#34;TOKENIZERS_PARALLELISM&#34;] = &#34;false&#34;
        else:
            self.mm_processor = self.processor = None

            if server_args.skip_tokenizer_init:
                self.tokenizer = None
            else:
                self.tokenizer = get_tokenizer(
                    server_args.tokenizer_path,
                    tokenizer_mode=server_args.tokenizer_mode,
                    trust_remote_code=server_args.trust_remote_code,
                    revision=server_args.revision,
                )

        # Init inter-process communication
        context = zmq.asyncio.Context(2)
        self.recv_from_detokenizer = get_zmq_socket(
            context, zmq.PULL, port_args.tokenizer_ipc_name, True
        )
        self.send_to_scheduler = get_zmq_socket(
            context, zmq.PUSH, port_args.scheduler_input_ipc_name, True
        )

        # Request states
        self.no_create_loop = False
        self.rid_to_state: Dict[str, ReqState] = {}
        self.asyncio_tasks = set()

        # Health check
        self.server_status = ServerStatus.Starting
        self.gracefully_exit = False
        self.last_receive_tstamp = 0

        # Dumping
        self.dump_requests_folder = &#34;&#34;  # By default do not dump
        self.dump_requests_threshold = 1000
        self.dump_request_list: List[Tuple] = []
        self.log_request_metadata = self.get_log_request_metadata()
        self.crash_dump_request_list: deque[Tuple] = deque()
        self.crash_dump_performed = False  # Flag to ensure dump is only called once

        # Session
        self.session_futures = {}  # session_id -&gt; asyncio event

        # Weight updates
        # The event to notify the weight sync is finished.
        self.model_update_lock = RWLock()
        self.model_update_result: Optional[Awaitable[UpdateWeightFromDiskReqOutput]] = (
            None
        )
        self.is_pause = False
        self.is_pause_cond = asyncio.Condition()

        # LoRA
        # Initialize the `LoRARegistry` with initial LoRA adapter paths provided in `server_args`.
        # The registry dynamically updates as adapters are loaded / unloaded during runtime. It
        # serves as the source of truth for available adapters and maps user-friendly LoRA names
        # to internally used unique LoRA IDs.
        self.lora_registry = LoRARegistry(self.server_args.lora_paths)
        # Lock to serialize LoRA update operations.
        # Please note that, unlike `model_update_lock`, this does not block inference, allowing
        # LoRA updates and inference to overlap.
        self.lora_update_lock = asyncio.Lock()

        # For PD disaggregtion
        self.disaggregation_mode = DisaggregationMode(
            self.server_args.disaggregation_mode
        )
        self.disaggregation_transfer_backend = TransferBackend(
            self.server_args.disaggregation_transfer_backend
        )
        # Start kv boostrap server on prefill
        if self.disaggregation_mode == DisaggregationMode.PREFILL:
            # only start bootstrap server on prefill tm
            kv_bootstrap_server_class = get_kv_class(
                self.disaggregation_transfer_backend, KVClassType.BOOTSTRAP_SERVER
            )
            self.bootstrap_server = kv_bootstrap_server_class(
                self.server_args.disaggregation_bootstrap_port
            )
            is_create_store = (
                self.server_args.node_rank == 0
                and self.server_args.disaggregation_transfer_backend == &#34;ascend&#34;
            )
            if is_create_store:
                try:
                    from mf_adapter import create_config_store

                    ascend_url = os.getenv(&#34;ASCEND_MF_STORE_URL&#34;)
                    create_config_store(ascend_url)
                except Exception as e:
                    error_message = f&#34;Failed create mf store, invalid ascend_url.&#34;
                    error_message += f&#34; With exception {e}&#34;
                    raise error_message

        # For load balancing
        self.current_load = 0
        self.current_load_lock = asyncio.Lock()

        # Metrics
        if self.enable_metrics:
            self.metrics_collector = TokenizerMetricsCollector(
                labels={
                    &#34;model_name&#34;: self.server_args.served_model_name,
                    # TODO: Add lora name/path in the future,
                },
                bucket_time_to_first_token=self.server_args.bucket_time_to_first_token,
                bucket_e2e_request_latency=self.server_args.bucket_e2e_request_latency,
                bucket_inter_token_latency=self.server_args.bucket_inter_token_latency,
                collect_tokens_histogram=self.server_args.collect_tokens_histogram,
            )

        # Configure GC warning
        if self.server_args.gc_warning_threshold_secs &gt; 0.0:
            configure_gc_warning(self.server_args.gc_warning_threshold_secs)

        # Communicators
        self.init_weights_update_group_communicator = _Communicator(
            self.send_to_scheduler, server_args.dp_size
        )
        self.update_weights_from_distributed_communicator = _Communicator(
            self.send_to_scheduler, server_args.dp_size
        )
        self.update_weights_from_tensor_communicator = _Communicator(
            self.send_to_scheduler, server_args.dp_size
        )
        self.get_weights_by_name_communicator = _Communicator(
            self.send_to_scheduler, server_args.dp_size
        )
        self.release_memory_occupation_communicator = _Communicator(
            self.send_to_scheduler, server_args.dp_size
        )
        self.resume_memory_occupation_communicator = _Communicator(
            self.send_to_scheduler, server_args.dp_size
        )
        self.slow_down_communicator = _Communicator(
            self.send_to_scheduler, server_args.dp_size
        )
        self.flush_cache_communicator = _Communicator(
            self.send_to_scheduler, server_args.dp_size
        )
        self.clear_hicache_storage_communicator = _Communicator(
            self.send_to_scheduler, server_args.dp_size
        )
        self.profile_communicator = _Communicator(
            self.send_to_scheduler, server_args.dp_size
        )
        self.get_internal_state_communicator = _Communicator(
            self.send_to_scheduler, server_args.dp_size
        )
        self.set_internal_state_communicator = _Communicator(
            self.send_to_scheduler, server_args.dp_size
        )
        self.expert_distribution_communicator = _Communicator(
            self.send_to_scheduler, server_args.dp_size
        )
        self.update_lora_adapter_communicator = _Communicator(
            self.send_to_scheduler, server_args.dp_size
        )

        self._result_dispatcher = TypeBasedDispatcher(
            [
                (
                    (
                        BatchStrOut,
                        BatchEmbeddingOut,
                        BatchTokenIDOut,
                        BatchMultimodalOut,
                    ),
                    self._handle_batch_output,
                ),
                (AbortReq, self._handle_abort_req),
                (OpenSessionReqOutput, self._handle_open_session_req_output),
                (
                    UpdateWeightFromDiskReqOutput,
                    self._handle_update_weights_from_disk_req_output,
                ),
                (
                    InitWeightsUpdateGroupReqOutput,
                    self.init_weights_update_group_communicator.handle_recv,
                ),
                (
                    UpdateWeightsFromDistributedReqOutput,
                    self.update_weights_from_distributed_communicator.handle_recv,
                ),
                (
                    UpdateWeightsFromTensorReqOutput,
                    self.update_weights_from_tensor_communicator.handle_recv,
                ),
                (
                    GetWeightsByNameReqOutput,
                    self.get_weights_by_name_communicator.handle_recv,
                ),
                (
                    ReleaseMemoryOccupationReqOutput,
                    self.release_memory_occupation_communicator.handle_recv,
                ),
                (
                    ResumeMemoryOccupationReqOutput,
                    self.resume_memory_occupation_communicator.handle_recv,
                ),
                (
                    SlowDownReqOutput,
                    self.slow_down_communicator.handle_recv,
                ),
                (
                    ClearHiCacheReqOutput,
                    self.clear_hicache_storage_communicator.handle_recv,
                ),
                (
                    FlushCacheReqOutput,
                    self.flush_cache_communicator.handle_recv,
                ),
                (
                    ProfileReqOutput,
                    self.profile_communicator.handle_recv,
                ),
                (
                    FreezeGCReq,
                    lambda x: None,
                ),  # For handling case when scheduler skips detokenizer and forwards back to the tokenizer manager, we ignore it.
                (
                    GetInternalStateReqOutput,
                    self.get_internal_state_communicator.handle_recv,
                ),
                (
                    SetInternalStateReqOutput,
                    self.set_internal_state_communicator.handle_recv,
                ),
                (
                    ExpertDistributionReqOutput,
                    self.expert_distribution_communicator.handle_recv,
                ),
                (
                    LoRAUpdateResult,
                    self.update_lora_adapter_communicator.handle_recv,
                ),
                (HealthCheckOutput, lambda x: None),
            ]
        )

    async def generate_request(
        self,
        obj: Union[GenerateReqInput, EmbeddingReqInput],
        request: Optional[fastapi.Request] = None,
    ):
        created_time = time.time()
        self.auto_create_handle_loop()
        obj.normalize_batch_and_arguments()

        if self.log_requests:
            max_length, skip_names, _ = self.log_request_metadata
            logger.info(
                f&#34;Receive: obj={dataclass_to_string_truncated(obj, max_length, skip_names=skip_names)}&#34;
            )

        async with self.is_pause_cond:
            await self.is_pause_cond.wait_for(lambda: not self.is_pause)

        async with self.model_update_lock.reader_lock:
            if self.server_args.enable_lora and obj.lora_path:
                # Look up the LoRA ID from the registry and start tracking ongoing LoRA requests.
                obj.lora_id = await self.lora_registry.acquire(obj.lora_path)

            if obj.is_single:
                tokenized_obj = await self._tokenize_one_request(obj)
                state = self._send_one_request(obj, tokenized_obj, created_time)
                async for response in self._wait_one_response(obj, state, request):
                    yield response
            else:
                async for response in self._handle_batch_request(
                    obj, request, created_time
                ):
                    yield response

    async def _tokenize_one_request(
        self,
        obj: Union[GenerateReqInput, EmbeddingReqInput],
    ):
        &#34;&#34;&#34;Tokenize one request.&#34;&#34;&#34;
        # Tokenize
        input_embeds = None
        input_text = obj.text
        token_type_ids = None
        is_cross_encoder_request = (
            isinstance(obj, EmbeddingReqInput) and obj.is_cross_encoder_request
        )
        if obj.input_embeds is not None:
            if not self.server_args.disable_radix_cache:
                raise ValueError(
                    &#34;input_embeds is provided while disable_radix_cache is False. &#34;
                    &#34;Please add `--disable-radix-cache` when you launch the server &#34;
                    &#34;if you want to use input_embeds as inputs.&#34;
                )
            input_embeds = obj.input_embeds
            input_ids = obj.input_ids
        elif obj.input_ids is not None:
            input_ids = obj.input_ids
        else:
            if self.tokenizer is None:
                raise ValueError(
                    &#34;The engine initialized with skip_tokenizer_init=True cannot &#34;
                    &#34;accept text prompts. Please provide input_ids or re-initialize &#34;
                    &#34;the engine with skip_tokenizer_init=False.&#34;
                )
            encoded = self.tokenizer(
                input_text, return_token_type_ids=is_cross_encoder_request
            )

            input_ids = encoded[&#34;input_ids&#34;]
            if is_cross_encoder_request:
                input_ids = encoded[&#34;input_ids&#34;][0]
                token_type_ids = encoded.get(&#34;token_type_ids&#34;, [None])[0]

        if self.mm_processor and obj.contains_mm_input():
            if not isinstance(obj.image_data, list):
                obj.image_data = [obj.image_data]
            if not isinstance(obj.audio_data, list):
                obj.audio_data = [obj.audio_data]
            mm_inputs: Dict = await self.mm_processor.process_mm_data_async(
                image_data=obj.image_data,
                audio_data=obj.audio_data,
                input_text=input_text or input_ids,
                request_obj=obj,
                max_req_input_len=self.max_req_input_len,
            )
            if mm_inputs and &#34;input_ids&#34; in mm_inputs:
                input_ids = mm_inputs[&#34;input_ids&#34;]
        else:
            mm_inputs = None

        self._validate_one_request(obj, input_ids)
        return self._create_tokenized_object(
            obj, input_text, input_ids, input_embeds, mm_inputs, token_type_ids
        )

    def _validate_one_request(
        self, obj: Union[GenerateReqInput, EmbeddingReqInput], input_ids: List[int]
    ) -&gt; None:
        &#34;&#34;&#34;Validates that the input token count and the requested token count doesn&#39;t exceed the model&#39;s context length.&#34;&#34;&#34;
        # FIXME: unify the length validation logic with the one in the scheduler.
        _max_req_len = self.context_len

        input_token_num = len(input_ids) if input_ids is not None else 0
        if input_token_num &gt;= self.context_len:
            if self.server_args.allow_auto_truncate:
                logger.warning(
                    f&#34;The input ({input_token_num} tokens) is longer than the &#34;
                    f&#34;model&#39;s context length ({self.context_len} tokens). &#34;
                    &#34;Truncating the input.&#34;
                )
                del input_ids[_max_req_len:]
                input_token_num = len(input_ids)
            else:
                raise ValueError(
                    f&#34;The input ({input_token_num} tokens) is longer than the &#34;
                    f&#34;model&#39;s context length ({self.context_len} tokens).&#34;
                )

        if isinstance(obj, EmbeddingReqInput) and self.is_generation:
            raise ValueError(
                &#34;This model does not appear to be an embedding model by default. &#34;
                &#34;Please add `--is-embedding` when launching the server or try another model.&#34;
            )

        # Check total tokens (input + max_new_tokens)
        max_new_tokens = obj.sampling_params.get(&#34;max_new_tokens&#34;)
        if (
            max_new_tokens is not None
            and (max_new_tokens + input_token_num) &gt;= _max_req_len
        ):
            if self.server_args.allow_auto_truncate:
                logger.warning(
                    f&#34;Requested token count ({input_token_num} input + {max_new_tokens} new) &#34;
                    f&#34;exceeds the model&#39;s context length ({self.context_len} tokens). &#34;
                    &#34;Truncating max_new_tokens.&#34;
                )
                obj.sampling_params[&#34;max_new_tokens&#34;] = max(
                    0, _max_req_len - input_token_num
                )
            else:
                total_tokens = max_new_tokens + input_token_num
                error_msg = (
                    f&#34;Requested token count exceeds the model&#39;s maximum context length &#34;
                    f&#34;of {self.context_len} tokens. You requested a total of {total_tokens} &#34;
                    f&#34;tokens: {input_token_num} tokens from the input messages and &#34;
                    f&#34;{max_new_tokens} tokens for the completion. Please reduce the number &#34;
                    f&#34;of tokens in the input messages or the completion to fit within the limit.&#34;
                )
                raise ValueError(error_msg)

        if isinstance(obj, GenerateReqInput):
            if (
                obj.return_hidden_states
                and not self.server_args.enable_return_hidden_states
            ):
                raise ValueError(
                    &#34;The server is not configured to return the hidden states. &#34;
                    &#34;Please set `--enable-return-hidden-states` to enable this feature.&#34;
                )
            if (
                obj.custom_logit_processor
                and not self.server_args.enable_custom_logit_processor
            ):
                raise ValueError(
                    &#34;The server is not configured to enable custom logit processor. &#34;
                    &#34;Please set `--enable-custom-logits-processor` to enable this feature.&#34;
                )

    def _validate_input_ids_in_vocab(
        self, input_ids: List[int], vocab_size: int
    ) -&gt; None:
        if any(id &gt;= vocab_size for id in input_ids):
            raise ValueError(
                f&#34;The input_ids {input_ids} contains values greater than the vocab size ({vocab_size}).&#34;
            )

    def _create_tokenized_object(
        self,
        obj: Union[GenerateReqInput, EmbeddingReqInput],
        input_text: str,
        input_ids: List[int],
        input_embeds: Optional[Union[List[float], None]] = None,
        mm_inputs: Optional[Dict] = None,
        token_type_ids: Optional[List[int]] = None,
    ) -&gt; Union[TokenizedGenerateReqInput, TokenizedEmbeddingReqInput]:
        &#34;&#34;&#34;Create a tokenized request object from common parameters.&#34;&#34;&#34;
        # Parse sampling parameters
        # Note: if there are preferred sampling params, we use them if they are not
        # explicitly passed in sampling_params
        if self.preferred_sampling_params:
            sampling_kwargs = {**self.preferred_sampling_params, **obj.sampling_params}
        else:
            sampling_kwargs = obj.sampling_params
        sampling_params = SamplingParams(**sampling_kwargs)
        sampling_params.normalize(self.tokenizer)
        sampling_params.verify(self.model_config.vocab_size)

        # Build return object
        if isinstance(obj, GenerateReqInput):
            session_params = (
                SessionParams(**obj.session_params) if obj.session_params else None
            )

            tokenized_obj = TokenizedGenerateReqInput(
                obj.rid,
                input_text,
                input_ids,
                mm_inputs,
                sampling_params,
                obj.return_logprob,
                obj.logprob_start_len,
                obj.top_logprobs_num,
                obj.token_ids_logprob,
                obj.stream,
                bootstrap_host=obj.bootstrap_host,
                bootstrap_port=obj.bootstrap_port,
                bootstrap_room=obj.bootstrap_room,
                lora_id=obj.lora_id,
                input_embeds=input_embeds,
                session_params=session_params,
                custom_logit_processor=obj.custom_logit_processor,
                return_hidden_states=obj.return_hidden_states,
                data_parallel_rank=obj.data_parallel_rank,
            )
        elif isinstance(obj, EmbeddingReqInput):
            tokenized_obj = TokenizedEmbeddingReqInput(
                obj.rid,
                input_text,
                input_ids,
                mm_inputs,
                token_type_ids,
                sampling_params,
            )

        return tokenized_obj

    async def _batch_tokenize_and_process(
        self, batch_size: int, obj: Union[GenerateReqInput, EmbeddingReqInput]
    ) -&gt; List[Union[TokenizedGenerateReqInput, TokenizedEmbeddingReqInput]]:
        &#34;&#34;&#34;Handle batch tokenization for text inputs only.&#34;&#34;&#34;
        logger.debug(f&#34;Starting batch tokenization for {batch_size} text requests&#34;)

        # Collect requests and texts
        requests = [obj[i] for i in range(batch_size)]
        texts = [req.text for req in requests]

        # Batch tokenize all texts
        encoded = self.tokenizer(texts)
        input_ids_list = encoded[&#34;input_ids&#34;]

        # Process all requests
        tokenized_objs = []
        for i, req in enumerate(requests):
            self._validate_one_request(obj[i], input_ids_list[i])
            tokenized_objs.append(
                self._create_tokenized_object(
                    req, req.text, input_ids_list[i], None, None
                )
            )
        logger.debug(f&#34;Completed batch processing for {batch_size} requests&#34;)
        return tokenized_objs

    def _validate_batch_tokenization_constraints(
        self, batch_size: int, obj: Union[GenerateReqInput, EmbeddingReqInput]
    ) -&gt; None:
        &#34;&#34;&#34;Validate constraints for batch tokenization processing.&#34;&#34;&#34;
        for i in range(batch_size):
            if self.is_generation and obj[i].contains_mm_input():
                raise ValueError(
                    &#34;For multimodal input processing do not set `enable_tokenizer_batch_encode`.&#34;
                )
            if obj[i].input_ids is not None:
                raise ValueError(
                    &#34;Batch tokenization is not needed for pre-tokenized input_ids. Do not set `enable_tokenizer_batch_encode`.&#34;
                )
            if obj[i].input_embeds is not None:
                raise ValueError(
                    &#34;Batch tokenization is not needed for input_embeds. Do not set `enable_tokenizer_batch_encode`.&#34;
                )

    def _send_one_request(
        self,
        obj: Union[GenerateReqInput, EmbeddingReqInput],
        tokenized_obj: Union[TokenizedGenerateReqInput, TokenizedEmbeddingReqInput],
        created_time: Optional[float] = None,
    ):
        self.send_to_scheduler.send_pyobj(tokenized_obj)
        state = ReqState([], False, asyncio.Event(), obj, created_time=created_time)
        self.rid_to_state[obj.rid] = state
        return state

    def _send_batch_request(
        self,
        obj: Union[GenerateReqInput, EmbeddingReqInput],
        tokenized_objs: List[
            Union[TokenizedGenerateReqInput, TokenizedEmbeddingReqInput]
        ],
        created_time: Optional[float] = None,
    ):
        &#34;&#34;&#34;Send a batch of tokenized requests as a single batched request to the scheduler.&#34;&#34;&#34;
        if isinstance(tokenized_objs[0], TokenizedGenerateReqInput):
            batch_req = BatchTokenizedGenerateReqInput(batch=tokenized_objs)
        else:
            batch_req = BatchTokenizedEmbeddingReqInput(batch=tokenized_objs)

        self.send_to_scheduler.send_pyobj(batch_req)

        # Create states for each individual request in the batch
        for i, tokenized_obj in enumerate(tokenized_objs):
            tmp_obj = obj[i]
            state = ReqState(
                [], False, asyncio.Event(), tmp_obj, created_time=created_time
            )
            self.rid_to_state[tmp_obj.rid] = state

    async def _wait_one_response(
        self,
        obj: Union[GenerateReqInput, EmbeddingReqInput],
        state: ReqState,
        request: Optional[fastapi.Request] = None,
    ):
        &#34;&#34;&#34;Wait for the response of one request.&#34;&#34;&#34;
        while True:
            try:
                await asyncio.wait_for(state.event.wait(), timeout=4)
            except asyncio.TimeoutError:
                if (
                    request is not None
                    and not obj.background
                    and await request.is_disconnected()
                ):
                    # Abort the request for disconnected requests (non-streaming, waiting queue)
                    self.abort_request(obj.rid)
                    # Use exception to kill the whole call stack and asyncio task
                    raise ValueError(
                        f&#34;Request is disconnected from the client side (type 1). Abort request {obj.rid=}&#34;
                    )
                continue

            out = state.out_list[-1]

            state.out_list = []
            if state.finished:
                if self.log_requests:
                    max_length, skip_names, out_skip_names = self.log_request_metadata
                    if self.model_config.is_multimodal_gen:
                        msg = f&#34;Finish: obj={dataclass_to_string_truncated(obj, max_length, skip_names=skip_names)}&#34;
                    else:
                        msg = f&#34;Finish: obj={dataclass_to_string_truncated(obj, max_length, skip_names=skip_names)}, out={dataclass_to_string_truncated(out, max_length, skip_names=out_skip_names)}&#34;
                    logger.info(msg)

                # Check if this was an abort/error created by scheduler
                if isinstance(out[&#34;meta_info&#34;].get(&#34;finish_reason&#34;), dict):
                    finish_reason = out[&#34;meta_info&#34;][&#34;finish_reason&#34;]
                    if (
                        finish_reason.get(&#34;type&#34;) == &#34;abort&#34;
                        and finish_reason.get(&#34;status_code&#34;) == HTTPStatus.BAD_REQUEST
                    ):
                        raise ValueError(finish_reason[&#34;message&#34;])

                    if finish_reason.get(&#34;type&#34;) == &#34;abort&#34; and finish_reason.get(
                        &#34;status_code&#34;
                    ) in (
                        HTTPStatus.SERVICE_UNAVAILABLE,
                        HTTPStatus.INTERNAL_SERVER_ERROR,
                    ):
                        # This is an abort request initiated by scheduler.
                        # Delete the key to prevent resending abort request to the scheduler and
                        # to ensure aborted request state is cleaned up.
                        if state.obj.rid in self.rid_to_state:
                            del self.rid_to_state[state.obj.rid]

                        # Mark ongoing LoRA request as finished.
                        if self.server_args.enable_lora and state.obj.lora_path:
                            await self.lora_registry.release(state.obj.lora_id)

                        raise fastapi.HTTPException(
                            status_code=finish_reason[&#34;status_code&#34;],
                            detail=finish_reason[&#34;message&#34;],
                        )
                yield out
                break

            state.event.clear()

            if obj.stream:
                yield out
            else:
                if (
                    request is not None
                    and not obj.background
                    and await request.is_disconnected()
                ):
                    # Abort the request for disconnected requests (non-streaming, running)
                    self.abort_request(obj.rid)
                    # Use exception to kill the whole call stack and asyncio task
                    raise ValueError(
                        f&#34;Request is disconnected from the client side (type 3). Abort request {obj.rid=}&#34;
                    )

    async def _handle_batch_request(
        self,
        obj: Union[GenerateReqInput, EmbeddingReqInput],
        request: Optional[fastapi.Request] = None,
        created_time: Optional[float] = None,
    ):
        batch_size = obj.batch_size

        generators = []
        rids = []
        if getattr(obj, &#34;parallel_sample_num&#34;, 1) == 1:
            if self.server_args.enable_tokenizer_batch_encode:
                # Validate batch tokenization constraints
                self._validate_batch_tokenization_constraints(batch_size, obj)

                tokenized_objs = await self._batch_tokenize_and_process(batch_size, obj)

                # Send as a single batched request
                self._send_batch_request(obj, tokenized_objs, created_time)

                # Set up generators for each request in the batch
                for i in range(batch_size):
                    tmp_obj = obj[i]
                    generators.append(
                        self._wait_one_response(
                            tmp_obj, self.rid_to_state[tmp_obj.rid], request
                        )
                    )
                    rids.append(tmp_obj.rid)
            else:
                # Sequential tokenization and processing
                with (
                    input_blocker_guard_region(send_to_scheduler=self.send_to_scheduler)
                    if get_bool_env_var(&#34;SGLANG_ENABLE_COLOCATED_BATCH_GEN&#34;)
                    else nullcontext()
                ):
                    for i in range(batch_size):
                        tmp_obj = obj[i]
                        tokenized_obj = await self._tokenize_one_request(tmp_obj)
                        state = self._send_one_request(
                            tmp_obj, tokenized_obj, created_time
                        )
                        generators.append(
                            self._wait_one_response(tmp_obj, state, request)
                        )
                        rids.append(tmp_obj.rid)
        else:
            # FIXME: When using batch and parallel_sample_num together, the perf is not optimal.
            if batch_size &gt; 128:
                logger.warning(
                    &#34;Sending a single large batch with parallel sampling (n &gt; 1) has not been well optimized. &#34;
                    &#34;The performance might be better if you just duplicate the requests n times or use &#34;
                    &#34;many threads to send them one by one with parallel sampling (n &gt; 1).&#34;
                )

            # Tokenize all requests
            objs = [obj[i] for i in range(batch_size)]
            tokenized_objs = await asyncio.gather(
                *(self._tokenize_one_request(obj) for obj in objs)
            )

            # Cache the common prefix for parallel sampling
            for i in range(batch_size):
                tmp_obj = copy.copy(objs[i])
                tokenized_obj = copy.copy(tokenized_objs[i])
                tokenized_obj.rid = tmp_obj.regenerate_rid()
                tokenized_obj.sampling_params = copy.copy(tokenized_obj.sampling_params)
                tokenized_obj.sampling_params.max_new_tokens = 0
                tokenized_obj.stream = False
                state = self._send_one_request(tmp_obj, tokenized_obj, created_time)
                await self._wait_one_response(tmp_obj, state, request).__anext__()

            # Expand requests, assign new rids for them, and send them
            for i in range(batch_size):
                for _ in range(obj.parallel_sample_num):
                    tmp_obj = copy.copy(objs[i])
                    tokenized_obj = copy.copy(tokenized_objs[i])
                    tokenized_obj.rid = tmp_obj.regenerate_rid()
                    state = self._send_one_request(tmp_obj, tokenized_obj, created_time)
                    generators.append(self._wait_one_response(tmp_obj, state, request))
                    rids.append(tmp_obj.rid)

        # Wait for all requests
        is_stream = hasattr(obj, &#34;stream&#34;) and obj.stream
        if not is_stream:
            outputs = await asyncio.gather(*(gen.__anext__() for gen in generators))
            yield outputs
        else:
            rid_to_index = {rid: i for i, rid in enumerate(rids)}
            task_map = {asyncio.create_task(gen.__anext__()): gen for gen in generators}
            while task_map:
                done, _ = await asyncio.wait(
                    task_map.keys(), return_when=asyncio.FIRST_COMPLETED
                )

                for task in done:
                    gen = task_map.pop(task)
                    try:
                        result = task.result()
                        result[&#34;index&#34;] = rid_to_index[result[&#34;meta_info&#34;][&#34;id&#34;]]
                        yield result
                        new_task = asyncio.create_task(gen.__anext__())
                        task_map[new_task] = gen
                    except StopAsyncIteration:
                        pass

    async def flush_cache(self) -&gt; FlushCacheReqOutput:
        return (await self.flush_cache_communicator(FlushCacheReqInput()))[0]

    async def clear_hicache_storage(self) -&gt; ClearHiCacheReqOutput:
        &#34;&#34;&#34;Clear the hierarchical cache storage.&#34;&#34;&#34;
        # Delegate to the scheduler to handle HiCacheStorage clearing
        return (await self.clear_hicache_storage_communicator(ClearHiCacheReqInput()))[
            0
        ]

    def abort_request(self, rid: str = &#34;&#34;, abort_all: bool = False):
        if not abort_all and rid not in self.rid_to_state:
            return
        req = AbortReq(rid, abort_all)
        self.send_to_scheduler.send_pyobj(req)

        if self.enable_metrics:
            self.metrics_collector.observe_one_aborted_request()

    async def start_profile(
        self,
        output_dir: Optional[str] = None,
        start_step: Optional[int] = None,
        num_steps: Optional[int] = None,
        activities: Optional[List[str]] = None,
        with_stack: Optional[bool] = None,
        record_shapes: Optional[bool] = None,
        profile_by_stage: bool = False,
    ):
        self.auto_create_handle_loop()
        env_with_stack: bool = get_bool_env_var(&#34;SGLANG_PROFILE_WITH_STACK&#34;, &#34;true&#34;)
        with_stack = False if with_stack is False or env_with_stack is False else True
        req = ProfileReq(
            type=ProfileReqType.START_PROFILE,
            output_dir=output_dir,
            start_step=start_step,
            num_steps=num_steps,
            activities=activities,
            with_stack=with_stack,
            record_shapes=record_shapes,
            profile_by_stage=profile_by_stage,
            profile_id=str(time.time()),
        )
        return await self._execute_profile(req)

    async def stop_profile(self):
        self.auto_create_handle_loop()
        req = ProfileReq(type=ProfileReqType.STOP_PROFILE)
        return await self._execute_profile(req)

    async def _execute_profile(self, req: ProfileReq):
        result = (await self.profile_communicator(req))[0]
        if not result.success:
            raise RuntimeError(result.message)
        return result

    async def start_expert_distribution_record(self):
        self.auto_create_handle_loop()
        await self.expert_distribution_communicator(ExpertDistributionReq.START_RECORD)

    async def stop_expert_distribution_record(self):
        self.auto_create_handle_loop()
        await self.expert_distribution_communicator(ExpertDistributionReq.STOP_RECORD)

    async def dump_expert_distribution_record(self):
        self.auto_create_handle_loop()
        await self.expert_distribution_communicator(ExpertDistributionReq.DUMP_RECORD)

    async def pause_generation(self):
        async with self.is_pause_cond:
            self.is_pause = True
            self.abort_request(abort_all=True)

    async def continue_generation(self):
        async with self.is_pause_cond:
            self.is_pause = False
            self.is_pause_cond.notify_all()

    async def update_weights_from_disk(
        self,
        obj: UpdateWeightFromDiskReqInput,
        request: Optional[fastapi.Request] = None,
    ) -&gt; Tuple[bool, str]:
        self.auto_create_handle_loop()

        # default the load format to the server_args
        if obj.load_format is None:
            obj.load_format = self.server_args.load_format
        logger.info(&#34;Start update_weights. Load format=%s&#34;, obj.load_format)

        if obj.abort_all_requests:
            self.abort_request(abort_all=True)

        if True:  # Keep this redundant check to simplify some internal code sync
            # Hold the lock if it is not async. This means that weight sync
            # cannot run while requests are in progress.
            async with self.model_update_lock.writer_lock:
                return await self._wait_for_model_update_from_disk(obj)

    async def _wait_for_model_update_from_disk(
        self, obj: UpdateWeightFromDiskReqInput
    ) -&gt; Tuple[bool, str]:
        self.send_to_scheduler.send_pyobj(obj)
        self.model_update_result = asyncio.Future()
        if self.server_args.dp_size == 1:
            result = await self.model_update_result
            if result.success:
                self.served_model_name = obj.model_path
                self.server_args.model_path = obj.model_path
                self.server_args.load_format = obj.load_format
                self.model_path = obj.model_path
            return result.success, result.message, result.num_paused_requests
        else:  # self.server_args.dp_size &gt; 1
            self.model_update_tmp = []
            result = await self.model_update_result

            all_success = all([r.success for r in result])
            if all_success is True:
                self.server_args.model_path = obj.model_path
                self.server_args.load_format = obj.load_format
                self.model_path = obj.model_path
            all_message = [r.message for r in result]
            all_message = &#34; | &#34;.join(all_message)
            all_paused_requests = [r.num_paused_requests for r in result]
            return all_success, all_message, all_paused_requests

    async def init_weights_update_group(
        self,
        obj: InitWeightsUpdateGroupReqInput,
        request: Optional[fastapi.Request] = None,
    ) -&gt; Tuple[bool, str]:
        self.auto_create_handle_loop()
        assert (
            self.server_args.dp_size == 1
        ), &#34;dp_size must be 1 for init parameter update group&#34;
        result = (await self.init_weights_update_group_communicator(obj))[0]
        return result.success, result.message

    async def update_weights_from_distributed(
        self,
        obj: UpdateWeightsFromDistributedReqInput,
        request: Optional[fastapi.Request] = None,
    ) -&gt; Tuple[bool, str]:
        self.auto_create_handle_loop()
        assert (
            self.server_args.dp_size == 1 or self.server_args.enable_dp_attention
        ), &#34;dp_size must be 1 or dp attention must be enabled for update weights from distributed&#34;

        if obj.abort_all_requests:
            self.abort_request(abort_all=True)

        # This means that weight sync
        # cannot run while requests are in progress.
        async with self.model_update_lock.writer_lock:
            result = (await self.update_weights_from_distributed_communicator(obj))[0]
            return result.success, result.message

    async def update_weights_from_tensor(
        self,
        obj: UpdateWeightsFromTensorReqInput,
        request: Optional[fastapi.Request] = None,
    ) -&gt; Tuple[bool, str]:
        self.auto_create_handle_loop()
        assert (
            self.server_args.dp_size == 1 or self.server_args.enable_dp_attention
        ), &#34;dp_size must be 1 or dp attention must be enabled for update weights from tensor&#34;

        if obj.abort_all_requests:
            self.abort_request(abort_all=True)

        # This means that weight sync
        # cannot run while requests are in progress.
        async with self.model_update_lock.writer_lock:
            result = (await self.update_weights_from_tensor_communicator(obj))[0]
            return result.success, result.message

    async def load_lora_adapter(
        self,
        obj: LoadLoRAAdapterReqInput,
        _: Optional[fastapi.Request] = None,
    ) -&gt; LoadLoRAAdapterReqOutput:
        self.auto_create_handle_loop()

        try:
            if not self.server_args.enable_lora:
                raise ValueError(
                    &#34;LoRA is not enabled. Please set `--enable-lora` to enable LoRA.&#34;
                )

            # TODO (lifuhuang): Remove this after we verify that dynamic lora loading works
            # with dp_size &gt; 1.
            assert (
                self.server_args.dp_size == 1
            ), &#34;dp_size must be 1 for dynamic lora loading&#34;
            logger.info(
                &#34;Start load Lora adapter. Lora name=%s, path=%s&#34;,
                obj.lora_name,
                obj.lora_path,
            )

            async with self.lora_update_lock:
                if (
                    self.server_args.max_loaded_loras is not None
                    and self.lora_registry.num_registered_loras
                    &gt;= self.server_args.max_loaded_loras
                ):
                    raise ValueError(
                        f&#34;Cannot load LoRA adapter {obj.lora_name} at path {obj.lora_path}. &#34;
                        f&#34;Maximum number of loaded LoRA adapters is {self.server_args.max_loaded_loras}. &#34;
                        &#34;Please unload some LoRA adapters before loading new ones.&#34;
                    )

                # Generate new uniquely identifiable LoRARef object.
                new_adapter = LoRARef(
                    lora_name=obj.lora_name,
                    lora_path=obj.lora_path,
                    pinned=obj.pinned,
                )

                # Trigger the actual loading operation at the backend processes.
                obj.lora_id = new_adapter.lora_id
                result = (await self.update_lora_adapter_communicator(obj))[0]

                # Register the LoRA adapter only after loading is successful.
                if result.success:
                    await self.lora_registry.register(new_adapter)

                return result
        except ValueError as e:
            return LoadLoRAAdapterReqOutput(
                success=False,
                error_message=str(e),
            )

    async def unload_lora_adapter(
        self,
        obj: UnloadLoRAAdapterReqInput,
        _: Optional[fastapi.Request] = None,
    ) -&gt; UnloadLoRAAdapterReqOutput:
        self.auto_create_handle_loop()

        try:
            if not self.server_args.enable_lora:
                raise ValueError(
                    &#34;LoRA is not enabled. Please set `--enable-lora` to enable LoRA.&#34;
                )

            assert (
                obj.lora_name is not None
            ), &#34;lora_name must be provided to unload LoRA adapter&#34;

            # TODO (lifuhuang): Remove this after we verify that dynamic lora loading works
            # with dp_size &gt; 1.
            assert (
                self.server_args.dp_size == 1
            ), &#34;dp_size must be 1 for dynamic lora loading&#34;
            logger.info(
                &#34;Start unload Lora adapter. Lora name=%s&#34;,
                obj.lora_name,
            )

            async with self.lora_update_lock:
                # Unregister the LoRA adapter from the registry to stop new requests for this adapter
                # from being started.
                lora_id = await self.lora_registry.unregister(obj.lora_name)
                obj.lora_id = lora_id

                # Initiate the actual unloading operation at the backend processes only after all
                # ongoing requests using this LoRA adapter are finished.
                await self.lora_registry.wait_for_unload(lora_id)
                result = (await self.update_lora_adapter_communicator(obj))[0]

                return result
        except ValueError as e:
            return UnloadLoRAAdapterReqOutput(success=False, error_message=str(e))

    async def get_weights_by_name(
        self, obj: GetWeightsByNameReqInput, request: Optional[fastapi.Request] = None
    ):
        self.auto_create_handle_loop()
        results = await self.get_weights_by_name_communicator(obj)
        all_parameters = [r.parameter for r in results]
        if self.server_args.dp_size == 1:
            return all_parameters[0]
        else:
            return all_parameters

    async def release_memory_occupation(
        self,
        obj: ReleaseMemoryOccupationReqInput,
        request: Optional[fastapi.Request] = None,
    ):
        self.auto_create_handle_loop()
        await self.release_memory_occupation_communicator(obj)

    async def resume_memory_occupation(
        self,
        obj: ResumeMemoryOccupationReqInput,
        request: Optional[fastapi.Request] = None,
    ):
        self.auto_create_handle_loop()
        await self.resume_memory_occupation_communicator(obj)

    async def slow_down(
        self,
        obj: SlowDownReqInput,
        request: Optional[fastapi.Request] = None,
    ):
        self.auto_create_handle_loop()
        await self.slow_down_communicator(obj)

    async def open_session(
        self, obj: OpenSessionReqInput, request: Optional[fastapi.Request] = None
    ):
        self.auto_create_handle_loop()

        if obj.session_id is None:
            obj.session_id = uuid.uuid4().hex
        elif obj.session_id in self.session_futures:
            return None

        self.send_to_scheduler.send_pyobj(obj)

        self.session_futures[obj.session_id] = asyncio.Future()
        session_id = await self.session_futures[obj.session_id]
        del self.session_futures[obj.session_id]
        return session_id

    async def close_session(
        self, obj: CloseSessionReqInput, request: Optional[fastapi.Request] = None
    ):
        await self.send_to_scheduler.send_pyobj(obj)

    async def get_internal_state(self) -&gt; List[Dict[Any, Any]]:
        req = GetInternalStateReq()
        responses: List[GetInternalStateReqOutput] = (
            await self.get_internal_state_communicator(req)
        )
        # Many DP ranks
        return [res.internal_state for res in responses]

    async def set_internal_state(self, obj: SetInternalStateReq) -&gt; List[bool]:
        responses: List[SetInternalStateReqOutput] = (
            await self.set_internal_state_communicator(obj)
        )
        return [res.updated for res in responses]

    async def get_load(self) -&gt; dict:
        # TODO(lsyin): fake load report server
        if not self.current_load_lock.locked():
            async with self.current_load_lock:
                internal_state = await self.get_internal_state()
                self.current_load = internal_state[0][&#34;load&#34;]
        return {&#34;load&#34;: self.current_load}

    def get_log_request_metadata(self):
        max_length = None
        skip_names = None
        out_skip_names = None
        if self.log_requests:
            if self.log_requests_level == 0:
                max_length = 1 &lt;&lt; 30
                skip_names = set(
                    [
                        &#34;text&#34;,
                        &#34;input_ids&#34;,
                        &#34;input_embeds&#34;,
                        &#34;image_data&#34;,
                        &#34;audio_data&#34;,
                        &#34;lora_path&#34;,
                        &#34;sampling_params&#34;,
                    ]
                )
                out_skip_names = set(
                    [
                        &#34;text&#34;,
                        &#34;output_ids&#34;,
                        &#34;embedding&#34;,
                    ]
                )
            elif self.log_requests_level == 1:
                max_length = 1 &lt;&lt; 30
                skip_names = set(
                    [
                        &#34;text&#34;,
                        &#34;input_ids&#34;,
                        &#34;input_embeds&#34;,
                        &#34;image_data&#34;,
                        &#34;audio_data&#34;,
                        &#34;lora_path&#34;,
                    ]
                )
                out_skip_names = set(
                    [
                        &#34;text&#34;,
                        &#34;output_ids&#34;,
                        &#34;embedding&#34;,
                    ]
                )
            elif self.log_requests_level == 2:
                max_length = 2048
            elif self.log_requests_level == 3:
                max_length = 1 &lt;&lt; 30
            else:
                raise ValueError(
                    f&#34;Invalid --log-requests-level: {self.log_requests_level=}&#34;
                )
        return max_length, skip_names, out_skip_names

    def configure_logging(self, obj: ConfigureLoggingReq):
        if obj.log_requests is not None:
            self.log_requests = obj.log_requests
        if obj.log_requests_level is not None:
            self.log_requests_level = obj.log_requests_level
        if obj.dump_requests_folder is not None:
            self.dump_requests_folder = obj.dump_requests_folder
        if obj.dump_requests_threshold is not None:
            self.dump_requests_threshold = obj.dump_requests_threshold
        if obj.crash_dump_folder is not None:
            self.crash_dump_folder = obj.crash_dump_folder
        logging.info(f&#34;Config logging: {obj=}&#34;)
        self.log_request_metadata = self.get_log_request_metadata()

    async def freeze_gc(self):
        &#34;&#34;&#34;Send a freeze_gc message to the scheduler first, then freeze locally.&#34;&#34;&#34;
        self.send_to_scheduler.send_pyobj(FreezeGCReq())
        freeze_gc(&#34;Tokenizer Manager&#34;)
        return None

    def create_abort_task(self, obj: GenerateReqInput):
        # Abort the request if the client is disconnected.
        async def abort_request():
            await asyncio.sleep(2)
            if obj.is_single:
                self.abort_request(obj.rid)
            else:
                for rid in obj.rid:
                    self.abort_request(rid)

        background_tasks = BackgroundTasks()
        background_tasks.add_task(abort_request)
        return background_tasks

    def auto_create_handle_loop(self):
        if self.no_create_loop:
            return

        self.no_create_loop = True
        loop = asyncio.get_event_loop()
        self.asyncio_tasks.add(
            loop.create_task(print_exception_wrapper(self.handle_loop))
        )

        self.event_loop = loop

        # We cannot add signal handler when the tokenizer manager is not in
        # the main thread due to the CPython limitation.
        if threading.current_thread() is threading.main_thread():
            signal_handler = SignalHandler(self)
            loop.add_signal_handler(signal.SIGTERM, signal_handler.sigterm_handler)
            # Update the signal handler for the process. It overrides the sigquit handler in the launch phase.
            loop.add_signal_handler(
                signal.SIGQUIT, signal_handler.running_phase_sigquit_handler
            )
        else:
            logger.warning(
                &#34;Signal handler is not added because the tokenizer manager is &#34;
                &#34;not in the main thread. This disables graceful shutdown of the &#34;
                &#34;tokenizer manager when SIGTERM is received.&#34;
            )
        self.asyncio_tasks.add(
            loop.create_task(print_exception_wrapper(self.sigterm_watchdog))
        )

    def dump_requests_before_crash(self):
        if self.crash_dump_performed:
            logger.info(
                &#34;SIGTERM/SIGQUIT/Exception triggered, but crash dump already performed, skipping.&#34;
            )
            return

        if not self.crash_dump_folder:
            return

        logger.error(f&#34;Dumping requests before crash. {self.crash_dump_folder=}&#34;)
        self.crash_dump_performed = True

        # Check if NFS directory is available
        # expected_nfs_dir = &#34;/&#34; + self.crash_dump_folder.lstrip(&#34;/&#34;).split(&#34;/&#34;)[0]
        # use_nfs_dir = os.path.isdir(expected_nfs_dir) and os.access(
        #     expected_nfs_dir, os.W_OK
        # )
        use_nfs_dir = False
        if not use_nfs_dir:
            logger.error(
                f&#34;Expected NFS directory is not available or writable. Uploading to GCS.&#34;
            )

        data_to_dump = []
        if self.crash_dump_request_list:
            data_to_dump.extend(self.crash_dump_request_list)

        # Add unfinished requests from rid_to_state
        unfinished_requests = []
        for rid, state in self.rid_to_state.items():
            if not state.finished:
                unfinished_requests.append(
                    (
                        state.obj,
                        state.out_list[-1] if state.out_list else {},
                        state.created_time,
                        time.time(),
                    )
                )
        if unfinished_requests:
            data_to_dump.extend(unfinished_requests)

        if not data_to_dump:
            return

        object_name = f&#39;crash_dump_{datetime.now().strftime(&#34;%Y-%m-%d_%H-%M-%S&#34;)}.pkl&#39;
        filename = os.path.join(
            self.crash_dump_folder,
            os.getenv(&#34;HOSTNAME&#34;, None),
            object_name,
        )

        os.makedirs(os.path.dirname(filename), exist_ok=True)
        # Include server_args in the dump
        data_to_dump_with_server_args = {
            &#34;server_args&#34;: self.server_args,
            &#34;requests&#34;: data_to_dump,
        }
        with open(filename, &#34;wb&#34;) as f:
            pickle.dump(data_to_dump_with_server_args, f)
        logger.error(
            f&#34;Dumped {len(self.crash_dump_request_list)} finished and {len(unfinished_requests)} unfinished requests before crash to {filename}&#34;
        )

        def _upload_file_to_gcs(bucket_name, source_file_path, object_name):
            from google.cloud import storage

            client = storage.Client()
            bucket = client.bucket(bucket_name)
            blob = bucket.blob(object_name)
            blob.upload_from_filename(source_file_path, if_generation_match=0)
            logger.error(
                f&#34;Successfully uploaded {source_file_path} to gs://{bucket_name}/{object_name}&#34;
            )

        if not use_nfs_dir:
            _upload_file_to_gcs(
                &#34;sglang_crash_dump&#34;,
                filename,
                os.getenv(&#34;HOSTNAME&#34;, None) + &#34;/&#34; + object_name,
            )

    async def sigterm_watchdog(self):
        while not self.gracefully_exit:
            await asyncio.sleep(5)

        # Drain requests
        while True:
            remain_num_req = len(self.rid_to_state)

            if self.server_status == ServerStatus.UnHealthy:
                # if health check failed, we should exit immediately
                logger.error(
                    &#34;Signal SIGTERM received while health check failed. Exiting... remaining number of requests: %d&#34;,
                    remain_num_req,
                )
                self.dump_requests_before_crash()
                break

            elif get_bool_env_var(&#34;SGL_FORCE_SHUTDOWN&#34;):
                # if force shutdown flag set, exit immediately
                logger.error(
                    &#34;Signal SIGTERM received while force shutdown flag set. Force exiting... remaining number of requests: %d&#34;,
                    remain_num_req,
                )
                break

            logger.info(
                f&#34;Gracefully exiting... remaining number of requests {remain_num_req}&#34;
            )
            if remain_num_req &gt; 0:
                await asyncio.sleep(5)
            else:
                self.dump_requests_before_crash()
                break

        kill_process_tree(os.getpid(), include_parent=True)
        sys.exit(0)

    async def handle_loop(self):
        &#34;&#34;&#34;The event loop that handles requests&#34;&#34;&#34;

        while True:
            recv_obj = await self.recv_from_detokenizer.recv_pyobj()
            self._result_dispatcher(recv_obj)
            self.last_receive_tstamp = time.time()

    def _handle_batch_output(
        self,
        recv_obj: Union[
            BatchStrOut, BatchEmbeddingOut, BatchMultimodalOut, BatchTokenIDOut
        ],
    ):
        for i, rid in enumerate(recv_obj.rids):
            state = self.rid_to_state.get(rid, None)
            if state is None:
                logger.error(
                    f&#34;Received output for {rid=} but the state was deleted in TokenizerManager.&#34;
                )
                continue

            # Build meta_info and return value
            meta_info = {
                &#34;id&#34;: rid,
                &#34;finish_reason&#34;: recv_obj.finished_reasons[i],
                &#34;prompt_tokens&#34;: recv_obj.prompt_tokens[i],
                &#34;weight_version&#34;: self.server_args.weight_version,
            }

            if getattr(state.obj, &#34;return_logprob&#34;, False):
                self.convert_logprob_style(
                    meta_info,
                    state,
                    state.obj.top_logprobs_num,
                    state.obj.token_ids_logprob,
                    state.obj.return_text_in_logprobs
                    and not self.server_args.skip_tokenizer_init,
                    recv_obj,
                    i,
                )

            if not isinstance(recv_obj, BatchEmbeddingOut):
                meta_info.update(
                    {
                        &#34;completion_tokens&#34;: recv_obj.completion_tokens[i],
                        &#34;cached_tokens&#34;: recv_obj.cached_tokens[i],
                    }
                )

            if getattr(recv_obj, &#34;output_hidden_states&#34;, None):
                meta_info[&#34;hidden_states&#34;] = recv_obj.output_hidden_states[i]

            if isinstance(recv_obj, BatchStrOut):
                state.text += recv_obj.output_strs[i]
                if state.obj.stream:
                    state.output_ids.extend(recv_obj.output_ids[i])
                    output_token_ids = state.output_ids[state.last_output_offset :]
                    state.last_output_offset = len(state.output_ids)
                else:
                    state.output_ids.extend(recv_obj.output_ids[i])
                    output_token_ids = state.output_ids.copy()

                out_dict = {
                    &#34;text&#34;: state.text,
                    &#34;output_ids&#34;: output_token_ids,
                    &#34;meta_info&#34;: meta_info,
                }
            elif isinstance(recv_obj, BatchTokenIDOut):
                if self.server_args.stream_output and state.obj.stream:
                    state.output_ids.extend(recv_obj.output_ids[i])
                    output_token_ids = state.output_ids[state.last_output_offset :]
                    state.last_output_offset = len(state.output_ids)
                else:
                    state.output_ids.extend(recv_obj.output_ids[i])
                    output_token_ids = state.output_ids.copy()

                out_dict = {
                    &#34;output_ids&#34;: output_token_ids,
                    &#34;meta_info&#34;: meta_info,
                }
            elif isinstance(recv_obj, BatchMultimodalOut):
                raise NotImplementedError(&#34;BatchMultimodalOut not implemented&#34;)
            else:
                assert isinstance(recv_obj, BatchEmbeddingOut)
                out_dict = {
                    &#34;embedding&#34;: recv_obj.embeddings[i],
                    &#34;meta_info&#34;: meta_info,
                }

            state.finished = recv_obj.finished_reasons[i] is not None
            if state.finished:
                if self.server_args.speculative_algorithm:
                    meta_info[&#34;spec_verify_ct&#34;] = recv_obj.spec_verify_ct[i]
                state.finished_time = time.time()
                meta_info[&#34;e2e_latency&#34;] = state.finished_time - state.created_time
                del self.rid_to_state[rid]

                # Mark ongoing LoRA request as finished.
                if self.server_args.enable_lora and state.obj.lora_path:
                    asyncio.create_task(self.lora_registry.release(state.obj.lora_id))

            state.out_list.append(out_dict)
            state.event.set()

            # Log metrics and dump
            if self.enable_metrics and state.obj.log_metrics:
                self.collect_metrics(state, recv_obj, i)
            if self.dump_requests_folder and state.finished and state.obj.log_metrics:
                self.dump_requests(state, out_dict)
            if self.crash_dump_folder and state.finished and state.obj.log_metrics:
                self.record_request_for_crash_dump(state, out_dict)

    def convert_logprob_style(
        self,
        meta_info: dict,
        state: ReqState,
        top_logprobs_num: int,
        token_ids_logprob: List[int],
        return_text_in_logprobs: bool,
        recv_obj: BatchStrOut,
        recv_obj_index: int,
    ):
        if recv_obj.input_token_logprobs_val is None:
            return

        if len(recv_obj.input_token_logprobs_val) &gt; 0:
            state.input_token_logprobs_val.extend(
                recv_obj.input_token_logprobs_val[recv_obj_index]
            )
            state.input_token_logprobs_idx.extend(
                recv_obj.input_token_logprobs_idx[recv_obj_index]
            )
        state.output_token_logprobs_val.extend(
            recv_obj.output_token_logprobs_val[recv_obj_index]
        )
        state.output_token_logprobs_idx.extend(
            recv_obj.output_token_logprobs_idx[recv_obj_index]
        )
        meta_info[&#34;input_token_logprobs&#34;] = self.detokenize_logprob_tokens(
            state.input_token_logprobs_val,
            state.input_token_logprobs_idx,
            return_text_in_logprobs,
        )
        meta_info[&#34;output_token_logprobs&#34;] = self.detokenize_logprob_tokens(
            state.output_token_logprobs_val,
            state.output_token_logprobs_idx,
            return_text_in_logprobs,
        )

        if top_logprobs_num &gt; 0:
            if len(recv_obj.input_top_logprobs_val) &gt; 0:
                state.input_top_logprobs_val.extend(
                    recv_obj.input_top_logprobs_val[recv_obj_index]
                )
                state.input_top_logprobs_idx.extend(
                    recv_obj.input_top_logprobs_idx[recv_obj_index]
                )
            state.output_top_logprobs_val.extend(
                recv_obj.output_top_logprobs_val[recv_obj_index]
            )
            state.output_top_logprobs_idx.extend(
                recv_obj.output_top_logprobs_idx[recv_obj_index]
            )
            meta_info[&#34;input_top_logprobs&#34;] = self.detokenize_top_logprobs_tokens(
                state.input_top_logprobs_val,
                state.input_top_logprobs_idx,
                return_text_in_logprobs,
            )
            meta_info[&#34;output_top_logprobs&#34;] = self.detokenize_top_logprobs_tokens(
                state.output_top_logprobs_val,
                state.output_top_logprobs_idx,
                return_text_in_logprobs,
            )

        if token_ids_logprob is not None:
            if len(recv_obj.input_token_ids_logprobs_val) &gt; 0:
                state.input_token_ids_logprobs_val.extend(
                    recv_obj.input_token_ids_logprobs_val[recv_obj_index]
                )
                state.input_token_ids_logprobs_idx.extend(
                    recv_obj.input_token_ids_logprobs_idx[recv_obj_index]
                )
            state.output_token_ids_logprobs_val.extend(
                recv_obj.output_token_ids_logprobs_val[recv_obj_index]
            )
            state.output_token_ids_logprobs_idx.extend(
                recv_obj.output_token_ids_logprobs_idx[recv_obj_index]
            )
            meta_info[&#34;input_token_ids_logprobs&#34;] = self.detokenize_top_logprobs_tokens(
                state.input_token_ids_logprobs_val,
                state.input_token_ids_logprobs_idx,
                return_text_in_logprobs,
            )
            meta_info[&#34;output_token_ids_logprobs&#34;] = (
                self.detokenize_top_logprobs_tokens(
                    state.output_token_ids_logprobs_val,
                    state.output_token_ids_logprobs_idx,
                    return_text_in_logprobs,
                )
            )

    def detokenize_logprob_tokens(
        self,
        token_logprobs_val: List[float],
        token_logprobs_idx: List[int],
        decode_to_text: bool,
    ):
        if not decode_to_text:
            return [
                (logprob, token_id, None)
                for logprob, token_id in zip(token_logprobs_val, token_logprobs_idx)
            ]
        else:
            assert self.tokenizer is not None
            token_texts = self.tokenizer.batch_decode(token_logprobs_idx)
            return list(zip(token_logprobs_val, token_logprobs_idx, token_texts))

    def detokenize_top_logprobs_tokens(
        self,
        token_logprobs_val: List[float],
        token_logprobs_idx: List[int],
        decode_to_text: bool,
    ):
        # TODO: The current implementation only batches the detokenization for top-k tokens per single position.
        # We should batch all top-k tokens in all positions.
        ret = []
        for i in range(len(token_logprobs_val)):
            if token_logprobs_val[i]:
                ret.append(
                    self.detokenize_logprob_tokens(
                        token_logprobs_val[i], token_logprobs_idx[i], decode_to_text
                    )
                )
            else:
                ret.append(None)
        return ret

    def collect_metrics(self, state: ReqState, recv_obj: BatchStrOut, i: int):
        completion_tokens = (
            recv_obj.completion_tokens[i]
            if getattr(recv_obj, &#34;completion_tokens&#34;, None)
            else 0
        )

        if (
            state.first_token_time == 0.0
            and self.disaggregation_mode != DisaggregationMode.PREFILL
        ):
            state.first_token_time = state.last_time = time.time()
            state.last_completion_tokens = completion_tokens
            self.metrics_collector.observe_time_to_first_token(
                state.first_token_time - state.created_time
            )
        else:
            num_new_tokens = completion_tokens - state.last_completion_tokens
            if num_new_tokens:
                new_time = time.time()
                interval = new_time - state.last_time
                self.metrics_collector.observe_inter_token_latency(
                    interval,
                    num_new_tokens,
                )
                state.last_time = new_time
                state.last_completion_tokens = completion_tokens

        if state.finished:
            has_grammar = (
                state.obj.sampling_params.get(&#34;json_schema&#34;, None)
                or state.obj.sampling_params.get(&#34;regex&#34;, None)
                or state.obj.sampling_params.get(&#34;ebnf&#34;, None)
                or state.obj.sampling_params.get(&#34;structural_tag&#34;, None)
            )
            self.metrics_collector.observe_one_finished_request(
                recv_obj.prompt_tokens[i],
                completion_tokens,
                recv_obj.cached_tokens[i],
                state.finished_time - state.created_time,
                has_grammar,
            )

    def dump_requests(self, state: ReqState, out_dict: dict):
        self.dump_request_list.append(
            (state.obj, out_dict, state.created_time, time.time())
        )

        if len(self.dump_request_list) &gt;= self.dump_requests_threshold:
            filename = os.path.join(
                self.dump_requests_folder,
                datetime.now().strftime(&#34;%Y-%m-%d_%H-%M-%S&#34;) + &#34;.pkl&#34;,
            )
            self._dump_data_to_file(
                data_list=self.dump_request_list,
                filename=filename,
                log_message=f&#34;Dump {len(self.dump_request_list)} requests to {filename}&#34;,
            )
            self.dump_request_list = []

    def record_request_for_crash_dump(self, state: ReqState, out_dict: dict):
        current_time = time.time()
        self.crash_dump_request_list.append(
            (state.obj, out_dict, state.created_time, current_time)
        )
        # Remove requests older than 5 minutes based on finish time
        while (
            self.crash_dump_request_list
            and current_time - self.crash_dump_request_list[0][3] &gt;= 300
        ):
            self.crash_dump_request_list.popleft()

    def _dump_data_to_file(
        self, data_list: List[Tuple], filename: str, log_message: str
    ):
        logger.info(log_message)
        to_dump_with_server_args = {
            &#34;server_args&#34;: self.server_args,
            &#34;requests&#34;: data_list.copy(),
        }

        def background_task():
            os.makedirs(os.path.dirname(filename), exist_ok=True)
            with open(filename, &#34;wb&#34;) as f:
                pickle.dump(to_dump_with_server_args, f)

        asyncio.create_task(asyncio.to_thread(background_task))

    def _handle_abort_req(self, recv_obj):
        if is_health_check_generate_req(recv_obj):
            return
        state = self.rid_to_state[recv_obj.rid]
        state.finished = True
        if recv_obj.finished_reason:
            out = {
                &#34;meta_info&#34;: {
                    &#34;id&#34;: recv_obj.rid,
                    &#34;finish_reason&#34;: recv_obj.finished_reason,
                },
            }
        else:
            out = {
                &#34;text&#34;: &#34;&#34;,
                &#34;meta_info&#34;: {
                    &#34;id&#34;: recv_obj.rid,
                    &#34;finish_reason&#34;: {
                        &#34;type&#34;: &#34;abort&#34;,
                        &#34;message&#34;: &#34;Abort before prefill&#34;,
                    },
                    &#34;prompt_tokens&#34;: 0,
                    &#34;completion_tokens&#34;: 0,
                },
            }
        state.out_list.append(out)
        state.event.set()

    def _handle_open_session_req_output(self, recv_obj):
        self.session_futures[recv_obj.session_id].set_result(
            recv_obj.session_id if recv_obj.success else None
        )

    def _handle_update_weights_from_disk_req_output(self, recv_obj):
        if self.server_args.dp_size == 1:
            self.model_update_result.set_result(recv_obj)
        else:  # self.server_args.dp_size &gt; 1
            self.model_update_tmp.append(recv_obj)
            # set future if the all results are received
            if len(self.model_update_tmp) == self.server_args.dp_size:
                self.model_update_result.set_result(self.model_update_tmp)

    async def score_request(
        self,
        query: Optional[Union[str, List[int]]] = None,
        items: Optional[Union[str, List[str], List[List[int]]]] = None,
        label_token_ids: Optional[List[int]] = None,
        apply_softmax: bool = False,
        item_first: bool = False,
        request: Optional[Any] = None,
    ) -&gt; List[List[float]]:
        &#34;&#34;&#34;
        See Engine.score() for more details.
        &#34;&#34;&#34;
        if label_token_ids is None:
            raise ValueError(&#34;label_token_ids must be provided&#34;)

        if self.tokenizer is not None:
            vocab_size = self.tokenizer.vocab_size
            for token_id in label_token_ids:
                if token_id &gt;= vocab_size:
                    raise ValueError(
                        f&#34;Token ID {token_id} is out of vocabulary (vocab size: {vocab_size})&#34;
                    )

        batch_request = GenerateReqInput(
            token_ids_logprob=label_token_ids,
            return_logprob=True,
            stream=False,
            sampling_params={&#34;max_new_tokens&#34;: 0},
        )

        # Handle string or tokenized query/items
        if isinstance(query, str) and (
            isinstance(items, str)
            or (isinstance(items, list) and (not items or isinstance(items[0], str)))
        ):
            # Both query and items are text
            items_list = [items] if isinstance(items, str) else items
            if item_first:
                prompts = [f&#34;{item}{query}&#34; for item in items_list]
            else:
                prompts = [f&#34;{query}{item}&#34; for item in items_list]

            batch_request.text = prompts

        elif (
            isinstance(query, list)
            and isinstance(items, list)
            and items
            and isinstance(items[0], list)
        ):
            # Both query and items are token IDs
            if item_first:
                input_ids_list = [item + query for item in items]
            else:
                input_ids_list = [query + item for item in items]

            batch_request.input_ids = input_ids_list
        else:
            raise ValueError(
                &#34;Invalid combination of query/items types for score_request.&#34;
            )

        results = await self.generate_request(batch_request, request).__anext__()
        scores = []

        for result in results:
            # Get logprobs for each token
            logprobs = {}

            # For scoring requests, we read from output_token_ids_logprobs since we want
            # the logprobs for specific tokens mentioned in the label_token_ids at
            # the next position after the last token in the prompt
            output_logprobs = result[&#34;meta_info&#34;].get(&#34;output_token_ids_logprobs&#34;, [])

            # Throw an error here if output_logprobs is None
            if output_logprobs is None:
                raise RuntimeError(
                    f&#34;output_logprobs is None for request {result[&#39;meta_info&#39;].get(&#39;id&#39;, &#39;&lt;unknown&gt;&#39;)}. &#34;
                    &#34;This usually indicates a problem with the scoring request or the backend output.&#34;
                )

            for logprob, token_id, _ in output_logprobs[0]:
                if token_id in label_token_ids:
                    logprobs[token_id] = logprob

            # Get scores in order of label_token_ids
            score_list = [
                logprobs.get(token_id, float(&#34;-inf&#34;)) for token_id in label_token_ids
            ]

            # Apply softmax to logprobs if needed
            if apply_softmax:
                score_list = torch.softmax(torch.tensor(score_list), dim=0).tolist()
            else:
                # Convert logprobs to probabilities if not using softmax
                score_list = [
                    math.exp(x) if x != float(&#34;-inf&#34;) else 0.0 for x in score_list
                ]

            scores.append(score_list)

        return scores</code></pre>
</details>
<div class="desc"><p>TokenizerManager is a process that tokenizes the text.</p></div>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.abort_request"><code class="name flex">
<span>def <span class="ident">abort_request</span></span>(<span>self, rid: str = '', abort_all: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def abort_request(self, rid: str = &#34;&#34;, abort_all: bool = False):
    if not abort_all and rid not in self.rid_to_state:
        return
    req = AbortReq(rid, abort_all)
    self.send_to_scheduler.send_pyobj(req)

    if self.enable_metrics:
        self.metrics_collector.observe_one_aborted_request()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.auto_create_handle_loop"><code class="name flex">
<span>def <span class="ident">auto_create_handle_loop</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def auto_create_handle_loop(self):
    if self.no_create_loop:
        return

    self.no_create_loop = True
    loop = asyncio.get_event_loop()
    self.asyncio_tasks.add(
        loop.create_task(print_exception_wrapper(self.handle_loop))
    )

    self.event_loop = loop

    # We cannot add signal handler when the tokenizer manager is not in
    # the main thread due to the CPython limitation.
    if threading.current_thread() is threading.main_thread():
        signal_handler = SignalHandler(self)
        loop.add_signal_handler(signal.SIGTERM, signal_handler.sigterm_handler)
        # Update the signal handler for the process. It overrides the sigquit handler in the launch phase.
        loop.add_signal_handler(
            signal.SIGQUIT, signal_handler.running_phase_sigquit_handler
        )
    else:
        logger.warning(
            &#34;Signal handler is not added because the tokenizer manager is &#34;
            &#34;not in the main thread. This disables graceful shutdown of the &#34;
            &#34;tokenizer manager when SIGTERM is received.&#34;
        )
    self.asyncio_tasks.add(
        loop.create_task(print_exception_wrapper(self.sigterm_watchdog))
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.clear_hicache_storage"><code class="name flex">
<span>async def <span class="ident">clear_hicache_storage</span></span>(<span>self) ‑> <a title="sglang.srt.managers.io_struct.ClearHiCacheReqOutput" href="io_struct.html#sglang.srt.managers.io_struct.ClearHiCacheReqOutput">ClearHiCacheReqOutput</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def clear_hicache_storage(self) -&gt; ClearHiCacheReqOutput:
    &#34;&#34;&#34;Clear the hierarchical cache storage.&#34;&#34;&#34;
    # Delegate to the scheduler to handle HiCacheStorage clearing
    return (await self.clear_hicache_storage_communicator(ClearHiCacheReqInput()))[
        0
    ]</code></pre>
</details>
<div class="desc"><p>Clear the hierarchical cache storage.</p></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.close_session"><code class="name flex">
<span>async def <span class="ident">close_session</span></span>(<span>self,<br>obj: <a title="sglang.srt.managers.io_struct.CloseSessionReqInput" href="io_struct.html#sglang.srt.managers.io_struct.CloseSessionReqInput">CloseSessionReqInput</a>,<br>request: starlette.requests.Request | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def close_session(
    self, obj: CloseSessionReqInput, request: Optional[fastapi.Request] = None
):
    await self.send_to_scheduler.send_pyobj(obj)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.collect_metrics"><code class="name flex">
<span>def <span class="ident">collect_metrics</span></span>(<span>self,<br>state: <a title="sglang.srt.managers.tokenizer_manager.ReqState" href="#sglang.srt.managers.tokenizer_manager.ReqState">ReqState</a>,<br>recv_obj: <a title="sglang.srt.managers.io_struct.BatchStrOut" href="io_struct.html#sglang.srt.managers.io_struct.BatchStrOut">BatchStrOut</a>,<br>i: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def collect_metrics(self, state: ReqState, recv_obj: BatchStrOut, i: int):
    completion_tokens = (
        recv_obj.completion_tokens[i]
        if getattr(recv_obj, &#34;completion_tokens&#34;, None)
        else 0
    )

    if (
        state.first_token_time == 0.0
        and self.disaggregation_mode != DisaggregationMode.PREFILL
    ):
        state.first_token_time = state.last_time = time.time()
        state.last_completion_tokens = completion_tokens
        self.metrics_collector.observe_time_to_first_token(
            state.first_token_time - state.created_time
        )
    else:
        num_new_tokens = completion_tokens - state.last_completion_tokens
        if num_new_tokens:
            new_time = time.time()
            interval = new_time - state.last_time
            self.metrics_collector.observe_inter_token_latency(
                interval,
                num_new_tokens,
            )
            state.last_time = new_time
            state.last_completion_tokens = completion_tokens

    if state.finished:
        has_grammar = (
            state.obj.sampling_params.get(&#34;json_schema&#34;, None)
            or state.obj.sampling_params.get(&#34;regex&#34;, None)
            or state.obj.sampling_params.get(&#34;ebnf&#34;, None)
            or state.obj.sampling_params.get(&#34;structural_tag&#34;, None)
        )
        self.metrics_collector.observe_one_finished_request(
            recv_obj.prompt_tokens[i],
            completion_tokens,
            recv_obj.cached_tokens[i],
            state.finished_time - state.created_time,
            has_grammar,
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.configure_logging"><code class="name flex">
<span>def <span class="ident">configure_logging</span></span>(<span>self,<br>obj: <a title="sglang.srt.managers.io_struct.ConfigureLoggingReq" href="io_struct.html#sglang.srt.managers.io_struct.ConfigureLoggingReq">ConfigureLoggingReq</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure_logging(self, obj: ConfigureLoggingReq):
    if obj.log_requests is not None:
        self.log_requests = obj.log_requests
    if obj.log_requests_level is not None:
        self.log_requests_level = obj.log_requests_level
    if obj.dump_requests_folder is not None:
        self.dump_requests_folder = obj.dump_requests_folder
    if obj.dump_requests_threshold is not None:
        self.dump_requests_threshold = obj.dump_requests_threshold
    if obj.crash_dump_folder is not None:
        self.crash_dump_folder = obj.crash_dump_folder
    logging.info(f&#34;Config logging: {obj=}&#34;)
    self.log_request_metadata = self.get_log_request_metadata()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.continue_generation"><code class="name flex">
<span>async def <span class="ident">continue_generation</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def continue_generation(self):
    async with self.is_pause_cond:
        self.is_pause = False
        self.is_pause_cond.notify_all()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.convert_logprob_style"><code class="name flex">
<span>def <span class="ident">convert_logprob_style</span></span>(<span>self,<br>meta_info: dict,<br>state: <a title="sglang.srt.managers.tokenizer_manager.ReqState" href="#sglang.srt.managers.tokenizer_manager.ReqState">ReqState</a>,<br>top_logprobs_num: int,<br>token_ids_logprob: List[int],<br>return_text_in_logprobs: bool,<br>recv_obj: <a title="sglang.srt.managers.io_struct.BatchStrOut" href="io_struct.html#sglang.srt.managers.io_struct.BatchStrOut">BatchStrOut</a>,<br>recv_obj_index: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convert_logprob_style(
    self,
    meta_info: dict,
    state: ReqState,
    top_logprobs_num: int,
    token_ids_logprob: List[int],
    return_text_in_logprobs: bool,
    recv_obj: BatchStrOut,
    recv_obj_index: int,
):
    if recv_obj.input_token_logprobs_val is None:
        return

    if len(recv_obj.input_token_logprobs_val) &gt; 0:
        state.input_token_logprobs_val.extend(
            recv_obj.input_token_logprobs_val[recv_obj_index]
        )
        state.input_token_logprobs_idx.extend(
            recv_obj.input_token_logprobs_idx[recv_obj_index]
        )
    state.output_token_logprobs_val.extend(
        recv_obj.output_token_logprobs_val[recv_obj_index]
    )
    state.output_token_logprobs_idx.extend(
        recv_obj.output_token_logprobs_idx[recv_obj_index]
    )
    meta_info[&#34;input_token_logprobs&#34;] = self.detokenize_logprob_tokens(
        state.input_token_logprobs_val,
        state.input_token_logprobs_idx,
        return_text_in_logprobs,
    )
    meta_info[&#34;output_token_logprobs&#34;] = self.detokenize_logprob_tokens(
        state.output_token_logprobs_val,
        state.output_token_logprobs_idx,
        return_text_in_logprobs,
    )

    if top_logprobs_num &gt; 0:
        if len(recv_obj.input_top_logprobs_val) &gt; 0:
            state.input_top_logprobs_val.extend(
                recv_obj.input_top_logprobs_val[recv_obj_index]
            )
            state.input_top_logprobs_idx.extend(
                recv_obj.input_top_logprobs_idx[recv_obj_index]
            )
        state.output_top_logprobs_val.extend(
            recv_obj.output_top_logprobs_val[recv_obj_index]
        )
        state.output_top_logprobs_idx.extend(
            recv_obj.output_top_logprobs_idx[recv_obj_index]
        )
        meta_info[&#34;input_top_logprobs&#34;] = self.detokenize_top_logprobs_tokens(
            state.input_top_logprobs_val,
            state.input_top_logprobs_idx,
            return_text_in_logprobs,
        )
        meta_info[&#34;output_top_logprobs&#34;] = self.detokenize_top_logprobs_tokens(
            state.output_top_logprobs_val,
            state.output_top_logprobs_idx,
            return_text_in_logprobs,
        )

    if token_ids_logprob is not None:
        if len(recv_obj.input_token_ids_logprobs_val) &gt; 0:
            state.input_token_ids_logprobs_val.extend(
                recv_obj.input_token_ids_logprobs_val[recv_obj_index]
            )
            state.input_token_ids_logprobs_idx.extend(
                recv_obj.input_token_ids_logprobs_idx[recv_obj_index]
            )
        state.output_token_ids_logprobs_val.extend(
            recv_obj.output_token_ids_logprobs_val[recv_obj_index]
        )
        state.output_token_ids_logprobs_idx.extend(
            recv_obj.output_token_ids_logprobs_idx[recv_obj_index]
        )
        meta_info[&#34;input_token_ids_logprobs&#34;] = self.detokenize_top_logprobs_tokens(
            state.input_token_ids_logprobs_val,
            state.input_token_ids_logprobs_idx,
            return_text_in_logprobs,
        )
        meta_info[&#34;output_token_ids_logprobs&#34;] = (
            self.detokenize_top_logprobs_tokens(
                state.output_token_ids_logprobs_val,
                state.output_token_ids_logprobs_idx,
                return_text_in_logprobs,
            )
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.create_abort_task"><code class="name flex">
<span>def <span class="ident">create_abort_task</span></span>(<span>self,<br>obj: <a title="sglang.srt.managers.io_struct.GenerateReqInput" href="io_struct.html#sglang.srt.managers.io_struct.GenerateReqInput">GenerateReqInput</a>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_abort_task(self, obj: GenerateReqInput):
    # Abort the request if the client is disconnected.
    async def abort_request():
        await asyncio.sleep(2)
        if obj.is_single:
            self.abort_request(obj.rid)
        else:
            for rid in obj.rid:
                self.abort_request(rid)

    background_tasks = BackgroundTasks()
    background_tasks.add_task(abort_request)
    return background_tasks</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.detokenize_logprob_tokens"><code class="name flex">
<span>def <span class="ident">detokenize_logprob_tokens</span></span>(<span>self,<br>token_logprobs_val: List[float],<br>token_logprobs_idx: List[int],<br>decode_to_text: bool)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def detokenize_logprob_tokens(
    self,
    token_logprobs_val: List[float],
    token_logprobs_idx: List[int],
    decode_to_text: bool,
):
    if not decode_to_text:
        return [
            (logprob, token_id, None)
            for logprob, token_id in zip(token_logprobs_val, token_logprobs_idx)
        ]
    else:
        assert self.tokenizer is not None
        token_texts = self.tokenizer.batch_decode(token_logprobs_idx)
        return list(zip(token_logprobs_val, token_logprobs_idx, token_texts))</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.detokenize_top_logprobs_tokens"><code class="name flex">
<span>def <span class="ident">detokenize_top_logprobs_tokens</span></span>(<span>self,<br>token_logprobs_val: List[float],<br>token_logprobs_idx: List[int],<br>decode_to_text: bool)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def detokenize_top_logprobs_tokens(
    self,
    token_logprobs_val: List[float],
    token_logprobs_idx: List[int],
    decode_to_text: bool,
):
    # TODO: The current implementation only batches the detokenization for top-k tokens per single position.
    # We should batch all top-k tokens in all positions.
    ret = []
    for i in range(len(token_logprobs_val)):
        if token_logprobs_val[i]:
            ret.append(
                self.detokenize_logprob_tokens(
                    token_logprobs_val[i], token_logprobs_idx[i], decode_to_text
                )
            )
        else:
            ret.append(None)
    return ret</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.dump_expert_distribution_record"><code class="name flex">
<span>async def <span class="ident">dump_expert_distribution_record</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def dump_expert_distribution_record(self):
    self.auto_create_handle_loop()
    await self.expert_distribution_communicator(ExpertDistributionReq.DUMP_RECORD)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.dump_requests"><code class="name flex">
<span>def <span class="ident">dump_requests</span></span>(<span>self,<br>state: <a title="sglang.srt.managers.tokenizer_manager.ReqState" href="#sglang.srt.managers.tokenizer_manager.ReqState">ReqState</a>,<br>out_dict: dict)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump_requests(self, state: ReqState, out_dict: dict):
    self.dump_request_list.append(
        (state.obj, out_dict, state.created_time, time.time())
    )

    if len(self.dump_request_list) &gt;= self.dump_requests_threshold:
        filename = os.path.join(
            self.dump_requests_folder,
            datetime.now().strftime(&#34;%Y-%m-%d_%H-%M-%S&#34;) + &#34;.pkl&#34;,
        )
        self._dump_data_to_file(
            data_list=self.dump_request_list,
            filename=filename,
            log_message=f&#34;Dump {len(self.dump_request_list)} requests to {filename}&#34;,
        )
        self.dump_request_list = []</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.dump_requests_before_crash"><code class="name flex">
<span>def <span class="ident">dump_requests_before_crash</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump_requests_before_crash(self):
    if self.crash_dump_performed:
        logger.info(
            &#34;SIGTERM/SIGQUIT/Exception triggered, but crash dump already performed, skipping.&#34;
        )
        return

    if not self.crash_dump_folder:
        return

    logger.error(f&#34;Dumping requests before crash. {self.crash_dump_folder=}&#34;)
    self.crash_dump_performed = True

    # Check if NFS directory is available
    # expected_nfs_dir = &#34;/&#34; + self.crash_dump_folder.lstrip(&#34;/&#34;).split(&#34;/&#34;)[0]
    # use_nfs_dir = os.path.isdir(expected_nfs_dir) and os.access(
    #     expected_nfs_dir, os.W_OK
    # )
    use_nfs_dir = False
    if not use_nfs_dir:
        logger.error(
            f&#34;Expected NFS directory is not available or writable. Uploading to GCS.&#34;
        )

    data_to_dump = []
    if self.crash_dump_request_list:
        data_to_dump.extend(self.crash_dump_request_list)

    # Add unfinished requests from rid_to_state
    unfinished_requests = []
    for rid, state in self.rid_to_state.items():
        if not state.finished:
            unfinished_requests.append(
                (
                    state.obj,
                    state.out_list[-1] if state.out_list else {},
                    state.created_time,
                    time.time(),
                )
            )
    if unfinished_requests:
        data_to_dump.extend(unfinished_requests)

    if not data_to_dump:
        return

    object_name = f&#39;crash_dump_{datetime.now().strftime(&#34;%Y-%m-%d_%H-%M-%S&#34;)}.pkl&#39;
    filename = os.path.join(
        self.crash_dump_folder,
        os.getenv(&#34;HOSTNAME&#34;, None),
        object_name,
    )

    os.makedirs(os.path.dirname(filename), exist_ok=True)
    # Include server_args in the dump
    data_to_dump_with_server_args = {
        &#34;server_args&#34;: self.server_args,
        &#34;requests&#34;: data_to_dump,
    }
    with open(filename, &#34;wb&#34;) as f:
        pickle.dump(data_to_dump_with_server_args, f)
    logger.error(
        f&#34;Dumped {len(self.crash_dump_request_list)} finished and {len(unfinished_requests)} unfinished requests before crash to {filename}&#34;
    )

    def _upload_file_to_gcs(bucket_name, source_file_path, object_name):
        from google.cloud import storage

        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(object_name)
        blob.upload_from_filename(source_file_path, if_generation_match=0)
        logger.error(
            f&#34;Successfully uploaded {source_file_path} to gs://{bucket_name}/{object_name}&#34;
        )

    if not use_nfs_dir:
        _upload_file_to_gcs(
            &#34;sglang_crash_dump&#34;,
            filename,
            os.getenv(&#34;HOSTNAME&#34;, None) + &#34;/&#34; + object_name,
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.flush_cache"><code class="name flex">
<span>async def <span class="ident">flush_cache</span></span>(<span>self) ‑> <a title="sglang.srt.managers.io_struct.FlushCacheReqOutput" href="io_struct.html#sglang.srt.managers.io_struct.FlushCacheReqOutput">FlushCacheReqOutput</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def flush_cache(self) -&gt; FlushCacheReqOutput:
    return (await self.flush_cache_communicator(FlushCacheReqInput()))[0]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.freeze_gc"><code class="name flex">
<span>async def <span class="ident">freeze_gc</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def freeze_gc(self):
    &#34;&#34;&#34;Send a freeze_gc message to the scheduler first, then freeze locally.&#34;&#34;&#34;
    self.send_to_scheduler.send_pyobj(FreezeGCReq())
    freeze_gc(&#34;Tokenizer Manager&#34;)
    return None</code></pre>
</details>
<div class="desc"><p>Send a freeze_gc message to the scheduler first, then freeze locally.</p></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.generate_request"><code class="name flex">
<span>async def <span class="ident">generate_request</span></span>(<span>self,<br>obj: <a title="sglang.srt.managers.io_struct.GenerateReqInput" href="io_struct.html#sglang.srt.managers.io_struct.GenerateReqInput">GenerateReqInput</a> | <a title="sglang.srt.managers.io_struct.EmbeddingReqInput" href="io_struct.html#sglang.srt.managers.io_struct.EmbeddingReqInput">EmbeddingReqInput</a>,<br>request: starlette.requests.Request | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def generate_request(
    self,
    obj: Union[GenerateReqInput, EmbeddingReqInput],
    request: Optional[fastapi.Request] = None,
):
    created_time = time.time()
    self.auto_create_handle_loop()
    obj.normalize_batch_and_arguments()

    if self.log_requests:
        max_length, skip_names, _ = self.log_request_metadata
        logger.info(
            f&#34;Receive: obj={dataclass_to_string_truncated(obj, max_length, skip_names=skip_names)}&#34;
        )

    async with self.is_pause_cond:
        await self.is_pause_cond.wait_for(lambda: not self.is_pause)

    async with self.model_update_lock.reader_lock:
        if self.server_args.enable_lora and obj.lora_path:
            # Look up the LoRA ID from the registry and start tracking ongoing LoRA requests.
            obj.lora_id = await self.lora_registry.acquire(obj.lora_path)

        if obj.is_single:
            tokenized_obj = await self._tokenize_one_request(obj)
            state = self._send_one_request(obj, tokenized_obj, created_time)
            async for response in self._wait_one_response(obj, state, request):
                yield response
        else:
            async for response in self._handle_batch_request(
                obj, request, created_time
            ):
                yield response</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.get_internal_state"><code class="name flex">
<span>async def <span class="ident">get_internal_state</span></span>(<span>self) ‑> List[Dict[Any, Any]]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def get_internal_state(self) -&gt; List[Dict[Any, Any]]:
    req = GetInternalStateReq()
    responses: List[GetInternalStateReqOutput] = (
        await self.get_internal_state_communicator(req)
    )
    # Many DP ranks
    return [res.internal_state for res in responses]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.get_load"><code class="name flex">
<span>async def <span class="ident">get_load</span></span>(<span>self) ‑> dict</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def get_load(self) -&gt; dict:
    # TODO(lsyin): fake load report server
    if not self.current_load_lock.locked():
        async with self.current_load_lock:
            internal_state = await self.get_internal_state()
            self.current_load = internal_state[0][&#34;load&#34;]
    return {&#34;load&#34;: self.current_load}</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.get_log_request_metadata"><code class="name flex">
<span>def <span class="ident">get_log_request_metadata</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_log_request_metadata(self):
    max_length = None
    skip_names = None
    out_skip_names = None
    if self.log_requests:
        if self.log_requests_level == 0:
            max_length = 1 &lt;&lt; 30
            skip_names = set(
                [
                    &#34;text&#34;,
                    &#34;input_ids&#34;,
                    &#34;input_embeds&#34;,
                    &#34;image_data&#34;,
                    &#34;audio_data&#34;,
                    &#34;lora_path&#34;,
                    &#34;sampling_params&#34;,
                ]
            )
            out_skip_names = set(
                [
                    &#34;text&#34;,
                    &#34;output_ids&#34;,
                    &#34;embedding&#34;,
                ]
            )
        elif self.log_requests_level == 1:
            max_length = 1 &lt;&lt; 30
            skip_names = set(
                [
                    &#34;text&#34;,
                    &#34;input_ids&#34;,
                    &#34;input_embeds&#34;,
                    &#34;image_data&#34;,
                    &#34;audio_data&#34;,
                    &#34;lora_path&#34;,
                ]
            )
            out_skip_names = set(
                [
                    &#34;text&#34;,
                    &#34;output_ids&#34;,
                    &#34;embedding&#34;,
                ]
            )
        elif self.log_requests_level == 2:
            max_length = 2048
        elif self.log_requests_level == 3:
            max_length = 1 &lt;&lt; 30
        else:
            raise ValueError(
                f&#34;Invalid --log-requests-level: {self.log_requests_level=}&#34;
            )
    return max_length, skip_names, out_skip_names</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.get_weights_by_name"><code class="name flex">
<span>async def <span class="ident">get_weights_by_name</span></span>(<span>self,<br>obj: <a title="sglang.srt.managers.io_struct.GetWeightsByNameReqInput" href="io_struct.html#sglang.srt.managers.io_struct.GetWeightsByNameReqInput">GetWeightsByNameReqInput</a>,<br>request: starlette.requests.Request | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def get_weights_by_name(
    self, obj: GetWeightsByNameReqInput, request: Optional[fastapi.Request] = None
):
    self.auto_create_handle_loop()
    results = await self.get_weights_by_name_communicator(obj)
    all_parameters = [r.parameter for r in results]
    if self.server_args.dp_size == 1:
        return all_parameters[0]
    else:
        return all_parameters</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.handle_loop"><code class="name flex">
<span>async def <span class="ident">handle_loop</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def handle_loop(self):
    &#34;&#34;&#34;The event loop that handles requests&#34;&#34;&#34;

    while True:
        recv_obj = await self.recv_from_detokenizer.recv_pyobj()
        self._result_dispatcher(recv_obj)
        self.last_receive_tstamp = time.time()</code></pre>
</details>
<div class="desc"><p>The event loop that handles requests</p></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.init_weights_update_group"><code class="name flex">
<span>async def <span class="ident">init_weights_update_group</span></span>(<span>self,<br>obj: <a title="sglang.srt.managers.io_struct.InitWeightsUpdateGroupReqInput" href="io_struct.html#sglang.srt.managers.io_struct.InitWeightsUpdateGroupReqInput">InitWeightsUpdateGroupReqInput</a>,<br>request: starlette.requests.Request | None = None) ‑> Tuple[bool, str]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def init_weights_update_group(
    self,
    obj: InitWeightsUpdateGroupReqInput,
    request: Optional[fastapi.Request] = None,
) -&gt; Tuple[bool, str]:
    self.auto_create_handle_loop()
    assert (
        self.server_args.dp_size == 1
    ), &#34;dp_size must be 1 for init parameter update group&#34;
    result = (await self.init_weights_update_group_communicator(obj))[0]
    return result.success, result.message</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.load_lora_adapter"><code class="name flex">
<span>async def <span class="ident">load_lora_adapter</span></span>(<span>self,<br>obj: <a title="sglang.srt.managers.io_struct.LoadLoRAAdapterReqInput" href="io_struct.html#sglang.srt.managers.io_struct.LoadLoRAAdapterReqInput">LoadLoRAAdapterReqInput</a>) ‑> <a title="sglang.srt.managers.io_struct.LoRAUpdateResult" href="io_struct.html#sglang.srt.managers.io_struct.LoRAUpdateResult">LoRAUpdateResult</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def load_lora_adapter(
    self,
    obj: LoadLoRAAdapterReqInput,
    _: Optional[fastapi.Request] = None,
) -&gt; LoadLoRAAdapterReqOutput:
    self.auto_create_handle_loop()

    try:
        if not self.server_args.enable_lora:
            raise ValueError(
                &#34;LoRA is not enabled. Please set `--enable-lora` to enable LoRA.&#34;
            )

        # TODO (lifuhuang): Remove this after we verify that dynamic lora loading works
        # with dp_size &gt; 1.
        assert (
            self.server_args.dp_size == 1
        ), &#34;dp_size must be 1 for dynamic lora loading&#34;
        logger.info(
            &#34;Start load Lora adapter. Lora name=%s, path=%s&#34;,
            obj.lora_name,
            obj.lora_path,
        )

        async with self.lora_update_lock:
            if (
                self.server_args.max_loaded_loras is not None
                and self.lora_registry.num_registered_loras
                &gt;= self.server_args.max_loaded_loras
            ):
                raise ValueError(
                    f&#34;Cannot load LoRA adapter {obj.lora_name} at path {obj.lora_path}. &#34;
                    f&#34;Maximum number of loaded LoRA adapters is {self.server_args.max_loaded_loras}. &#34;
                    &#34;Please unload some LoRA adapters before loading new ones.&#34;
                )

            # Generate new uniquely identifiable LoRARef object.
            new_adapter = LoRARef(
                lora_name=obj.lora_name,
                lora_path=obj.lora_path,
                pinned=obj.pinned,
            )

            # Trigger the actual loading operation at the backend processes.
            obj.lora_id = new_adapter.lora_id
            result = (await self.update_lora_adapter_communicator(obj))[0]

            # Register the LoRA adapter only after loading is successful.
            if result.success:
                await self.lora_registry.register(new_adapter)

            return result
    except ValueError as e:
        return LoadLoRAAdapterReqOutput(
            success=False,
            error_message=str(e),
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.open_session"><code class="name flex">
<span>async def <span class="ident">open_session</span></span>(<span>self,<br>obj: <a title="sglang.srt.managers.io_struct.OpenSessionReqInput" href="io_struct.html#sglang.srt.managers.io_struct.OpenSessionReqInput">OpenSessionReqInput</a>,<br>request: starlette.requests.Request | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def open_session(
    self, obj: OpenSessionReqInput, request: Optional[fastapi.Request] = None
):
    self.auto_create_handle_loop()

    if obj.session_id is None:
        obj.session_id = uuid.uuid4().hex
    elif obj.session_id in self.session_futures:
        return None

    self.send_to_scheduler.send_pyobj(obj)

    self.session_futures[obj.session_id] = asyncio.Future()
    session_id = await self.session_futures[obj.session_id]
    del self.session_futures[obj.session_id]
    return session_id</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.pause_generation"><code class="name flex">
<span>async def <span class="ident">pause_generation</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def pause_generation(self):
    async with self.is_pause_cond:
        self.is_pause = True
        self.abort_request(abort_all=True)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.record_request_for_crash_dump"><code class="name flex">
<span>def <span class="ident">record_request_for_crash_dump</span></span>(<span>self,<br>state: <a title="sglang.srt.managers.tokenizer_manager.ReqState" href="#sglang.srt.managers.tokenizer_manager.ReqState">ReqState</a>,<br>out_dict: dict)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def record_request_for_crash_dump(self, state: ReqState, out_dict: dict):
    current_time = time.time()
    self.crash_dump_request_list.append(
        (state.obj, out_dict, state.created_time, current_time)
    )
    # Remove requests older than 5 minutes based on finish time
    while (
        self.crash_dump_request_list
        and current_time - self.crash_dump_request_list[0][3] &gt;= 300
    ):
        self.crash_dump_request_list.popleft()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.release_memory_occupation"><code class="name flex">
<span>async def <span class="ident">release_memory_occupation</span></span>(<span>self,<br>obj: <a title="sglang.srt.managers.io_struct.ReleaseMemoryOccupationReqInput" href="io_struct.html#sglang.srt.managers.io_struct.ReleaseMemoryOccupationReqInput">ReleaseMemoryOccupationReqInput</a>,<br>request: starlette.requests.Request | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def release_memory_occupation(
    self,
    obj: ReleaseMemoryOccupationReqInput,
    request: Optional[fastapi.Request] = None,
):
    self.auto_create_handle_loop()
    await self.release_memory_occupation_communicator(obj)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.resume_memory_occupation"><code class="name flex">
<span>async def <span class="ident">resume_memory_occupation</span></span>(<span>self,<br>obj: <a title="sglang.srt.managers.io_struct.ResumeMemoryOccupationReqInput" href="io_struct.html#sglang.srt.managers.io_struct.ResumeMemoryOccupationReqInput">ResumeMemoryOccupationReqInput</a>,<br>request: starlette.requests.Request | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def resume_memory_occupation(
    self,
    obj: ResumeMemoryOccupationReqInput,
    request: Optional[fastapi.Request] = None,
):
    self.auto_create_handle_loop()
    await self.resume_memory_occupation_communicator(obj)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.score_request"><code class="name flex">
<span>async def <span class="ident">score_request</span></span>(<span>self,<br>query: str | List[int] | None = None,<br>items: str | List[str] | List[List[int]] | None = None,<br>label_token_ids: List[int] | None = None,<br>apply_softmax: bool = False,<br>item_first: bool = False,<br>request: Any | None = None) ‑> List[List[float]]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def score_request(
    self,
    query: Optional[Union[str, List[int]]] = None,
    items: Optional[Union[str, List[str], List[List[int]]]] = None,
    label_token_ids: Optional[List[int]] = None,
    apply_softmax: bool = False,
    item_first: bool = False,
    request: Optional[Any] = None,
) -&gt; List[List[float]]:
    &#34;&#34;&#34;
    See Engine.score() for more details.
    &#34;&#34;&#34;
    if label_token_ids is None:
        raise ValueError(&#34;label_token_ids must be provided&#34;)

    if self.tokenizer is not None:
        vocab_size = self.tokenizer.vocab_size
        for token_id in label_token_ids:
            if token_id &gt;= vocab_size:
                raise ValueError(
                    f&#34;Token ID {token_id} is out of vocabulary (vocab size: {vocab_size})&#34;
                )

    batch_request = GenerateReqInput(
        token_ids_logprob=label_token_ids,
        return_logprob=True,
        stream=False,
        sampling_params={&#34;max_new_tokens&#34;: 0},
    )

    # Handle string or tokenized query/items
    if isinstance(query, str) and (
        isinstance(items, str)
        or (isinstance(items, list) and (not items or isinstance(items[0], str)))
    ):
        # Both query and items are text
        items_list = [items] if isinstance(items, str) else items
        if item_first:
            prompts = [f&#34;{item}{query}&#34; for item in items_list]
        else:
            prompts = [f&#34;{query}{item}&#34; for item in items_list]

        batch_request.text = prompts

    elif (
        isinstance(query, list)
        and isinstance(items, list)
        and items
        and isinstance(items[0], list)
    ):
        # Both query and items are token IDs
        if item_first:
            input_ids_list = [item + query for item in items]
        else:
            input_ids_list = [query + item for item in items]

        batch_request.input_ids = input_ids_list
    else:
        raise ValueError(
            &#34;Invalid combination of query/items types for score_request.&#34;
        )

    results = await self.generate_request(batch_request, request).__anext__()
    scores = []

    for result in results:
        # Get logprobs for each token
        logprobs = {}

        # For scoring requests, we read from output_token_ids_logprobs since we want
        # the logprobs for specific tokens mentioned in the label_token_ids at
        # the next position after the last token in the prompt
        output_logprobs = result[&#34;meta_info&#34;].get(&#34;output_token_ids_logprobs&#34;, [])

        # Throw an error here if output_logprobs is None
        if output_logprobs is None:
            raise RuntimeError(
                f&#34;output_logprobs is None for request {result[&#39;meta_info&#39;].get(&#39;id&#39;, &#39;&lt;unknown&gt;&#39;)}. &#34;
                &#34;This usually indicates a problem with the scoring request or the backend output.&#34;
            )

        for logprob, token_id, _ in output_logprobs[0]:
            if token_id in label_token_ids:
                logprobs[token_id] = logprob

        # Get scores in order of label_token_ids
        score_list = [
            logprobs.get(token_id, float(&#34;-inf&#34;)) for token_id in label_token_ids
        ]

        # Apply softmax to logprobs if needed
        if apply_softmax:
            score_list = torch.softmax(torch.tensor(score_list), dim=0).tolist()
        else:
            # Convert logprobs to probabilities if not using softmax
            score_list = [
                math.exp(x) if x != float(&#34;-inf&#34;) else 0.0 for x in score_list
            ]

        scores.append(score_list)

    return scores</code></pre>
</details>
<div class="desc"><p>See Engine.score() for more details.</p></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.set_internal_state"><code class="name flex">
<span>async def <span class="ident">set_internal_state</span></span>(<span>self,<br>obj: <a title="sglang.srt.managers.io_struct.SetInternalStateReq" href="io_struct.html#sglang.srt.managers.io_struct.SetInternalStateReq">SetInternalStateReq</a>) ‑> List[bool]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def set_internal_state(self, obj: SetInternalStateReq) -&gt; List[bool]:
    responses: List[SetInternalStateReqOutput] = (
        await self.set_internal_state_communicator(obj)
    )
    return [res.updated for res in responses]</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.sigterm_watchdog"><code class="name flex">
<span>async def <span class="ident">sigterm_watchdog</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def sigterm_watchdog(self):
    while not self.gracefully_exit:
        await asyncio.sleep(5)

    # Drain requests
    while True:
        remain_num_req = len(self.rid_to_state)

        if self.server_status == ServerStatus.UnHealthy:
            # if health check failed, we should exit immediately
            logger.error(
                &#34;Signal SIGTERM received while health check failed. Exiting... remaining number of requests: %d&#34;,
                remain_num_req,
            )
            self.dump_requests_before_crash()
            break

        elif get_bool_env_var(&#34;SGL_FORCE_SHUTDOWN&#34;):
            # if force shutdown flag set, exit immediately
            logger.error(
                &#34;Signal SIGTERM received while force shutdown flag set. Force exiting... remaining number of requests: %d&#34;,
                remain_num_req,
            )
            break

        logger.info(
            f&#34;Gracefully exiting... remaining number of requests {remain_num_req}&#34;
        )
        if remain_num_req &gt; 0:
            await asyncio.sleep(5)
        else:
            self.dump_requests_before_crash()
            break

    kill_process_tree(os.getpid(), include_parent=True)
    sys.exit(0)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.slow_down"><code class="name flex">
<span>async def <span class="ident">slow_down</span></span>(<span>self,<br>obj: <a title="sglang.srt.managers.io_struct.SlowDownReqInput" href="io_struct.html#sglang.srt.managers.io_struct.SlowDownReqInput">SlowDownReqInput</a>,<br>request: starlette.requests.Request | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def slow_down(
    self,
    obj: SlowDownReqInput,
    request: Optional[fastapi.Request] = None,
):
    self.auto_create_handle_loop()
    await self.slow_down_communicator(obj)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.start_expert_distribution_record"><code class="name flex">
<span>async def <span class="ident">start_expert_distribution_record</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def start_expert_distribution_record(self):
    self.auto_create_handle_loop()
    await self.expert_distribution_communicator(ExpertDistributionReq.START_RECORD)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.start_profile"><code class="name flex">
<span>async def <span class="ident">start_profile</span></span>(<span>self,<br>output_dir: str | None = None,<br>start_step: int | None = None,<br>num_steps: int | None = None,<br>activities: List[str] | None = None,<br>with_stack: bool | None = None,<br>record_shapes: bool | None = None,<br>profile_by_stage: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def start_profile(
    self,
    output_dir: Optional[str] = None,
    start_step: Optional[int] = None,
    num_steps: Optional[int] = None,
    activities: Optional[List[str]] = None,
    with_stack: Optional[bool] = None,
    record_shapes: Optional[bool] = None,
    profile_by_stage: bool = False,
):
    self.auto_create_handle_loop()
    env_with_stack: bool = get_bool_env_var(&#34;SGLANG_PROFILE_WITH_STACK&#34;, &#34;true&#34;)
    with_stack = False if with_stack is False or env_with_stack is False else True
    req = ProfileReq(
        type=ProfileReqType.START_PROFILE,
        output_dir=output_dir,
        start_step=start_step,
        num_steps=num_steps,
        activities=activities,
        with_stack=with_stack,
        record_shapes=record_shapes,
        profile_by_stage=profile_by_stage,
        profile_id=str(time.time()),
    )
    return await self._execute_profile(req)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.stop_expert_distribution_record"><code class="name flex">
<span>async def <span class="ident">stop_expert_distribution_record</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def stop_expert_distribution_record(self):
    self.auto_create_handle_loop()
    await self.expert_distribution_communicator(ExpertDistributionReq.STOP_RECORD)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.stop_profile"><code class="name flex">
<span>async def <span class="ident">stop_profile</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def stop_profile(self):
    self.auto_create_handle_loop()
    req = ProfileReq(type=ProfileReqType.STOP_PROFILE)
    return await self._execute_profile(req)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.unload_lora_adapter"><code class="name flex">
<span>async def <span class="ident">unload_lora_adapter</span></span>(<span>self,<br>obj: <a title="sglang.srt.managers.io_struct.UnloadLoRAAdapterReqInput" href="io_struct.html#sglang.srt.managers.io_struct.UnloadLoRAAdapterReqInput">UnloadLoRAAdapterReqInput</a>) ‑> <a title="sglang.srt.managers.io_struct.LoRAUpdateResult" href="io_struct.html#sglang.srt.managers.io_struct.LoRAUpdateResult">LoRAUpdateResult</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def unload_lora_adapter(
    self,
    obj: UnloadLoRAAdapterReqInput,
    _: Optional[fastapi.Request] = None,
) -&gt; UnloadLoRAAdapterReqOutput:
    self.auto_create_handle_loop()

    try:
        if not self.server_args.enable_lora:
            raise ValueError(
                &#34;LoRA is not enabled. Please set `--enable-lora` to enable LoRA.&#34;
            )

        assert (
            obj.lora_name is not None
        ), &#34;lora_name must be provided to unload LoRA adapter&#34;

        # TODO (lifuhuang): Remove this after we verify that dynamic lora loading works
        # with dp_size &gt; 1.
        assert (
            self.server_args.dp_size == 1
        ), &#34;dp_size must be 1 for dynamic lora loading&#34;
        logger.info(
            &#34;Start unload Lora adapter. Lora name=%s&#34;,
            obj.lora_name,
        )

        async with self.lora_update_lock:
            # Unregister the LoRA adapter from the registry to stop new requests for this adapter
            # from being started.
            lora_id = await self.lora_registry.unregister(obj.lora_name)
            obj.lora_id = lora_id

            # Initiate the actual unloading operation at the backend processes only after all
            # ongoing requests using this LoRA adapter are finished.
            await self.lora_registry.wait_for_unload(lora_id)
            result = (await self.update_lora_adapter_communicator(obj))[0]

            return result
    except ValueError as e:
        return UnloadLoRAAdapterReqOutput(success=False, error_message=str(e))</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.update_weights_from_disk"><code class="name flex">
<span>async def <span class="ident">update_weights_from_disk</span></span>(<span>self,<br>obj: <a title="sglang.srt.managers.io_struct.UpdateWeightFromDiskReqInput" href="io_struct.html#sglang.srt.managers.io_struct.UpdateWeightFromDiskReqInput">UpdateWeightFromDiskReqInput</a>,<br>request: starlette.requests.Request | None = None) ‑> Tuple[bool, str]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def update_weights_from_disk(
    self,
    obj: UpdateWeightFromDiskReqInput,
    request: Optional[fastapi.Request] = None,
) -&gt; Tuple[bool, str]:
    self.auto_create_handle_loop()

    # default the load format to the server_args
    if obj.load_format is None:
        obj.load_format = self.server_args.load_format
    logger.info(&#34;Start update_weights. Load format=%s&#34;, obj.load_format)

    if obj.abort_all_requests:
        self.abort_request(abort_all=True)

    if True:  # Keep this redundant check to simplify some internal code sync
        # Hold the lock if it is not async. This means that weight sync
        # cannot run while requests are in progress.
        async with self.model_update_lock.writer_lock:
            return await self._wait_for_model_update_from_disk(obj)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.update_weights_from_distributed"><code class="name flex">
<span>async def <span class="ident">update_weights_from_distributed</span></span>(<span>self,<br>obj: <a title="sglang.srt.managers.io_struct.UpdateWeightsFromDistributedReqInput" href="io_struct.html#sglang.srt.managers.io_struct.UpdateWeightsFromDistributedReqInput">UpdateWeightsFromDistributedReqInput</a>,<br>request: starlette.requests.Request | None = None) ‑> Tuple[bool, str]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def update_weights_from_distributed(
    self,
    obj: UpdateWeightsFromDistributedReqInput,
    request: Optional[fastapi.Request] = None,
) -&gt; Tuple[bool, str]:
    self.auto_create_handle_loop()
    assert (
        self.server_args.dp_size == 1 or self.server_args.enable_dp_attention
    ), &#34;dp_size must be 1 or dp attention must be enabled for update weights from distributed&#34;

    if obj.abort_all_requests:
        self.abort_request(abort_all=True)

    # This means that weight sync
    # cannot run while requests are in progress.
    async with self.model_update_lock.writer_lock:
        result = (await self.update_weights_from_distributed_communicator(obj))[0]
        return result.success, result.message</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.tokenizer_manager.TokenizerManager.update_weights_from_tensor"><code class="name flex">
<span>async def <span class="ident">update_weights_from_tensor</span></span>(<span>self,<br>obj: <a title="sglang.srt.managers.io_struct.UpdateWeightsFromTensorReqInput" href="io_struct.html#sglang.srt.managers.io_struct.UpdateWeightsFromTensorReqInput">UpdateWeightsFromTensorReqInput</a>,<br>request: starlette.requests.Request | None = None) ‑> Tuple[bool, str]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">async def update_weights_from_tensor(
    self,
    obj: UpdateWeightsFromTensorReqInput,
    request: Optional[fastapi.Request] = None,
) -&gt; Tuple[bool, str]:
    self.auto_create_handle_loop()
    assert (
        self.server_args.dp_size == 1 or self.server_args.enable_dp_attention
    ), &#34;dp_size must be 1 or dp attention must be enabled for update weights from tensor&#34;

    if obj.abort_all_requests:
        self.abort_request(abort_all=True)

    # This means that weight sync
    # cannot run while requests are in progress.
    async with self.model_update_lock.writer_lock:
        result = (await self.update_weights_from_tensor_communicator(obj))[0]
        return result.success, result.message</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.managers" href="index.html">sglang.srt.managers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.managers.tokenizer_manager.print_exception_wrapper" href="#sglang.srt.managers.tokenizer_manager.print_exception_wrapper">print_exception_wrapper</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.managers.tokenizer_manager.ReqState" href="#sglang.srt.managers.tokenizer_manager.ReqState">ReqState</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.created_time" href="#sglang.srt.managers.tokenizer_manager.ReqState.created_time">created_time</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.event" href="#sglang.srt.managers.tokenizer_manager.ReqState.event">event</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.finished" href="#sglang.srt.managers.tokenizer_manager.ReqState.finished">finished</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.finished_time" href="#sglang.srt.managers.tokenizer_manager.ReqState.finished_time">finished_time</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.first_token_time" href="#sglang.srt.managers.tokenizer_manager.ReqState.first_token_time">first_token_time</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.input_token_ids_logprobs_idx" href="#sglang.srt.managers.tokenizer_manager.ReqState.input_token_ids_logprobs_idx">input_token_ids_logprobs_idx</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.input_token_ids_logprobs_val" href="#sglang.srt.managers.tokenizer_manager.ReqState.input_token_ids_logprobs_val">input_token_ids_logprobs_val</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.input_token_logprobs_idx" href="#sglang.srt.managers.tokenizer_manager.ReqState.input_token_logprobs_idx">input_token_logprobs_idx</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.input_token_logprobs_val" href="#sglang.srt.managers.tokenizer_manager.ReqState.input_token_logprobs_val">input_token_logprobs_val</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.input_top_logprobs_idx" href="#sglang.srt.managers.tokenizer_manager.ReqState.input_top_logprobs_idx">input_top_logprobs_idx</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.input_top_logprobs_val" href="#sglang.srt.managers.tokenizer_manager.ReqState.input_top_logprobs_val">input_top_logprobs_val</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.last_completion_tokens" href="#sglang.srt.managers.tokenizer_manager.ReqState.last_completion_tokens">last_completion_tokens</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.last_output_offset" href="#sglang.srt.managers.tokenizer_manager.ReqState.last_output_offset">last_output_offset</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.last_time" href="#sglang.srt.managers.tokenizer_manager.ReqState.last_time">last_time</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.obj" href="#sglang.srt.managers.tokenizer_manager.ReqState.obj">obj</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.out_list" href="#sglang.srt.managers.tokenizer_manager.ReqState.out_list">out_list</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.output_ids" href="#sglang.srt.managers.tokenizer_manager.ReqState.output_ids">output_ids</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.output_token_ids_logprobs_idx" href="#sglang.srt.managers.tokenizer_manager.ReqState.output_token_ids_logprobs_idx">output_token_ids_logprobs_idx</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.output_token_ids_logprobs_val" href="#sglang.srt.managers.tokenizer_manager.ReqState.output_token_ids_logprobs_val">output_token_ids_logprobs_val</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.output_token_logprobs_idx" href="#sglang.srt.managers.tokenizer_manager.ReqState.output_token_logprobs_idx">output_token_logprobs_idx</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.output_token_logprobs_val" href="#sglang.srt.managers.tokenizer_manager.ReqState.output_token_logprobs_val">output_token_logprobs_val</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.output_top_logprobs_idx" href="#sglang.srt.managers.tokenizer_manager.ReqState.output_top_logprobs_idx">output_top_logprobs_idx</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.output_top_logprobs_val" href="#sglang.srt.managers.tokenizer_manager.ReqState.output_top_logprobs_val">output_top_logprobs_val</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ReqState.text" href="#sglang.srt.managers.tokenizer_manager.ReqState.text">text</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.managers.tokenizer_manager.ServerStatus" href="#sglang.srt.managers.tokenizer_manager.ServerStatus">ServerStatus</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.tokenizer_manager.ServerStatus.Starting" href="#sglang.srt.managers.tokenizer_manager.ServerStatus.Starting">Starting</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ServerStatus.UnHealthy" href="#sglang.srt.managers.tokenizer_manager.ServerStatus.UnHealthy">UnHealthy</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.ServerStatus.Up" href="#sglang.srt.managers.tokenizer_manager.ServerStatus.Up">Up</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.managers.tokenizer_manager.SignalHandler" href="#sglang.srt.managers.tokenizer_manager.SignalHandler">SignalHandler</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.tokenizer_manager.SignalHandler.running_phase_sigquit_handler" href="#sglang.srt.managers.tokenizer_manager.SignalHandler.running_phase_sigquit_handler">running_phase_sigquit_handler</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.SignalHandler.sigterm_handler" href="#sglang.srt.managers.tokenizer_manager.SignalHandler.sigterm_handler">sigterm_handler</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager">TokenizerManager</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.abort_request" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.abort_request">abort_request</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.auto_create_handle_loop" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.auto_create_handle_loop">auto_create_handle_loop</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.clear_hicache_storage" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.clear_hicache_storage">clear_hicache_storage</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.close_session" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.close_session">close_session</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.collect_metrics" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.collect_metrics">collect_metrics</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.configure_logging" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.configure_logging">configure_logging</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.continue_generation" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.continue_generation">continue_generation</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.convert_logprob_style" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.convert_logprob_style">convert_logprob_style</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.create_abort_task" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.create_abort_task">create_abort_task</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.detokenize_logprob_tokens" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.detokenize_logprob_tokens">detokenize_logprob_tokens</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.detokenize_top_logprobs_tokens" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.detokenize_top_logprobs_tokens">detokenize_top_logprobs_tokens</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.dump_expert_distribution_record" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.dump_expert_distribution_record">dump_expert_distribution_record</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.dump_requests" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.dump_requests">dump_requests</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.dump_requests_before_crash" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.dump_requests_before_crash">dump_requests_before_crash</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.flush_cache" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.flush_cache">flush_cache</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.freeze_gc" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.freeze_gc">freeze_gc</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.generate_request" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.generate_request">generate_request</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.get_internal_state" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.get_internal_state">get_internal_state</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.get_load" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.get_load">get_load</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.get_log_request_metadata" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.get_log_request_metadata">get_log_request_metadata</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.get_weights_by_name" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.get_weights_by_name">get_weights_by_name</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.handle_loop" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.handle_loop">handle_loop</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.init_weights_update_group" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.init_weights_update_group">init_weights_update_group</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.load_lora_adapter" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.load_lora_adapter">load_lora_adapter</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.open_session" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.open_session">open_session</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.pause_generation" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.pause_generation">pause_generation</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.record_request_for_crash_dump" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.record_request_for_crash_dump">record_request_for_crash_dump</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.release_memory_occupation" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.release_memory_occupation">release_memory_occupation</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.resume_memory_occupation" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.resume_memory_occupation">resume_memory_occupation</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.score_request" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.score_request">score_request</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.set_internal_state" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.set_internal_state">set_internal_state</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.sigterm_watchdog" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.sigterm_watchdog">sigterm_watchdog</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.slow_down" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.slow_down">slow_down</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.start_expert_distribution_record" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.start_expert_distribution_record">start_expert_distribution_record</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.start_profile" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.start_profile">start_profile</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.stop_expert_distribution_record" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.stop_expert_distribution_record">stop_expert_distribution_record</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.stop_profile" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.stop_profile">stop_profile</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.unload_lora_adapter" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.unload_lora_adapter">unload_lora_adapter</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.update_weights_from_disk" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.update_weights_from_disk">update_weights_from_disk</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.update_weights_from_distributed" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.update_weights_from_distributed">update_weights_from_distributed</a></code></li>
<li><code><a title="sglang.srt.managers.tokenizer_manager.TokenizerManager.update_weights_from_tensor" href="#sglang.srt.managers.tokenizer_manager.TokenizerManager.update_weights_from_tensor">update_weights_from_tensor</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
