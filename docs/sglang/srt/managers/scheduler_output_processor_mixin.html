<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.managers.scheduler_output_processor_mixin API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.managers.scheduler_output_processor_mixin</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin"><code class="flex name class">
<span>class <span class="ident">SchedulerOutputProcessorMixin</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SchedulerOutputProcessorMixin:
    &#34;&#34;&#34;
    This class implements the output processing logic for Scheduler.
    We put them into a separate file to make the `scheduler.py` shorter.
    &#34;&#34;&#34;

    def process_batch_result_prefill(
        self: Scheduler,
        batch: ScheduleBatch,
        result: Union[GenerationBatchResult, EmbeddingBatchResult],
        launch_done: Optional[threading.Event] = None,
    ):
        skip_stream_req = None

        if self.is_generation:
            (
                logits_output,
                next_token_ids,
                extend_input_len_per_req,
                extend_logprob_start_len_per_req,
            ) = (
                result.logits_output,
                result.next_token_ids,
                result.extend_input_len_per_req,
                result.extend_logprob_start_len_per_req,
            )

            if self.enable_overlap:
                logits_output, next_token_ids, _ = (
                    self.tp_worker.resolve_last_batch_result(launch_done)
                )
            else:
                # Move next_token_ids and logprobs to cpu
                next_token_ids = next_token_ids.tolist()
                if batch.return_logprob:
                    if logits_output.next_token_logprobs is not None:
                        logits_output.next_token_logprobs = (
                            logits_output.next_token_logprobs.tolist()
                        )
                    if logits_output.input_token_logprobs is not None:
                        logits_output.input_token_logprobs = tuple(
                            logits_output.input_token_logprobs.tolist()
                        )

            hidden_state_offset = 0

            # Check finish conditions
            logprob_pt = 0
            for i, (req, next_token_id) in enumerate(zip(batch.reqs, next_token_ids)):
                if req.is_retracted:
                    continue

                if self.is_mixed_chunk and self.enable_overlap and req.finished():
                    # Free the one delayed token for the mixed decode batch
                    j = len(batch.out_cache_loc) - len(batch.reqs) + i
                    self.token_to_kv_pool_allocator.free(batch.out_cache_loc[j : j + 1])
                    continue

                if req.is_chunked &lt;= 0:
                    # req output_ids are set here
                    req.output_ids.append(next_token_id)
                    req.check_finished()

                    if req.finished():
                        self.tree_cache.cache_finished_req(req)
                        req.time_stats.completion_time = time.time()
                    elif not batch.decoding_reqs or req not in batch.decoding_reqs:
                        # This updates radix so others can match
                        self.tree_cache.cache_unfinished_req(req)

                    if batch.return_logprob:
                        assert extend_logprob_start_len_per_req is not None
                        assert extend_input_len_per_req is not None
                        extend_logprob_start_len = extend_logprob_start_len_per_req[i]
                        extend_input_len = extend_input_len_per_req[i]
                        num_input_logprobs = extend_input_len - extend_logprob_start_len
                        if req.return_logprob:
                            self.add_logprob_return_values(
                                i,
                                req,
                                logprob_pt,
                                next_token_ids,
                                num_input_logprobs,
                                logits_output,
                            )
                        logprob_pt += num_input_logprobs

                    if (
                        req.return_hidden_states
                        and logits_output.hidden_states is not None
                    ):
                        req.hidden_states.append(
                            logits_output.hidden_states[
                                hidden_state_offset : (
                                    hidden_state_offset := hidden_state_offset
                                    + len(req.origin_input_ids)
                                )
                            ]
                            .cpu()
                            .clone()
                            .tolist()
                        )

                    if req.grammar is not None:
                        # FIXME: this try-except block is for handling unexpected xgrammar issue.
                        try:
                            req.grammar.accept_token(next_token_id)
                        except ValueError as e:
                            # Grammar accept_token can raise ValueError if the token is not in the grammar.
                            # This can happen if the grammar is not set correctly or the token is invalid.
                            logger.error(
                                f&#34;Grammar accept_token failed for req {req.rid} with token {next_token_id}: {e}&#34;
                            )
                            self.abort_request(AbortReq(req.rid))
                        req.grammar.finished = req.finished()
                else:
                    # being chunked reqs&#39; prefill is not finished
                    req.is_chunked -= 1
                    # There is only at most one request being currently chunked.
                    # Because this request does not finish prefill,
                    # we don&#39;t want to stream the request currently being chunked.
                    skip_stream_req = req

                    # Incrementally update input logprobs.
                    if batch.return_logprob:
                        extend_logprob_start_len = extend_logprob_start_len_per_req[i]
                        extend_input_len = extend_input_len_per_req[i]
                        if extend_logprob_start_len &lt; extend_input_len:
                            # Update input logprobs.
                            num_input_logprobs = (
                                extend_input_len - extend_logprob_start_len
                            )
                            if req.return_logprob:
                                self.add_input_logprob_return_values(
                                    i,
                                    req,
                                    logits_output,
                                    logprob_pt,
                                    num_input_logprobs,
                                    last_prefill_chunk=False,
                                )
                            logprob_pt += num_input_logprobs

            self.set_next_batch_sampling_info_done(batch)

        else:  # embedding or reward model
            embeddings, bid = result.embeddings, result.bid
            embeddings = embeddings.tolist()

            # Check finish conditions
            for i, req in enumerate(batch.reqs):
                if req.is_retracted:
                    continue

                req.embedding = embeddings[i]
                if req.is_chunked &lt;= 0:
                    # Dummy output token for embedding models
                    req.output_ids.append(0)
                    req.check_finished()

                    if req.finished():
                        self.tree_cache.cache_finished_req(req)
                    else:
                        self.tree_cache.cache_unfinished_req(req)
                else:
                    # being chunked reqs&#39; prefill is not finished
                    req.is_chunked -= 1

        self.stream_output(batch.reqs, batch.return_logprob, skip_stream_req)

    def process_batch_result_decode(
        self: Scheduler,
        batch: ScheduleBatch,
        result: GenerationBatchResult,
        launch_done: Optional[threading.Event] = None,
    ):
        logits_output, next_token_ids, can_run_cuda_graph = (
            result.logits_output,
            result.next_token_ids,
            result.can_run_cuda_graph,
        )
        self.num_generated_tokens += len(batch.reqs)

        if self.enable_overlap:
            logits_output, next_token_ids, can_run_cuda_graph = (
                self.tp_worker.resolve_last_batch_result(launch_done)
            )
            next_token_logprobs = logits_output.next_token_logprobs
        elif batch.spec_algorithm.is_none():
            # spec decoding handles output logprobs inside verify process.
            next_token_ids = next_token_ids.tolist()
            if batch.return_logprob:
                next_token_logprobs = logits_output.next_token_logprobs.tolist()

        self.token_to_kv_pool_allocator.free_group_begin()

        # Check finish condition
        # NOTE: the length of reqs and next_token_ids don&#39;t match if it is spec decoding.
        # We should ignore using next_token_ids for spec decoding cases.
        for i, (req, next_token_id) in enumerate(zip(batch.reqs, next_token_ids)):
            if req.is_retracted:
                continue

            if self.enable_overlap and req.finished():
                # Free the one extra delayed token
                if self.page_size == 1:
                    self.token_to_kv_pool_allocator.free(batch.out_cache_loc[i : i + 1])
                else:
                    # Only free when the extra token is in a new page
                    if (
                        len(req.origin_input_ids) + len(req.output_ids) - 1
                    ) % self.page_size == 0:
                        self.token_to_kv_pool_allocator.free(
                            batch.out_cache_loc[i : i + 1]
                        )
                continue

            if batch.spec_algorithm.is_none():
                # speculative worker will solve the output_ids in speculative decoding
                req.output_ids.append(next_token_id)

            req.check_finished()
            if req.finished():
                self.tree_cache.cache_finished_req(req)
                req.time_stats.completion_time = time.time()

            if req.return_logprob and batch.spec_algorithm.is_none():
                # speculative worker handles logprob in speculative decoding
                req.output_token_logprobs_val.append(next_token_logprobs[i])
                req.output_token_logprobs_idx.append(next_token_id)
                if req.top_logprobs_num &gt; 0:
                    req.output_top_logprobs_val.append(
                        logits_output.next_token_top_logprobs_val[i]
                    )
                    req.output_top_logprobs_idx.append(
                        logits_output.next_token_top_logprobs_idx[i]
                    )
                if req.token_ids_logprob is not None:
                    req.output_token_ids_logprobs_val.append(
                        logits_output.next_token_token_ids_logprobs_val[i]
                    )
                    req.output_token_ids_logprobs_idx.append(
                        logits_output.next_token_token_ids_logprobs_idx[i]
                    )

            if req.return_hidden_states and logits_output.hidden_states is not None:
                req.hidden_states.append(
                    logits_output.hidden_states[i].cpu().clone().tolist()
                )

            if req.grammar is not None and batch.spec_algorithm.is_none():
                # FIXME: this try-except block is for handling unexpected xgrammar issue.
                try:
                    req.grammar.accept_token(next_token_id)
                except ValueError as e:
                    # Grammar accept_token can raise ValueError if the token is not in the grammar.
                    # This can happen if the grammar is not set correctly or the token is invalid.
                    logger.error(
                        f&#34;Grammar accept_token failed for req {req.rid} with token {next_token_id}: {e}&#34;
                    )
                    self.abort_request(AbortReq(req.rid))
                req.grammar.finished = req.finished()

        self.set_next_batch_sampling_info_done(batch)
        self.stream_output(batch.reqs, batch.return_logprob)
        self.token_to_kv_pool_allocator.free_group_end()

        self.forward_ct_decode = (self.forward_ct_decode + 1) % (1 &lt;&lt; 30)
        if (
            self.current_scheduler_metrics_enabled()
            and self.forward_ct_decode % self.server_args.decode_log_interval == 0
        ):
            self.log_decode_stats(can_run_cuda_graph, running_batch=batch)

    def add_input_logprob_return_values(
        self: Scheduler,
        i: int,
        req: Req,
        output: LogitsProcessorOutput,
        logprob_pt: int,
        num_input_logprobs: int,
        last_prefill_chunk: bool,  # If True, it means prefill is finished.
    ):
        &#34;&#34;&#34;Incrementally add input logprobs to `req`.

        Args:
            i: The request index in a batch.
            req: The request. Input logprobs inside req are modified as a
                consequence of the API
            fill_ids: The prefill ids processed.
            output: Logit processor output that&#39;s used to compute input logprobs
            last_prefill_chunk: True if it is the last prefill (when chunked).
                Some of input logprob operation should only happen at the last
                prefill (e.g., computing input token logprobs).
        &#34;&#34;&#34;
        assert output.input_token_logprobs is not None
        if req.input_token_logprobs is None:
            req.input_token_logprobs = []
        if req.temp_input_top_logprobs_val is None:
            req.temp_input_top_logprobs_val = []
        if req.temp_input_top_logprobs_idx is None:
            req.temp_input_top_logprobs_idx = []
        if req.temp_input_token_ids_logprobs_val is None:
            req.temp_input_token_ids_logprobs_val = []
        if req.temp_input_token_ids_logprobs_idx is None:
            req.temp_input_token_ids_logprobs_idx = []

        if req.input_token_logprobs_val is not None:
            # The input logprob has been already computed. It only happens
            # upon retract.
            if req.top_logprobs_num &gt; 0:
                assert req.input_token_logprobs_val is not None
            return

        # Important for the performance.
        assert isinstance(output.input_token_logprobs, tuple)
        input_token_logprobs: Tuple[int] = output.input_token_logprobs
        input_token_logprobs = input_token_logprobs[
            logprob_pt : logprob_pt + num_input_logprobs
        ]
        req.input_token_logprobs.extend(input_token_logprobs)

        if req.top_logprobs_num &gt; 0:
            req.temp_input_top_logprobs_val.append(output.input_top_logprobs_val[i])
            req.temp_input_top_logprobs_idx.append(output.input_top_logprobs_idx[i])

        if req.token_ids_logprob is not None:
            req.temp_input_token_ids_logprobs_val.append(
                output.input_token_ids_logprobs_val[i]
            )
            req.temp_input_token_ids_logprobs_idx.append(
                output.input_token_ids_logprobs_idx[i]
            )

        if last_prefill_chunk:
            input_token_logprobs = req.input_token_logprobs
            req.input_token_logprobs = None
            assert req.input_token_logprobs_val is None
            assert req.input_token_logprobs_idx is None
            assert req.input_top_logprobs_val is None
            assert req.input_top_logprobs_idx is None

            # Compute input_token_logprobs_val
            # Always pad the first one with None.
            req.input_token_logprobs_val = [None]
            req.input_token_logprobs_val.extend(input_token_logprobs)
            # The last input logprob is for sampling, so just pop it out.
            req.input_token_logprobs_val.pop()

            # Compute input_token_logprobs_idx
            input_token_logprobs_idx = req.origin_input_ids[req.logprob_start_len :]
            # Clip the padded hash values from image tokens.
            # Otherwise, it will lead to detokenization errors.
            input_token_logprobs_idx = [
                x if x &lt; self.model_config.vocab_size - 1 else 0
                for x in input_token_logprobs_idx
            ]
            req.input_token_logprobs_idx = input_token_logprobs_idx

            if req.top_logprobs_num &gt; 0:
                req.input_top_logprobs_val = [None]
                req.input_top_logprobs_idx = [None]
                assert len(req.temp_input_token_ids_logprobs_val) == len(
                    req.temp_input_token_ids_logprobs_idx
                )
                for val, idx in zip(
                    req.temp_input_top_logprobs_val,
                    req.temp_input_top_logprobs_idx,
                    strict=True,
                ):
                    req.input_top_logprobs_val.extend(val)
                    req.input_top_logprobs_idx.extend(idx)

                # Last token is a sample token.
                req.input_top_logprobs_val.pop()
                req.input_top_logprobs_idx.pop()
                req.temp_input_top_logprobs_idx = None
                req.temp_input_top_logprobs_val = None

            if req.token_ids_logprob is not None:
                req.input_token_ids_logprobs_val = [None]
                req.input_token_ids_logprobs_idx = [None]

                for val, idx in zip(
                    req.temp_input_token_ids_logprobs_val,
                    req.temp_input_token_ids_logprobs_idx,
                    strict=True,
                ):
                    req.input_token_ids_logprobs_val.extend(val)
                    req.input_token_ids_logprobs_idx.extend(idx)

                # Last token is a sample token.
                req.input_token_ids_logprobs_val.pop()
                req.input_token_ids_logprobs_idx.pop()
                req.temp_input_token_ids_logprobs_idx = None
                req.temp_input_token_ids_logprobs_val = None

            if req.return_logprob:
                relevant_tokens_len = len(req.origin_input_ids) - req.logprob_start_len
                assert len(req.input_token_logprobs_val) == relevant_tokens_len
                assert len(req.input_token_logprobs_idx) == relevant_tokens_len
                if req.top_logprobs_num &gt; 0:
                    assert len(req.input_top_logprobs_val) == relevant_tokens_len
                    assert len(req.input_top_logprobs_idx) == relevant_tokens_len
                if req.token_ids_logprob is not None:
                    assert len(req.input_token_ids_logprobs_val) == relevant_tokens_len
                    assert len(req.input_token_ids_logprobs_idx) == relevant_tokens_len

    def add_logprob_return_values(
        self: Scheduler,
        i: int,
        req: Req,
        pt: int,
        next_token_ids: List[int],
        num_input_logprobs: int,
        output: LogitsProcessorOutput,
    ):
        &#34;&#34;&#34;Attach logprobs to the return values.&#34;&#34;&#34;
        req.output_token_logprobs_val.append(output.next_token_logprobs[i])
        req.output_token_logprobs_idx.append(next_token_ids[i])

        self.add_input_logprob_return_values(
            i, req, output, pt, num_input_logprobs, last_prefill_chunk=True
        )

        if req.top_logprobs_num &gt; 0:
            req.output_top_logprobs_val.append(output.next_token_top_logprobs_val[i])
            req.output_top_logprobs_idx.append(output.next_token_top_logprobs_idx[i])

        if req.token_ids_logprob is not None:
            req.output_token_ids_logprobs_val.append(
                output.next_token_token_ids_logprobs_val[i]
            )
            req.output_token_ids_logprobs_idx.append(
                output.next_token_token_ids_logprobs_idx[i]
            )

        return num_input_logprobs

    def stream_output(
        self: Scheduler,
        reqs: List[Req],
        return_logprob: bool,
        skip_req: Optional[Req] = None,
    ):
        &#34;&#34;&#34;Stream the output to detokenizer.&#34;&#34;&#34;
        if self.is_generation:
            self.stream_output_generation(reqs, return_logprob, skip_req)
        else:  # embedding or reward model
            self.stream_output_embedding(reqs)

    def stream_output_generation(
        self: Scheduler,
        reqs: List[Req],
        return_logprob: bool,
        skip_req: Optional[Req] = None,
    ):
        rids = []
        finished_reasons: List[BaseFinishReason] = []

        decoded_texts = []
        decode_ids_list = []
        read_offsets = []
        output_ids = []

        skip_special_tokens = []
        spaces_between_special_tokens = []
        no_stop_trim = []
        prompt_tokens = []
        completion_tokens = []
        cached_tokens = []
        spec_verify_ct = []
        output_hidden_states = None

        if return_logprob:
            input_token_logprobs_val = []
            input_token_logprobs_idx = []
            output_token_logprobs_val = []
            output_token_logprobs_idx = []
            input_top_logprobs_val = []
            input_top_logprobs_idx = []
            output_top_logprobs_val = []
            output_top_logprobs_idx = []
            input_token_ids_logprobs_val = []
            input_token_ids_logprobs_idx = []
            output_token_ids_logprobs_val = []
            output_token_ids_logprobs_idx = []
        else:
            input_token_logprobs_val = input_token_logprobs_idx = (
                output_token_logprobs_val
            ) = output_token_logprobs_idx = input_top_logprobs_val = (
                input_top_logprobs_idx
            ) = output_top_logprobs_val = output_top_logprobs_idx = (
                input_token_ids_logprobs_val
            ) = input_token_ids_logprobs_idx = output_token_ids_logprobs_val = (
                output_token_ids_logprobs_idx
            ) = None

        for req in reqs:
            if req is skip_req:
                continue

            # Multimodal partial stream chunks break the detokenizer, so drop aborted requests here.
            if self.model_config.is_multimodal_gen and req.to_abort:
                continue

            if req.finished():
                if req.finished_output:
                    # With the overlap schedule, a request will try to output twice and hit this line twice
                    # because of the one additional delayed token. This &#34;continue&#34; prevented the dummy output.
                    continue
                req.finished_output = True
                should_output = True
            else:
                if req.stream:
                    stream_interval = (
                        req.sampling_params.stream_interval or self.stream_interval
                    )
                    should_output = (
                        len(req.output_ids) % stream_interval == 1
                        if not self.model_config.is_multimodal_gen
                        and stream_interval &gt; 1
                        else len(req.output_ids) % stream_interval == 0
                    )
                else:
                    should_output = (
                        len(req.output_ids) % DEFAULT_FORCE_STREAM_INTERVAL == 0
                        if not self.model_config.is_multimodal_gen
                        else False
                    )

            if should_output:
                send_token_offset = req.send_token_offset
                send_output_token_logprobs_offset = (
                    req.send_output_token_logprobs_offset
                )
                rids.append(req.rid)
                finished_reasons.append(
                    req.finished_reason.to_json() if req.finished_reason else None
                )
                decoded_texts.append(req.decoded_text)
                decode_ids, read_offset = req.init_incremental_detokenize()

                if self.model_config.is_multimodal_gen:
                    decode_ids_list.append(decode_ids)
                else:
                    decode_ids_list.append(decode_ids[req.send_decode_id_offset :])

                req.send_decode_id_offset = len(decode_ids)
                read_offsets.append(read_offset)
                output_ids.append(req.output_ids[send_token_offset:])
                req.send_token_offset = len(req.output_ids)
                skip_special_tokens.append(req.sampling_params.skip_special_tokens)
                spaces_between_special_tokens.append(
                    req.sampling_params.spaces_between_special_tokens
                )
                no_stop_trim.append(req.sampling_params.no_stop_trim)
                prompt_tokens.append(len(req.origin_input_ids))
                completion_tokens.append(len(req.output_ids))
                cached_tokens.append(req.cached_tokens)

                if not self.spec_algorithm.is_none():
                    spec_verify_ct.append(req.spec_verify_ct)

                if return_logprob:
                    if (
                        req.return_logprob
                        and not req.input_logprob_sent
                        # Decode server does not send input logprobs
                        and self.disaggregation_mode != DisaggregationMode.DECODE
                    ):
                        input_token_logprobs_val.append(req.input_token_logprobs_val)
                        input_token_logprobs_idx.append(req.input_token_logprobs_idx)
                        input_top_logprobs_val.append(req.input_top_logprobs_val)
                        input_top_logprobs_idx.append(req.input_top_logprobs_idx)
                        input_token_ids_logprobs_val.append(
                            req.input_token_ids_logprobs_val
                        )
                        input_token_ids_logprobs_idx.append(
                            req.input_token_ids_logprobs_idx
                        )
                        req.input_logprob_sent = True
                    else:
                        input_token_logprobs_val.append([])
                        input_token_logprobs_idx.append([])
                        input_top_logprobs_val.append([])
                        input_top_logprobs_idx.append([])
                        input_token_ids_logprobs_val.append([])
                        input_token_ids_logprobs_idx.append([])

                    if req.return_logprob:
                        output_token_logprobs_val.append(
                            req.output_token_logprobs_val[
                                send_output_token_logprobs_offset:
                            ]
                        )
                        output_token_logprobs_idx.append(
                            req.output_token_logprobs_idx[
                                send_output_token_logprobs_offset:
                            ]
                        )
                        output_top_logprobs_val.append(
                            req.output_top_logprobs_val[
                                send_output_token_logprobs_offset:
                            ]
                        )
                        output_top_logprobs_idx.append(
                            req.output_top_logprobs_idx[
                                send_output_token_logprobs_offset:
                            ]
                        )
                        output_token_ids_logprobs_val.append(
                            req.output_token_ids_logprobs_val[
                                send_output_token_logprobs_offset:
                            ]
                        )
                        output_token_ids_logprobs_idx.append(
                            req.output_token_ids_logprobs_idx[
                                send_output_token_logprobs_offset:
                            ]
                        )
                        req.send_output_token_logprobs_offset = len(
                            req.output_token_logprobs_val
                        )
                    else:
                        output_token_logprobs_val.append([])
                        output_token_logprobs_idx.append([])
                        output_top_logprobs_val.append([])
                        output_top_logprobs_idx.append([])
                        output_token_ids_logprobs_val.append([])
                        output_token_ids_logprobs_idx.append([])

                if req.return_hidden_states:
                    if output_hidden_states is None:
                        output_hidden_states = []
                    output_hidden_states.append(req.hidden_states)

            if (
                req.finished()
                and self.tp_rank == 0
                and self.server_args.enable_request_time_stats_logging
            ):
                req.log_time_stats()

        # Send to detokenizer
        if rids:
            if self.model_config.is_multimodal_gen:
                return

            self.send_to_detokenizer.send_pyobj(
                BatchTokenIDOut(
                    rids,
                    finished_reasons,
                    decoded_texts,
                    decode_ids_list,
                    read_offsets,
                    output_ids,
                    skip_special_tokens,
                    spaces_between_special_tokens,
                    no_stop_trim,
                    prompt_tokens,
                    completion_tokens,
                    cached_tokens,
                    spec_verify_ct,
                    input_token_logprobs_val,
                    input_token_logprobs_idx,
                    output_token_logprobs_val,
                    output_token_logprobs_idx,
                    input_top_logprobs_val,
                    input_top_logprobs_idx,
                    output_top_logprobs_val,
                    output_top_logprobs_idx,
                    input_token_ids_logprobs_val,
                    input_token_ids_logprobs_idx,
                    output_token_ids_logprobs_val,
                    output_token_ids_logprobs_idx,
                    output_hidden_states,
                )
            )

    def stream_output_embedding(self: Scheduler, reqs: List[Req]):
        rids = []
        finished_reasons: List[BaseFinishReason] = []

        embeddings = []
        prompt_tokens = []
        cached_tokens = []
        for req in reqs:
            if req.finished():
                rids.append(req.rid)
                finished_reasons.append(req.finished_reason.to_json())
                embeddings.append(req.embedding)
                prompt_tokens.append(len(req.origin_input_ids))
                cached_tokens.append(req.cached_tokens)
        self.send_to_detokenizer.send_pyobj(
            BatchEmbeddingOut(
                rids, finished_reasons, embeddings, prompt_tokens, cached_tokens
            )
        )</code></pre>
</details>
<div class="desc"><p>This class implements the output processing logic for Scheduler.
We put them into a separate file to make the <code>scheduler.py</code> shorter.</p></div>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.managers.scheduler.Scheduler" href="scheduler.html#sglang.srt.managers.scheduler.Scheduler">Scheduler</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.add_input_logprob_return_values"><code class="name flex">
<span>def <span class="ident">add_input_logprob_return_values</span></span>(<span>self: Scheduler,<br>i: int,<br>req: Req,<br>output: LogitsProcessorOutput,<br>logprob_pt: int,<br>num_input_logprobs: int,<br>last_prefill_chunk: bool)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_input_logprob_return_values(
    self: Scheduler,
    i: int,
    req: Req,
    output: LogitsProcessorOutput,
    logprob_pt: int,
    num_input_logprobs: int,
    last_prefill_chunk: bool,  # If True, it means prefill is finished.
):
    &#34;&#34;&#34;Incrementally add input logprobs to `req`.

    Args:
        i: The request index in a batch.
        req: The request. Input logprobs inside req are modified as a
            consequence of the API
        fill_ids: The prefill ids processed.
        output: Logit processor output that&#39;s used to compute input logprobs
        last_prefill_chunk: True if it is the last prefill (when chunked).
            Some of input logprob operation should only happen at the last
            prefill (e.g., computing input token logprobs).
    &#34;&#34;&#34;
    assert output.input_token_logprobs is not None
    if req.input_token_logprobs is None:
        req.input_token_logprobs = []
    if req.temp_input_top_logprobs_val is None:
        req.temp_input_top_logprobs_val = []
    if req.temp_input_top_logprobs_idx is None:
        req.temp_input_top_logprobs_idx = []
    if req.temp_input_token_ids_logprobs_val is None:
        req.temp_input_token_ids_logprobs_val = []
    if req.temp_input_token_ids_logprobs_idx is None:
        req.temp_input_token_ids_logprobs_idx = []

    if req.input_token_logprobs_val is not None:
        # The input logprob has been already computed. It only happens
        # upon retract.
        if req.top_logprobs_num &gt; 0:
            assert req.input_token_logprobs_val is not None
        return

    # Important for the performance.
    assert isinstance(output.input_token_logprobs, tuple)
    input_token_logprobs: Tuple[int] = output.input_token_logprobs
    input_token_logprobs = input_token_logprobs[
        logprob_pt : logprob_pt + num_input_logprobs
    ]
    req.input_token_logprobs.extend(input_token_logprobs)

    if req.top_logprobs_num &gt; 0:
        req.temp_input_top_logprobs_val.append(output.input_top_logprobs_val[i])
        req.temp_input_top_logprobs_idx.append(output.input_top_logprobs_idx[i])

    if req.token_ids_logprob is not None:
        req.temp_input_token_ids_logprobs_val.append(
            output.input_token_ids_logprobs_val[i]
        )
        req.temp_input_token_ids_logprobs_idx.append(
            output.input_token_ids_logprobs_idx[i]
        )

    if last_prefill_chunk:
        input_token_logprobs = req.input_token_logprobs
        req.input_token_logprobs = None
        assert req.input_token_logprobs_val is None
        assert req.input_token_logprobs_idx is None
        assert req.input_top_logprobs_val is None
        assert req.input_top_logprobs_idx is None

        # Compute input_token_logprobs_val
        # Always pad the first one with None.
        req.input_token_logprobs_val = [None]
        req.input_token_logprobs_val.extend(input_token_logprobs)
        # The last input logprob is for sampling, so just pop it out.
        req.input_token_logprobs_val.pop()

        # Compute input_token_logprobs_idx
        input_token_logprobs_idx = req.origin_input_ids[req.logprob_start_len :]
        # Clip the padded hash values from image tokens.
        # Otherwise, it will lead to detokenization errors.
        input_token_logprobs_idx = [
            x if x &lt; self.model_config.vocab_size - 1 else 0
            for x in input_token_logprobs_idx
        ]
        req.input_token_logprobs_idx = input_token_logprobs_idx

        if req.top_logprobs_num &gt; 0:
            req.input_top_logprobs_val = [None]
            req.input_top_logprobs_idx = [None]
            assert len(req.temp_input_token_ids_logprobs_val) == len(
                req.temp_input_token_ids_logprobs_idx
            )
            for val, idx in zip(
                req.temp_input_top_logprobs_val,
                req.temp_input_top_logprobs_idx,
                strict=True,
            ):
                req.input_top_logprobs_val.extend(val)
                req.input_top_logprobs_idx.extend(idx)

            # Last token is a sample token.
            req.input_top_logprobs_val.pop()
            req.input_top_logprobs_idx.pop()
            req.temp_input_top_logprobs_idx = None
            req.temp_input_top_logprobs_val = None

        if req.token_ids_logprob is not None:
            req.input_token_ids_logprobs_val = [None]
            req.input_token_ids_logprobs_idx = [None]

            for val, idx in zip(
                req.temp_input_token_ids_logprobs_val,
                req.temp_input_token_ids_logprobs_idx,
                strict=True,
            ):
                req.input_token_ids_logprobs_val.extend(val)
                req.input_token_ids_logprobs_idx.extend(idx)

            # Last token is a sample token.
            req.input_token_ids_logprobs_val.pop()
            req.input_token_ids_logprobs_idx.pop()
            req.temp_input_token_ids_logprobs_idx = None
            req.temp_input_token_ids_logprobs_val = None

        if req.return_logprob:
            relevant_tokens_len = len(req.origin_input_ids) - req.logprob_start_len
            assert len(req.input_token_logprobs_val) == relevant_tokens_len
            assert len(req.input_token_logprobs_idx) == relevant_tokens_len
            if req.top_logprobs_num &gt; 0:
                assert len(req.input_top_logprobs_val) == relevant_tokens_len
                assert len(req.input_top_logprobs_idx) == relevant_tokens_len
            if req.token_ids_logprob is not None:
                assert len(req.input_token_ids_logprobs_val) == relevant_tokens_len
                assert len(req.input_token_ids_logprobs_idx) == relevant_tokens_len</code></pre>
</details>
<div class="desc"><p>Incrementally add input logprobs to <code>req</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>i</code></strong></dt>
<dd>The request index in a batch.</dd>
<dt><strong><code>req</code></strong></dt>
<dd>The request. Input logprobs inside req are modified as a
consequence of the API</dd>
<dt><strong><code>fill_ids</code></strong></dt>
<dd>The prefill ids processed.</dd>
<dt><strong><code>output</code></strong></dt>
<dd>Logit processor output that's used to compute input logprobs</dd>
<dt><strong><code>last_prefill_chunk</code></strong></dt>
<dd>True if it is the last prefill (when chunked).
Some of input logprob operation should only happen at the last
prefill (e.g., computing input token logprobs).</dd>
</dl></div>
</dd>
<dt id="sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.add_logprob_return_values"><code class="name flex">
<span>def <span class="ident">add_logprob_return_values</span></span>(<span>self: Scheduler,<br>i: int,<br>req: Req,<br>pt: int,<br>next_token_ids: List[int],<br>num_input_logprobs: int,<br>output: LogitsProcessorOutput)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_logprob_return_values(
    self: Scheduler,
    i: int,
    req: Req,
    pt: int,
    next_token_ids: List[int],
    num_input_logprobs: int,
    output: LogitsProcessorOutput,
):
    &#34;&#34;&#34;Attach logprobs to the return values.&#34;&#34;&#34;
    req.output_token_logprobs_val.append(output.next_token_logprobs[i])
    req.output_token_logprobs_idx.append(next_token_ids[i])

    self.add_input_logprob_return_values(
        i, req, output, pt, num_input_logprobs, last_prefill_chunk=True
    )

    if req.top_logprobs_num &gt; 0:
        req.output_top_logprobs_val.append(output.next_token_top_logprobs_val[i])
        req.output_top_logprobs_idx.append(output.next_token_top_logprobs_idx[i])

    if req.token_ids_logprob is not None:
        req.output_token_ids_logprobs_val.append(
            output.next_token_token_ids_logprobs_val[i]
        )
        req.output_token_ids_logprobs_idx.append(
            output.next_token_token_ids_logprobs_idx[i]
        )

    return num_input_logprobs</code></pre>
</details>
<div class="desc"><p>Attach logprobs to the return values.</p></div>
</dd>
<dt id="sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.process_batch_result_decode"><code class="name flex">
<span>def <span class="ident">process_batch_result_decode</span></span>(<span>self: Scheduler,<br>batch: ScheduleBatch,<br>result: GenerationBatchResult,<br>launch_done: Optional[threading.Event] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_batch_result_decode(
    self: Scheduler,
    batch: ScheduleBatch,
    result: GenerationBatchResult,
    launch_done: Optional[threading.Event] = None,
):
    logits_output, next_token_ids, can_run_cuda_graph = (
        result.logits_output,
        result.next_token_ids,
        result.can_run_cuda_graph,
    )
    self.num_generated_tokens += len(batch.reqs)

    if self.enable_overlap:
        logits_output, next_token_ids, can_run_cuda_graph = (
            self.tp_worker.resolve_last_batch_result(launch_done)
        )
        next_token_logprobs = logits_output.next_token_logprobs
    elif batch.spec_algorithm.is_none():
        # spec decoding handles output logprobs inside verify process.
        next_token_ids = next_token_ids.tolist()
        if batch.return_logprob:
            next_token_logprobs = logits_output.next_token_logprobs.tolist()

    self.token_to_kv_pool_allocator.free_group_begin()

    # Check finish condition
    # NOTE: the length of reqs and next_token_ids don&#39;t match if it is spec decoding.
    # We should ignore using next_token_ids for spec decoding cases.
    for i, (req, next_token_id) in enumerate(zip(batch.reqs, next_token_ids)):
        if req.is_retracted:
            continue

        if self.enable_overlap and req.finished():
            # Free the one extra delayed token
            if self.page_size == 1:
                self.token_to_kv_pool_allocator.free(batch.out_cache_loc[i : i + 1])
            else:
                # Only free when the extra token is in a new page
                if (
                    len(req.origin_input_ids) + len(req.output_ids) - 1
                ) % self.page_size == 0:
                    self.token_to_kv_pool_allocator.free(
                        batch.out_cache_loc[i : i + 1]
                    )
            continue

        if batch.spec_algorithm.is_none():
            # speculative worker will solve the output_ids in speculative decoding
            req.output_ids.append(next_token_id)

        req.check_finished()
        if req.finished():
            self.tree_cache.cache_finished_req(req)
            req.time_stats.completion_time = time.time()

        if req.return_logprob and batch.spec_algorithm.is_none():
            # speculative worker handles logprob in speculative decoding
            req.output_token_logprobs_val.append(next_token_logprobs[i])
            req.output_token_logprobs_idx.append(next_token_id)
            if req.top_logprobs_num &gt; 0:
                req.output_top_logprobs_val.append(
                    logits_output.next_token_top_logprobs_val[i]
                )
                req.output_top_logprobs_idx.append(
                    logits_output.next_token_top_logprobs_idx[i]
                )
            if req.token_ids_logprob is not None:
                req.output_token_ids_logprobs_val.append(
                    logits_output.next_token_token_ids_logprobs_val[i]
                )
                req.output_token_ids_logprobs_idx.append(
                    logits_output.next_token_token_ids_logprobs_idx[i]
                )

        if req.return_hidden_states and logits_output.hidden_states is not None:
            req.hidden_states.append(
                logits_output.hidden_states[i].cpu().clone().tolist()
            )

        if req.grammar is not None and batch.spec_algorithm.is_none():
            # FIXME: this try-except block is for handling unexpected xgrammar issue.
            try:
                req.grammar.accept_token(next_token_id)
            except ValueError as e:
                # Grammar accept_token can raise ValueError if the token is not in the grammar.
                # This can happen if the grammar is not set correctly or the token is invalid.
                logger.error(
                    f&#34;Grammar accept_token failed for req {req.rid} with token {next_token_id}: {e}&#34;
                )
                self.abort_request(AbortReq(req.rid))
            req.grammar.finished = req.finished()

    self.set_next_batch_sampling_info_done(batch)
    self.stream_output(batch.reqs, batch.return_logprob)
    self.token_to_kv_pool_allocator.free_group_end()

    self.forward_ct_decode = (self.forward_ct_decode + 1) % (1 &lt;&lt; 30)
    if (
        self.current_scheduler_metrics_enabled()
        and self.forward_ct_decode % self.server_args.decode_log_interval == 0
    ):
        self.log_decode_stats(can_run_cuda_graph, running_batch=batch)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.process_batch_result_prefill"><code class="name flex">
<span>def <span class="ident">process_batch_result_prefill</span></span>(<span>self: Scheduler,<br>batch: ScheduleBatch,<br>result: Union[GenerationBatchResult, EmbeddingBatchResult],<br>launch_done: Optional[threading.Event] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_batch_result_prefill(
    self: Scheduler,
    batch: ScheduleBatch,
    result: Union[GenerationBatchResult, EmbeddingBatchResult],
    launch_done: Optional[threading.Event] = None,
):
    skip_stream_req = None

    if self.is_generation:
        (
            logits_output,
            next_token_ids,
            extend_input_len_per_req,
            extend_logprob_start_len_per_req,
        ) = (
            result.logits_output,
            result.next_token_ids,
            result.extend_input_len_per_req,
            result.extend_logprob_start_len_per_req,
        )

        if self.enable_overlap:
            logits_output, next_token_ids, _ = (
                self.tp_worker.resolve_last_batch_result(launch_done)
            )
        else:
            # Move next_token_ids and logprobs to cpu
            next_token_ids = next_token_ids.tolist()
            if batch.return_logprob:
                if logits_output.next_token_logprobs is not None:
                    logits_output.next_token_logprobs = (
                        logits_output.next_token_logprobs.tolist()
                    )
                if logits_output.input_token_logprobs is not None:
                    logits_output.input_token_logprobs = tuple(
                        logits_output.input_token_logprobs.tolist()
                    )

        hidden_state_offset = 0

        # Check finish conditions
        logprob_pt = 0
        for i, (req, next_token_id) in enumerate(zip(batch.reqs, next_token_ids)):
            if req.is_retracted:
                continue

            if self.is_mixed_chunk and self.enable_overlap and req.finished():
                # Free the one delayed token for the mixed decode batch
                j = len(batch.out_cache_loc) - len(batch.reqs) + i
                self.token_to_kv_pool_allocator.free(batch.out_cache_loc[j : j + 1])
                continue

            if req.is_chunked &lt;= 0:
                # req output_ids are set here
                req.output_ids.append(next_token_id)
                req.check_finished()

                if req.finished():
                    self.tree_cache.cache_finished_req(req)
                    req.time_stats.completion_time = time.time()
                elif not batch.decoding_reqs or req not in batch.decoding_reqs:
                    # This updates radix so others can match
                    self.tree_cache.cache_unfinished_req(req)

                if batch.return_logprob:
                    assert extend_logprob_start_len_per_req is not None
                    assert extend_input_len_per_req is not None
                    extend_logprob_start_len = extend_logprob_start_len_per_req[i]
                    extend_input_len = extend_input_len_per_req[i]
                    num_input_logprobs = extend_input_len - extend_logprob_start_len
                    if req.return_logprob:
                        self.add_logprob_return_values(
                            i,
                            req,
                            logprob_pt,
                            next_token_ids,
                            num_input_logprobs,
                            logits_output,
                        )
                    logprob_pt += num_input_logprobs

                if (
                    req.return_hidden_states
                    and logits_output.hidden_states is not None
                ):
                    req.hidden_states.append(
                        logits_output.hidden_states[
                            hidden_state_offset : (
                                hidden_state_offset := hidden_state_offset
                                + len(req.origin_input_ids)
                            )
                        ]
                        .cpu()
                        .clone()
                        .tolist()
                    )

                if req.grammar is not None:
                    # FIXME: this try-except block is for handling unexpected xgrammar issue.
                    try:
                        req.grammar.accept_token(next_token_id)
                    except ValueError as e:
                        # Grammar accept_token can raise ValueError if the token is not in the grammar.
                        # This can happen if the grammar is not set correctly or the token is invalid.
                        logger.error(
                            f&#34;Grammar accept_token failed for req {req.rid} with token {next_token_id}: {e}&#34;
                        )
                        self.abort_request(AbortReq(req.rid))
                    req.grammar.finished = req.finished()
            else:
                # being chunked reqs&#39; prefill is not finished
                req.is_chunked -= 1
                # There is only at most one request being currently chunked.
                # Because this request does not finish prefill,
                # we don&#39;t want to stream the request currently being chunked.
                skip_stream_req = req

                # Incrementally update input logprobs.
                if batch.return_logprob:
                    extend_logprob_start_len = extend_logprob_start_len_per_req[i]
                    extend_input_len = extend_input_len_per_req[i]
                    if extend_logprob_start_len &lt; extend_input_len:
                        # Update input logprobs.
                        num_input_logprobs = (
                            extend_input_len - extend_logprob_start_len
                        )
                        if req.return_logprob:
                            self.add_input_logprob_return_values(
                                i,
                                req,
                                logits_output,
                                logprob_pt,
                                num_input_logprobs,
                                last_prefill_chunk=False,
                            )
                        logprob_pt += num_input_logprobs

        self.set_next_batch_sampling_info_done(batch)

    else:  # embedding or reward model
        embeddings, bid = result.embeddings, result.bid
        embeddings = embeddings.tolist()

        # Check finish conditions
        for i, req in enumerate(batch.reqs):
            if req.is_retracted:
                continue

            req.embedding = embeddings[i]
            if req.is_chunked &lt;= 0:
                # Dummy output token for embedding models
                req.output_ids.append(0)
                req.check_finished()

                if req.finished():
                    self.tree_cache.cache_finished_req(req)
                else:
                    self.tree_cache.cache_unfinished_req(req)
            else:
                # being chunked reqs&#39; prefill is not finished
                req.is_chunked -= 1

    self.stream_output(batch.reqs, batch.return_logprob, skip_stream_req)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.stream_output"><code class="name flex">
<span>def <span class="ident">stream_output</span></span>(<span>self: Scheduler,<br>reqs: List[Req],<br>return_logprob: bool,<br>skip_req: Optional[Req] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stream_output(
    self: Scheduler,
    reqs: List[Req],
    return_logprob: bool,
    skip_req: Optional[Req] = None,
):
    &#34;&#34;&#34;Stream the output to detokenizer.&#34;&#34;&#34;
    if self.is_generation:
        self.stream_output_generation(reqs, return_logprob, skip_req)
    else:  # embedding or reward model
        self.stream_output_embedding(reqs)</code></pre>
</details>
<div class="desc"><p>Stream the output to detokenizer.</p></div>
</dd>
<dt id="sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.stream_output_embedding"><code class="name flex">
<span>def <span class="ident">stream_output_embedding</span></span>(<span>self: Scheduler, reqs: List[Req])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stream_output_embedding(self: Scheduler, reqs: List[Req]):
    rids = []
    finished_reasons: List[BaseFinishReason] = []

    embeddings = []
    prompt_tokens = []
    cached_tokens = []
    for req in reqs:
        if req.finished():
            rids.append(req.rid)
            finished_reasons.append(req.finished_reason.to_json())
            embeddings.append(req.embedding)
            prompt_tokens.append(len(req.origin_input_ids))
            cached_tokens.append(req.cached_tokens)
    self.send_to_detokenizer.send_pyobj(
        BatchEmbeddingOut(
            rids, finished_reasons, embeddings, prompt_tokens, cached_tokens
        )
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.stream_output_generation"><code class="name flex">
<span>def <span class="ident">stream_output_generation</span></span>(<span>self: Scheduler,<br>reqs: List[Req],<br>return_logprob: bool,<br>skip_req: Optional[Req] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stream_output_generation(
    self: Scheduler,
    reqs: List[Req],
    return_logprob: bool,
    skip_req: Optional[Req] = None,
):
    rids = []
    finished_reasons: List[BaseFinishReason] = []

    decoded_texts = []
    decode_ids_list = []
    read_offsets = []
    output_ids = []

    skip_special_tokens = []
    spaces_between_special_tokens = []
    no_stop_trim = []
    prompt_tokens = []
    completion_tokens = []
    cached_tokens = []
    spec_verify_ct = []
    output_hidden_states = None

    if return_logprob:
        input_token_logprobs_val = []
        input_token_logprobs_idx = []
        output_token_logprobs_val = []
        output_token_logprobs_idx = []
        input_top_logprobs_val = []
        input_top_logprobs_idx = []
        output_top_logprobs_val = []
        output_top_logprobs_idx = []
        input_token_ids_logprobs_val = []
        input_token_ids_logprobs_idx = []
        output_token_ids_logprobs_val = []
        output_token_ids_logprobs_idx = []
    else:
        input_token_logprobs_val = input_token_logprobs_idx = (
            output_token_logprobs_val
        ) = output_token_logprobs_idx = input_top_logprobs_val = (
            input_top_logprobs_idx
        ) = output_top_logprobs_val = output_top_logprobs_idx = (
            input_token_ids_logprobs_val
        ) = input_token_ids_logprobs_idx = output_token_ids_logprobs_val = (
            output_token_ids_logprobs_idx
        ) = None

    for req in reqs:
        if req is skip_req:
            continue

        # Multimodal partial stream chunks break the detokenizer, so drop aborted requests here.
        if self.model_config.is_multimodal_gen and req.to_abort:
            continue

        if req.finished():
            if req.finished_output:
                # With the overlap schedule, a request will try to output twice and hit this line twice
                # because of the one additional delayed token. This &#34;continue&#34; prevented the dummy output.
                continue
            req.finished_output = True
            should_output = True
        else:
            if req.stream:
                stream_interval = (
                    req.sampling_params.stream_interval or self.stream_interval
                )
                should_output = (
                    len(req.output_ids) % stream_interval == 1
                    if not self.model_config.is_multimodal_gen
                    and stream_interval &gt; 1
                    else len(req.output_ids) % stream_interval == 0
                )
            else:
                should_output = (
                    len(req.output_ids) % DEFAULT_FORCE_STREAM_INTERVAL == 0
                    if not self.model_config.is_multimodal_gen
                    else False
                )

        if should_output:
            send_token_offset = req.send_token_offset
            send_output_token_logprobs_offset = (
                req.send_output_token_logprobs_offset
            )
            rids.append(req.rid)
            finished_reasons.append(
                req.finished_reason.to_json() if req.finished_reason else None
            )
            decoded_texts.append(req.decoded_text)
            decode_ids, read_offset = req.init_incremental_detokenize()

            if self.model_config.is_multimodal_gen:
                decode_ids_list.append(decode_ids)
            else:
                decode_ids_list.append(decode_ids[req.send_decode_id_offset :])

            req.send_decode_id_offset = len(decode_ids)
            read_offsets.append(read_offset)
            output_ids.append(req.output_ids[send_token_offset:])
            req.send_token_offset = len(req.output_ids)
            skip_special_tokens.append(req.sampling_params.skip_special_tokens)
            spaces_between_special_tokens.append(
                req.sampling_params.spaces_between_special_tokens
            )
            no_stop_trim.append(req.sampling_params.no_stop_trim)
            prompt_tokens.append(len(req.origin_input_ids))
            completion_tokens.append(len(req.output_ids))
            cached_tokens.append(req.cached_tokens)

            if not self.spec_algorithm.is_none():
                spec_verify_ct.append(req.spec_verify_ct)

            if return_logprob:
                if (
                    req.return_logprob
                    and not req.input_logprob_sent
                    # Decode server does not send input logprobs
                    and self.disaggregation_mode != DisaggregationMode.DECODE
                ):
                    input_token_logprobs_val.append(req.input_token_logprobs_val)
                    input_token_logprobs_idx.append(req.input_token_logprobs_idx)
                    input_top_logprobs_val.append(req.input_top_logprobs_val)
                    input_top_logprobs_idx.append(req.input_top_logprobs_idx)
                    input_token_ids_logprobs_val.append(
                        req.input_token_ids_logprobs_val
                    )
                    input_token_ids_logprobs_idx.append(
                        req.input_token_ids_logprobs_idx
                    )
                    req.input_logprob_sent = True
                else:
                    input_token_logprobs_val.append([])
                    input_token_logprobs_idx.append([])
                    input_top_logprobs_val.append([])
                    input_top_logprobs_idx.append([])
                    input_token_ids_logprobs_val.append([])
                    input_token_ids_logprobs_idx.append([])

                if req.return_logprob:
                    output_token_logprobs_val.append(
                        req.output_token_logprobs_val[
                            send_output_token_logprobs_offset:
                        ]
                    )
                    output_token_logprobs_idx.append(
                        req.output_token_logprobs_idx[
                            send_output_token_logprobs_offset:
                        ]
                    )
                    output_top_logprobs_val.append(
                        req.output_top_logprobs_val[
                            send_output_token_logprobs_offset:
                        ]
                    )
                    output_top_logprobs_idx.append(
                        req.output_top_logprobs_idx[
                            send_output_token_logprobs_offset:
                        ]
                    )
                    output_token_ids_logprobs_val.append(
                        req.output_token_ids_logprobs_val[
                            send_output_token_logprobs_offset:
                        ]
                    )
                    output_token_ids_logprobs_idx.append(
                        req.output_token_ids_logprobs_idx[
                            send_output_token_logprobs_offset:
                        ]
                    )
                    req.send_output_token_logprobs_offset = len(
                        req.output_token_logprobs_val
                    )
                else:
                    output_token_logprobs_val.append([])
                    output_token_logprobs_idx.append([])
                    output_top_logprobs_val.append([])
                    output_top_logprobs_idx.append([])
                    output_token_ids_logprobs_val.append([])
                    output_token_ids_logprobs_idx.append([])

            if req.return_hidden_states:
                if output_hidden_states is None:
                    output_hidden_states = []
                output_hidden_states.append(req.hidden_states)

        if (
            req.finished()
            and self.tp_rank == 0
            and self.server_args.enable_request_time_stats_logging
        ):
            req.log_time_stats()

    # Send to detokenizer
    if rids:
        if self.model_config.is_multimodal_gen:
            return

        self.send_to_detokenizer.send_pyobj(
            BatchTokenIDOut(
                rids,
                finished_reasons,
                decoded_texts,
                decode_ids_list,
                read_offsets,
                output_ids,
                skip_special_tokens,
                spaces_between_special_tokens,
                no_stop_trim,
                prompt_tokens,
                completion_tokens,
                cached_tokens,
                spec_verify_ct,
                input_token_logprobs_val,
                input_token_logprobs_idx,
                output_token_logprobs_val,
                output_token_logprobs_idx,
                input_top_logprobs_val,
                input_top_logprobs_idx,
                output_top_logprobs_val,
                output_top_logprobs_idx,
                input_token_ids_logprobs_val,
                input_token_ids_logprobs_idx,
                output_token_ids_logprobs_val,
                output_token_ids_logprobs_idx,
                output_hidden_states,
            )
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.managers" href="index.html">sglang.srt.managers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin" href="#sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin">SchedulerOutputProcessorMixin</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.add_input_logprob_return_values" href="#sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.add_input_logprob_return_values">add_input_logprob_return_values</a></code></li>
<li><code><a title="sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.add_logprob_return_values" href="#sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.add_logprob_return_values">add_logprob_return_values</a></code></li>
<li><code><a title="sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.process_batch_result_decode" href="#sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.process_batch_result_decode">process_batch_result_decode</a></code></li>
<li><code><a title="sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.process_batch_result_prefill" href="#sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.process_batch_result_prefill">process_batch_result_prefill</a></code></li>
<li><code><a title="sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.stream_output" href="#sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.stream_output">stream_output</a></code></li>
<li><code><a title="sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.stream_output_embedding" href="#sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.stream_output_embedding">stream_output_embedding</a></code></li>
<li><code><a title="sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.stream_output_generation" href="#sglang.srt.managers.scheduler_output_processor_mixin.SchedulerOutputProcessorMixin.stream_output_generation">stream_output_generation</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
