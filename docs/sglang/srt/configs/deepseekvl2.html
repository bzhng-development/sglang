<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.configs.deepseekvl2 API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.configs.deepseekvl2</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.configs.deepseekvl2.select_best_resolution"><code class="name flex">
<span>def <span class="ident">select_best_resolution</span></span>(<span>image_size, candidate_resolutions)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_best_resolution(image_size, candidate_resolutions):
    # used for cropping
    original_width, original_height = image_size
    best_fit = None
    max_effective_resolution = 0
    min_wasted_resolution = float(&#34;inf&#34;)

    for width, height in candidate_resolutions:
        scale = min(width / original_width, height / original_height)
        downscaled_width, downscaled_height = int(original_width * scale), int(
            original_height * scale
        )
        effective_resolution = min(
            downscaled_width * downscaled_height, original_width * original_height
        )
        wasted_resolution = (width * height) - effective_resolution

        if effective_resolution &gt; max_effective_resolution or (
            effective_resolution == max_effective_resolution
            and wasted_resolution &lt; min_wasted_resolution
        ):
            max_effective_resolution = effective_resolution
            min_wasted_resolution = wasted_resolution
            best_fit = (width, height)

    return best_fit</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekV2Config"><code class="flex name class">
<span>class <span class="ident">DeepseekV2Config</span></span>
<span>(</span><span>vocab_size=102400,<br>hidden_size=4096,<br>intermediate_size=11008,<br>moe_intermediate_size=1407,<br>num_hidden_layers=30,<br>num_attention_heads=32,<br>num_key_value_heads=32,<br>n_shared_experts=None,<br>n_routed_experts=None,<br>ep_size=1,<br>routed_scaling_factor=1.0,<br>kv_lora_rank=512,<br>q_lora_rank=1536,<br>qk_rope_head_dim=64,<br>v_head_dim=128,<br>qk_nope_head_dim=128,<br>topk_method='gready',<br>n_group=None,<br>topk_group=None,<br>num_experts_per_tok=None,<br>moe_layer_freq=1,<br>first_k_dense_replace=0,<br>norm_topk_prob=False,<br>scoring_func='softmax',<br>aux_loss_alpha=0.001,<br>seq_aux=True,<br>hidden_act='silu',<br>max_position_embeddings=2048,<br>initializer_range=0.02,<br>rms_norm_eps=1e-06,<br>use_cache=True,<br>pad_token_id=None,<br>bos_token_id=100000,<br>eos_token_id=100001,<br>pretraining_tp=1,<br>tie_word_embeddings=False,<br>rope_theta=10000.0,<br>rope_scaling=None,<br>attention_bias=False,<br>attention_dropout=0.0,<br>use_mla=True,<br>**kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DeepseekV2Config(PretrainedConfig):

    model_type = &#34;deepseek_v2&#34;
    keys_to_ignore_at_inference = [&#34;past_key_values&#34;]

    def __init__(
        self,
        vocab_size=102400,
        hidden_size=4096,
        intermediate_size=11008,
        moe_intermediate_size=1407,
        num_hidden_layers=30,
        num_attention_heads=32,
        num_key_value_heads=32,
        n_shared_experts=None,
        n_routed_experts=None,
        ep_size=1,
        routed_scaling_factor=1.0,
        kv_lora_rank=512,
        q_lora_rank=1536,
        qk_rope_head_dim=64,
        v_head_dim=128,
        qk_nope_head_dim=128,
        topk_method=&#34;gready&#34;,
        n_group=None,
        topk_group=None,
        num_experts_per_tok=None,
        moe_layer_freq=1,
        first_k_dense_replace=0,
        norm_topk_prob=False,
        scoring_func=&#34;softmax&#34;,
        aux_loss_alpha=0.001,
        seq_aux=True,
        hidden_act=&#34;silu&#34;,
        max_position_embeddings=2048,
        initializer_range=0.02,
        rms_norm_eps=1e-6,
        use_cache=True,
        pad_token_id=None,
        bos_token_id=100000,
        eos_token_id=100001,
        pretraining_tp=1,
        tie_word_embeddings=False,
        rope_theta=10000.0,
        rope_scaling=None,
        attention_bias=False,
        attention_dropout=0.0,
        use_mla=True,
        **kwargs,
    ):
        self.vocab_size = vocab_size
        self.max_position_embeddings = max_position_embeddings
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.moe_intermediate_size = moe_intermediate_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.n_shared_experts = n_shared_experts
        self.n_routed_experts = n_routed_experts
        self.ep_size = ep_size
        self.routed_scaling_factor = routed_scaling_factor
        self.kv_lora_rank = kv_lora_rank
        self.q_lora_rank = q_lora_rank
        self.qk_rope_head_dim = qk_rope_head_dim
        self.v_head_dim = v_head_dim
        self.qk_nope_head_dim = qk_nope_head_dim
        self.topk_method = topk_method
        self.n_group = n_group
        self.topk_group = topk_group
        self.num_experts_per_tok = num_experts_per_tok
        self.moe_layer_freq = moe_layer_freq
        self.first_k_dense_replace = first_k_dense_replace
        self.norm_topk_prob = norm_topk_prob
        self.scoring_func = scoring_func
        self.aux_loss_alpha = aux_loss_alpha
        self.seq_aux = seq_aux
        # for backward compatibility
        if num_key_value_heads is None:
            num_key_value_heads = num_attention_heads

        self.num_key_value_heads = num_key_value_heads
        self.hidden_act = hidden_act
        self.initializer_range = initializer_range
        self.rms_norm_eps = float(rms_norm_eps)
        self.pretraining_tp = pretraining_tp
        self.use_cache = use_cache
        self.rope_theta = rope_theta
        self.rope_scaling = rope_scaling
        self.attention_bias = attention_bias
        self.attention_dropout = attention_dropout
        self.use_mla = use_mla

        super().__init__(
            pad_token_id=pad_token_id,
            bos_token_id=bos_token_id,
            eos_token_id=eos_token_id,
            tie_word_embeddings=tie_word_embeddings,
            **kwargs,
        )</code></pre>
</details>
<div class="desc"><p>Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as
methods for loading/downloading/saving configurations.</p>
<p><Tip></p>
<p>A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to
initialize a model does <strong>not</strong> load the model weights. It only affects the model's configuration.</p>
<p></Tip></p>
<p>Class attributes (overridden by derived classes):</p>
<ul>
<li><strong>model_type</strong> (<code>str</code>) &ndash; An identifier for the model type, serialized into the JSON file, and used to recreate
the correct object in [<code>~transformers.AutoConfig</code>].</li>
<li><strong>has_no_defaults_at_init</strong> (<code>bool</code>) &ndash; Whether the config class can be initialized without providing input arguments.
Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,
(but not necessarily) such as [<code>~transformers.EncoderDecoderConfig</code>] or [<code>~RagConfig</code>]. They have to be initialized from
two or more configs of type [<code>~transformers.PretrainedConfig</code>].</li>
<li><strong>keys_to_ignore_at_inference</strong> (<code>list[str]</code>) &ndash; A list of keys to ignore by default when looking at dictionary
outputs of the model during inference.</li>
<li><strong>attribute_map</strong> (<code>dict[str, str]</code>) &ndash; A dict that maps model specific attribute names to the standardized
naming of attributes.</li>
<li><strong>base_model_tp_plan</strong> (<code>dict[str, Any]</code>) &ndash; A dict that maps sub-modules FQNs of a base model to a tensor
parallel plan applied to the sub-module when <code>model.tensor_parallel</code> is called.</li>
<li><strong>base_model_pp_plan</strong> (<code>dict[str, tuple[list[str]]]</code>) &ndash; A dict that maps child-modules of a base model to a
pipeline parallel plan that enables users to place the child-module on the appropriate device.</li>
</ul>
<p>Common attributes (present in all subclasses):</p>
<ul>
<li><strong>vocab_size</strong> (<code>int</code>) &ndash; The number of tokens in the vocabulary, which is also the first dimension of the
embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).</li>
<li><strong>hidden_size</strong> (<code>int</code>) &ndash; The hidden size of the model.</li>
<li><strong>num_attention_heads</strong> (<code>int</code>) &ndash; The number of attention heads used in the multi-head attention layers of the
model.</li>
<li><strong>num_hidden_layers</strong> (<code>int</code>) &ndash; The number of blocks in the model.</li>
</ul>
<p><Tip warning={true}></p>
<p>Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading
some of them will still be possible, but attempting to overwrite them will throw an exception &ndash; you should set
them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more
information about the individual parameters.</p>
<p></Tip></p>
<h2 id="arg">Arg</h2>
<p>name_or_path (<code>str</code>, <em>optional</em>, defaults to <code>""</code>):
Store the string that was passed to [<code>PreTrainedModel.from_pretrained</code>] or
[<code>TFPreTrainedModel.from_pretrained</code>] as <code>pretrained_model_name_or_path</code> if the configuration was created
with such a method.
output_hidden_states (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should return all hidden-states.
output_attentions (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should returns all attentions.
return_dict (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should return a [<code>~transformers.utils.ModelOutput</code>] instead of a plain tuple.
is_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether the model is used as an encoder/decoder or not.
is_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether to only use the decoder in an encoder-decoder architecture, otherwise it has no effect on
decoder-only or encoder-only architectures.
cross_attention_hidden_size (<code>bool</code>, <em>optional</em>):
The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder
setting and the cross-attention hidden dimension differs from <code>self.config.hidden_size</code>.
add_cross_attention (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether cross-attention layers should be added to the model. Note, this option is only relevant for models
that can be used as decoder models within the [<code>EncoderDecoderModel</code>] class, which consists of all models
in <code>AUTO_MODELS_FOR_CAUSAL_LM</code>.
tie_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder
and decoder model to have the exact same parameter names.
prune_heads (<code>dict[int, list[int]]</code>, <em>optional</em>, defaults to <code>{}</code>):
Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of
heads to prune in said layer.</p>
<pre><code>For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.
</code></pre>
<p>chunk_size_feed_forward (<code>int</code>, <em>optional</em>, defaults to <code>0</code>):
The chunk size of all feed forward layers in the residual attention blocks. A chunk size of <code>0</code> means that
the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes <code>n</code> &lt;
sequence_length embeddings at a time. For more information on feed forward chunking, see <a href="../glossary.html#feed-forward-chunking">How does Feed
Forward Chunking work?</a>.</p>
<blockquote>
<p>Parameters for fine-tuning tasks</p>
</blockquote>
<p>architectures (<code>list[str]</code>, <em>optional</em>):
Model architectures that can be used with the model pretrained weights.
finetuning_task (<code>str</code>, <em>optional</em>):
Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow
or PyTorch) checkpoint.
id2label (<code>dict[int, str]</code>, <em>optional</em>):
A map from index (for instance prediction index, or target index) to label.
label2id (<code>dict[str, int]</code>, <em>optional</em>):
A map from label to index for the model.
num_labels (<code>int</code>, <em>optional</em>):
Number of labels to use in the last layer added to the model, typically for a classification task.
task_specific_params (<code>dict[str, Any]</code>, <em>optional</em>):
Additional keyword arguments to store for the current task.
problem_type (<code>str</code>, <em>optional</em>):
Problem type for <code>XxxForSequenceClassification</code> models. Can be one of <code>"regression"</code>,
<code>"single_label_classification"</code> or <code>"multi_label_classification"</code>.</p>
<blockquote>
<p>Parameters linked to the tokenizer</p>
</blockquote>
<p>tokenizer_class (<code>str</code>, <em>optional</em>):
The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the
model by default).
prefix (<code>str</code>, <em>optional</em>):
A specific prompt that should be added at the beginning of each text before calling the model.
bos_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>beginning-of-stream</em> token.
pad_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>padding</em> token.
eos_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>end-of-stream</em> token.
decoder_start_token_id (<code>int</code>, <em>optional</em>):
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.
sep_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>separation</em> token.</p>
<blockquote>
<p>PyTorch specific parameters</p>
</blockquote>
<p>torchscript (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should be used with Torchscript.
tie_word_embeddings (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the
model has a output word embedding layer.
dtype (<code>str</code>, <em>optional</em>):
The <code>dtype</code> of the weights. This attribute can be used to initialize the model to a non-default <code>dtype</code>
(which is normally <code>float32</code>) and thus allow for optimal storage allocation. For example, if the saved
model is <code>float16</code>, ideally we want to load it back using the minimal amount of memory needed to load
<code>float16</code> weights.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.configuration_utils.PretrainedConfig</li>
<li>transformers.utils.hub.PushToHubMixin</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekV2Config.keys_to_ignore_at_inference"><code class="name">var <span class="ident">keys_to_ignore_at_inference</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekV2Config.model_type"><code class="name">var <span class="ident">model_type</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2Config"><code class="flex name class">
<span>class <span class="ident">DeepseekVL2Config</span></span>
<span>(</span><span>tile_tag: str = 'tile_tag',<br>global_view_pos: str = 'head',<br>candidate_resolutions: Tuple[Tuple[int, int]] = ((384, 384),),<br>**kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DeepseekVL2Config(PretrainedConfig):
    model_type = &#34;deepseek_vl_v2&#34;
    vision_config: DeepseekVL2VisionEncoderConfig
    projector_config: DeepseekVL2MlpProjectorConfig
    language_config: DeepseekV2Config

    tile_tag: str = &#34;2D&#34;
    global_view_pos: str = &#34;head&#34;
    candidate_resolutions: Tuple[Tuple[int, int]] = ((384, 384),)

    def __init__(
        self,
        tile_tag: str = &#34;tile_tag&#34;,
        global_view_pos: str = &#34;head&#34;,
        candidate_resolutions: Tuple[Tuple[int, int]] = ((384, 384),),
        **kwargs,
    ):
        super().__init__(**kwargs)

        vision_config = kwargs.get(&#34;vision_config&#34;, {})
        self.vision_config = DeepseekVL2VisionEncoderConfig(**vision_config)

        projector_config = kwargs.get(&#34;projector_config&#34;, {})
        self.projector_config = DeepseekVL2MlpProjectorConfig(**projector_config)

        language_config = kwargs.get(&#34;language_config&#34;, {})
        if isinstance(language_config, DeepseekV2Config):
            self.language_config = language_config
        else:
            self.language_config = DeepseekV2Config(**language_config)

        self.tile_tag = tile_tag
        self.global_view_pos = global_view_pos
        self.candidate_resolutions = candidate_resolutions
        self.architectures = [&#34;DeepseekVL2ForCausalLM&#34;]</code></pre>
</details>
<div class="desc"><p>Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as
methods for loading/downloading/saving configurations.</p>
<p><Tip></p>
<p>A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to
initialize a model does <strong>not</strong> load the model weights. It only affects the model's configuration.</p>
<p></Tip></p>
<p>Class attributes (overridden by derived classes):</p>
<ul>
<li><strong>model_type</strong> (<code>str</code>) &ndash; An identifier for the model type, serialized into the JSON file, and used to recreate
the correct object in [<code>~transformers.AutoConfig</code>].</li>
<li><strong>has_no_defaults_at_init</strong> (<code>bool</code>) &ndash; Whether the config class can be initialized without providing input arguments.
Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,
(but not necessarily) such as [<code>~transformers.EncoderDecoderConfig</code>] or [<code>~RagConfig</code>]. They have to be initialized from
two or more configs of type [<code>~transformers.PretrainedConfig</code>].</li>
<li><strong>keys_to_ignore_at_inference</strong> (<code>list[str]</code>) &ndash; A list of keys to ignore by default when looking at dictionary
outputs of the model during inference.</li>
<li><strong>attribute_map</strong> (<code>dict[str, str]</code>) &ndash; A dict that maps model specific attribute names to the standardized
naming of attributes.</li>
<li><strong>base_model_tp_plan</strong> (<code>dict[str, Any]</code>) &ndash; A dict that maps sub-modules FQNs of a base model to a tensor
parallel plan applied to the sub-module when <code>model.tensor_parallel</code> is called.</li>
<li><strong>base_model_pp_plan</strong> (<code>dict[str, tuple[list[str]]]</code>) &ndash; A dict that maps child-modules of a base model to a
pipeline parallel plan that enables users to place the child-module on the appropriate device.</li>
</ul>
<p>Common attributes (present in all subclasses):</p>
<ul>
<li><strong>vocab_size</strong> (<code>int</code>) &ndash; The number of tokens in the vocabulary, which is also the first dimension of the
embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).</li>
<li><strong>hidden_size</strong> (<code>int</code>) &ndash; The hidden size of the model.</li>
<li><strong>num_attention_heads</strong> (<code>int</code>) &ndash; The number of attention heads used in the multi-head attention layers of the
model.</li>
<li><strong>num_hidden_layers</strong> (<code>int</code>) &ndash; The number of blocks in the model.</li>
</ul>
<p><Tip warning={true}></p>
<p>Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading
some of them will still be possible, but attempting to overwrite them will throw an exception &ndash; you should set
them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more
information about the individual parameters.</p>
<p></Tip></p>
<h2 id="arg">Arg</h2>
<p>name_or_path (<code>str</code>, <em>optional</em>, defaults to <code>""</code>):
Store the string that was passed to [<code>PreTrainedModel.from_pretrained</code>] or
[<code>TFPreTrainedModel.from_pretrained</code>] as <code>pretrained_model_name_or_path</code> if the configuration was created
with such a method.
output_hidden_states (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should return all hidden-states.
output_attentions (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should returns all attentions.
return_dict (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should return a [<code>~transformers.utils.ModelOutput</code>] instead of a plain tuple.
is_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether the model is used as an encoder/decoder or not.
is_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether to only use the decoder in an encoder-decoder architecture, otherwise it has no effect on
decoder-only or encoder-only architectures.
cross_attention_hidden_size (<code>bool</code>, <em>optional</em>):
The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder
setting and the cross-attention hidden dimension differs from <code>self.config.hidden_size</code>.
add_cross_attention (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether cross-attention layers should be added to the model. Note, this option is only relevant for models
that can be used as decoder models within the [<code>EncoderDecoderModel</code>] class, which consists of all models
in <code>AUTO_MODELS_FOR_CAUSAL_LM</code>.
tie_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder
and decoder model to have the exact same parameter names.
prune_heads (<code>dict[int, list[int]]</code>, <em>optional</em>, defaults to <code>{}</code>):
Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of
heads to prune in said layer.</p>
<pre><code>For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.
</code></pre>
<p>chunk_size_feed_forward (<code>int</code>, <em>optional</em>, defaults to <code>0</code>):
The chunk size of all feed forward layers in the residual attention blocks. A chunk size of <code>0</code> means that
the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes <code>n</code> &lt;
sequence_length embeddings at a time. For more information on feed forward chunking, see <a href="../glossary.html#feed-forward-chunking">How does Feed
Forward Chunking work?</a>.</p>
<blockquote>
<p>Parameters for fine-tuning tasks</p>
</blockquote>
<p>architectures (<code>list[str]</code>, <em>optional</em>):
Model architectures that can be used with the model pretrained weights.
finetuning_task (<code>str</code>, <em>optional</em>):
Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow
or PyTorch) checkpoint.
id2label (<code>dict[int, str]</code>, <em>optional</em>):
A map from index (for instance prediction index, or target index) to label.
label2id (<code>dict[str, int]</code>, <em>optional</em>):
A map from label to index for the model.
num_labels (<code>int</code>, <em>optional</em>):
Number of labels to use in the last layer added to the model, typically for a classification task.
task_specific_params (<code>dict[str, Any]</code>, <em>optional</em>):
Additional keyword arguments to store for the current task.
problem_type (<code>str</code>, <em>optional</em>):
Problem type for <code>XxxForSequenceClassification</code> models. Can be one of <code>"regression"</code>,
<code>"single_label_classification"</code> or <code>"multi_label_classification"</code>.</p>
<blockquote>
<p>Parameters linked to the tokenizer</p>
</blockquote>
<p>tokenizer_class (<code>str</code>, <em>optional</em>):
The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the
model by default).
prefix (<code>str</code>, <em>optional</em>):
A specific prompt that should be added at the beginning of each text before calling the model.
bos_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>beginning-of-stream</em> token.
pad_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>padding</em> token.
eos_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>end-of-stream</em> token.
decoder_start_token_id (<code>int</code>, <em>optional</em>):
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.
sep_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>separation</em> token.</p>
<blockquote>
<p>PyTorch specific parameters</p>
</blockquote>
<p>torchscript (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should be used with Torchscript.
tie_word_embeddings (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the
model has a output word embedding layer.
dtype (<code>str</code>, <em>optional</em>):
The <code>dtype</code> of the weights. This attribute can be used to initialize the model to a non-default <code>dtype</code>
(which is normally <code>float32</code>) and thus allow for optimal storage allocation. For example, if the saved
model is <code>float16</code>, ideally we want to load it back using the minimal amount of memory needed to load
<code>float16</code> weights.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.configuration_utils.PretrainedConfig</li>
<li>transformers.utils.hub.PushToHubMixin</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2Config.candidate_resolutions"><code class="name">var <span class="ident">candidate_resolutions</span> : Tuple[Tuple[int, int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2Config.global_view_pos"><code class="name">var <span class="ident">global_view_pos</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2Config.language_config"><code class="name">var <span class="ident">language_config</span> : <a title="sglang.srt.configs.deepseekvl2.DeepseekV2Config" href="#sglang.srt.configs.deepseekvl2.DeepseekV2Config">DeepseekV2Config</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2Config.model_type"><code class="name">var <span class="ident">model_type</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2Config.projector_config"><code class="name">var <span class="ident">projector_config</span> : <a title="sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig">DeepseekVL2MlpProjectorConfig</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2Config.tile_tag"><code class="name">var <span class="ident">tile_tag</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2Config.vision_config"><code class="name">var <span class="ident">vision_config</span> : <a title="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig">DeepseekVL2VisionEncoderConfig</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig"><code class="flex name class">
<span>class <span class="ident">DeepseekVL2MlpProjectorConfig</span></span>
<span>(</span><span>projector_type: str = 'downsample_mlp_gelu',<br>input_dim: int = 1152,<br>n_embed: int = 2048,<br>depth: int = 2,<br>mlp_ratio: int = 1,<br>downsample_ratio: int = 2,<br>**kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DeepseekVL2MlpProjectorConfig(PretrainedConfig):
    model_type = &#34;mlp_projector&#34;
    projector_type: str = &#34;downsample_mlp_gelu&#34;
    input_dim: int = 1152
    n_embed: int = 2048
    depth: int = 2
    mlp_ratio: int = 1
    downsample_ratio: int = 2
    token_pooling: bool = False

    def __init__(
        self,
        projector_type: str = &#34;downsample_mlp_gelu&#34;,
        input_dim: int = 1152,
        n_embed: int = 2048,
        depth: int = 2,
        mlp_ratio: int = 1,
        downsample_ratio: int = 2,
        **kwargs,
    ):
        self.projector_type = projector_type
        self.input_dim = input_dim
        self.n_embed = n_embed
        self.depth = depth
        self.mlp_ratio = mlp_ratio
        self.downsample_ratio = downsample_ratio

        super().__init__(**kwargs)</code></pre>
</details>
<div class="desc"><p>Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as
methods for loading/downloading/saving configurations.</p>
<p><Tip></p>
<p>A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to
initialize a model does <strong>not</strong> load the model weights. It only affects the model's configuration.</p>
<p></Tip></p>
<p>Class attributes (overridden by derived classes):</p>
<ul>
<li><strong>model_type</strong> (<code>str</code>) &ndash; An identifier for the model type, serialized into the JSON file, and used to recreate
the correct object in [<code>~transformers.AutoConfig</code>].</li>
<li><strong>has_no_defaults_at_init</strong> (<code>bool</code>) &ndash; Whether the config class can be initialized without providing input arguments.
Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,
(but not necessarily) such as [<code>~transformers.EncoderDecoderConfig</code>] or [<code>~RagConfig</code>]. They have to be initialized from
two or more configs of type [<code>~transformers.PretrainedConfig</code>].</li>
<li><strong>keys_to_ignore_at_inference</strong> (<code>list[str]</code>) &ndash; A list of keys to ignore by default when looking at dictionary
outputs of the model during inference.</li>
<li><strong>attribute_map</strong> (<code>dict[str, str]</code>) &ndash; A dict that maps model specific attribute names to the standardized
naming of attributes.</li>
<li><strong>base_model_tp_plan</strong> (<code>dict[str, Any]</code>) &ndash; A dict that maps sub-modules FQNs of a base model to a tensor
parallel plan applied to the sub-module when <code>model.tensor_parallel</code> is called.</li>
<li><strong>base_model_pp_plan</strong> (<code>dict[str, tuple[list[str]]]</code>) &ndash; A dict that maps child-modules of a base model to a
pipeline parallel plan that enables users to place the child-module on the appropriate device.</li>
</ul>
<p>Common attributes (present in all subclasses):</p>
<ul>
<li><strong>vocab_size</strong> (<code>int</code>) &ndash; The number of tokens in the vocabulary, which is also the first dimension of the
embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).</li>
<li><strong>hidden_size</strong> (<code>int</code>) &ndash; The hidden size of the model.</li>
<li><strong>num_attention_heads</strong> (<code>int</code>) &ndash; The number of attention heads used in the multi-head attention layers of the
model.</li>
<li><strong>num_hidden_layers</strong> (<code>int</code>) &ndash; The number of blocks in the model.</li>
</ul>
<p><Tip warning={true}></p>
<p>Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading
some of them will still be possible, but attempting to overwrite them will throw an exception &ndash; you should set
them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more
information about the individual parameters.</p>
<p></Tip></p>
<h2 id="arg">Arg</h2>
<p>name_or_path (<code>str</code>, <em>optional</em>, defaults to <code>""</code>):
Store the string that was passed to [<code>PreTrainedModel.from_pretrained</code>] or
[<code>TFPreTrainedModel.from_pretrained</code>] as <code>pretrained_model_name_or_path</code> if the configuration was created
with such a method.
output_hidden_states (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should return all hidden-states.
output_attentions (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should returns all attentions.
return_dict (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should return a [<code>~transformers.utils.ModelOutput</code>] instead of a plain tuple.
is_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether the model is used as an encoder/decoder or not.
is_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether to only use the decoder in an encoder-decoder architecture, otherwise it has no effect on
decoder-only or encoder-only architectures.
cross_attention_hidden_size (<code>bool</code>, <em>optional</em>):
The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder
setting and the cross-attention hidden dimension differs from <code>self.config.hidden_size</code>.
add_cross_attention (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether cross-attention layers should be added to the model. Note, this option is only relevant for models
that can be used as decoder models within the [<code>EncoderDecoderModel</code>] class, which consists of all models
in <code>AUTO_MODELS_FOR_CAUSAL_LM</code>.
tie_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder
and decoder model to have the exact same parameter names.
prune_heads (<code>dict[int, list[int]]</code>, <em>optional</em>, defaults to <code>{}</code>):
Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of
heads to prune in said layer.</p>
<pre><code>For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.
</code></pre>
<p>chunk_size_feed_forward (<code>int</code>, <em>optional</em>, defaults to <code>0</code>):
The chunk size of all feed forward layers in the residual attention blocks. A chunk size of <code>0</code> means that
the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes <code>n</code> &lt;
sequence_length embeddings at a time. For more information on feed forward chunking, see <a href="../glossary.html#feed-forward-chunking">How does Feed
Forward Chunking work?</a>.</p>
<blockquote>
<p>Parameters for fine-tuning tasks</p>
</blockquote>
<p>architectures (<code>list[str]</code>, <em>optional</em>):
Model architectures that can be used with the model pretrained weights.
finetuning_task (<code>str</code>, <em>optional</em>):
Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow
or PyTorch) checkpoint.
id2label (<code>dict[int, str]</code>, <em>optional</em>):
A map from index (for instance prediction index, or target index) to label.
label2id (<code>dict[str, int]</code>, <em>optional</em>):
A map from label to index for the model.
num_labels (<code>int</code>, <em>optional</em>):
Number of labels to use in the last layer added to the model, typically for a classification task.
task_specific_params (<code>dict[str, Any]</code>, <em>optional</em>):
Additional keyword arguments to store for the current task.
problem_type (<code>str</code>, <em>optional</em>):
Problem type for <code>XxxForSequenceClassification</code> models. Can be one of <code>"regression"</code>,
<code>"single_label_classification"</code> or <code>"multi_label_classification"</code>.</p>
<blockquote>
<p>Parameters linked to the tokenizer</p>
</blockquote>
<p>tokenizer_class (<code>str</code>, <em>optional</em>):
The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the
model by default).
prefix (<code>str</code>, <em>optional</em>):
A specific prompt that should be added at the beginning of each text before calling the model.
bos_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>beginning-of-stream</em> token.
pad_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>padding</em> token.
eos_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>end-of-stream</em> token.
decoder_start_token_id (<code>int</code>, <em>optional</em>):
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.
sep_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>separation</em> token.</p>
<blockquote>
<p>PyTorch specific parameters</p>
</blockquote>
<p>torchscript (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should be used with Torchscript.
tie_word_embeddings (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the
model has a output word embedding layer.
dtype (<code>str</code>, <em>optional</em>):
The <code>dtype</code> of the weights. This attribute can be used to initialize the model to a non-default <code>dtype</code>
(which is normally <code>float32</code>) and thus allow for optimal storage allocation. For example, if the saved
model is <code>float16</code>, ideally we want to load it back using the minimal amount of memory needed to load
<code>float16</code> weights.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.configuration_utils.PretrainedConfig</li>
<li>transformers.utils.hub.PushToHubMixin</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.depth"><code class="name">var <span class="ident">depth</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.downsample_ratio"><code class="name">var <span class="ident">downsample_ratio</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.input_dim"><code class="name">var <span class="ident">input_dim</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.mlp_ratio"><code class="name">var <span class="ident">mlp_ratio</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.model_type"><code class="name">var <span class="ident">model_type</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.n_embed"><code class="name">var <span class="ident">n_embed</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.projector_type"><code class="name">var <span class="ident">projector_type</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.token_pooling"><code class="name">var <span class="ident">token_pooling</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig"><code class="flex name class">
<span>class <span class="ident">DeepseekVL2VisionEncoderConfig</span></span>
<span>(</span><span>model_name: str = 'siglip_large_patch16_384',<br>image_size: int = 384,<br>patch_size: int = 16,<br>width: int = 1024,<br>layers: int = 24,<br>heads: int = 16,<br>mlp_ratio: int = 4,<br>global_pool: str = 'map',<br>ignore_head: bool = True,<br>class_token: bool = False,<br>num_classes: int = 0,<br>use_checkpoint: bool = False,<br>**kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DeepseekVL2VisionEncoderConfig(PretrainedConfig):
    model_type: str = &#34;vision&#34;

    model_name: str = &#34;siglip_large_patch16_384&#34;
    image_size: int = 384
    patch_size: int = 16
    width: int = 1024
    layers: int = 24
    heads: int = 16
    mlp_ratio: int = 4
    global_pool: str = &#34;map&#34;
    ignore_head: bool = True
    class_token: bool = False
    num_classes: int = 0
    use_checkpoint: bool = False
    weight_init: str = &#34;skip&#34;
    deterministic: bool = False
    num_recomputing_layers: int = 0

    def __init__(
        self,
        model_name: str = &#34;siglip_large_patch16_384&#34;,
        image_size: int = 384,
        patch_size: int = 16,
        width: int = 1024,
        layers: int = 24,
        heads: int = 16,
        mlp_ratio: int = 4,
        global_pool: str = &#34;map&#34;,
        ignore_head: bool = True,
        class_token: bool = False,
        num_classes: int = 0,
        use_checkpoint: bool = False,
        **kwargs,
    ):
        self.model_name = model_name
        self.image_size = image_size
        self.patch_size = patch_size
        self.width = width
        self.layers = layers
        self.heads = heads
        self.mlp_ratio = mlp_ratio
        self.global_pool = global_pool
        self.ignore_head = ignore_head
        self.class_token = class_token
        self.num_classes = num_classes
        self.use_checkpoint = use_checkpoint

        super().__init__(**kwargs)</code></pre>
</details>
<div class="desc"><p>Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as
methods for loading/downloading/saving configurations.</p>
<p><Tip></p>
<p>A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to
initialize a model does <strong>not</strong> load the model weights. It only affects the model's configuration.</p>
<p></Tip></p>
<p>Class attributes (overridden by derived classes):</p>
<ul>
<li><strong>model_type</strong> (<code>str</code>) &ndash; An identifier for the model type, serialized into the JSON file, and used to recreate
the correct object in [<code>~transformers.AutoConfig</code>].</li>
<li><strong>has_no_defaults_at_init</strong> (<code>bool</code>) &ndash; Whether the config class can be initialized without providing input arguments.
Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,
(but not necessarily) such as [<code>~transformers.EncoderDecoderConfig</code>] or [<code>~RagConfig</code>]. They have to be initialized from
two or more configs of type [<code>~transformers.PretrainedConfig</code>].</li>
<li><strong>keys_to_ignore_at_inference</strong> (<code>list[str]</code>) &ndash; A list of keys to ignore by default when looking at dictionary
outputs of the model during inference.</li>
<li><strong>attribute_map</strong> (<code>dict[str, str]</code>) &ndash; A dict that maps model specific attribute names to the standardized
naming of attributes.</li>
<li><strong>base_model_tp_plan</strong> (<code>dict[str, Any]</code>) &ndash; A dict that maps sub-modules FQNs of a base model to a tensor
parallel plan applied to the sub-module when <code>model.tensor_parallel</code> is called.</li>
<li><strong>base_model_pp_plan</strong> (<code>dict[str, tuple[list[str]]]</code>) &ndash; A dict that maps child-modules of a base model to a
pipeline parallel plan that enables users to place the child-module on the appropriate device.</li>
</ul>
<p>Common attributes (present in all subclasses):</p>
<ul>
<li><strong>vocab_size</strong> (<code>int</code>) &ndash; The number of tokens in the vocabulary, which is also the first dimension of the
embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).</li>
<li><strong>hidden_size</strong> (<code>int</code>) &ndash; The hidden size of the model.</li>
<li><strong>num_attention_heads</strong> (<code>int</code>) &ndash; The number of attention heads used in the multi-head attention layers of the
model.</li>
<li><strong>num_hidden_layers</strong> (<code>int</code>) &ndash; The number of blocks in the model.</li>
</ul>
<p><Tip warning={true}></p>
<p>Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading
some of them will still be possible, but attempting to overwrite them will throw an exception &ndash; you should set
them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more
information about the individual parameters.</p>
<p></Tip></p>
<h2 id="arg">Arg</h2>
<p>name_or_path (<code>str</code>, <em>optional</em>, defaults to <code>""</code>):
Store the string that was passed to [<code>PreTrainedModel.from_pretrained</code>] or
[<code>TFPreTrainedModel.from_pretrained</code>] as <code>pretrained_model_name_or_path</code> if the configuration was created
with such a method.
output_hidden_states (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should return all hidden-states.
output_attentions (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should returns all attentions.
return_dict (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should return a [<code>~transformers.utils.ModelOutput</code>] instead of a plain tuple.
is_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether the model is used as an encoder/decoder or not.
is_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether to only use the decoder in an encoder-decoder architecture, otherwise it has no effect on
decoder-only or encoder-only architectures.
cross_attention_hidden_size (<code>bool</code>, <em>optional</em>):
The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder
setting and the cross-attention hidden dimension differs from <code>self.config.hidden_size</code>.
add_cross_attention (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether cross-attention layers should be added to the model. Note, this option is only relevant for models
that can be used as decoder models within the [<code>EncoderDecoderModel</code>] class, which consists of all models
in <code>AUTO_MODELS_FOR_CAUSAL_LM</code>.
tie_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder
and decoder model to have the exact same parameter names.
prune_heads (<code>dict[int, list[int]]</code>, <em>optional</em>, defaults to <code>{}</code>):
Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of
heads to prune in said layer.</p>
<pre><code>For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.
</code></pre>
<p>chunk_size_feed_forward (<code>int</code>, <em>optional</em>, defaults to <code>0</code>):
The chunk size of all feed forward layers in the residual attention blocks. A chunk size of <code>0</code> means that
the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes <code>n</code> &lt;
sequence_length embeddings at a time. For more information on feed forward chunking, see <a href="../glossary.html#feed-forward-chunking">How does Feed
Forward Chunking work?</a>.</p>
<blockquote>
<p>Parameters for fine-tuning tasks</p>
</blockquote>
<p>architectures (<code>list[str]</code>, <em>optional</em>):
Model architectures that can be used with the model pretrained weights.
finetuning_task (<code>str</code>, <em>optional</em>):
Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow
or PyTorch) checkpoint.
id2label (<code>dict[int, str]</code>, <em>optional</em>):
A map from index (for instance prediction index, or target index) to label.
label2id (<code>dict[str, int]</code>, <em>optional</em>):
A map from label to index for the model.
num_labels (<code>int</code>, <em>optional</em>):
Number of labels to use in the last layer added to the model, typically for a classification task.
task_specific_params (<code>dict[str, Any]</code>, <em>optional</em>):
Additional keyword arguments to store for the current task.
problem_type (<code>str</code>, <em>optional</em>):
Problem type for <code>XxxForSequenceClassification</code> models. Can be one of <code>"regression"</code>,
<code>"single_label_classification"</code> or <code>"multi_label_classification"</code>.</p>
<blockquote>
<p>Parameters linked to the tokenizer</p>
</blockquote>
<p>tokenizer_class (<code>str</code>, <em>optional</em>):
The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the
model by default).
prefix (<code>str</code>, <em>optional</em>):
A specific prompt that should be added at the beginning of each text before calling the model.
bos_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>beginning-of-stream</em> token.
pad_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>padding</em> token.
eos_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>end-of-stream</em> token.
decoder_start_token_id (<code>int</code>, <em>optional</em>):
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.
sep_token_id (<code>int</code>, <em>optional</em>):
The id of the <em>separation</em> token.</p>
<blockquote>
<p>PyTorch specific parameters</p>
</blockquote>
<p>torchscript (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether or not the model should be used with Torchscript.
tie_word_embeddings (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the
model has a output word embedding layer.
dtype (<code>str</code>, <em>optional</em>):
The <code>dtype</code> of the weights. This attribute can be used to initialize the model to a non-default <code>dtype</code>
(which is normally <code>float32</code>) and thus allow for optimal storage allocation. For example, if the saved
model is <code>float16</code>, ideally we want to load it back using the minimal amount of memory needed to load
<code>float16</code> weights.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.configuration_utils.PretrainedConfig</li>
<li>transformers.utils.hub.PushToHubMixin</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.class_token"><code class="name">var <span class="ident">class_token</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.deterministic"><code class="name">var <span class="ident">deterministic</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.global_pool"><code class="name">var <span class="ident">global_pool</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.heads"><code class="name">var <span class="ident">heads</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.ignore_head"><code class="name">var <span class="ident">ignore_head</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.image_size"><code class="name">var <span class="ident">image_size</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.layers"><code class="name">var <span class="ident">layers</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.mlp_ratio"><code class="name">var <span class="ident">mlp_ratio</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.model_name"><code class="name">var <span class="ident">model_name</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.model_type"><code class="name">var <span class="ident">model_type</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.num_classes"><code class="name">var <span class="ident">num_classes</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.num_recomputing_layers"><code class="name">var <span class="ident">num_recomputing_layers</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.patch_size"><code class="name">var <span class="ident">patch_size</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.use_checkpoint"><code class="name">var <span class="ident">use_checkpoint</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.weight_init"><code class="name">var <span class="ident">weight_init</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.width"><code class="name">var <span class="ident">width</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor"><code class="flex name class">
<span>class <span class="ident">DeepseekVLV2Processor</span></span>
<span>(</span><span>tokenizer: transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast,<br>candidate_resolutions: Tuple[Tuple[int, int]],<br>patch_size: int,<br>downsample_ratio: int,<br>image_mean: Tuple[float, float, float] = (0.5, 0.5, 0.5),<br>image_std: Tuple[float, float, float] = (0.5, 0.5, 0.5),<br>normalize: bool = True,<br>image_token: str = &#x27;&lt;image&gt;&#x27;,<br>pad_token: str = &#x27;&lt;｜▁pad▁｜&gt;&#x27;,<br>add_special_token: bool = False,<br>sft_format: str = 'deepseek',<br>mask_prompt: bool = True,<br>ignore_id: int = -100,<br>**kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DeepseekVLV2Processor(ProcessorMixin):
    tokenizer_class = (&#34;LlamaTokenizer&#34;, &#34;LlamaTokenizerFast&#34;)
    attributes = [&#34;tokenizer&#34;]

    def __init__(
        self,
        tokenizer: LlamaTokenizerFast,
        candidate_resolutions: Tuple[Tuple[int, int]],
        patch_size: int,
        downsample_ratio: int,
        image_mean: Tuple[float, float, float] = (0.5, 0.5, 0.5),
        image_std: Tuple[float, float, float] = (0.5, 0.5, 0.5),
        normalize: bool = True,
        image_token: str = &#34;&lt;image&gt;&#34;,
        pad_token: str = &#34;&lt;｜▁pad▁｜&gt;&#34;,
        add_special_token: bool = False,
        sft_format: str = &#34;deepseek&#34;,
        mask_prompt: bool = True,
        ignore_id: int = -100,
        **kwargs,
    ):

        self.candidate_resolutions = candidate_resolutions
        self.image_size = candidate_resolutions[0][0]
        self.patch_size = patch_size
        self.image_mean = image_mean
        self.image_std = image_std
        self.normalize = normalize
        self.downsample_ratio = downsample_ratio

        self.image_transform = ImageTransform(
            mean=image_mean, std=image_std, normalize=normalize
        )
        self.tokenizer = tokenizer
        # must set this，padding side with make a difference in batch inference
        self.tokenizer.padding_side = &#34;left&#34;

        # add the pad_token as special token to use &#39;tokenizer.pad_token&#39; and &#39;tokenizer.pad_token_id&#39;
        if tokenizer.pad_token is None:
            self.tokenizer.add_special_tokens({&#34;pad_token&#34;: pad_token})

        # add image token
        image_token_id = self.tokenizer.vocab.get(image_token)
        if image_token_id is None:
            special_tokens = [image_token]
            special_tokens_dict = {&#34;additional_special_tokens&#34;: special_tokens}
            self.tokenizer.add_special_tokens(special_tokens_dict)
        self.image_token_id = self.tokenizer.vocab.get(image_token)

        # add five special tokens for grounding-related tasks
        # &lt;|ref|&gt;, &lt;|/ref|&gt;, &lt;|det|&gt;, &lt;|/det|&gt;, &lt;|grounding|&gt;
        special_tokens = [&#34;&lt;|ref|&gt;&#34;, &#34;&lt;|/ref|&gt;&#34;, &#34;&lt;|det|&gt;&#34;, &#34;&lt;|/det|&gt;&#34;, &#34;&lt;|grounding|&gt;&#34;]
        special_tokens_dict = {&#34;additional_special_tokens&#34;: special_tokens}
        self.tokenizer.add_special_tokens(special_tokens_dict)

        # add special tokens for SFT data
        special_tokens = [&#34;&lt;|User|&gt;&#34;, &#34;&lt;|Assistant|&gt;&#34;]
        special_tokens_dict = {&#34;additional_special_tokens&#34;: special_tokens}
        self.tokenizer.add_special_tokens(special_tokens_dict)

        self.image_token = image_token
        self.pad_token = pad_token
        self.add_special_token = add_special_token
        self.sft_format = sft_format
        self.mask_prompt = mask_prompt
        self.ignore_id = ignore_id

        super().__init__(
            tokenizer,
            **kwargs,
        )

    def format_messages_v2(self, messages, pil_images, max_req_input_len=-1):
        &#34;&#34;&#34;play the role of format_messages_v2 and get_images_info in the last version&#34;&#34;&#34;
        tokenized_data = []
        masked_tokenized_data = []  # labels
        images_list = []
        images_seq_mask = []
        images_spatial_crop = []

        image_index = 0
        image_token_cnt = messages.count(self.image_token)
        tokenized_str, images, seq_mask, spatial_crop = self.tokenize_with_images(
            messages,
            pil_images[image_index : image_index + image_token_cnt],
            bos=True,
            eos=True,
            cropping=len(pil_images) &lt;= 2,
            max_req_input_len=max_req_input_len,
        )

        image_index = image_token_cnt
        tokenized_data += tokenized_str
        if self.mask_prompt:
            masked_tokenized_data += [self.ignore_id] * len(tokenized_str)
        else:
            masked_tokenized_data += tokenized_str
        images_list += images
        images_seq_mask += seq_mask
        images_spatial_crop += spatial_crop

        assert len(tokenized_data) == len(
            images_seq_mask
        ), f&#34;format_messages_v2: tokenized_str&#39;s length {len(tokenized_str)} is not equal to imags_seq_mask&#39;s length {len(images_seq_mask)}&#34;

        return (
            tokenized_data,
            masked_tokenized_data,
            images_list,
            images_seq_mask,
            images_spatial_crop,
        )

    @property
    def bos_id(self):
        return self.tokenizer.bos_token_id

    @property
    def eos_id(self):
        return self.tokenizer.eos_token_id

    @property
    def pad_id(self):
        return self.tokenizer.pad_token_id

    def encode(self, text: str, bos: bool = True, eos: bool = False):
        t = self.tokenizer.encode(text, add_special_tokens=False)

        if bos:
            t = [self.bos_id] + t
        if eos:
            t = t + [self.eos_id]

        return t

    def decode(self, t: List[int], **kwargs) -&gt; str:
        return self.tokenizer.decode(t, **kwargs)

    def process_one(
        self,
        prompt: str = None,
        conversations: List[Dict[str, str]] = None,
        images: List[Image.Image] = None,
        apply_sft_format: bool = False,
        inference_mode: bool = True,
        system_prompt: str = &#34;&#34;,
        max_req_input_len: int = -1,
        **kwargs,
    ):
        &#34;&#34;&#34;

        Args:
            prompt (str): the formatted prompt;
            conversations (List[Dict]): conversations with a list of messages;
            images (List[ImageType]): the list of images;
            apply_sft_format (bool): if prompt is not None, then apply the SFT format to prompt;
                if conversations is not None, then it will always apply the SFT format to conversations;
            inference_mode (bool): if True, then remove the last eos token;
            system_prompt (str): the system prompt;
            **kwargs:

        Returns:
            outputs (BaseProcessorOutput): the output of the processor,
                - input_ids (torch.LongTensor): [N + image tokens]
                - target_ids (torch.LongTensor): [N + image tokens]
                - images (torch.FloatTensor): [n_images, 3, H, W]
                - image_id (int): the id of the image token
                - num_image_tokens (List[int]): the number of image tokens
        &#34;&#34;&#34;

        assert (
            prompt is None or conversations is None
        ), &#34;prompt and conversations cannot be used at the same time.&#34;

        (
            tokenized_str,
            masked_tokenized_str,
            images_list,
            images_seq_mask,
            images_spatial_crop,
        ) = self.format_messages_v2(conversations, images, max_req_input_len)

        assert (
            len(tokenized_str) == len(images_seq_mask) == len(masked_tokenized_str)
        ), (
            f&#34;tokenized_str&#39;s length {len(tokenized_str)}, input_ids&#39; length {len(masked_tokenized_str)}, &#34;
            f&#34;imags_seq_mask&#39;s length {len(images_seq_mask)}, are not equal&#34;
        )

        input_ids = torch.LongTensor(tokenized_str)
        target_ids = torch.LongTensor(masked_tokenized_str)
        images_seq_mask = torch.tensor(images_seq_mask, dtype=torch.bool)

        # set input_ids &lt; 0 | input_ids == self.image_token_id as ignore_id
        target_ids[(input_ids &lt; 0) | (input_ids == self.image_token_id)] = (
            self.ignore_id
        )
        input_ids[input_ids &lt; 0] = self.pad_id

        if inference_mode:
            assert input_ids[-1] == self.eos_id
            input_ids = input_ids[:-1]
            target_ids = target_ids[:-1]
            images_seq_mask = images_seq_mask[:-1]

        if len(images_list) == 0:
            images = torch.zeros((1, 3, self.image_size, self.image_size))
            images_spatial_crop = torch.zeros((1, 2), dtype=torch.long)
        else:
            images = torch.stack(images_list, dim=0)
            images_spatial_crop = torch.tensor(images_spatial_crop, dtype=torch.long)

        images_spatial_crop = torch.stack(
            [images_spatial_crop], dim=0
        )  # stack the tensor to make it a batch of 1

        prepare = VLChatProcessorOutput(
            input_ids=input_ids,
            target_ids=target_ids,
            pixel_values=images,
            images_seq_mask=images_seq_mask,
            images_spatial_crop=images_spatial_crop,
        )

        return prepare

    def __call__(
        self,
        *,
        prompt: str = None,
        conversations: List[Dict[str, str]] = None,
        images: List[Image.Image] = None,
        apply_sft_format: bool = False,
        inference_mode: bool = True,
        system_prompt: str = &#34;&#34;,
        max_req_input_len: int = -1,
        **kwargs,
    ):
        prepare = self.process_one(
            prompt=prompt,
            conversations=conversations,
            images=images,
            apply_sft_format=apply_sft_format,
            inference_mode=inference_mode,
            system_prompt=system_prompt,
            max_req_input_len=max_req_input_len,
        )

        return prepare

    def find_all_indices(self, messages, target_value):
        indices = []
        for index, item in enumerate(messages):
            if item == target_value:
                indices.append(index)
        return indices

    def tokenize_with_images(
        self,
        conversation: str,
        images: List[Image.Image],
        bos: bool = True,
        eos: bool = True,
        cropping: bool = True,
        max_req_input_len: int = -1,
    ):
        &#34;&#34;&#34;Tokenize text with &lt;image&gt; tags.&#34;&#34;&#34;
        images_list, images_seq_mask, images_spatial_crop = [], [], []
        text_splits = conversation.split(self.image_token)
        tokenized_str = []
        for text_sep, image in zip(text_splits, images):
            &#34;&#34;&#34;encode text_sep&#34;&#34;&#34;
            tokenized_sep = self.encode(text_sep, bos=False, eos=False)
            tokenized_str += tokenized_sep
            images_seq_mask += [False] * len(tokenized_sep)

            &#34;&#34;&#34;select best resolution for anyres&#34;&#34;&#34;
            if cropping:
                best_width, best_height = select_best_resolution(
                    image.size, self.candidate_resolutions
                )
            else:
                best_width, best_height = self.image_size, self.image_size
            # print(image.size, (best_width, best_height)) # check the select_best_resolutions func

            &#34;&#34;&#34;process the global view&#34;&#34;&#34;
            global_view = ImageOps.pad(
                image,
                (self.image_size, self.image_size),
                color=tuple(int(x * 255) for x in self.image_transform.mean),
            )
            images_list.append(self.image_transform(global_view))

            &#34;&#34;&#34;process the local views&#34;&#34;&#34;
            local_view = ImageOps.pad(
                image,
                (best_width, best_height),
                color=tuple(int(x * 255) for x in self.image_transform.mean),
            )
            for i in range(0, best_height, self.image_size):
                for j in range(0, best_width, self.image_size):
                    images_list.append(
                        self.image_transform(
                            local_view.crop(
                                (j, i, j + self.image_size, i + self.image_size)
                            )
                        )
                    )

            &#34;&#34;&#34;record height / width crop num&#34;&#34;&#34;
            num_width_tiles, num_height_tiles = (
                best_width // self.image_size,
                best_height // self.image_size,
            )
            images_spatial_crop.append([num_width_tiles, num_height_tiles])

            &#34;&#34;&#34;add image tokens&#34;&#34;&#34;
            h = w = math.ceil(
                (self.image_size // self.patch_size) / self.downsample_ratio
            )
            # global views tokens h * (w + 1), 1 is for line separator
            tokenized_image = [self.image_token_id] * h * (w + 1)
            # add a separator between global and local views
            tokenized_image += [self.image_token_id]
            # local views tokens, (num_height_tiles * h) * (num_width_tiles * w + 1)
            tokenized_image += (
                [self.image_token_id]
                * (num_height_tiles * h)
                * (num_width_tiles * w + 1)
            )

            tokenized_str += tokenized_image
            images_seq_mask += [True] * len(tokenized_image)
            # print(width_crop_num, height_crop_num, len(tokenized_image)) # test the correctness of the number of image-related tokens

        &#34;&#34;&#34;process the last text split&#34;&#34;&#34;
        tokenized_sep = self.encode(text_splits[-1], bos=False, eos=False)
        # deal with video, limit with request len
        if max_req_input_len &gt; -1:
            if max_req_input_len &lt; len(tokenized_sep) + len(tokenized_str) - 1:
                rest = max_req_input_len - len(tokenized_sep) - 1 - 1024
                tokenized_str = tokenized_str[:rest]
                images_seq_mask = images_seq_mask[:rest]
        tokenized_str += tokenized_sep
        images_seq_mask += [False] * len(tokenized_sep)

        &#34;&#34;&#34;add the bos and eos tokens&#34;&#34;&#34;
        if bos:
            tokenized_str = [self.bos_id] + tokenized_str
            images_seq_mask = [False] + images_seq_mask
        if eos:
            tokenized_str = tokenized_str + [self.eos_id]
            images_seq_mask = images_seq_mask + [False]

        assert len(tokenized_str) == len(
            images_seq_mask
        ), f&#34;tokenize_with_images func: tokenized_str&#39;s length {len(tokenized_str)} is not equal to imags_seq_mask&#39;s length {len(images_seq_mask)}&#34;

        return tokenized_str, images_list, images_seq_mask, images_spatial_crop</code></pre>
</details>
<div class="desc"><p>This is a mixin used to provide saving/loading functionality for all processor classes.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>transformers.processing_utils.ProcessorMixin</li>
<li>transformers.utils.hub.PushToHubMixin</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.attributes"><code class="name">var <span class="ident">attributes</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.tokenizer_class"><code class="name">var <span class="ident">tokenizer_class</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.bos_id"><code class="name">prop <span class="ident">bos_id</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def bos_id(self):
    return self.tokenizer.bos_token_id</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.eos_id"><code class="name">prop <span class="ident">eos_id</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def eos_id(self):
    return self.tokenizer.eos_token_id</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.pad_id"><code class="name">prop <span class="ident">pad_id</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def pad_id(self):
    return self.tokenizer.pad_token_id</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.decode"><code class="name flex">
<span>def <span class="ident">decode</span></span>(<span>self, t: List[int], **kwargs) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decode(self, t: List[int], **kwargs) -&gt; str:
    return self.tokenizer.decode(t, **kwargs)</code></pre>
</details>
<div class="desc"><p>This method forwards all its arguments to PreTrainedTokenizer's [<code>~PreTrainedTokenizer.decode</code>]. Please refer to
the docstring of this method for more information.</p></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.encode"><code class="name flex">
<span>def <span class="ident">encode</span></span>(<span>self, text: str, bos: bool = True, eos: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def encode(self, text: str, bos: bool = True, eos: bool = False):
    t = self.tokenizer.encode(text, add_special_tokens=False)

    if bos:
        t = [self.bos_id] + t
    if eos:
        t = t + [self.eos_id]

    return t</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.find_all_indices"><code class="name flex">
<span>def <span class="ident">find_all_indices</span></span>(<span>self, messages, target_value)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_all_indices(self, messages, target_value):
    indices = []
    for index, item in enumerate(messages):
        if item == target_value:
            indices.append(index)
    return indices</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.format_messages_v2"><code class="name flex">
<span>def <span class="ident">format_messages_v2</span></span>(<span>self, messages, pil_images, max_req_input_len=-1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def format_messages_v2(self, messages, pil_images, max_req_input_len=-1):
    &#34;&#34;&#34;play the role of format_messages_v2 and get_images_info in the last version&#34;&#34;&#34;
    tokenized_data = []
    masked_tokenized_data = []  # labels
    images_list = []
    images_seq_mask = []
    images_spatial_crop = []

    image_index = 0
    image_token_cnt = messages.count(self.image_token)
    tokenized_str, images, seq_mask, spatial_crop = self.tokenize_with_images(
        messages,
        pil_images[image_index : image_index + image_token_cnt],
        bos=True,
        eos=True,
        cropping=len(pil_images) &lt;= 2,
        max_req_input_len=max_req_input_len,
    )

    image_index = image_token_cnt
    tokenized_data += tokenized_str
    if self.mask_prompt:
        masked_tokenized_data += [self.ignore_id] * len(tokenized_str)
    else:
        masked_tokenized_data += tokenized_str
    images_list += images
    images_seq_mask += seq_mask
    images_spatial_crop += spatial_crop

    assert len(tokenized_data) == len(
        images_seq_mask
    ), f&#34;format_messages_v2: tokenized_str&#39;s length {len(tokenized_str)} is not equal to imags_seq_mask&#39;s length {len(images_seq_mask)}&#34;

    return (
        tokenized_data,
        masked_tokenized_data,
        images_list,
        images_seq_mask,
        images_spatial_crop,
    )</code></pre>
</details>
<div class="desc"><p>play the role of format_messages_v2 and get_images_info in the last version</p></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.process_one"><code class="name flex">
<span>def <span class="ident">process_one</span></span>(<span>self,<br>prompt: str = None,<br>conversations: List[Dict[str, str]] = None,<br>images: List[PIL.Image.Image] = None,<br>apply_sft_format: bool = False,<br>inference_mode: bool = True,<br>system_prompt: str = '',<br>max_req_input_len: int = -1,<br>**kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_one(
    self,
    prompt: str = None,
    conversations: List[Dict[str, str]] = None,
    images: List[Image.Image] = None,
    apply_sft_format: bool = False,
    inference_mode: bool = True,
    system_prompt: str = &#34;&#34;,
    max_req_input_len: int = -1,
    **kwargs,
):
    &#34;&#34;&#34;

    Args:
        prompt (str): the formatted prompt;
        conversations (List[Dict]): conversations with a list of messages;
        images (List[ImageType]): the list of images;
        apply_sft_format (bool): if prompt is not None, then apply the SFT format to prompt;
            if conversations is not None, then it will always apply the SFT format to conversations;
        inference_mode (bool): if True, then remove the last eos token;
        system_prompt (str): the system prompt;
        **kwargs:

    Returns:
        outputs (BaseProcessorOutput): the output of the processor,
            - input_ids (torch.LongTensor): [N + image tokens]
            - target_ids (torch.LongTensor): [N + image tokens]
            - images (torch.FloatTensor): [n_images, 3, H, W]
            - image_id (int): the id of the image token
            - num_image_tokens (List[int]): the number of image tokens
    &#34;&#34;&#34;

    assert (
        prompt is None or conversations is None
    ), &#34;prompt and conversations cannot be used at the same time.&#34;

    (
        tokenized_str,
        masked_tokenized_str,
        images_list,
        images_seq_mask,
        images_spatial_crop,
    ) = self.format_messages_v2(conversations, images, max_req_input_len)

    assert (
        len(tokenized_str) == len(images_seq_mask) == len(masked_tokenized_str)
    ), (
        f&#34;tokenized_str&#39;s length {len(tokenized_str)}, input_ids&#39; length {len(masked_tokenized_str)}, &#34;
        f&#34;imags_seq_mask&#39;s length {len(images_seq_mask)}, are not equal&#34;
    )

    input_ids = torch.LongTensor(tokenized_str)
    target_ids = torch.LongTensor(masked_tokenized_str)
    images_seq_mask = torch.tensor(images_seq_mask, dtype=torch.bool)

    # set input_ids &lt; 0 | input_ids == self.image_token_id as ignore_id
    target_ids[(input_ids &lt; 0) | (input_ids == self.image_token_id)] = (
        self.ignore_id
    )
    input_ids[input_ids &lt; 0] = self.pad_id

    if inference_mode:
        assert input_ids[-1] == self.eos_id
        input_ids = input_ids[:-1]
        target_ids = target_ids[:-1]
        images_seq_mask = images_seq_mask[:-1]

    if len(images_list) == 0:
        images = torch.zeros((1, 3, self.image_size, self.image_size))
        images_spatial_crop = torch.zeros((1, 2), dtype=torch.long)
    else:
        images = torch.stack(images_list, dim=0)
        images_spatial_crop = torch.tensor(images_spatial_crop, dtype=torch.long)

    images_spatial_crop = torch.stack(
        [images_spatial_crop], dim=0
    )  # stack the tensor to make it a batch of 1

    prepare = VLChatProcessorOutput(
        input_ids=input_ids,
        target_ids=target_ids,
        pixel_values=images,
        images_seq_mask=images_seq_mask,
        images_spatial_crop=images_spatial_crop,
    )

    return prepare</code></pre>
</details>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>prompt</code></strong> :&ensp;<code>str</code></dt>
<dd>the formatted prompt;</dd>
<dt><strong><code>conversations</code></strong> :&ensp;<code>List[Dict]</code></dt>
<dd>conversations with a list of messages;</dd>
<dt><strong><code>images</code></strong> :&ensp;<code>List[ImageType]</code></dt>
<dd>the list of images;</dd>
<dt><strong><code>apply_sft_format</code></strong> :&ensp;<code>bool</code></dt>
<dd>if prompt is not None, then apply the SFT format to prompt;
if conversations is not None, then it will always apply the SFT format to conversations;</dd>
<dt><strong><code>inference_mode</code></strong> :&ensp;<code>bool</code></dt>
<dd>if True, then remove the last eos token;</dd>
<dt><strong><code>system_prompt</code></strong> :&ensp;<code>str</code></dt>
<dd>the system prompt;</dd>
</dl>
<p>**kwargs:</p>
<h2 id="returns">Returns</h2>
<p>outputs (BaseProcessorOutput): the output of the processor,
- input_ids (torch.LongTensor): [N + image tokens]
- target_ids (torch.LongTensor): [N + image tokens]
- images (torch.FloatTensor): [n_images, 3, H, W]
- image_id (int): the id of the image token
- num_image_tokens (List[int]): the number of image tokens</p></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.tokenize_with_images"><code class="name flex">
<span>def <span class="ident">tokenize_with_images</span></span>(<span>self,<br>conversation: str,<br>images: List[PIL.Image.Image],<br>bos: bool = True,<br>eos: bool = True,<br>cropping: bool = True,<br>max_req_input_len: int = -1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tokenize_with_images(
    self,
    conversation: str,
    images: List[Image.Image],
    bos: bool = True,
    eos: bool = True,
    cropping: bool = True,
    max_req_input_len: int = -1,
):
    &#34;&#34;&#34;Tokenize text with &lt;image&gt; tags.&#34;&#34;&#34;
    images_list, images_seq_mask, images_spatial_crop = [], [], []
    text_splits = conversation.split(self.image_token)
    tokenized_str = []
    for text_sep, image in zip(text_splits, images):
        &#34;&#34;&#34;encode text_sep&#34;&#34;&#34;
        tokenized_sep = self.encode(text_sep, bos=False, eos=False)
        tokenized_str += tokenized_sep
        images_seq_mask += [False] * len(tokenized_sep)

        &#34;&#34;&#34;select best resolution for anyres&#34;&#34;&#34;
        if cropping:
            best_width, best_height = select_best_resolution(
                image.size, self.candidate_resolutions
            )
        else:
            best_width, best_height = self.image_size, self.image_size
        # print(image.size, (best_width, best_height)) # check the select_best_resolutions func

        &#34;&#34;&#34;process the global view&#34;&#34;&#34;
        global_view = ImageOps.pad(
            image,
            (self.image_size, self.image_size),
            color=tuple(int(x * 255) for x in self.image_transform.mean),
        )
        images_list.append(self.image_transform(global_view))

        &#34;&#34;&#34;process the local views&#34;&#34;&#34;
        local_view = ImageOps.pad(
            image,
            (best_width, best_height),
            color=tuple(int(x * 255) for x in self.image_transform.mean),
        )
        for i in range(0, best_height, self.image_size):
            for j in range(0, best_width, self.image_size):
                images_list.append(
                    self.image_transform(
                        local_view.crop(
                            (j, i, j + self.image_size, i + self.image_size)
                        )
                    )
                )

        &#34;&#34;&#34;record height / width crop num&#34;&#34;&#34;
        num_width_tiles, num_height_tiles = (
            best_width // self.image_size,
            best_height // self.image_size,
        )
        images_spatial_crop.append([num_width_tiles, num_height_tiles])

        &#34;&#34;&#34;add image tokens&#34;&#34;&#34;
        h = w = math.ceil(
            (self.image_size // self.patch_size) / self.downsample_ratio
        )
        # global views tokens h * (w + 1), 1 is for line separator
        tokenized_image = [self.image_token_id] * h * (w + 1)
        # add a separator between global and local views
        tokenized_image += [self.image_token_id]
        # local views tokens, (num_height_tiles * h) * (num_width_tiles * w + 1)
        tokenized_image += (
            [self.image_token_id]
            * (num_height_tiles * h)
            * (num_width_tiles * w + 1)
        )

        tokenized_str += tokenized_image
        images_seq_mask += [True] * len(tokenized_image)
        # print(width_crop_num, height_crop_num, len(tokenized_image)) # test the correctness of the number of image-related tokens

    &#34;&#34;&#34;process the last text split&#34;&#34;&#34;
    tokenized_sep = self.encode(text_splits[-1], bos=False, eos=False)
    # deal with video, limit with request len
    if max_req_input_len &gt; -1:
        if max_req_input_len &lt; len(tokenized_sep) + len(tokenized_str) - 1:
            rest = max_req_input_len - len(tokenized_sep) - 1 - 1024
            tokenized_str = tokenized_str[:rest]
            images_seq_mask = images_seq_mask[:rest]
    tokenized_str += tokenized_sep
    images_seq_mask += [False] * len(tokenized_sep)

    &#34;&#34;&#34;add the bos and eos tokens&#34;&#34;&#34;
    if bos:
        tokenized_str = [self.bos_id] + tokenized_str
        images_seq_mask = [False] + images_seq_mask
    if eos:
        tokenized_str = tokenized_str + [self.eos_id]
        images_seq_mask = images_seq_mask + [False]

    assert len(tokenized_str) == len(
        images_seq_mask
    ), f&#34;tokenize_with_images func: tokenized_str&#39;s length {len(tokenized_str)} is not equal to imags_seq_mask&#39;s length {len(images_seq_mask)}&#34;

    return tokenized_str, images_list, images_seq_mask, images_spatial_crop</code></pre>
</details>
<div class="desc"><p>Tokenize text with <image> tags.</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DictOutput"><code class="flex name class">
<span>class <span class="ident">DictOutput</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DictOutput(object):
    def items(self):
        return self.__dict__.items()

    def keys(self):
        return self.__dict__.keys()

    def __getitem__(self, item):
        return self.__dict__[item]

    def __contains__(self, key):
        return key in self.__dict__

    def __setitem__(self, key, value):
        self.__dict__[key] = value</code></pre>
</details>
<div class="desc"></div>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.configs.deepseekvl2.VLChatProcessorOutput" href="#sglang.srt.configs.deepseekvl2.VLChatProcessorOutput">VLChatProcessorOutput</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.configs.deepseekvl2.DictOutput.items"><code class="name flex">
<span>def <span class="ident">items</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def items(self):
    return self.__dict__.items()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.DictOutput.keys"><code class="name flex">
<span>def <span class="ident">keys</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def keys(self):
    return self.__dict__.keys()</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.ImageTransform"><code class="flex name class">
<span>class <span class="ident">ImageTransform</span></span>
<span>(</span><span>mean: Tuple[float, float, float] | None = (0.5, 0.5, 0.5),<br>std: Tuple[float, float, float] | None = (0.5, 0.5, 0.5),<br>normalize: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ImageTransform(object):
    def __init__(
        self,
        mean: Optional[Tuple[float, float, float]] = (0.5, 0.5, 0.5),
        std: Optional[Tuple[float, float, float]] = (0.5, 0.5, 0.5),
        normalize: bool = True,
    ):
        self.mean = mean
        self.std = std
        self.normalize = normalize

        # only load torchvision.transforms when needed
        try:
            import torchvision.transforms as T

            # FIXME: add version check for gguf
        except ImportError as err:
            raise ImportError(
                &#34;Please install torchvision via `pip install torchvision` to use Deepseek-VL2.&#34;
            ) from err

        transform_pipelines = [T.ToTensor()]

        if normalize:
            transform_pipelines.append(T.Normalize(mean, std))

        self.transform = T.Compose(transform_pipelines)

    def __call__(self, pil_img: Image.Image):
        x = self.transform(pil_img)
        return x</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.VLChatProcessorOutput"><code class="flex name class">
<span>class <span class="ident">VLChatProcessorOutput</span></span>
<span>(</span><span>input_ids: torch.LongTensor,<br>target_ids: torch.LongTensor,<br>pixel_values: torch.Tensor,<br>images_seq_mask: torch.BoolTensor,<br>images_spatial_crop: torch.LongTensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class VLChatProcessorOutput(DictOutput):
    input_ids: torch.LongTensor
    target_ids: torch.LongTensor
    pixel_values: (
        torch.Tensor
    )  # rename from &#34;images&#34; to &#34;pixel_values&#34; for compatibility
    images_seq_mask: torch.BoolTensor
    images_spatial_crop: torch.LongTensor

    def __len__(self):
        return len(self.input_ids)</code></pre>
</details>
<div class="desc"><p>VLChatProcessorOutput(input_ids: torch.LongTensor, target_ids: torch.LongTensor, pixel_values: torch.Tensor, images_seq_mask: torch.BoolTensor, images_spatial_crop: torch.LongTensor)</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.configs.deepseekvl2.DictOutput" href="#sglang.srt.configs.deepseekvl2.DictOutput">DictOutput</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.configs.deepseekvl2.VLChatProcessorOutput.images_seq_mask"><code class="name">var <span class="ident">images_seq_mask</span> : torch.BoolTensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.VLChatProcessorOutput.images_spatial_crop"><code class="name">var <span class="ident">images_spatial_crop</span> : torch.LongTensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.VLChatProcessorOutput.input_ids"><code class="name">var <span class="ident">input_ids</span> : torch.LongTensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.VLChatProcessorOutput.pixel_values"><code class="name">var <span class="ident">pixel_values</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.configs.deepseekvl2.VLChatProcessorOutput.target_ids"><code class="name">var <span class="ident">target_ids</span> : torch.LongTensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.configs" href="index.html">sglang.srt.configs</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.configs.deepseekvl2.select_best_resolution" href="#sglang.srt.configs.deepseekvl2.select_best_resolution">select_best_resolution</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.configs.deepseekvl2.DeepseekV2Config" href="#sglang.srt.configs.deepseekvl2.DeepseekV2Config">DeepseekV2Config</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekV2Config.keys_to_ignore_at_inference" href="#sglang.srt.configs.deepseekvl2.DeepseekV2Config.keys_to_ignore_at_inference">keys_to_ignore_at_inference</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekV2Config.model_type" href="#sglang.srt.configs.deepseekvl2.DeepseekV2Config.model_type">model_type</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2Config" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2Config">DeepseekVL2Config</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2Config.candidate_resolutions" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2Config.candidate_resolutions">candidate_resolutions</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2Config.global_view_pos" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2Config.global_view_pos">global_view_pos</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2Config.language_config" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2Config.language_config">language_config</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2Config.model_type" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2Config.model_type">model_type</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2Config.projector_config" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2Config.projector_config">projector_config</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2Config.tile_tag" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2Config.tile_tag">tile_tag</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2Config.vision_config" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2Config.vision_config">vision_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig">DeepseekVL2MlpProjectorConfig</a></code></h4>
<ul class="two-column">
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.depth" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.depth">depth</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.downsample_ratio" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.downsample_ratio">downsample_ratio</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.input_dim" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.input_dim">input_dim</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.mlp_ratio" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.mlp_ratio">mlp_ratio</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.model_type" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.model_type">model_type</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.n_embed" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.n_embed">n_embed</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.projector_type" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.projector_type">projector_type</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.token_pooling" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig.token_pooling">token_pooling</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig">DeepseekVL2VisionEncoderConfig</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.class_token" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.class_token">class_token</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.deterministic" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.deterministic">deterministic</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.global_pool" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.global_pool">global_pool</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.heads" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.heads">heads</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.ignore_head" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.ignore_head">ignore_head</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.image_size" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.image_size">image_size</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.layers" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.layers">layers</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.mlp_ratio" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.mlp_ratio">mlp_ratio</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.model_name" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.model_name">model_name</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.model_type" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.model_type">model_type</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.num_classes" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.num_classes">num_classes</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.num_recomputing_layers" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.num_recomputing_layers">num_recomputing_layers</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.patch_size" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.patch_size">patch_size</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.use_checkpoint" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.use_checkpoint">use_checkpoint</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.weight_init" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.weight_init">weight_init</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.width" href="#sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig.width">width</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor" href="#sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor">DeepseekVLV2Processor</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.attributes" href="#sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.attributes">attributes</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.bos_id" href="#sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.bos_id">bos_id</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.decode" href="#sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.decode">decode</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.encode" href="#sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.encode">encode</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.eos_id" href="#sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.eos_id">eos_id</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.find_all_indices" href="#sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.find_all_indices">find_all_indices</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.format_messages_v2" href="#sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.format_messages_v2">format_messages_v2</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.pad_id" href="#sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.pad_id">pad_id</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.process_one" href="#sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.process_one">process_one</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.tokenize_with_images" href="#sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.tokenize_with_images">tokenize_with_images</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.tokenizer_class" href="#sglang.srt.configs.deepseekvl2.DeepseekVLV2Processor.tokenizer_class">tokenizer_class</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.configs.deepseekvl2.DictOutput" href="#sglang.srt.configs.deepseekvl2.DictOutput">DictOutput</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.configs.deepseekvl2.DictOutput.items" href="#sglang.srt.configs.deepseekvl2.DictOutput.items">items</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.DictOutput.keys" href="#sglang.srt.configs.deepseekvl2.DictOutput.keys">keys</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.configs.deepseekvl2.ImageTransform" href="#sglang.srt.configs.deepseekvl2.ImageTransform">ImageTransform</a></code></h4>
</li>
<li>
<h4><code><a title="sglang.srt.configs.deepseekvl2.VLChatProcessorOutput" href="#sglang.srt.configs.deepseekvl2.VLChatProcessorOutput">VLChatProcessorOutput</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.configs.deepseekvl2.VLChatProcessorOutput.images_seq_mask" href="#sglang.srt.configs.deepseekvl2.VLChatProcessorOutput.images_seq_mask">images_seq_mask</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.VLChatProcessorOutput.images_spatial_crop" href="#sglang.srt.configs.deepseekvl2.VLChatProcessorOutput.images_spatial_crop">images_spatial_crop</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.VLChatProcessorOutput.input_ids" href="#sglang.srt.configs.deepseekvl2.VLChatProcessorOutput.input_ids">input_ids</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.VLChatProcessorOutput.pixel_values" href="#sglang.srt.configs.deepseekvl2.VLChatProcessorOutput.pixel_values">pixel_values</a></code></li>
<li><code><a title="sglang.srt.configs.deepseekvl2.VLChatProcessorOutput.target_ids" href="#sglang.srt.configs.deepseekvl2.VLChatProcessorOutput.target_ids">target_ids</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
