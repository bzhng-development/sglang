<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.models.phi4mm_audio API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.models.phi4mm_audio</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.models.phi4mm_audio.AudioEmbedding"><code class="flex name class">
<span>class <span class="ident">AudioEmbedding</span></span>
<span>(</span><span>config:Â transformers.configuration_utils.PretrainedConfig, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AudioEmbedding(nn.Module):
    &#34;&#34;&#34;Image embedding.&#34;&#34;&#34;

    def __init__(self, config: PretrainedConfig, **kwargs) -&gt; None:
        super().__init__()
        self.config = config
        # n_embed or hidden_size for text LM
        hidden_size = config.n_embd if hasattr(config, &#34;n_embd&#34;) else config.hidden_size

        # self.wte = nn.Embedding(config.vocab_size, hidden_size)

        audio_dim_out = (
            None  # Set this variable according to the actual audio processor
        )
        self.layer_idx = -2

        if (
            isinstance(config.audio_processor, dict)
            and config.audio_processor.get(&#34;name&#34;, None) == &#34;cascades&#34;
        ):
            encoder_config = config.audio_processor.get(&#34;config&#34;, None)
            assert encoder_config is not None
            self.encoder = ConformerEncoder(**encoder_config)

            audio_dim_out = encoder_config[&#34;attention_dim&#34;]
            n_mels = encoder_config[&#34;input_size&#34;]
        else:
            raise NotImplementedError(&#34;&#34;)

        assert audio_dim_out is not None, &#34;Remember to set values for audio_dim_out&#34;
        self.audio_dim_out = audio_dim_out
        self.audio_dim_in = n_mels

        self.freeze_audio_processor = kwargs.get(&#34;freeze_audio_processor&#34;, False)

        self.downsample_rate = kwargs.get(&#34;downsample_rate&#34;, 1)

        if kwargs.get(&#34;use_qformer&#34;, False):
            qformer_config = kwargs.get(&#34;qformer_config&#34;, {})
            qformer_config[&#34;attention_dim&#34;] = audio_dim_out
            self.qformer = WindowQformer(**qformer_config)
        else:
            self.qformer = None

        if kwargs.get(&#34;use_conv_downsample&#34;, False):
            assert (
                self.qformer is None
            ), &#34;don&#39;t support use qformer and conv downsample together&#34;
            nemo_conv_settings = kwargs.get(&#34;nemo_conv_settings&#34;, {})
            default_nemo_conv_settings = {
                &#34;subsampling&#34;: &#34;dw_striding&#34;,
                &#34;subsampling_factor&#34;: self.downsample_rate,
                &#34;feat_in&#34;: audio_dim_out,
                &#34;feat_out&#34;: audio_dim_out,
                &#34;conv_channels&#34;: 256,
                &#34;subsampling_conv_chunking_factor&#34;: 1,
                &#34;activation&#34;: nn.ReLU(),
                &#34;is_causal&#34;: False,
            }
            # Override any of the defaults with the incoming, user settings
            if nemo_conv_settings:
                default_nemo_conv_settings.update(nemo_conv_settings)
                for i in [&#34;subsampling_factor&#34;, &#34;feat_in&#34;, &#34;feat_out&#34;]:
                    assert (
                        i not in nemo_conv_settings
                    ), &#34;{i} should be specified outside of the NeMo dictionary&#34;

            self.conv_ds = NemoConvSubsampling(
                **default_nemo_conv_settings,
            )
        else:
            self.conv_ds = None

        projection_cls = kwargs.get(&#34;projection_cls&#34;, &#34;linear&#34;)
        if projection_cls == &#34;linear&#34;:
            self.audio_projection = nn.Linear(audio_dim_out, hidden_size)
        elif projection_cls == &#34;mlp&#34;:
            # follow llava-v1.5&#39;s implementation
            # (do not use image_projection and image_proj_norm)
            dim_projection = hidden_size
            depth = 2
            self.linear_downsample_rate = (
                1 if (self.qformer or self.conv_ds) else self.downsample_rate
            )
            layers = [
                nn.Linear(audio_dim_out * self.linear_downsample_rate, dim_projection)
            ]
            for _ in range(1, depth):
                layers.extend([nn.GELU(), nn.Linear(dim_projection, dim_projection)])
            self.audio_projection = nn.Sequential(*layers)
            # NOTE vision-speech tasks use a separate projection layer
            layers = [
                nn.Linear(audio_dim_out * self.linear_downsample_rate, dim_projection)
            ]
            for _ in range(1, depth):
                layers.extend([nn.GELU(), nn.Linear(dim_projection, dim_projection)])
            self.audio_projection_for_vision = nn.Sequential(*layers)
        else:
            raise NotImplementedError(
                f&#34;projection_cls = {projection_cls}, not implemented&#34;
            )

        # TODO: audio sequence compression - Qformer
        self.vocab_size = config.vocab_size
        self.input_embeds = None
        self.audio_embed_sizes = None

    def set_audio_embeds(self, input_embeds: torch.FloatTensor) -&gt; None:
        self.input_embeds = input_embeds

    def set_audio_embed_sizes(self, audio_embed_sizes: torch.LongTensor) -&gt; None:
        self.audio_embed_sizes = audio_embed_sizes

    def get_audio_features(
        self,
        input_embeds: torch.FloatTensor,
        audio_attention_mask: torch.Tensor = None,
        audio_projection_mode: str = &#34;speech&#34;,
    ) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;
        arguments:
            input_embeds: audio features (B, T, D)  B: num audios in a sequence
        &#34;&#34;&#34;
        if self.freeze_audio_processor:
            with torch.no_grad():
                audio_features, masks = self.encoder(input_embeds, audio_attention_mask)
        else:
            audio_features, masks = self.encoder(input_embeds, audio_attention_mask)

        if self.qformer is not None:
            audio_features, _ = self.qformer(audio_features, mask=None)

        if self.conv_ds is not None:
            if masks is not None:
                masks = masks.squeeze(1)

            audio_features, masks = self.conv_ds(audio_features, mask=masks)

        if self.linear_downsample_rate != 1:
            bs, seq_len, feat_dim = audio_features.size()
            padding = seq_len % self.linear_downsample_rate
            if padding &gt; 0:
                audio_features = F.pad(
                    audio_features,
                    (0, 0, 0, self.linear_downsample_rate - padding),
                    &#34;constant&#34;,
                    0,
                )

            seq_len = audio_features.size(1)
            audio_features = audio_features.view(
                bs,
                seq_len // self.linear_downsample_rate,
                feat_dim * self.linear_downsample_rate,
            )

        if audio_projection_mode == &#34;speech&#34;:
            audio_set_tensor = self.audio_projection(audio_features)
        elif audio_projection_mode == &#34;vision&#34;:
            audio_set_tensor = self.audio_projection_for_vision(audio_features)
        else:
            raise ValueError(
                f&#34;audio_projection_mode = {audio_projection_mode} not &#34; &#34;implemented&#34;
            )

        return audio_set_tensor

    def forward(
        self,
        audio_features: torch.FloatTensor,
        audio_attention_mask: torch.Tensor = None,
        audio_projection_mode: str = &#34;speech&#34;,
    ) -&gt; torch.FloatTensor:
        &#34;&#34;&#34;
        arguments:
            audio_features: audio features (num_audio_tokens, T, D)

        returns:
            audio_embeds: audio embeddings (num_audio_tokens, hidden_dim)
        &#34;&#34;&#34;
        audio_embeds = self.get_audio_features(
            audio_features,
            audio_attention_mask=audio_attention_mask,
            audio_projection_mode=audio_projection_mode,
        )
        return audio_embeds</code></pre>
</details>
<div class="desc"><p>Image embedding.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_audio.AudioEmbedding.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>audio_features:Â torch.FloatTensor,<br>audio_attention_mask:Â torch.TensorÂ =Â None,<br>audio_projection_mode:Â strÂ =Â 'speech') â>Â torch.FloatTensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    audio_features: torch.FloatTensor,
    audio_attention_mask: torch.Tensor = None,
    audio_projection_mode: str = &#34;speech&#34;,
) -&gt; torch.FloatTensor:
    &#34;&#34;&#34;
    arguments:
        audio_features: audio features (num_audio_tokens, T, D)

    returns:
        audio_embeds: audio embeddings (num_audio_tokens, hidden_dim)
    &#34;&#34;&#34;
    audio_embeds = self.get_audio_features(
        audio_features,
        audio_attention_mask=audio_attention_mask,
        audio_projection_mode=audio_projection_mode,
    )
    return audio_embeds</code></pre>
</details>
<div class="desc"><p>arguments:
audio_features: audio features (num_audio_tokens, T, D)</p>
<p>returns:
audio_embeds: audio embeddings (num_audio_tokens, hidden_dim)</p></div>
</dd>
<dt id="sglang.srt.models.phi4mm_audio.AudioEmbedding.get_audio_features"><code class="name flex">
<span>def <span class="ident">get_audio_features</span></span>(<span>self,<br>input_embeds:Â torch.FloatTensor,<br>audio_attention_mask:Â torch.TensorÂ =Â None,<br>audio_projection_mode:Â strÂ =Â 'speech') â>Â torch.FloatTensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_audio_features(
    self,
    input_embeds: torch.FloatTensor,
    audio_attention_mask: torch.Tensor = None,
    audio_projection_mode: str = &#34;speech&#34;,
) -&gt; torch.FloatTensor:
    &#34;&#34;&#34;
    arguments:
        input_embeds: audio features (B, T, D)  B: num audios in a sequence
    &#34;&#34;&#34;
    if self.freeze_audio_processor:
        with torch.no_grad():
            audio_features, masks = self.encoder(input_embeds, audio_attention_mask)
    else:
        audio_features, masks = self.encoder(input_embeds, audio_attention_mask)

    if self.qformer is not None:
        audio_features, _ = self.qformer(audio_features, mask=None)

    if self.conv_ds is not None:
        if masks is not None:
            masks = masks.squeeze(1)

        audio_features, masks = self.conv_ds(audio_features, mask=masks)

    if self.linear_downsample_rate != 1:
        bs, seq_len, feat_dim = audio_features.size()
        padding = seq_len % self.linear_downsample_rate
        if padding &gt; 0:
            audio_features = F.pad(
                audio_features,
                (0, 0, 0, self.linear_downsample_rate - padding),
                &#34;constant&#34;,
                0,
            )

        seq_len = audio_features.size(1)
        audio_features = audio_features.view(
            bs,
            seq_len // self.linear_downsample_rate,
            feat_dim * self.linear_downsample_rate,
        )

    if audio_projection_mode == &#34;speech&#34;:
        audio_set_tensor = self.audio_projection(audio_features)
    elif audio_projection_mode == &#34;vision&#34;:
        audio_set_tensor = self.audio_projection_for_vision(audio_features)
    else:
        raise ValueError(
            f&#34;audio_projection_mode = {audio_projection_mode} not &#34; &#34;implemented&#34;
        )

    return audio_set_tensor</code></pre>
</details>
<div class="desc"><p>arguments:
input_embeds: audio features (B, T, D)
B: num audios in a sequence</p></div>
</dd>
<dt id="sglang.srt.models.phi4mm_audio.AudioEmbedding.set_audio_embed_sizes"><code class="name flex">
<span>def <span class="ident">set_audio_embed_sizes</span></span>(<span>self, audio_embed_sizes:Â torch.LongTensor) â>Â None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_audio_embed_sizes(self, audio_embed_sizes: torch.LongTensor) -&gt; None:
    self.audio_embed_sizes = audio_embed_sizes</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.phi4mm_audio.AudioEmbedding.set_audio_embeds"><code class="name flex">
<span>def <span class="ident">set_audio_embeds</span></span>(<span>self, input_embeds:Â torch.FloatTensor) â>Â None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_audio_embeds(self, input_embeds: torch.FloatTensor) -&gt; None:
    self.input_embeds = input_embeds</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.phi4mm_audio.ConformerEncoder"><code class="flex name class">
<span>class <span class="ident">ConformerEncoder</span></span>
<span>(</span><span>input_size,<br>chunk_size,<br>left_chunk,<br>num_lang=None,<br>attention_dim=256,<br>attention_heads=4,<br>linear_units=2048,<br>num_blocks=6,<br>dropout_rate=0.1,<br>input_layer='nemo_conv',<br>causal=True,<br>batch_norm=False,<br>cnn_out=-1,<br>cnn_layer_norm=False,<br>ext_pw_out_channel=0,<br>ext_pw_kernel_size=1,<br>depthwise_seperable_out_channel=256,<br>depthwise_multiplier=1,<br>chunk_se=0,<br>kernel_size=3,<br>activation='relu',<br>conv_activation='relu',<br>conv_glu_type='sigmoid',<br>bias_in_glu=True,<br>linear_glu_in_convm=False,<br>attention_glu_type='swish',<br>export=False,<br>extra_layer_output_idx=-1,<br>extra_multi_layer_output_idxs=[],<br>activation_checkpointing='',<br>relative_attention_bias_args=None,<br>time_reduction=4,<br>use_pt_scaled_dot_product_attention=False,<br>nemo_conv_settings=None,<br>conv2d_extra_padding:Â Literal['feat',Â 'feat_time',Â 'none',Â True]Â =Â 'none',<br>replication_pad_for_subsample_embedding=False,<br>attention_group_size=1,<br>encoder_embedding_config=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConformerEncoder(TransformerEncoderBase):
    &#34;&#34;&#34;ConformerEncoder module.
    see original paper for more details:
        https://arxiv.org/abs/2005.08100

    Please set causal = True in streaming model
    Args:
        input_size: int
            input feature dimension.
        chunk_size: int, list(int)
            Number of frames for each chunk
            This variable can take 2 forms:
            int:  Used for inference, or single chunk size training
            list(int) : Used only for variable chunk size training
            Some examples for the 2 cases:
            chunk_size = 12
            chunk_size = [6, 8, 12, 24]
        left_chunk: int, list(int)
            Number of chunks used for masking in streaming mode.
            This variable can take 2 forms:
            int:  Used for inference, or single chunk size training
            list(int) : Used only for variable chunk size training. When
            chunk_size is a list, left_chunk must be a list with same length.
            Some examples for the 2 cases:
            left_chunk = 6
            left_chunk = [12, 9, 6, 3]
        left_chunk: int
            number of chunks used for masking in streaming mode.
        num_lang: int
            This parameter is used to store the number of languages in the
            lang_dict, only used for multiseed/multilingual models.
            default None.
        attention_dim: int, optional
            attention dimension. default 256.
        attention_heads: int, optional
            the number of heads. default 4
        linear_units:
            the number of units of position-wise feed forward.
            default 2048
        num_block:
            number of Transformer layer. default 6
        dropout_rate: float, optional
            dropout rate. default 0.1
        input_layer: str, optional
            input layer type before Conformer,
            one of [&#34;linear&#34;, &#34;conv2d&#34;, &#34;custom&#34;, &#34;vgg2l&#34;, &#34;embed&#34;],
            default &#34;conv2d&#34;
        causal: bool, optional
            if set to True, convolution have no access
             to future frames. default False.
        batch_norm: bool, optional
            if set to True, apply batchnorm before activation
            in ConvModule layer of the conformer.
            default False
        cnn_out: int, optional
            the number of CNN channels before Conformer.
            default -1.
        cnn_layer_norm: bool, optional
            layer norm between Conformer and the first CNN.
            default False.
        ext_pw_out_channel: int, optional
            the number of channel for CNN
            before depthwise_seperable_CNN.
            If 0 then use linear. default 0.
        ext_pw_kernel_size: int, optional
            kernel size of N before depthwise_seperable_CNN.
            only work for ext_pw_out_channel &gt; 0.
            default 1
        depthwise_seperable_out_channel: int, optional
            the number of channel for
            depthwise_seperable_CNN.
            default 256.
        depthwise_multiplier: int, optional
            the number of multiplier for
            depthwise_seperable_CNN.
            default 1.
        chunk_se: int, optional
            0 for offline SE.
            1 for streaming SE, where mean is computed
             by accumulated history until current chunk_se.
            2 for streaming SE, where mean is computed
             by only the current chunk.
            default 0.
        kernel_size: int, optional
            the number of kernels for depthwise_seperable_CNN.
            default 3.
        activation: str, optional
            FeedForward block activation.
            one of [&#34;relu&#34;, &#34;swish&#34;, &#34;sigmoid&#34;]
            default &#34;relu&#34;.
        conv_activation: str, optional
            activation function used in ConvModule part
            of the conformer, default &#34;relu&#34;.
        conv_glu_type: str, optional
            activation used use glu in depthwise_seperable_CNN,
            default &#34;sigmoid&#34;
        bias_in_glu: bool, optional
            if set to True, use additive bias in the weight module
             before GLU. default True
        linear_glu_in_convm: bool, optional
            if set to True, use GLULinear module,
             otherwise, used GLUPointWiseConv module.
              default to False.
        attention_glu_type: str
            only work for glu_in_attention !=0
            default &#34;swish&#34;.
        export: bool, optional
            if set to True, it remove the padding from convolutional layers
             and allow the onnx conversion for inference.
              default False.
        activation_checkpointing: str, optional
            a dictionarry of {&#34;module&#34;,&#34;interval&#34;,&#34;offload&#34;}, where
                &#34;module&#34;: str
                    accept [&#34;transformer&#34;, &#34;attention&#34;] to select
                    which module should do activation checkpointing.
                &#34;interval&#34;: int, default 1,
                    interval of applying activation checkpointing,
                    interval = 1 means that we apply checkpointing
                    on every layer (if activation), otherwise,
                    we apply it every x interval.
                &#34;offload&#34;: bool, default False,
                    if set to True, we offload activation to cpu and
                    reload it during backward, otherwise,
                    we recalculate activation in backward.
            default &#34;&#34;.
        extra_layer_output_idx: int
            the layer index to be exposed.
        relative_attention_bias_args: dict, optional
            use more efficient scalar bias-based relative multihead attention
            (Q*K^T + B) implemented in cmb.basics.embedding.
            [T5/ALiBi]RelativeAttentionLogitBias
            usage: relative_attention_bias_args={&#34;type&#34;: t5/alibi}
            additional method-specific arguments can be provided (see
            transformer_base.py)
        time_reduction: int optional
            time reduction factor
            default 4
        use_pt_scaled_dot_product_attention: whether to use pytorch scaled
            dot product attention in training.
            Default: False
        nemo_conv_settings: dict, optional
            A dictionary of settings for NeMo Subsampling.
            default: None
            usage: nemo_conv_settings=
                {
                    &#34;subsampling&#34;:
                    dw_striding/striding/dw_striding_conv1d/striding_conv1d,
                    &#34;conv_channels&#34;: int,
                    &#34;subsampling_conv_chunking_factor&#34;: int,
                    &#34;is_causal&#34;: True/False
                }
        conv2d_extra_padding: str, optional
            Add extra padding in conv2d subsampling layers. Choices are
            (feat, feat_time, none, True)
            Default: none
        replication_pad_for_subsample_embedding:  For batched-streaming
            decoding, use &#34;replication&#34; padding for the cache at start of
            utterance.
            Default: False
        attention_group_size: int, optional
            the number of groups to use for attention, default 1
            (Multi-Head Attention),
            1 = typical Multi-Head Attention,
            1 &lt; attention_group_size &lt; attention_heads = Grouped-Query
            Attention
            attention_group_size = attention_heads = Multi-Query Attention
    &#34;&#34;&#34;

    extra_multi_layer_output_idxs: list[int]

    def __init__(  # pylint: disable-all
        self,
        input_size,
        chunk_size,
        left_chunk,
        num_lang=None,
        attention_dim=256,
        attention_heads=4,
        linear_units=2048,
        num_blocks=6,
        dropout_rate=0.1,
        input_layer=&#34;nemo_conv&#34;,
        causal=True,
        batch_norm=False,
        cnn_out=-1,
        cnn_layer_norm=False,
        ext_pw_out_channel=0,
        ext_pw_kernel_size=1,
        depthwise_seperable_out_channel=256,
        depthwise_multiplier=1,
        chunk_se=0,
        kernel_size=3,
        activation=&#34;relu&#34;,
        conv_activation=&#34;relu&#34;,
        conv_glu_type=&#34;sigmoid&#34;,
        bias_in_glu=True,
        linear_glu_in_convm=False,
        attention_glu_type=&#34;swish&#34;,
        export=False,
        extra_layer_output_idx=-1,
        extra_multi_layer_output_idxs=[],  # noqa
        activation_checkpointing=&#34;&#34;,
        relative_attention_bias_args=None,
        time_reduction=4,
        use_pt_scaled_dot_product_attention=False,
        nemo_conv_settings=None,
        conv2d_extra_padding: Literal[&#34;feat&#34;, &#34;feat_time&#34;, &#34;none&#34;, True] = &#34;none&#34;,
        replication_pad_for_subsample_embedding=False,
        attention_group_size=1,
        encoder_embedding_config=None,
    ):
        super().__init__(
            input_size,
            chunk_size,
            left_chunk,
            attention_dim,
            attention_heads,
            input_layer,
            cnn_out,
            cnn_layer_norm,
            time_reduction,
            dropout_rate=dropout_rate,
            relative_attention_bias_args=relative_attention_bias_args,
            positional_dropout_rate=0.0,
            nemo_conv_settings=nemo_conv_settings,
            conv2d_extra_padding=conv2d_extra_padding,
            attention_group_size=attention_group_size,
            encoder_embedding_config=encoder_embedding_config,
        )
        self.num_blocks = num_blocks
        self.num_lang = num_lang
        self.kernel_size = kernel_size
        self.replication_pad_for_subsample_embedding: bool = (
            replication_pad_for_subsample_embedding
        )
        assert (
            self.num_heads % attention_group_size == 0
        ), &#34;attention_group_size must divide n_head&#34;
        self.num_heads_k = self.num_heads // attention_group_size

        self.encoders = MultiSequential(
            *[
                ConformerEncoderLayer(
                    d_model=attention_dim,
                    ext_pw_out_channel=ext_pw_out_channel,
                    depthwise_seperable_out_channel=depthwise_seperable_out_channel,
                    depthwise_multiplier=depthwise_multiplier,
                    n_head=attention_heads,
                    d_ffn=linear_units,
                    ext_pw_kernel_size=ext_pw_kernel_size,
                    kernel_size=kernel_size,
                    dropout_rate=dropout_rate,
                    causal=causal,
                    batch_norm=batch_norm,
                    activation=activation,
                    chunk_se=chunk_se,
                    chunk_size=chunk_size,
                    conv_activation=conv_activation,
                    conv_glu_type=conv_glu_type,
                    bias_in_glu=bias_in_glu,
                    linear_glu_in_convm=linear_glu_in_convm,
                    attention_glu_type=attention_glu_type,
                    activation_checkpointing=activation_checkpointing,
                    export=export,
                    use_pt_scaled_dot_product_attention=use_pt_scaled_dot_product_attention,
                    attn_group_sizes=attention_group_size,
                )
                for _ in range(num_blocks)
            ]
        )
        self.extra_layer_output_idx = extra_layer_output_idx
        self.extra_multi_layer_output_idxs = extra_multi_layer_output_idxs
        # Make a zeros scalar we can use in get_initial_state to determine
        # the device and the needed dtype:
        self.register_buffer(&#34;dev_type&#34;, torch.zeros(()), persistent=False)

    def init_relative_attention_bias(self, input_tensor):
        if self.relative_attention_bias_layer:
            return self.relative_attention_bias_layer(input_tensor)

    def calculate_hs_mask(self, xs_pad, device, mask):
        max_audio_length = xs_pad.shape[1]
        batch_size = xs_pad.shape[0]
        enc_streaming_mask = self._streaming_mask(
            max_audio_length, batch_size, self.chunk_size, self.left_chunk
        )
        enc_streaming_mask = enc_streaming_mask.to(device)
        if mask is None:
            return enc_streaming_mask

        feature_lens = mask.sum(1)
        padding_length = feature_lens
        pad_mask = torch.arange(0, max_audio_length, device=device).expand(
            padding_length.size(0), -1
        ) &lt; padding_length.unsqueeze(1)
        pad_mask = pad_mask.unsqueeze(1)
        pad_mask = pad_mask &amp; enc_streaming_mask
        return pad_mask

    @torch.jit.ignore
    def forward(self, xs_pad, masks):
        &#34;&#34;&#34;Conformer Forward function

        Args:
            xs_pad: torch.Tensor
                input tensor
            masks: torch.Tensor
                post-embedding input lengths
        &#34;&#34;&#34;
        xs_pad = self.encoder_embedding(xs_pad)
        input_tensor, pos_k, pos_v, hs_mask, masks = self.forward_embeddings(
            xs_pad, masks
        )

        unfolded = False
        ori_bz, seq_len, D = input_tensor.shape
        max_seq_len = 500  # maximum position for absolute positional encoding
        if seq_len &gt; max_seq_len:
            # audio sequence is longer than max_seq_len, unfold it into chunks
            # of max_seq_len
            unfolded = True
            # the unfold op will drop residual frames, pad it to the multiple
            # of max_seq_len
            if seq_len % max_seq_len &gt; 0:
                chunk_pad_size = max_seq_len - (seq_len % max_seq_len)
            else:
                chunk_pad_size = 0
            if chunk_pad_size &gt; 0:
                input_tensor_pad = F.pad(
                    input_tensor, (0, 0, 0, chunk_pad_size), &#34;constant&#34;, 0
                )
                input_tensor = input_tensor_pad.to(input_tensor.device)
            input_tensor = unfold_tensor(input_tensor, max_seq_len)
            if masks is not None:
                # revise hs_mask here because the previous calculated hs_mask
                # did not consider extra pad
                subsampled_pad_mask = masks.squeeze(
                    1
                )  # [bz, subsampled_unmask_seq_len]
                extra_padded_subsamlped_pad_mask = F.pad(
                    subsampled_pad_mask, (0, chunk_pad_size), &#34;constant&#34;, False
                )  # extra padding to the pad mask
                extra_padded_subsamlped_pad_mask = (
                    extra_padded_subsamlped_pad_mask.unsqueeze(-1).float()
                )
                masks_unfold = unfold_tensor(
                    extra_padded_subsamlped_pad_mask, max_seq_len
                )  # unfold the pad mask like we did to the input tensor
                masks_unfold = masks_unfold.squeeze(
                    -1
                ).bool()  # unfold op does not support bool tensor
            else:
                masks_unfold = None
            hs_mask = self.calculate_hs_mask(
                input_tensor, input_tensor.device, masks_unfold
            )  # calculate hs_mask based on the unfolded pad mask

        # layer_emb = None

        relative_attention_bias = self.init_relative_attention_bias(input_tensor)

        _simplified_path = (
            self.extra_layer_output_idx == -1 and relative_attention_bias is None
        )

        if _simplified_path:
            input_tensor, *_ = self.encoders(input_tensor, pos_k, pos_v, hs_mask)
        else:
            for i, layer in enumerate(self.encoders):
                input_tensor, _, _, _ = layer(
                    input_tensor,
                    pos_k,
                    pos_v,
                    hs_mask,
                    relative_attention_bias=relative_attention_bias,
                )

                # if i == self.extra_layer_output_idx:
                #     layer_emb = input_tensor

        if unfolded:
            embed_dim = input_tensor.shape[-1]
            input_tensor = input_tensor.reshape(ori_bz, -1, embed_dim)
            # if we ever padded before unfolding, we need to remove the padding
            if chunk_pad_size &gt; 0:
                input_tensor = input_tensor[:, :-chunk_pad_size, :]

        return input_tensor, masks  # , layer_emb</code></pre>
</details>
<div class="desc"><p>ConformerEncoder module.
see original paper for more details:
<a href="https://arxiv.org/abs/2005.08100">https://arxiv.org/abs/2005.08100</a></p>
<p>Please set causal = True in streaming model</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_size</code></strong></dt>
<dd>int
input feature dimension.</dd>
<dt><strong><code>chunk_size</code></strong></dt>
<dd>int, list(int)
Number of frames for each chunk
This variable can take 2 forms:
int:
Used for inference, or single chunk size training
list(int) : Used only for variable chunk size training
Some examples for the 2 cases:
chunk_size = 12
chunk_size = [6, 8, 12, 24]</dd>
<dt><strong><code>left_chunk</code></strong></dt>
<dd>int, list(int)
Number of chunks used for masking in streaming mode.
This variable can take 2 forms:
int:
Used for inference, or single chunk size training
list(int) : Used only for variable chunk size training. When
chunk_size is a list, left_chunk must be a list with same length.
Some examples for the 2 cases:
left_chunk = 6
left_chunk = [12, 9, 6, 3]</dd>
<dt><strong><code>left_chunk</code></strong></dt>
<dd>int
number of chunks used for masking in streaming mode.</dd>
<dt><strong><code>num_lang</code></strong></dt>
<dd>int
This parameter is used to store the number of languages in the
lang_dict, only used for multiseed/multilingual models.
default None.</dd>
<dt><strong><code>attention_dim</code></strong></dt>
<dd>int, optional
attention dimension. default 256.</dd>
<dt><strong><code>attention_heads</code></strong></dt>
<dd>int, optional
the number of heads. default 4</dd>
<dt>linear_units:</dt>
<dt>the number of units of position-wise feed forward.</dt>
<dt>default 2048</dt>
<dt>num_block:</dt>
<dt>number of Transformer layer. default 6</dt>
<dt><strong><code>dropout_rate</code></strong></dt>
<dd>float, optional
dropout rate. default 0.1</dd>
<dt><strong><code>input_layer</code></strong></dt>
<dd>str, optional
input layer type before Conformer,
one of ["linear", "conv2d", "custom", "vgg2l", "embed"],
default "conv2d"</dd>
<dt><strong><code>causal</code></strong></dt>
<dd>bool, optional
if set to True, convolution have no access
to future frames. default False.</dd>
<dt><strong><code>batch_norm</code></strong></dt>
<dd>bool, optional
if set to True, apply batchnorm before activation
in ConvModule layer of the conformer.
default False</dd>
<dt><strong><code>cnn_out</code></strong></dt>
<dd>int, optional
the number of CNN channels before Conformer.
default -1.</dd>
<dt><strong><code>cnn_layer_norm</code></strong></dt>
<dd>bool, optional
layer norm between Conformer and the first CNN.
default False.</dd>
<dt><strong><code>ext_pw_out_channel</code></strong></dt>
<dd>int, optional
the number of channel for CNN
before depthwise_seperable_CNN.
If 0 then use linear. default 0.</dd>
<dt><strong><code>ext_pw_kernel_size</code></strong></dt>
<dd>int, optional
kernel size of N before depthwise_seperable_CNN.
only work for ext_pw_out_channel &gt; 0.
default 1</dd>
<dt><strong><code>depthwise_seperable_out_channel</code></strong></dt>
<dd>int, optional
the number of channel for
depthwise_seperable_CNN.
default 256.</dd>
<dt><strong><code>depthwise_multiplier</code></strong></dt>
<dd>int, optional
the number of multiplier for
depthwise_seperable_CNN.
default 1.</dd>
<dt><strong><code>chunk_se</code></strong></dt>
<dd>int, optional
0 for offline SE.
1 for streaming SE, where mean is computed
by accumulated history until current chunk_se.
2 for streaming SE, where mean is computed
by only the current chunk.
default 0.</dd>
<dt><strong><code>kernel_size</code></strong></dt>
<dd>int, optional
the number of kernels for depthwise_seperable_CNN.
default 3.</dd>
<dt><strong><code>activation</code></strong></dt>
<dd>str, optional
FeedForward block activation.
one of ["relu", "swish", "sigmoid"]
default "relu".</dd>
<dt><strong><code>conv_activation</code></strong></dt>
<dd>str, optional
activation function used in ConvModule part
of the conformer, default "relu".</dd>
<dt><strong><code>conv_glu_type</code></strong></dt>
<dd>str, optional
activation used use glu in depthwise_seperable_CNN,
default "sigmoid"</dd>
<dt><strong><code>bias_in_glu</code></strong></dt>
<dd>bool, optional
if set to True, use additive bias in the weight module
before GLU. default True</dd>
<dt><strong><code>linear_glu_in_convm</code></strong></dt>
<dd>bool, optional
if set to True, use GLULinear module,
otherwise, used GLUPointWiseConv module.
default to False.</dd>
<dt><strong><code>attention_glu_type</code></strong></dt>
<dd>str
only work for glu_in_attention !=0
default "swish".</dd>
<dt><strong><code>export</code></strong></dt>
<dd>bool, optional
if set to True, it remove the padding from convolutional layers
and allow the onnx conversion for inference.
default False.</dd>
<dt><strong><code>activation_checkpointing</code></strong></dt>
<dd>str, optional
a dictionarry of {"module","interval","offload"}, where
"module": str
accept ["transformer", "attention"] to select
which module should do activation checkpointing.
"interval": int, default 1,
interval of applying activation checkpointing,
interval = 1 means that we apply checkpointing
on every layer (if activation), otherwise,
we apply it every x interval.
"offload": bool, default False,
if set to True, we offload activation to cpu and
reload it during backward, otherwise,
we recalculate activation in backward.
default "".</dd>
<dt><strong><code>extra_layer_output_idx</code></strong></dt>
<dd>int
the layer index to be exposed.</dd>
<dt><strong><code>relative_attention_bias_args</code></strong></dt>
<dd>dict, optional
use more efficient scalar bias-based relative multihead attention
(Q*K^T + B) implemented in cmb.basics.embedding.
[T5/ALiBi]RelativeAttentionLogitBias
usage: relative_attention_bias_args={"type": t5/alibi}
additional method-specific arguments can be provided (see
transformer_base.py)</dd>
<dt><strong><code>time_reduction</code></strong></dt>
<dd>int optional
time reduction factor
default 4</dd>
<dt><strong><code>use_pt_scaled_dot_product_attention</code></strong></dt>
<dd>whether to use pytorch scaled
dot product attention in training.
Default: False</dd>
<dt><strong><code>nemo_conv_settings</code></strong></dt>
<dd>dict, optional
A dictionary of settings for NeMo Subsampling.
default: None
usage: nemo_conv_settings=
{
"subsampling":
dw_striding/striding/dw_striding_conv1d/striding_conv1d,
"conv_channels": int,
"subsampling_conv_chunking_factor": int,
"is_causal": True/False
}</dd>
<dt><strong><code>conv2d_extra_padding</code></strong></dt>
<dd>str, optional
Add extra padding in conv2d subsampling layers. Choices are
(feat, feat_time, none, True)
Default: none</dd>
<dt><strong><code>replication_pad_for_subsample_embedding</code></strong></dt>
<dd>For batched-streaming
decoding, use "replication" padding for the cache at start of
utterance.
Default: False</dd>
<dt><strong><code>attention_group_size</code></strong></dt>
<dd>int, optional
the number of groups to use for attention, default 1
(Multi-Head Attention),
1 = typical Multi-Head Attention,
1 &lt; attention_group_size &lt; attention_heads = Grouped-Query
Attention
attention_group_size = attention_heads = Multi-Query Attention</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.models.phi4mm_audio.TransformerEncoderBase" href="#sglang.srt.models.phi4mm_audio.TransformerEncoderBase">TransformerEncoderBase</a></li>
<li>abc.ABC</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_audio.ConformerEncoder.extra_multi_layer_output_idxs"><code class="name">var <span class="ident">extra_multi_layer_output_idxs</span> :Â list[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_audio.ConformerEncoder.calculate_hs_mask"><code class="name flex">
<span>def <span class="ident">calculate_hs_mask</span></span>(<span>self, xs_pad, device, mask)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_hs_mask(self, xs_pad, device, mask):
    max_audio_length = xs_pad.shape[1]
    batch_size = xs_pad.shape[0]
    enc_streaming_mask = self._streaming_mask(
        max_audio_length, batch_size, self.chunk_size, self.left_chunk
    )
    enc_streaming_mask = enc_streaming_mask.to(device)
    if mask is None:
        return enc_streaming_mask

    feature_lens = mask.sum(1)
    padding_length = feature_lens
    pad_mask = torch.arange(0, max_audio_length, device=device).expand(
        padding_length.size(0), -1
    ) &lt; padding_length.unsqueeze(1)
    pad_mask = pad_mask.unsqueeze(1)
    pad_mask = pad_mask &amp; enc_streaming_mask
    return pad_mask</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.models.phi4mm_audio.ConformerEncoder.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, xs_pad, masks) â>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.jit.ignore
def forward(self, xs_pad, masks):
    &#34;&#34;&#34;Conformer Forward function

    Args:
        xs_pad: torch.Tensor
            input tensor
        masks: torch.Tensor
            post-embedding input lengths
    &#34;&#34;&#34;
    xs_pad = self.encoder_embedding(xs_pad)
    input_tensor, pos_k, pos_v, hs_mask, masks = self.forward_embeddings(
        xs_pad, masks
    )

    unfolded = False
    ori_bz, seq_len, D = input_tensor.shape
    max_seq_len = 500  # maximum position for absolute positional encoding
    if seq_len &gt; max_seq_len:
        # audio sequence is longer than max_seq_len, unfold it into chunks
        # of max_seq_len
        unfolded = True
        # the unfold op will drop residual frames, pad it to the multiple
        # of max_seq_len
        if seq_len % max_seq_len &gt; 0:
            chunk_pad_size = max_seq_len - (seq_len % max_seq_len)
        else:
            chunk_pad_size = 0
        if chunk_pad_size &gt; 0:
            input_tensor_pad = F.pad(
                input_tensor, (0, 0, 0, chunk_pad_size), &#34;constant&#34;, 0
            )
            input_tensor = input_tensor_pad.to(input_tensor.device)
        input_tensor = unfold_tensor(input_tensor, max_seq_len)
        if masks is not None:
            # revise hs_mask here because the previous calculated hs_mask
            # did not consider extra pad
            subsampled_pad_mask = masks.squeeze(
                1
            )  # [bz, subsampled_unmask_seq_len]
            extra_padded_subsamlped_pad_mask = F.pad(
                subsampled_pad_mask, (0, chunk_pad_size), &#34;constant&#34;, False
            )  # extra padding to the pad mask
            extra_padded_subsamlped_pad_mask = (
                extra_padded_subsamlped_pad_mask.unsqueeze(-1).float()
            )
            masks_unfold = unfold_tensor(
                extra_padded_subsamlped_pad_mask, max_seq_len
            )  # unfold the pad mask like we did to the input tensor
            masks_unfold = masks_unfold.squeeze(
                -1
            ).bool()  # unfold op does not support bool tensor
        else:
            masks_unfold = None
        hs_mask = self.calculate_hs_mask(
            input_tensor, input_tensor.device, masks_unfold
        )  # calculate hs_mask based on the unfolded pad mask

    # layer_emb = None

    relative_attention_bias = self.init_relative_attention_bias(input_tensor)

    _simplified_path = (
        self.extra_layer_output_idx == -1 and relative_attention_bias is None
    )

    if _simplified_path:
        input_tensor, *_ = self.encoders(input_tensor, pos_k, pos_v, hs_mask)
    else:
        for i, layer in enumerate(self.encoders):
            input_tensor, _, _, _ = layer(
                input_tensor,
                pos_k,
                pos_v,
                hs_mask,
                relative_attention_bias=relative_attention_bias,
            )

            # if i == self.extra_layer_output_idx:
            #     layer_emb = input_tensor

    if unfolded:
        embed_dim = input_tensor.shape[-1]
        input_tensor = input_tensor.reshape(ori_bz, -1, embed_dim)
        # if we ever padded before unfolding, we need to remove the padding
        if chunk_pad_size &gt; 0:
            input_tensor = input_tensor[:, :-chunk_pad_size, :]

    return input_tensor, masks  # , layer_emb</code></pre>
</details>
<div class="desc"><p>Conformer Forward function</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>xs_pad</code></strong></dt>
<dd>torch.Tensor
input tensor</dd>
<dt><strong><code>masks</code></strong></dt>
<dd>torch.Tensor
post-embedding input lengths</dd>
</dl></div>
</dd>
<dt id="sglang.srt.models.phi4mm_audio.ConformerEncoder.init_relative_attention_bias"><code class="name flex">
<span>def <span class="ident">init_relative_attention_bias</span></span>(<span>self, input_tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_relative_attention_bias(self, input_tensor):
    if self.relative_attention_bias_layer:
        return self.relative_attention_bias_layer(input_tensor)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.models.phi4mm_audio.TransformerEncoderBase" href="#sglang.srt.models.phi4mm_audio.TransformerEncoderBase">TransformerEncoderBase</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.models.phi4mm_audio.TransformerEncoderBase.compute_lens_change" href="#sglang.srt.models.phi4mm_audio.TransformerEncoderBase.compute_lens_change">compute_lens_change</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_audio.TransformerEncoderBase.forward_embeddings" href="#sglang.srt.models.phi4mm_audio.TransformerEncoderBase.forward_embeddings">forward_embeddings</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_audio.TransformerEncoderBase.get_offset" href="#sglang.srt.models.phi4mm_audio.TransformerEncoderBase.get_offset">get_offset</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.models.phi4mm_audio.ConformerEncoderLayer"><code class="flex name class">
<span>class <span class="ident">ConformerEncoderLayer</span></span>
<span>(</span><span>d_model=512,<br>ext_pw_out_channel=0,<br>depthwise_seperable_out_channel=256,<br>depthwise_multiplier=1,<br>n_head=4,<br>d_ffn=2048,<br>ext_pw_kernel_size=1,<br>kernel_size=3,<br>dropout_rate=0.1,<br>causal=False,<br>batch_norm=False,<br>activation='relu',<br>chunk_se=0,<br>chunk_size=18,<br>conv_activation='relu',<br>conv_glu_type='sigmoid',<br>bias_in_glu=True,<br>linear_glu_in_convm=False,<br>attention_inner_dim=-1,<br>attention_glu_type='swish',<br>activation_checkpointing='',<br>export=False,<br>use_pt_scaled_dot_product_attention=False,<br>attn_group_sizes:Â intÂ =Â 1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConformerEncoderLayer(nn.Module):
    &#34;&#34;&#34;ConformerEncoder Layer module.
    for more details see conformer paper:
        https://arxiv.org/abs/2005.08100
    This module implement the Conformer block layer.

    Args:
        d_model: int
            attention dim.
        ext_pw_out_channel: int
            if &gt; 0, ext_pw_out_channel is a dim channel size
             for the last pointwise conv after swish activation.
        depthwise_seperable_out_channel: int
            if set different to 0, the number of
             depthwise_seperable_out_channel will be used as a
             channel_out of the second conv1d layer.
             otherwise, it equal to 0, the second conv1d layer is skipped.
        depthwise_multiplier: int
            number of input_dim channels duplication. this value
             will be used to compute the hidden channels of the Conv1D.
        n_head: int
            the number of heads for multihead attention module.
        d_ffn: int
            output size of the feed_forward blocks.
        ext_pw_kernel_size: int
            kernel size of the conv pointwise of the conformer.
        kernel_size: int
            kernel size.
        dropout_rate: float
            dropout rate.
        causal: bool, optional
            if set to True, convolution have no access
             to future frames. default False.
        batch_norm: bool, optional
            if set to True, apply batchnorm before activation
            in ConvModule layer of the conformer.
            default False
        activation: str, optional
            activation function name,
            one of [&#34;relu&#34;, &#34;swish&#34;, &#34;sigmoid&#34;],
            sigmoid activation is only used with &#34;glu_in_fnn=True&#34;,
            default &#34;relu&#34;.
        chunk_se: int, optional
            0 for offline SE.
            1 for streaming SE, where mean is computed
             by accumulated history until current chunk_se.
            2 for streaming SE, where mean is computed
             by only the current chunk.
            default 0.
        chunk_size: int, optional
            chunk_size for cnn. default 18
        conv_activation: str, optional
            activation function used in ConvModule part
            of the conformer, default &#34;relu&#34;.
        conv_glu_type: str, optional
            activation function used for the glu inside
            the ConvModule part of the conformer.
            default: &#34;sigmoid&#34;.
        bias_in_glu: bool, optional
            if set to True, use additive bias in the weight module
             before GLU.
        linear_glu_in_convm: bool, optional
            if set to True, use GLULinear module,
             otherwise, used GLUPointWiseConv module.
              default to False.
        attention_inner_dim: int, optional
            if equal to -1, attention dim for linears k/q/v is
            equal to d_model. otherwise attention_inner_dim is used.
            default -1.
        attention_glu_type: str, optional
            activation function for glu used in the multihead attention,
             default &#34;swish&#34;.
        activation_checkpointing: str, optional
            a dictionarry of {&#34;module&#34;,&#34;interval&#34;,&#34;offload&#34;}, where
                &#34;module&#34;: str
                    accept [&#34;transformer&#34;, &#34;attention&#34;] to select
                    which module should do activation checkpointing.
                &#34;interval&#34;: int, default 1,
                    interval of applying activation checkpointing,
                    interval = 1 means that we apply checkpointing
                    on every layer (if activation), otherwise,
                    we apply it every x interval.
                &#34;offload&#34;: bool, default False,
                    if set to True, we offload activation to cpu and
                    reload it during backward, otherwise,
                    we recalculate activation in backward.
            default &#34;&#34;.
        export: bool, optional
            if set to True, it remove the padding from convolutional layers
             and allow the onnx conversion for inference.
              default False.
        use_pt_scaled_dot_product_attention: bool, optional
            if set to True, use pytorch&#39;s scaled dot product attention
            implementation in training.
        attn_group_sizes: int, optional
            the number of groups to use for attention, default 1
            (Multi-Head Attention),
            1 = typical Multi-Head Attention,
            1 &lt; attn_group_sizes &lt; attention_heads = Grouped-Query Attention
            attn_group_sizes = attention_heads = Multi-Query Attention
    &#34;&#34;&#34;

    def __init__(
        self,
        d_model=512,
        ext_pw_out_channel=0,
        depthwise_seperable_out_channel=256,
        depthwise_multiplier=1,
        n_head=4,
        d_ffn=2048,
        ext_pw_kernel_size=1,
        kernel_size=3,
        dropout_rate=0.1,
        causal=False,
        batch_norm=False,
        activation=&#34;relu&#34;,
        chunk_se=0,
        chunk_size=18,
        conv_activation=&#34;relu&#34;,
        conv_glu_type=&#34;sigmoid&#34;,
        bias_in_glu=True,
        linear_glu_in_convm=False,
        attention_inner_dim=-1,
        attention_glu_type=&#34;swish&#34;,
        activation_checkpointing=&#34;&#34;,
        export=False,
        use_pt_scaled_dot_product_attention=False,
        attn_group_sizes: int = 1,
    ):
        super().__init__()

        self.feed_forward_in = FeedForward(
            d_model=d_model,
            d_inner=d_ffn,
            dropout_rate=dropout_rate,
            activation=activation,
            bias_in_glu=bias_in_glu,
        )

        self.self_attn = MultiHeadedAttention(
            n_head,
            d_model,
            dropout_rate,
            attention_inner_dim,
            attention_glu_type,
            bias_in_glu,
            use_pt_scaled_dot_product_attention=use_pt_scaled_dot_product_attention,
            group_size=attn_group_sizes,
        )
        self.conv = ConvModule(
            d_model,
            ext_pw_out_channel,
            depthwise_seperable_out_channel,
            ext_pw_kernel_size,
            kernel_size,
            depthwise_multiplier,
            dropout_rate,
            causal,
            batch_norm,
            chunk_se,
            chunk_size,
            conv_activation,
            conv_glu_type,
            bias_in_glu,
            linear_glu_in_convm,
            export=export,
        )

        self.feed_forward_out = FeedForward(
            d_model=d_model,
            d_inner=d_ffn,
            dropout_rate=dropout_rate,
            activation=activation,
            bias_in_glu=bias_in_glu,
        )

        self.layer_norm_att = nn.LayerNorm(d_model)
        self.layer_norm = nn.LayerNorm(d_model)

    def forward(
        self,
        x,
        pos_k,
        pos_v,
        mask,
        relative_attention_bias: Optional[Tensor] = None,
    ):
        &#34;&#34;&#34;ConformerEncoder forward.

        Args:
            x: torch.Tensor
                input feature of shape (batch, max_time_in, size)
            pos_k: torch.Tensor
                positional key embedding.
            mask: torch.Tensor
                mask for x (batch, max_time_in)
            relative_attention_bias: Optional[torch.Tensor]
                bias added to attention logits w.r.t. relative positions
                (1, n_head, time1, time2)
        &#34;&#34;&#34;
        x = x + 0.5 * self.feed_forward_in(x)
        norm_x = self.layer_norm_att(x)

        x = x + self.self_attn(
            norm_x,
            norm_x,
            norm_x,
            pos_k,
            pos_v,
            mask,
            relative_attention_bias=relative_attention_bias,
        )
        x = x + self.conv(x)
        x = x + 0.5 * self.feed_forward_out(x)

        out = self.layer_norm(x)

        return out, pos_k, pos_v, mask</code></pre>
</details>
<div class="desc"><p>ConformerEncoder Layer module.
for more details see conformer paper:
<a href="https://arxiv.org/abs/2005.08100">https://arxiv.org/abs/2005.08100</a>
This module implement the Conformer block layer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>d_model</code></strong></dt>
<dd>int
attention dim.</dd>
<dt><strong><code>ext_pw_out_channel</code></strong></dt>
<dd>int
if &gt; 0, ext_pw_out_channel is a dim channel size
for the last pointwise conv after swish activation.</dd>
<dt><strong><code>depthwise_seperable_out_channel</code></strong></dt>
<dd>int
if set different to 0, the number of
depthwise_seperable_out_channel will be used as a
channel_out of the second conv1d layer.
otherwise, it equal to 0, the second conv1d layer is skipped.</dd>
<dt><strong><code>depthwise_multiplier</code></strong></dt>
<dd>int
number of input_dim channels duplication. this value
will be used to compute the hidden channels of the Conv1D.</dd>
<dt><strong><code>n_head</code></strong></dt>
<dd>int
the number of heads for multihead attention module.</dd>
<dt><strong><code>d_ffn</code></strong></dt>
<dd>int
output size of the feed_forward blocks.</dd>
<dt><strong><code>ext_pw_kernel_size</code></strong></dt>
<dd>int
kernel size of the conv pointwise of the conformer.</dd>
<dt><strong><code>kernel_size</code></strong></dt>
<dd>int
kernel size.</dd>
<dt><strong><code>dropout_rate</code></strong></dt>
<dd>float
dropout rate.</dd>
<dt><strong><code>causal</code></strong></dt>
<dd>bool, optional
if set to True, convolution have no access
to future frames. default False.</dd>
<dt><strong><code>batch_norm</code></strong></dt>
<dd>bool, optional
if set to True, apply batchnorm before activation
in ConvModule layer of the conformer.
default False</dd>
<dt><strong><code>activation</code></strong></dt>
<dd>str, optional
activation function name,
one of ["relu", "swish", "sigmoid"],
sigmoid activation is only used with "glu_in_fnn=True",
default "relu".</dd>
<dt><strong><code>chunk_se</code></strong></dt>
<dd>int, optional
0 for offline SE.
1 for streaming SE, where mean is computed
by accumulated history until current chunk_se.
2 for streaming SE, where mean is computed
by only the current chunk.
default 0.</dd>
<dt><strong><code>chunk_size</code></strong></dt>
<dd>int, optional
chunk_size for cnn. default 18</dd>
<dt><strong><code>conv_activation</code></strong></dt>
<dd>str, optional
activation function used in ConvModule part
of the conformer, default "relu".</dd>
<dt><strong><code>conv_glu_type</code></strong></dt>
<dd>str, optional
activation function used for the glu inside
the ConvModule part of the conformer.
default: "sigmoid".</dd>
<dt><strong><code>bias_in_glu</code></strong></dt>
<dd>bool, optional
if set to True, use additive bias in the weight module
before GLU.</dd>
<dt><strong><code>linear_glu_in_convm</code></strong></dt>
<dd>bool, optional
if set to True, use GLULinear module,
otherwise, used GLUPointWiseConv module.
default to False.</dd>
<dt><strong><code>attention_inner_dim</code></strong></dt>
<dd>int, optional
if equal to -1, attention dim for linears k/q/v is
equal to d_model. otherwise attention_inner_dim is used.
default -1.</dd>
<dt><strong><code>attention_glu_type</code></strong></dt>
<dd>str, optional
activation function for glu used in the multihead attention,
default "swish".</dd>
<dt><strong><code>activation_checkpointing</code></strong></dt>
<dd>str, optional
a dictionarry of {"module","interval","offload"}, where
"module": str
accept ["transformer", "attention"] to select
which module should do activation checkpointing.
"interval": int, default 1,
interval of applying activation checkpointing,
interval = 1 means that we apply checkpointing
on every layer (if activation), otherwise,
we apply it every x interval.
"offload": bool, default False,
if set to True, we offload activation to cpu and
reload it during backward, otherwise,
we recalculate activation in backward.
default "".</dd>
<dt><strong><code>export</code></strong></dt>
<dd>bool, optional
if set to True, it remove the padding from convolutional layers
and allow the onnx conversion for inference.
default False.</dd>
<dt><strong><code>use_pt_scaled_dot_product_attention</code></strong></dt>
<dd>bool, optional
if set to True, use pytorch's scaled dot product attention
implementation in training.</dd>
<dt><strong><code>attn_group_sizes</code></strong></dt>
<dd>int, optional
the number of groups to use for attention, default 1
(Multi-Head Attention),
1 = typical Multi-Head Attention,
1 &lt; attn_group_sizes &lt; attention_heads = Grouped-Query Attention
attn_group_sizes = attention_heads = Multi-Query Attention</dd>
</dl>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_audio.ConformerEncoderLayer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x, pos_k, pos_v, mask, relative_attention_bias:Â torch.TensorÂ |Â NoneÂ =Â None) â>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    x,
    pos_k,
    pos_v,
    mask,
    relative_attention_bias: Optional[Tensor] = None,
):
    &#34;&#34;&#34;ConformerEncoder forward.

    Args:
        x: torch.Tensor
            input feature of shape (batch, max_time_in, size)
        pos_k: torch.Tensor
            positional key embedding.
        mask: torch.Tensor
            mask for x (batch, max_time_in)
        relative_attention_bias: Optional[torch.Tensor]
            bias added to attention logits w.r.t. relative positions
            (1, n_head, time1, time2)
    &#34;&#34;&#34;
    x = x + 0.5 * self.feed_forward_in(x)
    norm_x = self.layer_norm_att(x)

    x = x + self.self_attn(
        norm_x,
        norm_x,
        norm_x,
        pos_k,
        pos_v,
        mask,
        relative_attention_bias=relative_attention_bias,
    )
    x = x + self.conv(x)
    x = x + 0.5 * self.feed_forward_out(x)

    out = self.layer_norm(x)

    return out, pos_k, pos_v, mask</code></pre>
</details>
<div class="desc"><p>ConformerEncoder forward.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>torch.Tensor
input feature of shape (batch, max_time_in, size)</dd>
<dt><strong><code>pos_k</code></strong></dt>
<dd>torch.Tensor
positional key embedding.</dd>
<dt><strong><code>mask</code></strong></dt>
<dd>torch.Tensor
mask for x (batch, max_time_in)</dd>
<dt><strong><code>relative_attention_bias</code></strong></dt>
<dd>Optional[torch.Tensor]
bias added to attention logits w.r.t. relative positions
(1, n_head, time1, time2)</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.phi4mm_audio.TransformerEncoderBase"><code class="flex name class">
<span>class <span class="ident">TransformerEncoderBase</span></span>
<span>(</span><span>input_size,<br>chunk_size,<br>left_chunk,<br>attention_dim=256,<br>attention_heads=4,<br>input_layer='nemo_conv',<br>cnn_out=-1,<br>cnn_layer_norm=False,<br>time_reduction=4,<br>dropout_rate=0.0,<br>padding_idx=-1,<br>relative_attention_bias_args=None,<br>positional_dropout_rate=0.0,<br>nemo_conv_settings=None,<br>conv2d_extra_padding:Â Literal['feat',Â 'feat_time',Â 'none',Â True]Â =Â 'none',<br>attention_group_size=1,<br>encoder_embedding_config=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TransformerEncoderBase(abc.ABC, nn.Module):
    &#34;&#34;&#34;The Base class for Transformer based encoders

    Please set causal = True in streaming model
    Args:
        input_size: int
            input feature dimension.
        chunk_size: int, list(int)
            Number of frames for each chunk
            This variable can take 2 forms:
            int:  Used for inference, or single chunk size training
            list(int) : Used only for variable chunk size training
            Some examples for the 2 cases:
            chunk_size = 12
            chunk_size = [6, 8, 12, 24]
        left_chunk: int, list(int)
            Number of chunks used for masking in streaming mode.
            This variable can take 2 forms:
            int:  Used for inference, or single chunk size training
            list(int) : Used only for variable chunk size training. When
            chunk_size is a list, left_chunk must be a list with same length.
            Some examples for the 2 cases:
            left_chunk = 6
            left_chunk = [12, 9, 6, 3]
        attention_dim: int, optional
            attention dimension. default 256.
        attention_heads: int, optional
            the number of heads. default 4
        input_layer: str, optional
            input layer type before Conformer,
            one of [&#34;linear&#34;, &#34;conv2d&#34;, &#34;custom&#34;, &#34;vgg2l&#34;, &#34;embed&#34;],
            default &#34;conv2d&#34;
        cnn_out: int, optional
            the number of CNN channels before Conformer.
            default -1.
        cnn_layer_norm: bool, optional
            layer norm between Conformer and the first CNN.
            default False.
        time_reduction: int, optional
            time reduction factor
            default 4
        dropout_rate: float, optional
            dropout rate. default 0.1
        padding_idx: int, optional
            padding index for input_layer=embed
            default -1
        relative_attention_bias_args: dict, optional
            use more efficient scalar bias-based relative multihead attention
            (Q*K^T + B) implemented in cmb.basics.embedding.
            [T5/ALiBi]RelativeAttentionLogitBias
            usage: relative_attention_bias_args={&#34;type&#34;: t5/alibi}
            additional method-specific arguments can be provided (see
            transformer_base.py)
        positional_dropout_rate: float, optional
            dropout rate after positional encoding. default 0.0
        nemo_conv_settings: dict, optional
            A dictionary of settings for NeMo Subsampling.
            default None
        conv2d_extra_padding: str, optional
            Add extra padding in conv2d subsampling layers. Choices are
            (feat, feat_time, none, True).
            if True or feat_time, the extra padding is added into non full
            supraframe utts in batch.
            Default: none
        attention_group_size: int, optional
            the number of groups to use for attention, default 1
            (Multi-Head Attention),
            1 = typical Multi-Head Attention,
            1 &lt; attention_group_size &lt; attention_heads = Grouped-Query
            Attention
            attention_group_size = attention_heads = Multi-Query Attention
    &#34;&#34;&#34;

    def __init__(
        self,
        input_size,
        chunk_size,
        left_chunk,
        attention_dim=256,
        attention_heads=4,
        input_layer=&#34;nemo_conv&#34;,
        cnn_out=-1,
        cnn_layer_norm=False,
        time_reduction=4,
        dropout_rate=0.0,
        padding_idx=-1,
        relative_attention_bias_args=None,
        positional_dropout_rate=0.0,
        nemo_conv_settings=None,
        conv2d_extra_padding: Literal[&#34;feat&#34;, &#34;feat_time&#34;, &#34;none&#34;, True] = &#34;none&#34;,
        attention_group_size=1,
        encoder_embedding_config=None,
    ):
        super().__init__()
        self.input_size = input_size
        self.input_layer = input_layer
        self.chunk_size = chunk_size
        self.left_chunk = left_chunk
        self.attention_dim = attention_dim
        self.num_heads = attention_heads
        self.attention_group_size = attention_group_size
        self.time_reduction = time_reduction
        self.nemo_conv_settings = nemo_conv_settings
        self.encoder_embedding_config = encoder_embedding_config

        if self.input_layer == &#34;nemo_conv&#34;:
            default_nemo_conv_settings = {
                &#34;subsampling&#34;: &#34;dw_striding&#34;,
                &#34;subsampling_factor&#34;: self.time_reduction,
                &#34;feat_in&#34;: input_size,
                &#34;feat_out&#34;: attention_dim,
                &#34;conv_channels&#34;: 256,
                &#34;subsampling_conv_chunking_factor&#34;: 1,
                &#34;activation&#34;: nn.ReLU(),
                &#34;is_causal&#34;: False,
            }
            # Override any of the defaults with the incoming, user settings
            if nemo_conv_settings:
                default_nemo_conv_settings.update(nemo_conv_settings)
                for i in [&#34;subsampling_factor&#34;, &#34;feat_in&#34;, &#34;feat_out&#34;]:
                    assert (
                        i not in nemo_conv_settings
                    ), &#34;{i} should be specified outside of the NeMo dictionary&#34;

            self.embed = NemoConvSubsampling(
                **default_nemo_conv_settings,
            )
        else:
            raise ValueError(&#34;unknown input_layer: &#34; + input_layer)

        self.pos_emb = AbsolutePositionalEncoding(
            attention_dim, positional_dropout_rate
        )

        self.relative_attention_bias_type = (
            relative_attention_bias_args.get(&#34;type&#34;)
            if relative_attention_bias_args
            else None
        )
        if self.relative_attention_bias_type == &#34;t5&#34;:
            assert (
                self.num_heads % self.attention_group_size == 0
            ), &#34;attention_group_size must divide n_head&#34;
            self.relative_attention_bias_layer = T5RelativeAttentionLogitBias(
                self.num_heads // self.attention_group_size,
                max_distance=relative_attention_bias_args.get(
                    &#34;t5_bias_max_distance&#34;, 1000
                ),
                symmetric=relative_attention_bias_args.get(&#34;t5_bias_symmetric&#34;, False),
            )
        else:
            raise NotImplementedError

        self.encoder_embedding = MeanVarianceNormLayer(
            self.encoder_embedding_config[&#34;input_size&#34;]
        )

    def compute_lens_change(self, feature_lens):
        &#34;&#34;&#34;feature_lens: int
        return updated feature lens.

        This used to return a different lambda function for each case that
        computed the right thing.  That does not work within Torchscript.
        If you really need this to be faster, create nn.Module()-s for all
        the cases and return one of them.  Torchscript does support that.
        &#34;&#34;&#34;
        if self.input_layer == &#34;nemo_conv&#34;:
            # Handle the special causal case
            subsampling_causal_cond = self.nemo_conv_settings.get(
                &#34;subsampling&#34;, &#34;dw_striding&#34;
            ) in [
                &#34;dw_striding&#34;,
                &#34;striding&#34;,
                &#34;striding_conv1d&#34;,
            ]
            is_causal = self.nemo_conv_settings.get(&#34;is_causal&#34;, False)
            if is_causal and subsampling_causal_cond:
                lens_change = (
                    torch.ceil(feature_lens / self.time_reduction).long()
                    if isinstance(feature_lens, Tensor)
                    else math.ceil(feature_lens / self.time_reduction)
                )
                feature_lens_remainder = feature_lens % self.time_reduction
                if isinstance(feature_lens, Tensor):
                    lens_change[feature_lens_remainder != 1] += 1
                elif feature_lens_remainder != 1:
                    lens_change += 1
                return lens_change
            ceil_func = math.ceil if isinstance(feature_lens, int) else torch.ceil
            return ceil_func(feature_lens / self.time_reduction)

    @abc.abstractmethod
    def forward(self):
        &#34;&#34;&#34;Abstract forward method implementation.&#34;&#34;&#34;

    def _chunk_size_selection(self, chunk_size=None, left_chunk=None):
        &#34;&#34;&#34;If chunk size is a list, we will randomly select a chunk size.&#34;&#34;&#34;

        if chunk_size is None:
            chunk_size = self.chunk_size
        if left_chunk is None:
            left_chunk = self.left_chunk
        if isinstance(chunk_size, list):
            # Variable chunk size during training
            chunk_size_index = int(
                torch.randint(low=0, high=len(chunk_size), size=(1,))
            )
            chunk_size_train_eff = chunk_size[chunk_size_index]
            if not isinstance(left_chunk, list):
                raise ValueError(
                    &#34;Since chunk_size is a list, left_chunk must be a list&#34;
                )
            if len(left_chunk) != len(chunk_size):
                raise ValueError(
                    &#34;The length of left_chunk must be the same as length of &#34;
                    &#34;chunk_size.&#34;
                )
            left_chunk_train_eff = left_chunk[chunk_size_index]
        else:
            chunk_size_train_eff = chunk_size
            left_chunk_train_eff = left_chunk

        return chunk_size_train_eff, left_chunk_train_eff

    def _get_embed_class(self, embed):
        # pylint: disable=protected-access
        is_embed_using_act_chkpt = isinstance(embed, CheckpointWrapper)
        is_embed_fsdp_wrapped = isinstance(embed, FullyShardedDataParallel)
        embed_class = embed
        if is_embed_using_act_chkpt:
            embed_class = embed._checkpoint_wrapped_module
        if is_embed_fsdp_wrapped:
            embed_class = embed.module
        return embed_class

    def _forward_embeddings_core(self, input_tensor, masks):
        embed_class = self._get_embed_class(self.embed)
        assert isinstance(embed_class, NemoConvSubsampling)
        input_tensor, masks = self.embed(input_tensor, masks)
        return input_tensor, masks

    def _position_embedding(self, input_tensor):
        pos_k = None
        pos_v = None
        if self.relative_attention_bias_layer is None:
            input_tensor = self.pos_emb(
                input_tensor
            )  # default to add abs sinusoid embedding
        return pos_k, pos_v

    def _streaming_mask(self, seq_len, batch_size, chunk_size, left_chunk):
        chunk_size_train_eff, left_chunk_train_eff = self._chunk_size_selection(
            chunk_size, left_chunk
        )

        # Create mask matrix for streaming
        # S stores start index. if chunksize is 18, s is [0,18,36,....]
        chunk_start_idx = np.arange(0, seq_len, chunk_size_train_eff)

        enc_streaming_mask = (
            adaptive_enc_mask(
                seq_len, chunk_start_idx, left_window=left_chunk_train_eff
            )
            .unsqueeze(0)
            .expand([batch_size, -1, -1])
        )
        return enc_streaming_mask

    def forward_embeddings(self, xs_pad, masks, chunk_size_nc=None, left_chunk_nc=None):
        &#34;&#34;&#34;Forwarding the inputs through the top embedding layers

        Args:
            xs_pad: torch.Tensor
                input tensor
            masks: torch.Tensor
                input mask
            chunk_size_nc: (optional, default is None) chunk size for
                            non-causal layers
            left_chunk_nc: (optional, default is None) # of left chunks for
                            non-causal layers
        &#34;&#34;&#34;
        # pylint: disable=R0915
        # get new lens.
        seq_len = int(self.compute_lens_change(xs_pad.shape[1]))
        if seq_len &lt;= 0:
            raise ValueError(
                f&#34;&#34;&#34;The sequence length after time reduction is invalid:
                {seq_len}. Your input feature is too short. Consider
                filtering out the very short sentence from data
                loader&#34;&#34;&#34;,
            )

        batch_size = xs_pad.shape[0]

        enc_streaming_mask = self._streaming_mask(
            seq_len, batch_size, self.chunk_size, self.left_chunk
        )

        if xs_pad.is_cuda:
            enc_streaming_mask = enc_streaming_mask.cuda()
            xs_pad = xs_pad.cuda()

        input_tensor = xs_pad
        input_tensor, masks = self._forward_embeddings_core(input_tensor, masks)

        streaming_mask = enc_streaming_mask
        if streaming_mask is not None and masks is not None:
            hs_mask = masks &amp; streaming_mask
        elif masks is not None:
            hs_mask = masks
        else:
            hs_mask = streaming_mask

        if chunk_size_nc is not None:
            enc_streaming_mask_nc = self._streaming_mask(
                seq_len, batch_size, chunk_size_nc, left_chunk_nc
            )
            if xs_pad.is_cuda:
                enc_streaming_mask_nc = enc_streaming_mask_nc.cuda()
            if masks is not None:
                hs_mask_nc = masks &amp; enc_streaming_mask_nc
            else:
                hs_mask_nc = enc_streaming_mask_nc
        else:
            hs_mask_nc = None

        pos_k, pos_v = self._position_embedding(input_tensor)

        if chunk_size_nc is None:
            return input_tensor, pos_k, pos_v, hs_mask, masks
        return input_tensor, pos_k, pos_v, hs_mask, masks, hs_mask_nc

    def get_offset(self):
        &#34;&#34;&#34;Returns offset used when retaining inputs for decoding.

        This is essentially, how many additional frames have to be added to
        the front-end CNN input to ensure it can produce a single output.
        So if the &#34;padding&#34; parameter is 0, typically offset will be &gt; 0.
        &#34;&#34;&#34;
        return get_offset(self.input_layer, self.time_reduction)</code></pre>
</details>
<div class="desc"><p>The Base class for Transformer based encoders</p>
<p>Please set causal = True in streaming model</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_size</code></strong></dt>
<dd>int
input feature dimension.</dd>
<dt><strong><code>chunk_size</code></strong></dt>
<dd>int, list(int)
Number of frames for each chunk
This variable can take 2 forms:
int:
Used for inference, or single chunk size training
list(int) : Used only for variable chunk size training
Some examples for the 2 cases:
chunk_size = 12
chunk_size = [6, 8, 12, 24]</dd>
<dt><strong><code>left_chunk</code></strong></dt>
<dd>int, list(int)
Number of chunks used for masking in streaming mode.
This variable can take 2 forms:
int:
Used for inference, or single chunk size training
list(int) : Used only for variable chunk size training. When
chunk_size is a list, left_chunk must be a list with same length.
Some examples for the 2 cases:
left_chunk = 6
left_chunk = [12, 9, 6, 3]</dd>
<dt><strong><code>attention_dim</code></strong></dt>
<dd>int, optional
attention dimension. default 256.</dd>
<dt><strong><code>attention_heads</code></strong></dt>
<dd>int, optional
the number of heads. default 4</dd>
<dt><strong><code>input_layer</code></strong></dt>
<dd>str, optional
input layer type before Conformer,
one of ["linear", "conv2d", "custom", "vgg2l", "embed"],
default "conv2d"</dd>
<dt><strong><code>cnn_out</code></strong></dt>
<dd>int, optional
the number of CNN channels before Conformer.
default -1.</dd>
<dt><strong><code>cnn_layer_norm</code></strong></dt>
<dd>bool, optional
layer norm between Conformer and the first CNN.
default False.</dd>
<dt><strong><code>time_reduction</code></strong></dt>
<dd>int, optional
time reduction factor
default 4</dd>
<dt><strong><code>dropout_rate</code></strong></dt>
<dd>float, optional
dropout rate. default 0.1</dd>
<dt><strong><code>padding_idx</code></strong></dt>
<dd>int, optional
padding index for input_layer=embed
default -1</dd>
<dt><strong><code>relative_attention_bias_args</code></strong></dt>
<dd>dict, optional
use more efficient scalar bias-based relative multihead attention
(Q*K^T + B) implemented in cmb.basics.embedding.
[T5/ALiBi]RelativeAttentionLogitBias
usage: relative_attention_bias_args={"type": t5/alibi}
additional method-specific arguments can be provided (see
transformer_base.py)</dd>
<dt><strong><code>positional_dropout_rate</code></strong></dt>
<dd>float, optional
dropout rate after positional encoding. default 0.0</dd>
<dt><strong><code>nemo_conv_settings</code></strong></dt>
<dd>dict, optional
A dictionary of settings for NeMo Subsampling.
default None</dd>
<dt><strong><code>conv2d_extra_padding</code></strong></dt>
<dd>str, optional
Add extra padding in conv2d subsampling layers. Choices are
(feat, feat_time, none, True).
if True or feat_time, the extra padding is added into non full
supraframe utts in batch.
Default: none</dd>
<dt><strong><code>attention_group_size</code></strong></dt>
<dd>int, optional
the number of groups to use for attention, default 1
(Multi-Head Attention),
1 = typical Multi-Head Attention,
1 &lt; attention_group_size &lt; attention_heads = Grouped-Query
Attention
attention_group_size = attention_heads = Multi-Query Attention</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.models.phi4mm_audio.ConformerEncoder" href="#sglang.srt.models.phi4mm_audio.ConformerEncoder">ConformerEncoder</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_audio.TransformerEncoderBase.compute_lens_change"><code class="name flex">
<span>def <span class="ident">compute_lens_change</span></span>(<span>self, feature_lens)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_lens_change(self, feature_lens):
    &#34;&#34;&#34;feature_lens: int
    return updated feature lens.

    This used to return a different lambda function for each case that
    computed the right thing.  That does not work within Torchscript.
    If you really need this to be faster, create nn.Module()-s for all
    the cases and return one of them.  Torchscript does support that.
    &#34;&#34;&#34;
    if self.input_layer == &#34;nemo_conv&#34;:
        # Handle the special causal case
        subsampling_causal_cond = self.nemo_conv_settings.get(
            &#34;subsampling&#34;, &#34;dw_striding&#34;
        ) in [
            &#34;dw_striding&#34;,
            &#34;striding&#34;,
            &#34;striding_conv1d&#34;,
        ]
        is_causal = self.nemo_conv_settings.get(&#34;is_causal&#34;, False)
        if is_causal and subsampling_causal_cond:
            lens_change = (
                torch.ceil(feature_lens / self.time_reduction).long()
                if isinstance(feature_lens, Tensor)
                else math.ceil(feature_lens / self.time_reduction)
            )
            feature_lens_remainder = feature_lens % self.time_reduction
            if isinstance(feature_lens, Tensor):
                lens_change[feature_lens_remainder != 1] += 1
            elif feature_lens_remainder != 1:
                lens_change += 1
            return lens_change
        ceil_func = math.ceil if isinstance(feature_lens, int) else torch.ceil
        return ceil_func(feature_lens / self.time_reduction)</code></pre>
</details>
<div class="desc"><p>feature_lens: int
return updated feature lens.</p>
<p>This used to return a different lambda function for each case that
computed the right thing.
That does not work within Torchscript.
If you really need this to be faster, create nn.Module()-s for all
the cases and return one of them.
Torchscript does support that.</p></div>
</dd>
<dt id="sglang.srt.models.phi4mm_audio.TransformerEncoderBase.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self) â>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abc.abstractmethod
def forward(self):
    &#34;&#34;&#34;Abstract forward method implementation.&#34;&#34;&#34;</code></pre>
</details>
<div class="desc"><p>Abstract forward method implementation.</p></div>
</dd>
<dt id="sglang.srt.models.phi4mm_audio.TransformerEncoderBase.forward_embeddings"><code class="name flex">
<span>def <span class="ident">forward_embeddings</span></span>(<span>self, xs_pad, masks, chunk_size_nc=None, left_chunk_nc=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_embeddings(self, xs_pad, masks, chunk_size_nc=None, left_chunk_nc=None):
    &#34;&#34;&#34;Forwarding the inputs through the top embedding layers

    Args:
        xs_pad: torch.Tensor
            input tensor
        masks: torch.Tensor
            input mask
        chunk_size_nc: (optional, default is None) chunk size for
                        non-causal layers
        left_chunk_nc: (optional, default is None) # of left chunks for
                        non-causal layers
    &#34;&#34;&#34;
    # pylint: disable=R0915
    # get new lens.
    seq_len = int(self.compute_lens_change(xs_pad.shape[1]))
    if seq_len &lt;= 0:
        raise ValueError(
            f&#34;&#34;&#34;The sequence length after time reduction is invalid:
            {seq_len}. Your input feature is too short. Consider
            filtering out the very short sentence from data
            loader&#34;&#34;&#34;,
        )

    batch_size = xs_pad.shape[0]

    enc_streaming_mask = self._streaming_mask(
        seq_len, batch_size, self.chunk_size, self.left_chunk
    )

    if xs_pad.is_cuda:
        enc_streaming_mask = enc_streaming_mask.cuda()
        xs_pad = xs_pad.cuda()

    input_tensor = xs_pad
    input_tensor, masks = self._forward_embeddings_core(input_tensor, masks)

    streaming_mask = enc_streaming_mask
    if streaming_mask is not None and masks is not None:
        hs_mask = masks &amp; streaming_mask
    elif masks is not None:
        hs_mask = masks
    else:
        hs_mask = streaming_mask

    if chunk_size_nc is not None:
        enc_streaming_mask_nc = self._streaming_mask(
            seq_len, batch_size, chunk_size_nc, left_chunk_nc
        )
        if xs_pad.is_cuda:
            enc_streaming_mask_nc = enc_streaming_mask_nc.cuda()
        if masks is not None:
            hs_mask_nc = masks &amp; enc_streaming_mask_nc
        else:
            hs_mask_nc = enc_streaming_mask_nc
    else:
        hs_mask_nc = None

    pos_k, pos_v = self._position_embedding(input_tensor)

    if chunk_size_nc is None:
        return input_tensor, pos_k, pos_v, hs_mask, masks
    return input_tensor, pos_k, pos_v, hs_mask, masks, hs_mask_nc</code></pre>
</details>
<div class="desc"><p>Forwarding the inputs through the top embedding layers</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>xs_pad</code></strong></dt>
<dd>torch.Tensor
input tensor</dd>
<dt><strong><code>masks</code></strong></dt>
<dd>torch.Tensor
input mask</dd>
<dt><strong><code>chunk_size_nc</code></strong></dt>
<dd>(optional, default is None) chunk size for
non-causal layers</dd>
<dt><strong><code>left_chunk_nc</code></strong></dt>
<dd>(optional, default is None) # of left chunks for
non-causal layers</dd>
</dl></div>
</dd>
<dt id="sglang.srt.models.phi4mm_audio.TransformerEncoderBase.get_offset"><code class="name flex">
<span>def <span class="ident">get_offset</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_offset(self):
    &#34;&#34;&#34;Returns offset used when retaining inputs for decoding.

    This is essentially, how many additional frames have to be added to
    the front-end CNN input to ensure it can produce a single output.
    So if the &#34;padding&#34; parameter is 0, typically offset will be &gt; 0.
    &#34;&#34;&#34;
    return get_offset(self.input_layer, self.time_reduction)</code></pre>
</details>
<div class="desc"><p>Returns offset used when retaining inputs for decoding.</p>
<p>This is essentially, how many additional frames have to be added to
the front-end CNN input to ensure it can produce a single output.
So if the "padding" parameter is 0, typically offset will be &gt; 0.</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.models.phi4mm_audio.WindowQformer"><code class="flex name class">
<span>class <span class="ident">WindowQformer</span></span>
<span>(</span><span>window_size:Â intÂ =Â 8,<br>num_queries:Â intÂ =Â 1,<br>num_blocks:Â intÂ =Â 2,<br>attention_dim:Â intÂ =Â 512,<br>attention_heads:Â intÂ =Â 8,<br>linear_units:Â intÂ =Â 2048,<br>dropout_rate:Â floatÂ =Â 0.0,<br>normalize_before:Â boolÂ =Â True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class WindowQformer(nn.Module):
    &#34;&#34;&#34;Window-level Qformer&#34;&#34;&#34;

    def __init__(
        self,
        window_size: int = 8,
        num_queries: int = 1,
        num_blocks: int = 2,
        attention_dim: int = 512,
        attention_heads: int = 8,
        linear_units: int = 2048,
        dropout_rate: float = 0.0,
        normalize_before: bool = True,
    ):
        super().__init__()

        self.decoders = nn.ModuleList(
            [
                nn.TransformerDecoderLayer(
                    d_model=attention_dim,
                    nhead=attention_heads,
                    dim_feedforward=linear_units,
                    dropout=dropout_rate,
                    activation=&#34;relu&#34;,
                    batch_first=True,
                    norm_first=normalize_before,  # TODO need to verify
                )
                for _ in range(num_blocks)
            ]
        )

        self.queries = nn.Parameter(torch.zeros(1, num_queries, attention_dim))
        self.after_norm = (
            nn.LayerNorm(attention_dim, eps=1e-12) if normalize_before else None
        )
        self.window_size = window_size

    def forward(self, audio_embed, mask, embed_len=None):
        &#34;&#34;&#34;forward decoder&#34;&#34;&#34;
        # audio_embed: N x T x D =&gt; N x D x T

        audio_embed = audio_embed.transpose(1, 2)
        # audio_embed: N x D x 1 x T =&gt; N x DK x T&#39;
        padding = audio_embed.shape[-1] % self.window_size
        if padding &gt; 0:
            audio_embed = F.pad(
                audio_embed, (0, self.window_size - padding), &#34;constant&#34;, 0
            )

        embed_chunk = F.unfold(
            audio_embed[..., None, :],
            kernel_size=(1, self.window_size),
            stride=(1, self.window_size),
        )
        bsz, _, slen = embed_chunk.shape
        # N x D x K x T&#39;
        embed_chunk = embed_chunk.view(bsz, -1, self.window_size, slen)
        # N x T&#39; x K x D
        embed_chunk = embed_chunk.transpose(1, 3).contiguous()
        # NT&#39; x K x D
        embed_chunk = embed_chunk.view(bsz * slen, self.window_size, -1)
        # NT&#39; x 1 x D
        q = self.queries.expand(bsz * slen, -1, -1)
        for layer in self.decoders:
            q = layer(tgt=q, memory=embed_chunk, tgt_mask=None, memory_mask=mask)

        if self.after_norm is not None:
            q = self.after_norm(q)

        if embed_len is not None:
            embed_len = embed_len // self.window_size
        # N x T&#39; x D
        out = q.view(bsz, slen, -1)

        return out, embed_len</code></pre>
</details>
<div class="desc"><p>Window-level Qformer</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.models.phi4mm_audio.WindowQformer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, audio_embed, mask, embed_len=None) â>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, audio_embed, mask, embed_len=None):
    &#34;&#34;&#34;forward decoder&#34;&#34;&#34;
    # audio_embed: N x T x D =&gt; N x D x T

    audio_embed = audio_embed.transpose(1, 2)
    # audio_embed: N x D x 1 x T =&gt; N x DK x T&#39;
    padding = audio_embed.shape[-1] % self.window_size
    if padding &gt; 0:
        audio_embed = F.pad(
            audio_embed, (0, self.window_size - padding), &#34;constant&#34;, 0
        )

    embed_chunk = F.unfold(
        audio_embed[..., None, :],
        kernel_size=(1, self.window_size),
        stride=(1, self.window_size),
    )
    bsz, _, slen = embed_chunk.shape
    # N x D x K x T&#39;
    embed_chunk = embed_chunk.view(bsz, -1, self.window_size, slen)
    # N x T&#39; x K x D
    embed_chunk = embed_chunk.transpose(1, 3).contiguous()
    # NT&#39; x K x D
    embed_chunk = embed_chunk.view(bsz * slen, self.window_size, -1)
    # NT&#39; x 1 x D
    q = self.queries.expand(bsz * slen, -1, -1)
    for layer in self.decoders:
        q = layer(tgt=q, memory=embed_chunk, tgt_mask=None, memory_mask=mask)

    if self.after_norm is not None:
        q = self.after_norm(q)

    if embed_len is not None:
        embed_len = embed_len // self.window_size
    # N x T&#39; x D
    out = q.view(bsz, slen, -1)

    return out, embed_len</code></pre>
</details>
<div class="desc"><p>forward decoder</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.models" href="index.html">sglang.srt.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_audio.AudioEmbedding" href="#sglang.srt.models.phi4mm_audio.AudioEmbedding">AudioEmbedding</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_audio.AudioEmbedding.forward" href="#sglang.srt.models.phi4mm_audio.AudioEmbedding.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_audio.AudioEmbedding.get_audio_features" href="#sglang.srt.models.phi4mm_audio.AudioEmbedding.get_audio_features">get_audio_features</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_audio.AudioEmbedding.set_audio_embed_sizes" href="#sglang.srt.models.phi4mm_audio.AudioEmbedding.set_audio_embed_sizes">set_audio_embed_sizes</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_audio.AudioEmbedding.set_audio_embeds" href="#sglang.srt.models.phi4mm_audio.AudioEmbedding.set_audio_embeds">set_audio_embeds</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_audio.ConformerEncoder" href="#sglang.srt.models.phi4mm_audio.ConformerEncoder">ConformerEncoder</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_audio.ConformerEncoder.calculate_hs_mask" href="#sglang.srt.models.phi4mm_audio.ConformerEncoder.calculate_hs_mask">calculate_hs_mask</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_audio.ConformerEncoder.extra_multi_layer_output_idxs" href="#sglang.srt.models.phi4mm_audio.ConformerEncoder.extra_multi_layer_output_idxs">extra_multi_layer_output_idxs</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_audio.ConformerEncoder.forward" href="#sglang.srt.models.phi4mm_audio.ConformerEncoder.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_audio.ConformerEncoder.init_relative_attention_bias" href="#sglang.srt.models.phi4mm_audio.ConformerEncoder.init_relative_attention_bias">init_relative_attention_bias</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_audio.ConformerEncoderLayer" href="#sglang.srt.models.phi4mm_audio.ConformerEncoderLayer">ConformerEncoderLayer</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_audio.ConformerEncoderLayer.forward" href="#sglang.srt.models.phi4mm_audio.ConformerEncoderLayer.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_audio.TransformerEncoderBase" href="#sglang.srt.models.phi4mm_audio.TransformerEncoderBase">TransformerEncoderBase</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_audio.TransformerEncoderBase.compute_lens_change" href="#sglang.srt.models.phi4mm_audio.TransformerEncoderBase.compute_lens_change">compute_lens_change</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_audio.TransformerEncoderBase.forward" href="#sglang.srt.models.phi4mm_audio.TransformerEncoderBase.forward">forward</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_audio.TransformerEncoderBase.forward_embeddings" href="#sglang.srt.models.phi4mm_audio.TransformerEncoderBase.forward_embeddings">forward_embeddings</a></code></li>
<li><code><a title="sglang.srt.models.phi4mm_audio.TransformerEncoderBase.get_offset" href="#sglang.srt.models.phi4mm_audio.TransformerEncoderBase.get_offset">get_offset</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.models.phi4mm_audio.WindowQformer" href="#sglang.srt.models.phi4mm_audio.WindowQformer">WindowQformer</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.models.phi4mm_audio.WindowQformer.forward" href="#sglang.srt.models.phi4mm_audio.WindowQformer.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
