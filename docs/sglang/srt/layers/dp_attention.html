<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.layers.dp_attention API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.layers.dp_attention</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.layers.dp_attention.attn_tp_all_gather"><code class="name flex">
<span>def <span class="ident">attn_tp_all_gather</span></span>(<span>output_list: List[torch.Tensor], input: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def attn_tp_all_gather(output_list: List[torch.Tensor], input: torch.Tensor):
    return get_attention_tp_group().all_gather(input, output_tensor_list=output_list)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.attn_tp_all_gather_into_tensor"><code class="name flex">
<span>def <span class="ident">attn_tp_all_gather_into_tensor</span></span>(<span>output: torch.Tensor, input: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def attn_tp_all_gather_into_tensor(output: torch.Tensor, input: torch.Tensor):
    return get_attention_tp_group().all_gather_into_tensor(output, input)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.attn_tp_reduce_scatter_tensor"><code class="name flex">
<span>def <span class="ident">attn_tp_reduce_scatter_tensor</span></span>(<span>output: torch.Tensor, input: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def attn_tp_reduce_scatter_tensor(output: torch.Tensor, input: torch.Tensor):
    return get_attention_tp_group().reduce_scatter_tensor(output, input)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.compute_dp_attention_local_info"><code class="name flex">
<span>def <span class="ident">compute_dp_attention_local_info</span></span>(<span>enable_dp_attention, tp_rank, tp_size, dp_size, moe_dense_tp_size)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_dp_attention_local_info(
    enable_dp_attention, tp_rank, tp_size, dp_size, moe_dense_tp_size
):
    if not enable_dp_attention:
        return tp_rank, tp_size, 0

    local_tp_size = moe_dense_tp_size if moe_dense_tp_size else tp_size
    local_tp_rank = tp_rank % local_tp_size
    local_dp_size = max(1, dp_size // (tp_size // local_tp_size))

    local_attn_tp_size = local_tp_size // local_dp_size
    local_attn_dp_rank = local_tp_rank // local_attn_tp_size
    local_attn_tp_rank = local_tp_rank % local_attn_tp_size

    return local_attn_tp_rank, local_attn_tp_size, local_attn_dp_rank</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.compute_dp_attention_world_info"><code class="name flex">
<span>def <span class="ident">compute_dp_attention_world_info</span></span>(<span>enable_dp_attention, tp_rank, tp_size, dp_size)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_dp_attention_world_info(enable_dp_attention, tp_rank, tp_size, dp_size):
    if not enable_dp_attention:
        return tp_rank, tp_size, 0

    attn_tp_size = tp_size // dp_size
    attn_dp_rank = tp_rank // attn_tp_size
    attn_tp_rank = tp_rank % attn_tp_size

    return attn_tp_rank, attn_tp_size, attn_dp_rank</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.disable_dp_size"><code class="name flex">
<span>def <span class="ident">disable_dp_size</span></span>(<span>)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def disable_dp_size():
    &#34;&#34;&#34;Patch the tp group temporarily until this function ends.

    This method is for draft workers of speculative decoding to run draft model
    with different tp degree from that of target model workers.

    Args:
        tp_group (GroupCoordinator): the tp group coordinator
    &#34;&#34;&#34;
    global _ATTN_DP_SIZE
    assert _ATTN_DP_SIZE is not None, &#34;dp attention not initialized!&#34;

    old_dp_size = _ATTN_DP_SIZE
    _ATTN_DP_SIZE = 1
    try:
        yield
    finally:
        _ATTN_DP_SIZE = old_dp_size</code></pre>
</details>
<div class="desc"><p>Patch the tp group temporarily until this function ends.</p>
<p>This method is for draft workers of speculative decoding to run draft model
with different tp degree from that of target model workers.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tp_group</code></strong> :&ensp;<code>GroupCoordinator</code></dt>
<dd>the tp group coordinator</dd>
</dl></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.dp_gather_partial"><code class="name flex">
<span>def <span class="ident">dp_gather_partial</span></span>(<span>global_tokens: torch.Tensor,<br>local_tokens: torch.Tensor,<br>forward_batch: ForwardBatch)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dp_gather_partial(
    global_tokens: torch.Tensor,
    local_tokens: torch.Tensor,
    forward_batch: ForwardBatch,
):
    _dp_gather(global_tokens, local_tokens, forward_batch, is_partial=True)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.dp_gather_replicate"><code class="name flex">
<span>def <span class="ident">dp_gather_replicate</span></span>(<span>global_tokens: torch.Tensor,<br>local_tokens: torch.Tensor,<br>forward_batch: ForwardBatch)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dp_gather_replicate(
    global_tokens: torch.Tensor,
    local_tokens: torch.Tensor,
    forward_batch: ForwardBatch,
):
    _dp_gather(global_tokens, local_tokens, forward_batch, is_partial=False)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.dp_reduce_scatter_tensor"><code class="name flex">
<span>def <span class="ident">dp_reduce_scatter_tensor</span></span>(<span>output: torch.Tensor, input: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dp_reduce_scatter_tensor(output: torch.Tensor, input: torch.Tensor):
    if get_tensor_model_parallel_world_size() == get_attention_dp_size():
        get_tp_group().reduce_scatter_tensor(output, input)
    else:
        scattered_local_tokens = input.tensor_split(
            get_tensor_model_parallel_world_size()
        )[get_tensor_model_parallel_rank()]
        get_tp_group().reduce_scatter_tensor(scattered_local_tokens, input)
        get_attention_tp_group().all_gather_into_tensor(output, scattered_local_tokens)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.dp_scatter"><code class="name flex">
<span>def <span class="ident">dp_scatter</span></span>(<span>local_tokens: torch.Tensor,<br>global_tokens: torch.Tensor,<br>forward_batch: ForwardBatch)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dp_scatter(
    local_tokens: torch.Tensor,  # output
    global_tokens: torch.Tensor,  # input
    forward_batch: ForwardBatch,
):
    # local_num_tokens is not necessarily the same as local_tokens.shape[0],
    # since local_tokens may be padded for cuda graph
    local_start_pos, local_num_tokens = get_dp_local_info(forward_batch)

    local_tokens.fill_(0)
    assert local_tokens.is_contiguous()
    assert global_tokens.is_contiguous()
    if local_tokens.shape[0] &gt; 0:
        assert (
            local_tokens.untyped_storage() is not global_tokens.untyped_storage()
        ), &#34;aliasing between local_tokens and global_tokens not allowed&#34;

        memcpy_triton(
            local_tokens, global_tokens, 0, local_start_pos, local_num_tokens, True
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.get_attention_dp_rank"><code class="name flex">
<span>def <span class="ident">get_attention_dp_rank</span></span>(<span>) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_attention_dp_rank() -&gt; int:
    assert _ATTN_DP_RANK is not None, &#34;dp attention not initialized!&#34;
    return _ATTN_DP_RANK</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.get_attention_dp_size"><code class="name flex">
<span>def <span class="ident">get_attention_dp_size</span></span>(<span>) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_attention_dp_size() -&gt; int:
    assert _ATTN_DP_SIZE is not None, &#34;dp attention not initialized!&#34;
    return _ATTN_DP_SIZE</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.get_attention_tp_group"><code class="name flex">
<span>def <span class="ident">get_attention_tp_group</span></span>(<span>) ‑> <a title="sglang.srt.distributed.parallel_state.GroupCoordinator" href="../distributed/parallel_state.html#sglang.srt.distributed.parallel_state.GroupCoordinator">GroupCoordinator</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_attention_tp_group() -&gt; GroupCoordinator:
    assert _ATTN_TP_GROUP is not None, &#34;dp attention not initialized!&#34;
    return _ATTN_TP_GROUP</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.get_attention_tp_rank"><code class="name flex">
<span>def <span class="ident">get_attention_tp_rank</span></span>(<span>) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_attention_tp_rank() -&gt; int:
    assert _ATTN_TP_RANK is not None, &#34;dp attention not initialized!&#34;
    return _ATTN_TP_RANK</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.get_attention_tp_size"><code class="name flex">
<span>def <span class="ident">get_attention_tp_size</span></span>(<span>) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_attention_tp_size() -&gt; int:
    assert _ATTN_TP_SIZE is not None, &#34;dp attention not initialized!&#34;
    return _ATTN_TP_SIZE</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.get_dp_global_num_tokens"><code class="name flex">
<span>def <span class="ident">get_dp_global_num_tokens</span></span>(<span>) ‑> List[int]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dp_global_num_tokens() -&gt; List[int]:
    return _DpGatheredBufferWrapper.get_dp_global_num_tokens()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.get_dp_local_info"><code class="name flex">
<span>def <span class="ident">get_dp_local_info</span></span>(<span>forward_batch: ForwardBatch) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dp_local_info(forward_batch: ForwardBatch) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    # `get_dp_local_info` is only called in global DP gather and scatter. We use global DP rank here.
    dp_rank = get_attention_dp_rank()

    if forward_batch.dp_local_start_pos is None:
        cumtokens = torch.cumsum(forward_batch.global_num_tokens_gpu, dim=0)
        if dp_rank == 0:
            local_start_pos = torch.zeros_like(cumtokens[0])
        else:
            local_start_pos = cumtokens[dp_rank - 1]
        local_num_tokens = forward_batch.global_num_tokens_gpu[dp_rank]

        forward_batch.dp_local_start_pos = local_start_pos
        forward_batch.dp_local_num_tokens = local_num_tokens

    return forward_batch.dp_local_start_pos, forward_batch.dp_local_num_tokens</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.get_global_dp_buffer"><code class="name flex">
<span>def <span class="ident">get_global_dp_buffer</span></span>(<span>) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_global_dp_buffer() -&gt; torch.Tensor:
    return _DpGatheredBufferWrapper.get_global_dp_buffer()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.get_global_dp_buffer_len"><code class="name flex">
<span>def <span class="ident">get_global_dp_buffer_len</span></span>(<span>) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_global_dp_buffer_len() -&gt; int:
    return _DpGatheredBufferWrapper.get_global_dp_buffer_len()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.get_local_attention_dp_rank"><code class="name flex">
<span>def <span class="ident">get_local_attention_dp_rank</span></span>(<span>) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_local_attention_dp_rank() -&gt; int:
    assert _LOCAL_ATTN_DP_RANK is not None, &#34;dp attention not initialized!&#34;
    return _LOCAL_ATTN_DP_RANK</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.get_local_attention_dp_size"><code class="name flex">
<span>def <span class="ident">get_local_attention_dp_size</span></span>(<span>) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_local_attention_dp_size() -&gt; int:
    assert _LOCAL_ATTN_DP_SIZE is not None, &#34;dp attention not initialized!&#34;
    return _LOCAL_ATTN_DP_SIZE</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.get_local_dp_buffer"><code class="name flex">
<span>def <span class="ident">get_local_dp_buffer</span></span>(<span>) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_local_dp_buffer() -&gt; torch.Tensor:
    return _DpGatheredBufferWrapper.get_local_dp_buffer()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.get_local_dp_buffer_len"><code class="name flex">
<span>def <span class="ident">get_local_dp_buffer_len</span></span>(<span>) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_local_dp_buffer_len() -&gt; int:
    return _DpGatheredBufferWrapper.get_local_dp_buffer_len()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.initialize_dp_attention"><code class="name flex">
<span>def <span class="ident">initialize_dp_attention</span></span>(<span>server_args: ServerArgs, model_config: ModelConfig)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize_dp_attention(
    server_args: ServerArgs,
    model_config: ModelConfig,
):
    global _ATTN_TP_GROUP, _ATTN_TP_RANK, _ATTN_TP_SIZE, _ATTN_DP_RANK, _ATTN_DP_SIZE
    global _LOCAL_ATTN_DP_SIZE, _LOCAL_ATTN_DP_RANK, _ENABLE_DP_ATTENTION_FLAG

    from sglang.srt.layers.sampler import SYNC_TOKEN_IDS_ACROSS_TP

    enable_dp_attention = server_args.enable_dp_attention
    tp_size = server_args.tp_size
    dp_size = server_args.dp_size
    moe_dense_tp_size = server_args.moe_dense_tp_size
    pp_size = server_args.pp_size

    tp_rank = get_tensor_model_parallel_rank()

    _ENABLE_DP_ATTENTION_FLAG = enable_dp_attention

    _ATTN_TP_RANK, _ATTN_TP_SIZE, _ATTN_DP_RANK = compute_dp_attention_world_info(
        enable_dp_attention, tp_rank, tp_size, dp_size
    )
    _, _, _LOCAL_ATTN_DP_RANK = compute_dp_attention_local_info(
        enable_dp_attention, tp_rank, tp_size, dp_size, moe_dense_tp_size
    )

    if enable_dp_attention:
        _ATTN_DP_SIZE = dp_size
        if moe_dense_tp_size is None:
            _LOCAL_ATTN_DP_SIZE = _ATTN_DP_SIZE
        else:
            _LOCAL_ATTN_DP_SIZE = max(1, dp_size // (tp_size // moe_dense_tp_size))
    else:
        _ATTN_DP_SIZE = 1
        _LOCAL_ATTN_DP_SIZE = 1

    tp_group = get_tp_group()
    _ATTN_TP_GROUP = GroupCoordinator(
        [
            list(range(head, head + _ATTN_TP_SIZE))
            for head in range(0, pp_size * tp_size, _ATTN_TP_SIZE)
        ],
        tp_group.local_rank,
        torch.distributed.get_backend(tp_group.device_group),
        use_pynccl=SYNC_TOKEN_IDS_ACROSS_TP,
        use_pymscclpp=False,
        use_custom_allreduce=False,
        use_hpu_communicator=False,
        use_xpu_communicator=False,
        use_npu_communicator=False,
        group_name=&#34;attention_tp&#34;,
    )

    _DpGatheredBufferWrapper.set_metadata(
        hidden_size=model_config.hidden_size,
        dtype=model_config.dtype,
        device=torch.device(server_args.device),
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.is_dp_attention_enabled"><code class="name flex">
<span>def <span class="ident">is_dp_attention_enabled</span></span>(<span>) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_dp_attention_enabled() -&gt; bool:
    return _ENABLE_DP_ATTENTION_FLAG</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.memcpy_triton"><code class="name flex">
<span>def <span class="ident">memcpy_triton</span></span>(<span>dst, src, dim, offset, sz, offset_src)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def memcpy_triton(dst, src, dim, offset, sz, offset_src):
    max_size = min(src.numel(), dst.numel())
    assert dim == 0, &#34;dim != 0 unsupported&#34;
    assert src.shape[1:] == dst.shape[1:], &#34;src and dst must have same shape&#34;
    chunk_size = prod(src.shape[1:])
    BLOCK_SIZE = 8192
    grid = (triton.cdiv(max_size, BLOCK_SIZE),)

    memcpy_triton_kernel[grid](dst, src, offset, sz, offset_src, chunk_size, BLOCK_SIZE)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.prod"><code class="name flex">
<span>def <span class="ident">prod</span></span>(<span>x)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prod(x):
    return functools.reduce(lambda a, b: a * b, x, 1)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.set_dp_buffer_len"><code class="name flex">
<span>def <span class="ident">set_dp_buffer_len</span></span>(<span>global_dp_buffer_len: int,<br>local_dp_buffer_len: int,<br>global_num_tokens: Optional[List[int]] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_dp_buffer_len(
    global_dp_buffer_len: int,
    local_dp_buffer_len: int,
    global_num_tokens: Optional[List[int]] = None,
):
    _DpGatheredBufferWrapper.set_dp_buffer_len(
        global_dp_buffer_len, local_dp_buffer_len, global_num_tokens
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.layers.dp_attention.DpPaddingMode"><code class="flex name class">
<span>class <span class="ident">DpPaddingMode</span></span>
<span>(</span><span>*args, **kwds)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DpPaddingMode(IntEnum):

    # Padding tokens to max length and then gather tokens using `all_gather_into_tensor`
    MAX_LEN = auto()
    # Padding tokens to sum length and then gather tokens using `all_reduce`
    SUM_LEN = auto()

    def is_max_len(self):
        return self == DpPaddingMode.MAX_LEN

    def is_sum_len(self):
        return self == DpPaddingMode.SUM_LEN

    @classmethod
    def get_dp_padding_mode(cls, global_num_tokens: List[int]) -&gt; DpPaddingMode:
        # we choose the mode that minimizes the communication cost
        max_len = max(global_num_tokens)
        sum_len = sum(global_num_tokens)
        if sum_len * 2 &gt; max_len * get_attention_dp_size():
            return cls.MAX_LEN
        else:
            return cls.SUM_LEN

    @classmethod
    def get_default_mode_in_cuda_graph(cls) -&gt; DpPaddingMode:
        return cls.MAX_LEN</code></pre>
</details>
<div class="desc"><p>Enum where members are also (and must be) ints</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.IntEnum</li>
<li>builtins.int</li>
<li>enum.ReprEnum</li>
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.layers.dp_attention.DpPaddingMode.MAX_LEN"><code class="name">var <span class="ident">MAX_LEN</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.DpPaddingMode.SUM_LEN"><code class="name">var <span class="ident">SUM_LEN</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.layers.dp_attention.DpPaddingMode.get_default_mode_in_cuda_graph"><code class="name flex">
<span>def <span class="ident">get_default_mode_in_cuda_graph</span></span>(<span>) ‑> <a title="sglang.srt.layers.dp_attention.DpPaddingMode" href="#sglang.srt.layers.dp_attention.DpPaddingMode">DpPaddingMode</a></span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.DpPaddingMode.get_dp_padding_mode"><code class="name flex">
<span>def <span class="ident">get_dp_padding_mode</span></span>(<span>global_num_tokens: List[int]) ‑> <a title="sglang.srt.layers.dp_attention.DpPaddingMode" href="#sglang.srt.layers.dp_attention.DpPaddingMode">DpPaddingMode</a></span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.dp_attention.DpPaddingMode.is_max_len"><code class="name flex">
<span>def <span class="ident">is_max_len</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_max_len(self):
    return self == DpPaddingMode.MAX_LEN</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.dp_attention.DpPaddingMode.is_sum_len"><code class="name flex">
<span>def <span class="ident">is_sum_len</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_sum_len(self):
    return self == DpPaddingMode.SUM_LEN</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.layers" href="index.html">sglang.srt.layers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.layers.dp_attention.attn_tp_all_gather" href="#sglang.srt.layers.dp_attention.attn_tp_all_gather">attn_tp_all_gather</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.attn_tp_all_gather_into_tensor" href="#sglang.srt.layers.dp_attention.attn_tp_all_gather_into_tensor">attn_tp_all_gather_into_tensor</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.attn_tp_reduce_scatter_tensor" href="#sglang.srt.layers.dp_attention.attn_tp_reduce_scatter_tensor">attn_tp_reduce_scatter_tensor</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.compute_dp_attention_local_info" href="#sglang.srt.layers.dp_attention.compute_dp_attention_local_info">compute_dp_attention_local_info</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.compute_dp_attention_world_info" href="#sglang.srt.layers.dp_attention.compute_dp_attention_world_info">compute_dp_attention_world_info</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.disable_dp_size" href="#sglang.srt.layers.dp_attention.disable_dp_size">disable_dp_size</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.dp_gather_partial" href="#sglang.srt.layers.dp_attention.dp_gather_partial">dp_gather_partial</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.dp_gather_replicate" href="#sglang.srt.layers.dp_attention.dp_gather_replicate">dp_gather_replicate</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.dp_reduce_scatter_tensor" href="#sglang.srt.layers.dp_attention.dp_reduce_scatter_tensor">dp_reduce_scatter_tensor</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.dp_scatter" href="#sglang.srt.layers.dp_attention.dp_scatter">dp_scatter</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.get_attention_dp_rank" href="#sglang.srt.layers.dp_attention.get_attention_dp_rank">get_attention_dp_rank</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.get_attention_dp_size" href="#sglang.srt.layers.dp_attention.get_attention_dp_size">get_attention_dp_size</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.get_attention_tp_group" href="#sglang.srt.layers.dp_attention.get_attention_tp_group">get_attention_tp_group</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.get_attention_tp_rank" href="#sglang.srt.layers.dp_attention.get_attention_tp_rank">get_attention_tp_rank</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.get_attention_tp_size" href="#sglang.srt.layers.dp_attention.get_attention_tp_size">get_attention_tp_size</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.get_dp_global_num_tokens" href="#sglang.srt.layers.dp_attention.get_dp_global_num_tokens">get_dp_global_num_tokens</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.get_dp_local_info" href="#sglang.srt.layers.dp_attention.get_dp_local_info">get_dp_local_info</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.get_global_dp_buffer" href="#sglang.srt.layers.dp_attention.get_global_dp_buffer">get_global_dp_buffer</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.get_global_dp_buffer_len" href="#sglang.srt.layers.dp_attention.get_global_dp_buffer_len">get_global_dp_buffer_len</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.get_local_attention_dp_rank" href="#sglang.srt.layers.dp_attention.get_local_attention_dp_rank">get_local_attention_dp_rank</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.get_local_attention_dp_size" href="#sglang.srt.layers.dp_attention.get_local_attention_dp_size">get_local_attention_dp_size</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.get_local_dp_buffer" href="#sglang.srt.layers.dp_attention.get_local_dp_buffer">get_local_dp_buffer</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.get_local_dp_buffer_len" href="#sglang.srt.layers.dp_attention.get_local_dp_buffer_len">get_local_dp_buffer_len</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.initialize_dp_attention" href="#sglang.srt.layers.dp_attention.initialize_dp_attention">initialize_dp_attention</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.is_dp_attention_enabled" href="#sglang.srt.layers.dp_attention.is_dp_attention_enabled">is_dp_attention_enabled</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.memcpy_triton" href="#sglang.srt.layers.dp_attention.memcpy_triton">memcpy_triton</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.prod" href="#sglang.srt.layers.dp_attention.prod">prod</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.set_dp_buffer_len" href="#sglang.srt.layers.dp_attention.set_dp_buffer_len">set_dp_buffer_len</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.layers.dp_attention.DpPaddingMode" href="#sglang.srt.layers.dp_attention.DpPaddingMode">DpPaddingMode</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.dp_attention.DpPaddingMode.MAX_LEN" href="#sglang.srt.layers.dp_attention.DpPaddingMode.MAX_LEN">MAX_LEN</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.DpPaddingMode.SUM_LEN" href="#sglang.srt.layers.dp_attention.DpPaddingMode.SUM_LEN">SUM_LEN</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.DpPaddingMode.get_default_mode_in_cuda_graph" href="#sglang.srt.layers.dp_attention.DpPaddingMode.get_default_mode_in_cuda_graph">get_default_mode_in_cuda_graph</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.DpPaddingMode.get_dp_padding_mode" href="#sglang.srt.layers.dp_attention.DpPaddingMode.get_dp_padding_mode">get_dp_padding_mode</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.DpPaddingMode.is_max_len" href="#sglang.srt.layers.dp_attention.DpPaddingMode.is_max_len">is_max_len</a></code></li>
<li><code><a title="sglang.srt.layers.dp_attention.DpPaddingMode.is_sum_len" href="#sglang.srt.layers.dp_attention.DpPaddingMode.is_sum_len">is_sum_len</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
