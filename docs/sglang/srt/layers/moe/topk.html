<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.layers.moe.topk API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.layers.moe.topk</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.layers.moe.topk.apply_topk_weights_cpu"><code class="name flex">
<span>def <span class="ident">apply_topk_weights_cpu</span></span>(<span>need_apply, topk_weights, inputs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_topk_weights_cpu(need_apply, topk_weights, inputs):
    if not need_apply:
        return inputs, topk_weights

    # TODO: fuse below processing in fused_experts_cpu kernel
    inputs = inputs * topk_weights.to(inputs.dtype)
    topk_weights = torch.ones_like(
        topk_weights, dtype=torch.float32
    )  # clear topk_weights as already applied

    return inputs, topk_weights</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.biased_grouped_topk"><code class="name flex">
<span>def <span class="ident">biased_grouped_topk</span></span>(<span>hidden_states: torch.Tensor,<br>gating_output: torch.Tensor,<br>correction_bias: torch.Tensor,<br>topk: int,<br>renormalize: bool,<br>num_expert_group: Optional[int] = None,<br>topk_group: Optional[int] = None,<br>num_fused_shared_experts: int = 0,<br>routed_scaling_factor: Optional[float] = None,<br>num_token_non_padded: Optional[torch.Tensor] = None,<br>expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,<br>apply_routed_scaling_factor_on_output: Optional[bool] = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def biased_grouped_topk_gpu(
    hidden_states: torch.Tensor,
    gating_output: torch.Tensor,
    correction_bias: torch.Tensor,
    topk: int,
    renormalize: bool,
    num_expert_group: Optional[int] = None,
    topk_group: Optional[int] = None,
    num_fused_shared_experts: int = 0,
    routed_scaling_factor: Optional[float] = None,
    num_token_non_padded: Optional[torch.Tensor] = None,
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,
    apply_routed_scaling_factor_on_output: Optional[bool] = False,
):
    assert (
        routed_scaling_factor is not None
    ), &#34;routed_scaling_factor is required for biased_grouped_topk&#34;
    # TODO: moe_fused_gate kernel is not supported for num_fused_shared_experts &gt; 0 now.
    if (
        _is_cuda
        and gating_output.shape[1] // num_expert_group
        &lt;= 32  # moe_fused_gate kernel ensure that num_experts/num_expert_group does not exceed MAX_VPT=32 now. And when kernel can handle MAX_VPT &gt; 32, we can remove this assertion.
        and is_power_of_two(correction_bias.shape[0])
    ):
        topk_weights, topk_ids = moe_fused_gate(
            gating_output.to(dtype=torch.float32),
            correction_bias,
            num_expert_group,
            topk_group,
            topk,
            num_fused_shared_experts,
            routed_scaling_factor,
            apply_routed_scaling_factor_on_output,
        )
        # TODO merge into kernel
        if (expert_location_dispatch_info is not None) or (
            num_token_non_padded is not None
        ):
            topk_ids = _biased_grouped_topk_postprocess(
                topk_ids, expert_location_dispatch_info, num_token_non_padded
            )
        return topk_weights, topk_ids
    elif _use_aiter:
        assert not apply_routed_scaling_factor_on_output, &#34;Not implemented&#34;
        token = gating_output.shape[0]
        device = gating_output.device
        assert (
            hidden_states.shape[0] == gating_output.shape[0]
        ), f&#34;Number of tokens mismatch: hidden_states.shape[0] = {hidden_states.shape[0]}, gating_output.shape[0] = {gating_output.shape[0]}&#34;
        topk_weights = torch.empty((token, topk), dtype=torch.float32, device=device)
        topk_ids = torch.empty((token, topk), dtype=torch.int32, device=device)
        aiter_biased_grouped_topk(
            gating_output.to(dtype=torch.float32),
            correction_bias,
            topk_weights,
            topk_ids,
            num_expert_group,
            topk_group,
            renormalize,
            routed_scaling_factor,
        )
        return topk_weights, topk_ids
    else:
        return biased_grouped_topk_impl(
            hidden_states,
            gating_output,
            correction_bias,
            topk,
            renormalize,
            num_expert_group,
            topk_group,
            num_fused_shared_experts=num_fused_shared_experts,
            routed_scaling_factor=routed_scaling_factor,
            num_token_non_padded=num_token_non_padded,
            expert_location_dispatch_info=expert_location_dispatch_info,
            apply_routed_scaling_factor_on_output=apply_routed_scaling_factor_on_output,
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.biased_grouped_topk_cpu"><code class="name flex">
<span>def <span class="ident">biased_grouped_topk_cpu</span></span>(<span>hidden_states: torch.Tensor,<br>gating_output: torch.Tensor,<br>correction_bias: torch.Tensor,<br>topk: int,<br>renormalize: bool,<br>num_expert_group: Optional[int] = None,<br>topk_group: Optional[int] = None,<br>compiled: bool = True,<br>num_fused_shared_experts: int = 0,<br>routed_scaling_factor: Optional[float] = None,<br>num_token_non_padded: Optional[torch.Tensor] = None,<br>expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,<br>apply_routed_scaling_factor_on_output: Optional[bool] = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def biased_grouped_topk_cpu(
    hidden_states: torch.Tensor,
    gating_output: torch.Tensor,
    correction_bias: torch.Tensor,
    topk: int,
    renormalize: bool,
    num_expert_group: Optional[int] = None,
    topk_group: Optional[int] = None,
    compiled: bool = True,
    num_fused_shared_experts: int = 0,
    routed_scaling_factor: Optional[float] = None,
    num_token_non_padded: Optional[torch.Tensor] = None,
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,
    apply_routed_scaling_factor_on_output: Optional[bool] = False,
):
    assert expert_location_dispatch_info is None
    assert not apply_routed_scaling_factor_on_output, &#34;Not implemented&#34;
    return torch.ops.sgl_kernel.biased_grouped_topk_cpu(
        hidden_states,
        gating_output,
        correction_bias,
        topk,
        renormalize,
        num_expert_group,
        topk_group,
        num_fused_shared_experts,
        routed_scaling_factor,
        num_token_non_padded,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.biased_grouped_topk_gpu"><code class="name flex">
<span>def <span class="ident">biased_grouped_topk_gpu</span></span>(<span>hidden_states: torch.Tensor,<br>gating_output: torch.Tensor,<br>correction_bias: torch.Tensor,<br>topk: int,<br>renormalize: bool,<br>num_expert_group: Optional[int] = None,<br>topk_group: Optional[int] = None,<br>num_fused_shared_experts: int = 0,<br>routed_scaling_factor: Optional[float] = None,<br>num_token_non_padded: Optional[torch.Tensor] = None,<br>expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,<br>apply_routed_scaling_factor_on_output: Optional[bool] = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def biased_grouped_topk_gpu(
    hidden_states: torch.Tensor,
    gating_output: torch.Tensor,
    correction_bias: torch.Tensor,
    topk: int,
    renormalize: bool,
    num_expert_group: Optional[int] = None,
    topk_group: Optional[int] = None,
    num_fused_shared_experts: int = 0,
    routed_scaling_factor: Optional[float] = None,
    num_token_non_padded: Optional[torch.Tensor] = None,
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,
    apply_routed_scaling_factor_on_output: Optional[bool] = False,
):
    assert (
        routed_scaling_factor is not None
    ), &#34;routed_scaling_factor is required for biased_grouped_topk&#34;
    # TODO: moe_fused_gate kernel is not supported for num_fused_shared_experts &gt; 0 now.
    if (
        _is_cuda
        and gating_output.shape[1] // num_expert_group
        &lt;= 32  # moe_fused_gate kernel ensure that num_experts/num_expert_group does not exceed MAX_VPT=32 now. And when kernel can handle MAX_VPT &gt; 32, we can remove this assertion.
        and is_power_of_two(correction_bias.shape[0])
    ):
        topk_weights, topk_ids = moe_fused_gate(
            gating_output.to(dtype=torch.float32),
            correction_bias,
            num_expert_group,
            topk_group,
            topk,
            num_fused_shared_experts,
            routed_scaling_factor,
            apply_routed_scaling_factor_on_output,
        )
        # TODO merge into kernel
        if (expert_location_dispatch_info is not None) or (
            num_token_non_padded is not None
        ):
            topk_ids = _biased_grouped_topk_postprocess(
                topk_ids, expert_location_dispatch_info, num_token_non_padded
            )
        return topk_weights, topk_ids
    elif _use_aiter:
        assert not apply_routed_scaling_factor_on_output, &#34;Not implemented&#34;
        token = gating_output.shape[0]
        device = gating_output.device
        assert (
            hidden_states.shape[0] == gating_output.shape[0]
        ), f&#34;Number of tokens mismatch: hidden_states.shape[0] = {hidden_states.shape[0]}, gating_output.shape[0] = {gating_output.shape[0]}&#34;
        topk_weights = torch.empty((token, topk), dtype=torch.float32, device=device)
        topk_ids = torch.empty((token, topk), dtype=torch.int32, device=device)
        aiter_biased_grouped_topk(
            gating_output.to(dtype=torch.float32),
            correction_bias,
            topk_weights,
            topk_ids,
            num_expert_group,
            topk_group,
            renormalize,
            routed_scaling_factor,
        )
        return topk_weights, topk_ids
    else:
        return biased_grouped_topk_impl(
            hidden_states,
            gating_output,
            correction_bias,
            topk,
            renormalize,
            num_expert_group,
            topk_group,
            num_fused_shared_experts=num_fused_shared_experts,
            routed_scaling_factor=routed_scaling_factor,
            num_token_non_padded=num_token_non_padded,
            expert_location_dispatch_info=expert_location_dispatch_info,
            apply_routed_scaling_factor_on_output=apply_routed_scaling_factor_on_output,
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.biased_grouped_topk_impl"><code class="name flex">
<span>def <span class="ident">biased_grouped_topk_impl</span></span>(<span>hidden_states: torch.Tensor,<br>gating_output: torch.Tensor,<br>correction_bias: torch.Tensor,<br>topk: int,<br>renormalize: bool,<br>num_expert_group: Optional[int] = None,<br>topk_group: Optional[int] = None,<br>num_fused_shared_experts: int = 0,<br>routed_scaling_factor: Optional[float] = None,<br>num_token_non_padded: Optional[torch.Tensor] = None,<br>expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,<br>apply_routed_scaling_factor_on_output: Optional[bool] = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.compile(dynamic=True, backend=get_compiler_backend(), disable=_is_npu)
def biased_grouped_topk_impl(
    hidden_states: torch.Tensor,
    gating_output: torch.Tensor,
    correction_bias: torch.Tensor,
    topk: int,
    renormalize: bool,
    num_expert_group: Optional[int] = None,
    topk_group: Optional[int] = None,
    num_fused_shared_experts: int = 0,
    routed_scaling_factor: Optional[float] = None,
    num_token_non_padded: Optional[torch.Tensor] = None,
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,
    apply_routed_scaling_factor_on_output: Optional[bool] = False,
):
    assert hidden_states.shape[0] == gating_output.shape[0], &#34;Number of tokens mismatch&#34;

    scores = gating_output.sigmoid()
    num_token = scores.shape[0]
    num_experts = scores.shape[1]
    scores_for_choice = scores.view(num_token, -1) + correction_bias.unsqueeze(0)
    group_scores = (
        scores_for_choice.view(num_token, num_expert_group, -1)
        .topk(2, dim=-1)[0]
        .sum(dim=-1)
    )  # [n, n_group]
    group_idx = torch.topk(group_scores, k=topk_group, dim=-1, sorted=False)[
        1
    ]  # [n, top_k_group]
    group_mask = torch.zeros_like(group_scores)  # [n, n_group]
    group_mask.scatter_(1, group_idx, 1)  # [n, n_group]
    score_mask = (
        group_mask.unsqueeze(-1)
        .expand(num_token, num_expert_group, scores.shape[-1] // num_expert_group)
        .reshape(num_token, -1)
    )  # [n, e]
    tmp_scores = scores_for_choice.masked_fill(
        ~score_mask.bool(), float(&#34;-inf&#34;)
    )  # [n, e]
    # TODO: NPU can&#39;t support directly evaluating a comparison for now
    _, topk_ids = torch.topk(
        tmp_scores,
        k=topk,
        dim=-1,
        sorted=(True if num_fused_shared_experts &gt; 0 else False),
    )
    topk_weights = scores.gather(1, topk_ids)

    if num_fused_shared_experts:
        topk_ids[:, -1] = torch.randint(
            low=num_experts,
            high=num_experts + num_fused_shared_experts,
            size=(topk_ids.size(0),),
            dtype=topk_ids.dtype,
            device=topk_ids.device,
        )
        topk_weights[:, -1] = topk_weights[:, :-1].sum(dim=-1) / routed_scaling_factor

    if renormalize:
        topk_weights_sum = (
            topk_weights.sum(dim=-1, keepdim=True)
            if num_fused_shared_experts == 0
            else topk_weights[:, :-1].sum(dim=-1, keepdim=True)
        )
        topk_weights = topk_weights / topk_weights_sum
        if apply_routed_scaling_factor_on_output:
            topk_weights *= routed_scaling_factor

    topk_weights, topk_ids = topk_weights.to(torch.float32), topk_ids.to(torch.int32)
    topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info)
    _mask_topk_ids_padded_region(topk_ids, num_token_non_padded)
    return topk_weights, topk_ids</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.fused_topk"><code class="name flex">
<span>def <span class="ident">fused_topk</span></span>(<span>hidden_states: torch.Tensor,<br>gating_output: torch.Tensor,<br>topk: int,<br>renormalize: bool,<br>num_token_non_padded: Optional[torch.Tensor] = None,<br>expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fused_topk(
    hidden_states: torch.Tensor,
    gating_output: torch.Tensor,
    topk: int,
    renormalize: bool,
    num_token_non_padded: Optional[torch.Tensor] = None,
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,
):
    assert hidden_states.shape[0] == gating_output.shape[0], &#34;Number of tokens mismatch&#34;

    M, _ = hidden_states.shape

    topk_weights = torch.empty(
        M, topk, dtype=torch.float32, device=hidden_states.device
    )
    topk_ids = torch.empty(M, topk, dtype=torch.int32, device=hidden_states.device)

    topk_softmax(
        topk_weights,
        topk_ids,
        gating_output,
        renormalize,
    )

    topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info)
    _mask_topk_ids_padded_region(topk_ids, num_token_non_padded)
    return topk_weights, topk_ids</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.fused_topk_cpu"><code class="name flex">
<span>def <span class="ident">fused_topk_cpu</span></span>(<span>hidden_states: torch.Tensor,<br>gating_output: torch.Tensor,<br>topk: int,<br>renormalize: bool,<br>num_token_non_padded: Optional[torch.Tensor] = None,<br>expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,<br>correction_bias: torch.Tensor = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fused_topk_cpu(
    hidden_states: torch.Tensor,
    gating_output: torch.Tensor,
    topk: int,
    renormalize: bool,
    num_token_non_padded: Optional[torch.Tensor] = None,
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,
    correction_bias: torch.Tensor = None,
):
    topk_weights, topk_ids = torch.ops.sgl_kernel.topk_softmax_cpu(
        hidden_states=hidden_states,
        gating_output=gating_output,
        topk=topk,
        renormalize=renormalize,
    )
    topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info)
    _mask_topk_ids_padded_region(topk_ids, num_token_non_padded)
    return topk_weights, topk_ids</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.fused_topk_native"><code class="name flex">
<span>def <span class="ident">fused_topk_native</span></span>(<span>hidden_states: torch.Tensor,<br>gating_output: torch.Tensor,<br>topk: int,<br>renormalize: bool,<br>correction_bias: torch.Tensor = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fused_topk_torch_native(
    hidden_states: torch.Tensor,
    gating_output: torch.Tensor,
    topk: int,
    renormalize: bool,
    correction_bias: torch.Tensor = None,
):
    if correction_bias is not None:
        n_routed_experts = gating_output.shape[-1]
        scores = gating_output.softmax(dim=-1)
        scores_for_choice = scores.view(
            -1, n_routed_experts
        ) + correction_bias.unsqueeze(0)
        topk_ids = torch.topk(scores_for_choice, k=topk, dim=-1, sorted=False)[1]
        topk_weights = scores.gather(1, topk_ids)
    else:
        assert (
            hidden_states.shape[0] == gating_output.shape[0]
        ), f&#34;Number of tokens mismatch, {hidden_states.shape=} vs {gating_output.shape=}&#34;
        M, _ = hidden_states.shape
        topk_weights = torch.empty(
            M, topk, dtype=torch.float32, device=hidden_states.device
        )
        topk_ids = torch.empty(M, topk, dtype=torch.int32, device=hidden_states.device)
        topk_weights = F.softmax(gating_output.float(), dim=-1)
        topk_weights, topk_ids = torch.topk(topk_weights, topk, dim=-1)

    if renormalize:
        topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)
    return topk_weights, topk_ids</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.fused_topk_torch_native"><code class="name flex">
<span>def <span class="ident">fused_topk_torch_native</span></span>(<span>hidden_states: torch.Tensor,<br>gating_output: torch.Tensor,<br>topk: int,<br>renormalize: bool,<br>correction_bias: torch.Tensor = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fused_topk_torch_native(
    hidden_states: torch.Tensor,
    gating_output: torch.Tensor,
    topk: int,
    renormalize: bool,
    correction_bias: torch.Tensor = None,
):
    if correction_bias is not None:
        n_routed_experts = gating_output.shape[-1]
        scores = gating_output.softmax(dim=-1)
        scores_for_choice = scores.view(
            -1, n_routed_experts
        ) + correction_bias.unsqueeze(0)
        topk_ids = torch.topk(scores_for_choice, k=topk, dim=-1, sorted=False)[1]
        topk_weights = scores.gather(1, topk_ids)
    else:
        assert (
            hidden_states.shape[0] == gating_output.shape[0]
        ), f&#34;Number of tokens mismatch, {hidden_states.shape=} vs {gating_output.shape=}&#34;
        M, _ = hidden_states.shape
        topk_weights = torch.empty(
            M, topk, dtype=torch.float32, device=hidden_states.device
        )
        topk_ids = torch.empty(M, topk, dtype=torch.int32, device=hidden_states.device)
        topk_weights = F.softmax(gating_output.float(), dim=-1)
        topk_weights, topk_ids = torch.topk(topk_weights, topk, dim=-1)

    if renormalize:
        topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)
    return topk_weights, topk_ids</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.grouped_topk"><code class="name flex">
<span>def <span class="ident">grouped_topk</span></span>(<span>hidden_states: torch.Tensor,<br>gating_output: torch.Tensor,<br>topk: int,<br>renormalize: bool,<br>num_expert_group: Optional[int] = None,<br>topk_group: Optional[int] = None,<br>num_fused_shared_experts: int = 0,<br>routed_scaling_factor: Optional[float] = None,<br>num_token_non_padded: Optional[torch.Tensor] = None,<br>expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,<br>apply_routed_scaling_factor_on_output: Optional[bool] = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.compile(dynamic=True, backend=get_compiler_backend())
def grouped_topk_gpu(
    hidden_states: torch.Tensor,
    gating_output: torch.Tensor,
    topk: int,
    renormalize: bool,
    num_expert_group: Optional[int] = None,
    topk_group: Optional[int] = None,
    num_fused_shared_experts: int = 0,
    routed_scaling_factor: Optional[float] = None,
    num_token_non_padded: Optional[torch.Tensor] = None,
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,
    apply_routed_scaling_factor_on_output: Optional[bool] = False,
):
    assert hidden_states.shape[0] == gating_output.shape[0], &#34;Number of tokens mismatch&#34;

    scores = torch.softmax(gating_output, dim=-1)
    # NPU compiler limitation
    if _is_npu and scores.dtype == torch.bfloat16:
        scores = scores.to(torch.float16)
    num_token = scores.shape[0]
    num_experts = scores.shape[1]
    group_scores = (
        scores.view(num_token, num_expert_group, -1).max(dim=-1).values
    )  # [n, n_group]
    group_idx = torch.topk(group_scores, k=topk_group, dim=-1, sorted=False)[
        1
    ]  # [n, top_k_group]
    group_mask = torch.zeros_like(group_scores)  # [n, n_group]
    group_mask.scatter_(1, group_idx, 1)  # [n, n_group]
    score_mask = (
        group_mask.unsqueeze(-1)
        .expand(num_token, num_expert_group, scores.shape[-1] // num_expert_group)
        .reshape(num_token, -1)
    )  # [n, e]
    tmp_scores = scores.masked_fill(~score_mask.bool(), 0.0)  # [n, e]
    # TODO: NPU can&#39;t support directly evaluating a comparison for now
    topk_weights, topk_ids = torch.topk(
        tmp_scores,
        k=topk,
        dim=-1,
        sorted=(True if num_fused_shared_experts &gt; 0 else False),
    )
    if num_fused_shared_experts:
        topk_ids[:, -1] = torch.randint(
            low=num_experts,
            high=num_experts + num_fused_shared_experts,
            size=(topk_ids.size(0),),
            dtype=topk_ids.dtype,
            device=topk_ids.device,
        )
        topk_weights[:, -1] = topk_weights[:, :-1].sum(dim=-1) / routed_scaling_factor

    if renormalize:
        topk_weights_sum = (
            topk_weights.sum(dim=-1, keepdim=True)
            if num_fused_shared_experts == 0
            else topk_weights[:, :-1].sum(dim=-1, keepdim=True)
        )
        topk_weights = topk_weights / topk_weights_sum
        if apply_routed_scaling_factor_on_output:
            topk_weights *= routed_scaling_factor

    topk_weights, topk_ids = topk_weights.to(torch.float32), topk_ids.to(torch.int32)
    topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info)
    _mask_topk_ids_padded_region(topk_ids, num_token_non_padded)
    return topk_weights, topk_ids</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.grouped_topk_cpu"><code class="name flex">
<span>def <span class="ident">grouped_topk_cpu</span></span>(<span>hidden_states: torch.Tensor,<br>gating_output: torch.Tensor,<br>topk: int,<br>renormalize: bool,<br>num_expert_group: Optional[int] = None,<br>topk_group: Optional[int] = None,<br>num_fused_shared_experts: int = 0,<br>routed_scaling_factor: Optional[float] = None,<br>num_token_non_padded: Optional[torch.Tensor] = None,<br>expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,<br>apply_routed_scaling_factor_on_output: Optional[bool] = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def grouped_topk_cpu(
    hidden_states: torch.Tensor,
    gating_output: torch.Tensor,
    topk: int,
    renormalize: bool,
    num_expert_group: Optional[int] = None,
    topk_group: Optional[int] = None,
    num_fused_shared_experts: int = 0,
    routed_scaling_factor: Optional[float] = None,
    num_token_non_padded: Optional[torch.Tensor] = None,
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,
    apply_routed_scaling_factor_on_output: Optional[bool] = False,
):
    assert not apply_routed_scaling_factor_on_output
    assert expert_location_dispatch_info is None
    return torch.ops.sgl_kernel.grouped_topk_cpu(
        hidden_states,
        gating_output,
        topk,
        renormalize,
        num_expert_group,
        topk_group,
        num_fused_shared_experts,
        routed_scaling_factor,
        num_token_non_padded,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.grouped_topk_gpu"><code class="name flex">
<span>def <span class="ident">grouped_topk_gpu</span></span>(<span>hidden_states: torch.Tensor,<br>gating_output: torch.Tensor,<br>topk: int,<br>renormalize: bool,<br>num_expert_group: Optional[int] = None,<br>topk_group: Optional[int] = None,<br>num_fused_shared_experts: int = 0,<br>routed_scaling_factor: Optional[float] = None,<br>num_token_non_padded: Optional[torch.Tensor] = None,<br>expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,<br>apply_routed_scaling_factor_on_output: Optional[bool] = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.compile(dynamic=True, backend=get_compiler_backend())
def grouped_topk_gpu(
    hidden_states: torch.Tensor,
    gating_output: torch.Tensor,
    topk: int,
    renormalize: bool,
    num_expert_group: Optional[int] = None,
    topk_group: Optional[int] = None,
    num_fused_shared_experts: int = 0,
    routed_scaling_factor: Optional[float] = None,
    num_token_non_padded: Optional[torch.Tensor] = None,
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,
    apply_routed_scaling_factor_on_output: Optional[bool] = False,
):
    assert hidden_states.shape[0] == gating_output.shape[0], &#34;Number of tokens mismatch&#34;

    scores = torch.softmax(gating_output, dim=-1)
    # NPU compiler limitation
    if _is_npu and scores.dtype == torch.bfloat16:
        scores = scores.to(torch.float16)
    num_token = scores.shape[0]
    num_experts = scores.shape[1]
    group_scores = (
        scores.view(num_token, num_expert_group, -1).max(dim=-1).values
    )  # [n, n_group]
    group_idx = torch.topk(group_scores, k=topk_group, dim=-1, sorted=False)[
        1
    ]  # [n, top_k_group]
    group_mask = torch.zeros_like(group_scores)  # [n, n_group]
    group_mask.scatter_(1, group_idx, 1)  # [n, n_group]
    score_mask = (
        group_mask.unsqueeze(-1)
        .expand(num_token, num_expert_group, scores.shape[-1] // num_expert_group)
        .reshape(num_token, -1)
    )  # [n, e]
    tmp_scores = scores.masked_fill(~score_mask.bool(), 0.0)  # [n, e]
    # TODO: NPU can&#39;t support directly evaluating a comparison for now
    topk_weights, topk_ids = torch.topk(
        tmp_scores,
        k=topk,
        dim=-1,
        sorted=(True if num_fused_shared_experts &gt; 0 else False),
    )
    if num_fused_shared_experts:
        topk_ids[:, -1] = torch.randint(
            low=num_experts,
            high=num_experts + num_fused_shared_experts,
            size=(topk_ids.size(0),),
            dtype=topk_ids.dtype,
            device=topk_ids.device,
        )
        topk_weights[:, -1] = topk_weights[:, :-1].sum(dim=-1) / routed_scaling_factor

    if renormalize:
        topk_weights_sum = (
            topk_weights.sum(dim=-1, keepdim=True)
            if num_fused_shared_experts == 0
            else topk_weights[:, :-1].sum(dim=-1, keepdim=True)
        )
        topk_weights = topk_weights / topk_weights_sum
        if apply_routed_scaling_factor_on_output:
            topk_weights *= routed_scaling_factor

    topk_weights, topk_ids = topk_weights.to(torch.float32), topk_ids.to(torch.int32)
    topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info)
    _mask_topk_ids_padded_region(topk_ids, num_token_non_padded)
    return topk_weights, topk_ids</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.is_power_of_two"><code class="name flex">
<span>def <span class="ident">is_power_of_two</span></span>(<span>n)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_power_of_two(n):
    return n &gt; 0 and math.log2(n).is_integer()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.select_experts"><code class="name flex">
<span>def <span class="ident">select_experts</span></span>(<span>hidden_states: torch.Tensor,<br>router_logits: torch.Tensor,<br>topk_config: <a title="sglang.srt.layers.moe.topk.TopKConfig" href="#sglang.srt.layers.moe.topk.TopKConfig">TopKConfig</a>,<br>*,<br>num_token_non_padded: Optional[torch.Tensor] = None,<br>expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None) ‑> <a title="sglang.srt.layers.moe.topk.StandardTopKOutput" href="#sglang.srt.layers.moe.topk.StandardTopKOutput">StandardTopKOutput</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_experts(
    hidden_states: torch.Tensor,
    router_logits: torch.Tensor,
    topk_config: TopKConfig,
    *,
    num_token_non_padded: Optional[torch.Tensor] = None,
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,
) -&gt; StandardTopKOutput:

    top_k = topk_config.top_k
    use_grouped_topk = topk_config.use_grouped_topk
    topk_group = topk_config.topk_group
    num_expert_group = topk_config.num_expert_group
    renormalize = topk_config.renormalize
    num_fused_shared_experts = topk_config.num_fused_shared_experts
    custom_routing_function = topk_config.custom_routing_function
    correction_bias = topk_config.correction_bias
    torch_native = topk_config.torch_native
    routed_scaling_factor = topk_config.routed_scaling_factor
    apply_routed_scaling_factor_on_output = (
        topk_config.apply_routed_scaling_factor_on_output
    )

    router_logits, correction_bias = (
        expert_location_dispatch.transform_select_experts_inputs(
            router_logits=router_logits,
            correction_bias=correction_bias,
            info=expert_location_dispatch_info,
        )
    )

    # DeepSeek V2/V3/R1 series models use grouped_top_k
    if use_grouped_topk:
        assert topk_group is not None
        assert num_expert_group is not None
        if correction_bias is None:
            topk_weights, topk_ids = grouped_topk(
                hidden_states=hidden_states,
                gating_output=router_logits,
                topk=top_k,
                renormalize=renormalize,
                num_expert_group=num_expert_group,
                topk_group=topk_group,
                num_fused_shared_experts=num_fused_shared_experts,
                routed_scaling_factor=routed_scaling_factor,
                num_token_non_padded=num_token_non_padded,
                expert_location_dispatch_info=expert_location_dispatch_info,
                apply_routed_scaling_factor_on_output=apply_routed_scaling_factor_on_output,
            )
        else:
            topk_weights, topk_ids = biased_grouped_topk(
                hidden_states=hidden_states,
                gating_output=router_logits,
                correction_bias=correction_bias,
                topk=top_k,
                renormalize=renormalize,
                num_expert_group=num_expert_group,
                topk_group=topk_group,
                num_fused_shared_experts=num_fused_shared_experts,
                routed_scaling_factor=routed_scaling_factor,
                num_token_non_padded=num_token_non_padded,
                expert_location_dispatch_info=expert_location_dispatch_info,
                apply_routed_scaling_factor_on_output=apply_routed_scaling_factor_on_output,
            )
    elif torch_native and custom_routing_function is None:
        assert (
            num_token_non_padded is None
        ), &#34;num_token_non_padded is not yet supported in fused_topk_native&#34;
        assert expert_location_dispatch_info is None
        assert not apply_routed_scaling_factor_on_output, &#34;Not implemented&#34;
        topk_weights, topk_ids = fused_topk_native(
            hidden_states=hidden_states,
            gating_output=router_logits,
            topk=top_k,
            renormalize=renormalize,
            correction_bias=correction_bias,
        )
    elif custom_routing_function is None:
        assert not apply_routed_scaling_factor_on_output, &#34;Not implemented&#34;
        # Qwen3MOE uses fused_topk
        topk_weights, topk_ids = fused_topk(
            hidden_states=hidden_states,
            gating_output=router_logits,
            topk=top_k,
            renormalize=renormalize,
            num_token_non_padded=num_token_non_padded,
            expert_location_dispatch_info=expert_location_dispatch_info,
        )
    else:
        assert (
            num_token_non_padded is None
        ), &#34;num_token_non_padded is not yet supported in custom_routing_function&#34;
        assert expert_location_dispatch_info is None
        assert not apply_routed_scaling_factor_on_output, &#34;Not implemented&#34;
        topk_weights, topk_ids = custom_routing_function(
            hidden_states=hidden_states,
            gating_output=router_logits,
            topk=top_k,
            renormalize=renormalize,
        )

    get_global_expert_distribution_recorder().on_select_experts(topk_ids=topk_ids)

    return StandardTopKOutput(topk_weights, topk_ids, router_logits)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.layers.moe.topk.BypassedTopKOutput"><code class="flex name class">
<span>class <span class="ident">BypassedTopKOutput</span></span>
<span>(</span><span>hidden_states: ForwardRef('torch.Tensor'),<br>router_logits: ForwardRef('torch.Tensor'),<br>topk_config: ForwardRef('<a title="sglang.srt.layers.moe.topk.TopKConfig" href="#sglang.srt.layers.moe.topk.TopKConfig">TopKConfig</a>'),<br>num_token_non_padded: ForwardRef('Optional[torch.Tensor]') = None,<br>expert_location_dispatch_info: ForwardRef('Optional[ExpertLocationDispatchInfo]') = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BypassedTopKOutput(NamedTuple):
    &#34;&#34;&#34;Bypassed top-k output format.&#34;&#34;&#34;

    hidden_states: torch.Tensor
    router_logits: torch.Tensor
    topk_config: TopKConfig
    num_token_non_padded: Optional[torch.Tensor] = None
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None

    @property
    def format(self) -&gt; TopKOutputFormat:
        return TopKOutputFormat.BYPASSED</code></pre>
</details>
<div class="desc"><p>Bypassed top-k output format.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.layers.moe.topk.BypassedTopKOutput.expert_location_dispatch_info"><code class="name">var <span class="ident">expert_location_dispatch_info</span> : <a title="sglang.srt.eplb.expert_location_dispatch.ExpertLocationDispatchInfo" href="../../eplb/expert_location_dispatch.html#sglang.srt.eplb.expert_location_dispatch.ExpertLocationDispatchInfo">ExpertLocationDispatchInfo</a> | None</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BypassedTopKOutput(NamedTuple):
    &#34;&#34;&#34;Bypassed top-k output format.&#34;&#34;&#34;

    hidden_states: torch.Tensor
    router_logits: torch.Tensor
    topk_config: TopKConfig
    num_token_non_padded: Optional[torch.Tensor] = None
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None

    @property
    def format(self) -&gt; TopKOutputFormat:
        return TopKOutputFormat.BYPASSED</code></pre>
</details>
<div class="desc"><p>Alias for field number 4</p></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.BypassedTopKOutput.format"><code class="name">prop <span class="ident">format</span> : <a title="sglang.srt.layers.moe.topk.TopKOutputFormat" href="#sglang.srt.layers.moe.topk.TopKOutputFormat">TopKOutputFormat</a></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def format(self) -&gt; TopKOutputFormat:
    return TopKOutputFormat.BYPASSED</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.BypassedTopKOutput.hidden_states"><code class="name">var <span class="ident">hidden_states</span> : torch.Tensor</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BypassedTopKOutput(NamedTuple):
    &#34;&#34;&#34;Bypassed top-k output format.&#34;&#34;&#34;

    hidden_states: torch.Tensor
    router_logits: torch.Tensor
    topk_config: TopKConfig
    num_token_non_padded: Optional[torch.Tensor] = None
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None

    @property
    def format(self) -&gt; TopKOutputFormat:
        return TopKOutputFormat.BYPASSED</code></pre>
</details>
<div class="desc"><p>Alias for field number 0</p></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.BypassedTopKOutput.num_token_non_padded"><code class="name">var <span class="ident">num_token_non_padded</span> : torch.Tensor | None</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BypassedTopKOutput(NamedTuple):
    &#34;&#34;&#34;Bypassed top-k output format.&#34;&#34;&#34;

    hidden_states: torch.Tensor
    router_logits: torch.Tensor
    topk_config: TopKConfig
    num_token_non_padded: Optional[torch.Tensor] = None
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None

    @property
    def format(self) -&gt; TopKOutputFormat:
        return TopKOutputFormat.BYPASSED</code></pre>
</details>
<div class="desc"><p>Alias for field number 3</p></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.BypassedTopKOutput.router_logits"><code class="name">var <span class="ident">router_logits</span> : torch.Tensor</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BypassedTopKOutput(NamedTuple):
    &#34;&#34;&#34;Bypassed top-k output format.&#34;&#34;&#34;

    hidden_states: torch.Tensor
    router_logits: torch.Tensor
    topk_config: TopKConfig
    num_token_non_padded: Optional[torch.Tensor] = None
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None

    @property
    def format(self) -&gt; TopKOutputFormat:
        return TopKOutputFormat.BYPASSED</code></pre>
</details>
<div class="desc"><p>Alias for field number 1</p></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.BypassedTopKOutput.topk_config"><code class="name">var <span class="ident">topk_config</span> : <a title="sglang.srt.layers.moe.topk.TopKConfig" href="#sglang.srt.layers.moe.topk.TopKConfig">TopKConfig</a></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BypassedTopKOutput(NamedTuple):
    &#34;&#34;&#34;Bypassed top-k output format.&#34;&#34;&#34;

    hidden_states: torch.Tensor
    router_logits: torch.Tensor
    topk_config: TopKConfig
    num_token_non_padded: Optional[torch.Tensor] = None
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None

    @property
    def format(self) -&gt; TopKOutputFormat:
        return TopKOutputFormat.BYPASSED</code></pre>
</details>
<div class="desc"><p>Alias for field number 2</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.layers.moe.topk.StandardTopKOutput"><code class="flex name class">
<span>class <span class="ident">StandardTopKOutput</span></span>
<span>(</span><span>topk_weights: ForwardRef('torch.Tensor'),<br>topk_ids: ForwardRef('torch.Tensor'),<br>router_logits: ForwardRef('torch.Tensor'))</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StandardTopKOutput(NamedTuple):
    &#34;&#34;&#34;Standard top-k output format.&#34;&#34;&#34;

    topk_weights: torch.Tensor
    topk_ids: torch.Tensor
    router_logits: torch.Tensor

    @property
    def format(self) -&gt; TopKOutputFormat:
        return TopKOutputFormat.STANDARD</code></pre>
</details>
<div class="desc"><p>Standard top-k output format.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.layers.moe.topk.StandardTopKOutput.format"><code class="name">prop <span class="ident">format</span> : <a title="sglang.srt.layers.moe.topk.TopKOutputFormat" href="#sglang.srt.layers.moe.topk.TopKOutputFormat">TopKOutputFormat</a></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def format(self) -&gt; TopKOutputFormat:
    return TopKOutputFormat.STANDARD</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.StandardTopKOutput.router_logits"><code class="name">var <span class="ident">router_logits</span> : torch.Tensor</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StandardTopKOutput(NamedTuple):
    &#34;&#34;&#34;Standard top-k output format.&#34;&#34;&#34;

    topk_weights: torch.Tensor
    topk_ids: torch.Tensor
    router_logits: torch.Tensor

    @property
    def format(self) -&gt; TopKOutputFormat:
        return TopKOutputFormat.STANDARD</code></pre>
</details>
<div class="desc"><p>Alias for field number 2</p></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.StandardTopKOutput.topk_ids"><code class="name">var <span class="ident">topk_ids</span> : torch.Tensor</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StandardTopKOutput(NamedTuple):
    &#34;&#34;&#34;Standard top-k output format.&#34;&#34;&#34;

    topk_weights: torch.Tensor
    topk_ids: torch.Tensor
    router_logits: torch.Tensor

    @property
    def format(self) -&gt; TopKOutputFormat:
        return TopKOutputFormat.STANDARD</code></pre>
</details>
<div class="desc"><p>Alias for field number 1</p></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.StandardTopKOutput.topk_weights"><code class="name">var <span class="ident">topk_weights</span> : torch.Tensor</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StandardTopKOutput(NamedTuple):
    &#34;&#34;&#34;Standard top-k output format.&#34;&#34;&#34;

    topk_weights: torch.Tensor
    topk_ids: torch.Tensor
    router_logits: torch.Tensor

    @property
    def format(self) -&gt; TopKOutputFormat:
        return TopKOutputFormat.STANDARD</code></pre>
</details>
<div class="desc"><p>Alias for field number 0</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopK"><code class="flex name class">
<span>class <span class="ident">TopK</span></span>
<span>(</span><span>top_k: int,<br>*,<br>use_grouped_topk: bool = False,<br>topk_group: Optional[int] = None,<br>num_expert_group: Optional[int] = None,<br>renormalize: bool = True,<br>num_fused_shared_experts: int = 0,<br>custom_routing_function: Optional[Callable] = None,<br>scoring_func: str = 'softmax',<br>correction_bias: Optional[torch.Tensor] = None,<br>routed_scaling_factor: Optional[float] = None,<br>apply_routed_scaling_factor_on_output: Optional[bool] = False,<br>force_topk: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TopK(CustomOp):

    def __init__(
        self,
        top_k: int,
        *,
        use_grouped_topk: bool = False,
        topk_group: Optional[int] = None,
        num_expert_group: Optional[int] = None,
        renormalize: bool = True,
        num_fused_shared_experts: int = 0,
        custom_routing_function: Optional[Callable] = None,
        scoring_func: str = &#34;softmax&#34;,
        correction_bias: Optional[torch.Tensor] = None,
        routed_scaling_factor: Optional[float] = None,
        apply_routed_scaling_factor_on_output: Optional[bool] = False,
        force_topk: bool = False,
    ):
        # NOTE: scoring_func is not used for now, but we keep it for future use
        # see https://github.com/sgl-project/sglang/pull/4505 for more details
        super().__init__()

        if use_grouped_topk:
            assert num_expert_group is not None and topk_group is not None

        self.topk_config = TopKConfig(
            top_k=top_k,
            use_grouped_topk=use_grouped_topk,
            renormalize=renormalize,
            topk_group=topk_group,
            num_expert_group=num_expert_group,
            num_fused_shared_experts=num_fused_shared_experts,
            custom_routing_function=custom_routing_function,
            correction_bias=correction_bias,
            routed_scaling_factor=routed_scaling_factor,
            apply_routed_scaling_factor_on_output=apply_routed_scaling_factor_on_output,
        )

        self.use_triton_kernels = get_moe_runner_backend().is_triton_kernel()
        self.force_topk = force_topk

    def forward_native(
        self,
        hidden_states: torch.Tensor,
        router_logits: torch.Tensor,
        *,
        num_token_non_padded: Optional[torch.Tensor] = None,
        expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,
    ) -&gt; TopKOutput:
        self.topk_config.torch_native = True
        return select_experts(
            hidden_states=hidden_states,
            router_logits=router_logits,
            topk_config=self.topk_config,
            num_token_non_padded=num_token_non_padded,
            expert_location_dispatch_info=expert_location_dispatch_info,
        )

    def forward_cuda(
        self,
        hidden_states: torch.Tensor,
        router_logits: torch.Tensor,
        *,
        num_token_non_padded: Optional[torch.Tensor] = None,
        expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,
    ) -&gt; TopKOutput:
        if self.use_triton_kernels:
            # renormalize=True is equivalent to sm_first=False
            routing_data, gather_idx, scatter_idx = routing(
                router_logits,
                self.topk_config.top_k,
                sm_first=not self.topk_config.renormalize,
            )
            return TritonKernelTopKOutput(routing_data, gather_idx, scatter_idx)
        elif not self.force_topk and (
            should_use_flashinfer_trtllm_moe()
            or get_moe_runner_backend().is_flashinfer_mxfp4()
        ):
            return BypassedTopKOutput(
                hidden_states=hidden_states,
                router_logits=router_logits,
                topk_config=self.topk_config,
                num_token_non_padded=num_token_non_padded,
                expert_location_dispatch_info=expert_location_dispatch_info,
            )
        else:
            self.topk_config.torch_native = False
            return select_experts(
                hidden_states=hidden_states,
                router_logits=router_logits,
                topk_config=self.topk_config,
                num_token_non_padded=num_token_non_padded,
                expert_location_dispatch_info=expert_location_dispatch_info,
            )

    def forward_cpu(
        self,
        hidden_states: torch.Tensor,
        router_logits: torch.Tensor,
        *,
        num_token_non_padded: Optional[torch.Tensor] = None,
        expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,
    ) -&gt; TopKOutput:
        return select_experts(
            hidden_states=hidden_states,
            router_logits=router_logits,
            topk_config=self.topk_config,
            num_token_non_padded=num_token_non_padded,
            expert_location_dispatch_info=expert_location_dispatch_info,
        )

    def forward_npu(
        self,
        hidden_states: torch.Tensor,
        router_logits: torch.Tensor,
        *,
        num_token_non_padded: Optional[torch.Tensor] = None,
        expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,
    ) -&gt; TopKOutput:
        global_num_experts = router_logits.shape[-1]

        # NOTE: now npu_moe_gating_top_k can only support `group_count=256` pattern
        if global_num_experts == 256:

            routed_scaling_factor = self.topk_config.routed_scaling_factor or 1
            router_logits = router_logits.to(torch.float32)

            topk_weights, topk_ids, _ = torch_npu.npu_moe_gating_top_k(
                router_logits,
                k=self.topk_config.top_k,
                bias=self.topk_config.correction_bias.to(torch.float32),
                k_group=self.topk_config.topk_group,
                group_count=self.topk_config.num_expert_group,
                group_select_mode=1,
                renorm=0,
                norm_type=1,
                routed_scaling_factor=routed_scaling_factor,
                eps=float(1e-20),
            )

            if self.topk_config.renormalize:
                topk_weights_sum = (
                    topk_weights.sum(dim=-1, keepdim=True)
                    if self.topk_config.num_fused_shared_experts == 0
                    else topk_weights[:, :-1].sum(dim=-1, keepdim=True)
                )
                topk_weights = topk_weights / topk_weights_sum

            return StandardTopKOutput(topk_weights, topk_ids, _)
        else:
            self.topk_config.torch_native = True
            return select_experts(
                hidden_states=hidden_states,
                router_logits=router_logits,
                topk_config=self.topk_config,
                num_token_non_padded=num_token_non_padded,
                expert_location_dispatch_info=expert_location_dispatch_info,
            )

    def empty_topk_output(self, device: torch.device) -&gt; TopKOutput:
        topk = self.topk_config.top_k - self.topk_config.num_fused_shared_experts
        topk_weights = torch.empty((0, topk), dtype=torch.float32, device=device)
        topk_idx = torch.full((0, topk), -1, dtype=torch.int32, device=device)
        router_logits = torch.empty((0, topk), dtype=torch.float32, device=device)
        return StandardTopKOutput(topk_weights, topk_idx, router_logits)</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing them to be nested in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F


class Model(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will also have their
parameters converted when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.custom_op.CustomOp" href="../../custom_op.html#sglang.srt.custom_op.CustomOp">CustomOp</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.moe.topk.TopK.empty_topk_output"><code class="name flex">
<span>def <span class="ident">empty_topk_output</span></span>(<span>self, device: torch.device) ‑> <a title="sglang.srt.layers.moe.topk.TopKOutput" href="#sglang.srt.layers.moe.topk.TopKOutput">TopKOutput</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def empty_topk_output(self, device: torch.device) -&gt; TopKOutput:
    topk = self.topk_config.top_k - self.topk_config.num_fused_shared_experts
    topk_weights = torch.empty((0, topk), dtype=torch.float32, device=device)
    topk_idx = torch.full((0, topk), -1, dtype=torch.int32, device=device)
    router_logits = torch.empty((0, topk), dtype=torch.float32, device=device)
    return StandardTopKOutput(topk_weights, topk_idx, router_logits)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopK.forward_cpu"><code class="name flex">
<span>def <span class="ident">forward_cpu</span></span>(<span>self,<br>hidden_states: torch.Tensor,<br>router_logits: torch.Tensor,<br>*,<br>num_token_non_padded: Optional[torch.Tensor] = None,<br>expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None) ‑> <a title="sglang.srt.layers.moe.topk.TopKOutput" href="#sglang.srt.layers.moe.topk.TopKOutput">TopKOutput</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_cpu(
    self,
    hidden_states: torch.Tensor,
    router_logits: torch.Tensor,
    *,
    num_token_non_padded: Optional[torch.Tensor] = None,
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,
) -&gt; TopKOutput:
    return select_experts(
        hidden_states=hidden_states,
        router_logits=router_logits,
        topk_config=self.topk_config,
        num_token_non_padded=num_token_non_padded,
        expert_location_dispatch_info=expert_location_dispatch_info,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopK.forward_cuda"><code class="name flex">
<span>def <span class="ident">forward_cuda</span></span>(<span>self,<br>hidden_states: torch.Tensor,<br>router_logits: torch.Tensor,<br>*,<br>num_token_non_padded: Optional[torch.Tensor] = None,<br>expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None) ‑> <a title="sglang.srt.layers.moe.topk.TopKOutput" href="#sglang.srt.layers.moe.topk.TopKOutput">TopKOutput</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_cuda(
    self,
    hidden_states: torch.Tensor,
    router_logits: torch.Tensor,
    *,
    num_token_non_padded: Optional[torch.Tensor] = None,
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,
) -&gt; TopKOutput:
    if self.use_triton_kernels:
        # renormalize=True is equivalent to sm_first=False
        routing_data, gather_idx, scatter_idx = routing(
            router_logits,
            self.topk_config.top_k,
            sm_first=not self.topk_config.renormalize,
        )
        return TritonKernelTopKOutput(routing_data, gather_idx, scatter_idx)
    elif not self.force_topk and (
        should_use_flashinfer_trtllm_moe()
        or get_moe_runner_backend().is_flashinfer_mxfp4()
    ):
        return BypassedTopKOutput(
            hidden_states=hidden_states,
            router_logits=router_logits,
            topk_config=self.topk_config,
            num_token_non_padded=num_token_non_padded,
            expert_location_dispatch_info=expert_location_dispatch_info,
        )
    else:
        self.topk_config.torch_native = False
        return select_experts(
            hidden_states=hidden_states,
            router_logits=router_logits,
            topk_config=self.topk_config,
            num_token_non_padded=num_token_non_padded,
            expert_location_dispatch_info=expert_location_dispatch_info,
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopK.forward_native"><code class="name flex">
<span>def <span class="ident">forward_native</span></span>(<span>self,<br>hidden_states: torch.Tensor,<br>router_logits: torch.Tensor,<br>*,<br>num_token_non_padded: Optional[torch.Tensor] = None,<br>expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None) ‑> <a title="sglang.srt.layers.moe.topk.TopKOutput" href="#sglang.srt.layers.moe.topk.TopKOutput">TopKOutput</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_native(
    self,
    hidden_states: torch.Tensor,
    router_logits: torch.Tensor,
    *,
    num_token_non_padded: Optional[torch.Tensor] = None,
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,
) -&gt; TopKOutput:
    self.topk_config.torch_native = True
    return select_experts(
        hidden_states=hidden_states,
        router_logits=router_logits,
        topk_config=self.topk_config,
        num_token_non_padded=num_token_non_padded,
        expert_location_dispatch_info=expert_location_dispatch_info,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopK.forward_npu"><code class="name flex">
<span>def <span class="ident">forward_npu</span></span>(<span>self,<br>hidden_states: torch.Tensor,<br>router_logits: torch.Tensor,<br>*,<br>num_token_non_padded: Optional[torch.Tensor] = None,<br>expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None) ‑> <a title="sglang.srt.layers.moe.topk.TopKOutput" href="#sglang.srt.layers.moe.topk.TopKOutput">TopKOutput</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_npu(
    self,
    hidden_states: torch.Tensor,
    router_logits: torch.Tensor,
    *,
    num_token_non_padded: Optional[torch.Tensor] = None,
    expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo] = None,
) -&gt; TopKOutput:
    global_num_experts = router_logits.shape[-1]

    # NOTE: now npu_moe_gating_top_k can only support `group_count=256` pattern
    if global_num_experts == 256:

        routed_scaling_factor = self.topk_config.routed_scaling_factor or 1
        router_logits = router_logits.to(torch.float32)

        topk_weights, topk_ids, _ = torch_npu.npu_moe_gating_top_k(
            router_logits,
            k=self.topk_config.top_k,
            bias=self.topk_config.correction_bias.to(torch.float32),
            k_group=self.topk_config.topk_group,
            group_count=self.topk_config.num_expert_group,
            group_select_mode=1,
            renorm=0,
            norm_type=1,
            routed_scaling_factor=routed_scaling_factor,
            eps=float(1e-20),
        )

        if self.topk_config.renormalize:
            topk_weights_sum = (
                topk_weights.sum(dim=-1, keepdim=True)
                if self.topk_config.num_fused_shared_experts == 0
                else topk_weights[:, :-1].sum(dim=-1, keepdim=True)
            )
            topk_weights = topk_weights / topk_weights_sum

        return StandardTopKOutput(topk_weights, topk_ids, _)
    else:
        self.topk_config.torch_native = True
        return select_experts(
            hidden_states=hidden_states,
            router_logits=router_logits,
            topk_config=self.topk_config,
            num_token_non_padded=num_token_non_padded,
            expert_location_dispatch_info=expert_location_dispatch_info,
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.custom_op.CustomOp" href="../../custom_op.html#sglang.srt.custom_op.CustomOp">CustomOp</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.custom_op.CustomOp.forward" href="../../custom_op.html#sglang.srt.custom_op.CustomOp.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKConfig"><code class="flex name class">
<span>class <span class="ident">TopKConfig</span></span>
<span>(</span><span>top_k: int,<br>use_grouped_topk: bool = False,<br>topk_group: Optional[int] = None,<br>num_expert_group: Optional[int] = None,<br>renormalize: bool = True,<br>num_fused_shared_experts: int = 0,<br>custom_routing_function: Optional[Callable] = None,<br>correction_bias: Optional[torch.Tensor] = None,<br>torch_native: bool = False,<br>routed_scaling_factor: Optional[float] = None,<br>apply_routed_scaling_factor_on_output: bool = False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class TopKConfig:
    top_k: int
    use_grouped_topk: bool = False
    topk_group: Optional[int] = None
    num_expert_group: Optional[int] = None
    renormalize: bool = True
    num_fused_shared_experts: int = 0
    custom_routing_function: Optional[Callable] = None
    correction_bias: Optional[torch.Tensor] = None
    torch_native: bool = False
    routed_scaling_factor: Optional[float] = None
    apply_routed_scaling_factor_on_output: bool = False</code></pre>
</details>
<div class="desc"><p>TopKConfig(top_k: 'int', use_grouped_topk: 'bool' = False, topk_group: 'Optional[int]' = None, num_expert_group: 'Optional[int]' = None, renormalize: 'bool' = True, num_fused_shared_experts: 'int' = 0, custom_routing_function: 'Optional[Callable]' = None, correction_bias: 'Optional[torch.Tensor]' = None, torch_native: 'bool' = False, routed_scaling_factor: 'Optional[float]' = None, apply_routed_scaling_factor_on_output: 'bool' = False)</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.layers.moe.topk.TopKConfig.apply_routed_scaling_factor_on_output"><code class="name">var <span class="ident">apply_routed_scaling_factor_on_output</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKConfig.correction_bias"><code class="name">var <span class="ident">correction_bias</span> : torch.Tensor | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKConfig.custom_routing_function"><code class="name">var <span class="ident">custom_routing_function</span> : Callable | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKConfig.num_expert_group"><code class="name">var <span class="ident">num_expert_group</span> : int | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKConfig.num_fused_shared_experts"><code class="name">var <span class="ident">num_fused_shared_experts</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKConfig.renormalize"><code class="name">var <span class="ident">renormalize</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKConfig.routed_scaling_factor"><code class="name">var <span class="ident">routed_scaling_factor</span> : float | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKConfig.top_k"><code class="name">var <span class="ident">top_k</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKConfig.topk_group"><code class="name">var <span class="ident">topk_group</span> : int | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKConfig.torch_native"><code class="name">var <span class="ident">torch_native</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKConfig.use_grouped_topk"><code class="name">var <span class="ident">use_grouped_topk</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKOutput"><code class="flex name class">
<span>class <span class="ident">TopKOutput</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@runtime_checkable
class TopKOutput(Protocol):
    &#34;&#34;&#34;Protocol for top-k outputs in different formats.&#34;&#34;&#34;

    @property
    def format(self) -&gt; TopKOutputFormat:
        &#34;&#34;&#34;The format of the output.&#34;&#34;&#34;
        ...</code></pre>
</details>
<div class="desc"><p>Protocol for top-k outputs in different formats.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>typing.Protocol</li>
<li>typing.Generic</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.layers.moe.topk.TopKOutput.format"><code class="name">prop <span class="ident">format</span> : <a title="sglang.srt.layers.moe.topk.TopKOutputFormat" href="#sglang.srt.layers.moe.topk.TopKOutputFormat">TopKOutputFormat</a></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def format(self) -&gt; TopKOutputFormat:
    &#34;&#34;&#34;The format of the output.&#34;&#34;&#34;
    ...</code></pre>
</details>
<div class="desc"><p>The format of the output.</p></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKOutputChecker"><code class="flex name class">
<span>class <span class="ident">TopKOutputChecker</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TopKOutputChecker:

    @staticmethod
    def format_is_standard(topk_output: TopKOutput) -&gt; TypeGuard[StandardTopKOutput]:
        return topk_output.format.is_standard()

    @staticmethod
    def format_is_triton_kernel(
        topk_output: TopKOutput,
    ) -&gt; TypeGuard[TritonKernelTopKOutput]:
        return topk_output.format.is_triton_kernel()

    @staticmethod
    def format_is_bypassed(topk_output: TopKOutput) -&gt; TypeGuard[BypassedTopKOutput]:
        return topk_output.format.is_bypassed()</code></pre>
</details>
<div class="desc"></div>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.layers.moe.topk.TopKOutputChecker.format_is_bypassed"><code class="name flex">
<span>def <span class="ident">format_is_bypassed</span></span>(<span>topk_output: <a title="sglang.srt.layers.moe.topk.TopKOutput" href="#sglang.srt.layers.moe.topk.TopKOutput">TopKOutput</a>) ‑> TypeGuard[<a title="sglang.srt.layers.moe.topk.BypassedTopKOutput" href="#sglang.srt.layers.moe.topk.BypassedTopKOutput">BypassedTopKOutput</a>]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def format_is_bypassed(topk_output: TopKOutput) -&gt; TypeGuard[BypassedTopKOutput]:
    return topk_output.format.is_bypassed()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKOutputChecker.format_is_standard"><code class="name flex">
<span>def <span class="ident">format_is_standard</span></span>(<span>topk_output: <a title="sglang.srt.layers.moe.topk.TopKOutput" href="#sglang.srt.layers.moe.topk.TopKOutput">TopKOutput</a>) ‑> TypeGuard[<a title="sglang.srt.layers.moe.topk.StandardTopKOutput" href="#sglang.srt.layers.moe.topk.StandardTopKOutput">StandardTopKOutput</a>]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def format_is_standard(topk_output: TopKOutput) -&gt; TypeGuard[StandardTopKOutput]:
    return topk_output.format.is_standard()</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKOutputChecker.format_is_triton_kernel"><code class="name flex">
<span>def <span class="ident">format_is_triton_kernel</span></span>(<span>topk_output: <a title="sglang.srt.layers.moe.topk.TopKOutput" href="#sglang.srt.layers.moe.topk.TopKOutput">TopKOutput</a>) ‑> TypeGuard[<a title="sglang.srt.layers.moe.topk.TritonKernelTopKOutput" href="#sglang.srt.layers.moe.topk.TritonKernelTopKOutput">TritonKernelTopKOutput</a>]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def format_is_triton_kernel(
    topk_output: TopKOutput,
) -&gt; TypeGuard[TritonKernelTopKOutput]:
    return topk_output.format.is_triton_kernel()</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKOutputFormat"><code class="flex name class">
<span>class <span class="ident">TopKOutputFormat</span></span>
<span>(</span><span>*args, **kwds)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TopKOutputFormat(Enum):
    STANDARD = auto()
    TRITON_KERNEL = auto()
    BYPASSED = auto()

    def is_standard(self) -&gt; bool:
        return self == TopKOutputFormat.STANDARD

    def is_triton_kernel(self) -&gt; bool:
        return self == TopKOutputFormat.TRITON_KERNEL

    def is_bypassed(self) -&gt; bool:
        return self == TopKOutputFormat.BYPASSED</code></pre>
</details>
<div class="desc"><p>Create a collection of name/value pairs.</p>
<p>Example enumeration:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; class Color(Enum):
...     RED = 1
...     BLUE = 2
...     GREEN = 3
</code></pre>
<p>Access them by:</p>
<ul>
<li>attribute access:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color.RED
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<ul>
<li>value lookup:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color(1)
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<ul>
<li>name lookup:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color['RED']
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<p>Enumerations can be iterated over, and know how many members they have:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; len(Color)
3
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; list(Color)
[&lt;Color.RED: 1&gt;, &lt;Color.BLUE: 2&gt;, &lt;Color.GREEN: 3&gt;]
</code></pre>
<p>Methods can be added to enumerations, and members can have their own
attributes &ndash; see the documentation for details.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.layers.moe.topk.TopKOutputFormat.BYPASSED"><code class="name">var <span class="ident">BYPASSED</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKOutputFormat.STANDARD"><code class="name">var <span class="ident">STANDARD</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKOutputFormat.TRITON_KERNEL"><code class="name">var <span class="ident">TRITON_KERNEL</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.moe.topk.TopKOutputFormat.is_bypassed"><code class="name flex">
<span>def <span class="ident">is_bypassed</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_bypassed(self) -&gt; bool:
    return self == TopKOutputFormat.BYPASSED</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKOutputFormat.is_standard"><code class="name flex">
<span>def <span class="ident">is_standard</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_standard(self) -&gt; bool:
    return self == TopKOutputFormat.STANDARD</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TopKOutputFormat.is_triton_kernel"><code class="name flex">
<span>def <span class="ident">is_triton_kernel</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_triton_kernel(self) -&gt; bool:
    return self == TopKOutputFormat.TRITON_KERNEL</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.layers.moe.topk.TritonKernelTopKOutput"><code class="flex name class">
<span>class <span class="ident">TritonKernelTopKOutput</span></span>
<span>(</span><span>routing_data: ForwardRef('RoutingData'),<br>gather_indx: ForwardRef('GatherIndx'),<br>scatter_indx: ForwardRef('ScatterIndx'))</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TritonKernelTopKOutput(NamedTuple):
    &#34;&#34;&#34;Triton kernel top-k output format.&#34;&#34;&#34;

    routing_data: RoutingData
    gather_indx: GatherIndx
    scatter_indx: ScatterIndx

    @property
    def format(self) -&gt; TopKOutputFormat:
        return TopKOutputFormat.TRITON_KERNEL</code></pre>
</details>
<div class="desc"><p>Triton kernel top-k output format.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.layers.moe.topk.TritonKernelTopKOutput.format"><code class="name">prop <span class="ident">format</span> : <a title="sglang.srt.layers.moe.topk.TopKOutputFormat" href="#sglang.srt.layers.moe.topk.TopKOutputFormat">TopKOutputFormat</a></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def format(self) -&gt; TopKOutputFormat:
    return TopKOutputFormat.TRITON_KERNEL</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TritonKernelTopKOutput.gather_indx"><code class="name">var <span class="ident">gather_indx</span> : triton_kernels.routing.GatherIndx</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TritonKernelTopKOutput(NamedTuple):
    &#34;&#34;&#34;Triton kernel top-k output format.&#34;&#34;&#34;

    routing_data: RoutingData
    gather_indx: GatherIndx
    scatter_indx: ScatterIndx

    @property
    def format(self) -&gt; TopKOutputFormat:
        return TopKOutputFormat.TRITON_KERNEL</code></pre>
</details>
<div class="desc"><p>Alias for field number 1</p></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TritonKernelTopKOutput.routing_data"><code class="name">var <span class="ident">routing_data</span> : triton_kernels.routing.RoutingData</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TritonKernelTopKOutput(NamedTuple):
    &#34;&#34;&#34;Triton kernel top-k output format.&#34;&#34;&#34;

    routing_data: RoutingData
    gather_indx: GatherIndx
    scatter_indx: ScatterIndx

    @property
    def format(self) -&gt; TopKOutputFormat:
        return TopKOutputFormat.TRITON_KERNEL</code></pre>
</details>
<div class="desc"><p>Alias for field number 0</p></div>
</dd>
<dt id="sglang.srt.layers.moe.topk.TritonKernelTopKOutput.scatter_indx"><code class="name">var <span class="ident">scatter_indx</span> : triton_kernels.routing.ScatterIndx</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TritonKernelTopKOutput(NamedTuple):
    &#34;&#34;&#34;Triton kernel top-k output format.&#34;&#34;&#34;

    routing_data: RoutingData
    gather_indx: GatherIndx
    scatter_indx: ScatterIndx

    @property
    def format(self) -&gt; TopKOutputFormat:
        return TopKOutputFormat.TRITON_KERNEL</code></pre>
</details>
<div class="desc"><p>Alias for field number 2</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.layers.moe" href="index.html">sglang.srt.layers.moe</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.layers.moe.topk.apply_topk_weights_cpu" href="#sglang.srt.layers.moe.topk.apply_topk_weights_cpu">apply_topk_weights_cpu</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.biased_grouped_topk" href="#sglang.srt.layers.moe.topk.biased_grouped_topk">biased_grouped_topk</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.biased_grouped_topk_cpu" href="#sglang.srt.layers.moe.topk.biased_grouped_topk_cpu">biased_grouped_topk_cpu</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.biased_grouped_topk_gpu" href="#sglang.srt.layers.moe.topk.biased_grouped_topk_gpu">biased_grouped_topk_gpu</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.biased_grouped_topk_impl" href="#sglang.srt.layers.moe.topk.biased_grouped_topk_impl">biased_grouped_topk_impl</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.fused_topk" href="#sglang.srt.layers.moe.topk.fused_topk">fused_topk</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.fused_topk_cpu" href="#sglang.srt.layers.moe.topk.fused_topk_cpu">fused_topk_cpu</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.fused_topk_native" href="#sglang.srt.layers.moe.topk.fused_topk_native">fused_topk_native</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.fused_topk_torch_native" href="#sglang.srt.layers.moe.topk.fused_topk_torch_native">fused_topk_torch_native</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.grouped_topk" href="#sglang.srt.layers.moe.topk.grouped_topk">grouped_topk</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.grouped_topk_cpu" href="#sglang.srt.layers.moe.topk.grouped_topk_cpu">grouped_topk_cpu</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.grouped_topk_gpu" href="#sglang.srt.layers.moe.topk.grouped_topk_gpu">grouped_topk_gpu</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.is_power_of_two" href="#sglang.srt.layers.moe.topk.is_power_of_two">is_power_of_two</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.select_experts" href="#sglang.srt.layers.moe.topk.select_experts">select_experts</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.layers.moe.topk.BypassedTopKOutput" href="#sglang.srt.layers.moe.topk.BypassedTopKOutput">BypassedTopKOutput</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.moe.topk.BypassedTopKOutput.expert_location_dispatch_info" href="#sglang.srt.layers.moe.topk.BypassedTopKOutput.expert_location_dispatch_info">expert_location_dispatch_info</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.BypassedTopKOutput.format" href="#sglang.srt.layers.moe.topk.BypassedTopKOutput.format">format</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.BypassedTopKOutput.hidden_states" href="#sglang.srt.layers.moe.topk.BypassedTopKOutput.hidden_states">hidden_states</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.BypassedTopKOutput.num_token_non_padded" href="#sglang.srt.layers.moe.topk.BypassedTopKOutput.num_token_non_padded">num_token_non_padded</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.BypassedTopKOutput.router_logits" href="#sglang.srt.layers.moe.topk.BypassedTopKOutput.router_logits">router_logits</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.BypassedTopKOutput.topk_config" href="#sglang.srt.layers.moe.topk.BypassedTopKOutput.topk_config">topk_config</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.moe.topk.StandardTopKOutput" href="#sglang.srt.layers.moe.topk.StandardTopKOutput">StandardTopKOutput</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.moe.topk.StandardTopKOutput.format" href="#sglang.srt.layers.moe.topk.StandardTopKOutput.format">format</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.StandardTopKOutput.router_logits" href="#sglang.srt.layers.moe.topk.StandardTopKOutput.router_logits">router_logits</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.StandardTopKOutput.topk_ids" href="#sglang.srt.layers.moe.topk.StandardTopKOutput.topk_ids">topk_ids</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.StandardTopKOutput.topk_weights" href="#sglang.srt.layers.moe.topk.StandardTopKOutput.topk_weights">topk_weights</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.moe.topk.TopK" href="#sglang.srt.layers.moe.topk.TopK">TopK</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.moe.topk.TopK.empty_topk_output" href="#sglang.srt.layers.moe.topk.TopK.empty_topk_output">empty_topk_output</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopK.forward_cpu" href="#sglang.srt.layers.moe.topk.TopK.forward_cpu">forward_cpu</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopK.forward_cuda" href="#sglang.srt.layers.moe.topk.TopK.forward_cuda">forward_cuda</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopK.forward_native" href="#sglang.srt.layers.moe.topk.TopK.forward_native">forward_native</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopK.forward_npu" href="#sglang.srt.layers.moe.topk.TopK.forward_npu">forward_npu</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.moe.topk.TopKConfig" href="#sglang.srt.layers.moe.topk.TopKConfig">TopKConfig</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.moe.topk.TopKConfig.apply_routed_scaling_factor_on_output" href="#sglang.srt.layers.moe.topk.TopKConfig.apply_routed_scaling_factor_on_output">apply_routed_scaling_factor_on_output</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopKConfig.correction_bias" href="#sglang.srt.layers.moe.topk.TopKConfig.correction_bias">correction_bias</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopKConfig.custom_routing_function" href="#sglang.srt.layers.moe.topk.TopKConfig.custom_routing_function">custom_routing_function</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopKConfig.num_expert_group" href="#sglang.srt.layers.moe.topk.TopKConfig.num_expert_group">num_expert_group</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopKConfig.num_fused_shared_experts" href="#sglang.srt.layers.moe.topk.TopKConfig.num_fused_shared_experts">num_fused_shared_experts</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopKConfig.renormalize" href="#sglang.srt.layers.moe.topk.TopKConfig.renormalize">renormalize</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopKConfig.routed_scaling_factor" href="#sglang.srt.layers.moe.topk.TopKConfig.routed_scaling_factor">routed_scaling_factor</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopKConfig.top_k" href="#sglang.srt.layers.moe.topk.TopKConfig.top_k">top_k</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopKConfig.topk_group" href="#sglang.srt.layers.moe.topk.TopKConfig.topk_group">topk_group</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopKConfig.torch_native" href="#sglang.srt.layers.moe.topk.TopKConfig.torch_native">torch_native</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopKConfig.use_grouped_topk" href="#sglang.srt.layers.moe.topk.TopKConfig.use_grouped_topk">use_grouped_topk</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.moe.topk.TopKOutput" href="#sglang.srt.layers.moe.topk.TopKOutput">TopKOutput</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.moe.topk.TopKOutput.format" href="#sglang.srt.layers.moe.topk.TopKOutput.format">format</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.moe.topk.TopKOutputChecker" href="#sglang.srt.layers.moe.topk.TopKOutputChecker">TopKOutputChecker</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.moe.topk.TopKOutputChecker.format_is_bypassed" href="#sglang.srt.layers.moe.topk.TopKOutputChecker.format_is_bypassed">format_is_bypassed</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopKOutputChecker.format_is_standard" href="#sglang.srt.layers.moe.topk.TopKOutputChecker.format_is_standard">format_is_standard</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopKOutputChecker.format_is_triton_kernel" href="#sglang.srt.layers.moe.topk.TopKOutputChecker.format_is_triton_kernel">format_is_triton_kernel</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.moe.topk.TopKOutputFormat" href="#sglang.srt.layers.moe.topk.TopKOutputFormat">TopKOutputFormat</a></code></h4>
<ul class="two-column">
<li><code><a title="sglang.srt.layers.moe.topk.TopKOutputFormat.BYPASSED" href="#sglang.srt.layers.moe.topk.TopKOutputFormat.BYPASSED">BYPASSED</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopKOutputFormat.STANDARD" href="#sglang.srt.layers.moe.topk.TopKOutputFormat.STANDARD">STANDARD</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopKOutputFormat.TRITON_KERNEL" href="#sglang.srt.layers.moe.topk.TopKOutputFormat.TRITON_KERNEL">TRITON_KERNEL</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopKOutputFormat.is_bypassed" href="#sglang.srt.layers.moe.topk.TopKOutputFormat.is_bypassed">is_bypassed</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopKOutputFormat.is_standard" href="#sglang.srt.layers.moe.topk.TopKOutputFormat.is_standard">is_standard</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TopKOutputFormat.is_triton_kernel" href="#sglang.srt.layers.moe.topk.TopKOutputFormat.is_triton_kernel">is_triton_kernel</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.moe.topk.TritonKernelTopKOutput" href="#sglang.srt.layers.moe.topk.TritonKernelTopKOutput">TritonKernelTopKOutput</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.moe.topk.TritonKernelTopKOutput.format" href="#sglang.srt.layers.moe.topk.TritonKernelTopKOutput.format">format</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TritonKernelTopKOutput.gather_indx" href="#sglang.srt.layers.moe.topk.TritonKernelTopKOutput.gather_indx">gather_indx</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TritonKernelTopKOutput.routing_data" href="#sglang.srt.layers.moe.topk.TritonKernelTopKOutput.routing_data">routing_data</a></code></li>
<li><code><a title="sglang.srt.layers.moe.topk.TritonKernelTopKOutput.scatter_indx" href="#sglang.srt.layers.moe.topk.TritonKernelTopKOutput.scatter_indx">scatter_indx</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
