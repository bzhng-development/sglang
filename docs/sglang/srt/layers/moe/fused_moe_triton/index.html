<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.layers.moe.fused_moe_triton API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.layers.moe.fused_moe_triton</code></h1>
</header>
<section id="section-intro">
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="sglang.srt.layers.moe.fused_moe_triton.fused_moe" href="fused_moe.html">sglang.srt.layers.moe.fused_moe_triton.fused_moe</a></code></dt>
<dd>
<div class="desc"><p>Fused MoE kernel.</p></div>
</dd>
<dt><code class="name"><a title="sglang.srt.layers.moe.fused_moe_triton.layer" href="layer.html">sglang.srt.layers.moe.fused_moe_triton.layer</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="sglang.srt.layers.moe.fused_moe_triton.triton_kernels_moe" href="triton_kernels_moe.html">sglang.srt.layers.moe.fused_moe_triton.triton_kernels_moe</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.layers.moe.fused_moe_triton.fused_experts"><code class="name flex">
<span>def <span class="ident">fused_experts</span></span>(<span>hidden_states: torch.Tensor,<br>w1: torch.Tensor,<br>w2: torch.Tensor,<br>topk_output: StandardTopKOutput,<br>moe_runner_config: MoeRunnerConfig,<br>b1: Optional[torch.Tensor] = None,<br>b2: Optional[torch.Tensor] = None,<br>use_fp8_w8a8: bool = False,<br>use_int8_w8a8: bool = False,<br>use_int8_w8a16: bool = False,<br>use_int4_w4a16: bool = False,<br>per_channel_quant: bool = False,<br>w1_scale: Optional[torch.Tensor] = None,<br>w2_scale: Optional[torch.Tensor] = None,<br>w1_zp: Optional[torch.Tensor] = None,<br>w2_zp: Optional[torch.Tensor] = None,<br>a1_scale: Optional[torch.Tensor] = None,<br>a2_scale: Optional[torch.Tensor] = None,<br>block_shape: Optional[List[int]] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fused_experts(
    hidden_states: torch.Tensor,
    w1: torch.Tensor,
    w2: torch.Tensor,
    topk_output: StandardTopKOutput,
    moe_runner_config: MoeRunnerConfig,
    b1: Optional[torch.Tensor] = None,
    b2: Optional[torch.Tensor] = None,
    use_fp8_w8a8: bool = False,
    use_int8_w8a8: bool = False,
    use_int8_w8a16: bool = False,
    use_int4_w4a16: bool = False,
    per_channel_quant: bool = False,
    w1_scale: Optional[torch.Tensor] = None,
    w2_scale: Optional[torch.Tensor] = None,
    w1_zp: Optional[torch.Tensor] = None,
    w2_zp: Optional[torch.Tensor] = None,
    a1_scale: Optional[torch.Tensor] = None,
    a2_scale: Optional[torch.Tensor] = None,
    block_shape: Optional[List[int]] = None,
):
    topk_weights, topk_ids, _ = topk_output
    if moe_runner_config.inplace:
        assert not moe_runner_config.no_combine, &#34;no combine + inplace makes no sense&#34;
        torch.ops.sglang.inplace_fused_experts(
            hidden_states,
            w1,
            w2,
            topk_weights,
            topk_ids,
            b1,
            b2,
            moe_runner_config.activation,
            moe_runner_config.apply_router_weight_on_input,
            use_fp8_w8a8,
            use_int8_w8a8,
            use_int8_w8a16,
            use_int4_w4a16,
            per_channel_quant,
            w1_scale,
            w2_scale,
            w1_zp,
            w2_zp,
            a1_scale,
            a2_scale,
            block_shape,
            moe_runner_config.routed_scaling_factor,
            moe_runner_config.gemm1_alpha,
            moe_runner_config.gemm1_clamp_limit,
        )
        return hidden_states
    else:
        return torch.ops.sglang.outplace_fused_experts(
            hidden_states,
            w1,
            w2,
            topk_weights,
            topk_ids,
            b1,
            b2,
            moe_runner_config.activation,
            moe_runner_config.apply_router_weight_on_input,
            use_fp8_w8a8,
            use_int8_w8a8,
            use_int8_w8a16,
            use_int4_w4a16,
            per_channel_quant,
            w1_scale,
            w2_scale,
            w1_zp,
            w2_zp,
            a1_scale,
            a2_scale,
            block_shape,
            no_combine=moe_runner_config.no_combine,
            routed_scaling_factor=moe_runner_config.routed_scaling_factor,
            gemm1_alpha=moe_runner_config.gemm1_alpha,
            gemm1_limit=moe_runner_config.gemm1_clamp_limit,
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.fused_moe_triton.get_config"><code class="name flex">
<span>def <span class="ident">get_config</span></span>(<span>) ‑> Dict[str, Any] | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config() -&gt; Optional[Dict[str, Any]]:
    return _config</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.fused_moe_triton.get_config_file_name"><code class="name flex">
<span>def <span class="ident">get_config_file_name</span></span>(<span>E: int, N: int, dtype: Optional[str], block_shape: Optional[int] = None) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_config_file_name(
    E: int, N: int, dtype: Optional[str], block_shape: Optional[int] = None
) -&gt; str:
    device_name = get_device_name().replace(&#34; &#34;, &#34;_&#34;)
    dtype_selector = &#34;&#34; if not dtype else f&#34;,dtype={dtype}&#34;
    block_shape_selector = (
        &#34;&#34; if not block_shape or not all(block_shape) else f&#34;,block_shape={block_shape}&#34;
    )
    return f&#34;E={E},N={N},device_name={device_name}{dtype_selector}{block_shape_selector}.json&#34;</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.fused_moe_triton.moe_align_block_size"><code class="name flex">
<span>def <span class="ident">moe_align_block_size</span></span>(<span>topk_ids: torch.Tensor, block_size: int, num_experts: int) ‑> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def moe_align_block_size(
    topk_ids: torch.Tensor, block_size: int, num_experts: int
) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;
    Aligns the token distribution across experts to be compatible with block
    size for matrix multiplication.

    Parameters:
    - topk_ids: A tensor of shape [total_tokens, top_k] representing the
        top-k expert indices for each token.
    - block_size: The block size used in block matrix multiplication.
    - num_experts: The total number of experts.

    Returns:
    - sorted_token_ids: A tensor containing the sorted token indices according
        to their allocated expert.
    - expert_ids: A tensor indicating the assigned expert index for each block.
    - num_tokens_post_padded: The total number of tokens after padding,
        ensuring divisibility by block_size.

    This function pads the number of tokens that each expert needs to process
    so that it is divisible by block_size.
    Padding ensures that during block matrix multiplication, the dimensions
    align correctly.

    Example:
    Given topk_ids = [[2, 3, 4], [1, 2, 4], [1, 3, 4], [1, 2, 3]],
    block_size = 4, and num_experts = 4:
    - We initially have 12 tokens (after repeating &#39;top_k&#39; times) and 4 experts,
        with each expert needing to process 3 tokens.
    - As block_size is 4, we pad 1 token for each expert.
    - First, flatten topk_ids to [2, 3, 4, 1, 2, 4, 1, 3, 4, 1, 2, 3].
    - Then append padding tokens [12, 12, 12, 12] for each block.
    - After sorting by expert index, we obtain token_ids
        [3, 6, 9, 12, 0, 4, 10, 12, 1, 7, 11, 12, 2, 5, 8, 12].
        Tokens 12 are non-existent (padding) and are ignored in
        the subsequent matrix multiplication.
    - The padding ensures that the total number of tokens is now divisible
        by block_size for proper block matrix operations.
    &#34;&#34;&#34;
    max_num_tokens_padded = topk_ids.numel() + (num_experts + 1) * (block_size - 1)
    sorted_ids = torch.empty(
        (max_num_tokens_padded,), dtype=torch.int32, device=topk_ids.device
    )
    max_num_m_blocks = triton.cdiv(max_num_tokens_padded, block_size)
    expert_ids = torch.empty(
        (max_num_m_blocks,), dtype=torch.int32, device=topk_ids.device
    )
    num_tokens_post_pad = torch.empty((1), dtype=torch.int32, device=topk_ids.device)

    # In EP, expert_ids for filtered experts are -1. We have num_experts + 1 ids in total.
    cumsum_buffer = torch.empty(
        (num_experts + 2,), dtype=torch.int32, device=topk_ids.device
    )

    # Threshold based on benchmark results
    fuse_sorted_ids_padding = sorted_ids.shape[0] &lt;= 4096
    if not fuse_sorted_ids_padding:
        sorted_ids.fill_(topk_ids.numel())

    sgl_moe_align_block_size(
        topk_ids,
        num_experts + 1,
        block_size,
        sorted_ids,
        expert_ids,
        num_tokens_post_pad,
        cumsum_buffer,
        fuse_sorted_ids_padding,
    )
    return sorted_ids, expert_ids, num_tokens_post_pad</code></pre>
</details>
<div class="desc"><p>Aligns the token distribution across experts to be compatible with block
size for matrix multiplication.</p>
<p>Parameters:
- topk_ids: A tensor of shape [total_tokens, top_k] representing the
top-k expert indices for each token.
- block_size: The block size used in block matrix multiplication.
- num_experts: The total number of experts.</p>
<p>Returns:
- sorted_token_ids: A tensor containing the sorted token indices according
to their allocated expert.
- expert_ids: A tensor indicating the assigned expert index for each block.
- num_tokens_post_padded: The total number of tokens after padding,
ensuring divisibility by block_size.</p>
<p>This function pads the number of tokens that each expert needs to process
so that it is divisible by block_size.
Padding ensures that during block matrix multiplication, the dimensions
align correctly.</p>
<p>Example:
Given topk_ids = [[2, 3, 4], [1, 2, 4], [1, 3, 4], [1, 2, 3]],
block_size = 4, and num_experts = 4:
- We initially have 12 tokens (after repeating 'top_k' times) and 4 experts,
with each expert needing to process 3 tokens.
- As block_size is 4, we pad 1 token for each expert.
- First, flatten topk_ids to [2, 3, 4, 1, 2, 4, 1, 3, 4, 1, 2, 3].
- Then append padding tokens [12, 12, 12, 12] for each block.
- After sorting by expert index, we obtain token_ids
[3, 6, 9, 12, 0, 4, 10, 12, 1, 7, 11, 12, 2, 5, 8, 12].
Tokens 12 are non-existent (padding) and are ignored in
the subsequent matrix multiplication.
- The padding ensures that the total number of tokens is now divisible
by block_size for proper block matrix operations.</p></div>
</dd>
<dt id="sglang.srt.layers.moe.fused_moe_triton.override_config"><code class="name flex">
<span>def <span class="ident">override_config</span></span>(<span>config)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def override_config(config):
    global _config
    old_config = _config
    _config = config
    yield
    _config = old_config</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.fused_moe_triton.try_get_optimal_moe_config"><code class="name flex">
<span>def <span class="ident">try_get_optimal_moe_config</span></span>(<span>w1_shape: Tuple[int, ...],<br>w2_shape: Tuple[int, ...],<br>top_k: int,<br>dtype: Optional[str],<br>M: int,<br>is_marlin: bool = False,<br>block_shape: Optional[List[int]] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def try_get_optimal_moe_config(
    w1_shape: Tuple[int, ...],
    w2_shape: Tuple[int, ...],
    top_k: int,
    dtype: Optional[str],
    M: int,
    is_marlin: bool = False,
    block_shape: Optional[List[int]] = None,
):
    from sglang.srt.layers.moe.fused_moe_triton import get_config

    override_config = get_config()
    if override_config:
        config = override_config
    else:
        # First try to load optimal config from the file
        E, _, N = w2_shape
        block_n = block_shape[0] if block_shape else 0
        block_k = block_shape[1] if block_shape else 0
        configs = get_moe_configs(E, N, dtype, block_n, block_k)

        if configs:
            # If an optimal configuration map has been found, look up the
            # optimal config
            config = configs[min(configs.keys(), key=lambda x: abs(x - M))]
        else:
            # Else use the default config
            config = get_default_config(
                M, E, N, w1_shape[2], top_k, dtype, is_marlin, block_shape
            )
    return config</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.layers.moe.fused_moe_triton.FusedMoE"><code class="flex name class">
<span>class <span class="ident">FusedMoE</span></span>
<span>(</span><span>num_experts: int,<br>hidden_size: int,<br>intermediate_size: int,<br>layer_id: int,<br>top_k: int | None = None,<br>num_fused_shared_experts: int = 0,<br>params_dtype: torch.dtype | None = None,<br>reduce_results: bool = False,<br>quant_config: <a title="sglang.srt.layers.quantization.base_config.QuantizationConfig" href="../../quantization/base_config.html#sglang.srt.layers.quantization.base_config.QuantizationConfig">QuantizationConfig</a> | None = None,<br>prefix: str = '',<br>activation: str = 'silu',<br>apply_router_weight_on_input: bool = False,<br>use_presharded_weights: bool = False,<br>inplace: bool = True,<br>no_combine: bool = False,<br>routed_scaling_factor: float | None = None,<br>gemm1_alpha: float | None = None,<br>gemm1_clamp_limit: float | None = None,<br>use_weight_loader_fused: bool = False,<br>with_bias=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FusedMoE(torch.nn.Module):
    &#34;&#34;&#34;FusedMoE layer for MoE models.

    This layer contains both MergedColumnParallel weights (gate_up_proj /
    w13) and RowParallelLinear weights (down_proj/ w2).

    Note: Mixtral uses w1, w2, and w3 for gate, up, and down_proj. We
    copy that naming convention here and handle any remapping in the
    load_weights function in each model implementation.

    Args:
        num_experts: Number of experts in the model
        top_k: Number of experts selected for each token
        hidden_size: Input hidden state size of the transformer
        intermediate_size: Intermediate size of the experts
        params_dtype: Data type for the parameters.
        reduce_results: Whether to apply all_reduce on the output of the layer
        quant_config: Quantization configuration.
        inplace: suggestion to compute inplace (modify input activation).
    &#34;&#34;&#34;

    def __init__(
        self,
        num_experts: int,
        hidden_size: int,
        intermediate_size: int,
        layer_id: int,
        top_k: Optional[int] = None,
        num_fused_shared_experts: int = 0,
        params_dtype: Optional[torch.dtype] = None,
        reduce_results: bool = False,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = &#34;&#34;,
        activation: str = &#34;silu&#34;,
        apply_router_weight_on_input: bool = False,
        use_presharded_weights: bool = False,
        inplace: bool = True,
        no_combine: bool = False,
        routed_scaling_factor: Optional[float] = None,
        gemm1_alpha: Optional[float] = None,
        gemm1_clamp_limit: Optional[float] = None,
        use_weight_loader_fused: bool = False,
        with_bias=False,
    ):
        super().__init__()

        if params_dtype is None:
            params_dtype = torch.get_default_dtype()

        self.layer_id = layer_id
        self.top_k = top_k
        self.hidden_size = hidden_size
        self.num_experts = num_experts
        self.num_fused_shared_experts = num_fused_shared_experts
        self.expert_map_cpu = None
        self.expert_map_gpu = None

        self.moe_runner_config = MoeRunnerConfig(
            activation=activation,
            apply_router_weight_on_input=apply_router_weight_on_input,
            inplace=inplace,
            no_combine=no_combine,
            routed_scaling_factor=routed_scaling_factor,
            gemm1_alpha=gemm1_alpha,
            gemm1_clamp_limit=gemm1_clamp_limit,
        )

        enable_flashinfer_cutlass_moe = get_moe_runner_backend().is_flashinfer_cutlass()

        if enable_flashinfer_cutlass_moe and quant_config is None:
            logger.warning(&#34;Disable flashinfer MoE when quantization config is None.&#34;)
            enable_flashinfer_cutlass_moe = False

        self.enable_flashinfer_cutlass_moe = enable_flashinfer_cutlass_moe
        self.moe_ep_size = get_moe_expert_parallel_world_size()
        self.moe_ep_rank = get_moe_expert_parallel_rank()
        self.moe_tp_size = get_moe_tensor_parallel_world_size()
        self.moe_tp_rank = get_moe_tensor_parallel_rank()
        assert num_experts % self.moe_ep_size == 0
        self.num_local_experts = num_experts // self.moe_ep_size
        if self.moe_ep_size &gt; 1:
            # TODO(ch-wan): support shared experts fusion
            # Create a tensor of size num_experts filled with -1
            self.expert_map_cpu = torch.full(
                (self.num_experts,), -1, dtype=torch.int32, device=&#34;cpu&#34;
            )
            # Create a expert map for the local experts
            self.expert_map_cpu[
                self.moe_ep_rank
                * self.num_local_experts : (self.moe_ep_rank + 1)
                * self.num_local_experts
            ] = torch.arange(0, self.num_local_experts, dtype=torch.int32, device=&#34;cpu&#34;)

        assert intermediate_size % self.moe_tp_size == 0
        self.intermediate_size_per_partition = intermediate_size // self.moe_tp_size
        self.reduce_results = reduce_results
        self.use_presharded_weights = use_presharded_weights

        self.use_triton_kernels = get_moe_runner_backend().is_triton_kernel()
        if quant_config is None:
            self.quant_method: Optional[QuantizeMethodBase] = UnquantizedFusedMoEMethod(
                self.use_triton_kernels
            )
        else:
            self.quant_method = quant_config.get_quant_method(self, prefix)
        assert self.quant_method is not None

        self.quant_config = quant_config
        self.use_flashinfer_mxfp4_moe = get_moe_runner_backend().is_flashinfer_mxfp4()
        # TODO maybe we should remove this `if`, since `Mxfp4MoEMethod` does another round-up logic
        if (
            self.quant_config is not None
            and self.quant_config.get_name() == &#34;mxfp4&#34;
            and self.use_flashinfer_mxfp4_moe
        ):
            hidden_size = round_up(hidden_size, 256)
        self.quant_method.create_weights(
            layer=self,
            num_experts=self.num_local_experts,
            hidden_size=hidden_size,
            # FIXME: figure out which intermediate_size to use
            intermediate_size=self.intermediate_size_per_partition,
            intermediate_size_per_partition=self.intermediate_size_per_partition,
            params_dtype=params_dtype,
            weight_loader=(
                self.weight_loader
                if not use_weight_loader_fused
                else self.weight_loader_fused
            ),
            with_bias=with_bias,
        )

    def _load_per_tensor_weight_scale(
        self,
        shard_id: str,
        param: torch.nn.Parameter,
        loaded_weight: torch.Tensor,
        expert_id: int,
    ):
        param_data = param.data
        # for per tensor weight quantization
        if shard_id in (&#34;w1&#34;, &#34;w3&#34;):
            # We have to keep the weight scales of w1 and w3 because
            # we need to re-quantize w1/w3 weights after weight loading.
            idx = 0 if shard_id == &#34;w1&#34; else 1
            param_data[expert_id][idx] = loaded_weight
        # If we are in the row parallel case (down_proj)
        elif shard_id == &#34;w2&#34;:
            param_data[expert_id] = loaded_weight

    def _load_model_weight_or_group_weight_scale(
        self,
        shard_dim: int,
        expert_data: torch.Tensor,
        shard_id: str,
        loaded_weight: torch.Tensor,
        tp_rank: int,
        is_bias: bool = False,
    ):
        # Load grouped weight scales for group quantization
        # or model weights
        if shard_id == &#34;w2&#34;:
            self._load_w2(
                shard_id=shard_id,
                shard_dim=shard_dim,
                loaded_weight=loaded_weight,
                expert_data=expert_data,
                tp_rank=tp_rank,
                is_bias=is_bias,
            )
        elif shard_id in (&#34;w1&#34;, &#34;w3&#34;, &#34;w13&#34;):
            self._load_w13(
                shard_id=shard_id,
                shard_dim=shard_dim,
                loaded_weight=loaded_weight,
                expert_data=expert_data,
                tp_rank=tp_rank,
                is_bias=is_bias,
            )

    def _load_per_channel_weight_scale(
        self,
        expert_data: torch.Tensor,
        shard_dim: int,
        shard_id: str,
        loaded_weight: torch.Tensor,
        tp_rank: int,
    ):
        # for per channel weight quantization
        if shard_id == &#34;w2&#34;:
            expert_data.copy_(loaded_weight)
        elif shard_id in (&#34;w1&#34;, &#34;w3&#34;):
            self._load_w13(
                shard_id=shard_id,
                shard_dim=shard_dim,
                loaded_weight=loaded_weight,
                expert_data=expert_data,
                tp_rank=tp_rank,
            )

    def _load_w13(
        self,
        expert_data: torch.Tensor,
        shard_dim: int,
        shard_id: str,
        loaded_weight: torch.Tensor,
        tp_rank: int,
        is_bias: bool = False,
    ):

        # Index the loaded weight for tp sharding.
        # gate_up_proj: &#34;MergedColumnParallel&#34;, so tp sharding on output_dim
        assert shard_id in {&#34;w1&#34;, &#34;w3&#34;, &#34;w13&#34;}

        if is_bias:
            # if this weight is a bias, the last dimension must be the sharded dimension
            shard_dim = -1

        if shard_id in {&#34;w1&#34;, &#34;w3&#34;}:
            # non-fused version
            shard_size = expert_data.shape[shard_dim] // 2
        elif shard_id in {&#34;w13&#34;}:
            # fused version
            shard_size = expert_data.shape[shard_dim]
        else:
            raise NotImplementedError

        # Narrow parameter and load.
        # w1, gate_proj: Load into first logical weight of w13.
        # w3, up_proj: Load into second logical weight of w13.
        # trtllm cutlass kernel assumes differently
        switch_w13 = getattr(self.quant_method, &#34;load_up_proj_weight_first&#34;, False)
        if (switch_w13 and shard_id == &#34;w1&#34;) or (not switch_w13 and shard_id == &#34;w3&#34;):
            start = shard_size
        else:
            start = 0

        if _is_cpu:
            expert_data, loaded_weight = narrow_padded_param_and_loaded_weight(
                expert_data,
                loaded_weight,
                start,
                shard_size * tp_rank,
                shard_dim,
                shard_size,
                not self.use_presharded_weights,
            )
        else:
            if not self.use_presharded_weights:
                if not is_bias and self.use_triton_kernels:
                    # do not transpose for bias
                    loaded_weight = loaded_weight.transpose(-2, -1)
                loaded_weight = loaded_weight.narrow(
                    shard_dim, shard_size * tp_rank, shard_size
                )

            expert_data = expert_data.narrow(shard_dim, start, shard_size)
        expert_data.copy_(loaded_weight)

    def _load_w2(
        self,
        expert_data: torch.Tensor,
        shard_dim: int,
        shard_id: str,
        loaded_weight: torch.Tensor,
        tp_rank: int,
        is_bias: bool = False,
    ):
        &#34;&#34;&#34;Load w2 weights for down projection.

        Args:
            expert_data: The expert data tensor to load into
            shard_dim: The dimension to shard along
            shard_id: The shard ID (must be &#34;w2&#34;)
            loaded_weight: The weight tensor to load from
            tp_rank: The tensor parallel rank
        &#34;&#34;&#34;
        if not isinstance(expert_data, torch.Tensor) or not isinstance(
            loaded_weight, torch.Tensor
        ):
            raise ValueError(&#34;expert_data and loaded_weight must be torch.Tensor&#34;)

        if (
            self.quant_config is not None
            and &#34;modelopt&#34; in self.quant_config.get_name()
            and (expert_data.dim() != 2 or loaded_weight.dim() != 2)
        ):
            raise ValueError(
                f&#34;Expected 2D tensors, got expert_data shape {expert_data.shape} and loaded_weight shape {loaded_weight.shape}&#34;
            )

        if shard_id != &#34;w2&#34;:
            raise ValueError(f&#34;shard_id must be &#39;w2&#39;, got {shard_id}&#34;)

        # Index the loaded weight for tp sharding.
        # down_proj: &#34;RowParallel&#34; so tp sharding on input_dim
        # Narrow parameter and load.
        if is_bias:
            # this expert_data is a bias, not weight,
            # for w2_weight_bias in TP, it does not need to be sharded
            shard_size = expert_data.shape[-1]
        else:
            # this parameter is a weight matrix
            # for w2 in TP, it shards the input_features, i.e., shard_dim=2
            shard_size = expert_data.shape[shard_dim]

        if _is_cpu:
            expert_data, loaded_weight = narrow_padded_param_and_loaded_weight(
                expert_data,
                loaded_weight,
                0,  # param_data_start
                shard_size * tp_rank,
                shard_dim,
                shard_size,
                not self.use_presharded_weights,
            )
        else:
            if not is_bias and not self.use_presharded_weights:
                if self.use_triton_kernels:
                    loaded_weight = loaded_weight.transpose(-2, -1)
                loaded_weight = loaded_weight.narrow(
                    shard_dim, shard_size * tp_rank, shard_size
                )

        # w2, down_proj: Load into only logical weight of w2.
        expert_data.copy_(loaded_weight)

    def _load_single_value(
        self, param: torch.nn.Parameter, loaded_weight: torch.Tensor, expert_id: int
    ):
        param_data = param.data

        # Input scales can be loaded directly and should be equal.
        param_data[expert_id] = loaded_weight

    def _load_g_idx(
        self,
        shard_id: str,
        expert_data: torch.Tensor,
        shard_dim: int,
        loaded_weight: torch.Tensor,
        tp_rank: int,
    ):

        if shard_id == &#34;w2&#34;:
            self._load_w2(
                shard_id=shard_id,
                shard_dim=shard_dim,
                loaded_weight=loaded_weight,
                expert_data=expert_data,
                tp_rank=tp_rank,
            )
        else:
            assert shard_id in (&#34;w1&#34;, &#34;w3&#34;)
            expert_data.copy_(loaded_weight)

    def _map_global_expert_id_to_local_expert_id(self, expert_id: int) -&gt; int:
        if self.expert_map_cpu is None:
            return expert_id
        return self.expert_map_cpu[expert_id].item()

    def weight_loader(
        self,
        param: torch.nn.Parameter,
        loaded_weight: torch.Tensor,
        weight_name: str,
        shard_id: str,
        expert_id: Optional[int],
    ) -&gt; None:

        # if expert_id is None, then
        # all the experts are loaded at the same time
        if (
            not expert_id
            and self.quant_config is not None
            and self.quant_config.get_name() == &#34;mxfp4&#34;
            and self.quant_config.is_static_cfg()
        ):
            if &#34;bias&#34; in weight_name:
                dim1 = loaded_weight.shape[1]
                param.data[:, :dim1].copy_(loaded_weight)
            else:
                dim1 = loaded_weight.shape[1]
                dim2 = loaded_weight.shape[2]
                param.data[:, :dim1, :dim2].copy_(loaded_weight)
            return

        global_expert_location_metadata = get_global_expert_location_metadata()
        if global_expert_location_metadata is None:
            self._weight_loader_impl(
                param=param,
                loaded_weight=loaded_weight,
                weight_name=weight_name,
                shard_id=shard_id,
                expert_id=expert_id,
            )
            return

        if expert_id &gt;= self.num_experts - self.num_fused_shared_experts:
            # This is a shared expert.
            physical_expert_ids = [expert_id]
        else:
            physical_expert_ids = (
                global_expert_location_metadata.logical_to_all_physical(
                    self.layer_id, expert_id
                )
            )

        for physical_expert_id in physical_expert_ids:
            self._weight_loader_physical(
                param=param,
                loaded_weight=loaded_weight,
                weight_name=weight_name,
                shard_id=shard_id,
                expert_id=physical_expert_id,
            )

    def _weight_loader_physical(
        self,
        param: torch.nn.Parameter,
        loaded_weight: torch.Tensor,
        weight_name: str,
        shard_id: str,
        expert_id: int,
    ) -&gt; None:

        expert_id = self._map_global_expert_id_to_local_expert_id(expert_id)
        if expert_id == -1:
            return
        self._weight_loader_impl(
            param=param,
            loaded_weight=loaded_weight,
            weight_name=weight_name,
            shard_id=shard_id,
            expert_id=expert_id,
        )

    def _weight_loader_impl(
        self,
        param: torch.nn.Parameter,
        loaded_weight: torch.Tensor,
        weight_name: str,
        shard_id: str,
        expert_id: int,
    ) -&gt; None:

        tp_rank = self.moe_tp_rank

        # compressed-tensors checkpoints with packed weights are stored flipped
        # TODO (mgoin): check self.quant_method.quant_config.quant_format
        # against known CompressionFormat enum values that have this quality
        loaded_weight = (
            loaded_weight.t().contiguous()
            if (
                self.quant_method.__class__.__name__
                == &#34;CompressedTensorsWNA16MoEMethod&#34;
            )
            else loaded_weight
        )

        if shard_id not in (&#34;w1&#34;, &#34;w2&#34;, &#34;w3&#34;):
            raise ValueError(
                f&#34;shard_id must be [&#39;w1&#39;,&#39;w2&#39;,&#39;w3&#39;] but &#34; f&#34;got {shard_id}.&#34;
            )

        # Flashinfer assumes w31 format for w13_weight. Same for the scales.
        if should_use_flashinfer_trtllm_moe():
            shard_id = {&#34;w1&#34;: &#34;w3&#34;, &#34;w3&#34;: &#34;w1&#34;, &#34;w2&#34;: &#34;w2&#34;}[shard_id]

        WEIGHT_SCALE_SUPPORTED = [e.value for e in FusedMoeWeightScaleSupported]
        # Fetch the dim to shard the parameter/loaded weight
        # based on the shard id. This will be whatever
        # dimension intermediate_size is used.
        SHARD_ID_TO_SHARDED_DIM = {&#34;w1&#34;: 0, &#34;w2&#34;: 1, &#34;w3&#34;: 0}

        expert_data = param.data[expert_id]

        # is_transposed: if the dim to shard the weight
        # should be flipped. Required by GPTQ, compressed-tensors
        # should be whatever dimension intermediate_size is
        is_transposed = getattr(param, &#34;is_transposed&#34;, False)
        shard_dim = SHARD_ID_TO_SHARDED_DIM[shard_id]
        if self.use_triton_kernels:
            is_transposed = True
        if is_transposed:
            shard_dim = int(not shard_dim)

        # Case input scale: input_scale loading is only supported for fp8
        if &#34;input_scale&#34; in weight_name:
            # INT4-FP8 (INT4 MoE Weight, FP8 Compute): Adjust input_scale for e4m3fnuz (AMD)
            if _is_hip and get_bool_env_var(&#34;SGLANG_INT4_WEIGHT&#34;):
                loaded_weight = loaded_weight * 2.0

            # this is needed for compressed-tensors only
            loaded_weight = loaded_weight.to(param.data.device)

            if (
                &#34;compressed&#34; in self.quant_method.__class__.__name__.lower()
                and param.data[expert_id] != 1
                and (param.data[expert_id] - loaded_weight).abs() &gt; 1e-5
            ):
                raise ValueError(
                    &#34;input_scales of w1 and w3 of a layer &#34;
                    f&#34;must be equal. But got {param.data[expert_id]} &#34;
                    f&#34;vs. {loaded_weight}&#34;
                )

            self._load_single_value(
                param=param, loaded_weight=loaded_weight, expert_id=expert_id
            )
            return

        # Case g_idx
        if &#34;g_idx&#34; in weight_name:
            self._load_g_idx(
                shard_dim=0,
                shard_id=shard_id,
                loaded_weight=loaded_weight,
                expert_data=expert_data,
                tp_rank=tp_rank,
            )
            return

        if &#34;ModelOpt&#34; in self.quant_method.__class__.__name__:
            # Determine per-tensor weight scale patterns based on variant
            is_fp4_variant = isinstance(self.quant_method, ModelOptNvFp4FusedMoEMethod)

            # FP4 uses &#34;weight_scale_2&#34; for per-tensor, FP8 uses &#34;weight_scale&#34; for per-tensor
            per_tensor_conditions = (
                &#34;weight_scale_2&#34; in weight_name
                if is_fp4_variant
                else &#34;weight_scale&#34; in weight_name
            ) or &#34;input_scale&#34; in weight_name

            if per_tensor_conditions:
                self._load_per_tensor_weight_scale(
                    shard_id=shard_id,
                    param=param,
                    loaded_weight=loaded_weight,
                    expert_id=expert_id,
                )
            elif &#34;weight&#34; in weight_name:
                self._load_model_weight_or_group_weight_scale(
                    shard_id=shard_id,
                    shard_dim=shard_dim,
                    loaded_weight=loaded_weight,
                    expert_data=expert_data,
                    tp_rank=tp_rank,
                )
            return

        # Case weight scales and zero_points
        if &#34;scale&#34; in weight_name or &#34;zero&#34; in weight_name or &#34;offset&#34; in weight_name:
            # load the weight scales and zp based on the quantization scheme
            # supported weight scales/zp can be found in
            # FusedMoeWeightScaleSupported
            # TODO @dsikka: once hardened, refactor to use vLLM Parameters
            # specific to each case
            quant_method = getattr(param, &#34;quant_method&#34;, None)
            if quant_method == FusedMoeWeightScaleSupported.CHANNEL.value:
                # INT4-FP8 (INT4 MoE Weight, FP8 Compute): Adjust INT4 column-wise scaling number to e4m3fnuz (AMD)
                if _is_hip and get_bool_env_var(&#34;SGLANG_INT4_WEIGHT&#34;):
                    loaded_weight = loaded_weight * 0.5

                self._load_per_channel_weight_scale(
                    shard_id=shard_id,
                    shard_dim=shard_dim,
                    loaded_weight=loaded_weight,
                    expert_data=expert_data,
                    tp_rank=tp_rank,
                )
            elif quant_method in [
                FusedMoeWeightScaleSupported.GROUP.value,
                FusedMoeWeightScaleSupported.BLOCK.value,
            ]:
                self._load_model_weight_or_group_weight_scale(
                    shard_id=shard_id,
                    shard_dim=shard_dim,
                    loaded_weight=loaded_weight,
                    expert_data=expert_data,
                    tp_rank=tp_rank,
                )
            elif quant_method == FusedMoeWeightScaleSupported.TENSOR.value:
                # INT4-FP8 (INT4 MoE Weight, FP8 Compute): Adjust FP8 per-tensor scaling number for e4m3fnuz (AMD)
                if _is_hip and get_bool_env_var(&#34;SGLANG_INT4_WEIGHT&#34;):
                    loaded_weight = loaded_weight * 2.0

                self._load_per_tensor_weight_scale(
                    shard_id=shard_id,
                    param=param,
                    loaded_weight=loaded_weight,
                    expert_id=expert_id,
                )
            else:
                raise ValueError(
                    f&#34;quant method must be one of {WEIGHT_SCALE_SUPPORTED}&#34;
                )
            return

        # Case weight_shape
        if &#34;weight_shape&#34; in weight_name:
            # only required by compressed-tensors
            self._load_single_value(
                param=param, loaded_weight=loaded_weight, expert_id=expert_id
            )
            return

        # Case model weights
        if &#34;weight&#34; in weight_name:
            self._load_model_weight_or_group_weight_scale(
                shard_id=shard_id,
                shard_dim=shard_dim,
                loaded_weight=loaded_weight,
                expert_data=expert_data,
                tp_rank=tp_rank,
            )
            return

    def weight_loader_fused(
        self,
        param: torch.nn.Parameter,
        loaded_weight: torch.Tensor,
        weight_name: str,
        shard_id: str,
    ) -&gt; None:
        tp_rank = self.moe_tp_rank

        if (
            self.quant_config is not None
            and self.quant_config.get_name() == &#34;mxfp4&#34;
            and self.quant_config.is_static_cfg()
        ):
            if &#34;bias&#34; in weight_name:
                dim1 = loaded_weight.shape[1]
                param.data[:, :dim1].copy_(loaded_weight)
            elif &#34;scale&#34; in weight_name:
                param.data.copy_(loaded_weight)
            else:
                dim1 = loaded_weight.shape[1]
                dim2 = loaded_weight.shape[2]
                param.data[:, :dim1, :dim2].copy_(loaded_weight)
            return

        # compressed-tensors checkpoints with packed weights are stored flipped
        # TODO: check self.quant_method.quant_config.quant_format
        # against known CompressionFormat enum values that have this quality
        loaded_weight = (
            loaded_weight.t().contiguous()
            if (
                self.quant_method.__class__.__name__
                == &#34;CompressedTensorsWNA16MoEMethod&#34;
            )
            else loaded_weight
        )

        if shard_id not in (&#34;w13&#34;, &#34;w2&#34;):
            raise ValueError(f&#34;shard_id must be [&#39;w13&#39;,&#39;w2&#39;] but &#34; f&#34;got {shard_id}.&#34;)

        # Fetch the dim to shard the parameter/loaded weight
        # based on the shard id. This will be whatever
        # dimension intermediate_size is used.
        SHARD_ID_TO_SHARDED_DIM = {&#34;w13&#34;: 1, &#34;w2&#34;: 2}
        SHARD_ID_TO_SHARDED_DIM_TRANSPOSE = {&#34;w13&#34;: 2, &#34;w2&#34;: 1}

        expert_data = param.data
        is_bias = expert_data.dim() == 2

        # is_transposed: if the dim to shard the weight
        # should be flipped. Required by GPTQ, compressed-tensors
        # should be whatever dimension intermediate_size is
        is_transposed = getattr(param, &#34;is_transposed&#34;, False)

        if self.use_triton_kernels:
            is_transposed = True
        shard_dim = (
            SHARD_ID_TO_SHARDED_DIM[shard_id]
            if not is_transposed
            else SHARD_ID_TO_SHARDED_DIM_TRANSPOSE[shard_id]
        )

        # Case model weights
        if &#34;weight&#34; in weight_name:
            self._load_model_weight_or_group_weight_scale(
                shard_id=shard_id,
                shard_dim=shard_dim,
                loaded_weight=loaded_weight,
                expert_data=expert_data,
                tp_rank=tp_rank,
                is_bias=is_bias,
            )
            return
        else:
            logging.warning(
                f&#34;Unsupported weight_name {weight_name} for FusedMoE weight_loader_fused. Nothing is loaded.&#34;
            )

    def forward(self, hidden_states: torch.Tensor, topk_output: TopKOutput):
        origin_hidden_states_dim = hidden_states.shape[-1]
        assert self.quant_method is not None

        if self.moe_ep_size &gt; 1 and not self.enable_flashinfer_cutlass_moe:
            if self.expert_map_cpu is not None and self.expert_map_gpu is None:
                # If we are in EP mode, we need to move the expert map to GPU.
                self.expert_map_gpu = self.expert_map_cpu.to(device=&#34;cuda&#34;)

        if self.expert_map_gpu is not None:
            if TopKOutputChecker.format_is_standard(topk_output):
                topk_output = topk_output._replace(
                    topk_ids=self.expert_map_gpu[topk_output.topk_ids]
                )
            elif TopKOutputChecker.format_is_triton_kernel(topk_output):
                raise NotImplementedError()

        # Matrix multiply.
        with use_symmetric_memory(get_tp_group()) as sm:

            final_hidden_states = self.quant_method.apply(
                layer=self,
                x=hidden_states,
                topk_output=topk_output,
                moe_runner_config=self.moe_runner_config,
            )
            sm.tag(final_hidden_states)

        final_hidden_states = final_hidden_states[
            ..., :origin_hidden_states_dim
        ].contiguous()

        if self.reduce_results and (self.moe_tp_size &gt; 1 or self.moe_ep_size &gt; 1):
            final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)

        return final_hidden_states

    @classmethod
    def make_expert_params_mapping(
        cls,
        ckpt_gate_proj_name: str,
        ckpt_down_proj_name: str,
        ckpt_up_proj_name: str,
        num_experts: int,
    ) -&gt; List[Tuple[str, str, int, str]]:

        return [
            # (param_name, weight_name, expert_id, shard_id)
            (
                (
                    &#34;experts.w13_&#34;
                    if weight_name in [ckpt_gate_proj_name, ckpt_up_proj_name]
                    else &#34;experts.w2_&#34;
                ),
                f&#34;experts.{expert_id}.{weight_name}.&#34;,
                expert_id,
                shard_id,
            )
            for expert_id in range(num_experts)
            for shard_id, weight_name in [
                (&#34;w1&#34;, ckpt_gate_proj_name),
                (&#34;w2&#34;, ckpt_down_proj_name),
                (&#34;w3&#34;, ckpt_up_proj_name),
            ]
        ]

    @classmethod
    def make_expert_params_mapping_fused(
        cls,
        ckpt_gate_up_proj_name: str,
        ckpt_down_proj_name: str,
        ckpt_gate_up_proj_bias_name: str,
        ckpt_down_proj_bias_name: str,
    ):
        return [
            (&#34;experts.w13_weight&#34;, f&#34;experts.{ckpt_gate_up_proj_name}&#34;, &#34;w13&#34;),
            (
                &#34;experts.w13_weight_bias&#34;,
                f&#34;experts.{ckpt_gate_up_proj_bias_name}&#34;,
                &#34;w13&#34;,
            ),
            (&#34;experts.w2_weight&#34;, f&#34;experts.{ckpt_down_proj_name}&#34;, &#34;w2&#34;),
            (&#34;experts.w2_weight_bias&#34;, f&#34;experts.{ckpt_down_proj_bias_name}&#34;, &#34;w2&#34;),
        ]

    @classmethod
    def make_expert_params_mapping_fused_mxfp4(
        cls,
        ckpt_gate_up_proj_name: str,
        ckpt_down_proj_name: str,
        ckpt_gate_up_proj_bias_name: str,
        ckpt_down_proj_bias_name: str,
        ckpt_gate_up_proj_scale_name: str,
        ckpt_down_proj_scale_name: str,
    ):
        return [
            (&#34;experts.w13_weight&#34;, f&#34;experts.{ckpt_gate_up_proj_name}&#34;, &#34;w13&#34;),
            (
                &#34;experts.w13_weight_bias&#34;,
                f&#34;experts.{ckpt_gate_up_proj_bias_name}&#34;,
                &#34;w13&#34;,
            ),
            (&#34;experts.w2_weight&#34;, f&#34;experts.{ckpt_down_proj_name}&#34;, &#34;w2&#34;),
            (&#34;experts.w2_weight_bias&#34;, f&#34;experts.{ckpt_down_proj_bias_name}&#34;, &#34;w2&#34;),
            (
                &#34;experts.w13_weight_scale&#34;,
                f&#34;experts.{ckpt_gate_up_proj_scale_name}&#34;,
                &#34;w13&#34;,
            ),
            (&#34;experts.w2_weight_scale&#34;, f&#34;experts.{ckpt_down_proj_scale_name}&#34;, &#34;w2&#34;),
        ]

    @classmethod
    def make_expert_input_scale_params_mapping(
        cls,
        num_experts: int,
    ) -&gt; List[Tuple[str, str, int, str]]:
        # (param_name, weight_name, expert_id, shard_id)
        return [
            (
                &#34;experts.w13_&#34; if shard_id in [&#34;w1&#34;, &#34;w3&#34;] else &#34;experts.w2_&#34;,
                f&#34;experts.{expert_id}.{shard_id}.&#34;,
                expert_id,
                shard_id,
            )
            for expert_id in range(num_experts)
            for shard_id in [&#34;w1&#34;, &#34;w2&#34;, &#34;w3&#34;]
        ]

    def should_fuse_routed_scaling_factor_in_topk(self):
        return isinstance(self.quant_method, ModelOptNvFp4FusedMoEMethod) or (
            isinstance(self.quant_method, Fp8MoEMethod)
            and self.quant_method.use_cutlass_fused_experts_fp8
        )</code></pre>
</details>
<div class="desc"><p>FusedMoE layer for MoE models.</p>
<p>This layer contains both MergedColumnParallel weights (gate_up_proj /
w13) and RowParallelLinear weights (down_proj/ w2).</p>
<p>Note: Mixtral uses w1, w2, and w3 for gate, up, and down_proj. We
copy that naming convention here and handle any remapping in the
load_weights function in each model implementation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_experts</code></strong></dt>
<dd>Number of experts in the model</dd>
<dt><strong><code>top_k</code></strong></dt>
<dd>Number of experts selected for each token</dd>
<dt><strong><code>hidden_size</code></strong></dt>
<dd>Input hidden state size of the transformer</dd>
<dt><strong><code>intermediate_size</code></strong></dt>
<dd>Intermediate size of the experts</dd>
<dt><strong><code>params_dtype</code></strong></dt>
<dd>Data type for the parameters.</dd>
<dt><strong><code>reduce_results</code></strong></dt>
<dd>Whether to apply all_reduce on the output of the layer</dd>
<dt><strong><code>quant_config</code></strong></dt>
<dd>Quantization configuration.</dd>
<dt><strong><code>inplace</code></strong></dt>
<dd>suggestion to compute inplace (modify input activation).</dd>
</dl>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.moe.ep_moe.layer.EPMoE" href="../ep_moe/layer.html#sglang.srt.layers.moe.ep_moe.layer.EPMoE">EPMoE</a></li>
<li><a title="sglang.srt.layers.moe.fused_moe_triton.layer.FlashInferFP4MoE" href="layer.html#sglang.srt.layers.moe.fused_moe_triton.layer.FlashInferFP4MoE">FlashInferFP4MoE</a></li>
<li><a title="sglang.srt.layers.moe.fused_moe_triton.layer.FlashInferFusedMoE" href="layer.html#sglang.srt.layers.moe.fused_moe_triton.layer.FlashInferFusedMoE">FlashInferFusedMoE</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.layers.moe.fused_moe_triton.FusedMoE.make_expert_input_scale_params_mapping"><code class="name flex">
<span>def <span class="ident">make_expert_input_scale_params_mapping</span></span>(<span>num_experts: int) ‑> List[Tuple[str, str, int, str]]</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.fused_moe_triton.FusedMoE.make_expert_params_mapping"><code class="name flex">
<span>def <span class="ident">make_expert_params_mapping</span></span>(<span>ckpt_gate_proj_name: str,<br>ckpt_down_proj_name: str,<br>ckpt_up_proj_name: str,<br>num_experts: int) ‑> List[Tuple[str, str, int, str]]</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.fused_moe_triton.FusedMoE.make_expert_params_mapping_fused"><code class="name flex">
<span>def <span class="ident">make_expert_params_mapping_fused</span></span>(<span>ckpt_gate_up_proj_name: str,<br>ckpt_down_proj_name: str,<br>ckpt_gate_up_proj_bias_name: str,<br>ckpt_down_proj_bias_name: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.fused_moe_triton.FusedMoE.make_expert_params_mapping_fused_mxfp4"><code class="name flex">
<span>def <span class="ident">make_expert_params_mapping_fused_mxfp4</span></span>(<span>ckpt_gate_up_proj_name: str,<br>ckpt_down_proj_name: str,<br>ckpt_gate_up_proj_bias_name: str,<br>ckpt_down_proj_bias_name: str,<br>ckpt_gate_up_proj_scale_name: str,<br>ckpt_down_proj_scale_name: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.moe.fused_moe_triton.FusedMoE.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>hidden_states: torch.Tensor,<br>topk_output: <a title="sglang.srt.layers.moe.topk.TopKOutput" href="../topk.html#sglang.srt.layers.moe.topk.TopKOutput">TopKOutput</a>) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, hidden_states: torch.Tensor, topk_output: TopKOutput):
    origin_hidden_states_dim = hidden_states.shape[-1]
    assert self.quant_method is not None

    if self.moe_ep_size &gt; 1 and not self.enable_flashinfer_cutlass_moe:
        if self.expert_map_cpu is not None and self.expert_map_gpu is None:
            # If we are in EP mode, we need to move the expert map to GPU.
            self.expert_map_gpu = self.expert_map_cpu.to(device=&#34;cuda&#34;)

    if self.expert_map_gpu is not None:
        if TopKOutputChecker.format_is_standard(topk_output):
            topk_output = topk_output._replace(
                topk_ids=self.expert_map_gpu[topk_output.topk_ids]
            )
        elif TopKOutputChecker.format_is_triton_kernel(topk_output):
            raise NotImplementedError()

    # Matrix multiply.
    with use_symmetric_memory(get_tp_group()) as sm:

        final_hidden_states = self.quant_method.apply(
            layer=self,
            x=hidden_states,
            topk_output=topk_output,
            moe_runner_config=self.moe_runner_config,
        )
        sm.tag(final_hidden_states)

    final_hidden_states = final_hidden_states[
        ..., :origin_hidden_states_dim
    ].contiguous()

    if self.reduce_results and (self.moe_tp_size &gt; 1 or self.moe_ep_size &gt; 1):
        final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)

    return final_hidden_states</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
<dt id="sglang.srt.layers.moe.fused_moe_triton.FusedMoE.should_fuse_routed_scaling_factor_in_topk"><code class="name flex">
<span>def <span class="ident">should_fuse_routed_scaling_factor_in_topk</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def should_fuse_routed_scaling_factor_in_topk(self):
    return isinstance(self.quant_method, ModelOptNvFp4FusedMoEMethod) or (
        isinstance(self.quant_method, Fp8MoEMethod)
        and self.quant_method.use_cutlass_fused_experts_fp8
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.fused_moe_triton.FusedMoE.weight_loader"><code class="name flex">
<span>def <span class="ident">weight_loader</span></span>(<span>self,<br>param: torch.nn.parameter.Parameter,<br>loaded_weight: torch.Tensor,<br>weight_name: str,<br>shard_id: str,<br>expert_id: int | None) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def weight_loader(
    self,
    param: torch.nn.Parameter,
    loaded_weight: torch.Tensor,
    weight_name: str,
    shard_id: str,
    expert_id: Optional[int],
) -&gt; None:

    # if expert_id is None, then
    # all the experts are loaded at the same time
    if (
        not expert_id
        and self.quant_config is not None
        and self.quant_config.get_name() == &#34;mxfp4&#34;
        and self.quant_config.is_static_cfg()
    ):
        if &#34;bias&#34; in weight_name:
            dim1 = loaded_weight.shape[1]
            param.data[:, :dim1].copy_(loaded_weight)
        else:
            dim1 = loaded_weight.shape[1]
            dim2 = loaded_weight.shape[2]
            param.data[:, :dim1, :dim2].copy_(loaded_weight)
        return

    global_expert_location_metadata = get_global_expert_location_metadata()
    if global_expert_location_metadata is None:
        self._weight_loader_impl(
            param=param,
            loaded_weight=loaded_weight,
            weight_name=weight_name,
            shard_id=shard_id,
            expert_id=expert_id,
        )
        return

    if expert_id &gt;= self.num_experts - self.num_fused_shared_experts:
        # This is a shared expert.
        physical_expert_ids = [expert_id]
    else:
        physical_expert_ids = (
            global_expert_location_metadata.logical_to_all_physical(
                self.layer_id, expert_id
            )
        )

    for physical_expert_id in physical_expert_ids:
        self._weight_loader_physical(
            param=param,
            loaded_weight=loaded_weight,
            weight_name=weight_name,
            shard_id=shard_id,
            expert_id=physical_expert_id,
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.fused_moe_triton.FusedMoE.weight_loader_fused"><code class="name flex">
<span>def <span class="ident">weight_loader_fused</span></span>(<span>self,<br>param: torch.nn.parameter.Parameter,<br>loaded_weight: torch.Tensor,<br>weight_name: str,<br>shard_id: str) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def weight_loader_fused(
    self,
    param: torch.nn.Parameter,
    loaded_weight: torch.Tensor,
    weight_name: str,
    shard_id: str,
) -&gt; None:
    tp_rank = self.moe_tp_rank

    if (
        self.quant_config is not None
        and self.quant_config.get_name() == &#34;mxfp4&#34;
        and self.quant_config.is_static_cfg()
    ):
        if &#34;bias&#34; in weight_name:
            dim1 = loaded_weight.shape[1]
            param.data[:, :dim1].copy_(loaded_weight)
        elif &#34;scale&#34; in weight_name:
            param.data.copy_(loaded_weight)
        else:
            dim1 = loaded_weight.shape[1]
            dim2 = loaded_weight.shape[2]
            param.data[:, :dim1, :dim2].copy_(loaded_weight)
        return

    # compressed-tensors checkpoints with packed weights are stored flipped
    # TODO: check self.quant_method.quant_config.quant_format
    # against known CompressionFormat enum values that have this quality
    loaded_weight = (
        loaded_weight.t().contiguous()
        if (
            self.quant_method.__class__.__name__
            == &#34;CompressedTensorsWNA16MoEMethod&#34;
        )
        else loaded_weight
    )

    if shard_id not in (&#34;w13&#34;, &#34;w2&#34;):
        raise ValueError(f&#34;shard_id must be [&#39;w13&#39;,&#39;w2&#39;] but &#34; f&#34;got {shard_id}.&#34;)

    # Fetch the dim to shard the parameter/loaded weight
    # based on the shard id. This will be whatever
    # dimension intermediate_size is used.
    SHARD_ID_TO_SHARDED_DIM = {&#34;w13&#34;: 1, &#34;w2&#34;: 2}
    SHARD_ID_TO_SHARDED_DIM_TRANSPOSE = {&#34;w13&#34;: 2, &#34;w2&#34;: 1}

    expert_data = param.data
    is_bias = expert_data.dim() == 2

    # is_transposed: if the dim to shard the weight
    # should be flipped. Required by GPTQ, compressed-tensors
    # should be whatever dimension intermediate_size is
    is_transposed = getattr(param, &#34;is_transposed&#34;, False)

    if self.use_triton_kernels:
        is_transposed = True
    shard_dim = (
        SHARD_ID_TO_SHARDED_DIM[shard_id]
        if not is_transposed
        else SHARD_ID_TO_SHARDED_DIM_TRANSPOSE[shard_id]
    )

    # Case model weights
    if &#34;weight&#34; in weight_name:
        self._load_model_weight_or_group_weight_scale(
            shard_id=shard_id,
            shard_dim=shard_dim,
            loaded_weight=loaded_weight,
            expert_data=expert_data,
            tp_rank=tp_rank,
            is_bias=is_bias,
        )
        return
    else:
        logging.warning(
            f&#34;Unsupported weight_name {weight_name} for FusedMoE weight_loader_fused. Nothing is loaded.&#34;
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.layers.moe.fused_moe_triton.FusedMoeWeightScaleSupported"><code class="flex name class">
<span>class <span class="ident">FusedMoeWeightScaleSupported</span></span>
<span>(</span><span>*args, **kwds)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FusedMoeWeightScaleSupported(Enum):
    TENSOR = &#34;tensor&#34;
    CHANNEL = &#34;channel&#34;
    GROUP = &#34;group&#34;
    BLOCK = &#34;block&#34;</code></pre>
</details>
<div class="desc"><p>Create a collection of name/value pairs.</p>
<p>Example enumeration:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; class Color(Enum):
...     RED = 1
...     BLUE = 2
...     GREEN = 3
</code></pre>
<p>Access them by:</p>
<ul>
<li>attribute access:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color.RED
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<ul>
<li>value lookup:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color(1)
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<ul>
<li>name lookup:</li>
</ul>
<blockquote>
<blockquote>
<blockquote>
<p>Color['RED']
<Color.RED: 1></p>
</blockquote>
</blockquote>
</blockquote>
<p>Enumerations can be iterated over, and know how many members they have:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; len(Color)
3
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; list(Color)
[&lt;Color.RED: 1&gt;, &lt;Color.BLUE: 2&gt;, &lt;Color.GREEN: 3&gt;]
</code></pre>
<p>Methods can be added to enumerations, and members can have their own
attributes &ndash; see the documentation for details.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.layers.moe.fused_moe_triton.FusedMoeWeightScaleSupported.BLOCK"><code class="name">var <span class="ident">BLOCK</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.fused_moe_triton.FusedMoeWeightScaleSupported.CHANNEL"><code class="name">var <span class="ident">CHANNEL</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.fused_moe_triton.FusedMoeWeightScaleSupported.GROUP"><code class="name">var <span class="ident">GROUP</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.moe.fused_moe_triton.FusedMoeWeightScaleSupported.TENSOR"><code class="name">var <span class="ident">TENSOR</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.layers.moe" href="../index.html">sglang.srt.layers.moe</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.fused_moe" href="fused_moe.html">sglang.srt.layers.moe.fused_moe_triton.fused_moe</a></code></li>
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.layer" href="layer.html">sglang.srt.layers.moe.fused_moe_triton.layer</a></code></li>
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.triton_kernels_moe" href="triton_kernels_moe.html">sglang.srt.layers.moe.fused_moe_triton.triton_kernels_moe</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.fused_experts" href="#sglang.srt.layers.moe.fused_moe_triton.fused_experts">fused_experts</a></code></li>
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.get_config" href="#sglang.srt.layers.moe.fused_moe_triton.get_config">get_config</a></code></li>
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.get_config_file_name" href="#sglang.srt.layers.moe.fused_moe_triton.get_config_file_name">get_config_file_name</a></code></li>
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.moe_align_block_size" href="#sglang.srt.layers.moe.fused_moe_triton.moe_align_block_size">moe_align_block_size</a></code></li>
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.override_config" href="#sglang.srt.layers.moe.fused_moe_triton.override_config">override_config</a></code></li>
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.try_get_optimal_moe_config" href="#sglang.srt.layers.moe.fused_moe_triton.try_get_optimal_moe_config">try_get_optimal_moe_config</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.layers.moe.fused_moe_triton.FusedMoE" href="#sglang.srt.layers.moe.fused_moe_triton.FusedMoE">FusedMoE</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.FusedMoE.forward" href="#sglang.srt.layers.moe.fused_moe_triton.FusedMoE.forward">forward</a></code></li>
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.FusedMoE.make_expert_input_scale_params_mapping" href="#sglang.srt.layers.moe.fused_moe_triton.FusedMoE.make_expert_input_scale_params_mapping">make_expert_input_scale_params_mapping</a></code></li>
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.FusedMoE.make_expert_params_mapping" href="#sglang.srt.layers.moe.fused_moe_triton.FusedMoE.make_expert_params_mapping">make_expert_params_mapping</a></code></li>
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.FusedMoE.make_expert_params_mapping_fused" href="#sglang.srt.layers.moe.fused_moe_triton.FusedMoE.make_expert_params_mapping_fused">make_expert_params_mapping_fused</a></code></li>
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.FusedMoE.make_expert_params_mapping_fused_mxfp4" href="#sglang.srt.layers.moe.fused_moe_triton.FusedMoE.make_expert_params_mapping_fused_mxfp4">make_expert_params_mapping_fused_mxfp4</a></code></li>
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.FusedMoE.should_fuse_routed_scaling_factor_in_topk" href="#sglang.srt.layers.moe.fused_moe_triton.FusedMoE.should_fuse_routed_scaling_factor_in_topk">should_fuse_routed_scaling_factor_in_topk</a></code></li>
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.FusedMoE.weight_loader" href="#sglang.srt.layers.moe.fused_moe_triton.FusedMoE.weight_loader">weight_loader</a></code></li>
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.FusedMoE.weight_loader_fused" href="#sglang.srt.layers.moe.fused_moe_triton.FusedMoE.weight_loader_fused">weight_loader_fused</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.moe.fused_moe_triton.FusedMoeWeightScaleSupported" href="#sglang.srt.layers.moe.fused_moe_triton.FusedMoeWeightScaleSupported">FusedMoeWeightScaleSupported</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.FusedMoeWeightScaleSupported.BLOCK" href="#sglang.srt.layers.moe.fused_moe_triton.FusedMoeWeightScaleSupported.BLOCK">BLOCK</a></code></li>
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.FusedMoeWeightScaleSupported.CHANNEL" href="#sglang.srt.layers.moe.fused_moe_triton.FusedMoeWeightScaleSupported.CHANNEL">CHANNEL</a></code></li>
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.FusedMoeWeightScaleSupported.GROUP" href="#sglang.srt.layers.moe.fused_moe_triton.FusedMoeWeightScaleSupported.GROUP">GROUP</a></code></li>
<li><code><a title="sglang.srt.layers.moe.fused_moe_triton.FusedMoeWeightScaleSupported.TENSOR" href="#sglang.srt.layers.moe.fused_moe_triton.FusedMoeWeightScaleSupported.TENSOR">TENSOR</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
