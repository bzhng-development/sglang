<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.layers.attention.trtllm_mla_backend API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.layers.attention.trtllm_mla_backend</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend"><code class="flex name class">
<span>class <span class="ident">TRTLLMMLABackend</span></span>
<span>(</span><span>model_runner: ModelRunner,<br>skip_prefill: bool = False,<br>kv_indptr_buf: Optional[torch.Tensor] = None,<br>q_indptr_decode_buf: Optional[torch.Tensor] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TRTLLMMLABackend(FlashInferMLAAttnBackend):
    &#34;&#34;&#34;TRTLLM MLA attention kernel from flashinfer.&#34;&#34;&#34;

    def __init__(
        self,
        model_runner: ModelRunner,
        skip_prefill: bool = False,
        kv_indptr_buf: Optional[torch.Tensor] = None,
        q_indptr_decode_buf: Optional[torch.Tensor] = None,
    ):
        super().__init__(model_runner, skip_prefill, kv_indptr_buf, q_indptr_decode_buf)

        config = model_runner.model_config

        # Model parameters
        self.num_q_heads = config.num_attention_heads // get_attention_tp_size()
        self.num_kv_heads = config.get_num_kv_heads(get_attention_tp_size())
        self.num_local_heads = config.num_attention_heads // get_attention_tp_size()

        # MLA-specific dimensions
        self.kv_lora_rank = config.kv_lora_rank
        self.qk_nope_head_dim = config.qk_nope_head_dim
        self.qk_rope_head_dim = config.qk_rope_head_dim
        self.v_head_dim = config.v_head_dim
        self.kv_cache_dim = self.kv_lora_rank + self.qk_rope_head_dim

        # Runtime parameters
        self.scaling = config.scaling
        self.data_type = model_runner.kv_cache_dtype
        self.q_data_type = model_runner.dtype
        self.page_size = model_runner.page_size
        self.req_to_token = model_runner.req_to_token_pool.req_to_token

        # Workspace allocation
        self.workspace_size = DEFAULT_WORKSPACE_SIZE_MB * 1024 * 1024
        global global_zero_init_workspace_buffer
        if global_zero_init_workspace_buffer is None:
            global_zero_init_workspace_buffer = torch.zeros(
                self.workspace_size,
                dtype=torch.uint8,
                device=model_runner.device,
            )
        self.workspace_buffer = global_zero_init_workspace_buffer

        # CUDA graph state
        self.decode_cuda_graph_metadata = {}
        self.decode_cuda_graph_kv_indices = None
        self.forward_metadata: Union[TRTLLMMLADecodeMetadata, None] = None

    def _calc_padded_blocks(self, max_seq_len: int) -&gt; int:
        &#34;&#34;&#34;
        Calculate padded block count that satisfies both TRT-LLM and Triton constraints.

        Args:
            max_seq_len: Maximum sequence length in tokens

        Returns:
            Number of blocks padded to satisfy all constraints
        &#34;&#34;&#34;
        blocks = triton.cdiv(max_seq_len, self.page_size)

        # Apply dual constraints (take LCM to satisfy both):
        # 1. TRT-LLM: block_num % (128 / page_size) == 0
        # 2. Triton: page table builder uses 64-index bursts, needs multiple of 64
        trtllm_constraint = TRTLLM_BLOCK_CONSTRAINT // self.page_size
        constraint_lcm = math.lcm(trtllm_constraint, TRITON_PAD_NUM_PAGE_PER_BLOCK)

        if blocks % constraint_lcm != 0:
            blocks = triton.cdiv(blocks, constraint_lcm) * constraint_lcm
        return blocks

    def _create_block_kv_indices(
        self,
        batch_size: int,
        max_blocks: int,
        req_pool_indices: torch.Tensor,
        seq_lens: torch.Tensor,
        device: torch.device,
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;
        Create block KV indices tensor using Triton kernel.

        Args:
            batch_size: Batch size
            max_blocks: Maximum number of blocks per sequence
            req_pool_indices: Request pool indices
            seq_lens: Sequence lengths
            device: Target device

        Returns:
            Block KV indices tensor
        &#34;&#34;&#34;
        block_kv_indices = torch.full(
            (batch_size, max_blocks), -1, dtype=torch.int32, device=device
        )

        create_flashmla_kv_indices_triton[(batch_size,)](
            self.req_to_token,
            req_pool_indices,
            seq_lens,
            None,
            block_kv_indices,
            self.req_to_token.stride(0),
            max_blocks,
            NUM_PAGE_PER_BLOCK=TRITON_PAD_NUM_PAGE_PER_BLOCK,
            PAGED_SIZE=self.page_size,
        )

        return block_kv_indices

    def init_cuda_graph_state(
        self,
        max_bs: int,
        max_num_tokens: int,
        kv_indices_buf: Optional[torch.Tensor] = None,
    ):
        &#34;&#34;&#34;Initialize CUDA graph state for TRTLLM MLA.&#34;&#34;&#34;

        max_blocks_per_seq = self._calc_padded_blocks(self.max_context_len)

        self.decode_cuda_graph_kv_indices = torch.full(
            (max_bs, max_blocks_per_seq), -1, dtype=torch.int32, device=self.device
        )
        self.decode_cuda_graph_workspace = torch.empty(
            self.workspace_size, dtype=torch.int8, device=self.device
        )

        super().init_cuda_graph_state(max_bs, max_num_tokens, kv_indices_buf)

    def init_forward_metadata_capture_cuda_graph(
        self,
        bs: int,
        num_tokens: int,
        req_pool_indices: torch.Tensor,
        seq_lens: torch.Tensor,
        encoder_lens: Optional[torch.Tensor],
        forward_mode: ForwardMode,
        spec_info: Optional[SpecInfo],
    ):
        &#34;&#34;&#34;Initialize metadata for CUDA graph capture.&#34;&#34;&#34;

        # Delegate to parent for non-decode modes.
        if not forward_mode.is_decode_or_idle():
            return super().init_forward_metadata_capture_cuda_graph(
                bs,
                num_tokens,
                req_pool_indices,
                seq_lens,
                encoder_lens,
                forward_mode,
                spec_info,
            )

        # Custom fast-path for decode/idle.
        # Capture with full width so future longer sequences are safe during replay
        max_blocks_per_seq = self._calc_padded_blocks(self.max_context_len)
        block_kv_indices = self.decode_cuda_graph_kv_indices[:bs, :max_blocks_per_seq]

        create_flashmla_kv_indices_triton[(bs,)](
            self.req_to_token,
            req_pool_indices,
            seq_lens,
            None,
            block_kv_indices,
            self.req_to_token.stride(0),
            max_blocks_per_seq,
            NUM_PAGE_PER_BLOCK=TRITON_PAD_NUM_PAGE_PER_BLOCK,
            PAGED_SIZE=self.page_size,
        )

        # Record the true maximum sequence length for this capture batch so that
        # the kernel launch path (which requires an int not a tensor) can reuse
        # it safely during both capture and replay.
        max_seq_len_val = int(seq_lens.max().item())

        metadata = TRTLLMMLADecodeMetadata(
            self.decode_cuda_graph_workspace,
            block_kv_indices,
            max_seq_len_val,
        )
        self.decode_cuda_graph_metadata[bs] = metadata
        self.forward_metadata = metadata

    def init_forward_metadata_replay_cuda_graph(
        self,
        bs: int,
        req_pool_indices: torch.Tensor,
        seq_lens: torch.Tensor,
        seq_lens_sum: int,
        encoder_lens: Optional[torch.Tensor],
        forward_mode: ForwardMode,
        spec_info: Optional[SpecInfo],
        seq_lens_cpu: Optional[torch.Tensor],
    ):
        &#34;&#34;&#34;Replay CUDA graph with new inputs.&#34;&#34;&#34;
        # Delegate to parent for non-decode modes.
        if not forward_mode.is_decode_or_idle():
            return super().init_forward_metadata_replay_cuda_graph(
                bs,
                req_pool_indices,
                seq_lens,
                seq_lens_sum,
                encoder_lens,
                forward_mode,
                spec_info,
                seq_lens_cpu,
            )

        metadata = self.decode_cuda_graph_metadata[bs]

        # Update block indices for new sequences.
        create_flashmla_kv_indices_triton[(bs,)](
            self.req_to_token,
            req_pool_indices[:bs],
            seq_lens[:bs],
            None,
            metadata.block_kv_indices,
            self.req_to_token.stride(0),
            metadata.block_kv_indices.shape[1],
            NUM_PAGE_PER_BLOCK=TRITON_PAD_NUM_PAGE_PER_BLOCK,
            PAGED_SIZE=self.page_size,
        )

        # Update stored max_seq_len so subsequent kernel calls use the correct value
        # Prefer CPU tensor to avoid GPU synchronization when available.
        if seq_lens_cpu is not None:
            metadata.max_seq_len = int(seq_lens_cpu.max().item())
        else:
            metadata.max_seq_len = int(seq_lens.max().item())

    def get_cuda_graph_seq_len_fill_value(self) -&gt; int:
        &#34;&#34;&#34;Get the fill value for sequence lengths in CUDA graph.&#34;&#34;&#34;
        return 1

    def init_forward_metadata(self, forward_batch: ForwardBatch):
        &#34;&#34;&#34;Initialize the metadata for a forward pass.&#34;&#34;&#34;
        # Delegate to parent for non-decode modes.
        if not forward_batch.forward_mode.is_decode_or_idle():
            return super().init_forward_metadata(forward_batch)

        bs = forward_batch.batch_size

        # Get maximum sequence length.
        if getattr(forward_batch, &#34;seq_lens_cpu&#34;, None) is not None:
            max_seq = forward_batch.seq_lens_cpu.max().item()
        else:
            max_seq = forward_batch.seq_lens.max().item()

        max_seqlen_pad = self._calc_padded_blocks(max_seq)
        block_kv_indices = self._create_block_kv_indices(
            bs,
            max_seqlen_pad,
            forward_batch.req_pool_indices,
            forward_batch.seq_lens,
            forward_batch.seq_lens.device,
        )

        max_seq_len_val = int(max_seq)
        self.forward_metadata = TRTLLMMLADecodeMetadata(
            self.workspace_buffer, block_kv_indices, max_seq_len_val
        )
        forward_batch.decode_trtllm_mla_metadata = self.forward_metadata

    def quantize_and_rope_for_fp8(
        self,
        q_nope: torch.Tensor,
        q_rope: torch.Tensor,
        k_nope: torch.Tensor,
        k_rope: torch.Tensor,
        forward_batch: ForwardBatch,
        cos_sin_cache: torch.Tensor,
        is_neox: bool,
    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;Quantize and apply RoPE for FP8 attention path.

        This function handles the FP8 quantization and RoPE application for MLA attention.
        It takes separate query/key nope and rope components, applies RoPE to the rope parts,
        quantizes all components to FP8, and merges the query components into a single tensor.

        Args:
            q_nope: Query no-position-encoding component [seq_len, num_heads, kv_lora_rank]
                - expected dtype: torch.bfloat16
            q_rope: Query RoPE component [seq_len, num_heads, qk_rope_head_dim]
                - expected dtype: torch.bfloat16
            k_nope: Key no-position-encoding component [seq_len, num_heads, kv_lora_rank]
                - expected dtype: torch.bfloat16
            k_rope: Key RoPE component [seq_len, num_heads, qk_rope_head_dim]
                - expected dtype: torch.bfloat16
            forward_batch: Forward batch containing position information
            cos_sin_cache: Precomputed cosine/sine cache for RoPE
                - expected dtype: matches q_/k_ input dtype (torch.bfloat16)
            is_neox: Whether to use NeoX-style RoPE (interleaved) or GPT-style (half rotation)

        Returns:
            tuple: (merged_q_out, k_nope_out, k_rope_out) quantized to FP8
                - merged_q_out: [seq_len, num_heads, kv_lora_rank + qk_rope_head_dim], dtype=torch.float8_e4m3fn
                - k_nope_out:   [seq_len, num_heads, kv_lora_rank], dtype=torch.float8_e4m3fn
                - k_rope_out:   [seq_len, num_heads, qk_rope_head_dim], dtype=torch.float8_e4m3fn
        &#34;&#34;&#34;
        attn_dtype = torch.float8_e4m3fn
        q_len, num_heads = q_rope.shape[0], q_rope.shape[1]

        # Allocate output tensors with FP8 dtype
        # Query output will contain merged nope + rope components
        q_out = q_rope.new_empty(
            q_len,
            num_heads,
            self.kv_lora_rank + self.qk_rope_head_dim,
            dtype=attn_dtype,
        )

        # Key outputs maintain original shapes but with FP8 dtype
        k_rope_out = k_rope.new_empty(k_rope.shape, dtype=attn_dtype)
        k_nope_out = k_nope.new_empty(k_nope.shape, dtype=attn_dtype)

        # Apply RoPE and quantize all components in a single fused kernel call
        # This kernel handles:
        # 1. RoPE application to q_rope and k_rope using cos_sin_cache and positions
        # 2. Quantization of all components to FP8 format
        # 3. Output placement into pre-allocated tensors
        flashinfer.rope.mla_rope_quantize_fp8(
            q_rope=q_rope,
            k_rope=k_rope,
            q_nope=q_nope,
            k_nope=k_nope,
            cos_sin_cache=cos_sin_cache,
            pos_ids=forward_batch.positions,
            is_neox=is_neox,
            quantize_dtype=attn_dtype,
            # Output tensor slicing: q_out contains [nope_part, rope_part]
            q_rope_out=q_out[..., self.kv_lora_rank :],  # RoPE part goes to end
            k_rope_out=k_rope_out,
            q_nope_out=q_out[..., : self.kv_lora_rank],  # Nope part goes to beginning
            k_nope_out=k_nope_out,
            # Quantization scales (set to 1.0 for no additional scaling)
            quant_scale_q=1.0,
            quant_scale_kv=1.0,
        )

        return q_out, k_nope_out, k_rope_out

    def forward_decode(
        self,
        q: torch.Tensor,  # q_nope
        k: torch.Tensor,  # k_nope
        v: torch.Tensor,  # not used in this backend
        layer: RadixAttention,
        forward_batch: ForwardBatch,
        save_kv_cache: bool = True,
        q_rope: Optional[torch.Tensor] = None,
        k_rope: Optional[torch.Tensor] = None,
        cos_sin_cache: Optional[torch.Tensor] = None,
        is_neox: Optional[bool] = False,
    ) -&gt; torch.Tensor:
        &#34;&#34;&#34;Run forward for decode using TRTLLM MLA kernel.&#34;&#34;&#34;
        merge_query = q_rope is not None
        if self.data_type == torch.float8_e4m3fn:
            # For FP8 path, we quantize the query and rope parts and merge them into a single tensor
            # Note: rope application in deepseek_v2.py:forward_absorb_prepare is skipped for FP8 decode path of this trtllm_mla backend
            assert all(
                x is not None for x in [q_rope, k_rope, cos_sin_cache]
            ), &#34;For FP8 path and using flashinfer.rope.mla_rope_quantize we need all of q_rope, k_rope and cos_sin_cache to be not None.&#34;
            q, k, k_rope = self.quantize_and_rope_for_fp8(
                q,
                q_rope,
                k.squeeze(1),
                k_rope.squeeze(1),
                forward_batch,
                cos_sin_cache,
                is_neox,
            )
            merge_query = False

        # Save KV cache if requested
        if save_kv_cache:
            assert (
                k is not None and k_rope is not None
            ), &#34;For populating trtllm_mla kv cache, both k_nope and k_rope should be not None.&#34;
            forward_batch.token_to_kv_pool.set_mla_kv_buffer(
                layer, forward_batch.out_cache_loc, k, k_rope
            )

        # Prepare query tensor inline
        if merge_query:
            # For FP16 path, we merge the query and rope parts into a single tensor
            q_nope = q.view(-1, layer.tp_q_head_num, layer.v_head_dim)
            q_rope_reshaped = q_rope.view(
                -1, layer.tp_q_head_num, layer.head_dim - layer.v_head_dim
            )
            query = torch.cat([q_nope, q_rope_reshaped], dim=-1)
        else:
            # For FP8 path, we already have the query and rope parts merged because of the quantize_and_rope_for_fp8 function
            query = q.view(-1, layer.tp_q_head_num, layer.head_dim)

        # Ensure query has shape [bs, acc_q_len, num_q_heads, head_dim] when seq_len 1
        if query.dim() == 3:
            query = query.unsqueeze(1)

        # Prepare KV cache inline
        k_cache = forward_batch.token_to_kv_pool.get_key_buffer(layer.layer_id)
        kv_cache = k_cache.view(-1, self.page_size, self.kv_cache_dim).unsqueeze(1)

        # Get metadata
        metadata = (
            getattr(forward_batch, &#34;decode_trtllm_mla_metadata&#34;, None)
            or self.forward_metadata
        )

        # Scale computation for TRTLLM MLA kernel BMM1 operation:
        # The final BMM1 scale is computed as: q_scale * k_scale * softmax_scale
        # Scale components:
        # - q_scale: Query scaling factor (set to 1.0 for both FP16/FP8 paths)
        # - k_scale: Key scaling factor from model checkpoint (defaults to 1.0 if not available)
        # - softmax_scale: Attention softmax scaling = 1/sqrt(head_dim), pre-computed as layer.scaling
        # This unified approach works for both FP16 and FP8 quantized attention paths.
        q_scale = 1.0
        k_scale = (
            layer.k_scale_float
            if getattr(layer, &#34;k_scale_float&#34;, None) is not None
            else 1.0
        )

        bmm1_scale = q_scale * k_scale * layer.scaling

        # Call TRT-LLM kernel
        raw_out = flashinfer.decode.trtllm_batch_decode_with_kv_cache_mla(
            query=query,
            kv_cache=kv_cache,
            workspace_buffer=metadata.workspace,
            qk_nope_head_dim=self.qk_nope_head_dim,
            kv_lora_rank=self.kv_lora_rank,
            qk_rope_head_dim=self.qk_rope_head_dim,
            block_tables=metadata.block_kv_indices,
            seq_lens=forward_batch.seq_lens.to(torch.int32),
            max_seq_len=metadata.max_seq_len,
            bmm1_scale=bmm1_scale,
        )

        # Reshape output directly without slicing
        output = raw_out.view(-1, layer.tp_q_head_num * layer.v_head_dim)
        return output</code></pre>
</details>
<div class="desc"><p>TRTLLM MLA attention kernel from flashinfer.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.attention.flashinfer_mla_backend.FlashInferMLAAttnBackend" href="flashinfer_mla_backend.html#sglang.srt.layers.attention.flashinfer_mla_backend.FlashInferMLAAttnBackend">FlashInferMLAAttnBackend</a></li>
<li><a title="sglang.srt.layers.attention.base_attn_backend.AttentionBackend" href="base_attn_backend.html#sglang.srt.layers.attention.base_attn_backend.AttentionBackend">AttentionBackend</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.forward_decode"><code class="name flex">
<span>def <span class="ident">forward_decode</span></span>(<span>self,<br>q: torch.Tensor,<br>k: torch.Tensor,<br>v: torch.Tensor,<br>layer: RadixAttention,<br>forward_batch: ForwardBatch,<br>save_kv_cache: bool = True,<br>q_rope: Optional[torch.Tensor] = None,<br>k_rope: Optional[torch.Tensor] = None,<br>cos_sin_cache: Optional[torch.Tensor] = None,<br>is_neox: Optional[bool] = False) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_decode(
    self,
    q: torch.Tensor,  # q_nope
    k: torch.Tensor,  # k_nope
    v: torch.Tensor,  # not used in this backend
    layer: RadixAttention,
    forward_batch: ForwardBatch,
    save_kv_cache: bool = True,
    q_rope: Optional[torch.Tensor] = None,
    k_rope: Optional[torch.Tensor] = None,
    cos_sin_cache: Optional[torch.Tensor] = None,
    is_neox: Optional[bool] = False,
) -&gt; torch.Tensor:
    &#34;&#34;&#34;Run forward for decode using TRTLLM MLA kernel.&#34;&#34;&#34;
    merge_query = q_rope is not None
    if self.data_type == torch.float8_e4m3fn:
        # For FP8 path, we quantize the query and rope parts and merge them into a single tensor
        # Note: rope application in deepseek_v2.py:forward_absorb_prepare is skipped for FP8 decode path of this trtllm_mla backend
        assert all(
            x is not None for x in [q_rope, k_rope, cos_sin_cache]
        ), &#34;For FP8 path and using flashinfer.rope.mla_rope_quantize we need all of q_rope, k_rope and cos_sin_cache to be not None.&#34;
        q, k, k_rope = self.quantize_and_rope_for_fp8(
            q,
            q_rope,
            k.squeeze(1),
            k_rope.squeeze(1),
            forward_batch,
            cos_sin_cache,
            is_neox,
        )
        merge_query = False

    # Save KV cache if requested
    if save_kv_cache:
        assert (
            k is not None and k_rope is not None
        ), &#34;For populating trtllm_mla kv cache, both k_nope and k_rope should be not None.&#34;
        forward_batch.token_to_kv_pool.set_mla_kv_buffer(
            layer, forward_batch.out_cache_loc, k, k_rope
        )

    # Prepare query tensor inline
    if merge_query:
        # For FP16 path, we merge the query and rope parts into a single tensor
        q_nope = q.view(-1, layer.tp_q_head_num, layer.v_head_dim)
        q_rope_reshaped = q_rope.view(
            -1, layer.tp_q_head_num, layer.head_dim - layer.v_head_dim
        )
        query = torch.cat([q_nope, q_rope_reshaped], dim=-1)
    else:
        # For FP8 path, we already have the query and rope parts merged because of the quantize_and_rope_for_fp8 function
        query = q.view(-1, layer.tp_q_head_num, layer.head_dim)

    # Ensure query has shape [bs, acc_q_len, num_q_heads, head_dim] when seq_len 1
    if query.dim() == 3:
        query = query.unsqueeze(1)

    # Prepare KV cache inline
    k_cache = forward_batch.token_to_kv_pool.get_key_buffer(layer.layer_id)
    kv_cache = k_cache.view(-1, self.page_size, self.kv_cache_dim).unsqueeze(1)

    # Get metadata
    metadata = (
        getattr(forward_batch, &#34;decode_trtllm_mla_metadata&#34;, None)
        or self.forward_metadata
    )

    # Scale computation for TRTLLM MLA kernel BMM1 operation:
    # The final BMM1 scale is computed as: q_scale * k_scale * softmax_scale
    # Scale components:
    # - q_scale: Query scaling factor (set to 1.0 for both FP16/FP8 paths)
    # - k_scale: Key scaling factor from model checkpoint (defaults to 1.0 if not available)
    # - softmax_scale: Attention softmax scaling = 1/sqrt(head_dim), pre-computed as layer.scaling
    # This unified approach works for both FP16 and FP8 quantized attention paths.
    q_scale = 1.0
    k_scale = (
        layer.k_scale_float
        if getattr(layer, &#34;k_scale_float&#34;, None) is not None
        else 1.0
    )

    bmm1_scale = q_scale * k_scale * layer.scaling

    # Call TRT-LLM kernel
    raw_out = flashinfer.decode.trtllm_batch_decode_with_kv_cache_mla(
        query=query,
        kv_cache=kv_cache,
        workspace_buffer=metadata.workspace,
        qk_nope_head_dim=self.qk_nope_head_dim,
        kv_lora_rank=self.kv_lora_rank,
        qk_rope_head_dim=self.qk_rope_head_dim,
        block_tables=metadata.block_kv_indices,
        seq_lens=forward_batch.seq_lens.to(torch.int32),
        max_seq_len=metadata.max_seq_len,
        bmm1_scale=bmm1_scale,
    )

    # Reshape output directly without slicing
    output = raw_out.view(-1, layer.tp_q_head_num * layer.v_head_dim)
    return output</code></pre>
</details>
<div class="desc"><p>Run forward for decode using TRTLLM MLA kernel.</p></div>
</dd>
<dt id="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.get_cuda_graph_seq_len_fill_value"><code class="name flex">
<span>def <span class="ident">get_cuda_graph_seq_len_fill_value</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_cuda_graph_seq_len_fill_value(self) -&gt; int:
    &#34;&#34;&#34;Get the fill value for sequence lengths in CUDA graph.&#34;&#34;&#34;
    return 1</code></pre>
</details>
<div class="desc"><p>Get the fill value for sequence lengths in CUDA graph.</p></div>
</dd>
<dt id="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.init_cuda_graph_state"><code class="name flex">
<span>def <span class="ident">init_cuda_graph_state</span></span>(<span>self,<br>max_bs: int,<br>max_num_tokens: int,<br>kv_indices_buf: Optional[torch.Tensor] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_cuda_graph_state(
    self,
    max_bs: int,
    max_num_tokens: int,
    kv_indices_buf: Optional[torch.Tensor] = None,
):
    &#34;&#34;&#34;Initialize CUDA graph state for TRTLLM MLA.&#34;&#34;&#34;

    max_blocks_per_seq = self._calc_padded_blocks(self.max_context_len)

    self.decode_cuda_graph_kv_indices = torch.full(
        (max_bs, max_blocks_per_seq), -1, dtype=torch.int32, device=self.device
    )
    self.decode_cuda_graph_workspace = torch.empty(
        self.workspace_size, dtype=torch.int8, device=self.device
    )

    super().init_cuda_graph_state(max_bs, max_num_tokens, kv_indices_buf)</code></pre>
</details>
<div class="desc"><p>Initialize CUDA graph state for TRTLLM MLA.</p></div>
</dd>
<dt id="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.init_forward_metadata"><code class="name flex">
<span>def <span class="ident">init_forward_metadata</span></span>(<span>self, forward_batch: ForwardBatch)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_forward_metadata(self, forward_batch: ForwardBatch):
    &#34;&#34;&#34;Initialize the metadata for a forward pass.&#34;&#34;&#34;
    # Delegate to parent for non-decode modes.
    if not forward_batch.forward_mode.is_decode_or_idle():
        return super().init_forward_metadata(forward_batch)

    bs = forward_batch.batch_size

    # Get maximum sequence length.
    if getattr(forward_batch, &#34;seq_lens_cpu&#34;, None) is not None:
        max_seq = forward_batch.seq_lens_cpu.max().item()
    else:
        max_seq = forward_batch.seq_lens.max().item()

    max_seqlen_pad = self._calc_padded_blocks(max_seq)
    block_kv_indices = self._create_block_kv_indices(
        bs,
        max_seqlen_pad,
        forward_batch.req_pool_indices,
        forward_batch.seq_lens,
        forward_batch.seq_lens.device,
    )

    max_seq_len_val = int(max_seq)
    self.forward_metadata = TRTLLMMLADecodeMetadata(
        self.workspace_buffer, block_kv_indices, max_seq_len_val
    )
    forward_batch.decode_trtllm_mla_metadata = self.forward_metadata</code></pre>
</details>
<div class="desc"><p>Initialize the metadata for a forward pass.</p></div>
</dd>
<dt id="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.init_forward_metadata_capture_cuda_graph"><code class="name flex">
<span>def <span class="ident">init_forward_metadata_capture_cuda_graph</span></span>(<span>self,<br>bs: int,<br>num_tokens: int,<br>req_pool_indices: torch.Tensor,<br>seq_lens: torch.Tensor,<br>encoder_lens: Optional[torch.Tensor],<br>forward_mode: ForwardMode,<br>spec_info: Optional[SpecInfo])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_forward_metadata_capture_cuda_graph(
    self,
    bs: int,
    num_tokens: int,
    req_pool_indices: torch.Tensor,
    seq_lens: torch.Tensor,
    encoder_lens: Optional[torch.Tensor],
    forward_mode: ForwardMode,
    spec_info: Optional[SpecInfo],
):
    &#34;&#34;&#34;Initialize metadata for CUDA graph capture.&#34;&#34;&#34;

    # Delegate to parent for non-decode modes.
    if not forward_mode.is_decode_or_idle():
        return super().init_forward_metadata_capture_cuda_graph(
            bs,
            num_tokens,
            req_pool_indices,
            seq_lens,
            encoder_lens,
            forward_mode,
            spec_info,
        )

    # Custom fast-path for decode/idle.
    # Capture with full width so future longer sequences are safe during replay
    max_blocks_per_seq = self._calc_padded_blocks(self.max_context_len)
    block_kv_indices = self.decode_cuda_graph_kv_indices[:bs, :max_blocks_per_seq]

    create_flashmla_kv_indices_triton[(bs,)](
        self.req_to_token,
        req_pool_indices,
        seq_lens,
        None,
        block_kv_indices,
        self.req_to_token.stride(0),
        max_blocks_per_seq,
        NUM_PAGE_PER_BLOCK=TRITON_PAD_NUM_PAGE_PER_BLOCK,
        PAGED_SIZE=self.page_size,
    )

    # Record the true maximum sequence length for this capture batch so that
    # the kernel launch path (which requires an int not a tensor) can reuse
    # it safely during both capture and replay.
    max_seq_len_val = int(seq_lens.max().item())

    metadata = TRTLLMMLADecodeMetadata(
        self.decode_cuda_graph_workspace,
        block_kv_indices,
        max_seq_len_val,
    )
    self.decode_cuda_graph_metadata[bs] = metadata
    self.forward_metadata = metadata</code></pre>
</details>
<div class="desc"><p>Initialize metadata for CUDA graph capture.</p></div>
</dd>
<dt id="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.init_forward_metadata_replay_cuda_graph"><code class="name flex">
<span>def <span class="ident">init_forward_metadata_replay_cuda_graph</span></span>(<span>self,<br>bs: int,<br>req_pool_indices: torch.Tensor,<br>seq_lens: torch.Tensor,<br>seq_lens_sum: int,<br>encoder_lens: Optional[torch.Tensor],<br>forward_mode: ForwardMode,<br>spec_info: Optional[SpecInfo],<br>seq_lens_cpu: Optional[torch.Tensor])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_forward_metadata_replay_cuda_graph(
    self,
    bs: int,
    req_pool_indices: torch.Tensor,
    seq_lens: torch.Tensor,
    seq_lens_sum: int,
    encoder_lens: Optional[torch.Tensor],
    forward_mode: ForwardMode,
    spec_info: Optional[SpecInfo],
    seq_lens_cpu: Optional[torch.Tensor],
):
    &#34;&#34;&#34;Replay CUDA graph with new inputs.&#34;&#34;&#34;
    # Delegate to parent for non-decode modes.
    if not forward_mode.is_decode_or_idle():
        return super().init_forward_metadata_replay_cuda_graph(
            bs,
            req_pool_indices,
            seq_lens,
            seq_lens_sum,
            encoder_lens,
            forward_mode,
            spec_info,
            seq_lens_cpu,
        )

    metadata = self.decode_cuda_graph_metadata[bs]

    # Update block indices for new sequences.
    create_flashmla_kv_indices_triton[(bs,)](
        self.req_to_token,
        req_pool_indices[:bs],
        seq_lens[:bs],
        None,
        metadata.block_kv_indices,
        self.req_to_token.stride(0),
        metadata.block_kv_indices.shape[1],
        NUM_PAGE_PER_BLOCK=TRITON_PAD_NUM_PAGE_PER_BLOCK,
        PAGED_SIZE=self.page_size,
    )

    # Update stored max_seq_len so subsequent kernel calls use the correct value
    # Prefer CPU tensor to avoid GPU synchronization when available.
    if seq_lens_cpu is not None:
        metadata.max_seq_len = int(seq_lens_cpu.max().item())
    else:
        metadata.max_seq_len = int(seq_lens.max().item())</code></pre>
</details>
<div class="desc"><p>Replay CUDA graph with new inputs.</p></div>
</dd>
<dt id="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.quantize_and_rope_for_fp8"><code class="name flex">
<span>def <span class="ident">quantize_and_rope_for_fp8</span></span>(<span>self,<br>q_nope: torch.Tensor,<br>q_rope: torch.Tensor,<br>k_nope: torch.Tensor,<br>k_rope: torch.Tensor,<br>forward_batch: ForwardBatch,<br>cos_sin_cache: torch.Tensor,<br>is_neox: bool) ‑> tuple[torch.Tensor, torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def quantize_and_rope_for_fp8(
    self,
    q_nope: torch.Tensor,
    q_rope: torch.Tensor,
    k_nope: torch.Tensor,
    k_rope: torch.Tensor,
    forward_batch: ForwardBatch,
    cos_sin_cache: torch.Tensor,
    is_neox: bool,
) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;Quantize and apply RoPE for FP8 attention path.

    This function handles the FP8 quantization and RoPE application for MLA attention.
    It takes separate query/key nope and rope components, applies RoPE to the rope parts,
    quantizes all components to FP8, and merges the query components into a single tensor.

    Args:
        q_nope: Query no-position-encoding component [seq_len, num_heads, kv_lora_rank]
            - expected dtype: torch.bfloat16
        q_rope: Query RoPE component [seq_len, num_heads, qk_rope_head_dim]
            - expected dtype: torch.bfloat16
        k_nope: Key no-position-encoding component [seq_len, num_heads, kv_lora_rank]
            - expected dtype: torch.bfloat16
        k_rope: Key RoPE component [seq_len, num_heads, qk_rope_head_dim]
            - expected dtype: torch.bfloat16
        forward_batch: Forward batch containing position information
        cos_sin_cache: Precomputed cosine/sine cache for RoPE
            - expected dtype: matches q_/k_ input dtype (torch.bfloat16)
        is_neox: Whether to use NeoX-style RoPE (interleaved) or GPT-style (half rotation)

    Returns:
        tuple: (merged_q_out, k_nope_out, k_rope_out) quantized to FP8
            - merged_q_out: [seq_len, num_heads, kv_lora_rank + qk_rope_head_dim], dtype=torch.float8_e4m3fn
            - k_nope_out:   [seq_len, num_heads, kv_lora_rank], dtype=torch.float8_e4m3fn
            - k_rope_out:   [seq_len, num_heads, qk_rope_head_dim], dtype=torch.float8_e4m3fn
    &#34;&#34;&#34;
    attn_dtype = torch.float8_e4m3fn
    q_len, num_heads = q_rope.shape[0], q_rope.shape[1]

    # Allocate output tensors with FP8 dtype
    # Query output will contain merged nope + rope components
    q_out = q_rope.new_empty(
        q_len,
        num_heads,
        self.kv_lora_rank + self.qk_rope_head_dim,
        dtype=attn_dtype,
    )

    # Key outputs maintain original shapes but with FP8 dtype
    k_rope_out = k_rope.new_empty(k_rope.shape, dtype=attn_dtype)
    k_nope_out = k_nope.new_empty(k_nope.shape, dtype=attn_dtype)

    # Apply RoPE and quantize all components in a single fused kernel call
    # This kernel handles:
    # 1. RoPE application to q_rope and k_rope using cos_sin_cache and positions
    # 2. Quantization of all components to FP8 format
    # 3. Output placement into pre-allocated tensors
    flashinfer.rope.mla_rope_quantize_fp8(
        q_rope=q_rope,
        k_rope=k_rope,
        q_nope=q_nope,
        k_nope=k_nope,
        cos_sin_cache=cos_sin_cache,
        pos_ids=forward_batch.positions,
        is_neox=is_neox,
        quantize_dtype=attn_dtype,
        # Output tensor slicing: q_out contains [nope_part, rope_part]
        q_rope_out=q_out[..., self.kv_lora_rank :],  # RoPE part goes to end
        k_rope_out=k_rope_out,
        q_nope_out=q_out[..., : self.kv_lora_rank],  # Nope part goes to beginning
        k_nope_out=k_nope_out,
        # Quantization scales (set to 1.0 for no additional scaling)
        quant_scale_q=1.0,
        quant_scale_kv=1.0,
    )

    return q_out, k_nope_out, k_rope_out</code></pre>
</details>
<div class="desc"><p>Quantize and apply RoPE for FP8 attention path.</p>
<p>This function handles the FP8 quantization and RoPE application for MLA attention.
It takes separate query/key nope and rope components, applies RoPE to the rope parts,
quantizes all components to FP8, and merges the query components into a single tensor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>q_nope</code></strong></dt>
<dd>Query no-position-encoding component [seq_len, num_heads, kv_lora_rank]
- expected dtype: torch.bfloat16</dd>
<dt><strong><code>q_rope</code></strong></dt>
<dd>Query RoPE component [seq_len, num_heads, qk_rope_head_dim]
- expected dtype: torch.bfloat16</dd>
<dt><strong><code>k_nope</code></strong></dt>
<dd>Key no-position-encoding component [seq_len, num_heads, kv_lora_rank]
- expected dtype: torch.bfloat16</dd>
<dt><strong><code>k_rope</code></strong></dt>
<dd>Key RoPE component [seq_len, num_heads, qk_rope_head_dim]
- expected dtype: torch.bfloat16</dd>
<dt><strong><code>forward_batch</code></strong></dt>
<dd>Forward batch containing position information</dd>
<dt><strong><code>cos_sin_cache</code></strong></dt>
<dd>Precomputed cosine/sine cache for RoPE
- expected dtype: matches q_/k_ input dtype (torch.bfloat16)</dd>
<dt><strong><code>is_neox</code></strong></dt>
<dd>Whether to use NeoX-style RoPE (interleaved) or GPT-style (half rotation)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>(merged_q_out, k_nope_out, k_rope_out) quantized to FP8
- merged_q_out: [seq_len, num_heads, kv_lora_rank + qk_rope_head_dim], dtype=torch.float8_e4m3fn
- k_nope_out:
[seq_len, num_heads, kv_lora_rank], dtype=torch.float8_e4m3fn
- k_rope_out:
[seq_len, num_heads, qk_rope_head_dim], dtype=torch.float8_e4m3fn</dd>
</dl></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.layers.attention.flashinfer_mla_backend.FlashInferMLAAttnBackend" href="flashinfer_mla_backend.html#sglang.srt.layers.attention.flashinfer_mla_backend.FlashInferMLAAttnBackend">FlashInferMLAAttnBackend</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.layers.attention.flashinfer_mla_backend.FlashInferMLAAttnBackend.forward" href="base_attn_backend.html#sglang.srt.layers.attention.base_attn_backend.AttentionBackend.forward">forward</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashinfer_mla_backend.FlashInferMLAAttnBackend.forward_extend" href="base_attn_backend.html#sglang.srt.layers.attention.base_attn_backend.AttentionBackend.forward_extend">forward_extend</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashinfer_mla_backend.FlashInferMLAAttnBackend.init_mha_chunk_metadata" href="flashinfer_mla_backend.html#sglang.srt.layers.attention.flashinfer_mla_backend.FlashInferMLAAttnBackend.init_mha_chunk_metadata">init_mha_chunk_metadata</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashinfer_mla_backend.FlashInferMLAAttnBackend.support_triton" href="base_attn_backend.html#sglang.srt.layers.attention.base_attn_backend.AttentionBackend.support_triton">support_triton</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLADecodeMetadata"><code class="flex name class">
<span>class <span class="ident">TRTLLMMLADecodeMetadata</span></span>
<span>(</span><span>workspace: Optional[torch.Tensor] = None,<br>block_kv_indices: Optional[torch.Tensor] = None,<br>max_seq_len: Optional[int] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class TRTLLMMLADecodeMetadata:
    &#34;&#34;&#34;Metadata for TRTLLM MLA decode operations.&#34;&#34;&#34;

    workspace: Optional[torch.Tensor] = None
    block_kv_indices: Optional[torch.Tensor] = None
    max_seq_len: Optional[int] = None</code></pre>
</details>
<div class="desc"><p>Metadata for TRTLLM MLA decode operations.</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLADecodeMetadata.block_kv_indices"><code class="name">var <span class="ident">block_kv_indices</span> : torch.Tensor | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLADecodeMetadata.max_seq_len"><code class="name">var <span class="ident">max_seq_len</span> : int | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLADecodeMetadata.workspace"><code class="name">var <span class="ident">workspace</span> : torch.Tensor | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLAMultiStepDraftBackend"><code class="flex name class">
<span>class <span class="ident">TRTLLMMLAMultiStepDraftBackend</span></span>
<span>(</span><span>model_runner: "'ModelRunner'", topk: int, speculative_num_steps: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TRTLLMMLAMultiStepDraftBackend(FlashInferMLAMultiStepDraftBackend):
    &#34;&#34;&#34;Multi-step draft backend for TRT-LLM MLA used by EAGLE.&#34;&#34;&#34;

    def __init__(
        self, model_runner: &#34;ModelRunner&#34;, topk: int, speculative_num_steps: int
    ):
        super().__init__(model_runner, topk, speculative_num_steps)

        for i in range(self.speculative_num_steps):
            self.attn_backends[i] = TRTLLMMLABackend(
                model_runner,
                skip_prefill=True,
                kv_indptr_buf=self.kv_indptr[i],
                q_indptr_decode_buf=self.q_indptr_decode,
            )</code></pre>
</details>
<div class="desc"><p>Multi-step draft backend for TRT-LLM MLA used by EAGLE.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.attention.flashinfer_mla_backend.FlashInferMLAMultiStepDraftBackend" href="flashinfer_mla_backend.html#sglang.srt.layers.attention.flashinfer_mla_backend.FlashInferMLAMultiStepDraftBackend">FlashInferMLAMultiStepDraftBackend</a></li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.layers.attention" href="index.html">sglang.srt.layers.attention</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend" href="#sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend">TRTLLMMLABackend</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.forward_decode" href="#sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.forward_decode">forward_decode</a></code></li>
<li><code><a title="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.get_cuda_graph_seq_len_fill_value" href="#sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.get_cuda_graph_seq_len_fill_value">get_cuda_graph_seq_len_fill_value</a></code></li>
<li><code><a title="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.init_cuda_graph_state" href="#sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.init_cuda_graph_state">init_cuda_graph_state</a></code></li>
<li><code><a title="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.init_forward_metadata" href="#sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.init_forward_metadata">init_forward_metadata</a></code></li>
<li><code><a title="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.init_forward_metadata_capture_cuda_graph" href="#sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.init_forward_metadata_capture_cuda_graph">init_forward_metadata_capture_cuda_graph</a></code></li>
<li><code><a title="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.init_forward_metadata_replay_cuda_graph" href="#sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.init_forward_metadata_replay_cuda_graph">init_forward_metadata_replay_cuda_graph</a></code></li>
<li><code><a title="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.quantize_and_rope_for_fp8" href="#sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLABackend.quantize_and_rope_for_fp8">quantize_and_rope_for_fp8</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLADecodeMetadata" href="#sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLADecodeMetadata">TRTLLMMLADecodeMetadata</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLADecodeMetadata.block_kv_indices" href="#sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLADecodeMetadata.block_kv_indices">block_kv_indices</a></code></li>
<li><code><a title="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLADecodeMetadata.max_seq_len" href="#sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLADecodeMetadata.max_seq_len">max_seq_len</a></code></li>
<li><code><a title="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLADecodeMetadata.workspace" href="#sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLADecodeMetadata.workspace">workspace</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLAMultiStepDraftBackend" href="#sglang.srt.layers.attention.trtllm_mla_backend.TRTLLMMLAMultiStepDraftBackend">TRTLLMMLAMultiStepDraftBackend</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
