<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.layers.attention.flashattention_backend API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.layers.attention.flashattention_backend</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.layers.attention.flashattention_backend.cdiv"><code class="name flex">
<span>def <span class="ident">cdiv</span></span>(<span>a: int, b: int) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cdiv(a: int, b: int) -&gt; int:
    &#34;&#34;&#34;Ceiling division.&#34;&#34;&#34;
    return -(a // -b)</code></pre>
</details>
<div class="desc"><p>Ceiling division.</p></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.make_local_attention_virtual_batches"><code class="name flex">
<span>def <span class="ident">make_local_attention_virtual_batches</span></span>(<span>attn_chunk_size: int,<br>query_start_loc_np: np.ndarray,<br>seq_lens_np: np.ndarray,<br>block_table: torch.Tensor,<br>page_size: int = 0) ‑> tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_local_attention_virtual_batches(
    attn_chunk_size: int,
    query_start_loc_np: np.ndarray,
    seq_lens_np: np.ndarray,
    block_table: torch.Tensor,
    page_size: int = 0,
) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, torch.Tensor]:
    &#34;&#34;&#34;
    Take in `query_start_loc_np` and `seq_lens_np` and break the sequences into
    local attention blocks, where each block is passed to the attention kernel
    as an independent local (&#34;virtual&#34;) batch item.

    Args:
        attn_chunk_size: Size of local attention chunks
        query_start_loc_np: Cumulative sum of query lengths (numpy array)
        seq_lens_np: Sequence lengths (numpy array)
        block_table: Block table for KV cache
        page_size: Size of each page in the KV cache

    Returns:
        seqlens_q_local: Query sequence lengths for local attention
        cu_seqlens_q_local: Cumulative sum of query sequence lengths for local attention
        seqlens_k_local: Key sequence lengths for local attention
        block_table_local: Block table for local attention
    &#34;&#34;&#34;
    # Adjust attention_chunk_size based on the actual sequence length
    # to avoid index out of bounds errors
    max_seq_len = seq_lens_np.max()
    effective_chunk_size = min(attn_chunk_size, max_seq_len)
    # Make sure effective_chunk_size is divisible by page_size
    effective_chunk_size = (effective_chunk_size // page_size) * page_size
    if effective_chunk_size &lt; page_size:
        effective_chunk_size = page_size
    attn_chunk_size = effective_chunk_size

    q_seqlens = query_start_loc_np[1:] - query_start_loc_np[:-1]
    actual_batch_size = seq_lens_np.shape[0]

    # Handle if we are starting in the middle of a local attention block,
    #  we assume q_seqlens &gt; 0 (for all elements), for each batch idx we compute
    #  the number of tokens that are not in the first local attention block and
    #  then we can simply use a cdiv for the rest.
    # For example if we have:
    #   attn_chunk_size = 4
    #   q_seqlens = [4, 10, 5]
    #   k_seqlens = [6, 17, 9]
    # Then we would get:
    #   new_tokens_in_first_block = [2, 1, 4]
    #   local_blocks = [2, 4, 2]
    q_tokens_in_first_block = np.minimum(
        attn_chunk_size - ((seq_lens_np - q_seqlens) % attn_chunk_size), q_seqlens
    ).astype(np.int32)
    tokens_in_last_block = attn_chunk_size + (seq_lens_np % -attn_chunk_size)
    local_blocks = 1 + cdiv(q_seqlens - q_tokens_in_first_block, attn_chunk_size)

    # Once we know the number of local blocks we can compute the request spans
    #  for each batch idx, we can figure out the number of &#34;virtual&#34; requests we
    #  have to make,
    # For the above example we would get:
    #   seqlens_q_local = [2, 2, 1, 4, 4, 1, 4, 1]
    #
    # First Get batched arange. (E.g., [2, 4, 2] -&gt; [0, 1, 0, 1, 2, 3, 0, 1])
    #   (TODO: max a utility to share this code with _prepare_inputs)
    # arange step 1. [2, 4, 2] -&gt; [2, 6, 8]
    cu_num_blocks = np.cumsum(local_blocks)
    virtual_batches = cu_num_blocks[-1]
    # arange step 2. [2, 6, 8] -&gt; [0, 0, 2, 2, 2, 2, 6, 6]
    block_offsets = np.repeat(cu_num_blocks - local_blocks, local_blocks)
    # arange step 3. [0, 1, 0, 1, 2, 3, 0, 1]
    arange = np.arange(virtual_batches, dtype=np.int32) - block_offsets
    # also compute reverse arange (i.e. [1, 0, 3, 2, 1, 0, 1, 0])
    rarange = np.repeat(local_blocks, local_blocks) - arange - 1
    # Then we can compute the seqlens_q_local, handling the fact that the
    #  first and last blocks could be partial
    seqlens_q_local = np.repeat(q_seqlens - q_tokens_in_first_block, local_blocks)
    # set the first block since this may be a partial block
    seqlens_q_local[arange == 0] = q_tokens_in_first_block
    # set the remaining blocks
    seqlens_q_local[arange &gt; 0] = np.minimum(
        seqlens_q_local - attn_chunk_size * (arange - 1), attn_chunk_size
    )[arange &gt; 0]

    # convert from q_seqlens to cu_seqlens_q
    cu_seqlens_q_local = np.pad(np.cumsum(seqlens_q_local), (1, 0)).astype(np.int32)

    # compute the seqlens_k_local,
    #  basically a full local attention block for all but the last block in each
    #  batch
    # For our example this will be:
    #   seqlens_k_local = [4, 2, 4, 4, 4, 1, 4, 1]
    seqlens_k_local = np.full(cu_num_blocks[-1], attn_chunk_size, dtype=np.int32)
    seqlens_k_local[cu_num_blocks - 1] = tokens_in_last_block

    k_seqstarts_absolute = np.repeat(seq_lens_np, local_blocks) - (
        rarange * attn_chunk_size + np.repeat(tokens_in_last_block, local_blocks)
    )
    # For the example the local attention blocks start at:
    #                           _b0_  _____b1_____  _b2_
    #   k_seqstarts_absolute = [0, 4, 4, 8, 12, 16, 4, 8]
    block_starts = k_seqstarts_absolute // page_size

    assert attn_chunk_size % page_size == 0, (
        f&#34;attn_chunk_size {attn_chunk_size} is not &#34;
        f&#34;divisible by page_size {page_size}&#34;
    )
    pages_per_local_batch = attn_chunk_size // page_size

    # Create a block_table for the local attention blocks
    # For out example if we have a block-table like (assuming page_size=2):
    #   block_table = [
    #     [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],  &lt; batch 0
    #     [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],  &lt; batch 1
    #     [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],  &lt; batch 2
    #   ]
    # Then for the local batches we would want a block-table like
    #   block_table_local = [
    #     [  0,  1 ], &lt; local-batch 0, (batch 0, starting from k[0])
    #     [  2,  3 ], &lt; local-batch 1, (batch 0, starting from k[4])
    #     [ 12, 13 ], &lt; local-batch 2, (batch 1, starting from k[4])
    #     [ 14, 15 ], &lt; local-batch 3, (batch 1, starting from k[8])
    #     [ 16, 17 ], &lt; local-batch 4, (batch 1, starting from k[12])
    #     [ 18, 19 ], &lt; local-batch 5, (batch 1, starting from k[16])
    #     [ 22, 23 ], &lt; local-batch 6, (batch 2, starting from k[4])
    #     [ 24, 25 ], &lt; local-batch 7, (batch 2, starting from k[8])
    #   ]
    block_indices = np.broadcast_to(
        np.arange(pages_per_local_batch, dtype=np.int32),
        (virtual_batches, pages_per_local_batch),
    ) + np.expand_dims(block_starts, axis=1)
    # Ensure block_indices doesn&#39;t exceed block_table dimensions
    # This is a critical safety check that prevents index out of bounds errors
    # when dealing with large sequences (&gt;8192 tokens) or when the block_table
    # dimensions are smaller than what would be needed for the full attention chunk size.
    block_indices = block_indices.flatten().clip(max=block_table.shape[1] - 1)
    batch_indices = np.repeat(
        np.arange(actual_batch_size, dtype=np.int32),
        local_blocks * pages_per_local_batch,
    )
    block_table_local = block_table[batch_indices, block_indices].view(
        virtual_batches, -1
    )

    return seqlens_q_local, cu_seqlens_q_local, seqlens_k_local, block_table_local</code></pre>
</details>
<div class="desc"><p>Take in <code>query_start_loc_np</code> and <code>seq_lens_np</code> and break the sequences into
local attention blocks, where each block is passed to the attention kernel
as an independent local ("virtual") batch item.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>attn_chunk_size</code></strong></dt>
<dd>Size of local attention chunks</dd>
<dt><strong><code>query_start_loc_np</code></strong></dt>
<dd>Cumulative sum of query lengths (numpy array)</dd>
<dt><strong><code>seq_lens_np</code></strong></dt>
<dd>Sequence lengths (numpy array)</dd>
<dt><strong><code>block_table</code></strong></dt>
<dd>Block table for KV cache</dd>
<dt><strong><code>page_size</code></strong></dt>
<dd>Size of each page in the KV cache</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>seqlens_q_local</code></dt>
<dd>Query sequence lengths for local attention</dd>
<dt><code>cu_seqlens_q_local</code></dt>
<dd>Cumulative sum of query sequence lengths for local attention</dd>
<dt><code>seqlens_k_local</code></dt>
<dd>Key sequence lengths for local attention</dd>
<dt><code>block_table_local</code></dt>
<dd>Block table for local attention</dd>
</dl></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.merge_state_v2_wrapper"><code class="name flex">
<span>def <span class="ident">merge_state_v2_wrapper</span></span>(<span>o, s_a, o_exp, s_b)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch._dynamo.disable()
def merge_state_v2_wrapper(o, s_a, o_exp, s_b):
    return merge_state_v2(o, s_a, o_exp, s_b)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.normal_decode_set_metadata"><code class="name flex">
<span>def <span class="ident">normal_decode_set_metadata</span></span>(<span>cache_seqlens_int32: torch.Tensor,<br>cu_seqlens_k: torch.Tensor,<br>page_table: torch.Tensor,<br>req_to_token: torch.Tensor,<br>req_pool_indices: torch.Tensor,<br>strided_indices: torch.Tensor,<br>max_seq_pages: torch.Tensor,<br>seq_lens: torch.Tensor,<br>seq_len_delta: int,<br>page_size: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normal_decode_set_metadata(
    cache_seqlens_int32: torch.Tensor,
    cu_seqlens_k: torch.Tensor,
    page_table: torch.Tensor,
    req_to_token: torch.Tensor,
    req_pool_indices: torch.Tensor,
    strided_indices: torch.Tensor,
    max_seq_pages: torch.Tensor,
    seq_lens: torch.Tensor,
    seq_len_delta: int,
    page_size: int,
):
    cache_seqlens_int32.copy_(seq_lens + seq_len_delta)
    cu_seqlens_k[1:].copy_(torch.cumsum(cache_seqlens_int32, dim=0, dtype=torch.int32))
    page_indices = req_to_token[
        req_pool_indices[:, None],
        strided_indices[:max_seq_pages][None, :],
    ]
    page_table[:, :max_seq_pages].copy_(page_indices // page_size)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.prepare_swa_spec_page_table_triton"><code class="name flex">
<span>def <span class="ident">prepare_swa_spec_page_table_triton</span></span>(<span>page_table_dst: torch.Tensor,<br>page_table_a: torch.Tensor,<br>page_table_b: torch.Tensor,<br>seq_len_a: torch.Tensor,<br>seq_len_b: torch.Tensor,<br>speculative_num_draft_tokens: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_swa_spec_page_table_triton(
    page_table_dst: torch.Tensor,
    page_table_a: torch.Tensor,
    page_table_b: torch.Tensor,  # expand page table
    seq_len_a: torch.Tensor,
    seq_len_b: torch.Tensor,  # expand seq lens
    speculative_num_draft_tokens: int,
):
    # concat page_table and expand page_table by kv seq length
    bs = seq_len_a.numel()
    bs_expand = seq_len_b.numel()
    assert bs_expand == bs * speculative_num_draft_tokens

    LEN_A = page_table_a.shape[1]
    LEN_B = page_table_b.shape[1]
    LEN_OUT = LEN_A + LEN_B
    REPEAT_STEP = speculative_num_draft_tokens
    BLOCK_N = 256

    grid = (bs_expand, triton.cdiv(LEN_OUT, BLOCK_N))
    _prepare_swa_spec_page_table_kernel[grid](
        page_table_dst,
        page_table_a,
        page_table_b,
        seq_len_a,
        seq_len_b,
        page_table_dst.stride(0),
        page_table_dst.stride(1),
        page_table_a.stride(0),
        page_table_a.stride(1),
        page_table_b.stride(0),
        page_table_b.stride(1),
        LEN_A=LEN_A,
        LEN_B=LEN_B,
        REPEAT_STEP=REPEAT_STEP,
        BLOCK_N=BLOCK_N,
        num_warps=4,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend"><code class="flex name class">
<span>class <span class="ident">FlashAttentionBackend</span></span>
<span>(</span><span>model_runner: ModelRunner,<br>skip_prefill: bool = False,<br>speculative_step_id=0,<br>topk=0,<br>speculative_num_steps=0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FlashAttentionBackend(AttentionBackend):
    &#34;&#34;&#34;FlashAttention backend implementation.

    Note about the init:
    - If no spec decoding
        - FlashAttentionBackend will be init once when the server starts.
    - If spec decoding
        - FlashAttentionBackend will be init once for the target worker
        - FlashAttentionMultiStepBackend will be once for the draft worker
            - It will spawn num_steps FlashAttentionBackend for the draft worker

    Note about CUDA Graph:
    - We only support CUDA Graph for Decode (Normal Decode and Draft Decode) and Target Verify.
    - We don&#39;t support CUDA Graph for Extend and Draft Extend.
    - When server init, init_cuda_graph_state will be called first and then init_cuda_graph_capture will be called.
    - For each forward batch, init_replay_cuda_graph will be called first and then replay the graph.
    &#34;&#34;&#34;

    def __init__(
        self,
        model_runner: ModelRunner,
        skip_prefill: bool = False,
        speculative_step_id=0,
        topk=0,
        speculative_num_steps=0,
    ):
        super().__init__()

        assert not (
            model_runner.sliding_window_size is not None
            and model_runner.model_config.is_encoder_decoder
        ), &#34;Sliding window and cross attention are not supported together&#34;

        self.forward_metadata: FlashAttentionMetadata = None
        # extra metadata for handling speculative decoding topk &gt; 1, extended draft decode and verify
        self.forward_metadata_spec_decode_expand: FlashAttentionMetadata = None
        self.max_context_len = model_runner.model_config.context_len
        self.device = model_runner.device
        self.decode_cuda_graph_metadata = {}
        self.target_verify_metadata = {}
        self.req_to_token = model_runner.req_to_token_pool.req_to_token
        self.kv_cache_dtype = model_runner.kv_cache_dtype
        self.kv_cache_dtype_str = model_runner.server_args.kv_cache_dtype
        self.page_size = model_runner.page_size
        self.use_mla = model_runner.model_config.attention_arch == AttentionArch.MLA
        self.skip_prefill = skip_prefill
        self.is_hybrid = model_runner.is_hybrid
        if self.is_hybrid:
            self.full_to_swa_index_mapping = (
                model_runner.token_to_kv_pool.full_to_swa_index_mapping
            )
        self.topk = model_runner.server_args.speculative_eagle_topk or 0
        self.speculative_num_steps = speculative_num_steps
        self.speculative_num_draft_tokens = (
            model_runner.server_args.speculative_num_draft_tokens
        )
        self.speculative_step_id = speculative_step_id

        # Local attention settings
        self.attention_chunk_size = (
            model_runner.attention_chunk_size
            if hasattr(model_runner, &#34;attention_chunk_size&#34;)
            else None
        )

        # For each layer, the sliding_window_size can be different. This is only used for preparing SWA metadata.
        # We use `layer.sliding_window_size` to decide whether to use SWA for each layer.
        self.sliding_window_size = model_runner.sliding_window_size
        self.has_swa = (
            self.sliding_window_size is not None and self.sliding_window_size &gt; -1
        )

    def init_forward_metadata(self, forward_batch: ForwardBatch):
        &#34;&#34;&#34;Initialize forward metadata hence all layers in the forward pass can reuse it.&#34;&#34;&#34;
        metadata = FlashAttentionMetadata()
        seqlens_in_batch = forward_batch.seq_lens
        batch_size = forward_batch.batch_size
        device = seqlens_in_batch.device

        if forward_batch.forward_mode.is_decode_or_idle():
            # Draft Decode
            if forward_batch.spec_info is not None:
                if self.topk &lt;= 1:
                    metadata.cache_seqlens_int32 = (
                        seqlens_in_batch + (self.speculative_step_id + 1)
                    ).to(torch.int32)
                    metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item() + (
                        self.speculative_step_id + 1
                    )
                    metadata.cu_seqlens_q = torch.arange(
                        0, batch_size + 1, dtype=torch.int32, device=device
                    )
                    metadata.cu_seqlens_k = torch.nn.functional.pad(
                        torch.cumsum(
                            metadata.cache_seqlens_int32, dim=0, dtype=torch.int32
                        ),
                        (1, 0),
                    )
                    metadata.page_table = forward_batch.req_to_token_pool.req_to_token[
                        forward_batch.req_pool_indices, : metadata.max_seq_len_k
                    ]
                else:
                    metadata.cache_seqlens_int32 = (seqlens_in_batch).to(torch.int32)
                    metadata.max_seq_len_q = self.topk
                    metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item()
                    metadata.cu_seqlens_q = torch.arange(
                        0,
                        batch_size * self.topk + 1,
                        step=self.topk,
                        dtype=torch.int32,
                        device=device,
                    )
                    metadata.cu_seqlens_k = torch.nn.functional.pad(
                        torch.cumsum(
                            metadata.cache_seqlens_int32, dim=0, dtype=torch.int32
                        ),
                        (1, 0),
                    )
                    metadata.page_table = forward_batch.req_to_token_pool.req_to_token[
                        forward_batch.req_pool_indices, : metadata.max_seq_len_k
                    ]

                    metadata_expand = FlashAttentionMetadata()
                    decode_length = self.speculative_step_id + 1
                    metadata_expand.cache_seqlens_int32 = torch.full(
                        (seqlens_in_batch.numel() * self.topk,),
                        decode_length,
                        device=device,
                        dtype=torch.int32,
                    )
                    metadata_expand.max_seq_len_q = 1
                    metadata_expand.cu_seqlens_q = torch.arange(
                        0,
                        metadata_expand.cache_seqlens_int32.numel() + 1,
                        dtype=torch.int32,
                        device=device,
                    )
                    metadata_expand.cu_seqlens_k = torch.arange(
                        0,
                        metadata_expand.cache_seqlens_int32.numel() * decode_length + 1,
                        step=decode_length,
                        dtype=torch.int32,
                        device=device,
                    )
                    # shape: [bs, num_steps, topk] -&gt; [bs x topk, num_steps]
                    cache_loc = forward_batch.out_cache_loc.view(
                        -1, self.speculative_num_steps
                    )
                    metadata_expand.page_table = (
                        cache_loc[:, :decode_length].contiguous().to(torch.int32)
                    )
                    self.forward_metadata_spec_decode_expand = metadata_expand
            else:
                # Normal Decode
                metadata.cache_seqlens_int32 = seqlens_in_batch.to(torch.int32)
                metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item()
                metadata.cu_seqlens_q = torch.arange(
                    0, batch_size + 1, dtype=torch.int32, device=device
                )
                metadata.cu_seqlens_k = torch.nn.functional.pad(
                    torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0)
                )
                metadata.page_table = forward_batch.req_to_token_pool.req_to_token[
                    forward_batch.req_pool_indices, : metadata.max_seq_len_k
                ]
            # TODO: we need to test this part for llama 4 eagle case
            self._init_local_attn_metadata(forward_batch, metadata, device)
        elif forward_batch.forward_mode.is_target_verify():
            if self.topk &lt;= 1:
                metadata.cache_seqlens_int32 = (
                    forward_batch.seq_lens + self.speculative_num_draft_tokens
                ).to(torch.int32)
                metadata.max_seq_len_q = self.speculative_num_draft_tokens
                metadata.max_seq_len_k = (
                    forward_batch.seq_lens_cpu.max().item()
                    + self.speculative_num_draft_tokens
                )
                metadata.cu_seqlens_q = torch.arange(
                    0,
                    batch_size * self.speculative_num_draft_tokens + 1,
                    self.speculative_num_draft_tokens,
                    dtype=torch.int32,
                    device=device,
                )
                metadata.cu_seqlens_k = torch.nn.functional.pad(
                    torch.cumsum(
                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32
                    ),
                    (1, 0),
                )
                metadata.page_table = forward_batch.req_to_token_pool.req_to_token[
                    forward_batch.req_pool_indices, : metadata.max_seq_len_k
                ]

                self._init_local_attn_metadata(forward_batch, metadata, device)
            else:
                metadata.cache_seqlens_int32 = forward_batch.seq_lens.to(torch.int32)
                metadata.max_seq_len_q = self.speculative_num_draft_tokens
                metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item()
                metadata.cu_seqlens_q = torch.arange(
                    0,
                    batch_size * self.speculative_num_draft_tokens + 1,
                    step=self.speculative_num_draft_tokens,
                    dtype=torch.int32,
                    device=device,
                )
                metadata.cu_seqlens_k = torch.nn.functional.pad(
                    torch.cumsum(
                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32
                    ),
                    (1, 0),
                )
                metadata.page_table = forward_batch.req_to_token_pool.req_to_token[
                    forward_batch.req_pool_indices, : metadata.max_seq_len_k
                ]

                metadata_expand = FlashAttentionMetadata()

                metadata_expand.max_seq_len_q = 1
                metadata_expand.cu_seqlens_q = torch.arange(
                    0,
                    forward_batch.seq_lens.numel() * self.speculative_num_draft_tokens
                    + 1,
                    dtype=torch.int32,
                    device=device,
                )

                # create expand page table
                offsets = torch.arange(
                    self.speculative_num_draft_tokens, device=device
                ).unsqueeze(
                    0
                )  # shape: (1, self.speculative_num_draft_tokens)
                cols = offsets.expand(
                    forward_batch.seq_lens.numel(), -1
                ) + forward_batch.seq_lens.unsqueeze(1)
                cum_len = torch.nn.functional.pad(
                    torch.cumsum(
                        (
                            forward_batch.seq_lens + self.speculative_num_draft_tokens
                        ).repeat_interleave(self.speculative_num_draft_tokens),
                        dim=0,
                    ),
                    (1, 0),
                )[:-1]
                mask_extraction_indices = (
                    cols.repeat_interleave(self.speculative_num_draft_tokens, dim=0)
                    + cum_len[:, None]
                ).view(1, -1)
                mask = forward_batch.spec_info.custom_mask[
                    mask_extraction_indices
                ].view(
                    -1, self.speculative_num_draft_tokens
                )  # (bsz * draft_num, draft_num)

                # shift table indices to avoid padding
                # non_masked_page_table [[8, 9, 10],   mask (display with int format) [[1, 0, 0],
                #                        [8, 9, 10],                                   [1, 1, 0],
                #                        [8, 9, 10]]                                   [1, 0, 1]]
                # if masked with padding [[8, 0, 0],   our mask without padding       [[8, 9, 10],
                #                        [8, 9, 0],                                    [8, 9, 10],
                #                        [8, 0, 10]]                                   [8, 10, 9]]
                # note here cache_seqlens_int32 is [1, 2, 2] so extra page indices will be ignored in each row
                col_indices = offsets.expand(
                    mask.shape[0], self.speculative_num_draft_tokens
                )
                # Build keys: if an entry is valid (mask==True), keep its original index;
                # if not, add self.speculative_num_draft_tokens so that it sorts after all valid entries.
                keys = torch.where(
                    mask, col_indices, col_indices + self.speculative_num_draft_tokens
                )
                _, sort_order = torch.sort(keys, dim=1)
                non_masked_page_table = (
                    forward_batch.req_to_token_pool.req_to_token[
                        forward_batch.req_pool_indices, :
                    ]
                    .gather(1, cols)
                    .repeat_interleave(self.speculative_num_draft_tokens, dim=0)
                )  # (bsz, draft_num)
                metadata_expand.page_table = non_masked_page_table.gather(1, sort_order)
                metadata_expand.cache_seqlens_int32 = mask.sum(dim=1).to(torch.int32)
                metadata_expand.cu_seqlens_k = torch.nn.functional.pad(
                    torch.cumsum(
                        metadata_expand.cache_seqlens_int32, dim=0, dtype=torch.int32
                    ),
                    (1, 0),
                )
                self.forward_metadata_spec_decode_expand = metadata_expand

                if self.has_swa:
                    self._init_sliding_window_attn_spec_metadata(
                        metadata, metadata_expand
                    )

        elif forward_batch.forward_mode.is_extend_or_draft_extend_or_mixed():
            metadata.cache_seqlens_int32 = seqlens_in_batch.to(torch.int32)
            metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item()
            metadata.cu_seqlens_k = torch.nn.functional.pad(
                torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0)
            )
            metadata.page_table = forward_batch.req_to_token_pool.req_to_token[
                forward_batch.req_pool_indices, : metadata.max_seq_len_k
            ]

            if (
                any(forward_batch.extend_prefix_lens_cpu)
                or forward_batch.forward_mode == ForwardMode.DRAFT_EXTEND
            ):
                extend_seq_lens = forward_batch.extend_seq_lens
                metadata.max_seq_len_q = max(forward_batch.extend_seq_lens_cpu)
                metadata.cu_seqlens_q = torch.nn.functional.pad(
                    torch.cumsum(extend_seq_lens, dim=0, dtype=torch.int32), (1, 0)
                )
            else:
                metadata.max_seq_len_q = metadata.max_seq_len_k
                metadata.cu_seqlens_q = metadata.cu_seqlens_k

            # Setup local attention if enabled
            if forward_batch.forward_mode == ForwardMode.EXTEND:
                self._init_local_attn_metadata(forward_batch, metadata, device)

        # Encoder metadata for cross attention
        if forward_batch.encoder_lens is not None:
            assert (
                forward_batch.encoder_lens.numel() == 1
            ), &#34;Only encoder size 1 is supported for now&#34;

            metadata.encoder_lens_int32 = forward_batch.encoder_lens.to(torch.int32)
            metadata.encoder_cu_seqlens_k = torch.nn.functional.pad(
                torch.cumsum(metadata.encoder_lens_int32, dim=0, dtype=torch.int32),
                (1, 0),
            )
            metadata.encoder_max_seq_len_k = metadata.encoder_lens_int32.max().item()
            metadata.encoder_page_table = forward_batch.req_to_token_pool.req_to_token[
                forward_batch.req_pool_indices, : metadata.encoder_max_seq_len_k
            ]

            # Currently only support forward_batch.encoder_lens.numel() == 1
            metadata.page_table = forward_batch.req_to_token_pool.req_to_token[
                forward_batch.req_pool_indices,
                metadata.encoder_max_seq_len_k : (
                    metadata.encoder_max_seq_len_k + metadata.max_seq_len_k
                ),
            ]

        # Convert the page table to a strided format which is needed by FA3 API
        if self.page_size &gt; 1:
            self.strided_indices = torch.arange(
                0, metadata.page_table.shape[1], self.page_size, device=self.device
            )
            metadata.page_table = (
                metadata.page_table[:, self.strided_indices] // self.page_size
            )

        self.forward_metadata = metadata

    def forward_extend(
        self,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        layer: RadixAttention,
        forward_batch: ForwardBatch,
        save_kv_cache=True,
        # For multi-head latent attention
        q_rope: Optional[torch.Tensor] = None,
        k_rope: Optional[torch.Tensor] = None,
        sinks: Optional[torch.Tensor] = None,
    ):
        if k is not None:
            assert v is not None
            if save_kv_cache:
                cache_loc = (
                    forward_batch.out_cache_loc
                    if not layer.is_cross_attention
                    else forward_batch.encoder_out_cache_loc
                )
                if not self.use_mla:
                    forward_batch.token_to_kv_pool.set_kv_buffer(
                        layer, cache_loc, k, v, layer.k_scale, layer.v_scale
                    )
                else:
                    forward_batch.token_to_kv_pool.set_mla_kv_buffer(
                        layer,
                        cache_loc,
                        k,
                        k_rope,
                    )

        # Use precomputed metadata across all layers
        metadata = self.forward_metadata

        # Calculate window size (can be moved to metadata if layer properties don&#39;t change)
        # we don&#39;t do layer.sliding_window_size - 1 since in model.get_attention_sliding_window_size() we already - 1
        # here is two side inclusive
        is_swa = (
            layer.sliding_window_size is not None and layer.sliding_window_size &gt; -1
        )
        window_size = (layer.sliding_window_size, 0) if is_swa else (-1, -1)
        k_descale, v_descale = None, None
        # only use kv scaling if: 1) fp8 kv is explicitly enabled, 2) RadixAttention
        # has corresponding quantization method so that layer.k_scale is not None,
        # 3) layer.head_dim &lt;= 256 since fa3 kernel require fp16 and bf16 data type in this case.
        if self.kv_cache_dtype_str != &#34;auto&#34; and layer.head_dim &lt;= 256:
            if layer.k_scale is not None:
                descale_shape = (forward_batch.batch_size, layer.tp_k_head_num)
                k_descale = layer.k_scale.expand(descale_shape)
                v_descale = layer.v_scale.expand(descale_shape)
            q = q.to(self.kv_cache_dtype)
            q_rope = q_rope.to(self.kv_cache_dtype) if q_rope is not None else None
            k_rope = k_rope.to(self.kv_cache_dtype) if k_rope is not None else None
        causal = not layer.is_cross_attention

        # Check if we should use local attention
        use_local_attn = (
            self.attention_chunk_size is not None
            and metadata.local_attn_metadata is not None
            and (hasattr(layer, &#34;use_irope&#34;) and layer.use_irope)
        )

        # We do cascade attention for Target Verify with topk &gt; 1
        # We don&#39;t use cascade attention for Sliding Window Attention:
        # - Different window sizes should be passed in for each q in the first stage of cascade attention, but FA3 interface doesn&#39;t support pass in a list of window sizes.
        # - The overhead of duplicated computation of the common prefix part is small for sliding window layers (seq_len &lt;= window_size), so we can just expand it.
        use_cascade_attn = (
            forward_batch.forward_mode.is_target_verify()
            and self.topk &gt; 1
            and not is_swa
        )

        # For fa3 interface version compatibility, we put new fields into conditional keyword args
        kwargs = {}
        if sinks is not None:
            kwargs[&#34;sinks&#34;] = sinks

        # Get the appropriate page table based on whether we&#39;re using local attention
        if use_local_attn:
            local_metadata = metadata.local_attn_metadata
            page_table = local_metadata.local_block_table
            cu_seqlens_q = local_metadata.local_query_start_loc
            cache_seqlens = local_metadata.local_seqused_k
            max_seqlen_q = local_metadata.local_max_query_len
        elif is_swa and metadata.swa_spec_metadata is not None:
            swa_spec_metadata = metadata.swa_spec_metadata
            page_table = swa_spec_metadata.page_table
            cu_seqlens_q = swa_spec_metadata.cu_seqlens_q
            cache_seqlens = swa_spec_metadata.cache_seqlens_int32
            max_seqlen_q = swa_spec_metadata.max_seq_len_q
            cu_seqlens_k = swa_spec_metadata.cu_seqlens_k
        else:
            page_table = metadata.page_table
            cu_seqlens_q = metadata.cu_seqlens_q
            cache_seqlens = metadata.cache_seqlens_int32
            max_seqlen_q = metadata.max_seq_len_q
            cu_seqlens_k = metadata.cu_seqlens_k

        # Use Flash Attention for prefill
        if not self.use_mla:
            # Do multi-head attention
            key_cache, value_cache = forward_batch.token_to_kv_pool.get_kv_buffer(
                layer.layer_id
            )
            key_cache = key_cache.view(
                -1, self.page_size, layer.tp_k_head_num, layer.head_dim
            )
            value_cache = value_cache.view(
                -1, self.page_size, layer.tp_v_head_num, layer.head_dim
            )
            if layer.is_cross_attention:
                page_table = metadata.encoder_page_table
                cache_seqlens = metadata.encoder_lens_int32
                cu_seqlens_k = metadata.encoder_cu_seqlens_k
                window_size = (-1, -1)

            result = flash_attn_with_kvcache(
                q=q.contiguous().view(-1, layer.tp_q_head_num, layer.head_dim),
                k_cache=key_cache,
                v_cache=value_cache,
                page_table=page_table,
                cache_seqlens=cache_seqlens,
                cu_seqlens_q=cu_seqlens_q,
                cu_seqlens_k_new=cu_seqlens_k if not use_local_attn else None,
                max_seqlen_q=max_seqlen_q,
                softmax_scale=layer.scaling,
                causal=False if use_cascade_attn else causal,
                window_size=window_size,
                softcap=layer.logit_cap,
                k_descale=k_descale,
                v_descale=v_descale,
                return_softmax_lse=use_cascade_attn,
                **kwargs,
            )

            if use_cascade_attn:
                o, softmax_lse, *rest = result
                o_expand, softmax_lse_expand, *rest_expand = flash_attn_with_kvcache(
                    q=q.contiguous().view(-1, layer.tp_q_head_num, layer.head_dim),
                    k_cache=key_cache,
                    v_cache=value_cache,
                    page_table=self.forward_metadata_spec_decode_expand.page_table,
                    cache_seqlens=self.forward_metadata_spec_decode_expand.cache_seqlens_int32,
                    cu_seqlens_q=self.forward_metadata_spec_decode_expand.cu_seqlens_q,
                    cu_seqlens_k_new=self.forward_metadata_spec_decode_expand.cu_seqlens_k,
                    max_seqlen_q=self.forward_metadata_spec_decode_expand.max_seq_len_q,
                    softmax_scale=layer.scaling,
                    causal=False,
                    window_size=window_size,
                    softcap=layer.logit_cap,
                    k_descale=k_descale,
                    v_descale=v_descale,
                    return_softmax_lse=True,
                    **kwargs,
                )
                o, _ = merge_state_v2_wrapper(
                    o,
                    softmax_lse.T.contiguous(),
                    o_expand,
                    softmax_lse_expand.T.contiguous(),
                )
            else:
                o = result
        else:
            if (
                forward_batch.attn_attend_prefix_cache is not None
                and not forward_batch.forward_mode.is_target_verify()
                and not forward_batch.forward_mode.is_draft_extend()
            ):
                # Do multi-head attention with chunked prefix cache
                if forward_batch.attn_attend_prefix_cache:
                    assert not global_server_args_dict[&#34;disable_chunked_prefix_cache&#34;]
                    # MHA for chunked prefix kv cache when running model with MLA
                    assert forward_batch.prefix_chunk_idx is not None
                    assert forward_batch.prefix_chunk_cu_seq_lens is not None
                    assert forward_batch.prefix_chunk_max_seq_lens is not None

                    chunk_idx = forward_batch.prefix_chunk_idx
                    assert chunk_idx &gt;= 0

                    assert forward_batch.mha_return_lse
                    output = flash_attn_varlen_func(
                        q=q.view(-1, layer.tp_q_head_num, layer.head_dim),
                        k=k.view(-1, layer.tp_k_head_num, layer.head_dim).to(q.dtype),
                        v=v.view(-1, layer.tp_k_head_num, layer.v_head_dim).to(q.dtype),
                        cu_seqlens_q=metadata.cu_seqlens_q,
                        cu_seqlens_k=forward_batch.prefix_chunk_cu_seq_lens[chunk_idx],
                        max_seqlen_q=metadata.max_seq_len_q,
                        max_seqlen_k=forward_batch.prefix_chunk_max_seq_lens[chunk_idx],
                        softmax_scale=layer.scaling,
                        causal=False,
                        return_softmax_lse=True,
                    )
                else:
                    # MHA for extend part of sequence without attending prefix kv cache
                    output = flash_attn_varlen_func(
                        q=q.view(-1, layer.tp_q_head_num, layer.head_dim),
                        k=k.view(-1, layer.tp_k_head_num, layer.head_dim).to(q.dtype),
                        v=v.view(-1, layer.tp_k_head_num, layer.v_head_dim).to(q.dtype),
                        cu_seqlens_q=metadata.cu_seqlens_q,
                        cu_seqlens_k=metadata.cu_seqlens_q,
                        max_seqlen_q=metadata.max_seq_len_q,
                        max_seqlen_k=metadata.max_seq_len_q,
                        softmax_scale=layer.scaling,
                        causal=True,
                        return_softmax_lse=forward_batch.mha_return_lse,
                    )
                if forward_batch.mha_return_lse:
                    output, lse, *rest = output
                    lse = torch.transpose(lse, 0, 1).contiguous()
                    return output, lse
                return output
            else:
                # Do absorbed multi-latent attention
                kv_cache = forward_batch.token_to_kv_pool.get_key_buffer(
                    layer.layer_id
                ).to(q.dtype)
                k_rope = kv_cache[:, :, layer.v_head_dim :]
                c_kv = kv_cache[:, :, : layer.v_head_dim]
                k_rope_cache = k_rope.view(
                    -1,
                    self.page_size,
                    layer.tp_k_head_num,
                    layer.head_dim - layer.v_head_dim,
                )
                c_kv_cache = c_kv.view(
                    -1, self.page_size, layer.tp_v_head_num, layer.v_head_dim
                )
                if q_rope is not None:
                    q_nope = q.view(-1, layer.tp_q_head_num, layer.v_head_dim)
                    q_rope = q_rope.view(
                        -1, layer.tp_q_head_num, layer.head_dim - layer.v_head_dim
                    )
                else:
                    q_all = q.contiguous().view(-1, layer.tp_q_head_num, layer.head_dim)
                    q_nope = q_all[:, :, : layer.v_head_dim]
                    q_rope = q_all[:, :, layer.v_head_dim :]

                result = flash_attn_with_kvcache(
                    q=q_rope,
                    k_cache=k_rope_cache,
                    v_cache=c_kv_cache,
                    qv=q_nope,
                    page_table=page_table,
                    cache_seqlens=cache_seqlens,
                    cu_seqlens_q=cu_seqlens_q,
                    cu_seqlens_k_new=cu_seqlens_k if not use_local_attn else None,
                    max_seqlen_q=max_seqlen_q,
                    softmax_scale=layer.scaling,
                    causal=False if use_cascade_attn else causal,
                    softcap=layer.logit_cap,
                    k_descale=k_descale,
                    v_descale=v_descale,
                    return_softmax_lse=use_cascade_attn,
                )
                if use_cascade_attn:
                    o, softmax_lse, *rest = result
                    o_expand, softmax_lse_expand, *rest_expand = (
                        flash_attn_with_kvcache(
                            q=q_rope,
                            k_cache=k_rope_cache,
                            v_cache=c_kv_cache,
                            qv=q_nope,
                            page_table=self.forward_metadata_spec_decode_expand.page_table,
                            cache_seqlens=self.forward_metadata_spec_decode_expand.cache_seqlens_int32,
                            cu_seqlens_q=self.forward_metadata_spec_decode_expand.cu_seqlens_q,
                            cu_seqlens_k_new=self.forward_metadata_spec_decode_expand.cu_seqlens_k,
                            max_seqlen_q=self.forward_metadata_spec_decode_expand.max_seq_len_q,
                            softmax_scale=layer.scaling,
                            causal=False,
                            window_size=window_size,
                            softcap=layer.logit_cap,
                            k_descale=k_descale,
                            v_descale=v_descale,
                            return_softmax_lse=True,
                        )
                    )
                    o, _ = merge_state_v2_wrapper(
                        o,
                        softmax_lse.T.contiguous(),
                        o_expand,
                        softmax_lse_expand.T.contiguous(),
                    )
                else:
                    o = result

        return o.view(-1, layer.tp_q_head_num * layer.v_head_dim)

    def forward_decode(
        self,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        layer: RadixAttention,
        forward_batch: ForwardBatch,
        save_kv_cache=True,
        # For multi-head latent attention
        q_rope: Optional[torch.Tensor] = None,
        k_rope: Optional[torch.Tensor] = None,
        sinks: Optional[torch.Tensor] = None,
    ) -&gt; torch.Tensor:
        if k is not None:
            assert v is not None
            if save_kv_cache:
                cache_loc = (
                    forward_batch.out_cache_loc
                    if not layer.is_cross_attention
                    else forward_batch.encoder_out_cache_loc
                )
                if not self.use_mla:
                    forward_batch.token_to_kv_pool.set_kv_buffer(
                        layer, cache_loc, k, v, layer.k_scale, layer.v_scale
                    )
                else:
                    forward_batch.token_to_kv_pool.set_mla_kv_buffer(
                        layer,
                        cache_loc,
                        k,
                        k_rope,
                    )

        # Use precomputed metadata across all layers
        metadata = self.forward_metadata
        local_attn_metadata = getattr(metadata, &#34;local_attn_metadata&#34;, None)
        use_local_attn = (
            self.attention_chunk_size is not None
            and local_attn_metadata is not None
            and (hasattr(layer, &#34;use_irope&#34;) and layer.use_irope)
        )

        # When Spec Decode enabled, forward_decode would be called with two mode:
        # 1. DRAFT_DECODE: we enable cascade attention when top_k &gt; 1
        # 2. IDLE: we don’t need cascade attention, spec_info will be none in this case
        use_cascade_attn = forward_batch.spec_info is not None and self.topk &gt; 1

        # Calculate window size (can be moved to metadata if layer properties don&#39;t change)
        # we don&#39;t do layer.sliding_window_size - 1 since in model.get_attention_sliding_window_size() we already - 1
        # here is two side inclusive
        window_size = (
            (layer.sliding_window_size, 0)
            if layer.sliding_window_size is not None and layer.sliding_window_size &gt; -1
            else (-1, -1)
        )
        causal = not layer.is_cross_attention

        # For fa3 interface version compatibility, we put new fields into conditional keyword args
        kwargs = {}
        if sinks is not None:
            kwargs[&#34;sinks&#34;] = sinks

        k_descale, v_descale = None, None
        # only use kv scaling if: 1) fp8 kv is explicitly enabled, 2) RadixAttention
        # has corresponding quantization method so that layer.k_scale is not None,
        # 3) layer.head_dim &lt;= 256 since fa3 kernel require fp16 and bf16 data type in this case.
        if self.kv_cache_dtype_str != &#34;auto&#34; and layer.head_dim &lt;= 256:
            if layer.k_scale is not None:
                descale_shape = (forward_batch.batch_size, layer.tp_k_head_num)
                k_descale = layer.k_scale.expand(descale_shape)
                v_descale = layer.v_scale.expand(descale_shape)
            q = q.to(self.kv_cache_dtype)
            q_rope = q_rope.to(self.kv_cache_dtype) if q_rope is not None else None
            k_rope = k_rope.to(self.kv_cache_dtype) if k_rope is not None else None
        if not self.use_mla:
            # Do multi-head attention

            key_cache, value_cache = forward_batch.token_to_kv_pool.get_kv_buffer(
                layer.layer_id
            )
            key_cache = key_cache.view(
                -1, self.page_size, layer.tp_k_head_num, layer.head_dim
            )
            value_cache = value_cache.view(
                -1, self.page_size, layer.tp_v_head_num, layer.head_dim
            )

            if layer.is_cross_attention:
                # Always use non-chunked logic for cross-attention
                o = flash_attn_with_kvcache(
                    q=q.contiguous().view(-1, layer.tp_q_head_num, layer.head_dim),
                    k_cache=key_cache,
                    v_cache=value_cache,
                    page_table=metadata.encoder_page_table,
                    cache_seqlens=metadata.encoder_lens_int32,
                    cu_seqlens_q=metadata.cu_seqlens_q,
                    cu_seqlens_k_new=metadata.encoder_cu_seqlens_k,
                    max_seqlen_q=1,
                    softmax_scale=layer.scaling,
                    causal=False,
                    window_size=(-1, -1),
                    softcap=layer.logit_cap,
                    k_descale=k_descale,
                    v_descale=v_descale,
                    **kwargs,
                )
            elif use_local_attn:
                # Use chunked (local) attention batching for self-attention
                o = flash_attn_with_kvcache(
                    q=q.contiguous().view(-1, layer.tp_q_head_num, layer.head_dim),
                    k_cache=key_cache,
                    v_cache=value_cache,
                    page_table=local_attn_metadata.local_block_table,
                    cache_seqlens=local_attn_metadata.local_seqused_k,
                    cu_seqlens_q=local_attn_metadata.local_query_start_loc,
                    cu_seqlens_k_new=None,
                    max_seqlen_q=local_attn_metadata.local_max_query_len,
                    softmax_scale=layer.scaling,
                    causal=True,
                    window_size=(-1, -1),
                    softcap=layer.logit_cap,
                    k_descale=k_descale,
                    v_descale=v_descale,
                    **kwargs,
                )
            else:
                page_table = metadata.page_table
                cache_seqlens = metadata.cache_seqlens_int32
                cu_seqlens_k = metadata.cu_seqlens_k
                max_seqlen_q = metadata.max_seq_len_q
                q_reshaped = q.contiguous().view(
                    -1, layer.tp_q_head_num, layer.head_dim
                )

                # Default: single-token self-attention
                result = flash_attn_with_kvcache(
                    q=q_reshaped,
                    k_cache=key_cache,
                    v_cache=value_cache,
                    page_table=page_table,
                    cache_seqlens=cache_seqlens,
                    cu_seqlens_q=metadata.cu_seqlens_q,
                    cu_seqlens_k_new=cu_seqlens_k,
                    max_seqlen_q=max_seqlen_q,
                    softmax_scale=layer.scaling,
                    causal=False if use_cascade_attn else causal,
                    window_size=window_size,
                    softcap=layer.logit_cap,
                    k_descale=k_descale,
                    v_descale=v_descale,
                    return_softmax_lse=use_cascade_attn,
                    **kwargs,
                )
                if use_cascade_attn:
                    o, softmax_lse, *rest = result
                    o_expand, softmax_lse_expand, *rest_expand = (
                        flash_attn_with_kvcache(
                            q=q_reshaped,
                            k_cache=key_cache,
                            v_cache=value_cache,
                            page_table=self.forward_metadata_spec_decode_expand.page_table,
                            cache_seqlens=self.forward_metadata_spec_decode_expand.cache_seqlens_int32,
                            cu_seqlens_q=self.forward_metadata_spec_decode_expand.cu_seqlens_q,
                            cu_seqlens_k_new=self.forward_metadata_spec_decode_expand.cu_seqlens_k,
                            max_seqlen_q=self.forward_metadata_spec_decode_expand.max_seq_len_q,
                            softmax_scale=layer.scaling,
                            causal=False,
                            window_size=window_size,
                            softcap=layer.logit_cap,
                            k_descale=k_descale,
                            v_descale=v_descale,
                            return_softmax_lse=True,
                            **kwargs,
                        )
                    )
                    o, _ = merge_state_v2(
                        o,
                        softmax_lse.T.contiguous(),
                        o_expand,
                        softmax_lse_expand.T.contiguous(),
                    )
                else:
                    o = result
        else:
            # Do absorbed multi-latent attention
            kv_cache = forward_batch.token_to_kv_pool.get_key_buffer(layer.layer_id).to(
                q.dtype
            )
            k_rope = kv_cache[:, :, layer.v_head_dim :]
            c_kv = kv_cache[:, :, : layer.v_head_dim]
            k_rope_cache = k_rope.view(
                -1,
                self.page_size,
                layer.tp_k_head_num,
                layer.head_dim - layer.v_head_dim,
            )
            c_kv_cache = c_kv.view(
                -1, self.page_size, layer.tp_v_head_num, layer.v_head_dim
            )

            if q_rope is not None:
                q_nope = q.view(-1, layer.tp_q_head_num, layer.v_head_dim)
                q_rope = q_rope.view(
                    -1, layer.tp_q_head_num, layer.head_dim - layer.v_head_dim
                )
            else:
                q_all = q.contiguous().view(-1, layer.tp_q_head_num, layer.head_dim)
                q_nope = q_all[:, :, : layer.v_head_dim]
                q_rope = q_all[:, :, layer.v_head_dim :]
            max_seqlen_q = metadata.max_seq_len_q

            result = flash_attn_with_kvcache(
                q=q_rope,
                k_cache=k_rope_cache,
                v_cache=c_kv_cache,
                qv=q_nope,
                page_table=metadata.page_table,
                cache_seqlens=metadata.cache_seqlens_int32,
                cu_seqlens_q=metadata.cu_seqlens_q,
                cu_seqlens_k_new=metadata.cu_seqlens_k,
                max_seqlen_q=max_seqlen_q,
                softmax_scale=layer.scaling,
                causal=False if use_cascade_attn else causal,
                softcap=layer.logit_cap,
                k_descale=k_descale,
                v_descale=v_descale,
                return_softmax_lse=use_cascade_attn,  # softmax_lse is needed for merge states
            )
            if use_cascade_attn:
                o, softmax_lse, *rest = result
                o_expand, softmax_lse_expand, *rest_expand = flash_attn_with_kvcache(
                    q=q_rope,
                    k_cache=k_rope_cache,
                    v_cache=c_kv_cache,
                    qv=q_nope,
                    page_table=self.forward_metadata_spec_decode_expand.page_table,
                    cache_seqlens=self.forward_metadata_spec_decode_expand.cache_seqlens_int32,
                    cu_seqlens_q=self.forward_metadata_spec_decode_expand.cu_seqlens_q,
                    cu_seqlens_k_new=self.forward_metadata_spec_decode_expand.cu_seqlens_k,
                    max_seqlen_q=self.forward_metadata_spec_decode_expand.max_seq_len_q,
                    softmax_scale=layer.scaling,
                    causal=False,
                    window_size=window_size,
                    softcap=layer.logit_cap,
                    k_descale=k_descale,
                    v_descale=v_descale,
                    return_softmax_lse=True,
                )
                o, _ = merge_state_v2(
                    o,
                    softmax_lse.T.contiguous(),
                    o_expand,
                    softmax_lse_expand.T.contiguous(),
                )
            else:
                o = result

        return o.view(-1, layer.tp_q_head_num * layer.v_head_dim)

    def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int):
        &#34;&#34;&#34;Initialize CUDA graph state for the attention backend.

        Args:
            max_bs (int): Maximum batch size to support in CUDA graphs

        This creates fixed-size tensors that will be reused during CUDA graph replay
        to avoid memory allocations.
        &#34;&#34;&#34;
        max_num_pages = (self.max_context_len + self.page_size - 1) // self.page_size

        # This is being used by normal decode and draft decode when topk == 1
        self.decode_cuda_graph_metadata = {
            &#34;cache_seqlens&#34;: torch.zeros(max_bs, dtype=torch.int32, device=self.device),
            &#34;cu_seqlens_q&#34;: torch.arange(
                0, max_bs + 1, dtype=torch.int32, device=self.device
            ),
            &#34;cu_seqlens_k&#34;: torch.zeros(
                max_bs + 1, dtype=torch.int32, device=self.device
            ),
            &#34;page_table&#34;: torch.zeros(
                max_bs,
                max_num_pages,
                dtype=torch.int32,
                device=self.device,
            ),
            &#34;strided_indices&#34;: torch.arange(
                0, self.max_context_len, self.page_size, device=self.device
            ),
        }
        # Only allocate local attention buffers if local attention is enabled
        # This prevents OOM errors when local attention is not being used
        if self.attention_chunk_size is not None:
            # Estimate maximum sizes for local attention metadata
            max_seq_len = self.max_context_len
            page_size = self.page_size or 1
            attn_chunk_size = self.attention_chunk_size
            max_virtual_batches = max_bs * (
                (max_seq_len + attn_chunk_size - 1) // attn_chunk_size
            )
            max_pages_per_block = (attn_chunk_size + page_size - 1) // page_size

            self.decode_cuda_graph_local_attn_metadata = {
                &#34;local_query_start_loc&#34;: torch.zeros(
                    max_virtual_batches + 1, dtype=torch.int32, device=self.device
                ),
                &#34;local_seqused_k&#34;: torch.zeros(
                    max_virtual_batches, dtype=torch.int32, device=self.device
                ),
                &#34;local_block_table&#34;: torch.zeros(
                    max_virtual_batches,
                    max_pages_per_block,
                    dtype=torch.int32,
                    device=self.device,
                ),
            }

        # This is used by draft decode&#39;s first half of metadata when topk &gt; 1
        if self.topk &gt; 1:
            self.draft_decode_metadata_topk_normal = {
                &#34;cache_seqlens&#34;: torch.zeros(
                    max_bs, dtype=torch.int32, device=self.device
                ),
                &#34;cu_seqlens_q&#34;: torch.arange(
                    0,
                    max_bs * self.topk + 1,
                    step=self.topk,
                    dtype=torch.int32,
                    device=self.device,
                ),
                &#34;cu_seqlens_k&#34;: torch.zeros(
                    max_bs + 1, dtype=torch.int32, device=self.device
                ),
                &#34;page_table&#34;: torch.zeros(
                    max_bs,
                    self.max_context_len,
                    dtype=torch.int32,
                    device=self.device,
                ),
            }

            # This is used by draft decode&#39;s second half of metadata when topk &gt; 1
            decode_length = self.speculative_step_id + 1
            self.draft_decode_metadata_topk_expand = {
                &#34;cache_seqlens&#34;: torch.full(
                    (max_bs * self.topk,),
                    decode_length,
                    device=self.device,
                    dtype=torch.int32,
                ),
                &#34;cu_seqlens_q&#34;: torch.arange(
                    0,
                    max_bs * self.topk + 1,
                    dtype=torch.int32,
                    device=self.device,
                ),
                &#34;cu_seqlens_k&#34;: torch.arange(
                    0,
                    max_bs * self.topk * decode_length + 1,
                    step=decode_length,
                    dtype=torch.int32,
                    device=self.device,
                ),
                &#34;page_table&#34;: torch.zeros(
                    max_bs * self.topk,
                    decode_length,
                    dtype=torch.int32,
                    device=self.device,
                ),
            }

        if (
            self.speculative_num_draft_tokens is not None
            and self.speculative_num_draft_tokens &gt; 0
        ):
            # &#34;page_table_draft_decode&#34; will be set only when spec decoding enabled to save memory
            self.decode_cuda_graph_metadata[&#34;page_table_draft_decode&#34;] = torch.zeros(
                max_bs,
                max_num_pages,
                dtype=torch.int32,
                device=self.device,
            )

            self.target_verify_metadata = {
                &#34;cache_seqlens&#34;: torch.zeros(
                    max_bs, dtype=torch.int32, device=self.device
                ),
                &#34;cu_seqlens_q&#34;: torch.arange(
                    0,
                    max_bs * self.speculative_num_draft_tokens + 1,
                    step=self.speculative_num_draft_tokens,
                    dtype=torch.int32,
                    device=self.device,
                ),
                &#34;cu_seqlens_k&#34;: torch.zeros(
                    max_bs + 1, dtype=torch.int32, device=self.device
                ),
                &#34;page_table&#34;: torch.zeros(
                    max_bs,
                    max_num_pages,
                    dtype=torch.int32,
                    device=self.device,
                ),
                &#34;strided_indices&#34;: torch.arange(
                    0, self.max_context_len, self.page_size, device=self.device
                ),
            }

            self.draft_extend_metadata = {
                &#34;cache_seqlens&#34;: torch.zeros(
                    max_bs, dtype=torch.int32, device=self.device
                ),
                &#34;cu_seqlens_q&#34;: torch.zeros(
                    max_bs + 1,
                    dtype=torch.int32,
                    device=self.device,
                ),
                &#34;cu_seqlens_k&#34;: torch.zeros(
                    max_bs + 1, dtype=torch.int32, device=self.device
                ),
                &#34;page_table&#34;: torch.zeros(
                    max_bs,
                    max_num_pages,
                    dtype=torch.int32,
                    device=self.device,
                ),
                &#34;strided_indices&#34;: torch.arange(
                    0, self.max_context_len, self.page_size, device=self.device
                ),
            }

        if self.topk &gt; 1:
            self.target_verify_metadata_topk_normal = {
                &#34;cache_seqlens&#34;: torch.zeros(
                    max_bs, dtype=torch.int32, device=self.device
                ),
                &#34;cu_seqlens_q&#34;: torch.arange(
                    0,
                    max_bs * self.speculative_num_draft_tokens + 1,
                    step=self.speculative_num_draft_tokens,
                    dtype=torch.int32,
                    device=self.device,
                ),
                &#34;cu_seqlens_k&#34;: torch.zeros(
                    max_bs + 1, dtype=torch.int32, device=self.device
                ),
                &#34;page_table&#34;: torch.zeros(
                    max_bs,
                    self.max_context_len,
                    dtype=torch.int32,
                    device=self.device,
                ),
            }

            self.target_verify_metadata_topk_expand = {
                &#34;cache_seqlens&#34;: torch.zeros(
                    max_bs * self.speculative_num_draft_tokens,
                    dtype=torch.int32,
                    device=self.device,
                ),
                &#34;cu_seqlens_k&#34;: torch.zeros(
                    max_bs * self.speculative_num_draft_tokens + 1,
                    dtype=torch.int32,
                    device=self.device,
                ),
                &#34;cu_seqlens_q&#34;: torch.arange(
                    0,
                    max_bs * self.speculative_num_draft_tokens + 1,
                    dtype=torch.int32,
                    device=self.device,
                ),
                &#34;page_table&#34;: torch.zeros(
                    max_bs * self.speculative_num_draft_tokens,
                    self.speculative_num_draft_tokens,
                    dtype=torch.int32,
                    device=self.device,
                ),
            }

            if self.has_swa:
                self.target_verify_metadata_topk_swa = {
                    &#34;cache_seqlens&#34;: torch.zeros(
                        max_bs * self.speculative_num_draft_tokens,
                        dtype=torch.int32,
                        device=self.device,
                    ),
                    &#34;cu_seqlens_k&#34;: torch.zeros(
                        max_bs * self.speculative_num_draft_tokens + 1,
                        dtype=torch.int32,
                        device=self.device,
                    ),
                    &#34;cu_seqlens_q&#34;: torch.arange(
                        0,
                        max_bs * self.speculative_num_draft_tokens + 1,
                        dtype=torch.int32,
                        device=self.device,
                    ),
                    &#34;page_table&#34;: torch.zeros(
                        max_bs * self.speculative_num_draft_tokens,
                        self.max_context_len,
                        dtype=torch.int32,
                        device=self.device,
                    ),
                }

        self.encoder_metadata = {
            &#34;encoder_page_table&#34;: torch.zeros(
                max_bs,
                self.max_context_len,
                dtype=torch.int32,
                device=self.device,
            ),
            &#34;encoder_lens_int32&#34;: torch.zeros(
                max_bs, dtype=torch.int32, device=self.device
            ),
            &#34;encoder_cu_seqlens_k&#34;: torch.zeros(
                max_bs + 1, dtype=torch.int32, device=self.device
            ),
        }

    def init_forward_metadata_capture_cuda_graph(
        self,
        bs: int,
        num_tokens: int,
        req_pool_indices: torch.Tensor,
        seq_lens: torch.Tensor,
        encoder_lens: Optional[torch.Tensor],
        forward_mode: ForwardMode,
        spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]],
    ):
        &#34;&#34;&#34;Initialize forward metadata for capturing CUDA graph.&#34;&#34;&#34;
        metadata = FlashAttentionMetadata()

        # metadata_expand is needed for Spec Decoding when top k &gt; 1
        metadata_expand = FlashAttentionMetadata()

        device = seq_lens.device
        if forward_mode.is_decode_or_idle():
            if spec_info is not None:
                # Draft Decode
                if self.topk &lt;= 1:
                    # When topk = 1, we use the normal decode metadata
                    metadata.cache_seqlens_int32 = self.decode_cuda_graph_metadata[
                        &#34;cache_seqlens&#34;
                    ][:bs]
                    metadata.max_seq_len_k = seq_lens.max().item() + (
                        self.speculative_step_id + 1
                    )
                    metadata.cu_seqlens_q = self.decode_cuda_graph_metadata[
                        &#34;cu_seqlens_q&#34;
                    ][: bs + 1]
                    metadata.cu_seqlens_k = torch.nn.functional.pad(
                        torch.cumsum(
                            metadata.cache_seqlens_int32, dim=0, dtype=torch.int32
                        ),
                        (1, 0),
                    )
                    metadata.page_table = self.decode_cuda_graph_metadata[
                        &#34;page_table_draft_decode&#34;
                    ][:bs, :]
                    self.decode_cuda_graph_metadata[bs] = metadata
                else:
                    # When top k &gt; 1, we need two specific draft decode metadata, and then merge states
                    # 1. The first half of metadata for prefix tokens
                    metadata.cache_seqlens_int32 = (
                        self.draft_decode_metadata_topk_normal[&#34;cache_seqlens&#34;][:bs]
                    )
                    metadata.max_seq_len_q = self.topk
                    metadata.max_seq_len_k = seq_lens.max().item()
                    metadata.cu_seqlens_q = self.draft_decode_metadata_topk_normal[
                        &#34;cu_seqlens_q&#34;
                    ][: bs + 1]
                    metadata.cu_seqlens_k = self.draft_decode_metadata_topk_normal[
                        &#34;cu_seqlens_k&#34;
                    ][: bs + 1]
                    metadata.page_table = self.draft_decode_metadata_topk_normal[
                        &#34;page_table&#34;
                    ][:bs, :]

                    # 2. The second half of metadata for draft tokens (per_batch_num_tokens = topk)
                    metadata_expand.cache_seqlens_int32 = (
                        self.draft_decode_metadata_topk_expand[&#34;cache_seqlens&#34;][
                            : bs * self.topk
                        ]
                    )
                    metadata_expand.max_seq_len_q = 1
                    metadata_expand.cu_seqlens_q = (
                        self.draft_decode_metadata_topk_expand[&#34;cu_seqlens_q&#34;][
                            : bs * self.topk + 1
                        ]
                    )
                    metadata_expand.cu_seqlens_k = (
                        self.draft_decode_metadata_topk_expand[&#34;cu_seqlens_k&#34;][
                            : bs * self.topk + 1
                        ]
                    )
                    metadata_expand.page_table = self.draft_decode_metadata_topk_expand[
                        &#34;page_table&#34;
                    ][: bs * self.topk]
                    self.draft_decode_metadata_topk_normal[bs] = metadata
                    self.draft_decode_metadata_topk_expand[bs] = metadata_expand
            else:
                # Normal Decode
                # Get sequence information
                metadata.cache_seqlens_int32 = seq_lens.to(torch.int32)
                batch_size = len(seq_lens)
                device = seq_lens.device
                metadata.cu_seqlens_k = torch.nn.functional.pad(
                    torch.cumsum(seq_lens, dim=0, dtype=torch.int32), (1, 0)
                )
                # Precompute maximum sequence length
                metadata.max_seq_len_k = seq_lens.max().item()
                # Precompute page table
                metadata.page_table = self.decode_cuda_graph_metadata[&#34;page_table&#34;][
                    :bs, :
                ]
                # Precompute cumulative sequence lengths
                metadata.cu_seqlens_q = torch.arange(
                    0, batch_size + 1, dtype=torch.int32, device=device
                )
                self.decode_cuda_graph_metadata[bs] = metadata

                if self.attention_chunk_size is not None:
                    self._update_local_attn_metadata_for_capture(metadata, batch_size)

        elif forward_mode.is_target_verify():
            if self.topk &lt;= 1:
                metadata.cache_seqlens_int32 = self.target_verify_metadata[
                    &#34;cache_seqlens&#34;
                ][:bs]
                metadata.cache_seqlens_int32.copy_(
                    (seq_lens + self.speculative_num_draft_tokens)
                )

                metadata.max_seq_len_q = self.speculative_num_draft_tokens
                metadata.max_seq_len_k = (
                    seq_lens.max().item() + self.speculative_num_draft_tokens
                )

                metadata.cu_seqlens_q = torch.arange(
                    0,
                    bs * self.speculative_num_draft_tokens + 1,
                    self.speculative_num_draft_tokens,
                    dtype=torch.int32,
                    device=device,
                )

                metadata.cu_seqlens_k = self.target_verify_metadata[&#34;cu_seqlens_k&#34;][
                    : (bs + 1)
                ]

                metadata.page_table = self.target_verify_metadata[&#34;page_table&#34;][:bs, :]

                self.target_verify_metadata[bs] = metadata
            else:
                # When topk &gt; 1, we need two specific target verify metadata, and then merge states
                # 1. The first half of metadata for prefix tokens
                metadata.cache_seqlens_int32 = self.target_verify_metadata_topk_normal[
                    &#34;cache_seqlens&#34;
                ][:bs]
                metadata.max_seq_len_q = self.speculative_num_draft_tokens
                # metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item(), do this in replay
                metadata.cu_seqlens_q = self.target_verify_metadata_topk_normal[
                    &#34;cu_seqlens_q&#34;
                ][: bs + 1]
                metadata.cu_seqlens_k = self.target_verify_metadata_topk_normal[
                    &#34;cu_seqlens_k&#34;
                ][: bs + 1]
                metadata.page_table = self.target_verify_metadata_topk_normal[
                    &#34;page_table&#34;
                ][:bs, :]

                # 2. The second half of metadata for draft tokens (per_batch_num_tokens = topk)
                metadata_expand.cache_seqlens_int32 = (
                    self.target_verify_metadata_topk_expand[&#34;cache_seqlens&#34;][
                        : bs * self.speculative_num_draft_tokens
                    ]
                )
                metadata_expand.max_seq_len_q = 1
                metadata_expand.cu_seqlens_q = self.target_verify_metadata_topk_expand[
                    &#34;cu_seqlens_q&#34;
                ][: bs * self.speculative_num_draft_tokens + 1]
                metadata_expand.cu_seqlens_k = self.target_verify_metadata_topk_expand[
                    &#34;cu_seqlens_k&#34;
                ][: bs * self.speculative_num_draft_tokens + 1]

                metadata_expand.page_table = self.target_verify_metadata_topk_expand[
                    &#34;page_table&#34;
                ][: bs * self.speculative_num_draft_tokens]

                self.target_verify_metadata_topk_normal[bs] = metadata
                self.target_verify_metadata_topk_expand[bs] = metadata_expand

                if self.has_swa:
                    metadata_swa = FlashAttentionMetadata()
                    metadata_swa.cache_seqlens_int32 = (
                        self.target_verify_metadata_topk_swa[&#34;cache_seqlens&#34;][
                            : bs * self.speculative_num_draft_tokens
                        ]
                    )
                    metadata_swa.max_seq_len_q = 1
                    metadata_swa.cu_seqlens_q = self.target_verify_metadata_topk_swa[
                        &#34;cu_seqlens_q&#34;
                    ][: bs * self.speculative_num_draft_tokens + 1]
                    metadata_swa.cu_seqlens_k = self.target_verify_metadata_topk_swa[
                        &#34;cu_seqlens_k&#34;
                    ][: bs * self.speculative_num_draft_tokens + 1]

                    metadata_swa.page_table = self.target_verify_metadata_topk_swa[
                        &#34;page_table&#34;
                    ][: bs * self.speculative_num_draft_tokens]
                    self.target_verify_metadata_topk_swa[bs] = metadata_swa
                    metadata.swa_spec_metadata = metadata_swa

        elif forward_mode.is_draft_extend():
            metadata.cache_seqlens_int32 = self.draft_extend_metadata[&#34;cache_seqlens&#34;][
                :bs
            ]
            metadata.cache_seqlens_int32.copy_(seq_lens)

            num_tokens_per_bs = num_tokens // bs
            metadata.max_seq_len_q = num_tokens_per_bs
            metadata.max_seq_len_k = seq_lens.max().item()

            metadata.cu_seqlens_q = torch.arange(
                0,
                bs * num_tokens_per_bs + 1,
                num_tokens_per_bs,
                dtype=torch.int32,
                device=device,
            )

            metadata.cu_seqlens_k = self.draft_extend_metadata[&#34;cu_seqlens_k&#34;][
                : (bs + 1)
            ]
            metadata.page_table = self.draft_extend_metadata[&#34;page_table&#34;][:bs, :]

            self.draft_extend_metadata[bs] = metadata

        if encoder_lens is not None:
            encoder_bs = encoder_lens.numel()
            metadata.encoder_lens_int32 = self.encoder_metadata[&#34;encoder_lens_int32&#34;][
                :encoder_bs
            ]
            metadata.encoder_cu_seqlens_k = self.encoder_metadata[
                &#34;encoder_cu_seqlens_k&#34;
            ][: (encoder_bs + 1)]

            metadata.encoder_page_table = self.encoder_metadata[&#34;encoder_page_table&#34;][
                :bs, :
            ]

        self.forward_metadata = metadata
        self.forward_metadata_spec_decode_expand = metadata_expand

    def init_forward_metadata_replay_cuda_graph(
        self,
        bs: int,
        req_pool_indices: torch.Tensor,
        seq_lens: torch.Tensor,
        seq_lens_sum: int,
        encoder_lens: Optional[torch.Tensor],
        forward_mode: ForwardMode,
        spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]],
        seq_lens_cpu: Optional[torch.Tensor],
        out_cache_loc: Optional[torch.Tensor] = None,
    ):
        &#34;&#34;&#34;Initialize forward metadata for replaying CUDA graph.&#34;&#34;&#34;
        seq_lens = seq_lens[:bs]
        seq_lens_cpu = seq_lens_cpu[:bs]
        req_pool_indices = req_pool_indices[:bs]
        device = seq_lens.device
        metadata = None
        metadata_expand = None

        if forward_mode.is_decode_or_idle():

            if spec_info is not None:
                # Draft Decode
                if self.topk &lt;= 1:
                    # When topk = 1, we use the normal decode metadata
                    metadata = self.decode_cuda_graph_metadata[bs]
                    max_len = seq_lens_cpu.max().item()
                    metadata.max_seq_len_k = max_len + self.speculative_step_id + 1
                    max_seq_pages = (
                        metadata.max_seq_len_k + self.page_size - 1
                    ) // self.page_size

                    normal_decode_set_metadata(
                        metadata.cache_seqlens_int32,
                        metadata.cu_seqlens_k,
                        metadata.page_table,
                        self.req_to_token,
                        req_pool_indices,
                        self.decode_cuda_graph_metadata[&#34;strided_indices&#34;],
                        max_seq_pages,
                        seq_lens,
                        self.speculative_step_id + 1,
                        self.page_size,
                    )

                else:
                    # When top k &gt; 1, we need two specific draft decode metadata, and then merge states
                    # 1. The first half of metadata for prefix tokens
                    metadata = self.draft_decode_metadata_topk_normal[bs]
                    metadata.cache_seqlens_int32.copy_(seq_lens)
                    # metadata.max_seq_len_q = self.topk, already set in capture
                    metadata.max_seq_len_k = seq_lens_cpu.max().item()
                    # metadata.cu_seqlens_q already set in capture
                    metadata.cu_seqlens_k[1:].copy_(
                        torch.cumsum(
                            metadata.cache_seqlens_int32, dim=0, dtype=torch.int32
                        )
                    )

                    page_table = self.req_to_token[
                        req_pool_indices, : metadata.max_seq_len_k
                    ]

                    metadata.page_table[:, : metadata.max_seq_len_k].copy_(page_table)

                    # 2. The second half of metadata for draft tokens (per_batch_num_tokens = topk)
                    metadata_expand = self.draft_decode_metadata_topk_expand[bs]
                    decode_length = self.speculative_step_id + 1
                    # shape: [bs, num_steps, topk] -&gt; [bs x topk, num_steps]
                    cache_loc = out_cache_loc.view(-1, self.speculative_num_steps)
                    metadata_expand.page_table[: cache_loc.shape[0]].copy_(
                        cache_loc[:, :decode_length]
                    )
                # TODO: Handle local attention metadata for draft decode when llama4 eagle is supported
            else:
                # Normal Decode
                metadata = self.decode_cuda_graph_metadata[bs]
                max_len = seq_lens_cpu.max().item()
                max_seq_pages = (max_len + self.page_size - 1) // self.page_size
                metadata.max_seq_len_k = max_len

                normal_decode_set_metadata(
                    metadata.cache_seqlens_int32,
                    metadata.cu_seqlens_k,
                    metadata.page_table,
                    self.req_to_token,
                    req_pool_indices,
                    self.decode_cuda_graph_metadata[&#34;strided_indices&#34;],
                    max_seq_pages,
                    seq_lens,
                    0,
                    self.page_size,
                )

                self._update_local_attn_metadata_for_replay(
                    metadata,
                    bs,
                )
        elif forward_mode.is_target_verify():
            if self.topk &lt;= 1:
                metadata = self.target_verify_metadata[bs]
                metadata.cache_seqlens_int32.copy_(
                    (seq_lens + self.speculative_num_draft_tokens)
                )

                metadata.max_seq_len_k = (
                    seq_lens_cpu.max().item() + self.speculative_num_draft_tokens
                )
                metadata.cu_seqlens_k[1:].copy_(
                    torch.cumsum(metadata.cache_seqlens_int32, dim=0, dtype=torch.int32)
                )
                max_seq_pages = (
                    metadata.max_seq_len_k + self.page_size - 1
                ) // self.page_size
                page_indices = self.req_to_token[
                    req_pool_indices[:, None],
                    self.decode_cuda_graph_metadata[&#34;strided_indices&#34;][:max_seq_pages],
                ]
                page_indices //= self.page_size
                metadata.page_table[:, :max_seq_pages].copy_(page_indices)
            else:
                # When topk &gt; 1, we need two specific target verify metadata, and then merge states
                # 1. The first half of metadata for prefix tokens
                metadata = self.target_verify_metadata_topk_normal[bs]
                metadata.cache_seqlens_int32.copy_(seq_lens)
                # metadata.max_seq_len_q = self.speculative_num_draft_tokens, already set in capture
                metadata.max_seq_len_k = seq_lens_cpu.max().item()
                # metadata.cu_seqlens_q already set in capture
                metadata.cu_seqlens_k[1:].copy_(
                    torch.cumsum(metadata.cache_seqlens_int32, dim=0, dtype=torch.int32)
                )
                page_table = self.req_to_token[
                    req_pool_indices, : metadata.max_seq_len_k
                ]
                metadata.page_table[:, : metadata.max_seq_len_k].copy_(page_table)

                # 2. The second half of metadata for draft tokens (per_batch_num_tokens = topk)
                metadata_expand = self.target_verify_metadata_topk_expand[bs]

                # metadata_expand.max_seq_len_q = 1, already set in capture
                # metadata_expand.cu_seqlens_q already set in capture
                offsets = torch.arange(
                    self.speculative_num_draft_tokens, device=device
                ).unsqueeze(
                    0
                )  # shape: (1, self.speculative_num_draft_tokens)

                cols = offsets.expand(seq_lens.numel(), -1) + seq_lens.unsqueeze(1)
                cum_len = torch.nn.functional.pad(
                    torch.cumsum(
                        (
                            seq_lens + self.speculative_num_draft_tokens
                        ).repeat_interleave(self.speculative_num_draft_tokens),
                        dim=0,
                    ),
                    (1, 0),
                )[:-1]
                mask_extraction_indices = (
                    cols.repeat_interleave(self.speculative_num_draft_tokens, dim=0)
                    + cum_len[:, None]
                ).view(1, -1)
                # avoid extracting padded seq indices which will be out of boundary
                mask_extraction_indices[
                    :,
                    spec_info.positions.numel() * self.speculative_num_draft_tokens :,
                ].fill_(0)
                mask = spec_info.custom_mask[mask_extraction_indices].view(
                    -1, self.speculative_num_draft_tokens
                )  # (bsz * draft_num, draft_num)

                col_indices = offsets.expand(
                    mask.shape[0], self.speculative_num_draft_tokens
                )
                keys = torch.where(
                    mask,
                    col_indices,
                    col_indices + self.speculative_num_draft_tokens,
                )
                _, sort_order = torch.sort(keys, dim=1)

                non_masked_page_table = (
                    self.req_to_token[req_pool_indices, :]
                    .gather(1, cols)
                    .repeat_interleave(self.speculative_num_draft_tokens, dim=0)
                )  # (bsz, draft_num)

                metadata_expand.page_table.copy_(
                    non_masked_page_table.gather(1, sort_order)
                )
                metadata_expand.cache_seqlens_int32.copy_(mask.sum(dim=1))
                metadata_expand.cu_seqlens_k[1:].copy_(
                    torch.cumsum(
                        metadata_expand.cache_seqlens_int32,
                        dim=0,
                        dtype=torch.int32,
                    )
                )

                if self.has_swa:
                    metadata_swa = self.target_verify_metadata_topk_swa[bs]
                    self._init_sliding_window_attn_spec_metadata(
                        metadata, metadata_expand, metadata_swa
                    )

        elif forward_mode.is_draft_extend():
            metadata = self.draft_extend_metadata[bs]
            metadata.cache_seqlens_int32.copy_(seq_lens)

            metadata.max_seq_len_k = seq_lens_cpu.max().item()
            metadata.cu_seqlens_k[1:].copy_(
                torch.cumsum(metadata.cache_seqlens_int32, dim=0, dtype=torch.int32)
            )
            accept_length = spec_info.accept_length[:bs]
            if spec_info.accept_length_cpu:
                metadata.max_seq_len_q = max(spec_info.accept_length_cpu) + 1
            else:
                metadata.max_seq_len_q = 1

            metadata.cu_seqlens_q[1:].copy_(
                torch.cumsum(accept_length, dim=0, dtype=torch.int32)
            )

            max_seq_pages = (
                metadata.max_seq_len_k + self.page_size - 1
            ) // self.page_size
            page_indices = self.req_to_token[
                req_pool_indices[:, None],
                self.draft_extend_metadata[&#34;strided_indices&#34;][:max_seq_pages],
            ]
            metadata.page_table[:, :max_seq_pages].copy_(page_indices // self.page_size)

        if encoder_lens is not None:
            # Only support encoder size 1 for now
            metadata.encoder_max_seq_len_k = encoder_lens[0]
            metadata.encoder_lens_int32.copy_(encoder_lens[:1])
            metadata.encoder_cu_seqlens_k[1:].copy_(
                torch.cumsum(metadata.encoder_lens_int32, dim=0, dtype=torch.int32)
            )

            metadata.encoder_page_table[:, : metadata.encoder_max_seq_len_k].copy_(
                self.req_to_token[req_pool_indices, : metadata.encoder_max_seq_len_k]
            )

            # Update the regular page table
            page_table = self.req_to_token[
                req_pool_indices,
                metadata.encoder_max_seq_len_k : (
                    metadata.encoder_max_seq_len_k + metadata.max_seq_len_k
                ),
            ]
            metadata.page_table[:, : metadata.max_seq_len_k].copy_(page_table)

        self.forward_metadata = metadata
        self.forward_metadata_spec_decode_expand = metadata_expand

    def get_cuda_graph_seq_len_fill_value(self):
        &#34;&#34;&#34;Get the fill value for sequence length in CUDA graph.&#34;&#34;&#34;
        return 1

    def _init_local_attn_metadata(
        self, forwardbatch: ForwardBatch, metadata: FlashAttentionMetadata, device
    ):
        &#34;&#34;&#34;Centralized utility to initialize local_attn_metadata if chunked attention is enabled.&#34;&#34;&#34;
        if self.attention_chunk_size is None:
            metadata.local_attn_metadata = None
            return

        cu_seqlens_q = metadata.cu_seqlens_q
        cache_seqlens_int32 = metadata.cache_seqlens_int32
        if self.is_hybrid:
            page_table = self.full_to_swa_index_mapping[metadata.page_table].to(
                torch.int32
            )
        else:
            page_table = metadata.page_table
        if cu_seqlens_q is None or cache_seqlens_int32 is None or page_table is None:
            metadata.local_attn_metadata = None
            return

        cu_seqlens_q_np = cu_seqlens_q.cpu().numpy()
        seq_lens_np = cache_seqlens_int32.cpu().numpy()
        (
            seqlens_q_local_np,
            cu_seqlens_q_local_np,
            seqlens_k_local_np,
            block_table_local,
        ) = make_local_attention_virtual_batches(
            self.attention_chunk_size,
            cu_seqlens_q_np,
            seq_lens_np,
            page_table,
            self.page_size,
        )

        local_metadata = FlashAttentionMetadata.LocalAttentionMetadata(
            local_query_start_loc=torch.from_numpy(cu_seqlens_q_local_np).to(device),
            local_seqused_k=torch.from_numpy(seqlens_k_local_np).to(device),
            local_block_table=block_table_local.to(device),
            local_max_query_len=int(seqlens_q_local_np.max()),
            local_max_seq_len=int(seqlens_k_local_np.max()),
        )
        metadata.local_attn_metadata = local_metadata

    def _update_local_attn_metadata_for_capture(
        self, metadata: FlashAttentionMetadata, bs: int
    ):
        &#34;&#34;&#34;Update local attention metadata during CUDA graph capture phase.

        This method calculates the exact buffer sizes needed for local attention metadata
        during the CUDA graph capture phase, optimizing memory usage by creating views of
        pre-allocated buffers with exactly the sizes needed.
        &#34;&#34;&#34;
        seq_lens_capture = metadata.cache_seqlens_int32
        max_seq_len = int(seq_lens_capture.max().item())
        page_table_capture = metadata.page_table

        cu_seqlens_q_np = metadata.cu_seqlens_q.cpu().numpy()
        seqlens_np = seq_lens_capture.cpu().numpy()
        (
            seqlens_q_local_np,
            cu_seqlens_q_local_np,
            seqlens_k_local_np,
            block_table_local_np,
        ) = make_local_attention_virtual_batches(
            self.attention_chunk_size,
            cu_seqlens_q_np,
            seqlens_np,
            page_table_capture,
            self.page_size,
        )

        # Get exact dimensions from the calculation
        q_len = len(cu_seqlens_q_local_np)
        k_len = len(seqlens_k_local_np)
        b0 = block_table_local_np.shape[0] if block_table_local_np.shape[0] &gt; 0 else bs
        b1 = block_table_local_np.shape[1] if block_table_local_np.shape[1] &gt; 0 else 1

        # Create views of the pre-allocated buffers with exactly these sizes
        # This is the key optimization - we only use the memory we actually need
        local_query_start_loc = self.decode_cuda_graph_local_attn_metadata[
            &#34;local_query_start_loc&#34;
        ][:q_len]

        local_seqused_k = self.decode_cuda_graph_local_attn_metadata[&#34;local_seqused_k&#34;][
            :k_len
        ]

        local_block_table = self.decode_cuda_graph_local_attn_metadata[
            &#34;local_block_table&#34;
        ][:b0, :b1]

        metadata.local_attn_metadata = FlashAttentionMetadata.LocalAttentionMetadata(
            local_query_start_loc=local_query_start_loc,
            local_seqused_k=local_seqused_k,
            local_block_table=local_block_table,
            local_max_query_len=1,
            local_max_seq_len=max_seq_len,
        )

    def _update_local_attn_metadata_for_replay(
        self,
        metadata: FlashAttentionMetadata,
        bs: int,
    ):
        &#34;&#34;&#34;Update preallocated local attention metadata in-place before CUDA graph replay.&#34;&#34;&#34;
        if self.attention_chunk_size is None:
            return

        # Access preallocated buffers
        local_q_buf = self.decode_cuda_graph_local_attn_metadata[
            &#34;local_query_start_loc&#34;
        ]
        local_k_buf = self.decode_cuda_graph_local_attn_metadata[&#34;local_seqused_k&#34;]
        local_block_buf = self.decode_cuda_graph_local_attn_metadata[
            &#34;local_block_table&#34;
        ]
        cu_seqlens_q = self.decode_cuda_graph_metadata[&#34;cu_seqlens_q&#34;]

        # Create a modified version for local attention that only processes the last token
        # This mimics the normal decode pattern
        cu_seqlens_q = torch.arange(
            bs + 1, device=cu_seqlens_q.device, dtype=cu_seqlens_q.dtype
        )
        seqlens = metadata.cache_seqlens_int32[:bs]
        # Slice the page_table to match the batch size and actual sequence length
        # This serves three important purposes:
        # 1. Ensures we only process the actual batch size (bs) and not the maximum batch size
        # 2. Limits the sequence length to prevent processing padding tokens or garbage values
        # 3. Prevents zeros in the block table which can cause garbage output during replay
        #
        # Without this slicing, the pre-allocated page_table may contain zeros or invalid indices
        # beyond the actual sequence length, leading to incorrect attention calculations
        max_seq_len = int(seqlens.max().item())
        if self.is_hybrid:
            sliced_page_table = self.full_to_swa_index_mapping[
                metadata.page_table[:bs, :max_seq_len]
            ].to(torch.int32)
        else:
            sliced_page_table = metadata.page_table[:bs, :max_seq_len]

        cu_seqlens_q_np = cu_seqlens_q.cpu().numpy()
        seqlens_np = seqlens.cpu().numpy()
        (
            seqlens_q_local_np,
            cu_seqlens_q_local_np,
            seqlens_k_local_np,
            block_table_local,
        ) = make_local_attention_virtual_batches(
            self.attention_chunk_size,
            cu_seqlens_q_np,
            seqlens_np,
            sliced_page_table,
            self.page_size,
        )

        # Convert back to tensors
        device = local_q_buf.device
        cu_seqlens_q_local = torch.from_numpy(cu_seqlens_q_local_np).to(device)
        seqlens_k_local = torch.from_numpy(seqlens_k_local_np).to(device)
        block_table_local = block_table_local.to(device)
        # Get sizes
        q_len = cu_seqlens_q_local.shape[0]
        k_len = seqlens_k_local.shape[0]
        b0, b1 = block_table_local.shape

        # In-place updates into preallocated tensors and zero out the unused space
        local_q_buf[:q_len].copy_(cu_seqlens_q_local)
        local_q_buf[q_len:].fill_(0)
        local_k_buf[:k_len].copy_(seqlens_k_local)
        local_k_buf[k_len:].fill_(0)
        local_block_buf[:b0, :b1].copy_(block_table_local)
        local_block_buf[b0:, :].fill_(0)
        local_block_buf[:b0, b1:].fill_(0)

        if metadata.local_attn_metadata is not None:
            lam = metadata.local_attn_metadata
            lam.local_max_query_len = int(seqlens_q_local_np.max())
            lam.local_max_seq_len = int(seqlens_k_local_np.max())

    def _init_sliding_window_attn_spec_metadata(
        self,
        metadata: FlashAttentionMetadata,
        metadata_expand: FlashAttentionMetadata,
        metadata_swa: Optional[FlashAttentionMetadata] = None,
    ):
        # TODO: support page_size &gt; 1 for swa spec
        assert (
            self.page_size == 1
        ), &#34;FlashAttention backend doesn&#39;t support topk &gt; 1 speculative decoding with page size &gt; 1 sliding window attention&#34;

        cache_seqlens_int32 = (
            metadata.cache_seqlens_int32.repeat_interleave(
                self.speculative_num_draft_tokens
            )
            + metadata_expand.cache_seqlens_int32
        )
        cu_seqlens_k = torch.nn.functional.pad(
            torch.cumsum(cache_seqlens_int32, dim=0, dtype=torch.int32), (1, 0)
        )
        bs = cache_seqlens_int32.shape[0]
        page_table = (
            metadata.page_table.new_zeros(
                (bs, metadata.max_seq_len_k + metadata_expand.page_table.shape[1])
            )
            if metadata_swa is None
            else metadata_swa.page_table
        )

        prepare_swa_spec_page_table_triton(
            page_table,
            metadata.page_table,
            metadata_expand.page_table,
            metadata.cache_seqlens_int32,
            metadata_expand.cache_seqlens_int32,
            self.speculative_num_draft_tokens,
        )

        if metadata_swa is None:
            metadata_swa = FlashAttentionMetadata()
            metadata_swa.max_seq_len_q = 1
            metadata_swa.cu_seqlens_q = metadata_expand.cu_seqlens_q
            metadata_swa.cache_seqlens_int32 = cache_seqlens_int32
            metadata_swa.cu_seqlens_k = cu_seqlens_k
            metadata_swa.page_table = page_table
        else:
            metadata_swa.cache_seqlens_int32.copy_(cache_seqlens_int32)
            metadata_swa.cu_seqlens_k.copy_(cu_seqlens_k)

        metadata.swa_spec_metadata = metadata_swa</code></pre>
</details>
<div class="desc"><p>FlashAttention backend implementation.</p>
<p>Note about the init:
- If no spec decoding
- FlashAttentionBackend will be init once when the server starts.
- If spec decoding
- FlashAttentionBackend will be init once for the target worker
- FlashAttentionMultiStepBackend will be once for the draft worker
- It will spawn num_steps FlashAttentionBackend for the draft worker</p>
<p>Note about CUDA Graph:
- We only support CUDA Graph for Decode (Normal Decode and Draft Decode) and Target Verify.
- We don't support CUDA Graph for Extend and Draft Extend.
- When server init, init_cuda_graph_state will be called first and then init_cuda_graph_capture will be called.
- For each forward batch, init_replay_cuda_graph will be called first and then replay the graph.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.attention.base_attn_backend.AttentionBackend" href="base_attn_backend.html#sglang.srt.layers.attention.base_attn_backend.AttentionBackend">AttentionBackend</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend.get_cuda_graph_seq_len_fill_value"><code class="name flex">
<span>def <span class="ident">get_cuda_graph_seq_len_fill_value</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_cuda_graph_seq_len_fill_value(self):
    &#34;&#34;&#34;Get the fill value for sequence length in CUDA graph.&#34;&#34;&#34;
    return 1</code></pre>
</details>
<div class="desc"><p>Get the fill value for sequence length in CUDA graph.</p></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend.init_cuda_graph_state"><code class="name flex">
<span>def <span class="ident">init_cuda_graph_state</span></span>(<span>self, max_bs: int, max_num_tokens: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int):
    &#34;&#34;&#34;Initialize CUDA graph state for the attention backend.

    Args:
        max_bs (int): Maximum batch size to support in CUDA graphs

    This creates fixed-size tensors that will be reused during CUDA graph replay
    to avoid memory allocations.
    &#34;&#34;&#34;
    max_num_pages = (self.max_context_len + self.page_size - 1) // self.page_size

    # This is being used by normal decode and draft decode when topk == 1
    self.decode_cuda_graph_metadata = {
        &#34;cache_seqlens&#34;: torch.zeros(max_bs, dtype=torch.int32, device=self.device),
        &#34;cu_seqlens_q&#34;: torch.arange(
            0, max_bs + 1, dtype=torch.int32, device=self.device
        ),
        &#34;cu_seqlens_k&#34;: torch.zeros(
            max_bs + 1, dtype=torch.int32, device=self.device
        ),
        &#34;page_table&#34;: torch.zeros(
            max_bs,
            max_num_pages,
            dtype=torch.int32,
            device=self.device,
        ),
        &#34;strided_indices&#34;: torch.arange(
            0, self.max_context_len, self.page_size, device=self.device
        ),
    }
    # Only allocate local attention buffers if local attention is enabled
    # This prevents OOM errors when local attention is not being used
    if self.attention_chunk_size is not None:
        # Estimate maximum sizes for local attention metadata
        max_seq_len = self.max_context_len
        page_size = self.page_size or 1
        attn_chunk_size = self.attention_chunk_size
        max_virtual_batches = max_bs * (
            (max_seq_len + attn_chunk_size - 1) // attn_chunk_size
        )
        max_pages_per_block = (attn_chunk_size + page_size - 1) // page_size

        self.decode_cuda_graph_local_attn_metadata = {
            &#34;local_query_start_loc&#34;: torch.zeros(
                max_virtual_batches + 1, dtype=torch.int32, device=self.device
            ),
            &#34;local_seqused_k&#34;: torch.zeros(
                max_virtual_batches, dtype=torch.int32, device=self.device
            ),
            &#34;local_block_table&#34;: torch.zeros(
                max_virtual_batches,
                max_pages_per_block,
                dtype=torch.int32,
                device=self.device,
            ),
        }

    # This is used by draft decode&#39;s first half of metadata when topk &gt; 1
    if self.topk &gt; 1:
        self.draft_decode_metadata_topk_normal = {
            &#34;cache_seqlens&#34;: torch.zeros(
                max_bs, dtype=torch.int32, device=self.device
            ),
            &#34;cu_seqlens_q&#34;: torch.arange(
                0,
                max_bs * self.topk + 1,
                step=self.topk,
                dtype=torch.int32,
                device=self.device,
            ),
            &#34;cu_seqlens_k&#34;: torch.zeros(
                max_bs + 1, dtype=torch.int32, device=self.device
            ),
            &#34;page_table&#34;: torch.zeros(
                max_bs,
                self.max_context_len,
                dtype=torch.int32,
                device=self.device,
            ),
        }

        # This is used by draft decode&#39;s second half of metadata when topk &gt; 1
        decode_length = self.speculative_step_id + 1
        self.draft_decode_metadata_topk_expand = {
            &#34;cache_seqlens&#34;: torch.full(
                (max_bs * self.topk,),
                decode_length,
                device=self.device,
                dtype=torch.int32,
            ),
            &#34;cu_seqlens_q&#34;: torch.arange(
                0,
                max_bs * self.topk + 1,
                dtype=torch.int32,
                device=self.device,
            ),
            &#34;cu_seqlens_k&#34;: torch.arange(
                0,
                max_bs * self.topk * decode_length + 1,
                step=decode_length,
                dtype=torch.int32,
                device=self.device,
            ),
            &#34;page_table&#34;: torch.zeros(
                max_bs * self.topk,
                decode_length,
                dtype=torch.int32,
                device=self.device,
            ),
        }

    if (
        self.speculative_num_draft_tokens is not None
        and self.speculative_num_draft_tokens &gt; 0
    ):
        # &#34;page_table_draft_decode&#34; will be set only when spec decoding enabled to save memory
        self.decode_cuda_graph_metadata[&#34;page_table_draft_decode&#34;] = torch.zeros(
            max_bs,
            max_num_pages,
            dtype=torch.int32,
            device=self.device,
        )

        self.target_verify_metadata = {
            &#34;cache_seqlens&#34;: torch.zeros(
                max_bs, dtype=torch.int32, device=self.device
            ),
            &#34;cu_seqlens_q&#34;: torch.arange(
                0,
                max_bs * self.speculative_num_draft_tokens + 1,
                step=self.speculative_num_draft_tokens,
                dtype=torch.int32,
                device=self.device,
            ),
            &#34;cu_seqlens_k&#34;: torch.zeros(
                max_bs + 1, dtype=torch.int32, device=self.device
            ),
            &#34;page_table&#34;: torch.zeros(
                max_bs,
                max_num_pages,
                dtype=torch.int32,
                device=self.device,
            ),
            &#34;strided_indices&#34;: torch.arange(
                0, self.max_context_len, self.page_size, device=self.device
            ),
        }

        self.draft_extend_metadata = {
            &#34;cache_seqlens&#34;: torch.zeros(
                max_bs, dtype=torch.int32, device=self.device
            ),
            &#34;cu_seqlens_q&#34;: torch.zeros(
                max_bs + 1,
                dtype=torch.int32,
                device=self.device,
            ),
            &#34;cu_seqlens_k&#34;: torch.zeros(
                max_bs + 1, dtype=torch.int32, device=self.device
            ),
            &#34;page_table&#34;: torch.zeros(
                max_bs,
                max_num_pages,
                dtype=torch.int32,
                device=self.device,
            ),
            &#34;strided_indices&#34;: torch.arange(
                0, self.max_context_len, self.page_size, device=self.device
            ),
        }

    if self.topk &gt; 1:
        self.target_verify_metadata_topk_normal = {
            &#34;cache_seqlens&#34;: torch.zeros(
                max_bs, dtype=torch.int32, device=self.device
            ),
            &#34;cu_seqlens_q&#34;: torch.arange(
                0,
                max_bs * self.speculative_num_draft_tokens + 1,
                step=self.speculative_num_draft_tokens,
                dtype=torch.int32,
                device=self.device,
            ),
            &#34;cu_seqlens_k&#34;: torch.zeros(
                max_bs + 1, dtype=torch.int32, device=self.device
            ),
            &#34;page_table&#34;: torch.zeros(
                max_bs,
                self.max_context_len,
                dtype=torch.int32,
                device=self.device,
            ),
        }

        self.target_verify_metadata_topk_expand = {
            &#34;cache_seqlens&#34;: torch.zeros(
                max_bs * self.speculative_num_draft_tokens,
                dtype=torch.int32,
                device=self.device,
            ),
            &#34;cu_seqlens_k&#34;: torch.zeros(
                max_bs * self.speculative_num_draft_tokens + 1,
                dtype=torch.int32,
                device=self.device,
            ),
            &#34;cu_seqlens_q&#34;: torch.arange(
                0,
                max_bs * self.speculative_num_draft_tokens + 1,
                dtype=torch.int32,
                device=self.device,
            ),
            &#34;page_table&#34;: torch.zeros(
                max_bs * self.speculative_num_draft_tokens,
                self.speculative_num_draft_tokens,
                dtype=torch.int32,
                device=self.device,
            ),
        }

        if self.has_swa:
            self.target_verify_metadata_topk_swa = {
                &#34;cache_seqlens&#34;: torch.zeros(
                    max_bs * self.speculative_num_draft_tokens,
                    dtype=torch.int32,
                    device=self.device,
                ),
                &#34;cu_seqlens_k&#34;: torch.zeros(
                    max_bs * self.speculative_num_draft_tokens + 1,
                    dtype=torch.int32,
                    device=self.device,
                ),
                &#34;cu_seqlens_q&#34;: torch.arange(
                    0,
                    max_bs * self.speculative_num_draft_tokens + 1,
                    dtype=torch.int32,
                    device=self.device,
                ),
                &#34;page_table&#34;: torch.zeros(
                    max_bs * self.speculative_num_draft_tokens,
                    self.max_context_len,
                    dtype=torch.int32,
                    device=self.device,
                ),
            }

    self.encoder_metadata = {
        &#34;encoder_page_table&#34;: torch.zeros(
            max_bs,
            self.max_context_len,
            dtype=torch.int32,
            device=self.device,
        ),
        &#34;encoder_lens_int32&#34;: torch.zeros(
            max_bs, dtype=torch.int32, device=self.device
        ),
        &#34;encoder_cu_seqlens_k&#34;: torch.zeros(
            max_bs + 1, dtype=torch.int32, device=self.device
        ),
    }</code></pre>
</details>
<div class="desc"><p>Initialize CUDA graph state for the attention backend.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>max_bs</code></strong> :&ensp;<code>int</code></dt>
<dd>Maximum batch size to support in CUDA graphs</dd>
</dl>
<p>This creates fixed-size tensors that will be reused during CUDA graph replay
to avoid memory allocations.</p></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend.init_forward_metadata"><code class="name flex">
<span>def <span class="ident">init_forward_metadata</span></span>(<span>self, forward_batch: ForwardBatch)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_forward_metadata(self, forward_batch: ForwardBatch):
    &#34;&#34;&#34;Initialize forward metadata hence all layers in the forward pass can reuse it.&#34;&#34;&#34;
    metadata = FlashAttentionMetadata()
    seqlens_in_batch = forward_batch.seq_lens
    batch_size = forward_batch.batch_size
    device = seqlens_in_batch.device

    if forward_batch.forward_mode.is_decode_or_idle():
        # Draft Decode
        if forward_batch.spec_info is not None:
            if self.topk &lt;= 1:
                metadata.cache_seqlens_int32 = (
                    seqlens_in_batch + (self.speculative_step_id + 1)
                ).to(torch.int32)
                metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item() + (
                    self.speculative_step_id + 1
                )
                metadata.cu_seqlens_q = torch.arange(
                    0, batch_size + 1, dtype=torch.int32, device=device
                )
                metadata.cu_seqlens_k = torch.nn.functional.pad(
                    torch.cumsum(
                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32
                    ),
                    (1, 0),
                )
                metadata.page_table = forward_batch.req_to_token_pool.req_to_token[
                    forward_batch.req_pool_indices, : metadata.max_seq_len_k
                ]
            else:
                metadata.cache_seqlens_int32 = (seqlens_in_batch).to(torch.int32)
                metadata.max_seq_len_q = self.topk
                metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item()
                metadata.cu_seqlens_q = torch.arange(
                    0,
                    batch_size * self.topk + 1,
                    step=self.topk,
                    dtype=torch.int32,
                    device=device,
                )
                metadata.cu_seqlens_k = torch.nn.functional.pad(
                    torch.cumsum(
                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32
                    ),
                    (1, 0),
                )
                metadata.page_table = forward_batch.req_to_token_pool.req_to_token[
                    forward_batch.req_pool_indices, : metadata.max_seq_len_k
                ]

                metadata_expand = FlashAttentionMetadata()
                decode_length = self.speculative_step_id + 1
                metadata_expand.cache_seqlens_int32 = torch.full(
                    (seqlens_in_batch.numel() * self.topk,),
                    decode_length,
                    device=device,
                    dtype=torch.int32,
                )
                metadata_expand.max_seq_len_q = 1
                metadata_expand.cu_seqlens_q = torch.arange(
                    0,
                    metadata_expand.cache_seqlens_int32.numel() + 1,
                    dtype=torch.int32,
                    device=device,
                )
                metadata_expand.cu_seqlens_k = torch.arange(
                    0,
                    metadata_expand.cache_seqlens_int32.numel() * decode_length + 1,
                    step=decode_length,
                    dtype=torch.int32,
                    device=device,
                )
                # shape: [bs, num_steps, topk] -&gt; [bs x topk, num_steps]
                cache_loc = forward_batch.out_cache_loc.view(
                    -1, self.speculative_num_steps
                )
                metadata_expand.page_table = (
                    cache_loc[:, :decode_length].contiguous().to(torch.int32)
                )
                self.forward_metadata_spec_decode_expand = metadata_expand
        else:
            # Normal Decode
            metadata.cache_seqlens_int32 = seqlens_in_batch.to(torch.int32)
            metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item()
            metadata.cu_seqlens_q = torch.arange(
                0, batch_size + 1, dtype=torch.int32, device=device
            )
            metadata.cu_seqlens_k = torch.nn.functional.pad(
                torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0)
            )
            metadata.page_table = forward_batch.req_to_token_pool.req_to_token[
                forward_batch.req_pool_indices, : metadata.max_seq_len_k
            ]
        # TODO: we need to test this part for llama 4 eagle case
        self._init_local_attn_metadata(forward_batch, metadata, device)
    elif forward_batch.forward_mode.is_target_verify():
        if self.topk &lt;= 1:
            metadata.cache_seqlens_int32 = (
                forward_batch.seq_lens + self.speculative_num_draft_tokens
            ).to(torch.int32)
            metadata.max_seq_len_q = self.speculative_num_draft_tokens
            metadata.max_seq_len_k = (
                forward_batch.seq_lens_cpu.max().item()
                + self.speculative_num_draft_tokens
            )
            metadata.cu_seqlens_q = torch.arange(
                0,
                batch_size * self.speculative_num_draft_tokens + 1,
                self.speculative_num_draft_tokens,
                dtype=torch.int32,
                device=device,
            )
            metadata.cu_seqlens_k = torch.nn.functional.pad(
                torch.cumsum(
                    metadata.cache_seqlens_int32, dim=0, dtype=torch.int32
                ),
                (1, 0),
            )
            metadata.page_table = forward_batch.req_to_token_pool.req_to_token[
                forward_batch.req_pool_indices, : metadata.max_seq_len_k
            ]

            self._init_local_attn_metadata(forward_batch, metadata, device)
        else:
            metadata.cache_seqlens_int32 = forward_batch.seq_lens.to(torch.int32)
            metadata.max_seq_len_q = self.speculative_num_draft_tokens
            metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item()
            metadata.cu_seqlens_q = torch.arange(
                0,
                batch_size * self.speculative_num_draft_tokens + 1,
                step=self.speculative_num_draft_tokens,
                dtype=torch.int32,
                device=device,
            )
            metadata.cu_seqlens_k = torch.nn.functional.pad(
                torch.cumsum(
                    metadata.cache_seqlens_int32, dim=0, dtype=torch.int32
                ),
                (1, 0),
            )
            metadata.page_table = forward_batch.req_to_token_pool.req_to_token[
                forward_batch.req_pool_indices, : metadata.max_seq_len_k
            ]

            metadata_expand = FlashAttentionMetadata()

            metadata_expand.max_seq_len_q = 1
            metadata_expand.cu_seqlens_q = torch.arange(
                0,
                forward_batch.seq_lens.numel() * self.speculative_num_draft_tokens
                + 1,
                dtype=torch.int32,
                device=device,
            )

            # create expand page table
            offsets = torch.arange(
                self.speculative_num_draft_tokens, device=device
            ).unsqueeze(
                0
            )  # shape: (1, self.speculative_num_draft_tokens)
            cols = offsets.expand(
                forward_batch.seq_lens.numel(), -1
            ) + forward_batch.seq_lens.unsqueeze(1)
            cum_len = torch.nn.functional.pad(
                torch.cumsum(
                    (
                        forward_batch.seq_lens + self.speculative_num_draft_tokens
                    ).repeat_interleave(self.speculative_num_draft_tokens),
                    dim=0,
                ),
                (1, 0),
            )[:-1]
            mask_extraction_indices = (
                cols.repeat_interleave(self.speculative_num_draft_tokens, dim=0)
                + cum_len[:, None]
            ).view(1, -1)
            mask = forward_batch.spec_info.custom_mask[
                mask_extraction_indices
            ].view(
                -1, self.speculative_num_draft_tokens
            )  # (bsz * draft_num, draft_num)

            # shift table indices to avoid padding
            # non_masked_page_table [[8, 9, 10],   mask (display with int format) [[1, 0, 0],
            #                        [8, 9, 10],                                   [1, 1, 0],
            #                        [8, 9, 10]]                                   [1, 0, 1]]
            # if masked with padding [[8, 0, 0],   our mask without padding       [[8, 9, 10],
            #                        [8, 9, 0],                                    [8, 9, 10],
            #                        [8, 0, 10]]                                   [8, 10, 9]]
            # note here cache_seqlens_int32 is [1, 2, 2] so extra page indices will be ignored in each row
            col_indices = offsets.expand(
                mask.shape[0], self.speculative_num_draft_tokens
            )
            # Build keys: if an entry is valid (mask==True), keep its original index;
            # if not, add self.speculative_num_draft_tokens so that it sorts after all valid entries.
            keys = torch.where(
                mask, col_indices, col_indices + self.speculative_num_draft_tokens
            )
            _, sort_order = torch.sort(keys, dim=1)
            non_masked_page_table = (
                forward_batch.req_to_token_pool.req_to_token[
                    forward_batch.req_pool_indices, :
                ]
                .gather(1, cols)
                .repeat_interleave(self.speculative_num_draft_tokens, dim=0)
            )  # (bsz, draft_num)
            metadata_expand.page_table = non_masked_page_table.gather(1, sort_order)
            metadata_expand.cache_seqlens_int32 = mask.sum(dim=1).to(torch.int32)
            metadata_expand.cu_seqlens_k = torch.nn.functional.pad(
                torch.cumsum(
                    metadata_expand.cache_seqlens_int32, dim=0, dtype=torch.int32
                ),
                (1, 0),
            )
            self.forward_metadata_spec_decode_expand = metadata_expand

            if self.has_swa:
                self._init_sliding_window_attn_spec_metadata(
                    metadata, metadata_expand
                )

    elif forward_batch.forward_mode.is_extend_or_draft_extend_or_mixed():
        metadata.cache_seqlens_int32 = seqlens_in_batch.to(torch.int32)
        metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item()
        metadata.cu_seqlens_k = torch.nn.functional.pad(
            torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0)
        )
        metadata.page_table = forward_batch.req_to_token_pool.req_to_token[
            forward_batch.req_pool_indices, : metadata.max_seq_len_k
        ]

        if (
            any(forward_batch.extend_prefix_lens_cpu)
            or forward_batch.forward_mode == ForwardMode.DRAFT_EXTEND
        ):
            extend_seq_lens = forward_batch.extend_seq_lens
            metadata.max_seq_len_q = max(forward_batch.extend_seq_lens_cpu)
            metadata.cu_seqlens_q = torch.nn.functional.pad(
                torch.cumsum(extend_seq_lens, dim=0, dtype=torch.int32), (1, 0)
            )
        else:
            metadata.max_seq_len_q = metadata.max_seq_len_k
            metadata.cu_seqlens_q = metadata.cu_seqlens_k

        # Setup local attention if enabled
        if forward_batch.forward_mode == ForwardMode.EXTEND:
            self._init_local_attn_metadata(forward_batch, metadata, device)

    # Encoder metadata for cross attention
    if forward_batch.encoder_lens is not None:
        assert (
            forward_batch.encoder_lens.numel() == 1
        ), &#34;Only encoder size 1 is supported for now&#34;

        metadata.encoder_lens_int32 = forward_batch.encoder_lens.to(torch.int32)
        metadata.encoder_cu_seqlens_k = torch.nn.functional.pad(
            torch.cumsum(metadata.encoder_lens_int32, dim=0, dtype=torch.int32),
            (1, 0),
        )
        metadata.encoder_max_seq_len_k = metadata.encoder_lens_int32.max().item()
        metadata.encoder_page_table = forward_batch.req_to_token_pool.req_to_token[
            forward_batch.req_pool_indices, : metadata.encoder_max_seq_len_k
        ]

        # Currently only support forward_batch.encoder_lens.numel() == 1
        metadata.page_table = forward_batch.req_to_token_pool.req_to_token[
            forward_batch.req_pool_indices,
            metadata.encoder_max_seq_len_k : (
                metadata.encoder_max_seq_len_k + metadata.max_seq_len_k
            ),
        ]

    # Convert the page table to a strided format which is needed by FA3 API
    if self.page_size &gt; 1:
        self.strided_indices = torch.arange(
            0, metadata.page_table.shape[1], self.page_size, device=self.device
        )
        metadata.page_table = (
            metadata.page_table[:, self.strided_indices] // self.page_size
        )

    self.forward_metadata = metadata</code></pre>
</details>
<div class="desc"><p>Initialize forward metadata hence all layers in the forward pass can reuse it.</p></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend.init_forward_metadata_capture_cuda_graph"><code class="name flex">
<span>def <span class="ident">init_forward_metadata_capture_cuda_graph</span></span>(<span>self,<br>bs: int,<br>num_tokens: int,<br>req_pool_indices: torch.Tensor,<br>seq_lens: torch.Tensor,<br>encoder_lens: Optional[torch.Tensor],<br>forward_mode: ForwardMode,<br>spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_forward_metadata_capture_cuda_graph(
    self,
    bs: int,
    num_tokens: int,
    req_pool_indices: torch.Tensor,
    seq_lens: torch.Tensor,
    encoder_lens: Optional[torch.Tensor],
    forward_mode: ForwardMode,
    spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]],
):
    &#34;&#34;&#34;Initialize forward metadata for capturing CUDA graph.&#34;&#34;&#34;
    metadata = FlashAttentionMetadata()

    # metadata_expand is needed for Spec Decoding when top k &gt; 1
    metadata_expand = FlashAttentionMetadata()

    device = seq_lens.device
    if forward_mode.is_decode_or_idle():
        if spec_info is not None:
            # Draft Decode
            if self.topk &lt;= 1:
                # When topk = 1, we use the normal decode metadata
                metadata.cache_seqlens_int32 = self.decode_cuda_graph_metadata[
                    &#34;cache_seqlens&#34;
                ][:bs]
                metadata.max_seq_len_k = seq_lens.max().item() + (
                    self.speculative_step_id + 1
                )
                metadata.cu_seqlens_q = self.decode_cuda_graph_metadata[
                    &#34;cu_seqlens_q&#34;
                ][: bs + 1]
                metadata.cu_seqlens_k = torch.nn.functional.pad(
                    torch.cumsum(
                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32
                    ),
                    (1, 0),
                )
                metadata.page_table = self.decode_cuda_graph_metadata[
                    &#34;page_table_draft_decode&#34;
                ][:bs, :]
                self.decode_cuda_graph_metadata[bs] = metadata
            else:
                # When top k &gt; 1, we need two specific draft decode metadata, and then merge states
                # 1. The first half of metadata for prefix tokens
                metadata.cache_seqlens_int32 = (
                    self.draft_decode_metadata_topk_normal[&#34;cache_seqlens&#34;][:bs]
                )
                metadata.max_seq_len_q = self.topk
                metadata.max_seq_len_k = seq_lens.max().item()
                metadata.cu_seqlens_q = self.draft_decode_metadata_topk_normal[
                    &#34;cu_seqlens_q&#34;
                ][: bs + 1]
                metadata.cu_seqlens_k = self.draft_decode_metadata_topk_normal[
                    &#34;cu_seqlens_k&#34;
                ][: bs + 1]
                metadata.page_table = self.draft_decode_metadata_topk_normal[
                    &#34;page_table&#34;
                ][:bs, :]

                # 2. The second half of metadata for draft tokens (per_batch_num_tokens = topk)
                metadata_expand.cache_seqlens_int32 = (
                    self.draft_decode_metadata_topk_expand[&#34;cache_seqlens&#34;][
                        : bs * self.topk
                    ]
                )
                metadata_expand.max_seq_len_q = 1
                metadata_expand.cu_seqlens_q = (
                    self.draft_decode_metadata_topk_expand[&#34;cu_seqlens_q&#34;][
                        : bs * self.topk + 1
                    ]
                )
                metadata_expand.cu_seqlens_k = (
                    self.draft_decode_metadata_topk_expand[&#34;cu_seqlens_k&#34;][
                        : bs * self.topk + 1
                    ]
                )
                metadata_expand.page_table = self.draft_decode_metadata_topk_expand[
                    &#34;page_table&#34;
                ][: bs * self.topk]
                self.draft_decode_metadata_topk_normal[bs] = metadata
                self.draft_decode_metadata_topk_expand[bs] = metadata_expand
        else:
            # Normal Decode
            # Get sequence information
            metadata.cache_seqlens_int32 = seq_lens.to(torch.int32)
            batch_size = len(seq_lens)
            device = seq_lens.device
            metadata.cu_seqlens_k = torch.nn.functional.pad(
                torch.cumsum(seq_lens, dim=0, dtype=torch.int32), (1, 0)
            )
            # Precompute maximum sequence length
            metadata.max_seq_len_k = seq_lens.max().item()
            # Precompute page table
            metadata.page_table = self.decode_cuda_graph_metadata[&#34;page_table&#34;][
                :bs, :
            ]
            # Precompute cumulative sequence lengths
            metadata.cu_seqlens_q = torch.arange(
                0, batch_size + 1, dtype=torch.int32, device=device
            )
            self.decode_cuda_graph_metadata[bs] = metadata

            if self.attention_chunk_size is not None:
                self._update_local_attn_metadata_for_capture(metadata, batch_size)

    elif forward_mode.is_target_verify():
        if self.topk &lt;= 1:
            metadata.cache_seqlens_int32 = self.target_verify_metadata[
                &#34;cache_seqlens&#34;
            ][:bs]
            metadata.cache_seqlens_int32.copy_(
                (seq_lens + self.speculative_num_draft_tokens)
            )

            metadata.max_seq_len_q = self.speculative_num_draft_tokens
            metadata.max_seq_len_k = (
                seq_lens.max().item() + self.speculative_num_draft_tokens
            )

            metadata.cu_seqlens_q = torch.arange(
                0,
                bs * self.speculative_num_draft_tokens + 1,
                self.speculative_num_draft_tokens,
                dtype=torch.int32,
                device=device,
            )

            metadata.cu_seqlens_k = self.target_verify_metadata[&#34;cu_seqlens_k&#34;][
                : (bs + 1)
            ]

            metadata.page_table = self.target_verify_metadata[&#34;page_table&#34;][:bs, :]

            self.target_verify_metadata[bs] = metadata
        else:
            # When topk &gt; 1, we need two specific target verify metadata, and then merge states
            # 1. The first half of metadata for prefix tokens
            metadata.cache_seqlens_int32 = self.target_verify_metadata_topk_normal[
                &#34;cache_seqlens&#34;
            ][:bs]
            metadata.max_seq_len_q = self.speculative_num_draft_tokens
            # metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item(), do this in replay
            metadata.cu_seqlens_q = self.target_verify_metadata_topk_normal[
                &#34;cu_seqlens_q&#34;
            ][: bs + 1]
            metadata.cu_seqlens_k = self.target_verify_metadata_topk_normal[
                &#34;cu_seqlens_k&#34;
            ][: bs + 1]
            metadata.page_table = self.target_verify_metadata_topk_normal[
                &#34;page_table&#34;
            ][:bs, :]

            # 2. The second half of metadata for draft tokens (per_batch_num_tokens = topk)
            metadata_expand.cache_seqlens_int32 = (
                self.target_verify_metadata_topk_expand[&#34;cache_seqlens&#34;][
                    : bs * self.speculative_num_draft_tokens
                ]
            )
            metadata_expand.max_seq_len_q = 1
            metadata_expand.cu_seqlens_q = self.target_verify_metadata_topk_expand[
                &#34;cu_seqlens_q&#34;
            ][: bs * self.speculative_num_draft_tokens + 1]
            metadata_expand.cu_seqlens_k = self.target_verify_metadata_topk_expand[
                &#34;cu_seqlens_k&#34;
            ][: bs * self.speculative_num_draft_tokens + 1]

            metadata_expand.page_table = self.target_verify_metadata_topk_expand[
                &#34;page_table&#34;
            ][: bs * self.speculative_num_draft_tokens]

            self.target_verify_metadata_topk_normal[bs] = metadata
            self.target_verify_metadata_topk_expand[bs] = metadata_expand

            if self.has_swa:
                metadata_swa = FlashAttentionMetadata()
                metadata_swa.cache_seqlens_int32 = (
                    self.target_verify_metadata_topk_swa[&#34;cache_seqlens&#34;][
                        : bs * self.speculative_num_draft_tokens
                    ]
                )
                metadata_swa.max_seq_len_q = 1
                metadata_swa.cu_seqlens_q = self.target_verify_metadata_topk_swa[
                    &#34;cu_seqlens_q&#34;
                ][: bs * self.speculative_num_draft_tokens + 1]
                metadata_swa.cu_seqlens_k = self.target_verify_metadata_topk_swa[
                    &#34;cu_seqlens_k&#34;
                ][: bs * self.speculative_num_draft_tokens + 1]

                metadata_swa.page_table = self.target_verify_metadata_topk_swa[
                    &#34;page_table&#34;
                ][: bs * self.speculative_num_draft_tokens]
                self.target_verify_metadata_topk_swa[bs] = metadata_swa
                metadata.swa_spec_metadata = metadata_swa

    elif forward_mode.is_draft_extend():
        metadata.cache_seqlens_int32 = self.draft_extend_metadata[&#34;cache_seqlens&#34;][
            :bs
        ]
        metadata.cache_seqlens_int32.copy_(seq_lens)

        num_tokens_per_bs = num_tokens // bs
        metadata.max_seq_len_q = num_tokens_per_bs
        metadata.max_seq_len_k = seq_lens.max().item()

        metadata.cu_seqlens_q = torch.arange(
            0,
            bs * num_tokens_per_bs + 1,
            num_tokens_per_bs,
            dtype=torch.int32,
            device=device,
        )

        metadata.cu_seqlens_k = self.draft_extend_metadata[&#34;cu_seqlens_k&#34;][
            : (bs + 1)
        ]
        metadata.page_table = self.draft_extend_metadata[&#34;page_table&#34;][:bs, :]

        self.draft_extend_metadata[bs] = metadata

    if encoder_lens is not None:
        encoder_bs = encoder_lens.numel()
        metadata.encoder_lens_int32 = self.encoder_metadata[&#34;encoder_lens_int32&#34;][
            :encoder_bs
        ]
        metadata.encoder_cu_seqlens_k = self.encoder_metadata[
            &#34;encoder_cu_seqlens_k&#34;
        ][: (encoder_bs + 1)]

        metadata.encoder_page_table = self.encoder_metadata[&#34;encoder_page_table&#34;][
            :bs, :
        ]

    self.forward_metadata = metadata
    self.forward_metadata_spec_decode_expand = metadata_expand</code></pre>
</details>
<div class="desc"><p>Initialize forward metadata for capturing CUDA graph.</p></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend.init_forward_metadata_replay_cuda_graph"><code class="name flex">
<span>def <span class="ident">init_forward_metadata_replay_cuda_graph</span></span>(<span>self,<br>bs: int,<br>req_pool_indices: torch.Tensor,<br>seq_lens: torch.Tensor,<br>seq_lens_sum: int,<br>encoder_lens: Optional[torch.Tensor],<br>forward_mode: ForwardMode,<br>spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]],<br>seq_lens_cpu: Optional[torch.Tensor],<br>out_cache_loc: Optional[torch.Tensor] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_forward_metadata_replay_cuda_graph(
    self,
    bs: int,
    req_pool_indices: torch.Tensor,
    seq_lens: torch.Tensor,
    seq_lens_sum: int,
    encoder_lens: Optional[torch.Tensor],
    forward_mode: ForwardMode,
    spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]],
    seq_lens_cpu: Optional[torch.Tensor],
    out_cache_loc: Optional[torch.Tensor] = None,
):
    &#34;&#34;&#34;Initialize forward metadata for replaying CUDA graph.&#34;&#34;&#34;
    seq_lens = seq_lens[:bs]
    seq_lens_cpu = seq_lens_cpu[:bs]
    req_pool_indices = req_pool_indices[:bs]
    device = seq_lens.device
    metadata = None
    metadata_expand = None

    if forward_mode.is_decode_or_idle():

        if spec_info is not None:
            # Draft Decode
            if self.topk &lt;= 1:
                # When topk = 1, we use the normal decode metadata
                metadata = self.decode_cuda_graph_metadata[bs]
                max_len = seq_lens_cpu.max().item()
                metadata.max_seq_len_k = max_len + self.speculative_step_id + 1
                max_seq_pages = (
                    metadata.max_seq_len_k + self.page_size - 1
                ) // self.page_size

                normal_decode_set_metadata(
                    metadata.cache_seqlens_int32,
                    metadata.cu_seqlens_k,
                    metadata.page_table,
                    self.req_to_token,
                    req_pool_indices,
                    self.decode_cuda_graph_metadata[&#34;strided_indices&#34;],
                    max_seq_pages,
                    seq_lens,
                    self.speculative_step_id + 1,
                    self.page_size,
                )

            else:
                # When top k &gt; 1, we need two specific draft decode metadata, and then merge states
                # 1. The first half of metadata for prefix tokens
                metadata = self.draft_decode_metadata_topk_normal[bs]
                metadata.cache_seqlens_int32.copy_(seq_lens)
                # metadata.max_seq_len_q = self.topk, already set in capture
                metadata.max_seq_len_k = seq_lens_cpu.max().item()
                # metadata.cu_seqlens_q already set in capture
                metadata.cu_seqlens_k[1:].copy_(
                    torch.cumsum(
                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32
                    )
                )

                page_table = self.req_to_token[
                    req_pool_indices, : metadata.max_seq_len_k
                ]

                metadata.page_table[:, : metadata.max_seq_len_k].copy_(page_table)

                # 2. The second half of metadata for draft tokens (per_batch_num_tokens = topk)
                metadata_expand = self.draft_decode_metadata_topk_expand[bs]
                decode_length = self.speculative_step_id + 1
                # shape: [bs, num_steps, topk] -&gt; [bs x topk, num_steps]
                cache_loc = out_cache_loc.view(-1, self.speculative_num_steps)
                metadata_expand.page_table[: cache_loc.shape[0]].copy_(
                    cache_loc[:, :decode_length]
                )
            # TODO: Handle local attention metadata for draft decode when llama4 eagle is supported
        else:
            # Normal Decode
            metadata = self.decode_cuda_graph_metadata[bs]
            max_len = seq_lens_cpu.max().item()
            max_seq_pages = (max_len + self.page_size - 1) // self.page_size
            metadata.max_seq_len_k = max_len

            normal_decode_set_metadata(
                metadata.cache_seqlens_int32,
                metadata.cu_seqlens_k,
                metadata.page_table,
                self.req_to_token,
                req_pool_indices,
                self.decode_cuda_graph_metadata[&#34;strided_indices&#34;],
                max_seq_pages,
                seq_lens,
                0,
                self.page_size,
            )

            self._update_local_attn_metadata_for_replay(
                metadata,
                bs,
            )
    elif forward_mode.is_target_verify():
        if self.topk &lt;= 1:
            metadata = self.target_verify_metadata[bs]
            metadata.cache_seqlens_int32.copy_(
                (seq_lens + self.speculative_num_draft_tokens)
            )

            metadata.max_seq_len_k = (
                seq_lens_cpu.max().item() + self.speculative_num_draft_tokens
            )
            metadata.cu_seqlens_k[1:].copy_(
                torch.cumsum(metadata.cache_seqlens_int32, dim=0, dtype=torch.int32)
            )
            max_seq_pages = (
                metadata.max_seq_len_k + self.page_size - 1
            ) // self.page_size
            page_indices = self.req_to_token[
                req_pool_indices[:, None],
                self.decode_cuda_graph_metadata[&#34;strided_indices&#34;][:max_seq_pages],
            ]
            page_indices //= self.page_size
            metadata.page_table[:, :max_seq_pages].copy_(page_indices)
        else:
            # When topk &gt; 1, we need two specific target verify metadata, and then merge states
            # 1. The first half of metadata for prefix tokens
            metadata = self.target_verify_metadata_topk_normal[bs]
            metadata.cache_seqlens_int32.copy_(seq_lens)
            # metadata.max_seq_len_q = self.speculative_num_draft_tokens, already set in capture
            metadata.max_seq_len_k = seq_lens_cpu.max().item()
            # metadata.cu_seqlens_q already set in capture
            metadata.cu_seqlens_k[1:].copy_(
                torch.cumsum(metadata.cache_seqlens_int32, dim=0, dtype=torch.int32)
            )
            page_table = self.req_to_token[
                req_pool_indices, : metadata.max_seq_len_k
            ]
            metadata.page_table[:, : metadata.max_seq_len_k].copy_(page_table)

            # 2. The second half of metadata for draft tokens (per_batch_num_tokens = topk)
            metadata_expand = self.target_verify_metadata_topk_expand[bs]

            # metadata_expand.max_seq_len_q = 1, already set in capture
            # metadata_expand.cu_seqlens_q already set in capture
            offsets = torch.arange(
                self.speculative_num_draft_tokens, device=device
            ).unsqueeze(
                0
            )  # shape: (1, self.speculative_num_draft_tokens)

            cols = offsets.expand(seq_lens.numel(), -1) + seq_lens.unsqueeze(1)
            cum_len = torch.nn.functional.pad(
                torch.cumsum(
                    (
                        seq_lens + self.speculative_num_draft_tokens
                    ).repeat_interleave(self.speculative_num_draft_tokens),
                    dim=0,
                ),
                (1, 0),
            )[:-1]
            mask_extraction_indices = (
                cols.repeat_interleave(self.speculative_num_draft_tokens, dim=0)
                + cum_len[:, None]
            ).view(1, -1)
            # avoid extracting padded seq indices which will be out of boundary
            mask_extraction_indices[
                :,
                spec_info.positions.numel() * self.speculative_num_draft_tokens :,
            ].fill_(0)
            mask = spec_info.custom_mask[mask_extraction_indices].view(
                -1, self.speculative_num_draft_tokens
            )  # (bsz * draft_num, draft_num)

            col_indices = offsets.expand(
                mask.shape[0], self.speculative_num_draft_tokens
            )
            keys = torch.where(
                mask,
                col_indices,
                col_indices + self.speculative_num_draft_tokens,
            )
            _, sort_order = torch.sort(keys, dim=1)

            non_masked_page_table = (
                self.req_to_token[req_pool_indices, :]
                .gather(1, cols)
                .repeat_interleave(self.speculative_num_draft_tokens, dim=0)
            )  # (bsz, draft_num)

            metadata_expand.page_table.copy_(
                non_masked_page_table.gather(1, sort_order)
            )
            metadata_expand.cache_seqlens_int32.copy_(mask.sum(dim=1))
            metadata_expand.cu_seqlens_k[1:].copy_(
                torch.cumsum(
                    metadata_expand.cache_seqlens_int32,
                    dim=0,
                    dtype=torch.int32,
                )
            )

            if self.has_swa:
                metadata_swa = self.target_verify_metadata_topk_swa[bs]
                self._init_sliding_window_attn_spec_metadata(
                    metadata, metadata_expand, metadata_swa
                )

    elif forward_mode.is_draft_extend():
        metadata = self.draft_extend_metadata[bs]
        metadata.cache_seqlens_int32.copy_(seq_lens)

        metadata.max_seq_len_k = seq_lens_cpu.max().item()
        metadata.cu_seqlens_k[1:].copy_(
            torch.cumsum(metadata.cache_seqlens_int32, dim=0, dtype=torch.int32)
        )
        accept_length = spec_info.accept_length[:bs]
        if spec_info.accept_length_cpu:
            metadata.max_seq_len_q = max(spec_info.accept_length_cpu) + 1
        else:
            metadata.max_seq_len_q = 1

        metadata.cu_seqlens_q[1:].copy_(
            torch.cumsum(accept_length, dim=0, dtype=torch.int32)
        )

        max_seq_pages = (
            metadata.max_seq_len_k + self.page_size - 1
        ) // self.page_size
        page_indices = self.req_to_token[
            req_pool_indices[:, None],
            self.draft_extend_metadata[&#34;strided_indices&#34;][:max_seq_pages],
        ]
        metadata.page_table[:, :max_seq_pages].copy_(page_indices // self.page_size)

    if encoder_lens is not None:
        # Only support encoder size 1 for now
        metadata.encoder_max_seq_len_k = encoder_lens[0]
        metadata.encoder_lens_int32.copy_(encoder_lens[:1])
        metadata.encoder_cu_seqlens_k[1:].copy_(
            torch.cumsum(metadata.encoder_lens_int32, dim=0, dtype=torch.int32)
        )

        metadata.encoder_page_table[:, : metadata.encoder_max_seq_len_k].copy_(
            self.req_to_token[req_pool_indices, : metadata.encoder_max_seq_len_k]
        )

        # Update the regular page table
        page_table = self.req_to_token[
            req_pool_indices,
            metadata.encoder_max_seq_len_k : (
                metadata.encoder_max_seq_len_k + metadata.max_seq_len_k
            ),
        ]
        metadata.page_table[:, : metadata.max_seq_len_k].copy_(page_table)

    self.forward_metadata = metadata
    self.forward_metadata_spec_decode_expand = metadata_expand</code></pre>
</details>
<div class="desc"><p>Initialize forward metadata for replaying CUDA graph.</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.layers.attention.base_attn_backend.AttentionBackend" href="base_attn_backend.html#sglang.srt.layers.attention.base_attn_backend.AttentionBackend">AttentionBackend</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.layers.attention.base_attn_backend.AttentionBackend.forward" href="base_attn_backend.html#sglang.srt.layers.attention.base_attn_backend.AttentionBackend.forward">forward</a></code></li>
<li><code><a title="sglang.srt.layers.attention.base_attn_backend.AttentionBackend.forward_decode" href="base_attn_backend.html#sglang.srt.layers.attention.base_attn_backend.AttentionBackend.forward_decode">forward_decode</a></code></li>
<li><code><a title="sglang.srt.layers.attention.base_attn_backend.AttentionBackend.forward_extend" href="base_attn_backend.html#sglang.srt.layers.attention.base_attn_backend.AttentionBackend.forward_extend">forward_extend</a></code></li>
<li><code><a title="sglang.srt.layers.attention.base_attn_backend.AttentionBackend.support_triton" href="base_attn_backend.html#sglang.srt.layers.attention.base_attn_backend.AttentionBackend.support_triton">support_triton</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata"><code class="flex name class">
<span>class <span class="ident">FlashAttentionMetadata</span></span>
<span>(</span><span>cache_seqlens_int32: torch.Tensor = None,<br>max_seq_len_q: int = 1,<br>max_seq_len_k: int = 0,<br>cu_seqlens_q: torch.Tensor = None,<br>cu_seqlens_k: torch.Tensor = None,<br>window_size: tuple = (-1, -1),<br>page_table: torch.Tensor = None,<br>encoder_cu_seqlens_k: torch.Tensor = None,<br>encoder_max_seq_len_k: int = 0,<br>encoder_lens_int32: torch.Tensor = None,<br>encoder_page_table: torch.Tensor = None,<br>local_attn_metadata: Optional[LocalAttentionMetadata] = None,<br>swa_spec_metadata: Optional[<a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata">FlashAttentionMetadata</a>] = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class FlashAttentionMetadata:
    &#34;&#34;&#34;Metadata to be init once in the model forward pass,
    each layer&#39;s forward pass can reuse the metadata.

    For each init metadata function, we will try set up them in below order
    &#34;&#34;&#34;

    # Sequence lengths for the forward batch
    cache_seqlens_int32: torch.Tensor = None
    # Maximum sequence length for query
    max_seq_len_q: int = 1
    # Maximum sequence length for key
    max_seq_len_k: int = 0
    # Cumulative sequence lengths for query
    cu_seqlens_q: torch.Tensor = None
    # Cumulative sequence lengths for key
    cu_seqlens_k: torch.Tensor = None
    # Window size (typically used by Gemma)
    window_size: tuple = (-1, -1)
    # Page table, the index of KV Cache Tables/Blocks
    page_table: torch.Tensor = None

    # Encoder metadata
    # Cumulative sequence lengths for encoder key
    encoder_cu_seqlens_k: torch.Tensor = None
    # Maximum sequence length for encoder key
    encoder_max_seq_len_k: int = 0
    # Sequence lengths for the forward batch
    encoder_lens_int32: torch.Tensor = None
    # Page table for the encoder
    encoder_page_table: torch.Tensor = None

    @dataclass
    class LocalAttentionMetadata:
        local_query_start_loc: torch.Tensor = None  # cu_seqlens_q for local attention
        local_seqused_k: torch.Tensor = None  # sequence lengths for local attention
        local_block_table: torch.Tensor = None  # block table for local attention
        local_max_query_len: int = 0  # max query length for local attention
        local_max_seq_len: int = 0  # max sequence length for local attention

    local_attn_metadata: Optional[LocalAttentionMetadata] = None

    # For sliding window attention topk&gt;1 spec decoding
    swa_spec_metadata: Optional[FlashAttentionMetadata] = None</code></pre>
</details>
<div class="desc"><p>Metadata to be init once in the model forward pass,
each layer's forward pass can reuse the metadata.</p>
<p>For each init metadata function, we will try set up them in below order</p></div>
<h3>Class variables</h3>
<dl>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.LocalAttentionMetadata"><code class="name">var <span class="ident">LocalAttentionMetadata</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.cache_seqlens_int32"><code class="name">var <span class="ident">cache_seqlens_int32</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.cu_seqlens_k"><code class="name">var <span class="ident">cu_seqlens_k</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.cu_seqlens_q"><code class="name">var <span class="ident">cu_seqlens_q</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.encoder_cu_seqlens_k"><code class="name">var <span class="ident">encoder_cu_seqlens_k</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.encoder_lens_int32"><code class="name">var <span class="ident">encoder_lens_int32</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.encoder_max_seq_len_k"><code class="name">var <span class="ident">encoder_max_seq_len_k</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.encoder_page_table"><code class="name">var <span class="ident">encoder_page_table</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.local_attn_metadata"><code class="name">var <span class="ident">local_attn_metadata</span> : <a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.LocalAttentionMetadata" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.LocalAttentionMetadata">FlashAttentionMetadata.LocalAttentionMetadata</a> | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.max_seq_len_k"><code class="name">var <span class="ident">max_seq_len_k</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.max_seq_len_q"><code class="name">var <span class="ident">max_seq_len_q</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.page_table"><code class="name">var <span class="ident">page_table</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.swa_spec_metadata"><code class="name">var <span class="ident">swa_spec_metadata</span> : <a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata">FlashAttentionMetadata</a> | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.window_size"><code class="name">var <span class="ident">window_size</span> : tuple</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMultiStepBackend"><code class="flex name class">
<span>class <span class="ident">FlashAttentionMultiStepBackend</span></span>
<span>(</span><span>model_runner: ModelRunner, topk: int, speculative_num_steps: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FlashAttentionMultiStepBackend:

    def __init__(
        self, model_runner: ModelRunner, topk: int, speculative_num_steps: int
    ):
        self.model_runner = model_runner
        self.topk = topk
        self.speculative_num_steps = speculative_num_steps
        self.attn_backends = []
        for i in range(self.speculative_num_steps):
            self.attn_backends.append(
                FlashAttentionBackend(
                    model_runner,
                    speculative_step_id=i,
                    topk=self.topk,
                    speculative_num_steps=self.speculative_num_steps,
                )
            )

    def init_forward_metadata(self, forward_batch: ForwardBatch):
        for i in range(self.speculative_num_steps - 1):
            self.attn_backends[i].init_forward_metadata(forward_batch)

    def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int):
        for i in range(self.speculative_num_steps):
            self.attn_backends[i].init_cuda_graph_state(max_bs, max_num_tokens)

    def init_forward_metadata_capture_cuda_graph(
        self,
        forward_batch: ForwardBatch,
    ):
        assert forward_batch.spec_info is not None
        assert isinstance(forward_batch.spec_info, EagleDraftInput)

        for i in range(self.speculative_num_steps - 1):
            self.attn_backends[i].init_forward_metadata_capture_cuda_graph(
                forward_batch.batch_size,
                forward_batch.batch_size * self.topk,
                forward_batch.req_pool_indices,
                forward_batch.seq_lens,
                encoder_lens=forward_batch.encoder_lens,
                forward_mode=ForwardMode.DECODE,
                spec_info=forward_batch.spec_info,
            )

    def init_forward_metadata_replay_cuda_graph(
        self, forward_batch: ForwardBatch, bs: int
    ):
        assert forward_batch.spec_info is not None
        assert isinstance(forward_batch.spec_info, EagleDraftInput)

        for i in range(self.speculative_num_steps - 1):
            # TODO: incrementally update the metadata for the later steps,
            # so that they do not need to recompute everything from scratch.
            self.attn_backends[i].init_forward_metadata_replay_cuda_graph(
                bs,
                forward_batch.req_pool_indices,
                forward_batch.seq_lens,
                forward_batch.seq_lens_sum,
                encoder_lens=forward_batch.encoder_lens,
                forward_mode=ForwardMode.DECODE,
                spec_info=forward_batch.spec_info,
                seq_lens_cpu=forward_batch.seq_lens_cpu,
                out_cache_loc=forward_batch.out_cache_loc,
            )</code></pre>
</details>
<div class="desc"></div>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMultiStepBackend.init_cuda_graph_state"><code class="name flex">
<span>def <span class="ident">init_cuda_graph_state</span></span>(<span>self, max_bs: int, max_num_tokens: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int):
    for i in range(self.speculative_num_steps):
        self.attn_backends[i].init_cuda_graph_state(max_bs, max_num_tokens)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMultiStepBackend.init_forward_metadata"><code class="name flex">
<span>def <span class="ident">init_forward_metadata</span></span>(<span>self, forward_batch: ForwardBatch)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_forward_metadata(self, forward_batch: ForwardBatch):
    for i in range(self.speculative_num_steps - 1):
        self.attn_backends[i].init_forward_metadata(forward_batch)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMultiStepBackend.init_forward_metadata_capture_cuda_graph"><code class="name flex">
<span>def <span class="ident">init_forward_metadata_capture_cuda_graph</span></span>(<span>self, forward_batch: ForwardBatch)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_forward_metadata_capture_cuda_graph(
    self,
    forward_batch: ForwardBatch,
):
    assert forward_batch.spec_info is not None
    assert isinstance(forward_batch.spec_info, EagleDraftInput)

    for i in range(self.speculative_num_steps - 1):
        self.attn_backends[i].init_forward_metadata_capture_cuda_graph(
            forward_batch.batch_size,
            forward_batch.batch_size * self.topk,
            forward_batch.req_pool_indices,
            forward_batch.seq_lens,
            encoder_lens=forward_batch.encoder_lens,
            forward_mode=ForwardMode.DECODE,
            spec_info=forward_batch.spec_info,
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMultiStepBackend.init_forward_metadata_replay_cuda_graph"><code class="name flex">
<span>def <span class="ident">init_forward_metadata_replay_cuda_graph</span></span>(<span>self, forward_batch: ForwardBatch, bs: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_forward_metadata_replay_cuda_graph(
    self, forward_batch: ForwardBatch, bs: int
):
    assert forward_batch.spec_info is not None
    assert isinstance(forward_batch.spec_info, EagleDraftInput)

    for i in range(self.speculative_num_steps - 1):
        # TODO: incrementally update the metadata for the later steps,
        # so that they do not need to recompute everything from scratch.
        self.attn_backends[i].init_forward_metadata_replay_cuda_graph(
            bs,
            forward_batch.req_pool_indices,
            forward_batch.seq_lens,
            forward_batch.seq_lens_sum,
            encoder_lens=forward_batch.encoder_lens,
            forward_mode=ForwardMode.DECODE,
            spec_info=forward_batch.spec_info,
            seq_lens_cpu=forward_batch.seq_lens_cpu,
            out_cache_loc=forward_batch.out_cache_loc,
        )</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.layers.attention" href="index.html">sglang.srt.layers.attention</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.cdiv" href="#sglang.srt.layers.attention.flashattention_backend.cdiv">cdiv</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.make_local_attention_virtual_batches" href="#sglang.srt.layers.attention.flashattention_backend.make_local_attention_virtual_batches">make_local_attention_virtual_batches</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.merge_state_v2_wrapper" href="#sglang.srt.layers.attention.flashattention_backend.merge_state_v2_wrapper">merge_state_v2_wrapper</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.normal_decode_set_metadata" href="#sglang.srt.layers.attention.flashattention_backend.normal_decode_set_metadata">normal_decode_set_metadata</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.prepare_swa_spec_page_table_triton" href="#sglang.srt.layers.attention.flashattention_backend.prepare_swa_spec_page_table_triton">prepare_swa_spec_page_table_triton</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend">FlashAttentionBackend</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend.get_cuda_graph_seq_len_fill_value" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend.get_cuda_graph_seq_len_fill_value">get_cuda_graph_seq_len_fill_value</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend.init_cuda_graph_state" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend.init_cuda_graph_state">init_cuda_graph_state</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend.init_forward_metadata" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend.init_forward_metadata">init_forward_metadata</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend.init_forward_metadata_capture_cuda_graph" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend.init_forward_metadata_capture_cuda_graph">init_forward_metadata_capture_cuda_graph</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend.init_forward_metadata_replay_cuda_graph" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend.init_forward_metadata_replay_cuda_graph">init_forward_metadata_replay_cuda_graph</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata">FlashAttentionMetadata</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.LocalAttentionMetadata" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.LocalAttentionMetadata">LocalAttentionMetadata</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.cache_seqlens_int32" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.cache_seqlens_int32">cache_seqlens_int32</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.cu_seqlens_k" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.cu_seqlens_k">cu_seqlens_k</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.cu_seqlens_q" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.cu_seqlens_q">cu_seqlens_q</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.encoder_cu_seqlens_k" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.encoder_cu_seqlens_k">encoder_cu_seqlens_k</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.encoder_lens_int32" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.encoder_lens_int32">encoder_lens_int32</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.encoder_max_seq_len_k" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.encoder_max_seq_len_k">encoder_max_seq_len_k</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.encoder_page_table" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.encoder_page_table">encoder_page_table</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.local_attn_metadata" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.local_attn_metadata">local_attn_metadata</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.max_seq_len_k" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.max_seq_len_k">max_seq_len_k</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.max_seq_len_q" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.max_seq_len_q">max_seq_len_q</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.page_table" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.page_table">page_table</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.swa_spec_metadata" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.swa_spec_metadata">swa_spec_metadata</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.window_size" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMetadata.window_size">window_size</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMultiStepBackend" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMultiStepBackend">FlashAttentionMultiStepBackend</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMultiStepBackend.init_cuda_graph_state" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMultiStepBackend.init_cuda_graph_state">init_cuda_graph_state</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMultiStepBackend.init_forward_metadata" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMultiStepBackend.init_forward_metadata">init_forward_metadata</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMultiStepBackend.init_forward_metadata_capture_cuda_graph" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMultiStepBackend.init_forward_metadata_capture_cuda_graph">init_forward_metadata_capture_cuda_graph</a></code></li>
<li><code><a title="sglang.srt.layers.attention.flashattention_backend.FlashAttentionMultiStepBackend.init_forward_metadata_replay_cuda_graph" href="#sglang.srt.layers.attention.flashattention_backend.FlashAttentionMultiStepBackend.init_forward_metadata_replay_cuda_graph">init_forward_metadata_replay_cuda_graph</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
