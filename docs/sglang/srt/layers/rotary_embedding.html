<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>sglang.srt.layers.rotary_embedding API documentation</title>
<meta name="description" content="Rotary Positional Embeddings.">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sglang.srt.layers.rotary_embedding</code></h1>
</header>
<section id="section-intro">
<p>Rotary Positional Embeddings.</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sglang.srt.layers.rotary_embedding.apply_rotary_pos_emb"><code class="name flex">
<span>def <span class="ident">apply_rotary_pos_emb</span></span>(<span>q: torch.Tensor,<br>k: torch.Tensor,<br>cos: torch.Tensor,<br>sin: torch.Tensor,<br>unsqueeze_dim=1) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_rotary_pos_emb_native(
    q: torch.Tensor,
    k: torch.Tensor,
    cos: torch.Tensor,
    sin: torch.Tensor,
    unsqueeze_dim=1,
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    orig_q_dtype = q.dtype
    orig_k_dtype = k.dtype
    q, k = q.float(), k.float()

    # embedding is performed in float
    cos = cos.unsqueeze(unsqueeze_dim).float()
    sin = sin.unsqueeze(unsqueeze_dim).float()
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)

    q_embed = q_embed.to(orig_q_dtype)
    k_embed = k_embed.to(orig_k_dtype)

    return q_embed, k_embed</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.apply_rotary_pos_emb_native"><code class="name flex">
<span>def <span class="ident">apply_rotary_pos_emb_native</span></span>(<span>q: torch.Tensor,<br>k: torch.Tensor,<br>cos: torch.Tensor,<br>sin: torch.Tensor,<br>unsqueeze_dim=1) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_rotary_pos_emb_native(
    q: torch.Tensor,
    k: torch.Tensor,
    cos: torch.Tensor,
    sin: torch.Tensor,
    unsqueeze_dim=1,
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    orig_q_dtype = q.dtype
    orig_k_dtype = k.dtype
    q, k = q.float(), k.float()

    # embedding is performed in float
    cos = cos.unsqueeze(unsqueeze_dim).float()
    sin = sin.unsqueeze(unsqueeze_dim).float()
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)

    q_embed = q_embed.to(orig_q_dtype)
    k_embed = k_embed.to(orig_k_dtype)

    return q_embed, k_embed</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.apply_rotary_pos_emb_npu"><code class="name flex">
<span>def <span class="ident">apply_rotary_pos_emb_npu</span></span>(<span>q: torch.Tensor,<br>k: torch.Tensor,<br>cos: torch.Tensor,<br>sin: torch.Tensor,<br>unsqueeze_dim=1) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_rotary_pos_emb_npu(
    q: torch.Tensor,
    k: torch.Tensor,
    cos: torch.Tensor,
    sin: torch.Tensor,
    unsqueeze_dim=1,
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    if q.shape[1] != 128:
        return apply_rotary_pos_emb_native(q, k, cos, sin, unsqueeze_dim)
    cos = cos.unsqueeze(unsqueeze_dim)
    cos = torch.transpose(cos, 1, 2)
    sin = sin.unsqueeze(unsqueeze_dim)
    sin = torch.transpose(sin, 1, 2)
    q = torch.transpose(q, 1, 2)
    k = torch.transpose(k, 1, 2)
    q_embed, k_embed = torch_npu.npu_apply_rotary_pos_emb(q, k, cos, sin)
    q_embed = torch.transpose(q_embed, 1, 2)
    k_embed = torch.transpose(k_embed, 1, 2)
    return q_embed, k_embed</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.get_rope"><code class="name flex">
<span>def <span class="ident">get_rope</span></span>(<span>head_size: int,<br>rotary_dim: int,<br>max_position: int,<br>base: int,<br>is_neox_style: bool = True,<br>rope_scaling: Dict[str, Any] | None = None,<br>dtype: torch.dtype | None = None,<br>partial_rotary_factor: float = 1.0,<br>dual_chunk_attention_config: Dict[str, Any] | None = None) ‑> <a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding">RotaryEmbedding</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_rope(
    head_size: int,
    rotary_dim: int,
    max_position: int,
    base: int,
    is_neox_style: bool = True,
    rope_scaling: Optional[Dict[str, Any]] = None,
    dtype: Optional[torch.dtype] = None,
    partial_rotary_factor: float = 1.0,
    dual_chunk_attention_config: Optional[Dict[str, Any]] = None,
) -&gt; RotaryEmbedding:
    if dtype is None:
        dtype = torch.get_default_dtype()
    if rope_scaling is not None:
        # Transforms every value that is a list into a tuple for caching calls
        rope_scaling_tuple = {
            k: tuple(v) if isinstance(v, list) else v for k, v in rope_scaling.items()
        }
        rope_scaling_args = tuple(rope_scaling_tuple.items())
    else:
        rope_scaling_args = None

    if dual_chunk_attention_config is not None:
        dual_chunk_attention_tuple = {
            k: tuple(v) if isinstance(v, list) else v
            for k, v in dual_chunk_attention_config.items()
            if k != &#34;sparse_attention_config&#34;
        }
        dual_chunk_attention_args = tuple(dual_chunk_attention_tuple.items())
    else:
        dual_chunk_attention_args = None

    if partial_rotary_factor &lt; 1.0:
        rotary_dim = int(rotary_dim * partial_rotary_factor)
    key = (
        head_size,
        rotary_dim,
        max_position,
        base,
        is_neox_style,
        rope_scaling_args,
        dual_chunk_attention_args,
        dtype,
    )
    if key in _ROPE_DICT:
        return _ROPE_DICT[key]

    if dual_chunk_attention_config is not None:
        extra_kwargs = {
            k: v
            for k, v in dual_chunk_attention_config.items()
            if k in (&#34;chunk_size&#34;, &#34;local_size&#34;)
        }
        rotary_emb = DualChunkRotaryEmbedding(
            head_size,
            rotary_dim,
            max_position,
            base,
            is_neox_style,
            dtype,
            **extra_kwargs,
        )
    elif rope_scaling is None:
        rotary_emb = RotaryEmbedding(
            head_size, rotary_dim, max_position, base, is_neox_style, dtype
        )
    else:
        if &#34;rope_type&#34; in rope_scaling:
            scaling_type = rope_scaling[&#34;rope_type&#34;]
        elif &#34;type&#34; in rope_scaling:
            scaling_type = rope_scaling[&#34;type&#34;]
        else:
            raise ValueError(&#34;Unknown RoPE scaling type&#34;)

        if scaling_type == &#34;llama3&#34;:
            scaling_factor = rope_scaling[&#34;factor&#34;]
            low_freq_factor = rope_scaling[&#34;low_freq_factor&#34;]
            high_freq_factor = rope_scaling[&#34;high_freq_factor&#34;]
            original_max_position = rope_scaling[&#34;original_max_position_embeddings&#34;]
            rotary_emb = Llama3RotaryEmbedding(
                head_size,
                rotary_dim,
                max_position,
                base,
                is_neox_style,
                dtype,
                scaling_factor,
                low_freq_factor,
                high_freq_factor,
                original_max_position,
            )
        elif scaling_type == &#34;default&#34;:
            if &#34;mrope_section&#34; in rope_scaling:
                rotary_emb = MRotaryEmbedding(
                    head_size,
                    rotary_dim,
                    max_position,
                    base,
                    is_neox_style,
                    dtype,
                    mrope_section=rope_scaling[&#34;mrope_section&#34;],
                )
            else:
                rotary_emb = RotaryEmbedding(
                    head_size,
                    rotary_dim,
                    max_position,
                    base,
                    is_neox_style,
                    dtype,
                )
        elif scaling_type == &#34;linear&#34;:
            scaling_factor = rope_scaling[&#34;factor&#34;]
            rotary_emb = LinearScalingRotaryEmbedding(
                head_size,
                rotary_dim,
                max_position,
                base,
                is_neox_style,
                scaling_factor,
                dtype,
            )
        elif scaling_type == &#34;dynamic&#34;:
            scaling_factor = rope_scaling[&#34;factor&#34;]
            if &#34;alpha&#34; in rope_scaling:
                rotary_emb = DynamicNTKAlphaRotaryEmbedding(
                    head_size,
                    rotary_dim,
                    max_position,
                    base,
                    is_neox_style,
                    rope_scaling[&#34;alpha&#34;],
                    dtype,
                )
            else:
                rotary_emb = DynamicNTKScalingRotaryEmbedding(
                    head_size,
                    rotary_dim,
                    max_position,
                    base,
                    is_neox_style,
                    scaling_factor,
                    dtype,
                )
        elif scaling_type == &#34;yarn&#34;:
            scaling_factor = rope_scaling[&#34;factor&#34;]
            original_max_position = rope_scaling[&#34;original_max_position_embeddings&#34;]
            extra_kwargs = {
                k: v
                for k, v in rope_scaling.items()
                if k
                in (&#34;extrapolation_factor&#34;, &#34;attn_factor&#34;, &#34;beta_fast&#34;, &#34;beta_slow&#34;)
            }
            rotary_emb = YaRNScalingRotaryEmbedding(
                head_size,
                rotary_dim,
                original_max_position,
                base,
                is_neox_style,
                scaling_factor,
                dtype,
                **extra_kwargs,
            )
        elif scaling_type == &#34;deepseek_yarn&#34;:
            scaling_factor = rope_scaling[&#34;factor&#34;]
            original_max_position = rope_scaling[&#34;original_max_position_embeddings&#34;]
            # assert max_position == original_max_position * scaling_factor
            extra_kwargs = {
                k: v
                for k, v in rope_scaling.items()
                if k
                in (
                    &#34;extrapolation_factor&#34;,
                    &#34;attn_factor&#34;,
                    &#34;beta_fast&#34;,
                    &#34;beta_slow&#34;,
                    &#34;mscale&#34;,
                    &#34;mscale_all_dim&#34;,
                )
            }
            rotary_emb = DeepseekScalingRotaryEmbedding(
                head_size,
                rotary_dim,
                original_max_position,
                base,
                is_neox_style,
                scaling_factor,
                dtype,
                **extra_kwargs,
            )
        elif scaling_type == &#34;longrope&#34;:
            short_factor = rope_scaling[&#34;short_factor&#34;]
            long_factor = rope_scaling[&#34;long_factor&#34;]
            original_max_position = rope_scaling[&#34;original_max_position_embeddings&#34;]
            extra_kwargs = {
                k: v
                for k, v in rope_scaling.items()
                if k in (&#34;short_mscale&#34;, &#34;long_mscale&#34;)
            }
            rotary_emb = Phi3LongRoPEScaledRotaryEmbedding(
                head_size,
                rotary_dim,
                max_position,
                original_max_position,
                base,
                is_neox_style,
                dtype,
                short_factor,
                long_factor,
                **extra_kwargs,
            )
        else:
            raise ValueError(f&#34;Unknown RoPE scaling type {scaling_type}&#34;)
    _ROPE_DICT[key] = rotary_emb
    return rotary_emb</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.get_rope_cpu"><code class="name flex">
<span>def <span class="ident">get_rope_cpu</span></span>(<span>head_size: int,<br>rotary_dim: int,<br>max_position: int,<br>base: int,<br>is_neox_style: bool = True,<br>rope_scaling: Dict[str, Any] | None = None,<br>dtype: torch.dtype | None = None,<br>partial_rotary_factor: float = 1.0,<br>device: str | None = None) ‑> <a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding">RotaryEmbedding</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_rope_cpu(
    head_size: int,
    rotary_dim: int,
    max_position: int,
    base: int,
    is_neox_style: bool = True,
    rope_scaling: Optional[Dict[str, Any]] = None,
    dtype: Optional[torch.dtype] = None,
    partial_rotary_factor: float = 1.0,
    device: Optional[str] = None,
) -&gt; RotaryEmbedding:
    if dtype is None:
        dtype = torch.get_default_dtype()
    if rope_scaling is not None:
        # Transforms every value that is a list into a tuple for caching calls
        rope_scaling_tuple = {
            k: tuple(v) if isinstance(v, list) else v for k, v in rope_scaling.items()
        }
        rope_scaling_args = tuple(rope_scaling_tuple.items())
    else:
        rope_scaling_args = None
    if partial_rotary_factor &lt; 1.0:
        rotary_dim = int(rotary_dim * partial_rotary_factor)
    key = (
        head_size,
        rotary_dim,
        max_position,
        base,
        is_neox_style,
        rope_scaling_args,
        dtype,
    )
    if key in _ROPE_DICT:
        return _ROPE_DICT[key]

    assert rope_scaling is not None
    scaling_type = rope_scaling[&#34;rope_type&#34;]
    assert (
        scaling_type == &#34;deepseek_yarn&#34;
    ), &#34;Only deepseek_yarn is supported for CPU for now&#34;

    scaling_factor = rope_scaling[&#34;factor&#34;]
    original_max_position = rope_scaling[&#34;original_max_position_embeddings&#34;]
    extra_kwargs = {
        k: v
        for k, v in rope_scaling.items()
        if k
        in (
            &#34;extrapolation_factor&#34;,
            &#34;attn_factor&#34;,
            &#34;beta_fast&#34;,
            &#34;beta_slow&#34;,
            &#34;mscale&#34;,
            &#34;mscale_all_dim&#34;,
        )
    }
    extra_kwargs[&#34;device&#34;] = device
    rotary_emb = DeepseekScalingRotaryEmbedding(
        head_size,
        rotary_dim,
        original_max_position,
        base,
        is_neox_style,
        scaling_factor,
        dtype,
        **extra_kwargs,
    )

    _ROPE_DICT[key] = rotary_emb
    return rotary_emb</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.get_rope_wrapper"><code class="name flex">
<span>def <span class="ident">get_rope_wrapper</span></span>(<span>head_size: int,<br>rotary_dim: int,<br>max_position: int,<br>base: int,<br>is_neox_style: bool = True,<br>rope_scaling: Dict[str, Any] | None = None,<br>dtype: torch.dtype | None = None,<br>partial_rotary_factor: float = 1.0,<br>device: str | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_rope_wrapper(
    head_size: int,
    rotary_dim: int,
    max_position: int,
    base: int,
    is_neox_style: bool = True,
    rope_scaling: Optional[Dict[str, Any]] = None,
    dtype: Optional[torch.dtype] = None,
    partial_rotary_factor: float = 1.0,
    device: Optional[str] = None,
):
    if device != &#34;cpu&#34;:
        wrapper = aiter_get_rope if _use_aiter else get_rope
        return wrapper(
            head_size,
            rotary_dim,
            max_position,
            base,
            is_neox_style,
            rope_scaling,
            dtype,
            partial_rotary_factor,
        )

    return get_rope_cpu(
        head_size,
        rotary_dim,
        max_position,
        base,
        is_neox_style,
        rope_scaling,
        dtype,
        partial_rotary_factor,
        device,
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.rotate_half"><code class="name flex">
<span>def <span class="ident">rotate_half</span></span>(<span>x)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rotate_half(x):
    &#34;&#34;&#34;Rotates half the hidden dims of the input.&#34;&#34;&#34;
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)</code></pre>
</details>
<div class="desc"><p>Rotates half the hidden dims of the input.</p></div>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.yarn_get_mscale"><code class="name flex">
<span>def <span class="ident">yarn_get_mscale</span></span>(<span>scale: float = 1, mscale: float = 1) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def yarn_get_mscale(scale: float = 1, mscale: float = 1) -&gt; float:
    if scale &lt;= 1:
        return 1.0
    return 0.1 * mscale * math.log(scale) + 1.0</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sglang.srt.layers.rotary_embedding.DeepseekScalingRotaryEmbedding"><code class="flex name class">
<span>class <span class="ident">DeepseekScalingRotaryEmbedding</span></span>
<span>(</span><span>head_size: int,<br>rotary_dim: int,<br>max_position_embeddings: int,<br>base: int,<br>is_neox_style: bool,<br>scaling_factor: float,<br>dtype: torch.dtype,<br>*,<br>extrapolation_factor: float = 1,<br>attn_factor: float = 1,<br>beta_fast: int = 32,<br>beta_slow: int = 1,<br>mscale: float = 1,<br>mscale_all_dim: float = 0,<br>device: str | None = 'cuda')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DeepseekScalingRotaryEmbedding(RotaryEmbedding):
    &#34;&#34;&#34;RotaryEmbedding extended with YaRN method.

    Credits to Peng et al. github.com/jquesnelle/yarn
    &#34;&#34;&#34;

    def __init__(
        self,
        head_size: int,
        rotary_dim: int,
        max_position_embeddings: int,
        base: int,
        is_neox_style: bool,
        scaling_factor: float,
        dtype: torch.dtype,
        *,
        extrapolation_factor: float = 1,
        attn_factor: float = 1,
        beta_fast: int = 32,
        beta_slow: int = 1,
        mscale: float = 1,
        mscale_all_dim: float = 0,
        device: Optional[str] = &#34;cuda&#34; if not _is_npu else &#34;npu&#34;,
    ) -&gt; None:
        self.scaling_factor = scaling_factor
        self.extrapolation_factor = extrapolation_factor
        self.attn_factor = attn_factor
        self.beta_fast = beta_fast
        self.beta_slow = beta_slow
        # Get n-d magnitude scaling corrected for interpolation.
        self.mscale = float(
            yarn_get_mscale(self.scaling_factor, float(mscale))
            / yarn_get_mscale(self.scaling_factor, float(mscale_all_dim))
            * attn_factor
        )
        self.device = device
        super().__init__(
            head_size, rotary_dim, max_position_embeddings, base, is_neox_style, dtype
        )

        # Re-dispatch
        if _is_hip:
            self._forward_method = self.forward_native

    def _compute_inv_freq(self, scaling_factor: float) -&gt; torch.Tensor:
        pos_freqs = self.base ** (
            torch.arange(0, self.rotary_dim, 2, dtype=torch.float, device=self.device)
            / self.rotary_dim
        )
        inv_freq_extrapolation = 1.0 / pos_freqs
        inv_freq_interpolation = 1.0 / (scaling_factor * pos_freqs)

        low, high = _yarn_find_correction_range(
            self.beta_fast,
            self.beta_slow,
            self.rotary_dim,
            self.base,
            self.max_position_embeddings,
        )
        # Get n-d rotational scaling corrected for extrapolation
        inv_freq_mask = (
            1
            - _yarn_linear_ramp_mask(
                low, high, self.rotary_dim // 2, dtype=torch.float, device=self.device
            )
        ) * self.extrapolation_factor
        inv_freq = (
            inv_freq_interpolation * (1 - inv_freq_mask)
            + inv_freq_extrapolation * inv_freq_mask
        )
        return inv_freq

    def _compute_cos_sin_cache(self) -&gt; torch.Tensor:
        inv_freq = self._compute_inv_freq(self.scaling_factor)
        t = torch.arange(
            self.max_position_embeddings * self.scaling_factor,
            device=self.device,
            dtype=torch.float32,
        )
        freqs = torch.einsum(&#34;i,j -&gt; ij&#34;, t, inv_freq)
        cos = freqs.cos() * self.mscale
        sin = freqs.sin() * self.mscale
        cache = torch.cat((cos, sin), dim=-1)
        return cache

    def forward_native(
        self,
        positions: torch.Tensor,
        query: torch.Tensor,
        key: torch.Tensor,
        offsets: Optional[torch.Tensor] = None,
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;PyTorch-native implementation equivalent to forward().&#34;&#34;&#34;
        dtype = query.dtype
        query_rot = query[..., : self.rotary_dim]
        key_rot = key[..., : self.rotary_dim]
        if self.rotary_dim &lt; self.head_size:
            query_pass = query[..., self.rotary_dim :]
            key_pass = key[..., self.rotary_dim :]

        self.cos_sin_cache: torch.Tensor = self.cos_sin_cache.to(positions.device)
        cos_sin = self.cos_sin_cache[
            torch.add(positions, offsets) if offsets is not None else positions
        ]
        cos, sin = cos_sin.chunk(2, dim=-1)
        if self.is_neox_style:
            # NOTE(woosuk): Here we assume that the positions tensor has the
            # shape [batch_size, seq_len].
            cos = cos.repeat(1, 1, 2).unsqueeze(-2)
            sin = sin.repeat(1, 1, 2).unsqueeze(-2)
        else:
            cos = cos.repeat_interleave(2, dim=-1).unsqueeze(-2)
            sin = sin.repeat_interleave(2, dim=-1).unsqueeze(-2)

        rotate_fn = _rotate_neox if self.is_neox_style else _rotate_gptj
        query_rot = query_rot * cos + rotate_fn(query_rot) * sin
        key_rot = key_rot * cos + rotate_fn(key_rot) * sin

        if self.rotary_dim &lt; self.head_size:
            query = torch.cat((query_rot, query_pass), dim=-1)
            key = torch.cat((key_rot, key_pass), dim=-1)
        else:
            query = query_rot
            key = key_rot
        return query.to(dtype), key.to(dtype)

    def forward_npu(
        self,
        positions: torch.Tensor,
        query: torch.Tensor,
        key: torch.Tensor,
        offsets: Optional[torch.Tensor] = None,
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        # NOTE: now npu_mrope can only support `numQHeads*headSize &lt;= 4096` pattern,
        # and generalization to more scenarios will be supported in the future.
        if query.shape[1] * query.shape[2] &gt; 4096:
            return self.forward_native(positions, query, key, offsets)
        num_tokens = query.shape[0]
        rotary_mode = &#34;half&#34; if self.is_neox_style else &#34;interleave&#34;
        self.cos_sin_cache: torch.Tensor = self.cos_sin_cache.to(positions.device)
        query_rot = query[..., : self.rotary_dim]
        key_rot = key[..., : self.rotary_dim]
        if self.rotary_dim &lt; self.head_size:
            query_pass = query[..., self.rotary_dim :]
            key_pass = key[..., self.rotary_dim :]

        query_rot, key_rot = torch_npu.npu_mrope(
            torch.add(positions, offsets) if offsets is not None else positions,
            query_rot.reshape(num_tokens, -1),
            key_rot.reshape(num_tokens, -1),
            self.cos_sin_cache,
            self.rotary_dim,
            mrope_section=[0, 0, 0],
            rotary_mode=rotary_mode,
        )
        query_rot = query_rot.reshape(num_tokens, -1, self.rotary_dim)
        key_rot = key_rot.reshape(num_tokens, -1, self.rotary_dim)

        if self.rotary_dim &lt; self.head_size:
            query = torch.cat((query_rot, query_pass), dim=-1)
            key = torch.cat((key_rot, key_pass), dim=-1)
        else:
            query = query_rot
            key = key_rot
        return query, key

    def forward_cpu(
        self,
        positions: torch.Tensor,
        query: torch.Tensor,
        key: torch.Tensor,
        offsets: Optional[torch.Tensor] = None,
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        positions = torch.add(positions, offsets) if offsets is not None else positions
        if _is_cpu_amx_available:
            return torch.ops.sgl_kernel.rotary_embedding_cpu(
                positions, query, key, self.head_size, self.cos_sin_cache, False
            )
        else:
            return self.forward_native(positions, query, key, offsets)</code></pre>
</details>
<div class="desc"><p>RotaryEmbedding extended with YaRN method.</p>
<p>Credits to Peng et al. github.com/jquesnelle/yarn</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding">RotaryEmbedding</a></li>
<li><a title="sglang.srt.custom_op.CustomOp" href="../custom_op.html#sglang.srt.custom_op.CustomOp">CustomOp</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.rotary_embedding.DeepseekScalingRotaryEmbedding.forward_cpu"><code class="name flex">
<span>def <span class="ident">forward_cpu</span></span>(<span>self,<br>positions: torch.Tensor,<br>query: torch.Tensor,<br>key: torch.Tensor,<br>offsets: torch.Tensor | None = None) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_cpu(
    self,
    positions: torch.Tensor,
    query: torch.Tensor,
    key: torch.Tensor,
    offsets: Optional[torch.Tensor] = None,
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    positions = torch.add(positions, offsets) if offsets is not None else positions
    if _is_cpu_amx_available:
        return torch.ops.sgl_kernel.rotary_embedding_cpu(
            positions, query, key, self.head_size, self.cos_sin_cache, False
        )
    else:
        return self.forward_native(positions, query, key, offsets)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.DeepseekScalingRotaryEmbedding.forward_native"><code class="name flex">
<span>def <span class="ident">forward_native</span></span>(<span>self,<br>positions: torch.Tensor,<br>query: torch.Tensor,<br>key: torch.Tensor,<br>offsets: torch.Tensor | None = None) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_native(
    self,
    positions: torch.Tensor,
    query: torch.Tensor,
    key: torch.Tensor,
    offsets: Optional[torch.Tensor] = None,
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;PyTorch-native implementation equivalent to forward().&#34;&#34;&#34;
    dtype = query.dtype
    query_rot = query[..., : self.rotary_dim]
    key_rot = key[..., : self.rotary_dim]
    if self.rotary_dim &lt; self.head_size:
        query_pass = query[..., self.rotary_dim :]
        key_pass = key[..., self.rotary_dim :]

    self.cos_sin_cache: torch.Tensor = self.cos_sin_cache.to(positions.device)
    cos_sin = self.cos_sin_cache[
        torch.add(positions, offsets) if offsets is not None else positions
    ]
    cos, sin = cos_sin.chunk(2, dim=-1)
    if self.is_neox_style:
        # NOTE(woosuk): Here we assume that the positions tensor has the
        # shape [batch_size, seq_len].
        cos = cos.repeat(1, 1, 2).unsqueeze(-2)
        sin = sin.repeat(1, 1, 2).unsqueeze(-2)
    else:
        cos = cos.repeat_interleave(2, dim=-1).unsqueeze(-2)
        sin = sin.repeat_interleave(2, dim=-1).unsqueeze(-2)

    rotate_fn = _rotate_neox if self.is_neox_style else _rotate_gptj
    query_rot = query_rot * cos + rotate_fn(query_rot) * sin
    key_rot = key_rot * cos + rotate_fn(key_rot) * sin

    if self.rotary_dim &lt; self.head_size:
        query = torch.cat((query_rot, query_pass), dim=-1)
        key = torch.cat((key_rot, key_pass), dim=-1)
    else:
        query = query_rot
        key = key_rot
    return query.to(dtype), key.to(dtype)</code></pre>
</details>
<div class="desc"><p>PyTorch-native implementation equivalent to forward().</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding">RotaryEmbedding</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.extra_repr" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.extra_repr">extra_repr</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward" href="../custom_op.html#sglang.srt.custom_op.CustomOp.forward">forward</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_npu" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_npu">forward_npu</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.DualChunkRotaryEmbedding"><code class="flex name class">
<span>class <span class="ident">DualChunkRotaryEmbedding</span></span>
<span>(</span><span>head_size: int,<br>rotary_dim: int,<br>max_position_embeddings: int,<br>base: int,<br>is_neox_style: bool,<br>dtype: torch.dtype,<br>chunk_size: int,<br>local_size: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DualChunkRotaryEmbedding(CustomOp):
    &#34;&#34;&#34;Rotary positional embedding for Dual Chunk Attention.&#34;&#34;&#34;

    def __init__(
        self,
        head_size: int,
        rotary_dim: int,
        max_position_embeddings: int,
        base: int,
        is_neox_style: bool,
        dtype: torch.dtype,
        chunk_size: int,
        local_size: int,
    ) -&gt; None:
        super().__init__()
        self.head_size = head_size
        self.rotary_dim = rotary_dim
        self.max_position_embeddings = max_position_embeddings
        self.base = base
        self.is_neox_style = is_neox_style
        self.chunk_size = chunk_size
        self.local_size = local_size
        self.dtype = dtype
        self.device = torch.device(f&#34;cuda:{torch.cuda.current_device()}&#34;)
        (q_cache, qc_cache, k_cache, qc_no_clamp_cache, q_inter_cache) = (
            self._compute_cos_sin_cache()
        )

        self.register_buffer(&#34;cos_sin_q_cache&#34;, q_cache, persistent=False)
        self.register_buffer(&#34;cos_sin_qc_cache&#34;, qc_cache, persistent=False)
        self.register_buffer(&#34;cos_sin_k_cache&#34;, k_cache, persistent=False)
        self.register_buffer(
            &#34;cos_sin_qc_no_clamp_cache&#34;, qc_no_clamp_cache, persistent=False
        )
        self.register_buffer(&#34;cos_sin_q_inter_cache&#34;, q_inter_cache, persistent=False)

    def _compute_inv_freq(self, base: Union[int, float]) -&gt; torch.Tensor:
        &#34;&#34;&#34;Compute the inverse frequency.&#34;&#34;&#34;
        # NOTE(woosuk): The HF implementation uses `torch.arange(...).float()`.
        # However, we use `torch.arange(..., dtype=torch.float)` instead to
        # avoid numerical issues with large base values (e.g., 10000000).
        # This may cause a slight numerical difference between the HF
        # implementation and ours.
        # NOTE(woosuk): To exactly match the HF implementation, we need to
        # use CPU to compute the cache and then move it to GPU. However, we
        # create the cache on GPU for faster initialization. This may cause
        # a slight numerical difference between the HF implementation and ours.
        inv_freq = 1.0 / (
            base
            ** (
                torch.arange(0, self.rotary_dim, 2, dtype=torch.float) / self.rotary_dim
            )
        )
        return inv_freq

    def _compute_cos_sin_cache(self) -&gt; torch.Tensor:
        &#34;&#34;&#34;Compute the cos and sin cache.&#34;&#34;&#34;
        inv_freq = self._compute_inv_freq(self.base)
        chunk_len = self.chunk_size - self.local_size
        q_t = torch.arange(chunk_len, dtype=torch.float)
        qc_t = (torch.arange(chunk_len, dtype=torch.float) + chunk_len).clamp(
            max=self.chunk_size
        )
        k_t = torch.arange(self.max_position_embeddings, dtype=torch.float) % chunk_len

        # count from chunk_len, no clamp(self.chunk_size) restriction
        qc_no_clamp_t = torch.arange(chunk_len, dtype=torch.float) + chunk_len
        # count from self.chunk_size for q_inter&#39;s rope
        q_inter_t = torch.arange(chunk_len, dtype=torch.float) + self.chunk_size

        q_freqs = torch.outer(q_t, inv_freq)
        qc_freqs = torch.outer(qc_t, inv_freq)
        k_freqs = torch.outer(k_t, inv_freq)
        qc_no_clamp_freqs = torch.outer(qc_no_clamp_t, inv_freq)
        q_inter_freqs = torch.outer(q_inter_t, inv_freq)

        q_cos = q_freqs.cos()
        q_sin = q_freqs.sin()
        qc_cos = qc_freqs.cos()
        qc_sin = qc_freqs.sin()
        k_cos = k_freqs.cos()
        k_sin = k_freqs.sin()

        qc_no_clamp_cos = qc_no_clamp_freqs.cos()
        qc_no_clamp_sin = qc_no_clamp_freqs.sin()
        q_inter_cos = q_inter_freqs.cos()
        q_inter_sin = q_inter_freqs.sin()

        q_cache = torch.cat((q_cos, q_sin), dim=-1).to(
            dtype=self.dtype, device=self.device
        )
        qc_cache = torch.cat((qc_cos, qc_sin), dim=-1).to(
            dtype=self.dtype, device=self.device
        )
        k_cache = torch.cat((k_cos, k_sin), dim=-1).to(
            dtype=self.dtype, device=self.device
        )
        qc_no_clamp_cache = torch.cat((qc_no_clamp_cos, qc_no_clamp_sin), dim=-1).to(
            dtype=self.dtype, device=self.device
        )
        q_inter_cache = torch.cat((q_inter_cos, q_inter_sin), dim=-1).to(
            dtype=self.dtype, device=self.device
        )
        return q_cache, qc_cache, k_cache, qc_no_clamp_cache, q_inter_cache

    def forward(
        self,
        positions: torch.Tensor,
        query: torch.Tensor,
        key: torch.Tensor,
        offsets: Optional[torch.Tensor] = None,
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        query = query.view(*query.shape[:-1], -1, self.head_size)
        key = key.view(*key.shape[:-1], -1, self.head_size)
        query_rot = query[..., : self.rotary_dim]
        key_rot = key[..., : self.rotary_dim]
        if self.rotary_dim &lt; self.head_size:
            query_pass = query[..., self.rotary_dim :]
            key_pass = key[..., self.rotary_dim :]
        else:
            query_pass = None
            key_pass = None

        positions_with_offsets = (
            torch.add(positions, offsets) if offsets is not None else positions
        )
        key = self._apply_rotary_embedding(
            self.cos_sin_k_cache[positions_with_offsets], key_rot, key_pass
        )
        chunk_len = self.chunk_size - self.local_size
        query = self._apply_rotary_embedding(
            self.cos_sin_q_cache[positions_with_offsets % chunk_len],
            query_rot,
            query_pass,
        )
        query_succ = self._apply_rotary_embedding(
            self.cos_sin_qc_cache[positions_with_offsets % chunk_len],
            query_rot,
            query_pass,
        )
        query_inter = self._apply_rotary_embedding(
            self.cos_sin_qc_cache[chunk_len - 1].repeat(positions.shape[0], 1),
            query_rot,
            query_pass,
        )
        query_succ_critical = self._apply_rotary_embedding(
            self.cos_sin_qc_no_clamp_cache[positions_with_offsets % chunk_len],
            query_rot,
            query_pass,
        )
        query_inter_critical = self._apply_rotary_embedding(
            self.cos_sin_q_inter_cache[positions_with_offsets % chunk_len],
            query_rot,
            query_pass,
        )

        # merge query into one tensor to simplify the interfaces
        query = torch.cat(
            (
                query,
                query_succ,
                query_inter,
                query_succ_critical,
                query_inter_critical,
            ),
            dim=-1,
        )
        return query, key

    def _apply_rotary_embedding(self, cos_sin, hidden_rot, hidden_pass):
        cos, sin = cos_sin.chunk(2, dim=-1)
        if self.is_neox_style:
            # NOTE(woosuk): Here we assume that the positions tensor has the
            # shape [batch_size, seq_len].
            cos = cos.repeat(1, 1, 2).unsqueeze(-2)
            sin = sin.repeat(1, 1, 2).unsqueeze(-2)
        else:
            cos = cos.repeat_interleave(2, dim=-1).unsqueeze(-2)
            sin = sin.repeat_interleave(2, dim=-1).unsqueeze(-2)
        rotate_fn = _rotate_neox if self.is_neox_style else _rotate_gptj
        hidden_rot = hidden_rot * cos + rotate_fn(hidden_rot) * sin

        if self.rotary_dim &lt; self.head_size:
            hidden = torch.cat((hidden_rot, hidden_pass), dim=-1)
        else:
            hidden = hidden_rot
        return hidden.flatten(-2).squeeze(0)

    def extra_repr(self) -&gt; str:
        s = f&#34;head_size={self.head_size}, rotary_dim={self.rotary_dim}&#34;
        s += f&#34;, max_position_embeddings={self.max_position_embeddings}&#34;
        s += f&#34;, base={self.base}, is_neox_style={self.is_neox_style}&#34;
        s += f&#34;, chunk_size={self.chunk_size}, local_size={self.local_size}&#34;
        return s</code></pre>
</details>
<div class="desc"><p>Rotary positional embedding for Dual Chunk Attention.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.custom_op.CustomOp" href="../custom_op.html#sglang.srt.custom_op.CustomOp">CustomOp</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.rotary_embedding.DualChunkRotaryEmbedding.extra_repr"><code class="name flex">
<span>def <span class="ident">extra_repr</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extra_repr(self) -&gt; str:
    s = f&#34;head_size={self.head_size}, rotary_dim={self.rotary_dim}&#34;
    s += f&#34;, max_position_embeddings={self.max_position_embeddings}&#34;
    s += f&#34;, base={self.base}, is_neox_style={self.is_neox_style}&#34;
    s += f&#34;, chunk_size={self.chunk_size}, local_size={self.local_size}&#34;
    return s</code></pre>
</details>
<div class="desc"><p>Return the extra representation of the module.</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.custom_op.CustomOp" href="../custom_op.html#sglang.srt.custom_op.CustomOp">CustomOp</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.custom_op.CustomOp.forward" href="../custom_op.html#sglang.srt.custom_op.CustomOp.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.DynamicNTKAlphaRotaryEmbedding"><code class="flex name class">
<span>class <span class="ident">DynamicNTKAlphaRotaryEmbedding</span></span>
<span>(</span><span>head_size: int,<br>rotary_dim: int,<br>max_position_embeddings: int,<br>base: int,<br>is_neox_style: bool,<br>scaling_alpha: float,<br>dtype: torch.dtype)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DynamicNTKAlphaRotaryEmbedding(RotaryEmbedding):
    &#34;&#34;&#34;RotaryEmbedding extended with Dynamic NTK scaling.

    Credits to the Reddit users /u/bloc97 and /u/emozilla
    &#34;&#34;&#34;

    def __init__(
        self,
        head_size: int,
        rotary_dim: int,
        max_position_embeddings: int,
        base: int,
        is_neox_style: bool,
        scaling_alpha: float,
        dtype: torch.dtype,
    ) -&gt; None:
        self.scaling_alpha = scaling_alpha
        super().__init__(
            head_size, rotary_dim, max_position_embeddings, base, is_neox_style, dtype
        )

    def _compute_cos_sin_cache(self) -&gt; torch.Tensor:
        max_len = self.max_position_embeddings
        base = self.base * self.scaling_alpha ** (
            self.rotary_dim / (self.rotary_dim - 2)
        )

        inv_freq = self._compute_inv_freq(base)
        t = torch.arange(max_len, dtype=torch.float)

        freqs = torch.einsum(&#34;i,j -&gt; ij&#34;, t, inv_freq)
        cos = freqs.cos()
        sin = freqs.sin()
        cache = torch.cat((cos, sin), dim=-1)
        return cache</code></pre>
</details>
<div class="desc"><p>RotaryEmbedding extended with Dynamic NTK scaling.</p>
<p>Credits to the Reddit users /u/bloc97 and /u/emozilla</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding">RotaryEmbedding</a></li>
<li><a title="sglang.srt.custom_op.CustomOp" href="../custom_op.html#sglang.srt.custom_op.CustomOp">CustomOp</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding">RotaryEmbedding</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.extra_repr" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.extra_repr">extra_repr</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward" href="../custom_op.html#sglang.srt.custom_op.CustomOp.forward">forward</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_native" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_native">forward_native</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_npu" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_npu">forward_npu</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.DynamicNTKScalingRotaryEmbedding"><code class="flex name class">
<span>class <span class="ident">DynamicNTKScalingRotaryEmbedding</span></span>
<span>(</span><span>head_size: int,<br>rotary_dim: int,<br>max_position_embeddings: int,<br>base: int,<br>is_neox_style: bool,<br>scaling_factor: float,<br>dtype: torch.dtype)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DynamicNTKScalingRotaryEmbedding(RotaryEmbedding):
    &#34;&#34;&#34;RotaryEmbedding extended with Dynamic NTK scaling.

    Credits to the Reddit users /u/bloc97 and /u/emozilla
    &#34;&#34;&#34;

    def __init__(
        self,
        head_size: int,
        rotary_dim: int,
        max_position_embeddings: int,
        base: int,
        is_neox_style: bool,
        scaling_factor: float,
        dtype: torch.dtype,
    ) -&gt; None:
        self.scaling_factor = scaling_factor
        super().__init__(
            head_size, rotary_dim, max_position_embeddings, base, is_neox_style, dtype
        )

    def _compute_cos_sin_cache(self) -&gt; torch.Tensor:
        # NOTE(woosuk): self.max_position_embeddings is the original
        # maximum length before applying the rope scaling.
        # Thus, the maximum length after applying the rope scaling is
        # self.max_position_embeddings * self.scaling_factor.
        max_len = self.max_position_embeddings * self.scaling_factor
        base = self.base * (
            (self.scaling_factor * max_len / self.max_position_embeddings)
            - (self.scaling_factor - 1)
        ) ** (self.rotary_dim / (self.rotary_dim - 2))
        inv_freq = self._compute_inv_freq(base)
        t = torch.arange(max_len, dtype=torch.float)

        freqs = torch.einsum(&#34;i,j -&gt; ij&#34;, t, inv_freq)
        cos = freqs.cos()
        sin = freqs.sin()
        cache = torch.cat((cos, sin), dim=-1)
        return cache</code></pre>
</details>
<div class="desc"><p>RotaryEmbedding extended with Dynamic NTK scaling.</p>
<p>Credits to the Reddit users /u/bloc97 and /u/emozilla</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding">RotaryEmbedding</a></li>
<li><a title="sglang.srt.custom_op.CustomOp" href="../custom_op.html#sglang.srt.custom_op.CustomOp">CustomOp</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding">RotaryEmbedding</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.extra_repr" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.extra_repr">extra_repr</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward" href="../custom_op.html#sglang.srt.custom_op.CustomOp.forward">forward</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_native" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_native">forward_native</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_npu" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_npu">forward_npu</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.LinearScalingRotaryEmbedding"><code class="flex name class">
<span>class <span class="ident">LinearScalingRotaryEmbedding</span></span>
<span>(</span><span>head_size: int,<br>rotary_dim: int,<br>max_position_embeddings: int,<br>base: int,<br>is_neox_style: bool,<br>scaling_factors: List[float] | float,<br>dtype: torch.dtype)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearScalingRotaryEmbedding(RotaryEmbedding):
    &#34;&#34;&#34;RotaryEmbedding extended with linear scaling.

    It supports multiple scaling factors. Since multiple LoRA adapters may have
    different scaling factors, we need multiple cos/sin caches. In this way,
    instead of running rotary embedding kernel per lora, we can run multiple
    lora in a batched way.

    In addition to that, we also keep the cos/sin cache for the scaling factor
    of 1 (default) at all times.

    Exemplary for two scaling factors x=1, y and z with embeddings
    [[x11, x12, ... x1m], ..., [xn1, xn2, ..., xnm]] and
    [[y11, y12, ... y1o], ..., [yn1, yn2, ..., yno]], and
    [[z11, z12, ... z1p], ..., [zn1, zn2, ..., znp]],

    we construct the cos/sin cache as follows:
    [[x11, x12, ... x1m, y11, y12, ... y1o, z11, z12, ... z1p],
        ...
     [xn1, xn2, ... xnm, yn1, yn2, ... yno, zn1, zn2, ... znp]]

    We then use offsets to index into the cos/sin cache for
    the respective scaling factors.

    The offset to cache can be accessed via `scaling_factor_to_offset` API.

    Credits to the Reddit user /u/kaiokendev
    &#34;&#34;&#34;

    def __init__(
        self,
        head_size: int,
        rotary_dim: int,
        max_position_embeddings: int,
        base: int,
        is_neox_style: bool,
        scaling_factors: Union[List[float], float],
        dtype: torch.dtype,
    ) -&gt; None:
        if isinstance(scaling_factors, float):
            scaling_factors = [scaling_factors]
        self.scaling_factors: List[float] = scaling_factors  # noqa
        super().__init__(
            head_size, rotary_dim, max_position_embeddings, base, is_neox_style, dtype
        )
        # Lazy initialized.
        self._scaling_factor_to_offset: Dict[float, int]

    def _compute_cos_sin_cache(self) -&gt; torch.Tensor:
        inv_freq = self._compute_inv_freq(self.base)
        cache_list: List[torch.Tensor] = []
        # offsets to the next cache in a tensor.
        # Each offset corresponds to the same index in scaling_factors.
        offsets: List[int] = []
        for scaling_factor in self.scaling_factors:
            # NOTE(woosuk): self.max_position_embeddings is the original
            # maximum length before applying the rope scaling.
            # Thus, the maximum length after applying the rope scaling is
            # self.max_position_embeddings * self.scaling_factor.
            max_len = self.max_position_embeddings * scaling_factor
            t = torch.arange(max_len, dtype=torch.float)
            t = t / scaling_factor

            freqs = torch.einsum(&#34;i,j -&gt; ij&#34;, t, inv_freq)
            cos = freqs.cos()
            sin = freqs.sin()
            cache = torch.cat((cos, sin), dim=-1)
            if not cache_list:
                offset = 0
            else:
                last_offset = offsets[-1]
                next_max_len = cache_list[-1].shape[0]
                offset = last_offset + next_max_len
            offsets.append(offset)
            cache_list.append(cache)
        self._scaling_factor_to_offset = {
            float(scaling_factor): offsets[i]
            for i, scaling_factor in enumerate(self.scaling_factors)
        }
        assert len(self.scaling_factors) == len(offsets)
        return torch.cat(cache_list, dim=0)

    @property
    def scaling_factor_to_offset(self) -&gt; Dict[float, int]:
        return self._scaling_factor_to_offset</code></pre>
</details>
<div class="desc"><p>RotaryEmbedding extended with linear scaling.</p>
<p>It supports multiple scaling factors. Since multiple LoRA adapters may have
different scaling factors, we need multiple cos/sin caches. In this way,
instead of running rotary embedding kernel per lora, we can run multiple
lora in a batched way.</p>
<p>In addition to that, we also keep the cos/sin cache for the scaling factor
of 1 (default) at all times.</p>
<p>Exemplary for two scaling factors x=1, y and z with embeddings
[[x11, x12, &hellip; x1m], &hellip;, [xn1, xn2, &hellip;, xnm]] and
[[y11, y12, &hellip; y1o], &hellip;, [yn1, yn2, &hellip;, yno]], and
[[z11, z12, &hellip; z1p], &hellip;, [zn1, zn2, &hellip;, znp]],</p>
<p>we construct the cos/sin cache as follows:
[[x11, x12, &hellip; x1m, y11, y12, &hellip; y1o, z11, z12, &hellip; z1p],
&hellip;
[xn1, xn2, &hellip; xnm, yn1, yn2, &hellip; yno, zn1, zn2, &hellip; znp]]</p>
<p>We then use offsets to index into the cos/sin cache for
the respective scaling factors.</p>
<p>The offset to cache can be accessed via <code>scaling_factor_to_offset</code> API.</p>
<p>Credits to the Reddit user /u/kaiokendev</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding">RotaryEmbedding</a></li>
<li><a title="sglang.srt.custom_op.CustomOp" href="../custom_op.html#sglang.srt.custom_op.CustomOp">CustomOp</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="sglang.srt.layers.rotary_embedding.LinearScalingRotaryEmbedding.scaling_factor_to_offset"><code class="name">prop <span class="ident">scaling_factor_to_offset</span> : Dict[float, int]</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def scaling_factor_to_offset(self) -&gt; Dict[float, int]:
    return self._scaling_factor_to_offset</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding">RotaryEmbedding</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.extra_repr" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.extra_repr">extra_repr</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward" href="../custom_op.html#sglang.srt.custom_op.CustomOp.forward">forward</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_native" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_native">forward_native</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_npu" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_npu">forward_npu</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.Llama3RotaryEmbedding"><code class="flex name class">
<span>class <span class="ident">Llama3RotaryEmbedding</span></span>
<span>(</span><span>head_size: int,<br>rotary_dim: int,<br>max_position_embeddings: int,<br>base: int,<br>is_neox_style: bool,<br>dtype: torch.dtype,<br>scaling_factor: float,<br>low_freq_factor: float,<br>high_freq_factor: float,<br>orig_max_position: int)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Llama3RotaryEmbedding(RotaryEmbedding):

    def __init__(
        self,
        head_size: int,
        rotary_dim: int,
        max_position_embeddings: int,
        base: int,
        is_neox_style: bool,
        dtype: torch.dtype,
        scaling_factor: float,
        low_freq_factor: float,
        high_freq_factor: float,
        orig_max_position: int,
    ) -&gt; None:
        self.scaling_factor = scaling_factor
        self.low_freq_factor = low_freq_factor
        self.high_freq_factor = high_freq_factor
        self.orig_max_position = orig_max_position
        super().__init__(
            head_size, rotary_dim, max_position_embeddings, base, is_neox_style, dtype
        )

    def _compute_inv_freq(self, base: Union[int, float]) -&gt; torch.Tensor:
        inv_freqs = super()._compute_inv_freq(base)
        low_freq_wavelen = self.orig_max_position / self.low_freq_factor
        high_freq_wavelen = self.orig_max_position / self.high_freq_factor

        wave_len = 2 * math.pi / inv_freqs
        if self.low_freq_factor != self.high_freq_factor:
            smooth = (self.orig_max_position / wave_len - self.low_freq_factor) / (
                self.high_freq_factor - self.low_freq_factor
            )
        else:
            smooth = 0
        new_freqs = torch.where(
            wave_len &lt; high_freq_wavelen,
            inv_freqs,
            torch.where(
                wave_len &gt; low_freq_wavelen,
                inv_freqs / self.scaling_factor,
                (1 - smooth) * inv_freqs / self.scaling_factor + smooth * inv_freqs,
            ),
        )
        return new_freqs</code></pre>
</details>
<div class="desc"><p>Original rotary positional embedding.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding">RotaryEmbedding</a></li>
<li><a title="sglang.srt.custom_op.CustomOp" href="../custom_op.html#sglang.srt.custom_op.CustomOp">CustomOp</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding">RotaryEmbedding</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.extra_repr" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.extra_repr">extra_repr</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward" href="../custom_op.html#sglang.srt.custom_op.CustomOp.forward">forward</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_native" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_native">forward_native</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_npu" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_npu">forward_npu</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.Llama4VisionRotaryEmbedding"><code class="flex name class">
<span>class <span class="ident">Llama4VisionRotaryEmbedding</span></span>
<span>(</span><span>head_size: int,<br>rotary_dim: int,<br>max_position_embeddings: int,<br>base: int,<br>is_neox_style: bool,<br>dtype: torch.dtype)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Llama4VisionRotaryEmbedding(RotaryEmbedding):

    def __init__(
        self,
        head_size: int,
        rotary_dim: int,
        max_position_embeddings: int,
        base: int,
        is_neox_style: bool,
        dtype: torch.dtype,
    ):
        super().__init__(
            head_size, rotary_dim, max_position_embeddings, base, is_neox_style, dtype
        )

    def _compute_inv_freq(self, base: Union[int, float]) -&gt; torch.Tensor:
        inv_freqs = super()._compute_inv_freq(base)
        inv_freqs = inv_freqs[: (self.rotary_dim // 2)]
        return inv_freqs

    def _compute_cos_sin_cache(self) -&gt; torch.Tensor:
        inv_freq = self._compute_inv_freq(self.base)

        # self.max_position_embeddings here is number of image patches
        # i.e. (image_size // patch_size) ** 2
        num_patches = self.max_position_embeddings
        img_idx = torch.arange(num_patches, dtype=torch.int32).reshape(num_patches, 1)
        img_idx = torch.cat([img_idx, img_idx[:1]], dim=0)
        img_idx[-1, -1] = -2  # set to ID_CLS_TOKEN
        num_patches_single_dim = int(math.sqrt(num_patches))
        frequencies_x = img_idx % num_patches_single_dim
        frequencies_y = img_idx // num_patches_single_dim
        freqs_x = (
            (frequencies_x + 1)[..., None] * inv_freq[None, None, :]
        ).repeat_interleave(2, dim=-1)
        freqs_y = (
            (frequencies_y + 1)[..., None] * inv_freq[None, None, :]
        ).repeat_interleave(2, dim=-1)
        freqs = torch.cat([freqs_x, freqs_y], dim=-1).float().contiguous()[..., ::2]
        freqs = freqs.masked_fill(img_idx.reshape(-1, 1, 1) &lt; 0, 0)
        cache = torch.view_as_complex(
            torch.stack([torch.cos(freqs), torch.sin(freqs)], dim=-1)
        )
        return cache

    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        self.cos_sin_cache: torch.Tensor = self.cos_sin_cache.to(query.device)
        query_ = torch.view_as_complex(query.float().reshape(*query.shape[:-1], -1, 2))
        key_ = torch.view_as_complex(key.float().reshape(*key.shape[:-1], -1, 2))
        broadcast_shape = [
            d if i == 1 or i == (query_.ndim - 1) else 1
            for i, d in enumerate(query_.shape)
        ]
        freqs_ci = self.cos_sin_cache.view(*broadcast_shape)
        query_out = torch.view_as_real(query_ * freqs_ci).flatten(3)
        key_out = torch.view_as_real(key_ * freqs_ci).flatten(3)
        return query_out.type_as(query), key_out.type_as(key)</code></pre>
</details>
<div class="desc"><p>Original rotary positional embedding.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding">RotaryEmbedding</a></li>
<li><a title="sglang.srt.custom_op.CustomOp" href="../custom_op.html#sglang.srt.custom_op.CustomOp">CustomOp</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding">RotaryEmbedding</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.extra_repr" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.extra_repr">extra_repr</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward" href="../custom_op.html#sglang.srt.custom_op.CustomOp.forward">forward</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_native" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_native">forward_native</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_npu" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_npu">forward_npu</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.MRotaryEmbedding"><code class="flex name class">
<span>class <span class="ident">MRotaryEmbedding</span></span>
<span>(</span><span>head_size: int,<br>rotary_dim: int,<br>max_position_embeddings: int,<br>base: int,<br>is_neox_style: bool,<br>dtype: torch.dtype,<br>mrope_section: List[int] | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MRotaryEmbedding(RotaryEmbedding):
    &#34;&#34;&#34;Rotary Embedding with Multimodal Sections.&#34;&#34;&#34;

    def __init__(
        self,
        head_size: int,
        rotary_dim: int,
        max_position_embeddings: int,
        base: int,
        is_neox_style: bool,
        dtype: torch.dtype,
        mrope_section: Optional[List[int]] = None,
    ) -&gt; None:
        super().__init__(
            head_size, rotary_dim, max_position_embeddings, base, is_neox_style, dtype
        )

        self.mrope_section = mrope_section
        if self.mrope_section:
            expected_sum = rotary_dim // 2
            actual_sum = sum(self.mrope_section)
            if actual_sum != expected_sum:
                print(
                    f&#34;MRoPE section sum mismatch: expected {expected_sum}, got {actual_sum}. &#34;
                    f&#34;Adjusting mrope_section to match rotary_dim // 2 = {expected_sum}&#34;
                )
                # Auto-correct by scaling the mrope_section proportionally
                if actual_sum &gt; 0:
                    scale_factor = expected_sum / actual_sum
                    self.mrope_section = [
                        max(1, int(section * scale_factor))
                        for section in self.mrope_section
                    ]
                    # Ensure the sum exactly matches by adjusting the last element
                    current_sum = sum(self.mrope_section)
                    if current_sum != expected_sum:
                        self.mrope_section[-1] += expected_sum - current_sum
                else:
                    # If all sections are 0, create a default distribution
                    self.mrope_section = [
                        expected_sum // len(self.mrope_section)
                    ] * len(self.mrope_section)
                    # Handle remainder
                    remainder = expected_sum % len(self.mrope_section)
                    for i in range(remainder):
                        self.mrope_section[i] += 1

                print(
                    f&#34;Corrected mrope_section: {self.mrope_section} (sum={sum(self.mrope_section)})&#34;
                )

    @torch.compile(dynamic=True)
    def forward(
        self,
        positions: torch.Tensor,
        query: torch.Tensor,
        key: torch.Tensor,
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;PyTorch-native implementation equivalent to forward().

        Args:
            positions:
                [num_tokens,] (text only) or
                [3, num_tokens] (T/H/W positions with multimodal inputs)
            query: [num_tokens, num_heads * head_size]
            key: [num_tokens, num_kv_heads * head_size]
        &#34;&#34;&#34;
        assert positions.ndim == 1 or positions.ndim == 2

        num_tokens = positions.shape[-1]
        cos_sin = self.cos_sin_cache[positions]
        cos, sin = cos_sin.chunk(2, dim=-1)
        if positions.ndim == 2:
            assert self.mrope_section

            cos = torch.cat(
                [m[i] for i, m in enumerate(cos.split(self.mrope_section, dim=-1))],
                dim=-1,
            )
            sin = torch.cat(
                [m[i] for i, m in enumerate(sin.split(self.mrope_section, dim=-1))],
                dim=-1,
            )

        query_shape = query.shape
        query = query.view(num_tokens, -1, self.head_size)
        query_rot = query[..., : self.rotary_dim]
        query_pass = query[..., self.rotary_dim :]
        query_rot = _apply_rotary_emb(query_rot, cos, sin, self.is_neox_style)
        query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)

        key_shape = key.shape
        key = key.view(num_tokens, -1, self.head_size)
        key_rot = key[..., : self.rotary_dim]
        key_pass = key[..., self.rotary_dim :]
        key_rot = _apply_rotary_emb(key_rot, cos, sin, self.is_neox_style)
        key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
        return query, key

    # Copied from https://github.com/huggingface/transformers/blob/c8e0e603de9b3d49161a15fe6e8ea84badfb5d02/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L1439
    @staticmethod
    def get_rope_index(
        spatial_merge_size: int,
        image_token_id: int,
        video_token_id: int,
        vision_start_token_id: int,
        model_type: str,
        tokens_per_second: Optional[int] = None,
        input_ids: Optional[torch.LongTensor] = None,
        image_grid_thw: Optional[torch.LongTensor] = None,
        video_grid_thw: Optional[torch.LongTensor] = None,
        second_per_grid_ts: Optional[torch.Tensor] = None,
        **kwargs,
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        mrope_position_deltas = []
        if input_ids is not None and (
            image_grid_thw is not None or video_grid_thw is not None
        ):
            total_input_ids = input_ids
            position_ids = torch.ones(
                3,
                input_ids.shape[0],
                input_ids.shape[1],
                dtype=input_ids.dtype,
                device=input_ids.device,
            )
            image_index, video_index = 0, 0
            for i, input_ids in enumerate(total_input_ids):
                image_nums, video_nums = 0, 0
                vision_start_indices = torch.argwhere(
                    input_ids == vision_start_token_id
                ).squeeze(1)
                vision_tokens = input_ids[vision_start_indices + 1]
                image_nums = (vision_tokens == image_token_id).sum()
                video_nums = (vision_tokens == video_token_id).sum()
                input_tokens = input_ids.tolist()
                llm_pos_ids_list: list = []
                st = 0
                remain_images, remain_videos = image_nums, video_nums
                for _ in range(image_nums + video_nums):
                    if image_token_id in input_tokens and remain_images &gt; 0:
                        ed_image = input_tokens.index(image_token_id, st)
                    else:
                        ed_image = len(input_tokens) + 1
                    if video_token_id in input_tokens and remain_videos &gt; 0:
                        ed_video = input_tokens.index(video_token_id, st)
                    else:
                        ed_video = len(input_tokens) + 1
                    if ed_image &lt; ed_video:
                        t, h, w = (
                            image_grid_thw[image_index][0],
                            image_grid_thw[image_index][1],
                            image_grid_thw[image_index][2],
                        )
                        second_per_grid_t = 0
                        image_index += 1
                        remain_images -= 1
                        ed = ed_image
                    else:
                        t, h, w = (
                            video_grid_thw[video_index][0],
                            video_grid_thw[video_index][1],
                            video_grid_thw[video_index][2],
                        )
                        if second_per_grid_ts is not None:
                            second_per_grid_t = second_per_grid_ts[video_index]
                        else:
                            second_per_grid_t = 1.0
                        video_index += 1
                        remain_videos -= 1
                        ed = ed_video
                    llm_grid_t, llm_grid_h, llm_grid_w = (
                        t.item(),
                        h.item() // spatial_merge_size,
                        w.item() // spatial_merge_size,
                    )
                    text_len = ed - st

                    st_idx = (
                        llm_pos_ids_list[-1].max() + 1
                        if len(llm_pos_ids_list) &gt; 0
                        else 0
                    )
                    llm_pos_ids_list.append(
                        torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx
                    )

                    if model_type == &#34;qwen2_5_vl&#34;:
                        range_tensor = torch.arange(llm_grid_t).view(-1, 1)
                        expanded_range = range_tensor.expand(
                            -1, llm_grid_h * llm_grid_w
                        )

                        time_tensor = (
                            expanded_range * second_per_grid_t * tokens_per_second
                        )

                        time_tensor_long = time_tensor.long()
                        t_index = time_tensor_long.flatten()
                    elif model_type == &#34;qwen2_vl&#34;:
                        t_index = (
                            torch.arange(llm_grid_t)
                            .view(-1, 1)
                            .expand(-1, llm_grid_h * llm_grid_w)
                            .flatten()
                        )
                    else:
                        raise RuntimeError(&#34;Unimplemented&#34;)
                    h_index = (
                        torch.arange(llm_grid_h)
                        .view(1, -1, 1)
                        .expand(llm_grid_t, -1, llm_grid_w)
                        .flatten()
                    )
                    w_index = (
                        torch.arange(llm_grid_w)
                        .view(1, 1, -1)
                        .expand(llm_grid_t, llm_grid_h, -1)
                        .flatten()
                    )
                    llm_pos_ids_list.append(
                        torch.stack([t_index, h_index, w_index]) + text_len + st_idx
                    )
                    st = ed + llm_grid_t * llm_grid_h * llm_grid_w

                if st &lt; len(input_tokens):
                    st_idx = (
                        llm_pos_ids_list[-1].max() + 1
                        if len(llm_pos_ids_list) &gt; 0
                        else 0
                    )
                    text_len = len(input_tokens) - st
                    llm_pos_ids_list.append(
                        torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx
                    )

                llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)
                position_ids[..., i, :] = llm_positions.to(position_ids.device)
                mrope_position_deltas.append(
                    llm_positions.max() + 1 - len(total_input_ids[i])
                )
            mrope_position_deltas = torch.tensor(
                mrope_position_deltas, device=input_ids.device
            ).unsqueeze(1)
            return position_ids, mrope_position_deltas
        else:
            s = input_ids.shape[1]
            position_ids = torch.arange(s)
            position_ids = (
                position_ids.unsqueeze(0).expand(3, -1, -1).to(input_ids.device)
            )
            max_position_ids = position_ids.max(0, keepdim=False)[0].max(
                -1, keepdim=True
            )[0]
            mrope_position_deltas = max_position_ids + 1 - s
            return position_ids, mrope_position_deltas

    # Adapted from https://github.com/vllm-project/vllm/blob/3779eb8c81449b924a23457fc77e45a0e6171178/vllm/model_executor/layers/rotary_embedding.py#L1120
    @staticmethod
    def get_rope_index_glm4v(
        input_ids: torch.Tensor,
        hf_config: Any,
        image_grid_thw: Union[list[list[int]], torch.Tensor],
        video_grid_thw: Union[list[list[int]], torch.Tensor],
        attention_mask: torch.Tensor,
        **kwargs,
    ) -&gt; tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;Get mrope input positions and delta value for GLM4V.&#34;&#34;&#34;
        image_token_id = hf_config.image_token_id
        video_start_token_id = hf_config.video_start_token_id
        video_end_token_id = hf_config.video_end_token_id
        spatial_merge_size = hf_config.vision_config.spatial_merge_size

        mrope_position_deltas = []
        if input_ids is not None and (
            image_grid_thw is not None or video_grid_thw is not None
        ):
            total_input_ids = input_ids
            if attention_mask is None:
                attention_mask = torch.ones_like(total_input_ids)
            position_ids = torch.ones(
                3,
                input_ids.shape[0],
                input_ids.shape[1],
                dtype=input_ids.dtype,
                device=input_ids.device,
            )
            image_index, video_index = 0, 0
            video_group_index = 0
            attention_mask = attention_mask.to(total_input_ids.device)
            for i, input_ids in enumerate(total_input_ids):
                input_ids = input_ids[attention_mask[i] == 1]
                input_tokens = input_ids.tolist()

                input_token_type = []
                video_check_flg = False
                for token in input_tokens:
                    if token == video_start_token_id:
                        video_check_flg = True
                    elif token == video_end_token_id:
                        video_check_flg = False

                    if token == image_token_id and not video_check_flg:
                        input_token_type.append(&#34;image&#34;)
                    elif token == image_token_id and video_check_flg:
                        input_token_type.append(&#34;video&#34;)
                    else:
                        input_token_type.append(&#34;text&#34;)

                input_type_group = []
                for key, group in itertools.groupby(
                    enumerate(input_token_type), lambda x: x[1]
                ):
                    group = list(group)
                    start_index = group[0][0]
                    end_index = group[-1][0] + 1
                    input_type_group.append((key, start_index, end_index))

                llm_pos_ids_list = []
                video_frame_num = 1
                for modality_type, start_idx, end_idx in input_type_group:
                    st_idx = (
                        llm_pos_ids_list[-1].max() + 1
                        if len(llm_pos_ids_list) &gt; 0
                        else 0
                    )

                    if modality_type == &#34;image&#34;:
                        t, h, w = (
                            image_grid_thw[image_index][0],
                            image_grid_thw[image_index][1],
                            image_grid_thw[image_index][2],
                        )
                        llm_grid_t, llm_grid_h, llm_grid_w = (
                            t.item(),
                            h.item() // spatial_merge_size,
                            w.item() // spatial_merge_size,
                        )

                        t_index = (
                            torch.arange(llm_grid_t)
                            .view(-1, 1)
                            .expand(-1, llm_grid_h * llm_grid_w)
                            .flatten()
                        )
                        h_index = (
                            torch.arange(llm_grid_h)
                            .view(1, -1, 1)
                            .expand(llm_grid_t, -1, llm_grid_w)
                            .flatten()
                        )
                        w_index = (
                            torch.arange(llm_grid_w)
                            .view(1, 1, -1)
                            .expand(llm_grid_t, llm_grid_h, -1)
                            .flatten()
                        )
                        llm_pos_ids_list.append(
                            torch.stack([t_index, h_index, w_index]) + st_idx
                        )

                        image_index += 1
                        video_frame_num = 1

                    elif modality_type == &#34;video&#34;:
                        t, h, w = (
                            video_frame_num,
                            video_grid_thw[video_index][1],
                            video_grid_thw[video_index][2],
                        )

                        llm_grid_t, llm_grid_h, llm_grid_w = (
                            t,
                            h.item() // spatial_merge_size,
                            w.item() // spatial_merge_size,
                        )

                        for t_idx in range(llm_grid_t):
                            t_index = (
                                torch.tensor(t_idx)
                                .view(-1, 1)
                                .expand(-1, llm_grid_h * llm_grid_w)
                                .flatten()
                            )

                            h_index = (
                                torch.arange(llm_grid_h)
                                .view(1, -1, 1)
                                .expand(1, -1, llm_grid_w)
                                .flatten()
                            )
                            w_index = (
                                torch.arange(llm_grid_w)
                                .view(1, 1, -1)
                                .expand(1, llm_grid_h, -1)
                                .flatten()
                            )
                            llm_pos_ids_list.append(
                                torch.stack([t_index, h_index, w_index]) + st_idx
                            )

                        video_group_index += 1

                        if video_group_index &gt;= video_grid_thw[video_index][0]:
                            video_index += 1
                            video_group_index = 0

                        video_frame_num += 1

                    else:
                        text_len = end_idx - start_idx
                        llm_pos_ids_list.append(
                            torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx
                        )

                        video_frame_num = 1

                llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)
                position_ids[..., i, attention_mask[i] == 1] = llm_positions.to(
                    position_ids.device
                )
                mrope_position_deltas.append(
                    llm_positions.max() + 1 - len(total_input_ids[i])
                )
            mrope_position_deltas = torch.tensor(
                mrope_position_deltas, device=input_ids.device
            ).unsqueeze(1)
            return position_ids, mrope_position_deltas
        else:
            if attention_mask is not None:
                position_ids = attention_mask.long().cumsum(-1) - 1
                position_ids.masked_fill_(attention_mask == 0, 1)
                position_ids = (
                    position_ids.unsqueeze(0)
                    .expand(3, -1, -1)
                    .to(attention_mask.device)
                )
                max_position_ids = position_ids.max(0, keepdim=False)[0].max(
                    -1, keepdim=True
                )[0]
                mrope_position_deltas = max_position_ids + 1 - attention_mask.shape[-1]
            else:
                position_ids = (
                    torch.arange(input_ids.shape[1], device=input_ids.device)
                    .view(1, 1, -1)
                    .expand(3, input_ids.shape[0], -1)
                )
                mrope_position_deltas = torch.zeros(
                    [input_ids.shape[0], 1],
                    device=input_ids.device,
                    dtype=input_ids.dtype,
                )

            return position_ids, mrope_position_deltas

    @staticmethod
    def get_next_input_positions(
        mrope_position_delta: int,
        context_len: int,
        seq_len: int,
    ) -&gt; torch.Tensor:
        return torch.tensor(
            [
                list(
                    range(
                        context_len + mrope_position_delta,
                        seq_len + mrope_position_delta,
                    )
                )
                for _ in range(3)
            ]
        )</code></pre>
</details>
<div class="desc"><p>Rotary Embedding with Multimodal Sections.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding">RotaryEmbedding</a></li>
<li><a title="sglang.srt.custom_op.CustomOp" href="../custom_op.html#sglang.srt.custom_op.CustomOp">CustomOp</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="sglang.srt.layers.rotary_embedding.MRotaryEmbedding.get_next_input_positions"><code class="name flex">
<span>def <span class="ident">get_next_input_positions</span></span>(<span>mrope_position_delta: int, context_len: int, seq_len: int) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_next_input_positions(
    mrope_position_delta: int,
    context_len: int,
    seq_len: int,
) -&gt; torch.Tensor:
    return torch.tensor(
        [
            list(
                range(
                    context_len + mrope_position_delta,
                    seq_len + mrope_position_delta,
                )
            )
            for _ in range(3)
        ]
    )</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.MRotaryEmbedding.get_rope_index"><code class="name flex">
<span>def <span class="ident">get_rope_index</span></span>(<span>spatial_merge_size: int,<br>image_token_id: int,<br>video_token_id: int,<br>vision_start_token_id: int,<br>model_type: str,<br>tokens_per_second: int | None = None,<br>input_ids: torch.LongTensor | None = None,<br>image_grid_thw: torch.LongTensor | None = None,<br>video_grid_thw: torch.LongTensor | None = None,<br>second_per_grid_ts: torch.Tensor | None = None,<br>**kwargs) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_rope_index(
    spatial_merge_size: int,
    image_token_id: int,
    video_token_id: int,
    vision_start_token_id: int,
    model_type: str,
    tokens_per_second: Optional[int] = None,
    input_ids: Optional[torch.LongTensor] = None,
    image_grid_thw: Optional[torch.LongTensor] = None,
    video_grid_thw: Optional[torch.LongTensor] = None,
    second_per_grid_ts: Optional[torch.Tensor] = None,
    **kwargs,
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    mrope_position_deltas = []
    if input_ids is not None and (
        image_grid_thw is not None or video_grid_thw is not None
    ):
        total_input_ids = input_ids
        position_ids = torch.ones(
            3,
            input_ids.shape[0],
            input_ids.shape[1],
            dtype=input_ids.dtype,
            device=input_ids.device,
        )
        image_index, video_index = 0, 0
        for i, input_ids in enumerate(total_input_ids):
            image_nums, video_nums = 0, 0
            vision_start_indices = torch.argwhere(
                input_ids == vision_start_token_id
            ).squeeze(1)
            vision_tokens = input_ids[vision_start_indices + 1]
            image_nums = (vision_tokens == image_token_id).sum()
            video_nums = (vision_tokens == video_token_id).sum()
            input_tokens = input_ids.tolist()
            llm_pos_ids_list: list = []
            st = 0
            remain_images, remain_videos = image_nums, video_nums
            for _ in range(image_nums + video_nums):
                if image_token_id in input_tokens and remain_images &gt; 0:
                    ed_image = input_tokens.index(image_token_id, st)
                else:
                    ed_image = len(input_tokens) + 1
                if video_token_id in input_tokens and remain_videos &gt; 0:
                    ed_video = input_tokens.index(video_token_id, st)
                else:
                    ed_video = len(input_tokens) + 1
                if ed_image &lt; ed_video:
                    t, h, w = (
                        image_grid_thw[image_index][0],
                        image_grid_thw[image_index][1],
                        image_grid_thw[image_index][2],
                    )
                    second_per_grid_t = 0
                    image_index += 1
                    remain_images -= 1
                    ed = ed_image
                else:
                    t, h, w = (
                        video_grid_thw[video_index][0],
                        video_grid_thw[video_index][1],
                        video_grid_thw[video_index][2],
                    )
                    if second_per_grid_ts is not None:
                        second_per_grid_t = second_per_grid_ts[video_index]
                    else:
                        second_per_grid_t = 1.0
                    video_index += 1
                    remain_videos -= 1
                    ed = ed_video
                llm_grid_t, llm_grid_h, llm_grid_w = (
                    t.item(),
                    h.item() // spatial_merge_size,
                    w.item() // spatial_merge_size,
                )
                text_len = ed - st

                st_idx = (
                    llm_pos_ids_list[-1].max() + 1
                    if len(llm_pos_ids_list) &gt; 0
                    else 0
                )
                llm_pos_ids_list.append(
                    torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx
                )

                if model_type == &#34;qwen2_5_vl&#34;:
                    range_tensor = torch.arange(llm_grid_t).view(-1, 1)
                    expanded_range = range_tensor.expand(
                        -1, llm_grid_h * llm_grid_w
                    )

                    time_tensor = (
                        expanded_range * second_per_grid_t * tokens_per_second
                    )

                    time_tensor_long = time_tensor.long()
                    t_index = time_tensor_long.flatten()
                elif model_type == &#34;qwen2_vl&#34;:
                    t_index = (
                        torch.arange(llm_grid_t)
                        .view(-1, 1)
                        .expand(-1, llm_grid_h * llm_grid_w)
                        .flatten()
                    )
                else:
                    raise RuntimeError(&#34;Unimplemented&#34;)
                h_index = (
                    torch.arange(llm_grid_h)
                    .view(1, -1, 1)
                    .expand(llm_grid_t, -1, llm_grid_w)
                    .flatten()
                )
                w_index = (
                    torch.arange(llm_grid_w)
                    .view(1, 1, -1)
                    .expand(llm_grid_t, llm_grid_h, -1)
                    .flatten()
                )
                llm_pos_ids_list.append(
                    torch.stack([t_index, h_index, w_index]) + text_len + st_idx
                )
                st = ed + llm_grid_t * llm_grid_h * llm_grid_w

            if st &lt; len(input_tokens):
                st_idx = (
                    llm_pos_ids_list[-1].max() + 1
                    if len(llm_pos_ids_list) &gt; 0
                    else 0
                )
                text_len = len(input_tokens) - st
                llm_pos_ids_list.append(
                    torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx
                )

            llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)
            position_ids[..., i, :] = llm_positions.to(position_ids.device)
            mrope_position_deltas.append(
                llm_positions.max() + 1 - len(total_input_ids[i])
            )
        mrope_position_deltas = torch.tensor(
            mrope_position_deltas, device=input_ids.device
        ).unsqueeze(1)
        return position_ids, mrope_position_deltas
    else:
        s = input_ids.shape[1]
        position_ids = torch.arange(s)
        position_ids = (
            position_ids.unsqueeze(0).expand(3, -1, -1).to(input_ids.device)
        )
        max_position_ids = position_ids.max(0, keepdim=False)[0].max(
            -1, keepdim=True
        )[0]
        mrope_position_deltas = max_position_ids + 1 - s
        return position_ids, mrope_position_deltas</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.MRotaryEmbedding.get_rope_index_glm4v"><code class="name flex">
<span>def <span class="ident">get_rope_index_glm4v</span></span>(<span>input_ids: torch.Tensor,<br>hf_config: Any,<br>image_grid_thw: list[list[int]] | torch.Tensor,<br>video_grid_thw: list[list[int]] | torch.Tensor,<br>attention_mask: torch.Tensor,<br>**kwargs) ‑> tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_rope_index_glm4v(
    input_ids: torch.Tensor,
    hf_config: Any,
    image_grid_thw: Union[list[list[int]], torch.Tensor],
    video_grid_thw: Union[list[list[int]], torch.Tensor],
    attention_mask: torch.Tensor,
    **kwargs,
) -&gt; tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;Get mrope input positions and delta value for GLM4V.&#34;&#34;&#34;
    image_token_id = hf_config.image_token_id
    video_start_token_id = hf_config.video_start_token_id
    video_end_token_id = hf_config.video_end_token_id
    spatial_merge_size = hf_config.vision_config.spatial_merge_size

    mrope_position_deltas = []
    if input_ids is not None and (
        image_grid_thw is not None or video_grid_thw is not None
    ):
        total_input_ids = input_ids
        if attention_mask is None:
            attention_mask = torch.ones_like(total_input_ids)
        position_ids = torch.ones(
            3,
            input_ids.shape[0],
            input_ids.shape[1],
            dtype=input_ids.dtype,
            device=input_ids.device,
        )
        image_index, video_index = 0, 0
        video_group_index = 0
        attention_mask = attention_mask.to(total_input_ids.device)
        for i, input_ids in enumerate(total_input_ids):
            input_ids = input_ids[attention_mask[i] == 1]
            input_tokens = input_ids.tolist()

            input_token_type = []
            video_check_flg = False
            for token in input_tokens:
                if token == video_start_token_id:
                    video_check_flg = True
                elif token == video_end_token_id:
                    video_check_flg = False

                if token == image_token_id and not video_check_flg:
                    input_token_type.append(&#34;image&#34;)
                elif token == image_token_id and video_check_flg:
                    input_token_type.append(&#34;video&#34;)
                else:
                    input_token_type.append(&#34;text&#34;)

            input_type_group = []
            for key, group in itertools.groupby(
                enumerate(input_token_type), lambda x: x[1]
            ):
                group = list(group)
                start_index = group[0][0]
                end_index = group[-1][0] + 1
                input_type_group.append((key, start_index, end_index))

            llm_pos_ids_list = []
            video_frame_num = 1
            for modality_type, start_idx, end_idx in input_type_group:
                st_idx = (
                    llm_pos_ids_list[-1].max() + 1
                    if len(llm_pos_ids_list) &gt; 0
                    else 0
                )

                if modality_type == &#34;image&#34;:
                    t, h, w = (
                        image_grid_thw[image_index][0],
                        image_grid_thw[image_index][1],
                        image_grid_thw[image_index][2],
                    )
                    llm_grid_t, llm_grid_h, llm_grid_w = (
                        t.item(),
                        h.item() // spatial_merge_size,
                        w.item() // spatial_merge_size,
                    )

                    t_index = (
                        torch.arange(llm_grid_t)
                        .view(-1, 1)
                        .expand(-1, llm_grid_h * llm_grid_w)
                        .flatten()
                    )
                    h_index = (
                        torch.arange(llm_grid_h)
                        .view(1, -1, 1)
                        .expand(llm_grid_t, -1, llm_grid_w)
                        .flatten()
                    )
                    w_index = (
                        torch.arange(llm_grid_w)
                        .view(1, 1, -1)
                        .expand(llm_grid_t, llm_grid_h, -1)
                        .flatten()
                    )
                    llm_pos_ids_list.append(
                        torch.stack([t_index, h_index, w_index]) + st_idx
                    )

                    image_index += 1
                    video_frame_num = 1

                elif modality_type == &#34;video&#34;:
                    t, h, w = (
                        video_frame_num,
                        video_grid_thw[video_index][1],
                        video_grid_thw[video_index][2],
                    )

                    llm_grid_t, llm_grid_h, llm_grid_w = (
                        t,
                        h.item() // spatial_merge_size,
                        w.item() // spatial_merge_size,
                    )

                    for t_idx in range(llm_grid_t):
                        t_index = (
                            torch.tensor(t_idx)
                            .view(-1, 1)
                            .expand(-1, llm_grid_h * llm_grid_w)
                            .flatten()
                        )

                        h_index = (
                            torch.arange(llm_grid_h)
                            .view(1, -1, 1)
                            .expand(1, -1, llm_grid_w)
                            .flatten()
                        )
                        w_index = (
                            torch.arange(llm_grid_w)
                            .view(1, 1, -1)
                            .expand(1, llm_grid_h, -1)
                            .flatten()
                        )
                        llm_pos_ids_list.append(
                            torch.stack([t_index, h_index, w_index]) + st_idx
                        )

                    video_group_index += 1

                    if video_group_index &gt;= video_grid_thw[video_index][0]:
                        video_index += 1
                        video_group_index = 0

                    video_frame_num += 1

                else:
                    text_len = end_idx - start_idx
                    llm_pos_ids_list.append(
                        torch.arange(text_len).view(1, -1).expand(3, -1) + st_idx
                    )

                    video_frame_num = 1

            llm_positions = torch.cat(llm_pos_ids_list, dim=1).reshape(3, -1)
            position_ids[..., i, attention_mask[i] == 1] = llm_positions.to(
                position_ids.device
            )
            mrope_position_deltas.append(
                llm_positions.max() + 1 - len(total_input_ids[i])
            )
        mrope_position_deltas = torch.tensor(
            mrope_position_deltas, device=input_ids.device
        ).unsqueeze(1)
        return position_ids, mrope_position_deltas
    else:
        if attention_mask is not None:
            position_ids = attention_mask.long().cumsum(-1) - 1
            position_ids.masked_fill_(attention_mask == 0, 1)
            position_ids = (
                position_ids.unsqueeze(0)
                .expand(3, -1, -1)
                .to(attention_mask.device)
            )
            max_position_ids = position_ids.max(0, keepdim=False)[0].max(
                -1, keepdim=True
            )[0]
            mrope_position_deltas = max_position_ids + 1 - attention_mask.shape[-1]
        else:
            position_ids = (
                torch.arange(input_ids.shape[1], device=input_ids.device)
                .view(1, 1, -1)
                .expand(3, input_ids.shape[0], -1)
            )
            mrope_position_deltas = torch.zeros(
                [input_ids.shape[0], 1],
                device=input_ids.device,
                dtype=input_ids.dtype,
            )

        return position_ids, mrope_position_deltas</code></pre>
</details>
<div class="desc"><p>Get mrope input positions and delta value for GLM4V.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.rotary_embedding.MRotaryEmbedding.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@torch.compile(dynamic=True)
def forward(
    self,
    positions: torch.Tensor,
    query: torch.Tensor,
    key: torch.Tensor,
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;PyTorch-native implementation equivalent to forward().

    Args:
        positions:
            [num_tokens,] (text only) or
            [3, num_tokens] (T/H/W positions with multimodal inputs)
        query: [num_tokens, num_heads * head_size]
        key: [num_tokens, num_kv_heads * head_size]
    &#34;&#34;&#34;
    assert positions.ndim == 1 or positions.ndim == 2

    num_tokens = positions.shape[-1]
    cos_sin = self.cos_sin_cache[positions]
    cos, sin = cos_sin.chunk(2, dim=-1)
    if positions.ndim == 2:
        assert self.mrope_section

        cos = torch.cat(
            [m[i] for i, m in enumerate(cos.split(self.mrope_section, dim=-1))],
            dim=-1,
        )
        sin = torch.cat(
            [m[i] for i, m in enumerate(sin.split(self.mrope_section, dim=-1))],
            dim=-1,
        )

    query_shape = query.shape
    query = query.view(num_tokens, -1, self.head_size)
    query_rot = query[..., : self.rotary_dim]
    query_pass = query[..., self.rotary_dim :]
    query_rot = _apply_rotary_emb(query_rot, cos, sin, self.is_neox_style)
    query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)

    key_shape = key.shape
    key = key.view(num_tokens, -1, self.head_size)
    key_rot = key[..., : self.rotary_dim]
    key_pass = key[..., self.rotary_dim :]
    key_rot = _apply_rotary_emb(key_rot, cos, sin, self.is_neox_style)
    key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
    return query, key</code></pre>
</details>
<div class="desc"><p>PyTorch-native implementation equivalent to forward().</p>
<h2 id="args">Args</h2>
<dl>
<dt>positions:</dt>
<dt>[num_tokens,] (text only) or</dt>
<dt>[3, num_tokens] (T/H/W positions with multimodal inputs)</dt>
<dt><strong><code>query</code></strong></dt>
<dd>[num_tokens, num_heads * head_size]</dd>
<dt><strong><code>key</code></strong></dt>
<dd>[num_tokens, num_kv_heads * head_size]</dd>
</dl></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding">RotaryEmbedding</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.extra_repr" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.extra_repr">extra_repr</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_native" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_native">forward_native</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_npu" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_npu">forward_npu</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.Phi3LongRoPEScaledRotaryEmbedding"><code class="flex name class">
<span>class <span class="ident">Phi3LongRoPEScaledRotaryEmbedding</span></span>
<span>(</span><span>head_size: int,<br>rotary_dim: int,<br>max_position_embeddings: int,<br>original_max_position_embeddings: int,<br>base: int,<br>is_neox_style: bool,<br>dtype: torch.dtype,<br>short_factor: List[float],<br>long_factor: List[float],<br>short_mscale: float | None = None,<br>long_mscale: float | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Phi3LongRoPEScaledRotaryEmbedding(nn.Module):
    &#34;&#34;&#34;Phi3 family of models scaled rotary embedding.

    Based on the original RotaryEmbedding implementation.
    &#34;&#34;&#34;

    def __init__(
        self,
        head_size: int,
        rotary_dim: int,
        max_position_embeddings: int,
        original_max_position_embeddings: int,
        base: int,
        is_neox_style: bool,
        dtype: torch.dtype,
        short_factor: List[float],
        long_factor: List[float],
        short_mscale: Optional[float] = None,
        long_mscale: Optional[float] = None,
    ):
        super().__init__()

        if is_neox_style is False:
            raise ValueError(
                &#34;`Phi3LongRoPEScaledRotaryEmbedding` only supports neox_style.&#34;
            )

        self.rotary_dim = rotary_dim
        self.head_size = head_size
        self.max_position_embeddings = max_position_embeddings
        self.original_max_position_embeddings = original_max_position_embeddings
        self.base = base
        self.short_factor = short_factor
        self.long_factor = long_factor

        scale = self.max_position_embeddings / self.original_max_position_embeddings
        if scale &lt;= 1.0:
            scaling_factor = 1.0
        else:
            scaling_factor = math.sqrt(
                1 + math.log(scale) / math.log(self.original_max_position_embeddings)
            )
        if short_mscale is None:
            short_mscale = scaling_factor
        if long_mscale is None:
            long_mscale = scaling_factor

        self.short_mscale = short_mscale
        self.long_mscale = long_mscale

        short_cache = self._compute_cos_sin_cache(
            original_max_position_embeddings, short_factor, short_mscale
        )
        short_cache = short_cache.to(dtype)
        self.register_buffer(&#34;short_cos_sin_cache&#34;, short_cache, persistent=False)

        long_cache = self._compute_cos_sin_cache(
            max_position_embeddings, long_factor, long_mscale
        )
        long_cache = long_cache.to(dtype)
        self.register_buffer(&#34;long_cos_sin_cache&#34;, long_cache, persistent=False)

        long_short_cache = torch.cat(
            [self.short_cos_sin_cache, self.long_cos_sin_cache], dim=0
        )
        self.register_buffer(
            &#34;long_short_cos_sin_cache&#34;, long_short_cache, persistent=False
        )

    def _compute_inv_freq(self, rescale_factors: List[float]) -&gt; torch.Tensor:
        rescale_factors = torch.tensor(rescale_factors, dtype=torch.float32)
        inv_freq = 1.0 / (
            rescale_factors
            * (
                self.base
                ** (
                    torch.arange(0, self.rotary_dim, 2, dtype=torch.float)
                    / self.rotary_dim
                )
            )
        )
        return inv_freq

    def _compute_cos_sin_cache(
        self,
        max_position_embeddings: int,
        rescale_factors: List[float],
        mscale: float,
    ) -&gt; torch.Tensor:
        inv_freq = self._compute_inv_freq(rescale_factors)
        t = torch.arange(max_position_embeddings, dtype=torch.float)
        freqs = torch.einsum(&#34;i,j -&gt; ij&#34;, t, inv_freq)
        cos = freqs.cos() * mscale
        sin = freqs.sin() * mscale
        cache = torch.cat((cos, sin), dim=-1)
        return cache

    def forward(
        self,
        positions: torch.Tensor,
        query: torch.Tensor,
        key: torch.Tensor,
        offsets: Optional[torch.Tensor] = None,
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        query = query.view(*query.shape[:-1], -1, self.head_size)
        key = key.view(*key.shape[:-1], -1, self.head_size)

        k = self.original_max_position_embeddings
        long_prompt_offset = (
            torch.any(positions &gt; k).float() * torch.full_like(positions, k)
        ).long()
        idx = (
            torch.add(positions, long_prompt_offset)
            if long_prompt_offset is not None
            else positions
        )
        self.long_short_cos_sin_cache: torch.Tensor = self.long_short_cos_sin_cache.to(
            idx.device
        )
        idx = torch.add(idx, offsets) if offsets is not None else idx
        cos_sin = torch.index_select(self.long_short_cos_sin_cache, 0, idx)

        cos, sin = cos_sin.chunk(2, dim=-1)
        cos = cos.repeat(1, 2).unsqueeze(-2)
        sin = sin.repeat(1, 2).unsqueeze(-2)

        query_rot = query[..., : self.rotary_dim]
        query_pass = query[..., self.rotary_dim :]
        query_rot = query_rot * cos + _rotate_neox(query_rot) * sin
        query = torch.cat((query_rot, query_pass), dim=-1)

        key_rot = key[..., : self.rotary_dim]
        key_pass = key[..., self.rotary_dim :]
        key_rot = key_rot * cos + _rotate_neox(key_rot) * sin
        key = torch.cat((key_rot, key_pass), dim=-1)

        return query.flatten(-2), key.flatten(-2)</code></pre>
</details>
<div class="desc"><p>Phi3 family of models scaled rotary embedding.</p>
<p>Based on the original RotaryEmbedding implementation.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.rotary_embedding.Phi3LongRoPEScaledRotaryEmbedding.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self,<br>positions: torch.Tensor,<br>query: torch.Tensor,<br>key: torch.Tensor,<br>offsets: torch.Tensor | None = None) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(
    self,
    positions: torch.Tensor,
    query: torch.Tensor,
    key: torch.Tensor,
    offsets: Optional[torch.Tensor] = None,
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    query = query.view(*query.shape[:-1], -1, self.head_size)
    key = key.view(*key.shape[:-1], -1, self.head_size)

    k = self.original_max_position_embeddings
    long_prompt_offset = (
        torch.any(positions &gt; k).float() * torch.full_like(positions, k)
    ).long()
    idx = (
        torch.add(positions, long_prompt_offset)
        if long_prompt_offset is not None
        else positions
    )
    self.long_short_cos_sin_cache: torch.Tensor = self.long_short_cos_sin_cache.to(
        idx.device
    )
    idx = torch.add(idx, offsets) if offsets is not None else idx
    cos_sin = torch.index_select(self.long_short_cos_sin_cache, 0, idx)

    cos, sin = cos_sin.chunk(2, dim=-1)
    cos = cos.repeat(1, 2).unsqueeze(-2)
    sin = sin.repeat(1, 2).unsqueeze(-2)

    query_rot = query[..., : self.rotary_dim]
    query_pass = query[..., self.rotary_dim :]
    query_rot = query_rot * cos + _rotate_neox(query_rot) * sin
    query = torch.cat((query_rot, query_pass), dim=-1)

    key_rot = key[..., : self.rotary_dim]
    key_pass = key[..., self.rotary_dim :]
    key_rot = key_rot * cos + _rotate_neox(key_rot) * sin
    key = torch.cat((key_rot, key_pass), dim=-1)

    return query.flatten(-2), key.flatten(-2)</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
</dl>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.RotaryEmbedding"><code class="flex name class">
<span>class <span class="ident">RotaryEmbedding</span></span>
<span>(</span><span>head_size: int,<br>rotary_dim: int,<br>max_position_embeddings: int,<br>base: int,<br>is_neox_style: bool,<br>dtype: torch.dtype)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RotaryEmbedding(CustomOp):
    &#34;&#34;&#34;Original rotary positional embedding.&#34;&#34;&#34;

    def __init__(
        self,
        head_size: int,
        rotary_dim: int,
        max_position_embeddings: int,
        base: int,
        is_neox_style: bool,
        dtype: torch.dtype,
    ) -&gt; None:
        super().__init__()
        self.head_size = head_size
        self.rotary_dim = rotary_dim
        self.max_position_embeddings = max_position_embeddings
        self.base = base
        self.is_neox_style = is_neox_style
        self.dtype = dtype

        cache = self._compute_cos_sin_cache()
        # NOTE(ByronHsu): cache needs to be in FP32 for numerical stability
        if not _is_cuda:
            cache = cache.to(dtype)

        if (
            not (_is_cuda or _is_npu) or self.head_size not in [64, 128, 256, 512]
        ) and not (_is_cpu and _is_cpu_amx_available):
            from vllm._custom_ops import rotary_embedding

            self.vllm_rotary_embedding = rotary_embedding

        self.cos_sin_cache: torch.Tensor
        self.register_buffer(&#34;cos_sin_cache&#34;, cache, persistent=False)

    def _compute_inv_freq(self, base: Union[int, float]) -&gt; torch.Tensor:
        &#34;&#34;&#34;Compute the inverse frequency.&#34;&#34;&#34;
        # NOTE(woosuk): To exactly match the HF implementation, we need to
        # use CPU to compute the cache and then move it to GPU. However, we
        # create the cache on GPU for faster initialization. This may cause
        # a slight numerical difference between the HF implementation and ours.
        inv_freq = 1.0 / (
            base
            ** (
                torch.arange(0, self.rotary_dim, 2, dtype=torch.float) / self.rotary_dim
            )
        )
        return inv_freq

    def _compute_cos_sin_cache(self) -&gt; torch.Tensor:
        &#34;&#34;&#34;Compute the cos and sin cache.&#34;&#34;&#34;
        inv_freq = self._compute_inv_freq(self.base)
        t = torch.arange(self.max_position_embeddings, dtype=torch.float)

        freqs = torch.einsum(&#34;i,j -&gt; ij&#34;, t, inv_freq)
        cos = freqs.cos()
        sin = freqs.sin()
        cache = torch.cat((cos, sin), dim=-1)
        return cache

    def forward_native(
        self,
        positions: torch.Tensor,
        query: torch.Tensor,
        key: torch.Tensor,
        offsets: Optional[torch.Tensor] = None,
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;A PyTorch-native implementation of forward().&#34;&#34;&#34;
        if offsets is not None:
            positions = positions + offsets
        positions = positions.flatten()
        num_tokens = positions.shape[0]
        cos_sin = self.cos_sin_cache.index_select(0, positions)
        cos, sin = cos_sin.chunk(2, dim=-1)

        query_shape = query.shape
        query = query.view(num_tokens, -1, self.head_size)
        query_rot = query[..., : self.rotary_dim]
        query_pass = query[..., self.rotary_dim :]
        query_rot = _apply_rotary_emb(query_rot, cos, sin, self.is_neox_style)
        query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)

        key_shape = key.shape
        key = key.view(num_tokens, -1, self.head_size)
        key_rot = key[..., : self.rotary_dim]
        key_pass = key[..., self.rotary_dim :]
        key_rot = _apply_rotary_emb(key_rot, cos, sin, self.is_neox_style)
        key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
        return query, key

    def forward_npu(
        self,
        positions: torch.Tensor,
        query: torch.Tensor,
        key: torch.Tensor,
        offsets: Optional[torch.Tensor] = None,
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;A PyTorch-npu implementation of forward().&#34;&#34;&#34;
        import os

        if get_bool_env_var(&#34;SGLANG_ENABLE_TORCH_COMPILE&#34;):
            return self.forward_native(positions, query, key, offsets)
        else:
            rotary_mode = &#34;half&#34;
            if self.is_neox_style:
                rotary_mode = &#34;half&#34;
            else:
                rotary_mode = &#34;interleave&#34;
            mrope_section = [0, 0, 0]
            query_out, key_out = torch_npu.npu_mrope(
                positions,
                query,
                key,
                self.cos_sin_cache,
                self.head_size,
                mrope_section=mrope_section,
                rotary_mode=rotary_mode,
            )
            return query_out, key_out

    def forward_cpu(
        self,
        positions: torch.Tensor,
        query: torch.Tensor,
        key: torch.Tensor,
        offsets: Optional[torch.Tensor] = None,
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        positions = torch.add(positions, offsets) if offsets is not None else positions
        if _is_cpu_amx_available:
            return torch.ops.sgl_kernel.rotary_embedding_cpu(
                positions,
                query,
                key,
                self.head_size,
                self.cos_sin_cache,
                self.is_neox_style,
            )
        else:
            return self.forward_native(positions, query, key, offsets)

    def forward_cuda(
        self,
        positions: torch.Tensor,
        query: torch.Tensor,
        key: torch.Tensor,
        offsets: Optional[torch.Tensor] = None,
        fused_set_kv_buffer_arg=None,  # Optional[FusedSetKVBufferArg]
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        if _is_cuda and (self.head_size in [64, 128, 256, 512]):
            apply_rope_with_cos_sin_cache_inplace(
                positions=positions,
                query=query,
                key=key,
                head_size=self.head_size,
                cos_sin_cache=self.cos_sin_cache,
                is_neox=self.is_neox_style,
                # Compatible with old sgl-kernel
                **(
                    dict(fused_set_kv_buffer_arg=fused_set_kv_buffer_arg)
                    if fused_set_kv_buffer_arg is not None
                    else {}
                ),
            )
        else:
            assert (
                fused_set_kv_buffer_arg is None
            ), &#34;save kv cache is not supported for vllm_rotary_embedding.&#34;
            self.cos_sin_cache = self.cos_sin_cache.to(query.device, dtype=query.dtype)
            self.vllm_rotary_embedding(
                positions,
                query,
                key,
                self.head_size,
                self.cos_sin_cache,
                self.is_neox_style,
            )
        return query, key

    def extra_repr(self) -&gt; str:
        s = f&#34;head_size={self.head_size}, rotary_dim={self.rotary_dim}&#34;
        s += f&#34;, max_position_embeddings={self.max_position_embeddings}&#34;
        s += f&#34;, base={self.base}, is_neox_style={self.is_neox_style}&#34;
        return s</code></pre>
</details>
<div class="desc"><p>Original rotary positional embedding.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.custom_op.CustomOp" href="../custom_op.html#sglang.srt.custom_op.CustomOp">CustomOp</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.rotary_embedding.DeepseekScalingRotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.DeepseekScalingRotaryEmbedding">DeepseekScalingRotaryEmbedding</a></li>
<li><a title="sglang.srt.layers.rotary_embedding.DynamicNTKAlphaRotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.DynamicNTKAlphaRotaryEmbedding">DynamicNTKAlphaRotaryEmbedding</a></li>
<li><a title="sglang.srt.layers.rotary_embedding.DynamicNTKScalingRotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.DynamicNTKScalingRotaryEmbedding">DynamicNTKScalingRotaryEmbedding</a></li>
<li><a title="sglang.srt.layers.rotary_embedding.LinearScalingRotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.LinearScalingRotaryEmbedding">LinearScalingRotaryEmbedding</a></li>
<li><a title="sglang.srt.layers.rotary_embedding.Llama3RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.Llama3RotaryEmbedding">Llama3RotaryEmbedding</a></li>
<li><a title="sglang.srt.layers.rotary_embedding.Llama4VisionRotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.Llama4VisionRotaryEmbedding">Llama4VisionRotaryEmbedding</a></li>
<li><a title="sglang.srt.layers.rotary_embedding.MRotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.MRotaryEmbedding">MRotaryEmbedding</a></li>
<li><a title="sglang.srt.layers.rotary_embedding.YaRNScalingRotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.YaRNScalingRotaryEmbedding">YaRNScalingRotaryEmbedding</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="sglang.srt.layers.rotary_embedding.RotaryEmbedding.extra_repr"><code class="name flex">
<span>def <span class="ident">extra_repr</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extra_repr(self) -&gt; str:
    s = f&#34;head_size={self.head_size}, rotary_dim={self.rotary_dim}&#34;
    s += f&#34;, max_position_embeddings={self.max_position_embeddings}&#34;
    s += f&#34;, base={self.base}, is_neox_style={self.is_neox_style}&#34;
    return s</code></pre>
</details>
<div class="desc"><p>Return the extra representation of the module.</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p></div>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_cpu"><code class="name flex">
<span>def <span class="ident">forward_cpu</span></span>(<span>self,<br>positions: torch.Tensor,<br>query: torch.Tensor,<br>key: torch.Tensor,<br>offsets: torch.Tensor | None = None) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_cpu(
    self,
    positions: torch.Tensor,
    query: torch.Tensor,
    key: torch.Tensor,
    offsets: Optional[torch.Tensor] = None,
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    positions = torch.add(positions, offsets) if offsets is not None else positions
    if _is_cpu_amx_available:
        return torch.ops.sgl_kernel.rotary_embedding_cpu(
            positions,
            query,
            key,
            self.head_size,
            self.cos_sin_cache,
            self.is_neox_style,
        )
    else:
        return self.forward_native(positions, query, key, offsets)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_cuda"><code class="name flex">
<span>def <span class="ident">forward_cuda</span></span>(<span>self,<br>positions: torch.Tensor,<br>query: torch.Tensor,<br>key: torch.Tensor,<br>offsets: torch.Tensor | None = None,<br>fused_set_kv_buffer_arg=None) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_cuda(
    self,
    positions: torch.Tensor,
    query: torch.Tensor,
    key: torch.Tensor,
    offsets: Optional[torch.Tensor] = None,
    fused_set_kv_buffer_arg=None,  # Optional[FusedSetKVBufferArg]
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    if _is_cuda and (self.head_size in [64, 128, 256, 512]):
        apply_rope_with_cos_sin_cache_inplace(
            positions=positions,
            query=query,
            key=key,
            head_size=self.head_size,
            cos_sin_cache=self.cos_sin_cache,
            is_neox=self.is_neox_style,
            # Compatible with old sgl-kernel
            **(
                dict(fused_set_kv_buffer_arg=fused_set_kv_buffer_arg)
                if fused_set_kv_buffer_arg is not None
                else {}
            ),
        )
    else:
        assert (
            fused_set_kv_buffer_arg is None
        ), &#34;save kv cache is not supported for vllm_rotary_embedding.&#34;
        self.cos_sin_cache = self.cos_sin_cache.to(query.device, dtype=query.dtype)
        self.vllm_rotary_embedding(
            positions,
            query,
            key,
            self.head_size,
            self.cos_sin_cache,
            self.is_neox_style,
        )
    return query, key</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_native"><code class="name flex">
<span>def <span class="ident">forward_native</span></span>(<span>self,<br>positions: torch.Tensor,<br>query: torch.Tensor,<br>key: torch.Tensor,<br>offsets: torch.Tensor | None = None) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_native(
    self,
    positions: torch.Tensor,
    query: torch.Tensor,
    key: torch.Tensor,
    offsets: Optional[torch.Tensor] = None,
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;A PyTorch-native implementation of forward().&#34;&#34;&#34;
    if offsets is not None:
        positions = positions + offsets
    positions = positions.flatten()
    num_tokens = positions.shape[0]
    cos_sin = self.cos_sin_cache.index_select(0, positions)
    cos, sin = cos_sin.chunk(2, dim=-1)

    query_shape = query.shape
    query = query.view(num_tokens, -1, self.head_size)
    query_rot = query[..., : self.rotary_dim]
    query_pass = query[..., self.rotary_dim :]
    query_rot = _apply_rotary_emb(query_rot, cos, sin, self.is_neox_style)
    query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)

    key_shape = key.shape
    key = key.view(num_tokens, -1, self.head_size)
    key_rot = key[..., : self.rotary_dim]
    key_pass = key[..., self.rotary_dim :]
    key_rot = _apply_rotary_emb(key_rot, cos, sin, self.is_neox_style)
    key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
    return query, key</code></pre>
</details>
<div class="desc"><p>A PyTorch-native implementation of forward().</p></div>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_npu"><code class="name flex">
<span>def <span class="ident">forward_npu</span></span>(<span>self,<br>positions: torch.Tensor,<br>query: torch.Tensor,<br>key: torch.Tensor,<br>offsets: torch.Tensor | None = None) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_npu(
    self,
    positions: torch.Tensor,
    query: torch.Tensor,
    key: torch.Tensor,
    offsets: Optional[torch.Tensor] = None,
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;A PyTorch-npu implementation of forward().&#34;&#34;&#34;
    import os

    if get_bool_env_var(&#34;SGLANG_ENABLE_TORCH_COMPILE&#34;):
        return self.forward_native(positions, query, key, offsets)
    else:
        rotary_mode = &#34;half&#34;
        if self.is_neox_style:
            rotary_mode = &#34;half&#34;
        else:
            rotary_mode = &#34;interleave&#34;
        mrope_section = [0, 0, 0]
        query_out, key_out = torch_npu.npu_mrope(
            positions,
            query,
            key,
            self.cos_sin_cache,
            self.head_size,
            mrope_section=mrope_section,
            rotary_mode=rotary_mode,
        )
        return query_out, key_out</code></pre>
</details>
<div class="desc"><p>A PyTorch-npu implementation of forward().</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.custom_op.CustomOp" href="../custom_op.html#sglang.srt.custom_op.CustomOp">CustomOp</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.custom_op.CustomOp.forward" href="../custom_op.html#sglang.srt.custom_op.CustomOp.forward">forward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="sglang.srt.layers.rotary_embedding.YaRNScalingRotaryEmbedding"><code class="flex name class">
<span>class <span class="ident">YaRNScalingRotaryEmbedding</span></span>
<span>(</span><span>head_size: int,<br>rotary_dim: int,<br>max_position_embeddings: int,<br>base: int,<br>is_neox_style: bool,<br>scaling_factor: float,<br>dtype: torch.dtype,<br>*,<br>extrapolation_factor: float = 1,<br>attn_factor: float = 1,<br>beta_fast: int = 32,<br>beta_slow: int = 1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class YaRNScalingRotaryEmbedding(RotaryEmbedding):
    &#34;&#34;&#34;RotaryEmbedding extended with YaRN method.

    Credits to Peng et al. github.com/jquesnelle/yarn
    &#34;&#34;&#34;

    def __init__(
        self,
        head_size: int,
        rotary_dim: int,
        max_position_embeddings: int,
        base: int,
        is_neox_style: bool,
        scaling_factor: float,
        dtype: torch.dtype,
        *,
        extrapolation_factor: float = 1,
        attn_factor: float = 1,
        beta_fast: int = 32,
        beta_slow: int = 1,
    ) -&gt; None:
        self.scaling_factor = scaling_factor
        self.extrapolation_factor = extrapolation_factor
        self.attn_factor = attn_factor
        self.beta_fast = beta_fast
        self.beta_slow = beta_slow
        # Get n-d magnitude scaling corrected for interpolation
        self.mscale = float(_yarn_get_mscale(self.scaling_factor) * attn_factor)
        super().__init__(
            head_size, rotary_dim, max_position_embeddings, base, is_neox_style, dtype
        )

    def _compute_inv_freq(self, scaling_factor: float) -&gt; torch.Tensor:
        pos_freqs = self.base ** (
            torch.arange(0, self.rotary_dim, 2, dtype=torch.float) / self.rotary_dim
        )
        inv_freq_extrapolation = 1.0 / pos_freqs
        inv_freq_interpolation = 1.0 / (scaling_factor * pos_freqs)

        low, high = _yarn_find_correction_range(
            self.beta_fast,
            self.beta_slow,
            self.rotary_dim,
            self.base,
            self.max_position_embeddings,
        )
        # Get n-d rotational scaling corrected for extrapolation
        inv_freq_mask = (
            1
            - _yarn_linear_ramp_mask(low, high, self.rotary_dim // 2, dtype=torch.float)
        ) * self.extrapolation_factor
        inv_freq = (
            inv_freq_interpolation * (1 - inv_freq_mask)
            + inv_freq_extrapolation * inv_freq_mask
        )
        return inv_freq

    def _compute_cos_sin_cache(self) -&gt; torch.Tensor:
        inv_freq = self._compute_inv_freq(self.scaling_factor)
        t = torch.arange(
            self.max_position_embeddings * self.scaling_factor, dtype=torch.float32
        )
        freqs = torch.einsum(&#34;i,j -&gt; ij&#34;, t, inv_freq)
        cos = freqs.cos() * self.mscale
        sin = freqs.sin() * self.mscale
        cache = torch.cat((cos, sin), dim=-1)
        return cache</code></pre>
</details>
<div class="desc"><p>RotaryEmbedding extended with YaRN method.</p>
<p>Credits to Peng et al. github.com/jquesnelle/yarn</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding">RotaryEmbedding</a></li>
<li><a title="sglang.srt.custom_op.CustomOp" href="../custom_op.html#sglang.srt.custom_op.CustomOp">CustomOp</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding">RotaryEmbedding</a></b></code>:
<ul class="hlist">
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.extra_repr" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.extra_repr">extra_repr</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward" href="../custom_op.html#sglang.srt.custom_op.CustomOp.forward">forward</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_native" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_native">forward_native</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_npu" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_npu">forward_npu</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sglang.srt.layers" href="index.html">sglang.srt.layers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sglang.srt.layers.rotary_embedding.apply_rotary_pos_emb" href="#sglang.srt.layers.rotary_embedding.apply_rotary_pos_emb">apply_rotary_pos_emb</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.apply_rotary_pos_emb_native" href="#sglang.srt.layers.rotary_embedding.apply_rotary_pos_emb_native">apply_rotary_pos_emb_native</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.apply_rotary_pos_emb_npu" href="#sglang.srt.layers.rotary_embedding.apply_rotary_pos_emb_npu">apply_rotary_pos_emb_npu</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.get_rope" href="#sglang.srt.layers.rotary_embedding.get_rope">get_rope</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.get_rope_cpu" href="#sglang.srt.layers.rotary_embedding.get_rope_cpu">get_rope_cpu</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.get_rope_wrapper" href="#sglang.srt.layers.rotary_embedding.get_rope_wrapper">get_rope_wrapper</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.rotate_half" href="#sglang.srt.layers.rotary_embedding.rotate_half">rotate_half</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.yarn_get_mscale" href="#sglang.srt.layers.rotary_embedding.yarn_get_mscale">yarn_get_mscale</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sglang.srt.layers.rotary_embedding.DeepseekScalingRotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.DeepseekScalingRotaryEmbedding">DeepseekScalingRotaryEmbedding</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.rotary_embedding.DeepseekScalingRotaryEmbedding.forward_cpu" href="#sglang.srt.layers.rotary_embedding.DeepseekScalingRotaryEmbedding.forward_cpu">forward_cpu</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.DeepseekScalingRotaryEmbedding.forward_native" href="#sglang.srt.layers.rotary_embedding.DeepseekScalingRotaryEmbedding.forward_native">forward_native</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.rotary_embedding.DualChunkRotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.DualChunkRotaryEmbedding">DualChunkRotaryEmbedding</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.rotary_embedding.DualChunkRotaryEmbedding.extra_repr" href="#sglang.srt.layers.rotary_embedding.DualChunkRotaryEmbedding.extra_repr">extra_repr</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.rotary_embedding.DynamicNTKAlphaRotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.DynamicNTKAlphaRotaryEmbedding">DynamicNTKAlphaRotaryEmbedding</a></code></h4>
</li>
<li>
<h4><code><a title="sglang.srt.layers.rotary_embedding.DynamicNTKScalingRotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.DynamicNTKScalingRotaryEmbedding">DynamicNTKScalingRotaryEmbedding</a></code></h4>
</li>
<li>
<h4><code><a title="sglang.srt.layers.rotary_embedding.LinearScalingRotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.LinearScalingRotaryEmbedding">LinearScalingRotaryEmbedding</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.rotary_embedding.LinearScalingRotaryEmbedding.scaling_factor_to_offset" href="#sglang.srt.layers.rotary_embedding.LinearScalingRotaryEmbedding.scaling_factor_to_offset">scaling_factor_to_offset</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.rotary_embedding.Llama3RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.Llama3RotaryEmbedding">Llama3RotaryEmbedding</a></code></h4>
</li>
<li>
<h4><code><a title="sglang.srt.layers.rotary_embedding.Llama4VisionRotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.Llama4VisionRotaryEmbedding">Llama4VisionRotaryEmbedding</a></code></h4>
</li>
<li>
<h4><code><a title="sglang.srt.layers.rotary_embedding.MRotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.MRotaryEmbedding">MRotaryEmbedding</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.rotary_embedding.MRotaryEmbedding.forward" href="#sglang.srt.layers.rotary_embedding.MRotaryEmbedding.forward">forward</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.MRotaryEmbedding.get_next_input_positions" href="#sglang.srt.layers.rotary_embedding.MRotaryEmbedding.get_next_input_positions">get_next_input_positions</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.MRotaryEmbedding.get_rope_index" href="#sglang.srt.layers.rotary_embedding.MRotaryEmbedding.get_rope_index">get_rope_index</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.MRotaryEmbedding.get_rope_index_glm4v" href="#sglang.srt.layers.rotary_embedding.MRotaryEmbedding.get_rope_index_glm4v">get_rope_index_glm4v</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.rotary_embedding.Phi3LongRoPEScaledRotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.Phi3LongRoPEScaledRotaryEmbedding">Phi3LongRoPEScaledRotaryEmbedding</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.rotary_embedding.Phi3LongRoPEScaledRotaryEmbedding.forward" href="#sglang.srt.layers.rotary_embedding.Phi3LongRoPEScaledRotaryEmbedding.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding">RotaryEmbedding</a></code></h4>
<ul class="">
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.extra_repr" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.extra_repr">extra_repr</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_cpu" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_cpu">forward_cpu</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_cuda" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_cuda">forward_cuda</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_native" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_native">forward_native</a></code></li>
<li><code><a title="sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_npu" href="#sglang.srt.layers.rotary_embedding.RotaryEmbedding.forward_npu">forward_npu</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="sglang.srt.layers.rotary_embedding.YaRNScalingRotaryEmbedding" href="#sglang.srt.layers.rotary_embedding.YaRNScalingRotaryEmbedding">YaRNScalingRotaryEmbedding</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
