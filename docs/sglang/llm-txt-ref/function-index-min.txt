
# python/sglang/bench_offline_throughput.py
  BenchArgs.add_cli_args(parser: argparse.ArgumentParser)
  BenchArgs.from_cli_args(cls, args: argparse.Namespace)
throughput_test_once(backend_name: str, backend, reqs: List[DatasetRow], ignore_eos: bool, extra_request_body: Dict, profile: bool)
monitor_trace_file(known_files, directory, interval)
throughput_test(server_args: ServerArgs, bench_args: BenchArgs)

# python/sglang/bench_one_batch.py
  BenchArgs.add_cli_args(parser: argparse.ArgumentParser)
  BenchArgs.from_cli_args(cls, args: argparse.Namespace)
load_model(server_args, port_args, tp_rank)
prepare_inputs_for_correctness_test(bench_args, tokenizer, custom_prompts)
prepare_extend_inputs_for_correctness_test(bench_args, input_ids, reqs, model_runner)
prepare_synthetic_inputs_for_latency_test(batch_size, input_len, custom_inputs)
extend(reqs, model_runner)
decode(input_token_ids, batch, model_runner)
correctness_test(server_args, port_args, bench_args, tp_rank)
synchronize(device)
latency_test_run_once(run_name, model_runner, rank_print, reqs, batch_size, input_len, output_len, device, log_decode_step, profile, profile_record_shapes, profile_filename_prefix)
latency_test(server_args, port_args, bench_args, tp_rank)
main(server_args, bench_args)

# python/sglang/bench_one_batch_server.py
  BenchArgs.add_cli_args(parser: argparse.ArgumentParser)
  BenchArgs.from_cli_args(cls, args: argparse.Namespace)
launch_server_internal(server_args)
launch_server_process(server_args: ServerArgs)
run_one_case(url: str, batch_size: int, input_len: int, output_len: int, temperature: float, return_logprob: bool, stream_interval: int, input_len_step_percentage: float, run_name: str, result_filename: str, tokenizer, profile: bool, profile_steps: int, profile_by_stage: bool)
get_report_summary(result: List[Tuple], server_args: ServerArgs, bench_args: BenchArgs)
run_benchmark(server_args: ServerArgs, bench_args: BenchArgs)
main()

# python/sglang/bench_serving.py
  RequestFuncOutput.init_new(request_func_input: RequestFuncInput)
remove_prefix(text: str, prefix: str) -> str
remove_suffix(text: str, suffix: str) -> str
get_auth_headers() -> Dict[str, str]
async_request_trt_llm(request_func_input: RequestFuncInput, pbar: Optional[tqdm]) -> RequestFuncOutput
async_request_openai_completions(request_func_input: RequestFuncInput, pbar: Optional[tqdm]) -> RequestFuncOutput
async_request_openai_chat_completions(request_func_input: RequestFuncInput, pbar: Optional[tqdm]) -> RequestFuncOutput
async_request_truss(request_func_input: RequestFuncInput, pbar: Optional[tqdm]) -> RequestFuncOutput
async_request_sglang_generate(request_func_input: RequestFuncInput, pbar: Optional[tqdm]) -> RequestFuncOutput
async_request_gserver(request_func_input: RequestFuncInput, pbar: Optional[tqdm]) -> RequestFuncOutput
async_request_profile(api_url: str) -> RequestFuncOutput
get_model(pretrained_model_name_or_path: str) -> str
get_tokenizer(pretrained_model_name_or_path: str) -> Union[PreTrainedTokenizer, PreTrainedTokenizerFast]
get_dataset(args, tokenizer)
download_and_cache_file(url: str, filename: Optional[str])
is_file_valid_json(path)
sample_mmmu_requests(num_requests: int, tokenizer: PreTrainedTokenizerBase, fixed_output_len: Optional[int], apply_chat_template: bool, random_sample: bool) -> List[DatasetRow]
sample_sharegpt_requests(dataset_path: str, num_requests: int, tokenizer: PreTrainedTokenizerBase, fixed_output_len: Optional[int], context_len: Optional[int], prompt_suffix: Optional[str], apply_chat_template) -> List[DatasetRow]
sample_random_requests(input_len: int, output_len: int, num_prompts: int, range_ratio: float, tokenizer: PreTrainedTokenizerBase, dataset_path: str, random_sample: bool, return_text: bool) -> List[DatasetRow]
parse_random_image_resolution(image_resolution: str) -> Tuple[int, int]
sample_random_image_requests(num_requests: int, num_images: int, input_len: int, output_len: int, range_ratio: float, tokenizer: PreTrainedTokenizerBase, apply_chat_template: bool, image_resolution: str) -> List[DatasetRow]
gen_prompt(tokenizer, token_num)
get_gen_prefix_cache_path(args, tokenizer)
sample_generated_shared_prefix_requests(num_groups: int, prompts_per_group: int, system_prompt_len: int, question_len: int, output_len: int, tokenizer: PreTrainedTokenizerBase, args: argparse.Namespace) -> List[DatasetRow]
get_request(input_requests: List[DatasetRow], request_rate: float) -> AsyncGenerator[DatasetRow, None]
calculate_metrics(input_requests: List[DatasetRow], outputs: List[RequestFuncOutput], dur_s: float, tokenizer: PreTrainedTokenizerBase, backend: str) -> Tuple[BenchmarkMetrics, List[int]]
benchmark(backend: str, api_url: str, base_url: str, model_id: str, tokenizer: PreTrainedTokenizerBase, input_requests: List[DatasetRow], request_rate: float, max_concurrency: Optional[int], disable_tqdm: bool, lora_names: List[str], extra_request_body: Dict[str, Any], profile: bool, pd_separated: bool, flush_cache: bool, warmup_requests: int)
check_chat_template(model_path)
set_global_args(args_: argparse.Namespace)
run_benchmark(args_: argparse.Namespace)
set_ulimit(target_soft_limit)
  LoRAPathAction.__call__(parser, namespace, values, option_string)

# python/sglang/check_env.py
is_cuda_v2()
get_package_versions(packages)
get_cuda_info()
get_gpu_topology()
get_hypervisor_vendor()
check_env()

# python/sglang/compile_deep_gemm.py
  CompileArgs.add_cli_args(parser: argparse.ArgumentParser)
  CompileArgs.from_cli_args(cls, args: argparse.Namespace)
warm_up_compile(disaggregation_mode: str, tokenizer_manager: TokenizerManager)
launch_server_internal(server_args)
launch_server_process_and_send_one_request(server_args: ServerArgs, compile_args: CompileArgs)
refine_server_args(server_args: ServerArgs, compile_args: CompileArgs)
run_compile(server_args: ServerArgs, compile_args: CompileArgs)

# python/sglang/eval/llama3_eval.py
fetch_responses(client, prompt, semaphore, index, provider, model_size, output_dir, max_tokens)
  CustomAsyncHTTPXClient.send(request: httpx.Request) -> httpx.Response
get_client(provider)
benchmark(args)
get_mmlu_answer(response)
get_mmlu_cot_answer(response)
get_answer_gsm8k(response)
get_dataset_from_task(task, response_path, model_size)
analyze(task, response_path, model_size)

# python/sglang/eval/loogle_eval.py
get_client(api_url: str) -> openai.AsyncOpenAI
get_dataset()
fetch_response(client: openai.AsyncOpenAI, context: str, question: str, semaphore: asyncio.Semaphore, index: int, model: str, output_dir: Path)
benchmark(args)
analyse(args)

# python/sglang/global_config.py
  GlobalConfig.__init__()

# python/sglang/lang/api.py
function(func: Optional[Callable], num_api_spec_tokens: Optional[int])
Runtime()
Engine()
set_default_backend(backend: BaseBackend)
flush_cache(backend: Optional[BaseBackend])
get_server_info(backend: Optional[BaseBackend])
gen(name: Optional[str], max_tokens: Optional[int], min_tokens: Optional[int], n: Optional[int], stop: Optional[Union[str, List[str]]], stop_token_ids: Optional[List[int]], temperature: Optional[float], top_p: Optional[float], top_k: Optional[int], min_p: Optional[float], frequency_penalty: Optional[float], presence_penalty: Optional[float], ignore_eos: Optional[bool], return_logprob: Optional[bool], logprob_start_len: Optional[int], top_logprobs_num: Optional[int], return_text_in_logprobs: Optional[bool], dtype: Optional[Union[type, str]], choices: Optional[List[str]], choices_method: Optional[ChoicesSamplingMethod], regex: Optional[str], json_schema: Optional[str])
gen_int(name: Optional[str], max_tokens: Optional[int], n: Optional[int], stop: Optional[Union[str, List[str]]], stop_token_ids: Optional[List[int]], temperature: Optional[float], top_p: Optional[float], top_k: Optional[int], min_p: Optional[float], frequency_penalty: Optional[float], presence_penalty: Optional[float], ignore_eos: Optional[bool], return_logprob: Optional[bool], logprob_start_len: Optional[int], top_logprobs_num: Optional[int], return_text_in_logprobs: Optional[bool])
gen_string(name: Optional[str], max_tokens: Optional[int], n: Optional[int], stop: Optional[Union[str, List[str]]], stop_token_ids: Optional[List[int]], temperature: Optional[float], top_p: Optional[float], top_k: Optional[int], min_p: Optional[float], frequency_penalty: Optional[float], presence_penalty: Optional[float], ignore_eos: Optional[bool], return_logprob: Optional[bool], logprob_start_len: Optional[int], top_logprobs_num: Optional[int], return_text_in_logprobs: Optional[bool])
image(expr: SglExpr)
video(path: str, num_frames: int)
select(name: Optional[str], choices: Optional[List[str]], temperature: float, choices_method: ChoicesSamplingMethod)
system(expr: Optional[SglExpr])
user(expr: Optional[SglExpr])
assistant(expr: Optional[SglExpr])
system_begin()
system_end()
user_begin()
user_end()
assistant_begin()
assistant_end()
separate_reasoning(expr: Optional[SglExpr], model_type: Optional[str])

# python/sglang/lang/backend/anthropic.py
  Anthropic.__init__(model_name)
  Anthropic.get_chat_template()
  Anthropic.generate(s: StreamExecutor, sampling_params: SglSamplingParams)
  Anthropic.generate_stream(s: StreamExecutor, sampling_params: SglSamplingParams)

# python/sglang/lang/backend/base_backend.py
  BaseBackend.__init__() -> None
  BaseBackend.get_model_name()
  BaseBackend.get_chat_template()
  BaseBackend.cache_prefix(prefix_str: str)
  BaseBackend.uncache_prefix(rid: str)
  BaseBackend.end_request(rid: Union[str, List[str]])
  BaseBackend.begin_program(s: StreamExecutor)
  BaseBackend.end_program(s: Union[StreamExecutor, List[StreamExecutor]])
  BaseBackend.commit_lazy_operations(s: StreamExecutor)
  BaseBackend.fork_program(src: StreamExecutor, dst: List[StreamExecutor], position_ids_offset: Optional[List[int]])
  BaseBackend.fill_image(s: StreamExecutor)
  BaseBackend.generate(s: StreamExecutor, sampling_params: SglSamplingParams)
  BaseBackend.generate_stream(s: StreamExecutor, sampling_params: SglSamplingParams)
  BaseBackend.select(s: StreamExecutor, choices: List[str], temperature: float, choices_method: Optional[ChoicesSamplingMethod]) -> ChoicesDecision
  BaseBackend.concatenate_and_append(src_rids: List[str], dst_rid: str)
  BaseBackend.shutdown()
  BaseBackend.flush_cache()
  BaseBackend.get_server_info()

# python/sglang/lang/backend/litellm.py
  LiteLLM.__init__(model_name, chat_template, api_key, organization: Optional[str], base_url: Optional[str], timeout: Optional[float], max_retries: Optional[int], default_headers: Optional[Mapping[str, str]])
  LiteLLM.get_chat_template()
  LiteLLM.generate(s: StreamExecutor, sampling_params: SglSamplingParams)
  LiteLLM.generate_stream(s: StreamExecutor, sampling_params: SglSamplingParams)

# python/sglang/lang/backend/openai.py
create_logit_bias_int(tokenizer)
  TokenUsage.reset()
  OpenAI.__init__(model_name: str, is_chat_model: Optional[bool], chat_template: Optional[ChatTemplate], is_azure: bool)
  OpenAI.get_chat_template()
  OpenAI.generate(s: StreamExecutor, sampling_params: SglSamplingParams, spec_var_name: str)
  OpenAI.spec_fill(value: str)
  OpenAI.spec_pattern_match(comp)
  OpenAI.role_end_generate(s: StreamExecutor)
  OpenAI.generate_stream(s: StreamExecutor, sampling_params: SglSamplingParams)
  OpenAI.select(s: StreamExecutor, choices: List[str], temperature: float, choices_method: ChoicesSamplingMethod) -> ChoicesDecision
openai_completion(client, token_usage, is_chat, retries, prompt) -> Union[str, List[str]]
openai_completion_stream(client, token_usage, is_chat, retries, prompt)

# python/sglang/lang/backend/runtime_endpoint.py
  RuntimeEndpoint.__init__(base_url: str, api_key: Optional[str], verify: Optional[str], chat_template_name: Optional[str])
  RuntimeEndpoint.get_model_name()
  RuntimeEndpoint.flush_cache()
  RuntimeEndpoint.get_server_info()
  RuntimeEndpoint.get_chat_template()
  RuntimeEndpoint.cache_prefix(prefix_str: str)
  RuntimeEndpoint.start_profile()
  RuntimeEndpoint.stop_profile()
  RuntimeEndpoint.commit_lazy_operations(s: StreamExecutor)
  RuntimeEndpoint.fill_image(s: StreamExecutor)
  RuntimeEndpoint.generate(s: StreamExecutor, sampling_params: SglSamplingParams)
  RuntimeEndpoint.generate_stream(s: StreamExecutor, sampling_params: SglSamplingParams)
  RuntimeEndpoint.select(s: StreamExecutor, choices: List[str], temperature: float, choices_method: ChoicesSamplingMethod) -> ChoicesDecision
  RuntimeEndpoint.concatenate_and_append(src_rids: List[str], dst_rid: str)
compute_normalized_prompt_logprobs(input_logprobs)
  Runtime.__init__(log_level: str)
  Runtime.shutdown()
  Runtime.start_profile()
  Runtime.stop_profile()
  Runtime.cache_prefix(prefix: str)
  Runtime.get_tokenizer()
  Runtime.async_generate(prompt: str, sampling_params: Optional[Dict])
  Runtime.generate(prompt: Union[str, List[str]], sampling_params: Optional[Dict], return_logprob: Optional[Union[List[bool], bool]], logprob_start_len: Optional[Union[List[int], int]], top_logprobs_num: Optional[Union[List[int], int]], lora_path: Optional[List[Optional[str]]])
  Runtime.encode(prompt: Union[str, List[str], List[Dict], List[List[Dict]]])
  Runtime.get_server_info()
  Runtime.__del__()

# python/sglang/lang/backend/vertexai.py
  VertexAI.__init__(model_name, safety_settings)
  VertexAI.get_chat_template()
  VertexAI.generate(s: StreamExecutor, sampling_params: SglSamplingParams)
  VertexAI.generate_stream(s: StreamExecutor, sampling_params: SglSamplingParams)
  VertexAI.text_to_vertexai_input(text, images)
  VertexAI.messages_to_vertexai_input(messages)

# python/sglang/lang/chat_template.py
  ChatTemplate.get_prefix_and_suffix(role: str, hist_messages: List[Dict]) -> Tuple[str, str]
  ChatTemplate.get_prompt(messages: List[Dict]) -> str
register_chat_template(template)
register_chat_template_matching_function(func)
get_chat_template(name)
get_chat_template_by_model_path(model_path)
match_deepseek(model_path: str)
match_deepseek_janus_pro(model_path: str)
match_dbrx(model_path: str)
match_vicuna(model_path: str)
match_llama2_chat(model_path: str)
match_mistral(model_path: str)
match_llama3_instruct(model_path: str)
match_chat_ml(model_path: str)
match_chat_yi(model_path: str)
match_gemma_it(model_path: str)
match_openbmb_minicpm(model_path: str)
match_c4ai_command_r(model_path: str)
match_granite_instruct(model_path: str)
match_gemma3_instruct(model_path: str)
match_internvl_chat(model_path: str)
match_interns1_chat(model_path: str)

# python/sglang/lang/choices.py
  ChoicesSamplingMethod.requires_unconditional_logprobs() -> bool
  ChoicesSamplingMethod.__call__() -> ChoicesDecision
  TokenLengthNormalized.__call__() -> ChoicesDecision
  GreedyTokenSelection.__call__() -> ChoicesDecision
  UnconditionalLikelihoodNormalized.requires_unconditional_logprobs() -> bool
  UnconditionalLikelihoodNormalized.__call__() -> ChoicesDecision

# python/sglang/lang/compiler.py
compile_func(function, backend)
  CompiledFunction.__init__(tracer, function)
  CompiledFunction.build_graph(tracer)
  CompiledFunction.topological_sort()
  CompiledFunction.print_graph()
  CompiledFunction.run_internal(backend, kwargs, default_sampling_para)
  CompiledFunction.run()
  CompiledFunction.run_batch(batch_kwargs)
  CompGraphNode.__init__(expr: SglExpr, prev_node, next_nodes, source_node)
  CompGraphNode.add_next_node(other)
  CompGraphNode.__repr__()

# python/sglang/lang/interpreter.py
run_internal(state, program, func_args, func_kwargs, sync)
run_program(program, backend, func_args, func_kwargs, default_sampling_para, stream, sync, use_thread)
run_program_batch(program, backend, batch_arguments, default_sampling_para, num_threads, progress_bar, generator_style)
cache_program(program, backend)
  StreamExecutor.__init__(backend, arguments, default_sampling_para, chat_template, stream, num_api_spec_tokens, use_thread)
  StreamExecutor.submit(expr: SglExpr)
  StreamExecutor.sync()
  StreamExecutor.get_var(name)
  StreamExecutor.set_var(name, value)
  StreamExecutor.get_meta_info(name, timeout)
  StreamExecutor.fork(size: int, position_ids_offset: Optional[List[int]])
  StreamExecutor.text()
  StreamExecutor.messages()
  StreamExecutor.error()
  StreamExecutor.end()
  StreamExecutor.__del__()
  ProgramState.__init__(stream_executor: StreamExecutor)
  ProgramState.system(expr: Optional[SglExpr])
  ProgramState.user(expr: Optional[SglExpr])
  ProgramState.assistant(expr: Optional[SglExpr])
  ProgramState.var_scope(name: str)
  ProgramState.fork(size: int, position_ids_offset: Optional[List[int]])
  ProgramState.copy(position_ids_offset: Optional[List[int]])
  ProgramState.text()
  ProgramState.messages()
  ProgramState.sync()
  ProgramState.error()
  ProgramState.text_iter(var_name: Optional[str])
  ProgramState.text_async_iter(var_name: Optional[str], return_meta_data: bool)
  ProgramState.get_var(name)
  ProgramState.set_var(name, value)
  ProgramState.get_meta_info(name)
  ProgramState.__iadd__(other)
  ProgramState.__getitem__(name)
  ProgramState.__setitem__(name, value)
  ProgramState.__contains__(name)
  ProgramState.__del__()
  ProgramState.__repr__() -> str
  ProgramStateGroup.__init__(states: List[ProgramState], src_state: Optional[ProgramState])
  ProgramStateGroup.join(mode: str)
  ProgramStateGroup.__getitem__(i: int)
  ProgramStateGroup.__setitem__(i: int, value)
  ProgramStateGroup.__iadd__(other)

# python/sglang/lang/ir.py
  SglSamplingParams.clone()
  SglSamplingParams.to_openai_kwargs()
  SglSamplingParams.to_vertexai_kwargs()
  SglSamplingParams.to_anthropic_kwargs()
  SglSamplingParams.to_litellm_kwargs()
  SglSamplingParams.to_srt_kwargs()
  SglFunction.__init__(func, num_api_spec_tokens, bind_arguments)
  SglFunction.bind()
  SglFunction.run()
  SglFunction.run_batch(batch_kwargs)
  SglFunction.trace()
  SglFunction.cache(backend)
  SglFunction.compile()
  SglFunction.__call__()
  SglExpr.__init__()
  SglExpr.__add__(other)
  SglExpr.__radd__(other)
  SglExpr.concatenate_ir(a, b)
  SglExpr.print_graph_dfs()
  SglExprList.__init__(expr_list: List[SglExpr])
  SglExprList.__repr__()
  SglArgument.__init__(name: str, value: str)
  SglArgument.__repr__()
  SglArgument.__len__()
  SglArgument.__getitem__(i)
  SglArgument.__int__()
  SglArgument.__bool__()
  SglArgument.__format__()
  SglImage.__init__(path: str)
  SglImage.__repr__() -> str
  SglVideo.__init__(path: str, num_frames: int)
  SglVideo.__repr__() -> str
  SglGen.__init__(name: Optional[str], max_new_tokens: Optional[int], min_new_tokens: Optional[int], n: Optional[int], stop: Optional[Union[str, List[str]]], stop_token_ids: Optional[List[int]], temperature: Optional[float], top_p: Optional[float], top_k: Optional[int], min_p: Optional[float], frequency_penalty: Optional[float], presence_penalty: Optional[float], ignore_eos: Optional[bool], return_logprob: Optional[bool], logprob_start_len: Optional[int], top_logprobs_num: Optional[int], return_text_in_logprobs: Optional[bool], dtype: Optional[type], regex: Optional[str], json_schema: Optional[str])
  SglGen.__repr__()
  SglConstantText.__init__(value: str)
  SglConstantText.__repr__()
  SglRoleBegin.__init__(role: str)
  SglRoleBegin.__repr__()
  SglRoleEnd.__init__(role: str)
  SglRoleEnd.__repr__()
  SglSelect.__init__(name: str, choices: List[str], temperature: float, choices_method: ChoicesSamplingMethod)
  SglSelect.__repr__()
  SglFork.__init__(number: int, position_ids_offset)
  SglFork.__repr__()
  SglGetForkItem.__init__(index: int)
  SglGetForkItem.__repr__()
  SglVariable.__init__(name: str, source)
  SglVariable.__repr__()
  SglVarScopeBegin.__init__(name: str)
  SglVarScopeBegin.__repr__()
  SglVarScopeEnd.__init__(name: str)
  SglVarScopeEnd.__repr__()
  SglConcateAndAppend.__init__(states)
  SglConcateAndAppend.__repr__()
  SglCommitLazy.__init__()
  SglCommitLazy.__repr__()
  SglSeparateReasoning.__init__(model_type: str, expr: SglExpr)
  SglSeparateReasoning.process_name_for_reasoning(name)
  SglSeparateReasoning.__repr__()

# python/sglang/lang/tracer.py
extract_prefix_by_tracing(program, backend)
trace_program(program, arguments, backend)
  TracerProgramState.__init__(backend, arguments, only_trace_prefix)
  TracerProgramState.fork(size: int, position_ids_offset: Optional[List[int]])
  TracerProgramState.__iadd__(other)
  TracerProgramState.get_var(name)
  TracerProgramState.flatten_nodes()
  TracerProgramState.__del__()
  TracingScope.__init__(tracer_state: TracerProgramState)
  TracingScope.__enter__()
  TracingScope.__exit__(exc_type, exc_value, traceback)
  TracingScope.get_current_scope()
  TracingScope.add_child_state(state: TracerProgramState)

# python/sglang/profiler.py
run_profile(url: Optional[str], num_steps: int, activities: List[str], output_dir: Optional[str], profile_name: Optional[str], profile_by_stage: bool)

# python/sglang/srt/_custom_ops.py
init_custom_ar(ipc_tensors: List[torch.Tensor], rank_data: torch.Tensor, rank: int, full_nvlink: bool) -> int
all_reduce(fa: int, inp: torch.Tensor, out: torch.Tensor, reg_buffer: int, reg_buffer_sz_bytes: int) -> None
dispose(fa: int) -> None
meta_size() -> int
register_buffer(fa: int, ipc_tensors: List[int]) -> None
get_graph_buffer_ipc_meta(fa: int) -> Tuple[List[int], List[int]]
register_graph_buffers(fa: int, handles: List[List[int]], offsets: List[List[int]]) -> None
init_custom_ar(meta: torch.Tensor, rank_data: torch.Tensor, handles: List[str], offsets: List[int], rank: int, full_nvlink: bool) -> int
all_reduce_reg(fa: int, inp: torch.Tensor, out: torch.Tensor) -> None
all_reduce_unreg(fa: int, inp: torch.Tensor, reg_buffer: torch.Tensor, out: torch.Tensor) -> None
dispose(fa: int) -> None
meta_size() -> int
register_buffer(fa: int, t: torch.Tensor, handles: List[str], offsets: List[int]) -> None
get_graph_buffer_ipc_meta(fa: int) -> Tuple[torch.Tensor, List[int]]
register_graph_buffers(fa: int, handles: List[str], offsets: List[List[int]]) -> None
allocate_meta_buffer(size: int) -> torch.Tensor
get_meta_buffer_ipc_handle(inp: torch.Tensor) -> torch.Tensor
init_custom_qr(rank: int, world_size: int, qr_max_size: Optional[int]) -> int
qr_get_handle(fa: int) -> torch.Tensor
qr_open_handles(fa: int, handles: list[torch.Tensor]) -> None
qr_all_reduce(fa: int, inp: torch.Tensor, out: torch.Tensor, quant_level: int, cast_bf2half: bool) -> None
qr_destroy(fa: int) -> None
qr_max_size() -> int
mscclpp_generate_unique_id() -> bytes
mscclpp_init_context(unique_id: bytes, rank: int, world_size: int, scratch: torch.Tensor, put_buffer: torch.Tensor, nranks_per_node: int, rank_to_node: List[int], rank_to_ib: List[int], context_selection: int) -> int
mscclpp_allreduce(context: int, inp: torch.Tensor, out: torch.Tensor, nthreads: int, nblocks: int) -> None

# python/sglang/srt/aio_rwlock.py
  RWLock.__init__()
  RWLock.reader_lock()
  RWLock.writer_lock()
  RWLock.acquire_reader()
  RWLock.release_reader()
  RWLock.acquire_writer()
  RWLock.release_writer()
  _ReaderLock.__init__(rwlock: RWLock)
  _ReaderLock.__aenter__()
  _ReaderLock.__aexit__(exc_type, exc_val, exc_tb)
  _WriterLock.__init__(rwlock: RWLock)
  _WriterLock.__aenter__()
  _WriterLock.__aexit__(exc_type, exc_val, exc_tb)

# python/sglang/srt/bench_utils.py
  suppress_stdout_stderr.__enter__()
  suppress_stdout_stderr.__exit__()
bench_kineto(fn, kernel_names, num_tests: int, suppress_kineto_output: bool, trace_path: str, flush_l2: bool, with_multiple_kernels: bool)

# python/sglang/srt/code_completion_parser.py
register_completion_template(template: CompletionTemplate, override: bool)
completion_template_exists(template_name: str) -> bool
is_completion_template_defined() -> bool
generate_completion_prompt_from_request(request: CompletionRequest) -> str
generate_completion_prompt(prompt: str, suffix: str, template_name: str) -> str

# python/sglang/srt/configs/chatglm.py
  ChatGLMConfig.__init__(num_layers, padded_vocab_size, hidden_size, ffn_hidden_size, kv_channels, num_attention_heads, seq_length, hidden_dropout, attention_dropout, layernorm_epsilon, rmsnorm, apply_residual_connection_post_layernorm, post_layer_norm, add_bias_linear, add_qkv_bias, interleaved_qkv, bias_dropout_fusion, multi_query_attention, multi_query_group_num, apply_query_key_layer_scaling, attention_softmax_in_fp32, fp32_residual_connection, quantization_bit, pre_seq_len, prefix_projection)

# python/sglang/srt/configs/dbrx.py
  DbrxAttentionConfig.__init__(attn_pdrop: float, clip_qkv: Optional[float], kv_n_heads: int, rope_theta: float)
  DbrxAttentionConfig.from_pretrained(cls, pretrained_model_name_or_path: str) -> 'PretrainedConfig'
  DbrxFFNConfig.__init__(ffn_act_fn: Optional[dict], ffn_hidden_size: int, moe_num_experts: int, moe_top_k: int, moe_jitter_eps: Optional[float], moe_loss_weight: float, moe_normalize_expert_weights: Optional[float], uniform_expert_assignment: bool)
  DbrxFFNConfig.from_pretrained(cls, pretrained_model_name_or_path: str) -> 'PretrainedConfig'
  DbrxConfig.__init__(d_model: int, n_heads: int, n_layers: int, max_seq_len: int, vocab_size: int, resid_pdrop: float, emb_pdrop: float, attn_config: Optional[DbrxAttentionConfig], ffn_config: Optional[DbrxFFNConfig], use_cache: bool, initializer_range: float, output_router_logits: bool, router_aux_loss_coef: float)

# python/sglang/srt/configs/deepseekvl2.py
select_best_resolution(image_size, candidate_resolutions)
  DictOutput.items()
  DictOutput.keys()
  DictOutput.__getitem__(item)
  DictOutput.__contains__(key)
  DictOutput.__setitem__(key, value)
  VLChatProcessorOutput.__len__()
  ImageTransform.__init__(mean: Optional[Tuple[float, float, float]], std: Optional[Tuple[float, float, float]], normalize: bool)
  ImageTransform.__call__(pil_img: Image.Image)
  DeepseekVLV2Processor.__init__(tokenizer: LlamaTokenizerFast, candidate_resolutions: Tuple[Tuple[int, int]], patch_size: int, downsample_ratio: int, image_mean: Tuple[float, float, float], image_std: Tuple[float, float, float], normalize: bool, image_token: str, pad_token: str, add_special_token: bool, sft_format: str, mask_prompt: bool, ignore_id: int)
  DeepseekVLV2Processor.format_messages_v2(messages, pil_images, max_req_input_len)
  DeepseekVLV2Processor.bos_id()
  DeepseekVLV2Processor.eos_id()
  DeepseekVLV2Processor.pad_id()
  DeepseekVLV2Processor.encode(text: str, bos: bool, eos: bool)
  DeepseekVLV2Processor.decode(t: List[int]) -> str
  DeepseekVLV2Processor.process_one(prompt: str, conversations: List[Dict[str, str]], images: List[Image.Image], apply_sft_format: bool, inference_mode: bool, system_prompt: str, max_req_input_len: int)
  DeepseekVLV2Processor.__call__()
  DeepseekVLV2Processor.find_all_indices(messages, target_value)
  DeepseekVLV2Processor.tokenize_with_images(conversation: str, images: List[Image.Image], bos: bool, eos: bool, cropping: bool, max_req_input_len: int)
  DeepseekVL2VisionEncoderConfig.__init__(model_name: str, image_size: int, patch_size: int, width: int, layers: int, heads: int, mlp_ratio: int, global_pool: str, ignore_head: bool, class_token: bool, num_classes: int, use_checkpoint: bool)
  DeepseekVL2MlpProjectorConfig.__init__(projector_type: str, input_dim: int, n_embed: int, depth: int, mlp_ratio: int, downsample_ratio: int)
  DeepseekV2Config.__init__(vocab_size, hidden_size, intermediate_size, moe_intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, n_shared_experts, n_routed_experts, ep_size, routed_scaling_factor, kv_lora_rank, q_lora_rank, qk_rope_head_dim, v_head_dim, qk_nope_head_dim, topk_method, n_group, topk_group, num_experts_per_tok, moe_layer_freq, first_k_dense_replace, norm_topk_prob, scoring_func, aux_loss_alpha, seq_aux, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, pretraining_tp, tie_word_embeddings, rope_theta, rope_scaling, attention_bias, attention_dropout, use_mla)
  DeepseekVL2Config.__init__(tile_tag: str, global_view_pos: str, candidate_resolutions: Tuple[Tuple[int, int]])

# python/sglang/srt/configs/device_config.py
  DeviceConfig.__init__(device: str) -> None

# python/sglang/srt/configs/exaone.py
  ExaoneConfig.__init__(vocab_size, max_position_embeddings, hidden_size, num_layers, num_attention_heads, num_key_value_heads, intermediate_size, activation_function, rope_theta, rope_scaling, embed_dropout, attention_dropout, layer_norm_epsilon, initializer_range, use_cache, bos_token_id, eos_token_id, tie_word_embeddings)

# python/sglang/srt/configs/internvl.py
  InternLM2Config.__init__(vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, tie_word_embeddings, bias, rope_theta, rope_scaling, attn_implementation)
  InternVisionConfig.__init__(num_channels, patch_size, image_size, qkv_bias, hidden_size, num_attention_heads, intermediate_size, qk_normalization, num_hidden_layers, use_flash_attn, hidden_act, layer_norm_eps, dropout, drop_path_rate, attention_dropout, initializer_range, initializer_factor)
  InternVisionConfig.from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike]) -> 'PretrainedConfig'
  InternVLChatConfig.__init__(vision_config, llm_config, use_backbone_lora, use_llm_lora, pad2square, select_layer, force_image_size, downsample_ratio, template, dynamic_image_size, use_thumbnail, ps_version, min_dynamic_patch, max_dynamic_patch)
  InternVLChatConfig.to_dict()
  InternLM2Tokenizer.__init__(vocab_file, unk_token, bos_token, eos_token, pad_token, sp_model_kwargs: Optional[Dict[str, Any]], add_bos_token, add_eos_token, decode_with_prefix_space, clean_up_tokenization_spaces)
  InternLM2Tokenizer.no_prefix_space_tokens()
  InternLM2Tokenizer.vocab_size()
  InternLM2Tokenizer.bos_token_id() -> Optional[int]
  InternLM2Tokenizer.eos_token_id() -> Optional[int]
  InternLM2Tokenizer.get_vocab()
  InternLM2Tokenizer.convert_tokens_to_string(tokens)
  InternLM2Tokenizer.save_vocabulary(save_directory, filename_prefix: Optional[str]) -> Tuple[str]
  InternLM2Tokenizer.build_inputs_with_special_tokens(token_ids_0, token_ids_1)
  InternLM2Tokenizer.get_special_tokens_mask(token_ids_0: List[int], token_ids_1: Optional[List[int]], already_has_special_tokens: bool) -> List[int]
  InternLM2Tokenizer.create_token_type_ids_from_sequences(token_ids_0: List[int], token_ids_1: Optional[List[int]]) -> List[int]

# python/sglang/srt/configs/janus_pro.py
  DictToObject.__init__(dictionary)
  VisionConfig.__init__()
  GenAlignerConfig.__init__()
  GenHeadConfig.__init__()
  AlignerConfig.__init__()
  GenVisionConfig.__init__()
  MultiModalityConfig.__init__()
  VLMImageProcessor.__init__(image_size: int, min_size: int, image_mean: Union[Tuple[float, float, float], List[float]], image_std: Union[Tuple[float, float, float], List[float]], rescale_factor: float, do_normalize: bool)
  VLMImageProcessor.resize(pil_img: Image) -> np.ndarray
  VLMImageProcessor.preprocess(images, return_tensors: str) -> BatchFeature
  VLMImageProcessor.default_shape()
  DictOutput.items()
  DictOutput.keys()
  DictOutput.__getitem__(item)
  DictOutput.__contains__(key)
  DictOutput.__setitem__(key, value)
  VLChatProcessorOutput.__len__()
  VLChatProcessor.__init__(image_processor: VLMImageProcessor, tokenizer: LlamaTokenizerFast, image_tag: str, image_start_tag: str, image_end_tag: str, pad_tag: str, num_image_tokens: int, add_special_token: bool, sft_format: str, mask_prompt: bool, ignore_id: int)
  VLChatProcessor.image_token()
  VLChatProcessor.image_id() -> int
  VLChatProcessor.image_start_id()
  VLChatProcessor.image_end_id()
  VLChatProcessor.image_start_token()
  VLChatProcessor.image_end_token()
  VLChatProcessor.pad_id()
  VLChatProcessor.add_image_token(image_indices: List[int], input_ids: torch.LongTensor)
  VLChatProcessor.process_one(prompt: str, images: List[Image])
  VLChatProcessor.__call__()
  VLChatProcessor.batchify(prepare_list: List[VLChatProcessorOutput]) -> BatchedVLChatProcessorOutput
  VLMImageProcessorConfig.__init__(image_size: int, min_size: int, image_mean: Union[Tuple[float, float, float], List[float]], image_std: Union[Tuple[float, float, float], List[float]], rescale_factor: float, do_normalize: bool)

# python/sglang/srt/configs/kimi_vl.py
  KimiVLConfig.__init__(vision_config: Optional[Union[dict, MoonViTConfig]], text_config: Optional[Union[dict, DeepseekV2Config]], ignore_index: int, media_placeholder_token_id: int, pad_token_id: int)

# python/sglang/srt/configs/kimi_vl_moonvit.py
  MoonViTConfig.__init__(patch_size: int, init_pos_emb_height: int, init_pos_emb_width: int, num_attention_heads: int, num_hidden_layers: int, hidden_size: int, intermediate_size: int, merge_kernel_size: tuple[int, int])

# python/sglang/srt/configs/load_config.py
  LoadConfig.__post_init__()

# python/sglang/srt/configs/longcat_flash.py
  LongcatFlashConfig.__init__(vocab_size, hidden_size, intermediate_size, ffn_hidden_size, expert_ffn_hidden_size, num_layers, num_hidden_layers, num_attention_heads, ep_size, kv_lora_rank, q_lora_rank, qk_rope_head_dim, qk_nope_head_dim, v_head_dim, n_routed_experts, moe_topk, norm_topk_prob, max_position_embeddings, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, pretraining_tp, tie_word_embeddings, rope_theta, rope_scaling, attention_bias, attention_dropout, mla_scale_q_lora, mla_scale_kv_lora, torch_dtype, params_dtype, rounter_params_dtype, router_bias, topk_method, routed_scaling_factor, zero_expert_num, zero_expert_type, nextn_use_scmoe, num_nextn_predict_layers)

# python/sglang/srt/configs/model_config.py
  ModelConfig.__init__(model_path: str, trust_remote_code: bool, revision: Optional[str], context_length: Optional[int], model_override_args: str, is_embedding: Optional[bool], enable_multimodal: Optional[bool], dtype: str, quantization: Optional[str], override_config_file: Optional[str], is_draft_model: bool, hybrid_kvcache_ratio: Optional[float], model_impl: Union[str, ModelImpl]) -> None
  ModelConfig.from_server_args(server_args: ServerArgs, model_path: str)
  ModelConfig.get_total_num_attention_heads() -> int
  ModelConfig.get_num_attention_heads(tensor_parallel_size) -> int
  ModelConfig.get_total_num_kv_heads() -> int
  ModelConfig.get_num_kv_heads(tensor_parallel_size) -> int
  ModelConfig.get_hf_eos_token_id() -> Optional[Set[int]]
  ModelConfig.maybe_pull_model_tokenizer_from_remote() -> None
is_generation_model(model_architectures: List[str], is_embedding: bool)
is_multimodal_model(model_architectures: List[str])
is_multimodal_gen_model(model_architectures: List[str])
is_image_gen_model(model_architectures: List[str])
is_audio_model(model_architectures: List[str])
is_encoder_decoder_model(model_architectures: List[str])
is_multimodal_chunked_prefill_supported(model_architectures: List[str])
yarn_get_mscale(scale: float, mscale: float) -> float
is_hybrid_model(model_architectures: List[str], hybrid_kvcache_ratio: Optional[float], context_length: Optional[int], attention_chunk_size: Optional[int])
get_hybrid_layer_ids(model_architectures: List[str], num_hidden_layers: int)

# python/sglang/srt/configs/step3_vl.py
  Step3VisionEncoderConfig.__init__(hidden_size, intermediate_size, output_hidden_size, num_hidden_layers, num_attention_heads, num_channels, image_size, patch_size, hidden_act, layer_norm_eps)
  Step3TextConfig.__init__(hidden_size: int, intermediate_size: int, num_attention_heads: int, num_attention_groups: int, num_hidden_layers: int, max_seq_len: int, vocab_size: int, rms_norm_eps: float, moe_intermediate_size: int, moe_num_experts: int, moe_top_k: int, rope_theta: float, rope_scaling: Optional[dict[str, Any]], max_position_embedding: int, share_expert_dim: int, share_q_dim: int, head_dim: int, norm_expert_weight: bool, moe_layers_enum: tuple[int]) -> None
  Step3VLConfig.__init__(vision_config: Optional[Union[dict, Step3VisionEncoderConfig]], text_config: Optional[Union[dict, Step3TextConfig]], understand_projector_stride: int, projector_bias: bool, image_token_id: int) -> None

# python/sglang/srt/configs/update_config.py
may_get_weight_block_size(model_config, load_config)
get_moe_padding_size(weight_block_size)
get_num_heads_padding_size(tp_size, weight_block_size)
update_intermediate_size(model_config, attr_name, intermediate_padding_size)
adjust_config_with_unaligned_cpu_tp(model_config: ModelConfig, load_config: LoadConfig, tp_size: int) -> ModelConfig

# python/sglang/srt/configs/utils.py
register_image_processor(config: Type[PretrainedConfig], image_processor: Type[BaseImageProcessor])
register_processor(config: Type[PretrainedConfig], processor: Type[ProcessorMixin])

# python/sglang/srt/connector/__init__.py
create_remote_connector(url) -> BaseConnector
get_connector_type(client: BaseConnector) -> ConnectorType

# python/sglang/srt/connector/base_connector.py
  BaseConnector.__init__(url: str)
  BaseConnector.get_local_dir()
  BaseConnector.weight_iterator(rank: int) -> Generator[Tuple[str, torch.Tensor], None, None]
  BaseConnector.pull_files(allow_pattern: Optional[List[str]], ignore_pattern: Optional[List[str]]) -> None
  BaseConnector.close()
  BaseConnector.__enter__()
  BaseConnector.__exit__(exc_type, exc_value, traceback)
  BaseConnector.__del__()
  BaseKVConnector.get(key: str) -> Optional[torch.Tensor]
  BaseKVConnector.getstr(key: str) -> Optional[str]
  BaseKVConnector.set(key: str, obj: torch.Tensor) -> None
  BaseKVConnector.setstr(key: str, obj: str) -> None
  BaseKVConnector.list(prefix: str) -> List[str]
  BaseFileConnector.glob(allow_pattern: str) -> List[str]

# python/sglang/srt/connector/redis.py
  RedisConnector.__init__(url: str)
  RedisConnector.get(key: str) -> Optional[torch.Tensor]
  RedisConnector.getstr(key: str) -> Optional[str]
  RedisConnector.set(key: str, tensor: torch.Tensor) -> None
  RedisConnector.setstr(key: str, obj: str) -> None
  RedisConnector.list(prefix: str) -> List[str]
  RedisConnector.weight_iterator(rank: int) -> Generator[Tuple[str, bytes], None, None]
  RedisConnector.pull_files(allow_pattern: Optional[List[str]], ignore_pattern: Optional[List[str]]) -> None
  RedisConnector.close()

# python/sglang/srt/connector/s3.py
list_files(s3, path: str, allow_pattern: Optional[list[str]], ignore_pattern: Optional[list[str]]) -> tuple[str, str, list[str]]
  S3Connector.__init__(url: str) -> None
  S3Connector.glob(allow_pattern: Optional[list[str]]) -> list[str]
  S3Connector.pull_files(allow_pattern: Optional[list[str]], ignore_pattern: Optional[list[str]]) -> None
  S3Connector.weight_iterator(rank: int) -> Generator[Tuple[str, torch.Tensor], None, None]
  S3Connector.close()

# python/sglang/srt/connector/serde/__init__.py
create_serde(serde_type: str) -> Tuple[Serializer, Deserializer]

# python/sglang/srt/connector/serde/safe_serde.py
  SafeSerializer.__init__()
  SafeSerializer.to_bytes(t: torch.Tensor) -> bytes
  SafeDeserializer.__init__()
  SafeDeserializer.from_bytes_normal(b: Union[bytearray, bytes]) -> torch.Tensor
  SafeDeserializer.from_bytes(b: Union[bytearray, bytes]) -> torch.Tensor

# python/sglang/srt/connector/serde/serde.py
  Serializer.to_bytes(t: torch.Tensor) -> bytes
  Deserializer.__init__(dtype)
  Deserializer.from_bytes(bs: bytes) -> torch.Tensor

# python/sglang/srt/connector/utils.py
parse_model_name(url: str) -> str
pull_files_from_db(connector: BaseConnector, model_name: str, allow_pattern: Optional[list[str]], ignore_pattern: Optional[list[str]]) -> None

# python/sglang/srt/constrained/base_grammar_backend.py
  BaseGrammarObject.__init__()
  BaseGrammarObject.accept_token(token: int) -> None
  BaseGrammarObject.rollback(k: int)
  BaseGrammarObject.is_terminated()
  BaseGrammarObject.allocate_vocab_mask(vocab_size: int, batch_size: int, device) -> torch.Tensor
  BaseGrammarObject.fill_vocab_mask(vocab_mask: torch.Tensor, idx: int) -> None
  BaseGrammarObject.move_vocab_mask(vocab_mask: torch.Tensor, device) -> torch.Tensor
  BaseGrammarObject.apply_vocab_mask(logits: torch.Tensor, vocab_mask: torch.Tensor) -> None
  BaseGrammarObject.copy() -> 'BaseGrammarObject'
  BaseGrammarObject.finished()
  BaseGrammarObject.finished(finished)
  BaseGrammarObject.try_jump_forward(tokenizer) -> Optional[Tuple[List[int], str]]
  BaseGrammarObject.jump_forward_str_state(helper: Tuple[List[int], str]) -> Tuple[str, int]
  BaseGrammarObject.jump_and_retokenize(old_output_ids: List[int], new_output_ids: List[int], next_state: int) -> None
  BaseGrammarBackend.__init__()
  BaseGrammarBackend.dispatch_fallback(key_type: str, key_string: str) -> Optional[BaseGrammarObject]
  BaseGrammarBackend.dispatch_json(key_string: str) -> Optional[BaseGrammarObject]
  BaseGrammarBackend.dispatch_regex(key_string: str) -> Optional[BaseGrammarObject]
  BaseGrammarBackend.dispatch_ebnf(key_string: str) -> Optional[BaseGrammarObject]
  BaseGrammarBackend.dispatch_structural_tag(key_string: str) -> Optional[BaseGrammarObject]
  BaseGrammarBackend.get_cached_or_future_value(key: Tuple[str, str]) -> Optional[BaseGrammarObject]
  BaseGrammarBackend.set_cache(key: Tuple[str, str], value: BaseGrammarObject)
  BaseGrammarBackend.reset()
create_grammar_backend(server_args: ServerArgs, tokenizer, vocab_size: int, eos_token_ids: Optional[set]) -> Optional[BaseGrammarBackend]

# python/sglang/srt/constrained/llguidance_backend.py
  GuidanceGrammar.__init__(llguidance_tokenizer: LLTokenizer, serialized_grammar: str)
  GuidanceGrammar.accept_token(token: int)
  GuidanceGrammar.fill_vocab_mask(vocab_mask: torch.Tensor, idx: int) -> None
  GuidanceGrammar.allocate_vocab_mask(vocab_size: int, batch_size: int, device) -> torch.Tensor
  GuidanceGrammar.move_vocab_mask(vocab_mask: torch.Tensor, device) -> torch.Tensor
  GuidanceGrammar.apply_vocab_mask(logits: torch.Tensor, vocab_mask: torch.Tensor) -> None
  GuidanceGrammar.copy()
  GuidanceGrammar.try_jump_forward(tokenizer) -> Optional[Tuple[List[int], str]]
  GuidanceGrammar.jump_forward_str_state(helper: Tuple[List[int], str]) -> Tuple[str, int]
  GuidanceGrammar.jump_and_retokenize(old_output_ids: List[int], new_output_ids: List[int], next_state: int)
  GuidanceBackend.__init__(tokenizer, whitespace_pattern: Optional[str], n_vocab: Optional[int])
  GuidanceBackend.dispatch_json(key_string: str) -> Optional[GuidanceGrammar]
  GuidanceBackend.dispatch_regex(key_string: str) -> Optional[GuidanceGrammar]
  GuidanceBackend.dispatch_ebnf(key_string: str) -> Optional[GuidanceGrammar]
  GuidanceBackend.dispatch_structural_tag(key_string: str) -> Optional[GuidanceGrammar]

# python/sglang/srt/constrained/outlines_backend.py
  OutlinesGrammar.__init__(guide: RegexGuide, jump_forward_map: Union[OutlinesJumpForwardMap, None]) -> None
  OutlinesGrammar.accept_token(token: int)
  OutlinesGrammar.allocate_vocab_mask(vocab_size: int, batch_size: int, device) -> torch.Tensor
  OutlinesGrammar.move_vocab_mask(vocab_mask: torch.Tensor, device) -> torch.Tensor
  OutlinesGrammar.fill_vocab_mask(vocab_mask: torch.Tensor, idx: int) -> None
  OutlinesGrammar.apply_vocab_mask(logits: torch.Tensor, vocab_mask: torch.Tensor)
  OutlinesGrammar.copy()
  OutlinesGrammar.try_jump_forward(tokenizer) -> Optional[Tuple]
  OutlinesGrammar.jump_forward_str_state(helper: Tuple[List[int], str]) -> Tuple[str, int]
  OutlinesGrammar.jump_and_retokenize(old_output_ids: List[int], new_output_ids: List[int], next_state: int)
  OutlinesGrammarBackend.__init__(tokenizer, whitespace_pattern: bool)
  OutlinesGrammarBackend.dispatch_ebnf(key_string: str)
  OutlinesGrammarBackend.dispatch_structural_tag(key_string: str)
  OutlinesGrammarBackend.dispatch_json(key_string: str)
  OutlinesGrammarBackend.dispatch_regex(key_string: str)
build_regex_from_object(object: Union[str, BaseModel, Dict], whitespace_pattern: Optional[str])

# python/sglang/srt/constrained/outlines_jump_forward.py
disk_cache(expire: Optional[float], typed, ignore)
init_state_to_jump_forward(regex_string)
  OutlinesJumpForwardMap.__init__(regex_string)
  OutlinesJumpForwardMap.jump_forward_symbol(state)
  OutlinesJumpForwardMap.jump_forward_byte(state)
  OutlinesJumpForwardMap.is_jump_forward_symbol_state(state)
test_main(regex_string)

# python/sglang/srt/constrained/reasoner_grammar_backend.py
  ReasonerGrammarObject.__init__(grammar: BaseGrammarObject, think_end_id)
  ReasonerGrammarObject.accept_token(token: int)
  ReasonerGrammarObject.allocate_vocab_mask(vocab_size: int, batch_size: int, device) -> torch.Tensor
  ReasonerGrammarObject.fill_vocab_mask(vocab_mask: torch.Tensor, idx: int) -> None
  ReasonerGrammarObject.move_vocab_mask(vocab_mask: torch.Tensor, device) -> torch.Tensor
  ReasonerGrammarObject.apply_vocab_mask()
  ReasonerGrammarObject.copy() -> BaseGrammarObject
  ReasonerGrammarObject.finished()
  ReasonerGrammarObject.finished(finished)
  ReasonerGrammarObject.try_jump_forward(tokenizer)
  ReasonerGrammarObject.jump_forward_str_state(helper)
  ReasonerGrammarObject.jump_and_retokenize(old_output_ids: List[int], new_output_ids: List[int], next_state: int)
  ReasonerGrammarBackend.__init__(grammar_backend: BaseGrammarBackend, think_end_id)

# python/sglang/srt/constrained/triton_ops/bitmask_ops.py
apply_token_bitmask_inplace_kernel(logits_ptr, bitmask_ptr, indices_ptr, num_rows, vocab_size, logits_strides, bitmask_strides, NUM_SMS: tl.constexpr, BLOCK_SIZE: tl.constexpr)
apply_token_bitmask_inplace_triton(logits: torch.Tensor, bitmask: torch.Tensor, indices: Optional[Union[List[int], torch.Tensor]])

# python/sglang/srt/constrained/xgrammar_backend.py
  XGrammarGrammar.__init__(matcher: GrammarMatcher, vocab_size: int, ctx: CompiledGrammar, override_stop_tokens: Optional[Union[List[int], int]], key_string: Optional[str]) -> None
  XGrammarGrammar.accept_token(token: int)
  XGrammarGrammar.rollback(k: int)
  XGrammarGrammar.is_terminated()
  XGrammarGrammar.allocate_vocab_mask(vocab_size: int, batch_size: int, device) -> torch.Tensor
  XGrammarGrammar.fill_vocab_mask(vocab_mask: torch.Tensor, idx: int) -> None
  XGrammarGrammar.move_vocab_mask(vocab_mask: torch.Tensor, device) -> torch.Tensor
  XGrammarGrammar.apply_vocab_mask(logits: torch.Tensor, vocab_mask: torch.Tensor) -> None
  XGrammarGrammar.copy()
  XGrammarGrammar.try_jump_forward(tokenizer) -> Optional[Tuple[List[int], str]]
  XGrammarGrammar.jump_forward_str_state(helper: Tuple[List[int], str]) -> Tuple[str, int]
  XGrammarGrammar.jump_and_retokenize(old_output_ids: List[int], new_output_ids: List[int], next_state: int)
  XGrammarGrammar.__repr__()
  XGrammarGrammarBackend.__init__(tokenizer, vocab_size: int, model_eos_token_ids: Optional[List[int]])
  XGrammarGrammarBackend.dispatch_json(key_string: str) -> Optional[XGrammarGrammar]
  XGrammarGrammarBackend.dispatch_ebnf(key_string: str) -> Optional[XGrammarGrammar]
  XGrammarGrammarBackend.dispatch_regex(key_string: str) -> Optional[XGrammarGrammar]
  XGrammarGrammarBackend.dispatch_structural_tag(key_string: str) -> Optional[XGrammarGrammar]
  XGrammarGrammarBackend.reset()

# python/sglang/srt/conversation.py
  Conversation.get_prompt() -> str
  Conversation.set_system_message(system_message: str)
  Conversation.append_message(role: str, message: str)
  Conversation.append_image(image: str, detail: Literal['auto', 'low', 'high'])
  Conversation.append_video(video: str)
  Conversation.append_audio(audio: str)
  Conversation.update_last_message(message: str)
  Conversation.to_gradio_chatbot()
  Conversation.to_openai_api_messages()
  Conversation.copy()
  Conversation.dict()
register_conv_template(template: Conversation, override: bool)
register_conv_template_matching_function(func)
get_conv_template_by_model_path(model_path)
chat_template_exists(template_name: str) -> bool
generate_embedding_convs(texts: List[str], images: List[str], template_name: str) -> List[Conversation]
generate_chat_conv(request: ChatCompletionRequest, template_name: str) -> Conversation
get_model_type(model_path: str) -> Optional[str]
match_internvl(model_path: str)
match_deepseek_janus_pro(model_path: str)
match_vicuna(model_path: str)
match_deepseek_vl(model_path: str)
match_qwen_chat_ml(model_path: str)
match_minicpm(model_path: str)
match_phi_4_mm(model_path: str)

# python/sglang/srt/custom_op.py
  CustomOp.__init__()
  CustomOp.enter_torch_compile(num_tokens: int)
  CustomOp.leave_torch_compile()
  CustomOp.forward()
  CustomOp.forward_native()
  CustomOp.forward_cuda()
  CustomOp.forward_npu()
  CustomOp.forward_hip()
  CustomOp.forward_xpu()
  CustomOp.forward_hpu()
  CustomOp.forward_cpu()
  CustomOp.dispatch_forward()

# python/sglang/srt/debug_utils/dump_comparator.py
main(args)
read_meta(directory)
check_tensor_pair(path_baseline, path_target)

# python/sglang/srt/debug_utils/dumper.py
  _Dumper.__init__()
  _Dumper.on_forward_pass_start()
  _Dumper.dump(name, value)
get_truncated_value(value)

# python/sglang/srt/debug_utils/text_comparator.py
main(args)

# python/sglang/srt/disaggregation/ascend/conn.py
  AscendKVManager.init_engine()
  AscendKVManager.register_buffer_to_engine()
  AscendKVManager.send_kvcache(mooncake_session_id: str, prefill_kv_indices: npt.NDArray[np.int32], dst_kv_ptrs: list[int], dst_kv_indices: npt.NDArray[np.int32], executor: concurrent.futures.ThreadPoolExecutor)

# python/sglang/srt/disaggregation/ascend/transfer_engine.py
  AscendTransferEngine.__init__(hostname: str, npu_id: int, disaggregation_mode: DisaggregationMode)
  AscendTransferEngine.initialize() -> None
  AscendTransferEngine.batch_register(ptrs: List[int], lengths: List[int])

# python/sglang/srt/disaggregation/base/conn.py
  BaseKVManager.__init__(args: KVArgs, disaggregation_mode: DisaggregationMode, server_args: ServerArgs, is_mla_backend: Optional[bool])
  BaseKVSender.__init__(mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: int, dest_tp_ranks: List[int], pp_rank: int)
  BaseKVSender.init(num_kv_indices: int, aux_index: Optional[int])
  BaseKVSender.send(kv_indices: npt.NDArray[np.int32])
  BaseKVSender.poll() -> KVPoll
  BaseKVSender.failure_exception()
  BaseKVReceiver.__init__(mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: Optional[int])
  BaseKVReceiver.init(kv_indices: npt.NDArray[np.int32], aux_index: Optional[int])
  BaseKVReceiver.poll() -> KVPoll
  BaseKVReceiver.failure_exception()
  BaseKVBootstrapServer.__init__(port: int)

# python/sglang/srt/disaggregation/common/conn.py
  CommonKVManager.__init__(args: KVArgs, disaggregation_mode: DisaggregationMode, server_args: ServerArgs, is_mla_backend: Optional[bool])
  CommonKVReceiver.__init__(mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: Optional[int], data_parallel_rank: Optional[int])
  CommonKVReceiver.failure_exception()
  CommonKVBootstrapServer.__init__(port: int)
  CommonKVBootstrapServer.run()
  CommonKVBootstrapServer.close()
  CommonKVBootstrapServer.poll() -> KVPoll

# python/sglang/srt/disaggregation/common/utils.py
  FastQueue.__init__()
  FastQueue.put(item)
  FastQueue.get()
group_concurrent_contiguous(src_indices: npt.NDArray[np.int32], dst_indices: npt.NDArray[np.int32]) -> Tuple[List[npt.NDArray[np.int32]], List[npt.NDArray[np.int32]]]

# python/sglang/srt/disaggregation/decode.py
  DecodeReqToTokenPool.__init__(size: int, max_context_len: int, device: str, enable_memory_saver: bool, pre_alloc_size: int)
  DecodeReqToTokenPool.write(indices, values)
  DecodeReqToTokenPool.available_size()
  DecodeReqToTokenPool.alloc(need_size: int) -> List[int]
  DecodeReqToTokenPool.free(free_index: Union[int, List[int]])
  DecodeReqToTokenPool.clear()
  DecodePreallocQueue.__init__(req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, draft_token_to_kv_pool: Optional[KVCache], req_to_metadata_buffer_idx_allocator: ReqToMetadataIdxAllocator, metadata_buffers: MetadataBuffers, scheduler: Scheduler, transfer_queue: DecodeTransferQueue, tree_cache: BasePrefixCache, gloo_group: ProcessGroup, tp_rank: int, tp_size: int, dp_size: int, gpu_id: int, bootstrap_port: int, max_total_num_tokens: int, prefill_pp_size: int, num_reserved_decode_tokens: int, transfer_backend: TransferBackend)
  DecodePreallocQueue.add(req: Req, is_retracted: bool) -> None
  DecodePreallocQueue.extend(reqs: List[Req], is_retracted: bool) -> None
  DecodePreallocQueue.resume_retracted_reqs() -> List[Req]
  DecodePreallocQueue.pop_preallocated() -> List[DecodeRequest]
  DecodePreallocQueue.num_tokens_pre_allocated()
  DecodeTransferQueue.__init__(gloo_group: ProcessGroup, req_to_metadata_buffer_idx_allocator: ReqToMetadataIdxAllocator, tp_rank: int, metadata_buffers: MetadataBuffers, scheduler: Scheduler, tree_cache: BasePrefixCache)
  DecodeTransferQueue.add(decode_req: DecodeRequest) -> None
  DecodeTransferQueue.extend(decode_reqs: List[DecodeRequest]) -> None
  DecodeTransferQueue.pop_transferred() -> List[Req]
  SchedulerDisaggregationDecodeMixin.event_loop_normal_disagg_decode(self: Scheduler)
  SchedulerDisaggregationDecodeMixin.event_loop_overlap_disagg_decode(self: Scheduler)
  SchedulerDisaggregationDecodeMixin.get_next_disagg_decode_batch_to_run(self: Scheduler) -> Optional[Tuple[ScheduleBatch, bool]]
  SchedulerDisaggregationDecodeMixin.get_new_prebuilt_batch(self: Scheduler) -> Optional[ScheduleBatch]
  SchedulerDisaggregationDecodeMixin.process_decode_queue(self: Scheduler)

# python/sglang/srt/disaggregation/decode_schedule_batch_mixin.py
  ScheduleBatchDisaggregationDecodeMixin.prepare_for_prebuilt_extend(self: ScheduleBatch)
  ScheduleBatchDisaggregationDecodeMixin.process_prebuilt_extend(self: ScheduleBatch, server_args: ServerArgs, model_config: ModelConfig)

# python/sglang/srt/disaggregation/fake/conn.py
  FakeKVSender.__init__(mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: int, dest_tp_ranks: List[int], pp_rank: int)
  FakeKVSender.poll() -> KVPoll
  FakeKVSender.init(kv_indices: list[int], aux_index: Optional[int])
  FakeKVSender.send(kv_indices: npt.NDArray[np.int32])
  FakeKVSender.failure_exception()
  FakeKVReceiver.__init__(mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: Optional[int], data_parallel_rank: Optional[int])
  FakeKVReceiver.poll() -> KVPoll
  FakeKVReceiver.init(kv_indices: list[int], aux_index: Optional[int])
  FakeKVReceiver.failure_exception()

# python/sglang/srt/disaggregation/kv_events.py
  EventPublisher.__init__(attn_dp_rank: int)
  EventPublisher.publish(events: EventBatch) -> None
  EventPublisher.shutdown() -> None
  NullEventPublisher.publish(events) -> None
  NullEventPublisher.shutdown() -> None
  ZmqEventPublisher.__init__(attn_dp_rank: int, endpoint: str, replay_endpoint: Optional[str], buffer_steps: int, hwm: int, max_queue_size: int, topic: str) -> None
  ZmqEventPublisher.publish(events: EventBatch) -> None
  ZmqEventPublisher.shutdown() -> None
  ZmqEventPublisher.offset_endpoint_port(endpoint: Optional[str], data_parallel_rank: int) -> Optional[str]
  KVEventsConfig.from_cli(cls, cli_value: str) -> 'KVEventsConfig'
  EventPublisherFactory.register_publisher(cls, name: str, ctor: Callable[..., EventPublisher]) -> None
  EventPublisherFactory.create(cls, config: Optional[str], attn_dp_rank: int) -> EventPublisher

# python/sglang/srt/disaggregation/launch_lb.py
  LBArgs.add_cli_args(parser: argparse.ArgumentParser)
  LBArgs.from_cli_args(cls, args: argparse.Namespace) -> 'LBArgs'
main()

# python/sglang/srt/disaggregation/mini_lb.py
setup_logger()
  MiniLoadBalancer.__init__(prefill_configs: List[PrefillConfig], decode_servers: List[str], timeout: int)
  MiniLoadBalancer.add_prefill_server(new_prefill_config: PrefillConfig)
  MiniLoadBalancer.add_decode_server(new_decode_server: str)
  MiniLoadBalancer.select_pair()
  MiniLoadBalancer.generate(modified_request, prefill_server, decode_server, endpoint) -> ORJSONResponse
  MiniLoadBalancer.generate_stream(modified_request, prefill_server, decode_server, endpoint)
health_check()
health_check()
flush_cache()
get_server_info()
get_model_info()
handle_generate_request(request_data: dict)
handle_chat_completion_request(request_data: dict)
handle_completion_request(request_data: dict)
get_models()
register(obj: PDRegistryRequest)
run(prefill_configs, decode_addrs, host, port, timeout)

# python/sglang/srt/disaggregation/mooncake/conn.py
  KVTransferError.__init__(bootstrap_room: int, failure_reason: str)
  KVTransferError.__str__()
  TransferInfo.from_zmq(cls, msg: List[bytes])
  KVArgsRegisterInfo.from_zmq(cls, msg: List[bytes])
  AuxDataCodec.serialize_data_from_buffer(src_addr, data_length)
  AuxDataCodec.deserialize_data_to_buffer(kv_args, buffer_index, aux_index, data)
  MooncakeKVManager.__init__(args: KVArgs, disaggregation_mode: DisaggregationMode, server_args: ServerArgs, is_mla_backend: Optional[bool])
  MooncakeKVManager.init_engine()
  MooncakeKVManager.register_buffer_to_engine()
  MooncakeKVManager.send_kvcache(mooncake_session_id: str, prefill_kv_indices: npt.NDArray[np.int32], dst_kv_ptrs: list[int], dst_kv_indices: npt.NDArray[np.int32], executor: concurrent.futures.ThreadPoolExecutor)
  MooncakeKVManager.send_kvcache_slice(mooncake_session_id: str, prefill_kv_indices: npt.NDArray[np.int64], dst_kv_ptrs: list[int], dst_kv_indices: npt.NDArray[np.int64], dst_tp_rank: int, dst_attn_tp_size: int, dst_kv_item_len: int, executor: concurrent.futures.ThreadPoolExecutor)
  MooncakeKVManager.send_aux(req: TransferInfo, prefill_aux_index: int, dst_aux_ptrs: list[int])
  MooncakeKVManager.send_aux_tcp(req: TransferInfo, prefill_aux_index: int, dst_aux_ptrs: list[int])
  MooncakeKVManager.send_aux_data_to_endpoint(remote: str, dst_port: int, room: int, buffer_index: int, aux_index: int, data: bytes)
  MooncakeKVManager.sync_status_to_decode_endpoint(remote: str, dst_port: int, room: int, status: int, prefill_rank: int)
  MooncakeKVManager.transfer_worker(queue: FastQueue, executor: concurrent.futures.ThreadPoolExecutor)
  MooncakeKVManager.start_prefill_thread()
  MooncakeKVManager.start_decode_thread()
  MooncakeKVManager.add_transfer_request(bootstrap_room: int, kv_indices: npt.NDArray[np.int32], index_slice: slice, is_last: bool, aux_index: Optional[int])
  MooncakeKVManager.check_status(bootstrap_room: int)
  MooncakeKVManager.update_status(bootstrap_room: int, status: KVPoll)
  MooncakeKVManager.record_failure(bootstrap_room: int, failure_reason: str)
  MooncakeKVManager.get_session_id()
  MooncakeKVSender.__init__(mgr: MooncakeKVManager, bootstrap_addr: str, bootstrap_room: int, dest_tp_ranks: List[int], pp_rank: int)
  MooncakeKVSender.init(num_kv_indices: int, aux_index: Optional[int])
  MooncakeKVSender.send(kv_indices: npt.NDArray[np.int32])
  MooncakeKVSender.poll() -> KVPoll
  MooncakeKVSender.clear() -> None
  MooncakeKVSender.failure_exception()
  MooncakeKVSender.abort()
  MooncakeKVReceiver.__init__(mgr: MooncakeKVManager, bootstrap_addr: str, bootstrap_room: Optional[int], data_parallel_rank: Optional[int])
  MooncakeKVReceiver.init(kv_indices: npt.NDArray[np.int32], aux_index: Optional[int])
  MooncakeKVReceiver.poll() -> KVPoll
  MooncakeKVReceiver.clear() -> None
  MooncakeKVReceiver.failure_exception()
  MooncakeKVReceiver.abort()
  MooncakeKVBootstrapServer.__init__(port: int)
  MooncakeKVBootstrapServer.run()
  MooncakeKVBootstrapServer.close()
  MooncakeKVBootstrapServer.poll() -> KVPoll

# python/sglang/srt/disaggregation/mooncake/transfer_engine.py
  MooncakeTransferEngine.__init__(hostname: str, gpu_id: int, ib_device: Optional[str])
  MooncakeTransferEngine.register(ptr, length)
  MooncakeTransferEngine.deregister(ptr)
  MooncakeTransferEngine.batch_register(ptrs: List[int], lengths: List[int]) -> int
  MooncakeTransferEngine.batch_deregister(ptrs: List[int]) -> int
  MooncakeTransferEngine.initialize(hostname: str, device_name: Optional[str]) -> None
  MooncakeTransferEngine.transfer_sync(session_id: str, buffer: int, peer_buffer_address: int, length: int) -> int
  MooncakeTransferEngine.batch_transfer_sync(session_id: str, buffers: List[int], peer_buffer_addresses: List[int], lengths: List[int]) -> int
  MooncakeTransferEngine.get_session_id()

# python/sglang/srt/disaggregation/nixl/conn.py
  TransferInfo.is_dummy()
  TransferInfo.from_zmq(cls, msg: List[bytes])
  KVArgsRegisterInfo.from_zmq(cls, msg: List[bytes])
  TransferStatus.is_done()
  NixlKVManager.__init__(args: KVArgs, disaggregation_mode: DisaggregationMode, server_args: ServerArgs, is_mla_backend: Optional[bool])
  NixlKVManager.check_status(bootstrap_room: int)
  NixlKVManager.update_status(bootstrap_room: int, status: KVPoll)
  NixlKVManager.register_buffer_to_engine()
  NixlKVManager.send_kvcache(peer_name: str, prefill_kv_indices: npt.NDArray[np.int32], dst_kv_ptrs: list[int], dst_kv_indices: npt.NDArray[np.int32], dst_gpu_id: int, notif: str)
  NixlKVManager.send_aux(peer_name: str, prefill_aux_index: int, dst_aux_ptrs: list[int], dst_aux_index: int, notif: str)
  NixlKVManager.add_transfer_request(bootstrap_room: int, kv_indices: npt.NDArray[np.int32], index_slice: slice, is_last: bool, chunk_id: int, aux_index: Optional[int])
  NixlKVManager.update_transfer_status()
  NixlKVManager.check_transfer_done(room: int)
  NixlKVSender.__init__(mgr: NixlKVManager, bootstrap_addr: str, bootstrap_room: int, dest_tp_ranks: List[int], pp_rank: int)
  NixlKVSender.init(num_kv_indices: int, aux_index: Optional[int])
  NixlKVSender.send(kv_indices: npt.NDArray[np.int32])
  NixlKVSender.poll() -> KVPoll
  NixlKVSender.failure_exception()
  NixlKVReceiver.__init__(mgr: NixlKVManager, bootstrap_addr: str, bootstrap_room: Optional[int], data_parallel_rank: Optional[int])
  NixlKVReceiver.init(kv_indices: npt.NDArray[np.int32], aux_index: Optional[int])
  NixlKVReceiver.poll() -> KVPoll
  NixlKVReceiver.failure_exception()

# python/sglang/srt/disaggregation/prefill.py
  PrefillBootstrapQueue.__init__(token_to_kv_pool: KVCache, draft_token_to_kv_pool: Optional[KVCache], req_to_metadata_buffer_idx_allocator: ReqToMetadataIdxAllocator, metadata_buffers: MetadataBuffers, tp_rank: int, tp_size: int, gpu_id: int, bootstrap_port: int, gloo_group: ProcessGroup, max_total_num_tokens: int, decode_tp_size: int, decode_dp_size: int, scheduler: Scheduler, pp_rank: int, pp_size: int, transfer_backend: TransferBackend)
  PrefillBootstrapQueue.add(req: Req, num_kv_heads: int) -> None
  PrefillBootstrapQueue.extend(reqs: List[Req], num_kv_heads: int) -> None
  PrefillBootstrapQueue.pop_bootstrapped(return_failed_reqs: bool, rids_to_check: Optional[List[str]]) -> List[Req]
  SchedulerDisaggregationPrefillMixin.event_loop_normal_disagg_prefill(self: Scheduler) -> None
  SchedulerDisaggregationPrefillMixin.event_loop_overlap_disagg_prefill(self: Scheduler) -> None
  SchedulerDisaggregationPrefillMixin.process_batch_result_disagg_prefill(self: Scheduler, batch: ScheduleBatch, result: GenerationBatchResult, launch_done: Optional[threading.Event]) -> None
  SchedulerDisaggregationPrefillMixin.process_disagg_prefill_inflight_queue(self: Scheduler, rids_to_check: Optional[List[str]]) -> List[Req]
  SchedulerDisaggregationPrefillMixin.get_transferred_rids(self: Scheduler) -> List[str]
  SchedulerDisaggregationPrefillMixin.process_prefill_chunk(self: Scheduler) -> None
  SchedulerDisaggregationPrefillMixin.send_kv_chunk(self: Scheduler, req: Req, last_chunk: bool, end_idx: Optional[int]) -> None
  SchedulerDisaggregationPrefillMixin.event_loop_pp_disagg_prefill(self: Scheduler)
  SchedulerDisaggregationPrefillMixin.send_pyobj_to_next_stage(data)
  SchedulerDisaggregationPrefillMixin.recv_pyobj_from_prev_stage()

# python/sglang/srt/disaggregation/utils.py
poll_and_all_reduce(pollers, gloo_group)
  ReqToMetadataIdxAllocator.__init__(size: int)
  ReqToMetadataIdxAllocator.available_size()
  ReqToMetadataIdxAllocator.alloc() -> Optional[int]
  ReqToMetadataIdxAllocator.free(free_index: int)
  MetadataBuffers.__init__(size: int, hidden_size: int, dtype: torch.dtype, max_top_logprobs_num: int, custom_mem_pool: torch.cuda.MemPool)
  MetadataBuffers.get_buf_infos()
  MetadataBuffers.get_buf(idx: int)
  MetadataBuffers.set_buf(req: Req)
get_kv_class(transfer_backend: TransferBackend, class_type: KVClassType)
kv_to_page_indices(kv_indices: np.ndarray, page_size: int)
kv_to_page_num(num_kv_indices: int, page_size: int)
  PDRegistryRequest.__post_init__()
register_disaggregation_server(mode: str, server_port: int, bootstrap_port: int, pdlb_url: str)
is_mla_backend(target_kv_pool) -> bool
prepare_abort(req: Req, error_message: str, status_code)

# python/sglang/srt/distributed/communication_op.py
tensor_model_parallel_all_reduce(input_: torch.Tensor) -> torch.Tensor
tensor_model_parallel_all_gather(input_: torch.Tensor, dim: int) -> torch.Tensor
tensor_model_parallel_gather(input_: torch.Tensor, dst: int, dim: int) -> Optional[torch.Tensor]
broadcast_tensor_dict(tensor_dict: Optional[Dict[Any, Union[torch.Tensor, Any]]], src: int)

# python/sglang/srt/distributed/device_communicators/cuda_wrapper.py
find_loaded_library(lib_name) -> Optional[str]
  CudaRTLibrary.__init__(so_file: Optional[str])
  CudaRTLibrary.CUDART_CHECK(result: cudaError_t) -> None
  CudaRTLibrary.cudaGetErrorString(error: cudaError_t) -> str
  CudaRTLibrary.cudaSetDevice(device: int) -> None
  CudaRTLibrary.cudaDeviceSynchronize() -> None
  CudaRTLibrary.cudaDeviceReset() -> None
  CudaRTLibrary.cudaMalloc(size: int) -> ctypes.c_void_p
  CudaRTLibrary.cudaFree(devPtr: ctypes.c_void_p) -> None
  CudaRTLibrary.cudaMemset(devPtr: ctypes.c_void_p, value: int, count: int) -> None
  CudaRTLibrary.cudaMemcpy(dst: ctypes.c_void_p, src: ctypes.c_void_p, count: int) -> None
  CudaRTLibrary.cudaIpcGetMemHandle(devPtr: ctypes.c_void_p) -> cudaIpcMemHandle_t
  CudaRTLibrary.cudaIpcOpenMemHandle(handle: cudaIpcMemHandle_t) -> ctypes.c_void_p

# python/sglang/srt/distributed/device_communicators/custom_all_reduce.py
  CustomAllreduce.__init__(group: ProcessGroup, device: Union[int, str, torch.device], max_size) -> None
  CustomAllreduce.create_shared_buffer(size_in_bytes: int, group: Optional[ProcessGroup]) -> List[int]
  CustomAllreduce.free_shared_buffer(pointers: List[int], group: Optional[ProcessGroup]) -> None
  CustomAllreduce.capture()
  CustomAllreduce.register_buffer(inp: torch.Tensor)
  CustomAllreduce.register_graph_buffers()
  CustomAllreduce.should_custom_ar(inp: torch.Tensor)
  CustomAllreduce.all_reduce_reg(inp: torch.Tensor, out: torch.Tensor)
  CustomAllreduce.all_reduce_unreg(inp: torch.Tensor, out: torch.Tensor)
  CustomAllreduce.all_reduce(inp: torch.Tensor)
  CustomAllreduce.custom_all_reduce(input: torch.Tensor) -> Optional[torch.Tensor]
  CustomAllreduce.close()
  CustomAllreduce.__del__()

# python/sglang/srt/distributed/device_communicators/custom_all_reduce_utils.py
update_environment_variables(envs: Dict[str, str])
producer(batch_src: Sequence[int], producer_queue, consumer_queue, result_queue, cuda_visible_devices: Optional[str])
consumer(batch_tgt: Sequence[int], producer_queue, consumer_queue, result_queue, cuda_visible_devices: Optional[str])
can_actually_p2p(batch_src: Sequence[int], batch_tgt: Sequence[int]) -> Sequence[bool]
gpu_p2p_access_check(src: int, tgt: int) -> bool
with_nvml_context(fn: Callable[_P, _R]) -> Callable[_P, _R]
is_full_nvlink(physical_device_ids: List[int], world_size: int) -> bool
is_weak_contiguous(inp: torch.Tensor)

# python/sglang/srt/distributed/device_communicators/hpu_communicator.py
  HpuCommunicator.__init__(group: ProcessGroup)
  HpuCommunicator.all_reduce(x: torch.Tensor) -> torch.Tensor
  HpuCommunicator.all_gather(x: torch.Tensor, dim: int) -> torch.Tensor

# python/sglang/srt/distributed/device_communicators/npu_communicator.py
  NpuCommunicator.__init__(group: ProcessGroup)
  NpuCommunicator.all_reduce(x: torch.Tensor) -> torch.Tensor
  NpuCommunicator.all_gather(x: torch.Tensor, dim: int) -> torch.Tensor

# python/sglang/srt/distributed/device_communicators/pymscclpp.py
mscclpp_is_weak_contiguous(inp: torch.Tensor)
mscclpp_convert_to_bytes(size_str)
mscclpp_bench_time(func, test_niter: int, warmup_niter: int)
  PyMscclppCommunicator.__init__(group: ProcessGroup, device: Union[int, str, torch.device], max_bytes) -> None
  PyMscclppCommunicator.pre_tune_config(dtype) -> bool
  PyMscclppCommunicator.should_mscclpp_allreduce(inp: torch.Tensor, op: ReduceOp) -> bool
  PyMscclppCommunicator.all_reduce(tensor: torch.Tensor, op: ReduceOp)
  PyMscclppCommunicator.change_state(enable: Optional[bool])

# python/sglang/srt/distributed/device_communicators/pynccl.py
  PyNcclCommunicator.__init__(group: Union[ProcessGroup, StatelessProcessGroup], device: Union[int, str, torch.device], library_path: Optional[str])
  PyNcclCommunicator.all_reduce(tensor: torch.Tensor, op: ReduceOp, stream)
  PyNcclCommunicator.all_gather(output_tensor: torch.Tensor, input_tensor: torch.Tensor, stream, sizes: Optional[list[int]])
  PyNcclCommunicator.reduce_scatter(output_tensor: torch.Tensor, input_tensor: torch.Tensor, op: ReduceOp, stream, sizes: Optional[list[int]])
  PyNcclCommunicator.send(tensor: torch.Tensor, dst: int, stream)
  PyNcclCommunicator.recv(tensor: torch.Tensor, src: int, stream)
  PyNcclCommunicator.broadcast(tensor: torch.Tensor, src: int, stream)
  PyNcclCommunicator.register_comm_window_raw(ptr: int, size: int)
  PyNcclCommunicator.deregister_comm_window(window)
  PyNcclCommunicator.group_start()
  PyNcclCommunicator.group_end()
  PyNcclCommunicator.change_state(enable: Optional[bool], stream: Optional[torch.cuda.Stream])

# python/sglang/srt/distributed/device_communicators/pynccl_allocator.py
is_symmetric_memory_enabled()
set_graph_pool_id(graph_pool_id)
get_nccl_mem_pool()
  use_symmetric_memory.__init__(group_coordinator: GroupCoordinator)
  use_symmetric_memory.__enter__()
  use_symmetric_memory.tag(tensor: torch.Tensor)
  use_symmetric_memory.__exit__(exc_type, exc_val, exc_tb)

# python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py
find_nccl_library() -> str
  ncclDataTypeEnum.from_torch(cls, dtype: torch.dtype) -> int
  ncclRedOpTypeEnum.from_torch(cls, op: ReduceOp) -> int
  NCCLLibrary.__init__(so_file: Optional[str])
  NCCLLibrary.ncclGetErrorString(result: ncclResult_t) -> str
  NCCLLibrary.NCCL_CHECK(result: ncclResult_t) -> None
  NCCLLibrary.ncclGetRawVersion() -> int
  NCCLLibrary.ncclGetVersion() -> str
  NCCLLibrary.ncclGetUniqueId() -> ncclUniqueId
  NCCLLibrary.ncclCommInitRank(world_size: int, unique_id: ncclUniqueId, rank: int) -> ncclComm_t
  NCCLLibrary.ncclAllReduce(sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, comm: ncclComm_t, stream: cudaStream_t) -> None
  NCCLLibrary.ncclReduce(sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, root: int, comm: ncclComm_t, stream: cudaStream_t) -> None
  NCCLLibrary.ncclReduceScatter(sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, comm: ncclComm_t, stream: cudaStream_t) -> None
  NCCLLibrary.ncclAllGather(sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, comm: ncclComm_t, stream: cudaStream_t) -> None
  NCCLLibrary.ncclSend(sendbuff: buffer_type, count: int, datatype: int, dest: int, comm: ncclComm_t, stream: cudaStream_t) -> None
  NCCLLibrary.ncclRecv(recvbuff: buffer_type, count: int, datatype: int, src: int, comm: ncclComm_t, stream: cudaStream_t) -> None
  NCCLLibrary.ncclBroadcast(sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, root: int, comm: ncclComm_t, stream: cudaStream_t) -> None
  NCCLLibrary.ncclCommDestroy(comm: ncclComm_t) -> None
  NCCLLibrary.ncclCommWindowRegister(comm: ncclComm_t, buff: buffer_type, size: int, win_flags: int) -> ncclWindow_t
  NCCLLibrary.ncclCommWindowDeregister(comm: ncclComm_t, window: ncclWindow_t) -> None
  NCCLLibrary.ncclGroupStart() -> None
  NCCLLibrary.ncclGroupEnd() -> None

# python/sglang/srt/distributed/device_communicators/quick_all_reduce.py
qr_rocm_arch_available()
  QuickAllReduce.__init__(group: ProcessGroup, device: Union[int, str, torch.device]) -> None
  QuickAllReduce.init_quick_all_reduce()
  QuickAllReduce.create_shared_buffer()
  QuickAllReduce.should_quick_allreduce(inp: torch.Tensor)
  QuickAllReduce.quick_all_reduce(inp: torch.Tensor)
  QuickAllReduce.close()
  QuickAllReduce.__del__()

# python/sglang/srt/distributed/device_communicators/shm_broadcast.py
  ShmRingBuffer.__init__(n_reader: int, max_chunk_bytes: int, max_chunks: int, name: Optional[str])
  ShmRingBuffer.__reduce__()
  ShmRingBuffer.__del__()
  ShmRingBuffer.get_data(current_idx: int)
  ShmRingBuffer.get_metadata(current_idx: int)
  MessageQueue.__init__(n_reader, n_local_reader, local_reader_ranks: Optional[List[int]], max_chunk_bytes: int, max_chunks: int, connect_ip: Optional[str])
  MessageQueue.export_handle() -> Handle
  MessageQueue.create_from_handle(handle: Handle, rank) -> 'MessageQueue'
  MessageQueue.wait_until_ready()
  MessageQueue.acquire_write()
  MessageQueue.acquire_read()
  MessageQueue.enqueue(obj)
  MessageQueue.dequeue()
  MessageQueue.broadcast_object(obj)
  MessageQueue.create_from_process_group(pg: ProcessGroup, max_chunk_bytes, max_chunks, writer_rank) -> 'MessageQueue'

# python/sglang/srt/distributed/device_communicators/xpu_communicator.py
  XpuCommunicator.__init__(group: ProcessGroup)
  XpuCommunicator.all_reduce(x: torch.Tensor) -> torch.Tensor
  XpuCommunicator.gather(input_: torch.Tensor, rank_in_group: int, dst: int, dim: int)

# python/sglang/srt/distributed/naive_distributed.py
  NaiveDistributed.__init__(rank: int, world_size: int, rendezvous: str)
  NaiveDistributed.get_rank()
  NaiveDistributed.get_world_size()
  NaiveDistributed.scatter(tensor: torch.Tensor, scatter_list: List[torch.Tensor], src: int)
  NaiveDistributed.all_gather_object(obj: Any) -> List[Any]
  NaiveDistributed.barrier()
get_naive_distributed()
set_naive_distributed(instance: NaiveDistributed)

# python/sglang/srt/distributed/parallel_state.py
inplace_all_reduce(tensor: torch.Tensor, group_name: str) -> None
inplace_all_reduce_fake(tensor: torch.Tensor, group_name: str) -> None
outplace_all_reduce(tensor: torch.Tensor, group_name: str, outplace_all_reduce_method: str) -> torch.Tensor
outplace_all_reduce_fake(tensor: torch.Tensor, group_name: str, outplace_all_reduce_method: str) -> torch.Tensor
reg_all_gather_into_tensor(output: torch.Tensor, input: torch.Tensor, group_name: str) -> None
reg_all_gather_into_tensor_fake(output: torch.Tensor, input: torch.Tensor, group_name: str) -> None
  GroupCoordinator.__init__(group_ranks: List[List[int]], local_rank: int, torch_distributed_backend: Union[str, Backend], use_pynccl: bool, use_pymscclpp: bool, use_custom_allreduce: bool, use_hpu_communicator: bool, use_xpu_communicator: bool, use_npu_communicator: bool, use_message_queue_broadcaster: bool, group_name: Optional[str])
  GroupCoordinator.__repr__()
  GroupCoordinator.first_rank()
  GroupCoordinator.last_rank()
  GroupCoordinator.is_first_rank()
  GroupCoordinator.is_last_rank()
  GroupCoordinator.next_rank()
  GroupCoordinator.prev_rank()
  GroupCoordinator.graph_capture(graph_capture_context: Optional[GraphCaptureContext])
  GroupCoordinator.all_reduce(input_: torch.Tensor) -> torch.Tensor
  GroupCoordinator.reduce_scatter_tensor(output: torch.Tensor, input: torch.Tensor) -> None
  GroupCoordinator.reduce_scatter(output: torch.Tensor, input_list: List[torch.Tensor]) -> None
  GroupCoordinator.reduce_scatterv(input_: torch.Tensor, output: Optional[torch.Tensor], sizes: Optional[List[int]]) -> torch.Tensor
  GroupCoordinator.all_gather_into_tensor(output: torch.Tensor, input: torch.Tensor)
  GroupCoordinator.all_gather(input_: torch.Tensor, dim: int, output_tensor_list: Optional[List[torch.Tensor]]) -> torch.Tensor
  GroupCoordinator.all_gatherv(input_: Union[torch.Tensor, List[torch.Tensor]], sizes: Optional[List[int]]) -> Union[torch.Tensor, List[torch.Tensor]]
  GroupCoordinator.gather(input_: torch.Tensor, dst: int, dim: int) -> Optional[torch.Tensor]
  GroupCoordinator.broadcast(input_: torch.Tensor, src: int)
  GroupCoordinator.broadcast_object(obj: Optional[Any], src: int)
  GroupCoordinator.broadcast_object_list(obj_list: List[Any], src: int, group: Optional[ProcessGroup])
  GroupCoordinator.all_gather_object(obj: Any) -> List[Any]
  GroupCoordinator.send_object(obj: Any, dst: int) -> None
  GroupCoordinator.recv_object(src: int) -> Any
  GroupCoordinator.broadcast_tensor_dict(tensor_dict: Optional[Dict[str, Union[torch.Tensor, Any]]], src: int, group: Optional[ProcessGroup], metadata_group: Optional[ProcessGroup]) -> Optional[Dict[str, Union[torch.Tensor, Any]]]
  GroupCoordinator.send_tensor_dict(tensor_dict: Dict[str, Union[torch.Tensor, Any]], dst: Optional[int], all_gather_group: Optional['GroupCoordinator']) -> Optional[Dict[str, Union[torch.Tensor, Any]]]
  GroupCoordinator.recv_tensor_dict(src: Optional[int], all_gather_group: Optional['GroupCoordinator']) -> Optional[Dict[str, Union[torch.Tensor, Any]]]
  GroupCoordinator.barrier()
  GroupCoordinator.send(tensor: torch.Tensor, dst: Optional[int]) -> None
  GroupCoordinator.recv(size: torch.Size, dtype: torch.dtype, src: Optional[int]) -> torch.Tensor
  GroupCoordinator.destroy()
get_world_group() -> GroupCoordinator
init_world_group(ranks: List[int], local_rank: int, backend: str) -> GroupCoordinator
init_model_parallel_group(group_ranks: List[List[int]], local_rank: int, backend: str, use_custom_allreduce: Optional[bool], use_message_queue_broadcaster: bool, group_name: Optional[str], use_mscclpp_allreduce: Optional[bool]) -> GroupCoordinator
set_pdmux_status(enable_prefill_multiplexing: bool)
get_tp_group() -> GroupCoordinator
get_moe_ep_group() -> GroupCoordinator
get_moe_tp_group() -> GroupCoordinator
get_pp_group() -> GroupCoordinator
graph_capture()
set_custom_all_reduce(enable: bool)
set_mscclpp_all_reduce(enable: bool)
init_distributed_environment(world_size: int, rank: int, distributed_init_method: str, local_rank: int, backend: str, timeout: Optional[int])
initialize_model_parallel(tensor_model_parallel_size: int, expert_model_parallel_size: int, pipeline_model_parallel_size: int, backend: Optional[str], duplicate_tp_group: bool) -> None
ensure_model_parallel_initialized(tensor_model_parallel_size: int, expert_model_parallel_size: int, pipeline_model_parallel_size: int, backend: Optional[str]) -> None
model_parallel_is_initialized()
patch_tensor_parallel_group(tp_group: GroupCoordinator)
get_tensor_model_parallel_world_size()
get_tensor_model_parallel_rank()
get_moe_expert_parallel_world_size()
get_moe_expert_parallel_rank()
get_moe_tensor_parallel_world_size()
get_moe_tensor_parallel_rank()
destroy_model_parallel()
destroy_distributed_environment()
cleanup_dist_env_and_memory(shutdown_ray: bool)
in_the_same_node_as(pg: ProcessGroup, source_rank: int) -> List[bool]
monkey_patch_vllm_parallel_state(reverse: bool)

# python/sglang/srt/distributed/utils.py
ensure_divisibility(numerator, denominator)
divide(numerator, denominator)
split_tensor_along_last_dim(tensor: torch.Tensor, num_partitions: int, contiguous_split_chunks: bool) -> Sequence[torch.Tensor]
get_pp_indices(num_hidden_layers: int, pp_rank: int, pp_size: int) -> Tuple[int, int]
  StatelessProcessGroup.__post_init__()
  StatelessProcessGroup.send_obj(obj: Any, dst: int)
  StatelessProcessGroup.expire_data()
  StatelessProcessGroup.recv_obj(src: int) -> Any
  StatelessProcessGroup.broadcast_obj(obj: Optional[Any], src: int) -> Any
  StatelessProcessGroup.all_gather_obj(obj: Any) -> list[Any]
  StatelessProcessGroup.barrier()
  StatelessProcessGroup.create(host: str, port: int, rank: int, world_size: int, data_expiration_seconds: int) -> 'StatelessProcessGroup'

# python/sglang/srt/entrypoints/EngineBase.py
  EngineBase.generate(prompt: Optional[Union[List[str], str]], sampling_params: Optional[Union[List[Dict], Dict]], input_ids: Optional[Union[List[List[int]], List[int]]], image_data: Optional[Union[List[str], str]], return_logprob: Optional[Union[List[bool], bool]], logprob_start_len: Optional[Union[List[int], int]], top_logprobs_num: Optional[Union[List[int], int]], token_ids_logprob: Optional[Union[List[List[int]], List[int]]], lora_path: Optional[Union[List[Optional[str]], Optional[str]]], custom_logit_processor: Optional[Union[List[str], str]], return_hidden_states: Optional[bool], stream: Optional[bool], bootstrap_host: Optional[Union[List[str], str]], bootstrap_port: Optional[Union[List[int], int]], bootstrap_room: Optional[Union[List[int], int]], data_parallel_rank: Optional[int]) -> Union[Dict, Iterator[Dict]]
  EngineBase.flush_cache()
  EngineBase.update_weights_from_tensor(named_tensors: List[Tuple[str, torch.Tensor]], load_format: Optional[str], flush_cache: bool)
  EngineBase.load_lora_adapter(lora_name: str, lora_path: str)
  EngineBase.unload_lora_adapter(lora_name: str)
  EngineBase.release_memory_occupation()
  EngineBase.resume_memory_occupation()
  EngineBase.shutdown()

# python/sglang/srt/entrypoints/context.py
  ConversationContext.append_output(output) -> None
  ConversationContext.call_tool() -> list[Message]
  ConversationContext.need_builtin_tool_call() -> bool
  ConversationContext.render_for_completion() -> list[int]
  SimpleContext.__init__()
  SimpleContext.append_output(output) -> None
  SimpleContext.need_builtin_tool_call() -> bool
  SimpleContext.call_tool() -> list[Message]
  SimpleContext.render_for_completion() -> list[int]
  HarmonyContext.__init__(messages: list, tool_sessions: dict[str, Union['ClientSession', Tool]])
  HarmonyContext.append_output(output) -> None
  HarmonyContext.messages() -> list
  HarmonyContext.need_builtin_tool_call() -> bool
  HarmonyContext.call_tool() -> list[Message]
  HarmonyContext.render_for_completion() -> list[int]
  HarmonyContext.call_search_tool(tool_session: Union['ClientSession', Tool], last_msg: Message) -> list[Message]
  HarmonyContext.call_python_tool(tool_session: Union['ClientSession', Tool], last_msg: Message) -> list[Message]
  StreamingHarmonyContext.__init__()
  StreamingHarmonyContext.messages() -> list
  StreamingHarmonyContext.append_output(output) -> None
  StreamingHarmonyContext.is_expecting_start() -> bool
  StreamingHarmonyContext.is_assistant_action_turn() -> bool
  StreamingHarmonyContext.render_for_completion() -> list[int]

# python/sglang/srt/entrypoints/engine.py
  Engine.__init__()
  Engine.generate(prompt: Optional[Union[List[str], str]], sampling_params: Optional[Union[List[Dict], Dict]], input_ids: Optional[Union[List[List[int]], List[int]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat], return_logprob: Optional[Union[List[bool], bool]], logprob_start_len: Optional[Union[List[int], int]], top_logprobs_num: Optional[Union[List[int], int]], token_ids_logprob: Optional[Union[List[List[int]], List[int]]], lora_path: Optional[List[Optional[str]]], custom_logit_processor: Optional[Union[List[str], str]], return_hidden_states: bool, stream: bool, bootstrap_host: Optional[Union[List[str], str]], bootstrap_port: Optional[Union[List[int], int]], bootstrap_room: Optional[Union[List[int], int]], data_parallel_rank: Optional[int]) -> Union[Dict, Iterator[Dict]]
  Engine.async_generate(prompt: Optional[Union[List[str], str]], sampling_params: Optional[Union[List[Dict], Dict]], input_ids: Optional[Union[List[List[int]], List[int]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat], return_logprob: Optional[Union[List[bool], bool]], logprob_start_len: Optional[Union[List[int], int]], top_logprobs_num: Optional[Union[List[int], int]], token_ids_logprob: Optional[Union[List[List[int]], List[int]]], lora_path: Optional[List[Optional[str]]], custom_logit_processor: Optional[Union[List[str], str]], return_hidden_states: bool, stream: bool, bootstrap_host: Optional[Union[List[str], str]], bootstrap_port: Optional[Union[List[int], int]], bootstrap_room: Optional[Union[List[int], int]], data_parallel_rank: Optional[int]) -> Union[Dict, AsyncIterator[Dict]]
  Engine.encode(prompt: Union[str, List[str], List[Dict], List[List[Dict]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat]) -> Dict
  Engine.async_encode(prompt: Union[str, List[str], List[Dict], List[List[Dict]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat]) -> Dict
  Engine.rerank(prompt: Union[List[List[str]]]) -> Dict
  Engine.shutdown()
  Engine.__enter__()
  Engine.__exit__(exc_type, exc_value, traceback)
  Engine.flush_cache()
  Engine.start_profile()
  Engine.stop_profile()
  Engine.start_expert_distribution_record()
  Engine.stop_expert_distribution_record()
  Engine.dump_expert_distribution_record()
  Engine.get_server_info()
  Engine.init_weights_update_group(master_address: str, master_port: int, rank_offset: int, world_size: int, group_name: str, backend: str)
  Engine.update_weights_from_distributed(names: list[str], dtypes: list[str], shapes: list[list[int]], group_name: str, flush_cache: bool)
  Engine.update_weights_from_tensor(named_tensors: List[Tuple[str, torch.Tensor]], load_format: Optional[str], flush_cache: bool)
  Engine.update_weights_from_disk(model_path: str, load_format: Optional[str])
  Engine.get_weights_by_name(name: str, truncate_size: int)
  Engine.load_lora_adapter(lora_name: str, lora_path: str, pinned: bool)
  Engine.unload_lora_adapter(lora_name: str)
  Engine.release_memory_occupation(tags: Optional[List[str]])
  Engine.resume_memory_occupation(tags: Optional[List[str]])
  Engine.freeze_gc()
  Engine.collective_rpc(method: str)
  Engine.save_remote_model()
  Engine.save_sharded_model()
  Engine.score(query: Optional[Union[str, List[int]]], items: Optional[Union[str, List[str], List[List[int]]]], label_token_ids: Optional[List[int]], apply_softmax: bool, item_first: bool) -> List[List[float]]
  Engine.async_score(query: Optional[Union[str, List[int]]], items: Optional[Union[str, List[str], List[List[int]]]], label_token_ids: Optional[List[int]], apply_softmax: bool, item_first: bool) -> List[List[float]]

# python/sglang/srt/entrypoints/harmony_utils.py
get_encoding()
get_system_message(model_identity: Optional[str], reasoning_effort: Optional[Literal['high', 'medium', 'low']], start_date: Optional[str], browser_description: Optional[str], python_description: Optional[str]) -> Message
get_developer_message(instructions: Optional[str], tools: Optional[list[Tool]]) -> Message
get_user_message(content: str) -> Message
parse_response_input(response_msg: ResponseInputOutputItem, prev_responses: list[Union[ResponseOutputItem, ResponseReasoningItem]]) -> Message
parse_response_output(output: ResponseOutputItem) -> Message
parse_chat_input(chat_msg) -> Message
render_for_completion(messages: list[Message]) -> list[int]
get_stop_tokens_for_assistant_actions() -> list[int]
get_streamable_parser_for_assistant() -> StreamableParser
parse_output_message(message: Message)
parse_remaining_state(parser: StreamableParser)
parse_output_into_messages(token_ids: Iterable[int])

# python/sglang/srt/entrypoints/http_server.py
set_global_state(global_state: _GlobalState)
lifespan(fast_api_app: FastAPI)
validation_exception_handler(request: Request, exc: HTTPException)
validation_exception_handler(request: Request, exc: RequestValidationError)
validate_json_request(raw_request: Request)
health_generate(request: Request) -> Response
get_model_info()
get_weight_version()
get_server_info()
get_load()
set_internal_state(obj: SetInternalStateReq, request: Request)
generate_request(obj: GenerateReqInput, request: Request)
generate_from_file_request(file: UploadFile, request: Request)
encode_request(obj: EmbeddingReqInput, request: Request)
classify_request(obj: EmbeddingReqInput, request: Request)
flush_cache()
clear_hicache_storage_backend()
start_profile_async(obj: Optional[ProfileReqInput])
stop_profile_async()
freeze_gc_async()
start_expert_distribution_record_async()
stop_expert_distribution_record_async()
dump_expert_distribution_record_async()
update_weights_from_disk(obj: UpdateWeightFromDiskReqInput, request: Request)
init_weights_update_group(obj: InitWeightsUpdateGroupReqInput, request: Request)
update_weights_from_tensor(obj: UpdateWeightsFromTensorReqInput, request: Request)
update_weights_from_distributed(obj: UpdateWeightsFromDistributedReqInput, request: Request)
update_weight_version(obj: UpdateWeightVersionReqInput, request: Request)
get_weights_by_name(obj: GetWeightsByNameReqInput, request: Request)
release_memory_occupation(obj: ReleaseMemoryOccupationReqInput, request: Request)
resume_memory_occupation(obj: ResumeMemoryOccupationReqInput, request: Request)
slow_down(obj: SlowDownReqInput, request: Request)
load_lora_adapter(obj: LoadLoRAAdapterReqInput, request: Request)
unload_lora_adapter(obj: UnloadLoRAAdapterReqInput, request: Request)
open_session(obj: OpenSessionReqInput, request: Request)
close_session(obj: CloseSessionReqInput, request: Request)
configure_logging(obj: ConfigureLoggingReq, request: Request)
abort_request(obj: AbortReq, request: Request)
parse_function_call_request(obj: ParseFunctionCallReq, request: Request)
separate_reasoning_request(obj: SeparateReasoningReqInput, request: Request)
pause_generation(request: Request)
continue_generation(request: Request)
openai_v1_completions(request: CompletionRequest, raw_request: Request)
openai_v1_chat_completions(request: ChatCompletionRequest, raw_request: Request)
openai_v1_embeddings(request: EmbeddingRequest, raw_request: Request)
available_models()
retrieve_model(model: str)
v1_score_request(request: ScoringRequest, raw_request: Request)
v1_responses_request(request: dict, raw_request: Request)
v1_retrieve_responses(response_id: str, raw_request: Request)
v1_cancel_responses(response_id: str, raw_request: Request)
v1_rerank_request(request: V1RerankReqInput, raw_request: Request)
sagemaker_health() -> Response
sagemaker_chat_completions(request: ChatCompletionRequest, raw_request: Request)
vertex_generate(vertex_req: VertexGenerateReqInput, raw_request: Request)
launch_server(server_args: ServerArgs, pipe_finish_writer: Optional[multiprocessing.connection.Connection], launch_callback: Optional[Callable[[], None]])

# python/sglang/srt/entrypoints/http_server_engine.py
launch_server_process(server_args: ServerArgs) -> multiprocessing.Process
  HttpServerEngineAdapter.__init__()
  HttpServerEngineAdapter.update_weights_from_tensor(named_tensors: List[Tuple[str, torch.Tensor]], load_format: Optional[str], flush_cache: bool)
  HttpServerEngineAdapter.shutdown()
  HttpServerEngineAdapter.generate(prompt, sampling_params, input_ids, image_data, return_logprob, logprob_start_len, top_logprobs_num, token_ids_logprob, lora_path, custom_logit_processor)
  HttpServerEngineAdapter.release_memory_occupation()
  HttpServerEngineAdapter.resume_memory_occupation()
  HttpServerEngineAdapter.flush_cache()

# python/sglang/srt/entrypoints/openai/protocol.py
  CompletionRequest.validate_max_tokens_positive(cls, v)
  ChatCompletionRequest.set_tool_choice_default(cls, values)
  ChatCompletionRequest.normalize_reasoning_inputs(cls, values: Dict)
  ChatCompletionRequest.set_json_schema(cls, values)
  ResponsesRequest.to_sampling_params(default_max_tokens: int, default_params: Optional[Dict]) -> Dict[str, Any]
  ResponsesResponse.from_request(cls, request: ResponsesRequest, sampling_params: Any, model_name: str, created_time: int, output: List[Union[ResponseOutputItem, ResponseReasoningItem, ResponseFunctionToolCall]], status: str, usage: Optional[UsageInfo]) -> 'ResponsesResponse'

# python/sglang/srt/entrypoints/openai/serving_base.py
  OpenAIServingBase.__init__(tokenizer_manager: TokenizerManager)
  OpenAIServingBase.handle_request(request: OpenAIServingRequest, raw_request: Request) -> Union[Any, StreamingResponse, ErrorResponse]
  OpenAIServingBase.create_error_response(message: str, err_type: str, status_code: int, param: Optional[str]) -> ORJSONResponse
  OpenAIServingBase.create_streaming_error_response(message: str, err_type: str, status_code: int) -> str

# python/sglang/srt/entrypoints/openai/serving_chat.py
  OpenAIServingChat.__init__(tokenizer_manager: TokenizerManager, template_manager: TemplateManager)

# python/sglang/srt/entrypoints/openai/serving_completions.py
  OpenAIServingCompletion.__init__(tokenizer_manager: TokenizerManager, template_manager: TemplateManager)

# python/sglang/srt/entrypoints/openai/serving_embedding.py
  OpenAIServingEmbedding.__init__(tokenizer_manager: TokenizerManager, template_manager: TemplateManager)

# python/sglang/srt/entrypoints/openai/serving_responses.py
  OpenAIServingResponses.__init__(tokenizer_manager: TokenizerManager, template_manager: TemplateManager) -> None
  OpenAIServingResponses.create_responses(request: ResponsesRequest, raw_request: Optional[Request]) -> Union[AsyncGenerator[str, None], ResponsesResponse, ORJSONResponse]
  OpenAIServingResponses.responses_full_generator(request: ResponsesRequest, sampling_params: Any, result_generator: AsyncIterator[Any], context: ConversationContext, model_name: str, tokenizer: Any, request_metadata: RequestResponseMetadata, created_time: Optional[int]) -> Union[ResponsesResponse, ORJSONResponse]
  OpenAIServingResponses.retrieve_responses(response_id: str) -> Union[ResponsesResponse, ORJSONResponse]
  OpenAIServingResponses.cancel_responses(response_id: str) -> Union[ResponsesResponse, ORJSONResponse]
  OpenAIServingResponses.responses_stream_generator(request: ResponsesRequest, sampling_params: Any, result_generator: AsyncIterator[StreamingHarmonyContext], context: StreamingHarmonyContext, model_name: str, tokenizer: Any, request_metadata: RequestResponseMetadata, created_time: Optional[int]) -> AsyncGenerator[str, None]

# python/sglang/srt/entrypoints/openai/tool_server.py
list_server_and_tools(server_url: str)
trim_schema(schema: dict) -> dict
post_process_tools_description(list_tools_result: 'ListToolsResult') -> 'ListToolsResult'
  ToolServer.has_tool(tool_name: str)
  ToolServer.get_tool_description(tool_name: str)
  ToolServer.get_tool_session(tool_name: str) -> AbstractAsyncContextManager[Any]
  MCPToolServer.__init__()
  MCPToolServer.add_tool_server(server_url: str)
  MCPToolServer.has_tool(tool_name: str)
  MCPToolServer.get_tool_description(tool_name: str)
  MCPToolServer.get_tool_session(tool_name: str)
  DemoToolServer.__init__()
  DemoToolServer.has_tool(tool_name: str)
  DemoToolServer.get_tool_description(tool_name: str)
  DemoToolServer.get_tool_session(tool_name: str)

# python/sglang/srt/entrypoints/openai/usage_processor.py
  UsageProcessor.calculate_response_usage(responses: List[Dict[str, Any]], n_choices: int, enable_cache_report: bool) -> UsageInfo
  UsageProcessor.calculate_streaming_usage(prompt_tokens: Mapping[int, int], completion_tokens: Mapping[int, int], cached_tokens: Mapping[int, int], n_choices: int, enable_cache_report: bool) -> UsageInfo
  UsageProcessor.calculate_token_usage(prompt_tokens: int, completion_tokens: int, cached_tokens: Optional[Dict[str, int]]) -> UsageInfo

# python/sglang/srt/entrypoints/openai/utils.py
to_openai_style_logprobs(input_token_logprobs, output_token_logprobs, input_top_logprobs, output_top_logprobs)
process_hidden_states_from_ret(ret_item: Dict[str, Any], request: Union[ChatCompletionRequest, CompletionRequest]) -> Optional[List]

# python/sglang/srt/entrypoints/tool.py
  Tool.get_result(context: 'ConversationContext') -> Any
  HarmonyBrowserTool.__init__()
  HarmonyBrowserTool.get_result(context: 'ConversationContext') -> Any
  HarmonyBrowserTool.tool_config() -> Any
  HarmonyPythonTool.__init__()
  HarmonyPythonTool.get_result(context: 'ConversationContext') -> Any
  HarmonyPythonTool.tool_config() -> Any

# python/sglang/srt/eplb/eplb_algorithms/__init__.py
rebalance_experts(tokens_per_expert: torch.Tensor, num_physical_experts: int, num_local_physical_experts: int, num_groups: Optional[int], num_nodes: int, algorithm: EplbAlgorithm)
compute_algorithm(raw_algorithm: str, num_groups: Optional[int], num_nodes: int) -> EplbAlgorithm

# python/sglang/srt/eplb/eplb_algorithms/deepseek.py
balanced_packing(weight: torch.Tensor, num_packs: int) -> Tuple[torch.Tensor, torch.Tensor]
replicate_experts(weight: torch.Tensor, num_phy: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
rebalance_experts_hierarchical(weight: torch.Tensor, num_physical_experts: int, num_groups: int, num_nodes: int, num_gpus: int)
rebalance_experts(weight: torch.Tensor, num_replicas: int, num_groups: int, num_nodes: int, num_gpus: int, enable_hierarchical: bool) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]

# python/sglang/srt/eplb/eplb_algorithms/deepseek_vec.py
pack_groups(tokens_per_group: torch.Tensor, num_nodes: int) -> torch.Tensor
make_redundant_experts_chunkwise(tokens_per_expert: torch.Tensor, num_physical_experts: int, num_local_physical_experts: int, num_physical_experts_per_chunk: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
decode_rebalance_experts(tokens_per_expert: torch.Tensor, num_physical_experts: int, num_local_physical_experts: int)
prefill_rebalance_experts(tokens_per_expert: torch.Tensor, num_physical_experts: int, num_local_physical_experts: int, num_groups: int, num_nodes: int)
rebalance_experts(tokens_per_expert: torch.Tensor, num_physical_experts: int, num_local_physical_experts: int, num_groups: Optional[int], num_nodes: int, enable_hierarchical: bool)

# python/sglang/srt/eplb/eplb_manager.py
  EPLBManager.__init__(model_runner: 'ModelRunner')
  EPLBManager.on_forward_pass_end()
  EPLBManager.rebalance()

# python/sglang/srt/eplb/eplb_simulator/reader.py
read_mode_per_pass(dir_data: Path)

# python/sglang/srt/eplb/expert_distribution.py
  ExpertDistributionRecorder.init_new(server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)
  ExpertDistributionRecorder.with_current_layer(layer_idx)
  ExpertDistributionRecorder.with_debug_name(debug_name)
  ExpertDistributionRecorder.disable_this_region()
  ExpertDistributionRecorder.with_forward_pass(forward_pass_id: int, forward_batch: ForwardBatch)
  ExpertDistributionRecorder.on_select_experts(topk_ids: torch.Tensor)
  ExpertDistributionRecorder.on_deepep_dispatch_normal(local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)
  ExpertDistributionRecorder.on_deepep_dispatch_low_latency(local_physical_count_of_layer: torch.Tensor)
  ExpertDistributionRecorder.start_record()
  ExpertDistributionRecorder.stop_record()
  ExpertDistributionRecorder.dump_record(output_mode: _OutputMode)
  ExpertDistributionRecorder.recording()
  _ExpertDistributionRecorderReal.__init__(server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)
  _ExpertDistributionRecorderReal.with_current_layer(layer_idx)
  _ExpertDistributionRecorderReal.with_debug_name(debug_name)
  _ExpertDistributionRecorderReal.with_forward_pass(forward_pass_id: int, forward_batch: ForwardBatch)
  _ExpertDistributionRecorderReal.disable_this_region()
  _ExpertDistributionRecorderReal.on_select_experts(topk_ids: torch.Tensor)
  _ExpertDistributionRecorderReal.on_deepep_dispatch_normal(local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)
  _ExpertDistributionRecorderReal.on_deepep_dispatch_low_latency(local_physical_count_of_layer: torch.Tensor)
  _ExpertDistributionRecorderReal.start_record()
  _ExpertDistributionRecorderReal.stop_record()
  _ExpertDistributionRecorderReal.dump_record(output_mode: _OutputMode)
  _ExpertDistributionRecorderReal.recording()
get_global_expert_distribution_recorder()
set_global_expert_distribution_recorder(value)
  _SinglePassGatherer.init_new(server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int) -> '_SinglePassGatherer'
  _SinglePassGatherer.__init__(expert_location_metadata: 'ExpertLocationMetadata', rank: int)
  _SinglePassGatherer.on_forward_pass_start(forward_batch: ForwardBatch)
  _SinglePassGatherer.on_select_experts(layer_idx: int, topk_ids: torch.Tensor)
  _SinglePassGatherer.on_deepep_dispatch_normal(layer_idx: int, local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)
  _SinglePassGatherer.on_deepep_dispatch_low_latency(layer_idx: int, local_physical_count_of_layer: torch.Tensor)
  _SinglePassGatherer.reset()
  _SinglePassGatherer.collect() -> Dict
  _DetailSinglePassGatherer.__init__(server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)
  _DetailSinglePassGatherer.on_forward_pass_start(forward_batch: ForwardBatch)
  _DetailSinglePassGatherer.on_select_experts(layer_idx: int, topk_ids: torch.Tensor)
  _DetailSinglePassGatherer.on_deepep_dispatch_normal(layer_idx: int, local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)
  _DetailSinglePassGatherer.reset()
  _DetailSinglePassGatherer.collect() -> Dict
  _LayerBasedCpuSinglePassGatherer.__init__()
  _LayerBasedCpuSinglePassGatherer.reset()
  _LayerBasedGpuSinglePassGatherer.__init__()
  _LayerBasedGpuSinglePassGatherer.reset()
  _LayerBasedGpuSinglePassGatherer.collect() -> Dict
  _SelectExpertsSinglePassGatherer.__init__()
  _SelectExpertsSinglePassGatherer.on_select_experts(layer_idx: int, topk_ids: torch.Tensor)
  _DeepepNormalSinglePassGatherer.__init__()
  _DeepepNormalSinglePassGatherer.on_deepep_dispatch_normal(layer_idx: int, local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)
  _DeepepNormalSinglePassGatherer.collect() -> Dict
  _DeepepLowLatencySinglePassGatherer.__init__()
  _DeepepLowLatencySinglePassGatherer.on_deepep_dispatch_low_latency(layer_idx: int, local_physical_count_of_layer: torch.Tensor)
  _Accumulator.init_new(server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int) -> '_Accumulator'
  _Accumulator.get_class(server_args: ServerArgs) -> Type['_Accumulator']
  _Accumulator.__init__(server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)
  _Accumulator.get_single_pass_gatherer_keys()
  _Accumulator.get_single_pass_gatherer_key(debug_name: Optional[str])
  _Accumulator.append(forward_pass_id: int, gatherer_key: str, single_pass_data: Dict)
  _Accumulator.reset()
  _Accumulator.dump(output_mode: _OutputMode)
  _UtilizationRateAccumulatorMixin.__init__()
  _UtilizationRateAccumulatorMixin.append(forward_pass_id: int, gatherer_key: str, single_pass_data: Dict)
  _UtilizationRateAccumulatorMixin.reset()
  _DequeCollection.__init__(maxlens: List[int])
  _DequeCollection.append(value)
  _DequeCollection.clear()
  _DequeCollection.mean() -> Dict[int, float]
  _DetailAccumulator.__init__()
  _DetailAccumulator.get_single_pass_gatherer_keys()
  _DetailAccumulator.get_single_pass_gatherer_key(debug_name: Optional[str])
  _DetailAccumulator.append(forward_pass_id: int, gatherer_key: str, single_pass_data: Dict)
  _DetailAccumulator.reset()
  _DetailAccumulator.dump(output_mode: _OutputMode)
  _StatAccumulator.__init__()
  _StatAccumulator.append(forward_pass_id: int, gatherer_key: str, single_pass_data: Dict)
  _StatAccumulator.reset()
  _StatAccumulator.dump(output_mode: _OutputMode)
  _Buffer.init_new(item_shape: Tuple, buffer_size: int, dtype, device)
  _Buffer.append(value: torch.Tensor)
  _Buffer.get_all() -> torch.Tensor
  _Buffer.reset()
  _CircularBuffer.__init__(item_shape: Tuple, buffer_size: int, dtype, device)
  _CircularBuffer.append(value: torch.Tensor)
  _CircularBuffer.get_all() -> torch.Tensor
  _CircularBuffer.reset()
  _InfiniteBuffer.__init__(item_shape: Tuple, dtype, device)
  _InfiniteBuffer.append(value: torch.Tensor)
  _InfiniteBuffer.get_all() -> torch.Tensor
  _InfiniteBuffer.reset()
compute_gpu_physical_count(physical_count_of_whatever: torch.Tensor, num_gpu: int)
compute_utilization_rate(gpu_physical_count_of_batch: torch.Tensor)

# python/sglang/srt/eplb/expert_location.py
  ExpertLocationMetadata.num_layers() -> int
  ExpertLocationMetadata.num_physical_experts() -> int
  ExpertLocationMetadata.num_local_physical_experts() -> int
  ExpertLocationMetadata.num_logical_experts() -> int
  ExpertLocationMetadata.ep_size()
  ExpertLocationMetadata.__post_init__()
  ExpertLocationMetadata.init_trivial(server_args: ServerArgs, model_config: ModelConfig)
  ExpertLocationMetadata.init_by_mapping(server_args: ServerArgs, model_config: ModelConfig, physical_to_logical_map)
  ExpertLocationMetadata.init_by_eplb(server_args: ServerArgs, model_config: ModelConfig, logical_count: torch.Tensor)
  ExpertLocationMetadata.update(other: 'ExpertLocationMetadata', update_layer_ids: List[int])
  ExpertLocationMetadata.logical_to_all_physical(layer_id: int, logical_expert_id: int) -> List[int]
get_global_expert_location_metadata()
set_global_expert_location_metadata(value)
compute_logical_to_rank_dispatch_physical_map(logical_to_all_physical_map: torch.Tensor, num_gpus: int, num_physical_experts: int, ep_rank: int, seed: int)
  ModelConfigForExpertLocation.from_model_config(model_config: ModelConfig)
compute_initial_expert_location_metadata(server_args: ServerArgs, model_config: ModelConfig) -> Optional[ExpertLocationMetadata]

# python/sglang/srt/eplb/expert_location_dispatch.py
  ExpertLocationDispatchInfo.init_new(cls, layer_id: int)
transform_select_experts_inputs(router_logits: torch.Tensor, correction_bias: Optional[torch.Tensor], info: Optional[ExpertLocationDispatchInfo])
topk_ids_logical_to_physical(topk_ids: torch.Tensor, info: Optional[ExpertLocationDispatchInfo]) -> torch.Tensor

# python/sglang/srt/eplb/expert_location_updater.py
  ExpertLocationUpdater.__init__()
  ExpertLocationUpdater.update(routed_experts_weights_of_layer: Dict[int, List[torch.Tensor]], new_expert_location_metadata: ExpertLocationMetadata, update_layer_ids: List[int], nnodes: int, rank: int)
create_temp_buffers(sample_tensors)
update_expert_weights_single_layer(routed_experts_weights: List[torch.Tensor], temp_buffers: List[torch.Tensor], old_physical_to_logical_map: List[int], new_physical_to_logical_map: List[int], num_local_physical_experts: int, num_gpu_per_node: int, rank: int, world_size: Optional[int], debug: bool, log_metrics: bool)
  _ChunkUtils.__init__()
  _ChunkUtils.chunk_value_from_element_value(element_value)
  _ChunkUtils.element_values_from_chunk_value(chunk_value) -> List

# python/sglang/srt/function_call/base_format_detector.py
  BaseFormatDetector.__init__()
  BaseFormatDetector.parse_base_json(action: Any, tools: List[Tool]) -> List[ToolCallItem]
  BaseFormatDetector.detect_and_parse(text: str, tools: List[Tool]) -> StreamingParseResult
  BaseFormatDetector.parse_streaming_increment(new_text: str, tools: List[Tool]) -> StreamingParseResult
  BaseFormatDetector.has_tool_call(text: str) -> bool
  BaseFormatDetector.supports_structural_tag() -> bool
  BaseFormatDetector.structure_info() -> _GetInfoFunc
  BaseFormatDetector.build_ebnf(tools: List[Tool]) -> str

# python/sglang/srt/function_call/deepseekv31_detector.py
  DeepSeekV31Detector.__init__()
  DeepSeekV31Detector.has_tool_call(text: str) -> bool
  DeepSeekV31Detector.detect_and_parse(text: str, tools: List[Tool]) -> StreamingParseResult
  DeepSeekV31Detector.parse_streaming_increment(new_text: str, tools: List[Tool]) -> StreamingParseResult
  DeepSeekV31Detector.structure_info() -> _GetInfoFunc
  DeepSeekV31Detector.build_ebnf(tools: List[Tool])

# python/sglang/srt/function_call/deepseekv3_detector.py
  DeepSeekV3Detector.__init__()
  DeepSeekV3Detector.has_tool_call(text: str) -> bool
  DeepSeekV3Detector.detect_and_parse(text: str, tools: List[Tool]) -> StreamingParseResult
  DeepSeekV3Detector.parse_streaming_increment(new_text: str, tools: List[Tool]) -> StreamingParseResult
  DeepSeekV3Detector.structure_info() -> _GetInfoFunc
  DeepSeekV3Detector.build_ebnf(tools: List[Tool])

# python/sglang/srt/function_call/ebnf_composer.py
  EBNFComposer.get_value_rule(prop: dict, function_format: Literal['pythonic', 'json', 'xml']) -> str
  EBNFComposer.get_type_mapping(function_format: str) -> Dict[str, str]
  EBNFComposer.build_ebnf(tools, function_format: Literal['pythonic', 'json', 'xml'], sequence_start_token: Optional[str], sequence_end_token: Optional[str], individual_call_start_token: Optional[str], individual_call_end_token: Optional[str], tool_call_separator: Optional[str], call_rule_fmt: Optional[str], key_value_rule_fmt: Optional[str], key_value_separator: str)

# python/sglang/srt/function_call/function_call_parser.py
  FunctionCallParser.__init__(tools: List[Tool], tool_call_parser: str)
  FunctionCallParser.has_tool_call(text: str) -> bool
  FunctionCallParser.parse_non_stream(full_text: str) -> Tuple[str, list[ToolCallItem]]
  FunctionCallParser.parse_stream_chunk(chunk_text: str) -> Tuple[str, list[ToolCallItem]]
  FunctionCallParser.get_structure_tag() -> StructuralTagResponseFormat
  FunctionCallParser.get_structure_constraint(tool_choice: Union[ToolChoice, Literal['auto', 'required']]) -> Optional[Tuple[str, Any]]
  FunctionCallParser.get_ebnf(tool_choice: Union[ToolChoice, Literal['required']]) -> Optional[str]

# python/sglang/srt/function_call/glm4_moe_detector.py
get_argument_type(func_name: str, arg_key: str, defined_tools: list)
parse_arguments(json_value)
  Glm4MoeDetector.__init__()
  Glm4MoeDetector.has_tool_call(text: str) -> bool
  Glm4MoeDetector.detect_and_parse(text: str, tools: List[Tool]) -> StreamingParseResult
  Glm4MoeDetector.parse_streaming_increment(new_text: str, tools: List[Tool]) -> StreamingParseResult
  Glm4MoeDetector.supports_structural_tag() -> bool
  Glm4MoeDetector.structure_info() -> _GetInfoFunc
  Glm4MoeDetector.build_ebnf(tools: List[Tool])

# python/sglang/srt/function_call/gpt_oss_detector.py
  GptOssDetector.__init__()
  GptOssDetector.has_tool_call(text: str) -> bool
  GptOssDetector.detect_and_parse(text: str, tools: List[Tool]) -> StreamingParseResult
  GptOssDetector.parse_streaming_increment(new_text: str, tools: List[Tool]) -> StreamingParseResult
  GptOssDetector.structure_info() -> _GetInfoFunc
  GptOssDetector.build_ebnf(tools: List[Tool]) -> str

# python/sglang/srt/function_call/kimik2_detector.py
  KimiK2Detector.__init__()
  KimiK2Detector.has_tool_call(text: str) -> bool
  KimiK2Detector.detect_and_parse(text: str, tools: List[Tool]) -> StreamingParseResult
  KimiK2Detector.parse_streaming_increment(new_text: str, tools: List[Tool]) -> StreamingParseResult
  KimiK2Detector.structure_info() -> _GetInfoFunc
  KimiK2Detector.build_ebnf(tools: List[Tool]) -> str

# python/sglang/srt/function_call/llama32_detector.py
  Llama32Detector.__init__()
  Llama32Detector.has_tool_call(text: str) -> bool
  Llama32Detector.detect_and_parse(text: str, tools: List[Tool]) -> StreamingParseResult
  Llama32Detector.structure_info() -> _GetInfoFunc
  Llama32Detector.build_ebnf(tools: List[Tool])

# python/sglang/srt/function_call/mistral_detector.py
  MistralDetector.__init__()
  MistralDetector.has_tool_call(text: str) -> bool
  MistralDetector.detect_and_parse(text: str, tools: List[Tool]) -> StreamingParseResult
  MistralDetector.structure_info() -> _GetInfoFunc
  MistralDetector.build_ebnf(tools: List[Tool])

# python/sglang/srt/function_call/pythonic_detector.py
  PythonicDetector.__init__()
  PythonicDetector.has_tool_call(text: str) -> bool
  PythonicDetector.detect_and_parse(text: str, tools: List[Tool]) -> StreamingParseResult
  PythonicDetector.parse_streaming_increment(new_text: str, tools: List[Tool]) -> StreamingParseResult
  PythonicDetector.supports_structural_tag() -> bool
  PythonicDetector.structure_info() -> _GetInfoFunc
  PythonicDetector.build_ebnf(tools: List[Tool]) -> Optional[str]

# python/sglang/srt/function_call/qwen25_detector.py
  Qwen25Detector.__init__()
  Qwen25Detector.has_tool_call(text: str) -> bool
  Qwen25Detector.detect_and_parse(text: str, tools: List[Tool]) -> StreamingParseResult
  Qwen25Detector.parse_streaming_increment(new_text: str, tools: List[Tool]) -> StreamingParseResult
  Qwen25Detector.structure_info() -> _GetInfoFunc
  Qwen25Detector.build_ebnf(tools: List[Tool])

# python/sglang/srt/function_call/qwen3_coder_detector.py
  Qwen3CoderDetector.__init__()
  Qwen3CoderDetector.has_tool_call(text: str) -> bool
  Qwen3CoderDetector.detect_and_parse(text: str, tools: List[Tool]) -> StreamingParseResult
  Qwen3CoderDetector.parse_streaming_increment(new_text: str, tools: List[Tool]) -> StreamingParseResult
  Qwen3CoderDetector.supports_structural_tag() -> bool
  Qwen3CoderDetector.structure_info() -> _GetInfoFunc
  Qwen3CoderDetector.build_ebnf(tools: List[Tool])

# python/sglang/srt/function_call/step3_detector.py
get_argument_type(func_name: str, arg_key: str, defined_tools: List[Tool]) -> str
parse_arguments(value: str) -> tuple[Any, bool]
  Step3Detector.__init__()
  Step3Detector.has_tool_call(text: str) -> bool
  Step3Detector.detect_and_parse(text: str, tools: List[Tool]) -> StreamingParseResult
  Step3Detector.parse_streaming_increment(new_text: str, tools: List[Tool]) -> StreamingParseResult
  Step3Detector.supports_structural_tag() -> bool
  Step3Detector.structure_info() -> _GetInfoFunc
  Step3Detector.build_ebnf(tools: List[Tool]) -> str

# python/sglang/srt/harmony_parser.py
prefix_hold(text: str, tokens: List[str]) -> Tuple[str, str]
iter_tokens(text: str, start_pos: int) -> Iterator[Token]
  CanonicalStrategy.__init__()
  CanonicalStrategy.parse(text: str) -> Tuple[List[Event], str]
  TextStrategy.__init__()
  TextStrategy.set_buffer_context(buffer: str)
  TextStrategy.parse(text: str) -> Tuple[List[Event], str]
  HarmonyParser.__init__()
  HarmonyParser.parse(chunk: str) -> List[Event]

# python/sglang/srt/hf_transformers_utils.py
download_from_hf(model_path: str, allow_patterns: Optional[Union[str, list]])
get_hf_text_config(config: PretrainedConfig)
get_config(model: str, trust_remote_code: bool, revision: Optional[str], model_override_args: Optional[dict])
get_generation_config(model: str, trust_remote_code: bool, revision: Optional[str])
get_sparse_attention_config(model: str, sparse_attention_config_filename: str) -> Dict[str, Any]
get_context_length(config)
get_tokenizer(tokenizer_name: str) -> Union[PreTrainedTokenizer, PreTrainedTokenizerFast]
get_tokenizer_from_processor(processor)
get_processor(tokenizer_name: str)
attach_additional_stop_token_ids(tokenizer)
check_gguf_file(model: Union[str, os.PathLike]) -> bool

# python/sglang/srt/host_shared_memory.py
  HostSharedMemoryManager.__init__(base_name: str)
  HostSharedMemoryManager.malloc()
get_host_shared_memory_manager()
set_host_shared_memory_manager(instance: HostSharedMemoryManager)

# python/sglang/srt/jinja_template_utils.py
detect_jinja_template_content_format(chat_template: str) -> str
process_content_for_template_format(msg_dict: dict, content_format: str, image_data: list, video_data: list, audio_data: list, modalities: list) -> dict

# python/sglang/srt/layers/activation.py
  SiluAndMul.forward_native(x: torch.Tensor) -> torch.Tensor
  SiluAndMul.forward_cuda(x: torch.Tensor) -> torch.Tensor
  SiluAndMul.forward_cpu(x: torch.Tensor) -> torch.Tensor
  SiluAndMul.forward_npu(x: torch.Tensor) -> torch.Tensor
  GeluAndMul.__init__(approximate)
  GeluAndMul.forward_native(x: torch.Tensor) -> torch.Tensor
  GeluAndMul.forward_cuda(x: torch.Tensor) -> torch.Tensor
  GeluAndMul.forward_npu(x: torch.Tensor) -> torch.Tensor
  NewGELU.forward_native(x: torch.Tensor) -> torch.Tensor
  NewGELU.forward_cuda(x: torch.Tensor) -> torch.Tensor
  ReLU2.forward(x: torch.Tensor) -> torch.Tensor
  QuickGELU.forward_native(x: torch.Tensor) -> torch.Tensor
  QuickGELU.forward_cuda(x: torch.Tensor) -> torch.Tensor
  QuickGELU.forward_hip(x: torch.Tensor) -> torch.Tensor
  QuickGELU.forward_npu(x: torch.Tensor) -> torch.Tensor
  ScaledActivation.__init__(act_module: nn.Module, intermediate_size: int, input_is_parallel: bool, params_dtype: Optional[torch.dtype])
  ScaledActivation.forward(x: torch.Tensor) -> torch.Tensor
  ScaledActivation.weight_loader(param: nn.Parameter, loaded_weight: torch.Tensor)
get_act_fn(act_fn_name: str, quant_config: Optional[QuantizationConfig], intermediate_size: Optional[int], input_is_parallel: bool, params_dtype: Optional[torch.dtype]) -> nn.Module
get_cross_encoder_activation_function(config: PretrainedConfig)

# python/sglang/srt/layers/amx_utils.py
amx_process_weight_after_loading(weight)
dim_is_supported(weight)
  PackWeightMethod.__init__(weight_names, transpose_dims)
  PackWeightMethod.process_weights_after_loading(module) -> None

# python/sglang/srt/layers/attention/aiter_backend.py
  AiterAttnBackend.__init__(model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor])
  AiterAttnBackend.init_forward_metadata(forward_batch: ForwardBatch)
  AiterAttnBackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])
  AiterAttnBackend.init_forward_metadata_capture_cuda_graph(bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])
  AiterAttnBackend.init_forward_metadata_replay_cuda_graph(bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])
  AiterAttnBackend.get_cuda_graph_seq_len_fill_value()
  AiterAttnBackend.forward_extend(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)
  AiterAttnBackend.forward_decode(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)
  AiterIndicesUpdaterPrefill.__init__(model_runner: ModelRunner, attn_backend: AttentionBackend)
  AiterIndicesUpdaterPrefill.update(req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, prefix_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], spec_info: Optional[SpecInfo])
  AiterIndicesUpdaterPrefill.update_single_wrapper(req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, prefix_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], spec_info: Optional[SpecInfo])
  AiterMlaIndicesUpdaterPrefill.__init__(model_runner: ModelRunner, attn_backend: AttentionBackend)
  AiterMlaIndicesUpdaterPrefill.update(req_pool_indices: torch.Tensor, kv_lens: torch.Tensor, kv_lens_sum: int, extend_lens: torch.Tensor, max_q_len: int, max_kv_len: int, spec_info: Optional[SpecInfo])
  AiterMlaIndicesUpdaterPrefill.update_single_wrapper(req_pool_indices: torch.Tensor, kv_lens: torch.Tensor, kv_lens_sum: int, extend_lens: torch.Tensor, max_q_len: int, max_kv_len: int, spec_info: Optional[SpecInfo])
  AiterMultiStepDraftBackend.__init__(model_runner: ModelRunner, topk: int, speculative_num_steps: int)
  AiterMultiStepDraftBackend.common_template(forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: int)
  AiterMultiStepDraftBackend.init_forward_metadata(forward_batch: ForwardBatch)
  AiterMultiStepDraftBackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int)
  AiterMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(forward_batch: ForwardBatch)
  AiterMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(forward_batch: ForwardBatch, bs: int)

# python/sglang/srt/layers/attention/ascend_backend.py
  AscendAttnBackend.gen_attention_mask(max_seq_len: int, dtype)
  AscendAttnBackend.__init__(model_runner: ModelRunner)
  AscendAttnBackend.init_forward_metadata(forward_batch: ForwardBatch)
  AscendAttnBackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int)
  AscendAttnBackend.init_forward_metadata_capture_cuda_graph(bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  AscendAttnBackend.init_forward_metadata_replay_cuda_graph(bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])
  AscendAttnBackend.get_cuda_graph_seq_len_fill_value()
  AscendAttnBackend.forward_extend(q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)
  AscendAttnBackend.forward_decode_graph(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor])
  AscendAttnBackend.forward_decode(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor])

# python/sglang/srt/layers/attention/base_attn_backend.py
  AttentionBackend.init_forward_metadata(forward_batch: ForwardBatch)
  AttentionBackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int)
  AttentionBackend.init_forward_metadata_capture_cuda_graph(bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  AttentionBackend.init_forward_metadata_replay_cuda_graph(bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])
  AttentionBackend.get_cuda_graph_seq_len_fill_value()
  AttentionBackend.forward(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)
  AttentionBackend.forward_decode(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)
  AttentionBackend.forward_extend(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)
  AttentionBackend.support_triton()

# python/sglang/srt/layers/attention/cutlass_mla_backend.py
  CutlassMLADecodeMetadata.__init__(workspace: Optional[torch.Tensor], block_kv_indices: Optional[torch.Tensor])
  CutlassMLABackend.__init__(model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], kv_last_page_len_buf: Optional[torch.Tensor])
  CutlassMLABackend.init_forward_metadata(forward_batch: ForwardBatch)
  CutlassMLABackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int, block_kv_indices: Optional[torch.Tensor])
  CutlassMLABackend.init_forward_metadata_capture_cuda_graph(bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])
  CutlassMLABackend.init_forward_metadata_replay_cuda_graph(bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])
  CutlassMLABackend.get_cuda_graph_seq_len_fill_value()
  CutlassMLABackend.forward_decode(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor])

# python/sglang/srt/layers/attention/double_sparsity_backend.py
  DoubleSparseAttnBackend.__init__(model_runner: ModelRunner)
  DoubleSparseAttnBackend.init_forward_metadata(forward_batch: ForwardBatch)
  DoubleSparseAttnBackend.forward_extend(q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)
  DoubleSparseAttnBackend.forward_decode(q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)

# python/sglang/srt/layers/attention/dual_chunk_flashattention_backend.py
  DualChunkFlashAttentionBackend.__init__(model_runner: 'ModelRunner') -> None
  DualChunkFlashAttentionBackend.get_sparse_attention_config(layer_idx) -> List[Dict[str, Any]]
  DualChunkFlashAttentionBackend.init_forward_metadata(forward_batch: ForwardBatch)
  DualChunkFlashAttentionBackend.forward_extend(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: 'RadixAttention', forward_batch: ForwardBatch, save_kv_cache)
  DualChunkFlashAttentionBackend.forward_decode(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: 'RadixAttention', forward_batch: ForwardBatch, save_kv_cache) -> torch.Tensor
  DualChunkFlashAttentionBackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int)
  DualChunkFlashAttentionBackend.init_forward_metadata_capture_cuda_graph(bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[None])
  DualChunkFlashAttentionBackend.init_forward_metadata_replay_cuda_graph(bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[None], seq_lens_cpu: Optional[torch.Tensor], out_cache_loc: torch.Tensor)
  DualChunkFlashAttentionBackend.get_cuda_graph_seq_len_fill_value()

# python/sglang/srt/layers/attention/flashattention_backend.py
make_local_attention_virtual_batches(attn_chunk_size: int, query_start_loc_np: np.ndarray, seq_lens_np: np.ndarray, block_table: torch.Tensor, page_size: int) -> tuple[np.ndarray, np.ndarray, np.ndarray, torch.Tensor]
cdiv(a: int, b: int) -> int
merge_state_v2_wrapper(o, s_a, o_exp, s_b)
  FlashAttentionBackend.__init__(model_runner: ModelRunner, skip_prefill: bool, speculative_step_id, topk, speculative_num_steps)
  FlashAttentionBackend.init_forward_metadata(forward_batch: ForwardBatch)
  FlashAttentionBackend.forward_extend(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor], sinks: Optional[torch.Tensor])
  FlashAttentionBackend.forward_decode(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor], sinks: Optional[torch.Tensor]) -> torch.Tensor
  FlashAttentionBackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int)
  FlashAttentionBackend.init_forward_metadata_capture_cuda_graph(bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  FlashAttentionBackend.init_forward_metadata_replay_cuda_graph(bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor], out_cache_loc: Optional[torch.Tensor])
  FlashAttentionBackend.get_cuda_graph_seq_len_fill_value()
prepare_swa_spec_page_table_triton(page_table_dst: torch.Tensor, page_table_a: torch.Tensor, page_table_b: torch.Tensor, seq_len_a: torch.Tensor, seq_len_b: torch.Tensor, speculative_num_draft_tokens: int)
  FlashAttentionMultiStepBackend.__init__(model_runner: ModelRunner, topk: int, speculative_num_steps: int)
  FlashAttentionMultiStepBackend.init_forward_metadata(forward_batch: ForwardBatch)
  FlashAttentionMultiStepBackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int)
  FlashAttentionMultiStepBackend.init_forward_metadata_capture_cuda_graph(forward_batch: ForwardBatch)
  FlashAttentionMultiStepBackend.init_forward_metadata_replay_cuda_graph(forward_batch: ForwardBatch, bs: int)
normal_decode_set_metadata(cache_seqlens_int32: torch.Tensor, cu_seqlens_k: torch.Tensor, page_table: torch.Tensor, req_to_token: torch.Tensor, req_pool_indices: torch.Tensor, strided_indices: torch.Tensor, max_seq_pages: torch.Tensor, seq_lens: torch.Tensor, seq_len_delta: int, page_size: int)

# python/sglang/srt/layers/attention/flashinfer_backend.py
  FlashInferAttnBackend.__init__(model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], kv_last_page_len_buf: Optional[torch.Tensor])
  FlashInferAttnBackend.init_forward_metadata(forward_batch: ForwardBatch)
  FlashInferAttnBackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])
  FlashInferAttnBackend.init_forward_metadata_capture_cuda_graph(bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  FlashInferAttnBackend.init_forward_metadata_replay_cuda_graph(bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])
  FlashInferAttnBackend.get_cuda_graph_seq_len_fill_value()
  FlashInferAttnBackend.forward_extend(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)
  FlashInferAttnBackend.forward_decode(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)
  FlashInferIndicesUpdaterDecode.__init__(model_runner: ModelRunner, attn_backend: FlashInferAttnBackend)
  FlashInferIndicesUpdaterDecode.update(req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  FlashInferIndicesUpdaterDecode.update_single_wrapper(req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  FlashInferIndicesUpdaterDecode.update_sliding_window(req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  FlashInferIndicesUpdaterDecode.update_cross_attention(req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  FlashInferIndicesUpdaterDecode.call_begin_forward(wrapper: BatchDecodeWithPagedKVCacheWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, kv_indptr: torch.Tensor, kv_start_idx: torch.Tensor, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor], use_sliding_window_kv_pool: bool)
  FlashInferIndicesUpdaterPrefill.__init__(model_runner: ModelRunner, attn_backend: FlashInferAttnBackend)
  FlashInferIndicesUpdaterPrefill.update(req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  FlashInferIndicesUpdaterPrefill.update_single_wrapper(req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  FlashInferIndicesUpdaterPrefill.update_sliding_window(req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  FlashInferIndicesUpdaterPrefill.update_cross_attention(req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  FlashInferIndicesUpdaterPrefill.call_begin_forward(wrapper_ragged: BatchPrefillWithRaggedKVCacheWrapper, wrapper_paged: BatchPrefillWithPagedKVCacheWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, seq_lens: torch.Tensor, prefix_lens: torch.Tensor, kv_start_idx: torch.Tensor, kv_indptr: torch.Tensor, qo_indptr: torch.Tensor, use_ragged: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], use_sliding_window_kv_pool: bool)
  FlashInferMultiStepDraftBackend.__init__(model_runner: ModelRunner, topk: int, speculative_num_steps: int)
  FlashInferMultiStepDraftBackend.common_template(forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: Callable)
  FlashInferMultiStepDraftBackend.init_forward_metadata(forward_batch: ForwardBatch)
  FlashInferMultiStepDraftBackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int)
  FlashInferMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(forward_batch: ForwardBatch)
  FlashInferMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(forward_batch: ForwardBatch, bs: int)
should_use_tensor_core(kv_cache_dtype: torch.dtype, num_attention_heads: int, num_kv_heads: int) -> bool
fast_decode_plan(indptr: torch.Tensor, indices: torch.Tensor, last_page_len: torch.Tensor, num_qo_heads: int, num_kv_heads: int, head_dim: int, page_size: int, pos_encoding_mode: str, window_left: int, logits_soft_cap: Optional[float], q_data_type: Optional[Union[str, torch.dtype]], kv_data_type: Optional[Union[str, torch.dtype]], data_type: Optional[Union[str, torch.dtype]], sm_scale: Optional[float], rope_scale: Optional[float], rope_theta: Optional[float], non_blocking: bool) -> None

# python/sglang/srt/layers/attention/flashinfer_mla_backend.py
  FlashInferMhaChunkKVRunner.__init__(model_runner: ModelRunner, attn_backend: 'FlashInferMlaAttnBackend')
  FlashInferMhaChunkKVRunner.update_prefix_chunks(num_prefix_chunks: int)
  FlashInferMhaChunkKVRunner.update_wrapper(forward_batch: ForwardBatch)
  FlashInferMhaChunkKVRunner.forward(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch)
  FlashInferMLAAttnBackend.__init__(model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], q_indptr_decode_buf: Optional[torch.Tensor])
  FlashInferMLAAttnBackend.init_forward_metadata(forward_batch: ForwardBatch)
  FlashInferMLAAttnBackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])
  FlashInferMLAAttnBackend.init_forward_metadata_capture_cuda_graph(bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])
  FlashInferMLAAttnBackend.init_forward_metadata_replay_cuda_graph(bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])
  FlashInferMLAAttnBackend.get_cuda_graph_seq_len_fill_value()
  FlashInferMLAAttnBackend.init_mha_chunk_metadata(forward_batch: ForwardBatch)
  FlashInferMLAAttnBackend.forward_extend(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor])
  FlashInferMLAAttnBackend.forward_decode(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor])
  FlashInferMLAIndicesUpdaterDecode.__init__(model_runner: ModelRunner, attn_backend: AttentionBackend)
  FlashInferMLAIndicesUpdaterDecode.update(req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, decode_wrapper: BatchMLAPagedAttentionWrapper, init_metadata_replay: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  FlashInferMLAIndicesUpdaterDecode.call_begin_forward(wrapper: BatchMLAPagedAttentionWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, q_indptr: torch.Tensor, kv_indptr: torch.Tensor, init_metadata_replay: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  FlashInferMLAIndicesUpdaterPrefill.__init__(model_runner: ModelRunner, attn_backend: AttentionBackend)
  FlashInferMLAIndicesUpdaterPrefill.update(req_pool_indices: torch.Tnesor, seq_lens: torch.Tensor, seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrapper_paged: BatchMLAPagedAttentionWrapper, use_ragged: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  FlashInferMLAIndicesUpdaterPrefill.call_begin_forward(wrapper_ragged: BatchPrefillWithRaggedKVCacheWrapper, wrapper_paged: BatchMLAPagedAttentionWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, seq_lens: torch.Tensor, prefix_lens: torch.Tensor, kv_indptr: torch.Tensor, qo_indptr: torch.Tensor, use_ragged: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  FlashInferMLAMultiStepDraftBackend.__init__(model_runner: ModelRunner, topk: int, speculative_num_steps: int)
  FlashInferMLAMultiStepDraftBackend.common_template(forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: Callable)
  FlashInferMLAMultiStepDraftBackend.init_forward_metadata(forward_batch: ForwardBatch)
  FlashInferMLAMultiStepDraftBackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int)
  FlashInferMLAMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(forward_batch: ForwardBatch)
  FlashInferMLAMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(forward_batch: ForwardBatch, bs: int)
fast_mla_decode_plan(qo_indptr_cpu: torch.Tensor, kv_indptr_cpu: torch.Tensor, kv_indices: torch.Tensor, kv_len_arr_cpu: torch.Tensor, num_heads: int, head_dim_ckv: int, head_dim_kpe: int, page_size: int, causal: bool, sm_scale: float, q_data_type: torch.dtype, kv_data_type: torch.dtype) -> None

# python/sglang/srt/layers/attention/flashmla_backend.py
  FlashMLADecodeMetadata.__init__(flashmla_metadata: Optional[Tuple[torch.Tensor, torch.Tensor]], num_splits: Optional[torch.Tensor], block_kv_indices: Optional[torch.Tensor])
  FlashMLABackend.__init__(model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], kv_last_page_len_buf: Optional[torch.Tensor])
  FlashMLABackend.init_forward_metadata(forward_batch: ForwardBatch)
  FlashMLABackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int, block_kv_indices: Optional[torch.Tensor])
  FlashMLABackend.init_forward_metadata_capture_cuda_graph(bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])
  FlashMLABackend.init_forward_metadata_replay_cuda_graph(bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])
  FlashMLABackend.get_cuda_graph_seq_len_fill_value()
  FlashMLABackend.forward_decode(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)
  FlashMLABackend.forward_extend(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)
  FlashMLAMultiStepDraftBackend.__init__(model_runner: ModelRunner, topk: int, speculative_num_steps: int)
  FlashMLAMultiStepDraftBackend.common_template(forward_batch: ForwardBatch, call_fn: Callable)
  FlashMLAMultiStepDraftBackend.init_forward_metadata(forward_batch: ForwardBatch)
  FlashMLAMultiStepDraftBackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int)
  FlashMLAMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(forward_batch: ForwardBatch)
  FlashMLAMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(forward_batch: ForwardBatch, bs: int)

# python/sglang/srt/layers/attention/hybrid_attn_backend.py
  HybridAttnBackend.__init__(model_runner: ModelRunner, prefill_backend: AttentionBackend, decode_backend: AttentionBackend)
  HybridAttnBackend.init_forward_metadata(forward_batch: ForwardBatch)
  HybridAttnBackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int)
  HybridAttnBackend.init_forward_metadata_capture_cuda_graph(bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  HybridAttnBackend.init_forward_metadata_replay_cuda_graph(bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])
  HybridAttnBackend.get_cuda_graph_seq_len_fill_value()
  HybridAttnBackend.forward_decode(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)
  HybridAttnBackend.forward_extend(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)

# python/sglang/srt/layers/attention/intel_amx_backend.py
  IntelAMXAttnBackend.__init__(model_runner: ModelRunner)
  IntelAMXAttnBackend.init_forward_metadata(forward_batch: ForwardBatch)
  IntelAMXAttnBackend.forward_extend(q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)
  IntelAMXAttnBackend.forward_decode(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)
  IntelAMXAttnBackend.support_triton()

# python/sglang/srt/layers/attention/merge_state.py
merge_state(prefix_output: torch.Tensor, prefix_lse: torch.Tensor, suffix_output: torch.Tensor, suffix_lse: torch.Tensor, output: Optional[torch.Tensor], output_lse: Optional[torch.Tensor]) -> Tuple[torch.Tensor, Optional[torch.Tensor]]

# python/sglang/srt/layers/attention/tbo_backend.py
  TboAttnBackend.__init__(primary: AttentionBackend, children: List[AttentionBackend])
  TboAttnBackend.init_new(cls, creator: Callable[[], AttentionBackend])
  TboAttnBackend.init_forward_metadata(forward_batch: 'ForwardBatch')
  TboAttnBackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int)
  TboAttnBackend.init_forward_metadata_capture_cuda_graph(bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: 'ForwardMode', spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  TboAttnBackend.init_forward_metadata_replay_cuda_graph(bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: 'ForwardMode', spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])
  TboAttnBackend.get_cuda_graph_seq_len_fill_value()
  TboAttnBackend.forward_extend()
  TboAttnBackend.forward_decode()

# python/sglang/srt/layers/attention/torch_native_backend.py
  TorchNativeAttnBackend.__init__(model_runner: ModelRunner)
  TorchNativeAttnBackend.init_forward_metadata(forward_batch: ForwardBatch)
  TorchNativeAttnBackend.forward_extend(q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)
  TorchNativeAttnBackend.forward_decode(q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)
  TorchNativeAttnBackend.support_triton()

# python/sglang/srt/layers/attention/triton_backend.py
logit_capping_mod(logit_capping_method, logit_cap)
  TritonAttnBackend.__init__(model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor])
  TritonAttnBackend.get_num_kv_splits(num_kv_splits: torch.Tensor, seq_lens: torch.Tensor)
  TritonAttnBackend.init_forward_metadata(forward_batch: ForwardBatch)
  TritonAttnBackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])
  TritonAttnBackend.init_forward_metadata_capture_cuda_graph(bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  TritonAttnBackend.init_forward_metadata_replay_cuda_graph(bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])
  TritonAttnBackend.get_cuda_graph_seq_len_fill_value()
  TritonAttnBackend.forward_extend(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache, sinks)
  TritonAttnBackend.forward_decode(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache, sinks)
  TritonMultiStepDraftBackend.__init__(model_runner: ModelRunner, topk: int, speculative_num_steps: int)
  TritonMultiStepDraftBackend.common_template(forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: int)
  TritonMultiStepDraftBackend.init_forward_metadata(forward_batch: ForwardBatch)
  TritonMultiStepDraftBackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int)
  TritonMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(forward_batch: ForwardBatch)
  TritonMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(forward_batch: ForwardBatch, bs: int)
get_num_kv_splits_triton(num_kv_splits_ptr, seq_lens_ptr, num_seq, num_group, num_head, num_kv_head, max_kv_splits, device_core_count, MAX_NUM_SEQ: tl.constexpr)
update_sliding_window_buffer(window_kv_indptr, req_to_token, sliding_window_size, seq_lens, req_pool_indices, bs, device, token_to_kv_pool_allocator)
update_sliding_window_buffer_cuda_graph(window_kv_indptr, window_kv_indices, req_to_token, sliding_window_size, seq_lens, req_pool_indices, bs, token_to_kv_pool_allocator)

# python/sglang/srt/layers/attention/triton_ops/decode_attention.py
tanh(x)
decode_attention_fwd_normal(q, k_buffer, v_buffer, o, kv_indptr, kv_indices, attn_logits, attn_lse, num_kv_splits, max_kv_splits, sm_scale, logit_cap, sinks, xai_temperature_len)
decode_attention_fwd_grouped(q, k_buffer, v_buffer, o, kv_indptr, kv_indices, attn_logits, attn_lse, num_kv_splits, max_kv_splits, sm_scale, logit_cap, sinks, xai_temperature_len)
decode_attention_fwd(q, k_buffer, v_buffer, o, kv_indptr, kv_indices, attn_logits, attn_lse, num_kv_splits, max_kv_splits, sm_scale, logit_cap, sinks, xai_temperature_len)

# python/sglang/srt/layers/attention/triton_ops/double_sparsity_attention.py
tanh(x)
flash_decode_stage1(q, k, v, Req_to_tokens, B_req_idx, B_Seqlen, max_len_in_batch, mid_out, mid_out_logsumexp, block_seq)
flash_decode_stage2(mid_out, mid_out_logexpsum, B_Seqlen, O, block_seq)
flash_decode_attention_fwd(q, k_buffer, v_buffer, o, req_to_token, b_req_idx, b_start_loc, b_seq_len, attn_logits, max_len_in_batch, sm_scale, logit_cap)
sparse_flash_decode_stage1(q_label, k_label_buffer, att_out, Req_to_tokens, B_Seqlen, max_len_in_batch, sm_scale, logit_cap)
sparse_flash_decode_stage2(q, k, v, Req_to_tokens, Topk_token_indices, heavy_token_num, mid_out, mid_out_logsumexp, block_seq, sm_scale)
sparse_flash_decode_stage3(Seqlen, mid_out, mid_out_logexpsum, O, block_seq)
flash_decode_sparse_attention_fwd(q, k_buffer, v_buffer, o, q_label, k_label_buffer, req_to_token, b_seq_len, max_len_in_batch, sm_scale, logit_cap, heavy_token_num, att_out_approx, mid_out, mid_o_logexpsum, BLOCK_SEQ)
extend_attention_fwd(q_extend, k_extend, v_extend, o_extend, k_buffer, v_buffer, req_to_tokens, b_req_idx, b_seq_len, b_seq_len_extend, b_start_loc_extend, max_len_extend, sm_scale, logit_cap)

# python/sglang/srt/layers/attention/triton_ops/extend_attention.py
tanh(x)
extend_attention_fwd(q_extend, k_extend, v_extend, o_extend, k_buffer, v_buffer, qo_indptr, kv_indptr, kv_indices, custom_mask, is_causal, mask_indptr, max_len_extend, sm_scale, logit_cap, skip_prefix_custom_mask, sliding_window_size, sinks, window_kv_offsets, xai_temperature_len)
redundant_attention(q_extend, o_extend, k_buffer, v_buffer, b_req_idx, b_start_loc, b_seq_len, b_seq_len_prefix, max_len_in_batch)

# python/sglang/srt/layers/attention/triton_ops/merge_state.py
merge_state_kernel(output, output_lse, prefix_output, prefix_lse, suffix_output, suffix_lse, HEAD_SIZE: tl.constexpr, PADDED_HEAD_SIZE: tl.constexpr, OUTPUT_LSE: tl.constexpr)
merge_state_triton(prefix_output: torch.Tensor, prefix_lse: torch.Tensor, suffix_output: torch.Tensor, suffix_lse: torch.Tensor, output: Optional[torch.Tensor], output_lse: Optional[torch.Tensor]) -> Tuple[torch.Tensor, Optional[torch.Tensor]]

# python/sglang/srt/layers/attention/triton_ops/prefill_attention.py
context_attention_fwd(q, k, v, o, b_start_loc, b_seq_len, max_input_len, is_causal)

# python/sglang/srt/layers/attention/triton_ops/rocm_mla_decode_rope.py
is_hip()
tanh(x)
decode_attention_fwd_grouped_rope(q, k_buffer, v_buffer, o, kv_indptr, kv_indices, k_pe_tokens, kv_lora_rank, rotary_dim, cos_sin_cache, positions, attn_logits, num_kv_splits, sm_scale, logit_cap, use_rope, is_neox_style)

# python/sglang/srt/layers/attention/trtllm_mha_backend.py
  TRTLLMHAAttnBackend.__init__(model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], kv_last_page_len_buf: Optional[torch.Tensor], speculative_step_id: int)
  TRTLLMHAAttnBackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])
  TRTLLMHAAttnBackend.init_forward_metadata_capture_cuda_graph(bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])
  TRTLLMHAAttnBackend.init_forward_metadata_replay_cuda_graph(bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])
  TRTLLMHAAttnBackend.get_cuda_graph_seq_len_fill_value() -> int
  TRTLLMHAAttnBackend.init_forward_metadata(forward_batch: ForwardBatch)
  TRTLLMHAAttnBackend.forward_decode(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool) -> torch.Tensor
  TRTLLMHAAttnBackend.forward_extend(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)
  TRTLLMHAAttnMultiStepDraftBackend.__init__(model_runner: ModelRunner, topk: int, speculative_num_steps: int)
  TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata(forward_batch: ForwardBatch)
  TRTLLMHAAttnMultiStepDraftBackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int)
  TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(forward_batch: ForwardBatch)
  TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(forward_batch: ForwardBatch, bs: int)

# python/sglang/srt/layers/attention/trtllm_mla_backend.py
  TRTLLMMLABackend.__init__(model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], q_indptr_decode_buf: Optional[torch.Tensor])
  TRTLLMMLABackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])
  TRTLLMMLABackend.init_forward_metadata_capture_cuda_graph(bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])
  TRTLLMMLABackend.init_forward_metadata_replay_cuda_graph(bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])
  TRTLLMMLABackend.get_cuda_graph_seq_len_fill_value() -> int
  TRTLLMMLABackend.init_forward_metadata(forward_batch: ForwardBatch)
  TRTLLMMLABackend.quantize_and_rope_for_fp8(q_nope: torch.Tensor, q_rope: torch.Tensor, k_nope: torch.Tensor, k_rope: torch.Tensor, forward_batch: ForwardBatch, cos_sin_cache: torch.Tensor, is_neox: bool) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]
  TRTLLMMLABackend.forward_decode(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor], cos_sin_cache: Optional[torch.Tensor], is_neox: Optional[bool]) -> torch.Tensor
  TRTLLMMLAMultiStepDraftBackend.__init__(model_runner: 'ModelRunner', topk: int, speculative_num_steps: int)

# python/sglang/srt/layers/attention/utils.py
create_flashinfer_kv_indices_triton(req_to_token_ptr, req_pool_indices_ptr, page_kernel_lens_ptr, kv_indptr, kv_start_idx, kv_indices_ptr, req_to_token_ptr_stride: tl.constexpr)
create_flashmla_kv_indices_triton(req_to_token_ptr, req_pool_indices_ptr, page_kernel_lens_ptr, kv_start_idx, kv_indices_ptr, req_to_token_ptr_stride: tl.constexpr, kv_indices_ptr_stride: tl.constexpr, NUM_PAGE_PER_BLOCK: tl.constexpr, PAGED_SIZE: tl.constexpr)

# python/sglang/srt/layers/attention/vision.py
  SingletonCache.set_data(value: Any) -> None
  SingletonCache.get_data() -> Optional[Any]
  SingletonCache.empty() -> bool
  VisionSdpaAttention.__init__(head_dim: int, num_heads: int, num_kv_heads: int, dropout: float, flatten_batch: bool, softmax_in_single_precision: bool)
  VisionSdpaAttention.generate_patch_attention_mask(s: int, cu_seqlens: Optional[torch.Tensor], flatten_batch: bool) -> Optional[torch.Tensor]
  VisionSdpaAttention.forward(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, bsz: int, cu_seqlens: Optional[torch.Tensor], attention_mask: Optional[torch.Tensor]) -> torch.Tensor
  VisionTritonAttention.__init__()
  VisionTritonAttention.forward(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens: Optional[torch.Tensor], bsz: int, seq_len: int) -> torch.Tensor
  VisionFlash3Attention.__init__()
  VisionFlash3Attention.forward(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens: Optional[Union[SingletonCache, torch.Tensor]], bsz: int, seq_len: int) -> torch.Tensor
  VisionAttention.__init__(embed_dim: int, num_heads: int, projection_size: int, use_qkv_parallel: bool, qkv_backend: Optional[str], quant_config: Optional[QuantizationConfig], dropout: float, softmax_in_single_precision: bool, flatten_batch: bool, prefix: str, proj_bias: bool, num_dummy_heads: int, qkv_bias: bool, qk_normalization: bool, layer_norm_eps: float, customized_position_embedding_applier: Callable[[torch.Tensor, torch.Tensor, Any, Any], Tuple[torch.Tensor, torch.Tensor]])
  VisionAttention.forward(x: torch.Tensor, cu_seqlens: Optional[torch.Tensor], position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]], attention_mask: Optional[torch.Tensor]) -> torch.Tensor

# python/sglang/srt/layers/attention/vision_utils.py
update_vit_attn_dummy_heads_config(config)
pad_vit_attn_dummy_heads(config, name: str, loaded_weight: torch.Tensor)

# python/sglang/srt/layers/attention/wave_backend.py
get_num_kv_splits_triton(num_kv_splits_ptr, seq_lens_ptr, num_seq, num_group, num_head, num_kv_head, max_kv_splits, device_core_count, MAX_NUM_SEQ: tl.constexpr)
  WaveAttnBackend.__init__(model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor])
  WaveAttnBackend.get_num_kv_splits(num_kv_splits: torch.Tensor, seq_lens: torch.Tensor)
  WaveAttnBackend.init_forward_metadata(forward_batch: ForwardBatch)
  WaveAttnBackend.init_cuda_graph_state(max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])
  WaveAttnBackend.init_forward_metadata_capture_cuda_graph(bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  WaveAttnBackend.init_forward_metadata_replay_cuda_graph(bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])
  WaveAttnBackend.get_cuda_graph_seq_len_fill_value()
  WaveAttnBackend.forward_extend(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)
  WaveAttnBackend.forward_decode(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)

# python/sglang/srt/layers/attention/wave_ops/decode_attention.py
get_wave_kernel(shape: paged_decode_attention_shape, max_kv_splits, input_dtype, output_dtype, logit_cap)
decode_attention_intermediate_arrays_shapes(num_seqs, head_size_kv, num_query_heads, max_kv_splits)
decode_attention_wave(q, k_buffer, v_buffer, o, b_req_idx, req_to_token, attn_logits, attn_logits_max, num_kv_splits, max_kv_splits, sm_scale, logit_cap)
decode_attention_fwd(q, k_buffer, v_buffer, o, b_req_idx, req_to_token, attn_logits, attn_logits_max, num_kv_splits, max_kv_splits, sm_scale, logit_cap)

# python/sglang/srt/layers/attention/wave_ops/extend_attention.py
get_wave_kernel(shape: AttentionShape, q_shape: tuple[int], k_shape: tuple[int], v_shape: tuple[int], k_cache_shape: tuple[int], v_cache_shape: tuple[int], o_shape: tuple[int], input_dtype: torch.dtype, output_dtype: torch.dtype, size_dtype: torch.dtype, is_causal: bool, logit_cap: float, layer_scaling: float)
extend_attention_wave(q_extend, k_extend, v_extend, k_buffer, v_buffer, qo_indptr, kv_indptr, kv_indices, custom_mask, mask_indptr, max_seq_len, output, is_causal, layer_scaling, logit_cap)

# python/sglang/srt/layers/attention/wave_ops/prefill_attention.py
prefill_attention_wave(q, k, v, o, b_start_loc, b_seq_len, max_seq_len, is_causal)

# python/sglang/srt/layers/communicator.py
  ScatterMode.model_input_output()
  _LayerModeComputationContext.previous_layer()
  LayerScatterModes.init_new(cls)
enable_moe_dense_fully_dp()
  LayerCommunicator.__init__(layer_scatter_modes: LayerScatterModes, input_layernorm: torch.nn.Module, post_attention_layernorm: torch.nn.Module, allow_reduce_scatter: bool, is_last_layer: bool)
  LayerCommunicator.prepare_attn(hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch)
  LayerCommunicator.prepare_mlp(hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch)
  LayerCommunicator.postprocess_layer(hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch)
  LayerCommunicator.should_use_reduce_scatter(forward_batch: ForwardBatch)
  LayerCommunicator.should_fuse_mlp_allreduce_with_next_layer(forward_batch: ForwardBatch) -> bool
  CommunicateContext.is_same_group_size(a: ScatterMode, b: ScatterMode)
  CommunicateContext.init_new(cls)
  CommunicateSimpleFn.get_fn(input_mode: ScatterMode, output_mode: ScatterMode, context: CommunicateContext)
  CommunicateWithAllReduceAndLayerNormFn.get_fn(hidden_states_input_mode: ScatterMode, residual_input_mode: ScatterMode, hidden_states_output_mode: ScatterMode, residual_output_mode: ScatterMode, context: CommunicateContext)
  CommunicateSummableTensorPairFn.execute(cls, hidden_states_input_mode, residual_input_mode, output_mode, context)
  CommunicateSummableTensorPairFn.get_fn(hidden_states_input_mode: ScatterMode, residual_input_mode: ScatterMode, output_mode: ScatterMode, context: CommunicateContext)

# python/sglang/srt/layers/dp_attention.py
  DpPaddingMode.is_max_len()
  DpPaddingMode.is_sum_len()
  DpPaddingMode.get_dp_padding_mode(cls, global_num_tokens: List[int]) -> DpPaddingMode
  DpPaddingMode.get_default_mode_in_cuda_graph(cls) -> DpPaddingMode
  _DpGatheredBufferWrapper.set_metadata(cls, hidden_size: int, dtype: torch.dtype, device: torch.device)
  _DpGatheredBufferWrapper.set_dp_buffer_len(cls, global_dp_buffer_len: int, local_dp_buffer_len: int, global_num_tokens: Optional[List[int]])
  _DpGatheredBufferWrapper.get_global_dp_buffer(cls) -> torch.Tensor
  _DpGatheredBufferWrapper.get_local_dp_buffer(cls) -> torch.Tensor
  _DpGatheredBufferWrapper.get_global_dp_buffer_len(cls) -> int
  _DpGatheredBufferWrapper.get_local_dp_buffer_len(cls) -> int
  _DpGatheredBufferWrapper.get_dp_global_num_tokens(cls) -> List[int]
set_dp_buffer_len(global_dp_buffer_len: int, local_dp_buffer_len: int, global_num_tokens: Optional[List[int]])
get_global_dp_buffer() -> torch.Tensor
get_local_dp_buffer() -> torch.Tensor
get_global_dp_buffer_len() -> int
get_local_dp_buffer_len() -> int
get_dp_global_num_tokens() -> List[int]
compute_dp_attention_world_info(enable_dp_attention, tp_rank, tp_size, dp_size)
compute_dp_attention_local_info(enable_dp_attention, tp_rank, tp_size, dp_size, moe_dense_tp_size)
initialize_dp_attention(server_args: ServerArgs, model_config: ModelConfig)
is_dp_attention_enabled() -> bool
get_attention_tp_group() -> GroupCoordinator
get_attention_tp_rank() -> int
get_attention_tp_size() -> int
get_attention_dp_rank() -> int
get_attention_dp_size() -> int
get_local_attention_dp_rank() -> int
get_local_attention_dp_size() -> int
disable_dp_size()
get_dp_local_info(forward_batch: ForwardBatch) -> Tuple[torch.Tensor, torch.Tensor]
memcpy_triton_kernel(dst_ptr, src_ptr, offset_ptr, sz_ptr, offset_src: tl.constexpr, chunk_size, BLOCK_SIZE: tl.constexpr)
prod(x)
memcpy_triton(dst, src, dim, offset, sz, offset_src)
dp_gather_partial(global_tokens: torch.Tensor, local_tokens: torch.Tensor, forward_batch: ForwardBatch)
dp_gather_replicate(global_tokens: torch.Tensor, local_tokens: torch.Tensor, forward_batch: ForwardBatch)
dp_scatter(local_tokens: torch.Tensor, global_tokens: torch.Tensor, forward_batch: ForwardBatch)
dp_reduce_scatter_tensor(output: torch.Tensor, input: torch.Tensor)
attn_tp_reduce_scatter_tensor(output: torch.Tensor, input: torch.Tensor)
attn_tp_all_gather_into_tensor(output: torch.Tensor, input: torch.Tensor)
attn_tp_all_gather(output_list: List[torch.Tensor], input: torch.Tensor)

# python/sglang/srt/layers/elementwise.py
fused_softcap_kernel(output_ptr, input_ptr, n_ele, softcap_const: tl.constexpr, BLOCK_SIZE: tl.constexpr)
fused_softcap(x, softcap_const, autotune)
  Softcap.__init__(softcap_const: float)
  Softcap.__call__()
  Softcap.forward(x: torch.Tensor) -> torch.Tensor
  Softcap.forward_native(x: torch.Tensor) -> torch.Tensor
  Softcap.forward_cuda(x: torch.Tensor, autotune) -> torch.Tensor
fused_dual_residual_rmsnorm_kernel(output_ptr, mid_ptr, activ_ptr, residual_ptr, weight1_ptr, weight2_ptr, eps: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)
fused_dual_residual_rmsnorm(x, residual, weight1, weight2, eps, autotune)
fused_rmsnorm_kernel(output_ptr, activ_ptr, weight_ptr, eps: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)
fused_rmsnorm(x, weight, eps, autotune, inplace)
  FusedDualResidualRMSNorm.__init__(rmsnorm1, rmsnorm2) -> None
  FusedDualResidualRMSNorm.__call__()
  FusedDualResidualRMSNorm.forward(x: torch.Tensor, residual: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
  FusedDualResidualRMSNorm.forward_cuda(x: torch.Tensor, residual: torch.Tensor, autotune) -> Tuple[torch.Tensor, torch.Tensor]
  FusedDualResidualRMSNorm.forward_flashinfer(x: torch.Tensor, residual: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
  FusedDualResidualRMSNorm.forward_native(x: torch.Tensor, residual: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
experts_combine_kernel(out_hidden_states, moe_hidden_states, mlp_hidden_states, combine_k: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)
experts_combine_triton(moe_hidden_states, mlp_hidden_states, output_buffer)
gelu_and_mul_kernel(out_hidden_states_ptr, out_scales_ptr, hidden_states_ptr, quant_max: tl.constexpr, static_scale: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)
gelu_and_mul_triton(hidden_states, scales, quantize, out)
silu_and_mul_kernel(out_hidden_states_ptr, out_scales_ptr, hidden_states_ptr, quant_max: tl.constexpr, static_scale: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)
silu_and_mul_triton(hidden_states, scales, quantize, out)

# python/sglang/srt/layers/flashinfer_comm_fusion.py
  FlashInferWorkspaceManager.__init__()
  FlashInferWorkspaceManager.initialize(world_size: int, rank: int, max_token_num: int, hidden_dim: int, group, use_fp32_lamport: bool)
  FlashInferWorkspaceManager.cleanup()
ensure_workspace_initialized(max_token_num: int, hidden_dim: int, use_fp32_lamport: bool)
flashinfer_allreduce_residual_rmsnorm(input_tensor: torch.Tensor, residual: torch.Tensor, weight: torch.Tensor, eps: float, max_token_num: int, use_oneshot: Optional[bool], trigger_completion_at_end: bool, fp32_acc: bool) -> Tuple[torch.Tensor, torch.Tensor]
fake_flashinfer_allreduce_residual_rmsnorm(input_tensor: torch.Tensor, residual: torch.Tensor, weight: torch.Tensor, eps: float, max_token_num: int, use_oneshot: Optional[bool], trigger_completion_at_end: bool, fp32_acc: bool) -> Tuple[torch.Tensor, torch.Tensor]
cleanup_flashinfer_workspace()

# python/sglang/srt/layers/layernorm.py
  RMSNorm.__init__(hidden_size: int, eps: float, var_hidden_size: Optional[int]) -> None
  RMSNorm.forward_cuda(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  RMSNorm.forward_npu(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  RMSNorm.forward_aiter(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  RMSNorm.forward_hip(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  RMSNorm.forward_native(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  RMSNorm.forward_cpu(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  RMSNorm.forward_with_allreduce_fusion(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  GemmaRMSNorm.__init__(hidden_size: int, eps: float) -> None
  GemmaRMSNorm.forward_native(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  GemmaRMSNorm.forward_cuda(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  GemmaRMSNorm.forward_npu(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  Gemma3RMSNorm.__init__(dim: int, eps: float)
  Gemma3RMSNorm.forward_native(x)
  Gemma3RMSNorm.forward_cuda(x)
  Gemma3RMSNorm.forward_npu(x)
  Gemma3RMSNorm.extra_repr()

# python/sglang/srt/layers/linear.py
adjust_marlin_shard(param, shard_size, shard_offset)
adjust_bitsandbytes_4bit_shard(param: Parameter, shard_offsets: Dict[str, Tuple[int, int]], loaded_shard_id: str) -> Tuple[int, int]
adjust_scalar_to_fused_array(param, loaded_weight, shard_id)
adjust_shard_offsets(shard_offsets, loaded_weight, dim)
  LinearBase.__init__(input_size: int, output_size: int, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str)
  LinearBase.forward(x: torch.Tensor) -> torch.Tensor
  ReplicatedLinear.__init__(input_size: int, output_size: int, bias: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str)
  ReplicatedLinear.weight_loader(param: Parameter, loaded_weight: torch.Tensor)
  ReplicatedLinear.forward(x: torch.Tensor) -> Tuple[torch.Tensor, Optional[torch.Tensor]]
  ReplicatedLinear.extra_repr() -> str
  ColumnParallelLinear.__init__(input_size: int, output_size: int, bias: bool, gather_output: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], output_sizes: Optional[List[int]], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], use_presharded_weights: bool)
  ColumnParallelLinear.weight_loader(param: Parameter, loaded_weight: torch.Tensor)
  ColumnParallelLinear.weight_loader_v2(param: Parameter, loaded_weight: torch.Tensor)
  ColumnParallelLinear.forward(input_)
  ColumnParallelLinear.extra_repr() -> str
  MergedColumnParallelLinear.__init__(input_size: int, output_sizes: List[int], bias: bool, gather_output: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], use_presharded_weights: bool)
  MergedColumnParallelLinear.weight_loader(param: Parameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[int])
  MergedColumnParallelLinear.weight_loader_v2(param: BasevLLMParameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[int])
  QKVParallelLinear.__init__(hidden_size: int, head_size: int, total_num_heads: int, total_num_kv_heads: Optional[int], bias: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], load_presharded_attn: bool)
  QKVParallelLinear.weight_loader_v2(param: BasevLLMParameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[str])
  QKVParallelLinear.weight_loader(param: Parameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[str])
  RowParallelLinear.__init__(input_size: int, output_size: int, bias: bool, input_is_parallel: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], reduce_results: bool, quant_config: Optional[QuantizationConfig], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], use_presharded_weights: bool)
  RowParallelLinear.weight_loader(param: Parameter, loaded_weight: torch.Tensor)
  RowParallelLinear.weight_loader_v2(param: BasevLLMParameter, loaded_weight: torch.Tensor)
  RowParallelLinear.forward(input_, skip_all_reduce)
  RowParallelLinear.extra_repr() -> str

# python/sglang/srt/layers/logits_processor.py
  LogitsMetadata.from_forward_batch(cls, forward_batch: ForwardBatch)
  LogitsMetadata.compute_dp_attention_metadata()
  LogitsProcessor.__init__(config, skip_all_gather: bool, logit_scale: Optional[float])
  LogitsProcessor.forward(input_ids, hidden_states, lm_head: VocabParallelEmbedding, logits_metadata: Union[LogitsMetadata, ForwardBatch], aux_hidden_states: Optional[torch.Tensor]) -> LogitsProcessorOutput
  LogitsProcessor.get_top_logprobs(all_logprobs: torch.Tensor, logits_metadata: LogitsMetadata)
  LogitsProcessor.get_token_ids_logprobs(all_logprobs: torch.Tensor, logits_metadata: LogitsMetadata)
  LogitsProcessor.compute_temp_top_p_normalized_logprobs(last_logits: torch.Tensor, logits_metadata: LogitsMetadata) -> torch.Tensor
fused_softcap_kernel(full_logits_ptr, softcapping_value, n_elements, BLOCK_SIZE: tl.constexpr)
fused_softcap(full_logits, final_logit_softcapping)

# python/sglang/srt/layers/moe/cutlass_moe.py
cutlass_fused_experts_fp8(a: torch.Tensor, w1_q: torch.Tensor, w2_q: torch.Tensor, w1_scale: torch.Tensor, w2_scale: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, a1_strides: torch.Tensor, c1_strides: torch.Tensor, a2_strides: torch.Tensor, c2_strides: torch.Tensor, workspace: torch.Tensor, a_ptrs: torch.Tensor, b_ptrs: torch.Tensor, out_ptrs: torch.Tensor, a_scales_ptrs: torch.Tensor, b_scales_ptrs: torch.Tensor, expert_offsets: torch.Tensor, problem_sizes1: torch.Tensor, problem_sizes2: torch.Tensor, use_fp8_blockscale: bool) -> torch.Tensor
cutlass_moe_fp4(a: torch.Tensor, a1_gscale: torch.Tensor, w1_fp4: torch.Tensor, w1_blockscale: torch.Tensor, w1_alphas: torch.Tensor, a2_gscale: torch.Tensor, w2_fp4: torch.Tensor, w2_blockscale: torch.Tensor, w2_alphas: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, params: CutlassMoEParams, apply_router_weight_on_input: bool)

# python/sglang/srt/layers/moe/cutlass_moe_params.py
  CutlassMoEParams.__init__(cutlass_moe_type: CutlassMoEType, device: torch.device, num_experts: int, intermediate_size_per_partition: int, hidden_size: int)
  CutlassMoEParams.to_gemm1_args() -> dict
  CutlassMoEParams.to_gemm2_args() -> dict

# python/sglang/srt/layers/moe/cutlass_w4a8_moe.py
cutlass_w4a8_moe(start_expert_id: int, end_expert_id: int, total_num_experts: int, a: torch.Tensor, w1_q: torch.Tensor, w2_q: torch.Tensor, w1_scale: torch.Tensor, w2_scale: torch.Tensor, topk_weights: torch.Tensor, topk_ids_: torch.Tensor, local_topk_ids: torch.Tensor, a_strides1: torch.Tensor, b_strides1: torch.Tensor, c_strides1: torch.Tensor, a_strides2: torch.Tensor, b_strides2: torch.Tensor, c_strides2: torch.Tensor, s_strides13: torch.Tensor, s_strides2: torch.Tensor, expert_offsets: torch.Tensor, problem_sizes1: torch.Tensor, problem_sizes2: torch.Tensor, a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], apply_router_weight_on_input: bool) -> torch.Tensor

# python/sglang/srt/layers/moe/ep_moe/kernels.py
deepep_permute_triton_kernel(input_ptr, gateup_input_ptr, src2dst_ptr, topk_ids_ptr, a1_scales_ptr, topk, hidden_size, BLOCK_SIZE: tl.constexpr)
deepep_post_reorder_triton_kernel(down_output_ptr, output_ptr, src2dst_ptr, topk_ids_ptr, topk_weights_ptr, topk, hidden_size, BLOCK_SIZE: tl.constexpr)
compute_src2dst_triton_kernel(reorder_ids, src2dst, num_toks, BLOCK_SIZE: tl.constexpr)
deepep_compute_src2dst_triton_kernel(reorder_ids, src2dst, num_toks, num_minus_one, BLOCK_SIZE: tl.constexpr)
deepep_run_moe_deep_preprocess(topk_ids: torch.Tensor, num_experts: int)
compute_seg_indptr_triton_kernel(reorder_topk_ids, seg_indptr, num_toks)
run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int)
run_cutlass_moe_ep_preproess(local_topk_ids: torch.Tensor, local_num_experts: int)
pre_reorder_triton_kernel_for_cutlass_moe(input_ptr, gateup_input_ptr, src2dst_ptr, topk_ids_ptr, a1_scales_ptr, num_experts, topk, hidden_size, BLOCK_SIZE: tl.constexpr)
pre_reorder_triton_kernel(input_ptr, gateup_input_ptr, src2dst_ptr, topk_ids_ptr, a1_scales_ptr, start_expert_id, end_expert_id, topk, hidden_size, BLOCK_SIZE: tl.constexpr, use_per_token_if_dynamic: tl.constexpr)
silu_and_mul_triton_kernel(gateup_output, down_input, hidden_size, reorder_topk_ids, scales, start_expert_id, end_expert_id, BLOCK_SIZE: tl.constexpr)
silu_and_mul_masked_post_quant_fwd(input: torch.Tensor, output: torch.Tensor, output_scale: torch.Tensor, quant_group_size: int, masked_m: torch.Tensor, scale_ue8m0: bool)
tanh(x)
gelu_and_mul_triton_kernel(gateup_output, down_input, hidden_size, reorder_topk_ids, scales, start_expert_id, end_expert_id, BLOCK_SIZE: tl.constexpr)
post_reorder_triton_kernel(down_output_ptr, output_ptr, src2dst_ptr, topk_ids_ptr, topk_weights_ptr, start_expert_id, end_expert_id, topk, hidden_size, dst_start, BLOCK_SIZE: tl.constexpr)
post_reorder_triton_kernel_for_cutlass_moe(down_output_ptr, output_ptr, src2dst_ptr, topk_ids_ptr, topk_weights_ptr, num_experts, topk, hidden_size, dst_start, BLOCK_SIZE: tl.constexpr)
compute_m_range(pid, batch_size, seg_indptr, weight_indices, m_num_tiles_indptr, BLOCK_SIZE_M: tl.constexpr)
grouped_gemm_triton_kernel(a, b, c, batch_size, N, K, seg_indptr, weight_indices, m_num_tiles_indptr, scale_a, scale_b, use_fp8_w8a8: tl.constexpr, group_n: tl.constexpr, group_k: tl.constexpr, a_stride_0: tl.constexpr, b_stride_0: tl.constexpr, b_stride_1: tl.constexpr, as_stride_0: tl.constexpr, as_stride_1: tl.constexpr, bs_stride_0: tl.constexpr, bs_stride_2: tl.constexpr, bs_stride_1: tl.constexpr, use_per_token_if_dynamic: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr)
compute_m_num_tiles_indptr(m_num_tiles_indptr, seg_indptr, batch_size: tl.constexpr, BLOCK_SIZE_M: tl.constexpr)
grouped_gemm_triton(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, batch_size: int, weight_column_major: bool, seg_indptr: Optional[torch.Tensor], weight_indices: Optional[torch.Tensor], use_fp8_w8a8: bool, scale_a: torch.Tensor, scale_b: torch.Tensor, block_shape: Optional[List[int]], c_dtype, use_per_token_if_dynamic: bool)
ep_scatter(recv_x: torch.Tensor, recv_x_scale: torch.Tensor, recv_topk: torch.Tensor, num_recv_tokens_per_expert: torch.Tensor, expert_start_loc: torch.Tensor, output_tensor: torch.Tensor, output_tensor_scale: torch.Tensor, m_indices: torch.Tensor, output_index: torch.Tensor, scale_ue8m0: bool)
ep_gather(input_tensor: torch.Tensor, recv_topk_ids: torch.Tensor, recv_topk_weight: torch.Tensor, input_index: torch.Tensor, output_tensor: torch.Tensor)
get_tma_aligned_size(x: int, element_size: int) -> int
tma_align_input_scale(input_scale: torch.Tensor)
compute_masked_m_triton_kernel(seg_indptr, masked_m)
deepgemm_compute_src2dst_triton_kernel(topk_ids, reorder_ids, seg_indptr, src2dst, m_max, num_toks, BLOCK_SIZE: tl.constexpr)
fill_gateup_input_triton_kernel(input_ptr, scale_ptr, gateup_input_ptr, gateup_input_scale_ptr, src2dst_ptr, topk_ids_ptr, start_expert_id, end_expert_id, topk, m_max, hidden_size, scale_size, BLOCK_SIZE: tl.constexpr)
moe_ep_deepgemm_preprocess(topk_ids: torch.Tensor, num_experts: int, hidden_states: torch.Tensor, top_k: int, start_expert_id, end_expert_id, block_shape, output_dtype: torch.dtype)
compute_identity_kernel(top_k, hidden_states_ptr, expert_scales_ptr, num_tokens, output_ptr, hidden_dim, scales_stride, BLOCK_SIZE: tl.constexpr)
zero_experts_compute_triton(expert_indices, expert_scales, num_experts, zero_expert_type, hidden_states)

# python/sglang/srt/layers/moe/ep_moe/layer.py
  EPMoE.__init__(num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, num_fused_shared_experts: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str, activation: str, routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_clamp_limit: Optional[float], with_bias: bool)
  EPMoE.forward(hidden_states: torch.Tensor, topk_output: TopKOutput)
  EPMoE.forward_deepgemm(hidden_states: torch.Tensor, topk_output: TopKOutput)
  DeepEPMoE.__init__(num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, num_fused_shared_experts: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str, activation: str, routed_scaling_factor: Optional[float])
  DeepEPMoE.forward(hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)
  DeepEPMoE.dispatch(hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)
  DeepEPMoE.moe_impl(dispatch_output: DispatchOutput)
  DeepEPMoE.combine(hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)
  DeepEPMoE.forward_aiter(dispatch_output: Union[DeepEPNormalOutput, DeepEPLLOutput])
  DeepEPMoE.forward_deepgemm_contiguous(dispatch_output: DeepEPNormalOutput)
  DeepEPMoE.forward_deepgemm_masked(dispatch_output: DeepEPLLOutput)
  DeepEPMoE.forward_npu(dispatch_output: DeepEPLLOutput)
get_moe_impl_class(quant_config: Optional[QuantizationConfig])

# python/sglang/srt/layers/moe/fused_moe_native.py
fused_moe_forward_native(layer: torch.nn.Module, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
moe_forward_native(layer: torch.nn.Module, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor

# python/sglang/srt/layers/moe/fused_moe_triton/__init__.py
override_config(config)
get_config() -> Optional[Dict[str, Any]]

# python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
write_zeros_to_output(c_ptr, stride_cm, stride_cn, pid_n, N, offs_token, token_mask, BLOCK_SIZE_M, BLOCK_SIZE_N, compute_type)
fused_moe_kernel_gptq_awq(a_ptr, b_ptr, c_ptr, b_scale_ptr, b_zp_ptr, topk_weights_ptr, sorted_token_ids_ptr, expert_ids_ptr, num_tokens_post_padded_ptr, N: tl.constexpr, K: tl.constexpr, EM, num_valid_tokens, stride_am, stride_ak, stride_be, stride_bk, stride_bn, stride_cm, stride_cn, stride_bse, stride_bsk, stride_bsn, stride_bze, stride_bzk, stride_bzn, group_size: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, MUL_ROUTED_WEIGHT: tl.constexpr, top_k: tl.constexpr, compute_type: tl.constexpr, has_zp: tl.constexpr, use_int4_w4a16: tl.constexpr, use_int8_w8a16: tl.constexpr, even_Ks: tl.constexpr)
fused_moe_kernel(a_ptr, b_ptr, bias_ptr, c_ptr, a_scale_ptr, b_scale_ptr, topk_weights_ptr, sorted_token_ids_ptr, expert_ids_ptr, num_tokens_post_padded_ptr, N, K, EM, num_valid_tokens, stride_am, stride_ak, stride_be, stride_bk, stride_bn, stride_bias_e, stride_bias_n, stride_cm, stride_cn, stride_asm, stride_ask, stride_bse, stride_bsk, stride_bsn, group_n: tl.constexpr, group_k: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, MUL_ROUTED_WEIGHT: tl.constexpr, top_k: tl.constexpr, compute_type: tl.constexpr, use_fp8_w8a8: tl.constexpr, use_int8_w8a8: tl.constexpr, use_int8_w8a16: tl.constexpr, per_channel_quant: tl.constexpr, even_Ks: tl.constexpr)
moe_align_block_size(topk_ids: torch.Tensor, block_size: int, num_experts: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
invoke_fused_moe_kernel(A: torch.Tensor, B: torch.Tensor, bias: Optional[torch.Tensor], C: torch.Tensor, A_scale: Optional[torch.Tensor], B_scale: Optional[torch.Tensor], B_zp: Optional[torch.Tensor], topk_weights: torch.Tensor, topk_ids: torch.Tensor, sorted_token_ids: torch.Tensor, expert_ids: torch.Tensor, num_tokens_post_padded: torch.Tensor, mul_routed_weight: bool, top_k: int, config: Dict[str, Any], compute_type: tl.dtype, use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, block_shape: Optional[List[int]], no_combine: bool) -> None
get_config_file_name(E: int, N: int, dtype: Optional[str], block_shape: Optional[int]) -> str
get_moe_configs(E: int, N: int, dtype: Optional[str], block_n: Optional[int], block_k: Optional[int]) -> Optional[Dict[int, Any]]
get_default_config(M: int, E: int, N: int, K: int, topk: int, dtype: Optional[str], is_marlin: bool, block_shape: Optional[List[int]]) -> Dict[str, int]
try_get_optimal_moe_config(w1_shape: Tuple[int, ...], w2_shape: Tuple[int, ...], top_k: int, dtype: Optional[str], M: int, is_marlin: bool, block_shape: Optional[List[int]])
get_config_dtype_str(dtype: torch.dtype, use_int8_w8a16: Optional[bool], use_int4_w4a16: Optional[bool], use_fp8_w8a8: Optional[bool], use_int8_w8a8: Optional[bool])
inplace_fused_experts(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]], routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_limit: Optional[float]) -> None
inplace_fused_experts_fake(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]], routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_limit: Optional[float]) -> None
outplace_fused_experts(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]], no_combine: bool, routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_limit: Optional[float]) -> torch.Tensor
outplace_fused_experts_fake(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]], no_combine: bool, routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_limit: Optional[float]) -> torch.Tensor
fused_experts(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]])
moe_sum_reduce_triton(input: torch.Tensor, output: torch.Tensor, routed_scaling_factor: float)
moe_sum_reduce_torch_compile(x, out, routed_scaling_factor)
swiglu_with_alpha_and_limit(x, gemm1_alpha, gemm1_limit)
fused_experts_impl(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], inplace: bool, activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]], no_combine: bool, routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_limit: Optional[float])
fused_moe(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]]) -> torch.Tensor

# python/sglang/srt/layers/moe/fused_moe_triton/layer.py
  FusedMoE.__init__(num_experts: int, hidden_size: int, intermediate_size: int, layer_id: int, top_k: Optional[int], num_fused_shared_experts: int, params_dtype: Optional[torch.dtype], reduce_results: bool, quant_config: Optional[QuantizationConfig], prefix: str, activation: str, apply_router_weight_on_input: bool, use_presharded_weights: bool, inplace: bool, no_combine: bool, routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_clamp_limit: Optional[float], use_weight_loader_fused: bool, with_bias)
  FusedMoE.weight_loader(param: torch.nn.Parameter, loaded_weight: torch.Tensor, weight_name: str, shard_id: str, expert_id: Optional[int]) -> None
  FusedMoE.weight_loader_fused(param: torch.nn.Parameter, loaded_weight: torch.Tensor, weight_name: str, shard_id: str) -> None
  FusedMoE.forward(hidden_states: torch.Tensor, topk_output: TopKOutput)
  FusedMoE.make_expert_params_mapping(cls, ckpt_gate_proj_name: str, ckpt_down_proj_name: str, ckpt_up_proj_name: str, num_experts: int) -> List[Tuple[str, str, int, str]]
  FusedMoE.make_expert_params_mapping_fused(cls, ckpt_gate_up_proj_name: str, ckpt_down_proj_name: str, ckpt_gate_up_proj_bias_name: str, ckpt_down_proj_bias_name: str)
  FusedMoE.make_expert_params_mapping_fused_mxfp4(cls, ckpt_gate_up_proj_name: str, ckpt_down_proj_name: str, ckpt_gate_up_proj_bias_name: str, ckpt_down_proj_bias_name: str, ckpt_gate_up_proj_scale_name: str, ckpt_down_proj_scale_name: str)
  FusedMoE.make_expert_input_scale_params_mapping(cls, num_experts: int) -> List[Tuple[str, str, int, str]]
  FusedMoE.should_fuse_routed_scaling_factor_in_topk()
  FlashInferFusedMoE.__init__()
  FlashInferFusedMoE.forward(hidden_states: torch.Tensor, topk_output: TopKOutput)
  FlashInferFP4MoE.__init__()
  FlashInferFP4MoE.forward(hidden_states: torch.Tensor, topk_output: TopKOutput)
get_fused_moe_impl_class()

# python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py
quantize(w, dtype, dev)
triton_kernel_moe_forward(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, per_channel_quant: bool, global_num_experts: int, expert_map: Optional[torch.Tensor], w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[list[int]]) -> torch.Tensor
triton_kernel_fused_experts(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, routing_data: RoutingData, gather_indx: GatherIndx, scatter_indx: ScatterIndx, inplace: bool, activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, per_channel_quant: bool, global_num_experts: int, expert_map: Optional[torch.Tensor], w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[list[int]]) -> torch.Tensor
triton_kernel_moe_with_bias_forward(hidden_states: torch.Tensor, w1: torch.Tensor, w1_pcg, b1: torch.Tensor, w2: torch.Tensor, w2_pcg, b2: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig, use_fp8_w8a8: bool, per_channel_quant: bool, global_num_experts: int, expert_map: Optional[torch.Tensor], w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[list[int]]) -> torch.Tensor
triton_kernel_fused_experts_with_bias(hidden_states: torch.Tensor, w1: torch.Tensor, w1_pcg, b1: torch.Tensor, w2: torch.Tensor, w2_pcg, b2: torch.Tensor, routing_data: RoutingData, gather_indx: GatherIndx, scatter_indx: ScatterIndx, inplace: bool, activation: str, use_fp8_w8a8: bool, per_channel_quant: bool, global_num_experts: int, expert_map: Optional[torch.Tensor], w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[list[int]], gemm1_alpha: Optional[float], gemm1_clamp_limit: Optional[float]) -> torch.Tensor

# python/sglang/srt/layers/moe/rocm_moe_utils.py
rocm_aiter_asm_moe_tkw1_impl(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, fc1_scale: Optional[torch.Tensor], fc2_scale: Optional[torch.Tensor], fc1_smooth_scale: Optional[torch.Tensor], fc2_smooth_scale: Optional[torch.Tensor], a16: bool, per_tensor_quant_scale: Optional[torch.Tensor], expert_mask: Optional[torch.Tensor], activation_method: int) -> torch.Tensor
rocm_aiter_asm_moe_tkw1_fake(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, fc1_scale: Optional[torch.Tensor], fc2_scale: Optional[torch.Tensor], fc1_smooth_scale: Optional[torch.Tensor], fc2_smooth_scale: Optional[torch.Tensor], a16: bool, per_tensor_quant_scale: Optional[torch.Tensor], expert_mask: Optional[torch.Tensor], activation_method: int) -> torch.Tensor
rocm_fused_experts_tkw1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[list[int]]) -> torch.Tensor

# python/sglang/srt/layers/moe/router.py
fused_moe_router_kernel(input_ptr, moe_router_weight_ptr, topk_weights_ptr, topk_ids_ptr, correction_bias_ptr, is_correction_bias: tl.constexpr, num_experts: tl.constexpr, topk: tl.constexpr, moe_softcapping: tl.constexpr, moe_renormalize: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)
fused_moe_router_impl(x: torch.Tensor, router_weight: torch.Tensor, topk: int, moe_softcapping: float, correction_bias: Optional[torch.Tensor])
fused_moe_router_large_bs_kernel(a_ptr, b_ptr, topk_weights_ptr, topk_ids_ptr, bs, num_experts: tl.constexpr, topk: tl.constexpr, moe_softcapping: tl.constexpr, moe_renormalize: tl.constexpr, K: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, stride_am: tl.constexpr, stride_bn: tl.constexpr)
fused_moe_router_large_bs_impl(x: torch.Tensor, router_weight: torch.Tensor, topk: int, moe_softcapping: float, BLOCK_SIZE_M: int, BLOCK_SIZE_N: int, BLOCK_SIZE_K: int)
fused_moe_router_shim(moe_softcapping, hidden_states, gating_output, topk, renormalize, correction_bias: Optional[torch.Tensor])
  FusedMoeRouter.__init__(router_linear, topk, moe_softcapping) -> None
  FusedMoeRouter.__call__()
  FusedMoeRouter.forward(x: torch.Tensor, residual: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
  FusedMoeRouter.forward_cuda(x: torch.Tensor, autotune) -> Tuple[torch.Tensor, torch.Tensor]
  FusedMoeRouter.forward_vllm(x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]

# python/sglang/srt/layers/moe/token_dispatcher/base_dispatcher.py
  DispatchOutputChecker.format_is_standard(dispatch_output: DispatchOutput) -> TypeGuard[StandardDispatchOutput]
  DispatchOutputChecker.format_is_deepep_normal(dispatch_output: DispatchOutput) -> TypeGuard[DeepEPNormalOutput]
  DispatchOutputChecker.format_is_deepep_ll(dispatch_output: DispatchOutput) -> TypeGuard[DeepEPLLOutput]
  DispatchOutputChecker.format_is_deepep(dispatch_output: DispatchOutput) -> TypeGuard[Union[DeepEPNormalOutput, DeepEPLLOutput]]
  DispatchOutputChecker.format_is_ascent_ll(dispatch_output: DispatchOutput) -> TypeGuard[AscendDeepEPLLOutput]
  DispatchOutputFormat.is_standard() -> bool
  DispatchOutputFormat.is_deepep_normal() -> bool
  DispatchOutputFormat.is_deepep_ll() -> bool
  DispatchOutputFormat.is_deepep() -> bool
  DispatchOutputFormat.is_ascent_ll() -> bool
  DispatchOutput.format() -> DispatchOutputFormat
  BaseDispatcher.dispatch() -> DispatchOutput
  BaseDispatcher.combine() -> torch.Tensor

# python/sglang/srt/layers/moe/token_dispatcher/deepep.py
  DeepEPNormalOutput.format() -> DispatchOutputFormat
  DeepEPLLOutput.format() -> DispatchOutputFormat
  AscendDeepEPLLOutput.format() -> DispatchOutputFormat
  DeepEPBuffer.get_deepep_buffer(cls, group: dist.ProcessGroup, hidden_size: int, param_bytes: int, deepep_mode: DeepEPMode, num_max_dispatch_tokens_per_rank: int, num_experts: int)
  DeepEPBuffer.clean_buffer(cls)
  DeepEPBuffer.set_dispatch_mode_as_normal(cls)
  DeepEPBuffer.set_dispatch_mode_as_low_latency(cls)
  DeepEPConfig.__init__()
  DeepEPConfig.get_instance(cls)
  _DeepEPDispatcherImplBase.__init__(group: torch.distributed.ProcessGroup, router_topk: int, permute_fusion: bool, num_experts: int, num_local_experts: int, hidden_size: int, params_dtype: torch.dtype, deepep_mode: DeepEPMode)
  _DeepEPDispatcherImplBase.dispatch_a(hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)
  _DeepEPDispatcherImplBase.dispatch_b()
  _DeepEPDispatcherImplBase.combine_a(hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)
  _DeepEPDispatcherImplBase.combine_b()
  _DeepEPDispatcherImplNormal.__init__(async_finish: bool)
  _DeepEPDispatcherImplNormal.dispatch_a(hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)
  _DeepEPDispatcherImplNormal.dispatch_b(hidden_states, topk_idx, topk_weights, previous_event)
  _DeepEPDispatcherImplNormal.combine_a(hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)
  _DeepEPDispatcherImplNormal.combine_b(output, previous_event)
  _DeepEPDispatcherImplLowLatency.__init__(return_recv_hook: bool)
  _DeepEPDispatcherImplLowLatency.dispatch_a(hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)
  _DeepEPDispatcherImplLowLatency.dispatch_b(hidden_states, topk_idx, topk_weights, masked_m, expected_m, event, hook)
  _DeepEPDispatcherImplLowLatency.combine_a(hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)
  _DeepEPDispatcherImplLowLatency.combine_b(hidden_states, event, hook)
  DeepEPDispatcher.__init__(group: torch.distributed.ProcessGroup, router_topk: int, permute_fusion: bool, num_experts: int, num_local_experts: int, hidden_size: int, params_dtype: torch.dtype, deepep_mode: DeepEPMode, async_finish: bool, return_recv_hook: bool)
  DeepEPDispatcher.dispatch() -> DispatchOutput
  DeepEPDispatcher.dispatch_a(hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)
  DeepEPDispatcher.dispatch_b()
  DeepEPDispatcher.combine() -> Tuple
  DeepEPDispatcher.combine_a(hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)
  DeepEPDispatcher.combine_b()

# python/sglang/srt/layers/moe/token_dispatcher/standard.py
  StandardDispatchOutput.format() -> DispatchOutputFormat

# python/sglang/srt/layers/moe/topk.py
  TopKOutputChecker.format_is_standard(topk_output: TopKOutput) -> TypeGuard[StandardTopKOutput]
  TopKOutputChecker.format_is_triton_kernel(topk_output: TopKOutput) -> TypeGuard[TritonKernelTopKOutput]
  TopKOutputChecker.format_is_bypassed(topk_output: TopKOutput) -> TypeGuard[BypassedTopKOutput]
  TopKOutputFormat.is_standard() -> bool
  TopKOutputFormat.is_triton_kernel() -> bool
  TopKOutputFormat.is_bypassed() -> bool
  TopKOutput.format() -> TopKOutputFormat
  StandardTopKOutput.format() -> TopKOutputFormat
  TritonKernelTopKOutput.format() -> TopKOutputFormat
  BypassedTopKOutput.format() -> TopKOutputFormat
  TopK.__init__(top_k: int)
  TopK.forward_native(hidden_states: torch.Tensor, router_logits: torch.Tensor) -> TopKOutput
  TopK.forward_cuda(hidden_states: torch.Tensor, router_logits: torch.Tensor) -> TopKOutput
  TopK.forward_cpu(hidden_states: torch.Tensor, router_logits: torch.Tensor) -> TopKOutput
  TopK.forward_npu(hidden_states: torch.Tensor, router_logits: torch.Tensor) -> TopKOutput
  TopK.empty_topk_output(device: torch.device) -> TopKOutput
fused_topk_torch_native(hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool, correction_bias: torch.Tensor)
fused_topk_cpu(hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool, num_token_non_padded: Optional[torch.Tensor], expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo], correction_bias: torch.Tensor)
apply_topk_weights_cpu(need_apply, topk_weights, inputs)
fused_topk(hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool, num_token_non_padded: Optional[torch.Tensor], expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo])
grouped_topk_gpu(hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool, num_expert_group: Optional[int], topk_group: Optional[int], num_fused_shared_experts: int, routed_scaling_factor: Optional[float], num_token_non_padded: Optional[torch.Tensor], expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo], apply_routed_scaling_factor_on_output: Optional[bool])
grouped_topk_cpu(hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool, num_expert_group: Optional[int], topk_group: Optional[int], num_fused_shared_experts: int, routed_scaling_factor: Optional[float], num_token_non_padded: Optional[torch.Tensor], expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo], apply_routed_scaling_factor_on_output: Optional[bool])
biased_grouped_topk_impl(hidden_states: torch.Tensor, gating_output: torch.Tensor, correction_bias: torch.Tensor, topk: int, renormalize: bool, num_expert_group: Optional[int], topk_group: Optional[int], num_fused_shared_experts: int, routed_scaling_factor: Optional[float], num_token_non_padded: Optional[torch.Tensor], expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo], apply_routed_scaling_factor_on_output: Optional[bool])
is_power_of_two(n)
biased_grouped_topk_gpu(hidden_states: torch.Tensor, gating_output: torch.Tensor, correction_bias: torch.Tensor, topk: int, renormalize: bool, num_expert_group: Optional[int], topk_group: Optional[int], num_fused_shared_experts: int, routed_scaling_factor: Optional[float], num_token_non_padded: Optional[torch.Tensor], expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo], apply_routed_scaling_factor_on_output: Optional[bool])
biased_grouped_topk_cpu(hidden_states: torch.Tensor, gating_output: torch.Tensor, correction_bias: torch.Tensor, topk: int, renormalize: bool, num_expert_group: Optional[int], topk_group: Optional[int], compiled: bool, num_fused_shared_experts: int, routed_scaling_factor: Optional[float], num_token_non_padded: Optional[torch.Tensor], expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo], apply_routed_scaling_factor_on_output: Optional[bool])
select_experts(hidden_states: torch.Tensor, router_logits: torch.Tensor, topk_config: TopKConfig) -> StandardTopKOutput

# python/sglang/srt/layers/moe/utils.py
  MoeA2ABackend.is_none()
  MoeA2ABackend.is_deepep()
  MoeRunnerBackend.is_auto()
  MoeRunnerBackend.is_triton()
  MoeRunnerBackend.is_triton_kernel()
  MoeRunnerBackend.is_flashinfer_trtllm()
  MoeRunnerBackend.is_flashinfer_cutlass()
  MoeRunnerBackend.is_flashinfer_mxfp4()
  DeepEPMode.enable_normal() -> bool
  DeepEPMode.enable_low_latency() -> bool
  DeepEPMode.resolve(is_extend_in_batch: bool) -> DeepEPMode
  DeepEPMode.is_normal() -> bool
  DeepEPMode.is_low_latency() -> bool
  DeepEPMode.is_auto() -> bool
initialize_moe_config(server_args: ServerArgs)
get_moe_a2a_backend() -> MoeA2ABackend
get_moe_runner_backend() -> MoeRunnerBackend
get_deepep_mode() -> DeepEPMode
get_deepep_config() -> str
is_tbo_enabled() -> bool
get_tbo_token_distribution_threshold() -> float
should_use_flashinfer_trtllm_moe()
should_use_flashinfer_cutlass_moe_fp4_allgather()

# python/sglang/srt/layers/multimodal.py
hash_tiles32_kernel_blocked(in_ptr, out_ptr, n_u32, seed1, seed2, FM_C1: tl.constexpr, FM_C2: tl.constexpr, POS_A: tl.constexpr, POS_B: tl.constexpr, TILE: tl.constexpr, BLOCK: tl.constexpr, USE_CG: tl.constexpr)
add_tree_reduce_u64_kernel(in_ptr, out_ptr, n_elems, CHUNK: tl.constexpr)
gpu_tensor_hash(tensor: torch.Tensor) -> int

# python/sglang/srt/layers/parameter.py
  BasevLLMParameter.__new__(cls, data: torch.Tensor)
  BasevLLMParameter.__init__(data: torch.Tensor, weight_loader: Callable)
  BasevLLMParameter.weight_loader()
  BasevLLMParameter.load_column_parallel_weight(loaded_weight: torch.Tensor)
  BasevLLMParameter.load_row_parallel_weight(loaded_weight: torch.Tensor)
  BasevLLMParameter.load_merged_column_weight(loaded_weight: torch.Tensor)
  BasevLLMParameter.load_qkv_weight(loaded_weight: torch.Tensor)
  _ColumnvLLMParameter.__init__(output_dim: int)
  _ColumnvLLMParameter.output_dim()
  _ColumnvLLMParameter.load_column_parallel_weight(loaded_weight: torch.Tensor, tp_rank: int, use_presharded_weights: bool)
  _ColumnvLLMParameter.load_merged_column_weight(loaded_weight: torch.Tensor)
  _ColumnvLLMParameter.load_qkv_weight(loaded_weight: torch.Tensor, tp_rank: int, use_presharded_weights: bool)
  RowvLLMParameter.__init__(input_dim: int)
  RowvLLMParameter.input_dim()
  RowvLLMParameter.load_row_parallel_weight(loaded_weight: torch.Tensor, tp_rank: int, use_presharded_weights: bool)
  PerTensorScaleParameter.__init__()
  PerTensorScaleParameter.load_row_parallel_weight()
  PerTensorScaleParameter.load_merged_column_weight()
  PerTensorScaleParameter.load_qkv_weight()
  PerTensorScaleParameter.load_column_parallel_weight()
  PackedColumnParameter.__init__(packed_factor: Union[int, Fraction], packed_dim: int, marlin_tile_size: Optional[int])
  PackedColumnParameter.packed_dim()
  PackedColumnParameter.packed_factor()
  PackedColumnParameter.marlin_tile_size()
  PackedColumnParameter.adjust_shard_indexes_for_packing(shard_size, shard_offset)
  PackedvLLMParameter.__init__(packed_factor: Union[int, Fraction], packed_dim: int, marlin_tile_size: Optional[int])
  PackedvLLMParameter.packed_dim()
  PackedvLLMParameter.packed_factor()
  PackedvLLMParameter.marlin_tile_size()
  PackedvLLMParameter.adjust_shard_indexes_for_packing(shard_size, shard_offset)
permute_param_layout_(param: BasevLLMParameter, input_dim: int, output_dim: int) -> BasevLLMParameter

# python/sglang/srt/layers/pooler.py
  Pooler.__init__(pooling_type: PoolingType, normalize: bool)
  Pooler.forward(hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> EmbeddingPoolerOutput
  CrossEncodingPooler.__init__(config: PretrainedConfig, classifier: nn.Module, pooler: Optional[nn.Module])
  CrossEncodingPooler.forward(hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> EmbeddingPoolerOutput

# python/sglang/srt/layers/quantization/__init__.py
  DummyConfig.override_quantization_method()
get_quantization_config(quantization: str) -> Type[QuantizationConfig]
monkey_patch_isinstance_for_vllm_base_layer(reverse: bool)
monkey_patch_moe_apply(class_obj: 'FusedMoEMethodBase')
monkey_patch_quant_configs()

# python/sglang/srt/layers/quantization/awq.py
is_layer_skipped_awq(prefix: str, modules_to_not_convert: List[str])
  AWQConfig.__init__(weight_bits: int, group_size: int, zero_point: bool, modules_to_not_convert: Optional[List[str]]) -> None
  AWQConfig.__repr__() -> str
  AWQConfig.get_scaled_act_names() -> List[str]
  AWQConfig.get_name() -> str
  AWQConfig.get_supported_act_dtypes() -> List[torch.dtype]
  AWQConfig.get_min_capability(cls) -> int
  AWQConfig.get_config_filenames() -> List[str]
  AWQConfig.from_config(cls, config: Dict[str, Any]) -> AWQConfig
  AWQConfig.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[LinearMethodBase]
  AWQMarlinConfig.__init__(weight_bits: int, group_size: int, zero_point: bool, lm_head_quantized: bool, modules_to_not_convert: Optional[list[str]], full_config: dict[str, Any]) -> None
  AWQMarlinConfig.__repr__() -> str
  AWQMarlinConfig.get_scaled_act_names() -> List[str]
  AWQMarlinConfig.get_name(cls) -> str
  AWQMarlinConfig.get_supported_act_dtypes(cls) -> list[torch.dtype]
  AWQMarlinConfig.get_min_capability(cls) -> int
  AWQMarlinConfig.get_config_filenames(cls) -> list[str]
  AWQMarlinConfig.from_config(cls, config: dict[str, Any]) -> AWQMarlinConfig
  AWQMarlinConfig.override_quantization_method(cls, hf_quant_cfg, user_quant) -> Optional[str]
  AWQMarlinConfig.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  AWQMarlinConfig.is_awq_marlin_compatible(cls, quant_config: dict[str, Any])
  AWQLinearMethod.__init__(quant_config: AWQConfig)
  AWQLinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  AWQLinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  AWQLinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  AWQMarlinLinearMethod.__init__(quant_config: AWQMarlinConfig) -> None
  AWQMarlinLinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype) -> None
  AWQMarlinLinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  AWQMarlinLinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  AWQMoEMethod.__init__(quant_config: AWQMarlinConfig)
  AWQMoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)
  AWQMoEMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  AWQMoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor

# python/sglang/srt/layers/quantization/awq_triton.py
awq_dequantize_kernel(qweight_ptr, scales_ptr, zeros_ptr, group_size, result_ptr, num_cols, num_rows, BLOCK_SIZE_X: tl.constexpr, BLOCK_SIZE_Y: tl.constexpr)
awq_gemm_kernel(a_ptr, b_ptr, c_ptr, zeros_ptr, scales_ptr, M, N, K, group_size, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, SPLIT_K: tl.constexpr)
awq_dequantize_triton(qweight: torch.Tensor, scales: torch.Tensor, zeros: torch.Tensor, block_size_x: int, block_size_y: int) -> torch.Tensor
awq_gemm_triton(input: torch.Tensor, qweight: torch.Tensor, scales: torch.Tensor, qzeros: torch.Tensor, split_k_iters: int, block_size_m: int, block_size_n: int, block_size_k: int) -> torch.Tensor

# python/sglang/srt/layers/quantization/base_config.py
  QuantizeMethodBase.create_weights(layer: torch.nn.Module)
  QuantizeMethodBase.apply(layer: torch.nn.Module) -> torch.Tensor
  QuantizeMethodBase.process_weights_after_loading(layer: nn.Module) -> None
  LinearMethodBase.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  LinearMethodBase.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  FusedMoEMethodBase.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)
  FusedMoEMethodBase.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  QuantizationConfig.__init__()
  QuantizationConfig.get_name() -> str
  QuantizationConfig.get_supported_act_dtypes() -> List[torch.dtype]
  QuantizationConfig.get_min_capability(cls) -> int
  QuantizationConfig.get_config_filenames() -> List[str]
  QuantizationConfig.from_config(cls, config: Dict[str, Any]) -> 'QuantizationConfig'
  QuantizationConfig.override_quantization_method(cls, hf_quant_cfg, user_quant) -> Optional[str]
  QuantizationConfig.get_from_keys(config: Dict[str, Any], keys: List[str]) -> Any
  QuantizationConfig.get_from_keys_or(config: Dict[str, Any], keys: List[str], default: Any) -> Any
  QuantizationConfig.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  QuantizationConfig.get_scaled_act_names() -> List[str]
method_has_implemented_embedding(method_class: Type[QuantizeMethodBase]) -> bool

# python/sglang/srt/layers/quantization/blockwise_int8.py
  BlockInt8Config.__init__(is_checkpoint_int8_serialized: bool, activation_scheme: str, ignored_layers: Optional[List[str]], weight_block_size: List[int]) -> None
  BlockInt8Config.get_name(cls) -> str
  BlockInt8Config.get_supported_act_dtypes(cls) -> List[torch.dtype]
  BlockInt8Config.get_min_capability(cls) -> int
  BlockInt8Config.get_config_filenames(cls) -> List[str]
  BlockInt8Config.from_config(cls, config: Dict[str, Any]) -> BlockInt8Config
  BlockInt8Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  BlockInt8Config.get_scaled_act_names() -> List[str]
  BlockInt8LinearMethod.__init__(quant_config: BlockInt8Config)
  BlockInt8LinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  BlockInt8LinearMethod.process_weights_after_loading(layer: Module) -> None
  BlockInt8LinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  BlockInt8MoEMethod.__init__(quant_config: BlockInt8Config)
  BlockInt8MoEMethod.create_weights(layer: Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)
  BlockInt8MoEMethod.process_weights_after_loading(layer: Module) -> None
  BlockInt8MoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor

# python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py
  DeviceCapability.as_version_str() -> str
  DeviceCapability.to_int() -> int
  CompressedTensorsConfig.__init__(target_scheme_map: Dict[str, Any], ignore: List[str], quant_format: str, sparsity_scheme_map: Dict[str, SparsityCompressionConfig], sparsity_ignore_list: List[str], kv_cache_scheme: Optional[Dict[str, Any]], config: Optional[Dict[str, Any]], packed_modules_mapping: Dict[str, List[str]])
  CompressedTensorsConfig.get_linear_method() -> CompressedTensorsLinearMethod
  CompressedTensorsConfig.get_supported_act_dtypes(cls) -> List[torch.dtype]
  CompressedTensorsConfig.get_min_capability(cls) -> int
  CompressedTensorsConfig.get_name() -> str
  CompressedTensorsConfig.get_scaled_act_names() -> List[str]
  CompressedTensorsConfig.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  CompressedTensorsConfig.from_config(cls, config: Dict[str, Any]) -> CompressedTensorsConfig
  CompressedTensorsConfig.get_config_filenames(cls) -> List[str]
  CompressedTensorsConfig.get_scheme(layer: torch.nn.Module, layer_name: Optional[str]) -> Optional[CompressedTensorsScheme]
  CompressedTensorsConfig.get_cache_scale(name: str) -> Optional[str]
  CompressedTensorsConfig.supports_cutlass_24(weight_quant: Optional[QuantizationArgs], input_quant: Optional[QuantizationArgs], sparsity_scheme: Optional[SparsityCompressionConfig]) -> bool
  CompressedTensorsLinearMethod.__init__(quantization_config: CompressedTensorsConfig)
  CompressedTensorsLinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  CompressedTensorsLinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  CompressedTensorsLinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])

# python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors_moe.py
  CompressedTensorsMoEMethod.__new__(cls)
  CompressedTensorsMoEMethod.get_moe_method(quant_config: CompressedTensorsConfig) -> 'CompressedTensorsMoEMethod'
  CompressedTensorsW8A8Fp8MoEMethod.__init__(quant_config: CompressedTensorsConfig)
  CompressedTensorsW8A8Fp8MoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)
  CompressedTensorsW8A8Fp8MoEMethod.process_weights_after_loading(layer: FusedMoE) -> None
  CompressedTensorsW8A8Fp8MoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  CompressedTensorsWNA16MoEMethod.__init__(quant_config: CompressedTensorsConfig)
  CompressedTensorsWNA16MoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)
  CompressedTensorsWNA16MoEMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  CompressedTensorsWNA16MoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor

# python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py
  CompressedTensorsScheme.get_min_capability(cls) -> int
  CompressedTensorsScheme.create_weights()
  CompressedTensorsScheme.apply_weights(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
  CompressedTensorsScheme.process_weights_after_loading(layer: torch.nn.Module)

# python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py
apply_fp8_marlin_linear()
prepare_fp8_layer_for_marlin()
  CompressedTensorsW8A16Fp8.__init__(strategy: str, is_static_input_scheme: bool)
  CompressedTensorsW8A16Fp8.get_min_capability(cls) -> int
  CompressedTensorsW8A16Fp8.process_weights_after_loading(layer) -> None
  CompressedTensorsW8A16Fp8.create_weights(layer: torch.nn.Module, input_size: int, output_partition_sizes: List[int], input_size_per_partition: int, params_dtype: torch.dtype, weight_loader: Callable)
  CompressedTensorsW8A16Fp8.apply_weights(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor

# python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
  CompressedTensorsW8A8Fp8.__init__(strategy: str, is_static_input_scheme: bool)
  CompressedTensorsW8A8Fp8.get_min_capability(cls) -> int
  CompressedTensorsW8A8Fp8.process_weights_after_loading(layer) -> None
  CompressedTensorsW8A8Fp8.create_weights(layer: torch.nn.Module, output_partition_sizes: List[int], input_size_per_partition: int, params_dtype: torch.dtype, weight_loader: Callable)
  CompressedTensorsW8A8Fp8.apply_weights(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor

# python/sglang/srt/layers/quantization/compressed_tensors/utils.py
is_activation_quantization_format(format: str) -> bool
should_ignore_layer(layer_name: Optional[str], ignore: Iterable[str], fused_mapping: Mapping[str, List[str]]) -> bool
check_equal_or_regex_match(layer_name: str, targets: Iterable[str]) -> bool
find_matched_target(layer_name: Optional[str], module: Module, targets: Iterable[str], fused_mapping: Mapping[str, List[str]]) -> str

# python/sglang/srt/layers/quantization/deep_gemm_wrapper/compile_utils.py
update_deep_gemm_config(gpu_id: int, server_args: ServerArgs)
  _BaseWarmupExecutor.create(kernel_type: DeepGemmKernelType)
  _BaseWarmupExecutor.execute(m)
  _NormalWarmupExecutor.__init__(max_m: int, n: int, k: int, num_groups: int)
  _NormalWarmupExecutor.execute(m)
  _GroupedContWarmupExecutor.__init__(max_m: int, n: int, k: int, num_groups: int)
  _GroupedContWarmupExecutor.execute(m)
  _GroupedMaskedWarmupExecutor.__init__(max_m: int, n: int, k: int, num_groups: int)
  _GroupedMaskedWarmupExecutor.execute(m)
deep_gemm_execution_hook(m: int, n: int, k: int, num_groups: int, kernel_type: DeepGemmKernelType)

# python/sglang/srt/layers/quantization/deep_gemm_wrapper/entrypoint.py
grouped_gemm_nt_f8f8bf16_masked(lhs: Tuple[torch.Tensor, torch.Tensor], rhs: Tuple[torch.Tensor, torch.Tensor], out: torch.Tensor, masked_m: torch.Tensor, expected_m: int)
grouped_gemm_nt_f8f8bf16_contig(lhs: Tuple[torch.Tensor, torch.Tensor], rhs: Tuple[torch.Tensor, torch.Tensor], out: torch.Tensor, m_indices: torch.Tensor)
gemm_nt_f8f8bf16(lhs: Tuple[torch.Tensor, torch.Tensor], rhs: Tuple[torch.Tensor, torch.Tensor], out: torch.Tensor)
update_deep_gemm_config(gpu_id: int, server_args: ServerArgs)
configure_deep_gemm_num_sms(num_sms)

# python/sglang/srt/layers/quantization/fp8.py
dummy_func()
  Fp8Config.__init__(is_checkpoint_fp8_serialized: bool, activation_scheme: str, ignored_layers: Optional[List[str]], weight_block_size: List[int]) -> None
  Fp8Config.get_name(cls) -> str
  Fp8Config.get_supported_act_dtypes(cls) -> List[torch.dtype]
  Fp8Config.get_min_capability(cls) -> int
  Fp8Config.get_config_filenames(cls) -> List[str]
  Fp8Config.from_config(cls, config: Dict[str, Any]) -> Fp8Config
  Fp8Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  Fp8Config.get_scaled_act_names() -> List[str]
  Fp8LinearMethod.__init__(quant_config: Union[Fp8Config, W4AFp8Config])
  Fp8LinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  Fp8LinearMethod.process_weights_after_loading(layer: Module) -> None
  Fp8LinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
get_tile_tokens_dim(num_tokens, top_k, num_experts)
  Fp8MoEMethod.__init__(quant_config: Fp8Config)
  Fp8MoEMethod.create_weights(layer: Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)
  Fp8MoEMethod.process_weights_after_loading(layer: Module) -> None
  Fp8MoEMethod.process_weights_hip_int4(layer: Module)
  Fp8MoEMethod.process_weights_hip_scale_padding(layer: Module)
  Fp8MoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  Fp8MoEMethod.apply_with_router_logits(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  Fp8MoEMethod.maybe_apply_hip_fused_experts(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, activation: str, no_combine: bool) -> Optional[torch.Tensor]
  Fp8KVCacheMethod.__init__(quant_config: Fp8Config)

# python/sglang/srt/layers/quantization/fp8_kernel.py
is_fp8_fnuz() -> bool
deep_gemm_fp8_fp8_bf16_nt(A: torch.Tensor, As: torch.Tensor, B: torch.Tensor, Bs: torch.Tensor, C: torch.Tensor) -> None
deep_gemm_fp8_fp8_bf16_nt_fake(A: torch.Tensor, As: torch.Tensor, B: torch.Tensor, Bs: torch.Tensor, C: torch.Tensor) -> None
per_token_group_quant_8bit(x: torch.Tensor, group_size: int, dst_dtype: torch.dtype, eps: float, column_major_scales: bool, scale_tma_aligned: bool, scale_ue8m0: bool, fuse_silu_and_mul: bool, masked_m: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
create_per_token_group_quant_fp8_output_scale(x_shape, device, group_size, column_major_scales: bool, scale_tma_aligned: bool, scale_ue8m0: bool)
sglang_per_token_group_quant_fp8(x: torch.Tensor, group_size: int, eps: float, column_major_scales: bool, scale_tma_aligned: bool, scale_ue8m0: bool, fuse_silu_and_mul: bool, masked_m: Optional[torch.Tensor])
sglang_per_token_group_quant_8bit(x: torch.Tensor, group_size: int, dst_dtype: torch.dtype, eps: float, column_major_scales: bool, scale_tma_aligned: bool, scale_ue8m0: bool, fuse_silu_and_mul: bool, masked_m: Optional[torch.Tensor])
sglang_per_token_quant_fp8(x: torch.Tensor, dtype: torch.dtype)
static_quant_fp8(x: torch.Tensor, x_s: torch.Tensor, repeat_scale: bool) -> Tuple[torch.Tensor, torch.Tensor]
get_w8a8_block_fp8_configs(N: int, K: int, block_n: int, block_k: int) -> Optional[Dict[int, Any]]
select_w8a8_block_fp8_matmul_kernel(M, N, META)
use_w8a8_block_fp8_matmul_unrolledx4(M, N, META)
select_w8a8_block_fp8_matmul_kernel(M, N, META)
prepare_block_fp8_matmul_inputs(A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype) -> Tuple[int, int, int]
w8a8_block_fp8_matmul_deepgemm(A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype) -> torch.Tensor
w8a8_block_fp8_matmul_triton(A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype) -> torch.Tensor
w8a8_block_fp8_matmul(A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype) -> torch.Tensor
per_tensor_quant_mla_fp8(x: torch.Tensor, x_s_out: torch.Tensor, eps: float) -> Tuple[torch.Tensor, torch.Tensor]
per_token_group_quant_mla_deep_gemm_masked_fp8(x: torch.Tensor, group_size: int, eps: float, dtype: torch.dtype) -> Tuple[torch.Tensor, torch.Tensor]
scaled_fp8_quant(input: torch.Tensor, scale: Optional[torch.Tensor], num_token_padding: Optional[int], use_per_token_if_dynamic: bool) -> tuple[torch.Tensor, torch.Tensor]
scaled_fp8_quant(input: torch.Tensor, scale: Optional[torch.Tensor], num_token_padding: Optional[int], use_per_token_if_dynamic: bool) -> tuple[torch.Tensor, torch.Tensor]
per_token_group_quant_fp8_hopper_moe_mn_major(A: torch.Tensor, expert_offsets: torch.Tensor, problem_sizes: torch.Tensor, group_size: int, expert_tokens_alignment: int) -> Tuple[torch.Tensor, torch.Tensor]
per_group_transpose(a: torch.Tensor, expert_offsets: torch.Tensor, M_ALIGNMENT: int) -> torch.Tensor
is_weak_contiguous(x: torch.Tensor)
scaled_mm_kernel(a_ptr, b_ptr, scale_a_ptr, scale_b_ptr, c_ptr, bias_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, ACCUMULATOR_DTYPE: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_SCALE_A: tl.constexpr, BLOCK_SIZE_SCALE_B: tl.constexpr)
triton_scaled_mm(input: torch.Tensor, weight: torch.Tensor, scale_a: torch.Tensor, scale_b: torch.Tensor, out_dtype: type[torch.dtype], bias: Optional[torch.Tensor], block_size_m: int, block_size_n: int, block_size_k: int, use_heuristic) -> torch.Tensor

# python/sglang/srt/layers/quantization/fp8_utils.py
use_rowwise_torch_scaled_mm()
cutlass_fp8_supported()
normalize_e4m3fn_to_e4m3fnuz(weight: torch.Tensor, weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]
cutlass_block_fp8_supported() -> bool
dispatch_w8a8_block_fp8_linear() -> Callable
flashinfer_gemm_w8a8_block_fp8_linear(input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], bias: Optional[torch.Tensor]) -> torch.Tensor
cutlass_w8a8_block_fp8_linear_with_fallback(input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], bias: Optional[torch.Tensor]) -> torch.Tensor
deepgemm_w8a8_block_fp8_linear_with_fallback(input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], bias: Optional[torch.Tensor]) -> torch.Tensor
aiter_w8a8_block_fp8_linear(input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], bias: Optional[torch.Tensor]) -> torch.Tensor
triton_w8a8_block_fp8_linear(input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], bias: Optional[torch.Tensor]) -> torch.Tensor
dequant_mxfp4(w_block: torch.Tensor, w_scale: torch.Tensor, out_dtype) -> torch.Tensor
input_to_float8(x: torch.Tensor, dtype: torch.dtype) -> Tuple[torch.Tensor, torch.Tensor]
block_quant_to_tensor_quant(x_q_block: torch.Tensor, x_s: torch.Tensor, block_size: List[int]) -> Tuple[torch.Tensor, torch.Tensor]
block_quant_dequant(x_q_block: torch.Tensor, x_s: torch.Tensor, block_size: List[int], dtype: torch.dtype) -> torch.Tensor
requant_weight_ue8m0_inplace(weight, weight_scale_inv, weight_block_size)
per_block_cast_to_fp8(x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
ceil_to_ue8m0(x: torch.Tensor)
channel_quant_to_tensor_quant(x_q_channel: torch.Tensor, x_s: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
apply_fp8_linear(input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], input_scale_ub: Optional[torch.Tensor], bias: Optional[torch.Tensor], cutlass_fp8_supported: bool, use_per_token_if_dynamic: bool, pad_output: Optional[bool], compressed_tensor_quant: bool) -> torch.Tensor
can_auto_enable_marlin_fp8() -> bool

# python/sglang/srt/layers/quantization/fpgemm_fp8.py
  FBGEMMFp8Config.__init__(ignore_list: list[str], input_scale_ub: float)
  FBGEMMFp8Config.get_name(cls) -> str
  FBGEMMFp8Config.get_supported_act_dtypes(cls) -> list[torch.dtype]
  FBGEMMFp8Config.get_min_capability(cls) -> int
  FBGEMMFp8Config.get_config_filenames(cls) -> list[str]
  FBGEMMFp8Config.from_config(cls, config: dict[str, Any]) -> FBGEMMFp8Config
  FBGEMMFp8Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  FBGEMMFp8Config.get_scaled_act_names() -> List[str]
  FBGEMMFp8LinearMethod.__init__(quant_config: FBGEMMFp8Config)
  FBGEMMFp8LinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  FBGEMMFp8LinearMethod.process_weights_after_loading(layer: Module) -> None
  FBGEMMFp8LinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor

# python/sglang/srt/layers/quantization/gptq.py
check_marlin_format(hf_quant_cfg: Dict[str, Any]) -> bool
gptq_marlin_moe_repack(b_q_weight: torch.Tensor, perm: torch.Tensor, size_k: int, size_n: int, num_bits: int) -> torch.Tensor
  GPTQConfig.__init__(weight_bits: int, group_size: int, desc_act: bool, lm_head_quantized: bool, dynamic: Dict[str, Dict[str, Union[int, bool]]]) -> None
  GPTQConfig.__repr__() -> str
  GPTQConfig.get_scaled_act_names() -> List[str]
  GPTQConfig.get_name(cls) -> str
  GPTQConfig.get_supported_act_dtypes(cls) -> List[torch.dtype]
  GPTQConfig.get_min_capability(cls) -> int
  GPTQConfig.get_config_filenames(cls) -> List[str]
  GPTQConfig.from_config(cls, config: Dict[str, Any]) -> GPTQConfig
  GPTQConfig.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[LinearMethodBase]
  GPTQMarlinConfig.__init__(weight_bits: int, group_size: int, desc_act: bool, is_sym: bool, lm_head_quantized: bool, dynamic: Dict[str, Dict[str, Union[int, bool]]], full_config: Dict[str, Any]) -> None
  GPTQMarlinConfig.__repr__() -> str
  GPTQMarlinConfig.get_scaled_act_names() -> List[str]
  GPTQMarlinConfig.get_name(cls) -> str
  GPTQMarlinConfig.get_supported_act_dtypes(cls) -> List[torch.dtype]
  GPTQMarlinConfig.get_min_capability(cls) -> int
  GPTQMarlinConfig.get_config_filenames(cls) -> List[str]
  GPTQMarlinConfig.from_config(cls, config: Dict[str, Any]) -> GPTQMarlinConfig
  GPTQMarlinConfig.override_quantization_method(cls, hf_quant_cfg, user_quant) -> Optional[str]
  GPTQMarlinConfig.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  GPTQMarlinConfig.is_gptq_marlin_compatible(cls, quant_config: Dict[str, Any])
  GPTQLinearMethod.__init__(quant_config: GPTQConfig)
  GPTQLinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  GPTQLinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  GPTQLinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  GPTQMarlinLinearMethod.__init__(quant_config: GPTQMarlinConfig) -> None
  GPTQMarlinLinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype) -> None
  GPTQMarlinLinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  GPTQMarlinLinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  GPTQMarlinMoEMethod.__init__(quant_config: GPTQMarlinConfig) -> None
  GPTQMarlinMoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)
  GPTQMarlinMoEMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  GPTQMarlinMoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor

# python/sglang/srt/layers/quantization/int8_kernel.py
per_token_quant_int8(x, scale_dtype, cal_sum)
per_token_group_quant_int8(x: torch.Tensor, group_size: int, eps: float, dtype: torch.dtype) -> Tuple[torch.Tensor, torch.Tensor]
sglang_per_token_group_quant_int8(x: torch.Tensor, group_size: int, eps: float, dtype: torch.dtype)
get_w8a8_block_int8_configs(N: int, K: int, block_n: int, block_k: int) -> Optional[Dict[int, Any]]
w8a8_block_int8_matmul(A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype) -> torch.Tensor

# python/sglang/srt/layers/quantization/int8_utils.py
apply_w8a8_block_int8_linear(input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], bias: Optional[torch.Tensor]) -> torch.Tensor
input_to_int8(x: torch.Tensor, dtype: torch.dtype) -> Tuple[torch.Tensor, torch.Tensor]
block_dequant(x_q_block: torch.Tensor, x_s: torch.Tensor, block_size: List[int]) -> torch.Tensor

# python/sglang/srt/layers/quantization/kv_cache.py
  BaseKVCacheMethod.__init__(quant_config: QuantizationConfig)
  BaseKVCacheMethod.create_weights(layer: torch.nn.Module)
  BaseKVCacheMethod.apply(layer: torch.nn.Module) -> torch.Tensor
  BaseKVCacheMethod.process_weights_after_loading(layer: RadixAttention) -> None

# python/sglang/srt/layers/quantization/marlin_utils.py
query_marlin_supported_quant_types(has_zp: Optional[bool], include_fp_type: bool, device_capability: Optional[int])
check_marlin_supported(quant_type: ScalarType, group_size: int, has_zp: bool, device_capability: Optional[int]) -> bool
verify_marlin_supported(quant_type: ScalarType, group_size: int, has_zp: bool) -> None
verify_marlin_supports_shape(output_size_per_partition: int, input_size_per_partition: int, input_size: int, group_size: int) -> None
check_marlin_supports_shape(output_size_per_partition: int, input_size_per_partition: int, input_size: int, group_size: int) -> tuple[bool, Optional[str]]
check_marlin_supports_layer(layer: LinearBase, group_size: int) -> bool
check_moe_marlin_supports_layer(layer: FusedMoE, group_size: int) -> bool
marlin_make_workspace(device: torch.device, max_blocks_per_sm: int) -> torch.Tensor
marlin_is_k_full(act_order: bool, is_row_parallel: bool) -> bool
marlin_repeat_scales_on_all_ranks(act_order: bool, group_size: int, is_row_parallel: bool) -> bool
marlin_make_empty_g_idx(device: torch.device) -> torch.Tensor
marlin_make_empty_zp(device: torch.device) -> torch.Tensor
marlin_sort_g_idx(g_idx: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]
get_scale_perms()
marlin_permute_scales(s: torch.Tensor, size_k: int, size_n: int, group_size: int) -> torch.Tensor
marlin_permute_bias(s: torch.Tensor) -> torch.Tensor
marlin_moe_permute_scales(s: torch.Tensor, size_k: int, size_n: int, group_size: int)
marlin_zero_points(zp: torch.Tensor, size_k: int, size_n: int, num_bits: int) -> torch.Tensor
awq_to_marlin_zero_points(q_zp_packed: torch.Tensor, size_k: int, size_n: int, num_bits: int) -> torch.Tensor
moe_awq_to_marlin_zero_points(q_zp_packed: torch.Tensor, size_k: int, size_n: int, num_bits: int)
maybe_warn_marlin_atomic_add(device, dtype)
maybe_warn_marlin_atomic_add_env()
should_use_atomic_add_reduce(m: int, n: int, k: int, device: torch.device, dtype: torch.dtype) -> bool
apply_gptq_marlin_linear(input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, weight_zp: torch.Tensor, g_idx: torch.Tensor, g_idx_sort_indices: torch.Tensor, workspace: torch.Tensor, wtype: ScalarType, output_size_per_partition: int, input_size_per_partition: int, is_k_full: bool, bias: Optional[torch.Tensor], use_fp32_reduce: bool) -> torch.Tensor
apply_awq_marlin_linear(input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, weight_zp: torch.Tensor, g_idx: torch.Tensor, g_idx_sort_indices: torch.Tensor, workspace: torch.Tensor, quant_type: ScalarType, output_size_per_partition: int, input_size_per_partition: int, bias: Optional[torch.Tensor], use_fp32_reduce: bool) -> torch.Tensor
  MarlinConfig.__init__(group_size: int, lm_head_quantized: bool) -> None
  MarlinConfig.__repr__() -> str
  MarlinConfig.get_name(cls) -> str
  MarlinConfig.get_supported_act_dtypes(cls) -> list[torch.dtype]
  MarlinConfig.get_min_capability(cls) -> int
  MarlinConfig.get_config_filenames(cls) -> list[str]
  MarlinConfig.from_config(cls, config: dict[str, Any]) -> 'MarlinConfig'
  MarlinConfig.override_quantization_method(cls, hf_quant_cfg, user_quant) -> Optional[str]
  MarlinConfig.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[MarlinLinearMethod]
  MarlinLinearMethod.__init__(quant_config: MarlinConfig)
  MarlinLinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  MarlinLinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  MarlinLinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor

# python/sglang/srt/layers/quantization/marlin_utils_fp8.py
fp8_fused_exponent_bias_into_scales(scales)
apply_fp8_marlin_linear(input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, workspace: torch.Tensor, size_n: int, size_k: int, bias: Optional[torch.Tensor], use_fp32_reduce: bool) -> torch.Tensor
prepare_fp8_layer_for_marlin(layer: torch.nn.Module, size_k_first: bool) -> None
prepare_moe_fp8_layer_for_marlin(layer: torch.nn.Module, size_k_first: bool) -> None
pack_fp8_to_int32(fp8_tensor: torch.Tensor, size_k_first: bool) -> torch.Tensor
marlin_quant_fp8_torch(weight, group_size)

# python/sglang/srt/layers/quantization/modelopt_quant.py
  ModelOptFp8Config.__init__(is_checkpoint_fp8_serialized: bool, kv_cache_quant_method: Optional[str], exclude_modules: Optional[List[str]]) -> None
  ModelOptFp8Config.get_name(cls) -> str
  ModelOptFp8Config.get_supported_act_dtypes(cls) -> List[torch.dtype]
  ModelOptFp8Config.get_min_capability(cls) -> int
  ModelOptFp8Config.get_config_filenames(cls) -> List[str]
  ModelOptFp8Config.from_config(cls, config: Dict[str, Any]) -> ModelOptFp8Config
  ModelOptFp8Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  ModelOptFp8Config.get_scaled_act_names() -> List[str]
  ModelOptFp8LinearMethod.__init__(quant_config: ModelOptFp8Config)
  ModelOptFp8LinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], params_dtype: torch.dtype) -> None
  ModelOptFp8LinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  ModelOptFp8LinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  ModelOptFp8KVCacheMethod.__init__(quant_config: ModelOptFp8Config)
  ModelOptFp8MoEMethod.__init__(quant_config: ModelOptFp8Config)
  ModelOptFp8MoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)
  ModelOptFp8MoEMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  ModelOptFp8MoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  ModelOptFp4Config.__init__(is_checkpoint_nvfp4_serialized: bool, kv_cache_quant_algo: str, group_size: int, exclude_modules: List[str]) -> None
  ModelOptFp4Config.get_name(cls) -> str
  ModelOptFp4Config.get_supported_act_dtypes(cls) -> List[torch.dtype]
  ModelOptFp4Config.get_min_capability(cls) -> int
  ModelOptFp4Config.get_config_filenames(cls) -> List[str]
  ModelOptFp4Config.from_config(cls, config: Dict[str, Any]) -> ModelOptFp4Config
  ModelOptFp4Config.is_layer_excluded(prefix: str, exclude_modules: list)
  ModelOptFp4Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  ModelOptFp4Config.get_scaled_act_names() -> List[str]
  ModelOptFp4LinearMethod.__init__(quant_config: ModelOptFp4Config)
  ModelOptFp4LinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  ModelOptFp4LinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  ModelOptFp4LinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  ModelOptNvFp4FusedMoEMethod.__init__(quant_config: ModelOptFp4Config)
  ModelOptNvFp4FusedMoEMethod.enable_flashinfer_cutlass_moe() -> bool
  ModelOptNvFp4FusedMoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)
  ModelOptNvFp4FusedMoEMethod.swizzle_blockscale(scale: torch.Tensor)
  ModelOptNvFp4FusedMoEMethod.prepare_static_weights_for_kernel(gemm1_weights, gemm2_weights, gemm1_scales_linear_fp4_bytes, gemm2_scales_linear_fp4_bytes, hidden_size, intermediate_size, num_experts)
  ModelOptNvFp4FusedMoEMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  ModelOptNvFp4FusedMoEMethod.load_up_proj_weight_first() -> bool
  ModelOptNvFp4FusedMoEMethod.apply(layer: FusedMoE, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor

# python/sglang/srt/layers/quantization/moe_wna16.py
get_weight_perm(num_bits: int)
  MoeWNA16Config.__init__(linear_quant_method: str, weight_bits: int, group_size: int, has_zp: bool, lm_head_quantized: bool, modules_to_not_convert: Optional[List[str]], full_config: Dict[str, Any]) -> None
  MoeWNA16Config.get_name(cls) -> str
  MoeWNA16Config.get_supported_act_dtypes(cls) -> List[torch.dtype]
  MoeWNA16Config.get_min_capability(cls) -> int
  MoeWNA16Config.get_config_filenames(cls) -> List[str]
  MoeWNA16Config.get_scaled_act_names() -> List[str]
  MoeWNA16Config.from_config(cls, config: Dict[str, Any]) -> MoeWNA16Config
  MoeWNA16Config.override_quantization_method(cls, hf_quant_cfg, user_quant) -> Optional[str]
  MoeWNA16Config.is_moe_wna16_compatible(cls, quant_config: Dict[str, Any])
  MoeWNA16Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
is_layer_skipped_quant(prefix: str, modules_to_not_convert: List[str])
  MoeWNA16Method.__init__(quant_config: MoeWNA16Config)
  MoeWNA16Method.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)
  MoeWNA16Method.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  MoeWNA16Method.get_weight_loader(layer, weight_loader)

# python/sglang/srt/layers/quantization/mxfp4.py
  Mxfp4Config.__init__(ignored_layers: Optional[list[str]], is_checkpoint_mxfp4_serialized: bool)
  Mxfp4Config.from_config(cls, config)
  Mxfp4Config.get_min_capability(cls) -> int
  Mxfp4Config.get_name(cls) -> str
  Mxfp4Config.get_supported_act_dtypes(cls) -> list[torch.dtype]
  Mxfp4Config.get_config_filenames(cls) -> list[str]
  Mxfp4Config.is_static_cfg()
  Mxfp4Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional['QuantizeMethodBase']
  Mxfp4Config.get_scaled_act_names() -> List[str]
  Mxfp4MoEMethod.__init__(prefix: str)
  Mxfp4MoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype, with_bias: bool)
  Mxfp4MoEMethod.process_weights_after_loading(layer)
  Mxfp4MoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  Mxfp4DynamicQuantMoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)
  Mxfp4DynamicQuantMoEMethod.mxfp4_quantize(w)
  Mxfp4DynamicQuantMoEMethod.process_weights_after_loading(layer: Module) -> None
  Mxfp4DynamicQuantMoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor

# python/sglang/srt/layers/quantization/mxfp4_tensor.py
  MXFP4QuantizeUtil.quantize(cls, input: torch.Tensor, block_size: Optional[int]) -> tuple
  MXFP4QuantizeUtil.dequantize(cls, quantized_data, dtype: torch.dtype, scale, block_sizes)

# python/sglang/srt/layers/quantization/petit.py
  PetitNvFp4Config.__init__(is_checkpoint_nvfp4_serialized: bool, kv_cache_quant_algo: str, group_size: int, exclude_modules: List[str]) -> None
  PetitNvFp4Config.get_name(cls) -> str
  PetitNvFp4Config.get_supported_act_dtypes(cls) -> List[torch.dtype]
  PetitNvFp4Config.get_min_capability(cls) -> int
  PetitNvFp4Config.get_config_filenames(cls) -> List[str]
  PetitNvFp4Config.from_config(cls, config: Dict[str, Any]) -> 'PetitNvFp4Config'
  PetitNvFp4Config.override_quantization_method(cls, hf_quant_cfg, user_quant) -> Optional[str]
  PetitNvFp4Config.is_petit_nvfp4_compatible(cls, quant_config: Dict[str, Any]) -> bool
  PetitNvFp4Config.is_layer_excluded(prefix: str, exclude_modules: list)
  PetitNvFp4Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional['QuantizeMethodBase']
  PetitNvFp4Config.get_scaled_act_names() -> List[str]
  PetitNvFp4LinearMethod.__init__(quant_config: PetitNvFp4Config)
  PetitNvFp4LinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  PetitNvFp4LinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  PetitNvFp4LinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor

# python/sglang/srt/layers/quantization/petit_utils.py
prepare_nvfp4_layer_for_petit(layer: torch.nn.Module) -> None
apply_petit_nvfp4_linear(input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, weight_scale_2: torch.Tensor, size_n: int, size_k: int, bias: Optional[torch.Tensor]) -> torch.Tensor
verify_petit_nvfp4_supported(quant_method: str, group_size: Optional[int]) -> None
prepare_nvfp4_layer_for_petit(layer: torch.nn.Module) -> None
apply_petit_nvfp4_linear(input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, weight_scale_2: torch.Tensor, size_n: int, size_k: int, bias: Optional[torch.Tensor]) -> torch.Tensor

# python/sglang/srt/layers/quantization/qoq.py
  QoQConfig.__init__(weight_bits: int, group_size: int) -> None
  QoQConfig.__repr__() -> str
  QoQConfig.get_supported_act_dtypes(cls) -> List[torch.dtype]
  QoQConfig.get_min_capability(cls) -> int
  QoQConfig.get_name(cls) -> str
  QoQConfig.get_config_filenames(cls) -> List[str]
  QoQConfig.from_config(cls, config: Dict[str, Any]) -> QoQConfig
  QoQConfig.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  QoQConfig.get_scaled_act_names() -> List[str]
  QoQLinearMethod.__init__(quant_config: QoQConfig)
  QoQLinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  QoQLinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  QoQLinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])

# python/sglang/srt/layers/quantization/quark/quark.py
  QuarkConfig.__init__(quant_config: dict[str, Any], kv_cache_group: Optional[list[str]], kv_cache_config: Optional[dict[str, Any]], pack_method: str)
  QuarkConfig.get_linear_method() -> 'QuarkLinearMethod'
  QuarkConfig.get_supported_act_dtypes(cls) -> list[torch.dtype]
  QuarkConfig.get_min_capability(cls) -> int
  QuarkConfig.get_name() -> str
  QuarkConfig.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional['QuantizeMethodBase']
  QuarkConfig.from_config(cls, config: dict[str, Any]) -> 'QuarkConfig'
  QuarkConfig.get_config_filenames(cls) -> list[str]
  QuarkConfig.get_scheme(layer: torch.nn.Module, layer_name: str) -> 'QuarkScheme'
  QuarkConfig.get_scaled_act_names() -> List[str]
  QuarkLinearMethod.__init__(quantization_config: QuarkConfig)
  QuarkLinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  QuarkLinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  QuarkLinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
  QuarkKVCacheMethod.__init__(quant_config: QuarkConfig)
  QuarkKVCacheMethod.validate_kv_cache_config(kv_cache_config: Optional[dict[str, Any]])

# python/sglang/srt/layers/quantization/quark/quark_moe.py
  QuarkMoEMethod.__new__(cls)
  QuarkMoEMethod.get_moe_method(quant_config: 'QuarkConfig', module: torch.nn.Module, layer_name: str) -> 'QuarkMoEMethod'
  QuarkW4A4MXFp4MoEMethod.__init__(weight_config: dict[str, Any], input_config: dict[str, Any])
  QuarkW4A4MXFp4MoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)
  QuarkW4A4MXFp4MoEMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  QuarkW4A4MXFp4MoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor

# python/sglang/srt/layers/quantization/quark/schemes/quark_scheme.py
  QuarkScheme.get_min_capability(cls) -> int
  QuarkScheme.create_weights()
  QuarkScheme.apply_weights(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
  QuarkScheme.process_weights_after_loading(layer: torch.nn.Module)

# python/sglang/srt/layers/quantization/quark/schemes/quark_w4a4_mxfp4.py
  QuarkW4A4MXFP4.__init__(weight_quant_spec: dict[str, Any], input_quant_spec: dict[str, Any])
  QuarkW4A4MXFP4.get_min_capability(cls) -> int
  QuarkW4A4MXFP4.process_weights_after_loading(layer: torch.nn.Module) -> None
  QuarkW4A4MXFP4.create_weights(layer: torch.nn.Module, output_partition_sizes: list[int], input_size_per_partition: int, params_dtype: torch.dtype, weight_loader: Callable)
  QuarkW4A4MXFP4.apply_weights(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor

# python/sglang/srt/layers/quantization/quark/utils.py
deep_compare(dict1: Any, dict2: Any) -> bool
should_ignore_layer(layer_name: Optional[str], ignore: Iterable[str], fused_mapping: Mapping[str, list[str]]) -> bool
check_equal_or_regex_match(layer_name: str, targets: Iterable[str]) -> bool

# python/sglang/srt/layers/quantization/unquant.py
  UnquantizedEmbeddingMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  UnquantizedEmbeddingMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  UnquantizedEmbeddingMethod.embedding(layer: torch.nn.Module, input_: torch.Tensor) -> torch.Tensor
  UnquantizedLinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  UnquantizedLinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  UnquantizedLinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  UnquantizedFusedMoEMethod.__init__(use_triton_kernels: bool)
  UnquantizedFusedMoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype, with_bias: bool)
  UnquantizedFusedMoEMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  UnquantizedFusedMoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  UnquantizedFusedMoEMethod.forward_cuda(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  UnquantizedFusedMoEMethod.forward_cpu(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  UnquantizedFusedMoEMethod.forward_npu(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  UnquantizedFusedMoEMethod.forward_tpu() -> torch.Tensor

# python/sglang/srt/layers/quantization/utils.py
get_scalar_types()
is_layer_skipped(prefix: str, ignored_layers: List[str], fused_mapping: Mapping[str, List[str]]) -> bool
per_tensor_dequantize(tensor: torch.Tensor, inv_scale: Union[float, torch.Tensor]) -> torch.Tensor
all_close_1d(x: torch.Tensor) -> bool
convert_to_channelwise(weight_scale: torch.Tensor, logical_widths: List[int]) -> Tuple[torch.Tensor, torch.Tensor]
requantize_with_max_scale(weight: torch.Tensor, weight_scale: torch.Tensor, logical_widths: List[int]) -> Tuple[torch.Tensor, torch.Tensor]
update_tensor_inplace(old: torch.Tensor, new: torch.Tensor) -> None
replace_parameter(mod: torch.nn.Module, name: str, new: Union[torch.Tensor, torch.nn.Parameter]) -> None
assert_fp8_all_close(a: torch.Tensor, b: torch.Tensor)
override_config(config: QuantizationConfig, prefix: str)
get_dynamic_override(config: QuantizationConfig, layer_name: str, key: Optional[str], default_value: Union[int, bool, None]) -> Union[Dict, int, bool, None]
get_linear_quant_method(config: QuantizationConfig, layer: torch.nn.Module, prefix: str, linear_method_cls: type)
get_pack_factor(num_bits)
permute_rows(q_w: torch.Tensor, w_ref: torch.Tensor, group_size: int, test_perm: Optional[torch.Tensor])
pack_cols(q_w: torch.Tensor, num_bits: int, size_k: int, size_n: int)
pack_rows(q_w: torch.Tensor, num_bits: int, size_k: int, size_n: int)
unpack_cols(packed_q_w: torch.Tensor, num_bits: int, size_k: int, size_n: int)
quantize_weights(w: torch.Tensor, quant_type: ScalarType, group_size: Optional[int], zero_points: bool, ref_zero_points_after_scales: bool)
gptq_quantize_weights(w: torch.Tensor, quant_type: ScalarType, group_size: int, act_order: bool, test_perm: Optional[torch.Tensor])
sort_weights(q_w: torch.Tensor, g_idx: torch.Tensor)

# python/sglang/srt/layers/quantization/w4afp8.py
  W4AFp8Config.__init__(is_checkpoint_fp8_serialized: bool, is_checkpoint_w4afp8_serialized: bool, linear_activation_scheme: str, moe_activation_scheme: str, ignored_layers: Optional[List[str]], weight_block_size: Optional[List[int]], group_size: int) -> None
  W4AFp8Config.get_name(cls) -> str
  W4AFp8Config.get_supported_act_dtypes(cls) -> List[torch.dtype]
  W4AFp8Config.get_min_capability(cls) -> int
  W4AFp8Config.get_config_filenames(cls) -> List[str]
  W4AFp8Config.from_config(cls, config: Dict[str, Any]) -> W4AFp8Config
  W4AFp8Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  W4AFp8Config.get_scaled_act_names() -> List[str]
  W4AFp8MoEMethod.__init__(quant_config: W4AFp8Config)
  W4AFp8MoEMethod.create_weights(layer: EPMoE, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)
  W4AFp8MoEMethod.process_weights_after_loading(layer: Module) -> None
  W4AFp8MoEMethod.apply(layer: EPMoE, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor

# python/sglang/srt/layers/quantization/w8a8_fp8.py
  W8A8Fp8Config.__init__(is_checkpoint_fp8_serialized: bool)
  W8A8Fp8Config.get_supported_act_dtypes(cls) -> List[torch.dtype]
  W8A8Fp8Config.get_min_capability(cls) -> int
  W8A8Fp8Config.get_name() -> str
  W8A8Fp8Config.get_config_filenames(cls) -> List[str]
  W8A8Fp8Config.from_config(cls, config: Dict[str, Any]) -> W8A8Fp8Config
  W8A8Fp8Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  W8A8Fp8Config.get_scaled_act_names() -> List[str]
  W8A8Fp8LinearMethod.__init__(quantization_config: W8A8Fp8Config)
  W8A8Fp8LinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  W8A8Fp8LinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  W8A8Fp8LinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
  W8A8FP8MoEMethod.__init__(quant_config: W8A8Fp8Config)
  W8A8FP8MoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)
  W8A8FP8MoEMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  W8A8FP8MoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor

# python/sglang/srt/layers/quantization/w8a8_int8.py
npu_wrapper_rmsnorm_init(func)
npu_wrapper_rmsnorm_forward(func)
npu_fused_experts(hidden_states: torch.Tensor, w13: torch.Tensor, w13_scale: torch.Tensor, w2: torch.Tensor, w2_scale: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, top_k: int)
  W8A8Int8Config.__init__(quant_config: Dict[str, Any])
  W8A8Int8Config.get_supported_act_dtypes(cls) -> List[torch.dtype]
  W8A8Int8Config.get_min_capability(cls) -> int
  W8A8Int8Config.get_name() -> str
  W8A8Int8Config.get_config_filenames(cls) -> List[str]
  W8A8Int8Config.from_config(cls, config: Dict[str, Any]) -> W8A8Int8Config
  W8A8Int8Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  W8A8Int8Config.is_layer_skipped(prefix: str, fused_mapping: Mapping[str, List[str]])
  W8A8Int8Config.get_scaled_act_names() -> List[str]
  W8A8Int8LinearMethod.__init__(quantization_config: W8A8Int8Config)
  W8A8Int8LinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  W8A8Int8LinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  W8A8Int8LinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
  W8A8Int8MoEMethod.__init__(quant_config: W8A8Int8Config)
  W8A8Int8MoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)
  W8A8Int8MoEMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  W8A8Int8MoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  NPU_W8A8LinearMethodImpl.__init__() -> None
  NPU_W8A8LinearMethodImpl.get_weight(input_size: int, output_size: int, params_dtype: torch.dtype) -> Dict[str, Any]
  NPU_W8A8LinearMethodImpl.get_pertensor_param(params_dtype: torch.dtype) -> Dict[str, Any]
  NPU_W8A8LinearMethodImpl.get_perchannel_param(output_size: int, params_dtype: torch.dtype) -> Dict[str, Any]
  NPU_W8A8LinearMethodImpl.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  NPU_W8A8LinearMethodImpl.process_weights_after_loading(layer)
  NPU_W8A8LinearMethodMTImpl.__init__() -> None
  NPU_W8A8LinearMethodMTImpl.get_weight(input_size: int, output_size: int, params_dtype: torch.dtype) -> Dict[str, Any]
  NPU_W8A8LinearMethodMTImpl.get_pertensor_param(params_dtype: torch.dtype) -> Dict[str, Any]
  NPU_W8A8LinearMethodMTImpl.get_perchannel_param(output_size: int, params_dtype: torch.dtype) -> Dict[str, Any]
  NPU_W8A8LinearMethodMTImpl.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  NPU_W8A8LinearMethodMTImpl.process_weights_after_loading(layer)
  NPU_W8A8LinearMethod.__init__(quantization_config: W8A8Int8Config) -> None
  NPU_W8A8LinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype) -> None
  NPU_W8A8LinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  NPU_W8A8LinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  NPU_W8A8DynamicLinearMethodImpl.__init__()
  NPU_W8A8DynamicLinearMethodImpl.get_weight(input_size: int, output_size: int, params_dtype: torch.dtype) -> Dict[str, Any]
  NPU_W8A8DynamicLinearMethodImpl.get_pertensor_param(params_dtype: torch.dtype) -> Dict[str, Any]
  NPU_W8A8DynamicLinearMethodImpl.get_perchannel_param(output_size: int, params_dtype: torch.dtype) -> Dict[str, Any]
  NPU_W8A8DynamicLinearMethodImpl.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor], tp_rank: Optional[int]) -> torch.Tensor
  NPU_W8A8DynamicLinearMethodImpl.process_weights_after_loading(layer)
  NPU_W8A8DynamicLinearMethod.__init__(quantization_config: W8A8Int8Config) -> None
  NPU_W8A8DynamicLinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype) -> None
  NPU_W8A8DynamicLinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  NPU_W8A8DynamicLinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  NPU_W8A8MoEMethod.__init__(quantization_config: W8A8Int8Config) -> None
  NPU_W8A8MoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype) -> None
  NPU_W8A8MoEMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  NPU_W8A8MoEMethod.apply(layer, x, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor

# python/sglang/srt/layers/radix_attention.py
  RadixAttention.__init__(num_heads: int, head_dim: int, scaling: float, num_kv_heads: int, layer_id: int, logit_cap: float, v_head_dim: int, sliding_window_size: int, is_cross_attention: bool, pos_encoding_mode: str, logit_capping_method: str, quant_config: Optional[QuantizationConfig], attn_type: AttentionType, use_irope: bool, prefix: str)
  RadixAttention.forward(q, k, v, forward_batch: ForwardBatch, save_kv_cache: bool)

# python/sglang/srt/layers/rotary_embedding.py
  RotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype) -> None
  RotaryEmbedding.forward_native(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  RotaryEmbedding.forward_npu(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  RotaryEmbedding.forward_cpu(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  RotaryEmbedding.forward_cuda(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor], fused_set_kv_buffer_arg) -> Tuple[torch.Tensor, torch.Tensor]
  RotaryEmbedding.extra_repr() -> str
  LinearScalingRotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factors: Union[List[float], float], dtype: torch.dtype) -> None
  LinearScalingRotaryEmbedding.scaling_factor_to_offset() -> Dict[float, int]
  DynamicNTKScalingRotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype) -> None
  YaRNScalingRotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype) -> None
  Phi3LongRoPEScaledRotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, original_max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, short_factor: List[float], long_factor: List[float], short_mscale: Optional[float], long_mscale: Optional[float])
  Phi3LongRoPEScaledRotaryEmbedding.forward(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
yarn_get_mscale(scale: float, mscale: float) -> float
  DeepseekScalingRotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype) -> None
  DeepseekScalingRotaryEmbedding.forward_native(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  DeepseekScalingRotaryEmbedding.forward_npu(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  DeepseekScalingRotaryEmbedding.forward_cpu(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  Llama3RotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, scaling_factor: float, low_freq_factor: float, high_freq_factor: float, orig_max_position: int) -> None
  Llama4VisionRotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype)
  Llama4VisionRotaryEmbedding.forward(query: torch.Tensor, key: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
  DynamicNTKAlphaRotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_alpha: float, dtype: torch.dtype) -> None
  MRotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, mrope_section: Optional[List[int]]) -> None
  MRotaryEmbedding.forward(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
  MRotaryEmbedding.get_rope_index(spatial_merge_size: int, image_token_id: int, video_token_id: int, vision_start_token_id: int, model_type: str, tokens_per_second: Optional[int], input_ids: Optional[torch.LongTensor], image_grid_thw: Optional[torch.LongTensor], video_grid_thw: Optional[torch.LongTensor], second_per_grid_ts: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  MRotaryEmbedding.get_rope_index_glm4v(input_ids: torch.Tensor, hf_config: Any, image_grid_thw: Union[list[list[int]], torch.Tensor], video_grid_thw: Union[list[list[int]], torch.Tensor], attention_mask: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]
  MRotaryEmbedding.get_next_input_positions(mrope_position_delta: int, context_len: int, seq_len: int) -> torch.Tensor
  DualChunkRotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, chunk_size: int, local_size: int) -> None
  DualChunkRotaryEmbedding.forward(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  DualChunkRotaryEmbedding.extra_repr() -> str
get_rope(head_size: int, rotary_dim: int, max_position: int, base: int, is_neox_style: bool, rope_scaling: Optional[Dict[str, Any]], dtype: Optional[torch.dtype], partial_rotary_factor: float, dual_chunk_attention_config: Optional[Dict[str, Any]]) -> RotaryEmbedding
rotate_half(x)
apply_rotary_pos_emb_native(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor, unsqueeze_dim) -> Tuple[torch.Tensor, torch.Tensor]
apply_rotary_pos_emb_npu(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor, unsqueeze_dim) -> Tuple[torch.Tensor, torch.Tensor]
get_rope_cpu(head_size: int, rotary_dim: int, max_position: int, base: int, is_neox_style: bool, rope_scaling: Optional[Dict[str, Any]], dtype: Optional[torch.dtype], partial_rotary_factor: float, device: Optional[str]) -> RotaryEmbedding
get_rope_wrapper(head_size: int, rotary_dim: int, max_position: int, base: int, is_neox_style: bool, rope_scaling: Optional[Dict[str, Any]], dtype: Optional[torch.dtype], partial_rotary_factor: float, device: Optional[str])

# python/sglang/srt/layers/sampler.py
  Sampler.__init__()
  Sampler.forward(logits_output: LogitsProcessorOutput, sampling_info: SamplingBatchInfo, return_logprob: bool, top_logprobs_nums: List[int], token_ids_logprobs: List[List[int]])
top_k_top_p_min_p_sampling_from_probs_torch(probs: torch.Tensor, top_ks: torch.Tensor, top_ps: torch.Tensor, min_ps: torch.Tensor, need_min_p_sampling: bool)
sampling_from_probs_torch(probs: torch.Tensor)
top_p_normalize_probs_torch(probs: torch.Tensor, top_ps: torch.Tensor)
get_top_logprobs(logprobs: torch.Tensor, top_logprobs_nums: List[int])
get_token_ids_logprobs(logprobs: torch.Tensor, token_ids_logprobs: List[List[int]])
apply_custom_logit_processor(logits: torch.Tensor, sampling_batch_info: SamplingBatchInfo, num_tokens_in_batch: int)

# python/sglang/srt/layers/torchao_utils.py
get_gemlite_cache_path() -> str
save_gemlite_cache(print_error: bool) -> bool
proj_filter(module: torch.nn.Module, fqn: str)
apply_torchao_config_to_model(model: torch.nn.Module, torchao_config: str, filter_fn: Optional[Callable])

# python/sglang/srt/layers/utils.py
get_layer_id(weight_name)
  PPMissingLayer.__init__()
  PPMissingLayer.forward()

# python/sglang/srt/layers/vocab_parallel_embedding.py
pad_vocab_size(vocab_size: int, pad_to: int) -> int
vocab_range_from_per_partition_vocab_size(per_partition_vocab_size: int, rank: int, offset: int) -> Sequence[int]
vocab_range_from_global_vocab_size(global_vocab_size: int, rank: int, world_size: int, offset: int) -> Sequence[int]
  VocabParallelEmbeddingShardIndices.num_org_elements() -> int
  VocabParallelEmbeddingShardIndices.num_added_elements() -> int
  VocabParallelEmbeddingShardIndices.num_org_elements_padded() -> int
  VocabParallelEmbeddingShardIndices.num_added_elements_padded() -> int
  VocabParallelEmbeddingShardIndices.num_org_vocab_padding() -> int
  VocabParallelEmbeddingShardIndices.num_added_vocab_padding() -> int
  VocabParallelEmbeddingShardIndices.num_elements_padded() -> int
  VocabParallelEmbeddingShardIndices.__post_init__()
get_masked_input_and_mask(input_: torch.Tensor, org_vocab_start_index: int, org_vocab_end_index: int, num_org_vocab_padding: int, added_vocab_start_index: int, added_vocab_end_index: int) -> Tuple[torch.Tensor, torch.Tensor]
  VocabParallelEmbedding.__init__(num_embeddings: int, embedding_dim: int)
  VocabParallelEmbedding.get_sharded_to_full_mapping() -> Optional[List[int]]
  VocabParallelEmbedding.weight_loader(param: Parameter, loaded_weight: torch.Tensor)
  VocabParallelEmbedding.forward(input_)
  VocabParallelEmbedding.extra_repr() -> str
  ParallelLMHead.__init__(num_embeddings: int, embedding_dim: int)
  ParallelLMHead.tie_weights(embed_tokens: VocabParallelEmbedding)
  ParallelLMHead.forward(input_)

# python/sglang/srt/lora/backend/base_backend.py
  BaseLoRABackend.__init__(name: str, batch_info: LoRABatchInfo)
  BaseLoRABackend.run_lora_a_sgemm(x: torch.Tensor, weights: torch.Tensor) -> torch.Tensor
  BaseLoRABackend.run_lora_b_sgemm(x: torch.Tensor, weights: torch.Tensor) -> torch.Tensor
  BaseLoRABackend.run_qkv_lora(x: torch.Tensor, qkv_lora_a: torch.Tensor, qkv_lora_b: Union[torch.Tensor, Tuple[torch.Tensor]]) -> torch.Tensor
  BaseLoRABackend.run_gate_up_lora(x: torch.Tensor, gate_up_lora_a: torch.Tensor, gate_up_lora_b: Union[torch.Tensor, Tuple[torch.Tensor]]) -> torch.Tensor
  BaseLoRABackend.set_batch_info(batch_info: LoRABatchInfo)
get_backend_from_name(name: str) -> BaseLoRABackend

# python/sglang/srt/lora/backend/triton_backend.py
  TritonLoRABackend.__init__(name: str, batch_info: LoRABatchInfo)
  TritonLoRABackend.run_lora_a_sgemm(x: torch.Tensor, weights: torch.Tensor) -> torch.Tensor
  TritonLoRABackend.run_lora_b_sgemm(x: torch.Tensor, weights: torch.Tensor, base_output: torch.Tensor) -> torch.Tensor
  TritonLoRABackend.run_qkv_lora(x: torch.Tensor, qkv_lora_a: torch.Tensor, qkv_lora_b: torch.Tensor, output_offset: torch.Tensor, max_qkv_out_dim: int, base_output: torch.Tensor) -> torch.Tensor
  TritonLoRABackend.run_gate_up_lora(x: torch.Tensor, gate_up_lora_a: torch.Tensor, gate_up_lora_b: torch.Tensor, base_output: torch.Tensor) -> torch.Tensor

# python/sglang/srt/lora/layers.py
  BaseLayerWithLoRA.__init__(base_layer: nn.Module, lora_backend: BaseLoRABackend)
  BaseLayerWithLoRA.forward(x: torch.Tensor)
  BaseLayerWithLoRA.set_lora_info()
  BaseLayerWithLoRA.slice_lora_a_weights(A: torch.Tensor, tp_rank: int)
  BaseLayerWithLoRA.slice_lora_b_weights(B: torch.Tensor, tp_rank: int)
  VocabParallelEmbeddingWithLoRA.__init__(base_layer: VocabParallelEmbedding, lora_backend: BaseLoRABackend) -> None
  ColumnParallelLinearWithLoRA.__init__(base_layer: ColumnParallelLinear, lora_backend: BaseLoRABackend) -> None
  ColumnParallelLinearWithLoRA.set_lora_info(A_buffer: torch.Tensor, B_buffer: torch.Tensor)
  ColumnParallelLinearWithLoRA.apply_lora(base_output: torch.Tensor, x: torch.Tensor) -> torch.Tensor
  ColumnParallelLinearWithLoRA.forward(input_: torch.Tensor)
  ColumnParallelLinearWithLoRA.slice_lora_a_weights(A: torch.Tensor, tp_rank: int)
  ColumnParallelLinearWithLoRA.slice_lora_b_weights(B: torch.Tensor, tp_rank: int)
  MergedColumnParallelLinearWithLoRA.__init__(base_layer: MergedColumnParallelLinear, lora_backend: BaseLoRABackend) -> None
  MergedColumnParallelLinearWithLoRA.set_lora_info(A_buffer: torch.Tensor, B_buffer: torch.Tensor)
  MergedColumnParallelLinearWithLoRA.apply_lora(base_output: torch.Tensor, x: torch.Tensor) -> torch.Tensor
  MergedColumnParallelLinearWithLoRA.slice_lora_a_weights(A: torch.Tensor, tp_rank: int)
  MergedColumnParallelLinearWithLoRA.slice_lora_b_weights(B: torch.Tensor, tp_rank: int)
  QKVParallelLinearWithLoRA.__init__(base_layer: QKVParallelLinear, lora_backend: BaseLoRABackend) -> None
  QKVParallelLinearWithLoRA.set_lora_info(A_buffer_qkv: torch.Tensor, B_buffer_qkv: torch.Tensor)
  QKVParallelLinearWithLoRA.apply_lora(base_output: torch.Tensor, x: torch.Tensor) -> torch.Tensor
  QKVParallelLinearWithLoRA.slice_lora_a_weights(A: torch.Tensor, tp_rank: int)
  QKVParallelLinearWithLoRA.slice_lora_b_weights(B: torch.Tensor, tp_rank: int) -> torch.Tensor
  RowParallelLinearWithLoRA.__init__(base_layer: RowParallelLinear, lora_backend: BaseLoRABackend) -> None
  RowParallelLinearWithLoRA.set_lora_info(A_buffer: torch.Tensor, B_buffer: torch.Tensor)
  RowParallelLinearWithLoRA.apply_lora(base_output: torch.Tensor, x: torch.Tensor) -> torch.Tensor
  RowParallelLinearWithLoRA.forward(input_: torch.Tensor, skip_all_reduce)
  RowParallelLinearWithLoRA.slice_lora_a_weights(A: torch.Tensor, tp_rank: int)
  RowParallelLinearWithLoRA.slice_lora_b_weights(B: torch.Tensor, tp_rank: int)
get_lora_layer(layer: nn.Module, lora_backend: BaseLoRABackend) -> BaseLayerWithLoRA

# python/sglang/srt/lora/lora.py
  LoRALayer.__init__(config: LoRAConfig, base_hf_config: AutoConfig)
  LoRAAdapter.__init__(uid: str, config: LoRAConfig, base_hf_config: AutoConfig, load_config: LoadConfig, lora_backend: BaseLoRABackend)
  LoRAAdapter.initialize_weights()
  LoRAAdapter.normalize_qkv_proj(weight_names: List[str], weights: Dict[str, torch.Tensor])
  LoRAAdapter.normalize_gate_up_proj(weight_names: List[str], weights: Dict[str, torch.Tensor])

# python/sglang/srt/lora/lora_config.py
  LoRAConfig.__init__(path: str) -> None
  LoRAConfig.get_lora_config(dummy)

# python/sglang/srt/lora/lora_manager.py
  LoRAManager.__init__(base_model: torch.nn.Module, base_hf_config: AutoConfig, max_loras_per_batch: int, load_config: LoadConfig, dtype: torch.dtype, lora_backend: str, tp_size: int, tp_rank: int, max_lora_rank: Optional[int], target_modules: Optional[Iterable[str]], lora_paths: Optional[List[LoRARef]])
  LoRAManager.init_cuda_graph_batch_info(max_bs_in_cuda_graph: int)
  LoRAManager.create_lora_update_result(success: bool, error_message: str) -> LoRAUpdateResult
  LoRAManager.load_lora_adapter(lora_ref: LoRARef) -> LoRAUpdateResult
  LoRAManager.validate_new_adapter(lora_config: LoRAConfig, lora_ref: LoRARef)
  LoRAManager.unload_lora_adapter(lora_ref: LoRARef) -> LoRAUpdateResult
  LoRAManager.validate_lora_batch(lora_ids: set[str]) -> bool
  LoRAManager.prepare_lora_batch(forward_batch: ForwardBatch)
  LoRAManager.update_lora_info()
  LoRAManager.init_state(max_lora_rank: Optional[int], target_modules: Optional[Iterable[str]], lora_paths: Optional[List[LoRARef]])
  LoRAManager.init_lora_adapters(lora_paths: Optional[List[LoRARef]])
  LoRAManager.init_lora_shapes(max_lora_rank: Optional[int], target_modules: Optional[Iterable[str]])
  LoRAManager.load_lora_weights(lora_ref: LoRARef)
  LoRAManager.init_memory_pool()
  LoRAManager.set_lora_module(module_name, module)
  LoRAManager.init_lora_modules()

# python/sglang/srt/lora/lora_registry.py
  LoRARef.__post_init__()
  LoRARef.__str__() -> str
  LoRARegistry.__init__(lora_paths: Optional[List[LoRARef]])
  LoRARegistry.register(lora_ref: LoRARef)
  LoRARegistry.unregister(lora_name: str) -> str
  LoRARegistry.acquire(lora_name: Union[str, List[str]]) -> Union[str, List[str]]
  LoRARegistry.release(lora_id: Union[str, List[str]])
  LoRARegistry.wait_for_unload(lora_id: str)
  LoRARegistry.num_registered_loras() -> int

# python/sglang/srt/lora/mem_pool.py
  EmptySlot.__repr__()
  EmptySlot.__new__(cls)
  LoRAMemoryPool.__init__(base_hf_config: AutoConfig, max_loras_per_batch: int, dtype: torch.dtype, tp_size: int, tp_rank: int, max_lora_rank: int, target_modules: Set[str], base_model: torch.nn.Module)
  LoRAMemoryPool.can_support(config: Union[LoRAConfig, Iterable[LoRAConfig]]) -> bool
  LoRAMemoryPool.get_lora_A_shape(module_name: str, base_model: torch.nn.Module, max_lora_dim: int) -> Tuple[int]
  LoRAMemoryPool.get_lora_B_shape(module_name: str, base_model: torch.nn.Module, max_lora_dim: int) -> Tuple[int]
  LoRAMemoryPool.init_buffers(base_model: torch.nn.Module)
  LoRAMemoryPool.prepare_lora_batch(cur_uids: Set[Optional[str]], lora_adapters: Dict[str, LoRAAdapter], lora_modules: List[Dict[str, BaseLayerWithLoRA]], lora_refs: Dict[str, LoRARef])
  LoRAMemoryPool.load_lora_weight_to_buffer(uid: str, buffer_id: int, lora_adapter: LoRAAdapter, lora_modules: List[Dict[str, BaseLayerWithLoRA]])
  LoRAMemoryPool.get_tensor(target_module: str, layer_id: int, lora_type: LoRAType) -> torch.Tensor
  LoRAMemoryPool.get_buffer_id(lora_uid: str)

# python/sglang/srt/lora/triton_ops/gate_up_lora_b.py
gate_up_lora_b_fwd(x: torch.Tensor, gate_up_lora_b: torch.Tensor, batch_info: LoRABatchInfo, output_dim: int, base_output: torch.Tensor) -> torch.Tensor

# python/sglang/srt/lora/triton_ops/qkv_lora_b.py
qkv_lora_b_fwd(x: torch.Tensor, qkv_lora_b: torch.Tensor, batch_info: LoRABatchInfo, output_offset: torch.Tensor, max_qkv_out_dim: int, base_output: torch.Tensor) -> torch.Tensor

# python/sglang/srt/lora/triton_ops/sgemm_lora_a.py
sgemm_lora_a_fwd(x: torch.Tensor, weights: torch.Tensor, batch_info: LoRABatchInfo, stack_num: int) -> torch.Tensor

# python/sglang/srt/lora/triton_ops/sgemm_lora_b.py
sgemm_lora_b_fwd(x: torch.Tensor, weights: torch.Tensor, batch_info: LoRABatchInfo, base_output: torch.Tensor) -> torch.Tensor

# python/sglang/srt/lora/utils.py
get_layer_id(name: str) -> int
get_hidden_dim(module_name: str, config: AutoConfig, base_model: torch.nn.Module) -> Tuple[int]
get_normalized_target_modules(target_modules: Iterable[str]) -> set[str]
get_stacked_multiply(module_name: str) -> int
get_target_module_name(full_module_name: str, target_modules: Set[str]) -> str

# python/sglang/srt/managers/cache_controller.py
  LayerDoneCounter.__init__(num_layers)
  LayerDoneCounter.next_producer()
  LayerDoneCounter.update_producer()
  LayerDoneCounter.set_consumer(index)
  LayerDoneCounter.increment()
  LayerDoneCounter.wait_until(threshold)
  LayerDoneCounter.reset()
  CacheOperation.__init__(host_indices: torch.Tensor, device_indices: torch.Tensor, node_id: int, priority: Optional[int])
  CacheOperation.merge(other: 'CacheOperation') -> None
  CacheOperation.split(factor) -> List['CacheOperation']
  CacheOperation.__lt__(other: 'CacheOperation')
  TransferBuffer.__init__(stop_event, buffer_count: int, max_buffer_size: int) -> None
  TransferBuffer.full() -> bool
  TransferBuffer.empty() -> bool
  TransferBuffer.put(item, block, timeout) -> None
  TransferBuffer.get(block, timeout) -> Optional[CacheOperation]
  TransferBuffer.clear()
  StorageOperation.__init__(host_indices: torch.Tensor, token_ids: List[int], last_hash: Optional[str], hash_value: Optional[List[str]])
  StorageOperation.__lt__(other: 'StorageOperation')
  PrefetchOperation.__init__(request_id: str, host_indices: torch.Tensor, token_ids: List[int], last_hash: Optional[str])
  PrefetchOperation.increment(num_tokens: int)
  PrefetchOperation.mark_done()
  PrefetchOperation.is_done() -> bool
  HiCacheController.__init__(token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, mem_pool_host: HostKVCache, page_size: int, tp_group: torch.distributed.ProcessGroup, load_cache_event: threading.Event, write_policy: str, io_backend: str, storage_backend: Optional[str], prefetch_threshold: int, model_name: Optional[str], storage_backend_extra_config: Optional[str])
  HiCacheController.reset()
  HiCacheController.write(device_indices: torch.Tensor, priority: Optional[int], node_id: int) -> Optional[torch.Tensor]
  HiCacheController.load(host_indices: torch.Tensor, priority: Optional[int], node_id: int) -> Optional[torch.Tensor]
  HiCacheController.move_indices(host_indices, device_indices)
  HiCacheController.write_thread_func_direct()
  HiCacheController.load_thread_func_layer_by_layer()
  HiCacheController.evict_device(device_indices: torch.Tensor, host_indices: torch.Tensor) -> int
  HiCacheController.evict_host(host_indices: torch.Tensor, backup_only: bool) -> int
  HiCacheController.prefetch(request_id: str, host_indices: torch.Tensor, new_input_tokens: List[int], last_hash: Optional[str]) -> PrefetchOperation
  HiCacheController.terminate_prefetch(operation)
  HiCacheController.append_host_mem_release(host_indices: torch.Tensor)
  HiCacheController.prefetch_io_aux_func()
  HiCacheController.prefetch_rate_limited() -> bool
  HiCacheController.prefetch_thread_func()
  HiCacheController.write_storage(host_indices: torch.Tensor, token_ids: List[int], hash_value: Optional[List[str]]) -> int
  HiCacheController.backup_thread_func()

# python/sglang/srt/managers/data_parallel_controller.py
  LoadBalanceMethod.from_str(cls, method: str)
  DataParallelController.__init__(server_args: ServerArgs, port_args: PortArgs, dp_balance_meta: DPBalanceMeta) -> None
  DataParallelController.launch_dp_schedulers(server_args, port_args)
  DataParallelController.launch_tensor_parallel_group_thread(server_args: ServerArgs, port_args: PortArgs, base_gpu_id: int, dp_rank: int, ready_event: threading.Event)
  DataParallelController.launch_dp_attention_schedulers(server_args, port_args)
  DataParallelController.launch_tensor_parallel_group(server_args: ServerArgs, port_args: PortArgs, base_gpu_id: int, dp_rank: int)
  DataParallelController.round_robin_scheduler(req: Req)
  DataParallelController.shortest_queue_scheduler(input_requests)
  DataParallelController.minimum_tokens_scheduler(req)
  DataParallelController.event_loop()
run_data_parallel_controller_process(server_args: ServerArgs, port_args: PortArgs, pipe_writer)

# python/sglang/srt/managers/detokenizer_manager.py
  DetokenizerManager.__init__(server_args: ServerArgs, port_args: PortArgs)
  DetokenizerManager.event_loop()
  DetokenizerManager.trim_matched_stop(output: Union[str, List[int]], finished_reason: Dict, no_stop_trim: bool)
  DetokenizerManager.handle_batch_embedding_out(recv_obj: BatchEmbeddingOut)
  DetokenizerManager.handle_batch_token_id_out(recv_obj: BatchTokenIDOut)
  DetokenizerManager.handle_multimodal_decode_req(recv_obj: BatchMultimodalDecodeReq)
  DetokenizerManager.handle_freeze_gc_req(recv_req: FreezeGCReq)
  LimitedCapacityDict.__init__(capacity: int)
  LimitedCapacityDict.__setitem__(key, value)
run_detokenizer_process(server_args: ServerArgs, port_args: PortArgs)

# python/sglang/srt/managers/io_struct.py
  GenerateReqInput.contains_mm_input() -> bool
  GenerateReqInput.normalize_batch_and_arguments()
  GenerateReqInput.regenerate_rid()
  GenerateReqInput.__getitem__(i)
  BatchTokenizedGenerateReqInput.__len__()
  BatchTokenizedGenerateReqInput.__getitem__(i)
  BatchTokenizedGenerateReqInput.__iter__()
  EmbeddingReqInput.normalize_batch_and_arguments()
  EmbeddingReqInput.regenerate_rid()
  EmbeddingReqInput.contains_mm_input() -> bool
  EmbeddingReqInput.__getitem__(i)
  BatchTokenizedEmbeddingReqInput.__len__()
  BatchTokenizedEmbeddingReqInput.__getitem__(i)
  BatchTokenizedEmbeddingReqInput.__iter__()
  LoadLoRAAdapterReqInput.to_ref() -> LoRARef
  UnloadLoRAAdapterReqInput.to_ref() -> LoRARef

# python/sglang/srt/managers/mm_utils.py
  TransportProxyTensor.__new__(cls, data: torch.Tensor, name: Optional[str], fields: Optional[Dict[str, Any]], transport_mode: TensorTransportMode)
  TransportProxyTensor.__getstate__()
  TransportProxyTensor.__setstate__(state: Dict[str, Any])
  TransportProxyTensor.name() -> Optional[str]
  TransportProxyTensor.fields() -> Dict[str, Any]
  TransportProxyTensor.transport_mode() -> TensorTransportMode
  MultiModalityDataPaddingPattern.pad_input_tokens(input_ids: List[int], mm_inputs: MultimodalInputs) -> List[int]
  MultiModalityDataPaddingPatternTokenPairs.__init__(data_token_pairs: Optional[List[Tuple[int, int]]], data_start_token_ids: Optional[List[int]]) -> None
  MultiModalityDataPaddingPatternTokenPairs.pad_input_tokens(input_ids: List[int], mm_inputs: MultimodalInputs) -> List[int]
  MultiModalityDataPaddingPatternMultimodalTokens.pad_input_tokens(input_ids: List[int], mm_inputs: MultimodalInputs) -> List[int]
init_embedding_cache(max_size: int)
get_embedding_hash(embedding_items: List[MultimodalDataItem]) -> int
get_embedding_chunk(embedding: torch.Tensor, extend_prefix_len: int, extend_seq_len: int, items_offset: List[Tuple[int, int]]) -> Tuple[torch.Tensor, int, int]
get_embedding_and_mask(data_embedding_func: Callable[[List[MultimodalDataItem]], torch.Tensor], embedding_items: List[MultimodalDataItem], placeholder_tensor: torch.Tensor, input_ids: torch.Tensor, items_size: List[int], prefix_length: List[int], extend_length: List[int], items_offset_list: List[List[Tuple[int, int]]]) -> Tuple[torch.Tensor, torch.Tensor]
embed_mm_inputs(mm_inputs_list: List[MultimodalInputs], extend_prefix_lens: List[int], extend_seq_lens: List[int], input_ids: torch.Tensor, input_embedding: nn.Embedding, multimodal_model: nn.Module, data_embedding_func_mapping: Dict[Modality, Callable[[List[MultimodalDataItem]], torch.Tensor]], placeholder_tokens: dict[Modality, List[int]]) -> Optional[torch.Tensor]
general_mm_embed_routine(input_ids: torch.Tensor, forward_batch: ForwardBatch, language_model: nn.Module, multimodal_model: Optional[nn.Module], data_embedding_funcs: Dict[Modality, Callable[[List[MultimodalDataItem]], torch.Tensor]], placeholder_tokens: Optional[dict[Modality, List[int]]]) -> torch.Tensor
get_multimodal_data_bounds(input_ids: torch.Tensor, pad_values: List[int], token_pairs: List[Tuple[int, int]]) -> torch.Tensor
data_hash(data) -> int
tensor_hash(tensor_list) -> int
hash_feature(f)

# python/sglang/srt/managers/multimodal_processor.py
import_processors()
get_mm_processor(hf_config, server_args: ServerArgs, processor, transport_mode) -> BaseMultimodalProcessor

# python/sglang/srt/managers/schedule_batch.py
  BaseFinishReason.__init__(is_error: bool)
  BaseFinishReason.to_json()
  FINISH_MATCHED_TOKEN.__init__(matched: Union[int, List[int]])
  FINISH_MATCHED_TOKEN.to_json()
  FINISH_MATCHED_STR.__init__(matched: str)
  FINISH_MATCHED_STR.to_json()
  FINISH_LENGTH.__init__(length: int)
  FINISH_LENGTH.to_json()
  FINISH_ABORT.__init__(message, status_code, err_type)
  FINISH_ABORT.to_json()
  Modality.from_str(modality_str: str)
  Modality.all()
  MultimodalDataItem.__getattr__(name: str)
  MultimodalDataItem.__setitem__(key: str, value: Any)
  MultimodalDataItem.set(key: str, value: Any)
  MultimodalDataItem.is_empty_list(l)
  MultimodalDataItem.set_pad_value()
  MultimodalDataItem.is_modality(modality: Modality) -> bool
  MultimodalDataItem.is_audio()
  MultimodalDataItem.is_image()
  MultimodalDataItem.is_video()
  MultimodalDataItem.is_valid() -> bool
  MultimodalDataItem.validate()
  MultimodalDataItem.from_dict(obj: dict)
  MultimodalDataItem.merge(other)
  MultimodalInputs.from_dict(obj: dict)
  MultimodalInputs.contains_image_inputs() -> bool
  MultimodalInputs.contains_video_inputs() -> bool
  MultimodalInputs.contains_audio_inputs() -> bool
  MultimodalInputs.contains_mm_input() -> bool
  MultimodalInputs.merge(other: MultimodalInputs)
  Req.__init__(rid: str, origin_input_text: str, origin_input_ids: List[int], sampling_params: SamplingParams, return_logprob: bool, top_logprobs_num: int, token_ids_logprob: List[int], stream: bool, origin_input_ids_unpadded: Optional[Tuple[int]], lora_id: Optional[str], input_embeds: Optional[List[List[float]]], token_type_ids: List[int], session_id: Optional[str], custom_logit_processor: Optional[str], return_hidden_states: bool, eos_token_ids: Optional[Set[int]], bootstrap_host: Optional[str], bootstrap_port: Optional[int], bootstrap_room: Optional[int], data_parallel_rank: Optional[int], vocab_size: Optional[int])
  Req.seqlen()
  Req.extend_image_inputs(image_inputs)
  Req.finished() -> bool
  Req.init_next_round_input(tree_cache: Optional[BasePrefixCache])
  Req.adjust_max_prefix_ids()
  Req.init_incremental_detokenize()
  Req.check_finished()
  Req.reset_for_retract()
  Req.offload_kv_cache(req_to_token_pool, token_to_kv_pool_allocator)
  Req.load_kv_cache(req_to_token_pool, token_to_kv_pool_allocator)
  Req.log_time_stats()
  Req.set_finish_with_abort(error_msg: str)
  Req.__repr__()
  ScheduleBatch.init_new(cls, reqs: List[Req], req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, tree_cache: BasePrefixCache, model_config: ModelConfig, enable_overlap: bool, spec_algorithm: SpeculativeAlgorithm, chunked_req: Optional[Req])
  ScheduleBatch.batch_size()
  ScheduleBatch.is_empty()
  ScheduleBatch.alloc_req_slots(num_reqs: int)
  ScheduleBatch.alloc_token_slots(num_tokens: int, backup_state: bool)
  ScheduleBatch.alloc_paged_token_slots_extend(prefix_lens: torch.Tensor, seq_lens: torch.Tensor, last_loc: torch.Tensor, extend_num_tokens: int, backup_state: bool)
  ScheduleBatch.alloc_paged_token_slots_decode(seq_lens: torch.Tensor, last_loc: torch.Tensor, backup_state: bool)
  ScheduleBatch.prepare_encoder_info_extend(input_ids: List[int], seq_lens: List[int])
  ScheduleBatch.prepare_for_extend()
  ScheduleBatch.prepare_for_split_prefill()
  ScheduleBatch.mix_with_running(running_batch: 'ScheduleBatch')
  ScheduleBatch.new_page_count_next_decode()
  ScheduleBatch.check_decode_mem(buf_multiplier)
  ScheduleBatch.retract_decode(server_args: ServerArgs)
  ScheduleBatch.prepare_encoder_info_decode()
  ScheduleBatch.prepare_for_idle()
  ScheduleBatch.prepare_for_decode()
  ScheduleBatch.filter_batch(chunked_req_to_exclude: Optional[Union[Req, List[Req]]], keep_indices: Optional[List[int]])
  ScheduleBatch.merge_batch(other: 'ScheduleBatch')
  ScheduleBatch.get_model_worker_batch(seq_lens_cpu_cache: Optional[torch.Tensor]) -> ModelWorkerBatch
  ScheduleBatch.copy()
  ScheduleBatch.__str__()
write_req_to_token_pool_triton(req_to_token_ptr, req_pool_indices, pre_lens, seq_lens, extend_lens, out_cache_loc, req_to_token_ptr_stride: tl.constexpr)
get_last_loc(req_to_token: torch.Tensor, req_pool_indices_tensor: torch.Tensor, prefix_lens_tensor: torch.Tensor) -> torch.Tensor
get_last_loc_torch(req_to_token: torch.Tensor, req_pool_indices_tensor: torch.Tensor, prefix_lens_tensor: torch.Tensor) -> torch.Tensor
get_last_loc_kernel(req_to_token, req_pool_indices_tensor, prefix_lens_tensor, result, num_tokens, req_to_token_stride, BLOCK_SIZE: tl.constexpr)
get_last_loc_triton(req_to_token: torch.Tensor, req_pool_indices_tensor: torch.Tensor, prefix_lens_tensor: torch.Tensor) -> torch.Tensor

# python/sglang/srt/managers/schedule_policy.py
  SchedulePolicy.__init__(policy: str, tree_cache: BasePrefixCache, enable_hierarchical_cache: bool)
  SchedulePolicy.calc_priority(waiting_queue: List[Req]) -> bool
  PrefillAdder.__init__(page_size: int, tree_cache: BasePrefixCache, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, running_batch: ScheduleBatch, new_token_ratio: float, rem_input_tokens: int, rem_chunk_tokens: Optional[int], mixed_with_decode_tokens: int)
  PrefillAdder.rem_total_tokens()
  PrefillAdder.cur_rem_tokens()
  PrefillAdder.ceil_paged_tokens(tokens: int) -> int
  PrefillAdder.budget_state()
  PrefillAdder.add_chunked_req(req: Req)
  PrefillAdder.add_one_req_ignore_eos(req: Req, has_chunked_req: bool)
  PrefillAdder.add_one_req(req: Req, has_chunked_req: bool)

# python/sglang/srt/managers/scheduler.py
  Scheduler.__init__(server_args: ServerArgs, port_args: PortArgs, gpu_id: int, tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int], dp_balance_meta: Optional[DPBalanceMeta])
  Scheduler.init_tokenizer()
  Scheduler.init_memory_pool_and_cache()
  Scheduler.init_disaggregation()
  Scheduler.init_moe_config()
  Scheduler.event_loop_normal()
  Scheduler.event_loop_overlap()
  Scheduler.event_loop_pp()
  Scheduler.recv_requests() -> List[Req]
  Scheduler.process_input_requests(recv_reqs: List)
  Scheduler.handle_generate_request(recv_req: TokenizedGenerateReqInput)
  Scheduler.handle_batch_generate_request(recv_req: BatchTokenizedGenerateReqInput)
  Scheduler.handle_embedding_request(recv_req: TokenizedEmbeddingReqInput)
  Scheduler.handle_batch_embedding_request(recv_req: BatchTokenizedEmbeddingReqInput)
  Scheduler.self_check_during_idle()
  Scheduler.check_memory()
  Scheduler.check_tree_cache()
  Scheduler.get_next_batch_to_run() -> Optional[ScheduleBatch]
  Scheduler.get_num_allocatable_reqs(running_bs)
  Scheduler.get_new_batch_prefill() -> Optional[ScheduleBatch]
  Scheduler.update_running_batch(batch: ScheduleBatch) -> Optional[ScheduleBatch]
  Scheduler.run_batch(batch: ScheduleBatch) -> Union[GenerationBatchResult, EmbeddingBatchResult]
  Scheduler.process_batch_result(batch: ScheduleBatch, result: Union[GenerationBatchResult, EmbeddingBatchResult], launch_done: Optional[threading.Event])
  Scheduler.maybe_send_health_check_signal()
  Scheduler.prepare_mlp_sync_batch(local_batch: ScheduleBatch)
  Scheduler.handle_dp_balance_data(local_batch: ScheduleBatch)
  Scheduler.prepare_mlp_sync_batch_raw(local_batch: ScheduleBatch, dp_size, attn_tp_size: int, tp_group, get_idle_batch, disable_cuda_graph: bool, spec_algorithm, speculative_num_draft_tokens, require_mlp_tp_gather: bool, disable_overlap_schedule: bool)
  Scheduler.get_idle_batch()
  Scheduler.move_ready_grammar_requests()
  Scheduler.set_next_batch_sampling_info_done(batch: ScheduleBatch)
  Scheduler.watchdog_thread()
  Scheduler.flush_cache_wrapped(recv_req: FlushCacheReqInput)
  Scheduler.clear_hicache_storage_wrapped(recv_req: ClearHiCacheReqInput)
  Scheduler.flush_cache()
  Scheduler.get_load()
  Scheduler.get_internal_state(recv_req: GetInternalStateReq)
  Scheduler.set_internal_state(recv_req: SetInternalStateReq)
  Scheduler.handle_rpc_request(recv_req: RpcReqInput)
  Scheduler.abort_request(recv_req: AbortReq)
  Scheduler.load_lora_adapter(recv_req: LoadLoRAAdapterReqInput) -> LoadLoRAAdapterReqOutput
  Scheduler.unload_lora_adapter(recv_req: UnloadLoRAAdapterReqInput) -> UnloadLoRAAdapterReqOutput
  Scheduler.slow_down(recv_req: SlowDownReqInput)
  Scheduler.expert_distribution_handle(recv_req: ExpertDistributionReq)
  Scheduler.open_session(recv_req: OpenSessionReqInput)
  Scheduler.close_session(recv_req: CloseSessionReqInput)
  Scheduler.get_print_prefix()
  Scheduler.current_scheduler_metrics_enabled()
  Scheduler.maybe_sleep_on_idle()
  Scheduler.handle_freeze_gc(recv_req: FreezeGCReq)
  IdleSleeper.__init__(sockets)
  IdleSleeper.maybe_sleep()
is_health_check_generate_req(recv_req)
is_work_request(recv_req)
run_scheduler_process(server_args: ServerArgs, port_args: PortArgs, gpu_id: int, tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int], pipe_writer, balance_meta: Optional[DPBalanceMeta])

# python/sglang/srt/managers/scheduler_input_blocker.py
  SchedulerInputBlocker.__init__(noop: bool)
  SchedulerInputBlocker.handle(recv_reqs: Optional[List[Any]])
input_blocker_guard_region(send_to_scheduler)

# python/sglang/srt/managers/scheduler_metrics_mixin.py
  KvMetrics.__init__()
  SchedulerMetricsMixin.init_metrics(tp_rank: int, pp_rank: int, dp_rank: Optional[int])
  SchedulerMetricsMixin.init_kv_events(kv_events_config: Optional[str])
  SchedulerMetricsMixin.log_prefill_stats(adder: PrefillAdder, can_run_list: List[Req], running_bs: int)
  SchedulerMetricsMixin.log_decode_stats(can_run_cuda_graph: bool, running_batch: ScheduleBatch)

# python/sglang/srt/managers/scheduler_output_processor_mixin.py
  SchedulerOutputProcessorMixin.process_batch_result_prefill(self: Scheduler, batch: ScheduleBatch, result: Union[GenerationBatchResult, EmbeddingBatchResult], launch_done: Optional[threading.Event])
  SchedulerOutputProcessorMixin.process_batch_result_decode(self: Scheduler, batch: ScheduleBatch, result: GenerationBatchResult, launch_done: Optional[threading.Event])
  SchedulerOutputProcessorMixin.add_input_logprob_return_values(self: Scheduler, i: int, req: Req, output: LogitsProcessorOutput, logprob_pt: int, num_input_logprobs: int, last_prefill_chunk: bool)
  SchedulerOutputProcessorMixin.add_logprob_return_values(self: Scheduler, i: int, req: Req, pt: int, next_token_ids: List[int], num_input_logprobs: int, output: LogitsProcessorOutput)
  SchedulerOutputProcessorMixin.stream_output(self: Scheduler, reqs: List[Req], return_logprob: bool, skip_req: Optional[Req])
  SchedulerOutputProcessorMixin.stream_output_generation(self: Scheduler, reqs: List[Req], return_logprob: bool, skip_req: Optional[Req])
  SchedulerOutputProcessorMixin.stream_output_embedding(self: Scheduler, reqs: List[Req])

# python/sglang/srt/managers/scheduler_profiler_mixin.py
  SchedulerProfilerMixin.init_profier()
  SchedulerProfilerMixin.init_profile(output_dir: Optional[str], start_step: Optional[int], num_steps: Optional[int], activities: Optional[List[str]], with_stack: Optional[bool], record_shapes: Optional[bool], profile_by_stage: bool, profile_id: str) -> ProfileReqOutput
  SchedulerProfilerMixin.start_profile(stage: Optional[ForwardMode]) -> ProfileReqOutput | None
  SchedulerProfilerMixin.stop_profile(stage: Optional[ForwardMode]) -> ProfileReqOutput | None
  SchedulerProfilerMixin.profile(recv_req: ProfileReq)

# python/sglang/srt/managers/scheduler_recv_skipper.py
  SchedulerRecvSkipper.maybe_create(server_args: ServerArgs)
  SchedulerRecvSkipper.__init__(server_args: ServerArgs)
  SchedulerRecvSkipper.handle(last_forward_mode: ForwardMode)

# python/sglang/srt/managers/scheduler_update_weights_mixin.py
  SchedulerUpdateWeightsMixin.update_weights_from_disk(recv_req: UpdateWeightFromDiskReqInput)
  SchedulerUpdateWeightsMixin.init_weights_update_group(recv_req: InitWeightsUpdateGroupReqInput)
  SchedulerUpdateWeightsMixin.update_weights_from_distributed(recv_req: UpdateWeightsFromDistributedReqInput) -> Tuple[bool, str]
  SchedulerUpdateWeightsMixin.update_weights_from_tensor(recv_req: UpdateWeightsFromTensorReqInput)
  SchedulerUpdateWeightsMixin.get_weights_by_name(recv_req: GetWeightsByNameReqInput)
  SchedulerUpdateWeightsMixin.release_memory_occupation(recv_req: ReleaseMemoryOccupationReqInput)
  SchedulerUpdateWeightsMixin.resume_memory_occupation(recv_req: ResumeMemoryOccupationReqInput)
  SchedulerUpdateWeightsMixin.save_remote_model(params)
  SchedulerUpdateWeightsMixin.save_sharded_model(params)

# python/sglang/srt/managers/session_controller.py
  SessionReqNode.__init__(req, parent, childs)
  SessionReqNode.clear_childs(req_dict)
  SessionReqNode.clear(req_dict)
  SessionReqNode.abort()
  SessionReqNode.__str__()
  Session.__init__(capacity_of_str_len: int, session_id: Optional[str])
  Session.create_req(req: TokenizedGenerateReqInput, tokenizer)

# python/sglang/srt/managers/template_manager.py
  TemplateManager.__init__()
  TemplateManager.chat_template_name() -> Optional[str]
  TemplateManager.completion_template_name() -> Optional[str]
  TemplateManager.jinja_template_content_format() -> Optional[str]
  TemplateManager.force_reasoning() -> bool
  TemplateManager.load_chat_template(tokenizer_manager, chat_template_arg: Optional[str], model_path: str) -> None
  TemplateManager.guess_chat_template_from_model_path(model_path: str) -> None
  TemplateManager.load_completion_template(completion_template_arg: str) -> None
  TemplateManager.initialize_templates(tokenizer_manager, model_path: str, chat_template: Optional[str], completion_template: Optional[str]) -> None

# python/sglang/srt/managers/tokenizer_manager.py
  TokenizerManager.__init__(server_args: ServerArgs, port_args: PortArgs)
  TokenizerManager.generate_request(obj: Union[GenerateReqInput, EmbeddingReqInput], request: Optional[fastapi.Request])
  TokenizerManager.flush_cache() -> FlushCacheReqOutput
  TokenizerManager.clear_hicache_storage() -> ClearHiCacheReqOutput
  TokenizerManager.abort_request(rid: str, abort_all: bool)
  TokenizerManager.start_profile(output_dir: Optional[str], start_step: Optional[int], num_steps: Optional[int], activities: Optional[List[str]], with_stack: Optional[bool], record_shapes: Optional[bool], profile_by_stage: bool)
  TokenizerManager.stop_profile()
  TokenizerManager.start_expert_distribution_record()
  TokenizerManager.stop_expert_distribution_record()
  TokenizerManager.dump_expert_distribution_record()
  TokenizerManager.pause_generation()
  TokenizerManager.continue_generation()
  TokenizerManager.update_weights_from_disk(obj: UpdateWeightFromDiskReqInput, request: Optional[fastapi.Request]) -> Tuple[bool, str]
  TokenizerManager.init_weights_update_group(obj: InitWeightsUpdateGroupReqInput, request: Optional[fastapi.Request]) -> Tuple[bool, str]
  TokenizerManager.update_weights_from_distributed(obj: UpdateWeightsFromDistributedReqInput, request: Optional[fastapi.Request]) -> Tuple[bool, str]
  TokenizerManager.update_weights_from_tensor(obj: UpdateWeightsFromTensorReqInput, request: Optional[fastapi.Request]) -> Tuple[bool, str]
  TokenizerManager.load_lora_adapter(obj: LoadLoRAAdapterReqInput, _: Optional[fastapi.Request]) -> LoadLoRAAdapterReqOutput
  TokenizerManager.unload_lora_adapter(obj: UnloadLoRAAdapterReqInput, _: Optional[fastapi.Request]) -> UnloadLoRAAdapterReqOutput
  TokenizerManager.get_weights_by_name(obj: GetWeightsByNameReqInput, request: Optional[fastapi.Request])
  TokenizerManager.release_memory_occupation(obj: ReleaseMemoryOccupationReqInput, request: Optional[fastapi.Request])
  TokenizerManager.resume_memory_occupation(obj: ResumeMemoryOccupationReqInput, request: Optional[fastapi.Request])
  TokenizerManager.slow_down(obj: SlowDownReqInput, request: Optional[fastapi.Request])
  TokenizerManager.open_session(obj: OpenSessionReqInput, request: Optional[fastapi.Request])
  TokenizerManager.close_session(obj: CloseSessionReqInput, request: Optional[fastapi.Request])
  TokenizerManager.get_internal_state() -> List[Dict[Any, Any]]
  TokenizerManager.set_internal_state(obj: SetInternalStateReq) -> List[bool]
  TokenizerManager.get_load() -> dict
  TokenizerManager.get_log_request_metadata()
  TokenizerManager.configure_logging(obj: ConfigureLoggingReq)
  TokenizerManager.freeze_gc()
  TokenizerManager.create_abort_task(obj: GenerateReqInput)
  TokenizerManager.auto_create_handle_loop()
  TokenizerManager.dump_requests_before_crash()
  TokenizerManager.sigterm_watchdog()
  TokenizerManager.handle_loop()
  TokenizerManager.convert_logprob_style(meta_info: dict, state: ReqState, top_logprobs_num: int, token_ids_logprob: List[int], return_text_in_logprobs: bool, recv_obj: BatchStrOut, recv_obj_index: int)
  TokenizerManager.detokenize_logprob_tokens(token_logprobs_val: List[float], token_logprobs_idx: List[int], decode_to_text: bool)
  TokenizerManager.detokenize_top_logprobs_tokens(token_logprobs_val: List[float], token_logprobs_idx: List[int], decode_to_text: bool)
  TokenizerManager.collect_metrics(state: ReqState, recv_obj: BatchStrOut, i: int)
  TokenizerManager.dump_requests(state: ReqState, out_dict: dict)
  TokenizerManager.record_request_for_crash_dump(state: ReqState, out_dict: dict)
  TokenizerManager.score_request(query: Optional[Union[str, List[int]]], items: Optional[Union[str, List[str], List[List[int]]]], label_token_ids: Optional[List[int]], apply_softmax: bool, item_first: bool, request: Optional[Any]) -> List[List[float]]
print_exception_wrapper(func)
  SignalHandler.__init__(tokenizer_manager: TokenizerManager)
  SignalHandler.sigterm_handler(signum, frame)
  SignalHandler.running_phase_sigquit_handler(signum, frame)
  _Communicator.__init__(sender, fan_out: int)
  _Communicator.__call__(obj)
  _Communicator.handle_recv(recv_obj: T)

# python/sglang/srt/managers/tp_worker.py
  TpModelWorker.__init__(server_args: ServerArgs, gpu_id: int, tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int], nccl_port: int, is_draft_worker: bool, req_to_token_pool: Optional[ReqToTokenPool], token_to_kv_pool_allocator: Optional[BaseTokenToKVPoolAllocator])
  TpModelWorker.register_hicache_layer_transfer_counter(counter)
  TpModelWorker.set_hicache_consumer(consumer_index)
  TpModelWorker.get_worker_info()
  TpModelWorker.sliding_window_size() -> Optional[int]
  TpModelWorker.is_hybrid() -> bool
  TpModelWorker.get_tokens_per_layer_info()
  TpModelWorker.get_pad_input_ids_func()
  TpModelWorker.get_tp_group()
  TpModelWorker.get_attention_tp_group()
  TpModelWorker.get_attention_tp_cpu_group()
  TpModelWorker.get_memory_pool()
  TpModelWorker.forward_batch_generation(model_worker_batch: ModelWorkerBatch, launch_done: Optional[threading.Event], skip_sample: bool) -> Tuple[Union[LogitsProcessorOutput, torch.Tensor], Optional[torch.Tensor], bool]
  TpModelWorker.forward_batch_embedding(model_worker_batch: ModelWorkerBatch)
  TpModelWorker.update_weights_from_disk(recv_req: UpdateWeightFromDiskReqInput)
  TpModelWorker.init_weights_update_group(recv_req: InitWeightsUpdateGroupReqInput)
  TpModelWorker.update_weights_from_distributed(recv_req: UpdateWeightsFromDistributedReqInput)
  TpModelWorker.update_weights_from_tensor(recv_req: UpdateWeightsFromTensorReqInput)
  TpModelWorker.get_weights_by_name(recv_req: GetWeightsByNameReqInput)
  TpModelWorker.load_lora_adapter(recv_req: LoadLoRAAdapterReqInput)
  TpModelWorker.unload_lora_adapter(recv_req: UnloadLoRAAdapterReqInput)
  TpModelWorker.can_run_lora_batch(lora_ids: list[str]) -> bool

# python/sglang/srt/managers/tp_worker_overlap_thread.py
resolve_future_token_ids(input_ids, future_token_ids_map)
  TpModelWorkerClient.__init__(server_args: ServerArgs, gpu_id: int, tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int], nccl_port: int)
  TpModelWorkerClient.register_hicache_layer_transfer_counter(counter)
  TpModelWorkerClient.set_hicache_consumer(consumer_index)
  TpModelWorkerClient.get_worker_info()
  TpModelWorkerClient.get_tokens_per_layer_info()
  TpModelWorkerClient.sliding_window_size() -> Optional[int]
  TpModelWorkerClient.is_hybrid() -> bool
  TpModelWorkerClient.get_pad_input_ids_func()
  TpModelWorkerClient.get_tp_group()
  TpModelWorkerClient.get_attention_tp_group()
  TpModelWorkerClient.get_attention_tp_cpu_group()
  TpModelWorkerClient.get_memory_pool()
  TpModelWorkerClient.get_kv_cache()
  TpModelWorkerClient.forward_thread_func()
  TpModelWorkerClient.forward_thread_func_()
  TpModelWorkerClient.resolve_last_batch_result(launch_done: Optional[threading.Event])
  TpModelWorkerClient.forward_batch_generation(model_worker_batch: ModelWorkerBatch) -> Tuple[None, torch.Tensor, bool]
  TpModelWorkerClient.update_weights_from_disk(recv_req: UpdateWeightFromDiskReqInput)
  TpModelWorkerClient.init_weights_update_group(recv_req: InitWeightsUpdateGroupReqInput)
  TpModelWorkerClient.update_weights_from_distributed(recv_req: UpdateWeightsFromDistributedReqInput)
  TpModelWorkerClient.update_weights_from_tensor(recv_req: UpdateWeightsFromTensorReqInput)
  TpModelWorkerClient.get_weights_by_name(recv_req: GetWeightsByNameReqInput)
  TpModelWorkerClient.load_lora_adapter(recv_req: LoadLoRAAdapterReqInput)
  TpModelWorkerClient.unload_lora_adapter(recv_req: UnloadLoRAAdapterReqInput)
  TpModelWorkerClient.can_run_lora_batch(lora_ids: list[str]) -> bool
  TpModelWorkerClient.__delete__()

# python/sglang/srt/managers/utils.py
validate_input_length(req: Req, max_req_input_len: int, allow_auto_truncate: bool) -> Optional[str]
get_logprob_dict_from_result(result: GenerationBatchResult) -> dict
get_logprob_from_pp_outputs(next_pp_outputs: PPProxyTensors) -> tuple[LogitsProcessorOutput, list[int], list[int]]
  DPBalanceMeta.__init__(num_workers: int)
  DPBalanceMeta.destructor()
  DPBalanceMeta.get_shared_onfly() -> List[Dict[int, int]]
  DPBalanceMeta.set_shared_onfly_info(data: List[Dict[int, int]])
  DPBalanceMeta.get_shared_local_tokens() -> List[int]
  DPBalanceMeta.set_shared_local_tokens(data: List[int])
  DPBalanceMeta.__getstate__()
  DPBalanceMeta.__setstate__(state)

# python/sglang/srt/mem_cache/allocator.py
  BaseTokenToKVPoolAllocator.__init__(size: int, page_size: int, dtype: torch.dtype, device: str, kvcache: KVCache, need_sort: bool)
  BaseTokenToKVPoolAllocator.debug_print() -> str
  BaseTokenToKVPoolAllocator.available_size()
  BaseTokenToKVPoolAllocator.get_kvcache()
  BaseTokenToKVPoolAllocator.restore_state(state)
  BaseTokenToKVPoolAllocator.backup_state()
  BaseTokenToKVPoolAllocator.free_group_begin()
  BaseTokenToKVPoolAllocator.free_group_end()
  BaseTokenToKVPoolAllocator.merge_and_sort_free()
  BaseTokenToKVPoolAllocator.get_cpu_copy()
  BaseTokenToKVPoolAllocator.load_cpu_copy()
  BaseTokenToKVPoolAllocator.alloc_extend()
  BaseTokenToKVPoolAllocator.alloc_decode()
  BaseTokenToKVPoolAllocator.clear()
  BaseTokenToKVPoolAllocator.alloc(need_size: int)
  BaseTokenToKVPoolAllocator.free(free_index: torch.Tensor)
  TokenToKVPoolAllocator.__init__(size: int, dtype: torch.dtype, device: str, kvcache: KVCache, need_sort: bool)
  TokenToKVPoolAllocator.clear()
  TokenToKVPoolAllocator.available_size()
  TokenToKVPoolAllocator.alloc(need_size: int)
  TokenToKVPoolAllocator.free(free_index: torch.Tensor)
  TokenToKVPoolAllocator.get_cpu_copy(indices)
  TokenToKVPoolAllocator.load_cpu_copy(kv_cache_cpu, indices)
  SWATokenToKVPoolAllocator.__init__(size: int, size_swa: int, dtype: torch.dtype, device: str, kvcache: SWAKVPool, need_sort: bool)
  SWATokenToKVPoolAllocator.available_size()
  SWATokenToKVPoolAllocator.full_available_size()
  SWATokenToKVPoolAllocator.swa_available_size()
  SWATokenToKVPoolAllocator.size_full()
  SWATokenToKVPoolAllocator.size_swa()
  SWATokenToKVPoolAllocator.debug_print() -> str
  SWATokenToKVPoolAllocator.get_kvcache()
  SWATokenToKVPoolAllocator.translate_loc_from_full_to_swa(kv_indices: torch.Tensor)
  SWATokenToKVPoolAllocator.alloc(need_size: int)
  SWATokenToKVPoolAllocator.free(free_index: torch.Tensor)
  SWATokenToKVPoolAllocator.free_swa(free_index: torch.Tensor)
  SWATokenToKVPoolAllocator.backup_state()
  SWATokenToKVPoolAllocator.restore_state(state)
  SWATokenToKVPoolAllocator.clear()
alloc_extend_kernel(pre_lens_ptr, seq_lens_ptr, last_loc_ptr, free_page_ptr, out_indices, ret_values, bs_upper: tl.constexpr, page_size: tl.constexpr, max_num_extend_tokens: tl.constexpr)
alloc_decode_kernel(seq_lens_ptr, last_loc_ptr, free_page_ptr, out_indices, ret_values, bs_upper: tl.constexpr, page_size: tl.constexpr)
  PagedTokenToKVPoolAllocator.__init__(size: int, page_size: int, dtype: torch.dtype, device: str, kvcache: KVCache, need_sort: bool)
  PagedTokenToKVPoolAllocator.alloc(need_size: int)
  PagedTokenToKVPoolAllocator.alloc_extend(prefix_lens: torch.Tensor, seq_lens: torch.Tensor, last_loc: torch.Tensor, extend_num_tokens: int)
  PagedTokenToKVPoolAllocator.alloc_decode(seq_lens: torch.Tensor, last_loc: torch.Tensor)
  PagedTokenToKVPoolAllocator.free(free_index: torch.Tensor)
  PagedTokenToKVPoolAllocator.clear()
  PagedTokenToKVPoolAllocator.get_cpu_copy(indices)
  PagedTokenToKVPoolAllocator.load_cpu_copy(kv_cache_cpu, indices)

# python/sglang/srt/mem_cache/allocator_ascend.py
alloc_extend_kernel_ascend(prefix_lens, seq_lens, last_loc, free_pages, out_indices, page_size, device)
  AscendPagedTokenToKVPoolAllocator.alloc_extend(prefix_lens: torch.Tensor, seq_lens: torch.Tensor, last_loc: torch.Tensor, extend_num_tokens: int)
  AscendPagedTokenToKVPoolAllocator.alloc_decode(seq_lens: torch.Tensor, last_loc: torch.Tensor)

# python/sglang/srt/mem_cache/base_prefix_cache.py
  BasePrefixCache.reset()
  BasePrefixCache.match_prefix(key: List[int]) -> MatchResult
  BasePrefixCache.cache_finished_req(req: Req)
  BasePrefixCache.cache_unfinished_req(req: Req)
  BasePrefixCache.evict(num_tokens: int)
  BasePrefixCache.inc_lock_ref(node: Any)
  BasePrefixCache.dec_lock_ref(node: Any, swa_uuid_for_lock: Optional[str])
  BasePrefixCache.evictable_size()
  BasePrefixCache.full_evictable_size()
  BasePrefixCache.swa_evictable_size()
  BasePrefixCache.protected_size()
  BasePrefixCache.full_protected_size()
  BasePrefixCache.swa_protected_size()
  BasePrefixCache.total_size()
  BasePrefixCache.pretty_print()
  BasePrefixCache.init_load_back(last_host_node: Any, host_hit_length: int) -> Tuple[torch.Tensor, Any]
  BasePrefixCache.ready_to_load_host_cache() -> Any
  BasePrefixCache.check_hicache_events() -> Any
  BasePrefixCache.take_events()

# python/sglang/srt/mem_cache/chunk_cache.py
  ChunkCache.__init__(req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, page_size: int)
  ChunkCache.reset()
  ChunkCache.match_prefix() -> MatchResult
  ChunkCache.cache_finished_req(req: Req)
  ChunkCache.cache_unfinished_req(req: Req, chunked)
  ChunkCache.evict(num_tokens: int)
  ChunkCache.inc_lock_ref(node: Any)
  ChunkCache.dec_lock_ref(node: Any, swa_uuid_for_lock: Optional[str])
  ChunkCache.pretty_print()
  SWAChunkCache.__init__(req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: SWATokenToKVPoolAllocator, page_size: int)
  SWAChunkCache.evict_swa(req: Req, prelen: int, attention_chunk_size: int)
  SWAChunkCache.evict(num_tokens: int)

# python/sglang/srt/mem_cache/cpp_radix_tree/radix_tree.py
  RadixTreeCpp.__init__(disabled: bool, host_size: Optional[int], page_size: int, write_through_threshold: int)
  RadixTreeCpp.match_prefix(prefix: List[int]) -> Tuple[List[torch.Tensor], int, TreeNodeCpp, TreeNodeCpp]
  RadixTreeCpp.evict(num_tokens: int) -> List[torch.Tensor]
  RadixTreeCpp.lock_ref(handle: TreeNodeCpp, lock: bool) -> None
  RadixTreeCpp.writing_through(key: List[int], indices: torch.Tensor) -> Tuple[List[Tuple[IOHandle, torch.Tensor, torch.Tensor]], int]
  RadixTreeCpp.loading_onboard(host_node: TreeNodeCpp, new_device_indices: torch.Tensor) -> Tuple[IOHandle, List[torch.Tensor]]
  RadixTreeCpp.commit_writing_through(handle: IOHandle, success: bool) -> None
  RadixTreeCpp.commit_loading_onboard(handle: IOHandle, success: bool) -> None
  RadixTreeCpp.evictable_size() -> int
  RadixTreeCpp.protected_size() -> int
  RadixTreeCpp.total_size() -> int
  RadixTreeCpp.reset() -> None
  RadixTreeCpp.debug_print() -> None

# python/sglang/srt/mem_cache/hicache_storage.py
get_hash_str(token_ids: List[int], prior_hash: str) -> str
  HiCacheStorage.get(key: str, target_location: Optional[Any], target_sizes: Optional[Any]) -> torch.Tensor | None
  HiCacheStorage.batch_get(keys: List[str], target_locations: Optional[Any], target_sizes: Optional[Any]) -> List[torch.Tensor | None] | int
  HiCacheStorage.set(key: str, value: Optional[Any], target_location: Optional[Any], target_sizes: Optional[Any]) -> bool
  HiCacheStorage.batch_set(keys: List[str], values: Optional[Any], target_locations: Optional[Any], target_sizes: Optional[Any]) -> bool
  HiCacheStorage.exists(key: str) -> bool
  HiCacheStorage.delete(key: str) -> bool
  HiCacheStorage.clear() -> bool
  HiCacheStorage.batch_exists(keys: List[str]) -> int
  HiCacheFile.__init__(storage_config: HiCacheStorageConfig, file_path: str)
  HiCacheFile.get(key: str, target_location: torch.Tensor, target_sizes: Optional[Any]) -> torch.Tensor | None
  HiCacheFile.batch_get(keys: List[str], target_locations: List[torch.Tensor], target_sizes: Optional[Any]) -> List[torch.Tensor | None]
  HiCacheFile.set(key: str, value: Optional[Any], target_location: Optional[Any], target_sizes: Optional[Any]) -> bool
  HiCacheFile.batch_set(keys: List[str], values: Optional[Any], target_locations: Optional[Any], target_sizes: Optional[Any]) -> bool
  HiCacheFile.exists(key: str) -> bool
  HiCacheFile.delete(key: str) -> None
  HiCacheFile.clear() -> bool

# python/sglang/srt/mem_cache/hiradix_cache.py
  HiRadixCache.__init__(req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, tp_cache_group: torch.distributed.ProcessGroup, page_size: int, hicache_ratio: float, hicache_size: int, hicache_write_policy: str, hicache_io_backend: str, hicache_mem_layout: str, hicache_storage_backend: Optional[str], hicache_storage_prefetch_policy: Optional[str], model_name: Optional[str], storage_backend_extra_config: Optional[str])
  HiRadixCache.reset()
  HiRadixCache.get_height(node: TreeNode)
  HiRadixCache.clear_storage_backend()
  HiRadixCache.write_backup(node: TreeNode, write_back)
  HiRadixCache.write_backup_storage(node: TreeNode)
  HiRadixCache.writing_check(write_back)
  HiRadixCache.loading_check()
  HiRadixCache.evictable_size()
  HiRadixCache.evict(num_tokens: int)
  HiRadixCache.evict_host(num_tokens: int)
  HiRadixCache.load_back(node: TreeNode, mem_quota: Optional[int]) -> Optional[torch.Tensor]
  HiRadixCache.init_load_back(last_node: TreeNode, host_hit_length: int, mem_quota: Optional[int])
  HiRadixCache.ready_to_load_host_cache()
  HiRadixCache.check_hicache_events()
  HiRadixCache.drain_storage_control_queues()
  HiRadixCache.can_terminate_prefetch(operation: PrefetchOperation)
  HiRadixCache.check_prefetch_progress(req_id: str) -> bool
  HiRadixCache.match_prefix(key: List[int])
  HiRadixCache.prefetch_from_storage(req_id: str, last_host_node: TreeNode, new_input_tokens: List[int], last_hash: Optional[str])
  HiRadixCache.insert(key: List, value, chunked)

# python/sglang/srt/mem_cache/lora_radix_cache.py
  LoRAKey.__init__(lora_id: str, token_ids: List[int])
  LoRAKey.__len__()
get_child_key(key: LoRAKey)
  LoRATreeNode.__init__(id: Optional[int])
  LoRATreeNode.evicted()
  LoRATreeNode.__lt__(other: 'LoRATreeNode')
  LoRARadixCache.__init__(req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, page_size: int, disable: bool)
  LoRARadixCache.reset()
  LoRARadixCache.match_prefix(key: List[int]) -> MatchResult
  LoRARadixCache.match_prefix_with_lora_id(key: LoRAKey) -> MatchResult
  LoRARadixCache.insert(key: LoRAKey, value)
  LoRARadixCache.cache_finished_req(req: Req)
  LoRARadixCache.cache_unfinished_req(req: Req, chunked)
  LoRARadixCache.pretty_print()
  LoRARadixCache.total_size()
  LoRARadixCache.evict(num_tokens: int)
  LoRARadixCache.inc_lock_ref(node: LoRATreeNode)
  LoRARadixCache.dec_lock_ref(node: LoRATreeNode)
  LoRARadixCache.evictable_size()
  LoRARadixCache.protected_size()
  LoRARadixCache.all_values_flatten()

# python/sglang/srt/mem_cache/memory_pool.py
  ReqToTokenPool.__init__(size: int, max_context_len: int, device: str, enable_memory_saver: bool)
  ReqToTokenPool.write(indices, values)
  ReqToTokenPool.available_size()
  ReqToTokenPool.alloc(need_size: int) -> List[int]
  ReqToTokenPool.free(free_index: Union[int, List[int]])
  ReqToTokenPool.clear()
  KVCache.__init__(size: int, page_size: int, dtype: torch.dtype, layer_num: int, device: str, enable_memory_saver: bool, start_layer: Optional[int], end_layer: Optional[int])
  KVCache.get_key_buffer(layer_id: int) -> torch.Tensor
  KVCache.get_value_buffer(layer_id: int) -> torch.Tensor
  KVCache.get_kv_buffer(layer_id: int) -> Tuple[torch.Tensor, torch.Tensor]
  KVCache.set_kv_buffer(layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor) -> None
  KVCache.register_layer_transfer_counter(layer_transfer_counter)
  KVCache.get_cpu_copy(indices)
  KVCache.load_cpu_copy(kv_cache_cpu, indices)
  MHATokenToKVPool.__init__(size: int, page_size: int, dtype: torch.dtype, head_num: int, head_dim: int, layer_num: int, device: str, enable_memory_saver: bool, start_layer: Optional[int], end_layer: Optional[int])
  MHATokenToKVPool.get_kv_size_bytes()
  MHATokenToKVPool.get_contiguous_buf_infos()
  MHATokenToKVPool.maybe_get_custom_mem_pool()
  MHATokenToKVPool.get_cpu_copy(indices)
  MHATokenToKVPool.load_cpu_copy(kv_cache_cpu, indices)
  MHATokenToKVPool.get_key_buffer(layer_id: int)
  MHATokenToKVPool.get_value_buffer(layer_id: int)
  MHATokenToKVPool.get_kv_buffer(layer_id: int)
  MHATokenToKVPool.set_kv_buffer(layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor, k_scale: Optional[float], v_scale: Optional[float], layer_id_override: Optional[int])
  MHATokenToKVPool.move_kv_cache(tgt_loc: torch.Tensor, src_loc: torch.Tensor)
  SWAKVPool.__init__(size: int, size_swa: int, dtype: torch.dtype, head_num: int, head_dim: int, swa_attention_layer_ids: List[int], full_attention_layer_ids: List[int], enable_kvcache_transpose: bool, device: str)
  SWAKVPool.get_kv_size_bytes()
  SWAKVPool.get_contiguous_buf_infos()
  SWAKVPool.get_key_buffer(layer_id: int)
  SWAKVPool.get_value_buffer(layer_id: int)
  SWAKVPool.get_kv_buffer(layer_id: int)
  SWAKVPool.translate_loc_from_full_to_swa(kv_indices: torch.Tensor)
  SWAKVPool.set_kv_buffer(layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor, k_scale: float, v_scale: float)
  AscendTokenToKVPool.get_contiguous_buf_infos()
  AscendTokenToKVPool.set_kv_buffer(layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor, k_scale: Optional[float], v_scale: Optional[float])
set_mla_kv_buffer_kernel(kv_buffer_ptr, cache_k_nope_ptr, cache_k_rope_ptr, loc_ptr, buffer_stride: tl.constexpr, nope_stride: tl.constexpr, rope_stride: tl.constexpr, nope_dim: tl.constexpr, rope_dim: tl.constexpr, BLOCK: tl.constexpr)
set_mla_kv_buffer_triton(kv_buffer: torch.Tensor, loc: torch.Tensor, cache_k_nope: torch.Tensor, cache_k_rope: torch.Tensor)
  MLATokenToKVPool.__init__(size: int, page_size: int, dtype: torch.dtype, kv_lora_rank: int, qk_rope_head_dim: int, layer_num: int, device: str, enable_memory_saver: bool, start_layer: Optional[int], end_layer: Optional[int])
  MLATokenToKVPool.get_kv_size_bytes()
  MLATokenToKVPool.get_contiguous_buf_infos()
  MLATokenToKVPool.maybe_get_custom_mem_pool()
  MLATokenToKVPool.get_key_buffer(layer_id: int)
  MLATokenToKVPool.get_value_buffer(layer_id: int)
  MLATokenToKVPool.get_kv_buffer(layer_id: int)
  MLATokenToKVPool.set_kv_buffer(layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor)
  MLATokenToKVPool.set_mla_kv_buffer(layer: RadixAttention, loc: torch.Tensor, cache_k_nope: torch.Tensor, cache_k_rope: torch.Tensor)
  MLATokenToKVPool.get_cpu_copy(indices)
  MLATokenToKVPool.load_cpu_copy(kv_cache_cpu, indices)
  AscendMLAPagedTokenToKVPool.__init__(size: int, page_size: int, dtype: torch.dtype, kv_lora_rank: int, qk_rope_head_dim: int, layer_num: int, device: str, enable_memory_saver: bool, start_layer: Optional[int], end_layer: Optional[int])
  AscendMLAPagedTokenToKVPool.get_kv_size_bytes()
  AscendMLAPagedTokenToKVPool.get_kv_buffer(layer_id: int)
  AscendMLAPagedTokenToKVPool.get_key_buffer(layer_id: int)
  AscendMLAPagedTokenToKVPool.get_value_buffer(layer_id: int)
  AscendMLAPagedTokenToKVPool.get_contiguous_buf_infos()
  AscendMLAPagedTokenToKVPool.set_kv_buffer(layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor)
  DoubleSparseTokenToKVPool.__init__(size: int, page_size: int, dtype: torch.dtype, head_num: int, head_dim: int, layer_num: int, device: str, heavy_channel_num: int, enable_memory_saver: bool, start_layer: Optional[int], end_layer: Optional[int])
  DoubleSparseTokenToKVPool.get_key_buffer(layer_id: int)
  DoubleSparseTokenToKVPool.get_value_buffer(layer_id: int)
  DoubleSparseTokenToKVPool.get_label_buffer(layer_id: int)
  DoubleSparseTokenToKVPool.get_kv_buffer(layer_id: int)
  DoubleSparseTokenToKVPool.set_kv_buffer(layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor, cache_label: torch.Tensor)
copy_all_layer_kv_cache(data_ptrs, strides, tgt_loc_ptr, src_loc_ptr, num_locs, num_locs_upper: tl.constexpr)

# python/sglang/srt/mem_cache/memory_pool_host.py
synchronized(debug_only)
  HostKVCache.__init__(device_pool: KVCache, host_to_device_ratio: float, host_size: int, page_size: int, layout: str, pin_memory: bool, device: str)
  HostKVCache.get_size_per_token()
  HostKVCache.init_kv_buffer()
  HostKVCache.load_to_device_per_layer(device_pool, host_indices, device_indices, layer_id, io_backend) -> None
  HostKVCache.backup_from_device_all_layer(device_pool, host_indices, device_indices, io_backend) -> None
  HostKVCache.get_flat_data_page(index) -> torch.Tensor
  HostKVCache.get_dummy_flat_data_page() -> torch.Tensor
  HostKVCache.set_from_flat_data_page(index: int, data_page: torch.Tensor) -> None
  HostKVCache.clear()
  HostKVCache.available_size()
  HostKVCache.alloc(need_size: int) -> torch.Tensor
  HostKVCache.free(indices: torch.Tensor) -> int
  HostKVCache.get_state(indices: torch.Tensor) -> MemoryStateInt
  HostKVCache.is_reserved(indices: torch.Tensor) -> bool
  HostKVCache.is_protected(indices: torch.Tensor) -> bool
  HostKVCache.is_synced(indices: torch.Tensor) -> bool
  HostKVCache.is_backup(indices: torch.Tensor) -> bool
  HostKVCache.update_backup(indices: torch.Tensor)
  HostKVCache.update_prefetch(indices: torch.Tensor)
  HostKVCache.update_synced(indices: torch.Tensor)
  HostKVCache.protect_write(indices: torch.Tensor)
  HostKVCache.protect_load(indices: torch.Tensor)
  HostKVCache.complete_io(indices: torch.Tensor)
  MHATokenToKVPoolHost.__init__(device_pool: MHATokenToKVPool, host_to_device_ratio: float, host_size: int, page_size: int, layout: str, pin_memory: bool, device: str)
  MHATokenToKVPoolHost.get_size_per_token()
  MHATokenToKVPoolHost.get_ksize_per_token()
  MHATokenToKVPoolHost.init_kv_buffer()
  MHATokenToKVPoolHost.k_buffer()
  MHATokenToKVPoolHost.v_buffer()
  MHATokenToKVPoolHost.load_to_device_per_layer(device_pool, host_indices, device_indices, layer_id, io_backend)
  MHATokenToKVPoolHost.backup_from_device_all_layer(device_pool, host_indices, device_indices, io_backend)
  MHATokenToKVPoolHost.get_flat_data_page(index) -> torch.Tensor
  MHATokenToKVPoolHost.get_dummy_flat_data_page() -> torch.Tensor
  MHATokenToKVPoolHost.set_from_flat_data_page(index: int, data_page: torch.Tensor) -> None
  MHATokenToKVPoolHost.get_buffer_meta(keys, indices, local_rank)
  MHATokenToKVPoolHost.get_buffer_with_hash(keys, indices)
  MLATokenToKVPoolHost.__init__(device_pool: MLATokenToKVPool, host_to_device_ratio: float, host_size: int, page_size: int, layout: str, pin_memory: bool, device: str)
  MLATokenToKVPoolHost.get_size_per_token()
  MLATokenToKVPoolHost.get_ksize_per_token()
  MLATokenToKVPoolHost.init_kv_buffer()
  MLATokenToKVPoolHost.load_to_device_per_layer(device_pool, host_indices, device_indices, layer_id, io_backend)
  MLATokenToKVPoolHost.backup_from_device_all_layer(device_pool, host_indices, device_indices, io_backend)
  MLATokenToKVPoolHost.get_flat_data_page(index) -> torch.Tensor
  MLATokenToKVPoolHost.get_dummy_flat_data_page() -> torch.Tensor
  MLATokenToKVPoolHost.set_from_flat_data_page(index: int, data_page: torch.Tensor) -> None
  MLATokenToKVPoolHost.get_buffer_meta(keys, indices, local_rank)
  MLATokenToKVPoolHost.get_buffer_with_hash(keys, indices)

# python/sglang/srt/mem_cache/multimodal_cache.py
  MultiModalCache.__init__(max_size: int)
  MultiModalCache.put(mm_hash: int, embedding: torch.Tensor) -> bool
  MultiModalCache.has(mm_hash: int) -> bool
  MultiModalCache.get(mm_hash: int) -> torch.Tensor
  MultiModalCache.clear()
  MultiModalCache.__len__()

# python/sglang/srt/mem_cache/radix_cache.py
  TreeNode.__init__(id: Optional[int])
  TreeNode.evicted()
  TreeNode.backuped()
  TreeNode.protect_host()
  TreeNode.release_host()
  TreeNode.get_last_hash_value() -> Optional[str]
  TreeNode.__lt__(other: 'TreeNode')
  RadixCache.__init__(req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, page_size: int, disable: bool, enable_kv_cache_events: bool)
  RadixCache.reset()
  RadixCache.match_prefix(key: List[int]) -> MatchResult
  RadixCache.insert(key: List, value, chunked)
  RadixCache.cache_finished_req(req: Req)
  RadixCache.cache_unfinished_req(req: Req, chunked)
  RadixCache.pretty_print()
  RadixCache.total_size()
  RadixCache.evict(num_tokens: int)
  RadixCache.inc_lock_ref(node: TreeNode)
  RadixCache.dec_lock_ref(node: TreeNode)
  RadixCache.evictable_size()
  RadixCache.protected_size()
  RadixCache.all_values_flatten()
  RadixCache.take_events()

# python/sglang/srt/mem_cache/radix_cache_cpp.py
  RadixCacheCpp.__init__(disable: bool, use_hicache: bool, req_to_token_pool: ReqToTokenPool, token_to_kv_pool: BaseTokenToKVPoolAllocator, tp_cache_group: torch.distributed.ProcessGroup, page_size: int, hicache_ratio: float, hicache_size: int, hicache_write_policy: str, enable_kv_cache_events: bool, hicache_oracle: bool, enable_write_cancel: bool)
  RadixCacheCpp.reset()
  RadixCacheCpp.match_prefix(key: List[int]) -> MatchResult
  RadixCacheCpp.dec_lock_ref(node: TreeNodeCpp)
  RadixCacheCpp.inc_lock_ref(node: TreeNodeCpp)
  RadixCacheCpp.evict(num_tokens: int)
  RadixCacheCpp.evictable_size()
  RadixCacheCpp.protected_size()
  RadixCacheCpp.total_size()
  RadixCacheCpp.cache_finished_req(req: Req)
  RadixCacheCpp.cache_unfinished_req(req: Req, chunked)
  RadixCacheCpp.pretty_print()

# python/sglang/srt/mem_cache/storage/hf3fs/client_hf3fs.py
rsynchronized()
wsynchronized()
  Hf3fsClient.__init__(path: str, size: int, bytes_per_page: int, entries: int)
  Hf3fsClient.batch_read(offsets: List[int], tensors: List[torch.Tensor]) -> List[int]
  Hf3fsClient.batch_write(offsets: List[int], tensors: List[torch.Tensor]) -> List[int]
  Hf3fsClient.check(offsets: List[int], tensors: List[torch.Tensor]) -> None
  Hf3fsClient.get_size() -> int
  Hf3fsClient.close() -> None
  Hf3fsClient.flush() -> None

# python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py
  RankMetadata.__init__(num_pages: int)
  RankMetadata.exists_keys(keys: List[str]) -> List[bool]
  RankMetadata.reserve_and_allocate_page_indices(keys: List[Tuple[str, str]]) -> List[Tuple[bool, int]]
  RankMetadata.confirm_write(written_keys_to_confirm: List[Tuple[str, int]], pages_to_release: List[int]) -> None
  RankMetadata.delete_keys(keys: List[str]) -> int
  RankMetadata.clear_all() -> None
  RankMetadata.get_page_indices(keys: List[str]) -> List[Optional[int]]
  GlobalMetadataState.__init__(persistence_path: Optional[str], save_interval: int)
  GlobalMetadataState.load_from_disk()
  GlobalMetadataState.save_to_disk()
  GlobalMetadataState.schedule_save()
  GlobalMetadataState.shutdown()
  Hf3fsMetadataServer.__init__(persistence_path: Optional[str], save_interval: int)
  Hf3fsMetadataServer.get_rank_metadata(rank: int) -> RankMetadata
  Hf3fsMetadataServer.initialize(rank: int, request: Request)
  Hf3fsMetadataServer.exists(rank: int, request: Request)
  Hf3fsMetadataServer.reserve_and_allocate_page_indices(rank: int, request: Request)
  Hf3fsMetadataServer.confirm_write(rank: int, request: Request)
  Hf3fsMetadataServer.delete_keys(rank: int, request: Request)
  Hf3fsMetadataServer.clear(rank: int)
  Hf3fsMetadataServer.get_page_indices(rank: int, request: Request)
  Hf3fsMetadataServer.run(host: str, port: int)
  Hf3fsGlobalMetadataClient.__init__(base_url: str, max_retries: int)
  Hf3fsGlobalMetadataClient.initialize(rank: int, num_pages: int) -> None
  Hf3fsGlobalMetadataClient.reserve_and_allocate_page_indices(rank: int, keys: List[Tuple[str, str]]) -> List[Tuple[bool, int]]
  Hf3fsGlobalMetadataClient.confirm_write(rank: int, written_keys_to_confirm: List[Tuple[str, int]], pages_to_release: List[int]) -> None
  Hf3fsGlobalMetadataClient.delete_keys(rank: int, keys: List[str]) -> None
  Hf3fsGlobalMetadataClient.exists(rank: int, keys: List[str]) -> List[bool]
  Hf3fsGlobalMetadataClient.clear(rank: int) -> None
  Hf3fsGlobalMetadataClient.get_page_indices(rank: int, keys: List[str]) -> List[Optional[int]]
  Hf3fsLocalMetadataClient.__init__()
  Hf3fsLocalMetadataClient.initialize(rank: int, num_pages: int) -> None
  Hf3fsLocalMetadataClient.reserve_and_allocate_page_indices(rank: int, keys: List[Tuple[str, str]]) -> List[Tuple[bool, int]]
  Hf3fsLocalMetadataClient.confirm_write(rank: int, written_keys_to_confirm: List[Tuple[str, int]], pages_to_release: List[int]) -> None
  Hf3fsLocalMetadataClient.delete_keys(rank: int, keys: List[str]) -> None
  Hf3fsLocalMetadataClient.exists(rank: int, keys: List[str]) -> List[bool]
  Hf3fsLocalMetadataClient.clear(rank: int) -> None
  Hf3fsLocalMetadataClient.get_page_indices(rank: int, keys: List[str]) -> List[Optional[int]]
run_metadata_server(host: str, port: int, persistence_path: Optional[str], save_interval: int)

# python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py
  Hf3fsMetadataInterface.initialize(rank: int, num_pages: int) -> None
  Hf3fsMetadataInterface.reserve_and_allocate_page_indices(rank: int, keys: List[Tuple[str, str]]) -> List[Tuple[bool, int]]
  Hf3fsMetadataInterface.confirm_write(rank: int, written_keys_to_confirm: List[Tuple[str, int]], pages_to_release: List[int]) -> None
  Hf3fsMetadataInterface.get_page_indices(rank: int, keys: List[str]) -> List[Optional[int]]
  Hf3fsMetadataInterface.delete_keys(rank: int, keys: List[str]) -> None
  Hf3fsMetadataInterface.exists(rank: int, keys: List[str]) -> List[bool]
  Hf3fsMetadataInterface.clear(rank: int) -> None
  AtomicCounter.__init__(n: int)
  AtomicCounter.next() -> int
synchronized()
  HiCacheHF3FS.__init__(rank: int, file_path: str, file_size: int, numjobs: int, bytes_per_page: int, entries: int, dtype: torch.dtype, metadata_client: Hf3fsMetadataInterface, is_mla_model: bool)
  HiCacheHF3FS.from_env_config(bytes_per_page: int, dtype: torch.dtype, storage_config: HiCacheStorageConfig) -> 'HiCacheHF3FS'
  HiCacheHF3FS.get(key: str, target_location: Optional[Any], target_sizes: Optional[Any]) -> torch.Tensor | None
  HiCacheHF3FS.batch_get(keys: List[str], target_locations: Optional[Any], target_sizes: Optional[Any]) -> List[torch.Tensor | None]
  HiCacheHF3FS.set(key: str, value: Optional[Any], target_location: Optional[Any], target_sizes: Optional[Any]) -> bool
  HiCacheHF3FS.batch_set(keys: List[str], values: Optional[Any], target_locations: Optional[Any], target_sizes: Optional[Any]) -> bool
  HiCacheHF3FS.delete(key: str) -> None
  HiCacheHF3FS.exists(key: str) -> bool
  HiCacheHF3FS.batch_exists(keys: List[str]) -> int
  HiCacheHF3FS.clear() -> bool
  HiCacheHF3FS.close() -> None

# python/sglang/srt/mem_cache/storage/hf3fs/test_hf3fs_utils.py
test_rw_shm()

# python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py
  MooncakeStoreConfig.from_file() -> 'MooncakeStoreConfig'
  MooncakeStoreConfig.load_from_env() -> 'MooncakeStoreConfig'
  MooncakeStoreConfig.__post_init__()
  MooncakeStore.__init__(storage_config: HiCacheStorageConfig)
  MooncakeStore.warmup()
  MooncakeStore.register_buffer(buffer: torch.Tensor) -> None
  MooncakeStore.set(key, value: Optional[Any], target_location: Optional[List[int]], target_sizes: Optional[List[int]]) -> bool
  MooncakeStore.batch_set(keys: List[str], values: Optional[List[torch.Tensor]], target_location: Optional[List[int]], target_sizes: Optional[List[int]]) -> bool
  MooncakeStore.get(key, target_location: Optional[Any], target_sizes: Optional[Any]) -> bool
  MooncakeStore.batch_get(keys: List[str], target_location: Optional[Any], target_sizes: Optional[Any]) -> int
  MooncakeStore.exists(key) -> bool
  MooncakeStore.batch_exists(keys) -> int
  MooncakeStore.delete(key) -> None
  MooncakeStore.close()
  MooncakeStore.clear() -> None

# python/sglang/srt/mem_cache/storage/mooncake_store/unit_test.py
test_init_and_warmup()
test_register_buffer()
test_set_and_get()
test_exists()

# python/sglang/srt/mem_cache/storage/nixl/hicache_nixl.py
  HiCacheNixl.__init__(file_path: str, plugin: str)
  HiCacheNixl.register_buffers(buffers: Union[torch.Tensor, List[torch.Tensor], List[tuple]]) -> Optional[Any]
  HiCacheNixl.register_files(file_paths: List[str], open_file: Optional[bool]) -> Optional[Any]
  HiCacheNixl.register_objects(keys: List[str], sizes: Optional[List[int]]) -> Optional[Any]
  HiCacheNixl.get(key: str, target_location: Optional[torch.Tensor | int], target_sizes: Optional[int]) -> torch.Tensor | None
  HiCacheNixl.batch_get(keys: List[str], target_locations: Optional[List[torch.Tensor | int]], target_sizes: Optional[List[int]]) -> List[torch.Tensor | None]
  HiCacheNixl.set(key: str, value: Optional[torch.Tensor], target_location: Optional[int], target_sizes: Optional[int]) -> bool
  HiCacheNixl.batch_set(keys: List[str], values: Optional[List[torch.Tensor]], target_locations: Optional[List[int]], target_sizes: Optional[List[int]]) -> bool
  HiCacheNixl.exists(key: str) -> bool

# python/sglang/srt/mem_cache/storage/nixl/nixl_utils.py
  NixlBackendSelection.__init__(plugin: str)
  NixlBackendSelection.set_bucket(bucket_name: str) -> None
  NixlBackendSelection.create_backend(agent) -> bool
  NixlRegistration.__init__(agent)
  NixlRegistration.create_query_tuples(key: str, mem_type: str, file_manager) -> List[Tuple]
  NixlFileManager.__init__(base_dir: str)
  NixlFileManager.get_file_path(key: str) -> str
  NixlFileManager.create_file(file_path: str) -> bool
  NixlFileManager.open_file(file_path: str) -> Optional[int]
  NixlFileManager.close_file(fd: int) -> bool
  NixlFileManager.files_to_nixl_tuples(file_paths: List[str]) -> List[Tuple[int, int, int, str]]

# python/sglang/srt/mem_cache/storage/nixl/test_hicache_nixl_storage.py
  TestNixlUnified.setUp()
  TestNixlUnified.tearDown()
  TestNixlUnified.delete_test_file(file_path: str) -> bool
  TestNixlUnified.verify_tensors_equal(expected: torch.Tensor, actual: torch.Tensor)
  TestNixlUnified.verify_tensor_lists_equal(expected: List[torch.Tensor], actual: List[torch.Tensor])
  TestNixlUnified.test_single_set_get()
  TestNixlUnified.test_batch_set_get()
  TestNixlUnified.test_mixed_operations()
  TestNixlUnified.test_data_integrity()
  TestNixlUnified.test_basic_file_operations()
  TestNixlUnified.test_create_nixl_tuples()
  TestNixlUnified.test_error_handling()
  TestNixlUnified.test_register_buffers()
  TestNixlUnified.test_register_files_with_tuples()

# python/sglang/srt/mem_cache/swa_radix_cache.py
  TreeNode.__init__(id: Optional[int])
  TreeNode.evicted()
  TreeNode.backuped()
  TreeNode.__lt__(other: 'TreeNode')
gen_swa_uuid() -> int
  LRUList.__init__(swa: bool)
  LRUList.reset_node_mru(node)
  LRUList.reset_node_and_parents_mru(node, root_node)
  LRUList.insert_mru(node)
  LRUList.remove_node(node: TreeNode)
  LRUList.get_lru_no_lock() -> Optional[TreeNode]
  LRUList.get_leaf_lru_no_lock() -> Optional[TreeNode]
  LRUList.get_prev_no_lock(node: TreeNode, check_id: bool) -> Optional[TreeNode]
  LRUList.get_prev_leaf_no_lock(node: TreeNode, check_id: bool)
  LRUList.in_list(node: Optional[TreeNode])
  LRUList.sanity_check_evictable_size()
  LRUList.sanity_check(tree_cache: 'SWARadixCache')
  SWARadixCache.__init__(req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: SWATokenToKVPoolAllocator, sliding_window_size: int, page_size: int, disable: bool)
  SWARadixCache.reset() -> None
  SWARadixCache.match_prefix(key: List[int]) -> MatchResult
  SWARadixCache.insert(key: List, value, prev_prefix_len: int) -> int
  SWARadixCache.cache_finished_req(req: Req) -> None
  SWARadixCache.cache_unfinished_req(req: Req, chunked) -> None
  SWARadixCache.pretty_print() -> None
  SWARadixCache.total_size() -> Tuple[int, int]
  SWARadixCache.evict(full_num_tokens: int, swa_num_tokens: int) -> None
  SWARadixCache.inc_lock_ref(node: TreeNode) -> Optional[int]
  SWARadixCache.dec_lock_ref(node: TreeNode, swa_uuid_for_lock: Optional[int])
  SWARadixCache.sanity_check()
  SWARadixCache.evictable_size() -> Tuple[int, int]
  SWARadixCache.full_evictable_size() -> int
  SWARadixCache.swa_evictable_size() -> int
  SWARadixCache.full_lru_list_evictable_size() -> int
  SWARadixCache.swa_lru_list_evictable_size() -> int
  SWARadixCache.protected_size() -> Tuple[int, int]
  SWARadixCache.full_protected_size() -> int
  SWARadixCache.swa_protected_size() -> int
  SWARadixCache.all_values_flatten() -> torch.Tensor

# python/sglang/srt/metrics/collector.py
  TimeStats.__str__() -> str
  TimeStats.format_duration(duration: float) -> str
  TimeStats.get_type() -> RequestType
  SchedulerMetricsCollector.__init__(labels: Dict[str, str]) -> None
  SchedulerMetricsCollector.increment_bootstrap_failed_reqs() -> None
  SchedulerMetricsCollector.increment_transfer_failed_reqs() -> None
  SchedulerMetricsCollector.log_stats(stats: SchedulerStats) -> None
  TokenizerMetricsCollector.__init__(labels: Dict[str, str], bucket_time_to_first_token: Optional[List[float]], bucket_inter_token_latency: Optional[List[float]], bucket_e2e_request_latency: Optional[List[float]], collect_tokens_histogram: bool) -> None
  TokenizerMetricsCollector.observe_one_finished_request(prompt_tokens: int, generation_tokens: int, cached_tokens: int, e2e_latency: float, has_grammar: bool)
  TokenizerMetricsCollector.observe_time_to_first_token(value: float)
  TokenizerMetricsCollector.observe_inter_token_latency(internval: float, num_new_tokens: int)
  TokenizerMetricsCollector.observe_one_aborted_request()

# python/sglang/srt/metrics/func_timer.py
enable_func_timer()
exponential_buckets(start: float, width: float, length: int) -> List[float]
time_func_latency(func: Callable, name: Optional[str]) -> Callable[..., Any]

# python/sglang/srt/model_executor/cuda_graph_runner.py
get_is_capture_mode()
model_capture_mode()
freeze_gc(enable_cudagraph_gc: bool)
patch_model(model: torch.nn.Module, enable_compile: bool, num_tokens: int, tp_group: GroupCoordinator)
set_torch_compile_config()
get_batch_sizes_to_capture(model_runner: ModelRunner)
get_global_graph_memory_pool()
set_global_graph_memory_pool(val)
  CudaGraphRunner.__init__(model_runner: ModelRunner)
  CudaGraphRunner.can_run(forward_batch: ForwardBatch)
  CudaGraphRunner.capture() -> None
  CudaGraphRunner.capture_one_batch_size(bs: int, forward: Callable)
  CudaGraphRunner.recapture_if_needed(forward_batch: ForwardBatch)
  CudaGraphRunner.replay_prepare(forward_batch: ForwardBatch, pp_proxy_tensors: Optional[PPProxyTensors])
  CudaGraphRunner.replay(forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors: Optional[PPProxyTensors]) -> Union[LogitsProcessorOutput, PPProxyTensors]
  CudaGraphRunner.get_spec_info(num_tokens: int)

# python/sglang/srt/model_executor/forward_batch_info.py
  ForwardMode.is_prefill()
  ForwardMode.is_extend()
  ForwardMode.is_decode()
  ForwardMode.is_mixed()
  ForwardMode.is_idle()
  ForwardMode.is_decode_or_idle()
  ForwardMode.is_target_verify()
  ForwardMode.is_draft_extend()
  ForwardMode.is_extend_or_draft_extend_or_mixed()
  ForwardMode.is_cuda_graph()
  ForwardMode.is_dummy_first()
  ForwardMode.is_split_prefill()
  CaptureHiddenMode.need_capture()
  CaptureHiddenMode.is_full()
  CaptureHiddenMode.is_last()
  CaptureHiddenMode.__lt__(other)
  ForwardBatch.init_new(cls, batch: ModelWorkerBatch, model_runner: ModelRunner)
  ForwardBatch.merge_mm_inputs() -> Optional[MultimodalInputs]
  ForwardBatch.contains_image_inputs() -> bool
  ForwardBatch.contains_audio_inputs() -> bool
  ForwardBatch.contains_video_inputs() -> bool
  ForwardBatch.contains_mm_inputs() -> bool
  ForwardBatch.get_max_chunk_capacity()
  ForwardBatch.set_prefix_chunk_idx(idx: int)
  ForwardBatch.set_attn_attend_prefix_cache(attn_attend_prefix_cache: bool)
  ForwardBatch.prepare_chunked_kv_indices(device: torch.device)
  ForwardBatch.prepare_mlp_sync_batch(model_runner: ModelRunner)
  ForwardBatch.post_forward_mlp_sync_batch(logits_output: LogitsProcessorOutput)
  ForwardBatch.get_prefix_chunk_seq_lens(prefix_lens: torch.Tensor, num_prefix_chunks: int, prefix_chunk_len: int)
  ForwardBatch.prepare_chunked_prefix_cache_info(device: torch.device)
  ForwardBatch.can_run_tbo()
enable_num_token_non_padded(server_args)
  PPProxyTensors.__init__(tensors)
  PPProxyTensors.__getitem__(key: Union[str, slice])
  PPProxyTensors.__setitem__(key: str, value: torch.Tensor)
  PPProxyTensors.__len__()
  PPProxyTensors.__eq__(other: object)
  PPProxyTensors.__repr__() -> str
compute_position(attn_backend: str, extend_prefix_lens: torch.Tensor, extend_seq_lens: torch.Tensor, extend_seq_lens_sum: int)
compute_position_triton(extend_prefix_lens: torch.Tensor, extend_seq_lens: torch.Tensor, extend_seq_lens_sum)
compute_position_kernel(positions, extend_start_loc, extend_prefix_lens, extend_seq_lens, has_prefix: tl.constexpr)
compute_position_torch(extend_prefix_lens: torch.Tensor, extend_seq_lens: torch.Tensor)
clamp_position(seq_lens)
create_chunked_prefix_cache_kv_indices(req_to_token_ptr, req_pool_indices_ptr, chunk_start_idx_ptr, chunk_seq_lens_ptr, chunk_cu_seq_lens_ptr, chunk_kv_indices_ptr, req_to_token_ptr_stride: tl.constexpr)

# python/sglang/srt/model_executor/model_runner.py
  RankZeroFilter.__init__(is_rank_zero)
  RankZeroFilter.filter(record)
  ModelRunner.__init__(model_config: ModelConfig, mem_fraction_static: float, gpu_id: int, tp_rank: int, tp_size: int, moe_ep_rank: int, moe_ep_size: int, pp_rank: int, pp_size: int, nccl_port: int, server_args: ServerArgs, dp_rank: Optional[int], is_draft_worker: bool, req_to_token_pool: Optional[ReqToTokenPool], token_to_kv_pool_allocator: Optional[BaseTokenToKVPoolAllocator])
  ModelRunner.initialize(min_per_gpu_memory: float)
  ModelRunner.model_specific_adjustment()
  ModelRunner.init_torch_distributed()
  ModelRunner.load_model()
  ModelRunner.update_expert_location(new_expert_location_metadata: ExpertLocationMetadata, update_layer_ids: List[int])
  ModelRunner.update_weights_from_disk(model_path: str, load_format: str) -> tuple[bool, str]
  ModelRunner.init_weights_update_group(master_address, master_port, rank_offset, world_size, group_name, backend)
  ModelRunner.update_weights_from_distributed(names, dtypes, shapes, group_name)
  ModelRunner.update_weights_from_tensor(named_tensors: List[Tuple[str, Union[torch.Tensor, 'LocalSerializedTensor']]], load_format: Optional[str])
  ModelRunner.get_weights_by_name(name: str, truncate_size: int) -> Optional[torch.Tensor]
  ModelRunner.init_lora_manager()
  ModelRunner.load_lora_adapter(lora_ref: LoRARef)
  ModelRunner.unload_lora_adapter(lora_ref: LoRARef)
  ModelRunner.profile_max_num_token(total_gpu_memory: int)
  ModelRunner.set_num_token_hybrid()
  ModelRunner.init_memory_pool(total_gpu_memory: int, max_num_reqs: Optional[int], max_total_tokens: Optional[int])
  ModelRunner.init_cublas()
  ModelRunner.init_attention_backend()
  ModelRunner.init_double_sparsity_channel_config(selected_channel)
  ModelRunner.init_device_graphs()
  ModelRunner.init_threads_binding()
  ModelRunner.apply_torch_tp()
  ModelRunner.forward_decode(forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors) -> LogitsProcessorOutput
  ModelRunner.forward_extend(forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors) -> LogitsProcessorOutput
  ModelRunner.forward_idle(forward_batch: ForwardBatch, pp_proxy_tensors) -> LogitsProcessorOutput
  ModelRunner.forward_split_prefill(forward_batch: ForwardBatch, reinit_attn_backend: bool, forward_count: int) -> LogitsProcessorOutput
  ModelRunner.forward(forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors: Optional[PPProxyTensors], reinit_attn_backend: bool, split_forward_count: int) -> Tuple[Union[LogitsProcessorOutput, PPProxyTensors], bool]
  ModelRunner.sample(logits_output: LogitsProcessorOutput, forward_batch: ForwardBatch) -> torch.Tensor
  ModelRunner.model_is_mrope() -> bool
  ModelRunner.save_remote_model(url: str)
  ModelRunner.save_sharded_model(path: str, pattern: Optional[str], max_size: Optional[int])
  LocalSerializedTensor.get(rank: int)

# python/sglang/srt/model_executor/npu_graph_runner.py
  NPUGraphRunner.__init__(model_runner: ModelRunner)
  NPUGraphRunner.replay(forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors: Optional[PPProxyTensors]) -> Union[LogitsProcessorOutput, PPProxyTensors]

# python/sglang/srt/model_loader/__init__.py
get_model() -> nn.Module

# python/sglang/srt/model_loader/loader.py
device_loading_context(module: torch.nn.Module, target_device: torch.device)
  BaseModelLoader.__init__(load_config: LoadConfig)
  BaseModelLoader.download_model(model_config: ModelConfig) -> None
  BaseModelLoader.load_model() -> nn.Module
  Source.init_new(cls, model_config: ModelConfig, model)
  DefaultModelLoader.__init__(load_config: LoadConfig)
  DefaultModelLoader.download_model(model_config: ModelConfig) -> None
  DefaultModelLoader.load_model() -> nn.Module
  DefaultModelLoader.load_weights_and_postprocess(model, weights, target_device)
  LayeredModelLoader.__init__(load_config: LoadConfig)
  LayeredModelLoader.load_model() -> nn.Module
  DummyModelLoader.__init__(load_config: LoadConfig)
  DummyModelLoader.download_model(model_config: ModelConfig) -> None
  DummyModelLoader.load_model() -> nn.Module
  ShardedStateLoader.__init__(load_config: LoadConfig)
  ShardedStateLoader.download_model(model_config: ModelConfig) -> None
  ShardedStateLoader.load_model() -> nn.Module
  ShardedStateLoader.save_model(model: torch.nn.Module, path: str, pattern: Optional[str], max_size: Optional[int]) -> None
  BitsAndBytesModelLoader.__init__(load_config: LoadConfig)
  BitsAndBytesModelLoader.download_model(model_config: ModelConfig) -> None
  BitsAndBytesModelLoader.load_model() -> nn.Module
  GGUFModelLoader.__init__(load_config: LoadConfig)
  GGUFModelLoader.download_model(model_config: ModelConfig) -> None
  GGUFModelLoader.load_model() -> nn.Module
  RemoteModelLoader.__init__(load_config: LoadConfig)
  RemoteModelLoader.download_model(model_config: ModelConfig) -> None
  RemoteModelLoader.save_model(model: torch.nn.Module, model_path: str, url: str) -> None
  RemoteModelLoader.load_model() -> nn.Module
load_model_with_cpu_quantization() -> nn.Module
get_model_loader(load_config: LoadConfig) -> BaseModelLoader

# python/sglang/srt/model_loader/utils.py
set_default_torch_dtype(dtype: torch.dtype)
resolve_transformers_arch(model_config: ModelConfig, architectures: list[str])
get_model_architecture(model_config: ModelConfig) -> Tuple[Type[nn.Module], str]
get_architecture_class_name(model_config: ModelConfig) -> str
post_load_weights(model: nn.Module, model_config: ModelConfig)

# python/sglang/srt/model_loader/weight_utils.py
enable_hf_transfer()
  DisabledTqdm.__init__()
get_lock(model_name_or_path: str, cache_dir: Optional[str])
convert_bin_to_safetensor_file(pt_filename: str, sf_filename: str) -> None
get_quant_config(model_config: ModelConfig, load_config: LoadConfig, packed_modules_mapping: Dict[str, List[str]]) -> QuantizationConfig
download_weights_from_hf(model_name_or_path: str, cache_dir: Optional[str], allow_patterns: List[str], revision: Optional[str], ignore_patterns: Optional[Union[str, List[str]]]) -> str
download_safetensors_index_file_from_hf(model_name_or_path: str, index_file: str, cache_dir: Optional[str], revision: Optional[str]) -> None
filter_duplicate_safetensors_files(hf_weights_files: List[str], hf_folder: str, index_file: str) -> List[str]
filter_files_not_needed_for_inference(hf_weights_files: List[str]) -> List[str]
np_cache_weights_iterator(model_name_or_path: str, cache_dir: Optional[str], hf_folder: str, hf_weights_files: List[str]) -> Generator[Tuple[str, torch.Tensor], None, None]
decrypt(fn, key)
safetensors_encrypted_weights_iterator(hf_weights_files: List[str], is_all_weights_sharded: bool, decryption_key: Optional[str])
safetensors_weights_iterator(hf_weights_files: List[str], is_all_weights_sharded: bool, decryption_key: Optional[str], disable_mmap: bool) -> Generator[Tuple[str, torch.Tensor], None, None]
multi_thread_safetensors_weights_iterator(hf_weights_files: List[str], is_all_weights_sharded: bool, decryption_key: Optional[str], max_workers: int, disable_mmap: bool) -> Generator[Tuple[str, torch.Tensor], None, None]
pt_weights_iterator(hf_weights_files: List[str]) -> Generator[Tuple[str, torch.Tensor], None, None]
multi_thread_pt_weights_iterator(hf_weights_files: List[str], max_workers: int) -> Generator[Tuple[str, torch.Tensor], None, None]
get_gguf_extra_tensor_names(gguf_file: str, gguf_to_hf_name_map: Dict[str, str]) -> List[str]
gguf_quant_weights_iterator(gguf_file: str, gguf_to_hf_name_map: Dict[str, str]) -> Generator[Tuple[str, torch.Tensor], None, None]
convert_pyslice_to_tensor(x: Any) -> torch.Tensor
default_weight_loader(param: torch.Tensor, loaded_weight: torch.Tensor) -> None
row_parallel_weight_loader(param: torch.Tensor, loaded_weight: torch.Tensor) -> None
sharded_weight_loader(shard_axis: int) -> LoaderFunction
composed_weight_loader(loader: LoaderFunction, fn: Callable[[torch.Tensor], torch.Tensor]) -> LoaderFunction
runai_safetensors_weights_iterator(hf_weights_files: List[str]) -> Generator[Tuple[str, torch.Tensor], None, None]
set_runai_streamer_env(load_config: LoadConfig)
initialize_dummy_weights(model: torch.nn.Module, low: float, high: float, seed: int) -> None
maybe_remap_kv_scale_name(name: str, params_dict: dict) -> Optional[str]
  KVCacheQuantSchema.check_is_fp8() -> 'KVCacheQuantSchema'
  KVCacheQuantSchema.check_tp_ranks(info: ValidationInfo) -> 'KVCacheQuantSchema'
  KVCacheQuantSchema.check_current_rank(info: ValidationInfo) -> 'KVCacheQuantSchema'
  QuantParamSchema.check_model_type(info: ValidationInfo) -> 'QuantParamSchema'
kv_cache_scales_loader(filename: str, tp_rank: int, tp_size: int, num_hidden_layers: int, model_type: Optional[str]) -> Iterable[Tuple[int, float]]
get_actual_shard_size(shard_size, weight_start, weight_end)
reset_param_data_if_needed(param_data, dim, start, length)
narrow_padded_param_and_loaded_weight(param_data, loaded_weight, param_data_start, weight_start, dim, shard_size, narrow_weight)

# python/sglang/srt/model_parallel.py
tensor_parallel(module: torch.nn.Module, device_mesh: Optional[DeviceMesh])

# python/sglang/srt/models/arcee.py
  ArceeMLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str, reduce_results: bool) -> None
  ArceeMLP.forward(x, forward_batch)
  ArceeAttention.__init__(config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str, bias: bool) -> None
  ArceeAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  ArceeDecoderLayer.__init__(config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  ArceeDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  ArceeModel.__init__(config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  ArceeModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors]) -> Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]], PPProxyTensors]
  ArceeModel.load_kv_cache_scales(quantization_param_path: str) -> None
  ArceeForCausalLM.__init__(config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  ArceeForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors]) -> LogitsProcessorOutput
  ArceeForCausalLM.start_layer()
  ArceeForCausalLM.end_layer()
  ArceeForCausalLM.get_input_embeddings() -> nn.Embedding
  ArceeForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  ArceeForCausalLM.load_kv_cache_scales(quantization_param_path: str) -> None

# python/sglang/srt/models/baichuan.py
  BaiChuanMLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
  BaiChuanMLP.forward(x)
  BaiChuanAttention.__init__(hidden_size: int, num_heads: int, position_embedding: str, rope_theta: float, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], layer_id: int, prefix: str)
  BaiChuanAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  BaiChuanDecoderLayer.__init__(config: PretrainedConfig, position_embedding: str, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  BaiChuanDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  BaiChuanModel.__init__(config: PretrainedConfig, position_embedding: str, quant_config: Optional[QuantizationConfig], prefix: str)
  BaiChuanModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  BaiChuanBaseForCausalLM.__init__(config: PretrainedConfig, position_embedding: str, quant_config: Optional[QuantizationConfig], prefix: str)
  BaiChuanBaseForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  BaiChuanBaseForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  BaichuanForCausalLM.__init__(config, quant_config: Optional[QuantizationConfig], prefix: str)

# python/sglang/srt/models/bailing_moe.py
  BailingAttention.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  BailingAttention.forward(hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  BailingMLP.__init__(intermediate_size: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], reduce_results: Optional[bool], prefix: str) -> None
  BailingMLP.forward(x)
  BailingMoE.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  BailingMoE.forward(hidden_states: torch.Tensor) -> torch.Tensor
  BailingMoeBlock.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  BailingMoeBlock.forward(hidden_states: torch.Tensor, position_ids: torch.Tensor, residual: Optional[torch.Tensor], forward_batch: ForwardBatch) -> Tuple[torch.Tensor, torch.Tensor]
  BailingMoeModel.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  BailingMoeModel.forward(input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor]) -> torch.Tensor
  BailingMoeForCausalLM.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig]) -> None
  BailingMoeForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor]) -> torch.Tensor
  BailingMoeForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/bert.py
  BertEmbedding.__init__(config: BertConfig)
  BertEmbedding.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  BertPooler.__init__(config: BertConfig)
  BertPooler.forward(hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  BertEncoder.__init__(config: BertConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  BertEncoder.forward(hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  BertLayer.__init__(config: BertConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  BertLayer.forward(hidden_states: torch.Tensor, forward_batch: ForwardBatch)
  BertAttention.__init__(hidden_size: int, num_attention_heads: int, layer_norm_eps: float, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  BertAttention.forward(hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  BertSelfAttention.__init__(hidden_size: int, num_attention_heads: int, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  BertSelfAttention.forward(hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  BertSelfOutput.__init__(hidden_size: int, layer_norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str)
  BertSelfOutput.forward(hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor
  BertIntermediate.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
  BertIntermediate.forward(hidden_states: torch.Tensor) -> torch.Tensor
  BertOutput.__init__(hidden_size: int, intermediate_size: int, layer_norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str)
  BertOutput.forward(hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor
  BertModel.__init__()
  BertModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool) -> torch.Tensor
  BertModel.load_weights(weights: Iterable[Tuple[str, torch.Tensor]]) -> Set[str]
  BertForSequenceClassification.__init__()
  BertForSequenceClassification.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  BertForSequenceClassification.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool) -> torch.Tensor

# python/sglang/srt/models/chatglm.py
  GLMAttention.__init__(config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  GLMAttention.forward(hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  GLMMLP.__init__(config, quant_config: Optional[QuantizationConfig], prefix: str)
  GLMMLP.forward(hidden_states)
  GLMBlock.__init__(config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  GLMBlock.forward(hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  GLMTransformer.__init__(config, quant_config: Optional[QuantizationConfig], prefix: str)
  GLMTransformer.forward(hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  ChatGLMM.__init__(config, quant_config: Optional[QuantizationConfig], prefix: str)
  ChatGLMM.forward(input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  ChatGLMForCausalLM.__init__(config: ChatGLMConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  ChatGLMForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  ChatGLMForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/clip.py
  CLIPVisionEmbeddings.__init__(config: CLIPVisionConfig)
  CLIPVisionEmbeddings.forward(pixel_values: torch.Tensor) -> torch.Tensor
  CLIPTextEmbeddings.__init__(config: CLIPTextConfig)
  CLIPTextEmbeddings.forward(input_ids: Optional[torch.LongTensor], position_ids: Optional[torch.LongTensor], inputs_embeds: Optional[torch.FloatTensor]) -> torch.Tensor
  CLIPMLP.__init__(config, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)
  CLIPMLP.forward(x: torch.Tensor) -> torch.Tensor
  CLIPEncoderLayer.__init__(config: CLIPVisionConfig, act_layer: Type[nn.Module], norm_layer: Type[nn.Module], attn_implementation: Optional[str], quant_config: Optional[QuantizationConfig], prefix: str) -> None
  CLIPEncoderLayer.forward(hidden_states: torch.Tensor, attention_mask: torch.Tensor, causal_attention_mask: torch.Tensor) -> torch.Tensor
  CLIPEncoder.__init__(config: CLIPVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  CLIPEncoder.forward(inputs_embeds: torch.Tensor, attention_mask: torch.Tensor, causal_attention_mask: torch.Tensor, return_all_hidden_states: bool) -> Union[torch.Tensor, list[torch.Tensor]]
  CLIPTextTransformer.__init__(config: CLIPTextConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  CLIPTextTransformer.device() -> torch.device
  CLIPTextTransformer.forward(input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor], position_ids: Optional[torch.Tensor])
  CLIPTextModel.__init__(config: CLIPTextConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  CLIPTextModel.forward(input_ids: torch.Tensor, position_ids: torch.Tensor)
  CLIPVisionTransformer.__init__(config: CLIPVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  CLIPVisionTransformer.device() -> torch.device
  CLIPVisionTransformer.forward(pixel_values: torch.Tensor) -> torch.Tensor
  CLIPVisionModel.__init__(config: CLIPVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  CLIPVisionModel.device() -> torch.device
  CLIPVisionModel.forward(pixel_values: torch.Tensor)
  CLIPModel.__init__(config: CLIPConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  CLIPModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)
  CLIPModel.pad_input_ids(input_ids: List[int], image_inputs: MultimodalInputs)
  CLIPModel.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
monkey_patch_weight_loader()

# python/sglang/srt/models/commandr.py
layer_norm_func(hidden_states, weight, variance_epsilon)
  LayerNorm.__init__(param_shape, eps)
  LayerNorm.forward(hidden_states, residuals)
  LayerNorm.weight_loader(param: Parameter, loaded_weight: torch.Tensor)
  CohereMLP.__init__(config, quant_config: Optional[QuantizationConfig], prefix: str)
  CohereMLP.forward(x)
  CohereAttention.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  CohereAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  CohereDecoderLayer.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  CohereDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  CohereModel.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  CohereModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  CohereForCausalLM.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  CohereForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  CohereForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/dbrx.py
  DbrxRouter.__init__(config: DbrxConfig, params_dtype: Optional[torch.dtype], prefix: str)
  DbrxRouter.forward(hidden_states: torch.Tensor) -> torch.Tensor
  DbrxExperts.__init__(config: DbrxConfig, quant_config: Optional[QuantizationConfig], params_dtype: Optional[torch.dtype], prefix: str)
  DbrxExperts.weight_loader(param: nn.Parameter, loaded_weight: torch.Tensor, weight_name: str)
  DbrxExperts.forward(hidden_states: torch.Tensor) -> torch.Tensor
  DbrxAttention.__init__(config: DbrxConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  DbrxAttention.forward(position_ids: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  DbrxFusedNormAttention.__init__(config: DbrxConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  DbrxFusedNormAttention.forward(position_ids: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> Tuple[torch.Tensor, torch.Tensor]
  DbrxBlock.__init__(config: DbrxConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  DbrxBlock.forward(position_ids: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  DbrxModel.__init__(config: DbrxConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  DbrxModel.forward(input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  DbrxForCausalLM.__init__(config: DbrxConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  DbrxForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  DbrxForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/deepseek.py
  DeepseekMLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str) -> None
  DeepseekMLP.forward(x)
  DeepseekMoE.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  DeepseekMoE.pack_params()
  DeepseekMoE.forward(hidden_states: torch.Tensor) -> torch.Tensor
  DeepseekAttention.__init__(hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  DeepseekAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  DeepseekDecoderLayer.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  DeepseekDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> torch.Tensor
  DeepseekModel.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  DeepseekModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  DeepseekForCausalLM.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  DeepseekForCausalLM.get_input_embeddings() -> nn.Embedding
  DeepseekForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  DeepseekForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/deepseek_janus_pro.py
named_apply(fn: Callable, module: nn.Module, name, depth_first: bool, include_root: bool) -> nn.Module
VQ_16()
trunc_normal_tf_(tensor: torch.Tensor, mean: float, std: float, a: float, b: float)
nchw_to(x: torch.Tensor, fmt: Format)
resample_patch_embed(patch_embed, new_size: List[int], interpolation: str, antialias: bool, verbose: bool)
  PatchEmbed.__init__(img_size: Optional[int], patch_size: int, in_chans: int, embed_dim: int, norm_layer: Optional[Callable], flatten: bool, output_fmt: Optional[str], bias: bool, strict_img_size: bool, dynamic_img_pad: bool)
  PatchEmbed.set_input_size(img_size: Optional[Union[int, Tuple[int, int]]], patch_size: Optional[Union[int, Tuple[int, int]]])
  PatchEmbed.feat_ratio(as_scalar) -> Union[Tuple[int, int], int]
  PatchEmbed.dynamic_feat_size(img_size: Tuple[int, int]) -> Tuple[int, int]
  PatchEmbed.forward(x)
  Mlp.__init__(in_features, hidden_features, out_features, act_layer, norm_layer, bias, drop, use_conv)
  Mlp.forward(x)
drop_path(x, drop_prob: float, training: bool, scale_by_keep: bool)
  DropPath.__init__(drop_prob: float, scale_by_keep: bool)
  DropPath.forward(x)
  DropPath.extra_repr()
  VisionTransformerBlock.__init__(dim: int, num_heads: int, mlp_ratio: float, qkv_bias: bool, qk_norm: bool, proj_drop: float, attn_drop: float, init_values: Optional[float], drop_path: float, act_layer: nn.Module, norm_layer: nn.Module, mlp_layer: nn.Module) -> None
  VisionTransformerBlock.forward(x: torch.Tensor) -> torch.Tensor
  PatchDropout.__init__(prob: float, num_prefix_tokens: int, ordered: bool, return_indices: bool)
  PatchDropout.forward(x) -> Union[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]]]
resample_abs_pos_embed(posemb: torch.Tensor, new_size: List[int], old_size: Optional[List[int]], num_prefix_tokens: int, interpolation: str, antialias: bool, verbose: bool)
init_weights()
init_weights_vit_timm(module: nn.Module, name: str) -> None
  VisionTransformer.__init__(img_size: Union[int, Tuple[int, int]], patch_size: Union[int, Tuple[int, int]], in_chans: int, num_classes: int, global_pool: Literal['', 'avg', 'token', 'map'], embed_dim: int, depth: int, num_heads: int, mlp_ratio: float, qkv_bias: bool, qk_norm: bool, init_values: Optional[float], class_token: bool, no_embed_class: bool, reg_tokens: int, pre_norm: bool, fc_norm: Optional[bool], dynamic_img_size: bool, dynamic_img_pad: bool, drop_rate: float, pos_drop_rate: float, patch_drop_rate: float, proj_drop_rate: float, attn_drop_rate: float, drop_path_rate: float, weight_init: Literal['skip', 'jax', 'jax_nlhb', 'moco', ''], embed_layer: Callable, _norm_layer: Optional[LayerType], _act_layer: Optional[LayerType], block_fn: Type[nn.Module], mlp_layer: Type[nn.Module], ignore_head: bool) -> None
  VisionTransformer.init_weights(mode: Literal['jax', 'jax_nlhb', 'moco', '']) -> None
  VisionTransformer.no_weight_decay() -> Set
  VisionTransformer.group_matcher(coarse: bool) -> Dict
  VisionTransformer.get_classifier() -> nn.Module
  VisionTransformer.reset_classifier(num_classes: int, global_pool) -> None
  VisionTransformer.forward_features(x: torch.Tensor) -> torch.Tensor
  VisionTransformer.forward_head(x: torch.Tensor, pre_logits: bool) -> torch.Tensor
  VisionTransformer.forward(x: torch.Tensor) -> torch.Tensor
model_name_to_cls(cls_name)
  vision_head.__init__(params)
  vision_head.forward(x)
create_siglip_vit(model_name: str, image_size: int, select_layer: int, ckpt_path: str)
  Normalize.__init__(mean, std, inplace)
  Normalize.forward(tensor: Tensor) -> Tensor
  Normalize.__repr__() -> str
  CLIPVisionTower.__init__(model_name: str, image_size: Union[Tuple[int, int], int], select_feature: str, select_layer: int, select_layers: list, ckpt_path: str, pixel_mean: Optional[List[float]], pixel_std: Optional[List[float]])
  CLIPVisionTower.device() -> torch.device
  CLIPVisionTower.dtype()
  CLIPVisionTower.build_vision_tower(vision_tower_params)
  CLIPVisionTower.feature_select(image_forward_outs)
  CLIPVisionTower.forward(images)
  MlpProjector.__init__(cfg)
  MlpProjector.forward(x_or_tuple: Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor])
  LayerScale.__init__(dim: int, init_values: float, inplace: bool) -> None
  LayerScale.forward(x: torch.Tensor) -> torch.Tensor
use_fused_attn(experimental: bool) -> bool
  AttentionPoolLatent.__init__(in_features: int, out_features: int, embed_dim: int, num_heads: int, feat_size: Optional[int], mlp_ratio: float, qkv_bias: bool, qk_norm: bool, latent_len: int, latent_dim: int, pos_embed: str, pool_type: str, norm_layer: Optional[nn.Module], drop: float)
  AttentionPoolLatent.init_weights()
  AttentionPoolLatent.forward(x)
  Encoder.__init__(in_channels, ch, ch_mult, num_res_blocks, norm_type, dropout, resamp_with_conv, z_channels)
  Encoder.forward(x)
  Decoder.__init__(z_channels, ch, ch_mult, num_res_blocks, norm_type, dropout, resamp_with_conv, out_channels)
  Decoder.last_layer()
  Decoder.forward(z)
  VectorQuantizer.__init__(n_e, e_dim, beta, entropy_loss_ratio, l2_norm, show_usage)
  VectorQuantizer.forward(z)
  VectorQuantizer.get_codebook_entry(indices, shape, channel_first)
  ResnetBlock.__init__(in_channels, out_channels, conv_shortcut, dropout, norm_type)
  ResnetBlock.forward(x)
  AttnBlock.__init__(in_channels, norm_type)
  AttnBlock.forward(x)
nonlinearity(x)
Normalize(in_channels, norm_type)
  Upsample.__init__(in_channels, with_conv)
  Upsample.forward(x)
  Downsample.__init__(in_channels, with_conv)
  Downsample.forward(x)
compute_entropy_loss(affinity, loss_type, temperature)
  VQModel.__init__(config: ModelArgs)
  VQModel.encode(x)
  VQModel.decode(quant)
  VQModel.decode_code(code_b, shape, channel_first)
  VQModel.forward(input)
  MultiModalityCausalLM.__init__(config: MultiModalityConfig, quant_config: Optional[QuantizationConfig])
  MultiModalityCausalLM.get_image_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  MultiModalityCausalLM.get_input_embeddings() -> nn.Embedding
  MultiModalityCausalLM.forward(input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool) -> torch.Tensor
  MultiModalityCausalLM.prepare_gen_img_embeds(image_ids: torch.LongTensor)
  MultiModalityCausalLM.pad_input_ids(input_ids: List[int], image_inputs: MultimodalInputs)
  MultiModalityCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/deepseek_nextn.py
  DeepseekModelNextN.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  DeepseekModelNextN.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  DeepseekV3ForCausalLMNextN.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  DeepseekV3ForCausalLMNextN.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  DeepseekV3ForCausalLMNextN.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/deepseek_v2.py
  DeepseekV2MLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str, tp_rank: Optional[int], tp_size: Optional[int]) -> None
  DeepseekV2MLP.forward(x, forward_batch, should_allreduce_fusion: bool, use_reduce_scatter: bool)
  MoEGate.__init__(config, prefix: str, is_nextn: bool)
  MoEGate.forward(hidden_states)
  DeepseekV2MoE.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream], is_nextn: bool)
  DeepseekV2MoE.get_moe_weights()
  DeepseekV2MoE.forward(hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch], should_allreduce_fusion: bool, use_reduce_scatter: bool) -> torch.Tensor
  DeepseekV2MoE.forward_normal_dual_stream(hidden_states: torch.Tensor, should_allreduce_fusion: bool, use_reduce_scatter: bool) -> torch.Tensor
  DeepseekV2MoE.forward_normal(hidden_states: torch.Tensor, should_allreduce_fusion: bool, use_reduce_scatter: bool) -> torch.Tensor
  DeepseekV2MoE.forward_cpu(hidden_states: torch.Tensor, should_allreduce_fusion: bool) -> torch.Tensor
  DeepseekV2MoE.forward_deepep(hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  DeepseekV2MoE.op_gate(state)
  DeepseekV2MoE.op_shared_experts(state)
  DeepseekV2MoE.op_select_experts(state)
  DeepseekV2MoE.op_dispatch_a(state)
  DeepseekV2MoE.op_dispatch_b(state)
  DeepseekV2MoE.op_experts(state)
  DeepseekV2MoE.op_combine_a(state)
  DeepseekV2MoE.op_combine_b(state)
  DeepseekV2MoE.op_output(state)
yarn_get_mscale(scale: float, mscale: float) -> float
  DeepseekV2AttentionMLA.__init__(config: PretrainedConfig, hidden_size: int, num_heads: int, qk_nope_head_dim: int, qk_rope_head_dim: int, v_head_dim: int, q_lora_rank: int, kv_lora_rank: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], reduce_results: bool, layer_id: int, prefix: str, alt_stream: Optional[torch.cuda.Stream]) -> None
  DeepseekV2AttentionMLA.dispatch_attn_forward_method(forward_batch: ForwardBatch) -> AttnForwardMethod
  DeepseekV2AttentionMLA.op_prepare(state)
  DeepseekV2AttentionMLA.op_core(state)
  DeepseekV2AttentionMLA.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)
  DeepseekV2AttentionMLA.forward_prepare(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)
  DeepseekV2AttentionMLA.forward_core(intermediate_state)
  DeepseekV2AttentionMLA.forward_normal_prepare(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)
  DeepseekV2AttentionMLA.forward_normal_core(q, k, v, forward_batch)
  DeepseekV2AttentionMLA.forward_absorb_prepare(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)
  DeepseekV2AttentionMLA.forward_absorb_core(q_pe, k_pe, q_nope_out, k_nope, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_prepare(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)
  DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_cpu_prepare(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)
  DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_core(q_input, key_cache_buf, val_cache_buf, attn_output, kv_indptr, kv_indices, k_pe_output, cos_sin_cache, positions, attn_logits, num_kv_split, sm_scale, enable_rope_fusion, k_input, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_cpu_core(q_input, k_input, v_input, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_normal_chunked_kv_prepare(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)
  DeepseekV2AttentionMLA.forward_normal_chunked_kv_core(q, k, v, forward_batch)
  DeepseekV2DecoderLayer.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], is_nextn: bool, prefix: str, alt_stream: Optional[torch.cuda.Stream]) -> None
  DeepseekV2DecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], zero_allocator: BumpAllocator) -> torch.Tensor
  DeepseekV2DecoderLayer.op_comm_prepare_attn(state, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], zero_allocator: BumpAllocator, tbo_subbatch_index: Optional[int])
  DeepseekV2DecoderLayer.op_comm_prepare_mlp(state)
  DeepseekV2DecoderLayer.op_mlp(state)
  DeepseekV2DecoderLayer.op_comm_postprocess_layer(state)
  DeepseekV2Model.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  DeepseekV2Model.get_input_embeddings() -> torch.Tensor
  DeepseekV2Model.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors]) -> Union[torch.Tensor, PPProxyTensors]
  DeepseekV2ForCausalLM.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  DeepseekV2ForCausalLM.routed_experts_weights_of_layer()
  DeepseekV2ForCausalLM.determine_num_fused_shared_experts(architecture: str)
  DeepseekV2ForCausalLM.get_input_embeddings() -> nn.Embedding
  DeepseekV2ForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors]) -> torch.Tensor
  DeepseekV2ForCausalLM.start_layer()
  DeepseekV2ForCausalLM.end_layer()
  DeepseekV2ForCausalLM.post_load_weights(is_nextn, weight_names)
  DeepseekV2ForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]], is_nextn)
  DeepseekV2ForCausalLM.get_embed_and_head()
  DeepseekV2ForCausalLM.set_embed_and_head(embed, head)
  DeepseekV2ForCausalLM.get_model_config_for_expert_location(cls, config)

# python/sglang/srt/models/deepseek_vl2.py
  DeepseekVL2MlpProjector.__init__(config: DeepseekVL2MlpProjectorConfig, quant_config: Optional[QuantizationConfig])
  DeepseekVL2MlpProjector.forward(x)
  DeepseekVL2ForCausalLM.__init__(config: DeepseekVL2Config, quant_config: Optional[QuantizationConfig])
  DeepseekVL2ForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
  DeepseekVL2ForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  DeepseekVL2ForCausalLM.pad_input_ids(input_ids: List[int], mm_inputs: MultimodalInputs)
  DeepseekVL2ForCausalLM.get_image_feature(items: List[MultimodalDataItem])

# python/sglang/srt/models/ernie4.py
  MoEGate.__init__(config, prefix: str)
  MoEGate.forward(hidden_states)
  Ernie4Moe.__init__(config: Ernie4_5_MoeConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  Ernie4Moe.forward(hidden_states: torch.Tensor) -> torch.Tensor
  Ernie4Moe.forward_normal(hidden_states: torch.Tensor) -> torch.Tensor
  Ernie4DecoderLayer.__init__(config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, is_mtp: bool)
  Ernie4DecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  Ernie4Model.__init__(config: Ernie4_5_MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Ernie4Model.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]]]
  Ernie4_5_ForCausalLM.__init__(config: Ernie4_5_MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Ernie4_5_ForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Ernie4_5_ForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  Ernie4_5_ForCausalLM.get_embed_and_head()
  Ernie4_5_MoeForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/ernie4_eagle.py
  Ernie4ModelMTP.__init__(config: Ernie4_5_MoeConfig, layer_id: int, prefix: str, quant_config: Optional[QuantizationConfig]) -> None
  Ernie4ModelMTP.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  Ernie4_5_MoeForCausalLMMTP.__init__(config: Ernie4_5_MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str, mtp_layer_id: int) -> None
  Ernie4_5_MoeForCausalLMMTP.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Ernie4_5_MoeForCausalLMMTP.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  Ernie4_5_MoeForCausalLMMTP.get_embed_and_head()
  Ernie4_5_MoeForCausalLMMTP.set_embed_and_head(embed, head)

# python/sglang/srt/models/exaone.py
  ExaoneGatedMLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  ExaoneGatedMLP.forward(x)
  ExaoneAttention.__init__(config, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  ExaoneAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  ExaoneDecoderLayer.__init__(config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  ExaoneDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  ExaoneModel.__init__(config, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  ExaoneModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  ExaoneForCausalLM.__init__(config, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  ExaoneForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> LogitsProcessorOutput
  ExaoneForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/gemma.py
  GemmaMLP.__init__(hidden_size: int, intermediate_size: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  GemmaMLP.forward(x)
  GemmaAttention.__init__(hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: int, layer_id: int, max_position_embeddings: int, rope_theta: float, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  GemmaAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  GemmaDecoderLayer.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  GemmaDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  GemmaModel.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  GemmaModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  GemmaForCausalLM.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  GemmaForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  GemmaForCausalLM.forward_split_prefill(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)
  GemmaForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/gemma2.py
get_attention_sliding_window_size(config)
  Gemma2MLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, hidden_activation: str, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Gemma2MLP.forward(x: torch.Tensor) -> torch.Tensor
  Gemma2Attention.__init__(layer_id: int, config: PretrainedConfig, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: int, max_position_embeddings: int, rope_theta: float, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Gemma2Attention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Gemma2DecoderLayer.__init__(layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Gemma2DecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  Gemma2Model.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Gemma2Model.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  Gemma2ForCausalLM.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Gemma2ForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  Gemma2ForCausalLM.forward_split_prefill(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)
  Gemma2ForCausalLM.get_attention_sliding_window_size()
  Gemma2ForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/gemma2_reward.py
  Gemma2ForSequenceClassification.__init__(config: Gemma2Config, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Gemma2ForSequenceClassification.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool) -> EmbeddingPoolerOutput
  Gemma2ForSequenceClassification.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/gemma3_causal.py
get_attention_sliding_window_size(config)
extract_layer_index(prefix: str) -> int
  Gemma3MLP.__init__(hidden_size: int, intermediate_size: int, hidden_activation: str, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Gemma3MLP.forward(x: torch.Tensor) -> torch.Tensor
  Gemma3Attention.__init__(layer_id: int, config: Gemma3TextConfig, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Gemma3Attention.naive_attn_with_masks(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, out: torch.Tensor) -> torch.Tensor
  Gemma3Attention.forward(hidden_states: torch.Tensor, position_embeddings: Tuple[torch.Tensor, torch.Tensor], forward_batch: ForwardBatch) -> torch.Tensor
  Gemma3DecoderLayer.__init__(layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Gemma3DecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, position_embeddings_global: torch.Tensor, position_embeddings_local: torch.Tensor, forward_batch: ForwardBatch) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]
  Gemma3RotaryEmbedding.__init__(config: Gemma3TextConfig, device)
  Gemma3RotaryEmbedding.forward(x, position_ids)
  Gemma3TextScaledWordEmbedding.__init__(num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: Optional[float])
  Gemma3TextScaledWordEmbedding.forward(input_ids: torch.Tensor)
  Gemma3TextModel.__init__(config: Gemma3TextConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Gemma3TextModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  Gemma3ForCausalLM.__init__(config: Gemma3TextConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Gemma3ForCausalLM.get_input_embeddings() -> nn.Embedding
  Gemma3ForCausalLM.get_attention_sliding_window_size()
  Gemma3ForCausalLM.dtype() -> torch.dtype
  Gemma3ForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> LogitsProcessor
  Gemma3ForCausalLM.forward_split_prefill(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)
  Gemma3ForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/gemma3_mm.py
  Gemma3MultiModalProjector.__init__(config: Gemma3Config)
  Gemma3MultiModalProjector.forward(vision_outputs: torch.Tensor) -> torch.Tensor
  Gemma3ForConditionalGeneration.__init__(config: Gemma3Config, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Gemma3ForConditionalGeneration.pad_input_ids(input_ids: List[int], image_inputs: MultimodalInputs) -> List[int]
  Gemma3ForConditionalGeneration.prepare_attn_masks(input_ids: torch.Tensor, positions: torch.Tensor, mask_dtype: torch.dtype) -> Dict
  Gemma3ForConditionalGeneration.get_input_embeddings() -> nn.Embedding
  Gemma3ForConditionalGeneration.get_attention_sliding_window_size()
  Gemma3ForConditionalGeneration.get_image_feature(items: List[MultimodalDataItem])
  Gemma3ForConditionalGeneration.forward(input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> LogitsProcessor
  Gemma3ForConditionalGeneration.tie_weights()
  Gemma3ForConditionalGeneration.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/gemma3n_audio.py
  Gemma3nCumulativeGroupNorm.__init__(num_channels: int, feature_dims: Sequence[int], eps: float)
  Gemma3nCumulativeGroupNorm.forward(x: torch.Tensor, mask: Optional[torch.Tensor]) -> torch.Tensor
  Gemma3nAudioRelativePositionEmbedding.__init__(config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Gemma3nAudioRelativePositionEmbedding.forward(queries: torch.Tensor, keys: torch.Tensor) -> torch.Tensor
  Gemma3nAudioAttention.__init__(config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Gemma3nAudioAttention.forward(x: torch.Tensor, mask: torch.BoolTensor) -> torch.Tensor
  Gemma3nAudioSSCPConvBlock.__init__(config: Gemma3nAudioConfig, idx: int, input_freq_dim: int, manual_padding: Tuple[int, int, int, int], quant_config: Optional[QuantizationConfig], prefix: str)
  Gemma3nAudioSSCPConvBlock.forward(audio_encodings: torch.Tensor) -> torch.Tensor
  Gemma3nAudioSubSampleConvProjection.__init__(config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Gemma3nAudioSubSampleConvProjection.forward(audio_encodings: torch.Tensor) -> torch.Tensor
  Gemma3nAudioConformerAttention.__init__(config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Gemma3nAudioConformerAttention.forward(audio_encodings: torch.Tensor, audio_mel_mask: torch.BoolTensor) -> torch.Tensor
  Gemma3nAudioConformerFeedForward.__init__(config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Gemma3nAudioConformerFeedForward.forward(audio_encodings: torch.Tensor) -> torch.Tensor
  Gemma3nAudioConformerLightConv1d.__init__(config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Gemma3nAudioConformerLightConv1d.forward(audio_encodings: torch.Tensor) -> torch.Tensor
  Gemma3nAudioConformerBlock.__init__(config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Gemma3nAudioConformerBlock.forward(audio_encodings: torch.Tensor, audio_mel_mask: torch.BoolTensor) -> torch.Tensor
  Gemma3nAudioEncoder.__init__(config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Gemma3nAudioEncoder.forward(audio_mel: torch.Tensor, audio_mel_mask: torch.BoolTensor) -> Tuple[torch.Tensor, torch.BoolTensor]

# python/sglang/srt/models/gemma3n_causal.py
get_attention_sliding_window_size(config)
  Gemma3nRMSNorm.__init__(dim: int, eps: float, with_scale: bool) -> None
  Gemma3nRMSNorm.forward(x: torch.Tensor) -> torch.Tensor
  Gemma3nTextMLP.__init__(hidden_size: int, intermediate_size: int, hidden_activation: str, activation_sparsity: float, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Gemma3nTextMLP.forward(x: torch.Tensor) -> torch.Tensor
  Gemma3nLaurelBlock.__init__(config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Gemma3nLaurelBlock.forward(x: torch.Tensor) -> torch.Tensor
  Gemma3nAltUp.__init__(config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Gemma3nAltUp.compute_router_modalities(x: torch.Tensor) -> torch.Tensor
  Gemma3nAltUp.predict(hidden_states: torch.Tensor) -> torch.Tensor
  Gemma3nAltUp.correct(predictions: torch.Tensor, activated: torch.Tensor) -> torch.Tensor
  Gemma3nAltUp.scale_corrected_output(corrected: torch.Tensor) -> torch.Tensor
  Gemma3nAltUp.forward(hidden_states: torch.Tensor, activated: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
  Gemma3nAttention.__init__(layer_id: int, config: Gemma3nTextConfig, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Gemma3nAttention.forward(hidden_states: torch.Tensor, positions: Tuple[torch.Tensor, torch.Tensor], forward_batch: ForwardBatch) -> torch.Tensor
  Gemma3nDecoderLayer.__init__(layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Gemma3nDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, per_layer_input: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Gemma3nTextModel.__init__(config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Gemma3nTextModel.get_input_embeddings() -> nn.Embedding
  Gemma3nTextModel.dtype() -> torch.dtype
  Gemma3nTextModel.get_per_layer_inputs(input_ids: torch.LongTensor) -> torch.Tensor
  Gemma3nTextModel.project_per_layer_inputs(inputs_embeds: torch.Tensor, per_layer_inputs: Optional[torch.Tensor]) -> torch.Tensor
  Gemma3nTextModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, per_layer_inputs: Optional[torch.Tensor]) -> torch.Tensor
  Gemma3nForCausalLM.__init__(config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Gemma3nForCausalLM.get_input_embeddings() -> nn.Embedding
  Gemma3nForCausalLM.get_attention_sliding_window_size()
  Gemma3nForCausalLM.dtype() -> torch.dtype
  Gemma3nForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, per_layer_inputs: Optional[torch.Tensor]) -> LogitsProcessor
  Gemma3nForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/gemma3n_mm.py
  Gemma3nMultimodalEmbedder.__init__(multimodal_config: Union[Gemma3nAudioConfig, Gemma3nVisionConfig], text_config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Gemma3nMultimodalEmbedder.forward(input_ids: Optional[torch.LongTensor], inputs_embeds: Optional[torch.Tensor]) -> torch.Tensor
  Gemma3nForConditionalGeneration.__init__(config: Gemma3nConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Gemma3nForConditionalGeneration.pad_input_ids(input_ids: List[int], mm_inputs: MultimodalInputs) -> List[int]
  Gemma3nForConditionalGeneration.get_input_embeddings() -> nn.Embedding
  Gemma3nForConditionalGeneration.get_attention_sliding_window_size()
  Gemma3nForConditionalGeneration.get_image_feature(items: List[MultimodalDataItem])
  Gemma3nForConditionalGeneration.get_audio_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  Gemma3nForConditionalGeneration.get_per_layer_inputs(input_ids: torch.LongTensor) -> Optional[torch.Tensor]
  Gemma3nForConditionalGeneration.project_per_layer_inputs(inputs_embeds: torch.Tensor, per_layer_inputs: Optional[torch.Tensor]) -> torch.Tensor
  Gemma3nForConditionalGeneration.forward(input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> LogitsProcessor
  Gemma3nForConditionalGeneration.tie_weights()
  Gemma3nForConditionalGeneration.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  Gemma3nForConditionalGeneration.should_apply_lora(module_name: str) -> bool
  Gemma3nForConditionalGeneration.get_hidden_dim(module_name)

# python/sglang/srt/models/glm4.py
  Glm4Attention.__init__(config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  Glm4Attention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Glm4DecoderLayer.__init__(config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  Glm4DecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  Glm4Model.__init__(config: Glm4Config, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Glm4Model.get_input_embeddings() -> nn.Embedding
  Glm4Model.dtype() -> torch.dtype
  Glm4Model.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]]]
  Glm4ForCausalLM.__init__(config: Glm4Config, quant_config: Optional[QuantizationConfig], prefix: str)
  Glm4ForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Glm4ForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/glm4_moe.py
  Glm4MoeMLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str, tp_rank: Optional[int], tp_size: Optional[int]) -> None
  Glm4MoeMLP.forward(x, forward_batch, should_allreduce_fusion)
  Glm4MoeAttention.__init__(hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, partial_rotary_factor: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, head_dim: Optional[int], rms_norm_eps: float, attention_bias: bool, quant_config: Optional[QuantizationConfig], use_qk_norm: bool, prefix: str, alt_stream: Optional[torch.cuda.Stream]) -> None
  Glm4MoeAttention.op_prepare(state)
  Glm4MoeAttention.op_core(state)
  Glm4MoeAttention.forward_prepare(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
  Glm4MoeAttention.forward_core(intermediate_state)
  Glm4MoeAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Glm4MoeGate.__init__(config, prefix: str, is_nextn: bool)
  Glm4MoeGate.forward(hidden_states)
  Glm4MoeSparseMoeBlock.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream], is_nextn: bool)
  Glm4MoeSparseMoeBlock.forward_normal_dual_stream(hidden_states: torch.Tensor, should_allreduce_fusion: bool, use_reduce_scatter: bool) -> torch.Tensor
  Glm4MoeSparseMoeBlock.forward_normal(hidden_states: torch.Tensor, should_allreduce_fusion: bool, use_reduce_scatter: bool) -> torch.Tensor
  Glm4MoeDecoderLayer.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], is_nextn: bool, prefix: str, alt_stream: Optional[torch.cuda.Stream]) -> None
  Glm4MoeDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], zero_allocator: BumpAllocator) -> torch.Tensor
  Glm4MoeModel.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Glm4MoeForCausalLM.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Glm4MoeForCausalLM.determine_num_fused_shared_experts(architecture: str)
  Glm4MoeForCausalLM.get_input_embeddings() -> nn.Embedding
  Glm4MoeForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]], is_nextn)

# python/sglang/srt/models/glm4_moe_nextn.py
  Glm4MoeModelNextN.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Glm4MoeModelNextN.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  Glm4MoeForCausalLMNextN.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Glm4MoeForCausalLMNextN.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Glm4MoeForCausalLMNextN.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/glm4v.py
  Glm4vRMSNorm.forward(x: torch.Tensor) -> torch.Tensor
  Glm4vVisionMLP.__init__(in_features: int, hidden_features: int, bias: bool, quant_config: Optional[QuantizationConfig], prefix: str)
  Glm4vVisionMLP.forward(x: torch.Tensor)
  Glm4vVisionBlock.__init__(config: Glm4vVisionConfig, norm_layer: Optional[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Glm4vVisionPatchEmbed.__init__(patch_size: int, temporal_patch_size: int, in_channels: int, hidden_size: int) -> None
  Glm4vVisionPatchEmbed.forward(x: torch.Tensor) -> torch.Tensor
  Glm4vPatchMerger.__init__(d_model: int, context_dim: int, quant_config: Optional[QuantizationConfig], bias: bool, prefix: str) -> None
  Glm4vPatchMerger.forward(x: torch.Tensor)
  Glm4vVisionEmbeddings.__init__(config: Glm4vVisionConfig)
  Glm4vVisionEmbeddings.forward(embeddings, lengths, image_shapes, h_coords, w_coords) -> torch.Tensor
  Glm4vVisionRotaryEmbedding.__init__(dim: int, theta: float) -> None
  Glm4vVisionRotaryEmbedding.update_freqs_cache(seqlen: int) -> None
  Glm4vVisionRotaryEmbedding.forward(seqlen: int) -> torch.Tensor
  Glm4vVisionModel.__init__(vision_config: Glm4vVisionConfig, norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Glm4vVisionModel.dtype() -> torch.dtype
  Glm4vVisionModel.device() -> torch.device
  Glm4vVisionModel.rot_pos_emb(grid_thw: torch.Tensor) -> torch.Tensor
  Glm4vVisionModel.forward(x: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor
  Glm4vForConditionalGeneration.__init__(config: Glm4vConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Glm4vForConditionalGeneration.get_image_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  Glm4vForConditionalGeneration.get_video_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  Glm4vForConditionalGeneration.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/glm4v_moe.py
  Glm4vMoeForConditionalGeneration.__init__(config: Glm4vMoeConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Glm4vMoeForConditionalGeneration.determine_num_fused_shared_experts(architecture: str)
  Glm4vMoeForConditionalGeneration.load_weights(weights: Iterable[Tuple[str, torch.Tensor]], is_nextn)

# python/sglang/srt/models/gpt2.py
  GPT2Attention.__init__(layer_id: int, config: GPT2Config, quant_config: Optional[QuantizationConfig], prefix: str)
  GPT2Attention.forward(hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  GPT2MLP.__init__(intermediate_size: int, config: GPT2Config, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)
  GPT2MLP.forward(hidden_states: torch.Tensor) -> torch.Tensor
  GPT2Block.__init__(layer_id: int, config: GPT2Config, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)
  GPT2Block.forward(hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  GPT2Model.__init__(config: GPT2Config, quant_config: Optional[QuantizationConfig], prefix: str)
  GPT2Model.forward(input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  GPT2LMHeadModel.__init__(config: GPT2Config, quant_config: Optional[QuantizationConfig], prefix: str)
  GPT2LMHeadModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  GPT2LMHeadModel.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/gpt_bigcode.py
  GPTBigCodeAttention.__init__(layer_id: int, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  GPTBigCodeAttention.forward(hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  GPTBigMLP.__init__(intermediate_size: int, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  GPTBigMLP.forward(hidden_states: torch.Tensor) -> torch.Tensor
  GPTBigCodeBlock.__init__(layer_id: int, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  GPTBigCodeBlock.forward(hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  GPTBigCodeModel.__init__(config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  GPTBigCodeModel.forward(input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  GPTBigCodeForCausalLM.__init__(config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  GPTBigCodeForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  GPTBigCodeForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/gpt_oss.py
  GptOssConfig.__init__()
get_attention_sliding_window_size(config)
  GptOssSparseMoeBlock.__init__(layer_id: int, config: GptOssConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  GptOssSparseMoeBlock.forward(hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch], should_allreduce_fusion: bool) -> torch.Tensor
  GptOssSparseMoeBlock.get_moe_weights()
  GptOssSparseMoeBlock.forward_normal(hidden_states: torch.Tensor, should_allreduce_fusion: bool) -> torch.Tensor
  GptOssAttention.__init__(hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, head_dim: Optional[int], rms_norm_eps: float, attention_bias: bool, quant_config: Optional[QuantizationConfig], prefix: str, sliding_window_size: int, layer_type: str, params_dtype: torch.dtype) -> None
  GptOssAttention.forward_prepare(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
  GptOssAttention.forward_core(intermediate_state)
  GptOssAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  GptOssDecoderLayer.__init__(config: GptOssConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, sliding_window_size: int | None) -> None
  GptOssDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  GptOssModel.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str, decoder_layer_type: type[nn.Module]) -> None
  GptOssModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors]) -> Union[torch.Tensor, PPProxyTensors]
  GptOssForCausalLM.__init__(config: GptOssConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  GptOssForCausalLM.routed_experts_weights_of_layer()
  GptOssForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors]) -> torch.Tensor
  GptOssForCausalLM.start_layer()
  GptOssForCausalLM.end_layer()
  GptOssForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]], is_nextn: bool, weight_name_mapping: dict)
  GptOssForCausalLM.get_embed_and_head()
  GptOssForCausalLM.set_embed_and_head(embed, head)
  GptOssForCausalLM.set_eagle3_layers_to_capture(layer_ids: Optional[List[int]])
  GptOssForCausalLM.get_model_config_for_expert_location(cls, config)
  GptOssForCausalLM.get_attention_sliding_window_size()
  _WeightCreator.__init__(fn)
  _WeightCreator.maybe_materialize(obj)

# python/sglang/srt/models/granite.py
  GraniteMLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  GraniteMLP.forward(x)
  GraniteAttention.__init__(config: GraniteConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  GraniteAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  GraniteDecoderLayer.__init__(config: GraniteConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  GraniteDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  GraniteModel.__init__(config: GraniteConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  GraniteModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  GraniteForCausalLM.__init__(config: GraniteConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  GraniteForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool) -> LogitsProcessorOutput
  GraniteForCausalLM.get_module_name_from_weight_name(name)
  GraniteForCausalLM.get_num_params()
  GraniteForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  GraniteForCausalLM.get_weights_by_name(name: str, truncate_size: int, tp_size: int) -> Optional[torch.Tensor]

# python/sglang/srt/models/granitemoe.py
  GraniteMoeMoE.__init__(num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], tp_size: Optional[int], prefix: str)
  GraniteMoeMoE.forward(hidden_states: torch.Tensor) -> torch.Tensor
  GraniteMoeAttention.__init__(hidden_size: int, num_heads: int, num_kv_heads: int, max_position: int, layer_id: int, rope_theta: float, quant_config: Optional[QuantizationConfig], attention_multiplier: Optional[float], prefix: str) -> None
  GraniteMoeAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  GraniteMoeDecoderLayer.__init__(config: GraniteConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  GraniteMoeDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  GraniteMoeModel.__init__(config: GraniteConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  GraniteMoeModel.get_input_embeddings(input_ids: torch.Tensor) -> torch.Tensor
  GraniteMoeModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor]) -> torch.Tensor
  GraniteMoeForCausalLM.__init__(config: GraniteConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  GraniteMoeForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool) -> LogitsProcessorOutput
  GraniteMoeForCausalLM.load_weights(weights: Iterable[tuple[str, torch.Tensor]]) -> set[str]

# python/sglang/srt/models/grok.py
  Grok1MLP.__init__(hidden_size: int, intermediate_size: int, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, reduce_results, use_presharded_weights: bool, split_gate_up: bool) -> None
  Grok1MLP.forward(x)
  Grok1MoE.__init__(config: PretrainedConfig, layer_id: int, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], tp_size: Optional[int], reduce_results: bool, use_presharded_weights: bool, inplace: bool, no_combine: bool, prefix: str)
  Grok1MoE.forward(hidden_states: torch.Tensor) -> torch.Tensor
get_rope_scaling(config)
  ScalingRotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype) -> None
  Grok1Attention.__init__(config: PretrainedConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, max_position: int, rope_theta: float, quant_config: Optional[QuantizationConfig], reduce_results: bool, alt_stream: Optional[torch.cuda.Stream], load_presharded_attn: bool, prefix: str) -> None
  Grok1Attention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Grok1DecoderLayer.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], load_presharded_moe: bool, load_presharded_attn: bool, load_presharded_mlp: bool, alt_stream: Optional[torch.cuda.Stream], skip_moe: bool, prefix: str) -> None
  Grok1DecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], deferred_norm: Optional[RMSNorm]) -> Tuple[torch.Tensor, torch.Tensor, RMSNorm]
  Grok1DecoderLayer.moe_with_rmoe(x)
  Grok1Model.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], load_presharded_moe: bool, load_presharded_embedding: bool, load_presharded_attn: bool, load_presharded_mlp: bool, replicate_embedding: bool, prefix: str) -> None
  Grok1Model.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  Grok1ForCausalLM.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Grok1ForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  Grok1ForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]], ignore_parent_name: bool, check_hit_names: bool, model_config: PretrainedConfig | None) -> dict[str, torch.Tensor]
  Grok1ForCausalLM.get_num_params_analytical()
  Grok1ForCausalLM.get_num_params_torch()

# python/sglang/srt/models/hunyuan.py
  HunYuanMLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], bias: bool, prefix: str, reduce_results: bool) -> None
  HunYuanMLP.forward(x)
  HunYuanSparseMoeBlock.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], layer_id: int)
  HunYuanSparseMoeBlock.forward(hidden_states: torch.Tensor) -> torch.Tensor
get_head_dim(config)
check_head_dim(config)
  HunYuanAttention.__init__(config: PretrainedConfig, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], bias: bool, prefix: str, attention_type: str, layer_id: int) -> None
  HunYuanAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, kv_states: Optional[Tuple[torch.Tensor]]) -> torch.Tensor
  HunYuanDecoderLayer.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str, layer_id: int) -> None
  HunYuanDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], kv_states: Optional[Tuple[torch.Tensor]]) -> Tuple[torch.Tensor, torch.Tensor]
  HunYuanModel.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  HunYuanModel.get_input_embeddings(input_ids: torch.Tensor) -> torch.Tensor
  HunYuanModel.forward(input_ids: Optional[torch.Tensor], positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor]) -> torch.Tensor
  HunYuanMoEV1ForCausalLM.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig]) -> None
  HunYuanMoEV1ForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  HunYuanMoEV1ForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  HunYuanMoEV1ForCausalLM.load_kv_cache_scales(quantization_param_path: str) -> None

# python/sglang/srt/models/idefics2.py
  Idefics2VisionMLP.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Idefics2VisionMLP.forward(hidden_states: torch.Tensor) -> torch.Tensor
  Idefics2EncoderLayer.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Idefics2EncoderLayer.forward(hidden_states: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor
  Idefics2Encoder.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Idefics2Encoder.forward(inputs_embeds: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor
  Idefics2VisionEmbeddings.__init__(config: PretrainedConfig)
  Idefics2VisionEmbeddings.get_position_ids(pixel_values: torch.FloatTensor, patch_attention_mask: torch.BoolTensor, tgt_sizes: Optional[torch.IntTensor])
  Idefics2VisionEmbeddings.forward(pixel_values: torch.FloatTensor, patch_attention_mask: torch.BoolTensor, tgt_sizes: Optional[torch.IntTensor]) -> torch.Tensor
  Idefics2VisionTransformer.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], require_post_norm: bool, prefix: str) -> None
  Idefics2VisionTransformer.get_input_embeddings() -> nn.Embedding
  Idefics2VisionTransformer.compute_cu_seqlens(tgt_sizes: Optional[torch.Tensor], input_embeds: Optional[torch.Tensor]) -> torch.Tensor
  Idefics2VisionTransformer.forward(pixel_values, patch_attention_mask: Optional[torch.BoolTensor], tgt_sizes: Optional[torch.IntTensor]) -> torch.Tensor

# python/sglang/srt/models/internlm2.py
  InternLM2MLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  InternLM2MLP.forward(x)
  InternLM2Attention.__init__(hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  InternLM2Attention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  InternLMDecoderLayer.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  InternLMDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  InternLM2Model.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  InternLM2Model.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  InternLM2ForCausalLM.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  InternLM2ForCausalLM.get_input_embeddings() -> nn.Embedding
  InternLM2ForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  InternLM2ForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/internlm2_reward.py
  InternLM2ForRewardModel.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  InternLM2ForRewardModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool) -> EmbeddingPoolerOutput
  InternLM2ForRewardModel.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/interns1.py
  InternS1ForConditionalGeneration.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], use_flash_attn) -> None
  InternS1ForConditionalGeneration.pixel_shuffle(x, scale_factor)
  InternS1ForConditionalGeneration.extract_feature(pixel_values)
  InternS1ForConditionalGeneration.get_image_feature(items: List[MultimodalDataItem])
  InternS1ForConditionalGeneration.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  InternS1ForConditionalGeneration.pad_input_ids(input_ids: List[int], mm_inputs: MultimodalInputs)
  InternS1ForConditionalGeneration.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/internvl.py
  InternAttention.__init__(config, quant_config: QuantizationConfig)
  InternAttention.forward(hidden_states: torch.Tensor, cu_seqlens: torch.Tensor) -> torch.Tensor
  InternVisionEmbeddings.__init__(config: PretrainedConfig)
  InternVisionEmbeddings.forward(pixel_values: torch.FloatTensor) -> torch.Tensor
  InternRMSNorm.__init__(hidden_size, eps)
  InternRMSNorm.forward(hidden_states)
  InternMLP.__init__(config: PretrainedConfig)
  InternMLP.forward(hidden_states: torch.Tensor) -> torch.Tensor
  InternVisionEncoderLayer.__init__(config: PretrainedConfig, drop_path_rate: float, quant_config: QuantizationConfig)
  InternVisionEncoderLayer.forward(hidden_states: torch.Tensor, cu_seqlens: torch.Tensor) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor], Optional[Tuple[torch.FloatTensor]]]
  InternVisionEncoder.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig])
  InternVisionEncoder.forward(inputs_embeds, output_hidden_states: Optional[bool], return_dict: Optional[bool]) -> Union[Tuple, BaseModelOutput]
  InternVisionModel.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig])
  InternVisionModel.resize_pos_embeddings(old_size, new_size, patch_size)
  InternVisionModel.get_input_embeddings()
  InternVisionModel.forward(pixel_values: Optional[torch.FloatTensor], output_hidden_states: Optional[bool], return_dict: Optional[bool], pixel_embeds: Optional[torch.FloatTensor]) -> Union[Tuple, BaseModelOutputWithPooling]
  InternVLChatModel.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], use_flash_attn) -> None
  InternVLChatModel.pixel_shuffle(x, scale_factor)
  InternVLChatModel.extract_feature(pixel_values)
  InternVLChatModel.get_image_feature(items: List[MultimodalDataItem])
  InternVLChatModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  InternVLChatModel.pad_input_ids(input_ids: List[int], mm_inputs: MultimodalInputs)
  InternVLChatModel.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/kimi_vl.py
  KimiVLMultiModalProjector.__init__(config: KimiVLConfig)
  KimiVLMultiModalProjector.forward(image_features: torch.Tensor) -> torch.Tensor
  KimiVLForConditionalGeneration.__init__(config: KimiVLConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  KimiVLForConditionalGeneration.get_image_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  KimiVLForConditionalGeneration.pad_input_ids(input_ids: List[int], mm_inputs: MultimodalInputs)
  KimiVLForConditionalGeneration.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)
  KimiVLForConditionalGeneration.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
get_spec_layer_idx_from_weight_name(config: DeepseekV2Config, weight_name: str) -> Optional[int]

# python/sglang/srt/models/kimi_vl_moonvit.py
multihead_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, q_cu_seqlens: Optional[torch.Tensor], k_cu_seqlens: Optional[torch.Tensor])
sdpa_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, q_cu_seqlens: Optional[torch.Tensor], k_cu_seqlens: Optional[torch.Tensor]) -> torch.Tensor
apply_rope(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]
  Learnable2DInterpPosEmb.__init__(height: int, width: int, dim: int, interpolation_mode: str) -> None
  Learnable2DInterpPosEmb.reset_parameters()
  Learnable2DInterpPosEmb.forward(x: torch.Tensor, grid_hws: torch.Tensor) -> torch.Tensor
  MoonVisionPatchEmbed.__init__(out_dim: int, in_dim: int, patch_size: Union[int, Tuple[int, int]], pos_emb_height: int, pos_emb_width: int)
  MoonVisionPatchEmbed.forward(x: torch.Tensor, grid_hw: torch.Tensor) -> torch.Tensor
  Rope2DPosEmb.__init__(dim: int, max_height: int, max_width: int, theta_base, device)
  Rope2DPosEmb.extra_repr()
  Rope2DPosEmb.precomputed_freqs_cis() -> torch.Tensor
  Rope2DPosEmb.get_freqs_cis_by_seqlens(grid_hws: torch.Tensor) -> torch.Tensor
  Rope2DPosEmb.get_freqs_cis_by_idx(pos_idx: torch.Tensor, pos_idx_mask: torch.Tensor) -> torch.Tensor
  MLP2.__init__(dims: list[int], activation, bias)
  MLP2.forward(x: torch.Tensor) -> torch.Tensor
  MoonVitEncoderLayer.__init__(num_heads: int, hidden_dim: int, mlp_dim: int)
  MoonVitEncoderLayer.attention_qkvpacked(x: torch.Tensor, cu_seqlens: torch.Tensor, rope_freqs_cis: Optional[torch.Tensor])
  MoonVitEncoderLayer.forward(hidden_states: torch.Tensor, cu_seqlens: torch.Tensor, rope_freqs_cis: Union[torch.Tensor, None]) -> torch.Tensor
  MoonVitEncoder.__init__(hidden_dim: int, num_layers: int, block_cfg: dict) -> None
  MoonVitEncoder.forward(hidden_states: torch.Tensor, grid_hw: torch.Tensor) -> torch.Tensor
patch_merger(x: torch.Tensor, grid_hw: torch.Tensor, merge_kernel_size: list[int, int]) -> List[torch.Tensor]
  MoonVitVLProjector.__init__(in_channels: int, merge_kernel_size: list[int, int], hidden_act: str, ln_eps: float, out_dim: int)
  MoonVitVLProjector.forward(hidden_states: torch.Tensor) -> torch.Tensor
  MoonVitPretrainedModel.__init__(config: MoonViTConfig)
  MoonVitPretrainedModel.forward(pixel_values: torch.Tensor, grid_hw: torch.Tensor) -> torch.Tensor

# python/sglang/srt/models/llama.py
  LlamaMLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str, reduce_results: bool) -> None
  LlamaMLP.forward(x, forward_batch, use_reduce_scatter: bool)
  LlamaAttention.__init__(config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str, bias: bool) -> None
  LlamaAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  LlamaDecoderLayer.__init__(config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlamaDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  LlamaModel.__init__(config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlamaModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors]) -> Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]], PPProxyTensors]
  LlamaModel.load_kv_cache_scales(quantization_param_path: str) -> None
  LlamaForCausalLM.__init__(config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlamaForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors]) -> LogitsProcessorOutput
  LlamaForCausalLM.forward_split_prefill(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor) -> Optional[LogitsProcessorOutput]
  LlamaForCausalLM.start_layer()
  LlamaForCausalLM.end_layer()
  LlamaForCausalLM.get_input_embeddings() -> nn.Embedding
  LlamaForCausalLM.get_module_name_from_weight_name(name)
  LlamaForCausalLM.get_num_params()
  LlamaForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  LlamaForCausalLM.get_weights_by_name(name: str, truncate_size: int, tp_size: int) -> Optional[torch.Tensor]
  LlamaForCausalLM.get_embed_and_head()
  LlamaForCausalLM.set_embed_and_head(embed, head)
  LlamaForCausalLM.get_embed()
  LlamaForCausalLM.set_embed(embed)
  LlamaForCausalLM.load_kv_cache_scales(quantization_param_path: str) -> None
  LlamaForCausalLM.set_eagle3_layers_to_capture(layer_ids: Optional[List[int]])

# python/sglang/srt/models/llama4.py
  Llama4MoE.custom_routing_function(hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool) -> Tuple[torch.Tensor, torch.Tensor]
  Llama4MoE.__init__(config: Llama4TextConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  Llama4MoE.forward(hidden_states, forward_batch: ForwardBatch, use_reduce_scatter: bool)
  Llama4Attention.__init__(config: Llama4TextConfig, layer_id: int, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], bias: bool, bias_o_proj: bool, prefix: str) -> None
  Llama4Attention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Llama4DecoderLayer.__init__(config: Llama4TextConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  Llama4DecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  Llama4Model.__init__(config: Llama4TextConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Llama4Model.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors]) -> Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]]]
  Llama4ForCausalLM.__init__(config: Llama4TextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Llama4ForCausalLM.get_input_embeddings()

# python/sglang/srt/models/llama_classification.py
  LlamaForClassification.__init__(config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlamaForClassification.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool) -> EmbeddingPoolerOutput
  LlamaForClassification.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/llama_eagle.py
  LlamaDecoderLayer.__init__(config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlamaModel.__init__(config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlamaModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors]) -> torch.Tensor
  LlamaForCausalLMEagle.__init__(config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlamaForCausalLMEagle.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/llama_eagle3.py
  LlamaDecoderLayer.__init__(config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlamaDecoderLayer.forward(positions: torch.Tensor, embeds: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  LlamaModel.__init__(config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlamaModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors]) -> torch.Tensor
  LlamaForCausalLMEagle3.__init__(config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlamaForCausalLMEagle3.load_weights(weights: Iterable[Tuple[str, torch.Tensor]]) -> None
  LlamaForCausalLMEagle3.get_hot_token_id()

# python/sglang/srt/models/llama_embedding.py
  LlamaEmbeddingModel.__init__(config: LlamaConfig, quant_config, prefix: str) -> None
  LlamaEmbeddingModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool) -> EmbeddingPoolerOutput
  LlamaEmbeddingModel.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/llama_reward.py
  LlamaForSequenceClassification.__init__(config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlamaForSequenceClassification.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool) -> EmbeddingPoolerOutput
  LlamaForSequenceClassification.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  Weights.__init__(hidden_size, num_label)
  Weights.forward(x)
  LlamaForSequenceClassificationWithNormal_Weights.__init__(config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlamaForSequenceClassificationWithNormal_Weights.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool) -> EmbeddingPoolerOutput
  LlamaForSequenceClassificationWithNormal_Weights.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/llava.py
  LlavaBaseForCausalLM.pad_input_ids(input_ids: List[int], image_inputs: MultimodalInputs)
  LlavaBaseForCausalLM.encode_images(pixel_values: Union[torch.Tensor, List[torch.Tensor]]) -> torch.Tensor
  LlavaBaseForCausalLM.forward(input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  LlavaBaseForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  LlavaBaseForCausalLM.num_patches_per_side()
  LlavaLlamaForCausalLM.__init__(config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlavaQwenForCausalLM.__init__(config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlavaMistralForCausalLM.__init__(config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlavaForConditionalGeneration.dtype()
  LlavaForConditionalGeneration.pad_input_ids(input_ids: List[int], image_inputs: MultimodalInputs)
  LlavaForConditionalGeneration.__init__(config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlavaForConditionalGeneration.get_image_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  LlavaForConditionalGeneration.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)
  LlavaForConditionalGeneration.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/llavavid.py
  LlavaVidForCausalLM.__init__(config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlavaVidForCausalLM.pad_input_ids(input_ids: List[int], image_inputs: MultimodalInputs)
  LlavaVidForCausalLM.encode_images(pixel_values: torch.Tensor) -> torch.Tensor
  LlavaVidForCausalLM.forward(input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  LlavaVidForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  LlavaVidForCausalLM.num_patches_per_side()

# python/sglang/srt/models/longcat_flash.py
  LongcatFlashMLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str) -> None
  LongcatFlashMLP.forward(x)
  LongcatFlashRouter.__init__(config, zero_expert_num, rounter_params_dtype, prefix: str)
  LongcatFlashRouter.forward(hidden_states)
  LongcatFlashMoE.__init__(config: LongcatFlashConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  LongcatFlashMoE.forward(hidden_states: torch.Tensor) -> torch.Tensor
  LongcatFlashMoE.get_moe_weights()
  LongcatFlashDecoderLayer.__init__(config: LongcatFlashConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream]) -> None
  LongcatFlashDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], zero_allocator: BumpAllocator) -> torch.Tensor
  LongcatFlashDecoderLayer.forward_mlp(hidden_states, positions, residual, forward_batch, zero_allocator)
  LongcatFlashModel.__init__(config: LongcatFlashConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LongcatFlashModel.get_input_embeddings() -> torch.Tensor
  LongcatFlashModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  LongcatFlashForCausalLM.__init__(config: LongcatFlashConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LongcatFlashForCausalLM.get_input_embeddings() -> nn.Embedding
  LongcatFlashForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  LongcatFlashForCausalLM.post_load_weights(weight_names)
  LongcatFlashForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  LongcatFlashForCausalLM.get_embed_and_head()
  LongcatFlashForCausalLM.set_embed_and_head(embed, head)
  LongcatFlashForCausalLM.get_model_config_for_expert_location(cls, config)

# python/sglang/srt/models/longcat_flash_nextn.py
  LongcatFlashDenseDecoderLayer.__init__(config: LongcatFlashConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream]) -> None
  LongcatFlashDenseDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], zero_allocator: BumpAllocator) -> torch.Tensor
  LongcatFlashModelNextN.__init__(config: LongcatFlashConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LongcatFlashModelNextN.get_input_embeddings() -> torch.Tensor
  LongcatFlashModelNextN.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  LongcatFlashForCausalLMNextN.__init__(config: LongcatFlashConfig, quant_config: Optional[QuantizationConfig]) -> None
  LongcatFlashForCausalLMNextN.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  LongcatFlashForCausalLMNextN.post_load_weights()
  LongcatFlashForCausalLMNextN.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/mimo.py
  MiMoModel.__init__(config: MiMoConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MiMoForCausalLM.__init__(config: MiMoConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MiMoForCausalLM.get_input_embeddings(input_ids: torch.Tensor) -> torch.Tensor
  MiMoForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool) -> torch.Tensor
  MiMoForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  MiMoForCausalLM.get_embed_and_head()
  MiMoForCausalLM.set_embed_and_head(embed, head)
  MiMoForCausalLM.load_kv_cache_scales(quantization_param_path: str) -> None

# python/sglang/srt/models/mimo_mtp.py
  MiMoMultiTokenPredictorLayer.__init__(config: PretrainedConfig, prefix: str, quant_config: Optional[QuantizationConfig]) -> None
  MiMoMultiTokenPredictorLayer.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  MiMoMTP.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MiMoMTP.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  MiMoMTP.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  MiMoMTP.map_model_name_to_mtp_param_name(name: str) -> str
  MiMoMTP.get_embed_and_head()
  MiMoMTP.set_embed_and_head(embed, head)

# python/sglang/srt/models/minicpm.py
  MiniCPMMLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MiniCPMMLP.forward(x)
  MiniCPMAttention.__init__(hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MiniCPMAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  MiniCPMDecoderLayer.__init__(config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MiniCPMDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  MiniCPMModel.__init__(config, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MiniCPMModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  MiniCPMForCausalLM.__init__(config, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MiniCPMForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  MiniCPMForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/minicpm3.py
  MiniCPM3MLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MiniCPM3MLP.forward(x)
input_to_float8(x, dtype)
  MiniCPM3AttentionMLA.__init__(config: PretrainedConfig, hidden_size: int, num_heads: int, qk_nope_head_dim: int, qk_rope_head_dim: int, v_head_dim: int, q_lora_rank: int, kv_lora_rank: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], layer_id, prefix: str) -> None
  MiniCPM3AttentionMLA.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  MiniCPM3DecoderLayer.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MiniCPM3DecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  MiniCPM3Model.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MiniCPM3Model.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  MiniCPM3ForCausalLM.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MiniCPM3ForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  MiniCPM3ForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/minicpmo.py
apply_spk_emb(input_ids: torch.Tensor, spk_emb: torch.Tensor, input_embeds: torch.Tensor, spk_emb_token_id: int, num_spk_embs: int)
make_streaming_chunk_mask_generation(inputs_embeds: torch.Tensor, past_seen_tokens: int, streaming_tts_text_mask: torch.Tensor, streaming_reserved_length: int, streaming_audio_chunk_size: int, streaming_text_chunk_size: int, num_spk_emb: int, use_spk_emb: bool) -> torch.Tensor
  ConvNeXtBlock.__init__(dim: int, intermediate_dim: int, kernel: int, dilation: int, layer_scale_init_value: float)
  ConvNeXtBlock.forward(x: torch.Tensor, cond) -> torch.Tensor
  DVAEDecoder.__init__(idim: int, odim: int, n_layer, bn_dim, hidden, kernel, dilation, up)
  DVAEDecoder.forward(x: torch.Tensor, conditioning) -> torch.Tensor
  GFSQ.__init__(dim: int, levels: List[int], G: int, R: int, eps, transpose)
  GFSQ.__call__(x: torch.Tensor) -> torch.Tensor
  GFSQ.forward(x: torch.Tensor) -> torch.Tensor
  DVAE.__init__()
  DVAE.forward(inp: torch.Tensor, mode: Literal['encode', 'decode']) -> torch.Tensor
  CustomRepetitionPenaltyLogitsProcessorRepeat.__init__(penalty: float, max_input_ids: int, past_window: int)
  CustomRepetitionPenaltyLogitsProcessorRepeat.__call__(input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor
  ConditionalChatTTS.__init__(config: PretrainedConfig)
  ConditionalChatTTS.merge_inputs_embeds(input_ids: torch.Tensor, lm_spk_emb_last_hidden_states: Optional[torch.Tensor])
  ConditionalChatTTS.prefill_text(input_ids: torch.Tensor, position_ids: torch.LongTensor, past_key_values: List[Tuple[torch.Tensor, torch.Tensor]], lm_spk_emb_last_hidden_states: Optional[torch.Tensor])
  ConditionalChatTTS.prefill_audio_ids(input_ids: torch.Tensor, past_key_values: List[Tuple[torch.Tensor, torch.Tensor]], streaming_tts_text_mask, add_audio_bos: bool)
  ConditionalChatTTS.generate(input_ids: torch.Tensor, past_key_values: List[Tuple[torch.Tensor, torch.Tensor]], temperature: torch.Tensor, eos_token: Union[int, torch.Tensor], streaming_tts_text_mask, force_no_stop, min_new_token, max_new_token, logits_warpers: List[LogitsWarper], logits_processors: List[CustomRepetitionPenaltyLogitsProcessorRepeat], show_tqdm)
  ConditionalChatTTS.decode_to_mel_specs(result_list: List[torch.Tensor])
  MiniCPMWhisperEncoderLayer.__init__(config: WhisperConfig, layer_idx: int)
  MiniCPMWhisperEncoderLayer.forward(hidden_states: torch.Tensor, attention_mask: torch.Tensor, layer_head_mask: torch.Tensor, output_attentions: bool, past_key_values: Optional[EncoderDecoderCache], use_cache: Optional[bool]) -> torch.Tensor
  MiniCPMWhisperEncoder.__init__(config: WhisperConfig)
  MiniCPMWhisperEncoder.forward(input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, past_key_values: Optional[EncoderDecoderCache], use_cache: Optional[bool])
  MultiModalProjector.__init__(in_dim, out_dim)
  MultiModalProjector.forward(audio_features)
  MiniCPMO.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig]) -> None
  MiniCPMO.init_tts_module()
  MiniCPMO.init_audio_module()
  MiniCPMO.init_llm(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> nn.Module
  MiniCPMO.init_vision_module(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  MiniCPMO.init_resampler(embed_dim: int, vision_dim: int, quant_config: Optional[QuantizationConfig], prefix: str) -> nn.Module
  MiniCPMO.pad_input_ids(input_ids: List[int], mm_input: MultimodalInputs)
  MiniCPMO.get_audio_embedding_streaming(items: List[MultimodalDataItem])
  MiniCPMO.subsequent_chunk_mask(size: int, chunk_size: int, num_left_chunks: int, device: torch.device, num_lookhead: int) -> torch.Tensor
  MiniCPMO.get_audio_embedding(items: List[MultimodalDataItem], chunk_length)
  MiniCPMO.get_audio_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  MiniCPMO.get_omni_embedding(items: List[MultimodalDataItem], chunk_length, stream_input)
  MiniCPMO.get_image_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  MiniCPMO.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  MiniCPMO.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/minicpmv.py
get_1d_sincos_pos_embed_from_grid(embed_dim: int, pos: np.ndarray, version: Tuple[int, int]) -> torch.Tensor
get_2d_sincos_pos_embed_from_grid(embed_dim: int, grid: np.ndarray, version: Tuple[int, int]) -> torch.Tensor
get_2d_sincos_pos_embed(embed_dim: int, grid_size: Union[int, Tuple[int, int]], cls_token: bool, version: Tuple[int, int]) -> torch.Tensor
  BaseResampler.__init__(num_queries: int, embed_dim: int, num_heads: int, kv_dim: Optional[int], norm_layer: Callable[[int], nn.LayerNorm], do_post_projection: bool, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Resampler2_5.__init__(num_queries: int, embed_dim: int, num_heads: int, kv_dim: Optional[int], norm_layer: Callable[[int], nn.LayerNorm], max_size: Tuple[int, int], quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Resampler2_5.forward(x: torch.Tensor, tgt_sizes: torch.Tensor) -> torch.Tensor
get_version_by_config(config: PretrainedConfig) -> Tuple[int, ...]
  MiniCPMBaseModel.__init__()
  MiniCPMBaseModel.get_embedding(input_ids: torch.Tensor, image_inputs: Optional[MiniCPMVImageInputs]) -> Tuple[torch.Tensor, torch.Tensor]
  MiniCPMBaseModel.get_input_embeddings() -> nn.Embedding
  MiniCPMBaseModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  MiniCPMBaseModel.init_llm(config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str) -> nn.Module
  MiniCPMBaseModel.init_vision_module(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> nn.Module
  MiniCPMBaseModel.init_resampler(embed_dim: int, vision_dim: int, quant_config: Optional[QuantizationConfig], prefix: str) -> nn.Module
  MiniCPMBaseModel.get_vision_embedding(pixel_values: List[torch.Tensor], patch_attn_mask: Optional[torch.Tensor], tgt_sizes: Optional[torch.Tensor]) -> torch.Tensor
  MiniCPMBaseModel.get_image_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  MiniCPMV2_6.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  MiniCPMV2_6.init_llm(config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str) -> nn.Module
  MiniCPMV2_6.init_vision_module(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> nn.Module
  MiniCPMV2_6.init_resampler(embed_dim: int, vision_dim: int, quant_config: Optional[QuantizationConfig], prefix: str) -> nn.Module
  MiniCPMV2_6.get_vision_embedding(pixel_values: List[torch.Tensor], patch_attn_mask: Optional[torch.Tensor], tgt_sizes: Optional[torch.Tensor]) -> torch.Tensor
  MiniCPMV2_6.get_image_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  MiniCPMV2_6.pad_input_ids(input_ids: List[int], image_inputs: MultimodalInputs)
  MiniCPMV.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MiniCPMV.__getattr__(name)
  MiniCPMV.__call__()
  MiniCPMV.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/mistral.py
  Mistral3ForConditionalGeneration.__init__()
  Mistral3ForConditionalGeneration.get_image_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  Mistral3ForConditionalGeneration.__getattr__(name)
  Mistral3ForConditionalGeneration.__hasattr__(name)
  Mistral3ForConditionalGeneration.__call__()

# python/sglang/srt/models/mixtral.py
  MixtralMoE.__init__(num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], tp_size: Optional[int], prefix: str)
  MixtralMoE.forward(hidden_states: torch.Tensor) -> torch.Tensor
  MixtralAttention.__init__(hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, max_position: int, rope_theta: float, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MixtralAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  MixtralDecoderLayer.__init__(config: MixtralConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MixtralDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> torch.Tensor
  MixtralModel.__init__(config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MixtralModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors]) -> Union[torch.Tensor, PPProxyTensors]
  MixtralForCausalLM.__init__(config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MixtralForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors]) -> torch.Tensor
  MixtralForCausalLM.start_layer()
  MixtralForCausalLM.end_layer()
  MixtralForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/mixtral_quant.py
  MixtralMLP.__init__(num_experts: int, hidden_size: int, intermediate_size: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MixtralMLP.forward(hidden_states: torch.Tensor) -> torch.Tensor
  MixtralMoE.__init__(config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  MixtralMoE.forward(hidden_states: torch.Tensor) -> torch.Tensor
  MixtralAttention.__init__(hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, max_position: int, rope_theta: float, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MixtralAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  MixtralDecoderLayer.__init__(config: MixtralConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MixtralDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> torch.Tensor
  MixtralModel.__init__(config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MixtralModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  QuantMixtralForCausalLM.__init__(config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  QuantMixtralForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  QuantMixtralForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/mllama.py
  ColumnParallelConv2dPatch.__init__(in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]], bias: bool) -> None
  ColumnParallelConv2dPatch.forward(x: torch.Tensor) -> torch.Tensor
  MllamaPrecomputedAspectRatioEmbedding.__init__(config: config_mllama.MllamaVisionConfig, is_gated: bool)
  MllamaPrecomputedAspectRatioEmbedding.forward(hidden_state: torch.Tensor, aspect_ratio_ids: torch.Tensor) -> torch.Tensor
  MllamaPrecomputedPositionEmbedding.__init__(config: config_mllama.MllamaVisionConfig)
  MllamaPrecomputedPositionEmbedding.forward(hidden_state: torch.Tensor, aspect_ratio_ids: torch.Tensor) -> torch.Tensor
  MllamaVisionMLP.__init__(config, quant_config: Optional[QuantizationConfig], prefix: str)
  MllamaVisionMLP.forward(hidden_states: torch.Tensor) -> torch.Tensor
  MllamaVisionEncoderLayer.__init__(config: config_mllama.MllamaVisionConfig, quant_config: Optional[QuantizationConfig], is_gated: bool, prefix: str)
  MllamaVisionEncoderLayer.forward(hidden_state: torch.Tensor, attention_mask: Optional[torch.Tensor])
  MllamaVisionEncoder.__init__(config: config_mllama.MllamaVisionConfig, quant_config: Optional[QuantizationConfig], num_layers, is_gated, output_hidden_states, prefix: str)
  MllamaVisionEncoder.forward(hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor]) -> Union[Tuple, BaseModelOutput]
  MllamaVisionModel.__init__(config: config_mllama.MllamaVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  MllamaVisionModel.apply_class_embedding(hidden_state: torch.Tensor) -> torch.Tensor
  MllamaVisionModel.forward(pixel_values: torch.Tensor, aspect_ratio_ids: torch.Tensor, aspect_ratio_mask: torch.Tensor) -> torch.Tensor
  MllamaTextRMSNorm.__init__(hidden_size, eps)
  MllamaTextRMSNorm.forward(hidden_states)
  MllamaTextRMSNorm.extra_repr()
  MllamaTextCrossAttention.__init__(config: Optional[config_mllama.MllamaTextConfig], layer_id: Optional[int], quant_config: Optional[QuantizationConfig], prefix: str)
  MllamaTextCrossAttention.forward(hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor], cross_attention_states: Optional[torch.Tensor], forward_batch: ForwardBatch) -> torch.Tensor
  MllamaCrossAttentionDecoderLayer.__init__(config: config_mllama.MllamaTextConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  MllamaCrossAttentionDecoderLayer.forward(hidden_states: torch.Tensor, cross_attention_states: torch.Tensor, cross_attention_mask: torch.Tensor, full_text_row_masked_out_mask: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  MllamaTextModel.__init__(config: config_mllama.MllamaTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  MllamaTextModel.forward(input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], cross_attention_states: Optional[torch.LongTensor], cross_attention_mask: Optional[torch.LongTensor], full_text_row_masked_out_mask: Optional[Tuple[torch.Tensor, torch.Tensor]], forward_batch: ForwardBatch, skip_cross_attention: bool) -> torch.Tensor
  MllamaForCausalLM.__init__(config: config_mllama.MllamaTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  MllamaForCausalLM.forward(input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], cross_attention_states: Optional[torch.LongTensor], cross_attention_mask: Optional[torch.LongTensor], full_text_row_masked_out_mask: Optional[Tuple[torch.Tensor, torch.Tensor]], forward_batch: ForwardBatch, skip_cross_attention: bool) -> torch.Tensor
  MllamaForConditionalGeneration.__init__(config: config_mllama.MllamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  MllamaForConditionalGeneration.pad_input_ids(input_ids: List[int], mm_inputs: MultimodalInputs)
  MllamaForConditionalGeneration.flat_encoder_result(cross_attention_states: torch.Tensor, encoder_lens_need: List[int])
  MllamaForConditionalGeneration.get_full_text_row_masked_out_mask(forward_batch: ForwardBatch)
  MllamaForConditionalGeneration.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> Union[Tuple, CausalLMOutputWithPast]
  MllamaForConditionalGeneration.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/mllama4.py
  Llama4VisionMLP.__init__(input_size: int, intermediate_size: int, output_size: int, bias: bool, output_activation: bool, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)
  Llama4VisionMLP.forward(hidden_states: torch.Tensor) -> torch.Tensor
pixel_shuffle(input_tensor, shuffle_ratio)
  Llama4VisionPixelShuffleMLP.__init__(config, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)
  Llama4VisionPixelShuffleMLP.forward(encoded_patches: torch.Tensor) -> torch.Tensor
apply_position_embedding(q, k, freqs_ci, shape)
  Llama4VisionEncoderLayer.__init__(config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)
  Llama4VisionEncoderLayer.forward(hidden_state: torch.Tensor, freqs_ci: torch.Tensor)
  Llama4VisionEncoder.__init__(config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)
  Llama4VisionEncoder.forward(hidden_states: torch.Tensor, freqs_ci: torch.Tensor) -> torch.Tensor
  Llama4UnfoldConvolution.__init__(config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)
  Llama4UnfoldConvolution.forward(hidden_states: torch.Tensor) -> torch.Tensor
  Llama4VisionRotaryEmbedding.__init__(config)
  Llama4VisionRotaryEmbedding.forward(hidden_states)
  Llama4VisionModel.__init__(config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Llama4VisionModel.forward(pixel_values: torch.Tensor) -> torch.Tensor
  Llama4ForConditionalGeneration.__init__(config: Llama4Config, quant_config: Optional[QuantizationConfig], prefix: str)
  Llama4ForConditionalGeneration.pad_input_ids(input_ids: List[int], mm_inputs: MultimodalInputs)
  Llama4ForConditionalGeneration.get_image_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  Llama4ForConditionalGeneration.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Llama4ForConditionalGeneration.permute_qk_weight_for_rotary(name: str, loaded_weight: torch.Tensor) -> Tuple[str, torch.Tensor]
  Llama4ForConditionalGeneration.load_weights(weights: Iterable[Tuple[str, torch.Tensor]]) -> Set[str]
  Llama4ForConditionalGeneration.set_eagle3_layers_to_capture(layer_ids: Optional[List[int]])
  Llama4ForConditionalGeneration.get_embed_and_head()
  Llama4ForConditionalGeneration.set_embed_and_head(embed, head)
  Llama4ForConditionalGeneration.get_embed()
  Llama4ForConditionalGeneration.set_embed(embed)

# python/sglang/srt/models/nemotron_nas.py
  DeciLMDecoderLayer.__init__(config: LlamaConfig, layer_idx: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  DeciLMDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  DeciModel.__init__()
  DeciModel.get_input_embeddings(input_ids: torch.Tensor) -> torch.Tensor
  DeciModel.forward(input_ids: Optional[torch.Tensor], positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor], pp_proxy_tensors: Optional[PPProxyTensors]) -> Union[torch.Tensor, PPProxyTensors]
  DeciLMForCausalLM.__init__()
  DeciLMForCausalLM.get_input_embeddings(input_ids: torch.Tensor) -> torch.Tensor
  DeciLMForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor], get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors]) -> LogitsProcessorOutput
  DeciLMForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]]) -> None

# python/sglang/srt/models/olmo.py
  OlmoAttention.__init__(config: OlmoConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  OlmoAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  OlmoMLP.__init__(config: OlmoConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  OlmoMLP.forward(x: torch.Tensor) -> torch.Tensor
  OlmoDecoderLayer.__init__(config: OlmoConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  OlmoDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]
  OlmoModel.__init__(config: OlmoConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  OlmoModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  OlmoForCausalLM.__init__(config: OlmoConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  OlmoForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  OlmoForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/olmo2.py
  Olmo2Attention.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  Olmo2Attention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Olmo2MLP.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Olmo2MLP.forward(x: torch.Tensor) -> torch.Tensor
  Olmo2DecoderLayer.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  Olmo2DecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Olmo2Model.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Olmo2Model.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  Olmo2ForCausalLM.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Olmo2ForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  Olmo2ForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/olmoe.py
  OlmoeMoE.__init__(num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], tp_size: Optional[int], layer_id: int, prefix: str)
  OlmoeMoE.forward(hidden_states: torch.Tensor) -> torch.Tensor
  OlmoeAttention.__init__(layer_id: int, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  OlmoeAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  OlmoeDecoderLayer.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  OlmoeDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> torch.Tensor
  OlmoeModel.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  OlmoeModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  OlmoeForCausalLM.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  OlmoeForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  OlmoeForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/persimmon.py
  PersimmonMLP.__init__(config: PersimmonConfig, quant_config: Optional[QuantizationConfig])
  PersimmonMLP.forward(hidden_states) -> torch.Tensor
  PersimmonAttention.__init__(config: PersimmonConfig, quant_config: Optional[QuantizationConfig], prefix: str, layer_id: int)
  PersimmonAttention.forward(position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor) -> torch.Tensor
  PersimmonDecoderLayer.__init__(config: PersimmonConfig, quant_config: Optional[QuantizationConfig], prefix: str, idx: int)
  PersimmonDecoderLayer.forward(position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor) -> torch.Tensor
  PersimmonModel.__init__(config: PersimmonConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  PersimmonModel.get_input_embeddings(input_ids: torch.Tensor) -> torch.Tensor
  PersimmonModel.forward(input_ids: torch.Tensor, forward_batch: ForwardBatch, positions: torch.Tensor, inputs_embeds: Optional[torch.Tensor]) -> torch.Tensor
  PersimmonForCausalLM.__init__(config: PersimmonConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  PersimmonForCausalLM.get_input_embeddings(input_ids: torch.Tensor) -> torch.Tensor
  PersimmonForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor]) -> LogitsProcessorOutput
  PersimmonForCausalLM.load_weights(weights: Iterable[tuple[str, torch.Tensor]])

# python/sglang/srt/models/phi.py
  PhiAttention.__init__(config: PhiConfig, quant_config: Optional[QuantizationConfig], prefix: str, layer_id: int)
  PhiAttention.forward(position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor) -> torch.Tensor
  PhiMLP.__init__(config: PhiConfig, quant_config: Optional[QuantizationConfig])
  PhiMLP.forward(hidden_states)
  PhiLayer.__init__(config: PhiConfig, quant_config: Optional[QuantizationConfig], prefix: str, idx: int)
  PhiLayer.forward(position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor) -> torch.Tensor
  PhiModel.__init__(config: PhiConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  PhiModel.get_input_embeddings(input_ids: torch.Tensor) -> torch.Tensor
  PhiModel.forward(input_ids: torch.Tensor, forward_batch: ForwardBatch, positions: torch.Tensor, inputs_embeds: Optional[torch.Tensor]) -> torch.Tensor
  PhiForCausalLM.__init__(config: PhiConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  PhiForCausalLM.get_input_embeddings(input_ids: torch.Tensor) -> torch.Tensor
  PhiForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor]) -> LogitsProcessorOutput
  PhiForCausalLM.load_weights(weights: Iterable[tuple[str, torch.Tensor]])

# python/sglang/srt/models/phi3_small.py
quick_gelu(x)
gegelu(input, limit: Optional[float])
  Phi3SmallMLP.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Phi3SmallMLP.forward(x)
  Phi3SmallSelfAttention.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Phi3SmallSelfAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]
  Phi3SmallDecoderLayer.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  Phi3SmallDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Phi3SmallModel.__init__(config: Phi3Config, quant_config: Optional[QuantizationConfig], prefix: str)
  Phi3SmallModel.get_input_embeddings(input_ids: torch.Tensor) -> torch.Tensor
  Phi3SmallModel.forward(input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor]) -> Union[torch.Tensor]
  Phi3SmallForCausalLM.__init__(config: Phi3Config, quant_config: Optional[QuantizationConfig], prefix: str)
  Phi3SmallForCausalLM.get_input_embeddings(input_ids: torch.Tensor) -> torch.Tensor
  Phi3SmallForCausalLM.set_input_embeddings(value)
  Phi3SmallForCausalLM.get_output_embeddings()
  Phi3SmallForCausalLM.set_output_embeddings(value)
  Phi3SmallForCausalLM.set_decoder(decoder)
  Phi3SmallForCausalLM.get_decoder()
  Phi3SmallForCausalLM.compute_logits(input_ids: torch.LongTensor, hidden_states: torch.Tensor, sampling_metadata) -> Optional[torch.Tensor]
  Phi3SmallForCausalLM.forward(input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor], get_embedding: bool) -> LogitsProcessorOutput
  Phi3SmallForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/phi4mm.py
  Phi4MMImageEncoder.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str, model_dir: str) -> None
  Phi4MMImageEncoder.get_img_features(img_embeds: torch.FloatTensor, attention_mask) -> torch.FloatTensor
  Phi4MMImageEncoder.forward(pixel_values: torch.FloatTensor, image_sizes: torch.Tensor, image_attention_mask: torch.Tensor) -> list[torch.FloatTensor]
  Phi4MMForCausalLM.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Phi4MMForCausalLM.get_image_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  Phi4MMForCausalLM.get_audio_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  Phi4MMForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Phi4MMForCausalLM.pad_input_ids(input_ids: List[int], mm_inputs: MultimodalInputs)
  Phi4MMForCausalLM.should_apply_lora(module_name: str) -> bool
  Phi4MMForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/phi4mm_audio.py
  ConformerEncoderLayer.__init__(d_model, ext_pw_out_channel, depthwise_seperable_out_channel, depthwise_multiplier, n_head, d_ffn, ext_pw_kernel_size, kernel_size, dropout_rate, causal, batch_norm, activation, chunk_se, chunk_size, conv_activation, conv_glu_type, bias_in_glu, linear_glu_in_convm, attention_inner_dim, attention_glu_type, activation_checkpointing, export, use_pt_scaled_dot_product_attention, attn_group_sizes: int)
  ConformerEncoderLayer.forward(x, pos_k, pos_v, mask, relative_attention_bias: Optional[Tensor])
  TransformerEncoderBase.__init__(input_size, chunk_size, left_chunk, attention_dim, attention_heads, input_layer, cnn_out, cnn_layer_norm, time_reduction, dropout_rate, padding_idx, relative_attention_bias_args, positional_dropout_rate, nemo_conv_settings, conv2d_extra_padding: Literal['feat', 'feat_time', 'none', True], attention_group_size, encoder_embedding_config)
  TransformerEncoderBase.compute_lens_change(feature_lens)
  TransformerEncoderBase.forward()
  TransformerEncoderBase.forward_embeddings(xs_pad, masks, chunk_size_nc, left_chunk_nc)
  TransformerEncoderBase.get_offset()
  ConformerEncoder.__init__(input_size, chunk_size, left_chunk, num_lang, attention_dim, attention_heads, linear_units, num_blocks, dropout_rate, input_layer, causal, batch_norm, cnn_out, cnn_layer_norm, ext_pw_out_channel, ext_pw_kernel_size, depthwise_seperable_out_channel, depthwise_multiplier, chunk_se, kernel_size, activation, conv_activation, conv_glu_type, bias_in_glu, linear_glu_in_convm, attention_glu_type, export, extra_layer_output_idx, extra_multi_layer_output_idxs, activation_checkpointing, relative_attention_bias_args, time_reduction, use_pt_scaled_dot_product_attention, nemo_conv_settings, conv2d_extra_padding: Literal['feat', 'feat_time', 'none', True], replication_pad_for_subsample_embedding, attention_group_size, encoder_embedding_config)
  ConformerEncoder.init_relative_attention_bias(input_tensor)
  ConformerEncoder.calculate_hs_mask(xs_pad, device, mask)
  ConformerEncoder.forward(xs_pad, masks)
  WindowQformer.__init__(window_size: int, num_queries: int, num_blocks: int, attention_dim: int, attention_heads: int, linear_units: int, dropout_rate: float, normalize_before: bool)
  WindowQformer.forward(audio_embed, mask, embed_len)
  AudioEmbedding.__init__(config: PretrainedConfig) -> None
  AudioEmbedding.set_audio_embeds(input_embeds: torch.FloatTensor) -> None
  AudioEmbedding.set_audio_embed_sizes(audio_embed_sizes: torch.LongTensor) -> None
  AudioEmbedding.get_audio_features(input_embeds: torch.FloatTensor, audio_attention_mask: torch.Tensor, audio_projection_mode: str) -> torch.FloatTensor
  AudioEmbedding.forward(audio_features: torch.FloatTensor, audio_attention_mask: torch.Tensor, audio_projection_mode: str) -> torch.FloatTensor

# python/sglang/srt/models/phi4mm_utils.py
  BlockBase.__init__(input_size, output_size)
get_activation(name)
adaptive_enc_mask(x_len, chunk_start_idx, left_window, right_window)
  Swish.__init__() -> None
  Swish.forward(x: Tensor) -> Tensor
  GLU.__init__(dim: int, act_name: str) -> None
  GLU.forward(x: Tensor) -> Tensor
  GLUPointWiseConv.__init__(input_dim, output_dim, kernel_size, glu_type, bias_in_glu, causal)
  GLUPointWiseConv.forward(x)
  DepthWiseSeperableConv1d.__init__(input_dim, depthwise_seperable_out_channel, kernel_size, depthwise_multiplier, padding)
  DepthWiseSeperableConv1d.forward(x)
  ConvModule.__init__(input_dim, ext_pw_out_channel, depthwise_seperable_out_channel, ext_pw_kernel_size, kernel_size, depthwise_multiplier, dropout_rate, causal, batch_norm, chunk_se, chunk_size, activation, glu_type, bias_in_glu, linear_glu_in_convm, export)
  ConvModule.forward(x)
  GLULinear.__init__(input_dim, output_dim, glu_type, bias_in_glu)
  GLULinear.forward(x)
  FeedForward.__init__(d_model, d_inner, dropout_rate, activation, bias_in_glu)
  FeedForward.forward(x)
  T5RelativeAttentionLogitBias.__init__(num_heads, num_buckets, max_distance, symmetric)
  T5RelativeAttentionLogitBias.forward(x)
  AbsolutePositionalEncoding.__init__(d_model, dropout_rate, max_len)
  AbsolutePositionalEncoding.extend_pe(x)
  AbsolutePositionalEncoding.forward(x: torch.Tensor)
  MeanVarianceNormLayer.__init__(input_size)
  MeanVarianceNormLayer.forward(input_: Tensor) -> Tensor
  CausalConv1D.__init__(in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: Union[str, int], dilation: int, groups: int, bias: bool, padding_mode: str, device, dtype) -> None
  CausalConv1D.update_cache(x, cache)
  CausalConv1D.forward(x, cache)
  CausalConv2D.__init__(in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: Union[str, int], dilation: int, groups: int, bias: bool, padding_mode: str, device, dtype) -> None
  CausalConv2D.forward(x)
  NemoConvSubsampling.__init__(feat_in, feat_out, subsampling_factor, subsampling, conv_channels, subsampling_conv_chunking_factor, activation, is_causal)
  NemoConvSubsampling.get_sampling_frames()
  NemoConvSubsampling.get_streaming_cache_size()
  NemoConvSubsampling.forward(x, mask)
  NemoConvSubsampling.reset_parameters()
  NemoConvSubsampling.conv_split_by_batch(x)
  NemoConvSubsampling.conv_split_by_channel(x)
  NemoConvSubsampling.channel_chunked_conv(conv, chunk_size, x)
  NemoConvSubsampling.change_subsampling_conv_chunking_factor(subsampling_conv_chunking_factor: int)
calc_length(lengths, all_paddings, kernel_size, stride, ceil_mode, repeat_num)
  AttModule.__init__()
  AttModule.set_export(mode)
  AttModule.forward(x: Tensor, memory: Optional[Tensor], pos_emb: Optional[Tensor], att_mask: Optional[Tensor]) -> tuple[Tensor, Tensor, Optional[Tensor], Optional[Tensor]]
  AttBlock.memory_dims(max_len)
masked_softmax(scores, mask: Optional[Tensor])
  MultiHeadedAttention.__init__(n_head, n_feat, dropout_rate, attention_inner_dim, glu_type, bias_in_glu, use_pt_scaled_dot_product_attention, n_value, group_size: int)
  MultiHeadedAttention.forward(query: Tensor, key: Tensor, value: Tensor, pos_k: Tensor, pos_v: Tensor, mask: Optional[Tensor], relative_attention_bias: Optional[Tensor])
  MultiSequential.forward()
get_offset(input_layer: str, time_reduction: int)
unfold_tensor(xs_pad, max_seq_len)

# python/sglang/srt/models/phimoe.py
  PhiMoEConfig.__init__(vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, head_dim, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, tie_word_embeddings, rope_theta, sliding_window, attention_dropout, num_experts_per_tok, num_local_experts, output_router_logits, router_aux_loss_coef, router_jitter_noise, attention_bias, lm_head_bias)
sparsemixer(scores, jitter_eps)
phimoe_routing_function(hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool)
  PhiMoE.__init__(num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
  PhiMoE.forward(hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch]) -> torch.Tensor
  PhiMoEAttention.__init__(hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: Optional[int], max_position: int, rope_theta: float, layer_id: int, attention_bias: bool, quant_config: Optional[QuantizationConfig], rope_scaling: Optional[dict], prefix: str) -> None
  PhiMoEAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  PhiMoEDecoderLayer.__init__(config: PhiMoEConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  PhiMoEDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, residual: Optional[torch.Tensor], forward_batch: ForwardBatch) -> Tuple[torch.Tensor, torch.Tensor]
  PhiMoEModel.__init__(config: PhiMoEConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  PhiMoEModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor]) -> Union[torch.Tensor]
  PhiMoEForCausalLM.__init__(config: PhiMoEConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  PhiMoEForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor], get_embedding: bool) -> LogitsProcessorOutput
  PhiMoEForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/pixtral.py
  PixtralHFMLP.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig]) -> None
  PixtralHFMLP.forward(x: torch.Tensor) -> torch.Tensor
  PixtralHFTransformerBlock.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig]) -> None
  PixtralHFTransformerBlock.forward(hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor], position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]]) -> torch.Tensor
  PixtralHFTransformer.__init__(config: PixtralVisionConfig, quant_config: Optional[QuantizationConfig]) -> None
  PixtralHFTransformer.forward(x: torch.Tensor, attention_mask: Optional[torch.Tensor], position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]], return_all_hidden_states: bool) -> Union[torch.Tensor, List[torch.Tensor]]
resolve_visual_encoder_outputs(outputs: Union[torch.Tensor, List[torch.Tensor]], feature_sample_layers: Optional[List[int]], post_norm: Optional[nn.Module], num_hidden_layers: int) -> torch.Tensor
  PixtralHFVisionModel.pad_input_ids(input_ids: List[int], mm_inputs: MultimodalInputs)
  PixtralHFVisionModel.__init__(config: PixtralVisionConfig, quant_config: Optional[QuantizationConfig]) -> None
  PixtralHFVisionModel.dtype()
  PixtralHFVisionModel.device()
  PixtralHFVisionModel.forward(pixel_values: torch.Tensor, image_sizes: list[tuple[int, int]], output_hidden_states: bool, feature_sample_layers: Optional[list[int]]) -> Union[torch.Tensor, tuple]
  PixtralHFVisionModel.load_weights(weights: Iterable[Tuple[str, torch.Tensor]]) -> Set[str]

# python/sglang/srt/models/qwen.py
  QWenMLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
  QWenMLP.forward(x)
  QWenAttention.__init__(hidden_size: int, num_heads: int, max_position_embeddings: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], quant_config: Optional[QuantizationConfig], prefix: str)
  QWenAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  QWenBlock.__init__(config: PretrainedConfig, layer_id, quant_config: Optional[QuantizationConfig], prefix: str)
  QWenBlock.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  QWenModel.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  QWenModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  QWenLMHeadModel.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  QWenLMHeadModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
  QWenLMHeadModel.forward_split_prefill(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int])
  QWenLMHeadModel.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/qwen2.py
  Qwen2MLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen2MLP.forward(x)
  Qwen2Attention.__init__(hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: Optional[int], layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], dual_chunk_attention_config: Optional[dict[str, Any]], prefix: str) -> None
  Qwen2Attention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Qwen2DecoderLayer.__init__(config: Qwen2Config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream]) -> None
  Qwen2DecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  Qwen2Model.__init__(config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str, decoder_layer_type: type[nn.Module], alt_stream: Optional[torch.cuda.Stream]) -> None
  Qwen2Model.get_input_embedding(input_ids: torch.Tensor) -> torch.Tensor
  Qwen2Model.get_input_embeddings() -> nn.Embedding
  Qwen2Model.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors]) -> Union[torch.Tensor, PPProxyTensors]
  Qwen2Model.load_kv_cache_scales(quantization_param_path: str) -> None
  Qwen2ForCausalLM.__init__(config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen2ForCausalLM.get_input_embedding(input_ids: torch.Tensor) -> torch.Tensor
  Qwen2ForCausalLM.get_input_embeddings() -> nn.Embedding
  Qwen2ForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors]) -> torch.Tensor
  Qwen2ForCausalLM.forward_split_prefill(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)
  Qwen2ForCausalLM.start_layer()
  Qwen2ForCausalLM.end_layer()
  Qwen2ForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  Qwen2ForCausalLM.get_embed_and_head()
  Qwen2ForCausalLM.set_embed_and_head(embed, head)
  Qwen2ForCausalLM.load_kv_cache_scales(quantization_param_path: str) -> None
  Qwen2ForCausalLM.set_eagle3_layers_to_capture(layer_ids: Optional[List[int]])

# python/sglang/srt/models/qwen2_5_vl.py
  Qwen2_5_VLMLP.__init__(in_features: int, hidden_features: int, bias: bool, hidden_act, quant_config: Optional[QuantizationConfig], prefix: str)
  Qwen2_5_VLMLP.forward(x: torch.Tensor) -> torch.Tensor
  Qwen2_5_VisionBlock.__init__(dim: int, intermediate_dim: int, num_heads: int, hidden_act, norm_layer: Type[nn.Module], attn_implementation: Optional[str], quant_config: Optional[QuantizationConfig], prefix: str, num_dummy_heads: int) -> None
  Qwen2_5_VisionBlock.forward(x: torch.Tensor, cu_seqlens: torch.Tensor, position_embeddings: torch.Tensor) -> torch.Tensor
  Qwen2_5_VisionPatchMerger.__init__(dim: int, context_dim: int, spatial_merge_size: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen2_5_VisionPatchMerger.forward(x: torch.Tensor) -> torch.Tensor
  Qwen2_5_VisionTransformer.__init__(vision_config: Qwen2_5_VLVisionConfig, norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen2_5_VisionTransformer.get_window_index(grid_thw)
  Qwen2_5_VisionTransformer.dtype() -> torch.dtype
  Qwen2_5_VisionTransformer.device() -> torch.device
  Qwen2_5_VisionTransformer.rot_pos_emb(grid_thw: torch.Tensor) -> torch.Tensor
  Qwen2_5_VisionTransformer.forward(x: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor
  Qwen2_5_VLForConditionalGeneration.__init__(config: Qwen2_5_VLConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen2_5_VLForConditionalGeneration.pad_input_ids(input_ids: List[int], mm_inputs: MultimodalInputs)
  Qwen2_5_VLForConditionalGeneration.get_image_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  Qwen2_5_VLForConditionalGeneration.get_video_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  Qwen2_5_VLForConditionalGeneration.get_input_embeddings()
  Qwen2_5_VLForConditionalGeneration.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)
  Qwen2_5_VLForConditionalGeneration.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/qwen2_audio.py
  Qwen2AudioForConditionalGeneration.__init__(config: Qwen2AudioConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen2AudioForConditionalGeneration.pad_input_ids(input_ids: List[int], mm_inputs: MultimodalInputs)
  Qwen2AudioForConditionalGeneration.get_audio_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  Qwen2AudioForConditionalGeneration.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Qwen2AudioForConditionalGeneration.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/qwen2_eagle.py
  Qwen2DecoderLayer.__init__(config: Qwen2Config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen2Model.__init__(config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen2Model.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors]) -> torch.Tensor
  Qwen2ForCausalLMEagle.__init__(config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen2ForCausalLMEagle.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/qwen2_moe.py
  Qwen2MoeMLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str) -> None
  Qwen2MoeMLP.forward(x, use_reduce_scatter: bool)
  Qwen2MoeSparseMoeBlock.__init__(layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Qwen2MoeSparseMoeBlock.forward(hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch], use_reduce_scatter: bool) -> torch.Tensor
  Qwen2MoeAttention.__init__(hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, qkv_bias: int, quant_config: Optional[QuantizationConfig], dual_chunk_attention_config: Optional[dict[str, Any]], prefix: str) -> None
  Qwen2MoeAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Qwen2MoeDecoderLayer.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream]) -> None
  Qwen2MoeDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  Qwen2MoeModel.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str, decoder_layer_type: type[nn.Module], alt_stream: Optional[torch.cuda.Stream]) -> None
  Qwen2MoeModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors]) -> Union[torch.Tensor, PPProxyTensors]
  Qwen2MoeForCausalLM.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen2MoeForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors]) -> torch.Tensor
  Qwen2MoeForCausalLM.forward_split_prefill(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)
  Qwen2MoeForCausalLM.start_layer()
  Qwen2MoeForCausalLM.end_layer()
  Qwen2MoeForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  Qwen2MoeForCausalLM.get_model_config_for_expert_location(cls, config)
  Qwen2MoeForCausalLM.set_eagle3_layers_to_capture(layer_ids: Optional[List[int]])

# python/sglang/srt/models/qwen2_rm.py
  Qwen2ForRewardModel.__init__(config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen2ForRewardModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool) -> EmbeddingPoolerOutput
  Qwen2ForRewardModel.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/qwen2_vl.py
  Qwen2VisionMLP.__init__(in_features: int, hidden_features: int, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)
  Qwen2VisionMLP.forward(x: torch.Tensor) -> torch.Tensor
  Qwen2VisionBlock.__init__(dim: int, num_heads: int, mlp_ratio: float, act_layer: Type[nn.Module], norm_layer: Type[nn.Module], attn_implementation: Optional[str], quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen2VisionBlock.forward(x: torch.Tensor, cu_seqlens: torch.Tensor, position_embeddings: torch.Tensor) -> torch.Tensor
  Qwen2VisionPatchEmbed.__init__(patch_size: int, temporal_patch_size: int, in_chans: int, embed_dim: int) -> None
  Qwen2VisionPatchEmbed.forward(x: torch.Tensor) -> torch.Tensor
  Qwen2VisionPatchMerger.__init__(d_model: int, context_dim: int, norm_layer: Type[nn.Module], spatial_merge_size: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen2VisionPatchMerger.forward(x: torch.Tensor) -> torch.Tensor
  Qwen2VisionRotaryEmbedding.__init__(dim: int, theta: float) -> None
  Qwen2VisionRotaryEmbedding.update_freqs_cache(seqlen: int) -> None
  Qwen2VisionRotaryEmbedding.forward(seqlen: int) -> torch.Tensor
  Qwen2VisionTransformer.__init__(vision_config: Qwen2VLVisionConfig, norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen2VisionTransformer.dtype() -> torch.dtype
  Qwen2VisionTransformer.device() -> torch.device
  Qwen2VisionTransformer.rot_pos_emb(grid_thw: torch.Tensor) -> torch.Tensor
  Qwen2VisionTransformer.forward(x: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor
  Qwen2VLForConditionalGeneration.__init__(config: Qwen2VLConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen2VLForConditionalGeneration.pad_input_ids(input_ids: List[int], mm_inputs: MultimodalInputs)
  Qwen2VLForConditionalGeneration.get_image_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  Qwen2VLForConditionalGeneration.get_video_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  Qwen2VLForConditionalGeneration.get_input_embeddings()
  Qwen2VLForConditionalGeneration.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)
  Qwen2VLForConditionalGeneration.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/qwen3.py
  Qwen3Attention.__init__(hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], head_dim: Optional[int], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], rms_norm_eps: float, attention_bias: bool, prefix: str, alt_stream: Optional[torch.cuda.Stream]) -> None
  Qwen3Attention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Qwen3DecoderLayer.__init__(config: Qwen3Config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream]) -> None
  Qwen3DecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  Qwen3Model.__init__(config: Qwen3Config, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen3ForCausalLM.__init__(config: Qwen3Config, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen3ForCausalLM.get_input_embeddings() -> nn.Embedding
  Qwen3ForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors]) -> torch.Tensor
  Qwen3ForCausalLM.forward_split_prefill(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)
  Qwen3ForCausalLM.start_layer()
  Qwen3ForCausalLM.end_layer()
  Qwen3ForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  Qwen3ForCausalLM.get_embed_and_head()
  Qwen3ForCausalLM.set_embed_and_head(embed, head)
  Qwen3ForCausalLM.load_kv_cache_scales(quantization_param_path: str) -> None
  Qwen3ForCausalLM.set_eagle3_layers_to_capture(layer_ids: Optional[List[int]])

# python/sglang/srt/models/qwen3_classification.py
  Qwen3ForSequenceClassification.__init__(config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen3ForSequenceClassification.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor], get_embedding: bool) -> EmbeddingPoolerOutput
  Qwen3ForSequenceClassification.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/qwen3_moe.py
  Qwen3MoeSparseMoeBlock.__init__(layer_id: int, config: Qwen3MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Qwen3MoeSparseMoeBlock.forward(hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch], use_reduce_scatter: bool) -> torch.Tensor
  Qwen3MoeSparseMoeBlock.get_moe_weights()
  Qwen3MoeSparseMoeBlock.forward_normal(hidden_states: torch.Tensor, use_reduce_scatter: bool) -> torch.Tensor
  Qwen3MoeSparseMoeBlock.forward_deepep(hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Qwen3MoeSparseMoeBlock.op_gate(state)
  Qwen3MoeSparseMoeBlock.op_select_experts(state)
  Qwen3MoeSparseMoeBlock.op_dispatch_a(state)
  Qwen3MoeSparseMoeBlock.op_dispatch_b(state)
  Qwen3MoeSparseMoeBlock.op_experts(state)
  Qwen3MoeSparseMoeBlock.op_combine_a(state)
  Qwen3MoeSparseMoeBlock.op_combine_b(state)
  Qwen3MoeSparseMoeBlock.op_output(state)
  Qwen3MoeAttention.__init__(hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, head_dim: Optional[int], rms_norm_eps: float, attention_bias: bool, quant_config: Optional[QuantizationConfig], prefix: str, dual_chunk_attention_config: Optional[dict[str, Any]], alt_stream: Optional[torch.cuda.Stream]) -> None
  Qwen3MoeAttention.op_prepare(state)
  Qwen3MoeAttention.op_core(state)
  Qwen3MoeAttention.forward_prepare(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
  Qwen3MoeAttention.forward_core(intermediate_state)
  Qwen3MoeAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Qwen3MoeDecoderLayer.__init__(config: Qwen3MoeConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream]) -> None
  Qwen3MoeDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  Qwen3MoeDecoderLayer.op_comm_prepare_attn(state, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], tbo_subbatch_index: Optional[int])
  Qwen3MoeDecoderLayer.op_comm_prepare_mlp(state)
  Qwen3MoeDecoderLayer.op_mlp(state)
  Qwen3MoeDecoderLayer.op_comm_postprocess_layer(state)
  Qwen3MoeModel.__init__(config: Qwen3MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen3MoeForCausalLM.__init__(config: Qwen3MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Qwen3MoeForCausalLM.get_input_embeddings() -> nn.Embedding
  Qwen3MoeForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors]) -> torch.Tensor
  Qwen3MoeForCausalLM.forward_split_prefill(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)
  Qwen3MoeForCausalLM.start_layer()
  Qwen3MoeForCausalLM.end_layer()
  Qwen3MoeForCausalLM.get_embed_and_head()
  Qwen3MoeForCausalLM.set_eagle3_layers_to_capture(layer_ids: Optional[List[int]])
  Qwen3MoeForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  Qwen3MoeForCausalLM.get_model_config_for_expert_location(cls, config)

# python/sglang/srt/models/registry.py
  _ModelRegistry.get_supported_archs() -> AbstractSet[str]
  _ModelRegistry.resolve_model_cls(architectures: Union[str, List[str]]) -> Tuple[Type[nn.Module], str]
import_model_classes()

# python/sglang/srt/models/roberta.py
  RobertaClassificationHead.__init__(config: RobertaConfig)
  RobertaClassificationHead.forward(features)
  RobertaEmbedding.__init__(config: RobertaConfig)
  RobertaEmbedding.forward(input_ids: torch.Tensor, seq_lens: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  XLMRobertaBaseModel.__init__()
  XLMRobertaBaseModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool) -> torch.Tensor
  XLMRobertaBaseModel.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length)
  XLMRobertaModel.__init__()
  XLMRobertaModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool) -> torch.Tensor
  XLMRobertaModel.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  XLMRobertaForSequenceClassification.__init__()
  XLMRobertaForSequenceClassification.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool) -> torch.Tensor
  XLMRobertaForSequenceClassification.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/siglip.py
  SiglipVisionEmbeddings.__init__(config: SiglipVisionConfig)
  SiglipVisionEmbeddings.forward(pixel_values: torch.Tensor) -> torch.Tensor
  SiglipMLP.__init__(config, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)
  SiglipMLP.forward(x: torch.Tensor) -> torch.Tensor
  SiglipEncoderLayer.__init__(config: SiglipVisionConfig, act_layer: Type[nn.Module], norm_layer: Type[nn.Module], attn_implementation: Optional[str], quant_config: Optional[QuantizationConfig], prefix: str) -> None
  SiglipEncoderLayer.forward(hidden_states: torch.Tensor, attention_mask: torch.Tensor, causal_attention_mask: torch.Tensor) -> torch.Tensor
  SiglipEncoder.__init__(config: SiglipVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  SiglipEncoder.forward(inputs_embeds: torch.Tensor, attention_mask: torch.Tensor, causal_attention_mask: torch.Tensor, return_all_hidden_states: bool) -> Union[torch.Tensor, list[torch.Tensor]]
  SiglipVisionTransformer.__init__(config: SiglipVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  SiglipVisionTransformer.device() -> torch.device
  SiglipVisionTransformer.forward(pixel_values: torch.Tensor) -> torch.Tensor
  SiglipVisionModel.__init__(config: SiglipVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  SiglipVisionModel.device() -> torch.device
  SiglipVisionModel.forward(pixel_values: torch.Tensor)

# python/sglang/srt/models/stablelm.py
  StablelmMLP.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  StablelmMLP.forward(x: torch.Tensor) -> torch.Tensor
  StablelmAttention.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  StablelmAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  StablelmDecoderLayer.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  StablelmDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> Tuple[torch.Tensor, torch.Tensor]
  StableLMEpochModel.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  StableLMEpochModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  StableLmForCausalLM.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  StableLmForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  StableLmForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/step3_vl.py
  Step3TextMLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Step3TextMLP.forward(x)
  Step3TextMoEMLP.__init__(layer_id: int, config: Step3TextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  Step3TextMoEMLP.forward(hidden_states: torch.Tensor) -> torch.Tensor
  Step3TextAttention.__init__(hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: int, share_q_dim: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], rms_norm_eps, prefix: str) -> None
  Step3TextAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  Step3TextDecoderLayer.__init__(config: Step3TextConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Step3TextDecoderLayer.moe_mlp_forward(hidden_states)
  Step3TextDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  Step3TextModel.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Step3TextModel.get_input_embeddings()
  Step3TextModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
get_abs_pos(abs_pos, tgt_size)
  Step3VisionMLP.__init__(dim: int, intermediate_size: int, bias: bool, hidden_act, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Step3VisionMLP.forward(hidden_states) -> torch.Tensor
  Step3VisionAttention.__init__(dim: int, num_heads: int, qkv_backend, quant_config, prefix: str) -> None
  Step3VisionAttention.forward(hidden_states: torch.Tensor) -> torch.Tensor
  Step3VisionEmbeddings.__init__(config: Step3VisionEncoderConfig)
  Step3VisionEmbeddings.forward(pixel_values: torch.Tensor) -> torch.Tensor
  Step3VisionEncoderLayer.__init__(config, attn_implementation: str) -> None
  Step3VisionEncoderLayer.forward(hidden_states) -> torch.Tensor
  Step3VisionTransformer.__init__(config: Step3VisionEncoderConfig)
  Step3VisionTransformer.dtype() -> torch.dtype
  Step3VisionTransformer.forward(pixel_values: torch.Tensor)
  Step3VisionEncoder.__init__(config: Step3VisionEncoderConfig)
  Step3VisionEncoder.forward(inputs_embeds) -> torch.Tensor
  Step3VLForConditionalGeneration.__init__(config: Step3VLConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  Step3VLForConditionalGeneration.get_image_feature(items: List[MultimodalDataItem]) -> torch.Tensor
  Step3VLForConditionalGeneration.pad_input_ids(input_ids: List[int], mm_inputs: MultimodalInputs)
  Step3VLForConditionalGeneration.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  Step3VLForConditionalGeneration.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  Step3VLForConditionalGeneration.get_model_config_for_expert_location(cls, config: Step3VLConfig)

# python/sglang/srt/models/torch_native_llama.py
gate_up_proj_weight_loader(param: Parameter, loaded_weight: torch.Tensor, loaded_shard_id: int)
  LlamaMLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlamaMLP.forward(x)
qkv_proj_weight_loader(param: Parameter, loaded_weight: torch.Tensor, loaded_shard_id: str)
  LlamaAttention.__init__(config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlamaAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  LlamaDecoderLayer.__init__(config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  LlamaDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  LlamaModel.__init__(config: LlamaConfig, quant_config: Optional[QuantizationConfig]) -> None
  LlamaModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  TorchNativeLlamaForCausalLM.__init__(config: LlamaConfig, quant_config: Optional[QuantizationConfig]) -> None
  TorchNativeLlamaForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> LogitsProcessorOutput
  TorchNativeLlamaForCausalLM.get_module_name_from_weight_name(name)
  TorchNativeLlamaForCausalLM.get_num_params()
  TorchNativeLlamaForCausalLM.load_weights_to_module(fqn: str, weights: Iterable[Tuple[str, torch.Tensor]])
  TorchNativeLlamaForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/transformers.py
maybe_prefix(prefix: str, name: str) -> str
sglang_flash_attention_forward(module: torch.nn.Module, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attention_mask: torch.Tensor, forward_batch: ForwardBatch, scaling: float, attention_instances: list[RadixAttention])
  HFColumnParallelLinear.forward(input: torch.Tensor) -> torch.Tensor
  HFRowParallelLinear.forward(input: torch.Tensor) -> torch.Tensor
replace_linear_class(linear: nn.Linear, style: Literal['colwise', 'rowwise'], quant_config: QuantizationConfig) -> Union[ColumnParallelLinear, RowParallelLinear]
  TransformersForCausalLM.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  TransformersForCausalLM.log_replacement(name: str, old_module: nn.Module, new_module: nn.Module)
  TransformersForCausalLM.tensor_parallel(tp_size: int)
  TransformersForCausalLM.replace_vocab_embed_class(module: nn.Module)
  TransformersForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool) -> LogitsProcessorOutput
  TransformersForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/vila.py
  VILAConfig.__init__(text_config: Optional[Dict[str, Any]], vision_config: Optional[Dict[str, Any]])
  DownSample3x3BlockFix.forward(x: Tensor) -> Tensor
  MultimodalProjector.__init__(config: VILAConfig)
  MultimodalProjector.device() -> torch.device
  MultimodalProjector.dtype() -> torch.dtype
  MultimodalProjector.forward(x: Tensor) -> Tensor
  VILAForConditionalGeneration.__init__(config: VILAConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  VILAForConditionalGeneration.dtype() -> torch.dtype
  VILAForConditionalGeneration.forward(input_ids: Tensor, positions: Tensor, forward_batch: ForwardBatch, get_embedding: bool) -> LogitsProcessorOutput
  VILAForConditionalGeneration.get_image_feature(mm_input: List[MultimodalDataItem]) -> Tensor
  VILAForConditionalGeneration.load_weights(weights: Iterable[Tuple[str, Tensor]]) -> None
  VILAForConditionalGeneration.pad_input_ids(input_ids: List[int], mm_inputs: MultimodalInputs) -> List[int]

# python/sglang/srt/models/xverse.py
  XverseMLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  XverseMLP.forward(x)
  XverseAttention.__init__(config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  XverseAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  XverseDecoderLayer.__init__(config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  XverseDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  XverseModel.__init__(config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  XverseModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  XverseForCausalLM.__init__(config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  XverseForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor) -> torch.Tensor
  XverseForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]], name, loaded_weight)

# python/sglang/srt/models/xverse_moe.py
  XverseMLP.__init__(hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str) -> None
  XverseMLP.forward(x)
  XverseMoE.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
  XverseMoE.pack_params()
  XverseMoE.forward(hidden_states: torch.Tensor) -> torch.Tensor
  XverseAttention.__init__(hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  XverseAttention.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  XverseDecoderLayer.__init__(config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  XverseDecoderLayer.forward(positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor]) -> torch.Tensor
  XverseModel.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  XverseModel.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  XverseMoeForCausalLM.__init__(config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  XverseMoeForCausalLM.forward(input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch) -> torch.Tensor
  XverseMoeForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])

# python/sglang/srt/models/yivl.py
  YiVLForCausalLM.__init__(config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str) -> None
  YiVLForCausalLM.load_weights(weights: Iterable[Tuple[str, torch.Tensor]])
  YiVLMultiModalProjector.__init__(config: LlavaConfig)
  YiVLMultiModalProjector.forward(image_features)

# python/sglang/srt/multimodal/mm_utils.py
has_valid_data(data) -> bool
select_best_resolution(original_size, possible_resolutions)
resize_and_pad_image(image, target_resolution)
divide_to_patches(image, patch_size)
get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size)
process_anyres_image(image, processor, grid_pinpoints)
load_image_from_base64(image)
expand2square(pil_img, background_color)
unpad_image(tensor, original_size)
unpad_image_shape(current_height, current_width, original_size)
process_images(images, image_processor, model_cfg)

# python/sglang/srt/multimodal/processors/base_processor.py
  BaseMultiModalProcessorOutput.organize_results() -> List[Tuple[Modality, Any]]
  MultimodalSpecialTokens.build(processor)
  MultimodalSpecialTokens.convert_to_str(token: Union[str, int], processor) -> str
  MultimodalSpecialTokens.convert_to_strs(processor)
  MultimodalSpecialTokens.get_modality_of_token(token: str) -> Optional[Modality]
  MultimodalSpecialTokens.get_token_id_by_modality(modality: Modality) -> Optional[int]
  MultimodalSpecialTokens.parse_regex()
  MultimodalSpecialTokens.get_combined_regex() -> re.Pattern
  BaseMultimodalProcessor.__init__(hf_config, server_args, _processor, transport_mode)
  BaseMultimodalProcessor.process_mm_data(input_text, images, videos, audios) -> dict
  BaseMultimodalProcessor.process_mm_data_async(image_data, audio_data, input_text, request_obj) -> Optional[Dict[str, Any]]
  BaseMultimodalProcessor.get_estimated_frames_list(image_data)
  BaseMultimodalProcessor.submit_data_loading_tasks(text_parts: List[str], multimodal_tokens: MultimodalSpecialTokens, data_iterators: dict[Modality, Iterator[Any]], discard_alpha_channel: bool, image_estimated_frames_iter: Optional[iter], image_scaling_factor: float, max_image_frames: int, audio_sample_rate: Optional[int]) -> Tuple[List, List]
  BaseMultimodalProcessor.load_mm_data(prompt: str, multimodal_tokens: MultimodalSpecialTokens, image_data: Optional[list], video_data: Optional[list], audio_data: Optional[list], return_text: Optional[bool], discard_alpha_channel: bool, audio_sample_rate: Optional[int]) -> BaseMultiModalProcessorOutput
  BaseMultimodalProcessor.get_mm_items_offset(input_ids: torch.Tensor, mm_token_id: int) -> List[Tuple[int, int]]
  BaseMultimodalProcessor.get_mm_items_offset_by_pair(input_ids: torch.Tensor, mm_start_id: int, mm_end_id: int) -> List[Tuple[int, int]]
  BaseMultimodalProcessor.collect_mm_items_from_processor_output(data_dict: dict) -> List[MultimodalDataItem]
  BaseMultimodalProcessor.process_and_combine_mm_data(base_output: BaseMultiModalProcessorOutput, mm_tokens: MultimodalSpecialTokens) -> Tuple[List[MultimodalDataItem], torch.Tensor, dict]

# python/sglang/srt/multimodal/processors/clip.py
  ClipImageProcessor.__init__(hf_config, server_args, _processor)
  ClipImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], input_text)

# python/sglang/srt/multimodal/processors/deepseek_vl_v2.py
  DeepseekVL2ImageProcessor.__init__(hf_config, server_args, _processor)
  DeepseekVL2ImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], input_text, request_obj, max_req_input_len)

# python/sglang/srt/multimodal/processors/gemma3.py
  Gemma3SGLangImageProcessor.__init__(hf_config, server_args, _processor)
  Gemma3SGLangImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes, Dict]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/gemma3n.py
  Gemma3nSGLangProcessor.__init__(hf_config, server_args, _processor)
  Gemma3nSGLangProcessor.process_mm_data_async(image_data: Optional[List[Union[str, bytes, Dict]]], audio_data: Optional[List[Union[str, bytes, Dict]]], input_text: str, request_obj)

# python/sglang/srt/multimodal/processors/glm4v.py
  Glm4vImageProcessor.__init__(hf_config, server_args, _processor)
  Glm4vImageProcessor.preprocess_video(vr: VideoReader)
  Glm4vImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/internvl.py
  InternVLImageProcessor.__init__(hf_config, server_args, _image_processor)
  InternVLImageProcessor.build_transform(input_size)
  InternVLImageProcessor.dynamic_preprocess(image, min_num, max_num, image_size, use_thumbnail)
  InternVLImageProcessor.get_index(bound, fps, max_frame, first_idx, num_segments)
  InternVLImageProcessor.load_video(video_path, bound, input_size, max_num, num_segments)
  InternVLImageProcessor.process_mm_data_async(image_data, input_text, request_obj)

# python/sglang/srt/multimodal/processors/janus_pro.py
  JanusProImageProcessor.__init__(hf_config, server_args, _processor)
  JanusProImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/kimi_vl.py
  KimiVLImageProcessor.__init__(hf_config, server_args, _processor)
  KimiVLImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes, Dict]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/llava.py
  LlavaImageProcessor.__init__(hf_config, server_args, _processor)
  LlavaImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes, ImageData]], input_text, request_obj)
  LlavaMultimodalProcessor.__init__(hf_config, server_args, _processor)
  LlavaMultimodalProcessor.process_mm_data_async()

# python/sglang/srt/multimodal/processors/minicpm.py
  MiniCPMMultimodalProcessor.__init__(hf_config, server_args, _processor)
  MiniCPMMultimodalProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], audio_data: List[Union[str, bytes]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/mlama.py
  MllamaImageProcessor.__init__(hf_config, server_args, _processor)
  MllamaImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], input_text)

# python/sglang/srt/multimodal/processors/mllama4.py
  Mllama4ImageProcessor.__init__(hf_config, server_args, _processor)
  Mllama4ImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], input_text)

# python/sglang/srt/multimodal/processors/phi4mm.py
  Phi4MMProcessorAdapter.__init__(_processor) -> None
  Phi4MMProcessorAdapter.__call__()
  Phi4MMMultimodalProcessor.__init__(hf_config, server_args, _processor)
  Phi4MMMultimodalProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], audio_data, input_text, request_obj)

# python/sglang/srt/multimodal/processors/pixtral.py
  PixtralProcessor.get_patch_grid_size() -> tuple[int, int]
  PixtralProcessor.__init__(hf_config, server_args, _processor)
  PixtralProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/qwen_audio.py
  Qwen2AudioMultimodalProcessor.__init__(hf_config, server_args, _processor)
  Qwen2AudioMultimodalProcessor.process_mm_data_async(audio_data, input_text)

# python/sglang/srt/multimodal/processors/qwen_vl.py
smart_resize(height: int, width: int, factor: int, min_pixels: int, max_pixels: int) -> tuple[int, int]
resize_image(image, size_factor: int) -> Image.Image
round_by_factor(number: int, factor: int) -> int
ceil_by_factor(number: int, factor: int) -> int
floor_by_factor(number: int, factor: int) -> int
resize_image_async(image)
smart_nframes(ele: dict, total_frames: int, video_fps: int | float) -> int
preprocess_video(vr, image_factor: int) -> torch.Tensor
  Qwen2_5VLImageProcessor.__init__(hf_config, server_args, _processor)
  Qwen2_5VLImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/step3_vl.py
  GPUToTensor.forward(raw_image: Union[np.ndarray, Image.Image]) -> torch.Tensor
  Step3VisionProcessor.__init__(size, interpolation_mode, patch_size)
  Step3VisionProcessor.__call__(image, is_patch)
  ImagePatcher.determine_window_size(long: int, short: int) -> int
  ImagePatcher.slide_window(width: int, height: int, sizes: list[tuple[int, int]], steps: list[tuple[int, int]], img_rate_thr: float) -> tuple[list[tuple[int, int, int, int]], tuple[int, int]]
  ImagePatcher.square_pad(img: Image.Image) -> Image.Image
  ImagePatcher.get_image_size_for_padding(img_width: int, img_height: int) -> tuple[int, int]
  ImagePatcher.get_image_size_for_preprocess(img_width: int, img_height: int) -> tuple[int, int]
  ImagePatcher.get_image_size_for_crop(img_width: int, img_height: int, window_size: int)
  ImagePatcher.patch_crop(img: Image.Image, i: int, j: int, th: int, tw: int)
  ImagePatcher.get_num_patches(img_width: int, img_height: int) -> tuple[int, int]
  ImagePatcher.__call__(img: Image.Image) -> tuple[Image.Image, list[Image.Image], list[bool] | None]
  Step3VLProcessor.__init__(config, tokenizer) -> None
  Step3VLProcessor.image_token_id() -> int
  Step3VLProcessor.get_num_image_tokens(img_width: int, img_height: int) -> int
  Step3VLProcessor.replace_placeholder(text: str, placeholder: str, repls: list[str]) -> str
  Step3VLProcessor.__call__(text: Optional[Union[str, list[str]]], images: Optional[Union[Image.Image, list[Image.Image]]], return_tensors: Optional[Union[str, TensorType]]) -> BatchFeature
  Step3VLImageProcessor.__init__(hf_config, server_args, _processor)
  Step3VLImageProcessor.preprocess(image)
  Step3VLImageProcessor.__call__(image)
  Step3VLImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], input_text: str | List[int], request_obj)

# python/sglang/srt/multimodal/processors/vila.py
  VILAMultimodalProcessor.__init__(hf_config: PretrainedConfig, server_args: ServerArgs, _processor: VILAProcessor) -> None
  VILAMultimodalProcessor.process_mm_data_async(image_data: Optional[ImageDataInputItem | List[ImageDataInputItem]], input_text: str | List[int], request_obj: GenerateReqInput | EmbeddingReqInput) -> Optional[Dict[str, Any]]

# python/sglang/srt/offloader.py
  BaseOffloader.wrap_modules(all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor], whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator])
  BaseOffloader.post_init()
get_offloader()
set_offloader(instance: BaseOffloader)
create_offloader_from_server_args(server_args: ServerArgs, dp_rank: int)
  OffloaderV1.__init__(cpu_offload_max_bytes: int)
  OffloaderV1.wrap_modules(all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor], whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator])
  OffloaderV1.maybe_offload_to_cpu(module: torch.nn.Module) -> torch.nn.Module
  OffloaderV2.__init__(group_size: int, num_in_group: int, prefetch_step: int, mode: str, dp_rank: int, dp_size: int)
  OffloaderV2.wrap_modules(all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor], whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator])
  OffloaderV2.post_init()
  _ModuleOffloader.__init__(mode: str, module: torch.nn.Module, alt_stream: torch.cuda.Stream, whitelist_param_names: List[str])
  _ModuleOffloader.post_init()
  _ModuleOffloader.start_onload()
  _ModuleOffloader.offload()
  _ModuleOffloader.wait_and_get_device_tensors()
  _BaseParamOffloader.create(mode: str) -> '_BaseParamOffloader'
  _BaseParamOffloader.__init__(module, param_name)
  _BaseParamOffloader.post_init()
  _BaseParamOffloader.create_device_tensor()
  _MetaParamOffloader.__init__(module, param_name)
  _MetaParamOffloader.create_device_tensor()
  _CpuParamOffloader.__init__(module, param_name)
  _CpuParamOffloader.create_device_tensor()
  _ShmCpuParamOffloader.__init__(module, param_name)
  _ShmCpuParamOffloader.post_init()
  _ShmCpuParamOffloader.create_device_tensor()
  _ShardedGpuParamOffloader.__init__(module, param_name)
  _ShardedGpuParamOffloader.post_init()
  _ShardedGpuParamOffloader.create_device_tensor()

# python/sglang/srt/operations.py
execute_operations(inputs, operations)
execute_overlapped_operations(inputs_arr: Sequence, operations_arr: Sequence, delta_stages: Sequence[int]) -> Sequence
  _StageExecutor.__init__(debug_name: str, stages: List[Stage], inputs: dict)
  _StageExecutor.next()
  _StageExecutor.output()
  _StageExecutor.done()
  _StageExecutor.num_stages()
  _StateDict.__init__()
  _StateDict.__setattr__(key, value)
  _StateDict.__getattr__(item)
  _StateDict.__delattr__(item)
  _StateDict.pop(item)
  _StateDict.update(values: Dict[str, Any])
  _StateDict.get(item)
  _StateDict.clear(expect_keys: Sequence[str])

# python/sglang/srt/operations_strategy.py
  OperationsStrategy.concat(cls, items: List['OperationsStrategy']) -> 'OperationsStrategy'
  OperationsStrategy.init_new_tbo(layers: torch.nn.ModuleList, forward_mode: ForwardMode) -> 'OperationsStrategy'

# python/sglang/srt/patch_torch.py
monkey_patch_torch_reductions()
monkey_patch_torch_compile()

# python/sglang/srt/poll_based_barrier.py
  PollBasedBarrier.__init__(noop: bool)
  PollBasedBarrier.local_arrive()
  PollBasedBarrier.poll_global_arrived() -> bool

# python/sglang/srt/reasoning_parser.py
  StreamingParseResult.__init__(normal_text: Optional[str], reasoning_text: Optional[str])
  BaseReasoningFormatDetector.__init__(think_start_token: str, think_end_token: str, force_reasoning: bool, stream_reasoning: bool)
  BaseReasoningFormatDetector.detect_and_parse(text: str) -> StreamingParseResult
  BaseReasoningFormatDetector.parse_streaming_increment(new_text: str) -> StreamingParseResult
  DeepSeekR1Detector.__init__(stream_reasoning: bool, force_reasoning: bool)
  Qwen3Detector.__init__(stream_reasoning: bool, force_reasoning: bool)
  KimiDetector.__init__(stream_reasoning: bool, force_reasoning: bool)
  GptOssDetector.__init__(stream_reasoning: bool, force_reasoning: bool)
  GptOssDetector.detect_and_parse(text: str) -> StreamingParseResult
  GptOssDetector.parse_streaming_increment(new_text: str) -> StreamingParseResult
  ReasoningParser.__init__(model_type: Optional[str], stream_reasoning: bool, force_reasoning: Optional[bool])
  ReasoningParser.parse_non_stream(full_text: str) -> Tuple[Optional[str], Optional[str]]
  ReasoningParser.parse_stream_chunk(chunk_text: str) -> Tuple[Optional[str], Optional[str]]

# python/sglang/srt/sampling/custom_logit_processor.py
  CustomLogitProcessor.__call__(logits: torch.Tensor, custom_param_list: Optional[List[Dict[str, Any]]]) -> torch.Tensor
  CustomLogitProcessor.to_str(cls) -> str
  CustomLogitProcessor.from_str(cls, json_str: str)
  DisallowedTokensLogitsProcessor.__call__(logits: torch.Tensor, custom_param_list: Optional[List[Dict[str, Any]]]) -> torch.Tensor

# python/sglang/srt/sampling/penaltylib/frequency_penalty.py
  BatchedFrequencyPenalizer.__init__(orchestrator: BatchedPenalizerOrchestrator)

# python/sglang/srt/sampling/penaltylib/min_new_tokens.py
  BatchedMinNewTokensPenalizer.__init__(orchestrator: BatchedPenalizerOrchestrator)

# python/sglang/srt/sampling/penaltylib/orchestrator.py
  BatchedPenalizerOrchestrator.__init__(vocab_size: int, batch: ScheduleBatch, penalizers: Set[Type['_BatchedPenalizer']])
  BatchedPenalizerOrchestrator.batch() -> ScheduleBatch | None
  BatchedPenalizerOrchestrator.batch(value: Optional[ScheduleBatch])
  BatchedPenalizerOrchestrator.reqs()
  BatchedPenalizerOrchestrator.cumulate_output_tokens(output_ids: torch.Tensor)
  BatchedPenalizerOrchestrator.apply(logits: torch.Tensor) -> torch.Tensor
  BatchedPenalizerOrchestrator.filter(keep_indices: torch.Tensor)
  BatchedPenalizerOrchestrator.merge(their: 'BatchedPenalizerOrchestrator')
  _BatchedPenalizer.is_prepared() -> bool
  _BatchedPenalizer.is_required() -> bool
  _BatchedPenalizer.prepare()
  _BatchedPenalizer.prepare_if_required()
  _BatchedPenalizer.teardown()
  _BatchedPenalizer.cumulate_output_tokens(output_ids: torch.Tensor)
  _BatchedPenalizer.apply(logits: torch.Tensor) -> torch.Tensor
  _BatchedPenalizer.filter(keep_indices: torch.Tensor)
  _BatchedPenalizer.merge(their: '_BatchedPenalizer')

# python/sglang/srt/sampling/penaltylib/presence_penalty.py
  BatchedPresencePenalizer.__init__(orchestrator: BatchedPenalizerOrchestrator)

# python/sglang/srt/sampling/sampling_batch_info.py
  SamplingBatchInfo.from_schedule_batch(cls, batch: ScheduleBatch, vocab_size: int)
  SamplingBatchInfo.__len__()
  SamplingBatchInfo.update_regex_vocab_mask()
  SamplingBatchInfo.update_penalties()
  SamplingBatchInfo.apply_logits_bias(logits: torch.Tensor)
  SamplingBatchInfo.filter_batch(keep_indices: List[int], keep_indices_device: torch.Tensor)
  SamplingBatchInfo.merge_custom_logit_processor(lhs: Optional[Dict[int, Tuple[CustomLogitProcessor, torch.Tensor]]], rhs: Optional[Dict[int, Tuple[CustomLogitProcessor, torch.Tensor]]], bs1: int, bs2: int, device: str)
  SamplingBatchInfo.merge_batch(other: 'SamplingBatchInfo')
merge_bias_tensor(lhs: Optional[torch.Tensor], rhs: Optional[torch.Tensor], bs1: int, bs2: int, device: str, default: float)

# python/sglang/srt/sampling/sampling_params.py
  SamplingParams.__init__(max_new_tokens: int, stop: Optional[Union[str, List[str]]], stop_token_ids: Optional[List[int]], temperature: float, top_p: float, top_k: int, min_p: float, frequency_penalty: float, presence_penalty: float, repetition_penalty: float, min_new_tokens: int, n: int, json_schema: Optional[str], regex: Optional[str], ebnf: Optional[str], structural_tag: Optional[str], ignore_eos: bool, skip_special_tokens: bool, spaces_between_special_tokens: bool, no_stop_trim: bool, custom_params: Optional[Dict[str, Any]], stream_interval: Optional[int], logit_bias: Optional[Dict[str, float]]) -> None
  SamplingParams.verify(vocab_size)
  SamplingParams.normalize(tokenizer)

# python/sglang/srt/server_args.py
add_load_format_choices(choices)
add_quantization_method_choices(choices)
add_attention_backend_choices(choices)
add_disagg_transfer_backend_choices(choices)
  ServerArgs.__post_init__()
  ServerArgs.add_cli_args(parser: argparse.ArgumentParser)
  ServerArgs.from_cli_args(cls, args: argparse.Namespace)
  ServerArgs.url()
  ServerArgs.get_hf_config()
  ServerArgs.check_server_args()
  ServerArgs.check_lora_server_args()
  ServerArgs.validate_disagg_tp_size(prefill_tp: int, decode_tp: int)
  ServerArgs.model_specific_adjustments()
  ServerArgs.adjust_mem_fraction_for_vlm(model_config)
prepare_server_args(argv: List[str]) -> ServerArgs
  PortArgs.init_new(server_args, dp_rank: Optional[int]) -> 'PortArgs'
  LoRAPathAction.__call__(parser, namespace, values, option_string)
  DeprecatedAction.__init__(option_strings, dest, nargs)
  DeprecatedAction.__call__(parser, namespace, values, option_string)
print_deprecated_warning(message: str)
auto_choose_speculative_params(self: ServerArgs)

# python/sglang/srt/speculative/build_eagle_tree.py
build_tree_kernel_efficient_preprocess(verified_id: torch.Tensor, score_list: List[torch.Tensor], token_list: List[torch.Tensor], parents_list: List[torch.Tensor], num_verify_tokens: int)
build_tree_kernel_efficient(verified_id: torch.Tensor, score_list: List[torch.Tensor], token_list: List[torch.Tensor], parents_list: List[torch.Tensor], seq_lens: torch.Tensor, seq_lens_sum: int, topk: int, spec_steps: int, num_verify_tokens: int, tree_mask_mode: TreeMaskMode, tree_mask_buf: Optional[torch.Tensor], position_buf: Optional[torch.Tensor])
test_build_tree_kernel_efficient()

# python/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py
  EAGLEDraftCudaGraphRunner.__init__(eagle_worker: EAGLEWorker)
  EAGLEDraftCudaGraphRunner.can_run(forward_batch: ForwardBatch)
  EAGLEDraftCudaGraphRunner.capture()
  EAGLEDraftCudaGraphRunner.capture_one_batch_size(num_seqs: int, forward: Callable)
  EAGLEDraftCudaGraphRunner.replay(forward_batch: ForwardBatch)

# python/sglang/srt/speculative/eagle_draft_extend_cuda_graph_runner.py
  EAGLEDraftExtendCudaGraphRunner.__init__(eagle_worker: EAGLEWorker)
  EAGLEDraftExtendCudaGraphRunner.can_run(forward_batch: ForwardBatch)
  EAGLEDraftExtendCudaGraphRunner.capture()
  EAGLEDraftExtendCudaGraphRunner.capture_one_batch_size(bs: int, forward: Callable)
  EAGLEDraftExtendCudaGraphRunner.replay(forward_batch: ForwardBatch)

# python/sglang/srt/speculative/eagle_utils.py
  EagleDraftInput.prepare_for_extend(batch: ScheduleBatch)
  EagleDraftInput.create_idle_input(cls, device: torch.device, hidden_size: int, dtype: torch.dtype, topk: int, capture_hidden_mode: CaptureHiddenMode)
  EagleDraftInput.prepare_extend_after_decode(batch: ScheduleBatch, speculative_num_steps: int)
  EagleDraftInput.generate_attn_arg_prefill(req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, req_to_token: torch.Tensor)
  EagleDraftInput.filter_batch(new_indices: torch.Tensor, has_been_filtered: bool)
  EagleDraftInput.merge_batch(spec_info: EagleDraftInput)
  EagleVerifyInput.create_idle_input(cls, topk: int, spec_steps: int, num_verify_tokens: int)
  EagleVerifyInput.prepare_for_verify(batch: ScheduleBatch, page_size: int)
  EagleVerifyInput.generate_attn_arg_prefill(req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, req_to_token: torch.Tensor)
  EagleVerifyInput.verify(batch: ScheduleBatch, logits_output: LogitsProcessorOutput, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, page_size: int, vocab_mask: Optional[torch.Tensor]) -> torch.Tensor
create_extend_after_decode_spec_info(verified_id, seq_lens, accept_lens, positions, new_verified_id, bs_upper: tl.constexpr)
assign_req_to_token_pool(req_pool_indices, req_to_token, start_offset, end_offset, out_cache_loc, pool_len: tl.constexpr, bs_upper: tl.constexpr)
assign_draft_cache_locs(req_pool_indices, req_to_token, seq_lens, extend_lens, num_new_pages_per_topk, out_cache_loc, pool_len: tl.constexpr, topk: tl.constexpr, speculative_num_steps: tl.constexpr, page_size: tl.constexpr, bs_upper: tl.constexpr, iter_upper: tl.constexpr)
generate_draft_decode_kv_indices(req_pool_indices, req_to_token, paged_kernel_lens, kv_indices, kv_indptr, positions, pool_len: tl.constexpr, kv_indices_stride: tl.constexpr, kv_indptr_stride: tl.constexpr, bs_upper: tl.constexpr, iter_upper: tl.constexpr, num_tokens_upper: tl.constexpr, page_size: tl.constexpr)
align_evict_mask_to_page_size(seq_lens, evict_mask, page_size: tl.constexpr, num_draft_tokens: tl.constexpr, BLOCK_SIZE: tl.constexpr)
get_target_cache_loc(tgt_cache_loc, to_free_slots, accept_length, to_free_num_slots, out_cache_loc, num_verify_tokens: tl.constexpr, num_verify_tokens_upper: tl.constexpr, bs_upper: tl.constexpr)
get_src_tgt_cache_loc(seq_lens: torch.Tensor, out_cache_loc: torch.Tensor, accept_index: torch.Tensor, accept_length: torch.Tensor, draft_token_num: int, page_size: int)
filter_finished_cache_loc_kernel(out_cache_loc, tgt_cache_loc, accept_length, accept_length_filter, bs_upper: tl.constexpr, num_verify_tokens_upper: tl.constexpr)
create_accept_length_filter(accept_length: torch.Tensor, unfinished_index_device: torch.Tensor, seq_lens: torch.Tensor)
select_top_k_tokens(i: int, topk_p: torch.Tensor, topk_index: torch.Tensor, hidden_states: torch.Tensor, scores: torch.Tensor, topk: int)
traverse_tree(retrieve_next_token: torch.Tensor, retrieve_next_sibling: torch.Tensor, draft_tokens: torch.Tensor, grammar: BaseGrammarObject, allocate_token_bitmask: torch.Tensor)
generate_token_bitmask(reqs: List[Req], verify_input: EagleVerifyInput, retrieve_next_token_cpu: torch.Tensor, retrieve_next_sibling_cpu: torch.Tensor, draft_tokens_cpu: torch.Tensor, vocab_size: int)

# python/sglang/srt/speculative/eagle_worker.py
draft_tp_context(tp_group: GroupCoordinator)
  EAGLEWorker.__init__(server_args: ServerArgs, gpu_id: int, tp_rank: int, dp_rank: Optional[int], moe_ep_rank: int, nccl_port: int, target_worker: TpModelWorker)
  EAGLEWorker.init_attention_backend()
  EAGLEWorker.init_cuda_graphs()
  EAGLEWorker.draft_model_runner()
  EAGLEWorker.forward_batch_speculative_generation(batch: ScheduleBatch) -> Tuple[LogitsProcessorOutput, torch.Tensor, int, int, bool]
  EAGLEWorker.check_forward_draft_extend_after_decode(batch: ScheduleBatch)
  EAGLEWorker.forward_target_extend(batch: ScheduleBatch) -> Tuple[LogitsProcessorOutput, torch.Tensor, int, Optional[torch.Tensor]]
  EAGLEWorker.draft(batch: ScheduleBatch)
  EAGLEWorker.draft_forward(forward_batch: ForwardBatch)
  EAGLEWorker.verify(batch: ScheduleBatch, spec_info: EagleVerifyInput)
  EAGLEWorker.add_logprob_values(batch: ScheduleBatch, res: EagleVerifyOutput, logits_output: LogitsProcessorOutput)
  EAGLEWorker.forward_draft_extend(batch: ScheduleBatch, hidden_states: torch.Tensor, next_token_ids: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor])
  EAGLEWorker.forward_draft_extend_after_decode(batch: ScheduleBatch)
  EAGLEWorker.capture_for_decode(logits_output: LogitsProcessorOutput, draft_input: EagleDraftInput)
load_token_map(token_map_path: str) -> List[int]
get_last_loc_large_page_size_top_k_1(req_to_token: torch.Tensor, req_pool_indices: torch.Tensor, seq_lens, speculative_num_steps: int)
get_last_loc_large_page_size_large_top_k(req_to_token: torch.Tensor, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, speculative_num_steps: int, topk: int, page_size: int)

# python/sglang/srt/speculative/spec_info.py
  SpeculativeAlgorithm.is_none()
  SpeculativeAlgorithm.is_eagle()
  SpeculativeAlgorithm.is_eagle3()
  SpeculativeAlgorithm.from_string(name: str)

# python/sglang/srt/tokenizer/tiktoken_tokenizer.py
  TiktokenProcessor.__init__(name: str)
  TiktokenProcessor.image_processor(image)
  TiktokenTokenizer.__init__(tokenizer_path)
  TiktokenTokenizer.encode(x, add_special_tokens)
  TiktokenTokenizer.decode(x)
  TiktokenTokenizer.batch_decode(batch, skip_special_tokens, spaces_between_special_tokens)
  TiktokenTokenizer.apply_chat_template(messages, tokenize, add_generation_prompt, tools, reasoning_effort)
  TiktokenTokenizer.__call__(text)
  TiktokenTokenizer.init_xgrammar()

# python/sglang/srt/torch_memory_saver_adapter.py
  TorchMemorySaverAdapter.create(enable: bool)
  TorchMemorySaverAdapter.check_validity(caller_name)
  TorchMemorySaverAdapter.configure_subprocess()
  TorchMemorySaverAdapter.region(tag: str)
  TorchMemorySaverAdapter.pause(tag: str)
  TorchMemorySaverAdapter.resume(tag: str)
  TorchMemorySaverAdapter.enabled()
  _TorchMemorySaverAdapterReal.configure_subprocess()
  _TorchMemorySaverAdapterReal.region(tag: str)
  _TorchMemorySaverAdapterReal.pause(tag: str)
  _TorchMemorySaverAdapterReal.resume(tag: str)
  _TorchMemorySaverAdapterReal.enabled()
  _TorchMemorySaverAdapterNoop.configure_subprocess()
  _TorchMemorySaverAdapterNoop.region(tag: str)
  _TorchMemorySaverAdapterNoop.pause(tag: str)
  _TorchMemorySaverAdapterNoop.resume(tag: str)
  _TorchMemorySaverAdapterNoop.enabled()

# python/sglang/srt/two_batch_overlap.py
get_token_num_per_seq(forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
compute_split_seq_index(forward_mode: 'ForwardMode', num_tokens: int, extend_lens: Optional[Sequence[int]], token_num_per_seq: Optional[int]) -> Optional[int]
split_spec_info(spec_info: Optional[EagleVerifyInput], start_seq_index: int, end_seq_index: int, start_token_index: int, end_token_index: int)
compute_split_token_index(split_seq_index: int, forward_mode: 'ForwardMode', extend_seq_lens: Optional[Sequence[int]], token_num_per_seq: Optional[int]) -> int
compute_split_indices_for_cuda_graph_replay(forward_mode: ForwardMode, cuda_graph_num_tokens: int, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  TboCudaGraphRunnerPlugin.__init__()
  TboCudaGraphRunnerPlugin.capture_one_batch_size(batch: ForwardBatch, num_tokens: int)
  TboCudaGraphRunnerPlugin.replay_prepare(forward_mode: ForwardMode, bs: int, num_token_non_padded: int, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  TboDPAttentionPreparer.prepare_all_gather(local_batch: ScheduleBatch)
  TboDPAttentionPreparer.compute_output(partial_global_info)
  TboForwardBatchPreparer.prepare(cls, batch: ForwardBatch, is_draft_worker: bool)
  TboForwardBatchPreparer.prepare_raw(cls, batch: ForwardBatch, tbo_children_num_token_non_padded: torch.Tensor)
  TboForwardBatchPreparer.derive_fields_related_to_seq_len_for_two_chunk(cls, batch: ForwardBatch)
  TboForwardBatchPreparer.filter_batch(cls, batch: ForwardBatch)
  TboForwardBatchPreparer.compute_tbo_children_num_token_non_padded(cls, batch: ForwardBatch)
  TboForwardBatchPreparer.compute_tbo_children_num_token_non_padded_raw(cls, tbo_split_token_index: int, num_token_non_padded: int)
model_forward_maybe_tbo(layers, enable_tbo: bool, positions: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor, input_data_scatter_mode: ScatterMode, residual: Optional[torch.Tensor], zero_allocator: Optional[BumpAllocator])
  MaybeTboDeepEPDispatcher.__init__()
  MaybeTboDeepEPDispatcher.dispatch() -> DispatchOutput
  MaybeTboDeepEPDispatcher.dispatch_a()
  MaybeTboDeepEPDispatcher.dispatch_b()
  MaybeTboDeepEPDispatcher.combine() -> torch.Tensor
  MaybeTboDeepEPDispatcher.combine_a()
  MaybeTboDeepEPDispatcher.combine_b()

# python/sglang/srt/utils.py
is_hip() -> bool
is_cuda()
is_cuda_alike()
is_hpu() -> bool
is_xpu() -> bool
is_npu() -> bool
is_host_cpu_x86() -> bool
is_cpu() -> bool
get_cuda_version()
is_blackwell()
is_sm100_supported(device) -> bool
is_sm90_supported(device) -> bool
get_bool_env_var(name: str, default: str) -> bool
get_int_env_var(name: str, default: int) -> int
support_triton(backend: str) -> bool
cpu_has_amx_support()
use_intel_amx_backend(layer)
is_flashinfer_available()
random_uuid() -> str
  DynamicGradMode.set_inference_mode(mode: bool)
  DynamicGradMode.__init__(mode)
  DynamicGradMode.__new__(cls, mode_or_orig_func)
  DynamicGradMode.__enter__() -> None
  DynamicGradMode.__exit__(exc_type: Any, exc_value: Any, traceback: Any) -> None
  DynamicGradMode.clone() -> 'DynamicGradMode'
enable_show_time_cost()
  TimeInfo.__init__(name, interval, color, indent)
  TimeInfo.check()
  TimeInfo.pretty_print()
mark_start(name, interval, color, indent)
mark_end(name)
calculate_time(show, min_cost_ms)
get_available_gpu_memory(device, gpu_id, distributed, empty_cache, cpu_group)
is_pin_memory_available() -> bool
  LayerFn.__call__(layer_id: int, prefix: str) -> torch.nn.Module
make_layers(num_hidden_layers: int, layer_fn: LayerFn, pp_rank: Optional[int], pp_size: Optional[int], prefix: str, return_tuple: bool, offloader_kwargs: Dict[str, Any]) -> Tuple[int, int, torch.nn.ModuleList]
set_random_seed(seed: int) -> None
find_process_using_port(port: int) -> Optional[psutil.Process]
wait_port_available(port: int, port_name: str, timeout_s: int, raise_exception: bool) -> bool
is_port_available(port)
get_free_port()
decode_video_base64(video_base64)
load_audio(audio_file: str, sr: Optional[int], mono: bool) -> np.ndarray
load_image(image_file: Union[Image.Image, str, ImageData, bytes]) -> tuple[Image.Image, tuple[int, int]]
load_video(video_file: Union[str, bytes], use_gpu: bool)
suppress_other_loggers()
assert_pkg_version(pkg: str, min_version: str, message: str)
kill_process_tree(parent_pid, include_parent: bool, skip_pid: int)
monkey_patch_p2p_access_check()
monkey_patch_vllm_gguf_config()
set_ulimit(target_soft_limit)
add_api_key_middleware(app, api_key: str)
prepare_model_and_tokenizer(model_path: str, tokenizer_path: str)
configure_logger(server_args, prefix: str)
replace_submodule(model: nn.Module, module_name: str, new_module: nn.Module) -> nn.Module
set_weight_attrs(weight: torch.Tensor, weight_attrs: Optional[Dict[str, Any]])
broadcast_pyobj(data: List[Any], rank: int, dist_group: Optional[torch.distributed.ProcessGroup], src: int, force_cpu_device: bool)
point_to_point_pyobj(data: List[Any], rank: int, group: Optional[torch.distributed.ProcessGroup], src: int, dst: int)
pytorch_profile(name, func)
get_zmq_socket(context: zmq.Context, socket_type: zmq.SocketType, endpoint: str, bind: bool)
dump_to_file(dirpath, name, value)
is_triton_3()
maybe_torch_compile()
delete_directory(dirpath)
set_prometheus_multiproc_dir()
add_prometheus_middleware(app)
bind_port(port)
get_amdgpu_memory_capacity()
get_device_sm()
get_nvgpu_memory_capacity()
get_hpu_memory_capacity()
get_npu_memory_capacity()
get_device_memory_capacity(device: str)
init_custom_process_group(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)
crash_on_warnings()
print_warning_once(msg: str) -> None
print_info_once(msg: str) -> None
get_device_name(device_id: int) -> str
is_habana_available() -> bool
get_device(device_id: Optional[int]) -> str
get_device_count() -> int
get_device_core_count(device_id: int) -> int
get_device_capability(device_id: int) -> Tuple[int, int]
get_npu_compiler_config()
get_compiler_backend() -> str
supports_custom_op() -> bool
direct_register_custom_op(op_name: str, op_func: Callable, mutates_args: List[str], fake_impl: Optional[Callable], target_lib: Optional[Library])
set_gpu_proc_affinity(tp_size: int, nnodes: int, gpu_id: int)
disable_request_logging() -> bool
dataclass_to_string_truncated(data, max_length, skip_names: Optional[Set[str]])
permute_weight(x: torch.Tensor) -> torch.Tensor
  MultiprocessingSerializer.serialize(obj, output_str: bool)
  MultiprocessingSerializer.deserialize(data)
debug_timing(func)
nullable_str(val: str)
pyspy_dump_schedulers()
kill_itself_when_parent_died()
set_uvicorn_logging_configs()
get_ip() -> str
get_open_port() -> int
is_valid_ipv6_address(address: str) -> bool
maybe_wrap_ipv6_address(address: str) -> str
format_tcp_address(ip: str, port: int) -> str
configure_ipv6(dist_init_addr)
launch_dummy_health_check_server(host, port, enable_metrics)
create_checksum(directory: str)
set_cuda_arch()
next_power_of_2(n: int)
round_up(x: int, y: int) -> int
  EmptyContextManager.__enter__()
  EmptyContextManager.__exit__(exc_type, exc_value, traceback)
empty_context()
add_prefix(name: str, prefix: str) -> str
is_remote_url(url: Union[str, Path]) -> bool
parse_connector_type(url: str) -> str
retry(fn, max_retry: int, initial_delay: float, max_delay: float, should_retry: Callable[[Any], bool])
flatten_nested_list(nested_list)
is_non_idle_and_non_empty(forward_mode, hidden_states)
fast_topk(values, topk, dim)
bind_or_assign(target, source)
get_local_ip_auto() -> str
get_local_ip_by_nic(interface: str) -> str
get_local_ip_by_remote() -> str
is_page_size_one(server_args)
is_no_spec_infer_or_topk_one(server_args)
is_fa3_default_architecture(hf_config)
  BumpAllocator.__init__(buffer_size: int, dtype, device)
  BumpAllocator.allocate(size: int)
log_info_on_rank0(logger, msg)
load_json_config(data: str)
dispose_tensor(x: torch.Tensor)
  Withable.__init__()
  Withable.value() -> T
  Withable.with_value(new_value: T)
require_mlp_tp_gather(server_args)
require_attn_tp_gather(server_args)
require_gathered_buffer(server_args)
require_mlp_sync(server_args)
find_local_repo_dir(repo_id: str, revision: Optional[str]) -> Optional[str]
read_system_prompt_from_file(model_name: str) -> str
bind_or_assign(target, source)
prepack_weight_if_needed(weight)
dim_is_supported(weight)
  PackWeightMethod.__init__(weight_names, transpose_dims)
  PackWeightMethod.process_weights_after_loading(module) -> None
  LazyValue.__init__(creator: Callable)
  LazyValue.value()
dynamic_import(func_path: str)
gc_object_counts()
configure_gc_warning(warn_threshold_secs)
freeze_gc(context: str)
configure_gc_logger()
align(x: int, y: int) -> int
ceil_div(x: int, y: int) -> int
parse_lscpu_topology()
get_physical_cpus_by_numa()
get_cpu_ids_by_node()
is_shm_available(dtype, world_size, local_size)
lru_cache_frozenset(maxsize)
apply_module_patch(target_module, target_function, wrappers)
parse_module_path(module_path, function_name, create_dummy)
mxfp_supported()
  ConcurrentCounter.__init__(initial: int)
  ConcurrentCounter.value() -> int
  ConcurrentCounter.__repr__() -> str
  ConcurrentCounter.increment(n: int, notify_all: bool)
  ConcurrentCounter.decrement(n: int, notify_all: bool)
  ConcurrentCounter.wait_for(condition: Callable[[int], bool])
  ConcurrentCounter.wait_for_zero()
is_triton_kernels_available() -> bool
check_cuda_result(raw_output)

# python/sglang/srt/warmup.py
warmup(name: str) -> callable
execute_warmups(disaggregation_mode: str, warmup_names: List[str], tokenizer_manager: TokenizerManager)
voice_chat(disaggregation_mode: str, tokenizer_manager: TokenizerManager)

# python/sglang/srt/weight_sync/tensor_bucket.py
  FlattenedTensorBucket.__init__(named_tensors: List[Tuple[str, torch.Tensor]], flattened_tensor: torch.Tensor, metadata: List[FlattenedTensorMetadata])
  FlattenedTensorBucket.get_flattened_tensor() -> torch.Tensor
  FlattenedTensorBucket.get_metadata() -> List[FlattenedTensorMetadata]
  FlattenedTensorBucket.reconstruct_tensors() -> List[Tuple[str, torch.Tensor]]

# python/sglang/srt/weight_sync/utils.py
update_weights(engine: Engine, params_batch: list[tuple[str, torch.Tensor]], device_mesh_key: str, device_mesh: DeviceMesh, load_format: Optional[str])

# python/sglang/test/attention/test_flashattn_backend.py
  MockModelRunner.__init__(page_size, num_heads, head_dim)
  TestFlashAttentionBackend.setUp()
  TestFlashAttentionBackend.test_forward_extend()
  TestFlashAttentionBackend.test_forward_decode()
  TestFlashAttentionBackend.test_forward_extend_with_prefix()
  TestFlashAttentionBackend.test_forward_extend_with_page_size_greater_than_1()
  TestFlashAttentionBackend.test_forward_decode_with_page_size_greater_than_1()

# python/sglang/test/attention/test_flashattn_mla_backend.py
  MockModelRunner.__init__(kv_lora_rank, qk_rope_head_dim)
  MockReqToTokenPool.__init__(batch_size, seq_len, device)
  TestFlashAttentionMLABackend.setUp()
  TestFlashAttentionMLABackend.test_forward_extend()
  TestFlashAttentionMLABackend.test_forward_decode()
  TestFlashAttentionMLABackend.test_forward_extend_with_prefix()

# python/sglang/test/attention/test_prefix_chunk_info.py
  MockForwardBatch.__init__(max_chunk_capacity: int)
  MockForwardBatch.get_max_chunk_capacity()
  MockReqToTokenPool.__init__(batch_size, seq_len, device)
check_kv_indices(forward_batch)
  TestPrefixChunkInfo.setUp()
  TestPrefixChunkInfo.test_prefix_chunk_info()

# python/sglang/test/attention/test_trtllm_mla_backend.py
build_rotary_emb(config, device)
  MockModelRunner.__init__(config)
compare_outputs(trtllm_out, reference_out, tolerance)
  TestTRTLLMMLA.test_basic_functionality()
  TestTRTLLMMLA.test_decode_output_match()
  TestTRTLLMMLA.test_page_size_consistency()
  TestTRTLLMMLA.test_shape_sanity()
  TestTRTLLMMLA.test_metadata_initialization()
  TestTRTLLMMLA.test_metadata_block_calculation()
  TestTRTLLMMLA.test_metadata_kv_indices_correctness()
  TestTRTLLMMLA.test_metadata_cuda_graph_compatibility()
  TestTRTLLMMLA.test_metadata_consistency_across_calls()

# python/sglang/test/doc_patch.py
patched_post_init()
launch_server_cmd(command: str, host: str, port: int)

# python/sglang/test/few_shot_gsm8k.py
get_one_example(lines, i, include_answer)
get_few_shot_examples(lines, k)
get_answer_value(answer_str)
run_eval(args)

# python/sglang/test/few_shot_gsm8k_engine.py
get_one_example(lines, i, include_answer)
get_few_shot_examples(lines, k)
get_answer_value(answer_str)
concurrent_generate(engine, prompts, sampling_param)
run_eval(args)

# python/sglang/test/run_eval.py
run_eval(args)

# python/sglang/test/runners.py
get_dtype_str(torch_dtype)
get_top_logprobs(logits, k)
get_token_ids_logprobs(logits, token_ids)
  HFRunner.__init__(model_path: str, torch_dtype: torch.dtype, model_type: str, output_str_only: bool, trust_remote_code: bool, patch_model_do_sample_false: bool)
  HFRunner.needs_trust_remote_code(model_path)
  HFRunner.start_model_process(in_queue, out_queue, model_path, torch_dtype)
  HFRunner.forward(prompts: Union[List[List[str]], List[str], List[torch.Tensor]], image_data: Optional[List[str]], max_new_tokens: int, lora_paths: Optional[List[str]], token_ids_logprob: Optional[int])
  HFRunner.terminate()
  HFRunner.__enter__()
  HFRunner.__exit__(exc_type, exc_value, traceback)
  HFRunner.forward_generation_raw(base_model, prompts: Union[List[str], List[torch.Tensor]], max_new_tokens: int, tokenizer, torch_dtype: torch.dtype, lora_paths: Optional[List[str]], output_str_only: bool, token_ids_logprob: Optional[int], patch_model_do_sample_false: Optional[bool]) -> ModelOutput
  SRTRunner.__init__(model_path: str, torch_dtype: torch.dtype, model_type: str, tp_size: int, model_impl: str, port: int, lora_paths: Optional[Union[List[str], List[dict[str, str]]]], max_loras_per_batch: int, attention_backend: Optional[str], prefill_attention_backend: Optional[str], decode_attention_backend: Optional[str], lora_backend: str, disable_cuda_graph: bool, disable_radix_cache: bool, chunked_prefill_size: Optional[int], dp_size: int, tokenizer_path: Optional[str], mem_fraction_static: float, trust_remote_code: bool, speculative_draft_model_path: Optional[str], speculative_algorithm: Optional[str], speculative_num_steps: Optional[int], speculative_eagle_topk: Optional[int], speculative_num_draft_tokens: Optional[int], disable_overlap_schedule: bool, disable_custom_all_reduce: bool, torchao_config: Optional[str], cuda_graph_max_bs: int, sleep_on_idle, max_lora_rank: Optional[int], lora_target_modules: Optional[List[str]], enable_lora: Optional[bool], max_loaded_loras: Optional[int])
  SRTRunner.load_lora_adapter(lora_name: str, lora_path: str, pinned: bool)
  SRTRunner.unload_lora_adapter(lora_name: str)
  SRTRunner.forward(prompts: Union[List[List[str]], List[str], List[torch.Tensor]], image_data: Optional[List[str]], max_new_tokens: int, lora_paths: Optional[List[str]], logprob_start_len: int, top_k: Optional[int], token_ids_logprob: Optional[List[int]])
  SRTRunner.batch_forward(prompts: Union[List[str], List[torch.Tensor]], image_data: Optional[List[str]], max_new_tokens, lora_paths)
  SRTRunner.__enter__()
  SRTRunner.__exit__(exc_type, exc_value, traceback)
  SRTRunner.forward_generation_raw(engine: Engine, prompts: Union[List[str], List[torch.Tensor]], max_new_tokens: int, lora_paths: Optional[List[str]], logprob_start_len: int, top_k: Optional[int], token_ids_logprob: Optional[List[int]])
  SRTRunner.batch_forward_generation_raw(prompts: Union[List[str], List[torch.Tensor]], max_new_tokens, lora_paths, engine)
monkey_patch_gemma2_sdpa()
check_close_model_outputs(hf_outputs: ModelOutput, srt_outputs: ModelOutput, prefill_tolerance: float, decode_tolerance: float, rouge_l_tolerance: float, debug_text: str, check_logprobs: bool)

# python/sglang/test/send_one.py
  BenchArgs.add_cli_args(parser: argparse.ArgumentParser)
  BenchArgs.from_cli_args(cls, args: argparse.Namespace)
send_one_prompt(args)

# python/sglang/test/simple_eval_common.py
  SamplerBase.__call__(message_list: MessageList) -> str
  Eval.__call__(sampler: SamplerBase) -> EvalResult
  LargerHttpxClient.__init__()
  ChatCompletionSampler.__init__(base_url: str, model: Optional[str], system_message: Optional[str], temperature: float, reasoning_effort: Optional[str], max_tokens: int)
  ChatCompletionSampler.__call__(message_list: MessageList) -> str
format_multichoice_question(row)
check_equality(sampler: SamplerBase, expr1: str, expr2: str)
aggregate_results(single_eval_results: List[SingleEvalResult], default_stats: Tuple[str], name2stats: Optional[Dict[str, Tuple[str]]]) -> EvalResult
map_with_progress(f: callable, xs: List[Any], num_threads: int)
message_to_html(message: Message) -> str
make_report(eval_result: EvalResult) -> str
make_report_from_example_htmls(htmls: List[str])
download_dataset(path, url)
set_ulimit(target_soft_limit)

# python/sglang/test/simple_eval_gpqa.py
  GPQAEval.__init__(filename: str, num_examples: Optional[int], num_threads: int, n_repeats: int)
  GPQAEval.__call__(sampler: SamplerBase) -> EvalResult

# python/sglang/test/simple_eval_humaneval.py
evaluate_functional_correctness(sample: Dict[str, str], completions: List[str], n_workers: int, timeout: float)
  HumanEval.__init__(num_examples: Optional[int], num_threads: int, num_samples_per_task: int, ks_passes: List[int], timeout: int)
  HumanEval.__call__(sampler: SamplerBase) -> EvalResult

# python/sglang/test/simple_eval_math.py
  MathEval.__init__(filename: str, equality_checker: SamplerBase, num_examples: Optional[int], num_threads: int)
  MathEval.__call__(sampler: SamplerBase) -> EvalResult

# python/sglang/test/simple_eval_mgsm.py
parse_answer(answer: str, answer_prefix: str) -> str
score_mgsm(target: str, prediction: str) -> bool
get_lang_examples(lang: str) -> list[dict[str, str]]
get_all_examples() -> list[dict[str, str]]
  MGSMEval.__init__(num_examples_per_lang: int, num_threads: int, languages: Optional[list[str]])
  MGSMEval.__call__(sampler: SamplerBase) -> EvalResult

# python/sglang/test/simple_eval_mmlu.py
  MMLUEval.__init__(filename: str, num_examples: Optional[int], num_threads: int)
  MMLUEval.__call__(sampler: SamplerBase) -> EvalResult

# python/sglang/test/test_activation.py
  TestGeluAndMul.setUpClass(cls)
  TestGeluAndMul.test_gelu_and_mul()
  TestQuickGELU.setUpClass(cls)
  TestQuickGELU.test_quick_gelu()

# python/sglang/test/test_block_fp8.py
native_per_token_group_quant_fp8(x, group_size, eps, dtype)
  TestPerTokenGroupQuantFP8.setUpClass(cls)
  TestPerTokenGroupQuantFP8.test_per_token_group_quant_fp8()
native_static_quant_fp8(x, x_s, dtype)
  TestStaticQuantFP8.setUpClass(cls)
  TestStaticQuantFP8.test_static_quant_fp8()
  TestPerTensorQuantMlaFP8.setUpClass(cls)
  TestPerTensorQuantMlaFP8.test_per_tensor_quant_mla_fp8()
  TestPerTokenGroupQuantMlaDeepGemmMaskedFP8.setUpClass(cls)
  TestPerTokenGroupQuantMlaDeepGemmMaskedFP8.test_per_token_group_quant_mla_deep_gemm_masked_fp8()
native_w8a8_block_fp8_matmul(A, B, As, Bs, block_size, output_dtype)
  TestW8A8BlockFP8Matmul.setUpClass(cls)
  TestW8A8BlockFP8Matmul.test_w8a8_block_fp8_matmul()
torch_w8a8_block_fp8_moe(a, w1, w2, w1_s, w2_s, score, topk, block_shape)
  TestW8A8BlockFP8FusedMoE.setUpClass(cls)
  TestW8A8BlockFP8FusedMoE.test_w8a8_block_fp8_fused_moe()
torch_w8a8_block_fp8_bmm(a, a_s, w, w_s, block_shape, out_dtype)
  TestW8A8BlockFP8BatchedDeepGemm.setUpClass(cls)
  TestW8A8BlockFP8BatchedDeepGemm.test_w8a8_block_fp8_batched_deep_gemm()

# python/sglang/test/test_block_fp8_deep_gemm_blackwell.py
ceil_div(x: int, y: int) -> int
align(x: int, y: int) -> int
per_token_group_quant_fp8(x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
per_block_quant_fp8(x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
ceil_to_ue8m0(x: torch.Tensor)
per_token_group_quant_mxfp8(x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
per_block_quant_mxfp8(x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
native_w8a8_block_fp8_matmul(A, B, As, Bs, block_size, output_dtype)
block_quant_dequant(x_q_block: torch.Tensor, x_s: torch.Tensor, block_size: List[int], dtype: torch.dtype) -> torch.Tensor
  TestDeepGemmBlackwell.setUpClass(cls)
  TestDeepGemmBlackwell.test_deep_gemm_blackwell()

# python/sglang/test/test_block_fp8_ep.py
ep_moe(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, router_logits: torch.Tensor, topk_config: TopKConfig, num_experts: int, fp8_dtype: torch.types, num_experts_per_partition: int, start_expert_id: int, end_expert_id: int, use_fp8_w8a8: bool, w1_scale_inv: Optional[torch.Tensor], w2_scale_inv: Optional[torch.Tensor], block_shape: Optional[List[int]])
block_dequant(x_q_block: torch.Tensor, x_s: torch.Tensor, block_size: List[int]) -> Tuple[torch.Tensor, torch.Tensor]
  TestW8A8BlockFP8EPMoE.setUpClass(cls)
  TestW8A8BlockFP8EPMoE.test_w8a8_block_fp8_ep_moe()

# python/sglang/test/test_custom_ops.py
test_scaled_fp8_quant_per_tensor(dtype) -> None
test_scaled_fp8_quant_per_token_dynamic(dtype) -> None
test_scaled_fp8_quant_with_padding(dtype) -> None

# python/sglang/test/test_cutlass_moe.py
calc_diff(x, y)
get_model_config(tp_size: int)
to_fp8(tensor: torch.Tensor) -> torch.Tensor
run_test(tp_size, batch_size, model_config, check)
main(tp_size, batch_sizes, check)

# python/sglang/test/test_cutlass_w4a8_moe.py
pack_int4_values_to_int8(int4_values_interleaved: torch.Tensor) -> torch.Tensor
pack_interleave(num_experts, ref_weight, ref_scale)
test_cutlass_w4a8_moe(M, N, K, E, ep_size, topk, group_size, dtype)
cutlass_moe(a: torch.Tensor, w1_q: torch.Tensor, w2_q: torch.Tensor, w1_scale: torch.Tensor, w2_scale: torch.Tensor, topk_weights: torch.Tensor, topk_ids_: torch.Tensor, a_strides1: torch.Tensor, b_strides1: torch.Tensor, c_strides1: torch.Tensor, a_strides2: torch.Tensor, b_strides2: torch.Tensor, c_strides2: torch.Tensor, s_strides13: torch.Tensor, s_strides2: torch.Tensor, start_expert_id: int, end_expert_id: int, E: int, a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], expert_map: Optional[torch.Tensor], apply_router_weight_on_input: bool)
ref(x: torch.Tensor, num_experts: int, topk_weights: torch.Tensor, topk_ids: torch.Tensor, ref_weight_1: torch.Tensor, ref_weight_2: torch.Tensor, ref_weight_scale_1: torch.Tensor, ref_weight_scale_2: torch.Tensor, has_pre_quant: bool, has_alpha: bool, pre_quant_scale_1: Optional[torch.Tensor], pre_quant_scale_2: Optional[torch.Tensor], alpha_1: Optional[torch.Tensor], alpha_2: Optional[torch.Tensor])

# python/sglang/test/test_deepep_utils.py
init_dist(local_rank: int, num_local_ranks: int)
calc_diff(x: torch.Tensor, y: torch.Tensor)
per_token_cast_to_fp8(x: torch.Tensor)
per_token_cast_back(x_fp8: torch.Tensor, x_scales: torch.Tensor)
inplace_unique(x: torch.Tensor, num_slots: int)
create_grouped_scores(scores: torch.Tensor, group_idx: torch.Tensor, num_groups: int)
bench(fn, num_warmups: int, num_tests: int, post_fn)
  empty_suppress.__enter__()
  empty_suppress.__exit__()
  suppress_stdout_stderr.__enter__()
  suppress_stdout_stderr.__exit__()
bench_kineto(fn, kernel_names, num_tests: int, suppress_kineto_output: bool, trace_path: Optional[str], barrier_comm_profiling: bool)
hash_tensor(t: torch.Tensor)

# python/sglang/test/test_dynamic_grad_mode.py
  TestDynamicGradMode.test_inference()
  TestDynamicGradMode.test_no_grad()
  TestDynamicGradMode.test_nested_inference()
  TestDynamicGradMode.test_nested_no_grad()

# python/sglang/test/test_fp4_moe.py
convert_swizzled_to_linear(a_sf_swizzled: torch.Tensor, m, k, block_size)
dequantize_nvfp4_to_dtype(tensor_fp4, tensor_sf, global_scale, dtype, device, block_size)
break_fp4_bytes(a, dtype)
torch_moe(a, w1, w2, score, topk, expert_map)
check_moe(m: int, n: int, k: int, e: int, topk: int, dtype: torch.dtype, moe_impl: Callable, flip_w13: bool)
test_cutlass_fp4_moe_no_graph(m: int, n: int, k: int, e: int, topk: int, dtype: torch.dtype)
test_flashinfer_fp4_moe_no_graph(m: int, n: int, k: int, e: int, topk: int, dtype: torch.dtype)

# python/sglang/test/test_layernorm.py
  TestRMSNorm.setUpClass(cls)
  TestRMSNorm.test_rms_norm()
  TestGemmaRMSNorm.setUpClass(cls)
  TestGemmaRMSNorm.test_gemma_rms_norm()

# python/sglang/test/test_marlin_moe.py
stack_and_dev(tensors: list[torch.Tensor])
torch_experts(a: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weight: torch.Tensor, topk_ids: torch.Tensor, global_num_experts: int, expert_map: Optional[torch.Tensor], quant_dtype: Optional[torch.dtype], apply_router_weights_on_input: bool) -> torch.Tensor
torch_moe(a: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, score: torch.Tensor, topk: int, global_num_experts: int, expert_map: Optional[torch.Tensor]) -> torch.Tensor
marlin_moe_generate_valid_test_cases()
test_fused_marlin_moe(m: int, n: int, k: int, e: int, topk: int, dtype: torch.dtype, group_size: int, act_order: bool, quant_type: ScalarType, is_k_full: bool)

# python/sglang/test/test_marlin_utils.py
  MarlinWorkspace.__init__(out_features, min_thread_n, max_parallel)
marlin_permute_weights(q_w, size_k, size_n, perm, tile)
marlin_weights(q_w, size_k, size_n, num_bits, perm)
get_weight_perm(num_bits: int)
marlin_quantize(w: torch.Tensor, quant_type: ScalarType, group_size: int, act_order: bool, test_perm: Optional[torch.Tensor])
awq_marlin_quantize(w: torch.Tensor, quant_type: ScalarType, group_size: int)

# python/sglang/test/test_programs.py
test_few_shot_qa()
test_mt_bench()
test_select(check_answer)
test_decode_int()
test_decode_json_regex()
test_decode_json()
test_expert_answer(check_answer)
test_tool_use()
test_react()
test_parallel_decoding()
test_parallel_encoding(check_answer)
test_image_qa()
test_stream()
test_regex()
test_dtype_gen()
test_completion_speculative()
test_chat_completion_speculative()
test_hellaswag_select()
test_gen_min_new_tokens()

# python/sglang/test/test_utils.py
is_in_ci()
is_in_amd_ci()
call_generate_lightllm(prompt, temperature, max_tokens, stop, url)
find_available_port(base_port: int)
call_generate_vllm(prompt, temperature, max_tokens, stop, n, url)
call_generate_outlines(prompt, temperature, max_tokens, stop, regex, n, url)
call_generate_srt_raw(prompt, temperature, max_tokens, stop, url)
call_generate_guidance(prompt, temperature, max_tokens, stop, n, regex, model)
call_select_lightllm(context, choices, url)
call_select_vllm(context, choices, url)
call_select_guidance(context, choices, model)
add_common_other_args_and_parse(parser: argparse.ArgumentParser)
auto_config_device() -> str
add_common_sglang_args_and_parse(parser: argparse.ArgumentParser)
select_sglang_backend(args: argparse.Namespace)
get_call_generate(args: argparse.Namespace)
get_call_select(args: argparse.Namespace)
try_cached_model(model_repo: str)
popen_launch_server(model: str, base_url: str, timeout: float, api_key: Optional[str], other_args: list[str], env: Optional[dict], return_stdout_stderr: Optional[tuple], device: str, pd_separated: bool)
popen_launch_pd_server(model: str, base_url: str, timeout: float, api_key: Optional[str], other_args: list[str], env: Optional[dict])
run_with_timeout(func: Callable, args: tuple, kwargs: Optional[dict], timeout: float)
run_unittest_files(files: List[TestFile], timeout_per_file: float)
get_similarities(vec1, vec2)
get_benchmark_args(base_url, dataset_name, dataset_path, tokenizer, num_prompts, sharegpt_output_len, random_input_len, random_output_len, sharegpt_context_len, request_rate, disable_stream, disable_ignore_eos, seed: int, device, pd_separated: bool, lora_name)
run_bench_serving(model, num_prompts, request_rate, other_server_args, dataset_name, dataset_path, tokenizer, random_input_len, random_output_len, sharegpt_context_len, disable_stream, disable_ignore_eos, need_warmup, seed: int, device, background_task: Optional[Callable[[str, asyncio.Event], Awaitable[None]]], lora_name: Optional[str])
run_bench_serving_multi(model, base_url, other_server_args, benchmark_args, need_warmup, pd_separated)
run_bench_one_batch(model, other_args)
run_bench_offline_throughput(model, other_args)
run_bench_one_batch_server(model, base_url, server_args, bench_args, other_server_args, simulate_spec_acc_lens)
lcs(X, Y)
calculate_rouge_l(output_strs_list1, output_strs_list2)
read_output(output_lines: List[str], filename: str)
run_and_check_memory_leak(workload_func, disable_radix_cache, enable_mixed_chunk, disable_overlap, chunked_prefill_size, assert_has_abort)
run_command_and_capture_output(command, env: Optional[dict])
run_mmlu_test(disable_radix_cache, enable_mixed_chunk, disable_overlap, chunked_prefill_size)
run_mulit_request_test(disable_radix_cache, enable_mixed_chunk, enable_overlap, chunked_prefill_size)
write_github_step_summary(content)
run_logprob_check(self: unittest.TestCase, arg: Tuple)
send_generate_requests(base_url: str, num_requests: int) -> List[str]
send_concurrent_generate_requests(base_url: str, num_requests: int) -> List[str]
dump_bench_raw_result(path: str, states, preds, labels)

# python/sglang/utils.py
execute_once(func)
info_once(message: str)
convert_json_schema_to_str(json_schema: Union[dict, str, Type[BaseModel]]) -> str
get_exception_traceback()
is_same_type(values: list)
read_jsonl(filename: str)
dump_state_text(filename: str, states: list, mode: str)
  HttpResponse.__init__(resp)
  HttpResponse.json()
  HttpResponse.status_code()
http_request(url, json, stream, api_key, verify, method: Optional[str])
encode_image_base64(image_path: Union[str, bytes])
encode_frame(frame)
encode_video_base64(video_path: str, num_frames: int)
find_printable_text(text: str)
  LazyImport.__init__(module_name: str, class_name: str)
  LazyImport.__getattr__(name: str)
  LazyImport.__call__()
download_and_cache_file(url: str, filename: Optional[str])
is_in_ci()
print_highlight(html_content: str)
reserve_port(host, start, end)
release_port(lock_socket)
execute_shell_command(command: str) -> subprocess.Popen
launch_server_cmd(command: str, host: str, port: int)
terminate_process(process)
wait_for_server(base_url: str, timeout: int) -> None
  TypeBasedDispatcher.__init__(mapping: List[Tuple[Type, Callable]])
  TypeBasedDispatcher.__call__(obj: Any)
trim_overlap(existing_text, new_chunk)
stream_and_merge(llm, prompt, sampling_params)
async_stream_and_merge(llm, prompt, sampling_params)
resolve_obj_by_qualname(qualname: str) -> Any
