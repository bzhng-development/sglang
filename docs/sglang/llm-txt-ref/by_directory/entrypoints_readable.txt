================================================================================
FUNCTION INDEX: entrypoints module
================================================================================
Total Functions: 146
Documented: 76


============================================================
FILE: python/sglang/srt/entrypoints/EngineBase.py
Functions: 8
============================================================


CLASS: EngineBase
----------------------------------------
  L  14: generate(self, prompt: Optional[Union[List[str], str]], sampling_params: Optional[Union[List[Dict], Dict]], input_ids: Optional[Union[List[List[int]], List[int]]], image_data: Optional[Union[List[str], str]], return_logprob: Optional[Union[List[bool], bool]], logprob_start_len: Optional[Union[List[int], int]], top_logprobs_num: Optional[Union[List[int], int]], token_ids_logprob: Optional[Union[List[List[int]], List[int]]], lora_path: Optional[Union[List[Optional[str]], Optional[str]]], custom_logit_processor: Optional[Union[List[str], str]], return_hidden_states: Optional[bool], stream: Optional[bool], bootstrap_host: Optional[Union[List[str], str]], bootstrap_port: Optional[Union[List[int], int]], bootstrap_room: Optional[Union[List[int], int]], data_parallel_rank: Optional[int])
         → Union[Dict, Iterator[Dict]]
         📝 Generate outputs based on given inputs.

  L  37: flush_cache(self)
         📝 Flush the cache of the engine.

  L  42: update_weights_from_tensor(self, named_tensors: List[Tuple[str, torch.Tensor]], load_format: Optional[str], flush_cache: bool)
         📝 Update model weights with in-memory tensor data.

  L  51: load_lora_adapter(self, lora_name: str, lora_path: str)
         📝 Load a new LoRA adapter without re-launching the engine.

  L  55: unload_lora_adapter(self, lora_name: str)
         📝 Unload a LoRA adapter without re-launching the engine.

  L  60: release_memory_occupation(self)
         📝 Release GPU memory occupation temporarily.

  L  65: resume_memory_occupation(self)
         📝 Resume GPU memory occupation which is previously released.

  L  70: shutdown(self)
         📝 Shutdown the engine and clean up resources.


============================================================
FILE: python/sglang/srt/entrypoints/context.py
Functions: 23
============================================================


CLASS: ConversationContext
----------------------------------------
  L  28: append_output(self, output)
         → None

  L  32: call_tool(self)
         → list[Message]

  L  36: need_builtin_tool_call(self)
         → bool

  L  40: render_for_completion(self)
         → list[int]


CLASS: HarmonyContext
----------------------------------------
  L  64: __init__(self, messages: list, tool_sessions: dict[str, Union['ClientSession', Tool]])

  L  82: append_output(self, output)
         → None

  L 114: messages(self)
         → list

  L 117: need_builtin_tool_call(self)
         → bool

  L 126: call_tool(self)
         → list[Message]

  L 142: render_for_completion(self)
         → list[int]

  L 145: call_search_tool(self, tool_session: Union['ClientSession', Tool], last_msg: Message)
         → list[Message]

  L 158: call_python_tool(self, tool_session: Union['ClientSession', Tool], last_msg: Message)
         → list[Message]


CLASS: SimpleContext
----------------------------------------
  L  46: __init__(self)

  L  49: append_output(self, output)
         → None

  L  52: need_builtin_tool_call(self)
         → bool

  L  55: call_tool(self)
         → list[Message]

  L  58: render_for_completion(self)
         → list[int]


CLASS: StreamingHarmonyContext
----------------------------------------
  L 184: __init__(self)

  L 193: messages(self)
         → list

  L 196: append_output(self, output)
         → None

  L 226: is_expecting_start(self)
         → bool

  L 229: is_assistant_action_turn(self)
         → bool

  L 232: render_for_completion(self)
         → list[int]


============================================================
FILE: python/sglang/srt/entrypoints/engine.py
Functions: 31
============================================================


CLASS: Engine
----------------------------------------
  L 103: __init__(self)
         📝 The arguments of this function is the same as `sglang/srt/server_args.py::ServerArgs`.
            Please refer to `ServerArgs` for the documentation.

  L 140: generate(self, prompt: Optional[Union[List[str], str]], sampling_params: Optional[Union[List[Dict], Dict]], input_ids: Optional[Union[List[List[int]], List[int]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat], return_logprob: Optional[Union[List[bool], bool]], logprob_start_len: Optional[Union[List[int], int]], top_logprobs_num: Optional[Union[List[int], int]], token_ids_logprob: Optional[Union[List[List[int]], List[int]]], lora_path: Optional[List[Optional[str]]], custom_logit_processor: Optional[Union[List[str], str]], return_hidden_states: bool, stream: bool, bootstrap_host: Optional[Union[List[str], str]], bootstrap_port: Optional[Union[List[int], int]], bootstrap_room: Optional[Union[List[int], int]], data_parallel_rank: Optional[int])
         → Union[Dict, Iterator[Dict]]
         📝 The arguments of this function is the same as `sglang/srt/managers/io_struct.py::GenerateReqInput`.
            Please refer to `GenerateReqInput` for the documentation.

  L 221: async_generate(self, prompt: Optional[Union[List[str], str]], sampling_params: Optional[Union[List[Dict], Dict]], input_ids: Optional[Union[List[List[int]], List[int]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat], return_logprob: Optional[Union[List[bool], bool]], logprob_start_len: Optional[Union[List[int], int]], top_logprobs_num: Optional[Union[List[int], int]], token_ids_logprob: Optional[Union[List[List[int]], List[int]]], lora_path: Optional[List[Optional[str]]], custom_logit_processor: Optional[Union[List[str], str]], return_hidden_states: bool, stream: bool, bootstrap_host: Optional[Union[List[str], str]], bootstrap_port: Optional[Union[List[int], int]], bootstrap_room: Optional[Union[List[int], int]], data_parallel_rank: Optional[int])
         → Union[Dict, AsyncIterator[Dict]]
         📝 The arguments of this function is the same as `sglang/srt/managers/io_struct.py::GenerateReqInput`.
            Please refer to `GenerateReqInput` for the documentation.

  L 293: encode(self, prompt: Union[str, List[str], List[Dict], List[List[Dict]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat])
         → Dict
         📝 The arguments of this function is the same as `sglang/srt/managers/io_struct.py::EmbeddingReqInput`.
            Please refer to `EmbeddingReqInput` for the documentation.

  L 315: async_encode(self, prompt: Union[str, List[str], List[Dict], List[List[Dict]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat])
         → Dict
         📝 Asynchronous version of encode method.
            The arguments of this function is the same as `sglang/srt/managers/io_struct.py::EmbeddingReqInput`.
            Please refer to `EmbeddingReqInput` for the documentation.

  L 337: rerank(self, prompt: Union[List[List[str]]])
         → Dict
         📝 The arguments of this function is the same as `sglang/srt/managers/io_struct.py::EmbeddingReqInput`.
            Please refer to `EmbeddingReqInput` for the documentation.

  L 351: shutdown(self)
         📝 Shutdown the engine

  L 355: __enter__(self)

  L 358: __exit__(self, exc_type, exc_value, traceback)

  L 362: flush_cache(self)

  L 366: start_profile(self)

  L 370: stop_profile(self)

  L 374: start_expert_distribution_record(self)

  L 380: stop_expert_distribution_record(self)

  L 386: dump_expert_distribution_record(self)

  L 392: get_server_info(self)

  L 404: init_weights_update_group(self, master_address: str, master_port: int, rank_offset: int, world_size: int, group_name: str, backend: str)
         📝 Initialize parameter update group.

  L 427: update_weights_from_distributed(self, names: list[str], dtypes: list[str], shapes: list[list[int]], group_name: str, flush_cache: bool)
         📝 Update weights from distributed source.

  L 448: update_weights_from_tensor(self, named_tensors: List[Tuple[str, torch.Tensor]], load_format: Optional[str], flush_cache: bool)
         📝 Update weights from distributed source. If there are going to be more updates, set `flush_cache` to be false
            to avoid duplicated cache cleaning operation.

  L 474: update_weights_from_disk(self, model_path: str, load_format: Optional[str])
         📝 Update the weights from disk inplace without re-launching the engine.
            This method allows updating the model weights from disk without restarting
            the engine. It can be used to load a different model or update weights with
            new training.

  L 495: get_weights_by_name(self, name: str, truncate_size: int)
         📝 Get weights by parameter name.

  L 503: load_lora_adapter(self, lora_name: str, lora_path: str, pinned: bool)
         📝 Load a new LoRA adapter without re-launching the engine.

  L 517: unload_lora_adapter(self, lora_name: str)
         📝 Unload a LoRA adapter without re-launching the engine.

  L 527: release_memory_occupation(self, tags: Optional[List[str]])

  L 534: resume_memory_occupation(self, tags: Optional[List[str]])

  L 541: freeze_gc(self)
         📝 To maintain a high performance server with low latency, we want to reduce the
            stalls caused by the garbage collector scanning through a large number of objects.
            It is usually helpful to start the server and warm it up with real requests to
            initialize many of the long-lived objects that do not need to be garbage collected.
            After sufficient warmup, we can call this function to freeze the garbage collector
            so that all objects created before this point are considered out of scope for garbage
            collection.

  L 561: collective_rpc(self, method: str)

  L 568: save_remote_model(self)

  L 571: save_sharded_model(self)

  L 574: score(self, query: Optional[Union[str, List[int]]], items: Optional[Union[str, List[str], List[List[int]]]], label_token_ids: Optional[List[int]], apply_softmax: bool, item_first: bool)
         → List[List[float]]
         📝 Score the probability of specified token IDs appearing after the given (query + item) pair. For example:
            query = "<|user|>Is the following city the capital of France? "
            items = ["Paris <|assistant|>", "London <|assistant|>", "Berlin <|assistant|>"]
            label_token_ids = [2332, 1223] # Token IDs for "Yes" and "No"
            item_first = False
            This would pass the following prompts to the model:
            "<|user|>Is the following city the capital of France? Paris <|assistant|>"
            "<|user|>Is the following city the capital of France? London <|assistant|>"
            "<|user|>Is the following city the capital of France? Berlin <|assistant|>"
            The api would then return the probabilities of the model producing "Yes" and "No" as the next token.
            The output would look like:
            [[0.9, 0.1], [0.2, 0.8], [0.1, 0.9]]
            Args:
            query: The query text or pre-tokenized query token IDs. Must be provided.
            items: The item text(s) or pre-tokenized item token IDs. Must be provided.
            label_token_ids: List of token IDs to compute probabilities for. If None, no token probabilities will be computed.
            apply_softmax: Whether to normalize probabilities using softmax.
            item_first: If True, prepend items to query. Otherwise append items to query.
            Returns:
            List of dictionaries mapping token IDs to their probabilities for each item.
            Each dictionary in the list corresponds to one item input.
            Raises:
            ValueError: If query is not provided, or if items is not provided,
            or if token IDs are out of vocabulary, or if logprobs are not available for the specified tokens.

  L 625: async_score(self, query: Optional[Union[str, List[int]]], items: Optional[Union[str, List[str], List[List[int]]]], label_token_ids: Optional[List[int]], apply_softmax: bool, item_first: bool)
         → List[List[float]]
         📝 Asynchronous version of score method.
            See score() for detailed documentation.


============================================================
FILE: python/sglang/srt/entrypoints/harmony_utils.py
Functions: 13
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  54: def get_encoding()

  L  61: def get_system_message(model_identity: Optional[str],
        reasoning_effort: Optional[Literal['high',
        'medium',
        'low']],
        start_date: Optional[str],
        browser_description: Optional[str],
        python_description: Optional[str])
         → Message

  L  86: def get_developer_message(instructions: Optional[str],
        tools: Optional[list[Tool]])
         → Message

  L 118: def get_user_message(content: str)
         → Message

  L 122: def parse_response_input(response_msg: ResponseInputOutputItem,
        prev_responses: list[Union[ResponseOutputItem,
        ResponseReasoningItem]])
         → Message

  L 174: def parse_response_output(output: ResponseOutputItem)
         → Message

  L 190: def parse_chat_input(chat_msg)
         → Message

  L 202: def render_for_completion(messages: list[Message])
         → list[int]

  L 210: def get_stop_tokens_for_assistant_actions()
         → list[int]

  L 214: def get_streamable_parser_for_assistant()
         → StreamableParser

  L 218: def parse_output_message(message: Message)

  L 324: def parse_remaining_state(parser: StreamableParser)

  L 368: def parse_output_into_messages(token_ids: Iterable[int])


============================================================
FILE: python/sglang/srt/entrypoints/http_server.py
Functions: 56
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 128: def set_global_state(global_state: _GlobalState)

  L 134: async def lifespan(fast_api_app: FastAPI)
         @asynccontextmanager

  L 212: async def validation_exception_handler(request: Request, exc: HTTPException)
         📝 Enrich HTTP exception with status code and other details
         @app.exception_handler(HTTPException)

  L 225: async def validation_exception_handler(request: Request,
        exc: RequestValidationError)
         📝 Override FastAPI's default 422 validation error with 400
         @app.exception_handler(RequestValidationError)

  L 247: async def validate_json_request(raw_request: Request)
         📝 Validate that the request content-type is application/json.

  L 268: async def health_generate(request: Request)
         → Response
         📝 Check the health of the inference server by sending a special request to generate one token.
            If the server is running something, this request will be ignored, so it creates zero overhead.
            If the server is not running anything, this request will be run, so we know whether the server is healthy.
         @app.get('/health')
         @app.get('/health_generate')

  L 339: async def get_model_info()
         📝 Get the model information.
         @app.get('/get_model_info')

  L 352: async def get_weight_version()
         📝 Get the current weight version.
         @app.get('/get_weight_version')

  L 360: async def get_server_info()
         @app.get('/get_server_info')

  L 374: async def get_load()
         @app.get('/get_load')

  L 381: async def set_internal_state(obj: SetInternalStateReq, request: Request)
         @app.api_route('/set_internal_state', methods=['POST', 'PUT'])

  L 388: async def generate_request(obj: GenerateReqInput, request: Request)
         📝 Handle a generate request.
         @app.api_route('/generate', methods=['POST', 'PUT'])

  L 425: async def generate_from_file_request(file: UploadFile, request: Request)
         📝 Handle a generate request, this is purely to work with input_embeds.
         @app.api_route('/generate_from_file', methods=['POST'])

  L 449: async def encode_request(obj: EmbeddingReqInput, request: Request)
         📝 Handle an embedding request.
         @app.api_route('/encode', methods=['POST', 'PUT'])

  L 461: async def classify_request(obj: EmbeddingReqInput, request: Request)
         📝 Handle a reward model request. Now the arguments and return values are the same as embedding models.
         @app.api_route('/classify', methods=['POST', 'PUT'])

  L 473: async def flush_cache()
         📝 Flush the radix cache.
         @app.api_route('/flush_cache', methods=['GET', 'POST'])

  L 484: async def clear_hicache_storage_backend()
         📝 Clear the hierarchical cache storage backend.
         @app.api_route('/clear_hicache_storage_backend', methods=['GET', 'POST'])

  L 494: async def start_profile_async(obj: Optional[ProfileReqInput])
         📝 Start profiling.
         @app.api_route('/start_profile', methods=['GET', 'POST'])

  L 515: async def stop_profile_async()
         📝 Stop profiling.
         @app.api_route('/stop_profile', methods=['GET', 'POST'])

  L 525: async def freeze_gc_async()
         📝 See engine.freeze_gc for more details.
         @app.api_route('/freeze_gc', methods=['GET', 'POST'])

  L 537: async def start_expert_distribution_record_async()
         📝 Start recording the expert distribution. Clear the previous record if any.
         @app.api_route('/start_expert_distribution_record', methods=['GET', 'POST'])

  L 547: async def stop_expert_distribution_record_async()
         📝 Stop recording the expert distribution.
         @app.api_route('/stop_expert_distribution_record', methods=['GET', 'POST'])

  L 557: async def dump_expert_distribution_record_async()
         📝 Dump expert distribution record.
         @app.api_route('/dump_expert_distribution_record', methods=['GET', 'POST'])

  L 567: async def update_weights_from_disk(obj: UpdateWeightFromDiskReqInput,
        request: Request)
         📝 Update the weights from disk inplace without re-launching the server.
         @app.post('/update_weights_from_disk')

  L 596: async def init_weights_update_group(obj: InitWeightsUpdateGroupReqInput,
        request: Request)
         📝 Initialize the parameter update group.
         @app.post('/init_weights_update_group')

  L 611: async def update_weights_from_tensor(obj: UpdateWeightsFromTensorReqInput,
        request: Request)
         📝 Update the weights from tensor inplace without re-launching the server.
            Notes:
            1. Ensure that the model is on the correct device (e.g., GPU) before calling this endpoint. If the model is moved to the CPU unexpectedly, it may cause performance issues or runtime errors.
            2. HTTP will transmit only the metadata of the tensor, while the tensor itself will be directly copied to the model.
            3. Any binary data in the named tensors should be base64 encoded.
         @app.post('/update_weights_from_tensor')

  L 637: async def update_weights_from_distributed(obj: UpdateWeightsFromDistributedReqInput,
        request: Request)
         📝 Update model parameter from distributed online.
         @app.post('/update_weights_from_distributed')

  L 660: async def update_weight_version(obj: UpdateWeightVersionReqInput,
        request: Request)
         📝 Update the weight version. This operation requires no active requests.
         @app.post('/update_weight_version')

  L 690: async def get_weights_by_name(obj: GetWeightsByNameReqInput, request: Request)
         📝 Get model parameter by name.
         @app.api_route('/get_weights_by_name', methods=['GET', 'POST'])

  L 703: async def release_memory_occupation(obj: ReleaseMemoryOccupationReqInput,
        request: Request)
         📝 Release GPU memory occupation temporarily.
         @app.api_route('/release_memory_occupation', methods=['GET', 'POST'])

  L 714: async def resume_memory_occupation(obj: ResumeMemoryOccupationReqInput,
        request: Request)
         📝 Resume GPU memory occupation.
         @app.api_route('/resume_memory_occupation', methods=['GET', 'POST'])

  L 725: async def slow_down(obj: SlowDownReqInput, request: Request)
         📝 Slow down the system deliberately. Only for testing. Example scenario:
            when we want to test performance of D in large-scale PD disaggregation and have no enough nodes for P,
            we can use this to slow down D to let it have enough running sequences, and then disable slowdown
            to let it run in full batch size.
         @app.api_route('/slow_down', methods=['GET', 'POST'])

  L 738: async def load_lora_adapter(obj: LoadLoRAAdapterReqInput, request: Request)
         📝 Load a new LoRA adapter without re-launching the server.
         @app.api_route('/load_lora_adapter', methods=['POST'])

  L 755: async def unload_lora_adapter(obj: UnloadLoRAAdapterReqInput, request: Request)
         📝 Load a new LoRA adapter without re-launching the server.
         @app.api_route('/unload_lora_adapter', methods=['POST'])

  L 772: async def open_session(obj: OpenSessionReqInput, request: Request)
         📝 Open a session, and return its unique session id.
         @app.api_route('/open_session', methods=['GET', 'POST'])

  L 786: async def close_session(obj: CloseSessionReqInput, request: Request)
         📝 Close the session.
         @app.api_route('/close_session', methods=['GET', 'POST'])

  L 796: async def configure_logging(obj: ConfigureLoggingReq, request: Request)
         📝 Configure the request logging options.
         @app.api_route('/configure_logging', methods=['GET', 'POST'])

  L 803: async def abort_request(obj: AbortReq, request: Request)
         📝 Abort a request.
         @app.post('/abort_request')

  L 815: async def parse_function_call_request(obj: ParseFunctionCallReq,
        request: Request)
         📝 A native API endpoint to parse function calls from a text.
         @app.post('/parse_function_call')

  L 837: async def separate_reasoning_request(obj: SeparateReasoningReqInput,
        request: Request)
         📝 A native API endpoint to separate reasoning from a text.
         @app.post('/separate_reasoning')

  L 857: async def pause_generation(request: Request)
         📝 Pause generation.
         @app.post('/pause_generation')

  L 867: async def continue_generation(request: Request)
         📝 Continue generation.
         @app.post('/continue_generation')

  L 880: async def openai_v1_completions(request: CompletionRequest,
        raw_request: Request)
         📝 OpenAI-compatible text completion endpoint.
         @app.post('/v1/completions', dependencies=[Depends(validate_json_request)])

  L 888: async def openai_v1_chat_completions(request: ChatCompletionRequest,
        raw_request: Request)
         📝 OpenAI-compatible chat completion endpoint.
         @app.post('/v1/chat/completions', dependencies=[Depends(validate_json_request)])

  L 902: async def openai_v1_embeddings(request: EmbeddingRequest, raw_request: Request)
         📝 OpenAI-compatible embeddings endpoint.
         @app.post('/v1/embeddings', response_class=ORJSONResponse, dependencies=[Depends(validate_json_request)])

  L 910: async def available_models()
         📝 Show available models. OpenAI-compatible endpoint.
         @app.get('/v1/models', response_class=ORJSONResponse)

  L 926: async def retrieve_model(model: str)
         📝 Retrieves a model instance, providing basic information about the model.
         @app.get('/v1/models/{model:path}', response_class=ORJSONResponse)

  L 951: async def v1_score_request(request: ScoringRequest, raw_request: Request)
         📝 Endpoint for the decoder-only scoring API. See Engine.score() for detailed documentation.
         @app.post('/v1/score', dependencies=[Depends(validate_json_request)])

  L 959: async def v1_responses_request(request: dict, raw_request: Request)
         📝 Endpoint for the responses API with reasoning support.
         @app.post('/v1/responses', dependencies=[Depends(validate_json_request)])

  L 979: async def v1_retrieve_responses(response_id: str, raw_request: Request)
         📝 Retrieve a response by ID.
         @app.get('/v1/responses/{response_id}')

  L 987: async def v1_cancel_responses(response_id: str, raw_request: Request)
         📝 Cancel a background response.
         @app.post('/v1/responses/{response_id}/cancel')

  L 997: async def v1_rerank_request(request: V1RerankReqInput, raw_request: Request)
         📝 Endpoint for reranking documents based on query relevance.
         @app.api_route('/v1/rerank', methods=['POST', 'PUT'], dependencies=[Depends(validate_json_request)])

  L1006: async def sagemaker_health()
         → Response
         📝 Check the health of the http server.
         @app.get('/ping')

  L1012: async def sagemaker_chat_completions(request: ChatCompletionRequest,
        raw_request: Request)
         📝 OpenAI-compatible chat completion endpoint.
         @app.post('/invocations')

  L1023: async def vertex_generate(vertex_req: VertexGenerateReqInput,
        raw_request: Request)
         @app.post(os.environ.get('AIP_PREDICT_ROUTE', '/vertex_generate'))

  L1061: def launch_server(server_args: ServerArgs,
        pipe_finish_writer: Optional[multiprocessing.connection.Connection],
        launch_callback: Optional[Callable[[],
        None]])
         📝 Launch SRT (SGLang Runtime) Server.
            The SRT server consists of an HTTP server and an SRT engine.
            - HTTP server: A FastAPI server that routes requests to the engine.
            - The engine consists of three components:
            1. TokenizerManager: Tokenizes the requests and sends them to the scheduler.
            2. Scheduler (subprocess): Receives requests from the Tokenizer Manager, schedules batches, forwards them, and sends the output tokens to the Detokenizer Manager.
            3. DetokenizerManager (subprocess): Detokenizes the output tokens and sends the result back to the Tokenizer Manager.
            Note:
            1. The HTTP server, Engine, and TokenizerManager both run in the main process.
            2. Inter-process communication is done through IPC (each process uses a different port) via the ZMQ library.


============================================================
FILE: python/sglang/srt/entrypoints/http_server_engine.py
Functions: 8
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  20: def launch_server_process(server_args: ServerArgs)
         → multiprocessing.Process


CLASS: HttpServerEngineAdapter
----------------------------------------
  L  58: __init__(self)

  L  78: update_weights_from_tensor(self, named_tensors: List[Tuple[str, torch.Tensor]], load_format: Optional[str], flush_cache: bool)
         📝 Update model weights from tensor data. The HTTP server will only post meta data, and the real weights will be copied directly from GPUs.
            Note: The model should be on GPUs rather than CPU for this functionality to work properly.
            If you encounter issues, ensure your model is loaded on GPU devices rather than CPU.

  L 102: shutdown(self)

  L 105: generate(self, prompt, sampling_params, input_ids, image_data, return_logprob, logprob_start_len, top_logprobs_num, token_ids_logprob, lora_path, custom_logit_processor)

  L 135: release_memory_occupation(self)

  L 138: resume_memory_occupation(self)

  L 141: flush_cache(self)


============================================================
FILE: python/sglang/srt/entrypoints/tool.py
Functions: 7
============================================================


CLASS: HarmonyBrowserTool
----------------------------------------
  L  25: __init__(self)

  L  45: get_result(self, context: 'ConversationContext')
         → Any

  L  56: tool_config(self)
         → Any


CLASS: HarmonyPythonTool
----------------------------------------
  L  62: __init__(self)

  L  75: get_result(self, context: 'ConversationContext')
         → Any

  L  86: tool_config(self)
         → Any


CLASS: Tool
----------------------------------------
  L  19: get_result(self, context: 'ConversationContext')
         → Any
