
# python/sglang/srt/layers/moe/fused_moe_triton/__init__.py
override_config(config)
get_config() -> Optional[Dict[str, Any]]

# python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
write_zeros_to_output(c_ptr, stride_cm, stride_cn, pid_n, N, offs_token, token_mask, BLOCK_SIZE_M, BLOCK_SIZE_N, compute_type)
fused_moe_kernel_gptq_awq(a_ptr, b_ptr, c_ptr, b_scale_ptr, b_zp_ptr, topk_weights_ptr, sorted_token_ids_ptr, expert_ids_ptr, num_tokens_post_padded_ptr, N: tl.constexpr, K: tl.constexpr, EM, num_valid_tokens, stride_am, stride_ak, stride_be, stride_bk, stride_bn, stride_cm, stride_cn, stride_bse, stride_bsk, stride_bsn, stride_bze, stride_bzk, stride_bzn, group_size: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, MUL_ROUTED_WEIGHT: tl.constexpr, top_k: tl.constexpr, compute_type: tl.constexpr, has_zp: tl.constexpr, use_int4_w4a16: tl.constexpr, use_int8_w8a16: tl.constexpr, even_Ks: tl.constexpr)
fused_moe_kernel(a_ptr, b_ptr, bias_ptr, c_ptr, a_scale_ptr, b_scale_ptr, topk_weights_ptr, sorted_token_ids_ptr, expert_ids_ptr, num_tokens_post_padded_ptr, N, K, EM, num_valid_tokens, stride_am, stride_ak, stride_be, stride_bk, stride_bn, stride_bias_e, stride_bias_n, stride_cm, stride_cn, stride_asm, stride_ask, stride_bse, stride_bsk, stride_bsn, group_n: tl.constexpr, group_k: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, MUL_ROUTED_WEIGHT: tl.constexpr, top_k: tl.constexpr, compute_type: tl.constexpr, use_fp8_w8a8: tl.constexpr, use_int8_w8a8: tl.constexpr, use_int8_w8a16: tl.constexpr, per_channel_quant: tl.constexpr, even_Ks: tl.constexpr)
moe_align_block_size(topk_ids: torch.Tensor, block_size: int, num_experts: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
invoke_fused_moe_kernel(A: torch.Tensor, B: torch.Tensor, bias: Optional[torch.Tensor], C: torch.Tensor, A_scale: Optional[torch.Tensor], B_scale: Optional[torch.Tensor], B_zp: Optional[torch.Tensor], topk_weights: torch.Tensor, topk_ids: torch.Tensor, sorted_token_ids: torch.Tensor, expert_ids: torch.Tensor, num_tokens_post_padded: torch.Tensor, mul_routed_weight: bool, top_k: int, config: Dict[str, Any], compute_type: tl.dtype, use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, block_shape: Optional[List[int]], no_combine: bool) -> None
get_config_file_name(E: int, N: int, dtype: Optional[str], block_shape: Optional[int]) -> str
get_moe_configs(E: int, N: int, dtype: Optional[str], block_n: Optional[int], block_k: Optional[int]) -> Optional[Dict[int, Any]]
get_default_config(M: int, E: int, N: int, K: int, topk: int, dtype: Optional[str], is_marlin: bool, block_shape: Optional[List[int]]) -> Dict[str, int]
try_get_optimal_moe_config(w1_shape: Tuple[int, ...], w2_shape: Tuple[int, ...], top_k: int, dtype: Optional[str], M: int, is_marlin: bool, block_shape: Optional[List[int]])
get_config_dtype_str(dtype: torch.dtype, use_int8_w8a16: Optional[bool], use_int4_w4a16: Optional[bool], use_fp8_w8a8: Optional[bool], use_int8_w8a8: Optional[bool])
inplace_fused_experts(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]], routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_limit: Optional[float]) -> None
inplace_fused_experts_fake(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]], routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_limit: Optional[float]) -> None
outplace_fused_experts(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]], no_combine: bool, routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_limit: Optional[float]) -> torch.Tensor
outplace_fused_experts_fake(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]], no_combine: bool, routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_limit: Optional[float]) -> torch.Tensor
fused_experts(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]])
moe_sum_reduce_triton(input: torch.Tensor, output: torch.Tensor, routed_scaling_factor: float)
moe_sum_reduce_torch_compile(x, out, routed_scaling_factor)
swiglu_with_alpha_and_limit(x, gemm1_alpha, gemm1_limit)
fused_experts_impl(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], inplace: bool, activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]], no_combine: bool, routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_limit: Optional[float])
fused_moe(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]]) -> torch.Tensor

# python/sglang/srt/layers/moe/fused_moe_triton/layer.py
  FusedMoE.__init__(num_experts: int, hidden_size: int, intermediate_size: int, layer_id: int, top_k: Optional[int], num_fused_shared_experts: int, params_dtype: Optional[torch.dtype], reduce_results: bool, quant_config: Optional[QuantizationConfig], prefix: str, activation: str, apply_router_weight_on_input: bool, use_presharded_weights: bool, inplace: bool, no_combine: bool, routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_clamp_limit: Optional[float], use_weight_loader_fused: bool, with_bias)
  FusedMoE.weight_loader(param: torch.nn.Parameter, loaded_weight: torch.Tensor, weight_name: str, shard_id: str, expert_id: Optional[int]) -> None
  FusedMoE.weight_loader_fused(param: torch.nn.Parameter, loaded_weight: torch.Tensor, weight_name: str, shard_id: str) -> None
  FusedMoE.forward(hidden_states: torch.Tensor, topk_output: TopKOutput)
  FusedMoE.make_expert_params_mapping(cls, ckpt_gate_proj_name: str, ckpt_down_proj_name: str, ckpt_up_proj_name: str, num_experts: int) -> List[Tuple[str, str, int, str]]
  FusedMoE.make_expert_params_mapping_fused(cls, ckpt_gate_up_proj_name: str, ckpt_down_proj_name: str, ckpt_gate_up_proj_bias_name: str, ckpt_down_proj_bias_name: str)
  FusedMoE.make_expert_params_mapping_fused_mxfp4(cls, ckpt_gate_up_proj_name: str, ckpt_down_proj_name: str, ckpt_gate_up_proj_bias_name: str, ckpt_down_proj_bias_name: str, ckpt_gate_up_proj_scale_name: str, ckpt_down_proj_scale_name: str)
  FusedMoE.make_expert_input_scale_params_mapping(cls, num_experts: int) -> List[Tuple[str, str, int, str]]
  FusedMoE.should_fuse_routed_scaling_factor_in_topk()
  FlashInferFusedMoE.__init__()
  FlashInferFusedMoE.forward(hidden_states: torch.Tensor, topk_output: TopKOutput)
  FlashInferFP4MoE.__init__()
  FlashInferFP4MoE.forward(hidden_states: torch.Tensor, topk_output: TopKOutput)
get_fused_moe_impl_class()

# python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py
quantize(w, dtype, dev)
triton_kernel_moe_forward(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, per_channel_quant: bool, global_num_experts: int, expert_map: Optional[torch.Tensor], w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[list[int]]) -> torch.Tensor
triton_kernel_fused_experts(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, routing_data: RoutingData, gather_indx: GatherIndx, scatter_indx: ScatterIndx, inplace: bool, activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, per_channel_quant: bool, global_num_experts: int, expert_map: Optional[torch.Tensor], w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[list[int]]) -> torch.Tensor
triton_kernel_moe_with_bias_forward(hidden_states: torch.Tensor, w1: torch.Tensor, w1_pcg, b1: torch.Tensor, w2: torch.Tensor, w2_pcg, b2: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig, use_fp8_w8a8: bool, per_channel_quant: bool, global_num_experts: int, expert_map: Optional[torch.Tensor], w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[list[int]]) -> torch.Tensor
triton_kernel_fused_experts_with_bias(hidden_states: torch.Tensor, w1: torch.Tensor, w1_pcg, b1: torch.Tensor, w2: torch.Tensor, w2_pcg, b2: torch.Tensor, routing_data: RoutingData, gather_indx: GatherIndx, scatter_indx: ScatterIndx, inplace: bool, activation: str, use_fp8_w8a8: bool, per_channel_quant: bool, global_num_experts: int, expert_map: Optional[torch.Tensor], w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[list[int]], gemm1_alpha: Optional[float], gemm1_clamp_limit: Optional[float]) -> torch.Tensor
