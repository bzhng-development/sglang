================================================================================
FUNCTION INDEX: device_communicators module
================================================================================
Total Functions: 115
Documented: 19


============================================================
FILE: python/sglang/srt/distributed/device_communicators/cuda_wrapper.py
Functions: 13
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  37: def find_loaded_library(lib_name)
         ‚Üí Optional[str]
         üìù According to according to https://man7.org/linux/man-pages/man5/proc_pid_maps.5.html,
            the file `/proc/self/maps` contains the memory maps of the process, which includes the
            shared libraries loaded by the process. We can use this file to find the path of the
            a loaded library.


CLASS: CudaRTLibrary
----------------------------------------
  L 114: __init__(self, so_file: Optional[str])

  L 133: CUDART_CHECK(self, result: cudaError_t)
         ‚Üí None

  L 138: cudaGetErrorString(self, error: cudaError_t)
         ‚Üí str

  L 141: cudaSetDevice(self, device: int)
         ‚Üí None

  L 144: cudaDeviceSynchronize(self)
         ‚Üí None

  L 147: cudaDeviceReset(self)
         ‚Üí None

  L 150: cudaMalloc(self, size: int)
         ‚Üí ctypes.c_void_p

  L 155: cudaFree(self, devPtr: ctypes.c_void_p)
         ‚Üí None

  L 158: cudaMemset(self, devPtr: ctypes.c_void_p, value: int, count: int)
         ‚Üí None

  L 161: cudaMemcpy(self, dst: ctypes.c_void_p, src: ctypes.c_void_p, count: int)
         ‚Üí None

  L 168: cudaIpcGetMemHandle(self, devPtr: ctypes.c_void_p)
         ‚Üí cudaIpcMemHandle_t

  L 175: cudaIpcOpenMemHandle(self, handle: cudaIpcMemHandle_t)
         ‚Üí ctypes.c_void_p


============================================================
FILE: python/sglang/srt/distributed/device_communicators/custom_all_reduce.py
Functions: 13
============================================================


CLASS: CustomAllreduce
----------------------------------------
  L  66: __init__(self, group: ProcessGroup, device: Union[int, str, torch.device], max_size)
         ‚Üí None
         üìù Args:
            group: the process group to work on. If None, it will use the
            default process group.
            device: the device to bind the CustomAllreduce to. If None,
            it will be bind to f"cuda:{local_rank}".
            It is the caller's responsibility to make sure each communicator
            is bind to a unique device, and all communicators in this group
            are in the same node.

  L 215: create_shared_buffer(size_in_bytes: int, group: Optional[ProcessGroup])
         ‚Üí List[int]
         üìù Creates a shared buffer and returns a list of pointers
            representing the buffer on all processes in the group.

  L 240: free_shared_buffer(pointers: List[int], group: Optional[ProcessGroup])
         ‚Üí None

  L 248: capture(self)
         üìù The main responsibility of this context manager is the
            `register_graph_buffers` call at the end of the context.
            It records all the buffer addresses used in the CUDA graph.

  L 296: register_buffer(self, inp: torch.Tensor)

  L 300: register_graph_buffers(self)

  L 326: should_custom_ar(self, inp: torch.Tensor)

  L 351: all_reduce_reg(self, inp: torch.Tensor, out: torch.Tensor)

  L 358: all_reduce_unreg(self, inp: torch.Tensor, out: torch.Tensor)

  L 364: all_reduce(self, inp: torch.Tensor)
         üìù Performs an out-of-place all reduce.
            If registered is True, this assumes inp's pointer is already
            IPC-registered. Otherwise, inp is first copied into a pre-registered
            buffer.

  L 387: custom_all_reduce(self, input: torch.Tensor)
         ‚Üí Optional[torch.Tensor]
         üìù The main allreduce API that provides support for cuda graph.

  L 412: close(self)

  L 420: __del__(self)


============================================================
FILE: python/sglang/srt/distributed/device_communicators/custom_all_reduce_utils.py
Functions: 8
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  50: def update_environment_variables(envs: Dict[str, str])

  L  62: def producer(batch_src: Sequence[int],
        producer_queue,
        consumer_queue,
        result_queue,
        cuda_visible_devices: Optional[str])

  L  96: def consumer(batch_tgt: Sequence[int],
        producer_queue,
        consumer_queue,
        result_queue,
        cuda_visible_devices: Optional[str])

  L 137: def can_actually_p2p(batch_src: Sequence[int], batch_tgt: Sequence[int])
         ‚Üí Sequence[bool]
         üìù Usually, checking if P2P access is enabled can be done by
            `torch.cuda.can_device_access_peer(src, tgt)`. However, sometimes
            the driver might be broken, and `torch.cuda.can_device_access_peer(src, tgt)`
            returns `True` even if P2P access is not actually possible.
            See https://github.com/vllm-project/vllm/issues/2728 and
            https://forums.developer.nvidia.com/t/direct-gpu-gpu-communication-does-not-seem-to-work-properly/283264/10
            Therefore, we have to perform a real P2P access to check if it is actually
            possible.
            Note on p2p and cuda IPC:
            Usually, one process uses one GPU:
            GPU src --> cuda context src --> tensor src --> process src
            We need to combine p2p and cuda IPC, so that:
            GPU src --> cuda context src --> tensor src --> process src
            |shared|
            GPU tgt --> cuda context tgt --> tensor tgt --> process tgt
            That is to say, process src creates a tensor in GPU src, passes IPC handle to
            process tgt, and process tgt accesses the tensor in GPU tgt. Any operation on the
            tensor in process tgt will be reflected in the tensor in process src, because
            they are the same memory segment.
            It is important to note that process tgt accesses the tensor in GPU tgt, not
            GPU src. That's why we need p2p access.
            The most time-consuming part is the process creation. To avoid creating
            processes for every pair of GPUs, we use batched testing. We create two
            processes for testing all pairs of GPUs in batch. The trick is to reset
            the device after each test (which is not available in PyTorch).

  L 237: def gpu_p2p_access_check(src: int, tgt: int)
         ‚Üí bool
         üìù Check if GPU src can access GPU tgt.

  L 312: def with_nvml_context(fn: Callable[_P, _R])
         ‚Üí Callable[_P, _R]

  L 332: def is_full_nvlink(physical_device_ids: List[int], world_size: int)
         ‚Üí bool
         @with_nvml_context

  L 373: def is_weak_contiguous(inp: torch.Tensor)


============================================================
FILE: python/sglang/srt/distributed/device_communicators/hpu_communicator.py
Functions: 3
============================================================


CLASS: HpuCommunicator
----------------------------------------
  L  15: __init__(self, group: ProcessGroup)

  L  23: all_reduce(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  31: all_gather(self, x: torch.Tensor, dim: int)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/distributed/device_communicators/npu_communicator.py
Functions: 3
============================================================


CLASS: NpuCommunicator
----------------------------------------
  L  10: __init__(self, group: ProcessGroup)

  L  18: all_reduce(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  22: all_gather(self, x: torch.Tensor, dim: int)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/distributed/device_communicators/pymscclpp.py
Functions: 8
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  39: def mscclpp_is_weak_contiguous(inp: torch.Tensor)

  L  46: def mscclpp_convert_to_bytes(size_str)
         üìù Converts a human-readable size string (e.g., "1MB", "2.5kb", "3 GB")
            into the equivalent number of bytes using binary units.
            Args:
            size_str (str): A string representing size with unit (KB, MB, GB).
            Returns:
            int: Number of bytes.

  L  87: def mscclpp_bench_time(func, test_niter: int, warmup_niter: int)


CLASS: PyMscclppCommunicator
----------------------------------------
  L 111: __init__(self, group: ProcessGroup, device: Union[int, str, torch.device], max_bytes)
         ‚Üí None
         üìù Args:
            group: the process group to work on. If None, it will use the
            default process group.
            device: the device to bind the CustomAllreduce to. If None,
            it will be bind to f"cuda:{local_rank}".
            It is the caller's responsibility to make sure each communicator
            is bind to a unique device, and all communicators in this group
            are in the same node.

  L 243: pre_tune_config(self, dtype)
         ‚Üí bool

  L 273: should_mscclpp_allreduce(self, inp: torch.Tensor, op: ReduceOp)
         ‚Üí bool

  L 289: all_reduce(self, tensor: torch.Tensor, op: ReduceOp)

  L 302: change_state(self, enable: Optional[bool])


============================================================
FILE: python/sglang/srt/distributed/device_communicators/pynccl.py
Functions: 12
============================================================


CLASS: PyNcclCommunicator
----------------------------------------
  L  28: __init__(self, group: Union[ProcessGroup, StatelessProcessGroup], device: Union[int, str, torch.device], library_path: Optional[str])
         üìù Args:
            group: the process group to work on. If None, it will use the
            default process group.
            device: the device to bind the PyNcclCommunicator to. If None,
            it will be bind to f"cuda:{local_rank}".
            library_path: the path to the NCCL library. If None, it will
            use the default library path.
            It is the caller's responsibility to make sure each communicator
            is bind to a unique device.

  L 126: all_reduce(self, tensor: torch.Tensor, op: ReduceOp, stream)

  L 150: all_gather(self, output_tensor: torch.Tensor, input_tensor: torch.Tensor, stream, sizes: Optional[list[int]])

  L 196: reduce_scatter(self, output_tensor: torch.Tensor, input_tensor: torch.Tensor, op: ReduceOp, stream, sizes: Optional[list[int]])

  L 245: send(self, tensor: torch.Tensor, dst: int, stream)

  L 263: recv(self, tensor: torch.Tensor, src: int, stream)

  L 281: broadcast(self, tensor: torch.Tensor, src: int, stream)

  L 307: register_comm_window_raw(self, ptr: int, size: int)

  L 310: deregister_comm_window(self, window)

  L 313: group_start(self)

  L 316: group_end(self)

  L 320: change_state(self, enable: Optional[bool], stream: Optional[torch.cuda.Stream])
         üìù A context manager to change the state of the communicator.


============================================================
FILE: python/sglang/srt/distributed/device_communicators/pynccl_allocator.py
Functions: 7
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  34: def is_symmetric_memory_enabled()

  L  38: def set_graph_pool_id(graph_pool_id)

  L  43: def get_nccl_mem_pool()


CLASS: use_symmetric_memory
----------------------------------------
  L  67: __init__(self, group_coordinator: GroupCoordinator)

  L  81: __enter__(self)

  L 104: tag(self, tensor: torch.Tensor)

  L 109: __exit__(self, exc_type, exc_val, exc_tb)


============================================================
FILE: python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py
Functions: 22
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  37: def find_nccl_library()
         ‚Üí str
         üìù We either use the library file specified by the `SGLANG_NCCL_SO_PATH`
            environment variable, or we find the library file brought by PyTorch.
            After importing `torch`, `libnccl.so.2` or `librccl.so.1` can be
            found by `ctypes` automatically.


CLASS: NCCLLibrary
----------------------------------------
  L 332: __init__(self, so_file: Optional[str])

  L 368: ncclGetErrorString(self, result: ncclResult_t)
         ‚Üí str

  L 371: NCCL_CHECK(self, result: ncclResult_t)
         ‚Üí None

  L 376: ncclGetRawVersion(self)
         ‚Üí int

  L 382: ncclGetVersion(self)
         ‚Üí str

  L 390: ncclGetUniqueId(self)
         ‚Üí ncclUniqueId

  L 395: ncclCommInitRank(self, world_size: int, unique_id: ncclUniqueId, rank: int)
         ‚Üí ncclComm_t

  L 406: ncclAllReduce(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 427: ncclReduce(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, root: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 449: ncclReduceScatter(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 470: ncclAllGather(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 489: ncclSend(self, sendbuff: buffer_type, count: int, datatype: int, dest: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 502: ncclRecv(self, recvbuff: buffer_type, count: int, datatype: int, src: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 515: ncclBroadcast(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, root: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 531: ncclCommDestroy(self, comm: ncclComm_t)
         ‚Üí None

  L 534: ncclCommWindowRegister(self, comm: ncclComm_t, buff: buffer_type, size: int, win_flags: int)
         ‚Üí ncclWindow_t

  L 545: ncclCommWindowDeregister(self, comm: ncclComm_t, window: ncclWindow_t)
         ‚Üí None

  L 548: ncclGroupStart(self)
         ‚Üí None

  L 551: ncclGroupEnd(self)
         ‚Üí None


CLASS: ncclDataTypeEnum
----------------------------------------
  L 102: from_torch(cls, dtype: torch.dtype)
         ‚Üí int


CLASS: ncclRedOpTypeEnum
----------------------------------------
  L 134: from_torch(cls, op: ReduceOp)
         ‚Üí int


============================================================
FILE: python/sglang/srt/distributed/device_communicators/quick_all_reduce.py
Functions: 8
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  34: def qr_rocm_arch_available()


CLASS: QuickAllReduce
----------------------------------------
  L  73: __init__(self, group: ProcessGroup, device: Union[int, str, torch.device])
         ‚Üí None
         üìù Custom allreduce provides non-destructive acceleration and is
            available for CUDA and ROCm MI300 series.
            Custom quick allreduce leverages quantization for further
            acceleration on ROCm. It currently supports Q8, Q6, and Q4
            quantization formats and FP(float16, bfloat16).
            Quick allreduce is designed as a complement to custom allreduce.
            Its initialization requires even stricter conditions.
            Only the ROCm MI300 series is supported for quick allreduce at
            this time.
            Args:
            group: the process group to work on. If None, it will use the
            default process group.
            device: the device to bind the CustomAllreduce to. If None,
            it will be bind to f"cuda:{local_rank}".
            It is the caller's responsibility to make sure each communicator
            is bind to a unique device, and all communicators in this group
            are in the same node.

  L 175: init_quick_all_reduce(self)

  L 219: create_shared_buffer(self)
         üìù Creates a shared buffer for quickreduce.
            Has to be called after init_custom_qr

  L 230: should_quick_allreduce(self, inp: torch.Tensor)
         üìù Check if quickreduce is available

  L 254: quick_all_reduce(self, inp: torch.Tensor)
         üìù Performs an out-of-place custom quick all reduce.

  L 265: close(self)

  L 272: __del__(self)


============================================================
FILE: python/sglang/srt/distributed/device_communicators/shm_broadcast.py
Functions: 15
============================================================


CLASS: MessageQueue
----------------------------------------
  L 176: __init__(self, n_reader, n_local_reader, local_reader_ranks: Optional[List[int]], max_chunk_bytes: int, max_chunks: int, connect_ip: Optional[str])

  L 257: export_handle(self)
         ‚Üí Handle

  L 261: create_from_handle(handle: Handle, rank)
         ‚Üí 'MessageQueue'

  L 304: wait_until_ready(self)
         üìù This is a collective operation. All processes (including the
            readers and the writer) should call this function.

  L 338: acquire_write(self)

  L 391: acquire_read(self)

  L 434: enqueue(self, obj)

  L 449: dequeue(self)

  L 468: broadcast_object(self, obj)

  L 476: create_from_process_group(pg: ProcessGroup, max_chunk_bytes, max_chunks, writer_rank)
         ‚Üí 'MessageQueue'


CLASS: ShmRingBuffer
----------------------------------------
  L  36: __init__(self, n_reader: int, max_chunk_bytes: int, max_chunks: int, name: Optional[str])
         üìù A shared memory ring buffer implementation for broadcast communication.
            Essentially, it is a queue where only one will `enqueue` and multiple
            will `dequeue`. The max size of each item, together with the max number
            of items that can be stored in the buffer are known in advance.
            In this case, we don't need to synchronize the access to
            the buffer.
            Buffer memory layout:
            data                                 metadata
            |                                      |
            | (current_idx)                        | (current_idx)
            v                                      v
            +-------------------------------+----------------------------------------+
            | chunk0 | chunk1 | ... | chunk | metadata0 | metadata1 | ... | metadata |
            +-------------------------------+----------------------------------------+
            | max_chunks x max_chunk_bytes  | max_chunks x (1 + n_reader) bytes      |
            metadata memory layout: each byte is a flag, the first byte is the written
            flag, and the rest are reader flags. The flags are set to 0 by default.
            +--------------+--------------+--------------+-----+--------------+
            | written_flag | reader0_flag | reader1_flag | ... | readerN_flag |
            +--------------+--------------+--------------+-----+--------------+
            The state of metadata is as follows:
            (case 1) 0???...???: the block is not written yet, cannot read, can write
            (case 2) 1000...000: the block is just written, can read, cannot write
            (case 3) 1???...???: the block is written and read by some readers, can read if not read, cannot write
            (case 4) 1111...111: the block is written and read by all readers, cannot read, can write
            State transition for readers:
            When a reader finds a block that it can read (case 2 or 3), it can yield the block for caller to read.
            Only after the caller finishes reading the block, the reader can mark the block as read.
            Readers only mark the block as read (from 0 to 1), the writer marks the block as ready to read (from 1 to 0).
            State transition for writer:
            When the writer writes to a block (case 1 or 4), it first resets the written flag to 0, converting either case
            to case 1. Then it can yield the block for caller to write. After the caller finishes writing the block, the writer
            can reset the reader flags to 0, and mark the block as written (from 0 to 1).
            NOTE: the order is important here, first reset the reader flags (so that we are still in case 1), then mark the block as written. The state transition is atomic. If we do it in the reverse order, it will go through case 3 and then back to case 2, and readers might read the intermediate case 3, which is not correct.
            During creation, `name` is None and the buffer is created. We can pass the
            created object to other processes by pickling it. The other processes will
            get the name of the shared memory and open it, so that they can access the
            same shared memory buffer.

  L 132: __reduce__(self)

  L 143: __del__(self)

  L 150: get_data(self, current_idx: int)

  L 157: get_metadata(self, current_idx: int)


============================================================
FILE: python/sglang/srt/distributed/device_communicators/xpu_communicator.py
Functions: 3
============================================================


CLASS: XpuCommunicator
----------------------------------------
  L  12: __init__(self, group: ProcessGroup)

  L  20: all_reduce(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  24: gather(self, input_: torch.Tensor, rank_in_group: int, dst: int, dim: int)
