
# python/sglang/srt/layers/quantization/deep_gemm_wrapper/compile_utils.py
update_deep_gemm_config(gpu_id: int, server_args: ServerArgs)
  _BaseWarmupExecutor.create(kernel_type: DeepGemmKernelType)
  _BaseWarmupExecutor.execute(m)
  _NormalWarmupExecutor.__init__(max_m: int, n: int, k: int, num_groups: int)
  _NormalWarmupExecutor.execute(m)
  _GroupedContWarmupExecutor.__init__(max_m: int, n: int, k: int, num_groups: int)
  _GroupedContWarmupExecutor.execute(m)
  _GroupedMaskedWarmupExecutor.__init__(max_m: int, n: int, k: int, num_groups: int)
  _GroupedMaskedWarmupExecutor.execute(m)
deep_gemm_execution_hook(m: int, n: int, k: int, num_groups: int, kernel_type: DeepGemmKernelType)

# python/sglang/srt/layers/quantization/deep_gemm_wrapper/entrypoint.py
grouped_gemm_nt_f8f8bf16_masked(lhs: Tuple[torch.Tensor, torch.Tensor], rhs: Tuple[torch.Tensor, torch.Tensor], out: torch.Tensor, masked_m: torch.Tensor, expected_m: int)
grouped_gemm_nt_f8f8bf16_contig(lhs: Tuple[torch.Tensor, torch.Tensor], rhs: Tuple[torch.Tensor, torch.Tensor], out: torch.Tensor, m_indices: torch.Tensor)
gemm_nt_f8f8bf16(lhs: Tuple[torch.Tensor, torch.Tensor], rhs: Tuple[torch.Tensor, torch.Tensor], out: torch.Tensor)
update_deep_gemm_config(gpu_id: int, server_args: ServerArgs)
configure_deep_gemm_num_sms(num_sms)
