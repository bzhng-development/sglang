================================================================================
FUNCTION INDEX: test module
================================================================================
Total Functions: 216
Documented: 31


============================================================
FILE: python/sglang/test/doc_patch.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  23: def patched_post_init(self)

  L  37: def launch_server_cmd(command: str, host: str, port: int)
         üìù Launch the server using the given command.
            If no port is specified, a free port is reserved.


============================================================
FILE: python/sglang/test/few_shot_gsm8k.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  22: def get_one_example(lines, i, include_answer)

  L  29: def get_few_shot_examples(lines, k)

  L  36: def get_answer_value(answer_str)

  L  47: def run_eval(args)


============================================================
FILE: python/sglang/test/few_shot_gsm8k_engine.py
Functions: 5
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  18: def get_one_example(lines, i, include_answer)

  L  25: def get_few_shot_examples(lines, k)

  L  32: def get_answer_value(answer_str)

  L  43: async def concurrent_generate(engine, prompts, sampling_param)

  L  52: def run_eval(args)


============================================================
FILE: python/sglang/test/run_eval.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  18: def run_eval(args)


============================================================
FILE: python/sglang/test/runners.py
Functions: 22
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  69: def get_dtype_str(torch_dtype)

  L  78: def get_top_logprobs(logits, k)

  L  85: def get_token_ids_logprobs(logits, token_ids)

  L 809: def monkey_patch_gemma2_sdpa()
         üìù Use sdpa by default to fix the OOM issue.
            Revert this commit:
            https://github.com/huggingface/transformers/commit/975b988bfe6e7ebb47390cd9a1556c6888804883#diff-5f76eac6f18f4b491521314c318a9692318feb4d19228e9576cce7bde4240834R660

  L 824: def check_close_model_outputs(hf_outputs: ModelOutput,
        srt_outputs: ModelOutput,
        prefill_tolerance: float,
        decode_tolerance: float,
        rouge_l_tolerance: float,
        debug_text: str,
        check_logprobs: bool)


CLASS: HFRunner
----------------------------------------
  L 130: __init__(self, model_path: str, torch_dtype: torch.dtype, model_type: str, output_str_only: bool, trust_remote_code: bool, patch_model_do_sample_false: bool)

  L 158: needs_trust_remote_code(self, model_path)

  L 228: start_model_process(self, in_queue, out_queue, model_path, torch_dtype)

  L 353: forward(self, prompts: Union[List[List[str]], List[str], List[torch.Tensor]], image_data: Optional[List[str]], max_new_tokens: int, lora_paths: Optional[List[str]], token_ids_logprob: Optional[int])

  L 368: terminate(self)

  L 372: __enter__(self)

  L 375: __exit__(self, exc_type, exc_value, traceback)

  L 380: forward_generation_raw(base_model, prompts: Union[List[str], List[torch.Tensor]], max_new_tokens: int, tokenizer, torch_dtype: torch.dtype, lora_paths: Optional[List[str]], output_str_only: bool, token_ids_logprob: Optional[int], patch_model_do_sample_false: Optional[bool])
         ‚Üí ModelOutput


CLASS: SRTRunner
----------------------------------------
  L 486: __init__(self, model_path: str, torch_dtype: torch.dtype, model_type: str, tp_size: int, model_impl: str, port: int, lora_paths: Optional[Union[List[str], List[dict[str, str]]]], max_loras_per_batch: int, attention_backend: Optional[str], prefill_attention_backend: Optional[str], decode_attention_backend: Optional[str], lora_backend: str, disable_cuda_graph: bool, disable_radix_cache: bool, chunked_prefill_size: Optional[int], dp_size: int, tokenizer_path: Optional[str], mem_fraction_static: float, trust_remote_code: bool, speculative_draft_model_path: Optional[str], speculative_algorithm: Optional[str], speculative_num_steps: Optional[int], speculative_eagle_topk: Optional[int], speculative_num_draft_tokens: Optional[int], disable_overlap_schedule: bool, disable_custom_all_reduce: bool, torchao_config: Optional[str], cuda_graph_max_bs: int, sleep_on_idle, max_lora_rank: Optional[int], lora_target_modules: Optional[List[str]], enable_lora: Optional[bool], max_loaded_loras: Optional[int])

  L 574: load_lora_adapter(self, lora_name: str, lora_path: str, pinned: bool)

  L 577: unload_lora_adapter(self, lora_name: str)

  L 580: forward(self, prompts: Union[List[List[str]], List[str], List[torch.Tensor]], image_data: Optional[List[str]], max_new_tokens: int, lora_paths: Optional[List[str]], logprob_start_len: int, top_k: Optional[int], token_ids_logprob: Optional[List[int]])

  L 623: batch_forward(self, prompts: Union[List[str], List[torch.Tensor]], image_data: Optional[List[str]], max_new_tokens, lora_paths)
         üìù testing serving by sending all prompts once
            only return output strings and no logprobs

  L 650: __enter__(self)

  L 653: __exit__(self, exc_type, exc_value, traceback)

  L 658: forward_generation_raw(engine: Engine, prompts: Union[List[str], List[torch.Tensor]], max_new_tokens: int, lora_paths: Optional[List[str]], logprob_start_len: int, top_k: Optional[int], token_ids_logprob: Optional[List[int]])

  L 788: batch_forward_generation_raw(prompts: Union[List[str], List[torch.Tensor]], max_new_tokens, lora_paths, engine)


============================================================
FILE: python/sglang/test/send_one.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  61: def send_one_prompt(args)


CLASS: BenchArgs
----------------------------------------
  L  34: add_cli_args(parser: argparse.ArgumentParser)

  L  56: from_cli_args(cls, args: argparse.Namespace)


============================================================
FILE: python/sglang/test/simple_eval_common.py
Functions: 14
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 254: def format_multichoice_question(row)

  L 258: def check_equality(sampler: SamplerBase, expr1: str, expr2: str)

  L 277: def aggregate_results(single_eval_results: List[SingleEvalResult],
        default_stats: Tuple[str],
        name2stats: Optional[Dict[str,
        Tuple[str]]])
         ‚Üí EvalResult
         üìù Aggregate results from multiple evaluations into a single EvalResult.

  L 310: def map_with_progress(f: callable, xs: List[Any], num_threads: int)
         üìù Apply f to each element of xs, using a ThreadPool, and show progress.

  L 339: def message_to_html(message: Message)
         ‚Üí str
         üìù Generate HTML snippet (inside a <div>) for a message.

  L 419: def make_report(eval_result: EvalResult)
         ‚Üí str
         üìù Create a standalone HTML report from an EvalResult.

  L 430: def make_report_from_example_htmls(htmls: List[str])
         üìù Create a standalone HTML report from a list of example htmls

  L 439: def download_dataset(path, url)

  L 464: def set_ulimit(target_soft_limit)


CLASS: ChatCompletionSampler
----------------------------------------
  L  88: __init__(self, base_url: str, model: Optional[str], system_message: Optional[str], temperature: float, reasoning_effort: Optional[str], max_tokens: int)

  L 133: __call__(self, message_list: MessageList)
         ‚Üí str


CLASS: Eval
----------------------------------------
  L  69: __call__(self, sampler: SamplerBase)
         ‚Üí EvalResult


CLASS: LargerHttpxClient
----------------------------------------
  L  74: __init__(self)


CLASS: SamplerBase
----------------------------------------
  L  36: __call__(self, message_list: MessageList)
         ‚Üí str


============================================================
FILE: python/sglang/test/simple_eval_gpqa.py
Functions: 2
============================================================


CLASS: GPQAEval
----------------------------------------
  L  29: __init__(self, filename: str, num_examples: Optional[int], num_threads: int, n_repeats: int)

  L  50: __call__(self, sampler: SamplerBase)
         ‚Üí EvalResult


============================================================
FILE: python/sglang/test/simple_eval_humaneval.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  34: def evaluate_functional_correctness(sample: Dict[str,
        str],
        completions: List[str],
        n_workers: int,
        timeout: float)
         üìù Evaluates the functional correctness of generated samples, and writes
            results to f"{sample_file}_results.jsonl.gz"


CLASS: HumanEval
----------------------------------------
  L  62: __init__(self, num_examples: Optional[int], num_threads: int, num_samples_per_task: int, ks_passes: List[int], timeout: int)

  L  82: __call__(self, sampler: SamplerBase)
         ‚Üí EvalResult


============================================================
FILE: python/sglang/test/simple_eval_math.py
Functions: 2
============================================================


CLASS: MathEval
----------------------------------------
  L  36: __init__(self, filename: str, equality_checker: SamplerBase, num_examples: Optional[int], num_threads: int)

  L  51: __call__(self, sampler: SamplerBase)
         ‚Üí EvalResult


============================================================
FILE: python/sglang/test/simple_eval_mgsm.py
Functions: 6
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  91: def parse_answer(answer: str, answer_prefix: str)
         ‚Üí str

  L 105: def score_mgsm(target: str, prediction: str)
         ‚Üí bool

  L 115: def get_lang_examples(lang: str)
         ‚Üí list[dict[str, str]]

  L 128: def get_all_examples()
         ‚Üí list[dict[str, str]]


CLASS: MGSMEval
----------------------------------------
  L 138: __init__(self, num_examples_per_lang: int, num_threads: int, languages: Optional[list[str]])

  L 163: __call__(self, sampler: SamplerBase)
         ‚Üí EvalResult


============================================================
FILE: python/sglang/test/simple_eval_mmlu.py
Functions: 2
============================================================


CLASS: MMLUEval
----------------------------------------
  L  88: __init__(self, filename: str, num_examples: Optional[int], num_threads: int)

  L  96: __call__(self, sampler: SamplerBase)
         ‚Üí EvalResult


============================================================
FILE: python/sglang/test/test_activation.py
Functions: 4
============================================================


CLASS: TestGeluAndMul
----------------------------------------
  L  20: setUpClass(cls)

  L  42: test_gelu_and_mul(self)


CLASS: TestQuickGELU
----------------------------------------
  L  65: setUpClass(cls)

  L  91: test_quick_gelu(self)


============================================================
FILE: python/sglang/test/test_block_fp8.py
Functions: 19
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  24: def native_per_token_group_quant_fp8(x, group_size, eps, dtype)
         üìù Function to perform per-token-group quantization on an input tensor `x` using native torch.
            It converts the tensor values into float8 values and returns the
            quantized tensor along with the scaling factor used for quantization.
            Note that only `torch.float8_e4m3fn` is supported for now.

  L  98: def native_static_quant_fp8(x, x_s, dtype)
         üìù Function to perform static quantization on an input tensor `x` using native torch.
            It converts the tensor values into float8 values and returns the
            quantized tensor along with the scaling factor used for quantization.

  L 274: def native_w8a8_block_fp8_matmul(A, B, As, Bs, block_size, output_dtype)
         üìù This function performs matrix multiplication with block-wise quantization using native torch.
            It takes two input tensors `A` and `B` with scales `As` and `Bs`.
            The output is returned in the specified `output_dtype`.

  L 419: def torch_w8a8_block_fp8_moe(a, w1, w2, w1_s, w2_s, score, topk, block_shape)
         üìù This function performs fused moe with block-wise quantization using native torch.

  L 551: def torch_w8a8_block_fp8_bmm(a, a_s, w, w_s, block_shape, out_dtype)
         üìù This function performs bmm with block-wise quantization using native torch.


CLASS: TestPerTensorQuantMlaFP8
----------------------------------------
  L 171: setUpClass(cls)

  L 197: test_per_tensor_quant_mla_fp8(self)


CLASS: TestPerTokenGroupQuantFP8
----------------------------------------
  L  60: setUpClass(cls)

  L  79: test_per_token_group_quant_fp8(self)


CLASS: TestPerTokenGroupQuantMlaDeepGemmMaskedFP8
----------------------------------------
  L 226: setUpClass(cls)

  L 253: test_per_token_group_quant_mla_deep_gemm_masked_fp8(self)


CLASS: TestStaticQuantFP8
----------------------------------------
  L 126: setUpClass(cls)

  L 146: test_static_quant_fp8(self)


CLASS: TestW8A8BlockFP8BatchedDeepGemm
----------------------------------------
  L 576: setUpClass(cls)

  L 637: test_w8a8_block_fp8_batched_deep_gemm(self)


CLASS: TestW8A8BlockFP8FusedMoE
----------------------------------------
  L 463: setUpClass(cls)

  L 526: test_w8a8_block_fp8_fused_moe(self)


CLASS: TestW8A8BlockFP8Matmul
----------------------------------------
  L 362: setUpClass(cls)

  L 400: test_w8a8_block_fp8_matmul(self)


============================================================
FILE: python/sglang/test/test_block_fp8_deep_gemm_blackwell.py
Functions: 11
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  15: def ceil_div(x: int, y: int)
         ‚Üí int

  L  19: def align(x: int, y: int)
         ‚Üí int

  L  23: def per_token_group_quant_fp8(x: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L  32: def per_block_quant_fp8(x: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L  48: def ceil_to_ue8m0(x: torch.Tensor)

  L  53: def per_token_group_quant_mxfp8(x: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L  62: def per_block_quant_mxfp8(x: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L  79: def native_w8a8_block_fp8_matmul(A, B, As, Bs, block_size, output_dtype)
         üìù This function performs matrix multiplication with block-wise quantization using native torch.
            It takes two input tensors `A` and `B` with scales `As` and `Bs`.
            The output is returned in the specified `output_dtype`.

  L 134: def block_quant_dequant(x_q_block: torch.Tensor,
        x_s: torch.Tensor,
        block_size: List[int],
        dtype: torch.dtype)
         ‚Üí torch.Tensor
         üìù This function converts block-wise quantization to unquantized.
            The inputs are block-wise quantization tensor `x_q_block`, block-wise quantization scale
            and the block size.
            The output is an unquantized tensor with dtype.


CLASS: TestDeepGemmBlackwell
----------------------------------------
  L 202: setUpClass(cls)

  L 233: test_deep_gemm_blackwell(self)


============================================================
FILE: python/sglang/test/test_block_fp8_ep.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  20: def ep_moe(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        router_logits: torch.Tensor,
        topk_config: TopKConfig,
        num_experts: int,
        fp8_dtype: torch.types,
        num_experts_per_partition: int,
        start_expert_id: int,
        end_expert_id: int,
        use_fp8_w8a8: bool,
        w1_scale_inv: Optional[torch.Tensor],
        w2_scale_inv: Optional[torch.Tensor],
        block_shape: Optional[List[int]])

  L 183: def block_dequant(x_q_block: torch.Tensor,
        x_s: torch.Tensor,
        block_size: List[int])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù This function converts block-wise quantization to tensor-wise quantization.
            The inputs are block-wise quantization tensor `x_q_block`, block-wise quantization scale
            and the block size.
            The outputs are tensor-wise quantization tensor and tensor-wise quantization scale.
            Note only float8 is supported for now.


CLASS: TestW8A8BlockFP8EPMoE
----------------------------------------
  L 241: setUpClass(cls)

  L 330: test_w8a8_block_fp8_ep_moe(self)


============================================================
FILE: python/sglang/test/test_custom_ops.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  16: def test_scaled_fp8_quant_per_tensor(dtype)
         ‚Üí None
         @pytest.mark.parametrize('dtype', [torch.float16, torch.bfloat16])

  L  59: def test_scaled_fp8_quant_per_token_dynamic(dtype)
         ‚Üí None
         @pytest.mark.parametrize('dtype', [torch.float16, torch.bfloat16])

  L  91: def test_scaled_fp8_quant_with_padding(dtype)
         ‚Üí None
         @pytest.mark.parametrize('dtype', [torch.float16, torch.bfloat16])


============================================================
FILE: python/sglang/test/test_cutlass_moe.py
Functions: 5
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  15: def calc_diff(x, y)

  L  22: def get_model_config(tp_size: int)

  L  41: def to_fp8(tensor: torch.Tensor)
         ‚Üí torch.Tensor
         üìù Converts tensor to FP8 E4M3, scaling values to fit the range.

  L  63: def run_test(tp_size, batch_size, model_config, check)

  L 244: def main(tp_size, batch_sizes, check)


============================================================
FILE: python/sglang/test/test_cutlass_w4a8_moe.py
Functions: 5
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  12: def pack_int4_values_to_int8(int4_values_interleaved: torch.Tensor)
         ‚Üí torch.Tensor

  L  28: def pack_interleave(num_experts, ref_weight, ref_scale)

  L  55: def test_cutlass_w4a8_moe(M, N, K, E, ep_size, topk, group_size, dtype)
         @pytest.mark.parametrize('M', [1, 2, 4, 8, 16])
         @pytest.mark.parametrize('N', [2048])
         @pytest.mark.parametrize('K', [7168])
         @pytest.mark.parametrize('E', [256])
         @pytest.mark.parametrize('ep_size', [8])
         @pytest.mark.parametrize('topk', [8])
         @pytest.mark.parametrize('group_size', [128])
         @pytest.mark.parametrize('dtype', [torch.bfloat16])

  L 161: def cutlass_moe(a: torch.Tensor,
        w1_q: torch.Tensor,
        w2_q: torch.Tensor,
        w1_scale: torch.Tensor,
        w2_scale: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids_: torch.Tensor,
        a_strides1: torch.Tensor,
        b_strides1: torch.Tensor,
        c_strides1: torch.Tensor,
        a_strides2: torch.Tensor,
        b_strides2: torch.Tensor,
        c_strides2: torch.Tensor,
        s_strides13: torch.Tensor,
        s_strides2: torch.Tensor,
        start_expert_id: int,
        end_expert_id: int,
        E: int,
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        expert_map: Optional[torch.Tensor],
        apply_router_weight_on_input: bool)

  L 228: def ref(x: torch.Tensor,
        num_experts: int,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        ref_weight_1: torch.Tensor,
        ref_weight_2: torch.Tensor,
        ref_weight_scale_1: torch.Tensor,
        ref_weight_scale_2: torch.Tensor,
        has_pre_quant: bool,
        has_alpha: bool,
        pre_quant_scale_1: Optional[torch.Tensor],
        pre_quant_scale_2: Optional[torch.Tensor],
        alpha_1: Optional[torch.Tensor],
        alpha_2: Optional[torch.Tensor])


============================================================
FILE: python/sglang/test/test_deepep_utils.py
Functions: 13
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  12: def init_dist(local_rank: int, num_local_ranks: int)

  L  37: def calc_diff(x: torch.Tensor, y: torch.Tensor)

  L  44: def per_token_cast_to_fp8(x: torch.Tensor)

  L  54: def per_token_cast_back(x_fp8: torch.Tensor, x_scales: torch.Tensor)

  L  60: def inplace_unique(x: torch.Tensor, num_slots: int)

  L  75: def create_grouped_scores(scores: torch.Tensor,
        group_idx: torch.Tensor,
        num_groups: int)

  L  85: def bench(fn, num_warmups: int, num_tests: int, post_fn)

  L 158: def bench_kineto(fn,
        kernel_names,
        num_tests: int,
        suppress_kineto_output: bool,
        trace_path: Optional[str],
        barrier_comm_profiling: bool)

  L 218: def hash_tensor(t: torch.Tensor)


CLASS: empty_suppress
----------------------------------------
  L 116: __enter__(self)

  L 119: __exit__(self)


CLASS: suppress_stdout_stderr
----------------------------------------
  L 124: __enter__(self)

  L 144: __exit__(self)


============================================================
FILE: python/sglang/test/test_dynamic_grad_mode.py
Functions: 4
============================================================


CLASS: TestDynamicGradMode
----------------------------------------
  L  10: test_inference(self)

  L  21: test_no_grad(self)

  L  32: test_nested_inference(self)

  L  44: test_nested_no_grad(self)


============================================================
FILE: python/sglang/test/test_fp4_moe.py
Functions: 7
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  28: def convert_swizzled_to_linear(a_sf_swizzled: torch.Tensor, m, k, block_size)

  L  38: def dequantize_nvfp4_to_dtype(tensor_fp4,
        tensor_sf,
        global_scale,
        dtype,
        device,
        block_size)
         üìù Dequantize the fp4 tensor back to high precision.

  L  57: def break_fp4_bytes(a, dtype)

  L  96: def torch_moe(a, w1, w2, score, topk, expert_map)

  L 117: def check_moe(m: int,
        n: int,
        k: int,
        e: int,
        topk: int,
        dtype: torch.dtype,
        moe_impl: Callable,
        flip_w13: bool)

  L 244: def test_cutlass_fp4_moe_no_graph(m: int,
        n: int,
        k: int,
        e: int,
        topk: int,
        dtype: torch.dtype)
         @pytest.mark.parametrize('m,n,k', MNK_FACTORS)
         @pytest.mark.parametrize('e', [40, 64, 256])
         @pytest.mark.parametrize('topk', [1, 6, 8])
         @pytest.mark.parametrize('dtype', [torch.half, torch.bfloat16])
         @torch.inference_mode()

  L 291: def test_flashinfer_fp4_moe_no_graph(m: int,
        n: int,
        k: int,
        e: int,
        topk: int,
        dtype: torch.dtype)
         @pytest.mark.parametrize('m,n,k', MNK_FACTORS)
         @pytest.mark.parametrize('e', [40, 64, 256])
         @pytest.mark.parametrize('topk', [1, 6, 8])
         @pytest.mark.parametrize('dtype', [torch.half, torch.bfloat16])
         @torch.inference_mode()


============================================================
FILE: python/sglang/test/test_layernorm.py
Functions: 4
============================================================


CLASS: TestGemmaRMSNorm
----------------------------------------
  L  68: setUpClass(cls)

  L  94: test_gemma_rms_norm(self)


CLASS: TestRMSNorm
----------------------------------------
  L  18: setUpClass(cls)

  L  42: test_rms_norm(self)


============================================================
FILE: python/sglang/test/test_marlin_moe.py
Functions: 5
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  13: def stack_and_dev(tensors: list[torch.Tensor])

  L  18: def torch_experts(a: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weight: torch.Tensor,
        topk_ids: torch.Tensor,
        global_num_experts: int,
        expert_map: Optional[torch.Tensor],
        quant_dtype: Optional[torch.dtype],
        apply_router_weights_on_input: bool)
         ‚Üí torch.Tensor

  L  73: def torch_moe(a: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        score: torch.Tensor,
        topk: int,
        global_num_experts: int,
        expert_map: Optional[torch.Tensor])
         ‚Üí torch.Tensor

  L  89: def marlin_moe_generate_valid_test_cases()

  L 146: def test_fused_marlin_moe(m: int,
        n: int,
        k: int,
        e: int,
        topk: int,
        dtype: torch.dtype,
        group_size: int,
        act_order: bool,
        quant_type: ScalarType,
        is_k_full: bool)
         @pytest.mark.flaky(reruns=2)
         @pytest.mark.parametrize('m, n, k, e, topk, dtype, group_size,act_order, quant_type, is_k_full', marlin_moe_generate_valid_test_cases())


============================================================
FILE: python/sglang/test/test_marlin_utils.py
Functions: 6
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  42: def marlin_permute_weights(q_w, size_k, size_n, perm, tile)

  L  57: def marlin_weights(q_w, size_k, size_n, num_bits, perm)

  L  76: def get_weight_perm(num_bits: int)

  L 106: def marlin_quantize(w: torch.Tensor,
        quant_type: ScalarType,
        group_size: int,
        act_order: bool,
        test_perm: Optional[torch.Tensor])

  L 145: def awq_marlin_quantize(w: torch.Tensor,
        quant_type: ScalarType,
        group_size: int)


CLASS: MarlinWorkspace
----------------------------------------
  L  30: __init__(self, out_features, min_thread_n, max_parallel)


============================================================
FILE: python/sglang/test/test_programs.py
Functions: 19
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  13: def test_few_shot_qa()

  L  41: def test_mt_bench()

  L  62: def test_select(check_answer)

  L  96: def test_decode_int()

  L 107: def test_decode_json_regex()

  L 135: def test_decode_json()

  L 159: def test_expert_answer(check_answer)

  L 182: def test_tool_use()

  L 204: def test_react()

  L 240: def test_parallel_decoding()

  L 278: def test_parallel_encoding(check_answer)

  L 310: def test_image_qa()

  L 328: def test_stream()

  L 352: def test_regex()

  L 369: def test_dtype_gen()

  L 393: def test_completion_speculative()

  L 435: def test_chat_completion_speculative()

  L 456: def test_hellaswag_select()
         üìù Benchmark the accuracy of sgl.select on the HellaSwag dataset.

  L 545: def test_gen_min_new_tokens()
         üìù Validate sgl.gen(min_tokens) functionality.
            The test asks a question where, without a min_tokens constraint, the generated answer is expected to be short.
            By enforcing the min_tokens parameter, we ensure the generated answer has at least the specified number of tokens.
            We verify that the number of tokens in the answer is >= the min_tokens threshold.


============================================================
FILE: python/sglang/test/test_utils.py
Functions: 41
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 104: def is_in_ci()
         üìù Return whether it is in CI runner.

  L 109: def is_in_amd_ci()
         üìù Return whether it is in an AMD CI runner.

  L 137: def call_generate_lightllm(prompt, temperature, max_tokens, stop, url)

  L 154: def find_available_port(base_port: int)

  L 165: def call_generate_vllm(prompt, temperature, max_tokens, stop, n, url)

  L 184: def call_generate_outlines(prompt, temperature, max_tokens, stop, regex, n, url)

  L 206: def call_generate_srt_raw(prompt, temperature, max_tokens, stop, url)

  L 224: def call_generate_guidance(prompt,
        temperature,
        max_tokens,
        stop,
        n,
        regex,
        model)

  L 247: def call_select_lightllm(context, choices, url)

  L 264: def call_select_vllm(context, choices, url)

  L 288: def call_select_guidance(context, choices, model)

  L 296: def add_common_other_args_and_parse(parser: argparse.ArgumentParser)

  L 333: def auto_config_device()
         ‚Üí str
         üìù Auto-config available device platform

  L 345: def add_common_sglang_args_and_parse(parser: argparse.ArgumentParser)

  L 364: def select_sglang_backend(args: argparse.Namespace)

  L 418: def get_call_generate(args: argparse.Namespace)

  L 431: def get_call_select(args: argparse.Namespace)

  L 464: def try_cached_model(model_repo: str)

  L 469: def popen_launch_server(model: str,
        base_url: str,
        timeout: float,
        api_key: Optional[str],
        other_args: list[str],
        env: Optional[dict],
        return_stdout_stderr: Optional[tuple],
        device: str,
        pd_separated: bool)
         üìù Launch a server process with automatic device detection.
            Args:
            device: Device type ("auto", "cuda", "rocm" or "cpu").
            If "auto", will detect available platforms automatically.

  L 583: def popen_launch_pd_server(model: str,
        base_url: str,
        timeout: float,
        api_key: Optional[str],
        other_args: list[str],
        env: Optional[dict])

  L 624: def run_with_timeout(func: Callable,
        args: tuple,
        kwargs: Optional[dict],
        timeout: float)
         üìù Run a function with timeout.

  L 654: def run_unittest_files(files: List[TestFile], timeout_per_file: float)

  L 709: def get_similarities(vec1, vec2)

  L 713: def get_benchmark_args(base_url,
        dataset_name,
        dataset_path,
        tokenizer,
        num_prompts,
        sharegpt_output_len,
        random_input_len,
        random_output_len,
        sharegpt_context_len,
        request_rate,
        disable_stream,
        disable_ignore_eos,
        seed: int,
        device,
        pd_separated: bool,
        lora_name)

  L 764: def run_bench_serving(model,
        num_prompts,
        request_rate,
        other_server_args,
        dataset_name,
        dataset_path,
        tokenizer,
        random_input_len,
        random_output_len,
        sharegpt_context_len,
        disable_stream,
        disable_ignore_eos,
        need_warmup,
        seed: int,
        device,
        background_task: Optional[Callable[[str,
        asyncio.Event],
        Awaitable[None]]],
        lora_name: Optional[str])

  L 845: def run_bench_serving_multi(model,
        base_url,
        other_server_args,
        benchmark_args,
        need_warmup,
        pd_separated)

  L 879: def run_bench_one_batch(model, other_args)
         üìù Launch a offline process with automatic device detection.
            Args:
            device: Device type ("auto", "cuda", "rocm" or "cpu").
            If "auto", will detect available platforms automatically.

  L 934: def run_bench_offline_throughput(model, other_args)

  L 972: def run_bench_one_batch_server(model,
        base_url,
        server_args,
        bench_args,
        other_server_args,
        simulate_spec_acc_lens)

  L1000: def lcs(X, Y)

  L1017: def calculate_rouge_l(output_strs_list1, output_strs_list2)
         üìù calculate the ROUGE-L score

  L1038: def read_output(output_lines: List[str], filename: str)
         üìù Print the output in real time with another thread.

  L1059: def run_and_check_memory_leak(workload_func,
        disable_radix_cache,
        enable_mixed_chunk,
        disable_overlap,
        chunked_prefill_size,
        assert_has_abort)

  L1132: def run_command_and_capture_output(command, env: Optional[dict])

  L1159: def run_mmlu_test(disable_radix_cache,
        enable_mixed_chunk,
        disable_overlap,
        chunked_prefill_size)

  L1191: def run_mulit_request_test(disable_radix_cache,
        enable_mixed_chunk,
        enable_overlap,
        chunked_prefill_size)

  L1230: def write_github_step_summary(content)

  L1239: def run_logprob_check(self: unittest.TestCase, arg: Tuple)

  L1314: def send_generate_requests(base_url: str, num_requests: int)
         ‚Üí List[str]
         üìù Sends generate request serially and returns status codes. Max concurrency is 1.

  L1338: async def send_concurrent_generate_requests(base_url: str, num_requests: int)
         ‚Üí List[str]
         üìù Sends generate request concurrently and returns status codes. Max concurrency is num_requests.

  L1377: def dump_bench_raw_result(path: str, states, preds, labels)
