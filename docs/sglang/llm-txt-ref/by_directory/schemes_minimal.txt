
# python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py
  CompressedTensorsScheme.get_min_capability(cls) -> int
  CompressedTensorsScheme.create_weights()
  CompressedTensorsScheme.apply_weights(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
  CompressedTensorsScheme.process_weights_after_loading(layer: torch.nn.Module)

# python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py
apply_fp8_marlin_linear()
prepare_fp8_layer_for_marlin()
  CompressedTensorsW8A16Fp8.__init__(strategy: str, is_static_input_scheme: bool)
  CompressedTensorsW8A16Fp8.get_min_capability(cls) -> int
  CompressedTensorsW8A16Fp8.process_weights_after_loading(layer) -> None
  CompressedTensorsW8A16Fp8.create_weights(layer: torch.nn.Module, input_size: int, output_partition_sizes: List[int], input_size_per_partition: int, params_dtype: torch.dtype, weight_loader: Callable)
  CompressedTensorsW8A16Fp8.apply_weights(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor

# python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
  CompressedTensorsW8A8Fp8.__init__(strategy: str, is_static_input_scheme: bool)
  CompressedTensorsW8A8Fp8.get_min_capability(cls) -> int
  CompressedTensorsW8A8Fp8.process_weights_after_loading(layer) -> None
  CompressedTensorsW8A8Fp8.create_weights(layer: torch.nn.Module, output_partition_sizes: List[int], input_size_per_partition: int, params_dtype: torch.dtype, weight_loader: Callable)
  CompressedTensorsW8A8Fp8.apply_weights(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor

# python/sglang/srt/layers/quantization/quark/schemes/quark_scheme.py
  QuarkScheme.get_min_capability(cls) -> int
  QuarkScheme.create_weights()
  QuarkScheme.apply_weights(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
  QuarkScheme.process_weights_after_loading(layer: torch.nn.Module)

# python/sglang/srt/layers/quantization/quark/schemes/quark_w4a4_mxfp4.py
  QuarkW4A4MXFP4.__init__(weight_quant_spec: dict[str, Any], input_quant_spec: dict[str, Any])
  QuarkW4A4MXFP4.get_min_capability(cls) -> int
  QuarkW4A4MXFP4.process_weights_after_loading(layer: torch.nn.Module) -> None
  QuarkW4A4MXFP4.create_weights(layer: torch.nn.Module, output_partition_sizes: list[int], input_size_per_partition: int, params_dtype: torch.dtype, weight_loader: Callable)
  QuarkW4A4MXFP4.apply_weights(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
