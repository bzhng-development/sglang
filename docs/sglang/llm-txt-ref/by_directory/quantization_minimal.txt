
# python/sglang/srt/layers/quantization/__init__.py
  DummyConfig.override_quantization_method()
get_quantization_config(quantization: str) -> Type[QuantizationConfig]
monkey_patch_isinstance_for_vllm_base_layer(reverse: bool)
monkey_patch_moe_apply(class_obj: 'FusedMoEMethodBase')
monkey_patch_quant_configs()

# python/sglang/srt/layers/quantization/awq.py
is_layer_skipped_awq(prefix: str, modules_to_not_convert: List[str])
  AWQConfig.__init__(weight_bits: int, group_size: int, zero_point: bool, modules_to_not_convert: Optional[List[str]]) -> None
  AWQConfig.__repr__() -> str
  AWQConfig.get_scaled_act_names() -> List[str]
  AWQConfig.get_name() -> str
  AWQConfig.get_supported_act_dtypes() -> List[torch.dtype]
  AWQConfig.get_min_capability(cls) -> int
  AWQConfig.get_config_filenames() -> List[str]
  AWQConfig.from_config(cls, config: Dict[str, Any]) -> AWQConfig
  AWQConfig.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[LinearMethodBase]
  AWQMarlinConfig.__init__(weight_bits: int, group_size: int, zero_point: bool, lm_head_quantized: bool, modules_to_not_convert: Optional[list[str]], full_config: dict[str, Any]) -> None
  AWQMarlinConfig.__repr__() -> str
  AWQMarlinConfig.get_scaled_act_names() -> List[str]
  AWQMarlinConfig.get_name(cls) -> str
  AWQMarlinConfig.get_supported_act_dtypes(cls) -> list[torch.dtype]
  AWQMarlinConfig.get_min_capability(cls) -> int
  AWQMarlinConfig.get_config_filenames(cls) -> list[str]
  AWQMarlinConfig.from_config(cls, config: dict[str, Any]) -> AWQMarlinConfig
  AWQMarlinConfig.override_quantization_method(cls, hf_quant_cfg, user_quant) -> Optional[str]
  AWQMarlinConfig.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  AWQMarlinConfig.is_awq_marlin_compatible(cls, quant_config: dict[str, Any])
  AWQLinearMethod.__init__(quant_config: AWQConfig)
  AWQLinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  AWQLinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  AWQLinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  AWQMarlinLinearMethod.__init__(quant_config: AWQMarlinConfig) -> None
  AWQMarlinLinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype) -> None
  AWQMarlinLinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  AWQMarlinLinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  AWQMoEMethod.__init__(quant_config: AWQMarlinConfig)
  AWQMoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)
  AWQMoEMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  AWQMoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor

# python/sglang/srt/layers/quantization/awq_triton.py
awq_dequantize_kernel(qweight_ptr, scales_ptr, zeros_ptr, group_size, result_ptr, num_cols, num_rows, BLOCK_SIZE_X: tl.constexpr, BLOCK_SIZE_Y: tl.constexpr)
awq_gemm_kernel(a_ptr, b_ptr, c_ptr, zeros_ptr, scales_ptr, M, N, K, group_size, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, SPLIT_K: tl.constexpr)
awq_dequantize_triton(qweight: torch.Tensor, scales: torch.Tensor, zeros: torch.Tensor, block_size_x: int, block_size_y: int) -> torch.Tensor
awq_gemm_triton(input: torch.Tensor, qweight: torch.Tensor, scales: torch.Tensor, qzeros: torch.Tensor, split_k_iters: int, block_size_m: int, block_size_n: int, block_size_k: int) -> torch.Tensor

# python/sglang/srt/layers/quantization/base_config.py
  QuantizeMethodBase.create_weights(layer: torch.nn.Module)
  QuantizeMethodBase.apply(layer: torch.nn.Module) -> torch.Tensor
  QuantizeMethodBase.process_weights_after_loading(layer: nn.Module) -> None
  LinearMethodBase.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  LinearMethodBase.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  FusedMoEMethodBase.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)
  FusedMoEMethodBase.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  QuantizationConfig.__init__()
  QuantizationConfig.get_name() -> str
  QuantizationConfig.get_supported_act_dtypes() -> List[torch.dtype]
  QuantizationConfig.get_min_capability(cls) -> int
  QuantizationConfig.get_config_filenames() -> List[str]
  QuantizationConfig.from_config(cls, config: Dict[str, Any]) -> 'QuantizationConfig'
  QuantizationConfig.override_quantization_method(cls, hf_quant_cfg, user_quant) -> Optional[str]
  QuantizationConfig.get_from_keys(config: Dict[str, Any], keys: List[str]) -> Any
  QuantizationConfig.get_from_keys_or(config: Dict[str, Any], keys: List[str], default: Any) -> Any
  QuantizationConfig.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  QuantizationConfig.get_scaled_act_names() -> List[str]
method_has_implemented_embedding(method_class: Type[QuantizeMethodBase]) -> bool

# python/sglang/srt/layers/quantization/blockwise_int8.py
  BlockInt8Config.__init__(is_checkpoint_int8_serialized: bool, activation_scheme: str, ignored_layers: Optional[List[str]], weight_block_size: List[int]) -> None
  BlockInt8Config.get_name(cls) -> str
  BlockInt8Config.get_supported_act_dtypes(cls) -> List[torch.dtype]
  BlockInt8Config.get_min_capability(cls) -> int
  BlockInt8Config.get_config_filenames(cls) -> List[str]
  BlockInt8Config.from_config(cls, config: Dict[str, Any]) -> BlockInt8Config
  BlockInt8Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  BlockInt8Config.get_scaled_act_names() -> List[str]
  BlockInt8LinearMethod.__init__(quant_config: BlockInt8Config)
  BlockInt8LinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  BlockInt8LinearMethod.process_weights_after_loading(layer: Module) -> None
  BlockInt8LinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  BlockInt8MoEMethod.__init__(quant_config: BlockInt8Config)
  BlockInt8MoEMethod.create_weights(layer: Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)
  BlockInt8MoEMethod.process_weights_after_loading(layer: Module) -> None
  BlockInt8MoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor

# python/sglang/srt/layers/quantization/fp8.py
dummy_func()
  Fp8Config.__init__(is_checkpoint_fp8_serialized: bool, activation_scheme: str, ignored_layers: Optional[List[str]], weight_block_size: List[int]) -> None
  Fp8Config.get_name(cls) -> str
  Fp8Config.get_supported_act_dtypes(cls) -> List[torch.dtype]
  Fp8Config.get_min_capability(cls) -> int
  Fp8Config.get_config_filenames(cls) -> List[str]
  Fp8Config.from_config(cls, config: Dict[str, Any]) -> Fp8Config
  Fp8Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  Fp8Config.get_scaled_act_names() -> List[str]
  Fp8LinearMethod.__init__(quant_config: Union[Fp8Config, W4AFp8Config])
  Fp8LinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  Fp8LinearMethod.process_weights_after_loading(layer: Module) -> None
  Fp8LinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
get_tile_tokens_dim(num_tokens, top_k, num_experts)
  Fp8MoEMethod.__init__(quant_config: Fp8Config)
  Fp8MoEMethod.create_weights(layer: Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)
  Fp8MoEMethod.process_weights_after_loading(layer: Module) -> None
  Fp8MoEMethod.process_weights_hip_int4(layer: Module)
  Fp8MoEMethod.process_weights_hip_scale_padding(layer: Module)
  Fp8MoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  Fp8MoEMethod.apply_with_router_logits(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  Fp8MoEMethod.maybe_apply_hip_fused_experts(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, activation: str, no_combine: bool) -> Optional[torch.Tensor]
  Fp8KVCacheMethod.__init__(quant_config: Fp8Config)

# python/sglang/srt/layers/quantization/fp8_kernel.py
is_fp8_fnuz() -> bool
deep_gemm_fp8_fp8_bf16_nt(A: torch.Tensor, As: torch.Tensor, B: torch.Tensor, Bs: torch.Tensor, C: torch.Tensor) -> None
deep_gemm_fp8_fp8_bf16_nt_fake(A: torch.Tensor, As: torch.Tensor, B: torch.Tensor, Bs: torch.Tensor, C: torch.Tensor) -> None
per_token_group_quant_8bit(x: torch.Tensor, group_size: int, dst_dtype: torch.dtype, eps: float, column_major_scales: bool, scale_tma_aligned: bool, scale_ue8m0: bool, fuse_silu_and_mul: bool, masked_m: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
create_per_token_group_quant_fp8_output_scale(x_shape, device, group_size, column_major_scales: bool, scale_tma_aligned: bool, scale_ue8m0: bool)
sglang_per_token_group_quant_fp8(x: torch.Tensor, group_size: int, eps: float, column_major_scales: bool, scale_tma_aligned: bool, scale_ue8m0: bool, fuse_silu_and_mul: bool, masked_m: Optional[torch.Tensor])
sglang_per_token_group_quant_8bit(x: torch.Tensor, group_size: int, dst_dtype: torch.dtype, eps: float, column_major_scales: bool, scale_tma_aligned: bool, scale_ue8m0: bool, fuse_silu_and_mul: bool, masked_m: Optional[torch.Tensor])
sglang_per_token_quant_fp8(x: torch.Tensor, dtype: torch.dtype)
static_quant_fp8(x: torch.Tensor, x_s: torch.Tensor, repeat_scale: bool) -> Tuple[torch.Tensor, torch.Tensor]
get_w8a8_block_fp8_configs(N: int, K: int, block_n: int, block_k: int) -> Optional[Dict[int, Any]]
select_w8a8_block_fp8_matmul_kernel(M, N, META)
use_w8a8_block_fp8_matmul_unrolledx4(M, N, META)
select_w8a8_block_fp8_matmul_kernel(M, N, META)
prepare_block_fp8_matmul_inputs(A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype) -> Tuple[int, int, int]
w8a8_block_fp8_matmul_deepgemm(A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype) -> torch.Tensor
w8a8_block_fp8_matmul_triton(A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype) -> torch.Tensor
w8a8_block_fp8_matmul(A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype) -> torch.Tensor
per_tensor_quant_mla_fp8(x: torch.Tensor, x_s_out: torch.Tensor, eps: float) -> Tuple[torch.Tensor, torch.Tensor]
per_token_group_quant_mla_deep_gemm_masked_fp8(x: torch.Tensor, group_size: int, eps: float, dtype: torch.dtype) -> Tuple[torch.Tensor, torch.Tensor]
scaled_fp8_quant(input: torch.Tensor, scale: Optional[torch.Tensor], num_token_padding: Optional[int], use_per_token_if_dynamic: bool) -> tuple[torch.Tensor, torch.Tensor]
scaled_fp8_quant(input: torch.Tensor, scale: Optional[torch.Tensor], num_token_padding: Optional[int], use_per_token_if_dynamic: bool) -> tuple[torch.Tensor, torch.Tensor]
per_token_group_quant_fp8_hopper_moe_mn_major(A: torch.Tensor, expert_offsets: torch.Tensor, problem_sizes: torch.Tensor, group_size: int, expert_tokens_alignment: int) -> Tuple[torch.Tensor, torch.Tensor]
per_group_transpose(a: torch.Tensor, expert_offsets: torch.Tensor, M_ALIGNMENT: int) -> torch.Tensor
is_weak_contiguous(x: torch.Tensor)
scaled_mm_kernel(a_ptr, b_ptr, scale_a_ptr, scale_b_ptr, c_ptr, bias_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, ACCUMULATOR_DTYPE: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_SCALE_A: tl.constexpr, BLOCK_SIZE_SCALE_B: tl.constexpr)
triton_scaled_mm(input: torch.Tensor, weight: torch.Tensor, scale_a: torch.Tensor, scale_b: torch.Tensor, out_dtype: type[torch.dtype], bias: Optional[torch.Tensor], block_size_m: int, block_size_n: int, block_size_k: int, use_heuristic) -> torch.Tensor

# python/sglang/srt/layers/quantization/fp8_utils.py
use_rowwise_torch_scaled_mm()
cutlass_fp8_supported()
normalize_e4m3fn_to_e4m3fnuz(weight: torch.Tensor, weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]
cutlass_block_fp8_supported() -> bool
dispatch_w8a8_block_fp8_linear() -> Callable
flashinfer_gemm_w8a8_block_fp8_linear(input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], bias: Optional[torch.Tensor]) -> torch.Tensor
cutlass_w8a8_block_fp8_linear_with_fallback(input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], bias: Optional[torch.Tensor]) -> torch.Tensor
deepgemm_w8a8_block_fp8_linear_with_fallback(input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], bias: Optional[torch.Tensor]) -> torch.Tensor
aiter_w8a8_block_fp8_linear(input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], bias: Optional[torch.Tensor]) -> torch.Tensor
triton_w8a8_block_fp8_linear(input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], bias: Optional[torch.Tensor]) -> torch.Tensor
dequant_mxfp4(w_block: torch.Tensor, w_scale: torch.Tensor, out_dtype) -> torch.Tensor
input_to_float8(x: torch.Tensor, dtype: torch.dtype) -> Tuple[torch.Tensor, torch.Tensor]
block_quant_to_tensor_quant(x_q_block: torch.Tensor, x_s: torch.Tensor, block_size: List[int]) -> Tuple[torch.Tensor, torch.Tensor]
block_quant_dequant(x_q_block: torch.Tensor, x_s: torch.Tensor, block_size: List[int], dtype: torch.dtype) -> torch.Tensor
requant_weight_ue8m0_inplace(weight, weight_scale_inv, weight_block_size)
per_block_cast_to_fp8(x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
ceil_to_ue8m0(x: torch.Tensor)
channel_quant_to_tensor_quant(x_q_channel: torch.Tensor, x_s: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
apply_fp8_linear(input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], input_scale_ub: Optional[torch.Tensor], bias: Optional[torch.Tensor], cutlass_fp8_supported: bool, use_per_token_if_dynamic: bool, pad_output: Optional[bool], compressed_tensor_quant: bool) -> torch.Tensor
can_auto_enable_marlin_fp8() -> bool

# python/sglang/srt/layers/quantization/fpgemm_fp8.py
  FBGEMMFp8Config.__init__(ignore_list: list[str], input_scale_ub: float)
  FBGEMMFp8Config.get_name(cls) -> str
  FBGEMMFp8Config.get_supported_act_dtypes(cls) -> list[torch.dtype]
  FBGEMMFp8Config.get_min_capability(cls) -> int
  FBGEMMFp8Config.get_config_filenames(cls) -> list[str]
  FBGEMMFp8Config.from_config(cls, config: dict[str, Any]) -> FBGEMMFp8Config
  FBGEMMFp8Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  FBGEMMFp8Config.get_scaled_act_names() -> List[str]
  FBGEMMFp8LinearMethod.__init__(quant_config: FBGEMMFp8Config)
  FBGEMMFp8LinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  FBGEMMFp8LinearMethod.process_weights_after_loading(layer: Module) -> None
  FBGEMMFp8LinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor

# python/sglang/srt/layers/quantization/gptq.py
check_marlin_format(hf_quant_cfg: Dict[str, Any]) -> bool
gptq_marlin_moe_repack(b_q_weight: torch.Tensor, perm: torch.Tensor, size_k: int, size_n: int, num_bits: int) -> torch.Tensor
  GPTQConfig.__init__(weight_bits: int, group_size: int, desc_act: bool, lm_head_quantized: bool, dynamic: Dict[str, Dict[str, Union[int, bool]]]) -> None
  GPTQConfig.__repr__() -> str
  GPTQConfig.get_scaled_act_names() -> List[str]
  GPTQConfig.get_name(cls) -> str
  GPTQConfig.get_supported_act_dtypes(cls) -> List[torch.dtype]
  GPTQConfig.get_min_capability(cls) -> int
  GPTQConfig.get_config_filenames(cls) -> List[str]
  GPTQConfig.from_config(cls, config: Dict[str, Any]) -> GPTQConfig
  GPTQConfig.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[LinearMethodBase]
  GPTQMarlinConfig.__init__(weight_bits: int, group_size: int, desc_act: bool, is_sym: bool, lm_head_quantized: bool, dynamic: Dict[str, Dict[str, Union[int, bool]]], full_config: Dict[str, Any]) -> None
  GPTQMarlinConfig.__repr__() -> str
  GPTQMarlinConfig.get_scaled_act_names() -> List[str]
  GPTQMarlinConfig.get_name(cls) -> str
  GPTQMarlinConfig.get_supported_act_dtypes(cls) -> List[torch.dtype]
  GPTQMarlinConfig.get_min_capability(cls) -> int
  GPTQMarlinConfig.get_config_filenames(cls) -> List[str]
  GPTQMarlinConfig.from_config(cls, config: Dict[str, Any]) -> GPTQMarlinConfig
  GPTQMarlinConfig.override_quantization_method(cls, hf_quant_cfg, user_quant) -> Optional[str]
  GPTQMarlinConfig.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  GPTQMarlinConfig.is_gptq_marlin_compatible(cls, quant_config: Dict[str, Any])
  GPTQLinearMethod.__init__(quant_config: GPTQConfig)
  GPTQLinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  GPTQLinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  GPTQLinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  GPTQMarlinLinearMethod.__init__(quant_config: GPTQMarlinConfig) -> None
  GPTQMarlinLinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype) -> None
  GPTQMarlinLinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  GPTQMarlinLinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  GPTQMarlinMoEMethod.__init__(quant_config: GPTQMarlinConfig) -> None
  GPTQMarlinMoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)
  GPTQMarlinMoEMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  GPTQMarlinMoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor

# python/sglang/srt/layers/quantization/int8_kernel.py
per_token_quant_int8(x, scale_dtype, cal_sum)
per_token_group_quant_int8(x: torch.Tensor, group_size: int, eps: float, dtype: torch.dtype) -> Tuple[torch.Tensor, torch.Tensor]
sglang_per_token_group_quant_int8(x: torch.Tensor, group_size: int, eps: float, dtype: torch.dtype)
get_w8a8_block_int8_configs(N: int, K: int, block_n: int, block_k: int) -> Optional[Dict[int, Any]]
w8a8_block_int8_matmul(A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype) -> torch.Tensor

# python/sglang/srt/layers/quantization/int8_utils.py
apply_w8a8_block_int8_linear(input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], bias: Optional[torch.Tensor]) -> torch.Tensor
input_to_int8(x: torch.Tensor, dtype: torch.dtype) -> Tuple[torch.Tensor, torch.Tensor]
block_dequant(x_q_block: torch.Tensor, x_s: torch.Tensor, block_size: List[int]) -> torch.Tensor

# python/sglang/srt/layers/quantization/kv_cache.py
  BaseKVCacheMethod.__init__(quant_config: QuantizationConfig)
  BaseKVCacheMethod.create_weights(layer: torch.nn.Module)
  BaseKVCacheMethod.apply(layer: torch.nn.Module) -> torch.Tensor
  BaseKVCacheMethod.process_weights_after_loading(layer: RadixAttention) -> None

# python/sglang/srt/layers/quantization/marlin_utils.py
query_marlin_supported_quant_types(has_zp: Optional[bool], include_fp_type: bool, device_capability: Optional[int])
check_marlin_supported(quant_type: ScalarType, group_size: int, has_zp: bool, device_capability: Optional[int]) -> bool
verify_marlin_supported(quant_type: ScalarType, group_size: int, has_zp: bool) -> None
verify_marlin_supports_shape(output_size_per_partition: int, input_size_per_partition: int, input_size: int, group_size: int) -> None
check_marlin_supports_shape(output_size_per_partition: int, input_size_per_partition: int, input_size: int, group_size: int) -> tuple[bool, Optional[str]]
check_marlin_supports_layer(layer: LinearBase, group_size: int) -> bool
check_moe_marlin_supports_layer(layer: FusedMoE, group_size: int) -> bool
marlin_make_workspace(device: torch.device, max_blocks_per_sm: int) -> torch.Tensor
marlin_is_k_full(act_order: bool, is_row_parallel: bool) -> bool
marlin_repeat_scales_on_all_ranks(act_order: bool, group_size: int, is_row_parallel: bool) -> bool
marlin_make_empty_g_idx(device: torch.device) -> torch.Tensor
marlin_make_empty_zp(device: torch.device) -> torch.Tensor
marlin_sort_g_idx(g_idx: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]
get_scale_perms()
marlin_permute_scales(s: torch.Tensor, size_k: int, size_n: int, group_size: int) -> torch.Tensor
marlin_permute_bias(s: torch.Tensor) -> torch.Tensor
marlin_moe_permute_scales(s: torch.Tensor, size_k: int, size_n: int, group_size: int)
marlin_zero_points(zp: torch.Tensor, size_k: int, size_n: int, num_bits: int) -> torch.Tensor
awq_to_marlin_zero_points(q_zp_packed: torch.Tensor, size_k: int, size_n: int, num_bits: int) -> torch.Tensor
moe_awq_to_marlin_zero_points(q_zp_packed: torch.Tensor, size_k: int, size_n: int, num_bits: int)
maybe_warn_marlin_atomic_add(device, dtype)
maybe_warn_marlin_atomic_add_env()
should_use_atomic_add_reduce(m: int, n: int, k: int, device: torch.device, dtype: torch.dtype) -> bool
apply_gptq_marlin_linear(input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, weight_zp: torch.Tensor, g_idx: torch.Tensor, g_idx_sort_indices: torch.Tensor, workspace: torch.Tensor, wtype: ScalarType, output_size_per_partition: int, input_size_per_partition: int, is_k_full: bool, bias: Optional[torch.Tensor], use_fp32_reduce: bool) -> torch.Tensor
apply_awq_marlin_linear(input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, weight_zp: torch.Tensor, g_idx: torch.Tensor, g_idx_sort_indices: torch.Tensor, workspace: torch.Tensor, quant_type: ScalarType, output_size_per_partition: int, input_size_per_partition: int, bias: Optional[torch.Tensor], use_fp32_reduce: bool) -> torch.Tensor
  MarlinConfig.__init__(group_size: int, lm_head_quantized: bool) -> None
  MarlinConfig.__repr__() -> str
  MarlinConfig.get_name(cls) -> str
  MarlinConfig.get_supported_act_dtypes(cls) -> list[torch.dtype]
  MarlinConfig.get_min_capability(cls) -> int
  MarlinConfig.get_config_filenames(cls) -> list[str]
  MarlinConfig.from_config(cls, config: dict[str, Any]) -> 'MarlinConfig'
  MarlinConfig.override_quantization_method(cls, hf_quant_cfg, user_quant) -> Optional[str]
  MarlinConfig.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[MarlinLinearMethod]
  MarlinLinearMethod.__init__(quant_config: MarlinConfig)
  MarlinLinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  MarlinLinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  MarlinLinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor

# python/sglang/srt/layers/quantization/marlin_utils_fp8.py
fp8_fused_exponent_bias_into_scales(scales)
apply_fp8_marlin_linear(input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, workspace: torch.Tensor, size_n: int, size_k: int, bias: Optional[torch.Tensor], use_fp32_reduce: bool) -> torch.Tensor
prepare_fp8_layer_for_marlin(layer: torch.nn.Module, size_k_first: bool) -> None
prepare_moe_fp8_layer_for_marlin(layer: torch.nn.Module, size_k_first: bool) -> None
pack_fp8_to_int32(fp8_tensor: torch.Tensor, size_k_first: bool) -> torch.Tensor
marlin_quant_fp8_torch(weight, group_size)

# python/sglang/srt/layers/quantization/modelopt_quant.py
  ModelOptFp8Config.__init__(is_checkpoint_fp8_serialized: bool, kv_cache_quant_method: Optional[str], exclude_modules: Optional[List[str]]) -> None
  ModelOptFp8Config.get_name(cls) -> str
  ModelOptFp8Config.get_supported_act_dtypes(cls) -> List[torch.dtype]
  ModelOptFp8Config.get_min_capability(cls) -> int
  ModelOptFp8Config.get_config_filenames(cls) -> List[str]
  ModelOptFp8Config.from_config(cls, config: Dict[str, Any]) -> ModelOptFp8Config
  ModelOptFp8Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  ModelOptFp8Config.get_scaled_act_names() -> List[str]
  ModelOptFp8LinearMethod.__init__(quant_config: ModelOptFp8Config)
  ModelOptFp8LinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], params_dtype: torch.dtype) -> None
  ModelOptFp8LinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  ModelOptFp8LinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  ModelOptFp8KVCacheMethod.__init__(quant_config: ModelOptFp8Config)
  ModelOptFp8MoEMethod.__init__(quant_config: ModelOptFp8Config)
  ModelOptFp8MoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)
  ModelOptFp8MoEMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  ModelOptFp8MoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  ModelOptFp4Config.__init__(is_checkpoint_nvfp4_serialized: bool, kv_cache_quant_algo: str, group_size: int, exclude_modules: List[str]) -> None
  ModelOptFp4Config.get_name(cls) -> str
  ModelOptFp4Config.get_supported_act_dtypes(cls) -> List[torch.dtype]
  ModelOptFp4Config.get_min_capability(cls) -> int
  ModelOptFp4Config.get_config_filenames(cls) -> List[str]
  ModelOptFp4Config.from_config(cls, config: Dict[str, Any]) -> ModelOptFp4Config
  ModelOptFp4Config.is_layer_excluded(prefix: str, exclude_modules: list)
  ModelOptFp4Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  ModelOptFp4Config.get_scaled_act_names() -> List[str]
  ModelOptFp4LinearMethod.__init__(quant_config: ModelOptFp4Config)
  ModelOptFp4LinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  ModelOptFp4LinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  ModelOptFp4LinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  ModelOptNvFp4FusedMoEMethod.__init__(quant_config: ModelOptFp4Config)
  ModelOptNvFp4FusedMoEMethod.enable_flashinfer_cutlass_moe() -> bool
  ModelOptNvFp4FusedMoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)
  ModelOptNvFp4FusedMoEMethod.swizzle_blockscale(scale: torch.Tensor)
  ModelOptNvFp4FusedMoEMethod.prepare_static_weights_for_kernel(gemm1_weights, gemm2_weights, gemm1_scales_linear_fp4_bytes, gemm2_scales_linear_fp4_bytes, hidden_size, intermediate_size, num_experts)
  ModelOptNvFp4FusedMoEMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  ModelOptNvFp4FusedMoEMethod.load_up_proj_weight_first() -> bool
  ModelOptNvFp4FusedMoEMethod.apply(layer: FusedMoE, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor

# python/sglang/srt/layers/quantization/moe_wna16.py
get_weight_perm(num_bits: int)
  MoeWNA16Config.__init__(linear_quant_method: str, weight_bits: int, group_size: int, has_zp: bool, lm_head_quantized: bool, modules_to_not_convert: Optional[List[str]], full_config: Dict[str, Any]) -> None
  MoeWNA16Config.get_name(cls) -> str
  MoeWNA16Config.get_supported_act_dtypes(cls) -> List[torch.dtype]
  MoeWNA16Config.get_min_capability(cls) -> int
  MoeWNA16Config.get_config_filenames(cls) -> List[str]
  MoeWNA16Config.get_scaled_act_names() -> List[str]
  MoeWNA16Config.from_config(cls, config: Dict[str, Any]) -> MoeWNA16Config
  MoeWNA16Config.override_quantization_method(cls, hf_quant_cfg, user_quant) -> Optional[str]
  MoeWNA16Config.is_moe_wna16_compatible(cls, quant_config: Dict[str, Any])
  MoeWNA16Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
is_layer_skipped_quant(prefix: str, modules_to_not_convert: List[str])
  MoeWNA16Method.__init__(quant_config: MoeWNA16Config)
  MoeWNA16Method.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)
  MoeWNA16Method.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  MoeWNA16Method.get_weight_loader(layer, weight_loader)

# python/sglang/srt/layers/quantization/mxfp4.py
  Mxfp4Config.__init__(ignored_layers: Optional[list[str]], is_checkpoint_mxfp4_serialized: bool)
  Mxfp4Config.from_config(cls, config)
  Mxfp4Config.get_min_capability(cls) -> int
  Mxfp4Config.get_name(cls) -> str
  Mxfp4Config.get_supported_act_dtypes(cls) -> list[torch.dtype]
  Mxfp4Config.get_config_filenames(cls) -> list[str]
  Mxfp4Config.is_static_cfg()
  Mxfp4Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional['QuantizeMethodBase']
  Mxfp4Config.get_scaled_act_names() -> List[str]
  Mxfp4MoEMethod.__init__(prefix: str)
  Mxfp4MoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype, with_bias: bool)
  Mxfp4MoEMethod.process_weights_after_loading(layer)
  Mxfp4MoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  Mxfp4DynamicQuantMoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)
  Mxfp4DynamicQuantMoEMethod.mxfp4_quantize(w)
  Mxfp4DynamicQuantMoEMethod.process_weights_after_loading(layer: Module) -> None
  Mxfp4DynamicQuantMoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor

# python/sglang/srt/layers/quantization/mxfp4_tensor.py
  MXFP4QuantizeUtil.quantize(cls, input: torch.Tensor, block_size: Optional[int]) -> tuple
  MXFP4QuantizeUtil.dequantize(cls, quantized_data, dtype: torch.dtype, scale, block_sizes)

# python/sglang/srt/layers/quantization/petit.py
  PetitNvFp4Config.__init__(is_checkpoint_nvfp4_serialized: bool, kv_cache_quant_algo: str, group_size: int, exclude_modules: List[str]) -> None
  PetitNvFp4Config.get_name(cls) -> str
  PetitNvFp4Config.get_supported_act_dtypes(cls) -> List[torch.dtype]
  PetitNvFp4Config.get_min_capability(cls) -> int
  PetitNvFp4Config.get_config_filenames(cls) -> List[str]
  PetitNvFp4Config.from_config(cls, config: Dict[str, Any]) -> 'PetitNvFp4Config'
  PetitNvFp4Config.override_quantization_method(cls, hf_quant_cfg, user_quant) -> Optional[str]
  PetitNvFp4Config.is_petit_nvfp4_compatible(cls, quant_config: Dict[str, Any]) -> bool
  PetitNvFp4Config.is_layer_excluded(prefix: str, exclude_modules: list)
  PetitNvFp4Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional['QuantizeMethodBase']
  PetitNvFp4Config.get_scaled_act_names() -> List[str]
  PetitNvFp4LinearMethod.__init__(quant_config: PetitNvFp4Config)
  PetitNvFp4LinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  PetitNvFp4LinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  PetitNvFp4LinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor

# python/sglang/srt/layers/quantization/petit_utils.py
prepare_nvfp4_layer_for_petit(layer: torch.nn.Module) -> None
apply_petit_nvfp4_linear(input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, weight_scale_2: torch.Tensor, size_n: int, size_k: int, bias: Optional[torch.Tensor]) -> torch.Tensor
verify_petit_nvfp4_supported(quant_method: str, group_size: Optional[int]) -> None
prepare_nvfp4_layer_for_petit(layer: torch.nn.Module) -> None
apply_petit_nvfp4_linear(input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, weight_scale_2: torch.Tensor, size_n: int, size_k: int, bias: Optional[torch.Tensor]) -> torch.Tensor

# python/sglang/srt/layers/quantization/qoq.py
  QoQConfig.__init__(weight_bits: int, group_size: int) -> None
  QoQConfig.__repr__() -> str
  QoQConfig.get_supported_act_dtypes(cls) -> List[torch.dtype]
  QoQConfig.get_min_capability(cls) -> int
  QoQConfig.get_name(cls) -> str
  QoQConfig.get_config_filenames(cls) -> List[str]
  QoQConfig.from_config(cls, config: Dict[str, Any]) -> QoQConfig
  QoQConfig.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  QoQConfig.get_scaled_act_names() -> List[str]
  QoQLinearMethod.__init__(quant_config: QoQConfig)
  QoQLinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  QoQLinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  QoQLinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])

# python/sglang/srt/layers/quantization/unquant.py
  UnquantizedEmbeddingMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  UnquantizedEmbeddingMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  UnquantizedEmbeddingMethod.embedding(layer: torch.nn.Module, input_: torch.Tensor) -> torch.Tensor
  UnquantizedLinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  UnquantizedLinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  UnquantizedLinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  UnquantizedFusedMoEMethod.__init__(use_triton_kernels: bool)
  UnquantizedFusedMoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype, with_bias: bool)
  UnquantizedFusedMoEMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  UnquantizedFusedMoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  UnquantizedFusedMoEMethod.forward_cuda(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  UnquantizedFusedMoEMethod.forward_cpu(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  UnquantizedFusedMoEMethod.forward_npu(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  UnquantizedFusedMoEMethod.forward_tpu() -> torch.Tensor

# python/sglang/srt/layers/quantization/utils.py
get_scalar_types()
is_layer_skipped(prefix: str, ignored_layers: List[str], fused_mapping: Mapping[str, List[str]]) -> bool
per_tensor_dequantize(tensor: torch.Tensor, inv_scale: Union[float, torch.Tensor]) -> torch.Tensor
all_close_1d(x: torch.Tensor) -> bool
convert_to_channelwise(weight_scale: torch.Tensor, logical_widths: List[int]) -> Tuple[torch.Tensor, torch.Tensor]
requantize_with_max_scale(weight: torch.Tensor, weight_scale: torch.Tensor, logical_widths: List[int]) -> Tuple[torch.Tensor, torch.Tensor]
update_tensor_inplace(old: torch.Tensor, new: torch.Tensor) -> None
replace_parameter(mod: torch.nn.Module, name: str, new: Union[torch.Tensor, torch.nn.Parameter]) -> None
assert_fp8_all_close(a: torch.Tensor, b: torch.Tensor)
override_config(config: QuantizationConfig, prefix: str)
get_dynamic_override(config: QuantizationConfig, layer_name: str, key: Optional[str], default_value: Union[int, bool, None]) -> Union[Dict, int, bool, None]
get_linear_quant_method(config: QuantizationConfig, layer: torch.nn.Module, prefix: str, linear_method_cls: type)
get_pack_factor(num_bits)
permute_rows(q_w: torch.Tensor, w_ref: torch.Tensor, group_size: int, test_perm: Optional[torch.Tensor])
pack_cols(q_w: torch.Tensor, num_bits: int, size_k: int, size_n: int)
pack_rows(q_w: torch.Tensor, num_bits: int, size_k: int, size_n: int)
unpack_cols(packed_q_w: torch.Tensor, num_bits: int, size_k: int, size_n: int)
quantize_weights(w: torch.Tensor, quant_type: ScalarType, group_size: Optional[int], zero_points: bool, ref_zero_points_after_scales: bool)
gptq_quantize_weights(w: torch.Tensor, quant_type: ScalarType, group_size: int, act_order: bool, test_perm: Optional[torch.Tensor])
sort_weights(q_w: torch.Tensor, g_idx: torch.Tensor)

# python/sglang/srt/layers/quantization/w4afp8.py
  W4AFp8Config.__init__(is_checkpoint_fp8_serialized: bool, is_checkpoint_w4afp8_serialized: bool, linear_activation_scheme: str, moe_activation_scheme: str, ignored_layers: Optional[List[str]], weight_block_size: Optional[List[int]], group_size: int) -> None
  W4AFp8Config.get_name(cls) -> str
  W4AFp8Config.get_supported_act_dtypes(cls) -> List[torch.dtype]
  W4AFp8Config.get_min_capability(cls) -> int
  W4AFp8Config.get_config_filenames(cls) -> List[str]
  W4AFp8Config.from_config(cls, config: Dict[str, Any]) -> W4AFp8Config
  W4AFp8Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  W4AFp8Config.get_scaled_act_names() -> List[str]
  W4AFp8MoEMethod.__init__(quant_config: W4AFp8Config)
  W4AFp8MoEMethod.create_weights(layer: EPMoE, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)
  W4AFp8MoEMethod.process_weights_after_loading(layer: Module) -> None
  W4AFp8MoEMethod.apply(layer: EPMoE, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor

# python/sglang/srt/layers/quantization/w8a8_fp8.py
  W8A8Fp8Config.__init__(is_checkpoint_fp8_serialized: bool)
  W8A8Fp8Config.get_supported_act_dtypes(cls) -> List[torch.dtype]
  W8A8Fp8Config.get_min_capability(cls) -> int
  W8A8Fp8Config.get_name() -> str
  W8A8Fp8Config.get_config_filenames(cls) -> List[str]
  W8A8Fp8Config.from_config(cls, config: Dict[str, Any]) -> W8A8Fp8Config
  W8A8Fp8Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  W8A8Fp8Config.get_scaled_act_names() -> List[str]
  W8A8Fp8LinearMethod.__init__(quantization_config: W8A8Fp8Config)
  W8A8Fp8LinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  W8A8Fp8LinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  W8A8Fp8LinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
  W8A8FP8MoEMethod.__init__(quant_config: W8A8Fp8Config)
  W8A8FP8MoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)
  W8A8FP8MoEMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  W8A8FP8MoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor

# python/sglang/srt/layers/quantization/w8a8_int8.py
npu_wrapper_rmsnorm_init(func)
npu_wrapper_rmsnorm_forward(func)
npu_fused_experts(hidden_states: torch.Tensor, w13: torch.Tensor, w13_scale: torch.Tensor, w2: torch.Tensor, w2_scale: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, top_k: int)
  W8A8Int8Config.__init__(quant_config: Dict[str, Any])
  W8A8Int8Config.get_supported_act_dtypes(cls) -> List[torch.dtype]
  W8A8Int8Config.get_min_capability(cls) -> int
  W8A8Int8Config.get_name() -> str
  W8A8Int8Config.get_config_filenames(cls) -> List[str]
  W8A8Int8Config.from_config(cls, config: Dict[str, Any]) -> W8A8Int8Config
  W8A8Int8Config.get_quant_method(layer: torch.nn.Module, prefix: str) -> Optional[QuantizeMethodBase]
  W8A8Int8Config.is_layer_skipped(prefix: str, fused_mapping: Mapping[str, List[str]])
  W8A8Int8Config.get_scaled_act_names() -> List[str]
  W8A8Int8LinearMethod.__init__(quantization_config: W8A8Int8Config)
  W8A8Int8LinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  W8A8Int8LinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
  W8A8Int8LinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
  W8A8Int8MoEMethod.__init__(quant_config: W8A8Int8Config)
  W8A8Int8MoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)
  W8A8Int8MoEMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  W8A8Int8MoEMethod.apply(layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
  NPU_W8A8LinearMethodImpl.__init__() -> None
  NPU_W8A8LinearMethodImpl.get_weight(input_size: int, output_size: int, params_dtype: torch.dtype) -> Dict[str, Any]
  NPU_W8A8LinearMethodImpl.get_pertensor_param(params_dtype: torch.dtype) -> Dict[str, Any]
  NPU_W8A8LinearMethodImpl.get_perchannel_param(output_size: int, params_dtype: torch.dtype) -> Dict[str, Any]
  NPU_W8A8LinearMethodImpl.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  NPU_W8A8LinearMethodImpl.process_weights_after_loading(layer)
  NPU_W8A8LinearMethodMTImpl.__init__() -> None
  NPU_W8A8LinearMethodMTImpl.get_weight(input_size: int, output_size: int, params_dtype: torch.dtype) -> Dict[str, Any]
  NPU_W8A8LinearMethodMTImpl.get_pertensor_param(params_dtype: torch.dtype) -> Dict[str, Any]
  NPU_W8A8LinearMethodMTImpl.get_perchannel_param(output_size: int, params_dtype: torch.dtype) -> Dict[str, Any]
  NPU_W8A8LinearMethodMTImpl.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  NPU_W8A8LinearMethodMTImpl.process_weights_after_loading(layer)
  NPU_W8A8LinearMethod.__init__(quantization_config: W8A8Int8Config) -> None
  NPU_W8A8LinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype) -> None
  NPU_W8A8LinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  NPU_W8A8LinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  NPU_W8A8DynamicLinearMethodImpl.__init__()
  NPU_W8A8DynamicLinearMethodImpl.get_weight(input_size: int, output_size: int, params_dtype: torch.dtype) -> Dict[str, Any]
  NPU_W8A8DynamicLinearMethodImpl.get_pertensor_param(params_dtype: torch.dtype) -> Dict[str, Any]
  NPU_W8A8DynamicLinearMethodImpl.get_perchannel_param(output_size: int, params_dtype: torch.dtype) -> Dict[str, Any]
  NPU_W8A8DynamicLinearMethodImpl.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor], tp_rank: Optional[int]) -> torch.Tensor
  NPU_W8A8DynamicLinearMethodImpl.process_weights_after_loading(layer)
  NPU_W8A8DynamicLinearMethod.__init__(quantization_config: W8A8Int8Config) -> None
  NPU_W8A8DynamicLinearMethod.create_weights(layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype) -> None
  NPU_W8A8DynamicLinearMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  NPU_W8A8DynamicLinearMethod.apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor]) -> torch.Tensor
  NPU_W8A8MoEMethod.__init__(quantization_config: W8A8Int8Config) -> None
  NPU_W8A8MoEMethod.create_weights(layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype) -> None
  NPU_W8A8MoEMethod.process_weights_after_loading(layer: torch.nn.Module) -> None
  NPU_W8A8MoEMethod.apply(layer, x, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig) -> torch.Tensor
