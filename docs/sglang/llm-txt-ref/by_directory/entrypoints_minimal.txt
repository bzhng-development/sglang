
# python/sglang/srt/entrypoints/EngineBase.py
  EngineBase.generate(prompt: Optional[Union[List[str], str]], sampling_params: Optional[Union[List[Dict], Dict]], input_ids: Optional[Union[List[List[int]], List[int]]], image_data: Optional[Union[List[str], str]], return_logprob: Optional[Union[List[bool], bool]], logprob_start_len: Optional[Union[List[int], int]], top_logprobs_num: Optional[Union[List[int], int]], token_ids_logprob: Optional[Union[List[List[int]], List[int]]], lora_path: Optional[Union[List[Optional[str]], Optional[str]]], custom_logit_processor: Optional[Union[List[str], str]], return_hidden_states: Optional[bool], stream: Optional[bool], bootstrap_host: Optional[Union[List[str], str]], bootstrap_port: Optional[Union[List[int], int]], bootstrap_room: Optional[Union[List[int], int]], data_parallel_rank: Optional[int]) -> Union[Dict, Iterator[Dict]]
  EngineBase.flush_cache()
  EngineBase.update_weights_from_tensor(named_tensors: List[Tuple[str, torch.Tensor]], load_format: Optional[str], flush_cache: bool)
  EngineBase.load_lora_adapter(lora_name: str, lora_path: str)
  EngineBase.unload_lora_adapter(lora_name: str)
  EngineBase.release_memory_occupation()
  EngineBase.resume_memory_occupation()
  EngineBase.shutdown()

# python/sglang/srt/entrypoints/context.py
  ConversationContext.append_output(output) -> None
  ConversationContext.call_tool() -> list[Message]
  ConversationContext.need_builtin_tool_call() -> bool
  ConversationContext.render_for_completion() -> list[int]
  SimpleContext.__init__()
  SimpleContext.append_output(output) -> None
  SimpleContext.need_builtin_tool_call() -> bool
  SimpleContext.call_tool() -> list[Message]
  SimpleContext.render_for_completion() -> list[int]
  HarmonyContext.__init__(messages: list, tool_sessions: dict[str, Union['ClientSession', Tool]])
  HarmonyContext.append_output(output) -> None
  HarmonyContext.messages() -> list
  HarmonyContext.need_builtin_tool_call() -> bool
  HarmonyContext.call_tool() -> list[Message]
  HarmonyContext.render_for_completion() -> list[int]
  HarmonyContext.call_search_tool(tool_session: Union['ClientSession', Tool], last_msg: Message) -> list[Message]
  HarmonyContext.call_python_tool(tool_session: Union['ClientSession', Tool], last_msg: Message) -> list[Message]
  StreamingHarmonyContext.__init__()
  StreamingHarmonyContext.messages() -> list
  StreamingHarmonyContext.append_output(output) -> None
  StreamingHarmonyContext.is_expecting_start() -> bool
  StreamingHarmonyContext.is_assistant_action_turn() -> bool
  StreamingHarmonyContext.render_for_completion() -> list[int]

# python/sglang/srt/entrypoints/engine.py
  Engine.__init__()
  Engine.generate(prompt: Optional[Union[List[str], str]], sampling_params: Optional[Union[List[Dict], Dict]], input_ids: Optional[Union[List[List[int]], List[int]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat], return_logprob: Optional[Union[List[bool], bool]], logprob_start_len: Optional[Union[List[int], int]], top_logprobs_num: Optional[Union[List[int], int]], token_ids_logprob: Optional[Union[List[List[int]], List[int]]], lora_path: Optional[List[Optional[str]]], custom_logit_processor: Optional[Union[List[str], str]], return_hidden_states: bool, stream: bool, bootstrap_host: Optional[Union[List[str], str]], bootstrap_port: Optional[Union[List[int], int]], bootstrap_room: Optional[Union[List[int], int]], data_parallel_rank: Optional[int]) -> Union[Dict, Iterator[Dict]]
  Engine.async_generate(prompt: Optional[Union[List[str], str]], sampling_params: Optional[Union[List[Dict], Dict]], input_ids: Optional[Union[List[List[int]], List[int]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat], return_logprob: Optional[Union[List[bool], bool]], logprob_start_len: Optional[Union[List[int], int]], top_logprobs_num: Optional[Union[List[int], int]], token_ids_logprob: Optional[Union[List[List[int]], List[int]]], lora_path: Optional[List[Optional[str]]], custom_logit_processor: Optional[Union[List[str], str]], return_hidden_states: bool, stream: bool, bootstrap_host: Optional[Union[List[str], str]], bootstrap_port: Optional[Union[List[int], int]], bootstrap_room: Optional[Union[List[int], int]], data_parallel_rank: Optional[int]) -> Union[Dict, AsyncIterator[Dict]]
  Engine.encode(prompt: Union[str, List[str], List[Dict], List[List[Dict]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat]) -> Dict
  Engine.async_encode(prompt: Union[str, List[str], List[Dict], List[List[Dict]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat]) -> Dict
  Engine.rerank(prompt: Union[List[List[str]]]) -> Dict
  Engine.shutdown()
  Engine.__enter__()
  Engine.__exit__(exc_type, exc_value, traceback)
  Engine.flush_cache()
  Engine.start_profile()
  Engine.stop_profile()
  Engine.start_expert_distribution_record()
  Engine.stop_expert_distribution_record()
  Engine.dump_expert_distribution_record()
  Engine.get_server_info()
  Engine.init_weights_update_group(master_address: str, master_port: int, rank_offset: int, world_size: int, group_name: str, backend: str)
  Engine.update_weights_from_distributed(names: list[str], dtypes: list[str], shapes: list[list[int]], group_name: str, flush_cache: bool)
  Engine.update_weights_from_tensor(named_tensors: List[Tuple[str, torch.Tensor]], load_format: Optional[str], flush_cache: bool)
  Engine.update_weights_from_disk(model_path: str, load_format: Optional[str])
  Engine.get_weights_by_name(name: str, truncate_size: int)
  Engine.load_lora_adapter(lora_name: str, lora_path: str, pinned: bool)
  Engine.unload_lora_adapter(lora_name: str)
  Engine.release_memory_occupation(tags: Optional[List[str]])
  Engine.resume_memory_occupation(tags: Optional[List[str]])
  Engine.freeze_gc()
  Engine.collective_rpc(method: str)
  Engine.save_remote_model()
  Engine.save_sharded_model()
  Engine.score(query: Optional[Union[str, List[int]]], items: Optional[Union[str, List[str], List[List[int]]]], label_token_ids: Optional[List[int]], apply_softmax: bool, item_first: bool) -> List[List[float]]
  Engine.async_score(query: Optional[Union[str, List[int]]], items: Optional[Union[str, List[str], List[List[int]]]], label_token_ids: Optional[List[int]], apply_softmax: bool, item_first: bool) -> List[List[float]]

# python/sglang/srt/entrypoints/harmony_utils.py
get_encoding()
get_system_message(model_identity: Optional[str], reasoning_effort: Optional[Literal['high', 'medium', 'low']], start_date: Optional[str], browser_description: Optional[str], python_description: Optional[str]) -> Message
get_developer_message(instructions: Optional[str], tools: Optional[list[Tool]]) -> Message
get_user_message(content: str) -> Message
parse_response_input(response_msg: ResponseInputOutputItem, prev_responses: list[Union[ResponseOutputItem, ResponseReasoningItem]]) -> Message
parse_response_output(output: ResponseOutputItem) -> Message
parse_chat_input(chat_msg) -> Message
render_for_completion(messages: list[Message]) -> list[int]
get_stop_tokens_for_assistant_actions() -> list[int]
get_streamable_parser_for_assistant() -> StreamableParser
parse_output_message(message: Message)
parse_remaining_state(parser: StreamableParser)
parse_output_into_messages(token_ids: Iterable[int])

# python/sglang/srt/entrypoints/http_server.py
set_global_state(global_state: _GlobalState)
lifespan(fast_api_app: FastAPI)
validation_exception_handler(request: Request, exc: HTTPException)
validation_exception_handler(request: Request, exc: RequestValidationError)
validate_json_request(raw_request: Request)
health_generate(request: Request) -> Response
get_model_info()
get_weight_version()
get_server_info()
get_load()
set_internal_state(obj: SetInternalStateReq, request: Request)
generate_request(obj: GenerateReqInput, request: Request)
generate_from_file_request(file: UploadFile, request: Request)
encode_request(obj: EmbeddingReqInput, request: Request)
classify_request(obj: EmbeddingReqInput, request: Request)
flush_cache()
clear_hicache_storage_backend()
start_profile_async(obj: Optional[ProfileReqInput])
stop_profile_async()
freeze_gc_async()
start_expert_distribution_record_async()
stop_expert_distribution_record_async()
dump_expert_distribution_record_async()
update_weights_from_disk(obj: UpdateWeightFromDiskReqInput, request: Request)
init_weights_update_group(obj: InitWeightsUpdateGroupReqInput, request: Request)
update_weights_from_tensor(obj: UpdateWeightsFromTensorReqInput, request: Request)
update_weights_from_distributed(obj: UpdateWeightsFromDistributedReqInput, request: Request)
update_weight_version(obj: UpdateWeightVersionReqInput, request: Request)
get_weights_by_name(obj: GetWeightsByNameReqInput, request: Request)
release_memory_occupation(obj: ReleaseMemoryOccupationReqInput, request: Request)
resume_memory_occupation(obj: ResumeMemoryOccupationReqInput, request: Request)
slow_down(obj: SlowDownReqInput, request: Request)
load_lora_adapter(obj: LoadLoRAAdapterReqInput, request: Request)
unload_lora_adapter(obj: UnloadLoRAAdapterReqInput, request: Request)
open_session(obj: OpenSessionReqInput, request: Request)
close_session(obj: CloseSessionReqInput, request: Request)
configure_logging(obj: ConfigureLoggingReq, request: Request)
abort_request(obj: AbortReq, request: Request)
parse_function_call_request(obj: ParseFunctionCallReq, request: Request)
separate_reasoning_request(obj: SeparateReasoningReqInput, request: Request)
pause_generation(request: Request)
continue_generation(request: Request)
openai_v1_completions(request: CompletionRequest, raw_request: Request)
openai_v1_chat_completions(request: ChatCompletionRequest, raw_request: Request)
openai_v1_embeddings(request: EmbeddingRequest, raw_request: Request)
available_models()
retrieve_model(model: str)
v1_score_request(request: ScoringRequest, raw_request: Request)
v1_responses_request(request: dict, raw_request: Request)
v1_retrieve_responses(response_id: str, raw_request: Request)
v1_cancel_responses(response_id: str, raw_request: Request)
v1_rerank_request(request: V1RerankReqInput, raw_request: Request)
sagemaker_health() -> Response
sagemaker_chat_completions(request: ChatCompletionRequest, raw_request: Request)
vertex_generate(vertex_req: VertexGenerateReqInput, raw_request: Request)
launch_server(server_args: ServerArgs, pipe_finish_writer: Optional[multiprocessing.connection.Connection], launch_callback: Optional[Callable[[], None]])

# python/sglang/srt/entrypoints/http_server_engine.py
launch_server_process(server_args: ServerArgs) -> multiprocessing.Process
  HttpServerEngineAdapter.__init__()
  HttpServerEngineAdapter.update_weights_from_tensor(named_tensors: List[Tuple[str, torch.Tensor]], load_format: Optional[str], flush_cache: bool)
  HttpServerEngineAdapter.shutdown()
  HttpServerEngineAdapter.generate(prompt, sampling_params, input_ids, image_data, return_logprob, logprob_start_len, top_logprobs_num, token_ids_logprob, lora_path, custom_logit_processor)
  HttpServerEngineAdapter.release_memory_occupation()
  HttpServerEngineAdapter.resume_memory_occupation()
  HttpServerEngineAdapter.flush_cache()

# python/sglang/srt/entrypoints/tool.py
  Tool.get_result(context: 'ConversationContext') -> Any
  HarmonyBrowserTool.__init__()
  HarmonyBrowserTool.get_result(context: 'ConversationContext') -> Any
  HarmonyBrowserTool.tool_config() -> Any
  HarmonyPythonTool.__init__()
  HarmonyPythonTool.get_result(context: 'ConversationContext') -> Any
  HarmonyPythonTool.tool_config() -> Any
