
# python/sglang/srt/layers/activation.py
  SiluAndMul.forward_native(x: torch.Tensor) -> torch.Tensor
  SiluAndMul.forward_cuda(x: torch.Tensor) -> torch.Tensor
  SiluAndMul.forward_cpu(x: torch.Tensor) -> torch.Tensor
  SiluAndMul.forward_npu(x: torch.Tensor) -> torch.Tensor
  GeluAndMul.__init__(approximate)
  GeluAndMul.forward_native(x: torch.Tensor) -> torch.Tensor
  GeluAndMul.forward_cuda(x: torch.Tensor) -> torch.Tensor
  GeluAndMul.forward_npu(x: torch.Tensor) -> torch.Tensor
  NewGELU.forward_native(x: torch.Tensor) -> torch.Tensor
  NewGELU.forward_cuda(x: torch.Tensor) -> torch.Tensor
  ReLU2.forward(x: torch.Tensor) -> torch.Tensor
  QuickGELU.forward_native(x: torch.Tensor) -> torch.Tensor
  QuickGELU.forward_cuda(x: torch.Tensor) -> torch.Tensor
  QuickGELU.forward_hip(x: torch.Tensor) -> torch.Tensor
  QuickGELU.forward_npu(x: torch.Tensor) -> torch.Tensor
  ScaledActivation.__init__(act_module: nn.Module, intermediate_size: int, input_is_parallel: bool, params_dtype: Optional[torch.dtype])
  ScaledActivation.forward(x: torch.Tensor) -> torch.Tensor
  ScaledActivation.weight_loader(param: nn.Parameter, loaded_weight: torch.Tensor)
get_act_fn(act_fn_name: str, quant_config: Optional[QuantizationConfig], intermediate_size: Optional[int], input_is_parallel: bool, params_dtype: Optional[torch.dtype]) -> nn.Module
get_cross_encoder_activation_function(config: PretrainedConfig)

# python/sglang/srt/layers/amx_utils.py
amx_process_weight_after_loading(weight)
dim_is_supported(weight)
  PackWeightMethod.__init__(weight_names, transpose_dims)
  PackWeightMethod.process_weights_after_loading(module) -> None

# python/sglang/srt/layers/communicator.py
  ScatterMode.model_input_output()
  _LayerModeComputationContext.previous_layer()
  LayerScatterModes.init_new(cls)
enable_moe_dense_fully_dp()
  LayerCommunicator.__init__(layer_scatter_modes: LayerScatterModes, input_layernorm: torch.nn.Module, post_attention_layernorm: torch.nn.Module, allow_reduce_scatter: bool, is_last_layer: bool)
  LayerCommunicator.prepare_attn(hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch)
  LayerCommunicator.prepare_mlp(hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch)
  LayerCommunicator.postprocess_layer(hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch)
  LayerCommunicator.should_use_reduce_scatter(forward_batch: ForwardBatch)
  LayerCommunicator.should_fuse_mlp_allreduce_with_next_layer(forward_batch: ForwardBatch) -> bool
  CommunicateContext.is_same_group_size(a: ScatterMode, b: ScatterMode)
  CommunicateContext.init_new(cls)
  CommunicateSimpleFn.get_fn(input_mode: ScatterMode, output_mode: ScatterMode, context: CommunicateContext)
  CommunicateWithAllReduceAndLayerNormFn.get_fn(hidden_states_input_mode: ScatterMode, residual_input_mode: ScatterMode, hidden_states_output_mode: ScatterMode, residual_output_mode: ScatterMode, context: CommunicateContext)
  CommunicateSummableTensorPairFn.execute(cls, hidden_states_input_mode, residual_input_mode, output_mode, context)
  CommunicateSummableTensorPairFn.get_fn(hidden_states_input_mode: ScatterMode, residual_input_mode: ScatterMode, output_mode: ScatterMode, context: CommunicateContext)

# python/sglang/srt/layers/dp_attention.py
  DpPaddingMode.is_max_len()
  DpPaddingMode.is_sum_len()
  DpPaddingMode.get_dp_padding_mode(cls, global_num_tokens: List[int]) -> DpPaddingMode
  DpPaddingMode.get_default_mode_in_cuda_graph(cls) -> DpPaddingMode
  _DpGatheredBufferWrapper.set_metadata(cls, hidden_size: int, dtype: torch.dtype, device: torch.device)
  _DpGatheredBufferWrapper.set_dp_buffer_len(cls, global_dp_buffer_len: int, local_dp_buffer_len: int, global_num_tokens: Optional[List[int]])
  _DpGatheredBufferWrapper.get_global_dp_buffer(cls) -> torch.Tensor
  _DpGatheredBufferWrapper.get_local_dp_buffer(cls) -> torch.Tensor
  _DpGatheredBufferWrapper.get_global_dp_buffer_len(cls) -> int
  _DpGatheredBufferWrapper.get_local_dp_buffer_len(cls) -> int
  _DpGatheredBufferWrapper.get_dp_global_num_tokens(cls) -> List[int]
set_dp_buffer_len(global_dp_buffer_len: int, local_dp_buffer_len: int, global_num_tokens: Optional[List[int]])
get_global_dp_buffer() -> torch.Tensor
get_local_dp_buffer() -> torch.Tensor
get_global_dp_buffer_len() -> int
get_local_dp_buffer_len() -> int
get_dp_global_num_tokens() -> List[int]
compute_dp_attention_world_info(enable_dp_attention, tp_rank, tp_size, dp_size)
compute_dp_attention_local_info(enable_dp_attention, tp_rank, tp_size, dp_size, moe_dense_tp_size)
initialize_dp_attention(server_args: ServerArgs, model_config: ModelConfig)
is_dp_attention_enabled() -> bool
get_attention_tp_group() -> GroupCoordinator
get_attention_tp_rank() -> int
get_attention_tp_size() -> int
get_attention_dp_rank() -> int
get_attention_dp_size() -> int
get_local_attention_dp_rank() -> int
get_local_attention_dp_size() -> int
disable_dp_size()
get_dp_local_info(forward_batch: ForwardBatch) -> Tuple[torch.Tensor, torch.Tensor]
memcpy_triton_kernel(dst_ptr, src_ptr, offset_ptr, sz_ptr, offset_src: tl.constexpr, chunk_size, BLOCK_SIZE: tl.constexpr)
prod(x)
memcpy_triton(dst, src, dim, offset, sz, offset_src)
dp_gather_partial(global_tokens: torch.Tensor, local_tokens: torch.Tensor, forward_batch: ForwardBatch)
dp_gather_replicate(global_tokens: torch.Tensor, local_tokens: torch.Tensor, forward_batch: ForwardBatch)
dp_scatter(local_tokens: torch.Tensor, global_tokens: torch.Tensor, forward_batch: ForwardBatch)
dp_reduce_scatter_tensor(output: torch.Tensor, input: torch.Tensor)
attn_tp_reduce_scatter_tensor(output: torch.Tensor, input: torch.Tensor)
attn_tp_all_gather_into_tensor(output: torch.Tensor, input: torch.Tensor)
attn_tp_all_gather(output_list: List[torch.Tensor], input: torch.Tensor)

# python/sglang/srt/layers/elementwise.py
fused_softcap_kernel(output_ptr, input_ptr, n_ele, softcap_const: tl.constexpr, BLOCK_SIZE: tl.constexpr)
fused_softcap(x, softcap_const, autotune)
  Softcap.__init__(softcap_const: float)
  Softcap.__call__()
  Softcap.forward(x: torch.Tensor) -> torch.Tensor
  Softcap.forward_native(x: torch.Tensor) -> torch.Tensor
  Softcap.forward_cuda(x: torch.Tensor, autotune) -> torch.Tensor
fused_dual_residual_rmsnorm_kernel(output_ptr, mid_ptr, activ_ptr, residual_ptr, weight1_ptr, weight2_ptr, eps: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)
fused_dual_residual_rmsnorm(x, residual, weight1, weight2, eps, autotune)
fused_rmsnorm_kernel(output_ptr, activ_ptr, weight_ptr, eps: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)
fused_rmsnorm(x, weight, eps, autotune, inplace)
  FusedDualResidualRMSNorm.__init__(rmsnorm1, rmsnorm2) -> None
  FusedDualResidualRMSNorm.__call__()
  FusedDualResidualRMSNorm.forward(x: torch.Tensor, residual: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
  FusedDualResidualRMSNorm.forward_cuda(x: torch.Tensor, residual: torch.Tensor, autotune) -> Tuple[torch.Tensor, torch.Tensor]
  FusedDualResidualRMSNorm.forward_flashinfer(x: torch.Tensor, residual: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
  FusedDualResidualRMSNorm.forward_native(x: torch.Tensor, residual: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
experts_combine_kernel(out_hidden_states, moe_hidden_states, mlp_hidden_states, combine_k: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)
experts_combine_triton(moe_hidden_states, mlp_hidden_states, output_buffer)
gelu_and_mul_kernel(out_hidden_states_ptr, out_scales_ptr, hidden_states_ptr, quant_max: tl.constexpr, static_scale: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)
gelu_and_mul_triton(hidden_states, scales, quantize, out)
silu_and_mul_kernel(out_hidden_states_ptr, out_scales_ptr, hidden_states_ptr, quant_max: tl.constexpr, static_scale: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)
silu_and_mul_triton(hidden_states, scales, quantize, out)

# python/sglang/srt/layers/flashinfer_comm_fusion.py
  FlashInferWorkspaceManager.__init__()
  FlashInferWorkspaceManager.initialize(world_size: int, rank: int, max_token_num: int, hidden_dim: int, group, use_fp32_lamport: bool)
  FlashInferWorkspaceManager.cleanup()
ensure_workspace_initialized(max_token_num: int, hidden_dim: int, use_fp32_lamport: bool)
flashinfer_allreduce_residual_rmsnorm(input_tensor: torch.Tensor, residual: torch.Tensor, weight: torch.Tensor, eps: float, max_token_num: int, use_oneshot: Optional[bool], trigger_completion_at_end: bool, fp32_acc: bool) -> Tuple[torch.Tensor, torch.Tensor]
fake_flashinfer_allreduce_residual_rmsnorm(input_tensor: torch.Tensor, residual: torch.Tensor, weight: torch.Tensor, eps: float, max_token_num: int, use_oneshot: Optional[bool], trigger_completion_at_end: bool, fp32_acc: bool) -> Tuple[torch.Tensor, torch.Tensor]
cleanup_flashinfer_workspace()

# python/sglang/srt/layers/layernorm.py
  RMSNorm.__init__(hidden_size: int, eps: float, var_hidden_size: Optional[int]) -> None
  RMSNorm.forward_cuda(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  RMSNorm.forward_npu(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  RMSNorm.forward_aiter(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  RMSNorm.forward_hip(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  RMSNorm.forward_native(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  RMSNorm.forward_cpu(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  RMSNorm.forward_with_allreduce_fusion(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  GemmaRMSNorm.__init__(hidden_size: int, eps: float) -> None
  GemmaRMSNorm.forward_native(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  GemmaRMSNorm.forward_cuda(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  GemmaRMSNorm.forward_npu(x: torch.Tensor, residual: Optional[torch.Tensor]) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
  Gemma3RMSNorm.__init__(dim: int, eps: float)
  Gemma3RMSNorm.forward_native(x)
  Gemma3RMSNorm.forward_cuda(x)
  Gemma3RMSNorm.forward_npu(x)
  Gemma3RMSNorm.extra_repr()

# python/sglang/srt/layers/linear.py
adjust_marlin_shard(param, shard_size, shard_offset)
adjust_bitsandbytes_4bit_shard(param: Parameter, shard_offsets: Dict[str, Tuple[int, int]], loaded_shard_id: str) -> Tuple[int, int]
adjust_scalar_to_fused_array(param, loaded_weight, shard_id)
adjust_shard_offsets(shard_offsets, loaded_weight, dim)
  LinearBase.__init__(input_size: int, output_size: int, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str)
  LinearBase.forward(x: torch.Tensor) -> torch.Tensor
  ReplicatedLinear.__init__(input_size: int, output_size: int, bias: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str)
  ReplicatedLinear.weight_loader(param: Parameter, loaded_weight: torch.Tensor)
  ReplicatedLinear.forward(x: torch.Tensor) -> Tuple[torch.Tensor, Optional[torch.Tensor]]
  ReplicatedLinear.extra_repr() -> str
  ColumnParallelLinear.__init__(input_size: int, output_size: int, bias: bool, gather_output: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], output_sizes: Optional[List[int]], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], use_presharded_weights: bool)
  ColumnParallelLinear.weight_loader(param: Parameter, loaded_weight: torch.Tensor)
  ColumnParallelLinear.weight_loader_v2(param: Parameter, loaded_weight: torch.Tensor)
  ColumnParallelLinear.forward(input_)
  ColumnParallelLinear.extra_repr() -> str
  MergedColumnParallelLinear.__init__(input_size: int, output_sizes: List[int], bias: bool, gather_output: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], use_presharded_weights: bool)
  MergedColumnParallelLinear.weight_loader(param: Parameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[int])
  MergedColumnParallelLinear.weight_loader_v2(param: BasevLLMParameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[int])
  QKVParallelLinear.__init__(hidden_size: int, head_size: int, total_num_heads: int, total_num_kv_heads: Optional[int], bias: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], load_presharded_attn: bool)
  QKVParallelLinear.weight_loader_v2(param: BasevLLMParameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[str])
  QKVParallelLinear.weight_loader(param: Parameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[str])
  RowParallelLinear.__init__(input_size: int, output_size: int, bias: bool, input_is_parallel: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], reduce_results: bool, quant_config: Optional[QuantizationConfig], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], use_presharded_weights: bool)
  RowParallelLinear.weight_loader(param: Parameter, loaded_weight: torch.Tensor)
  RowParallelLinear.weight_loader_v2(param: BasevLLMParameter, loaded_weight: torch.Tensor)
  RowParallelLinear.forward(input_, skip_all_reduce)
  RowParallelLinear.extra_repr() -> str

# python/sglang/srt/layers/logits_processor.py
  LogitsMetadata.from_forward_batch(cls, forward_batch: ForwardBatch)
  LogitsMetadata.compute_dp_attention_metadata()
  LogitsProcessor.__init__(config, skip_all_gather: bool, logit_scale: Optional[float])
  LogitsProcessor.forward(input_ids, hidden_states, lm_head: VocabParallelEmbedding, logits_metadata: Union[LogitsMetadata, ForwardBatch], aux_hidden_states: Optional[torch.Tensor]) -> LogitsProcessorOutput
  LogitsProcessor.get_top_logprobs(all_logprobs: torch.Tensor, logits_metadata: LogitsMetadata)
  LogitsProcessor.get_token_ids_logprobs(all_logprobs: torch.Tensor, logits_metadata: LogitsMetadata)
  LogitsProcessor.compute_temp_top_p_normalized_logprobs(last_logits: torch.Tensor, logits_metadata: LogitsMetadata) -> torch.Tensor
fused_softcap_kernel(full_logits_ptr, softcapping_value, n_elements, BLOCK_SIZE: tl.constexpr)
fused_softcap(full_logits, final_logit_softcapping)

# python/sglang/srt/layers/multimodal.py
hash_tiles32_kernel_blocked(in_ptr, out_ptr, n_u32, seed1, seed2, FM_C1: tl.constexpr, FM_C2: tl.constexpr, POS_A: tl.constexpr, POS_B: tl.constexpr, TILE: tl.constexpr, BLOCK: tl.constexpr, USE_CG: tl.constexpr)
add_tree_reduce_u64_kernel(in_ptr, out_ptr, n_elems, CHUNK: tl.constexpr)
gpu_tensor_hash(tensor: torch.Tensor) -> int

# python/sglang/srt/layers/parameter.py
  BasevLLMParameter.__new__(cls, data: torch.Tensor)
  BasevLLMParameter.__init__(data: torch.Tensor, weight_loader: Callable)
  BasevLLMParameter.weight_loader()
  BasevLLMParameter.load_column_parallel_weight(loaded_weight: torch.Tensor)
  BasevLLMParameter.load_row_parallel_weight(loaded_weight: torch.Tensor)
  BasevLLMParameter.load_merged_column_weight(loaded_weight: torch.Tensor)
  BasevLLMParameter.load_qkv_weight(loaded_weight: torch.Tensor)
  _ColumnvLLMParameter.__init__(output_dim: int)
  _ColumnvLLMParameter.output_dim()
  _ColumnvLLMParameter.load_column_parallel_weight(loaded_weight: torch.Tensor, tp_rank: int, use_presharded_weights: bool)
  _ColumnvLLMParameter.load_merged_column_weight(loaded_weight: torch.Tensor)
  _ColumnvLLMParameter.load_qkv_weight(loaded_weight: torch.Tensor, tp_rank: int, use_presharded_weights: bool)
  RowvLLMParameter.__init__(input_dim: int)
  RowvLLMParameter.input_dim()
  RowvLLMParameter.load_row_parallel_weight(loaded_weight: torch.Tensor, tp_rank: int, use_presharded_weights: bool)
  PerTensorScaleParameter.__init__()
  PerTensorScaleParameter.load_row_parallel_weight()
  PerTensorScaleParameter.load_merged_column_weight()
  PerTensorScaleParameter.load_qkv_weight()
  PerTensorScaleParameter.load_column_parallel_weight()
  PackedColumnParameter.__init__(packed_factor: Union[int, Fraction], packed_dim: int, marlin_tile_size: Optional[int])
  PackedColumnParameter.packed_dim()
  PackedColumnParameter.packed_factor()
  PackedColumnParameter.marlin_tile_size()
  PackedColumnParameter.adjust_shard_indexes_for_packing(shard_size, shard_offset)
  PackedvLLMParameter.__init__(packed_factor: Union[int, Fraction], packed_dim: int, marlin_tile_size: Optional[int])
  PackedvLLMParameter.packed_dim()
  PackedvLLMParameter.packed_factor()
  PackedvLLMParameter.marlin_tile_size()
  PackedvLLMParameter.adjust_shard_indexes_for_packing(shard_size, shard_offset)
permute_param_layout_(param: BasevLLMParameter, input_dim: int, output_dim: int) -> BasevLLMParameter

# python/sglang/srt/layers/pooler.py
  Pooler.__init__(pooling_type: PoolingType, normalize: bool)
  Pooler.forward(hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> EmbeddingPoolerOutput
  CrossEncodingPooler.__init__(config: PretrainedConfig, classifier: nn.Module, pooler: Optional[nn.Module])
  CrossEncodingPooler.forward(hidden_states: torch.Tensor, forward_batch: ForwardBatch) -> EmbeddingPoolerOutput

# python/sglang/srt/layers/radix_attention.py
  RadixAttention.__init__(num_heads: int, head_dim: int, scaling: float, num_kv_heads: int, layer_id: int, logit_cap: float, v_head_dim: int, sliding_window_size: int, is_cross_attention: bool, pos_encoding_mode: str, logit_capping_method: str, quant_config: Optional[QuantizationConfig], attn_type: AttentionType, use_irope: bool, prefix: str)
  RadixAttention.forward(q, k, v, forward_batch: ForwardBatch, save_kv_cache: bool)

# python/sglang/srt/layers/rotary_embedding.py
  RotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype) -> None
  RotaryEmbedding.forward_native(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  RotaryEmbedding.forward_npu(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  RotaryEmbedding.forward_cpu(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  RotaryEmbedding.forward_cuda(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor], fused_set_kv_buffer_arg) -> Tuple[torch.Tensor, torch.Tensor]
  RotaryEmbedding.extra_repr() -> str
  LinearScalingRotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factors: Union[List[float], float], dtype: torch.dtype) -> None
  LinearScalingRotaryEmbedding.scaling_factor_to_offset() -> Dict[float, int]
  DynamicNTKScalingRotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype) -> None
  YaRNScalingRotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype) -> None
  Phi3LongRoPEScaledRotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, original_max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, short_factor: List[float], long_factor: List[float], short_mscale: Optional[float], long_mscale: Optional[float])
  Phi3LongRoPEScaledRotaryEmbedding.forward(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
yarn_get_mscale(scale: float, mscale: float) -> float
  DeepseekScalingRotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype) -> None
  DeepseekScalingRotaryEmbedding.forward_native(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  DeepseekScalingRotaryEmbedding.forward_npu(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  DeepseekScalingRotaryEmbedding.forward_cpu(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  Llama3RotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, scaling_factor: float, low_freq_factor: float, high_freq_factor: float, orig_max_position: int) -> None
  Llama4VisionRotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype)
  Llama4VisionRotaryEmbedding.forward(query: torch.Tensor, key: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
  DynamicNTKAlphaRotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_alpha: float, dtype: torch.dtype) -> None
  MRotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, mrope_section: Optional[List[int]]) -> None
  MRotaryEmbedding.forward(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]
  MRotaryEmbedding.get_rope_index(spatial_merge_size: int, image_token_id: int, video_token_id: int, vision_start_token_id: int, model_type: str, tokens_per_second: Optional[int], input_ids: Optional[torch.LongTensor], image_grid_thw: Optional[torch.LongTensor], video_grid_thw: Optional[torch.LongTensor], second_per_grid_ts: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  MRotaryEmbedding.get_rope_index_glm4v(input_ids: torch.Tensor, hf_config: Any, image_grid_thw: Union[list[list[int]], torch.Tensor], video_grid_thw: Union[list[list[int]], torch.Tensor], attention_mask: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]
  MRotaryEmbedding.get_next_input_positions(mrope_position_delta: int, context_len: int, seq_len: int) -> torch.Tensor
  DualChunkRotaryEmbedding.__init__(head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, chunk_size: int, local_size: int) -> None
  DualChunkRotaryEmbedding.forward(positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]
  DualChunkRotaryEmbedding.extra_repr() -> str
get_rope(head_size: int, rotary_dim: int, max_position: int, base: int, is_neox_style: bool, rope_scaling: Optional[Dict[str, Any]], dtype: Optional[torch.dtype], partial_rotary_factor: float, dual_chunk_attention_config: Optional[Dict[str, Any]]) -> RotaryEmbedding
rotate_half(x)
apply_rotary_pos_emb_native(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor, unsqueeze_dim) -> Tuple[torch.Tensor, torch.Tensor]
apply_rotary_pos_emb_npu(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor, unsqueeze_dim) -> Tuple[torch.Tensor, torch.Tensor]
get_rope_cpu(head_size: int, rotary_dim: int, max_position: int, base: int, is_neox_style: bool, rope_scaling: Optional[Dict[str, Any]], dtype: Optional[torch.dtype], partial_rotary_factor: float, device: Optional[str]) -> RotaryEmbedding
get_rope_wrapper(head_size: int, rotary_dim: int, max_position: int, base: int, is_neox_style: bool, rope_scaling: Optional[Dict[str, Any]], dtype: Optional[torch.dtype], partial_rotary_factor: float, device: Optional[str])

# python/sglang/srt/layers/sampler.py
  Sampler.__init__()
  Sampler.forward(logits_output: LogitsProcessorOutput, sampling_info: SamplingBatchInfo, return_logprob: bool, top_logprobs_nums: List[int], token_ids_logprobs: List[List[int]])
top_k_top_p_min_p_sampling_from_probs_torch(probs: torch.Tensor, top_ks: torch.Tensor, top_ps: torch.Tensor, min_ps: torch.Tensor, need_min_p_sampling: bool)
sampling_from_probs_torch(probs: torch.Tensor)
top_p_normalize_probs_torch(probs: torch.Tensor, top_ps: torch.Tensor)
get_top_logprobs(logprobs: torch.Tensor, top_logprobs_nums: List[int])
get_token_ids_logprobs(logprobs: torch.Tensor, token_ids_logprobs: List[List[int]])
apply_custom_logit_processor(logits: torch.Tensor, sampling_batch_info: SamplingBatchInfo, num_tokens_in_batch: int)

# python/sglang/srt/layers/torchao_utils.py
get_gemlite_cache_path() -> str
save_gemlite_cache(print_error: bool) -> bool
proj_filter(module: torch.nn.Module, fqn: str)
apply_torchao_config_to_model(model: torch.nn.Module, torchao_config: str, filter_fn: Optional[Callable])

# python/sglang/srt/layers/utils.py
get_layer_id(weight_name)
  PPMissingLayer.__init__()
  PPMissingLayer.forward()

# python/sglang/srt/layers/vocab_parallel_embedding.py
pad_vocab_size(vocab_size: int, pad_to: int) -> int
vocab_range_from_per_partition_vocab_size(per_partition_vocab_size: int, rank: int, offset: int) -> Sequence[int]
vocab_range_from_global_vocab_size(global_vocab_size: int, rank: int, world_size: int, offset: int) -> Sequence[int]
  VocabParallelEmbeddingShardIndices.num_org_elements() -> int
  VocabParallelEmbeddingShardIndices.num_added_elements() -> int
  VocabParallelEmbeddingShardIndices.num_org_elements_padded() -> int
  VocabParallelEmbeddingShardIndices.num_added_elements_padded() -> int
  VocabParallelEmbeddingShardIndices.num_org_vocab_padding() -> int
  VocabParallelEmbeddingShardIndices.num_added_vocab_padding() -> int
  VocabParallelEmbeddingShardIndices.num_elements_padded() -> int
  VocabParallelEmbeddingShardIndices.__post_init__()
get_masked_input_and_mask(input_: torch.Tensor, org_vocab_start_index: int, org_vocab_end_index: int, num_org_vocab_padding: int, added_vocab_start_index: int, added_vocab_end_index: int) -> Tuple[torch.Tensor, torch.Tensor]
  VocabParallelEmbedding.__init__(num_embeddings: int, embedding_dim: int)
  VocabParallelEmbedding.get_sharded_to_full_mapping() -> Optional[List[int]]
  VocabParallelEmbedding.weight_loader(param: Parameter, loaded_weight: torch.Tensor)
  VocabParallelEmbedding.forward(input_)
  VocabParallelEmbedding.extra_repr() -> str
  ParallelLMHead.__init__(num_embeddings: int, embedding_dim: int)
  ParallelLMHead.tie_weights(embed_tokens: VocabParallelEmbedding)
  ParallelLMHead.forward(input_)
