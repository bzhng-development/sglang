================================================================================
FUNCTION INDEX: attention module
================================================================================
Total Functions: 273
Documented: 70


============================================================
FILE: python/sglang/srt/layers/attention/aiter_backend.py
Functions: 20
============================================================


CLASS: AiterAttnBackend
----------------------------------------
  L  65: __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor])

  L 157: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Init auxiliary variables for triton attention backend.

  L 341: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])

  L 364: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])

  L 499: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])

  L 572: get_cuda_graph_seq_len_fill_value(self)

  L 575: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)

  L 761: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)


CLASS: AiterIndicesUpdaterPrefill
----------------------------------------
  L 830: __init__(self, model_runner: ModelRunner, attn_backend: AttentionBackend)

  L 855: update(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, prefix_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], spec_info: Optional[SpecInfo])

  L 867: update_single_wrapper(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, prefix_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], spec_info: Optional[SpecInfo])


CLASS: AiterMlaIndicesUpdaterPrefill
----------------------------------------
  L 935: __init__(self, model_runner: ModelRunner, attn_backend: AttentionBackend)

  L 950: update(self, req_pool_indices: torch.Tensor, kv_lens: torch.Tensor, kv_lens_sum: int, extend_lens: torch.Tensor, max_q_len: int, max_kv_len: int, spec_info: Optional[SpecInfo])

  L 963: update_single_wrapper(self, req_pool_indices: torch.Tensor, kv_lens: torch.Tensor, kv_lens_sum: int, extend_lens: torch.Tensor, max_q_len: int, max_kv_len: int, spec_info: Optional[SpecInfo])


CLASS: AiterMultiStepDraftBackend
----------------------------------------
  L1022: __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)

  L1061: common_template(self, forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: int)

  L1093: init_forward_metadata(self, forward_batch: ForwardBatch)

  L1114: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L1125: init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)

  L1139: init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)


============================================================
FILE: python/sglang/srt/layers/attention/ascend_backend.py
Functions: 10
============================================================


CLASS: AscendAttnBackend
----------------------------------------
  L  40: gen_attention_mask(self, max_seq_len: int, dtype)

  L  58: __init__(self, model_runner: ModelRunner)

  L  84: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Init the metadata for a forward pass.

  L 105: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L 114: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 134: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])

  L 160: get_cuda_graph_seq_len_fill_value(self)

  L 163: forward_extend(self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)

  L 291: forward_decode_graph(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor])

  L 421: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor])


============================================================
FILE: python/sglang/srt/layers/attention/base_attn_backend.py
Functions: 9
============================================================


CLASS: AttentionBackend
----------------------------------------
  L  18: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Init the metadata for a forward pass.

  L  22: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)
         üìù Init the global shared states for cuda graph.

  L  26: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
         üìù Init the metadata for a forward pass for capturing a cuda graph.

  L  39: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])
         üìù Init the metadata for a forward pass for replaying a cuda graph.

  L  53: get_cuda_graph_seq_len_fill_value(self)
         üìù Get the fill value for padded seq lens. Typically, it is 0 or 1.

  L  57: forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)
         üìù Run forward on an attention layer.

  L  91: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)
         üìù Run a forward for decode.

  L 103: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)
         üìù Run a forward for extend.

  L 115: support_triton(self)
         üìù Check if the current backend supports triton.


============================================================
FILE: python/sglang/srt/layers/attention/cutlass_mla_backend.py
Functions: 8
============================================================


CLASS: CutlassMLABackend
----------------------------------------
  L  51: __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], kv_last_page_len_buf: Optional[torch.Tensor])

  L  82: init_forward_metadata(self, forward_batch: ForwardBatch)

  L 122: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, block_kv_indices: Optional[torch.Tensor])

  L 146: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])

  L 185: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])

  L 223: get_cuda_graph_seq_len_fill_value(self)

  L 226: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor])


CLASS: CutlassMLADecodeMetadata
----------------------------------------
  L  39: __init__(self, workspace: Optional[torch.Tensor], block_kv_indices: Optional[torch.Tensor])


============================================================
FILE: python/sglang/srt/layers/attention/double_sparsity_backend.py
Functions: 4
============================================================


CLASS: DoubleSparseAttnBackend
----------------------------------------
  L  17: __init__(self, model_runner: ModelRunner)

  L  52: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Init auxiliary variables for triton attention backend.

  L 113: forward_extend(self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)

  L 167: forward_decode(self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)


============================================================
FILE: python/sglang/srt/layers/attention/dual_chunk_flashattention_backend.py
Functions: 9
============================================================


CLASS: DualChunkFlashAttentionBackend
----------------------------------------
  L 102: __init__(self, model_runner: 'ModelRunner')
         ‚Üí None

  L 160: get_sparse_attention_config(self, layer_idx)
         ‚Üí List[Dict[str, Any]]

  L 168: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Initialize forward metadata hence all layers in the forward pass can reuse it.

  L 296: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: 'RadixAttention', forward_batch: ForwardBatch, save_kv_cache)

  L 409: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: 'RadixAttention', forward_batch: ForwardBatch, save_kv_cache)
         ‚Üí torch.Tensor

  L 486: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)
         üìù Initialize CUDA graph state for the attention backend.
            Args:
            max_bs (int): Maximum batch size to support in CUDA graphs
            This creates fixed-size tensors that will be reused during CUDA graph replay
            to avoid memory allocations.

  L 532: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[None])

  L 580: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[None], seq_lens_cpu: Optional[torch.Tensor], out_cache_loc: torch.Tensor)
         üìù Initialize forward metadata for replaying CUDA graph.

  L 670: get_cuda_graph_seq_len_fill_value(self)
         üìù Get the fill value for sequence length in CUDA graph.


============================================================
FILE: python/sglang/srt/layers/attention/flashattention_backend.py
Functions: 18
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 127: def make_local_attention_virtual_batches(attn_chunk_size: int,
        query_start_loc_np: np.ndarray,
        seq_lens_np: np.ndarray,
        block_table: torch.Tensor,
        page_size: int)
         ‚Üí tuple[np.ndarray, np.ndarray, np.ndarray, torch.Tensor]
         üìù Take in `query_start_loc_np` and `seq_lens_np` and break the sequences into
            local attention blocks, where each block is passed to the attention kernel
            as an independent local ("virtual") batch item.
            Args:
            attn_chunk_size: Size of local attention chunks
            query_start_loc_np: Cumulative sum of query lengths (numpy array)
            seq_lens_np: Sequence lengths (numpy array)
            block_table: Block table for KV cache
            page_size: Size of each page in the KV cache
            Returns:
            seqlens_q_local: Query sequence lengths for local attention
            cu_seqlens_q_local: Cumulative sum of query sequence lengths for local attention
            seqlens_k_local: Key sequence lengths for local attention
            block_table_local: Block table for local attention

  L 272: def cdiv(a: int, b: int)
         ‚Üí int
         üìù Ceiling division.

  L 279: def merge_state_v2_wrapper(o, s_a, o_exp, s_b)
         @torch._dynamo.disable()

  L2237: def prepare_swa_spec_page_table_triton(page_table_dst: torch.Tensor,
        page_table_a: torch.Tensor,
        page_table_b: torch.Tensor,
        seq_len_a: torch.Tensor,
        seq_len_b: torch.Tensor,
        speculative_num_draft_tokens: int)

  L2347: def normal_decode_set_metadata(cache_seqlens_int32: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        page_table: torch.Tensor,
        req_to_token: torch.Tensor,
        req_pool_indices: torch.Tensor,
        strided_indices: torch.Tensor,
        max_seq_pages: torch.Tensor,
        seq_lens: torch.Tensor,
        seq_len_delta: int,
        page_size: int)


CLASS: FlashAttentionBackend
----------------------------------------
  L 301: __init__(self, model_runner: ModelRunner, skip_prefill: bool, speculative_step_id, topk, speculative_num_steps)

  L 355: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Initialize forward metadata hence all layers in the forward pass can reuse it.

  L 639: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor], sinks: Optional[torch.Tensor])

  L 929: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor], sinks: Optional[torch.Tensor])
         ‚Üí torch.Tensor

  L1188: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)
         üìù Initialize CUDA graph state for the attention backend.
            Args:
            max_bs (int): Maximum batch size to support in CUDA graphs
            This creates fixed-size tensors that will be reused during CUDA graph replay
            to avoid memory allocations.

  L1448: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
         üìù Initialize forward metadata for capturing CUDA graph.

  L1683: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor], out_cache_loc: Optional[torch.Tensor])
         üìù Initialize forward metadata for replaying CUDA graph.

  L1939: get_cuda_graph_seq_len_fill_value(self)
         üìù Get the fill value for sequence length in CUDA graph.


CLASS: FlashAttentionMultiStepBackend
----------------------------------------
  L2279: __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)

  L2296: init_forward_metadata(self, forward_batch: ForwardBatch)

  L2300: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L2304: init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)

  L2322: init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)


============================================================
FILE: python/sglang/srt/layers/attention/flashinfer_backend.py
Functions: 28
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L1230: def should_use_tensor_core(kv_cache_dtype: torch.dtype,
        num_attention_heads: int,
        num_kv_heads: int)
         ‚Üí bool
         üìù Determine whether to use tensor cores for attention computation.
            Args:
            kv_cache_dtype: Data type of the KV cache
            num_attention_heads: Number of attention heads
            num_kv_heads: Number of key/value heads
            Returns:
            bool: Whether to use tensor cores

  L1284: def fast_decode_plan(self,
        indptr: torch.Tensor,
        indices: torch.Tensor,
        last_page_len: torch.Tensor,
        num_qo_heads: int,
        num_kv_heads: int,
        head_dim: int,
        page_size: int,
        pos_encoding_mode: str,
        window_left: int,
        logits_soft_cap: Optional[float],
        q_data_type: Optional[Union[str,
        torch.dtype]],
        kv_data_type: Optional[Union[str,
        torch.dtype]],
        data_type: Optional[Union[str,
        torch.dtype]],
        sm_scale: Optional[float],
        rope_scale: Optional[float],
        rope_theta: Optional[float],
        non_blocking: bool)
         ‚Üí None
         üìù A faster version of BatchDecodeWithPagedKVCacheWrapper::plan used for FlashInferMultiStepDraftBackend.
            Modifications:
            - Remove unnecessary device-to-device copy for the cuda graph buffers.
            - Remove unnecessary host-to-device copy for the metadata buffers.


CLASS: FlashInferAttnBackend
----------------------------------------
  L  80: __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], kv_last_page_len_buf: Optional[torch.Tensor])

  L 211: init_forward_metadata(self, forward_batch: ForwardBatch)

  L 278: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])

  L 312: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 417: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])

  L 465: get_cuda_graph_seq_len_fill_value(self)

  L 468: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)

  L 552: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)


CLASS: FlashInferIndicesUpdaterDecode
----------------------------------------
  L 602: __init__(self, model_runner: ModelRunner, attn_backend: FlashInferAttnBackend)

  L 631: update(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 644: update_single_wrapper(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 666: update_sliding_window(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 714: update_cross_attention(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 746: call_begin_forward(self, wrapper: BatchDecodeWithPagedKVCacheWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, kv_indptr: torch.Tensor, kv_start_idx: torch.Tensor, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor], use_sliding_window_kv_pool: bool)


CLASS: FlashInferIndicesUpdaterPrefill
----------------------------------------
  L 818: __init__(self, model_runner: ModelRunner, attn_backend: FlashInferAttnBackend)

  L 849: update(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 864: update_single_wrapper(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 900: update_sliding_window(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 946: update_cross_attention(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 985: call_begin_forward(self, wrapper_ragged: BatchPrefillWithRaggedKVCacheWrapper, wrapper_paged: BatchPrefillWithPagedKVCacheWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, seq_lens: torch.Tensor, prefix_lens: torch.Tensor, kv_start_idx: torch.Tensor, kv_indptr: torch.Tensor, qo_indptr: torch.Tensor, use_ragged: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], use_sliding_window_kv_pool: bool)


CLASS: FlashInferMultiStepDraftBackend
----------------------------------------
  L1079: __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)

  L1120: common_template(self, forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: Callable)

  L1165: init_forward_metadata(self, forward_batch: ForwardBatch)

  L1186: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L1198: init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)

  L1212: init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)


============================================================
FILE: python/sglang/srt/layers/attention/flashinfer_mla_backend.py
Functions: 26
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L1038: def fast_mla_decode_plan(self,
        qo_indptr_cpu: torch.Tensor,
        kv_indptr_cpu: torch.Tensor,
        kv_indices: torch.Tensor,
        kv_len_arr_cpu: torch.Tensor,
        num_heads: int,
        head_dim_ckv: int,
        head_dim_kpe: int,
        page_size: int,
        causal: bool,
        sm_scale: float,
        q_data_type: torch.dtype,
        kv_data_type: torch.dtype)
         ‚Üí None
         üìù A faster version of BatchMLAPagedAttentionWrapper::plan,
            for skipping the stream synchronization in original plan function during
            cuda graph replaying.


CLASS: FlashInferMLAAttnBackend
----------------------------------------
  L 179: __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], q_indptr_decode_buf: Optional[torch.Tensor])

  L 271: init_forward_metadata(self, forward_batch: ForwardBatch)

  L 323: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])

  L 354: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])

  L 434: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])

  L 491: get_cuda_graph_seq_len_fill_value(self)

  L 494: init_mha_chunk_metadata(self, forward_batch: ForwardBatch)
         üìù Init the metadata for a forward pass.

  L 498: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor])

  L 576: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor])


CLASS: FlashInferMLAIndicesUpdaterDecode
----------------------------------------
  L 638: __init__(self, model_runner: ModelRunner, attn_backend: AttentionBackend)

  L 655: update(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, decode_wrapper: BatchMLAPagedAttentionWrapper, init_metadata_replay: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 678: call_begin_forward(self, wrapper: BatchMLAPagedAttentionWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, q_indptr: torch.Tensor, kv_indptr: torch.Tensor, init_metadata_replay: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])


CLASS: FlashInferMLAIndicesUpdaterPrefill
----------------------------------------
  L 747: __init__(self, model_runner: ModelRunner, attn_backend: AttentionBackend)

  L 767: update(self, req_pool_indices: torch.Tnesor, seq_lens: torch.Tensor, seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrapper_paged: BatchMLAPagedAttentionWrapper, use_ragged: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 798: call_begin_forward(self, wrapper_ragged: BatchPrefillWithRaggedKVCacheWrapper, wrapper_paged: BatchMLAPagedAttentionWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, seq_lens: torch.Tensor, prefix_lens: torch.Tensor, kv_indptr: torch.Tensor, qo_indptr: torch.Tensor, use_ragged: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])


CLASS: FlashInferMLAMultiStepDraftBackend
----------------------------------------
  L 887: __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)

  L 933: common_template(self, forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: Callable)

  L 971: init_forward_metadata(self, forward_batch: ForwardBatch)

  L 994: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L1006: init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)

  L1020: init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)


CLASS: FlashInferMhaChunkKVRunner
----------------------------------------
  L  68: __init__(self, model_runner: ModelRunner, attn_backend: 'FlashInferMlaAttnBackend')

  L  89: update_prefix_chunks(self, num_prefix_chunks: int)

  L  96: update_wrapper(self, forward_batch: ForwardBatch)

  L 142: forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch)


============================================================
FILE: python/sglang/srt/layers/attention/flashmla_backend.py
Functions: 15
============================================================


CLASS: FlashMLABackend
----------------------------------------
  L  51: __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], kv_last_page_len_buf: Optional[torch.Tensor])

  L  81: init_forward_metadata(self, forward_batch: ForwardBatch)

  L 148: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, block_kv_indices: Optional[torch.Tensor])

  L 182: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])

  L 252: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])

  L 327: get_cuda_graph_seq_len_fill_value(self)

  L 330: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)

  L 387: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)


CLASS: FlashMLADecodeMetadata
----------------------------------------
  L  37: __init__(self, flashmla_metadata: Optional[Tuple[torch.Tensor, torch.Tensor]], num_splits: Optional[torch.Tensor], block_kv_indices: Optional[torch.Tensor])


CLASS: FlashMLAMultiStepDraftBackend
----------------------------------------
  L 456: __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)

  L 489: common_template(self, forward_batch: ForwardBatch, call_fn: Callable)

  L 499: init_forward_metadata(self, forward_batch: ForwardBatch)

  L 506: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L 512: init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)

  L 526: init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)


============================================================
FILE: python/sglang/srt/layers/attention/hybrid_attn_backend.py
Functions: 8
============================================================


CLASS: HybridAttnBackend
----------------------------------------
  L  15: __init__(self, model_runner: ModelRunner, prefill_backend: AttentionBackend, decode_backend: AttentionBackend)

  L  25: init_forward_metadata(self, forward_batch: ForwardBatch)

  L  31: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L  38: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L  69: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])

  L 103: get_cuda_graph_seq_len_fill_value(self)

  L 106: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)

  L 120: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)


============================================================
FILE: python/sglang/srt/layers/attention/intel_amx_backend.py
Functions: 5
============================================================


CLASS: IntelAMXAttnBackend
----------------------------------------
  L  16: __init__(self, model_runner: ModelRunner)

  L  32: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Init the metadata for a forward pass.

  L  52: forward_extend(self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)

  L  91: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)

  L 127: support_triton(self)


============================================================
FILE: python/sglang/srt/layers/attention/merge_state.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  26: def merge_state(prefix_output: torch.Tensor,
        prefix_lse: torch.Tensor,
        suffix_output: torch.Tensor,
        suffix_lse: torch.Tensor,
        output: Optional[torch.Tensor],
        output_lse: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, Optional[torch.Tensor]]


============================================================
FILE: python/sglang/srt/layers/attention/tbo_backend.py
Functions: 9
============================================================


CLASS: TboAttnBackend
----------------------------------------
  L  14: __init__(self, primary: AttentionBackend, children: List[AttentionBackend])

  L  20: init_new(cls, creator: Callable[[], AttentionBackend])

  L  26: init_forward_metadata(self, forward_batch: 'ForwardBatch')

  L  35: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L  41: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: 'ForwardMode', spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L  72: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: 'ForwardMode', spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])

  L 176: get_cuda_graph_seq_len_fill_value(self)

  L 182: forward_extend(self)

  L 185: forward_decode(self)


============================================================
FILE: python/sglang/srt/layers/attention/torch_native_backend.py
Functions: 5
============================================================


CLASS: TorchNativeAttnBackend
----------------------------------------
  L  18: __init__(self, model_runner: ModelRunner)

  L  23: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Init the metadata for a forward pass.

  L 182: forward_extend(self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)

  L 226: forward_decode(self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)

  L 269: support_triton(self)


============================================================
FILE: python/sglang/srt/layers/attention/triton_backend.py
Functions: 19
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  23: def logit_capping_mod(logit_capping_method, logit_cap)

  L 965: def get_num_kv_splits_triton(num_kv_splits_ptr,
        seq_lens_ptr,
        num_seq,
        num_group,
        num_head,
        num_kv_head,
        max_kv_splits,
        device_core_count,
        MAX_NUM_SEQ: tl.constexpr)
         @triton.jit

  L1016: def update_sliding_window_buffer(window_kv_indptr,
        req_to_token,
        sliding_window_size,
        seq_lens,
        req_pool_indices,
        bs,
        device,
        token_to_kv_pool_allocator)

  L1056: def update_sliding_window_buffer_cuda_graph(window_kv_indptr,
        window_kv_indices,
        req_to_token,
        sliding_window_size,
        seq_lens,
        req_pool_indices,
        bs,
        token_to_kv_pool_allocator)


CLASS: TritonAttnBackend
----------------------------------------
  L  50: __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor])

  L 131: get_num_kv_splits(self, num_kv_splits: torch.Tensor, seq_lens: torch.Tensor)

  L 167: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Init auxiliary variables for triton attention backend.

  L 369: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])

  L 427: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 583: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])

  L 705: get_cuda_graph_seq_len_fill_value(self)

  L 708: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache, sinks)

  L 771: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache, sinks)


CLASS: TritonMultiStepDraftBackend
----------------------------------------
  L 830: __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)

  L 868: common_template(self, forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: int)

  L 900: init_forward_metadata(self, forward_batch: ForwardBatch)

  L 921: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L 932: init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)

  L 946: init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)


============================================================
FILE: python/sglang/srt/layers/attention/trtllm_mha_backend.py
Functions: 13
============================================================


CLASS: TRTLLMHAAttnBackend
----------------------------------------
  L  58: __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], kv_last_page_len_buf: Optional[torch.Tensor], speculative_step_id: int)

  L 111: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])
         üìù Initialize CUDA graph state for TRTLLM MHA.

  L 196: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])
         üìù Initialize metadata for CUDA graph capture.

  L 309: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])
         üìù Replay CUDA graph with new inputs.

  L 411: get_cuda_graph_seq_len_fill_value(self)
         ‚Üí int
         üìù Get the fill value for sequence lengths in CUDA graph.

  L 415: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Initialize the metadata for a forward pass.

  L 517: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)
         ‚Üí torch.Tensor
         üìù Run forward for decode using TRTLLM MHA kernel.

  L 576: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)


CLASS: TRTLLMHAAttnMultiStepDraftBackend
----------------------------------------
  L 638: __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)

  L 651: init_forward_metadata(self, forward_batch: ForwardBatch)

  L 655: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L 659: init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)

  L 677: init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)


============================================================
FILE: python/sglang/srt/layers/attention/trtllm_mla_backend.py
Functions: 9
============================================================


CLASS: TRTLLMMLABackend
----------------------------------------
  L  60: __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], q_indptr_decode_buf: Optional[torch.Tensor])

  L 167: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])
         üìù Initialize CUDA graph state for TRTLLM MLA.

  L 186: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])
         üìù Initialize metadata for CUDA graph capture.

  L 240: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])
         üìù Replay CUDA graph with new inputs.

  L 287: get_cuda_graph_seq_len_fill_value(self)
         ‚Üí int
         üìù Get the fill value for sequence lengths in CUDA graph.

  L 291: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Initialize the metadata for a forward pass.

  L 320: quantize_and_rope_for_fp8(self, q_nope: torch.Tensor, q_rope: torch.Tensor, k_nope: torch.Tensor, k_rope: torch.Tensor, forward_batch: ForwardBatch, cos_sin_cache: torch.Tensor, is_neox: bool)
         ‚Üí tuple[torch.Tensor, torch.Tensor, torch.Tensor]
         üìù Quantize and apply RoPE for FP8 attention path.
            This function handles the FP8 quantization and RoPE application for MLA attention.
            It takes separate query/key nope and rope components, applies RoPE to the rope parts,
            quantizes all components to FP8, and merges the query components into a single tensor.
            Args:
            q_nope: Query no-position-encoding component [seq_len, num_heads, kv_lora_rank]
            - expected dtype: torch.bfloat16
            q_rope: Query RoPE component [seq_len, num_heads, qk_rope_head_dim]
            - expected dtype: torch.bfloat16
            k_nope: Key no-position-encoding component [seq_len, num_heads, kv_lora_rank]
            - expected dtype: torch.bfloat16
            k_rope: Key RoPE component [seq_len, num_heads, qk_rope_head_dim]
            - expected dtype: torch.bfloat16
            forward_batch: Forward batch containing position information
            cos_sin_cache: Precomputed cosine/sine cache for RoPE
            - expected dtype: matches q_/k_ input dtype (torch.bfloat16)
            is_neox: Whether to use NeoX-style RoPE (interleaved) or GPT-style (half rotation)
            Returns:
            tuple: (merged_q_out, k_nope_out, k_rope_out) quantized to FP8
            - merged_q_out: [seq_len, num_heads, kv_lora_rank + qk_rope_head_dim], dtype=torch.float8_e4m3fn
            - k_nope_out:   [seq_len, num_heads, kv_lora_rank], dtype=torch.float8_e4m3fn
            - k_rope_out:   [seq_len, num_heads, qk_rope_head_dim], dtype=torch.float8_e4m3fn

  L 398: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor], cos_sin_cache: Optional[torch.Tensor], is_neox: Optional[bool])
         ‚Üí torch.Tensor
         üìù Run forward for decode using TRTLLM MLA kernel.


CLASS: TRTLLMMLAMultiStepDraftBackend
----------------------------------------
  L 503: __init__(self, model_runner: 'ModelRunner', topk: int, speculative_num_steps: int)


============================================================
FILE: python/sglang/srt/layers/attention/utils.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  11: def create_flashinfer_kv_indices_triton(req_to_token_ptr,
        req_pool_indices_ptr,
        page_kernel_lens_ptr,
        kv_indptr,
        kv_start_idx,
        kv_indices_ptr,
        req_to_token_ptr_stride: tl.constexpr)
         @triton.jit

  L  50: def create_flashmla_kv_indices_triton(req_to_token_ptr,
        req_pool_indices_ptr,
        page_kernel_lens_ptr,
        kv_start_idx,
        kv_indices_ptr,
        req_to_token_ptr_stride: tl.constexpr,
        kv_indices_ptr_stride: tl.constexpr,
        NUM_PAGE_PER_BLOCK: tl.constexpr,
        PAGED_SIZE: tl.constexpr)
         @triton.jit


============================================================
FILE: python/sglang/srt/layers/attention/vision.py
Functions: 12
============================================================


CLASS: SingletonCache
----------------------------------------
  L  55: set_data(self, value: Any)
         ‚Üí None

  L  58: get_data(self)
         ‚Üí Optional[Any]

  L  61: empty(self)
         ‚Üí bool


CLASS: VisionAttention
----------------------------------------
  L 354: __init__(self, embed_dim: int, num_heads: int, projection_size: int, use_qkv_parallel: bool, qkv_backend: Optional[str], quant_config: Optional[QuantizationConfig], dropout: float, softmax_in_single_precision: bool, flatten_batch: bool, prefix: str, proj_bias: bool, num_dummy_heads: int, qkv_bias: bool, qk_normalization: bool, layer_norm_eps: float, customized_position_embedding_applier: Callable[[torch.Tensor, torch.Tensor, Any, Any], Tuple[torch.Tensor, torch.Tensor]])

  L 509: forward(self, x: torch.Tensor, cu_seqlens: Optional[torch.Tensor], position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]], attention_mask: Optional[torch.Tensor])
         ‚Üí torch.Tensor
         üìù Args:
            x: [b, s, embed_dim]
            cu_seqlens: [b]
            Returns:
            [s, b, head * head_size]


CLASS: VisionFlash3Attention
----------------------------------------
  L 284: __init__(self)

  L 292: forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens: Optional[Union[SingletonCache, torch.Tensor]], bsz: int, seq_len: int)
         ‚Üí torch.Tensor
         üìù Args:
            cu_seqlens: [b]
            Returns:
            [b * s, h, head_size]


CLASS: VisionSdpaAttention
----------------------------------------
  L  88: __init__(self, head_dim: int, num_heads: int, num_kv_heads: int, dropout: float, flatten_batch: bool, softmax_in_single_precision: bool)

  L 141: generate_patch_attention_mask(self, s: int, cu_seqlens: Optional[torch.Tensor], flatten_batch: bool)
         ‚Üí Optional[torch.Tensor]
         üìù Creates a non-causal 4D mask of shape `(b, 1, s, s)` or `(1, 1, s, s)`.
            Args:
            s: sequence length
            cu_seqlens: cumulative sequence lengths tensor. If not, returns an empty mask
            flatten_batch: whether to flatten batch dimension
            Returns:
            attention mask tensor or None

  L 163: forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, bsz: int, cu_seqlens: Optional[torch.Tensor], attention_mask: Optional[torch.Tensor])
         ‚Üí torch.Tensor
         üìù Args:
            cu_seqlens: [b]
            Returns:
            [b * s, h, head_size]


CLASS: VisionTritonAttention
----------------------------------------
  L 240: __init__(self)

  L 246: forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens: Optional[torch.Tensor], bsz: int, seq_len: int)
         ‚Üí torch.Tensor
         üìù Args:
            cu_seqlens: [b]
            Returns:
            [b * s, h, head_size]


============================================================
FILE: python/sglang/srt/layers/attention/vision_utils.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L   8: def update_vit_attn_dummy_heads_config(config)
         üìù Update HF config to ensure vision attention num_attention_heads is divisible by tp_size

  L  26: def pad_vit_attn_dummy_heads(config, name: str, loaded_weight: torch.Tensor)
         üìù Pad attention qkv weights for dummy heads


============================================================
FILE: python/sglang/srt/layers/attention/wave_backend.py
Functions: 10
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  26: def get_num_kv_splits_triton(num_kv_splits_ptr,
        seq_lens_ptr,
        num_seq,
        num_group,
        num_head,
        num_kv_head,
        max_kv_splits,
        device_core_count,
        MAX_NUM_SEQ: tl.constexpr)
         @triton.jit


CLASS: WaveAttnBackend
----------------------------------------
  L  91: __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor])

  L 162: get_num_kv_splits(self, num_kv_splits: torch.Tensor, seq_lens: torch.Tensor)

  L 195: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Init auxiliary variables for wave attention backend.

  L 344: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])

  L 388: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 472: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])

  L 540: get_cuda_graph_seq_len_fill_value(self)

  L 543: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)

  L 589: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)


============================================================
FILE: python/sglang/test/attention/test_flashattn_backend.py
Functions: 7
============================================================


CLASS: MockModelRunner
----------------------------------------
  L  16: __init__(self, page_size, num_heads, head_dim)


CLASS: TestFlashAttentionBackend
----------------------------------------
  L  74: setUp(self)

  L 324: test_forward_extend(self)
         üìù Test the standard extend operation.

  L 328: test_forward_decode(self)
         üìù Test the decode operation with cached tokens.

  L 332: test_forward_extend_with_prefix(self)
         üìù Test extending from cached prefix tokens.

  L 340: test_forward_extend_with_page_size_greater_than_1(self)
         üìù Test extending from cached prefix tokens with page size greater than 1.

  L 344: test_forward_decode_with_page_size_greater_than_1(self)
         üìù Test decode operation with page size greater than 1.


============================================================
FILE: python/sglang/test/attention/test_flashattn_mla_backend.py
Functions: 6
============================================================


CLASS: MockModelRunner
----------------------------------------
  L  15: __init__(self, kv_lora_rank, qk_rope_head_dim)


CLASS: MockReqToTokenPool
----------------------------------------
  L  63: __init__(self, batch_size, seq_len, device)


CLASS: TestFlashAttentionMLABackend
----------------------------------------
  L  73: setUp(self)

  L 267: test_forward_extend(self)
         üìù Test the standard extend operation.

  L 271: test_forward_decode(self)
         üìù Test the decode operation with cached tokens.

  L 275: test_forward_extend_with_prefix(self)
         üìù Test extending from cached prefix tokens.


============================================================
FILE: python/sglang/test/attention/test_prefix_chunk_info.py
Functions: 6
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 114: def check_kv_indices(forward_batch)


CLASS: MockForwardBatch
----------------------------------------
  L  96: __init__(self, max_chunk_capacity: int)

  L 100: get_max_chunk_capacity(self)


CLASS: MockReqToTokenPool
----------------------------------------
  L 105: __init__(self, batch_size, seq_len, device)


CLASS: TestPrefixChunkInfo
----------------------------------------
  L 138: setUp(self)

  L 168: test_prefix_chunk_info(self)
         üìù Test the standard extend operation.


============================================================
FILE: python/sglang/test/attention/test_trtllm_mla_backend.py
Functions: 12
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  59: def build_rotary_emb(config, device)

  L 266: def compare_outputs(trtllm_out, reference_out, tolerance)
         üìù Compare outputs with detailed analysis.


CLASS: MockModelRunner
----------------------------------------
  L 205: __init__(self, config)


CLASS: TestTRTLLMMLA
----------------------------------------
  L 458: test_basic_functionality(self)
         üìù Test basic functionality with minimal setup.

  L 523: test_decode_output_match(self)
         üìù Test that TRTLLM and FlashInfer MLA backends produce matching outputs.

  L 652: test_page_size_consistency(self)
         üìù Test output consistency across different page sizes.

  L 710: test_shape_sanity(self)
         üìù Smoke test decode across several configurations.

  L 792: test_metadata_initialization(self)
         üìù Test TRTLLM MLA metadata initialization and structure.

  L 861: test_metadata_block_calculation(self)
         üìù Test block count calculation logic.

  L 914: test_metadata_kv_indices_correctness(self)
         üìù Test KV indices creation and correctness.

  L 988: test_metadata_cuda_graph_compatibility(self)
         üìù Test metadata compatibility with CUDA graph capture/replay.

  L1057: test_metadata_consistency_across_calls(self)
         üìù Test metadata consistency across multiple forward calls.
