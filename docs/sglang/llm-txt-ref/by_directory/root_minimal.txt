
# python/sglang/bench_offline_throughput.py
  BenchArgs.add_cli_args(parser: argparse.ArgumentParser)
  BenchArgs.from_cli_args(cls, args: argparse.Namespace)
throughput_test_once(backend_name: str, backend, reqs: List[DatasetRow], ignore_eos: bool, extra_request_body: Dict, profile: bool)
monitor_trace_file(known_files, directory, interval)
throughput_test(server_args: ServerArgs, bench_args: BenchArgs)

# python/sglang/bench_one_batch.py
  BenchArgs.add_cli_args(parser: argparse.ArgumentParser)
  BenchArgs.from_cli_args(cls, args: argparse.Namespace)
load_model(server_args, port_args, tp_rank)
prepare_inputs_for_correctness_test(bench_args, tokenizer, custom_prompts)
prepare_extend_inputs_for_correctness_test(bench_args, input_ids, reqs, model_runner)
prepare_synthetic_inputs_for_latency_test(batch_size, input_len, custom_inputs)
extend(reqs, model_runner)
decode(input_token_ids, batch, model_runner)
correctness_test(server_args, port_args, bench_args, tp_rank)
synchronize(device)
latency_test_run_once(run_name, model_runner, rank_print, reqs, batch_size, input_len, output_len, device, log_decode_step, profile, profile_record_shapes, profile_filename_prefix)
latency_test(server_args, port_args, bench_args, tp_rank)
main(server_args, bench_args)

# python/sglang/bench_one_batch_server.py
  BenchArgs.add_cli_args(parser: argparse.ArgumentParser)
  BenchArgs.from_cli_args(cls, args: argparse.Namespace)
launch_server_internal(server_args)
launch_server_process(server_args: ServerArgs)
run_one_case(url: str, batch_size: int, input_len: int, output_len: int, temperature: float, return_logprob: bool, stream_interval: int, input_len_step_percentage: float, run_name: str, result_filename: str, tokenizer, profile: bool, profile_steps: int, profile_by_stage: bool)
get_report_summary(result: List[Tuple], server_args: ServerArgs, bench_args: BenchArgs)
run_benchmark(server_args: ServerArgs, bench_args: BenchArgs)
main()

# python/sglang/bench_serving.py
  RequestFuncOutput.init_new(request_func_input: RequestFuncInput)
remove_prefix(text: str, prefix: str) -> str
remove_suffix(text: str, suffix: str) -> str
get_auth_headers() -> Dict[str, str]
async_request_trt_llm(request_func_input: RequestFuncInput, pbar: Optional[tqdm]) -> RequestFuncOutput
async_request_openai_completions(request_func_input: RequestFuncInput, pbar: Optional[tqdm]) -> RequestFuncOutput
async_request_openai_chat_completions(request_func_input: RequestFuncInput, pbar: Optional[tqdm]) -> RequestFuncOutput
async_request_truss(request_func_input: RequestFuncInput, pbar: Optional[tqdm]) -> RequestFuncOutput
async_request_sglang_generate(request_func_input: RequestFuncInput, pbar: Optional[tqdm]) -> RequestFuncOutput
async_request_gserver(request_func_input: RequestFuncInput, pbar: Optional[tqdm]) -> RequestFuncOutput
async_request_profile(api_url: str) -> RequestFuncOutput
get_model(pretrained_model_name_or_path: str) -> str
get_tokenizer(pretrained_model_name_or_path: str) -> Union[PreTrainedTokenizer, PreTrainedTokenizerFast]
get_dataset(args, tokenizer)
download_and_cache_file(url: str, filename: Optional[str])
is_file_valid_json(path)
sample_mmmu_requests(num_requests: int, tokenizer: PreTrainedTokenizerBase, fixed_output_len: Optional[int], apply_chat_template: bool, random_sample: bool) -> List[DatasetRow]
sample_sharegpt_requests(dataset_path: str, num_requests: int, tokenizer: PreTrainedTokenizerBase, fixed_output_len: Optional[int], context_len: Optional[int], prompt_suffix: Optional[str], apply_chat_template) -> List[DatasetRow]
sample_random_requests(input_len: int, output_len: int, num_prompts: int, range_ratio: float, tokenizer: PreTrainedTokenizerBase, dataset_path: str, random_sample: bool, return_text: bool) -> List[DatasetRow]
parse_random_image_resolution(image_resolution: str) -> Tuple[int, int]
sample_random_image_requests(num_requests: int, num_images: int, input_len: int, output_len: int, range_ratio: float, tokenizer: PreTrainedTokenizerBase, apply_chat_template: bool, image_resolution: str) -> List[DatasetRow]
gen_prompt(tokenizer, token_num)
get_gen_prefix_cache_path(args, tokenizer)
sample_generated_shared_prefix_requests(num_groups: int, prompts_per_group: int, system_prompt_len: int, question_len: int, output_len: int, tokenizer: PreTrainedTokenizerBase, args: argparse.Namespace) -> List[DatasetRow]
get_request(input_requests: List[DatasetRow], request_rate: float) -> AsyncGenerator[DatasetRow, None]
calculate_metrics(input_requests: List[DatasetRow], outputs: List[RequestFuncOutput], dur_s: float, tokenizer: PreTrainedTokenizerBase, backend: str) -> Tuple[BenchmarkMetrics, List[int]]
benchmark(backend: str, api_url: str, base_url: str, model_id: str, tokenizer: PreTrainedTokenizerBase, input_requests: List[DatasetRow], request_rate: float, max_concurrency: Optional[int], disable_tqdm: bool, lora_names: List[str], extra_request_body: Dict[str, Any], profile: bool, pd_separated: bool, flush_cache: bool, warmup_requests: int)
check_chat_template(model_path)
set_global_args(args_: argparse.Namespace)
run_benchmark(args_: argparse.Namespace)
set_ulimit(target_soft_limit)
  LoRAPathAction.__call__(parser, namespace, values, option_string)

# python/sglang/check_env.py
is_cuda_v2()
get_package_versions(packages)
get_cuda_info()
get_gpu_topology()
get_hypervisor_vendor()
check_env()

# python/sglang/compile_deep_gemm.py
  CompileArgs.add_cli_args(parser: argparse.ArgumentParser)
  CompileArgs.from_cli_args(cls, args: argparse.Namespace)
warm_up_compile(disaggregation_mode: str, tokenizer_manager: TokenizerManager)
launch_server_internal(server_args)
launch_server_process_and_send_one_request(server_args: ServerArgs, compile_args: CompileArgs)
refine_server_args(server_args: ServerArgs, compile_args: CompileArgs)
run_compile(server_args: ServerArgs, compile_args: CompileArgs)

# python/sglang/global_config.py
  GlobalConfig.__init__()

# python/sglang/profiler.py
run_profile(url: Optional[str], num_steps: int, activities: List[str], output_dir: Optional[str], profile_name: Optional[str], profile_by_stage: bool)

# python/sglang/utils.py
execute_once(func)
info_once(message: str)
convert_json_schema_to_str(json_schema: Union[dict, str, Type[BaseModel]]) -> str
get_exception_traceback()
is_same_type(values: list)
read_jsonl(filename: str)
dump_state_text(filename: str, states: list, mode: str)
  HttpResponse.__init__(resp)
  HttpResponse.json()
  HttpResponse.status_code()
http_request(url, json, stream, api_key, verify, method: Optional[str])
encode_image_base64(image_path: Union[str, bytes])
encode_frame(frame)
encode_video_base64(video_path: str, num_frames: int)
find_printable_text(text: str)
  LazyImport.__init__(module_name: str, class_name: str)
  LazyImport.__getattr__(name: str)
  LazyImport.__call__()
download_and_cache_file(url: str, filename: Optional[str])
is_in_ci()
print_highlight(html_content: str)
reserve_port(host, start, end)
release_port(lock_socket)
execute_shell_command(command: str) -> subprocess.Popen
launch_server_cmd(command: str, host: str, port: int)
terminate_process(process)
wait_for_server(base_url: str, timeout: int) -> None
  TypeBasedDispatcher.__init__(mapping: List[Tuple[Type, Callable]])
  TypeBasedDispatcher.__call__(obj: Any)
trim_overlap(existing_text, new_chunk)
stream_and_merge(llm, prompt, sampling_params)
async_stream_and_merge(llm, prompt, sampling_params)
resolve_obj_by_qualname(qualname: str) -> Any
