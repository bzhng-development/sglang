
# python/sglang/srt/_custom_ops.py
init_custom_ar(ipc_tensors, rank_data, rank, full_nvlink)
all_reduce(fa, inp, out, reg_buffer, reg_buffer_sz_bytes)
dispose(fa)
meta_size()
register_buffer(fa, ipc_tensors)
get_graph_buffer_ipc_meta(fa)
register_graph_buffers(fa, handles, offsets)
init_custom_ar(meta, rank_data, handles, offsets, rank, full_nvlink)
all_reduce_reg(fa, inp, out)
all_reduce_unreg(fa, inp, reg_buffer, out)
dispose(fa)
meta_size()
register_buffer(fa, t, handles, offsets)
get_graph_buffer_ipc_meta(fa)
register_graph_buffers(fa, handles, offsets)
allocate_meta_buffer(size)
get_meta_buffer_ipc_handle(inp)
init_custom_qr(rank, world_size, qr_max_size)
qr_get_handle(fa)
qr_open_handles(fa, handles)
qr_all_reduce(fa, inp, out, quant_level, cast_bf2half)
qr_destroy(fa)
qr_max_size()
mscclpp_generate_unique_id()
mscclpp_init_context(unique_id, rank, world_size, scratch, put_buffer, nranks_per_node, rank_to_node, rank_to_ib, context_selection)
mscclpp_allreduce(context, inp, out, nthreads, nblocks)

# python/sglang/srt/aio_rwlock.py
  RWLock.__init__()
  RWLock.reader_lock()
  RWLock.writer_lock()
  RWLock.acquire_reader()
  RWLock.release_reader()
  RWLock.acquire_writer()
  RWLock.release_writer()
  _ReaderLock.__init__(rwlock)
  _ReaderLock.__aenter__()
  _ReaderLock.__aexit__(exc_type, exc_val, exc_tb)
  _WriterLock.__init__(rwlock)
  _WriterLock.__aenter__()
  _WriterLock.__aexit__(exc_type, exc_val, exc_tb)

# python/sglang/srt/bench_utils.py
  suppress_stdout_stderr.__enter__()
  suppress_stdout_stderr.__exit__()
bench_kineto(fn, kernel_names, num_tests, suppress_kineto_output, trace_path, flush_l2, with_multiple_kernels)

# python/sglang/srt/code_completion_parser.py
register_completion_template(template, override)
completion_template_exists(template_name)
is_completion_template_defined()
generate_completion_prompt_from_request(request)
generate_completion_prompt(prompt, suffix, template_name)

# python/sglang/srt/configs/chatglm.py
  ChatGLMConfig.__init__(num_layers, padded_vocab_size, hidden_size, ffn_hidden_size, kv_channels, num_attention_heads, seq_length, hidden_dropout, attention_dropout, layernorm_epsilon, rmsnorm, apply_residual_connection_post_layernorm, post_layer_norm, add_bias_linear, add_qkv_bias, interleaved_qkv, bias_dropout_fusion, multi_query_attention, multi_query_group_num, apply_query_key_layer_scaling, attention_softmax_in_fp32, fp32_residual_connection, quantization_bit, pre_seq_len, prefix_projection)

# python/sglang/srt/configs/dbrx.py
  DbrxAttentionConfig.__init__(attn_pdrop, clip_qkv, kv_n_heads, rope_theta)
  DbrxAttentionConfig.from_pretrained(cls, pretrained_model_name_or_path)
  DbrxFFNConfig.__init__(ffn_act_fn, ffn_hidden_size, moe_num_experts, moe_top_k, moe_jitter_eps, moe_loss_weight, moe_normalize_expert_weights, uniform_expert_assignment)
  DbrxFFNConfig.from_pretrained(cls, pretrained_model_name_or_path)
  DbrxConfig.__init__(d_model, n_heads, n_layers, max_seq_len, vocab_size, resid_pdrop, emb_pdrop, attn_config, ffn_config, use_cache, initializer_range, output_router_logits, router_aux_loss_coef)

# python/sglang/srt/configs/deepseekvl2.py
select_best_resolution(image_size, candidate_resolutions)
  DictOutput.items()
  DictOutput.keys()
  DictOutput.__getitem__(item)
  DictOutput.__contains__(key)
  DictOutput.__setitem__(key, value)
  VLChatProcessorOutput.__len__()
  ImageTransform.__init__(mean, float, float]], std, float, float]], normalize)
  ImageTransform.__call__(pil_img)
  DeepseekVLV2Processor.__init__(tokenizer, candidate_resolutions, int]], patch_size, downsample_ratio, image_mean, float, float], image_std, float, float], normalize, image_token, pad_token, add_special_token, sft_format, mask_prompt, ignore_id)
  DeepseekVLV2Processor.format_messages_v2(messages, pil_images, max_req_input_len)
  DeepseekVLV2Processor.bos_id()
  DeepseekVLV2Processor.eos_id()
  DeepseekVLV2Processor.pad_id()
  DeepseekVLV2Processor.encode(text, bos, eos)
  DeepseekVLV2Processor.decode(t)
  DeepseekVLV2Processor.process_one(prompt, conversations, str]], images, apply_sft_format, inference_mode, system_prompt, max_req_input_len)
  DeepseekVLV2Processor.__call__()
  DeepseekVLV2Processor.find_all_indices(messages, target_value)
  DeepseekVLV2Processor.tokenize_with_images(conversation, images, bos, eos, cropping, max_req_input_len)
  DeepseekVL2VisionEncoderConfig.__init__(model_name, image_size, patch_size, width, layers, heads, mlp_ratio, global_pool, ignore_head, class_token, num_classes, use_checkpoint)
  DeepseekVL2MlpProjectorConfig.__init__(projector_type, input_dim, n_embed, depth, mlp_ratio, downsample_ratio)
  DeepseekV2Config.__init__(vocab_size, hidden_size, intermediate_size, moe_intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, n_shared_experts, n_routed_experts, ep_size, routed_scaling_factor, kv_lora_rank, q_lora_rank, qk_rope_head_dim, v_head_dim, qk_nope_head_dim, topk_method, n_group, topk_group, num_experts_per_tok, moe_layer_freq, first_k_dense_replace, norm_topk_prob, scoring_func, aux_loss_alpha, seq_aux, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, pretraining_tp, tie_word_embeddings, rope_theta, rope_scaling, attention_bias, attention_dropout, use_mla)
  DeepseekVL2Config.__init__(tile_tag, global_view_pos, candidate_resolutions, int]])

# python/sglang/srt/configs/device_config.py
  DeviceConfig.__init__(device)

# python/sglang/srt/configs/exaone.py
  ExaoneConfig.__init__(vocab_size, max_position_embeddings, hidden_size, num_layers, num_attention_heads, num_key_value_heads, intermediate_size, activation_function, rope_theta, rope_scaling, embed_dropout, attention_dropout, layer_norm_epsilon, initializer_range, use_cache, bos_token_id, eos_token_id, tie_word_embeddings)

# python/sglang/srt/configs/internvl.py
  InternLM2Config.__init__(vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, tie_word_embeddings, bias, rope_theta, rope_scaling, attn_implementation)
  InternVisionConfig.__init__(num_channels, patch_size, image_size, qkv_bias, hidden_size, num_attention_heads, intermediate_size, qk_normalization, num_hidden_layers, use_flash_attn, hidden_act, layer_norm_eps, dropout, drop_path_rate, attention_dropout, initializer_range, initializer_factor)
  InternVisionConfig.from_pretrained(cls, pretrained_model_name_or_path, os.PathLike])
  InternVLChatConfig.__init__(vision_config, llm_config, use_backbone_lora, use_llm_lora, pad2square, select_layer, force_image_size, downsample_ratio, template, dynamic_image_size, use_thumbnail, ps_version, min_dynamic_patch, max_dynamic_patch)
  InternVLChatConfig.to_dict()
  InternLM2Tokenizer.__init__(vocab_file, unk_token, bos_token, eos_token, pad_token, sp_model_kwargs, Any]], add_bos_token, add_eos_token, decode_with_prefix_space, clean_up_tokenization_spaces)
  InternLM2Tokenizer.no_prefix_space_tokens()
  InternLM2Tokenizer.vocab_size()
  InternLM2Tokenizer.bos_token_id()
  InternLM2Tokenizer.eos_token_id()
  InternLM2Tokenizer.get_vocab()
  InternLM2Tokenizer.convert_tokens_to_string(tokens)
  InternLM2Tokenizer.save_vocabulary(save_directory, filename_prefix)
  InternLM2Tokenizer.build_inputs_with_special_tokens(token_ids_0, token_ids_1)
  InternLM2Tokenizer.get_special_tokens_mask(token_ids_0, token_ids_1, already_has_special_tokens)
  InternLM2Tokenizer.create_token_type_ids_from_sequences(token_ids_0, token_ids_1)

# python/sglang/srt/configs/janus_pro.py
  DictToObject.__init__(dictionary)
  VisionConfig.__init__()
  GenAlignerConfig.__init__()
  GenHeadConfig.__init__()
  AlignerConfig.__init__()
  GenVisionConfig.__init__()
  MultiModalityConfig.__init__()
  VLMImageProcessor.__init__(image_size, min_size, image_mean, float, float], List[float]], image_std, float, float], List[float]], rescale_factor, do_normalize)
  VLMImageProcessor.resize(pil_img)
  VLMImageProcessor.preprocess(images, return_tensors)
  VLMImageProcessor.default_shape()
  DictOutput.items()
  DictOutput.keys()
  DictOutput.__getitem__(item)
  DictOutput.__contains__(key)
  DictOutput.__setitem__(key, value)
  VLChatProcessorOutput.__len__()
  VLChatProcessor.__init__(image_processor, tokenizer, image_tag, image_start_tag, image_end_tag, pad_tag, num_image_tokens, add_special_token, sft_format, mask_prompt, ignore_id)
  VLChatProcessor.image_token()
  VLChatProcessor.image_id()
  VLChatProcessor.image_start_id()
  VLChatProcessor.image_end_id()
  VLChatProcessor.image_start_token()
  VLChatProcessor.image_end_token()
  VLChatProcessor.pad_id()
  VLChatProcessor.add_image_token(image_indices, input_ids)
  VLChatProcessor.process_one(prompt, images)
  VLChatProcessor.__call__()
  VLChatProcessor.batchify(prepare_list)
  VLMImageProcessorConfig.__init__(image_size, min_size, image_mean, float, float], List[float]], image_std, float, float], List[float]], rescale_factor, do_normalize)

# python/sglang/srt/configs/kimi_vl.py
  KimiVLConfig.__init__(vision_config, MoonViTConfig]], text_config, DeepseekV2Config]], ignore_index, media_placeholder_token_id, pad_token_id)

# python/sglang/srt/configs/kimi_vl_moonvit.py
  MoonViTConfig.__init__(patch_size, init_pos_emb_height, init_pos_emb_width, num_attention_heads, num_hidden_layers, hidden_size, intermediate_size, merge_kernel_size, int])

# python/sglang/srt/configs/load_config.py
  LoadConfig.__post_init__()

# python/sglang/srt/configs/model_config.py
  ModelConfig.__init__(model_path, trust_remote_code, revision, context_length, model_override_args, is_embedding, enable_multimodal, dtype, quantization, override_config_file, is_draft_model, hybrid_kvcache_ratio, model_impl, ModelImpl])
  ModelConfig.from_server_args(server_args, model_path)
  ModelConfig.get_total_num_attention_heads()
  ModelConfig.get_num_attention_heads(tensor_parallel_size)
  ModelConfig.get_total_num_kv_heads()
  ModelConfig.get_num_kv_heads(tensor_parallel_size)
  ModelConfig.get_hf_eos_token_id()
  ModelConfig.maybe_pull_model_tokenizer_from_remote()
is_generation_model(model_architectures, is_embedding)
is_multimodal_model(model_architectures)
is_multimodal_gen_model(model_architectures)
is_image_gen_model(model_architectures)
is_audio_model(model_architectures)
is_encoder_decoder_model(model_architectures)
is_multimodal_chunked_prefill_supported(model_architectures)
yarn_get_mscale(scale, mscale)
is_hybrid_model(model_architectures, hybrid_kvcache_ratio, context_length, attention_chunk_size)
get_hybrid_layer_ids(model_architectures, num_hidden_layers)

# python/sglang/srt/configs/step3_vl.py
  Step3VisionEncoderConfig.__init__(hidden_size, intermediate_size, output_hidden_size, num_hidden_layers, num_attention_heads, num_channels, image_size, patch_size, hidden_act, layer_norm_eps)
  Step3TextConfig.__init__(hidden_size, intermediate_size, num_attention_heads, num_attention_groups, num_hidden_layers, max_seq_len, vocab_size, rms_norm_eps, moe_intermediate_size, moe_num_experts, moe_top_k, rope_theta, rope_scaling, Any]], max_position_embedding, share_expert_dim, share_q_dim, head_dim, norm_expert_weight, moe_layers_enum)
  Step3VLConfig.__init__(vision_config, Step3VisionEncoderConfig]], text_config, Step3TextConfig]], understand_projector_stride, projector_bias, image_token_id)

# python/sglang/srt/configs/update_config.py
may_get_weight_block_size(model_config, load_config)
get_moe_padding_size(weight_block_size)
get_num_heads_padding_size(tp_size, weight_block_size)
update_intermediate_size(model_config, attr_name, intermediate_padding_size)
adjust_config_with_unaligned_cpu_tp(model_config, load_config, tp_size)

# python/sglang/srt/configs/utils.py
register_image_processor(config, image_processor)
register_processor(config, processor)

# python/sglang/srt/connector/__init__.py
create_remote_connector(url, device)
get_connector_type(client)

# python/sglang/srt/connector/base_connector.py
  BaseConnector.__init__(url, device)
  BaseConnector.get_local_dir()
  BaseConnector.weight_iterator(rank)
  BaseConnector.pull_files(allow_pattern, ignore_pattern)
  BaseConnector.close()
  BaseConnector.__enter__()
  BaseConnector.__exit__(exc_type, exc_value, traceback)
  BaseConnector.__del__()
  BaseKVConnector.get(key)
  BaseKVConnector.getstr(key)
  BaseKVConnector.set(key, obj)
  BaseKVConnector.setstr(key, obj)
  BaseKVConnector.list(prefix)
  BaseFileConnector.glob(allow_pattern)

# python/sglang/srt/connector/redis.py
  RedisConnector.__init__(url, device)
  RedisConnector.get(key)
  RedisConnector.getstr(key)
  RedisConnector.set(key, tensor)
  RedisConnector.setstr(key, obj)
  RedisConnector.list(prefix)
  RedisConnector.weight_iterator(rank)
  RedisConnector.pull_files(allow_pattern, ignore_pattern)
  RedisConnector.close()

# python/sglang/srt/connector/s3.py
list_files(s3, path, allow_pattern, ignore_pattern)
  S3Connector.__init__(url)
  S3Connector.glob(allow_pattern)
  S3Connector.pull_files(allow_pattern, ignore_pattern)
  S3Connector.weight_iterator(rank)
  S3Connector.close()

# python/sglang/srt/connector/serde/__init__.py
create_serde(serde_type)

# python/sglang/srt/connector/serde/safe_serde.py
  SafeSerializer.__init__()
  SafeSerializer.to_bytes(t)
  SafeDeserializer.__init__(dtype)
  SafeDeserializer.from_bytes_normal(b, bytes])
  SafeDeserializer.from_bytes(b, bytes])

# python/sglang/srt/connector/serde/serde.py
  Serializer.to_bytes(t)
  Deserializer.__init__(dtype)
  Deserializer.from_bytes(bs)

# python/sglang/srt/connector/utils.py
parse_model_name(url)
pull_files_from_db(connector, model_name, allow_pattern, ignore_pattern)

# python/sglang/srt/constrained/base_grammar_backend.py
  BaseGrammarObject.__init__()
  BaseGrammarObject.accept_token(token)
  BaseGrammarObject.rollback(k)
  BaseGrammarObject.is_terminated()
  BaseGrammarObject.allocate_vocab_mask(vocab_size, batch_size, device)
  BaseGrammarObject.fill_vocab_mask(vocab_mask, idx)
  BaseGrammarObject.move_vocab_mask(vocab_mask, device)
  BaseGrammarObject.apply_vocab_mask(logits, vocab_mask)
  BaseGrammarObject.copy()
  BaseGrammarObject.finished()
  BaseGrammarObject.finished(finished)
  BaseGrammarObject.try_jump_forward(tokenizer)
  BaseGrammarObject.jump_forward_str_state(helper, str])
  BaseGrammarObject.jump_and_retokenize(old_output_ids, new_output_ids, next_state)
  BaseGrammarBackend.__init__()
  BaseGrammarBackend.dispatch_fallback(key_type, key_string)
  BaseGrammarBackend.dispatch_json(key_string)
  BaseGrammarBackend.dispatch_regex(key_string)
  BaseGrammarBackend.dispatch_ebnf(key_string)
  BaseGrammarBackend.dispatch_structural_tag(key_string)
  BaseGrammarBackend.get_cached_or_future_value(key, str])
  BaseGrammarBackend.set_cache(key, str], value)
  BaseGrammarBackend.reset()
create_grammar_backend(server_args, tokenizer, vocab_size, eos_token_ids)

# python/sglang/srt/constrained/llguidance_backend.py
  GuidanceGrammar.__init__(llguidance_tokenizer, serialized_grammar)
  GuidanceGrammar.accept_token(token)
  GuidanceGrammar.fill_vocab_mask(vocab_mask, idx)
  GuidanceGrammar.allocate_vocab_mask(vocab_size, batch_size, device)
  GuidanceGrammar.move_vocab_mask(vocab_mask, device)
  GuidanceGrammar.apply_vocab_mask(logits, vocab_mask)
  GuidanceGrammar.copy()
  GuidanceGrammar.try_jump_forward(tokenizer)
  GuidanceGrammar.jump_forward_str_state(helper, str])
  GuidanceGrammar.jump_and_retokenize(old_output_ids, new_output_ids, next_state)
  GuidanceBackend.__init__(tokenizer, whitespace_pattern, n_vocab)
  GuidanceBackend.dispatch_json(key_string)
  GuidanceBackend.dispatch_regex(key_string)
  GuidanceBackend.dispatch_ebnf(key_string)
  GuidanceBackend.dispatch_structural_tag(key_string)

# python/sglang/srt/constrained/outlines_backend.py
  OutlinesGrammar.__init__(guide, jump_forward_map, None])
  OutlinesGrammar.accept_token(token)
  OutlinesGrammar.allocate_vocab_mask(vocab_size, batch_size, device)
  OutlinesGrammar.move_vocab_mask(vocab_mask, device)
  OutlinesGrammar.fill_vocab_mask(vocab_mask, idx)
  OutlinesGrammar.apply_vocab_mask(logits, vocab_mask)
  OutlinesGrammar.copy()
  OutlinesGrammar.try_jump_forward(tokenizer)
  OutlinesGrammar.jump_forward_str_state(helper, str])
  OutlinesGrammar.jump_and_retokenize(old_output_ids, new_output_ids, next_state)
  OutlinesGrammarBackend.__init__(tokenizer, whitespace_pattern)
  OutlinesGrammarBackend.dispatch_ebnf(key_string)
  OutlinesGrammarBackend.dispatch_structural_tag(key_string)
  OutlinesGrammarBackend.dispatch_json(key_string)
  OutlinesGrammarBackend.dispatch_regex(key_string)
build_regex_from_object(object, BaseModel, Dict], whitespace_pattern)

# python/sglang/srt/constrained/outlines_jump_forward.py
disk_cache(expire, typed, ignore)
init_state_to_jump_forward(regex_string)
  OutlinesJumpForwardMap.__init__(regex_string)
  OutlinesJumpForwardMap.jump_forward_symbol(state)
  OutlinesJumpForwardMap.jump_forward_byte(state)
  OutlinesJumpForwardMap.is_jump_forward_symbol_state(state)
test_main(regex_string)

# python/sglang/srt/constrained/reasoner_grammar_backend.py
  ReasonerGrammarObject.__init__(grammar, think_end_id)
  ReasonerGrammarObject.accept_token(token)
  ReasonerGrammarObject.allocate_vocab_mask(vocab_size, batch_size, device)
  ReasonerGrammarObject.fill_vocab_mask(vocab_mask, idx)
  ReasonerGrammarObject.move_vocab_mask(vocab_mask, device)
  ReasonerGrammarObject.apply_vocab_mask()
  ReasonerGrammarObject.copy()
  ReasonerGrammarObject.finished()
  ReasonerGrammarObject.finished(finished)
  ReasonerGrammarObject.try_jump_forward(tokenizer)
  ReasonerGrammarObject.jump_forward_str_state(helper)
  ReasonerGrammarObject.jump_and_retokenize(old_output_ids, new_output_ids, next_state)
  ReasonerGrammarBackend.__init__(grammar_backend, think_end_id)

# python/sglang/srt/constrained/triton_ops/bitmask_ops.py
apply_token_bitmask_inplace_kernel(logits_ptr, bitmask_ptr, indices_ptr, num_rows, vocab_size, logits_strides, bitmask_strides, NUM_SMS, BLOCK_SIZE)
apply_token_bitmask_inplace_triton(logits, bitmask, indices, torch.Tensor]])

# python/sglang/srt/constrained/xgrammar_backend.py
  XGrammarGrammar.__init__(matcher, vocab_size, ctx, override_stop_tokens, int]], key_string)
  XGrammarGrammar.accept_token(token)
  XGrammarGrammar.rollback(k)
  XGrammarGrammar.is_terminated()
  XGrammarGrammar.allocate_vocab_mask(vocab_size, batch_size, device)
  XGrammarGrammar.fill_vocab_mask(vocab_mask, idx)
  XGrammarGrammar.move_vocab_mask(vocab_mask, device)
  XGrammarGrammar.apply_vocab_mask(logits, vocab_mask)
  XGrammarGrammar.copy()
  XGrammarGrammar.try_jump_forward(tokenizer)
  XGrammarGrammar.jump_forward_str_state(helper, str])
  XGrammarGrammar.jump_and_retokenize(old_output_ids, new_output_ids, next_state)
  XGrammarGrammar.__repr__()
  XGrammarGrammarBackend.__init__(tokenizer, vocab_size, model_eos_token_ids)
  XGrammarGrammarBackend.dispatch_json(key_string)
  XGrammarGrammarBackend.dispatch_ebnf(key_string)
  XGrammarGrammarBackend.dispatch_regex(key_string)
  XGrammarGrammarBackend.dispatch_structural_tag(key_string)
  XGrammarGrammarBackend.reset()

# python/sglang/srt/conversation.py
  Conversation.get_prompt()
  Conversation.set_system_message(system_message)
  Conversation.append_message(role, message)
  Conversation.append_image(image, detail, 'low', 'high'])
  Conversation.append_video(video)
  Conversation.append_audio(audio)
  Conversation.update_last_message(message)
  Conversation.to_gradio_chatbot()
  Conversation.to_openai_api_messages()
  Conversation.copy()
  Conversation.dict()
register_conv_template(template, override)
register_conv_template_matching_function(func)
get_conv_template_by_model_path(model_path)
chat_template_exists(template_name)
generate_embedding_convs(texts, images, template_name)
generate_chat_conv(request, template_name)
get_model_type(model_path)
match_internvl(model_path)
match_deepseek_janus_pro(model_path)
match_vicuna(model_path)
match_deepseek_vl(model_path)
match_qwen_chat_ml(model_path)
match_minicpm(model_path)
match_phi_4_mm(model_path)

# python/sglang/srt/custom_op.py
  CustomOp.__init__()
  CustomOp.enter_torch_compile(num_tokens)
  CustomOp.leave_torch_compile()
  CustomOp.forward()
  CustomOp.forward_native()
  CustomOp.forward_cuda()
  CustomOp.forward_npu()
  CustomOp.forward_hip()
  CustomOp.forward_xpu()
  CustomOp.forward_hpu()
  CustomOp.forward_cpu()
  CustomOp.dispatch_forward()

# python/sglang/srt/debug_utils/dump_comparator.py
main(args)
read_meta(directory)
check_tensor_pair(path_baseline, path_target)

# python/sglang/srt/debug_utils/dumper.py
  _Dumper.__init__()
  _Dumper.on_forward_pass_start()
  _Dumper.dump(name, value)
get_truncated_value(value)

# python/sglang/srt/debug_utils/text_comparator.py
main(args)

# python/sglang/srt/disaggregation/ascend/conn.py
  AscendKVManager.init_engine()
  AscendKVManager.register_buffer_to_engine()

# python/sglang/srt/disaggregation/ascend/transfer_engine.py
  AscendTransferEngine.__init__(hostname, npu_id, disaggregation_mode)
  AscendTransferEngine.initialize()
  AscendTransferEngine.batch_register(ptrs, lengths)

# python/sglang/srt/disaggregation/base/conn.py
  BaseKVManager.__init__(args, disaggregation_mode, server_args, is_mla_backend)
  BaseKVSender.__init__(mgr, bootstrap_addr, bootstrap_room, dest_tp_ranks, pp_rank)
  BaseKVSender.init(num_kv_indices, aux_index)
  BaseKVSender.send(kv_indices)
  BaseKVSender.poll()
  BaseKVSender.failure_exception()
  BaseKVReceiver.__init__(mgr, bootstrap_addr, bootstrap_room)
  BaseKVReceiver.init(kv_indices, aux_index)
  BaseKVReceiver.poll()
  BaseKVReceiver.failure_exception()
  BaseKVBootstrapServer.__init__(port)

# python/sglang/srt/disaggregation/common/conn.py
  CommonKVManager.__init__(args, disaggregation_mode, server_args, is_mla_backend)
  CommonKVReceiver.__init__(mgr, bootstrap_addr, bootstrap_room, data_parallel_rank)
  CommonKVReceiver.failure_exception()
  CommonKVBootstrapServer.__init__(port)
  CommonKVBootstrapServer.run()
  CommonKVBootstrapServer.close()
  CommonKVBootstrapServer.poll()

# python/sglang/srt/disaggregation/common/utils.py
  FastQueue.__init__()
  FastQueue.put(item)
  FastQueue.get()
group_concurrent_contiguous(src_indices, dst_indices)

# python/sglang/srt/disaggregation/decode.py
  DecodeReqToTokenPool.__init__(size, max_context_len, device, enable_memory_saver, pre_alloc_size)
  DecodeReqToTokenPool.write(indices, values)
  DecodeReqToTokenPool.available_size()
  DecodeReqToTokenPool.alloc(need_size)
  DecodeReqToTokenPool.free(free_index, List[int]])
  DecodeReqToTokenPool.clear()
  DecodePreallocQueue.__init__(req_to_token_pool, token_to_kv_pool_allocator, draft_token_to_kv_pool, req_to_metadata_buffer_idx_allocator, metadata_buffers, scheduler, transfer_queue, tree_cache, gloo_group, tp_rank, tp_size, dp_size, gpu_id, bootstrap_port, max_total_num_tokens, prefill_pp_size, num_reserved_decode_tokens, transfer_backend)
  DecodePreallocQueue.add(req, is_retracted)
  DecodePreallocQueue.extend(reqs, is_retracted)
  DecodePreallocQueue.resume_retracted_reqs()
  DecodePreallocQueue.pop_preallocated()
  DecodePreallocQueue.num_tokens_pre_allocated()
  DecodeTransferQueue.__init__(gloo_group, req_to_metadata_buffer_idx_allocator, tp_rank, metadata_buffers, scheduler, tree_cache)
  DecodeTransferQueue.add(decode_req)
  DecodeTransferQueue.extend(decode_reqs)
  DecodeTransferQueue.pop_transferred()
  SchedulerDisaggregationDecodeMixin.event_loop_normal_disagg_decode()
  SchedulerDisaggregationDecodeMixin.event_loop_overlap_disagg_decode()
  SchedulerDisaggregationDecodeMixin.get_next_disagg_decode_batch_to_run()
  SchedulerDisaggregationDecodeMixin.get_new_prebuilt_batch()
  SchedulerDisaggregationDecodeMixin.process_decode_queue()

# python/sglang/srt/disaggregation/decode_schedule_batch_mixin.py
  ScheduleBatchDisaggregationDecodeMixin.prepare_for_prebuilt_extend()
  ScheduleBatchDisaggregationDecodeMixin.process_prebuilt_extend(server_args, model_config)

# python/sglang/srt/disaggregation/fake/conn.py
  FakeKVSender.__init__(mgr, bootstrap_addr, bootstrap_room, dest_tp_ranks, pp_rank)
  FakeKVSender.poll()
  FakeKVSender.init(kv_indices, aux_index)
  FakeKVSender.send(kv_indices)
  FakeKVSender.failure_exception()
  FakeKVReceiver.__init__(mgr, bootstrap_addr, bootstrap_room, data_parallel_rank)
  FakeKVReceiver.poll()
  FakeKVReceiver.init(kv_indices, aux_index)
  FakeKVReceiver.failure_exception()

# python/sglang/srt/disaggregation/kv_events.py
  EventPublisher.__init__(attn_dp_rank)
  EventPublisher.publish(events)
  EventPublisher.shutdown()
  NullEventPublisher.publish(events)
  NullEventPublisher.shutdown()
  ZmqEventPublisher.__init__(attn_dp_rank, endpoint, replay_endpoint, buffer_steps, hwm, max_queue_size, topic)
  ZmqEventPublisher.publish(events)
  ZmqEventPublisher.shutdown()
  ZmqEventPublisher.offset_endpoint_port(endpoint, data_parallel_rank)
  KVEventsConfig.from_cli(cls, cli_value)
  EventPublisherFactory.register_publisher(cls, name, ctor, EventPublisher])
  EventPublisherFactory.create(cls, config, attn_dp_rank)

# python/sglang/srt/disaggregation/launch_lb.py
  LBArgs.add_cli_args(parser)
  LBArgs.from_cli_args(cls, args)
  LBArgs.__post_init__()
main()

# python/sglang/srt/disaggregation/mini_lb.py
setup_logger()
  MiniLoadBalancer.__init__(prefill_configs, decode_servers, timeout)
  MiniLoadBalancer.add_prefill_server(new_prefill_config)
  MiniLoadBalancer.add_decode_server(new_decode_server)
  MiniLoadBalancer.select_pair()
  MiniLoadBalancer.generate(modified_request, prefill_server, decode_server, endpoint)
  MiniLoadBalancer.generate_stream(modified_request, prefill_server, decode_server, endpoint)
health_check()
health_check()
flush_cache()
get_server_info()
get_model_info()
handle_generate_request(request_data)
handle_chat_completion_request(request_data)
handle_completion_request(request_data)
get_models()
register(obj)
run(prefill_configs, decode_addrs, host, port, timeout)

# python/sglang/srt/disaggregation/mooncake/conn.py
  KVTransferError.__init__(bootstrap_room, failure_reason)
  KVTransferError.__str__()
  TransferInfo.from_zmq(cls, msg)
  KVArgsRegisterInfo.from_zmq(cls, msg)
  AuxDataCodec.serialize_data_from_buffer(src_addr, data_length)
  AuxDataCodec.deserialize_data_to_buffer(kv_args, buffer_index, aux_index, data)
  MooncakeKVManager.__init__(args, disaggregation_mode, server_args, is_mla_backend)
  MooncakeKVManager.init_engine()
  MooncakeKVManager.register_buffer_to_engine()
  MooncakeKVManager.send_kvcache(mooncake_session_id, prefill_kv_indices, dst_kv_ptrs, dst_kv_indices, executor)
  MooncakeKVManager.send_kvcache_slice(mooncake_session_id, prefill_kv_indices, dst_kv_ptrs, dst_kv_indices, dst_tp_rank, dst_attn_tp_size, dst_kv_item_len, executor)
  MooncakeKVManager.send_aux(req, prefill_aux_index, dst_aux_ptrs)
  MooncakeKVManager.send_aux_tcp(req, prefill_aux_index, dst_aux_ptrs)
  MooncakeKVManager.send_aux_data_to_endpoint(remote, dst_port, room, buffer_index, aux_index, data)
  MooncakeKVManager.sync_status_to_decode_endpoint(remote, dst_port, room, status, prefill_rank)
  MooncakeKVManager.transfer_worker(queue, executor)
  MooncakeKVManager.start_prefill_thread()
  MooncakeKVManager.start_decode_thread()
  MooncakeKVManager.add_transfer_request(bootstrap_room, kv_indices, index_slice, is_last, aux_index)
  MooncakeKVManager.check_status(bootstrap_room)
  MooncakeKVManager.update_status(bootstrap_room, status)
  MooncakeKVManager.record_failure(bootstrap_room, failure_reason)
  MooncakeKVManager.get_session_id()
  MooncakeKVSender.__init__(mgr, bootstrap_addr, bootstrap_room, dest_tp_ranks, pp_rank)
  MooncakeKVSender.init(num_kv_indices, aux_index)
  MooncakeKVSender.send(kv_indices)
  MooncakeKVSender.poll()
  MooncakeKVSender.clear()
  MooncakeKVSender.failure_exception()
  MooncakeKVSender.abort()
  MooncakeKVReceiver.__init__(mgr, bootstrap_addr, bootstrap_room, data_parallel_rank)
  MooncakeKVReceiver.init(kv_indices, aux_index)
  MooncakeKVReceiver.poll()
  MooncakeKVReceiver.clear()
  MooncakeKVReceiver.failure_exception()
  MooncakeKVReceiver.abort()
  MooncakeKVBootstrapServer.__init__(port)
  MooncakeKVBootstrapServer.run()
  MooncakeKVBootstrapServer.close()
  MooncakeKVBootstrapServer.poll()

# python/sglang/srt/disaggregation/mooncake/transfer_engine.py
  MooncakeTransferEngine.__init__(hostname, gpu_id, ib_device)
  MooncakeTransferEngine.register(ptr, length)
  MooncakeTransferEngine.deregister(ptr)
  MooncakeTransferEngine.batch_register(ptrs, lengths)
  MooncakeTransferEngine.batch_deregister(ptrs)
  MooncakeTransferEngine.initialize(hostname, device_name)
  MooncakeTransferEngine.transfer_sync(session_id, buffer, peer_buffer_address, length)
  MooncakeTransferEngine.batch_transfer_sync(session_id, buffers, peer_buffer_addresses, lengths)
  MooncakeTransferEngine.get_session_id()

# python/sglang/srt/disaggregation/nixl/conn.py
  TransferInfo.is_dummy()
  TransferInfo.from_zmq(cls, msg)
  KVArgsRegisterInfo.from_zmq(cls, msg)
  TransferStatus.is_done()
  NixlKVManager.__init__(args, disaggregation_mode, server_args, is_mla_backend)
  NixlKVManager.check_status(bootstrap_room)
  NixlKVManager.update_status(bootstrap_room, status)
  NixlKVManager.register_buffer_to_engine()
  NixlKVManager.send_kvcache(peer_name, prefill_kv_indices, dst_kv_ptrs, dst_kv_indices, dst_gpu_id, notif)
  NixlKVManager.send_aux(peer_name, prefill_aux_index, dst_aux_ptrs, dst_aux_index, notif)
  NixlKVManager.add_transfer_request(bootstrap_room, kv_indices, index_slice, is_last, chunk_id, aux_index)
  NixlKVManager.update_transfer_status()
  NixlKVManager.check_transfer_done(room)
  NixlKVSender.__init__(mgr, bootstrap_addr, bootstrap_room, dest_tp_ranks, pp_rank)
  NixlKVSender.init(num_kv_indices, aux_index)
  NixlKVSender.send(kv_indices)
  NixlKVSender.poll()
  NixlKVSender.failure_exception()
  NixlKVReceiver.__init__(mgr, bootstrap_addr, bootstrap_room, data_parallel_rank)
  NixlKVReceiver.init(kv_indices, aux_index)
  NixlKVReceiver.poll()
  NixlKVReceiver.failure_exception()

# python/sglang/srt/disaggregation/prefill.py
  PrefillBootstrapQueue.__init__(token_to_kv_pool, draft_token_to_kv_pool, req_to_metadata_buffer_idx_allocator, metadata_buffers, tp_rank, tp_size, gpu_id, bootstrap_port, gloo_group, max_total_num_tokens, decode_tp_size, decode_dp_size, scheduler, pp_rank, pp_size, transfer_backend)
  PrefillBootstrapQueue.add(req, num_kv_heads)
  PrefillBootstrapQueue.extend(reqs, num_kv_heads)
  PrefillBootstrapQueue.pop_bootstrapped(return_failed_reqs, rids_to_check)
  SchedulerDisaggregationPrefillMixin.event_loop_normal_disagg_prefill()
  SchedulerDisaggregationPrefillMixin.event_loop_overlap_disagg_prefill()
  SchedulerDisaggregationPrefillMixin.process_batch_result_disagg_prefill(batch, result, launch_done)
  SchedulerDisaggregationPrefillMixin.process_disagg_prefill_inflight_queue(rids_to_check)
  SchedulerDisaggregationPrefillMixin.get_transferred_rids()
  SchedulerDisaggregationPrefillMixin.process_prefill_chunk()
  SchedulerDisaggregationPrefillMixin.send_kv_chunk(req, last_chunk, end_idx)
  SchedulerDisaggregationPrefillMixin.event_loop_pp_disagg_prefill()
  SchedulerDisaggregationPrefillMixin.send_pyobj_to_next_stage(data)
  SchedulerDisaggregationPrefillMixin.recv_pyobj_from_prev_stage()

# python/sglang/srt/disaggregation/utils.py
poll_and_all_reduce(pollers, gloo_group)
  ReqToMetadataIdxAllocator.__init__(size)
  ReqToMetadataIdxAllocator.available_size()
  ReqToMetadataIdxAllocator.alloc()
  ReqToMetadataIdxAllocator.free(free_index)
  MetadataBuffers.__init__(size, hidden_size, dtype, max_top_logprobs_num, custom_mem_pool)
  MetadataBuffers.get_buf_infos()
  MetadataBuffers.get_buf(idx)
  MetadataBuffers.set_buf(req)
get_kv_class(transfer_backend, class_type)
kv_to_page_indices(kv_indices, page_size)
kv_to_page_num(num_kv_indices, page_size)
  PDRegistryRequest.__post_init__()
register_disaggregation_server(mode, server_port, bootstrap_port, pdlb_url)
is_mla_backend(target_kv_pool)
prepare_abort(req, error_message, status_code)

# python/sglang/srt/distributed/communication_op.py
tensor_model_parallel_all_reduce(input_)
tensor_model_parallel_all_gather(input_, dim)
tensor_model_parallel_gather(input_, dst, dim)
broadcast_tensor_dict(tensor_dict, Union[torch.Tensor, Any]]], src)

# python/sglang/srt/distributed/device_communicators/cuda_wrapper.py
find_loaded_library(lib_name)
  CudaRTLibrary.__init__(so_file)
  CudaRTLibrary.CUDART_CHECK(result)
  CudaRTLibrary.cudaGetErrorString(error)
  CudaRTLibrary.cudaSetDevice(device)
  CudaRTLibrary.cudaDeviceSynchronize()
  CudaRTLibrary.cudaDeviceReset()
  CudaRTLibrary.cudaMalloc(size)
  CudaRTLibrary.cudaFree(devPtr)
  CudaRTLibrary.cudaMemset(devPtr, value, count)
  CudaRTLibrary.cudaMemcpy(dst, src, count)
  CudaRTLibrary.cudaIpcGetMemHandle(devPtr)
  CudaRTLibrary.cudaIpcOpenMemHandle(handle)

# python/sglang/srt/distributed/device_communicators/custom_all_reduce.py
  CustomAllreduce.__init__(group, device, str, torch.device], max_size)
  CustomAllreduce.create_shared_buffer(size_in_bytes, group)
  CustomAllreduce.free_shared_buffer(pointers, group)
  CustomAllreduce.capture()
  CustomAllreduce.register_buffer(inp)
  CustomAllreduce.register_graph_buffers()
  CustomAllreduce.should_custom_ar(inp)
  CustomAllreduce.all_reduce_reg(inp, out)
  CustomAllreduce.all_reduce_unreg(inp, out)
  CustomAllreduce.all_reduce(inp)
  CustomAllreduce.custom_all_reduce(input)
  CustomAllreduce.close()
  CustomAllreduce.__del__()

# python/sglang/srt/distributed/device_communicators/custom_all_reduce_utils.py
update_environment_variables(envs, str])
producer(batch_src, producer_queue, consumer_queue, result_queue, cuda_visible_devices)
consumer(batch_tgt, producer_queue, consumer_queue, result_queue, cuda_visible_devices)
can_actually_p2p(batch_src, batch_tgt)
gpu_p2p_access_check(src, tgt)
with_nvml_context(fn, _R])
is_full_nvlink(physical_device_ids, world_size)
is_weak_contiguous(inp)

# python/sglang/srt/distributed/device_communicators/hpu_communicator.py
  HpuCommunicator.__init__(group)
  HpuCommunicator.all_reduce(x)
  HpuCommunicator.all_gather(x, dim)

# python/sglang/srt/distributed/device_communicators/npu_communicator.py
  NpuCommunicator.__init__(group)
  NpuCommunicator.all_reduce(x)
  NpuCommunicator.all_gather(x, dim)

# python/sglang/srt/distributed/device_communicators/pymscclpp.py
mscclpp_is_weak_contiguous(inp)
mscclpp_convert_to_bytes(size_str)
mscclpp_bench_time(func, test_niter, warmup_niter)
  PyMscclppCommunicator.__init__(group, device, str, torch.device], max_bytes)
  PyMscclppCommunicator.pre_tune_config(dtype)
  PyMscclppCommunicator.should_mscclpp_allreduce(inp, op)
  PyMscclppCommunicator.all_reduce(tensor, op)
  PyMscclppCommunicator.change_state(enable)

# python/sglang/srt/distributed/device_communicators/pynccl.py
  PyNcclCommunicator.__init__(group, StatelessProcessGroup], device, str, torch.device], library_path)
  PyNcclCommunicator.all_reduce(tensor, op, stream)
  PyNcclCommunicator.all_gather(output_tensor, input_tensor, stream, sizes)
  PyNcclCommunicator.reduce_scatter(output_tensor, input_tensor, op, stream, sizes)
  PyNcclCommunicator.send(tensor, dst, stream)
  PyNcclCommunicator.recv(tensor, src, stream)
  PyNcclCommunicator.broadcast(tensor, src, stream)
  PyNcclCommunicator.register_comm_window_raw(ptr, size)
  PyNcclCommunicator.deregister_comm_window(window)
  PyNcclCommunicator.group_start()
  PyNcclCommunicator.group_end()
  PyNcclCommunicator.change_state(enable, stream)

# python/sglang/srt/distributed/device_communicators/pynccl_allocator.py
is_symmetric_memory_enabled()
set_graph_pool_id(graph_pool_id)
get_nccl_mem_pool()
  use_symmetric_memory.__init__(group_coordinator)
  use_symmetric_memory.__enter__()
  use_symmetric_memory.tag(tensor)
  use_symmetric_memory.__exit__(exc_type, exc_val, exc_tb)

# python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py
find_nccl_library()
  ncclDataTypeEnum.from_torch(cls, dtype)
  ncclRedOpTypeEnum.from_torch(cls, op)
  NCCLLibrary.__init__(so_file)
  NCCLLibrary.ncclGetErrorString(result)
  NCCLLibrary.NCCL_CHECK(result)
  NCCLLibrary.ncclGetRawVersion()
  NCCLLibrary.ncclGetVersion()
  NCCLLibrary.ncclGetUniqueId()
  NCCLLibrary.ncclCommInitRank(world_size, unique_id, rank)
  NCCLLibrary.ncclAllReduce(sendbuff, recvbuff, count, datatype, op, comm, stream)
  NCCLLibrary.ncclReduce(sendbuff, recvbuff, count, datatype, op, root, comm, stream)
  NCCLLibrary.ncclReduceScatter(sendbuff, recvbuff, count, datatype, op, comm, stream)
  NCCLLibrary.ncclAllGather(sendbuff, recvbuff, count, datatype, comm, stream)
  NCCLLibrary.ncclSend(sendbuff, count, datatype, dest, comm, stream)
  NCCLLibrary.ncclRecv(recvbuff, count, datatype, src, comm, stream)
  NCCLLibrary.ncclBroadcast(sendbuff, recvbuff, count, datatype, root, comm, stream)
  NCCLLibrary.ncclCommDestroy(comm)
  NCCLLibrary.ncclCommWindowRegister(comm, buff, size, win_flags)
  NCCLLibrary.ncclCommWindowDeregister(comm, window)
  NCCLLibrary.ncclGroupStart()
  NCCLLibrary.ncclGroupEnd()

# python/sglang/srt/distributed/device_communicators/quick_all_reduce.py
qr_rocm_arch_available()
  QuickAllReduce.__init__(group, device, str, torch.device])
  QuickAllReduce.init_quick_all_reduce()
  QuickAllReduce.create_shared_buffer()
  QuickAllReduce.should_quick_allreduce(inp)
  QuickAllReduce.quick_all_reduce(inp)
  QuickAllReduce.close()
  QuickAllReduce.__del__()

# python/sglang/srt/distributed/device_communicators/shm_broadcast.py
  ShmRingBuffer.__init__(n_reader, max_chunk_bytes, max_chunks, name)
  ShmRingBuffer.__reduce__()
  ShmRingBuffer.__del__()
  ShmRingBuffer.get_data(current_idx)
  ShmRingBuffer.get_metadata(current_idx)
  MessageQueue.__init__(n_reader, n_local_reader, local_reader_ranks, max_chunk_bytes, max_chunks, connect_ip)
  MessageQueue.export_handle()
  MessageQueue.create_from_handle(handle, rank)
  MessageQueue.wait_until_ready()
  MessageQueue.acquire_write()
  MessageQueue.acquire_read()
  MessageQueue.enqueue(obj)
  MessageQueue.dequeue()
  MessageQueue.broadcast_object(obj)
  MessageQueue.create_from_process_group(pg, max_chunk_bytes, max_chunks, writer_rank)

# python/sglang/srt/distributed/device_communicators/xpu_communicator.py
  XpuCommunicator.__init__(group)
  XpuCommunicator.all_reduce(x)
  XpuCommunicator.gather(input_, rank_in_group, dst, dim)

# python/sglang/srt/distributed/naive_distributed.py
  NaiveDistributed.__init__(rank, world_size, rendezvous)
  NaiveDistributed.get_rank()
  NaiveDistributed.get_world_size()
  NaiveDistributed.scatter(tensor, scatter_list, src)
  NaiveDistributed.all_gather_object(obj)
  NaiveDistributed.barrier()
get_naive_distributed()
set_naive_distributed(instance)

# python/sglang/srt/distributed/parallel_state.py
inplace_all_reduce(tensor, group_name)
inplace_all_reduce_fake(tensor, group_name)
outplace_all_reduce(tensor, group_name, outplace_all_reduce_method)
outplace_all_reduce_fake(tensor, group_name, outplace_all_reduce_method)
reg_all_gather_into_tensor(output, input, group_name)
reg_all_gather_into_tensor_fake(output, input, group_name)
  GroupCoordinator.__init__(group_ranks, local_rank, torch_distributed_backend, Backend], use_pynccl, use_pymscclpp, use_custom_allreduce, use_hpu_communicator, use_xpu_communicator, use_npu_communicator, use_message_queue_broadcaster, group_name)
  GroupCoordinator.__repr__()
  GroupCoordinator.first_rank()
  GroupCoordinator.last_rank()
  GroupCoordinator.is_first_rank()
  GroupCoordinator.is_last_rank()
  GroupCoordinator.next_rank()
  GroupCoordinator.prev_rank()
  GroupCoordinator.graph_capture(graph_capture_context)
  GroupCoordinator.all_reduce(input_)
  GroupCoordinator.reduce_scatter_tensor(output, input)
  GroupCoordinator.reduce_scatter(output, input_list)
  GroupCoordinator.reduce_scatterv(input_, output, sizes)
  GroupCoordinator.all_gather_into_tensor(output, input)
  GroupCoordinator.all_gather(input_, dim, output_tensor_list)
  GroupCoordinator.all_gatherv(input_, List[torch.Tensor]], sizes)
  GroupCoordinator.gather(input_, dst, dim)
  GroupCoordinator.broadcast(input_, src)
  GroupCoordinator.broadcast_object(obj, src)
  GroupCoordinator.broadcast_object_list(obj_list, src, group)
  GroupCoordinator.send_object(obj, dst)
  GroupCoordinator.recv_object(src)
  GroupCoordinator.broadcast_tensor_dict(tensor_dict, Union[torch.Tensor, Any]]], src, group, metadata_group)
  GroupCoordinator.send_tensor_dict(tensor_dict, Union[torch.Tensor, Any]], dst, all_gather_group)
  GroupCoordinator.recv_tensor_dict(src, all_gather_group)
  GroupCoordinator.barrier()
  GroupCoordinator.send(tensor, dst)
  GroupCoordinator.recv(size, dtype, src)
  GroupCoordinator.destroy()
get_world_group()
init_world_group(ranks, local_rank, backend)
init_model_parallel_group(group_ranks, local_rank, backend, use_custom_allreduce, use_message_queue_broadcaster, group_name, use_mscclpp_allreduce)
set_pdmux_status(enable_prefill_multiplexing)
get_tp_group()
get_moe_ep_group()
get_moe_tp_group()
get_pp_group()
graph_capture()
set_custom_all_reduce(enable)
set_mscclpp_all_reduce(enable)
init_distributed_environment(world_size, rank, distributed_init_method, local_rank, backend, timeout)
initialize_model_parallel(tensor_model_parallel_size, expert_model_parallel_size, pipeline_model_parallel_size, backend, duplicate_tp_group)
ensure_model_parallel_initialized(tensor_model_parallel_size, expert_model_parallel_size, pipeline_model_parallel_size, backend)
model_parallel_is_initialized()
patch_tensor_parallel_group(tp_group)
get_tensor_model_parallel_world_size()
get_tensor_model_parallel_rank()
get_moe_expert_parallel_world_size()
get_moe_expert_parallel_rank()
get_moe_tensor_parallel_world_size()
get_moe_tensor_parallel_rank()
destroy_model_parallel()
destroy_distributed_environment()
cleanup_dist_env_and_memory(shutdown_ray)
in_the_same_node_as(pg, source_rank)
monkey_patch_vllm_parallel_state(reverse)

# python/sglang/srt/distributed/utils.py
ensure_divisibility(numerator, denominator)
divide(numerator, denominator)
split_tensor_along_last_dim(tensor, num_partitions, contiguous_split_chunks)
get_pp_indices(num_hidden_layers, pp_rank, pp_size)
  StatelessProcessGroup.__post_init__()
  StatelessProcessGroup.send_obj(obj, dst)
  StatelessProcessGroup.expire_data()
  StatelessProcessGroup.recv_obj(src)
  StatelessProcessGroup.broadcast_obj(obj, src)
  StatelessProcessGroup.all_gather_obj(obj)
  StatelessProcessGroup.barrier()
  StatelessProcessGroup.create(host, port, rank, world_size, data_expiration_seconds)

# python/sglang/srt/entrypoints/EngineBase.py
  EngineBase.generate(prompt, str]], sampling_params, Dict]], input_ids, List[int]]], image_data, str]], return_logprob, bool]], logprob_start_len, int]], top_logprobs_num, int]], token_ids_logprob, List[int]]], lora_path, Optional[str]]], custom_logit_processor, str]], return_hidden_states, stream, bootstrap_host, str]], bootstrap_port, int]], bootstrap_room, int]], data_parallel_rank)
  EngineBase.flush_cache()
  EngineBase.update_weights_from_tensor(named_tensors, torch.Tensor]], load_format, flush_cache)
  EngineBase.load_lora_adapter(lora_name, lora_path)
  EngineBase.unload_lora_adapter(lora_name)
  EngineBase.release_memory_occupation()
  EngineBase.resume_memory_occupation()
  EngineBase.shutdown()

# python/sglang/srt/entrypoints/context.py
  ConversationContext.append_output(output)
  ConversationContext.call_tool()
  ConversationContext.need_builtin_tool_call()
  ConversationContext.render_for_completion()
  SimpleContext.__init__()
  SimpleContext.append_output(output)
  SimpleContext.need_builtin_tool_call()
  SimpleContext.call_tool()
  SimpleContext.render_for_completion()
  HarmonyContext.__init__(messages, tool_sessions, Union['ClientSession', Tool]])
  HarmonyContext.append_output(output)
  HarmonyContext.messages()
  HarmonyContext.need_builtin_tool_call()
  HarmonyContext.call_tool()
  HarmonyContext.render_for_completion()
  HarmonyContext.call_search_tool(tool_session, Tool], last_msg)
  HarmonyContext.call_python_tool(tool_session, Tool], last_msg)
  StreamingHarmonyContext.__init__()
  StreamingHarmonyContext.messages()
  StreamingHarmonyContext.append_output(output)
  StreamingHarmonyContext.is_expecting_start()
  StreamingHarmonyContext.is_assistant_action_turn()
  StreamingHarmonyContext.render_for_completion()

# python/sglang/srt/entrypoints/engine.py
  Engine.__init__()
  Engine.generate(prompt, str]], sampling_params, Dict]], input_ids, List[int]]], image_data, audio_data, video_data, return_logprob, bool]], logprob_start_len, int]], top_logprobs_num, int]], token_ids_logprob, List[int]]], lora_path, custom_logit_processor, str]], return_hidden_states, stream, bootstrap_host, str]], bootstrap_port, int]], bootstrap_room, int]], data_parallel_rank)
  Engine.async_generate(prompt, str]], sampling_params, Dict]], input_ids, List[int]]], image_data, audio_data, video_data, return_logprob, bool]], logprob_start_len, int]], top_logprobs_num, int]], token_ids_logprob, List[int]]], lora_path, custom_logit_processor, str]], return_hidden_states, stream, bootstrap_host, str]], bootstrap_port, int]], bootstrap_room, int]], data_parallel_rank)
  Engine.encode(prompt, List[str], List[Dict], List[List[Dict]]], image_data, audio_data, video_data)
  Engine.async_encode(prompt, List[str], List[Dict], List[List[Dict]]], image_data, audio_data, video_data)
  Engine.rerank(prompt)
  Engine.shutdown()
  Engine.__enter__()
  Engine.__exit__(exc_type, exc_value, traceback)
  Engine.flush_cache()
  Engine.start_profile()
  Engine.stop_profile()
  Engine.start_expert_distribution_record()
  Engine.stop_expert_distribution_record()
  Engine.dump_expert_distribution_record()
  Engine.get_server_info()
  Engine.init_weights_update_group(master_address, master_port, rank_offset, world_size, group_name, backend)
  Engine.update_weights_from_distributed(names, dtypes, shapes, group_name, flush_cache)
  Engine.update_weights_from_tensor(named_tensors, torch.Tensor]], load_format, flush_cache)
  Engine.update_weights_from_disk(model_path, load_format)
  Engine.get_weights_by_name(name, truncate_size)
  Engine.load_lora_adapter(lora_name, lora_path, pinned)
  Engine.unload_lora_adapter(lora_name)
  Engine.release_memory_occupation(tags)
  Engine.resume_memory_occupation(tags)
  Engine.freeze_gc()
  Engine.collective_rpc(method)
  Engine.save_remote_model()
  Engine.save_sharded_model()
  Engine.score(query, List[int]]], items, List[str], List[List[int]]]], label_token_ids, apply_softmax, item_first)
  Engine.async_score(query, List[int]]], items, List[str], List[List[int]]]], label_token_ids, apply_softmax, item_first)

# python/sglang/srt/entrypoints/harmony_utils.py
get_encoding()
get_system_message(model_identity, reasoning_effort, 'medium', 'low']], start_date, browser_description, python_description)
get_developer_message(instructions, tools)
get_user_message(content)
parse_response_input(response_msg, prev_responses, ResponseReasoningItem]])
parse_response_output(output)
parse_chat_input(chat_msg)
render_for_completion(messages)
get_stop_tokens_for_assistant_actions()
get_streamable_parser_for_assistant()
parse_output_message(message)
parse_remaining_state(parser)
parse_output_into_messages(token_ids)

# python/sglang/srt/entrypoints/http_server.py
set_global_state(global_state)
lifespan(fast_api_app)
validation_exception_handler(request, exc)
validation_exception_handler(request, exc)
validate_json_request(raw_request)
health_generate(request)
get_model_info()
get_weight_version()
get_server_info()
get_load()
set_internal_state(obj, request)
generate_request(obj, request)
generate_from_file_request(file, request)
encode_request(obj, request)
classify_request(obj, request)
flush_cache()
start_profile_async(obj)
stop_profile_async()
freeze_gc_async()
start_expert_distribution_record_async()
stop_expert_distribution_record_async()
dump_expert_distribution_record_async()
update_weights_from_disk(obj, request)
init_weights_update_group(obj, request)
update_weights_from_tensor(obj, request)
update_weights_from_distributed(obj, request)
update_weight_version(obj, request)
get_weights_by_name(obj, request)
release_memory_occupation(obj, request)
resume_memory_occupation(obj, request)
slow_down(obj, request)
load_lora_adapter(obj, request)
unload_lora_adapter(obj, request)
open_session(obj, request)
close_session(obj, request)
configure_logging(obj, request)
abort_request(obj, request)
parse_function_call_request(obj, request)
separate_reasoning_request(obj, request)
pause_generation(request)
continue_generation(request)
openai_v1_completions(request, raw_request)
openai_v1_chat_completions(request, raw_request)
openai_v1_embeddings(request, raw_request)
available_models()
retrieve_model(model)
v1_score_request(request, raw_request)
v1_responses_request(request, raw_request)
v1_retrieve_responses(response_id, raw_request)
v1_cancel_responses(response_id, raw_request)
v1_rerank_request(request, raw_request)
sagemaker_health()
sagemaker_chat_completions(request, raw_request)
vertex_generate(vertex_req, raw_request)
launch_server(server_args, pipe_finish_writer, launch_callback, None]])

# python/sglang/srt/entrypoints/http_server_engine.py
launch_server_process(server_args)
  HttpServerEngineAdapter.__init__()
  HttpServerEngineAdapter.update_weights_from_tensor(named_tensors, torch.Tensor]], load_format, flush_cache)
  HttpServerEngineAdapter.shutdown()
  HttpServerEngineAdapter.generate(prompt, sampling_params, input_ids, image_data, return_logprob, logprob_start_len, top_logprobs_num, token_ids_logprob, lora_path, custom_logit_processor)
  HttpServerEngineAdapter.release_memory_occupation()
  HttpServerEngineAdapter.resume_memory_occupation()
  HttpServerEngineAdapter.flush_cache()

# python/sglang/srt/entrypoints/openai/protocol.py
  CompletionRequest.validate_max_tokens_positive(cls, v)
  ChatCompletionRequest.set_tool_choice_default(cls, values)
  ResponsesRequest.to_sampling_params(default_max_tokens, default_params)
  ResponsesResponse.from_request(cls, request, sampling_params, model_name, created_time, output, ResponseReasoningItem, ResponseFunctionToolCall]], status, usage)

# python/sglang/srt/entrypoints/openai/serving_base.py
  OpenAIServingBase.__init__(tokenizer_manager)
  OpenAIServingBase.handle_request(request, raw_request)
  OpenAIServingBase.create_error_response(message, err_type, status_code, param)
  OpenAIServingBase.create_streaming_error_response(message, err_type, status_code)

# python/sglang/srt/entrypoints/openai/serving_chat.py
  OpenAIServingChat.__init__(tokenizer_manager, template_manager)

# python/sglang/srt/entrypoints/openai/serving_completions.py
  OpenAIServingCompletion.__init__(tokenizer_manager, template_manager)

# python/sglang/srt/entrypoints/openai/serving_embedding.py
  OpenAIServingEmbedding.__init__(tokenizer_manager, template_manager)

# python/sglang/srt/entrypoints/openai/serving_responses.py
  OpenAIServingResponses.__init__(tokenizer_manager, template_manager)
  OpenAIServingResponses.create_responses(request, raw_request)
  OpenAIServingResponses.responses_full_generator(request, sampling_params, result_generator, context, model_name, tokenizer, request_metadata, created_time)
  OpenAIServingResponses.retrieve_responses(response_id)
  OpenAIServingResponses.cancel_responses(response_id)
  OpenAIServingResponses.responses_stream_generator(request, sampling_params, result_generator, context, model_name, tokenizer, request_metadata, created_time)

# python/sglang/srt/entrypoints/openai/tool_server.py
list_server_and_tools(server_url)
trim_schema(schema)
post_process_tools_description(list_tools_result)
  ToolServer.has_tool(tool_name)
  ToolServer.get_tool_description(tool_name)
  ToolServer.get_tool_session(tool_name)
  MCPToolServer.__init__()
  MCPToolServer.add_tool_server(server_url)
  MCPToolServer.has_tool(tool_name)
  MCPToolServer.get_tool_description(tool_name)
  MCPToolServer.get_tool_session(tool_name)
  DemoToolServer.__init__()
  DemoToolServer.has_tool(tool_name)
  DemoToolServer.get_tool_description(tool_name)
  DemoToolServer.get_tool_session(tool_name)

# python/sglang/srt/entrypoints/openai/usage_processor.py
  UsageProcessor.calculate_response_usage(responses, Any]], n_choices, enable_cache_report)
  UsageProcessor.calculate_streaming_usage(prompt_tokens, int], completion_tokens, int], cached_tokens, int], n_choices, enable_cache_report)
  UsageProcessor.calculate_token_usage(prompt_tokens, completion_tokens, cached_tokens, int]])

# python/sglang/srt/entrypoints/openai/utils.py
to_openai_style_logprobs(input_token_logprobs, output_token_logprobs, input_top_logprobs, output_top_logprobs)
process_hidden_states_from_ret(ret_item, Any], request, CompletionRequest])

# python/sglang/srt/entrypoints/tool.py
  Tool.get_result(context)
  HarmonyBrowserTool.__init__()
  HarmonyBrowserTool.get_result(context)
  HarmonyBrowserTool.tool_config()
  HarmonyPythonTool.__init__()
  HarmonyPythonTool.get_result(context)
  HarmonyPythonTool.tool_config()

# python/sglang/srt/eplb/eplb_algorithms/__init__.py
rebalance_experts(tokens_per_expert, num_physical_experts, num_local_physical_experts, num_groups, num_nodes, algorithm)
compute_algorithm(raw_algorithm, num_groups, num_nodes)

# python/sglang/srt/eplb/eplb_algorithms/deepseek.py
balanced_packing(weight, num_packs)
replicate_experts(weight, num_phy)
rebalance_experts_hierarchical(weight, num_physical_experts, num_groups, num_nodes, num_gpus)
rebalance_experts(weight, num_replicas, num_groups, num_nodes, num_gpus, enable_hierarchical)

# python/sglang/srt/eplb/eplb_algorithms/deepseek_vec.py
pack_groups(tokens_per_group, num_nodes)
make_redundant_experts_chunkwise(tokens_per_expert, num_physical_experts, num_local_physical_experts, num_physical_experts_per_chunk)
decode_rebalance_experts(tokens_per_expert, num_physical_experts, num_local_physical_experts)
prefill_rebalance_experts(tokens_per_expert, num_physical_experts, num_local_physical_experts, num_groups, num_nodes)
rebalance_experts(tokens_per_expert, num_physical_experts, num_local_physical_experts, num_groups, num_nodes, enable_hierarchical)

# python/sglang/srt/eplb/eplb_manager.py
  EPLBManager.__init__(model_runner)
  EPLBManager.on_forward_pass_end()
  EPLBManager.rebalance()

# python/sglang/srt/eplb/eplb_simulator/reader.py
read_mode_per_pass(dir_data)

# python/sglang/srt/eplb/expert_distribution.py
  ExpertDistributionRecorder.init_new(server_args, expert_location_metadata, rank)
  ExpertDistributionRecorder.with_current_layer(layer_idx)
  ExpertDistributionRecorder.with_debug_name(debug_name)
  ExpertDistributionRecorder.disable_this_region()
  ExpertDistributionRecorder.with_forward_pass(forward_pass_id, forward_batch)
  ExpertDistributionRecorder.on_select_experts(topk_ids)
  ExpertDistributionRecorder.on_deepep_dispatch_normal(local_physical_count_of_layer, num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)
  ExpertDistributionRecorder.on_deepep_dispatch_low_latency(local_physical_count_of_layer)
  ExpertDistributionRecorder.start_record()
  ExpertDistributionRecorder.stop_record()
  ExpertDistributionRecorder.dump_record(output_mode)
  ExpertDistributionRecorder.recording()
  _ExpertDistributionRecorderReal.__init__(server_args, expert_location_metadata, rank)
  _ExpertDistributionRecorderReal.with_current_layer(layer_idx)
  _ExpertDistributionRecorderReal.with_debug_name(debug_name)
  _ExpertDistributionRecorderReal.with_forward_pass(forward_pass_id, forward_batch)
  _ExpertDistributionRecorderReal.disable_this_region()
  _ExpertDistributionRecorderReal.on_select_experts(topk_ids)
  _ExpertDistributionRecorderReal.on_deepep_dispatch_normal(local_physical_count_of_layer, num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)
  _ExpertDistributionRecorderReal.on_deepep_dispatch_low_latency(local_physical_count_of_layer)
  _ExpertDistributionRecorderReal.start_record()
  _ExpertDistributionRecorderReal.stop_record()
  _ExpertDistributionRecorderReal.dump_record(output_mode)
  _ExpertDistributionRecorderReal.recording()
get_global_expert_distribution_recorder()
set_global_expert_distribution_recorder(value)
  _SinglePassGatherer.init_new(server_args, expert_location_metadata, rank)
  _SinglePassGatherer.__init__(expert_location_metadata, rank)
  _SinglePassGatherer.on_forward_pass_start(forward_batch)
  _SinglePassGatherer.on_select_experts(layer_idx, topk_ids)
  _SinglePassGatherer.on_deepep_dispatch_normal(layer_idx, local_physical_count_of_layer, num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)
  _SinglePassGatherer.on_deepep_dispatch_low_latency(layer_idx, local_physical_count_of_layer)
  _SinglePassGatherer.reset()
  _SinglePassGatherer.collect()
  _DetailSinglePassGatherer.__init__(server_args, expert_location_metadata, rank)
  _DetailSinglePassGatherer.on_forward_pass_start(forward_batch)
  _DetailSinglePassGatherer.on_select_experts(layer_idx, topk_ids)
  _DetailSinglePassGatherer.on_deepep_dispatch_normal(layer_idx, local_physical_count_of_layer, num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)
  _DetailSinglePassGatherer.reset()
  _DetailSinglePassGatherer.collect()
  _LayerBasedCpuSinglePassGatherer.__init__()
  _LayerBasedCpuSinglePassGatherer.reset()
  _LayerBasedGpuSinglePassGatherer.__init__()
  _LayerBasedGpuSinglePassGatherer.reset()
  _LayerBasedGpuSinglePassGatherer.collect()
  _SelectExpertsSinglePassGatherer.__init__()
  _SelectExpertsSinglePassGatherer.on_select_experts(layer_idx, topk_ids)
  _DeepepNormalSinglePassGatherer.__init__()
  _DeepepNormalSinglePassGatherer.on_deepep_dispatch_normal(layer_idx, local_physical_count_of_layer, num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)
  _DeepepNormalSinglePassGatherer.collect()
  _DeepepLowLatencySinglePassGatherer.__init__()
  _DeepepLowLatencySinglePassGatherer.on_deepep_dispatch_low_latency(layer_idx, local_physical_count_of_layer)
  _Accumulator.init_new(server_args, expert_location_metadata, rank)
  _Accumulator.get_class(server_args)
  _Accumulator.__init__(server_args, expert_location_metadata, rank)
  _Accumulator.get_single_pass_gatherer_keys()
  _Accumulator.get_single_pass_gatherer_key(debug_name)
  _Accumulator.append(forward_pass_id, gatherer_key, single_pass_data)
  _Accumulator.reset()
  _Accumulator.dump(output_mode)
  _UtilizationRateAccumulatorMixin.__init__()
  _UtilizationRateAccumulatorMixin.append(forward_pass_id, gatherer_key, single_pass_data)
  _UtilizationRateAccumulatorMixin.reset()
  _DequeCollection.__init__(maxlens)
  _DequeCollection.append(value)
  _DequeCollection.clear()
  _DequeCollection.mean()
  _DetailAccumulator.__init__()
  _DetailAccumulator.get_single_pass_gatherer_keys()
  _DetailAccumulator.get_single_pass_gatherer_key(debug_name)
  _DetailAccumulator.append(forward_pass_id, gatherer_key, single_pass_data)
  _DetailAccumulator.reset()
  _DetailAccumulator.dump(output_mode)
  _StatAccumulator.__init__()
  _StatAccumulator.append(forward_pass_id, gatherer_key, single_pass_data)
  _StatAccumulator.reset()
  _StatAccumulator.dump(output_mode)
  _Buffer.init_new(item_shape, buffer_size, dtype, device)
  _Buffer.append(value)
  _Buffer.get_all()
  _Buffer.reset()
  _CircularBuffer.__init__(item_shape, buffer_size, dtype, device)
  _CircularBuffer.append(value)
  _CircularBuffer.get_all()
  _CircularBuffer.reset()
  _InfiniteBuffer.__init__(item_shape, dtype, device)
  _InfiniteBuffer.append(value)
  _InfiniteBuffer.get_all()
  _InfiniteBuffer.reset()
compute_gpu_physical_count(physical_count_of_whatever, num_gpu)
compute_utilization_rate(gpu_physical_count_of_batch)

# python/sglang/srt/eplb/expert_location.py
  ExpertLocationMetadata.num_layers()
  ExpertLocationMetadata.num_physical_experts()
  ExpertLocationMetadata.num_local_physical_experts()
  ExpertLocationMetadata.num_logical_experts()
  ExpertLocationMetadata.ep_size()
  ExpertLocationMetadata.__post_init__()
  ExpertLocationMetadata.init_trivial(server_args, model_config)
  ExpertLocationMetadata.init_by_mapping(server_args, model_config, physical_to_logical_map)
  ExpertLocationMetadata.init_by_eplb(server_args, model_config, logical_count)
  ExpertLocationMetadata.update(other, update_layer_ids)
  ExpertLocationMetadata.logical_to_all_physical(layer_id, logical_expert_id)
get_global_expert_location_metadata()
set_global_expert_location_metadata(value)
compute_logical_to_rank_dispatch_physical_map(logical_to_all_physical_map, num_gpus, num_physical_experts, ep_rank, seed)
  ModelConfigForExpertLocation.from_model_config(model_config)
compute_initial_expert_location_metadata(server_args, model_config)

# python/sglang/srt/eplb/expert_location_dispatch.py
  ExpertLocationDispatchInfo.init_new(cls, layer_id)
transform_select_experts_inputs(router_logits, correction_bias, info)
topk_ids_logical_to_physical(topk_ids, info)

# python/sglang/srt/eplb/expert_location_updater.py
  ExpertLocationUpdater.__init__()
  ExpertLocationUpdater.update(routed_experts_weights_of_layer, List[torch.Tensor]], new_expert_location_metadata, update_layer_ids, nnodes, rank)
create_temp_buffers(sample_tensors)
update_expert_weights_single_layer(routed_experts_weights, temp_buffers, old_physical_to_logical_map, new_physical_to_logical_map, num_local_physical_experts, num_gpu_per_node, rank, world_size, debug, log_metrics)
  _ChunkUtils.__init__()
  _ChunkUtils.chunk_value_from_element_value(element_value)
  _ChunkUtils.element_values_from_chunk_value(chunk_value)

# python/sglang/srt/function_call/base_format_detector.py
  BaseFormatDetector.__init__()
  BaseFormatDetector.parse_base_json(action, tools)
  BaseFormatDetector.detect_and_parse(text, tools)
  BaseFormatDetector.parse_streaming_increment(new_text, tools)
  BaseFormatDetector.has_tool_call(text)
  BaseFormatDetector.supports_structural_tag()
  BaseFormatDetector.structure_info()
  BaseFormatDetector.build_ebnf(tools)

# python/sglang/srt/function_call/deepseekv31_detector.py
  DeepSeekV31Detector.__init__()
  DeepSeekV31Detector.has_tool_call(text)
  DeepSeekV31Detector.detect_and_parse(text, tools)
  DeepSeekV31Detector.parse_streaming_increment(new_text, tools)
  DeepSeekV31Detector.structure_info()
  DeepSeekV31Detector.build_ebnf(tools)

# python/sglang/srt/function_call/deepseekv3_detector.py
  DeepSeekV3Detector.__init__()
  DeepSeekV3Detector.has_tool_call(text)
  DeepSeekV3Detector.detect_and_parse(text, tools)
  DeepSeekV3Detector.parse_streaming_increment(new_text, tools)
  DeepSeekV3Detector.structure_info()
  DeepSeekV3Detector.build_ebnf(tools)

# python/sglang/srt/function_call/ebnf_composer.py
  EBNFComposer.get_value_rule(prop, function_format, 'json', 'xml'])
  EBNFComposer.get_type_mapping(function_format)
  EBNFComposer.build_ebnf(tools, function_format, 'json', 'xml'], sequence_start_token, sequence_end_token, individual_call_start_token, individual_call_end_token, tool_call_separator, call_rule_fmt, key_value_rule_fmt, key_value_separator)

# python/sglang/srt/function_call/function_call_parser.py
  FunctionCallParser.__init__(tools, tool_call_parser)
  FunctionCallParser.has_tool_call(text)
  FunctionCallParser.parse_non_stream(full_text)
  FunctionCallParser.parse_stream_chunk(chunk_text)
  FunctionCallParser.get_structure_tag()
  FunctionCallParser.get_structure_constraint(tool_choice, Literal['auto', 'required']])
  FunctionCallParser.get_ebnf(tool_choice, Literal['required']])

# python/sglang/srt/function_call/glm4_moe_detector.py
get_argument_type(func_name, arg_key, defined_tools)
parse_arguments(json_value)
  Glm4MoeDetector.__init__()
  Glm4MoeDetector.has_tool_call(text)
  Glm4MoeDetector.detect_and_parse(text, tools)
  Glm4MoeDetector.parse_streaming_increment(new_text, tools)
  Glm4MoeDetector.supports_structural_tag()
  Glm4MoeDetector.structure_info()
  Glm4MoeDetector.build_ebnf(tools)

# python/sglang/srt/function_call/gpt_oss_detector.py
  GptOssDetector.__init__()
  GptOssDetector.has_tool_call(text)
  GptOssDetector.detect_and_parse(text, tools)
  GptOssDetector.parse_streaming_increment(new_text, tools)
  GptOssDetector.structure_info()
  GptOssDetector.build_ebnf(tools)

# python/sglang/srt/function_call/kimik2_detector.py
  KimiK2Detector.__init__()
  KimiK2Detector.has_tool_call(text)
  KimiK2Detector.detect_and_parse(text, tools)
  KimiK2Detector.parse_streaming_increment(new_text, tools)
  KimiK2Detector.structure_info()
  KimiK2Detector.build_ebnf(tools)

# python/sglang/srt/function_call/llama32_detector.py
  Llama32Detector.__init__()
  Llama32Detector.has_tool_call(text)
  Llama32Detector.detect_and_parse(text, tools)
  Llama32Detector.structure_info()
  Llama32Detector.build_ebnf(tools)

# python/sglang/srt/function_call/mistral_detector.py
  MistralDetector.__init__()
  MistralDetector.has_tool_call(text)
  MistralDetector.detect_and_parse(text, tools)
  MistralDetector.structure_info()
  MistralDetector.build_ebnf(tools)

# python/sglang/srt/function_call/pythonic_detector.py
  PythonicDetector.__init__()
  PythonicDetector.has_tool_call(text)
  PythonicDetector.detect_and_parse(text, tools)
  PythonicDetector.parse_streaming_increment(new_text, tools)
  PythonicDetector.supports_structural_tag()
  PythonicDetector.structure_info()
  PythonicDetector.build_ebnf(tools)

# python/sglang/srt/function_call/qwen25_detector.py
  Qwen25Detector.__init__()
  Qwen25Detector.has_tool_call(text)
  Qwen25Detector.detect_and_parse(text, tools)
  Qwen25Detector.parse_streaming_increment(new_text, tools)
  Qwen25Detector.structure_info()
  Qwen25Detector.build_ebnf(tools)

# python/sglang/srt/function_call/qwen3_coder_detector.py
  Qwen3CoderDetector.__init__()
  Qwen3CoderDetector.has_tool_call(text)
  Qwen3CoderDetector.detect_and_parse(text, tools)
  Qwen3CoderDetector.parse_streaming_increment(new_text, tools)
  Qwen3CoderDetector.supports_structural_tag()
  Qwen3CoderDetector.structure_info()
  Qwen3CoderDetector.build_ebnf(tools)

# python/sglang/srt/function_call/step3_detector.py
get_argument_type(func_name, arg_key, defined_tools)
parse_arguments(value)
  Step3Detector.__init__()
  Step3Detector.has_tool_call(text)
  Step3Detector.detect_and_parse(text, tools)
  Step3Detector.parse_streaming_increment(new_text, tools)
  Step3Detector.supports_structural_tag()
  Step3Detector.structure_info()
  Step3Detector.build_ebnf(tools)

# python/sglang/srt/harmony_parser.py
prefix_hold(text, tokens)
iter_tokens(text, start_pos)
  CanonicalStrategy.__init__()
  CanonicalStrategy.parse(text)
  TextStrategy.__init__()
  TextStrategy.set_buffer_context(buffer)
  TextStrategy.parse(text)
  HarmonyParser.__init__()
  HarmonyParser.parse(chunk)

# python/sglang/srt/hf_transformers_utils.py
download_from_hf(model_path, allow_patterns, list]])
get_hf_text_config(config)
get_config(model, trust_remote_code, revision, model_override_args)
get_generation_config(model, trust_remote_code, revision)
get_sparse_attention_config(model, sparse_attention_config_filename)
get_context_length(config)
get_tokenizer(tokenizer_name)
get_tokenizer_from_processor(processor)
get_processor(tokenizer_name)
attach_additional_stop_token_ids(tokenizer)
check_gguf_file(model, os.PathLike])

# python/sglang/srt/host_shared_memory.py
  HostSharedMemoryManager.__init__(base_name)
  HostSharedMemoryManager.malloc()
get_host_shared_memory_manager()
set_host_shared_memory_manager(instance)

# python/sglang/srt/jinja_template_utils.py
detect_jinja_template_content_format(chat_template)
process_content_for_template_format(msg_dict, content_format, image_data, video_data, audio_data, modalities)

# python/sglang/srt/layers/activation.py
  SiluAndMul.forward_native(x)
  SiluAndMul.forward_cuda(x)
  SiluAndMul.forward_cpu(x)
  SiluAndMul.forward_npu(x)
  GeluAndMul.__init__(approximate)
  GeluAndMul.forward_native(x)
  GeluAndMul.forward_cuda(x)
  NewGELU.forward_native(x)
  NewGELU.forward_cuda(x)
  ReLU2.forward(x)
  QuickGELU.forward_native(x)
  QuickGELU.forward_cuda(x)
  QuickGELU.forward_hip(x)
  ScaledActivation.__init__(act_module, intermediate_size, input_is_parallel, params_dtype)
  ScaledActivation.forward(x)
  ScaledActivation.weight_loader(param, loaded_weight)
get_act_fn(act_fn_name, quant_config, intermediate_size, input_is_parallel, params_dtype)
get_cross_encoder_activation_function(config)

# python/sglang/srt/layers/amx_utils.py
amx_process_weight_after_loading(weight)
dim_is_supported(weight)
  PackWeightMethod.__init__(weight_names, transpose_dims)
  PackWeightMethod.process_weights_after_loading(module)

# python/sglang/srt/layers/attention/aiter_backend.py
  AiterAttnBackend.__init__(model_runner, skip_prefill, kv_indptr_buf)
  AiterAttnBackend.init_forward_metadata(forward_batch)
  AiterAttnBackend.init_cuda_graph_state(max_bs, max_num_tokens, kv_indices_buf)
  AiterAttnBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info)
  AiterAttnBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, seq_lens_cpu)
  AiterAttnBackend.get_cuda_graph_seq_len_fill_value()
  AiterAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  AiterAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)
  AiterIndicesUpdaterPrefill.__init__(model_runner, attn_backend)
  AiterIndicesUpdaterPrefill.update(req_pool_indices, seq_lens, seq_lens_sum, prefix_lens, encoder_lens, spec_info)
  AiterIndicesUpdaterPrefill.update_single_wrapper(req_pool_indices, seq_lens, seq_lens_sum, prefix_lens, encoder_lens, spec_info)
  AiterMlaIndicesUpdaterPrefill.__init__(model_runner, attn_backend)
  AiterMlaIndicesUpdaterPrefill.update(req_pool_indices, kv_lens, kv_lens_sum, extend_lens, max_q_len, max_kv_len, spec_info)
  AiterMlaIndicesUpdaterPrefill.update_single_wrapper(req_pool_indices, kv_lens, kv_lens_sum, extend_lens, max_q_len, max_kv_len, spec_info)
  AiterMultiStepDraftBackend.__init__(model_runner, topk, speculative_num_steps)
  AiterMultiStepDraftBackend.common_template(forward_batch, kv_indices_buffer, call_fn)
  AiterMultiStepDraftBackend.init_forward_metadata(forward_batch)
  AiterMultiStepDraftBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  AiterMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(forward_batch)
  AiterMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(forward_batch, bs)

# python/sglang/srt/layers/attention/ascend_backend.py
  AscendAttnBackend.gen_attention_mask(max_seq_len, dtype)
  AscendAttnBackend.__init__(model_runner)
  AscendAttnBackend.init_forward_metadata(forward_batch)
  AscendAttnBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  AscendAttnBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info, EagleVerifyInput]])
  AscendAttnBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, EagleVerifyInput]], seq_lens_cpu)
  AscendAttnBackend.get_cuda_graph_seq_len_fill_value()
  AscendAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  AscendAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache, q_rope, k_rope)

# python/sglang/srt/layers/attention/base_attn_backend.py
  AttentionBackend.init_forward_metadata(forward_batch)
  AttentionBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  AttentionBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info, EagleVerifyInput]])
  AttentionBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, EagleVerifyInput]], seq_lens_cpu)
  AttentionBackend.get_cuda_graph_seq_len_fill_value()
  AttentionBackend.forward(q, k, v, layer, forward_batch, save_kv_cache)
  AttentionBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)
  AttentionBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  AttentionBackend.support_triton()

# python/sglang/srt/layers/attention/cutlass_mla_backend.py
  CutlassMLADecodeMetadata.__init__(workspace, block_kv_indices)
  CutlassMLABackend.__init__(model_runner, skip_prefill, kv_indptr_buf, kv_last_page_len_buf)
  CutlassMLABackend.init_forward_metadata(forward_batch)
  CutlassMLABackend.init_cuda_graph_state(max_bs, max_num_tokens, block_kv_indices)
  CutlassMLABackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info)
  CutlassMLABackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, seq_lens_cpu)
  CutlassMLABackend.get_cuda_graph_seq_len_fill_value()
  CutlassMLABackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache, q_rope, k_rope)

# python/sglang/srt/layers/attention/double_sparsity_backend.py
  DoubleSparseAttnBackend.__init__(model_runner)
  DoubleSparseAttnBackend.init_forward_metadata(forward_batch)
  DoubleSparseAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  DoubleSparseAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)

# python/sglang/srt/layers/attention/dual_chunk_flashattention_backend.py
  DualChunkFlashAttentionBackend.__init__(model_runner)
  DualChunkFlashAttentionBackend.get_sparse_attention_config(layer_idx)
  DualChunkFlashAttentionBackend.init_forward_metadata(forward_batch)
  DualChunkFlashAttentionBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  DualChunkFlashAttentionBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)
  DualChunkFlashAttentionBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  DualChunkFlashAttentionBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info)
  DualChunkFlashAttentionBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, seq_lens_cpu, out_cache_loc)
  DualChunkFlashAttentionBackend.get_cuda_graph_seq_len_fill_value()

# python/sglang/srt/layers/attention/flashattention_backend.py
make_local_attention_virtual_batches(attn_chunk_size, query_start_loc_np, seq_lens_np, block_table, page_size)
cdiv(a, b)
merge_state_v2_wrapper(o, s_a, o_exp, s_b)
  FlashAttentionBackend.__init__(model_runner, skip_prefill, speculative_step_id, topk, speculative_num_steps)
  FlashAttentionBackend.init_forward_metadata(forward_batch)
  FlashAttentionBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache, q_rope, k_rope, sinks)
  FlashAttentionBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache, q_rope, k_rope, sinks)
  FlashAttentionBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  FlashAttentionBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info, EagleVerifyInput]])
  FlashAttentionBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, EagleVerifyInput]], seq_lens_cpu, out_cache_loc)
  FlashAttentionBackend.get_cuda_graph_seq_len_fill_value()
prepare_swa_spec_page_table_triton(page_table_dst, page_table_a, page_table_b, seq_len_a, seq_len_b, speculative_num_draft_tokens)
  FlashAttentionMultiStepBackend.__init__(model_runner, topk, speculative_num_steps)
  FlashAttentionMultiStepBackend.init_forward_metadata(forward_batch)
  FlashAttentionMultiStepBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  FlashAttentionMultiStepBackend.init_forward_metadata_capture_cuda_graph(forward_batch)
  FlashAttentionMultiStepBackend.init_forward_metadata_replay_cuda_graph(forward_batch, bs)
normal_decode_set_metadata(cache_seqlens_int32, cu_seqlens_k, page_table, req_to_token, req_pool_indices, strided_indices, max_seq_pages, seq_lens, seq_len_delta, page_size)

# python/sglang/srt/layers/attention/flashinfer_backend.py
  FlashInferAttnBackend.__init__(model_runner, skip_prefill, kv_indptr_buf, kv_last_page_len_buf)
  FlashInferAttnBackend.init_forward_metadata(forward_batch)
  FlashInferAttnBackend.init_cuda_graph_state(max_bs, max_num_tokens, kv_indices_buf)
  FlashInferAttnBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info, EagleVerifyInput]])
  FlashInferAttnBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, EagleVerifyInput]], seq_lens_cpu)
  FlashInferAttnBackend.get_cuda_graph_seq_len_fill_value()
  FlashInferAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  FlashInferAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)
  FlashInferIndicesUpdaterDecode.__init__(model_runner, attn_backend)
  FlashInferIndicesUpdaterDecode.update(req_pool_indices, seq_lens, seq_lens_cpu, seq_lens_sum, decode_wrappers, encoder_lens, spec_info, EagleVerifyInput]])
  FlashInferIndicesUpdaterDecode.update_single_wrapper(req_pool_indices, seq_lens, seq_lens_cpu, seq_lens_sum, decode_wrappers, encoder_lens, spec_info, EagleVerifyInput]])
  FlashInferIndicesUpdaterDecode.update_sliding_window(req_pool_indices, seq_lens, seq_lens_cpu, seq_lens_sum, decode_wrappers, encoder_lens, spec_info, EagleVerifyInput]])
  FlashInferIndicesUpdaterDecode.update_cross_attention(req_pool_indices, seq_lens, seq_lens_cpu, seq_lens_sum, decode_wrappers, encoder_lens, spec_info, EagleVerifyInput]])
  FlashInferIndicesUpdaterDecode.call_begin_forward(wrapper, req_pool_indices, paged_kernel_lens, paged_kernel_lens_sum, kv_indptr, kv_start_idx, spec_info, EagleVerifyInput]], seq_lens_cpu, use_sliding_window_kv_pool)
  FlashInferIndicesUpdaterPrefill.__init__(model_runner, attn_backend)
  FlashInferIndicesUpdaterPrefill.update(req_pool_indices, seq_lens, seq_lens_cpu, seq_lens_sum, prefix_lens, prefill_wrappers, use_ragged, encoder_lens, spec_info, EagleVerifyInput]])
  FlashInferIndicesUpdaterPrefill.update_single_wrapper(req_pool_indices, seq_lens, seq_lens_cpu, seq_lens_sum, prefix_lens, prefill_wrappers, use_ragged, encoder_lens, spec_info, EagleVerifyInput]])
  FlashInferIndicesUpdaterPrefill.update_sliding_window(req_pool_indices, seq_lens, seq_lens_cpu, seq_lens_sum, prefix_lens, prefill_wrappers, use_ragged, encoder_lens, spec_info, EagleVerifyInput]])
  FlashInferIndicesUpdaterPrefill.update_cross_attention(req_pool_indices, seq_lens, seq_lens_cpu, seq_lens_sum, prefix_lens, prefill_wrappers, use_ragged, encoder_lens, spec_info, EagleVerifyInput]])
  FlashInferIndicesUpdaterPrefill.call_begin_forward(wrapper_ragged, wrapper_paged, req_pool_indices, paged_kernel_lens, paged_kernel_lens_sum, seq_lens, prefix_lens, kv_start_idx, kv_indptr, qo_indptr, use_ragged, spec_info, EagleVerifyInput]], use_sliding_window_kv_pool)
  FlashInferMultiStepDraftBackend.__init__(model_runner, topk, speculative_num_steps)
  FlashInferMultiStepDraftBackend.common_template(forward_batch, kv_indices_buffer, call_fn)
  FlashInferMultiStepDraftBackend.init_forward_metadata(forward_batch)
  FlashInferMultiStepDraftBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  FlashInferMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(forward_batch)
  FlashInferMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(forward_batch, bs)
should_use_tensor_core(kv_cache_dtype, num_attention_heads, num_kv_heads)
fast_decode_plan(indptr, indices, last_page_len, num_qo_heads, num_kv_heads, head_dim, page_size, pos_encoding_mode, window_left, logits_soft_cap, q_data_type, torch.dtype]], kv_data_type, torch.dtype]], data_type, torch.dtype]], sm_scale, rope_scale, rope_theta, non_blocking)

# python/sglang/srt/layers/attention/flashinfer_mla_backend.py
  FlashInferMhaChunkKVRunner.__init__(model_runner, attn_backend)
  FlashInferMhaChunkKVRunner.update_prefix_chunks(num_prefix_chunks)
  FlashInferMhaChunkKVRunner.update_wrapper(forward_batch)
  FlashInferMhaChunkKVRunner.forward(q, k, v, layer, forward_batch)
  FlashInferMLAAttnBackend.__init__(model_runner, skip_prefill, kv_indptr_buf, q_indptr_decode_buf)
  FlashInferMLAAttnBackend.init_forward_metadata(forward_batch)
  FlashInferMLAAttnBackend.init_cuda_graph_state(max_bs, max_num_tokens, kv_indices_buf)
  FlashInferMLAAttnBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info)
  FlashInferMLAAttnBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, seq_lens_cpu)
  FlashInferMLAAttnBackend.get_cuda_graph_seq_len_fill_value()
  FlashInferMLAAttnBackend.init_mha_chunk_metadata(forward_batch)
  FlashInferMLAAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache, q_rope, k_rope)
  FlashInferMLAAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache, q_rope, k_rope)
  FlashInferMLAIndicesUpdaterDecode.__init__(model_runner, attn_backend)
  FlashInferMLAIndicesUpdaterDecode.update(req_pool_indices, seq_lens, seq_lens_sum, decode_wrapper, init_metadata_replay, spec_info, EagleVerifyInput]])
  FlashInferMLAIndicesUpdaterDecode.call_begin_forward(wrapper, req_pool_indices, paged_kernel_lens, paged_kernel_lens_sum, q_indptr, kv_indptr, init_metadata_replay, spec_info, EagleVerifyInput]])
  FlashInferMLAIndicesUpdaterPrefill.__init__(model_runner, attn_backend)
  FlashInferMLAIndicesUpdaterPrefill.update(req_pool_indices, seq_lens, seq_lens_sum, prefix_lens, prefill_wrapper_paged, use_ragged, spec_info, EagleVerifyInput]])
  FlashInferMLAIndicesUpdaterPrefill.call_begin_forward(wrapper_ragged, wrapper_paged, req_pool_indices, paged_kernel_lens, paged_kernel_lens_sum, seq_lens, prefix_lens, kv_indptr, qo_indptr, use_ragged, spec_info, EagleVerifyInput]])
  FlashInferMLAMultiStepDraftBackend.__init__(model_runner, topk, speculative_num_steps)
  FlashInferMLAMultiStepDraftBackend.common_template(forward_batch, kv_indices_buffer, call_fn)
  FlashInferMLAMultiStepDraftBackend.init_forward_metadata(forward_batch)
  FlashInferMLAMultiStepDraftBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  FlashInferMLAMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(forward_batch)
  FlashInferMLAMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(forward_batch, bs)
fast_mla_decode_plan(qo_indptr_cpu, kv_indptr_cpu, kv_indices, kv_len_arr_cpu, num_heads, head_dim_ckv, head_dim_kpe, page_size, causal, sm_scale, q_data_type, kv_data_type)

# python/sglang/srt/layers/attention/flashmla_backend.py
  FlashMLADecodeMetadata.__init__(flashmla_metadata, torch.Tensor]], num_splits, block_kv_indices)
  FlashMLABackend.__init__(model_runner, skip_prefill, kv_indptr_buf, kv_last_page_len_buf)
  FlashMLABackend.init_forward_metadata(forward_batch)
  FlashMLABackend.init_cuda_graph_state(max_bs, max_num_tokens, block_kv_indices)
  FlashMLABackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info)
  FlashMLABackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, seq_lens_cpu)
  FlashMLABackend.get_cuda_graph_seq_len_fill_value()
  FlashMLABackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)
  FlashMLABackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  FlashMLAMultiStepDraftBackend.__init__(model_runner, topk, speculative_num_steps)
  FlashMLAMultiStepDraftBackend.common_template(forward_batch, call_fn)
  FlashMLAMultiStepDraftBackend.init_forward_metadata(forward_batch)
  FlashMLAMultiStepDraftBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  FlashMLAMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(forward_batch)
  FlashMLAMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(forward_batch, bs)

# python/sglang/srt/layers/attention/hybrid_attn_backend.py
  HybridAttnBackend.__init__(model_runner, prefill_backend, decode_backend)
  HybridAttnBackend.init_forward_metadata(forward_batch)
  HybridAttnBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  HybridAttnBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info, EagleVerifyInput]])
  HybridAttnBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, EagleVerifyInput]], seq_lens_cpu)
  HybridAttnBackend.get_cuda_graph_seq_len_fill_value()
  HybridAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)
  HybridAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)

# python/sglang/srt/layers/attention/intel_amx_backend.py
  IntelAMXAttnBackend.__init__(model_runner)
  IntelAMXAttnBackend.init_forward_metadata(forward_batch)
  IntelAMXAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  IntelAMXAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)
  IntelAMXAttnBackend.support_triton()

# python/sglang/srt/layers/attention/merge_state.py
merge_state(prefix_output, prefix_lse, suffix_output, suffix_lse, output, output_lse)

# python/sglang/srt/layers/attention/tbo_backend.py
  TboAttnBackend.__init__(primary, children)
  TboAttnBackend.init_new(cls, creator, AttentionBackend])
  TboAttnBackend.init_forward_metadata(forward_batch)
  TboAttnBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  TboAttnBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info, EagleVerifyInput]])
  TboAttnBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, EagleVerifyInput]], seq_lens_cpu)
  TboAttnBackend.get_cuda_graph_seq_len_fill_value()
  TboAttnBackend.forward_extend()
  TboAttnBackend.forward_decode()

# python/sglang/srt/layers/attention/torch_native_backend.py
  TorchNativeAttnBackend.__init__(model_runner)
  TorchNativeAttnBackend.init_forward_metadata(forward_batch)
  TorchNativeAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  TorchNativeAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)
  TorchNativeAttnBackend.support_triton()

# python/sglang/srt/layers/attention/triton_backend.py
logit_capping_mod(logit_capping_method, logit_cap)
  TritonAttnBackend.__init__(model_runner, skip_prefill, kv_indptr_buf)
  TritonAttnBackend.get_num_kv_splits(num_kv_splits, seq_lens)
  TritonAttnBackend.init_forward_metadata(forward_batch)
  TritonAttnBackend.init_cuda_graph_state(max_bs, max_num_tokens, kv_indices_buf)
  TritonAttnBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info, EagleVerifyInput]])
  TritonAttnBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, EagleVerifyInput]], seq_lens_cpu)
  TritonAttnBackend.get_cuda_graph_seq_len_fill_value()
  TritonAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache, sinks)
  TritonAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache, sinks)
  TritonMultiStepDraftBackend.__init__(model_runner, topk, speculative_num_steps)
  TritonMultiStepDraftBackend.common_template(forward_batch, kv_indices_buffer, call_fn)
  TritonMultiStepDraftBackend.init_forward_metadata(forward_batch)
  TritonMultiStepDraftBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  TritonMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(forward_batch)
  TritonMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(forward_batch, bs)
get_num_kv_splits_triton(num_kv_splits_ptr, seq_lens_ptr, num_seq, num_group, num_head, num_kv_head, max_kv_splits, device_core_count, MAX_NUM_SEQ)
update_sliding_window_buffer(window_kv_indptr, req_to_token, sliding_window_size, seq_lens, req_pool_indices, bs, device, token_to_kv_pool_allocator)
update_sliding_window_buffer_cuda_graph(window_kv_indptr, window_kv_indices, req_to_token, sliding_window_size, seq_lens, req_pool_indices, bs, token_to_kv_pool_allocator)

# python/sglang/srt/layers/attention/triton_ops/decode_attention.py
tanh(x)
decode_attention_fwd_normal(q, k_buffer, v_buffer, o, kv_indptr, kv_indices, attn_logits, attn_lse, num_kv_splits, max_kv_splits, sm_scale, logit_cap, sinks, xai_temperature_len)
decode_attention_fwd_grouped(q, k_buffer, v_buffer, o, kv_indptr, kv_indices, attn_logits, attn_lse, num_kv_splits, max_kv_splits, sm_scale, logit_cap, sinks, xai_temperature_len)
decode_attention_fwd(q, k_buffer, v_buffer, o, kv_indptr, kv_indices, attn_logits, attn_lse, num_kv_splits, max_kv_splits, sm_scale, logit_cap, sinks, xai_temperature_len)

# python/sglang/srt/layers/attention/triton_ops/double_sparsity_attention.py
tanh(x)
flash_decode_stage1(q, k, v, Req_to_tokens, B_req_idx, B_Seqlen, max_len_in_batch, mid_out, mid_out_logsumexp, block_seq)
flash_decode_stage2(mid_out, mid_out_logexpsum, B_Seqlen, O, block_seq)
flash_decode_attention_fwd(q, k_buffer, v_buffer, o, req_to_token, b_req_idx, b_start_loc, b_seq_len, attn_logits, max_len_in_batch, sm_scale, logit_cap)
sparse_flash_decode_stage1(q_label, k_label_buffer, att_out, Req_to_tokens, B_Seqlen, max_len_in_batch, sm_scale, logit_cap)
sparse_flash_decode_stage2(q, k, v, Req_to_tokens, Topk_token_indices, heavy_token_num, mid_out, mid_out_logsumexp, block_seq, sm_scale)
sparse_flash_decode_stage3(Seqlen, mid_out, mid_out_logexpsum, O, block_seq)
flash_decode_sparse_attention_fwd(q, k_buffer, v_buffer, o, q_label, k_label_buffer, req_to_token, b_seq_len, max_len_in_batch, sm_scale, logit_cap, heavy_token_num, att_out_approx, mid_out, mid_o_logexpsum, BLOCK_SEQ)
extend_attention_fwd(q_extend, k_extend, v_extend, o_extend, k_buffer, v_buffer, req_to_tokens, b_req_idx, b_seq_len, b_seq_len_extend, b_start_loc_extend, max_len_extend, sm_scale, logit_cap)

# python/sglang/srt/layers/attention/triton_ops/extend_attention.py
tanh(x)
extend_attention_fwd(q_extend, k_extend, v_extend, o_extend, k_buffer, v_buffer, qo_indptr, kv_indptr, kv_indices, custom_mask, is_causal, mask_indptr, max_len_extend, sm_scale, logit_cap, skip_prefix_custom_mask, sliding_window_size, sinks, window_kv_offsets, xai_temperature_len)
redundant_attention(q_extend, o_extend, k_buffer, v_buffer, b_req_idx, b_start_loc, b_seq_len, b_seq_len_prefix, max_len_in_batch)

# python/sglang/srt/layers/attention/triton_ops/merge_state.py
merge_state_kernel(output, output_lse, prefix_output, prefix_lse, suffix_output, suffix_lse, HEAD_SIZE, PADDED_HEAD_SIZE, OUTPUT_LSE)
merge_state_triton(prefix_output, prefix_lse, suffix_output, suffix_lse, output, output_lse)

# python/sglang/srt/layers/attention/triton_ops/prefill_attention.py
context_attention_fwd(q, k, v, o, b_start_loc, b_seq_len, max_input_len, is_causal)

# python/sglang/srt/layers/attention/triton_ops/rocm_mla_decode_rope.py
is_hip()
tanh(x)
decode_attention_fwd_grouped_rope(q, k_buffer, v_buffer, o, kv_indptr, kv_indices, k_pe_tokens, kv_lora_rank, rotary_dim, cos_sin_cache, positions, attn_logits, num_kv_splits, sm_scale, logit_cap, use_rope, is_neox_style)

# python/sglang/srt/layers/attention/trtllm_mha_backend.py
  TRTLLMHAAttnBackend.__init__(model_runner, skip_prefill, kv_indptr_buf, kv_last_page_len_buf, speculative_step_id)
  TRTLLMHAAttnBackend.init_cuda_graph_state(max_bs, max_num_tokens, kv_indices_buf)
  TRTLLMHAAttnBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info)
  TRTLLMHAAttnBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, seq_lens_cpu)
  TRTLLMHAAttnBackend.get_cuda_graph_seq_len_fill_value()
  TRTLLMHAAttnBackend.init_forward_metadata(forward_batch)
  TRTLLMHAAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)
  TRTLLMHAAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  TRTLLMHAAttnMultiStepDraftBackend.__init__(model_runner, topk, speculative_num_steps)
  TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata(forward_batch)
  TRTLLMHAAttnMultiStepDraftBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(forward_batch)
  TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(forward_batch, bs)

# python/sglang/srt/layers/attention/trtllm_mla_backend.py
  TRTLLMMLABackend.__init__(model_runner, skip_prefill, kv_indptr_buf, q_indptr_decode_buf)
  TRTLLMMLABackend.init_cuda_graph_state(max_bs, max_num_tokens, kv_indices_buf)
  TRTLLMMLABackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info)
  TRTLLMMLABackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, seq_lens_cpu)
  TRTLLMMLABackend.get_cuda_graph_seq_len_fill_value()
  TRTLLMMLABackend.init_forward_metadata(forward_batch)
  TRTLLMMLABackend.quantize_and_rope_for_fp8(q_nope, q_rope, k_nope, k_rope, forward_batch, cos_sin_cache, is_neox)
  TRTLLMMLABackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache, q_rope, k_rope, cos_sin_cache, is_neox)
  TRTLLMMLAMultiStepDraftBackend.__init__(model_runner, topk, speculative_num_steps)

# python/sglang/srt/layers/attention/utils.py
create_flashinfer_kv_indices_triton(req_to_token_ptr, req_pool_indices_ptr, page_kernel_lens_ptr, kv_indptr, kv_start_idx, kv_indices_ptr, req_to_token_ptr_stride)
create_flashmla_kv_indices_triton(req_to_token_ptr, req_pool_indices_ptr, page_kernel_lens_ptr, kv_start_idx, kv_indices_ptr, req_to_token_ptr_stride, kv_indices_ptr_stride, NUM_PAGE_PER_BLOCK, PAGED_SIZE)

# python/sglang/srt/layers/attention/vision.py
  SingletonCache.set_data(value)
  SingletonCache.get_data()
  SingletonCache.empty()
  VisionSdpaAttention.__init__(head_dim, num_heads, num_kv_heads, dropout, flatten_batch, softmax_in_single_precision)
  VisionSdpaAttention.generate_patch_attention_mask(s, cu_seqlens, flatten_batch)
  VisionSdpaAttention.forward(q, k, v, bsz, cu_seqlens, attention_mask)
  VisionTritonAttention.__init__()
  VisionTritonAttention.forward(q, k, v, cu_seqlens, bsz, seq_len)
  VisionFlash3Attention.__init__()
  VisionFlash3Attention.forward(q, k, v, cu_seqlens, torch.Tensor]], bsz, seq_len)
  VisionAttention.__init__(embed_dim, num_heads, projection_size, use_qkv_parallel, qkv_backend, quant_config, dropout, softmax_in_single_precision, flatten_batch, prefix, proj_bias, num_dummy_heads, qkv_bias, qk_normalization, layer_norm_eps, customized_position_embedding_applier, torch.Tensor, Any, Any], Tuple[torch.Tensor, torch.Tensor]])
  VisionAttention.forward(x, cu_seqlens, position_embeddings, torch.Tensor]], attention_mask)

# python/sglang/srt/layers/attention/vision_utils.py
update_vit_attn_dummy_heads_config(config)
pad_vit_attn_dummy_heads(config, name, loaded_weight)

# python/sglang/srt/layers/attention/wave_backend.py
get_num_kv_splits_triton(num_kv_splits_ptr, seq_lens_ptr, num_seq, num_group, num_head, num_kv_head, max_kv_splits, device_core_count, MAX_NUM_SEQ)
  WaveAttnBackend.__init__(model_runner, skip_prefill, kv_indptr_buf)
  WaveAttnBackend.get_num_kv_splits(num_kv_splits, seq_lens)
  WaveAttnBackend.init_forward_metadata(forward_batch)
  WaveAttnBackend.init_cuda_graph_state(max_bs, max_num_tokens, kv_indices_buf)
  WaveAttnBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info, EagleVerifyInput]])
  WaveAttnBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, EagleVerifyInput]], seq_lens_cpu)
  WaveAttnBackend.get_cuda_graph_seq_len_fill_value()
  WaveAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  WaveAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)

# python/sglang/srt/layers/attention/wave_ops/decode_attention.py
get_wave_kernel(shape, max_kv_splits, input_dtype, output_dtype, logit_cap)
decode_attention_intermediate_arrays_shapes(num_seqs, head_size_kv, num_query_heads, max_kv_splits)
decode_attention_wave(q, k_buffer, v_buffer, o, b_req_idx, req_to_token, attn_logits, attn_logits_max, num_kv_splits, max_kv_splits, sm_scale, logit_cap)
decode_attention_fwd(q, k_buffer, v_buffer, o, b_req_idx, req_to_token, attn_logits, attn_logits_max, num_kv_splits, max_kv_splits, sm_scale, logit_cap)

# python/sglang/srt/layers/attention/wave_ops/extend_attention.py
get_wave_kernel(shape, q_shape, k_shape, v_shape, k_cache_shape, v_cache_shape, o_shape, input_dtype, output_dtype, size_dtype, is_causal, logit_cap, layer_scaling)
extend_attention_wave(q_extend, k_extend, v_extend, k_buffer, v_buffer, qo_indptr, kv_indptr, kv_indices, custom_mask, mask_indptr, max_seq_len, output, is_causal, layer_scaling, logit_cap)

# python/sglang/srt/layers/attention/wave_ops/prefill_attention.py
prefill_attention_wave(q, k, v, o, b_start_loc, b_seq_len, max_seq_len, is_causal)

# python/sglang/srt/layers/communicator.py
  ScatterMode.model_input_output()
  _LayerModeComputationContext.previous_layer()
  LayerScatterModes.init_new(cls)
enable_moe_dense_fully_dp()
  LayerCommunicator.__init__(layer_scatter_modes, input_layernorm, post_attention_layernorm, allow_reduce_scatter, is_last_layer)
  LayerCommunicator.prepare_attn(hidden_states, residual, forward_batch)
  LayerCommunicator.prepare_mlp(hidden_states, residual, forward_batch)
  LayerCommunicator.postprocess_layer(hidden_states, residual, forward_batch)
  LayerCommunicator.should_use_reduce_scatter(forward_batch)
  LayerCommunicator.should_fuse_mlp_allreduce_with_next_layer(forward_batch)
  CommunicateContext.is_same_group_size(a, b)
  CommunicateContext.init_new(cls)
  CommunicateSimpleFn.get_fn(input_mode, output_mode, context)
  CommunicateWithAllReduceAndLayerNormFn.get_fn(hidden_states_input_mode, residual_input_mode, hidden_states_output_mode, residual_output_mode, context)
  CommunicateSummableTensorPairFn.execute(cls, hidden_states_input_mode, residual_input_mode, output_mode, context)
  CommunicateSummableTensorPairFn.get_fn(hidden_states_input_mode, residual_input_mode, output_mode, context)

# python/sglang/srt/layers/dp_attention.py
  DpPaddingMode.is_max_len()
  DpPaddingMode.is_sum_len()
  DpPaddingMode.get_dp_padding_mode(cls, global_num_tokens)
  DpPaddingMode.get_default_mode_in_cuda_graph(cls)
  _DpGatheredBufferWrapper.set_metadata(cls, hidden_size, dtype, device)
  _DpGatheredBufferWrapper.set_dp_buffer_len(cls, global_dp_buffer_len, local_dp_buffer_len, global_num_tokens)
  _DpGatheredBufferWrapper.get_global_dp_buffer(cls)
  _DpGatheredBufferWrapper.get_local_dp_buffer(cls)
  _DpGatheredBufferWrapper.get_global_dp_buffer_len(cls)
  _DpGatheredBufferWrapper.get_local_dp_buffer_len(cls)
  _DpGatheredBufferWrapper.get_dp_global_num_tokens(cls)
set_dp_buffer_len(global_dp_buffer_len, local_dp_buffer_len, global_num_tokens)
get_global_dp_buffer()
get_local_dp_buffer()
get_global_dp_buffer_len()
get_local_dp_buffer_len()
get_dp_global_num_tokens()
compute_dp_attention_world_info(enable_dp_attention, tp_rank, tp_size, dp_size)
compute_dp_attention_local_info(enable_dp_attention, tp_rank, tp_size, dp_size, moe_dense_tp_size)
initialize_dp_attention(server_args, model_config)
is_dp_attention_enabled()
get_attention_tp_group()
get_attention_tp_rank()
get_attention_tp_size()
get_attention_dp_rank()
get_attention_dp_size()
get_local_attention_dp_rank()
get_local_attention_dp_size()
disable_dp_size()
get_dp_local_info(forward_batch)
memcpy_triton_kernel(dst_ptr, src_ptr, offset_ptr, sz_ptr, offset_src, chunk_size, BLOCK_SIZE)
prod(x)
memcpy_triton(dst, src, dim, offset, sz, offset_src)
dp_gather_partial(global_tokens, local_tokens, forward_batch)
dp_gather_replicate(global_tokens, local_tokens, forward_batch)
dp_scatter(local_tokens, global_tokens, forward_batch)
dp_reduce_scatter_tensor(output, input)
attn_tp_reduce_scatter_tensor(output, input)
attn_tp_all_gather_into_tensor(output, input)
attn_tp_all_gather(output_list, input)

# python/sglang/srt/layers/elementwise.py
fused_softcap_kernel(output_ptr, input_ptr, n_ele, softcap_const, BLOCK_SIZE)
fused_softcap(x, softcap_const, autotune)
  Softcap.__init__(softcap_const)
  Softcap.__call__()
  Softcap.forward(x)
  Softcap.forward_native(x)
  Softcap.forward_cuda(x, autotune)
fused_dual_residual_rmsnorm_kernel(output_ptr, mid_ptr, activ_ptr, residual_ptr, weight1_ptr, weight2_ptr, eps, hidden_dim, BLOCK_SIZE)
fused_dual_residual_rmsnorm(x, residual, weight1, weight2, eps, autotune)
fused_rmsnorm_kernel(output_ptr, activ_ptr, weight_ptr, eps, hidden_dim, BLOCK_SIZE)
fused_rmsnorm(x, weight, eps, autotune, inplace)
  FusedDualResidualRMSNorm.__init__(rmsnorm1, rmsnorm2)
  FusedDualResidualRMSNorm.__call__()
  FusedDualResidualRMSNorm.forward(x, residual)
  FusedDualResidualRMSNorm.forward_cuda(x, residual, autotune)
  FusedDualResidualRMSNorm.forward_flashinfer(x, residual)
  FusedDualResidualRMSNorm.forward_native(x, residual)
experts_combine_kernel(out_hidden_states, moe_hidden_states, mlp_hidden_states, combine_k, hidden_dim, BLOCK_SIZE)
experts_combine_triton(moe_hidden_states, mlp_hidden_states, output_buffer)
gelu_and_mul_kernel(out_hidden_states_ptr, out_scales_ptr, hidden_states_ptr, quant_max, static_scale, hidden_dim, BLOCK_SIZE)
gelu_and_mul_triton(hidden_states, scales, quantize, out)
silu_and_mul_kernel(out_hidden_states_ptr, out_scales_ptr, hidden_states_ptr, quant_max, static_scale, hidden_dim, BLOCK_SIZE)
silu_and_mul_triton(hidden_states, scales, quantize, out)

# python/sglang/srt/layers/flashinfer_comm_fusion.py
  FlashInferWorkspaceManager.__init__()
  FlashInferWorkspaceManager.initialize(world_size, rank, max_token_num, hidden_dim, group, use_fp32_lamport)
  FlashInferWorkspaceManager.cleanup()
ensure_workspace_initialized(max_token_num, hidden_dim, use_fp32_lamport)
flashinfer_allreduce_residual_rmsnorm(input_tensor, residual, weight, eps, max_token_num, use_oneshot, trigger_completion_at_end, fp32_acc)
fake_flashinfer_allreduce_residual_rmsnorm(input_tensor, residual, weight, eps, max_token_num, use_oneshot, trigger_completion_at_end, fp32_acc)
cleanup_flashinfer_workspace()

# python/sglang/srt/layers/layernorm.py
  RMSNorm.__init__(hidden_size, eps, var_hidden_size)
  RMSNorm.forward_cuda(x, residual)
  RMSNorm.forward_npu(x, residual)
  RMSNorm.forward_aiter(x, residual)
  RMSNorm.forward_hip(x, residual)
  RMSNorm.forward_native(x, residual)
  RMSNorm.forward_cpu(x, residual)
  RMSNorm.forward_with_allreduce_fusion(x, residual)
  GemmaRMSNorm.__init__(hidden_size, eps)
  GemmaRMSNorm.forward_native(x, residual)
  GemmaRMSNorm.forward_cuda(x, residual)
  Gemma3RMSNorm.__init__(dim, eps)
  Gemma3RMSNorm.forward(x)
  Gemma3RMSNorm.extra_repr()

# python/sglang/srt/layers/linear.py
adjust_marlin_shard(param, shard_size, shard_offset)
adjust_bitsandbytes_4bit_shard(param, shard_offsets, Tuple[int, int]], loaded_shard_id)
adjust_scalar_to_fused_array(param, loaded_weight, shard_id)
adjust_shard_offsets(shard_offsets, loaded_weight, dim)
  LinearBase.__init__(input_size, output_size, skip_bias_add, params_dtype, quant_config, prefix)
  LinearBase.forward(x)
  ReplicatedLinear.__init__(input_size, output_size, bias, skip_bias_add, params_dtype, quant_config, prefix)
  ReplicatedLinear.weight_loader(param, loaded_weight)
  ReplicatedLinear.forward(x)
  ReplicatedLinear.extra_repr()
  ColumnParallelLinear.__init__(input_size, output_size, bias, gather_output, skip_bias_add, params_dtype, quant_config, output_sizes, prefix, tp_rank, tp_size, use_presharded_weights)
  ColumnParallelLinear.weight_loader(param, loaded_weight)
  ColumnParallelLinear.weight_loader_v2(param, loaded_weight)
  ColumnParallelLinear.forward(input_)
  ColumnParallelLinear.extra_repr()
  MergedColumnParallelLinear.__init__(input_size, output_sizes, bias, gather_output, skip_bias_add, params_dtype, quant_config, prefix, tp_rank, tp_size, use_presharded_weights)
  MergedColumnParallelLinear.weight_loader(param, loaded_weight, loaded_shard_id)
  MergedColumnParallelLinear.weight_loader_v2(param, loaded_weight, loaded_shard_id)
  QKVParallelLinear.__init__(hidden_size, head_size, total_num_heads, total_num_kv_heads, bias, skip_bias_add, params_dtype, quant_config, prefix, tp_rank, tp_size, load_presharded_attn)
  QKVParallelLinear.weight_loader_v2(param, loaded_weight, loaded_shard_id)
  QKVParallelLinear.weight_loader(param, loaded_weight, loaded_shard_id)
  RowParallelLinear.__init__(input_size, output_size, bias, input_is_parallel, skip_bias_add, params_dtype, reduce_results, quant_config, prefix, tp_rank, tp_size, use_presharded_weights)
  RowParallelLinear.weight_loader(param, loaded_weight)
  RowParallelLinear.weight_loader_v2(param, loaded_weight)
  RowParallelLinear.forward(input_, skip_all_reduce)
  RowParallelLinear.extra_repr()

# python/sglang/srt/layers/logits_processor.py
  LogitsMetadata.from_forward_batch(cls, forward_batch)
  LogitsMetadata.compute_dp_attention_metadata()
  LogitsProcessor.__init__(config, skip_all_gather, logit_scale)
  LogitsProcessor.forward(input_ids, hidden_states, lm_head, logits_metadata, ForwardBatch], aux_hidden_states)
  LogitsProcessor.get_top_logprobs(all_logprobs, logits_metadata)
  LogitsProcessor.get_token_ids_logprobs(all_logprobs, logits_metadata)
  LogitsProcessor.compute_temp_top_p_normalized_logprobs(last_logits, logits_metadata)
fused_softcap_kernel(full_logits_ptr, softcapping_value, n_elements, BLOCK_SIZE)
fused_softcap(full_logits, final_logit_softcapping)

# python/sglang/srt/layers/moe/cutlass_moe.py
cutlass_fused_experts_fp8(a, w1_q, w2_q, w1_scale, w2_scale, topk_weights, topk_ids, a1_strides, c1_strides, a2_strides, c2_strides, workspace, a_ptrs, b_ptrs, out_ptrs, a_scales_ptrs, b_scales_ptrs, expert_offsets, problem_sizes1, problem_sizes2, use_fp8_blockscale)
cutlass_moe_fp4(a, a1_gscale, w1_fp4, w1_blockscale, w1_alphas, a2_gscale, w2_fp4, w2_blockscale, w2_alphas, topk_weights, topk_ids, params, apply_router_weight_on_input)

# python/sglang/srt/layers/moe/cutlass_moe_params.py
  CutlassMoEParams.__init__(cutlass_moe_type, device, num_experts, intermediate_size_per_partition, hidden_size)
  CutlassMoEParams.to_gemm1_args()
  CutlassMoEParams.to_gemm2_args()

# python/sglang/srt/layers/moe/cutlass_w4a8_moe.py
cutlass_w4a8_moe(start_expert_id, end_expert_id, total_num_experts, a, w1_q, w2_q, w1_scale, w2_scale, topk_weights, topk_ids_, local_topk_ids, a_strides1, b_strides1, c_strides1, a_strides2, b_strides2, c_strides2, s_strides13, s_strides2, expert_offsets, problem_sizes1, problem_sizes2, a1_scale, a2_scale, apply_router_weight_on_input)

# python/sglang/srt/layers/moe/ep_moe/kernels.py
deepep_permute_triton_kernel(input_ptr, gateup_input_ptr, src2dst_ptr, topk_ids_ptr, a1_scales_ptr, topk, hidden_size, BLOCK_SIZE)
deepep_post_reorder_triton_kernel(down_output_ptr, output_ptr, src2dst_ptr, topk_ids_ptr, topk_weights_ptr, topk, hidden_size, BLOCK_SIZE)
compute_src2dst_triton_kernel(reorder_ids, src2dst, num_toks, BLOCK_SIZE)
deepep_compute_src2dst_triton_kernel(reorder_ids, src2dst, num_toks, num_minus_one, BLOCK_SIZE)
deepep_run_moe_deep_preprocess(topk_ids, num_experts)
compute_seg_indptr_triton_kernel(reorder_topk_ids, seg_indptr, num_toks)
run_moe_ep_preproess(topk_ids, num_experts)
run_cutlass_moe_ep_preproess(local_topk_ids, local_num_experts)
pre_reorder_triton_kernel_for_cutlass_moe(input_ptr, gateup_input_ptr, src2dst_ptr, topk_ids_ptr, a1_scales_ptr, num_experts, topk, hidden_size, BLOCK_SIZE)
pre_reorder_triton_kernel(input_ptr, gateup_input_ptr, src2dst_ptr, topk_ids_ptr, a1_scales_ptr, start_expert_id, end_expert_id, topk, hidden_size, BLOCK_SIZE, use_per_token_if_dynamic)
silu_and_mul_triton_kernel(gateup_output, down_input, hidden_size, reorder_topk_ids, scales, start_expert_id, end_expert_id, BLOCK_SIZE)
silu_and_mul_masked_post_quant_fwd(input, output, output_scale, quant_group_size, masked_m, scale_ue8m0)
tanh(x)
gelu_and_mul_triton_kernel(gateup_output, down_input, hidden_size, reorder_topk_ids, scales, start_expert_id, end_expert_id, BLOCK_SIZE)
post_reorder_triton_kernel(down_output_ptr, output_ptr, src2dst_ptr, topk_ids_ptr, topk_weights_ptr, start_expert_id, end_expert_id, topk, hidden_size, dst_start, BLOCK_SIZE)
post_reorder_triton_kernel_for_cutlass_moe(down_output_ptr, output_ptr, src2dst_ptr, topk_ids_ptr, topk_weights_ptr, num_experts, topk, hidden_size, dst_start, BLOCK_SIZE)
compute_m_range(pid, batch_size, seg_indptr, weight_indices, m_num_tiles_indptr, BLOCK_SIZE_M)
grouped_gemm_triton_kernel(a, b, c, batch_size, N, K, seg_indptr, weight_indices, m_num_tiles_indptr, scale_a, scale_b, use_fp8_w8a8, group_n, group_k, a_stride_0, b_stride_0, b_stride_1, as_stride_0, as_stride_1, bs_stride_0, bs_stride_2, bs_stride_1, use_per_token_if_dynamic, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K)
compute_m_num_tiles_indptr(m_num_tiles_indptr, seg_indptr, batch_size, BLOCK_SIZE_M)
grouped_gemm_triton(a, b, c, batch_size, weight_column_major, seg_indptr, weight_indices, use_fp8_w8a8, scale_a, scale_b, block_shape, c_dtype, use_per_token_if_dynamic)
ep_scatter(recv_x, recv_x_scale, recv_topk, num_recv_tokens_per_expert, expert_start_loc, output_tensor, output_tensor_scale, m_indices, output_index, scale_ue8m0)
ep_gather(input_tensor, recv_topk_ids, recv_topk_weight, input_index, output_tensor)
get_tma_aligned_size(x, element_size)
tma_align_input_scale(input_scale)
compute_masked_m_triton_kernel(seg_indptr, masked_m)
deepgemm_compute_src2dst_triton_kernel(topk_ids, reorder_ids, seg_indptr, src2dst, m_max, num_toks, BLOCK_SIZE)
fill_gateup_input_triton_kernel(input_ptr, scale_ptr, gateup_input_ptr, gateup_input_scale_ptr, src2dst_ptr, topk_ids_ptr, start_expert_id, end_expert_id, topk, m_max, hidden_size, scale_size, BLOCK_SIZE)
moe_ep_deepgemm_preprocess(topk_ids, num_experts, hidden_states, top_k, start_expert_id, end_expert_id, block_shape, output_dtype)

# python/sglang/srt/layers/moe/ep_moe/layer.py
  EPMoE.__init__(num_experts, top_k, hidden_size, intermediate_size, layer_id, num_fused_shared_experts, params_dtype, quant_config, prefix, activation, routed_scaling_factor, gemm1_alpha, gemm1_clamp_limit, with_bias)
  EPMoE.forward(hidden_states, topk_output)
  EPMoE.forward_deepgemm(hidden_states, topk_output)
  DeepEPMoE.__init__(num_experts, top_k, hidden_size, intermediate_size, layer_id, num_fused_shared_experts, params_dtype, quant_config, prefix, activation, routed_scaling_factor)
  DeepEPMoE.forward(hidden_states, topk_idx, topk_weights, forward_batch)
  DeepEPMoE.dispatch(hidden_states, topk_idx, topk_weights, forward_batch)
  DeepEPMoE.moe_impl(dispatch_output)
  DeepEPMoE.combine(hidden_states, topk_idx, topk_weights, forward_batch)
  DeepEPMoE.forward_aiter(dispatch_output, DeepEPLLOutput])
  DeepEPMoE.forward_deepgemm_contiguous(dispatch_output)
  DeepEPMoE.forward_deepgemm_masked(dispatch_output)
  DeepEPMoE.forward_npu(dispatch_output)
get_moe_impl_class(quant_config)

# python/sglang/srt/layers/moe/fused_moe_native.py
fused_moe_forward_native(layer, x, topk_output, moe_runner_config)
moe_forward_native(layer, x, topk_output, moe_runner_config)

# python/sglang/srt/layers/moe/fused_moe_triton/__init__.py
override_config(config)
get_config()

# python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
write_zeros_to_output(c_ptr, stride_cm, stride_cn, pid_n, N, offs_token, token_mask, BLOCK_SIZE_M, BLOCK_SIZE_N, compute_type)
fused_moe_kernel_gptq_awq(a_ptr, b_ptr, c_ptr, b_scale_ptr, b_zp_ptr, topk_weights_ptr, sorted_token_ids_ptr, expert_ids_ptr, num_tokens_post_padded_ptr, N, K, EM, num_valid_tokens, stride_am, stride_ak, stride_be, stride_bk, stride_bn, stride_cm, stride_cn, stride_bse, stride_bsk, stride_bsn, stride_bze, stride_bzk, stride_bzn, group_size, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, GROUP_SIZE_M, MUL_ROUTED_WEIGHT, top_k, compute_type, has_zp, use_int4_w4a16, use_int8_w8a16, even_Ks)
fused_moe_kernel(a_ptr, b_ptr, bias_ptr, c_ptr, a_scale_ptr, b_scale_ptr, topk_weights_ptr, sorted_token_ids_ptr, expert_ids_ptr, num_tokens_post_padded_ptr, N, K, EM, num_valid_tokens, stride_am, stride_ak, stride_be, stride_bk, stride_bn, stride_bias_e, stride_bias_n, stride_cm, stride_cn, stride_asm, stride_ask, stride_bse, stride_bsk, stride_bsn, group_n, group_k, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, GROUP_SIZE_M, MUL_ROUTED_WEIGHT, top_k, compute_type, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, per_channel_quant, even_Ks)
moe_align_block_size(topk_ids, block_size, num_experts)
invoke_fused_moe_kernel(A, B, bias, C, A_scale, B_scale, B_zp, topk_weights, topk_ids, sorted_token_ids, expert_ids, num_tokens_post_padded, mul_routed_weight, top_k, config, Any], compute_type, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, use_int4_w4a16, per_channel_quant, block_shape, no_combine)
get_config_file_name(E, N, dtype, block_shape)
get_moe_configs(E, N, dtype, block_n, block_k)
get_default_config(M, E, N, K, topk, dtype, is_marlin, block_shape)
try_get_optimal_moe_config(w1_shape, ...], w2_shape, ...], top_k, dtype, M, is_marlin, block_shape)
get_config_dtype_str(dtype, use_int8_w8a16, use_int4_w4a16, use_fp8_w8a8, use_int8_w8a8)
inplace_fused_experts(hidden_states, w1, w2, topk_weights, topk_ids, b1, b2, activation, apply_router_weight_on_input, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, use_int4_w4a16, per_channel_quant, w1_scale, w2_scale, w1_zp, w2_zp, a1_scale, a2_scale, block_shape, routed_scaling_factor, gemm1_alpha, gemm1_limit)
inplace_fused_experts_fake(hidden_states, w1, w2, topk_weights, topk_ids, b1, b2, activation, apply_router_weight_on_input, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, use_int4_w4a16, per_channel_quant, w1_scale, w2_scale, w1_zp, w2_zp, a1_scale, a2_scale, block_shape, routed_scaling_factor, gemm1_alpha, gemm1_limit)
outplace_fused_experts(hidden_states, w1, w2, topk_weights, topk_ids, b1, b2, activation, apply_router_weight_on_input, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, use_int4_w4a16, per_channel_quant, w1_scale, w2_scale, w1_zp, w2_zp, a1_scale, a2_scale, block_shape, no_combine, routed_scaling_factor, gemm1_alpha, gemm1_limit)
outplace_fused_experts_fake(hidden_states, w1, w2, topk_weights, topk_ids, b1, b2, activation, apply_router_weight_on_input, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, use_int4_w4a16, per_channel_quant, w1_scale, w2_scale, w1_zp, w2_zp, a1_scale, a2_scale, block_shape, no_combine, routed_scaling_factor, gemm1_alpha, gemm1_limit)
fused_experts(hidden_states, w1, w2, topk_output, moe_runner_config, b1, b2, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, use_int4_w4a16, per_channel_quant, w1_scale, w2_scale, w1_zp, w2_zp, a1_scale, a2_scale, block_shape)
moe_sum_reduce_triton(input, output, routed_scaling_factor)
moe_sum_reduce_torch_compile(x, out, routed_scaling_factor)
swiglu_with_alpha_and_limit(x, gemm1_alpha, gemm1_limit)
fused_experts_impl(hidden_states, w1, w2, topk_weights, topk_ids, b1, b2, inplace, activation, apply_router_weight_on_input, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, use_int4_w4a16, per_channel_quant, w1_scale, w2_scale, w1_zp, w2_zp, a1_scale, a2_scale, block_shape, no_combine, routed_scaling_factor, gemm1_alpha, gemm1_limit)
fused_moe(hidden_states, w1, w2, topk_output, moe_runner_config, b1, b2, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, use_int4_w4a16, per_channel_quant, w1_scale, w2_scale, w1_zp, w2_zp, a1_scale, a2_scale, block_shape)

# python/sglang/srt/layers/moe/fused_moe_triton/layer.py
  FusedMoE.__init__(num_experts, hidden_size, intermediate_size, layer_id, top_k, num_fused_shared_experts, params_dtype, reduce_results, quant_config, prefix, activation, apply_router_weight_on_input, use_presharded_weights, inplace, no_combine, routed_scaling_factor, gemm1_alpha, gemm1_clamp_limit, use_weight_loader_fused, with_bias)
  FusedMoE.weight_loader(param, loaded_weight, weight_name, shard_id, expert_id)
  FusedMoE.weight_loader_fused(param, loaded_weight, weight_name, shard_id)
  FusedMoE.forward(hidden_states, topk_output)
  FusedMoE.make_expert_params_mapping(cls, ckpt_gate_proj_name, ckpt_down_proj_name, ckpt_up_proj_name, num_experts)
  FusedMoE.make_expert_params_mapping_fused(cls, ckpt_gate_up_proj_name, ckpt_down_proj_name, ckpt_gate_up_proj_bias_name, ckpt_down_proj_bias_name)
  FusedMoE.make_expert_params_mapping_fused_mxfp4(cls, ckpt_gate_up_proj_name, ckpt_down_proj_name, ckpt_gate_up_proj_bias_name, ckpt_down_proj_bias_name, ckpt_gate_up_proj_scale_name, ckpt_down_proj_scale_name)
  FusedMoE.make_expert_input_scale_params_mapping(cls, num_experts)
  FusedMoE.should_fuse_routed_scaling_factor_in_topk()
  FlashInferFusedMoE.__init__()
  FlashInferFusedMoE.forward(hidden_states, topk_output)
  FlashInferFP4MoE.__init__()
  FlashInferFP4MoE.forward(hidden_states, topk_output)
get_fused_moe_impl_class()

# python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py
quantize(w, dtype, dev)
triton_kernel_moe_forward(hidden_states, w1, w2, topk_output, moe_runner_config, apply_router_weight_on_input, use_fp8_w8a8, per_channel_quant, global_num_experts, expert_map, w1_scale, w2_scale, a1_scale, a2_scale, block_shape)
triton_kernel_fused_experts(hidden_states, w1, w2, routing_data, gather_indx, scatter_indx, inplace, activation, apply_router_weight_on_input, use_fp8_w8a8, per_channel_quant, global_num_experts, expert_map, w1_scale, w2_scale, a1_scale, a2_scale, block_shape)
triton_kernel_moe_with_bias_forward(hidden_states, w1, w1_pcg, b1, w2, w2_pcg, b2, topk_output, moe_runner_config, use_fp8_w8a8, per_channel_quant, global_num_experts, expert_map, w1_scale, w2_scale, a1_scale, a2_scale, block_shape)
triton_kernel_fused_experts_with_bias(hidden_states, w1, w1_pcg, b1, w2, w2_pcg, b2, routing_data, gather_indx, scatter_indx, inplace, activation, use_fp8_w8a8, per_channel_quant, global_num_experts, expert_map, w1_scale, w2_scale, a1_scale, a2_scale, block_shape, gemm1_alpha, gemm1_clamp_limit)

# python/sglang/srt/layers/moe/rocm_moe_utils.py
rocm_aiter_asm_moe_tkw1_impl(hidden_states, w1, w2, topk_weights, topk_ids, fc1_scale, fc2_scale, fc1_smooth_scale, fc2_smooth_scale, a16, per_tensor_quant_scale, expert_mask, activation_method)
rocm_aiter_asm_moe_tkw1_fake(hidden_states, w1, w2, topk_weights, topk_ids, fc1_scale, fc2_scale, fc1_smooth_scale, fc2_smooth_scale, a16, per_tensor_quant_scale, expert_mask, activation_method)
rocm_fused_experts_tkw1(hidden_states, w1, w2, topk_weights, topk_ids, activation, apply_router_weight_on_input, use_fp8_w8a8, per_channel_quant, w1_scale, w2_scale, a1_scale, a2_scale, block_shape)

# python/sglang/srt/layers/moe/router.py
fused_moe_router_kernel(input_ptr, moe_router_weight_ptr, topk_weights_ptr, topk_ids_ptr, correction_bias_ptr, is_correction_bias, num_experts, topk, moe_softcapping, moe_renormalize, hidden_dim, BLOCK_SIZE)
fused_moe_router_impl(x, router_weight, topk, moe_softcapping, correction_bias)
fused_moe_router_large_bs_kernel(a_ptr, b_ptr, topk_weights_ptr, topk_ids_ptr, bs, num_experts, topk, moe_softcapping, moe_renormalize, K, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, stride_am, stride_bn)
fused_moe_router_large_bs_impl(x, router_weight, topk, moe_softcapping, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K)
fused_moe_router_shim(moe_softcapping, hidden_states, gating_output, topk, renormalize, correction_bias)
  FusedMoeRouter.__init__(router_linear, topk, moe_softcapping)
  FusedMoeRouter.__call__()
  FusedMoeRouter.forward(x, residual)
  FusedMoeRouter.forward_cuda(x, autotune)
  FusedMoeRouter.forward_vllm(x)

# python/sglang/srt/layers/moe/token_dispatcher/base_dispatcher.py
  DispatchOutputChecker.format_is_standard(dispatch_output)
  DispatchOutputChecker.format_is_deepep_normal(dispatch_output)
  DispatchOutputChecker.format_is_deepep_ll(dispatch_output)
  DispatchOutputChecker.format_is_deepep(dispatch_output)
  DispatchOutputChecker.format_is_ascent_ll(dispatch_output)
  DispatchOutputFormat.is_standard()
  DispatchOutputFormat.is_deepep_normal()
  DispatchOutputFormat.is_deepep_ll()
  DispatchOutputFormat.is_deepep()
  DispatchOutputFormat.is_ascent_ll()
  DispatchOutput.format()
  BaseDispatcher.dispatch()
  BaseDispatcher.combine()

# python/sglang/srt/layers/moe/token_dispatcher/deepep.py
  DeepEPNormalOutput.format()
  DeepEPLLOutput.format()
  AscendDeepEPLLOutput.format()
  DeepEPBuffer.get_deepep_buffer(cls, group, hidden_size, param_bytes, deepep_mode, num_max_dispatch_tokens_per_rank, num_experts)
  DeepEPBuffer.clean_buffer(cls)
  DeepEPBuffer.set_dispatch_mode_as_normal(cls)
  DeepEPBuffer.set_dispatch_mode_as_low_latency(cls)
  DeepEPConfig.__init__()
  DeepEPConfig.get_instance(cls)
  _DeepEPDispatcherImplBase.__init__(group, router_topk, permute_fusion, num_experts, num_local_experts, hidden_size, params_dtype, deepep_mode)
  _DeepEPDispatcherImplBase.dispatch_a(hidden_states, topk_idx, topk_weights)
  _DeepEPDispatcherImplBase.dispatch_b()
  _DeepEPDispatcherImplBase.combine_a(hidden_states, topk_idx, topk_weights)
  _DeepEPDispatcherImplBase.combine_b()
  _DeepEPDispatcherImplNormal.__init__(async_finish)
  _DeepEPDispatcherImplNormal.dispatch_a(hidden_states, topk_idx, topk_weights)
  _DeepEPDispatcherImplNormal.dispatch_b(hidden_states, topk_idx, topk_weights, previous_event)
  _DeepEPDispatcherImplNormal.combine_a(hidden_states, topk_idx, topk_weights)
  _DeepEPDispatcherImplNormal.combine_b(output, previous_event)
  _DeepEPDispatcherImplLowLatency.__init__(return_recv_hook)
  _DeepEPDispatcherImplLowLatency.dispatch_a(hidden_states, topk_idx, topk_weights)
  _DeepEPDispatcherImplLowLatency.dispatch_b(hidden_states, topk_idx, topk_weights, masked_m, expected_m, event, hook)
  _DeepEPDispatcherImplLowLatency.combine_a(hidden_states, topk_idx, topk_weights)
  _DeepEPDispatcherImplLowLatency.combine_b(hidden_states, event, hook)
  DeepEPDispatcher.__init__(group, router_topk, permute_fusion, num_experts, num_local_experts, hidden_size, params_dtype, deepep_mode, async_finish, return_recv_hook)
  DeepEPDispatcher.dispatch()
  DeepEPDispatcher.dispatch_a(hidden_states, topk_idx, topk_weights, forward_batch)
  DeepEPDispatcher.dispatch_b()
  DeepEPDispatcher.combine()
  DeepEPDispatcher.combine_a(hidden_states, topk_idx, topk_weights, forward_batch)
  DeepEPDispatcher.combine_b()

# python/sglang/srt/layers/moe/token_dispatcher/standard.py
  StandardDispatchOutput.format()

# python/sglang/srt/layers/moe/topk.py
  TopKOutputChecker.format_is_standard(topk_output)
  TopKOutputChecker.format_is_triton_kernel(topk_output)
  TopKOutputChecker.format_is_bypassed(topk_output)
  TopKOutputFormat.is_standard()
  TopKOutputFormat.is_triton_kernel()
  TopKOutputFormat.is_bypassed()
  TopKOutput.format()
  StandardTopKOutput.format()
  TritonKernelTopKOutput.format()
  BypassedTopKOutput.format()
  TopK.__init__(top_k)
  TopK.forward_native(hidden_states, router_logits)
  TopK.forward_cuda(hidden_states, router_logits)
  TopK.forward_cpu(hidden_states, router_logits)
  TopK.forward_npu(hidden_states, router_logits)
  TopK.empty_topk_output(device)
fused_topk_torch_native(hidden_states, gating_output, topk, renormalize)
fused_topk_cpu(hidden_states, gating_output, topk, renormalize, num_token_non_padded, expert_location_dispatch_info)
apply_topk_weights_cpu(need_apply, topk_weights, inputs)
fused_topk(hidden_states, gating_output, topk, renormalize, num_token_non_padded, expert_location_dispatch_info)
grouped_topk_gpu(hidden_states, gating_output, topk, renormalize, num_expert_group, topk_group, num_fused_shared_experts, routed_scaling_factor, num_token_non_padded, expert_location_dispatch_info, apply_routed_scaling_factor_on_output)
grouped_topk_cpu(hidden_states, gating_output, topk, renormalize, num_expert_group, topk_group, num_fused_shared_experts, routed_scaling_factor, num_token_non_padded, expert_location_dispatch_info, apply_routed_scaling_factor_on_output)
biased_grouped_topk_impl(hidden_states, gating_output, correction_bias, topk, renormalize, num_expert_group, topk_group, num_fused_shared_experts, routed_scaling_factor, num_token_non_padded, expert_location_dispatch_info, apply_routed_scaling_factor_on_output)
is_power_of_two(n)
biased_grouped_topk_gpu(hidden_states, gating_output, correction_bias, topk, renormalize, num_expert_group, topk_group, num_fused_shared_experts, routed_scaling_factor, num_token_non_padded, expert_location_dispatch_info, apply_routed_scaling_factor_on_output)
biased_grouped_topk_cpu(hidden_states, gating_output, correction_bias, topk, renormalize, num_expert_group, topk_group, compiled, num_fused_shared_experts, routed_scaling_factor, num_token_non_padded, expert_location_dispatch_info, apply_routed_scaling_factor_on_output)
select_experts(hidden_states, router_logits, topk_config)

# python/sglang/srt/layers/moe/utils.py
  MoeA2ABackend.is_none()
  MoeA2ABackend.is_deepep()
  MoeRunnerBackend.is_auto()
  MoeRunnerBackend.is_triton()
  MoeRunnerBackend.is_triton_kernel()
  MoeRunnerBackend.is_flashinfer_trtllm()
  MoeRunnerBackend.is_flashinfer_cutlass()
  MoeRunnerBackend.is_flashinfer_mxfp4()
  DeepEPMode.enable_normal()
  DeepEPMode.enable_low_latency()
  DeepEPMode.resolve(is_extend_in_batch)
  DeepEPMode.is_normal()
  DeepEPMode.is_low_latency()
  DeepEPMode.is_auto()
initialize_moe_config(server_args)
get_moe_a2a_backend()
get_moe_runner_backend()
get_deepep_mode()
get_deepep_config()
is_tbo_enabled()
get_tbo_token_distribution_threshold()
should_use_flashinfer_trtllm_moe()
should_use_flashinfer_cutlass_moe_fp4_allgather()

# python/sglang/srt/layers/multimodal.py
hash_tiles32_kernel_blocked(in_ptr, out_ptr, n_u32, seed1, seed2, FM_C1, FM_C2, POS_A, POS_B, TILE, BLOCK, USE_CG)
add_tree_reduce_u64_kernel(in_ptr, out_ptr, n_elems, CHUNK)
gpu_tensor_hash(tensor)

# python/sglang/srt/layers/parameter.py
  BasevLLMParameter.__new__(cls, data)
  BasevLLMParameter.__init__(data, weight_loader)
  BasevLLMParameter.weight_loader()
  BasevLLMParameter.load_column_parallel_weight(loaded_weight)
  BasevLLMParameter.load_row_parallel_weight(loaded_weight)
  BasevLLMParameter.load_merged_column_weight(loaded_weight)
  BasevLLMParameter.load_qkv_weight(loaded_weight)
  _ColumnvLLMParameter.__init__(output_dim)
  _ColumnvLLMParameter.output_dim()
  _ColumnvLLMParameter.load_column_parallel_weight(loaded_weight, tp_rank, use_presharded_weights)
  _ColumnvLLMParameter.load_merged_column_weight(loaded_weight)
  _ColumnvLLMParameter.load_qkv_weight(loaded_weight, tp_rank, use_presharded_weights)
  RowvLLMParameter.__init__(input_dim)
  RowvLLMParameter.input_dim()
  RowvLLMParameter.load_row_parallel_weight(loaded_weight, tp_rank, use_presharded_weights)
  PerTensorScaleParameter.__init__()
  PerTensorScaleParameter.load_row_parallel_weight()
  PerTensorScaleParameter.load_merged_column_weight()
  PerTensorScaleParameter.load_qkv_weight()
  PerTensorScaleParameter.load_column_parallel_weight()
  PackedColumnParameter.__init__(packed_factor, Fraction], packed_dim, marlin_tile_size)
  PackedColumnParameter.packed_dim()
  PackedColumnParameter.packed_factor()
  PackedColumnParameter.marlin_tile_size()
  PackedColumnParameter.adjust_shard_indexes_for_packing(shard_size, shard_offset)
  PackedvLLMParameter.__init__(packed_factor, Fraction], packed_dim, marlin_tile_size)
  PackedvLLMParameter.packed_dim()
  PackedvLLMParameter.packed_factor()
  PackedvLLMParameter.marlin_tile_size()
  PackedvLLMParameter.adjust_shard_indexes_for_packing(shard_size, shard_offset)
permute_param_layout_(param, input_dim, output_dim)

# python/sglang/srt/layers/pooler.py
  Pooler.__init__(pooling_type, normalize)
  Pooler.forward(hidden_states, forward_batch)
  CrossEncodingPooler.__init__(config, classifier, pooler)
  CrossEncodingPooler.forward(hidden_states, forward_batch)

# python/sglang/srt/layers/quantization/__init__.py
  DummyConfig.override_quantization_method()
get_quantization_config(quantization)
monkey_patch_isinstance_for_vllm_base_layer(reverse)
monkey_patch_moe_apply(class_obj)
monkey_patch_quant_configs()

# python/sglang/srt/layers/quantization/awq.py
is_layer_skipped_awq(prefix, modules_to_not_convert)
  AWQConfig.__init__(weight_bits, group_size, zero_point, modules_to_not_convert)
  AWQConfig.__repr__()
  AWQConfig.get_scaled_act_names()
  AWQConfig.get_name()
  AWQConfig.get_supported_act_dtypes()
  AWQConfig.get_min_capability(cls)
  AWQConfig.get_config_filenames()
  AWQConfig.from_config(cls, config, Any])
  AWQConfig.get_quant_method(layer, prefix)
  AWQMarlinConfig.__init__(weight_bits, group_size, zero_point, lm_head_quantized, modules_to_not_convert, full_config, Any])
  AWQMarlinConfig.__repr__()
  AWQMarlinConfig.get_scaled_act_names()
  AWQMarlinConfig.get_name(cls)
  AWQMarlinConfig.get_supported_act_dtypes(cls)
  AWQMarlinConfig.get_min_capability(cls)
  AWQMarlinConfig.get_config_filenames(cls)
  AWQMarlinConfig.from_config(cls, config, Any])
  AWQMarlinConfig.override_quantization_method(cls, hf_quant_cfg, user_quant)
  AWQMarlinConfig.get_quant_method(layer, prefix)
  AWQMarlinConfig.is_awq_marlin_compatible(cls, quant_config, Any])
  AWQLinearMethod.__init__(quant_config)
  AWQLinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  AWQLinearMethod.process_weights_after_loading(layer)
  AWQLinearMethod.apply(layer, x, bias)
  AWQMarlinLinearMethod.__init__(quant_config)
  AWQMarlinLinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  AWQMarlinLinearMethod.process_weights_after_loading(layer)
  AWQMarlinLinearMethod.apply(layer, x, bias)
  AWQMoEMethod.__init__(quant_config)
  AWQMoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size_per_partition, params_dtype)
  AWQMoEMethod.process_weights_after_loading(layer)
  AWQMoEMethod.apply(layer, x, topk_output, moe_runner_config)

# python/sglang/srt/layers/quantization/awq_triton.py
awq_dequantize_kernel(qweight_ptr, scales_ptr, zeros_ptr, group_size, result_ptr, num_cols, num_rows, BLOCK_SIZE_X, BLOCK_SIZE_Y)
awq_gemm_kernel(a_ptr, b_ptr, c_ptr, zeros_ptr, scales_ptr, M, N, K, group_size, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, SPLIT_K)
awq_dequantize_triton(qweight, scales, zeros, block_size_x, block_size_y)
awq_gemm_triton(input, qweight, scales, qzeros, split_k_iters, block_size_m, block_size_n, block_size_k)

# python/sglang/srt/layers/quantization/base_config.py
  QuantizeMethodBase.create_weights(layer)
  QuantizeMethodBase.apply(layer)
  QuantizeMethodBase.process_weights_after_loading(layer)
  LinearMethodBase.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  LinearMethodBase.apply(layer, x, bias)
  FusedMoEMethodBase.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype)
  FusedMoEMethodBase.apply(layer, x, topk_output, moe_runner_config)
  QuantizationConfig.__init__()
  QuantizationConfig.get_name()
  QuantizationConfig.get_supported_act_dtypes()
  QuantizationConfig.get_min_capability(cls)
  QuantizationConfig.get_config_filenames()
  QuantizationConfig.from_config(cls, config, Any])
  QuantizationConfig.override_quantization_method(cls, hf_quant_cfg, user_quant)
  QuantizationConfig.get_from_keys(config, Any], keys)
  QuantizationConfig.get_from_keys_or(config, Any], keys, default)
  QuantizationConfig.get_quant_method(layer, prefix)
  QuantizationConfig.get_scaled_act_names()
method_has_implemented_embedding(method_class)

# python/sglang/srt/layers/quantization/blockwise_int8.py
  BlockInt8Config.__init__(is_checkpoint_int8_serialized, activation_scheme, ignored_layers, weight_block_size)
  BlockInt8Config.get_name(cls)
  BlockInt8Config.get_supported_act_dtypes(cls)
  BlockInt8Config.get_min_capability(cls)
  BlockInt8Config.get_config_filenames(cls)
  BlockInt8Config.from_config(cls, config, Any])
  BlockInt8Config.get_quant_method(layer, prefix)
  BlockInt8Config.get_scaled_act_names()
  BlockInt8LinearMethod.__init__(quant_config)
  BlockInt8LinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  BlockInt8LinearMethod.process_weights_after_loading(layer)
  BlockInt8LinearMethod.apply(layer, x, bias)
  BlockInt8MoEMethod.__init__(quant_config)
  BlockInt8MoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype)
  BlockInt8MoEMethod.process_weights_after_loading(layer)
  BlockInt8MoEMethod.apply(layer, x, topk_output, moe_runner_config)

# python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py
  DeviceCapability.as_version_str()
  DeviceCapability.to_int()
  CompressedTensorsConfig.__init__(target_scheme_map, Any], ignore, quant_format, sparsity_scheme_map, SparsityCompressionConfig], sparsity_ignore_list, kv_cache_scheme, Any]], config, Any]], packed_modules_mapping, List[str]])
  CompressedTensorsConfig.get_linear_method()
  CompressedTensorsConfig.get_supported_act_dtypes(cls)
  CompressedTensorsConfig.get_min_capability(cls)
  CompressedTensorsConfig.get_name()
  CompressedTensorsConfig.get_scaled_act_names()
  CompressedTensorsConfig.get_quant_method(layer, prefix)
  CompressedTensorsConfig.from_config(cls, config, Any])
  CompressedTensorsConfig.get_config_filenames(cls)
  CompressedTensorsConfig.get_scheme(layer, layer_name)
  CompressedTensorsConfig.get_cache_scale(name)
  CompressedTensorsConfig.supports_cutlass_24(weight_quant, input_quant, sparsity_scheme)
  CompressedTensorsLinearMethod.__init__(quantization_config)
  CompressedTensorsLinearMethod.process_weights_after_loading(layer)
  CompressedTensorsLinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  CompressedTensorsLinearMethod.apply(layer, x, bias)

# python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors_moe.py
  CompressedTensorsMoEMethod.__new__(cls)
  CompressedTensorsMoEMethod.get_moe_method(quant_config)
  CompressedTensorsW8A8Fp8MoEMethod.__init__(quant_config)
  CompressedTensorsW8A8Fp8MoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size_per_partition, params_dtype)
  CompressedTensorsW8A8Fp8MoEMethod.process_weights_after_loading(layer)
  CompressedTensorsW8A8Fp8MoEMethod.apply(layer, x, topk_output, moe_runner_config)
  CompressedTensorsWNA16MoEMethod.__init__(quant_config)
  CompressedTensorsWNA16MoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size_per_partition, params_dtype)
  CompressedTensorsWNA16MoEMethod.process_weights_after_loading(layer)
  CompressedTensorsWNA16MoEMethod.apply(layer, x, topk_output, moe_runner_config)

# python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py
  CompressedTensorsScheme.get_min_capability(cls)
  CompressedTensorsScheme.create_weights()
  CompressedTensorsScheme.apply_weights(layer, x, bias)
  CompressedTensorsScheme.process_weights_after_loading(layer)

# python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py
apply_fp8_marlin_linear()
prepare_fp8_layer_for_marlin()
  CompressedTensorsW8A16Fp8.__init__(strategy, is_static_input_scheme)
  CompressedTensorsW8A16Fp8.get_min_capability(cls)
  CompressedTensorsW8A16Fp8.process_weights_after_loading(layer)
  CompressedTensorsW8A16Fp8.create_weights(layer, input_size, output_partition_sizes, input_size_per_partition, params_dtype, weight_loader)
  CompressedTensorsW8A16Fp8.apply_weights(layer, x, bias)

# python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
  CompressedTensorsW8A8Fp8.__init__(strategy, is_static_input_scheme)
  CompressedTensorsW8A8Fp8.get_min_capability(cls)
  CompressedTensorsW8A8Fp8.process_weights_after_loading(layer)
  CompressedTensorsW8A8Fp8.create_weights(layer, output_partition_sizes, input_size_per_partition, params_dtype, weight_loader)
  CompressedTensorsW8A8Fp8.apply_weights(layer, x, bias)

# python/sglang/srt/layers/quantization/compressed_tensors/utils.py
is_activation_quantization_format(format)
should_ignore_layer(layer_name, ignore, fused_mapping, List[str]])
check_equal_or_regex_match(layer_name, targets)
find_matched_target(layer_name, module, targets, fused_mapping, List[str]])

# python/sglang/srt/layers/quantization/deep_gemm_wrapper/compile_utils.py
update_deep_gemm_config(gpu_id, server_args)
  _BaseWarmupExecutor.create(kernel_type)
  _BaseWarmupExecutor.execute(m)
  _NormalWarmupExecutor.__init__(max_m, n, k, num_groups)
  _NormalWarmupExecutor.execute(m)
  _GroupedContWarmupExecutor.__init__(max_m, n, k, num_groups)
  _GroupedContWarmupExecutor.execute(m)
  _GroupedMaskedWarmupExecutor.__init__(max_m, n, k, num_groups)
  _GroupedMaskedWarmupExecutor.execute(m)
deep_gemm_execution_hook(m, n, k, num_groups, kernel_type)

# python/sglang/srt/layers/quantization/deep_gemm_wrapper/entrypoint.py
grouped_gemm_nt_f8f8bf16_masked(lhs, torch.Tensor], rhs, torch.Tensor], out, masked_m, expected_m)
grouped_gemm_nt_f8f8bf16_contig(lhs, torch.Tensor], rhs, torch.Tensor], out, m_indices)
gemm_nt_f8f8bf16(lhs, torch.Tensor], rhs, torch.Tensor], out)
update_deep_gemm_config(gpu_id, server_args)
configure_deep_gemm_num_sms(num_sms)

# python/sglang/srt/layers/quantization/fp8.py
dummy_func()
  Fp8Config.__init__(is_checkpoint_fp8_serialized, activation_scheme, ignored_layers, weight_block_size)
  Fp8Config.get_name(cls)
  Fp8Config.get_supported_act_dtypes(cls)
  Fp8Config.get_min_capability(cls)
  Fp8Config.get_config_filenames(cls)
  Fp8Config.from_config(cls, config, Any])
  Fp8Config.get_quant_method(layer, prefix)
  Fp8Config.get_scaled_act_names()
  Fp8LinearMethod.__init__(quant_config, W4AFp8Config])
  Fp8LinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  Fp8LinearMethod.process_weights_after_loading(layer)
  Fp8LinearMethod.apply(layer, x, bias)
get_tile_tokens_dim(num_tokens, top_k, num_experts)
  Fp8MoEMethod.__init__(quant_config)
  Fp8MoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype)
  Fp8MoEMethod.process_weights_after_loading(layer)
  Fp8MoEMethod.process_weights_hip_int4(layer)
  Fp8MoEMethod.process_weights_hip_scale_padding(layer)
  Fp8MoEMethod.apply(layer, x, topk_output, moe_runner_config)
  Fp8MoEMethod.apply_with_router_logits(layer, x, topk_output, moe_runner_config)
  Fp8MoEMethod.maybe_apply_hip_fused_experts(layer, x, topk_output, activation, no_combine)
  Fp8KVCacheMethod.__init__(quant_config)

# python/sglang/srt/layers/quantization/fp8_kernel.py
is_fp8_fnuz()
deep_gemm_fp8_fp8_bf16_nt(A, As, B, Bs, C)
deep_gemm_fp8_fp8_bf16_nt_fake(A, As, B, Bs, C)
per_token_group_quant_8bit(x, group_size, dst_dtype, eps, column_major_scales, scale_tma_aligned, scale_ue8m0, fuse_silu_and_mul, masked_m)
create_per_token_group_quant_fp8_output_scale(x_shape, device, group_size, column_major_scales, scale_tma_aligned, scale_ue8m0)
sglang_per_token_group_quant_fp8(x, group_size, eps, column_major_scales, scale_tma_aligned, scale_ue8m0, fuse_silu_and_mul, masked_m)
sglang_per_token_group_quant_8bit(x, group_size, dst_dtype, eps, column_major_scales, scale_tma_aligned, scale_ue8m0, fuse_silu_and_mul, masked_m)
sglang_per_token_quant_fp8(x, dtype)
static_quant_fp8(x, x_s, repeat_scale)
get_w8a8_block_fp8_configs(N, K, block_n, block_k)
select_w8a8_block_fp8_matmul_kernel(M, N, META)
use_w8a8_block_fp8_matmul_unrolledx4(M, N, META)
select_w8a8_block_fp8_matmul_kernel(M, N, META)
prepare_block_fp8_matmul_inputs(A, B, As, Bs, block_size, output_dtype)
w8a8_block_fp8_matmul_deepgemm(A, B, As, Bs, block_size, output_dtype)
w8a8_block_fp8_matmul_triton(A, B, As, Bs, block_size, output_dtype)
w8a8_block_fp8_matmul(A, B, As, Bs, block_size, output_dtype)
per_tensor_quant_mla_fp8(x, x_s_out, eps)
per_token_group_quant_mla_deep_gemm_masked_fp8(x, group_size, eps, dtype)
scaled_fp8_quant(input, scale, num_token_padding, use_per_token_if_dynamic)
scaled_fp8_quant(input, scale, num_token_padding, use_per_token_if_dynamic)
per_token_group_quant_fp8_hopper_moe_mn_major(A, expert_offsets, problem_sizes, group_size, expert_tokens_alignment)
per_group_transpose(a, expert_offsets, M_ALIGNMENT)
is_weak_contiguous(x)
scaled_mm_kernel(a_ptr, b_ptr, scale_a_ptr, scale_b_ptr, c_ptr, bias_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, ACCUMULATOR_DTYPE, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, BLOCK_SIZE_SCALE_A, BLOCK_SIZE_SCALE_B)
triton_scaled_mm(input, weight, scale_a, scale_b, out_dtype, bias, block_size_m, block_size_n, block_size_k, use_heuristic)

# python/sglang/srt/layers/quantization/fp8_utils.py
use_rowwise_torch_scaled_mm()
cutlass_fp8_supported()
normalize_e4m3fn_to_e4m3fnuz(weight, weight_scale, input_scale)
cutlass_block_fp8_supported()
dispatch_w8a8_block_fp8_linear()
flashinfer_gemm_w8a8_block_fp8_linear(input, weight, block_size, weight_scale, input_scale, bias)
cutlass_w8a8_block_fp8_linear_with_fallback(input, weight, block_size, weight_scale, input_scale, bias)
deepgemm_w8a8_block_fp8_linear_with_fallback(input, weight, block_size, weight_scale, input_scale, bias)
aiter_w8a8_block_fp8_linear(input, weight, block_size, weight_scale, input_scale, bias)
triton_w8a8_block_fp8_linear(input, weight, block_size, weight_scale, input_scale, bias)
dequant_mxfp4(w_block, w_scale, out_dtype)
input_to_float8(x, dtype)
block_quant_to_tensor_quant(x_q_block, x_s, block_size)
block_quant_dequant(x_q_block, x_s, block_size, dtype)
requant_weight_ue8m0_inplace(weight, weight_scale_inv, weight_block_size)
per_block_cast_to_fp8(x)
ceil_to_ue8m0(x)
channel_quant_to_tensor_quant(x_q_channel, x_s)
apply_fp8_linear(input, weight, weight_scale, input_scale, input_scale_ub, bias, cutlass_fp8_supported, use_per_token_if_dynamic, pad_output, compressed_tensor_quant)
can_auto_enable_marlin_fp8()

# python/sglang/srt/layers/quantization/fpgemm_fp8.py
  FBGEMMFp8Config.__init__(ignore_list, input_scale_ub)
  FBGEMMFp8Config.get_name(cls)
  FBGEMMFp8Config.get_supported_act_dtypes(cls)
  FBGEMMFp8Config.get_min_capability(cls)
  FBGEMMFp8Config.get_config_filenames(cls)
  FBGEMMFp8Config.from_config(cls, config, Any])
  FBGEMMFp8Config.get_quant_method(layer, prefix)
  FBGEMMFp8Config.get_scaled_act_names()
  FBGEMMFp8LinearMethod.__init__(quant_config)
  FBGEMMFp8LinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  FBGEMMFp8LinearMethod.process_weights_after_loading(layer)
  FBGEMMFp8LinearMethod.apply(layer, x, bias)

# python/sglang/srt/layers/quantization/gptq.py
check_marlin_format(hf_quant_cfg, Any])
gptq_marlin_moe_repack(b_q_weight, perm, size_k, size_n, num_bits)
  GPTQConfig.__init__(weight_bits, group_size, desc_act, lm_head_quantized, dynamic, Dict[str, Union[int, bool]]])
  GPTQConfig.__repr__()
  GPTQConfig.get_scaled_act_names()
  GPTQConfig.get_name(cls)
  GPTQConfig.get_supported_act_dtypes(cls)
  GPTQConfig.get_min_capability(cls)
  GPTQConfig.get_config_filenames(cls)
  GPTQConfig.from_config(cls, config, Any])
  GPTQConfig.get_quant_method(layer, prefix)
  GPTQMarlinConfig.__init__(weight_bits, group_size, desc_act, is_sym, lm_head_quantized, dynamic, Dict[str, Union[int, bool]]], full_config, Any])
  GPTQMarlinConfig.__repr__()
  GPTQMarlinConfig.get_scaled_act_names()
  GPTQMarlinConfig.get_name(cls)
  GPTQMarlinConfig.get_supported_act_dtypes(cls)
  GPTQMarlinConfig.get_min_capability(cls)
  GPTQMarlinConfig.get_config_filenames(cls)
  GPTQMarlinConfig.from_config(cls, config, Any])
  GPTQMarlinConfig.override_quantization_method(cls, hf_quant_cfg, user_quant)
  GPTQMarlinConfig.get_quant_method(layer, prefix)
  GPTQMarlinConfig.is_gptq_marlin_compatible(cls, quant_config, Any])
  GPTQLinearMethod.__init__(quant_config)
  GPTQLinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  GPTQLinearMethod.process_weights_after_loading(layer)
  GPTQLinearMethod.apply(layer, x, bias)
  GPTQMarlinLinearMethod.__init__(quant_config)
  GPTQMarlinLinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  GPTQMarlinLinearMethod.process_weights_after_loading(layer)
  GPTQMarlinLinearMethod.apply(layer, x, bias)
  GPTQMarlinMoEMethod.__init__(quant_config)
  GPTQMarlinMoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size_per_partition, params_dtype)
  GPTQMarlinMoEMethod.process_weights_after_loading(layer)
  GPTQMarlinMoEMethod.apply(layer, x, topk_output, moe_runner_config)

# python/sglang/srt/layers/quantization/int8_kernel.py
per_token_quant_int8(x, scale_dtype, cal_sum)
per_token_group_quant_int8(x, group_size, eps, dtype)
sglang_per_token_group_quant_int8(x, group_size, eps, dtype)
get_w8a8_block_int8_configs(N, K, block_n, block_k)
w8a8_block_int8_matmul(A, B, As, Bs, block_size, output_dtype)

# python/sglang/srt/layers/quantization/int8_utils.py
apply_w8a8_block_int8_linear(input, weight, block_size, weight_scale, input_scale, bias)
input_to_int8(x, dtype)
block_dequant(x_q_block, x_s, block_size)

# python/sglang/srt/layers/quantization/kv_cache.py
  BaseKVCacheMethod.__init__(quant_config)
  BaseKVCacheMethod.create_weights(layer)
  BaseKVCacheMethod.apply(layer)
  BaseKVCacheMethod.process_weights_after_loading(layer)

# python/sglang/srt/layers/quantization/marlin_utils.py
query_marlin_supported_quant_types(has_zp, include_fp_type, device_capability)
check_marlin_supported(quant_type, group_size, has_zp, device_capability)
verify_marlin_supported(quant_type, group_size, has_zp)
verify_marlin_supports_shape(output_size_per_partition, input_size_per_partition, input_size, group_size)
check_marlin_supports_shape(output_size_per_partition, input_size_per_partition, input_size, group_size)
check_marlin_supports_layer(layer, group_size)
check_moe_marlin_supports_layer(layer, group_size)
marlin_make_workspace(device, max_blocks_per_sm)
marlin_is_k_full(act_order, is_row_parallel)
marlin_repeat_scales_on_all_ranks(act_order, group_size, is_row_parallel)
marlin_make_empty_g_idx(device)
marlin_make_empty_zp(device)
marlin_sort_g_idx(g_idx)
get_scale_perms()
marlin_permute_scales(s, size_k, size_n, group_size)
marlin_permute_bias(s)
marlin_moe_permute_scales(s, size_k, size_n, group_size)
marlin_zero_points(zp, size_k, size_n, num_bits)
awq_to_marlin_zero_points(q_zp_packed, size_k, size_n, num_bits)
moe_awq_to_marlin_zero_points(q_zp_packed, size_k, size_n, num_bits)
maybe_warn_marlin_atomic_add(device, dtype)
maybe_warn_marlin_atomic_add_env()
should_use_atomic_add_reduce(m, n, k, device, dtype)
apply_gptq_marlin_linear(input, weight, weight_scale, weight_zp, g_idx, g_idx_sort_indices, workspace, wtype, output_size_per_partition, input_size_per_partition, is_k_full, bias, use_fp32_reduce)
apply_awq_marlin_linear(input, weight, weight_scale, weight_zp, g_idx, g_idx_sort_indices, workspace, quant_type, output_size_per_partition, input_size_per_partition, bias, use_fp32_reduce)
  MarlinConfig.__init__(group_size, lm_head_quantized)
  MarlinConfig.__repr__()
  MarlinConfig.get_name(cls)
  MarlinConfig.get_supported_act_dtypes(cls)
  MarlinConfig.get_min_capability(cls)
  MarlinConfig.get_config_filenames(cls)
  MarlinConfig.from_config(cls, config, Any])
  MarlinConfig.override_quantization_method(cls, hf_quant_cfg, user_quant)
  MarlinConfig.get_quant_method(layer, prefix)
  MarlinLinearMethod.__init__(quant_config)
  MarlinLinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  MarlinLinearMethod.process_weights_after_loading(layer)
  MarlinLinearMethod.apply(layer, x, bias)

# python/sglang/srt/layers/quantization/marlin_utils_fp8.py
fp8_fused_exponent_bias_into_scales(scales)
apply_fp8_marlin_linear(input, weight, weight_scale, workspace, size_n, size_k, bias, use_fp32_reduce)
prepare_fp8_layer_for_marlin(layer, size_k_first)
prepare_moe_fp8_layer_for_marlin(layer, size_k_first)
pack_fp8_to_int32(fp8_tensor, size_k_first)
marlin_quant_fp8_torch(weight, group_size)

# python/sglang/srt/layers/quantization/modelopt_quant.py
  ModelOptFp8Config.__init__(is_checkpoint_fp8_serialized, kv_cache_quant_method, exclude_modules)
  ModelOptFp8Config.get_name(cls)
  ModelOptFp8Config.get_supported_act_dtypes(cls)
  ModelOptFp8Config.get_min_capability(cls)
  ModelOptFp8Config.get_config_filenames(cls)
  ModelOptFp8Config.from_config(cls, config, Any])
  ModelOptFp8Config.get_quant_method(layer, prefix)
  ModelOptFp8Config.get_scaled_act_names()
  ModelOptFp8LinearMethod.__init__(quant_config)
  ModelOptFp8LinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, params_dtype)
  ModelOptFp8LinearMethod.process_weights_after_loading(layer)
  ModelOptFp8LinearMethod.apply(layer, x, bias)
  ModelOptFp8KVCacheMethod.__init__(quant_config)
  ModelOptFp8MoEMethod.__init__(quant_config)
  ModelOptFp8MoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype)
  ModelOptFp8MoEMethod.process_weights_after_loading(layer)
  ModelOptFp8MoEMethod.apply(layer, x, topk_output, moe_runner_config)
  ModelOptFp4Config.__init__(is_checkpoint_nvfp4_serialized, kv_cache_quant_algo, group_size, exclude_modules)
  ModelOptFp4Config.get_name(cls)
  ModelOptFp4Config.get_supported_act_dtypes(cls)
  ModelOptFp4Config.get_min_capability(cls)
  ModelOptFp4Config.get_config_filenames(cls)
  ModelOptFp4Config.from_config(cls, config, Any])
  ModelOptFp4Config.is_layer_excluded(prefix, exclude_modules)
  ModelOptFp4Config.get_quant_method(layer, prefix)
  ModelOptFp4Config.get_scaled_act_names()
  ModelOptFp4LinearMethod.__init__(quant_config)
  ModelOptFp4LinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  ModelOptFp4LinearMethod.process_weights_after_loading(layer)
  ModelOptFp4LinearMethod.apply(layer, x, bias)
  ModelOptNvFp4FusedMoEMethod.__init__(quant_config)
  ModelOptNvFp4FusedMoEMethod.enable_flashinfer_cutlass_moe()
  ModelOptNvFp4FusedMoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size_per_partition, params_dtype)
  ModelOptNvFp4FusedMoEMethod.swizzle_blockscale(scale)
  ModelOptNvFp4FusedMoEMethod.prepare_static_weights_for_kernel(gemm1_weights, gemm2_weights, gemm1_scales_linear_fp4_bytes, gemm2_scales_linear_fp4_bytes, hidden_size, intermediate_size, num_experts)
  ModelOptNvFp4FusedMoEMethod.process_weights_after_loading(layer)
  ModelOptNvFp4FusedMoEMethod.load_up_proj_weight_first()
  ModelOptNvFp4FusedMoEMethod.apply(layer, x, topk_output, moe_runner_config)

# python/sglang/srt/layers/quantization/moe_wna16.py
get_weight_perm(num_bits)
  MoeWNA16Config.__init__(linear_quant_method, weight_bits, group_size, has_zp, lm_head_quantized, modules_to_not_convert, full_config, Any])
  MoeWNA16Config.get_name(cls)
  MoeWNA16Config.get_supported_act_dtypes(cls)
  MoeWNA16Config.get_min_capability(cls)
  MoeWNA16Config.get_config_filenames(cls)
  MoeWNA16Config.get_scaled_act_names()
  MoeWNA16Config.from_config(cls, config, Any])
  MoeWNA16Config.override_quantization_method(cls, hf_quant_cfg, user_quant)
  MoeWNA16Config.is_moe_wna16_compatible(cls, quant_config, Any])
  MoeWNA16Config.get_quant_method(layer, prefix)
is_layer_skipped_quant(prefix, modules_to_not_convert)
  MoeWNA16Method.__init__(quant_config)
  MoeWNA16Method.create_weights(layer, num_experts, hidden_size, intermediate_size_per_partition, params_dtype)
  MoeWNA16Method.apply(layer, x, topk_output, moe_runner_config)
  MoeWNA16Method.get_weight_loader(layer, weight_loader)

# python/sglang/srt/layers/quantization/mxfp4.py
  Mxfp4Config.__init__(ignored_layers, is_checkpoint_mxfp4_serialized)
  Mxfp4Config.from_config(cls, config)
  Mxfp4Config.get_min_capability(cls)
  Mxfp4Config.get_name(cls)
  Mxfp4Config.get_supported_act_dtypes(cls)
  Mxfp4Config.get_config_filenames(cls)
  Mxfp4Config.is_static_cfg()
  Mxfp4Config.get_quant_method(layer, prefix)
  Mxfp4Config.get_scaled_act_names()
  Mxfp4MoEMethod.__init__(prefix)
  Mxfp4MoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype, with_bias)
  Mxfp4MoEMethod.process_weights_after_loading(layer)
  Mxfp4MoEMethod.apply(layer, x, topk_output, moe_runner_config)
  Mxfp4DynamicQuantMoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size_per_partition, params_dtype)
  Mxfp4DynamicQuantMoEMethod.mxfp4_quantize(w)
  Mxfp4DynamicQuantMoEMethod.process_weights_after_loading(layer)
  Mxfp4DynamicQuantMoEMethod.apply(layer, x, topk_output, moe_runner_config)

# python/sglang/srt/layers/quantization/mxfp4_tensor.py
  MXFP4QuantizeUtil.quantize(cls, input, block_size)
  MXFP4QuantizeUtil.dequantize(cls, quantized_data, dtype, scale, block_sizes)

# python/sglang/srt/layers/quantization/petit.py
  PetitNvFp4Config.__init__(is_checkpoint_nvfp4_serialized, kv_cache_quant_algo, group_size, exclude_modules)
  PetitNvFp4Config.get_name(cls)
  PetitNvFp4Config.get_supported_act_dtypes(cls)
  PetitNvFp4Config.get_min_capability(cls)
  PetitNvFp4Config.get_config_filenames(cls)
  PetitNvFp4Config.from_config(cls, config, Any])
  PetitNvFp4Config.override_quantization_method(cls, hf_quant_cfg, user_quant)
  PetitNvFp4Config.is_petit_nvfp4_compatible(cls, quant_config, Any])
  PetitNvFp4Config.is_layer_excluded(prefix, exclude_modules)
  PetitNvFp4Config.get_quant_method(layer, prefix)
  PetitNvFp4Config.get_scaled_act_names()
  PetitNvFp4LinearMethod.__init__(quant_config)
  PetitNvFp4LinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  PetitNvFp4LinearMethod.process_weights_after_loading(layer)
  PetitNvFp4LinearMethod.apply(layer, x, bias)

# python/sglang/srt/layers/quantization/petit_utils.py
prepare_nvfp4_layer_for_petit(layer)
apply_petit_nvfp4_linear(input, weight, weight_scale, weight_scale_2, size_n, size_k, bias)
verify_petit_nvfp4_supported(quant_method, group_size)
prepare_nvfp4_layer_for_petit(layer)
apply_petit_nvfp4_linear(input, weight, weight_scale, weight_scale_2, size_n, size_k, bias)

# python/sglang/srt/layers/quantization/qoq.py
  QoQConfig.__init__(weight_bits, group_size)
  QoQConfig.__repr__()
  QoQConfig.get_supported_act_dtypes(cls)
  QoQConfig.get_min_capability(cls)
  QoQConfig.get_name(cls)
  QoQConfig.get_config_filenames(cls)
  QoQConfig.from_config(cls, config, Any])
  QoQConfig.get_quant_method(layer, prefix)
  QoQConfig.get_scaled_act_names()
  QoQLinearMethod.__init__(quant_config)
  QoQLinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  QoQLinearMethod.process_weights_after_loading(layer)
  QoQLinearMethod.apply(layer, x, bias)

# python/sglang/srt/layers/quantization/quark/quark.py
  QuarkConfig.__init__(quant_config, Any], kv_cache_group, kv_cache_config, Any]], pack_method)
  QuarkConfig.get_linear_method()
  QuarkConfig.get_supported_act_dtypes(cls)
  QuarkConfig.get_min_capability(cls)
  QuarkConfig.get_name()
  QuarkConfig.get_quant_method(layer, prefix)
  QuarkConfig.from_config(cls, config, Any])
  QuarkConfig.get_config_filenames(cls)
  QuarkConfig.get_scheme(layer, layer_name)
  QuarkConfig.get_scaled_act_names()
  QuarkLinearMethod.__init__(quantization_config)
  QuarkLinearMethod.process_weights_after_loading(layer)
  QuarkLinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  QuarkLinearMethod.apply(layer, x, bias)
  QuarkKVCacheMethod.__init__(quant_config)
  QuarkKVCacheMethod.validate_kv_cache_config(kv_cache_config, Any]])

# python/sglang/srt/layers/quantization/quark/quark_moe.py
  QuarkMoEMethod.__new__(cls)
  QuarkMoEMethod.get_moe_method(quant_config, module, layer_name)
  QuarkW4A4MXFp4MoEMethod.__init__(weight_config, Any], input_config, Any])
  QuarkW4A4MXFp4MoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size_per_partition, params_dtype)
  QuarkW4A4MXFp4MoEMethod.process_weights_after_loading(layer)
  QuarkW4A4MXFp4MoEMethod.apply(layer, x, topk_output, moe_runner_config)

# python/sglang/srt/layers/quantization/quark/schemes/quark_scheme.py
  QuarkScheme.get_min_capability(cls)
  QuarkScheme.create_weights()
  QuarkScheme.apply_weights(layer, x, bias)
  QuarkScheme.process_weights_after_loading(layer)

# python/sglang/srt/layers/quantization/quark/schemes/quark_w4a4_mxfp4.py
  QuarkW4A4MXFP4.__init__(weight_quant_spec, Any], input_quant_spec, Any])
  QuarkW4A4MXFP4.get_min_capability(cls)
  QuarkW4A4MXFP4.process_weights_after_loading(layer)
  QuarkW4A4MXFP4.create_weights(layer, output_partition_sizes, input_size_per_partition, params_dtype, weight_loader)
  QuarkW4A4MXFP4.apply_weights(layer, x, bias)

# python/sglang/srt/layers/quantization/quark/utils.py
deep_compare(dict1, dict2)
should_ignore_layer(layer_name, ignore, fused_mapping, list[str]])
check_equal_or_regex_match(layer_name, targets)

# python/sglang/srt/layers/quantization/unquant.py
  UnquantizedEmbeddingMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  UnquantizedEmbeddingMethod.apply(layer, x, bias)
  UnquantizedEmbeddingMethod.embedding(layer, input_)
  UnquantizedLinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  UnquantizedLinearMethod.process_weights_after_loading(layer)
  UnquantizedLinearMethod.apply(layer, x, bias)
  UnquantizedFusedMoEMethod.__init__(use_triton_kernels)
  UnquantizedFusedMoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype, with_bias)
  UnquantizedFusedMoEMethod.process_weights_after_loading(layer)
  UnquantizedFusedMoEMethod.apply(layer, x, topk_output, moe_runner_config)
  UnquantizedFusedMoEMethod.forward_cuda(layer, x, topk_output, moe_runner_config)
  UnquantizedFusedMoEMethod.forward_cpu(layer, x, topk_output, moe_runner_config)
  UnquantizedFusedMoEMethod.forward_npu(layer, x, topk_output, moe_runner_config)
  UnquantizedFusedMoEMethod.forward_tpu()

# python/sglang/srt/layers/quantization/utils.py
get_scalar_types()
is_layer_skipped(prefix, ignored_layers, fused_mapping, List[str]])
per_tensor_dequantize(tensor, inv_scale, torch.Tensor])
all_close_1d(x)
convert_to_channelwise(weight_scale, logical_widths)
requantize_with_max_scale(weight, weight_scale, logical_widths)
update_tensor_inplace(old, new)
replace_parameter(mod, name, new, torch.nn.Parameter])
assert_fp8_all_close(a, b)
override_config(config, prefix)
get_dynamic_override(config, layer_name, key, default_value, bool, None])
get_linear_quant_method(config, layer, prefix, linear_method_cls)
get_pack_factor(num_bits)
permute_rows(q_w, w_ref, group_size, test_perm)
pack_cols(q_w, num_bits, size_k, size_n)
pack_rows(q_w, num_bits, size_k, size_n)
unpack_cols(packed_q_w, num_bits, size_k, size_n)
quantize_weights(w, quant_type, group_size, zero_points, ref_zero_points_after_scales)
gptq_quantize_weights(w, quant_type, group_size, act_order, test_perm)
sort_weights(q_w, g_idx)

# python/sglang/srt/layers/quantization/w4afp8.py
  W4AFp8Config.__init__(is_checkpoint_fp8_serialized, is_checkpoint_w4afp8_serialized, linear_activation_scheme, moe_activation_scheme, ignored_layers, weight_block_size, group_size)
  W4AFp8Config.get_name(cls)
  W4AFp8Config.get_supported_act_dtypes(cls)
  W4AFp8Config.get_min_capability(cls)
  W4AFp8Config.get_config_filenames(cls)
  W4AFp8Config.from_config(cls, config, Any])
  W4AFp8Config.get_quant_method(layer, prefix)
  W4AFp8Config.get_scaled_act_names()
  W4AFp8MoEMethod.__init__(quant_config)
  W4AFp8MoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype)
  W4AFp8MoEMethod.process_weights_after_loading(layer)
  W4AFp8MoEMethod.apply(layer, x, topk_output, moe_runner_config)

# python/sglang/srt/layers/quantization/w8a8_fp8.py
  W8A8Fp8Config.__init__(is_checkpoint_fp8_serialized)
  W8A8Fp8Config.get_supported_act_dtypes(cls)
  W8A8Fp8Config.get_min_capability(cls)
  W8A8Fp8Config.get_name()
  W8A8Fp8Config.get_config_filenames(cls)
  W8A8Fp8Config.from_config(cls, config, Any])
  W8A8Fp8Config.get_quant_method(layer, prefix)
  W8A8Fp8Config.get_scaled_act_names()
  W8A8Fp8LinearMethod.__init__(quantization_config)
  W8A8Fp8LinearMethod.process_weights_after_loading(layer)
  W8A8Fp8LinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  W8A8Fp8LinearMethod.apply(layer, x, bias)
  W8A8FP8MoEMethod.__init__(quant_config)
  W8A8FP8MoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype)
  W8A8FP8MoEMethod.process_weights_after_loading(layer)
  W8A8FP8MoEMethod.apply(layer, x, topk_output, moe_runner_config)

# python/sglang/srt/layers/quantization/w8a8_int8.py
npu_wrapper_rmsnorm_init(func)
npu_wrapper_rmsnorm_forward(func)
npu_fused_experts(hidden_states, w13, w13_scale, w2, w2_scale, topk_weights, topk_ids, top_k)
  W8A8Int8Config.__init__(quant_config, Any])
  W8A8Int8Config.get_supported_act_dtypes(cls)
  W8A8Int8Config.get_min_capability(cls)
  W8A8Int8Config.get_name()
  W8A8Int8Config.get_config_filenames(cls)
  W8A8Int8Config.from_config(cls, config, Any])
  W8A8Int8Config.get_quant_method(layer, prefix)
  W8A8Int8Config.is_layer_skipped(prefix, fused_mapping, List[str]])
  W8A8Int8Config.get_scaled_act_names()
  W8A8Int8LinearMethod.__init__(quantization_config)
  W8A8Int8LinearMethod.process_weights_after_loading(layer)
  W8A8Int8LinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  W8A8Int8LinearMethod.apply(layer, x, bias)
  W8A8Int8MoEMethod.__init__(quant_config)
  W8A8Int8MoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype)
  W8A8Int8MoEMethod.process_weights_after_loading(layer)
  W8A8Int8MoEMethod.apply(layer, x, topk_output, moe_runner_config)
  NPU_W8A8LinearMethodImpl.__init__()
  NPU_W8A8LinearMethodImpl.get_weight(input_size, output_size, params_dtype)
  NPU_W8A8LinearMethodImpl.get_pertensor_param(params_dtype)
  NPU_W8A8LinearMethodImpl.get_perchannel_param(output_size, params_dtype)
  NPU_W8A8LinearMethodImpl.apply(layer, x, bias)
  NPU_W8A8LinearMethodImpl.process_weights_after_loading(layer)
  NPU_W8A8LinearMethodMTImpl.__init__()
  NPU_W8A8LinearMethodMTImpl.get_weight(input_size, output_size, params_dtype)
  NPU_W8A8LinearMethodMTImpl.get_pertensor_param(params_dtype)
  NPU_W8A8LinearMethodMTImpl.get_perchannel_param(output_size, params_dtype)
  NPU_W8A8LinearMethodMTImpl.apply(layer, x, bias)
  NPU_W8A8LinearMethodMTImpl.process_weights_after_loading(layer)
  NPU_W8A8LinearMethod.__init__(quantization_config)
  NPU_W8A8LinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  NPU_W8A8LinearMethod.process_weights_after_loading(layer)
  NPU_W8A8LinearMethod.apply(layer, x, bias)
  NPU_W8A8DynamicLinearMethodImpl.__init__()
  NPU_W8A8DynamicLinearMethodImpl.get_weight(input_size, output_size, params_dtype)
  NPU_W8A8DynamicLinearMethodImpl.get_pertensor_param(params_dtype)
  NPU_W8A8DynamicLinearMethodImpl.get_perchannel_param(output_size, params_dtype)
  NPU_W8A8DynamicLinearMethodImpl.apply(layer, x, bias, tp_rank)
  NPU_W8A8DynamicLinearMethodImpl.process_weights_after_loading(layer)
  NPU_W8A8DynamicLinearMethod.__init__(quantization_config)
  NPU_W8A8DynamicLinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  NPU_W8A8DynamicLinearMethod.process_weights_after_loading(layer)
  NPU_W8A8DynamicLinearMethod.apply(layer, x, bias)
  NPU_W8A8MoEMethod.__init__(quantization_config)
  NPU_W8A8MoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype)
  NPU_W8A8MoEMethod.process_weights_after_loading(layer)
  NPU_W8A8MoEMethod.apply(layer, x, topk_output, moe_runner_config)

# python/sglang/srt/layers/radix_attention.py
  RadixAttention.__init__(num_heads, head_dim, scaling, num_kv_heads, layer_id, logit_cap, v_head_dim, sliding_window_size, is_cross_attention, pos_encoding_mode, logit_capping_method, quant_config, attn_type, use_irope, prefix)
  RadixAttention.forward(q, k, v, forward_batch, save_kv_cache)

# python/sglang/srt/layers/rotary_embedding.py
  RotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, dtype)
  RotaryEmbedding.forward_native(positions, query, key, offsets)
  RotaryEmbedding.forward_npu(positions, query, key, offsets)
  RotaryEmbedding.forward_cpu(positions, query, key, offsets)
  RotaryEmbedding.forward_cuda(positions, query, key, offsets, fused_set_kv_buffer_arg)
  RotaryEmbedding.extra_repr()
  LinearScalingRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, scaling_factors, float], dtype)
  LinearScalingRotaryEmbedding.scaling_factor_to_offset()
  DynamicNTKScalingRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, scaling_factor, dtype)
  YaRNScalingRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, scaling_factor, dtype)
  Phi3LongRoPEScaledRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, original_max_position_embeddings, base, is_neox_style, dtype, short_factor, long_factor, short_mscale, long_mscale)
  Phi3LongRoPEScaledRotaryEmbedding.forward(positions, query, key, offsets)
yarn_get_mscale(scale, mscale)
  DeepseekScalingRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, scaling_factor, dtype)
  DeepseekScalingRotaryEmbedding.forward_native(positions, query, key, offsets)
  DeepseekScalingRotaryEmbedding.forward_npu(positions, query, key, offsets)
  DeepseekScalingRotaryEmbedding.forward_cpu(positions, query, key, offsets)
  Llama3RotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, dtype, scaling_factor, low_freq_factor, high_freq_factor, orig_max_position)
  Llama4VisionRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, dtype)
  Llama4VisionRotaryEmbedding.forward(query, key)
  DynamicNTKAlphaRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, scaling_alpha, dtype)
  MRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, dtype, mrope_section)
  MRotaryEmbedding.forward(positions, query, key)
  MRotaryEmbedding.get_rope_index(spatial_merge_size, image_token_id, video_token_id, vision_start_token_id, model_type, tokens_per_second, input_ids, image_grid_thw, video_grid_thw, second_per_grid_ts)
  MRotaryEmbedding.get_rope_index_glm4v(input_ids, hf_config, image_grid_thw, torch.Tensor], video_grid_thw, torch.Tensor], attention_mask)
  MRotaryEmbedding.get_next_input_positions(mrope_position_delta, context_len, seq_len)
  DualChunkRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, dtype, chunk_size, local_size)
  DualChunkRotaryEmbedding.forward(positions, query, key, offsets)
  DualChunkRotaryEmbedding.extra_repr()
get_rope(head_size, rotary_dim, max_position, base, is_neox_style, rope_scaling, Any]], dtype, partial_rotary_factor, dual_chunk_attention_config, Any]])
rotate_half(x)
apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim)
get_rope_cpu(head_size, rotary_dim, max_position, base, is_neox_style, rope_scaling, Any]], dtype, partial_rotary_factor, device)
get_rope_wrapper(head_size, rotary_dim, max_position, base, is_neox_style, rope_scaling, Any]], dtype, partial_rotary_factor, device)

# python/sglang/srt/layers/sampler.py
  Sampler.__init__()
  Sampler.forward(logits_output, sampling_info, return_logprob, top_logprobs_nums, token_ids_logprobs)
top_k_top_p_min_p_sampling_from_probs_torch(probs, top_ks, top_ps, min_ps, need_min_p_sampling)
sampling_from_probs_torch(probs)
top_p_normalize_probs_torch(probs, top_ps)
get_top_logprobs(logprobs, top_logprobs_nums)
get_token_ids_logprobs(logprobs, token_ids_logprobs)
apply_custom_logit_processor(logits, sampling_batch_info, num_tokens_in_batch)

# python/sglang/srt/layers/torchao_utils.py
get_gemlite_cache_path()
save_gemlite_cache(print_error)
proj_filter(module, fqn)
apply_torchao_config_to_model(model, torchao_config, filter_fn)

# python/sglang/srt/layers/utils.py
get_layer_id(weight_name)
  PPMissingLayer.__init__()
  PPMissingLayer.forward()

# python/sglang/srt/layers/vocab_parallel_embedding.py
pad_vocab_size(vocab_size, pad_to)
vocab_range_from_per_partition_vocab_size(per_partition_vocab_size, rank, offset)
vocab_range_from_global_vocab_size(global_vocab_size, rank, world_size, offset)
  VocabParallelEmbeddingShardIndices.num_org_elements()
  VocabParallelEmbeddingShardIndices.num_added_elements()
  VocabParallelEmbeddingShardIndices.num_org_elements_padded()
  VocabParallelEmbeddingShardIndices.num_added_elements_padded()
  VocabParallelEmbeddingShardIndices.num_org_vocab_padding()
  VocabParallelEmbeddingShardIndices.num_added_vocab_padding()
  VocabParallelEmbeddingShardIndices.num_elements_padded()
  VocabParallelEmbeddingShardIndices.__post_init__()
get_masked_input_and_mask(input_, org_vocab_start_index, org_vocab_end_index, num_org_vocab_padding, added_vocab_start_index, added_vocab_end_index)
  VocabParallelEmbedding.__init__(num_embeddings, embedding_dim)
  VocabParallelEmbedding.get_sharded_to_full_mapping()
  VocabParallelEmbedding.weight_loader(param, loaded_weight)
  VocabParallelEmbedding.forward(input_)
  VocabParallelEmbedding.extra_repr()
  ParallelLMHead.__init__(num_embeddings, embedding_dim)
  ParallelLMHead.tie_weights(embed_tokens)
  ParallelLMHead.forward(input_)

# python/sglang/srt/lora/backend/base_backend.py
  BaseLoRABackend.__init__(name, batch_info)
  BaseLoRABackend.run_lora_a_sgemm(x, weights)
  BaseLoRABackend.run_lora_b_sgemm(x, weights)
  BaseLoRABackend.run_qkv_lora(x, qkv_lora_a, qkv_lora_b, Tuple[torch.Tensor]])
  BaseLoRABackend.run_gate_up_lora(x, gate_up_lora_a, gate_up_lora_b, Tuple[torch.Tensor]])
  BaseLoRABackend.set_batch_info(batch_info)
get_backend_from_name(name)

# python/sglang/srt/lora/backend/triton_backend.py
  TritonLoRABackend.__init__(name, batch_info)
  TritonLoRABackend.run_lora_a_sgemm(x, weights)
  TritonLoRABackend.run_lora_b_sgemm(x, weights, base_output)
  TritonLoRABackend.run_qkv_lora(x, qkv_lora_a, qkv_lora_b, output_offset, max_qkv_out_dim, base_output)
  TritonLoRABackend.run_gate_up_lora(x, gate_up_lora_a, gate_up_lora_b, base_output)

# python/sglang/srt/lora/layers.py
  BaseLayerWithLoRA.__init__(base_layer, lora_backend)
  BaseLayerWithLoRA.forward(x)
  BaseLayerWithLoRA.set_lora_info()
  BaseLayerWithLoRA.slice_lora_a_weights(A, tp_rank)
  BaseLayerWithLoRA.slice_lora_b_weights(B, tp_rank)
  VocabParallelEmbeddingWithLoRA.__init__(base_layer, lora_backend)
  ColumnParallelLinearWithLoRA.__init__(base_layer, lora_backend)
  ColumnParallelLinearWithLoRA.set_lora_info(A_buffer, B_buffer)
  ColumnParallelLinearWithLoRA.apply_lora(base_output, x)
  ColumnParallelLinearWithLoRA.forward(input_)
  ColumnParallelLinearWithLoRA.slice_lora_a_weights(A, tp_rank)
  ColumnParallelLinearWithLoRA.slice_lora_b_weights(B, tp_rank)
  MergedColumnParallelLinearWithLoRA.__init__(base_layer, lora_backend)
  MergedColumnParallelLinearWithLoRA.set_lora_info(A_buffer, B_buffer)
  MergedColumnParallelLinearWithLoRA.apply_lora(base_output, x)
  MergedColumnParallelLinearWithLoRA.slice_lora_a_weights(A, tp_rank)
  MergedColumnParallelLinearWithLoRA.slice_lora_b_weights(B, tp_rank)
  QKVParallelLinearWithLoRA.__init__(base_layer, lora_backend)
  QKVParallelLinearWithLoRA.set_lora_info(A_buffer_qkv, B_buffer_qkv)
  QKVParallelLinearWithLoRA.apply_lora(base_output, x)
  QKVParallelLinearWithLoRA.slice_lora_a_weights(A, tp_rank)
  QKVParallelLinearWithLoRA.slice_lora_b_weights(B, tp_rank)
  RowParallelLinearWithLoRA.__init__(base_layer, lora_backend)
  RowParallelLinearWithLoRA.set_lora_info(A_buffer, B_buffer)
  RowParallelLinearWithLoRA.apply_lora(base_output, x)
  RowParallelLinearWithLoRA.forward(input_, skip_all_reduce)
  RowParallelLinearWithLoRA.slice_lora_a_weights(A, tp_rank)
  RowParallelLinearWithLoRA.slice_lora_b_weights(B, tp_rank)
get_lora_layer(layer, lora_backend)

# python/sglang/srt/lora/lora.py
  LoRALayer.__init__(config, base_hf_config)
  LoRAAdapter.__init__(uid, config, base_hf_config, load_config, lora_backend)
  LoRAAdapter.initialize_weights()
  LoRAAdapter.normalize_qkv_proj(weight_names, weights, torch.Tensor])
  LoRAAdapter.normalize_gate_up_proj(weight_names, weights, torch.Tensor])

# python/sglang/srt/lora/lora_config.py
  LoRAConfig.__init__(path)
  LoRAConfig.get_lora_config(dummy)

# python/sglang/srt/lora/lora_manager.py
  LoRAManager.__init__(base_model, base_hf_config, max_loras_per_batch, load_config, dtype, lora_backend, tp_size, tp_rank, max_lora_rank, target_modules, lora_paths)
  LoRAManager.init_cuda_graph_batch_info(max_bs_in_cuda_graph)
  LoRAManager.create_lora_update_result(success, error_message)
  LoRAManager.load_lora_adapter(lora_ref)
  LoRAManager.validate_new_adapter(lora_config, lora_ref)
  LoRAManager.unload_lora_adapter(lora_ref)
  LoRAManager.validate_lora_batch(lora_ids)
  LoRAManager.prepare_lora_batch(forward_batch)
  LoRAManager.update_lora_info()
  LoRAManager.init_state(max_lora_rank, target_modules, lora_paths)
  LoRAManager.init_lora_adapters(lora_paths)
  LoRAManager.init_lora_shapes(max_lora_rank, target_modules)
  LoRAManager.load_lora_weights(lora_ref)
  LoRAManager.init_memory_pool()
  LoRAManager.set_lora_module(module_name, module)
  LoRAManager.init_lora_modules()

# python/sglang/srt/lora/lora_registry.py
  LoRARef.__post_init__()
  LoRARef.__str__()
  LoRARegistry.__init__(lora_paths)
  LoRARegistry.register(lora_ref)
  LoRARegistry.unregister(lora_name)
  LoRARegistry.acquire(lora_name, List[str]])
  LoRARegistry.release(lora_id, List[str]])
  LoRARegistry.wait_for_unload(lora_id)
  LoRARegistry.num_registered_loras()

# python/sglang/srt/lora/mem_pool.py
  EmptySlot.__repr__()
  EmptySlot.__new__(cls)
  LoRAMemoryPool.__init__(base_hf_config, max_loras_per_batch, dtype, tp_size, tp_rank, max_lora_rank, target_modules, base_model)
  LoRAMemoryPool.can_support(config, Iterable[LoRAConfig]])
  LoRAMemoryPool.get_lora_A_shape(module_name, base_model, max_lora_dim)
  LoRAMemoryPool.get_lora_B_shape(module_name, base_model, max_lora_dim)
  LoRAMemoryPool.init_buffers(base_model)
  LoRAMemoryPool.prepare_lora_batch(cur_uids, lora_adapters, LoRAAdapter], lora_modules, BaseLayerWithLoRA]], lora_refs, LoRARef])
  LoRAMemoryPool.load_lora_weight_to_buffer(uid, buffer_id, lora_adapter, lora_modules, BaseLayerWithLoRA]])
  LoRAMemoryPool.get_tensor(target_module, layer_id, lora_type)
  LoRAMemoryPool.get_buffer_id(lora_uid)

# python/sglang/srt/lora/triton_ops/gate_up_lora_b.py
gate_up_lora_b_fwd(x, gate_up_lora_b, batch_info, output_dim, base_output)

# python/sglang/srt/lora/triton_ops/qkv_lora_b.py
qkv_lora_b_fwd(x, qkv_lora_b, batch_info, output_offset, max_qkv_out_dim, base_output)

# python/sglang/srt/lora/triton_ops/sgemm_lora_a.py
sgemm_lora_a_fwd(x, weights, batch_info, stack_num)

# python/sglang/srt/lora/triton_ops/sgemm_lora_b.py
sgemm_lora_b_fwd(x, weights, batch_info, base_output)

# python/sglang/srt/lora/utils.py
get_layer_id(name)
get_hidden_dim(module_name, config, base_model)
get_normalized_target_modules(target_modules)
get_stacked_multiply(module_name)
get_target_module_name(full_module_name, target_modules)

# python/sglang/srt/managers/cache_controller.py
  LayerDoneCounter.__init__(num_layers)
  LayerDoneCounter.next_producer()
  LayerDoneCounter.update_producer()
  LayerDoneCounter.set_consumer(index)
  LayerDoneCounter.increment()
  LayerDoneCounter.wait_until(threshold)
  LayerDoneCounter.reset()
  CacheOperation.__init__(host_indices, device_indices, node_id, priority)
  CacheOperation.merge(other)
  CacheOperation.split(factor)
  CacheOperation.__lt__(other)
  TransferBuffer.__init__(stop_event, buffer_count, max_buffer_size)
  TransferBuffer.full()
  TransferBuffer.empty()
  TransferBuffer.put(item, block, timeout)
  TransferBuffer.get(block, timeout)
  TransferBuffer.clear()
  StorageOperation.__init__(host_indices, token_ids, last_hash, hash_value)
  StorageOperation.__lt__(other)
  PrefetchOperation.__init__(request_id, host_indices, token_ids, last_hash)
  PrefetchOperation.increment(num_tokens)
  PrefetchOperation.mark_done()
  PrefetchOperation.is_done()
  HiCacheController.__init__(token_to_kv_pool_allocator, mem_pool_host, page_size, tp_group, load_cache_event, write_policy, io_backend, storage_backend, prefetch_threshold, model_name, storage_backend_extra_config)
  HiCacheController.reset()
  HiCacheController.write(device_indices, priority, node_id)
  HiCacheController.load(host_indices, priority, node_id)
  HiCacheController.move_indices(host_indices, device_indices)
  HiCacheController.write_thread_func_direct()
  HiCacheController.load_thread_func_layer_by_layer()
  HiCacheController.evict_device(device_indices, host_indices)
  HiCacheController.evict_host(host_indices, backup_only)
  HiCacheController.prefetch(request_id, host_indices, new_input_tokens, last_hash)
  HiCacheController.terminate_prefetch(operation)
  HiCacheController.is_mooncake_backend()
  HiCacheController.prefetch_io_aux_func()
  HiCacheController.prefetch_rate_limit_check()
  HiCacheController.prefetch_thread_func()
  HiCacheController.write_storage(host_indices, token_ids, hash_value)
  HiCacheController.backup_thread_func()

# python/sglang/srt/managers/data_parallel_controller.py
  LoadBalanceMethod.from_str(cls, method)
  DataParallelController.__init__(server_args, port_args, dp_balance_meta)
  DataParallelController.launch_dp_schedulers(server_args, port_args)
  DataParallelController.launch_tensor_parallel_group_thread(server_args, port_args, base_gpu_id, dp_rank, ready_event)
  DataParallelController.launch_dp_attention_schedulers(server_args, port_args)
  DataParallelController.launch_tensor_parallel_group(server_args, port_args, base_gpu_id, dp_rank)
  DataParallelController.round_robin_scheduler(req)
  DataParallelController.shortest_queue_scheduler(input_requests)
  DataParallelController.minimum_tokens_scheduler(req)
  DataParallelController.event_loop()
run_data_parallel_controller_process(server_args, port_args, pipe_writer)

# python/sglang/srt/managers/detokenizer_manager.py
  DetokenizerManager.__init__(server_args, port_args)
  DetokenizerManager.event_loop()
  DetokenizerManager.trim_matched_stop(output, List[int]], finished_reason, no_stop_trim)
  DetokenizerManager.handle_batch_embedding_out(recv_obj)
  DetokenizerManager.handle_batch_token_id_out(recv_obj)
  DetokenizerManager.handle_multimodal_decode_req(recv_obj)
  DetokenizerManager.handle_freeze_gc_req(recv_req)
  LimitedCapacityDict.__init__(capacity)
  LimitedCapacityDict.__setitem__(key, value)
run_detokenizer_process(server_args, port_args)

# python/sglang/srt/managers/io_struct.py
  GenerateReqInput.contains_mm_input()
  GenerateReqInput.normalize_batch_and_arguments()
  GenerateReqInput.regenerate_rid()
  GenerateReqInput.__getitem__(i)
  BatchTokenizedGenerateReqInput.__len__()
  BatchTokenizedGenerateReqInput.__getitem__(i)
  BatchTokenizedGenerateReqInput.__iter__()
  EmbeddingReqInput.normalize_batch_and_arguments()
  EmbeddingReqInput.regenerate_rid()
  EmbeddingReqInput.contains_mm_input()
  EmbeddingReqInput.__getitem__(i)
  BatchTokenizedEmbeddingReqInput.__len__()
  BatchTokenizedEmbeddingReqInput.__getitem__(i)
  BatchTokenizedEmbeddingReqInput.__iter__()
  LoadLoRAAdapterReqInput.to_ref()
  UnloadLoRAAdapterReqInput.to_ref()

# python/sglang/srt/managers/mm_utils.py
  TransportProxyTensor.__new__(cls, data, name, fields, Any]], transport_mode)
  TransportProxyTensor.__getstate__()
  TransportProxyTensor.__setstate__(state, Any])
  TransportProxyTensor.name()
  TransportProxyTensor.fields()
  TransportProxyTensor.transport_mode()
  MultiModalityDataPaddingPattern.pad_input_tokens(input_ids, mm_inputs)
  MultiModalityDataPaddingPatternTokenPairs.__init__(data_token_pairs, int]]], data_start_token_ids)
  MultiModalityDataPaddingPatternTokenPairs.pad_input_tokens(input_ids, mm_inputs)
  MultiModalityDataPaddingPatternMultimodalTokens.pad_input_tokens(input_ids, mm_inputs)
init_embedding_cache(max_size)
get_embedding_hash(embedding_items)
get_embedding_chunk(embedding, extend_prefix_len, extend_seq_len, items_offset, int]])
get_embedding_and_mask(data_embedding_func, torch.Tensor], embedding_items, placeholder_tensor, input_ids, items_size, prefix_length, extend_length, items_offset_list, int]]])
embed_mm_inputs(mm_inputs_list, extend_prefix_lens, extend_seq_lens, input_ids, input_embedding, multimodal_model, data_embedding_func_mapping, Callable[[List[MultimodalDataItem]], torch.Tensor]], placeholder_tokens, List[int]])
general_mm_embed_routine(input_ids, forward_batch, language_model, multimodal_model, data_embedding_funcs, Callable[[List[MultimodalDataItem]], torch.Tensor]], placeholder_tokens, List[int]]])
get_multimodal_data_bounds(input_ids, pad_values, token_pairs, int]])
data_hash(data)
tensor_hash(tensor_list)
hash_feature(f)

# python/sglang/srt/managers/multimodal_processor.py
import_processors()
get_mm_processor(hf_config, server_args, processor, transport_mode)

# python/sglang/srt/managers/schedule_batch.py
  BaseFinishReason.__init__(is_error)
  BaseFinishReason.to_json()
  FINISH_MATCHED_TOKEN.__init__(matched, List[int]])
  FINISH_MATCHED_TOKEN.to_json()
  FINISH_MATCHED_STR.__init__(matched)
  FINISH_MATCHED_STR.to_json()
  FINISH_LENGTH.__init__(length)
  FINISH_LENGTH.to_json()
  FINISH_ABORT.__init__(message, status_code, err_type)
  FINISH_ABORT.to_json()
  Modality.from_str(modality_str)
  Modality.all()
  MultimodalDataItem.__getattr__(name)
  MultimodalDataItem.__setitem__(key, value)
  MultimodalDataItem.set(key, value)
  MultimodalDataItem.is_empty_list(l)
  MultimodalDataItem.set_pad_value()
  MultimodalDataItem.is_modality(modality)
  MultimodalDataItem.is_audio()
  MultimodalDataItem.is_image()
  MultimodalDataItem.is_video()
  MultimodalDataItem.is_valid()
  MultimodalDataItem.validate()
  MultimodalDataItem.from_dict(obj)
  MultimodalDataItem.merge(other)
  MultimodalInputs.from_dict(obj)
  MultimodalInputs.contains_image_inputs()
  MultimodalInputs.contains_video_inputs()
  MultimodalInputs.contains_audio_inputs()
  MultimodalInputs.contains_mm_input()
  MultimodalInputs.merge(other)
  Req.__init__(rid, origin_input_text, origin_input_ids, sampling_params, return_logprob, top_logprobs_num, token_ids_logprob, stream, origin_input_ids_unpadded, lora_id, input_embeds, token_type_ids, session_id, custom_logit_processor, return_hidden_states, eos_token_ids, bootstrap_host, bootstrap_port, bootstrap_room, data_parallel_rank, vocab_size)
  Req.seqlen()
  Req.extend_image_inputs(image_inputs)
  Req.finished()
  Req.init_next_round_input(tree_cache)
  Req.adjust_max_prefix_ids()
  Req.init_incremental_detokenize()
  Req.check_finished()
  Req.reset_for_retract()
  Req.offload_kv_cache(req_to_token_pool, token_to_kv_pool_allocator)
  Req.load_kv_cache(req_to_token_pool, token_to_kv_pool_allocator)
  Req.log_time_stats()
  Req.set_finish_with_abort(error_msg)
  Req.__repr__()
  ScheduleBatch.init_new(cls, reqs, req_to_token_pool, token_to_kv_pool_allocator, tree_cache, model_config, enable_overlap, spec_algorithm, chunked_req)
  ScheduleBatch.batch_size()
  ScheduleBatch.is_empty()
  ScheduleBatch.alloc_req_slots(num_reqs)
  ScheduleBatch.alloc_token_slots(num_tokens, backup_state)
  ScheduleBatch.alloc_paged_token_slots_extend(prefix_lens, seq_lens, last_loc, extend_num_tokens, backup_state)
  ScheduleBatch.alloc_paged_token_slots_decode(seq_lens, last_loc, backup_state)
  ScheduleBatch.prepare_encoder_info_extend(input_ids, seq_lens)
  ScheduleBatch.prepare_for_extend()
  ScheduleBatch.prepare_for_split_prefill()
  ScheduleBatch.mix_with_running(running_batch)
  ScheduleBatch.new_page_count_next_decode()
  ScheduleBatch.check_decode_mem(buf_multiplier)
  ScheduleBatch.retract_decode(server_args)
  ScheduleBatch.prepare_encoder_info_decode()
  ScheduleBatch.prepare_for_idle()
  ScheduleBatch.prepare_for_decode()
  ScheduleBatch.filter_batch(chunked_req_to_exclude, List[Req]]], keep_indices)
  ScheduleBatch.merge_batch(other)
  ScheduleBatch.get_model_worker_batch(seq_lens_cpu_cache)
  ScheduleBatch.copy()
  ScheduleBatch.__str__()
write_req_to_token_pool_triton(req_to_token_ptr, req_pool_indices, pre_lens, seq_lens, extend_lens, out_cache_loc, req_to_token_ptr_stride)
get_last_loc(req_to_token, req_pool_indices_tensor, prefix_lens_tensor)
get_last_loc_torch(req_to_token, req_pool_indices_tensor, prefix_lens_tensor)
get_last_loc_kernel(req_to_token, req_pool_indices_tensor, prefix_lens_tensor, result, num_tokens, req_to_token_stride, BLOCK_SIZE)
get_last_loc_triton(req_to_token, req_pool_indices_tensor, prefix_lens_tensor)

# python/sglang/srt/managers/schedule_policy.py
  SchedulePolicy.__init__(policy, tree_cache, enable_hierarchical_cache)
  SchedulePolicy.calc_priority(waiting_queue)
  PrefillAdder.__init__(page_size, tree_cache, token_to_kv_pool_allocator, running_batch, new_token_ratio, rem_input_tokens, rem_chunk_tokens, mixed_with_decode_tokens)
  PrefillAdder.rem_total_tokens()
  PrefillAdder.cur_rem_tokens()
  PrefillAdder.ceil_paged_tokens(tokens)
  PrefillAdder.budget_state()
  PrefillAdder.add_chunked_req(req)
  PrefillAdder.add_one_req_ignore_eos(req, has_chunked_req)
  PrefillAdder.add_one_req(req, has_chunked_req)

# python/sglang/srt/managers/scheduler.py
  Scheduler.__init__(server_args, port_args, gpu_id, tp_rank, moe_ep_rank, pp_rank, dp_rank, dp_balance_meta)
  Scheduler.init_tokenizer()
  Scheduler.init_memory_pool_and_cache()
  Scheduler.init_disaggregation()
  Scheduler.init_moe_config()
  Scheduler.event_loop_normal()
  Scheduler.event_loop_overlap()
  Scheduler.event_loop_pp()
  Scheduler.recv_requests()
  Scheduler.process_input_requests(recv_reqs)
  Scheduler.handle_generate_request(recv_req)
  Scheduler.handle_batch_generate_request(recv_req)
  Scheduler.handle_embedding_request(recv_req)
  Scheduler.handle_batch_embedding_request(recv_req)
  Scheduler.self_check_during_idle()
  Scheduler.check_memory()
  Scheduler.check_tree_cache()
  Scheduler.get_next_batch_to_run()
  Scheduler.get_num_allocatable_reqs(running_bs)
  Scheduler.get_new_batch_prefill()
  Scheduler.update_running_batch(batch)
  Scheduler.run_batch(batch)
  Scheduler.process_batch_result(batch, result, EmbeddingBatchResult], launch_done)
  Scheduler.maybe_send_health_check_signal()
  Scheduler.prepare_mlp_sync_batch(local_batch)
  Scheduler.handle_dp_balance_data(local_batch)
  Scheduler.prepare_mlp_sync_batch_raw(local_batch, dp_size, attn_tp_size, tp_group, get_idle_batch, disable_cuda_graph, spec_algorithm, speculative_num_draft_tokens, require_mlp_tp_gather, disable_overlap_schedule)
  Scheduler.get_idle_batch()
  Scheduler.move_ready_grammar_requests()
  Scheduler.set_next_batch_sampling_info_done(batch)
  Scheduler.watchdog_thread()
  Scheduler.flush_cache_wrapped(recv_req)
  Scheduler.flush_cache()
  Scheduler.get_load()
  Scheduler.get_internal_state(recv_req)
  Scheduler.set_internal_state(recv_req)
  Scheduler.handle_rpc_request(recv_req)
  Scheduler.abort_request(recv_req)
  Scheduler.load_lora_adapter(recv_req)
  Scheduler.unload_lora_adapter(recv_req)
  Scheduler.slow_down(recv_req)
  Scheduler.expert_distribution_handle(recv_req)
  Scheduler.open_session(recv_req)
  Scheduler.close_session(recv_req)
  Scheduler.get_print_prefix()
  Scheduler.current_scheduler_metrics_enabled()
  Scheduler.maybe_sleep_on_idle()
  Scheduler.handle_freeze_gc(recv_req)
  IdleSleeper.__init__(sockets)
  IdleSleeper.maybe_sleep()
is_health_check_generate_req(recv_req)
is_work_request(recv_req)
run_scheduler_process(server_args, port_args, gpu_id, tp_rank, moe_ep_rank, pp_rank, dp_rank, pipe_writer, balance_meta)

# python/sglang/srt/managers/scheduler_input_blocker.py
  SchedulerInputBlocker.__init__(noop)
  SchedulerInputBlocker.handle(recv_reqs)
input_blocker_guard_region(send_to_scheduler)

# python/sglang/srt/managers/scheduler_metrics_mixin.py
  KvMetrics.__init__()
  SchedulerMetricsMixin.init_metrics(tp_rank, pp_rank, dp_rank)
  SchedulerMetricsMixin.init_kv_events(kv_events_config)
  SchedulerMetricsMixin.log_prefill_stats(adder, can_run_list, running_bs)
  SchedulerMetricsMixin.log_decode_stats(can_run_cuda_graph, running_batch)

# python/sglang/srt/managers/scheduler_output_processor_mixin.py
  SchedulerOutputProcessorMixin.process_batch_result_prefill(batch, result, EmbeddingBatchResult], launch_done)
  SchedulerOutputProcessorMixin.process_batch_result_decode(batch, result, launch_done)
  SchedulerOutputProcessorMixin.add_input_logprob_return_values(i, req, output, logprob_pt, num_input_logprobs, last_prefill_chunk)
  SchedulerOutputProcessorMixin.add_logprob_return_values(i, req, pt, next_token_ids, num_input_logprobs, output)
  SchedulerOutputProcessorMixin.stream_output(reqs, return_logprob, skip_req)
  SchedulerOutputProcessorMixin.stream_output_generation(reqs, return_logprob, skip_req)
  SchedulerOutputProcessorMixin.stream_output_embedding(reqs)

# python/sglang/srt/managers/scheduler_profiler_mixin.py
  SchedulerProfilerMixin.init_profier()
  SchedulerProfilerMixin.init_profile(output_dir, start_step, num_steps, activities, with_stack, record_shapes, profile_by_stage, profile_id)
  SchedulerProfilerMixin.start_profile(stage)
  SchedulerProfilerMixin.stop_profile(stage)
  SchedulerProfilerMixin.profile(recv_req)

# python/sglang/srt/managers/scheduler_recv_skipper.py
  SchedulerRecvSkipper.maybe_create(server_args)
  SchedulerRecvSkipper.__init__(server_args)
  SchedulerRecvSkipper.handle(last_forward_mode)

# python/sglang/srt/managers/scheduler_update_weights_mixin.py
  SchedulerUpdateWeightsMixin.update_weights_from_disk(recv_req)
  SchedulerUpdateWeightsMixin.init_weights_update_group(recv_req)
  SchedulerUpdateWeightsMixin.update_weights_from_distributed(recv_req)
  SchedulerUpdateWeightsMixin.update_weights_from_tensor(recv_req)
  SchedulerUpdateWeightsMixin.get_weights_by_name(recv_req)
  SchedulerUpdateWeightsMixin.release_memory_occupation(recv_req)
  SchedulerUpdateWeightsMixin.resume_memory_occupation(recv_req)
  SchedulerUpdateWeightsMixin.save_remote_model(params)
  SchedulerUpdateWeightsMixin.save_sharded_model(params)

# python/sglang/srt/managers/session_controller.py
  SessionReqNode.__init__(req, parent, childs)
  SessionReqNode.clear_childs(req_dict)
  SessionReqNode.clear(req_dict)
  SessionReqNode.abort()
  SessionReqNode.__str__()
  Session.__init__(capacity_of_str_len, session_id)
  Session.create_req(req, tokenizer)

# python/sglang/srt/managers/template_manager.py
  TemplateManager.__init__()
  TemplateManager.chat_template_name()
  TemplateManager.completion_template_name()
  TemplateManager.jinja_template_content_format()
  TemplateManager.force_reasoning()
  TemplateManager.load_chat_template(tokenizer_manager, chat_template_arg, model_path)
  TemplateManager.guess_chat_template_from_model_path(model_path)
  TemplateManager.load_completion_template(completion_template_arg)
  TemplateManager.initialize_templates(tokenizer_manager, model_path, chat_template, completion_template)

# python/sglang/srt/managers/tokenizer_manager.py
  TokenizerManager.__init__(server_args, port_args)
  TokenizerManager.generate_request(obj, EmbeddingReqInput], request)
  TokenizerManager.flush_cache()
  TokenizerManager.abort_request(rid, abort_all)
  TokenizerManager.start_profile(output_dir, start_step, num_steps, activities, with_stack, record_shapes, profile_by_stage)
  TokenizerManager.stop_profile()
  TokenizerManager.start_expert_distribution_record()
  TokenizerManager.stop_expert_distribution_record()
  TokenizerManager.dump_expert_distribution_record()
  TokenizerManager.pause_generation()
  TokenizerManager.continue_generation()
  TokenizerManager.update_weights_from_disk(obj, request)
  TokenizerManager.init_weights_update_group(obj, request)
  TokenizerManager.update_weights_from_distributed(obj, request)
  TokenizerManager.update_weights_from_tensor(obj, request)
  TokenizerManager.load_lora_adapter(obj, _)
  TokenizerManager.unload_lora_adapter(obj, _)
  TokenizerManager.get_weights_by_name(obj, request)
  TokenizerManager.release_memory_occupation(obj, request)
  TokenizerManager.resume_memory_occupation(obj, request)
  TokenizerManager.slow_down(obj, request)
  TokenizerManager.open_session(obj, request)
  TokenizerManager.close_session(obj, request)
  TokenizerManager.get_internal_state()
  TokenizerManager.set_internal_state(obj)
  TokenizerManager.get_load()
  TokenizerManager.get_log_request_metadata()
  TokenizerManager.configure_logging(obj)
  TokenizerManager.freeze_gc()
  TokenizerManager.create_abort_task(obj)
  TokenizerManager.auto_create_handle_loop()
  TokenizerManager.dump_requests_before_crash()
  TokenizerManager.sigterm_watchdog()
  TokenizerManager.handle_loop()
  TokenizerManager.convert_logprob_style(meta_info, state, top_logprobs_num, token_ids_logprob, return_text_in_logprobs, recv_obj, recv_obj_index)
  TokenizerManager.detokenize_logprob_tokens(token_logprobs_val, token_logprobs_idx, decode_to_text)
  TokenizerManager.detokenize_top_logprobs_tokens(token_logprobs_val, token_logprobs_idx, decode_to_text)
  TokenizerManager.collect_metrics(state, recv_obj, i)
  TokenizerManager.dump_requests(state, out_dict)
  TokenizerManager.record_request_for_crash_dump(state, out_dict)
  TokenizerManager.score_request(query, List[int]]], items, List[str], List[List[int]]]], label_token_ids, apply_softmax, item_first, request)
print_exception_wrapper(func)
  SignalHandler.__init__(tokenizer_manager)
  SignalHandler.sigterm_handler(signum, frame)
  SignalHandler.running_phase_sigquit_handler(signum, frame)
  _Communicator.__init__(sender, fan_out)
  _Communicator.__call__(obj)
  _Communicator.handle_recv(recv_obj)

# python/sglang/srt/managers/tp_worker.py
  TpModelWorker.__init__(server_args, gpu_id, tp_rank, moe_ep_rank, pp_rank, dp_rank, nccl_port, is_draft_worker, req_to_token_pool, token_to_kv_pool_allocator)
  TpModelWorker.register_hicache_layer_transfer_counter(counter)
  TpModelWorker.set_hicache_consumer(consumer_index)
  TpModelWorker.get_worker_info()
  TpModelWorker.sliding_window_size()
  TpModelWorker.is_hybrid()
  TpModelWorker.get_tokens_per_layer_info()
  TpModelWorker.get_pad_input_ids_func()
  TpModelWorker.get_tp_group()
  TpModelWorker.get_attention_tp_group()
  TpModelWorker.get_attention_tp_cpu_group()
  TpModelWorker.get_memory_pool()
  TpModelWorker.forward_batch_generation(model_worker_batch, launch_done, skip_sample)
  TpModelWorker.forward_batch_embedding(model_worker_batch)
  TpModelWorker.update_weights_from_disk(recv_req)
  TpModelWorker.init_weights_update_group(recv_req)
  TpModelWorker.update_weights_from_distributed(recv_req)
  TpModelWorker.update_weights_from_tensor(recv_req)
  TpModelWorker.get_weights_by_name(recv_req)
  TpModelWorker.load_lora_adapter(recv_req)
  TpModelWorker.unload_lora_adapter(recv_req)
  TpModelWorker.can_run_lora_batch(lora_ids)

# python/sglang/srt/managers/tp_worker_overlap_thread.py
resolve_future_token_ids(input_ids, future_token_ids_map)
  TpModelWorkerClient.__init__(server_args, gpu_id, tp_rank, moe_ep_rank, pp_rank, dp_rank, nccl_port)
  TpModelWorkerClient.register_hicache_layer_transfer_counter(counter)
  TpModelWorkerClient.set_hicache_consumer(consumer_index)
  TpModelWorkerClient.get_worker_info()
  TpModelWorkerClient.get_tokens_per_layer_info()
  TpModelWorkerClient.sliding_window_size()
  TpModelWorkerClient.is_hybrid()
  TpModelWorkerClient.get_pad_input_ids_func()
  TpModelWorkerClient.get_tp_group()
  TpModelWorkerClient.get_attention_tp_group()
  TpModelWorkerClient.get_attention_tp_cpu_group()
  TpModelWorkerClient.get_memory_pool()
  TpModelWorkerClient.get_kv_cache()
  TpModelWorkerClient.forward_thread_func()
  TpModelWorkerClient.forward_thread_func_()
  TpModelWorkerClient.resolve_last_batch_result(launch_done)
  TpModelWorkerClient.forward_batch_generation(model_worker_batch)
  TpModelWorkerClient.update_weights_from_disk(recv_req)
  TpModelWorkerClient.init_weights_update_group(recv_req)
  TpModelWorkerClient.update_weights_from_distributed(recv_req)
  TpModelWorkerClient.update_weights_from_tensor(recv_req)
  TpModelWorkerClient.get_weights_by_name(recv_req)
  TpModelWorkerClient.load_lora_adapter(recv_req)
  TpModelWorkerClient.unload_lora_adapter(recv_req)
  TpModelWorkerClient.can_run_lora_batch(lora_ids)
  TpModelWorkerClient.__delete__()

# python/sglang/srt/managers/utils.py
validate_input_length(req, max_req_input_len, allow_auto_truncate)
get_logprob_dict_from_result(result)
get_logprob_from_pp_outputs(next_pp_outputs)
  DPBalanceMeta.__init__(num_workers)
  DPBalanceMeta.destructor()
  DPBalanceMeta.get_shared_onfly()
  DPBalanceMeta.set_shared_onfly_info(data, int]])
  DPBalanceMeta.get_shared_local_tokens()
  DPBalanceMeta.set_shared_local_tokens(data)
  DPBalanceMeta.__getstate__()
  DPBalanceMeta.__setstate__(state)

# python/sglang/srt/mem_cache/allocator.py
  BaseTokenToKVPoolAllocator.__init__(size, page_size, dtype, device, kvcache, need_sort)
  BaseTokenToKVPoolAllocator.debug_print()
  BaseTokenToKVPoolAllocator.available_size()
  BaseTokenToKVPoolAllocator.get_kvcache()
  BaseTokenToKVPoolAllocator.restore_state(state)
  BaseTokenToKVPoolAllocator.backup_state()
  BaseTokenToKVPoolAllocator.free_group_begin()
  BaseTokenToKVPoolAllocator.free_group_end()
  BaseTokenToKVPoolAllocator.merge_and_sort_free()
  BaseTokenToKVPoolAllocator.get_cpu_copy()
  BaseTokenToKVPoolAllocator.load_cpu_copy()
  BaseTokenToKVPoolAllocator.alloc_extend()
  BaseTokenToKVPoolAllocator.alloc_decode()
  BaseTokenToKVPoolAllocator.clear()
  BaseTokenToKVPoolAllocator.alloc(need_size)
  BaseTokenToKVPoolAllocator.free(free_index)
  TokenToKVPoolAllocator.__init__(size, dtype, device, kvcache, need_sort)
  TokenToKVPoolAllocator.clear()
  TokenToKVPoolAllocator.available_size()
  TokenToKVPoolAllocator.alloc(need_size)
  TokenToKVPoolAllocator.free(free_index)
  TokenToKVPoolAllocator.get_cpu_copy(indices)
  TokenToKVPoolAllocator.load_cpu_copy(kv_cache_cpu, indices)
  SWATokenToKVPoolAllocator.__init__(size, size_swa, dtype, device, kvcache, need_sort)
  SWATokenToKVPoolAllocator.available_size()
  SWATokenToKVPoolAllocator.full_available_size()
  SWATokenToKVPoolAllocator.swa_available_size()
  SWATokenToKVPoolAllocator.size_full()
  SWATokenToKVPoolAllocator.size_swa()
  SWATokenToKVPoolAllocator.debug_print()
  SWATokenToKVPoolAllocator.get_kvcache()
  SWATokenToKVPoolAllocator.translate_loc_from_full_to_swa(kv_indices)
  SWATokenToKVPoolAllocator.alloc(need_size)
  SWATokenToKVPoolAllocator.free(free_index)
  SWATokenToKVPoolAllocator.free_swa(free_index)
  SWATokenToKVPoolAllocator.backup_state()
  SWATokenToKVPoolAllocator.restore_state(state)
  SWATokenToKVPoolAllocator.clear()
alloc_extend_kernel(pre_lens_ptr, seq_lens_ptr, last_loc_ptr, free_page_ptr, out_indices, ret_values, bs_upper, page_size, max_num_extend_tokens)
alloc_decode_kernel(seq_lens_ptr, last_loc_ptr, free_page_ptr, out_indices, ret_values, bs_upper, page_size)
  PagedTokenToKVPoolAllocator.__init__(size, page_size, dtype, device, kvcache, need_sort)
  PagedTokenToKVPoolAllocator.alloc(need_size)
  PagedTokenToKVPoolAllocator.alloc_extend(prefix_lens, seq_lens, last_loc, extend_num_tokens)
  PagedTokenToKVPoolAllocator.alloc_decode(seq_lens, last_loc)
  PagedTokenToKVPoolAllocator.free(free_index)
  PagedTokenToKVPoolAllocator.clear()
  PagedTokenToKVPoolAllocator.get_cpu_copy(indices)
  PagedTokenToKVPoolAllocator.load_cpu_copy(kv_cache_cpu, indices)

# python/sglang/srt/mem_cache/allocator_ascend.py
alloc_extend_kernel_ascend(prefix_lens, seq_lens, last_loc, free_pages, out_indices, page_size, device)
  AscendPagedTokenToKVPoolAllocator.alloc_extend(prefix_lens, seq_lens, last_loc, extend_num_tokens)
  AscendPagedTokenToKVPoolAllocator.alloc_decode(seq_lens, last_loc)

# python/sglang/srt/mem_cache/base_prefix_cache.py
  BasePrefixCache.reset()
  BasePrefixCache.match_prefix(key)
  BasePrefixCache.cache_finished_req(req)
  BasePrefixCache.cache_unfinished_req(req)
  BasePrefixCache.evict(num_tokens)
  BasePrefixCache.inc_lock_ref(node)
  BasePrefixCache.dec_lock_ref(node, swa_uuid_for_lock)
  BasePrefixCache.evictable_size()
  BasePrefixCache.full_evictable_size()
  BasePrefixCache.swa_evictable_size()
  BasePrefixCache.protected_size()
  BasePrefixCache.full_protected_size()
  BasePrefixCache.swa_protected_size()
  BasePrefixCache.total_size()
  BasePrefixCache.pretty_print()
  BasePrefixCache.init_load_back(last_host_node, host_hit_length)
  BasePrefixCache.ready_to_load_host_cache()
  BasePrefixCache.check_hicache_events()
  BasePrefixCache.take_events()

# python/sglang/srt/mem_cache/chunk_cache.py
  ChunkCache.__init__(req_to_token_pool, token_to_kv_pool_allocator, page_size)
  ChunkCache.reset()
  ChunkCache.match_prefix()
  ChunkCache.cache_finished_req(req)
  ChunkCache.cache_unfinished_req(req)
  ChunkCache.evict(num_tokens)
  ChunkCache.inc_lock_ref(node)
  ChunkCache.dec_lock_ref(node, swa_uuid_for_lock)
  ChunkCache.pretty_print()
  SWAChunkCache.__init__(req_to_token_pool, token_to_kv_pool_allocator, page_size)
  SWAChunkCache.evict_swa(req, prelen, attention_chunk_size)
  SWAChunkCache.evict(num_tokens)

# python/sglang/srt/mem_cache/cpp_radix_tree/radix_tree.py
  RadixTreeCpp.__init__(disabled, host_size, page_size, write_through_threshold)
  RadixTreeCpp.match_prefix(prefix)
  RadixTreeCpp.evict(num_tokens)
  RadixTreeCpp.lock_ref(handle, lock)
  RadixTreeCpp.writing_through(key, indices)
  RadixTreeCpp.loading_onboard(host_node, new_device_indices)
  RadixTreeCpp.commit_writing_through(handle, success)
  RadixTreeCpp.commit_loading_onboard(handle, success)
  RadixTreeCpp.evictable_size()
  RadixTreeCpp.protected_size()
  RadixTreeCpp.total_size()
  RadixTreeCpp.reset()
  RadixTreeCpp.debug_print()

# python/sglang/srt/mem_cache/hicache_storage.py
get_hash_str(token_ids, prior_hash)
  HiCacheStorage.get(key, target_location, target_sizes)
  HiCacheStorage.batch_get(keys, target_locations, target_sizes)
  HiCacheStorage.set(key, value, target_location, target_sizes)
  HiCacheStorage.batch_set(keys, values, target_locations, target_sizes)
  HiCacheStorage.exists(key)
  HiCacheStorage.batch_exists(keys)
  HiCacheFile.__init__(storage_config, file_path)
  HiCacheFile.get(key, target_location, target_sizes)
  HiCacheFile.batch_get(keys, target_locations, target_sizes)
  HiCacheFile.set(key, value, target_location, target_sizes)
  HiCacheFile.batch_set(keys, values, target_locations, target_sizes)
  HiCacheFile.exists(key)
  HiCacheFile.delete(key)
  HiCacheFile.clear()

# python/sglang/srt/mem_cache/hiradix_cache.py
  HiRadixCache.__init__(req_to_token_pool, token_to_kv_pool_allocator, tp_cache_group, page_size, hicache_ratio, hicache_size, hicache_write_policy, hicache_io_backend, hicache_mem_layout, hicache_storage_backend, hicache_storage_prefetch_policy, model_name, storage_backend_extra_config)
  HiRadixCache.reset()
  HiRadixCache.get_height(node)
  HiRadixCache.write_backup(node, write_back)
  HiRadixCache.write_backup_storage(node)
  HiRadixCache.inc_hit_count(node)
  HiRadixCache.writing_check(write_back)
  HiRadixCache.loading_check()
  HiRadixCache.evictable_size()
  HiRadixCache.evict(num_tokens)
  HiRadixCache.evict_host(num_tokens)
  HiRadixCache.load_back(node, mem_quota)
  HiRadixCache.init_load_back(last_node, host_hit_length, mem_quota)
  HiRadixCache.ready_to_load_host_cache()
  HiRadixCache.check_hicache_events()
  HiRadixCache.check_revoked_prefetch()
  HiRadixCache.check_backup_progress()
  HiRadixCache.can_terminate_prefetch(operation)
  HiRadixCache.check_prefetch_progress(req_id)
  HiRadixCache.match_prefix(key)
  HiRadixCache.prefetch_from_storage(req_id, last_host_node, new_input_tokens, last_hash)

# python/sglang/srt/mem_cache/lora_radix_cache.py
  LoRAKey.__init__(lora_id, token_ids)
  LoRAKey.__len__()
get_child_key(key)
  LoRATreeNode.__init__(id)
  LoRATreeNode.evicted()
  LoRATreeNode.__lt__(other)
  LoRARadixCache.__init__(req_to_token_pool, token_to_kv_pool_allocator, page_size, disable)
  LoRARadixCache.reset()
  LoRARadixCache.match_prefix(key)
  LoRARadixCache.match_prefix_with_lora_id(key)
  LoRARadixCache.insert(key, value)
  LoRARadixCache.cache_finished_req(req)
  LoRARadixCache.cache_unfinished_req(req)
  LoRARadixCache.pretty_print()
  LoRARadixCache.total_size()
  LoRARadixCache.evict(num_tokens)
  LoRARadixCache.inc_lock_ref(node)
  LoRARadixCache.dec_lock_ref(node)
  LoRARadixCache.evictable_size()
  LoRARadixCache.protected_size()
  LoRARadixCache.all_values_flatten()

# python/sglang/srt/mem_cache/memory_pool.py
  ReqToTokenPool.__init__(size, max_context_len, device, enable_memory_saver)
  ReqToTokenPool.write(indices, values)
  ReqToTokenPool.available_size()
  ReqToTokenPool.alloc(need_size)
  ReqToTokenPool.free(free_index, List[int]])
  ReqToTokenPool.clear()
  KVCache.__init__(size, page_size, dtype, layer_num, device, enable_memory_saver, start_layer, end_layer)
  KVCache.get_key_buffer(layer_id)
  KVCache.get_value_buffer(layer_id)
  KVCache.get_kv_buffer(layer_id)
  KVCache.set_kv_buffer(layer, loc, cache_k, cache_v)
  KVCache.register_layer_transfer_counter(layer_transfer_counter)
  KVCache.get_cpu_copy(indices)
  KVCache.load_cpu_copy(kv_cache_cpu, indices)
  MHATokenToKVPool.__init__(size, page_size, dtype, head_num, head_dim, layer_num, device, enable_memory_saver, start_layer, end_layer)
  MHATokenToKVPool.get_kv_size_bytes()
  MHATokenToKVPool.get_contiguous_buf_infos()
  MHATokenToKVPool.maybe_get_custom_mem_pool()
  MHATokenToKVPool.get_cpu_copy(indices)
  MHATokenToKVPool.load_cpu_copy(kv_cache_cpu, indices)
  MHATokenToKVPool.get_key_buffer(layer_id)
  MHATokenToKVPool.get_value_buffer(layer_id)
  MHATokenToKVPool.get_kv_buffer(layer_id)
  MHATokenToKVPool.set_kv_buffer(layer, loc, cache_k, cache_v, k_scale, v_scale, layer_id_override)
  MHATokenToKVPool.move_kv_cache(tgt_loc, src_loc)
  SWAKVPool.__init__(size, size_swa, dtype, head_num, head_dim, swa_attention_layer_ids, full_attention_layer_ids, enable_kvcache_transpose, device)
  SWAKVPool.get_kv_size_bytes()
  SWAKVPool.get_contiguous_buf_infos()
  SWAKVPool.get_key_buffer(layer_id)
  SWAKVPool.get_value_buffer(layer_id)
  SWAKVPool.get_kv_buffer(layer_id)
  SWAKVPool.translate_loc_from_full_to_swa(kv_indices)
  SWAKVPool.set_kv_buffer(layer, loc, cache_k, cache_v, k_scale, v_scale)
  AscendTokenToKVPool.get_contiguous_buf_infos()
  AscendTokenToKVPool.set_kv_buffer(layer, loc, cache_k, cache_v, k_scale, v_scale)
set_mla_kv_buffer_kernel(kv_buffer_ptr, cache_k_nope_ptr, cache_k_rope_ptr, loc_ptr, buffer_stride, nope_stride, rope_stride, nope_dim, rope_dim, BLOCK)
set_mla_kv_buffer_triton(kv_buffer, loc, cache_k_nope, cache_k_rope)
  MLATokenToKVPool.__init__(size, page_size, dtype, kv_lora_rank, qk_rope_head_dim, layer_num, device, enable_memory_saver, start_layer, end_layer)
  MLATokenToKVPool.get_kv_size_bytes()
  MLATokenToKVPool.get_contiguous_buf_infos()
  MLATokenToKVPool.maybe_get_custom_mem_pool()
  MLATokenToKVPool.get_key_buffer(layer_id)
  MLATokenToKVPool.get_value_buffer(layer_id)
  MLATokenToKVPool.get_kv_buffer(layer_id)
  MLATokenToKVPool.set_kv_buffer(layer, loc, cache_k, cache_v)
  MLATokenToKVPool.set_mla_kv_buffer(layer, loc, cache_k_nope, cache_k_rope)
  MLATokenToKVPool.get_cpu_copy(indices)
  MLATokenToKVPool.load_cpu_copy(kv_cache_cpu, indices)
  AscendMLAPagedTokenToKVPool.__init__(size, page_size, dtype, kv_lora_rank, qk_rope_head_dim, layer_num, device, enable_memory_saver, start_layer, end_layer)
  AscendMLAPagedTokenToKVPool.get_kv_size_bytes()
  AscendMLAPagedTokenToKVPool.get_kv_buffer(layer_id)
  AscendMLAPagedTokenToKVPool.get_key_buffer(layer_id)
  AscendMLAPagedTokenToKVPool.get_value_buffer(layer_id)
  AscendMLAPagedTokenToKVPool.get_contiguous_buf_infos()
  AscendMLAPagedTokenToKVPool.set_kv_buffer(layer, loc, cache_k, cache_v)
  DoubleSparseTokenToKVPool.__init__(size, page_size, dtype, head_num, head_dim, layer_num, device, heavy_channel_num, enable_memory_saver, start_layer, end_layer)
  DoubleSparseTokenToKVPool.get_key_buffer(layer_id)
  DoubleSparseTokenToKVPool.get_value_buffer(layer_id)
  DoubleSparseTokenToKVPool.get_label_buffer(layer_id)
  DoubleSparseTokenToKVPool.get_kv_buffer(layer_id)
  DoubleSparseTokenToKVPool.set_kv_buffer(layer, loc, cache_k, cache_v, cache_label)
copy_all_layer_kv_cache(data_ptrs, strides, tgt_loc_ptr, src_loc_ptr, num_locs, num_locs_upper)

# python/sglang/srt/mem_cache/memory_pool_host.py
synchronized(debug_only)
  HostKVCache.__init__(device_pool, host_to_device_ratio, host_size, page_size, layout, pin_memory, device)
  HostKVCache.get_size_per_token()
  HostKVCache.init_kv_buffer()
  HostKVCache.load_to_device_per_layer(device_pool, host_indices, device_indices, layer_id, io_backend)
  HostKVCache.backup_from_device_all_layer(device_pool, host_indices, device_indices, io_backend)
  HostKVCache.get_flat_data_page(index)
  HostKVCache.get_dummy_flat_data_page()
  HostKVCache.set_from_flat_data_page(index, data_page)
  HostKVCache.clear()
  HostKVCache.available_size()
  HostKVCache.alloc(need_size)
  HostKVCache.free(indices)
  HostKVCache.get_state(indices)
  HostKVCache.is_reserved(indices)
  HostKVCache.is_protected(indices)
  HostKVCache.is_synced(indices)
  HostKVCache.is_backup(indices)
  HostKVCache.update_backup(indices)
  HostKVCache.update_prefetch(indices)
  HostKVCache.update_synced(indices)
  HostKVCache.protect_write(indices)
  HostKVCache.protect_load(indices)
  HostKVCache.complete_io(indices)
  MHATokenToKVPoolHost.__init__(device_pool, host_to_device_ratio, host_size, page_size, layout, pin_memory, device)
  MHATokenToKVPoolHost.get_size_per_token()
  MHATokenToKVPoolHost.get_ksize_per_token()
  MHATokenToKVPoolHost.init_kv_buffer()
  MHATokenToKVPoolHost.k_buffer()
  MHATokenToKVPoolHost.v_buffer()
  MHATokenToKVPoolHost.load_to_device_per_layer(device_pool, host_indices, device_indices, layer_id, io_backend)
  MHATokenToKVPoolHost.backup_from_device_all_layer(device_pool, host_indices, device_indices, io_backend)
  MHATokenToKVPoolHost.get_flat_data_page(index)
  MHATokenToKVPoolHost.get_dummy_flat_data_page()
  MHATokenToKVPoolHost.set_from_flat_data_page(index, data_page)
  MHATokenToKVPoolHost.get_buffer_meta(keys, indices, local_rank)
  MHATokenToKVPoolHost.get_buffer_with_hash(keys, indices)
  MLATokenToKVPoolHost.__init__(device_pool, host_to_device_ratio, host_size, page_size, layout, pin_memory, device)
  MLATokenToKVPoolHost.get_size_per_token()
  MLATokenToKVPoolHost.get_ksize_per_token()
  MLATokenToKVPoolHost.init_kv_buffer()
  MLATokenToKVPoolHost.load_to_device_per_layer(device_pool, host_indices, device_indices, layer_id, io_backend)
  MLATokenToKVPoolHost.backup_from_device_all_layer(device_pool, host_indices, device_indices, io_backend)
  MLATokenToKVPoolHost.get_flat_data_page(index)
  MLATokenToKVPoolHost.get_dummy_flat_data_page()
  MLATokenToKVPoolHost.set_from_flat_data_page(index, data_page)
  MLATokenToKVPoolHost.get_buffer_meta(keys, indices, local_rank)
  MLATokenToKVPoolHost.get_buffer_with_hash(keys, indices)

# python/sglang/srt/mem_cache/multimodal_cache.py
  MultiModalCache.__init__(max_size)
  MultiModalCache.put(mm_hash, embedding)
  MultiModalCache.has(mm_hash)
  MultiModalCache.get(mm_hash)
  MultiModalCache.clear()
  MultiModalCache.__len__()

# python/sglang/srt/mem_cache/radix_cache.py
  TreeNode.__init__(id)
  TreeNode.evicted()
  TreeNode.backuped()
  TreeNode.protect_host()
  TreeNode.release_host()
  TreeNode.get_last_hash_value()
  TreeNode.__lt__(other)
  RadixCache.__init__(req_to_token_pool, token_to_kv_pool_allocator, page_size, disable, enable_kv_cache_events)
  RadixCache.reset()
  RadixCache.match_prefix(key)
  RadixCache.insert(key, value)
  RadixCache.cache_finished_req(req)
  RadixCache.cache_unfinished_req(req)
  RadixCache.pretty_print()
  RadixCache.total_size()
  RadixCache.evict(num_tokens)
  RadixCache.inc_lock_ref(node)
  RadixCache.dec_lock_ref(node)
  RadixCache.evictable_size()
  RadixCache.protected_size()
  RadixCache.all_values_flatten()
  RadixCache.take_events()

# python/sglang/srt/mem_cache/radix_cache_cpp.py
  RadixCacheCpp.__init__(disable, use_hicache, req_to_token_pool, token_to_kv_pool, tp_cache_group, page_size, hicache_ratio, hicache_size, hicache_write_policy, enable_kv_cache_events, hicache_oracle, enable_write_cancel)
  RadixCacheCpp.reset()
  RadixCacheCpp.match_prefix(key)
  RadixCacheCpp.dec_lock_ref(node)
  RadixCacheCpp.inc_lock_ref(node)
  RadixCacheCpp.evict(num_tokens)
  RadixCacheCpp.evictable_size()
  RadixCacheCpp.protected_size()
  RadixCacheCpp.total_size()
  RadixCacheCpp.cache_finished_req(req)
  RadixCacheCpp.cache_unfinished_req(req)
  RadixCacheCpp.pretty_print()

# python/sglang/srt/mem_cache/storage/hf3fs/client_hf3fs.py
rsynchronized()
wsynchronized()
  Hf3fsClient.__init__(path, size, bytes_per_page, entries)
  Hf3fsClient.batch_read(offsets, tensors)
  Hf3fsClient.batch_write(offsets, tensors)
  Hf3fsClient.check(offsets, tensors)
  Hf3fsClient.get_size()
  Hf3fsClient.close()
  Hf3fsClient.flush()

# python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py
  RankMetadata.__init__(num_pages)
  RankMetadata.exists_keys(keys)
  RankMetadata.reserve_and_allocate_page_indices(keys, str]])
  RankMetadata.confirm_write(written_keys_to_confirm, int]], pages_to_release)
  RankMetadata.delete_keys(keys)
  RankMetadata.clear_all()
  RankMetadata.get_page_indices(keys)
  GlobalMetadataState.__init__(persistence_path, save_interval)
  GlobalMetadataState.load_from_disk()
  GlobalMetadataState.save_to_disk()
  GlobalMetadataState.schedule_save()
  GlobalMetadataState.shutdown()
  Hf3fsMetadataServer.__init__(persistence_path, save_interval)
  Hf3fsMetadataServer.get_rank_metadata(rank)
  Hf3fsMetadataServer.initialize(rank, request)
  Hf3fsMetadataServer.exists(rank, request)
  Hf3fsMetadataServer.reserve_and_allocate_page_indices(rank, request)
  Hf3fsMetadataServer.confirm_write(rank, request)
  Hf3fsMetadataServer.delete_keys(rank, request)
  Hf3fsMetadataServer.clear(rank)
  Hf3fsMetadataServer.get_page_indices(rank, request)
  Hf3fsMetadataServer.run(host, port)
  Hf3fsGlobalMetadataClient.__init__(base_url, max_retries)
  Hf3fsGlobalMetadataClient.initialize(rank, num_pages)
  Hf3fsGlobalMetadataClient.reserve_and_allocate_page_indices(rank, keys, str]])
  Hf3fsGlobalMetadataClient.confirm_write(rank, written_keys_to_confirm, int]], pages_to_release)
  Hf3fsGlobalMetadataClient.delete_keys(rank, keys)
  Hf3fsGlobalMetadataClient.exists(rank, keys)
  Hf3fsGlobalMetadataClient.clear(rank)
  Hf3fsGlobalMetadataClient.get_page_indices(rank, keys)
  Hf3fsLocalMetadataClient.__init__()
  Hf3fsLocalMetadataClient.initialize(rank, num_pages)
  Hf3fsLocalMetadataClient.reserve_and_allocate_page_indices(rank, keys, str]])
  Hf3fsLocalMetadataClient.confirm_write(rank, written_keys_to_confirm, int]], pages_to_release)
  Hf3fsLocalMetadataClient.delete_keys(rank, keys)
  Hf3fsLocalMetadataClient.exists(rank, keys)
  Hf3fsLocalMetadataClient.clear(rank)
  Hf3fsLocalMetadataClient.get_page_indices(rank, keys)
run_metadata_server(host, port, persistence_path, save_interval)

# python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py
  Hf3fsMetadataInterface.initialize(rank, num_pages)
  Hf3fsMetadataInterface.reserve_and_allocate_page_indices(rank, keys, str]])
  Hf3fsMetadataInterface.confirm_write(rank, written_keys_to_confirm, int]], pages_to_release)
  Hf3fsMetadataInterface.get_page_indices(rank, keys)
  Hf3fsMetadataInterface.delete_keys(rank, keys)
  Hf3fsMetadataInterface.exists(rank, keys)
  Hf3fsMetadataInterface.clear(rank)
  AtomicCounter.__init__(n)
  AtomicCounter.next()
synchronized()
  HiCacheHF3FS.__init__(rank, file_path, file_size, numjobs, bytes_per_page, entries, dtype, metadata_client)
  HiCacheHF3FS.from_env_config(bytes_per_page, dtype, storage_config)
  HiCacheHF3FS.get(key, target_location, target_sizes)
  HiCacheHF3FS.batch_get(keys, target_locations, target_sizes)
  HiCacheHF3FS.set(key, value, target_location, target_sizes)
  HiCacheHF3FS.batch_set(keys, values, target_locations, target_sizes)
  HiCacheHF3FS.delete(key)
  HiCacheHF3FS.exists(key)
  HiCacheHF3FS.clear()
  HiCacheHF3FS.close()

# python/sglang/srt/mem_cache/storage/hf3fs/test_hf3fs_utils.py
test_rw_shm()

# python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py
  MooncakeStoreConfig.from_file()
  MooncakeStoreConfig.load_from_env()
  MooncakeStoreConfig.__post_init__()
  MooncakeStore.__init__(storage_config)
  MooncakeStore.warmup()
  MooncakeStore.register_buffer(buffer)
  MooncakeStore.set(key, value, target_location, target_sizes)
  MooncakeStore.batch_set(keys, target_location, target_sizes)
  MooncakeStore.get(key, target_location, target_sizes)
  MooncakeStore.batch_get(keys, target_location, target_sizes)
  MooncakeStore.exists(key)
  MooncakeStore.batch_exists(keys)
  MooncakeStore.delete(key)
  MooncakeStore.close()
  MooncakeStore.clear()

# python/sglang/srt/mem_cache/storage/mooncake_store/unit_test.py
test_init_and_warmup()
test_register_buffer()
test_set_and_get()
test_exists()

# python/sglang/srt/mem_cache/storage/nixl/hicache_nixl.py
  HiCacheNixl.__init__(file_path, plugin)
  HiCacheNixl.register_buffers(buffers, List[torch.Tensor], List[tuple]])
  HiCacheNixl.register_files(file_paths, open_file)
  HiCacheNixl.register_objects(keys, sizes)
  HiCacheNixl.get(key, target_location, target_sizes)
  HiCacheNixl.batch_get(keys, target_locations, target_sizes)
  HiCacheNixl.set(key, value, target_location, target_sizes)
  HiCacheNixl.batch_set(keys, values, target_locations, target_sizes)
  HiCacheNixl.exists(key)

# python/sglang/srt/mem_cache/storage/nixl/nixl_utils.py
  NixlBackendSelection.__init__(plugin)
  NixlBackendSelection.set_bucket(bucket_name)
  NixlBackendSelection.create_backend(agent)
  NixlRegistration.__init__(agent)
  NixlRegistration.create_query_tuples(key, mem_type, file_manager)
  NixlFileManager.__init__(base_dir)
  NixlFileManager.get_file_path(key)
  NixlFileManager.create_file(file_path)
  NixlFileManager.open_file(file_path)
  NixlFileManager.close_file(fd)
  NixlFileManager.files_to_nixl_tuples(file_paths)

# python/sglang/srt/mem_cache/storage/nixl/test_hicache_nixl_storage.py
  TestNixlUnified.setUp()
  TestNixlUnified.tearDown()
  TestNixlUnified.delete_test_file(file_path)
  TestNixlUnified.verify_tensors_equal(expected, actual)
  TestNixlUnified.verify_tensor_lists_equal(expected, actual)
  TestNixlUnified.test_single_set_get()
  TestNixlUnified.test_batch_set_get()
  TestNixlUnified.test_mixed_operations()
  TestNixlUnified.test_data_integrity()
  TestNixlUnified.test_basic_file_operations()
  TestNixlUnified.test_create_nixl_tuples()
  TestNixlUnified.test_error_handling()
  TestNixlUnified.test_register_buffers()
  TestNixlUnified.test_register_files_with_tuples()

# python/sglang/srt/mem_cache/swa_radix_cache.py
  TreeNode.__init__(id)
  TreeNode.evicted()
  TreeNode.backuped()
  TreeNode.__lt__(other)
gen_swa_uuid()
  LRUList.__init__(swa)
  LRUList.reset_node_mru(node)
  LRUList.reset_node_and_parents_mru(node, root_node)
  LRUList.insert_mru(node)
  LRUList.remove_node(node)
  LRUList.get_lru_no_lock()
  LRUList.get_leaf_lru_no_lock()
  LRUList.get_prev_no_lock(node, check_id)
  LRUList.get_prev_leaf_no_lock(node, check_id)
  LRUList.in_list(node)
  LRUList.sanity_check_evictable_size()
  LRUList.sanity_check(tree_cache)
  SWARadixCache.__init__(req_to_token_pool, token_to_kv_pool_allocator, sliding_window_size, page_size, disable)
  SWARadixCache.reset()
  SWARadixCache.match_prefix(key)
  SWARadixCache.insert(key, value, prev_prefix_len)
  SWARadixCache.cache_finished_req(req)
  SWARadixCache.cache_unfinished_req(req)
  SWARadixCache.pretty_print()
  SWARadixCache.total_size()
  SWARadixCache.evict(full_num_tokens, swa_num_tokens)
  SWARadixCache.inc_lock_ref(node)
  SWARadixCache.dec_lock_ref(node, swa_uuid_for_lock)
  SWARadixCache.sanity_check()
  SWARadixCache.evictable_size()
  SWARadixCache.full_evictable_size()
  SWARadixCache.swa_evictable_size()
  SWARadixCache.full_lru_list_evictable_size()
  SWARadixCache.swa_lru_list_evictable_size()
  SWARadixCache.protected_size()
  SWARadixCache.full_protected_size()
  SWARadixCache.swa_protected_size()
  SWARadixCache.all_values_flatten()

# python/sglang/srt/metrics/collector.py
  TimeStats.__str__()
  TimeStats.format_duration(duration)
  TimeStats.get_type()
  SchedulerMetricsCollector.__init__(labels, str])
  SchedulerMetricsCollector.increment_bootstrap_failed_reqs()
  SchedulerMetricsCollector.increment_transfer_failed_reqs()
  SchedulerMetricsCollector.log_stats(stats)
  TokenizerMetricsCollector.__init__(labels, str], bucket_time_to_first_token, bucket_inter_token_latency, bucket_e2e_request_latency, collect_tokens_histogram)
  TokenizerMetricsCollector.observe_one_finished_request(prompt_tokens, generation_tokens, cached_tokens, e2e_latency, has_grammar)
  TokenizerMetricsCollector.observe_time_to_first_token(value)
  TokenizerMetricsCollector.observe_inter_token_latency(internval, num_new_tokens)
  TokenizerMetricsCollector.observe_one_aborted_request()

# python/sglang/srt/metrics/func_timer.py
enable_func_timer()
exponential_buckets(start, width, length)
time_func_latency(func, name)

# python/sglang/srt/model_executor/cuda_graph_runner.py
get_is_capture_mode()
model_capture_mode()
freeze_gc(enable_cudagraph_gc)
patch_model(model, enable_compile, num_tokens, tp_group)
set_torch_compile_config()
get_batch_sizes_to_capture(model_runner)
get_global_graph_memory_pool()
set_global_graph_memory_pool(val)
  CudaGraphRunner.__init__(model_runner)
  CudaGraphRunner.can_run(forward_batch)
  CudaGraphRunner.capture()
  CudaGraphRunner.capture_one_batch_size(bs, forward)
  CudaGraphRunner.recapture_if_needed(forward_batch)
  CudaGraphRunner.replay_prepare(forward_batch, pp_proxy_tensors)
  CudaGraphRunner.replay(forward_batch, skip_attn_backend_init, pp_proxy_tensors)
  CudaGraphRunner.get_spec_info(num_tokens)

# python/sglang/srt/model_executor/forward_batch_info.py
  ForwardMode.is_prefill()
  ForwardMode.is_extend()
  ForwardMode.is_decode()
  ForwardMode.is_mixed()
  ForwardMode.is_idle()
  ForwardMode.is_decode_or_idle()
  ForwardMode.is_target_verify()
  ForwardMode.is_draft_extend()
  ForwardMode.is_extend_or_draft_extend_or_mixed()
  ForwardMode.is_cuda_graph()
  ForwardMode.is_dummy_first()
  ForwardMode.is_split_prefill()
  CaptureHiddenMode.need_capture()
  CaptureHiddenMode.is_full()
  CaptureHiddenMode.is_last()
  CaptureHiddenMode.__lt__(other)
  ForwardBatch.init_new(cls, batch, model_runner)
  ForwardBatch.merge_mm_inputs()
  ForwardBatch.contains_image_inputs()
  ForwardBatch.contains_audio_inputs()
  ForwardBatch.contains_video_inputs()
  ForwardBatch.contains_mm_inputs()
  ForwardBatch.get_max_chunk_capacity()
  ForwardBatch.set_prefix_chunk_idx(idx)
  ForwardBatch.set_attn_attend_prefix_cache(attn_attend_prefix_cache)
  ForwardBatch.prepare_chunked_kv_indices(device)
  ForwardBatch.prepare_mlp_sync_batch(model_runner)
  ForwardBatch.post_forward_mlp_sync_batch(logits_output)
  ForwardBatch.get_prefix_chunk_seq_lens(prefix_lens, num_prefix_chunks, prefix_chunk_len)
  ForwardBatch.prepare_chunked_prefix_cache_info(device)
  ForwardBatch.can_run_tbo()
enable_num_token_non_padded(server_args)
  PPProxyTensors.__init__(tensors)
  PPProxyTensors.__getitem__(key, slice])
  PPProxyTensors.__setitem__(key, value)
  PPProxyTensors.__len__()
  PPProxyTensors.__eq__(other)
  PPProxyTensors.__repr__()
compute_position(attn_backend, extend_prefix_lens, extend_seq_lens, extend_seq_lens_sum)
compute_position_triton(extend_prefix_lens, extend_seq_lens, extend_seq_lens_sum)
compute_position_kernel(positions, extend_start_loc, extend_prefix_lens, extend_seq_lens, has_prefix)
compute_position_torch(extend_prefix_lens, extend_seq_lens)
clamp_position(seq_lens)
create_chunked_prefix_cache_kv_indices(req_to_token_ptr, req_pool_indices_ptr, chunk_start_idx_ptr, chunk_seq_lens_ptr, chunk_cu_seq_lens_ptr, chunk_kv_indices_ptr, req_to_token_ptr_stride)

# python/sglang/srt/model_executor/model_runner.py
  RankZeroFilter.__init__(is_rank_zero)
  RankZeroFilter.filter(record)
  ModelRunner.__init__(model_config, mem_fraction_static, gpu_id, tp_rank, tp_size, moe_ep_rank, moe_ep_size, pp_rank, pp_size, nccl_port, server_args, dp_rank, is_draft_worker, req_to_token_pool, token_to_kv_pool_allocator)
  ModelRunner.initialize(min_per_gpu_memory)
  ModelRunner.model_specific_adjustment()
  ModelRunner.init_torch_distributed()
  ModelRunner.load_model()
  ModelRunner.update_expert_location(new_expert_location_metadata, update_layer_ids)
  ModelRunner.update_weights_from_disk(model_path, load_format)
  ModelRunner.init_weights_update_group(master_address, master_port, rank_offset, world_size, group_name, backend)
  ModelRunner.update_weights_from_distributed(names, dtypes, shapes, group_name)
  ModelRunner.update_weights_from_tensor(named_tensors, Union[torch.Tensor, 'LocalSerializedTensor']]], load_format)
  ModelRunner.get_weights_by_name(name, truncate_size)
  ModelRunner.init_lora_manager()
  ModelRunner.load_lora_adapter(lora_ref)
  ModelRunner.unload_lora_adapter(lora_ref)
  ModelRunner.profile_max_num_token(total_gpu_memory)
  ModelRunner.set_num_token_hybrid()
  ModelRunner.init_memory_pool(total_gpu_memory, max_num_reqs, max_total_tokens)
  ModelRunner.init_cublas()
  ModelRunner.init_attention_backend()
  ModelRunner.init_double_sparsity_channel_config(selected_channel)
  ModelRunner.init_device_graphs()
  ModelRunner.init_threads_binding()
  ModelRunner.apply_torch_tp()
  ModelRunner.forward_decode(forward_batch, skip_attn_backend_init, pp_proxy_tensors)
  ModelRunner.forward_extend(forward_batch, skip_attn_backend_init, pp_proxy_tensors)
  ModelRunner.forward_idle(forward_batch, pp_proxy_tensors)
  ModelRunner.forward_split_prefill(forward_batch, reinit_attn_backend, forward_count)
  ModelRunner.forward(forward_batch, skip_attn_backend_init, pp_proxy_tensors, reinit_attn_backend, split_forward_count)
  ModelRunner.sample(logits_output, forward_batch)
  ModelRunner.model_is_mrope()
  ModelRunner.save_remote_model(url)
  ModelRunner.save_sharded_model(path, pattern, max_size)
  LocalSerializedTensor.get(rank)

# python/sglang/srt/model_executor/npu_graph_runner.py
  NPUGraphRunner.__init__(model_runner)
  NPUGraphRunner.replay(forward_batch, skip_attn_backend_init, pp_proxy_tensors)

# python/sglang/srt/model_loader/__init__.py
get_model()

# python/sglang/srt/model_loader/loader.py
device_loading_context(module, target_device)
  BaseModelLoader.__init__(load_config)
  BaseModelLoader.download_model(model_config)
  BaseModelLoader.load_model()
  Source.init_new(cls, model_config, model)
  DefaultModelLoader.__init__(load_config)
  DefaultModelLoader.download_model(model_config)
  DefaultModelLoader.load_model()
  DefaultModelLoader.load_weights_and_postprocess(model, weights, target_device)
  LayeredModelLoader.__init__(load_config)
  LayeredModelLoader.load_model()
  DummyModelLoader.__init__(load_config)
  DummyModelLoader.download_model(model_config)
  DummyModelLoader.load_model()
  ShardedStateLoader.__init__(load_config)
  ShardedStateLoader.download_model(model_config)
  ShardedStateLoader.load_model()
  ShardedStateLoader.save_model(model, path, pattern, max_size)
  BitsAndBytesModelLoader.__init__(load_config)
  BitsAndBytesModelLoader.download_model(model_config)
  BitsAndBytesModelLoader.load_model()
  GGUFModelLoader.__init__(load_config)
  GGUFModelLoader.download_model(model_config)
  GGUFModelLoader.load_model()
  RemoteModelLoader.__init__(load_config)
  RemoteModelLoader.download_model(model_config)
  RemoteModelLoader.save_model(model, model_path, url)
  RemoteModelLoader.load_model()
load_model_with_cpu_quantization()
get_model_loader(load_config)

# python/sglang/srt/model_loader/utils.py
set_default_torch_dtype(dtype)
resolve_transformers_arch(model_config, architectures)
get_model_architecture(model_config)
get_architecture_class_name(model_config)

# python/sglang/srt/model_loader/weight_utils.py
enable_hf_transfer()
  DisabledTqdm.__init__()
get_lock(model_name_or_path, cache_dir)
convert_bin_to_safetensor_file(pt_filename, sf_filename)
get_quant_config(model_config, load_config, packed_modules_mapping, List[str]])
download_weights_from_hf(model_name_or_path, cache_dir, allow_patterns, revision, ignore_patterns, List[str]]])
download_safetensors_index_file_from_hf(model_name_or_path, index_file, cache_dir, revision)
filter_duplicate_safetensors_files(hf_weights_files, hf_folder, index_file)
filter_files_not_needed_for_inference(hf_weights_files)
np_cache_weights_iterator(model_name_or_path, cache_dir, hf_folder, hf_weights_files)
decrypt(fn, key)
safetensors_encrypted_weights_iterator(hf_weights_files, is_all_weights_sharded, decryption_key)
safetensors_weights_iterator(hf_weights_files, is_all_weights_sharded, decryption_key, disable_mmap)
multi_thread_safetensors_weights_iterator(hf_weights_files, is_all_weights_sharded, decryption_key, max_workers, disable_mmap)
pt_weights_iterator(hf_weights_files)
multi_thread_pt_weights_iterator(hf_weights_files, max_workers)
get_gguf_extra_tensor_names(gguf_file, gguf_to_hf_name_map, str])
gguf_quant_weights_iterator(gguf_file, gguf_to_hf_name_map, str])
convert_pyslice_to_tensor(x)
default_weight_loader(param, loaded_weight)
row_parallel_weight_loader(param, loaded_weight)
sharded_weight_loader(shard_axis)
composed_weight_loader(loader, fn, torch.Tensor])
runai_safetensors_weights_iterator(hf_weights_files)
set_runai_streamer_env(load_config)
initialize_dummy_weights(model, low, high, seed)
maybe_remap_kv_scale_name(name, params_dict)
  KVCacheQuantSchema.check_is_fp8()
  KVCacheQuantSchema.check_tp_ranks(info)
  KVCacheQuantSchema.check_current_rank(info)
  QuantParamSchema.check_model_type(info)
kv_cache_scales_loader(filename, tp_rank, tp_size, num_hidden_layers, model_type)
get_actual_shard_size(shard_size, weight_start, weight_end)
reset_param_data_if_needed(param_data, dim, start, length)
narrow_padded_param_and_loaded_weight(param_data, loaded_weight, param_data_start, weight_start, dim, shard_size, narrow_weight)

# python/sglang/srt/model_parallel.py
tensor_parallel(module, device_mesh)

# python/sglang/srt/models/arcee.py
  ArceeMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix, reduce_results)
  ArceeMLP.forward(x, forward_batch)
  ArceeAttention.__init__(config, hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], rope_is_neox_style, max_position_embeddings, quant_config, prefix, bias)
  ArceeAttention.forward(positions, hidden_states, forward_batch)
  ArceeDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  ArceeDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  ArceeModel.__init__(config, quant_config, prefix)
  ArceeModel.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  ArceeModel.load_kv_cache_scales(quantization_param_path)
  ArceeForCausalLM.__init__(config, quant_config, prefix)
  ArceeForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, get_embedding, pp_proxy_tensors)
  ArceeForCausalLM.start_layer()
  ArceeForCausalLM.end_layer()
  ArceeForCausalLM.get_input_embeddings()
  ArceeForCausalLM.load_weights(weights, torch.Tensor]])
  ArceeForCausalLM.load_kv_cache_scales(quantization_param_path)

# python/sglang/srt/models/baichuan.py
  BaiChuanMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  BaiChuanMLP.forward(x)
  BaiChuanAttention.__init__(hidden_size, num_heads, position_embedding, rope_theta, max_position_embeddings, quant_config, layer_id, prefix)
  BaiChuanAttention.forward(positions, hidden_states, forward_batch)
  BaiChuanDecoderLayer.__init__(config, position_embedding, layer_id, quant_config, prefix)
  BaiChuanDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  BaiChuanModel.__init__(config, position_embedding, quant_config, prefix)
  BaiChuanModel.forward(input_ids, positions, forward_batch)
  BaiChuanBaseForCausalLM.__init__(config, position_embedding, quant_config, prefix)
  BaiChuanBaseForCausalLM.forward(input_ids, positions, forward_batch)
  BaiChuanBaseForCausalLM.load_weights(weights, torch.Tensor]])
  BaichuanForCausalLM.__init__(config, quant_config, prefix)

# python/sglang/srt/models/bailing_moe.py
  BailingAttention.__init__(config, layer_id, quant_config, prefix)
  BailingAttention.forward(hidden_states, position_ids, forward_batch)
  BailingMLP.__init__(intermediate_size, config, quant_config, reduce_results, prefix)
  BailingMLP.forward(x)
  BailingMoE.__init__(config, layer_id, quant_config, prefix)
  BailingMoE.forward(hidden_states)
  BailingMoeBlock.__init__(config, layer_id, quant_config, prefix)
  BailingMoeBlock.forward(hidden_states, position_ids, residual, forward_batch)
  BailingMoeModel.__init__(config, quant_config, prefix)
  BailingMoeModel.forward(input_ids, position_ids, forward_batch, input_embeds)
  BailingMoeForCausalLM.__init__(config, quant_config)
  BailingMoeForCausalLM.forward(input_ids, positions, forward_batch, inputs_embeds)
  BailingMoeForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/bert.py
  BertEmbedding.__init__(config)
  BertEmbedding.forward(input_ids, positions, forward_batch)
  BertPooler.__init__(config)
  BertPooler.forward(hidden_states, forward_batch)
  BertEncoder.__init__(config, quant_config, prefix)
  BertEncoder.forward(hidden_states, forward_batch)
  BertLayer.__init__(config, layer_id, quant_config, prefix)
  BertLayer.forward(hidden_states, forward_batch)
  BertAttention.__init__(hidden_size, num_attention_heads, layer_norm_eps, layer_id, quant_config, prefix)
  BertAttention.forward(hidden_states, forward_batch)
  BertSelfAttention.__init__(hidden_size, num_attention_heads, layer_id, quant_config, prefix)
  BertSelfAttention.forward(hidden_states, forward_batch)
  BertSelfOutput.__init__(hidden_size, layer_norm_eps, quant_config, prefix)
  BertSelfOutput.forward(hidden_states, input_tensor)
  BertIntermediate.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  BertIntermediate.forward(hidden_states)
  BertOutput.__init__(hidden_size, intermediate_size, layer_norm_eps, quant_config, prefix)
  BertOutput.forward(hidden_states, input_tensor)
  BertModel.__init__()
  BertModel.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  BertModel.load_weights(weights, torch.Tensor]])
  BertForSequenceClassification.__init__()
  BertForSequenceClassification.load_weights(weights, torch.Tensor]])
  BertForSequenceClassification.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)

# python/sglang/srt/models/chatglm.py
  GLMAttention.__init__(config, layer_id, quant_config, prefix)
  GLMAttention.forward(hidden_states, position_ids, forward_batch)
  GLMMLP.__init__(config, quant_config, prefix)
  GLMMLP.forward(hidden_states)
  GLMBlock.__init__(config, layer_id, quant_config, prefix)
  GLMBlock.forward(hidden_states, position_ids, forward_batch)
  GLMTransformer.__init__(config, quant_config, prefix)
  GLMTransformer.forward(hidden_states, position_ids, forward_batch)
  ChatGLMM.__init__(config, quant_config, prefix)
  ChatGLMM.forward(input_ids, position_ids, forward_batch)
  ChatGLMForCausalLM.__init__(config, quant_config, prefix)
  ChatGLMForCausalLM.forward(input_ids, positions, forward_batch)
  ChatGLMForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/clip.py
  CLIPVisionEmbeddings.__init__(config)
  CLIPVisionEmbeddings.forward(pixel_values)
  CLIPTextEmbeddings.__init__(config)
  CLIPTextEmbeddings.forward(input_ids, position_ids, inputs_embeds)
  CLIPMLP.__init__(config, act_layer, quant_config, prefix)
  CLIPMLP.forward(x)
  CLIPEncoderLayer.__init__(config, act_layer, norm_layer, attn_implementation, quant_config, prefix)
  CLIPEncoderLayer.forward(hidden_states, attention_mask, causal_attention_mask)
  CLIPEncoder.__init__(config, quant_config, prefix)
  CLIPEncoder.forward(inputs_embeds, attention_mask, causal_attention_mask, return_all_hidden_states)
  CLIPTextTransformer.__init__(config, quant_config, prefix)
  CLIPTextTransformer.device()
  CLIPTextTransformer.forward(input_ids, attention_mask, position_ids)
  CLIPTextModel.__init__(config, quant_config, prefix)
  CLIPTextModel.forward(input_ids, position_ids)
  CLIPVisionTransformer.__init__(config, quant_config, prefix)
  CLIPVisionTransformer.device()
  CLIPVisionTransformer.forward(pixel_values)
  CLIPVisionModel.__init__(config, quant_config, prefix)
  CLIPVisionModel.device()
  CLIPVisionModel.forward(pixel_values)
  CLIPModel.__init__(config, quant_config, prefix)
  CLIPModel.forward(input_ids, positions, forward_batch, get_embedding)
  CLIPModel.pad_input_ids(input_ids, image_inputs)
  CLIPModel.load_weights(weights, torch.Tensor]])
monkey_patch_weight_loader()

# python/sglang/srt/models/commandr.py
layer_norm_func(hidden_states, weight, variance_epsilon)
  LayerNorm.__init__(param_shape, eps)
  LayerNorm.forward(hidden_states, residuals)
  LayerNorm.weight_loader(param, loaded_weight)
  CohereMLP.__init__(config, quant_config, prefix)
  CohereMLP.forward(x)
  CohereAttention.__init__(config, layer_id, quant_config, prefix)
  CohereAttention.forward(positions, hidden_states, forward_batch)
  CohereDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  CohereDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  CohereModel.__init__(config, quant_config, prefix)
  CohereModel.forward(input_ids, positions, forward_batch)
  CohereForCausalLM.__init__(config, quant_config, prefix)
  CohereForCausalLM.forward(input_ids, positions, forward_batch)
  CohereForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/dbrx.py
  DbrxRouter.__init__(config, params_dtype, prefix)
  DbrxRouter.forward(hidden_states)
  DbrxExperts.__init__(config, quant_config, params_dtype, prefix)
  DbrxExperts.weight_loader(param, loaded_weight, weight_name)
  DbrxExperts.forward(hidden_states)
  DbrxAttention.__init__(config, layer_id, quant_config, prefix)
  DbrxAttention.forward(position_ids, hidden_states, forward_batch)
  DbrxFusedNormAttention.__init__(config, layer_id, quant_config, prefix)
  DbrxFusedNormAttention.forward(position_ids, hidden_states, forward_batch)
  DbrxBlock.__init__(config, layer_id, quant_config, prefix)
  DbrxBlock.forward(position_ids, hidden_states, forward_batch)
  DbrxModel.__init__(config, quant_config, prefix)
  DbrxModel.forward(input_ids, position_ids, forward_batch, input_embeds)
  DbrxForCausalLM.__init__(config, quant_config, prefix)
  DbrxForCausalLM.forward(input_ids, positions, forward_batch)
  DbrxForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/deepseek.py
  DeepseekMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, reduce_results, prefix)
  DeepseekMLP.forward(x)
  DeepseekMoE.__init__(config, quant_config, prefix)
  DeepseekMoE.pack_params()
  DeepseekMoE.forward(hidden_states)
  DeepseekAttention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, prefix)
  DeepseekAttention.forward(positions, hidden_states, forward_batch)
  DeepseekDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  DeepseekDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  DeepseekModel.__init__(config, quant_config, prefix)
  DeepseekModel.forward(input_ids, positions, forward_batch, input_embeds)
  DeepseekForCausalLM.__init__(config, quant_config, prefix)
  DeepseekForCausalLM.get_input_embeddings()
  DeepseekForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  DeepseekForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/deepseek_janus_pro.py
named_apply(fn, module, name, depth_first, include_root)
VQ_16()
trunc_normal_tf_(tensor, mean, std, a, b)
nchw_to(x, fmt)
resample_patch_embed(patch_embed, new_size, interpolation, antialias, verbose)
  PatchEmbed.__init__(img_size, patch_size, in_chans, embed_dim, norm_layer, flatten, output_fmt, bias, strict_img_size, dynamic_img_pad)
  PatchEmbed.set_input_size(img_size, Tuple[int, int]]], patch_size, Tuple[int, int]]])
  PatchEmbed.feat_ratio(as_scalar)
  PatchEmbed.dynamic_feat_size(img_size, int])
  PatchEmbed.forward(x)
  Mlp.__init__(in_features, hidden_features, out_features, act_layer, norm_layer, bias, drop, use_conv)
  Mlp.forward(x)
drop_path(x, drop_prob, training, scale_by_keep)
  DropPath.__init__(drop_prob, scale_by_keep)
  DropPath.forward(x)
  DropPath.extra_repr()
  VisionTransformerBlock.__init__(dim, num_heads, mlp_ratio, qkv_bias, qk_norm, proj_drop, attn_drop, init_values, drop_path, act_layer, norm_layer, mlp_layer)
  VisionTransformerBlock.forward(x)
  PatchDropout.__init__(prob, num_prefix_tokens, ordered, return_indices)
  PatchDropout.forward(x)
resample_abs_pos_embed(posemb, new_size, old_size, num_prefix_tokens, interpolation, antialias, verbose)
init_weights()
init_weights_vit_timm(module, name)
  VisionTransformer.__init__(img_size, Tuple[int, int]], patch_size, Tuple[int, int]], in_chans, num_classes, global_pool, 'avg', 'token', 'map'], embed_dim, depth, num_heads, mlp_ratio, qkv_bias, qk_norm, init_values, class_token, no_embed_class, reg_tokens, pre_norm, fc_norm, dynamic_img_size, dynamic_img_pad, drop_rate, pos_drop_rate, patch_drop_rate, proj_drop_rate, attn_drop_rate, drop_path_rate, weight_init, 'jax', 'jax_nlhb', 'moco', ''], embed_layer, _norm_layer, _act_layer, block_fn, mlp_layer, ignore_head)
  VisionTransformer.init_weights(mode, 'jax_nlhb', 'moco', ''])
  VisionTransformer.no_weight_decay()
  VisionTransformer.group_matcher(coarse)
  VisionTransformer.get_classifier()
  VisionTransformer.reset_classifier(num_classes, global_pool)
  VisionTransformer.forward_features(x)
  VisionTransformer.forward_head(x, pre_logits)
  VisionTransformer.forward(x)
model_name_to_cls(cls_name)
  vision_head.__init__(params)
  vision_head.forward(x)
create_siglip_vit(model_name, image_size, select_layer, ckpt_path)
  Normalize.__init__(mean, std, inplace)
  Normalize.forward(tensor)
  Normalize.__repr__()
  CLIPVisionTower.__init__(model_name, image_size, int], int], select_feature, select_layer, select_layers, ckpt_path, pixel_mean, pixel_std)
  CLIPVisionTower.device()
  CLIPVisionTower.dtype()
  CLIPVisionTower.build_vision_tower(vision_tower_params)
  CLIPVisionTower.feature_select(image_forward_outs)
  CLIPVisionTower.forward(images)
  MlpProjector.__init__(cfg)
  MlpProjector.forward(x_or_tuple, torch.Tensor], torch.Tensor])
  LayerScale.__init__(dim, init_values, inplace)
  LayerScale.forward(x)
use_fused_attn(experimental)
  AttentionPoolLatent.__init__(in_features, out_features, embed_dim, num_heads, feat_size, mlp_ratio, qkv_bias, qk_norm, latent_len, latent_dim, pos_embed, pool_type, norm_layer, drop)
  AttentionPoolLatent.init_weights()
  AttentionPoolLatent.forward(x)
  Encoder.__init__(in_channels, ch, ch_mult, num_res_blocks, norm_type, dropout, resamp_with_conv, z_channels)
  Encoder.forward(x)
  Decoder.__init__(z_channels, ch, ch_mult, num_res_blocks, norm_type, dropout, resamp_with_conv, out_channels)
  Decoder.last_layer()
  Decoder.forward(z)
  VectorQuantizer.__init__(n_e, e_dim, beta, entropy_loss_ratio, l2_norm, show_usage)
  VectorQuantizer.forward(z)
  VectorQuantizer.get_codebook_entry(indices, shape, channel_first)
  ResnetBlock.__init__(in_channels, out_channels, conv_shortcut, dropout, norm_type)
  ResnetBlock.forward(x)
  AttnBlock.__init__(in_channels, norm_type)
  AttnBlock.forward(x)
nonlinearity(x)
Normalize(in_channels, norm_type)
  Upsample.__init__(in_channels, with_conv)
  Upsample.forward(x)
  Downsample.__init__(in_channels, with_conv)
  Downsample.forward(x)
compute_entropy_loss(affinity, loss_type, temperature)
  VQModel.__init__(config)
  VQModel.encode(x)
  VQModel.decode(quant)
  VQModel.decode_code(code_b, shape, channel_first)
  VQModel.forward(input)
  MultiModalityCausalLM.__init__(config, quant_config)
  MultiModalityCausalLM.get_image_feature(items)
  MultiModalityCausalLM.get_input_embeddings()
  MultiModalityCausalLM.forward(input_ids, positions, forward_batch, get_embedding)
  MultiModalityCausalLM.prepare_gen_img_embeds(image_ids)
  MultiModalityCausalLM.pad_input_ids(input_ids, image_inputs)
  MultiModalityCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/deepseek_nextn.py
  DeepseekModelNextN.__init__(config, quant_config, prefix)
  DeepseekModelNextN.forward(input_ids, positions, forward_batch, input_embeds)
  DeepseekV3ForCausalLMNextN.__init__(config, quant_config, prefix)
  DeepseekV3ForCausalLMNextN.forward(input_ids, positions, forward_batch)
  DeepseekV3ForCausalLMNextN.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/deepseek_v2.py
  DeepseekV2MLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, reduce_results, prefix, tp_rank, tp_size)
  DeepseekV2MLP.forward(x, forward_batch, should_allreduce_fusion, use_reduce_scatter)
  MoEGate.__init__(config, prefix, is_nextn)
  MoEGate.forward(hidden_states)
  DeepseekV2MoE.__init__(config, layer_id, quant_config, prefix, alt_stream, is_nextn)
  DeepseekV2MoE.get_moe_weights()
  DeepseekV2MoE.forward(hidden_states, forward_batch, should_allreduce_fusion, use_reduce_scatter)
  DeepseekV2MoE.forward_normal_dual_stream(hidden_states, should_allreduce_fusion, use_reduce_scatter)
  DeepseekV2MoE.forward_normal(hidden_states, should_allreduce_fusion, use_reduce_scatter)
  DeepseekV2MoE.forward_cpu(hidden_states, should_allreduce_fusion)
  DeepseekV2MoE.forward_deepep(hidden_states, forward_batch)
  DeepseekV2MoE.op_gate(state)
  DeepseekV2MoE.op_shared_experts(state)
  DeepseekV2MoE.op_select_experts(state)
  DeepseekV2MoE.op_dispatch_a(state)
  DeepseekV2MoE.op_dispatch_b(state)
  DeepseekV2MoE.op_experts(state)
  DeepseekV2MoE.op_combine_a(state)
  DeepseekV2MoE.op_combine_b(state)
  DeepseekV2MoE.op_output(state)
yarn_get_mscale(scale, mscale)
  DeepseekV2AttentionMLA.__init__(config, hidden_size, num_heads, qk_nope_head_dim, qk_rope_head_dim, v_head_dim, q_lora_rank, kv_lora_rank, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, reduce_results, layer_id, prefix, alt_stream)
  DeepseekV2AttentionMLA.dispatch_attn_forward_method(forward_batch)
  DeepseekV2AttentionMLA.op_prepare(state)
  DeepseekV2AttentionMLA.op_core(state)
  DeepseekV2AttentionMLA.forward(positions, hidden_states, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_prepare(positions, hidden_states, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_core(intermediate_state)
  DeepseekV2AttentionMLA.forward_normal_prepare(positions, hidden_states, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_normal_core(q, k, v, forward_batch)
  DeepseekV2AttentionMLA.forward_absorb_prepare(positions, hidden_states, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_absorb_core(q_pe, k_pe, q_nope_out, k_nope, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_prepare(positions, hidden_states, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_cpu_prepare(positions, hidden_states, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_core(q_input, key_cache_buf, val_cache_buf, attn_output, kv_indptr, kv_indices, k_pe_output, cos_sin_cache, positions, attn_logits, num_kv_split, sm_scale, enable_rope_fusion, k_input, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_cpu_core(q_input, k_input, v_input, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_normal_chunked_kv_prepare(positions, hidden_states, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_normal_chunked_kv_core(q, k, v, forward_batch)
  DeepseekV2DecoderLayer.__init__(config, layer_id, quant_config, is_nextn, prefix, alt_stream)
  DeepseekV2DecoderLayer.forward(positions, hidden_states, forward_batch, residual, zero_allocator)
  DeepseekV2DecoderLayer.op_comm_prepare_attn(state, positions, hidden_states, forward_batch, residual, zero_allocator, tbo_subbatch_index)
  DeepseekV2DecoderLayer.op_comm_prepare_mlp(state)
  DeepseekV2DecoderLayer.op_mlp(state)
  DeepseekV2DecoderLayer.op_comm_postprocess_layer(state)
  DeepseekV2Model.__init__(config, quant_config, prefix)
  DeepseekV2Model.get_input_embeddings()
  DeepseekV2Model.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  DeepseekV2ForCausalLM.__init__(config, quant_config, prefix)
  DeepseekV2ForCausalLM.routed_experts_weights_of_layer()
  DeepseekV2ForCausalLM.determine_num_fused_shared_experts(architecture)
  DeepseekV2ForCausalLM.get_input_embeddings()
  DeepseekV2ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  DeepseekV2ForCausalLM.start_layer()
  DeepseekV2ForCausalLM.end_layer()
  DeepseekV2ForCausalLM.post_load_weights(is_nextn, weight_names)
  DeepseekV2ForCausalLM.load_weights(weights, torch.Tensor]], is_nextn)
  DeepseekV2ForCausalLM.get_embed_and_head()
  DeepseekV2ForCausalLM.set_embed_and_head(embed, head)
  DeepseekV2ForCausalLM.get_model_config_for_expert_location(cls, config)

# python/sglang/srt/models/deepseek_vl2.py
  DeepseekVL2MlpProjector.__init__(config, quant_config)
  DeepseekVL2MlpProjector.forward(x)
  DeepseekVL2ForCausalLM.__init__(config, quant_config)
  DeepseekVL2ForCausalLM.forward(input_ids, positions, forward_batch)
  DeepseekVL2ForCausalLM.load_weights(weights, torch.Tensor]])
  DeepseekVL2ForCausalLM.pad_input_ids(input_ids, mm_inputs)
  DeepseekVL2ForCausalLM.get_image_feature(items)

# python/sglang/srt/models/ernie4.py
  MoEGate.__init__(config, prefix)
  MoEGate.forward(hidden_states)
  Ernie4Moe.__init__(config, layer_id, quant_config, prefix)
  Ernie4Moe.forward(hidden_states)
  Ernie4Moe.forward_normal(hidden_states)
  Ernie4DecoderLayer.__init__(config, layer_id, quant_config, prefix, is_mtp)
  Ernie4DecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  Ernie4Model.__init__(config, quant_config, prefix)
  Ernie4Model.forward(input_ids, positions, forward_batch, input_embeds)
  Ernie4_5_ForCausalLM.__init__(config, quant_config, prefix)
  Ernie4_5_ForCausalLM.forward(input_ids, positions, forward_batch)
  Ernie4_5_ForCausalLM.load_weights(weights, torch.Tensor]])
  Ernie4_5_ForCausalLM.get_embed_and_head()
  Ernie4_5_MoeForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/ernie4_eagle.py
  Ernie4ModelMTP.__init__(config, layer_id, prefix, quant_config)
  Ernie4ModelMTP.forward(input_ids, positions, forward_batch, input_embeds)
  Ernie4_5_MoeForCausalLMMTP.__init__(config, quant_config, prefix, mtp_layer_id)
  Ernie4_5_MoeForCausalLMMTP.forward(input_ids, positions, forward_batch)
  Ernie4_5_MoeForCausalLMMTP.load_weights(weights, torch.Tensor]])
  Ernie4_5_MoeForCausalLMMTP.get_embed_and_head()
  Ernie4_5_MoeForCausalLMMTP.set_embed_and_head(embed, head)

# python/sglang/srt/models/exaone.py
  ExaoneGatedMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  ExaoneGatedMLP.forward(x)
  ExaoneAttention.__init__(config, hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], rope_is_neox_style, max_position_embeddings, quant_config, prefix)
  ExaoneAttention.forward(positions, hidden_states, forward_batch)
  ExaoneDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  ExaoneDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  ExaoneModel.__init__(config, quant_config, prefix)
  ExaoneModel.forward(input_ids, positions, forward_batch, input_embeds)
  ExaoneForCausalLM.__init__(config, quant_config, prefix)
  ExaoneForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  ExaoneForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/gemma.py
  GemmaMLP.__init__(hidden_size, intermediate_size, quant_config, prefix)
  GemmaMLP.forward(x)
  GemmaAttention.__init__(hidden_size, num_heads, num_kv_heads, head_dim, layer_id, max_position_embeddings, rope_theta, quant_config, prefix)
  GemmaAttention.forward(positions, hidden_states, forward_batch)
  GemmaDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  GemmaDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  GemmaModel.__init__(config, quant_config, prefix)
  GemmaModel.forward(input_ids, positions, forward_batch, input_embeds)
  GemmaForCausalLM.__init__(config, quant_config, prefix)
  GemmaForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  GemmaForCausalLM.forward_split_prefill(input_ids, positions, forward_batch, split_interval, int], input_embeds)
  GemmaForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/gemma2.py
get_attention_sliding_window_size(config)
  Gemma2MLP.__init__(hidden_size, intermediate_size, hidden_act, hidden_activation, quant_config, prefix)
  Gemma2MLP.forward(x)
  Gemma2Attention.__init__(layer_id, config, hidden_size, num_heads, num_kv_heads, head_dim, max_position_embeddings, rope_theta, quant_config, prefix)
  Gemma2Attention.forward(positions, hidden_states, forward_batch)
  Gemma2DecoderLayer.__init__(layer_id, config, quant_config, prefix)
  Gemma2DecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  Gemma2Model.__init__(config, quant_config, prefix)
  Gemma2Model.forward(input_ids, positions, forward_batch, input_embeds)
  Gemma2ForCausalLM.__init__(config, quant_config, prefix)
  Gemma2ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  Gemma2ForCausalLM.forward_split_prefill(input_ids, positions, forward_batch, split_interval, int], input_embeds)
  Gemma2ForCausalLM.get_attention_sliding_window_size()
  Gemma2ForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/gemma2_reward.py
  Gemma2ForSequenceClassification.__init__(config, quant_config, prefix)
  Gemma2ForSequenceClassification.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  Gemma2ForSequenceClassification.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/gemma3_causal.py
get_attention_sliding_window_size(config)
extract_layer_index(prefix)
  Gemma3MLP.__init__(hidden_size, intermediate_size, hidden_activation, quant_config, prefix)
  Gemma3MLP.forward(x)
  Gemma3Attention.__init__(layer_id, config, max_position_embeddings, quant_config, prefix)
  Gemma3Attention.naive_attn_with_masks(q, k, v, out)
  Gemma3Attention.forward(hidden_states, position_embeddings, torch.Tensor], forward_batch)
  Gemma3DecoderLayer.__init__(layer_id, config, quant_config, prefix)
  Gemma3DecoderLayer.forward(positions, hidden_states, position_embeddings_global, position_embeddings_local, forward_batch)
  Gemma3RotaryEmbedding.__init__(config, device)
  Gemma3RotaryEmbedding.forward(x, position_ids)
  Gemma3TextScaledWordEmbedding.__init__(num_embeddings, embedding_dim, padding_idx, embed_scale)
  Gemma3TextScaledWordEmbedding.forward(input_ids)
  Gemma3TextModel.__init__(config, quant_config, prefix)
  Gemma3TextModel.forward(input_ids, positions, forward_batch, input_embeds)
  Gemma3ForCausalLM.__init__(config, quant_config, prefix)
  Gemma3ForCausalLM.get_input_embeddings()
  Gemma3ForCausalLM.get_attention_sliding_window_size()
  Gemma3ForCausalLM.dtype()
  Gemma3ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  Gemma3ForCausalLM.forward_split_prefill(input_ids, positions, forward_batch, split_interval, int], input_embeds)
  Gemma3ForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/gemma3_mm.py
  Gemma3MultiModalProjector.__init__(config)
  Gemma3MultiModalProjector.forward(vision_outputs)
  Gemma3ForConditionalGeneration.__init__(config, quant_config, prefix)
  Gemma3ForConditionalGeneration.pad_input_ids(input_ids, image_inputs)
  Gemma3ForConditionalGeneration.prepare_attn_masks(input_ids, positions, mask_dtype)
  Gemma3ForConditionalGeneration.get_input_embeddings()
  Gemma3ForConditionalGeneration.get_attention_sliding_window_size()
  Gemma3ForConditionalGeneration.get_image_feature(items)
  Gemma3ForConditionalGeneration.forward(input_ids, positions, forward_batch, input_embeds)
  Gemma3ForConditionalGeneration.tie_weights()
  Gemma3ForConditionalGeneration.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/gemma3n_audio.py
  Gemma3nCumulativeGroupNorm.__init__(num_channels, feature_dims, eps)
  Gemma3nCumulativeGroupNorm.forward(x, mask)
  Gemma3nAudioRelativePositionEmbedding.__init__(config, quant_config, prefix)
  Gemma3nAudioRelativePositionEmbedding.forward(queries, keys)
  Gemma3nAudioAttention.__init__(config, quant_config, prefix)
  Gemma3nAudioAttention.forward(x, mask)
  Gemma3nAudioSSCPConvBlock.__init__(config, idx, input_freq_dim, manual_padding, int, int, int], quant_config, prefix)
  Gemma3nAudioSSCPConvBlock.forward(audio_encodings)
  Gemma3nAudioSubSampleConvProjection.__init__(config, quant_config, prefix)
  Gemma3nAudioSubSampleConvProjection.forward(audio_encodings)
  Gemma3nAudioConformerAttention.__init__(config, quant_config, prefix)
  Gemma3nAudioConformerAttention.forward(audio_encodings, audio_mel_mask)
  Gemma3nAudioConformerFeedForward.__init__(config, quant_config, prefix)
  Gemma3nAudioConformerFeedForward.forward(audio_encodings)
  Gemma3nAudioConformerLightConv1d.__init__(config, quant_config, prefix)
  Gemma3nAudioConformerLightConv1d.forward(audio_encodings)
  Gemma3nAudioConformerBlock.__init__(config, quant_config, prefix)
  Gemma3nAudioConformerBlock.forward(audio_encodings, audio_mel_mask)
  Gemma3nAudioEncoder.__init__(config, quant_config, prefix)
  Gemma3nAudioEncoder.forward(audio_mel, audio_mel_mask)

# python/sglang/srt/models/gemma3n_causal.py
get_attention_sliding_window_size(config)
  Gemma3nRMSNorm.__init__(dim, eps, with_scale)
  Gemma3nRMSNorm.forward(x)
  Gemma3nTextMLP.__init__(hidden_size, intermediate_size, hidden_activation, activation_sparsity, quant_config, prefix)
  Gemma3nTextMLP.forward(x)
  Gemma3nLaurelBlock.__init__(config, quant_config, prefix)
  Gemma3nLaurelBlock.forward(x)
  Gemma3nAltUp.__init__(config, quant_config, prefix)
  Gemma3nAltUp.compute_router_modalities(x)
  Gemma3nAltUp.predict(hidden_states)
  Gemma3nAltUp.correct(predictions, activated)
  Gemma3nAltUp.scale_corrected_output(corrected)
  Gemma3nAltUp.forward(hidden_states, activated)
  Gemma3nAttention.__init__(layer_id, config, max_position_embeddings, quant_config, prefix)
  Gemma3nAttention.forward(hidden_states, positions, torch.Tensor], forward_batch)
  Gemma3nDecoderLayer.__init__(layer_id, config, quant_config, prefix)
  Gemma3nDecoderLayer.forward(positions, hidden_states, per_layer_input, forward_batch)
  Gemma3nTextModel.__init__(config, quant_config, prefix)
  Gemma3nTextModel.get_input_embeddings()
  Gemma3nTextModel.dtype()
  Gemma3nTextModel.get_per_layer_inputs(input_ids)
  Gemma3nTextModel.project_per_layer_inputs(inputs_embeds, per_layer_inputs)
  Gemma3nTextModel.forward(input_ids, positions, forward_batch, input_embeds, per_layer_inputs)
  Gemma3nForCausalLM.__init__(config, quant_config, prefix)
  Gemma3nForCausalLM.get_input_embeddings()
  Gemma3nForCausalLM.get_attention_sliding_window_size()
  Gemma3nForCausalLM.dtype()
  Gemma3nForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, per_layer_inputs)
  Gemma3nForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/gemma3n_mm.py
  Gemma3nMultimodalEmbedder.__init__(multimodal_config, Gemma3nVisionConfig], text_config, quant_config, prefix)
  Gemma3nMultimodalEmbedder.forward(input_ids, inputs_embeds)
  Gemma3nForConditionalGeneration.__init__(config, quant_config, prefix)
  Gemma3nForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)
  Gemma3nForConditionalGeneration.get_input_embeddings()
  Gemma3nForConditionalGeneration.get_attention_sliding_window_size()
  Gemma3nForConditionalGeneration.get_image_feature(items)
  Gemma3nForConditionalGeneration.get_audio_feature(items)
  Gemma3nForConditionalGeneration.get_per_layer_inputs(input_ids)
  Gemma3nForConditionalGeneration.project_per_layer_inputs(inputs_embeds, per_layer_inputs)
  Gemma3nForConditionalGeneration.forward(input_ids, positions, forward_batch, input_embeds)
  Gemma3nForConditionalGeneration.tie_weights()
  Gemma3nForConditionalGeneration.load_weights(weights, torch.Tensor]])
  Gemma3nForConditionalGeneration.should_apply_lora(module_name)
  Gemma3nForConditionalGeneration.get_hidden_dim(module_name)

# python/sglang/srt/models/glm4.py
  Glm4Attention.__init__(config, layer_id, quant_config, prefix)
  Glm4Attention.forward(positions, hidden_states, forward_batch)
  Glm4DecoderLayer.__init__(config, layer_id, quant_config, prefix)
  Glm4DecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  Glm4Model.__init__(config, quant_config, prefix)
  Glm4Model.get_input_embeddings()
  Glm4Model.dtype()
  Glm4Model.forward(input_ids, positions, forward_batch, input_embeds)
  Glm4ForCausalLM.__init__(config, quant_config, prefix)
  Glm4ForCausalLM.forward(input_ids, positions, forward_batch)
  Glm4ForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/glm4_moe.py
  Glm4MoeMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, reduce_results, prefix, tp_rank, tp_size)
  Glm4MoeMLP.forward(x, forward_batch, should_allreduce_fusion)
  Glm4MoeAttention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, partial_rotary_factor, rope_scaling, Any]], max_position_embeddings, head_dim, rms_norm_eps, attention_bias, quant_config, use_qk_norm, prefix, alt_stream)
  Glm4MoeAttention.op_prepare(state)
  Glm4MoeAttention.op_core(state)
  Glm4MoeAttention.forward_prepare(positions, hidden_states, forward_batch)
  Glm4MoeAttention.forward_core(intermediate_state)
  Glm4MoeAttention.forward(positions, hidden_states, forward_batch)
  Glm4MoeGate.__init__(config, prefix, is_nextn)
  Glm4MoeGate.forward(hidden_states)
  Glm4MoeSparseMoeBlock.__init__(config, layer_id, quant_config, prefix, alt_stream, is_nextn)
  Glm4MoeSparseMoeBlock.forward_normal_dual_stream(hidden_states, should_allreduce_fusion, use_reduce_scatter)
  Glm4MoeSparseMoeBlock.forward_normal(hidden_states, should_allreduce_fusion, use_reduce_scatter)
  Glm4MoeDecoderLayer.__init__(config, layer_id, quant_config, is_nextn, prefix, alt_stream)
  Glm4MoeDecoderLayer.forward(positions, hidden_states, forward_batch, residual, zero_allocator)
  Glm4MoeModel.__init__(config, quant_config, prefix)
  Glm4MoeForCausalLM.__init__(config, quant_config, prefix)
  Glm4MoeForCausalLM.determine_num_fused_shared_experts(architecture)
  Glm4MoeForCausalLM.get_input_embeddings()
  Glm4MoeForCausalLM.load_weights(weights, torch.Tensor]], is_nextn)

# python/sglang/srt/models/glm4_moe_nextn.py
  Glm4MoeModelNextN.__init__(config, quant_config, prefix)
  Glm4MoeModelNextN.forward(input_ids, positions, forward_batch, input_embeds)
  Glm4MoeForCausalLMNextN.__init__(config, quant_config, prefix)
  Glm4MoeForCausalLMNextN.forward(input_ids, positions, forward_batch)
  Glm4MoeForCausalLMNextN.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/glm4v.py
  Glm4vRMSNorm.forward(x)
  Glm4vVisionMLP.__init__(in_features, hidden_features, bias, quant_config, prefix)
  Glm4vVisionMLP.forward(x)
  Glm4vVisionBlock.__init__(config, norm_layer, quant_config, prefix)
  Glm4vVisionPatchEmbed.__init__(patch_size, temporal_patch_size, in_channels, hidden_size)
  Glm4vVisionPatchEmbed.forward(x)
  Glm4vPatchMerger.__init__(d_model, context_dim, quant_config, bias, prefix)
  Glm4vPatchMerger.forward(x)
  Glm4vVisionEmbeddings.__init__(config)
  Glm4vVisionEmbeddings.forward(embeddings, lengths, image_shapes, h_coords, w_coords)
  Glm4vVisionRotaryEmbedding.__init__(dim, theta)
  Glm4vVisionRotaryEmbedding.update_freqs_cache(seqlen)
  Glm4vVisionRotaryEmbedding.forward(seqlen)
  Glm4vVisionModel.__init__(vision_config, norm_eps, quant_config, prefix)
  Glm4vVisionModel.dtype()
  Glm4vVisionModel.device()
  Glm4vVisionModel.rot_pos_emb(grid_thw)
  Glm4vVisionModel.forward(x, grid_thw)
  Glm4vForConditionalGeneration.__init__(config, quant_config, prefix)
  Glm4vForConditionalGeneration.get_image_feature(items)
  Glm4vForConditionalGeneration.get_video_feature(items)
  Glm4vForConditionalGeneration.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/glm4v_moe.py
  Glm4vMoeForConditionalGeneration.__init__(config, quant_config, prefix)
  Glm4vMoeForConditionalGeneration.determine_num_fused_shared_experts(architecture)
  Glm4vMoeForConditionalGeneration.load_weights(weights, torch.Tensor]], is_nextn)

# python/sglang/srt/models/gpt2.py
  GPT2Attention.__init__(layer_id, config, quant_config, prefix)
  GPT2Attention.forward(hidden_states, forward_batch)
  GPT2MLP.__init__(intermediate_size, config, act_layer, quant_config, prefix)
  GPT2MLP.forward(hidden_states)
  GPT2Block.__init__(layer_id, config, act_layer, quant_config, prefix)
  GPT2Block.forward(hidden_states, forward_batch)
  GPT2Model.__init__(config, quant_config, prefix)
  GPT2Model.forward(input_ids, position_ids, forward_batch)
  GPT2LMHeadModel.__init__(config, quant_config, prefix)
  GPT2LMHeadModel.forward(input_ids, positions, forward_batch)
  GPT2LMHeadModel.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/gpt_bigcode.py
  GPTBigCodeAttention.__init__(layer_id, config, quant_config, prefix)
  GPTBigCodeAttention.forward(hidden_states, forward_batch)
  GPTBigMLP.__init__(intermediate_size, config, quant_config, prefix)
  GPTBigMLP.forward(hidden_states)
  GPTBigCodeBlock.__init__(layer_id, config, quant_config, prefix)
  GPTBigCodeBlock.forward(hidden_states, forward_batch)
  GPTBigCodeModel.__init__(config, quant_config, prefix)
  GPTBigCodeModel.forward(input_ids, position_ids, forward_batch)
  GPTBigCodeForCausalLM.__init__(config, quant_config, prefix)
  GPTBigCodeForCausalLM.forward(input_ids, positions, forward_batch)
  GPTBigCodeForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/gpt_oss.py
  GptOssConfig.__init__()
get_attention_sliding_window_size(config)
  GptOssSparseMoeBlock.__init__(layer_id, config, quant_config, prefix)
  GptOssSparseMoeBlock.forward(hidden_states, forward_batch, should_allreduce_fusion)
  GptOssSparseMoeBlock.get_moe_weights()
  GptOssSparseMoeBlock.forward_normal(hidden_states, should_allreduce_fusion)
  GptOssAttention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], max_position_embeddings, head_dim, rms_norm_eps, attention_bias, quant_config, prefix, sliding_window_size, layer_type, params_dtype)
  GptOssAttention.forward_prepare(positions, hidden_states, forward_batch)
  GptOssAttention.forward_core(intermediate_state)
  GptOssAttention.forward(positions, hidden_states, forward_batch)
  GptOssDecoderLayer.__init__(config, layer_id, quant_config, prefix, sliding_window_size)
  GptOssDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  GptOssModel.__init__(config, quant_config, prefix, decoder_layer_type)
  GptOssModel.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  GptOssForCausalLM.__init__(config, quant_config, prefix)
  GptOssForCausalLM.routed_experts_weights_of_layer()
  GptOssForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  GptOssForCausalLM.start_layer()
  GptOssForCausalLM.end_layer()
  GptOssForCausalLM.load_weights(weights, torch.Tensor]], is_nextn, weight_name_mapping)
  GptOssForCausalLM.get_embed_and_head()
  GptOssForCausalLM.set_embed_and_head(embed, head)
  GptOssForCausalLM.set_eagle3_layers_to_capture(layer_ids)
  GptOssForCausalLM.get_model_config_for_expert_location(cls, config)
  GptOssForCausalLM.get_attention_sliding_window_size()
  _WeightCreator.__init__(fn)
  _WeightCreator.maybe_materialize(obj)

# python/sglang/srt/models/granite.py
  GraniteMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  GraniteMLP.forward(x)
  GraniteAttention.__init__(config, hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], rope_is_neox_style, max_position_embeddings, quant_config, prefix)
  GraniteAttention.forward(positions, hidden_states, forward_batch)
  GraniteDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  GraniteDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  GraniteModel.__init__(config, quant_config, prefix)
  GraniteModel.forward(input_ids, positions, forward_batch, input_embeds)
  GraniteForCausalLM.__init__(config, quant_config, prefix)
  GraniteForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  GraniteForCausalLM.get_module_name_from_weight_name(name)
  GraniteForCausalLM.get_num_params()
  GraniteForCausalLM.load_weights(weights, torch.Tensor]])
  GraniteForCausalLM.get_weights_by_name(name, truncate_size, tp_size)

# python/sglang/srt/models/granitemoe.py
  GraniteMoeMoE.__init__(num_experts, top_k, hidden_size, intermediate_size, layer_id, params_dtype, quant_config, tp_size, prefix)
  GraniteMoeMoE.forward(hidden_states)
  GraniteMoeAttention.__init__(hidden_size, num_heads, num_kv_heads, max_position, layer_id, rope_theta, quant_config, attention_multiplier, prefix)
  GraniteMoeAttention.forward(positions, hidden_states, forward_batch)
  GraniteMoeDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  GraniteMoeDecoderLayer.forward(positions, hidden_states, forward_batch)
  GraniteMoeModel.__init__(config, quant_config, prefix)
  GraniteMoeModel.get_input_embeddings(input_ids)
  GraniteMoeModel.forward(input_ids, positions, forward_batch, inputs_embeds)
  GraniteMoeForCausalLM.__init__(config, quant_config, prefix)
  GraniteMoeForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  GraniteMoeForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/grok.py
  Grok1MLP.__init__(hidden_size, intermediate_size, layer_id, quant_config, prefix, reduce_results, use_presharded_weights, split_gate_up)
  Grok1MLP.forward(x)
  Grok1MoE.__init__(config, layer_id, num_experts, top_k, hidden_size, intermediate_size, params_dtype, quant_config, tp_size, reduce_results, use_presharded_weights, inplace, no_combine, prefix)
  Grok1MoE.forward(hidden_states)
get_rope_scaling(config)
  ScalingRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, scaling_factor, dtype)
  Grok1Attention.__init__(config, hidden_size, num_heads, num_kv_heads, layer_id, max_position, rope_theta, quant_config, reduce_results, alt_stream, load_presharded_attn, prefix)
  Grok1Attention.forward(positions, hidden_states, forward_batch)
  Grok1DecoderLayer.__init__(config, layer_id, quant_config, load_presharded_moe, load_presharded_attn, load_presharded_mlp, alt_stream, skip_moe, prefix)
  Grok1DecoderLayer.forward(positions, hidden_states, forward_batch, residual, deferred_norm)
  Grok1DecoderLayer.moe_with_rmoe(x)
  Grok1Model.__init__(config, quant_config, load_presharded_moe, load_presharded_embedding, load_presharded_attn, load_presharded_mlp, replicate_embedding, prefix)
  Grok1Model.forward(input_ids, positions, forward_batch, input_embeds)
  Grok1ForCausalLM.__init__(config, quant_config, prefix)
  Grok1ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  Grok1ForCausalLM.load_weights(weights, torch.Tensor]], ignore_parent_name, check_hit_names, model_config)
  Grok1ForCausalLM.get_num_params_analytical()
  Grok1ForCausalLM.get_num_params_torch()

# python/sglang/srt/models/hunyuan.py
  HunYuanMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, bias, prefix, reduce_results)
  HunYuanMLP.forward(x)
  HunYuanSparseMoeBlock.__init__(config, quant_config, layer_id)
  HunYuanSparseMoeBlock.forward(hidden_states)
get_head_dim(config)
check_head_dim(config)
  HunYuanAttention.__init__(config, hidden_size, num_heads, num_kv_heads, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, bias, prefix, attention_type, layer_id)
  HunYuanAttention.forward(positions, hidden_states, forward_batch, kv_states)
  HunYuanDecoderLayer.__init__(config, quant_config, prefix, layer_id)
  HunYuanDecoderLayer.forward(positions, hidden_states, forward_batch, residual, kv_states)
  HunYuanModel.__init__(config, quant_config, prefix)
  HunYuanModel.get_input_embeddings(input_ids)
  HunYuanModel.forward(input_ids, positions, forward_batch, input_embeds)
  HunYuanMoEV1ForCausalLM.__init__(config, quant_config)
  HunYuanMoEV1ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  HunYuanMoEV1ForCausalLM.load_weights(weights, torch.Tensor]])
  HunYuanMoEV1ForCausalLM.load_kv_cache_scales(quantization_param_path)

# python/sglang/srt/models/idefics2.py
  Idefics2VisionMLP.__init__(config, quant_config, prefix)
  Idefics2VisionMLP.forward(hidden_states)
  Idefics2EncoderLayer.__init__(config, quant_config, prefix)
  Idefics2EncoderLayer.forward(hidden_states, cu_seqlens)
  Idefics2Encoder.__init__(config, quant_config, prefix)
  Idefics2Encoder.forward(inputs_embeds, cu_seqlens)
  Idefics2VisionEmbeddings.__init__(config)
  Idefics2VisionEmbeddings.get_position_ids(pixel_values, patch_attention_mask, tgt_sizes)
  Idefics2VisionEmbeddings.forward(pixel_values, patch_attention_mask, tgt_sizes)
  Idefics2VisionTransformer.__init__(config, quant_config, require_post_norm, prefix)
  Idefics2VisionTransformer.get_input_embeddings()
  Idefics2VisionTransformer.compute_cu_seqlens(tgt_sizes, input_embeds)
  Idefics2VisionTransformer.forward(pixel_values, patch_attention_mask, tgt_sizes)

# python/sglang/srt/models/internlm2.py
  InternLM2MLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  InternLM2MLP.forward(x)
  InternLM2Attention.__init__(hidden_size, num_heads, num_kv_heads, rope_theta, rope_scaling, Any]], max_position_embeddings, layer_id, quant_config, prefix)
  InternLM2Attention.forward(positions, hidden_states, forward_batch)
  InternLMDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  InternLMDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  InternLM2Model.__init__(config, quant_config, prefix)
  InternLM2Model.forward(input_ids, positions, forward_batch, input_embeds)
  InternLM2ForCausalLM.__init__(config, quant_config, prefix)
  InternLM2ForCausalLM.get_input_embeddings()
  InternLM2ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  InternLM2ForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/internlm2_reward.py
  InternLM2ForRewardModel.__init__(config, quant_config, prefix)
  InternLM2ForRewardModel.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  InternLM2ForRewardModel.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/interns1.py
  InternS1ForConditionalGeneration.__init__(config, quant_config, use_flash_attn)
  InternS1ForConditionalGeneration.pixel_shuffle(x, scale_factor)
  InternS1ForConditionalGeneration.extract_feature(pixel_values)
  InternS1ForConditionalGeneration.get_image_feature(items)
  InternS1ForConditionalGeneration.forward(input_ids, positions, forward_batch, input_embeds)
  InternS1ForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)
  InternS1ForConditionalGeneration.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/internvl.py
  InternAttention.__init__(config, quant_config)
  InternAttention.forward(hidden_states, cu_seqlens)
  InternVisionEmbeddings.__init__(config)
  InternVisionEmbeddings.forward(pixel_values)
  InternRMSNorm.__init__(hidden_size, eps)
  InternRMSNorm.forward(hidden_states)
  InternMLP.__init__(config)
  InternMLP.forward(hidden_states)
  InternVisionEncoderLayer.__init__(config, drop_path_rate, quant_config)
  InternVisionEncoderLayer.forward(hidden_states, cu_seqlens)
  InternVisionEncoder.__init__(config, quant_config)
  InternVisionEncoder.forward(inputs_embeds, output_hidden_states, return_dict)
  InternVisionModel.__init__(config, quant_config)
  InternVisionModel.resize_pos_embeddings(old_size, new_size, patch_size)
  InternVisionModel.get_input_embeddings()
  InternVisionModel.forward(pixel_values, output_hidden_states, return_dict, pixel_embeds)
  InternVLChatModel.__init__(config, quant_config, use_flash_attn)
  InternVLChatModel.pixel_shuffle(x, scale_factor)
  InternVLChatModel.extract_feature(pixel_values)
  InternVLChatModel.get_image_feature(items)
  InternVLChatModel.forward(input_ids, positions, forward_batch, input_embeds)
  InternVLChatModel.pad_input_ids(input_ids, mm_inputs)
  InternVLChatModel.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/kimi_vl.py
  KimiVLMultiModalProjector.__init__(config)
  KimiVLMultiModalProjector.forward(image_features)
  KimiVLForConditionalGeneration.__init__(config, quant_config, prefix)
  KimiVLForConditionalGeneration.get_image_feature(items)
  KimiVLForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)
  KimiVLForConditionalGeneration.forward(input_ids, positions, forward_batch, get_embedding)
  KimiVLForConditionalGeneration.load_weights(weights, torch.Tensor]])
get_spec_layer_idx_from_weight_name(config, weight_name)

# python/sglang/srt/models/kimi_vl_moonvit.py
multihead_attention(q, k, v, q_cu_seqlens, k_cu_seqlens)
sdpa_attention(q, k, v, q_cu_seqlens, k_cu_seqlens)
apply_rope(xq, xk, freqs_cis)
  Learnable2DInterpPosEmb.__init__(height, width, dim, interpolation_mode)
  Learnable2DInterpPosEmb.reset_parameters()
  Learnable2DInterpPosEmb.forward(x, grid_hws)
  MoonVisionPatchEmbed.__init__(out_dim, in_dim, patch_size, Tuple[int, int]], pos_emb_height, pos_emb_width)
  MoonVisionPatchEmbed.forward(x, grid_hw)
  Rope2DPosEmb.__init__(dim, max_height, max_width, theta_base, device)
  Rope2DPosEmb.extra_repr()
  Rope2DPosEmb.precomputed_freqs_cis()
  Rope2DPosEmb.get_freqs_cis_by_seqlens(grid_hws)
  Rope2DPosEmb.get_freqs_cis_by_idx(pos_idx, pos_idx_mask)
  MLP2.__init__(dims, activation, bias)
  MLP2.forward(x)
  MoonVitEncoderLayer.__init__(num_heads, hidden_dim, mlp_dim)
  MoonVitEncoderLayer.attention_qkvpacked(x, cu_seqlens, rope_freqs_cis)
  MoonVitEncoderLayer.forward(hidden_states, cu_seqlens, rope_freqs_cis, None])
  MoonVitEncoder.__init__(hidden_dim, num_layers, block_cfg)
  MoonVitEncoder.forward(hidden_states, grid_hw)
patch_merger(x, grid_hw, merge_kernel_size, int])
  MoonVitVLProjector.__init__(in_channels, merge_kernel_size, int], hidden_act, ln_eps, out_dim)
  MoonVitVLProjector.forward(hidden_states)
  MoonVitPretrainedModel.__init__(config)
  MoonVitPretrainedModel.forward(pixel_values, grid_hw)

# python/sglang/srt/models/llama.py
  LlamaMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix, reduce_results)
  LlamaMLP.forward(x, forward_batch, use_reduce_scatter)
  LlamaAttention.__init__(config, hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], rope_is_neox_style, max_position_embeddings, quant_config, prefix, bias)
  LlamaAttention.forward(positions, hidden_states, forward_batch)
  LlamaDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  LlamaDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  LlamaModel.__init__(config, quant_config, prefix)
  LlamaModel.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  LlamaModel.load_kv_cache_scales(quantization_param_path)
  LlamaForCausalLM.__init__(config, quant_config, prefix)
  LlamaForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, get_embedding, pp_proxy_tensors)
  LlamaForCausalLM.forward_split_prefill(input_ids, positions, forward_batch, split_interval, int], input_embeds)
  LlamaForCausalLM.start_layer()
  LlamaForCausalLM.end_layer()
  LlamaForCausalLM.get_input_embeddings()
  LlamaForCausalLM.get_module_name_from_weight_name(name)
  LlamaForCausalLM.get_num_params()
  LlamaForCausalLM.load_weights(weights, torch.Tensor]])
  LlamaForCausalLM.get_weights_by_name(name, truncate_size, tp_size)
  LlamaForCausalLM.get_embed_and_head()
  LlamaForCausalLM.set_embed_and_head(embed, head)
  LlamaForCausalLM.get_embed()
  LlamaForCausalLM.set_embed(embed)
  LlamaForCausalLM.load_kv_cache_scales(quantization_param_path)
  LlamaForCausalLM.set_eagle3_layers_to_capture(layer_ids)

# python/sglang/srt/models/llama4.py
  Llama4MoE.custom_routing_function(hidden_states, gating_output, topk, renormalize)
  Llama4MoE.__init__(config, layer_id, quant_config, prefix)
  Llama4MoE.forward(hidden_states, forward_batch, use_reduce_scatter)
  Llama4Attention.__init__(config, layer_id, hidden_size, num_heads, num_kv_heads, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, bias, bias_o_proj, prefix)
  Llama4Attention.forward(positions, hidden_states, forward_batch)
  Llama4DecoderLayer.__init__(config, layer_id, quant_config, prefix)
  Llama4DecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  Llama4Model.__init__(config, quant_config, prefix)
  Llama4Model.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  Llama4ForCausalLM.__init__(config, quant_config, prefix)
  Llama4ForCausalLM.get_input_embeddings()

# python/sglang/srt/models/llama_classification.py
  LlamaForClassification.__init__(config, quant_config, prefix)
  LlamaForClassification.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  LlamaForClassification.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/llama_eagle.py
  LlamaDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  LlamaModel.__init__(config, quant_config, prefix)
  LlamaModel.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  LlamaForCausalLMEagle.__init__(config, quant_config, prefix)
  LlamaForCausalLMEagle.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/llama_eagle3.py
  LlamaDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  LlamaDecoderLayer.forward(positions, embeds, hidden_states, forward_batch, residual)
  LlamaModel.__init__(config, quant_config, prefix)
  LlamaModel.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  LlamaForCausalLMEagle3.__init__(config, quant_config, prefix)
  LlamaForCausalLMEagle3.load_weights(weights, torch.Tensor]])
  LlamaForCausalLMEagle3.get_hot_token_id()

# python/sglang/srt/models/llama_embedding.py
  LlamaEmbeddingModel.__init__(config, quant_config, prefix)
  LlamaEmbeddingModel.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  LlamaEmbeddingModel.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/llama_reward.py
  LlamaForSequenceClassification.__init__(config, quant_config, prefix)
  LlamaForSequenceClassification.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  LlamaForSequenceClassification.load_weights(weights, torch.Tensor]])
  Weights.__init__(hidden_size, num_label)
  Weights.forward(x)
  LlamaForSequenceClassificationWithNormal_Weights.__init__(config, quant_config, prefix)
  LlamaForSequenceClassificationWithNormal_Weights.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  LlamaForSequenceClassificationWithNormal_Weights.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/llava.py
  LlavaBaseForCausalLM.pad_input_ids(input_ids, image_inputs)
  LlavaBaseForCausalLM.encode_images(pixel_values, List[torch.Tensor]])
  LlavaBaseForCausalLM.forward(input_ids, positions, forward_batch)
  LlavaBaseForCausalLM.load_weights(weights, torch.Tensor]])
  LlavaBaseForCausalLM.num_patches_per_side()
  LlavaLlamaForCausalLM.__init__(config, quant_config, prefix)
  LlavaQwenForCausalLM.__init__(config, quant_config, prefix)
  LlavaMistralForCausalLM.__init__(config, quant_config, prefix)
  LlavaForConditionalGeneration.dtype()
  LlavaForConditionalGeneration.pad_input_ids(input_ids, image_inputs)
  LlavaForConditionalGeneration.__init__(config, quant_config, prefix)
  LlavaForConditionalGeneration.get_image_feature(items)
  LlavaForConditionalGeneration.forward(input_ids, positions, forward_batch, get_embedding)
  LlavaForConditionalGeneration.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/llavavid.py
  LlavaVidForCausalLM.__init__(config, quant_config, prefix)
  LlavaVidForCausalLM.pad_input_ids(input_ids, image_inputs)
  LlavaVidForCausalLM.encode_images(pixel_values)
  LlavaVidForCausalLM.forward(input_ids, positions, forward_batch)
  LlavaVidForCausalLM.load_weights(weights, torch.Tensor]])
  LlavaVidForCausalLM.num_patches_per_side()

# python/sglang/srt/models/mimo.py
  MiMoModel.__init__(config, quant_config, prefix)
  MiMoForCausalLM.__init__(config, quant_config, prefix)
  MiMoForCausalLM.get_input_embeddings(input_ids)
  MiMoForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  MiMoForCausalLM.load_weights(weights, torch.Tensor]])
  MiMoForCausalLM.get_embed_and_head()
  MiMoForCausalLM.set_embed_and_head(embed, head)
  MiMoForCausalLM.load_kv_cache_scales(quantization_param_path)

# python/sglang/srt/models/mimo_mtp.py
  MiMoMultiTokenPredictorLayer.__init__(config, prefix, quant_config)
  MiMoMultiTokenPredictorLayer.forward(input_ids, positions, forward_batch, input_embeds)
  MiMoMTP.__init__(config, quant_config, prefix)
  MiMoMTP.forward(input_ids, positions, forward_batch)
  MiMoMTP.load_weights(weights, torch.Tensor]])
  MiMoMTP.map_model_name_to_mtp_param_name(name)
  MiMoMTP.get_embed_and_head()
  MiMoMTP.set_embed_and_head(embed, head)

# python/sglang/srt/models/minicpm.py
  MiniCPMMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  MiniCPMMLP.forward(x)
  MiniCPMAttention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, prefix)
  MiniCPMAttention.forward(positions, hidden_states, forward_batch)
  MiniCPMDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  MiniCPMDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  MiniCPMModel.__init__(config, quant_config, prefix)
  MiniCPMModel.forward(input_ids, positions, forward_batch, input_embeds)
  MiniCPMForCausalLM.__init__(config, quant_config, prefix)
  MiniCPMForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  MiniCPMForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/minicpm3.py
  MiniCPM3MLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  MiniCPM3MLP.forward(x)
input_to_float8(x, dtype)
  MiniCPM3AttentionMLA.__init__(config, hidden_size, num_heads, qk_nope_head_dim, qk_rope_head_dim, v_head_dim, q_lora_rank, kv_lora_rank, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, layer_id, prefix)
  MiniCPM3AttentionMLA.forward(positions, hidden_states, forward_batch)
  MiniCPM3DecoderLayer.__init__(config, layer_id, quant_config, prefix)
  MiniCPM3DecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  MiniCPM3Model.__init__(config, quant_config, prefix)
  MiniCPM3Model.forward(input_ids, positions, forward_batch, input_embeds)
  MiniCPM3ForCausalLM.__init__(config, quant_config, prefix)
  MiniCPM3ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  MiniCPM3ForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/minicpmo.py
apply_spk_emb(input_ids, spk_emb, input_embeds, spk_emb_token_id, num_spk_embs)
make_streaming_chunk_mask_generation(inputs_embeds, past_seen_tokens, streaming_tts_text_mask, streaming_reserved_length, streaming_audio_chunk_size, streaming_text_chunk_size, num_spk_emb, use_spk_emb)
  ConvNeXtBlock.__init__(dim, intermediate_dim, kernel, dilation, layer_scale_init_value)
  ConvNeXtBlock.forward(x, cond)
  DVAEDecoder.__init__(idim, odim, n_layer, bn_dim, hidden, kernel, dilation, up)
  DVAEDecoder.forward(x, conditioning)
  GFSQ.__init__(dim, levels, G, R, eps, transpose)
  GFSQ.__call__(x)
  GFSQ.forward(x)
  DVAE.__init__()
  DVAE.forward(inp, mode, 'decode'])
  CustomRepetitionPenaltyLogitsProcessorRepeat.__init__(penalty, max_input_ids, past_window)
  CustomRepetitionPenaltyLogitsProcessorRepeat.__call__(input_ids, scores)
  ConditionalChatTTS.__init__(config)
  ConditionalChatTTS.merge_inputs_embeds(input_ids, lm_spk_emb_last_hidden_states)
  ConditionalChatTTS.prefill_text(input_ids, position_ids, past_key_values, torch.Tensor]], lm_spk_emb_last_hidden_states)
  ConditionalChatTTS.prefill_audio_ids(input_ids, past_key_values, torch.Tensor]], streaming_tts_text_mask, add_audio_bos)
  ConditionalChatTTS.generate(input_ids, past_key_values, torch.Tensor]], temperature, eos_token, torch.Tensor], streaming_tts_text_mask, force_no_stop, min_new_token, max_new_token, logits_warpers, logits_processors, show_tqdm)
  ConditionalChatTTS.decode_to_mel_specs(result_list)
  MiniCPMWhisperEncoderLayer.__init__(config, layer_idx)
  MiniCPMWhisperEncoderLayer.forward(hidden_states, attention_mask, layer_head_mask, output_attentions, past_key_values, use_cache)
  MiniCPMWhisperEncoder.__init__(config)
  MiniCPMWhisperEncoder.forward(input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, past_key_values, use_cache)
  MultiModalProjector.__init__(in_dim, out_dim)
  MultiModalProjector.forward(audio_features)
  MiniCPMO.__init__(config, quant_config)
  MiniCPMO.init_tts_module()
  MiniCPMO.init_audio_module()
  MiniCPMO.init_llm(config, quant_config, prefix)
  MiniCPMO.init_vision_module(config, quant_config, prefix)
  MiniCPMO.init_resampler(embed_dim, vision_dim, quant_config, prefix)
  MiniCPMO.pad_input_ids(input_ids, mm_input)
  MiniCPMO.get_audio_embedding_streaming(items)
  MiniCPMO.subsequent_chunk_mask(size, chunk_size, num_left_chunks, device, num_lookhead)
  MiniCPMO.get_audio_embedding(items, chunk_length)
  MiniCPMO.get_audio_feature(items)
  MiniCPMO.get_omni_embedding(items, chunk_length, stream_input)
  MiniCPMO.get_image_feature(items)
  MiniCPMO.forward(input_ids, positions, forward_batch)
  MiniCPMO.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/minicpmv.py
get_1d_sincos_pos_embed_from_grid(embed_dim, pos, version, int])
get_2d_sincos_pos_embed_from_grid(embed_dim, grid, version, int])
get_2d_sincos_pos_embed(embed_dim, grid_size, Tuple[int, int]], cls_token, version, int])
  BaseResampler.__init__(num_queries, embed_dim, num_heads, kv_dim, norm_layer, nn.LayerNorm], do_post_projection, quant_config, prefix)
  Resampler2_5.__init__(num_queries, embed_dim, num_heads, kv_dim, norm_layer, nn.LayerNorm], max_size, int], quant_config, prefix)
  Resampler2_5.forward(x, tgt_sizes)
get_version_by_config(config)
  MiniCPMBaseModel.__init__()
  MiniCPMBaseModel.get_embedding(input_ids, image_inputs)
  MiniCPMBaseModel.get_input_embeddings()
  MiniCPMBaseModel.forward(input_ids, positions, forward_batch)
  MiniCPMBaseModel.init_llm(config, quant_config, prefix)
  MiniCPMBaseModel.init_vision_module(config, quant_config, prefix)
  MiniCPMBaseModel.init_resampler(embed_dim, vision_dim, quant_config, prefix)
  MiniCPMBaseModel.get_vision_embedding(pixel_values, patch_attn_mask, tgt_sizes)
  MiniCPMBaseModel.get_image_feature(items)
  MiniCPMV2_6.__init__(config, quant_config, prefix)
  MiniCPMV2_6.init_llm(config, quant_config, prefix)
  MiniCPMV2_6.init_vision_module(config, quant_config, prefix)
  MiniCPMV2_6.init_resampler(embed_dim, vision_dim, quant_config, prefix)
  MiniCPMV2_6.get_vision_embedding(pixel_values, patch_attn_mask, tgt_sizes)
  MiniCPMV2_6.get_image_feature(items)
  MiniCPMV2_6.pad_input_ids(input_ids, image_inputs)
  MiniCPMV.__init__(config, quant_config, prefix)
  MiniCPMV.__getattr__(name)
  MiniCPMV.__call__()
  MiniCPMV.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/mistral.py
  Mistral3ForConditionalGeneration.__init__()
  Mistral3ForConditionalGeneration.get_image_feature(items)
  Mistral3ForConditionalGeneration.__getattr__(name)
  Mistral3ForConditionalGeneration.__hasattr__(name)
  Mistral3ForConditionalGeneration.__call__()

# python/sglang/srt/models/mixtral.py
  MixtralMoE.__init__(num_experts, top_k, hidden_size, intermediate_size, layer_id, params_dtype, quant_config, tp_size, prefix)
  MixtralMoE.forward(hidden_states)
  MixtralAttention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, max_position, rope_theta, quant_config, prefix)
  MixtralAttention.forward(positions, hidden_states, forward_batch)
  MixtralDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  MixtralDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  MixtralModel.__init__(config, quant_config, prefix)
  MixtralModel.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  MixtralForCausalLM.__init__(config, quant_config, prefix)
  MixtralForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  MixtralForCausalLM.start_layer()
  MixtralForCausalLM.end_layer()
  MixtralForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/mixtral_quant.py
  MixtralMLP.__init__(num_experts, hidden_size, intermediate_size, quant_config, prefix)
  MixtralMLP.forward(hidden_states)
  MixtralMoE.__init__(config, quant_config, prefix)
  MixtralMoE.forward(hidden_states)
  MixtralAttention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, max_position, rope_theta, quant_config, prefix)
  MixtralAttention.forward(positions, hidden_states, forward_batch)
  MixtralDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  MixtralDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  MixtralModel.__init__(config, quant_config, prefix)
  MixtralModel.forward(input_ids, positions, forward_batch, input_embeds)
  QuantMixtralForCausalLM.__init__(config, quant_config, prefix)
  QuantMixtralForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  QuantMixtralForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/mllama.py
  ColumnParallelConv2dPatch.__init__(in_channels, out_channels, kernel_size, Tuple[int, int]], stride, Tuple[int, int]], bias)
  ColumnParallelConv2dPatch.forward(x)
  MllamaPrecomputedAspectRatioEmbedding.__init__(config, is_gated)
  MllamaPrecomputedAspectRatioEmbedding.forward(hidden_state, aspect_ratio_ids)
  MllamaPrecomputedPositionEmbedding.__init__(config)
  MllamaPrecomputedPositionEmbedding.forward(hidden_state, aspect_ratio_ids)
  MllamaVisionMLP.__init__(config, quant_config, prefix)
  MllamaVisionMLP.forward(hidden_states)
  MllamaVisionEncoderLayer.__init__(config, quant_config, is_gated, prefix)
  MllamaVisionEncoderLayer.forward(hidden_state, attention_mask)
  MllamaVisionEncoder.__init__(config, quant_config, num_layers, is_gated, output_hidden_states, prefix)
  MllamaVisionEncoder.forward(hidden_states, attention_mask)
  MllamaVisionModel.__init__(config, quant_config, prefix)
  MllamaVisionModel.apply_class_embedding(hidden_state)
  MllamaVisionModel.forward(pixel_values, aspect_ratio_ids, aspect_ratio_mask)
  MllamaTextRMSNorm.__init__(hidden_size, eps)
  MllamaTextRMSNorm.forward(hidden_states)
  MllamaTextRMSNorm.extra_repr()
  MllamaTextCrossAttention.__init__(config, layer_id, quant_config, prefix)
  MllamaTextCrossAttention.forward(hidden_states, attention_mask, cross_attention_states, forward_batch)
  MllamaCrossAttentionDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  MllamaCrossAttentionDecoderLayer.forward(hidden_states, cross_attention_states, cross_attention_mask, full_text_row_masked_out_mask, forward_batch)
  MllamaTextModel.__init__(config, quant_config, prefix)
  MllamaTextModel.forward(input_ids, positions, cross_attention_states, cross_attention_mask, full_text_row_masked_out_mask, torch.Tensor]], forward_batch, skip_cross_attention)
  MllamaForCausalLM.__init__(config, quant_config, prefix)
  MllamaForCausalLM.forward(input_ids, positions, cross_attention_states, cross_attention_mask, full_text_row_masked_out_mask, torch.Tensor]], forward_batch, skip_cross_attention)
  MllamaForConditionalGeneration.__init__(config, quant_config, prefix)
  MllamaForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)
  MllamaForConditionalGeneration.flat_encoder_result(cross_attention_states, encoder_lens_need)
  MllamaForConditionalGeneration.get_full_text_row_masked_out_mask(forward_batch)
  MllamaForConditionalGeneration.forward(input_ids, positions, forward_batch)
  MllamaForConditionalGeneration.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/mllama4.py
  Llama4VisionMLP.__init__(input_size, intermediate_size, output_size, bias, output_activation, quant_config, prefix, use_data_parallel)
  Llama4VisionMLP.forward(hidden_states)
pixel_shuffle(input_tensor, shuffle_ratio)
  Llama4VisionPixelShuffleMLP.__init__(config, quant_config, prefix, use_data_parallel)
  Llama4VisionPixelShuffleMLP.forward(encoded_patches)
apply_position_embedding(q, k, freqs_ci, shape)
  Llama4VisionEncoderLayer.__init__(config, quant_config, prefix, use_data_parallel)
  Llama4VisionEncoderLayer.forward(hidden_state, freqs_ci)
  Llama4VisionEncoder.__init__(config, quant_config, prefix, use_data_parallel)
  Llama4VisionEncoder.forward(hidden_states, freqs_ci)
  Llama4UnfoldConvolution.__init__(config, quant_config, prefix, use_data_parallel)
  Llama4UnfoldConvolution.forward(hidden_states)
  Llama4VisionRotaryEmbedding.__init__(config)
  Llama4VisionRotaryEmbedding.forward(hidden_states)
  Llama4VisionModel.__init__(config, quant_config, prefix)
  Llama4VisionModel.forward(pixel_values)
  Llama4ForConditionalGeneration.__init__(config, quant_config, prefix)
  Llama4ForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)
  Llama4ForConditionalGeneration.get_image_feature(items)
  Llama4ForConditionalGeneration.forward(input_ids, positions, forward_batch)
  Llama4ForConditionalGeneration.permute_qk_weight_for_rotary(name, loaded_weight)
  Llama4ForConditionalGeneration.load_weights(weights, torch.Tensor]])
  Llama4ForConditionalGeneration.set_eagle3_layers_to_capture(layer_ids)
  Llama4ForConditionalGeneration.get_embed_and_head()
  Llama4ForConditionalGeneration.set_embed_and_head(embed, head)
  Llama4ForConditionalGeneration.get_embed()
  Llama4ForConditionalGeneration.set_embed(embed)

# python/sglang/srt/models/nemotron_nas.py
  DeciLMDecoderLayer.__init__(config, layer_idx, quant_config, prefix)
  DeciLMDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  DeciModel.__init__()
  DeciModel.get_input_embeddings(input_ids)
  DeciModel.forward(input_ids, positions, forward_batch, inputs_embeds, pp_proxy_tensors)
  DeciLMForCausalLM.__init__()
  DeciLMForCausalLM.get_input_embeddings(input_ids)
  DeciLMForCausalLM.forward(input_ids, positions, forward_batch, inputs_embeds, get_embedding, pp_proxy_tensors)
  DeciLMForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/olmo.py
  OlmoAttention.__init__(config, layer_id, quant_config, prefix)
  OlmoAttention.forward(positions, hidden_states, forward_batch)
  OlmoMLP.__init__(config, quant_config, prefix)
  OlmoMLP.forward(x)
  OlmoDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  OlmoDecoderLayer.forward(positions, hidden_states, forward_batch)
  OlmoModel.__init__(config, quant_config, prefix)
  OlmoModel.forward(input_ids, positions, forward_batch, input_embeds)
  OlmoForCausalLM.__init__(config, quant_config, prefix)
  OlmoForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  OlmoForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/olmo2.py
  Olmo2Attention.__init__(config, layer_id, quant_config, prefix)
  Olmo2Attention.forward(positions, hidden_states, forward_batch)
  Olmo2MLP.__init__(config, quant_config, prefix)
  Olmo2MLP.forward(x)
  Olmo2DecoderLayer.__init__(config, layer_id, quant_config, prefix)
  Olmo2DecoderLayer.forward(positions, hidden_states, forward_batch)
  Olmo2Model.__init__(config, quant_config, prefix)
  Olmo2Model.forward(input_ids, positions, forward_batch, input_embeds)
  Olmo2ForCausalLM.__init__(config, quant_config, prefix)
  Olmo2ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  Olmo2ForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/olmoe.py
  OlmoeMoE.__init__(num_experts, top_k, hidden_size, intermediate_size, params_dtype, quant_config, tp_size, layer_id, prefix)
  OlmoeMoE.forward(hidden_states)
  OlmoeAttention.__init__(layer_id, hidden_size, num_heads, num_kv_heads, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, prefix)
  OlmoeAttention.forward(positions, hidden_states, forward_batch)
  OlmoeDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  OlmoeDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  OlmoeModel.__init__(config, quant_config, prefix)
  OlmoeModel.forward(input_ids, positions, forward_batch, input_embeds)
  OlmoeForCausalLM.__init__(config, quant_config, prefix)
  OlmoeForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  OlmoeForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/persimmon.py
  PersimmonMLP.__init__(config, quant_config)
  PersimmonMLP.forward(hidden_states)
  PersimmonAttention.__init__(config, quant_config, prefix, layer_id)
  PersimmonAttention.forward(position_ids, forward_batch, hidden_states)
  PersimmonDecoderLayer.__init__(config, quant_config, prefix, idx)
  PersimmonDecoderLayer.forward(position_ids, forward_batch, hidden_states)
  PersimmonModel.__init__(config, quant_config, prefix)
  PersimmonModel.get_input_embeddings(input_ids)
  PersimmonModel.forward(input_ids, forward_batch, positions, inputs_embeds)
  PersimmonForCausalLM.__init__(config, quant_config, prefix)
  PersimmonForCausalLM.get_input_embeddings(input_ids)
  PersimmonForCausalLM.forward(input_ids, positions, forward_batch, inputs_embeds)
  PersimmonForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/phi.py
  PhiAttention.__init__(config, quant_config, prefix, layer_id)
  PhiAttention.forward(position_ids, forward_batch, hidden_states)
  PhiMLP.__init__(config, quant_config)
  PhiMLP.forward(hidden_states)
  PhiLayer.__init__(config, quant_config, prefix, idx)
  PhiLayer.forward(position_ids, forward_batch, hidden_states)
  PhiModel.__init__(config, quant_config, prefix)
  PhiModel.get_input_embeddings(input_ids)
  PhiModel.forward(input_ids, forward_batch, positions, inputs_embeds)
  PhiForCausalLM.__init__(config, quant_config, prefix)
  PhiForCausalLM.get_input_embeddings(input_ids)
  PhiForCausalLM.forward(input_ids, positions, forward_batch, inputs_embeds)
  PhiForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/phi3_small.py
quick_gelu(x)
gegelu(input, limit)
  Phi3SmallMLP.__init__(config, quant_config, prefix)
  Phi3SmallMLP.forward(x)
  Phi3SmallSelfAttention.__init__(config, layer_id, quant_config, prefix)
  Phi3SmallSelfAttention.forward(positions, hidden_states, forward_batch)
  Phi3SmallDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  Phi3SmallDecoderLayer.forward(positions, hidden_states, forward_batch)
  Phi3SmallModel.__init__(config, quant_config, prefix)
  Phi3SmallModel.get_input_embeddings(input_ids)
  Phi3SmallModel.forward(input_ids, positions, forward_batch, inputs_embeds)
  Phi3SmallForCausalLM.__init__(config, quant_config, prefix)
  Phi3SmallForCausalLM.get_input_embeddings(input_ids)
  Phi3SmallForCausalLM.set_input_embeddings(value)
  Phi3SmallForCausalLM.get_output_embeddings()
  Phi3SmallForCausalLM.set_output_embeddings(value)
  Phi3SmallForCausalLM.set_decoder(decoder)
  Phi3SmallForCausalLM.get_decoder()
  Phi3SmallForCausalLM.compute_logits(input_ids, hidden_states, sampling_metadata)
  Phi3SmallForCausalLM.forward(input_ids, positions, forward_batch, inputs_embeds, get_embedding)
  Phi3SmallForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/phi4mm.py
  Phi4MMImageEncoder.__init__(config, quant_config, prefix, model_dir)
  Phi4MMImageEncoder.get_img_features(img_embeds, attention_mask)
  Phi4MMImageEncoder.forward(pixel_values, image_sizes, image_attention_mask)
  Phi4MMForCausalLM.__init__(config, quant_config, prefix)
  Phi4MMForCausalLM.get_image_feature(items)
  Phi4MMForCausalLM.get_audio_feature(items)
  Phi4MMForCausalLM.forward(input_ids, positions, forward_batch)
  Phi4MMForCausalLM.pad_input_ids(input_ids, mm_inputs)
  Phi4MMForCausalLM.should_apply_lora(module_name)
  Phi4MMForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/phi4mm_audio.py
  ConformerEncoderLayer.__init__(d_model, ext_pw_out_channel, depthwise_seperable_out_channel, depthwise_multiplier, n_head, d_ffn, ext_pw_kernel_size, kernel_size, dropout_rate, causal, batch_norm, activation, chunk_se, chunk_size, conv_activation, conv_glu_type, bias_in_glu, linear_glu_in_convm, attention_inner_dim, attention_glu_type, activation_checkpointing, export, use_pt_scaled_dot_product_attention, attn_group_sizes)
  ConformerEncoderLayer.forward(x, pos_k, pos_v, mask, relative_attention_bias)
  TransformerEncoderBase.__init__(input_size, chunk_size, left_chunk, attention_dim, attention_heads, input_layer, cnn_out, cnn_layer_norm, time_reduction, dropout_rate, padding_idx, relative_attention_bias_args, positional_dropout_rate, nemo_conv_settings, conv2d_extra_padding, 'feat_time', 'none', True], attention_group_size, encoder_embedding_config)
  TransformerEncoderBase.compute_lens_change(feature_lens)
  TransformerEncoderBase.forward()
  TransformerEncoderBase.forward_embeddings(xs_pad, masks, chunk_size_nc, left_chunk_nc)
  TransformerEncoderBase.get_offset()
  ConformerEncoder.__init__(input_size, chunk_size, left_chunk, num_lang, attention_dim, attention_heads, linear_units, num_blocks, dropout_rate, input_layer, causal, batch_norm, cnn_out, cnn_layer_norm, ext_pw_out_channel, ext_pw_kernel_size, depthwise_seperable_out_channel, depthwise_multiplier, chunk_se, kernel_size, activation, conv_activation, conv_glu_type, bias_in_glu, linear_glu_in_convm, attention_glu_type, export, extra_layer_output_idx, extra_multi_layer_output_idxs, activation_checkpointing, relative_attention_bias_args, time_reduction, use_pt_scaled_dot_product_attention, nemo_conv_settings, conv2d_extra_padding, 'feat_time', 'none', True], replication_pad_for_subsample_embedding, attention_group_size, encoder_embedding_config)
  ConformerEncoder.init_relative_attention_bias(input_tensor)
  ConformerEncoder.calculate_hs_mask(xs_pad, device, mask)
  ConformerEncoder.forward(xs_pad, masks)
  WindowQformer.__init__(window_size, num_queries, num_blocks, attention_dim, attention_heads, linear_units, dropout_rate, normalize_before)
  WindowQformer.forward(audio_embed, mask, embed_len)
  AudioEmbedding.__init__(config)
  AudioEmbedding.set_audio_embeds(input_embeds)
  AudioEmbedding.set_audio_embed_sizes(audio_embed_sizes)
  AudioEmbedding.get_audio_features(input_embeds, audio_attention_mask, audio_projection_mode)
  AudioEmbedding.forward(audio_features, audio_attention_mask, audio_projection_mode)

# python/sglang/srt/models/phi4mm_utils.py
  BlockBase.__init__(input_size, output_size)
get_activation(name)
adaptive_enc_mask(x_len, chunk_start_idx, left_window, right_window)
  Swish.__init__()
  Swish.forward(x)
  GLU.__init__(dim, act_name)
  GLU.forward(x)
  GLUPointWiseConv.__init__(input_dim, output_dim, kernel_size, glu_type, bias_in_glu, causal)
  GLUPointWiseConv.forward(x)
  DepthWiseSeperableConv1d.__init__(input_dim, depthwise_seperable_out_channel, kernel_size, depthwise_multiplier, padding)
  DepthWiseSeperableConv1d.forward(x)
  ConvModule.__init__(input_dim, ext_pw_out_channel, depthwise_seperable_out_channel, ext_pw_kernel_size, kernel_size, depthwise_multiplier, dropout_rate, causal, batch_norm, chunk_se, chunk_size, activation, glu_type, bias_in_glu, linear_glu_in_convm, export)
  ConvModule.forward(x)
  GLULinear.__init__(input_dim, output_dim, glu_type, bias_in_glu)
  GLULinear.forward(x)
  FeedForward.__init__(d_model, d_inner, dropout_rate, activation, bias_in_glu)
  FeedForward.forward(x)
  T5RelativeAttentionLogitBias.__init__(num_heads, num_buckets, max_distance, symmetric)
  T5RelativeAttentionLogitBias.forward(x)
  AbsolutePositionalEncoding.__init__(d_model, dropout_rate, max_len)
  AbsolutePositionalEncoding.extend_pe(x)
  AbsolutePositionalEncoding.forward(x)
  MeanVarianceNormLayer.__init__(input_size)
  MeanVarianceNormLayer.forward(input_)
  CausalConv1D.__init__(in_channels, out_channels, kernel_size, stride, padding, int], dilation, groups, bias, padding_mode, device, dtype)
  CausalConv1D.update_cache(x, cache)
  CausalConv1D.forward(x, cache)
  CausalConv2D.__init__(in_channels, out_channels, kernel_size, stride, padding, int], dilation, groups, bias, padding_mode, device, dtype)
  CausalConv2D.forward(x)
  NemoConvSubsampling.__init__(feat_in, feat_out, subsampling_factor, subsampling, conv_channels, subsampling_conv_chunking_factor, activation, is_causal)
  NemoConvSubsampling.get_sampling_frames()
  NemoConvSubsampling.get_streaming_cache_size()
  NemoConvSubsampling.forward(x, mask)
  NemoConvSubsampling.reset_parameters()
  NemoConvSubsampling.conv_split_by_batch(x)
  NemoConvSubsampling.conv_split_by_channel(x)
  NemoConvSubsampling.channel_chunked_conv(conv, chunk_size, x)
  NemoConvSubsampling.change_subsampling_conv_chunking_factor(subsampling_conv_chunking_factor)
calc_length(lengths, all_paddings, kernel_size, stride, ceil_mode, repeat_num)
  AttModule.__init__()
  AttModule.set_export(mode)
  AttModule.forward(x, memory, pos_emb, att_mask)
  AttBlock.memory_dims(max_len)
masked_softmax(scores, mask)
  MultiHeadedAttention.__init__(n_head, n_feat, dropout_rate, attention_inner_dim, glu_type, bias_in_glu, use_pt_scaled_dot_product_attention, n_value, group_size)
  MultiHeadedAttention.forward(query, key, value, pos_k, pos_v, mask, relative_attention_bias)
  MultiSequential.forward()
get_offset(input_layer, time_reduction)
unfold_tensor(xs_pad, max_seq_len)

# python/sglang/srt/models/phimoe.py
  PhiMoEConfig.__init__(vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, head_dim, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, tie_word_embeddings, rope_theta, sliding_window, attention_dropout, num_experts_per_tok, num_local_experts, output_router_logits, router_aux_loss_coef, router_jitter_noise, attention_bias, lm_head_bias)
sparsemixer(scores, jitter_eps)
phimoe_routing_function(hidden_states, gating_output, topk, renormalize)
  PhiMoE.__init__(num_experts, top_k, hidden_size, intermediate_size, layer_id, quant_config, prefix)
  PhiMoE.forward(hidden_states, forward_batch)
  PhiMoEAttention.__init__(hidden_size, num_heads, num_kv_heads, head_dim, max_position, rope_theta, layer_id, attention_bias, quant_config, rope_scaling, prefix)
  PhiMoEAttention.forward(positions, hidden_states, forward_batch)
  PhiMoEDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  PhiMoEDecoderLayer.forward(positions, hidden_states, residual, forward_batch)
  PhiMoEModel.__init__(config, quant_config, prefix)
  PhiMoEModel.forward(input_ids, positions, forward_batch, input_embeds)
  PhiMoEForCausalLM.__init__(config, quant_config, prefix)
  PhiMoEForCausalLM.forward(input_ids, positions, forward_batch, inputs_embeds, get_embedding)
  PhiMoEForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/pixtral.py
  PixtralHFMLP.__init__(config, quant_config)
  PixtralHFMLP.forward(x)
  PixtralHFTransformerBlock.__init__(config, layer_id, quant_config)
  PixtralHFTransformerBlock.forward(hidden_states, attention_mask, position_embeddings, torch.Tensor]])
  PixtralHFTransformer.__init__(config, quant_config)
  PixtralHFTransformer.forward(x, attention_mask, position_embeddings, torch.Tensor]], return_all_hidden_states)
resolve_visual_encoder_outputs(outputs, List[torch.Tensor]], feature_sample_layers, post_norm, num_hidden_layers)
  PixtralHFVisionModel.pad_input_ids(input_ids, mm_inputs)
  PixtralHFVisionModel.__init__(config, quant_config)
  PixtralHFVisionModel.dtype()
  PixtralHFVisionModel.device()
  PixtralHFVisionModel.forward(pixel_values, image_sizes, int]], output_hidden_states, feature_sample_layers)
  PixtralHFVisionModel.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/qwen.py
  QWenMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  QWenMLP.forward(x)
  QWenAttention.__init__(hidden_size, num_heads, max_position_embeddings, layer_id, rope_theta, rope_scaling, Any]], quant_config, prefix)
  QWenAttention.forward(positions, hidden_states, forward_batch)
  QWenBlock.__init__(config, layer_id, quant_config, prefix)
  QWenBlock.forward(positions, hidden_states, forward_batch)
  QWenModel.__init__(config, quant_config, prefix)
  QWenModel.forward(input_ids, positions, forward_batch)
  QWenLMHeadModel.__init__(config, quant_config, prefix)
  QWenLMHeadModel.forward(input_ids, positions, forward_batch)
  QWenLMHeadModel.forward_split_prefill(input_ids, positions, forward_batch, split_interval, int])
  QWenLMHeadModel.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/qwen2.py
  Qwen2MLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  Qwen2MLP.forward(x)
  Qwen2Attention.__init__(hidden_size, num_heads, num_kv_heads, head_dim, layer_id, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, dual_chunk_attention_config, Any]], prefix)
  Qwen2Attention.forward(positions, hidden_states, forward_batch)
  Qwen2DecoderLayer.__init__(config, layer_id, quant_config, prefix, alt_stream)
  Qwen2DecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  Qwen2Model.__init__(config, quant_config, prefix, decoder_layer_type, alt_stream)
  Qwen2Model.get_input_embedding(input_ids)
  Qwen2Model.get_input_embeddings()
  Qwen2Model.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  Qwen2Model.load_kv_cache_scales(quantization_param_path)
  Qwen2ForCausalLM.__init__(config, quant_config, prefix)
  Qwen2ForCausalLM.get_input_embedding(input_ids)
  Qwen2ForCausalLM.get_input_embeddings()
  Qwen2ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, get_embedding, pp_proxy_tensors)
  Qwen2ForCausalLM.forward_split_prefill(input_ids, positions, forward_batch, split_interval, int], input_embeds)
  Qwen2ForCausalLM.start_layer()
  Qwen2ForCausalLM.end_layer()
  Qwen2ForCausalLM.load_weights(weights, torch.Tensor]])
  Qwen2ForCausalLM.get_embed_and_head()
  Qwen2ForCausalLM.set_embed_and_head(embed, head)
  Qwen2ForCausalLM.load_kv_cache_scales(quantization_param_path)

# python/sglang/srt/models/qwen2_5_vl.py
  Qwen2_5_VLMLP.__init__(in_features, hidden_features, bias, hidden_act, quant_config, prefix)
  Qwen2_5_VLMLP.forward(x)
  Qwen2_5_VisionBlock.__init__(dim, intermediate_dim, num_heads, hidden_act, norm_layer, attn_implementation, quant_config, prefix, num_dummy_heads)
  Qwen2_5_VisionBlock.forward(x, cu_seqlens, position_embeddings)
  Qwen2_5_VisionPatchMerger.__init__(dim, context_dim, spatial_merge_size, quant_config, prefix)
  Qwen2_5_VisionPatchMerger.forward(x)
  Qwen2_5_VisionTransformer.__init__(vision_config, norm_eps, quant_config, prefix)
  Qwen2_5_VisionTransformer.get_window_index(grid_thw)
  Qwen2_5_VisionTransformer.dtype()
  Qwen2_5_VisionTransformer.device()
  Qwen2_5_VisionTransformer.rot_pos_emb(grid_thw)
  Qwen2_5_VisionTransformer.forward(x, grid_thw)
  Qwen2_5_VLForConditionalGeneration.__init__(config, quant_config, prefix)
  Qwen2_5_VLForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)
  Qwen2_5_VLForConditionalGeneration.get_image_feature(items)
  Qwen2_5_VLForConditionalGeneration.get_video_feature(items)
  Qwen2_5_VLForConditionalGeneration.get_input_embeddings()
  Qwen2_5_VLForConditionalGeneration.forward(input_ids, positions, forward_batch, get_embedding)
  Qwen2_5_VLForConditionalGeneration.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/qwen2_audio.py
  Qwen2AudioForConditionalGeneration.__init__(config, quant_config, prefix)
  Qwen2AudioForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)
  Qwen2AudioForConditionalGeneration.get_audio_feature(items)
  Qwen2AudioForConditionalGeneration.forward(input_ids, positions, forward_batch)
  Qwen2AudioForConditionalGeneration.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/qwen2_eagle.py
  Qwen2DecoderLayer.__init__(config, layer_id, quant_config, prefix)
  Qwen2Model.__init__(config, quant_config, prefix)
  Qwen2Model.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  Qwen2ForCausalLMEagle.__init__(config, quant_config, prefix)
  Qwen2ForCausalLMEagle.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/qwen2_moe.py
  Qwen2MoeMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, reduce_results, prefix)
  Qwen2MoeMLP.forward(x, use_reduce_scatter)
  Qwen2MoeSparseMoeBlock.__init__(layer_id, config, quant_config, prefix)
  Qwen2MoeSparseMoeBlock.forward(hidden_states, forward_batch, use_reduce_scatter)
  Qwen2MoeAttention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], max_position_embeddings, qkv_bias, quant_config, dual_chunk_attention_config, Any]], prefix)
  Qwen2MoeAttention.forward(positions, hidden_states, forward_batch)
  Qwen2MoeDecoderLayer.__init__(config, layer_id, quant_config, prefix, alt_stream)
  Qwen2MoeDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  Qwen2MoeModel.__init__(config, quant_config, prefix, decoder_layer_type, alt_stream)
  Qwen2MoeModel.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  Qwen2MoeForCausalLM.__init__(config, quant_config, prefix)
  Qwen2MoeForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  Qwen2MoeForCausalLM.forward_split_prefill(input_ids, positions, forward_batch, split_interval, int], input_embeds)
  Qwen2MoeForCausalLM.start_layer()
  Qwen2MoeForCausalLM.end_layer()
  Qwen2MoeForCausalLM.load_weights(weights, torch.Tensor]])
  Qwen2MoeForCausalLM.get_model_config_for_expert_location(cls, config)

# python/sglang/srt/models/qwen2_rm.py
  Qwen2ForRewardModel.__init__(config, quant_config, prefix)
  Qwen2ForRewardModel.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  Qwen2ForRewardModel.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/qwen2_vl.py
  Qwen2VisionMLP.__init__(in_features, hidden_features, act_layer, quant_config, prefix)
  Qwen2VisionMLP.forward(x)
  Qwen2VisionBlock.__init__(dim, num_heads, mlp_ratio, act_layer, norm_layer, attn_implementation, quant_config, prefix)
  Qwen2VisionBlock.forward(x, cu_seqlens, position_embeddings)
  Qwen2VisionPatchEmbed.__init__(patch_size, temporal_patch_size, in_chans, embed_dim)
  Qwen2VisionPatchEmbed.forward(x)
  Qwen2VisionPatchMerger.__init__(d_model, context_dim, norm_layer, spatial_merge_size, quant_config, prefix)
  Qwen2VisionPatchMerger.forward(x)
  Qwen2VisionRotaryEmbedding.__init__(dim, theta)
  Qwen2VisionRotaryEmbedding.update_freqs_cache(seqlen)
  Qwen2VisionRotaryEmbedding.forward(seqlen)
  Qwen2VisionTransformer.__init__(vision_config, norm_eps, quant_config, prefix)
  Qwen2VisionTransformer.dtype()
  Qwen2VisionTransformer.device()
  Qwen2VisionTransformer.rot_pos_emb(grid_thw)
  Qwen2VisionTransformer.forward(x, grid_thw)
  Qwen2VLForConditionalGeneration.__init__(config, quant_config, prefix)
  Qwen2VLForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)
  Qwen2VLForConditionalGeneration.get_image_feature(items)
  Qwen2VLForConditionalGeneration.get_video_feature(items)
  Qwen2VLForConditionalGeneration.get_input_embeddings()
  Qwen2VLForConditionalGeneration.forward(input_ids, positions, forward_batch, get_embedding)
  Qwen2VLForConditionalGeneration.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/qwen3.py
  Qwen3Attention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], head_dim, max_position_embeddings, quant_config, rms_norm_eps, attention_bias, prefix, alt_stream)
  Qwen3Attention.forward(positions, hidden_states, forward_batch)
  Qwen3DecoderLayer.__init__(config, layer_id, quant_config, prefix, alt_stream)
  Qwen3DecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  Qwen3Model.__init__(config, quant_config, prefix)
  Qwen3ForCausalLM.__init__(config, quant_config, prefix)
  Qwen3ForCausalLM.get_input_embeddings()
  Qwen3ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, get_embedding, pp_proxy_tensors)
  Qwen3ForCausalLM.forward_split_prefill(input_ids, positions, forward_batch, split_interval, int], input_embeds)
  Qwen3ForCausalLM.start_layer()
  Qwen3ForCausalLM.end_layer()
  Qwen3ForCausalLM.load_weights(weights, torch.Tensor]])
  Qwen3ForCausalLM.get_embed_and_head()
  Qwen3ForCausalLM.set_embed_and_head(embed, head)
  Qwen3ForCausalLM.load_kv_cache_scales(quantization_param_path)
  Qwen3ForCausalLM.set_eagle3_layers_to_capture(layer_ids)

# python/sglang/srt/models/qwen3_classification.py
  Qwen3ForSequenceClassification.__init__(config, quant_config, prefix)
  Qwen3ForSequenceClassification.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  Qwen3ForSequenceClassification.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/qwen3_moe.py
  Qwen3MoeSparseMoeBlock.__init__(layer_id, config, quant_config, prefix)
  Qwen3MoeSparseMoeBlock.forward(hidden_states, forward_batch, use_reduce_scatter)
  Qwen3MoeSparseMoeBlock.get_moe_weights()
  Qwen3MoeSparseMoeBlock.forward_normal(hidden_states, use_reduce_scatter)
  Qwen3MoeSparseMoeBlock.forward_deepep(hidden_states, forward_batch)
  Qwen3MoeSparseMoeBlock.op_gate(state)
  Qwen3MoeSparseMoeBlock.op_select_experts(state)
  Qwen3MoeSparseMoeBlock.op_dispatch_a(state)
  Qwen3MoeSparseMoeBlock.op_dispatch_b(state)
  Qwen3MoeSparseMoeBlock.op_experts(state)
  Qwen3MoeSparseMoeBlock.op_combine_a(state)
  Qwen3MoeSparseMoeBlock.op_combine_b(state)
  Qwen3MoeSparseMoeBlock.op_output(state)
  Qwen3MoeAttention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], max_position_embeddings, head_dim, rms_norm_eps, attention_bias, quant_config, prefix, dual_chunk_attention_config, Any]], alt_stream)
  Qwen3MoeAttention.op_prepare(state)
  Qwen3MoeAttention.op_core(state)
  Qwen3MoeAttention.forward_prepare(positions, hidden_states, forward_batch)
  Qwen3MoeAttention.forward_core(intermediate_state)
  Qwen3MoeAttention.forward(positions, hidden_states, forward_batch)
  Qwen3MoeDecoderLayer.__init__(config, layer_id, quant_config, prefix, alt_stream)
  Qwen3MoeDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  Qwen3MoeDecoderLayer.op_comm_prepare_attn(state, positions, hidden_states, forward_batch, residual, tbo_subbatch_index)
  Qwen3MoeDecoderLayer.op_comm_prepare_mlp(state)
  Qwen3MoeDecoderLayer.op_mlp(state)
  Qwen3MoeDecoderLayer.op_comm_postprocess_layer(state)
  Qwen3MoeModel.__init__(config, quant_config, prefix)
  Qwen3MoeForCausalLM.__init__(config, quant_config, prefix)
  Qwen3MoeForCausalLM.get_input_embeddings()
  Qwen3MoeForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  Qwen3MoeForCausalLM.forward_split_prefill(input_ids, positions, forward_batch, split_interval, int], input_embeds)
  Qwen3MoeForCausalLM.start_layer()
  Qwen3MoeForCausalLM.end_layer()
  Qwen3MoeForCausalLM.get_embed_and_head()
  Qwen3MoeForCausalLM.set_eagle3_layers_to_capture(layer_ids)
  Qwen3MoeForCausalLM.load_weights(weights, torch.Tensor]])
  Qwen3MoeForCausalLM.get_model_config_for_expert_location(cls, config)

# python/sglang/srt/models/registry.py
  _ModelRegistry.get_supported_archs()
  _ModelRegistry.resolve_model_cls(architectures, List[str]])
import_model_classes()

# python/sglang/srt/models/roberta.py
  RobertaClassificationHead.__init__(config)
  RobertaClassificationHead.forward(features)
  RobertaEmbedding.__init__(config)
  RobertaEmbedding.forward(input_ids, seq_lens, position_ids, forward_batch)
  XLMRobertaBaseModel.__init__()
  XLMRobertaBaseModel.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  XLMRobertaBaseModel.load_weights(weights, torch.Tensor]])
create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length)
  XLMRobertaModel.__init__()
  XLMRobertaModel.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  XLMRobertaModel.load_weights(weights, torch.Tensor]])
  XLMRobertaForSequenceClassification.__init__()
  XLMRobertaForSequenceClassification.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  XLMRobertaForSequenceClassification.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/siglip.py
  SiglipVisionEmbeddings.__init__(config)
  SiglipVisionEmbeddings.forward(pixel_values)
  SiglipMLP.__init__(config, act_layer, quant_config, prefix)
  SiglipMLP.forward(x)
  SiglipEncoderLayer.__init__(config, act_layer, norm_layer, attn_implementation, quant_config, prefix)
  SiglipEncoderLayer.forward(hidden_states, attention_mask, causal_attention_mask)
  SiglipEncoder.__init__(config, quant_config, prefix)
  SiglipEncoder.forward(inputs_embeds, attention_mask, causal_attention_mask, return_all_hidden_states)
  SiglipVisionTransformer.__init__(config, quant_config, prefix)
  SiglipVisionTransformer.device()
  SiglipVisionTransformer.forward(pixel_values)
  SiglipVisionModel.__init__(config, quant_config, prefix)
  SiglipVisionModel.device()
  SiglipVisionModel.forward(pixel_values)

# python/sglang/srt/models/stablelm.py
  StablelmMLP.__init__(config, quant_config, prefix)
  StablelmMLP.forward(x)
  StablelmAttention.__init__(config, layer_id, quant_config, prefix)
  StablelmAttention.forward(positions, hidden_states, forward_batch)
  StablelmDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  StablelmDecoderLayer.forward(positions, hidden_states, forward_batch)
  StableLMEpochModel.__init__(config, quant_config, prefix)
  StableLMEpochModel.forward(input_ids, positions, forward_batch, input_embeds)
  StableLmForCausalLM.__init__(config, quant_config, prefix)
  StableLmForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  StableLmForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/step3_vl.py
  Step3TextMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  Step3TextMLP.forward(x)
  Step3TextMoEMLP.__init__(layer_id, config, quant_config, prefix)
  Step3TextMoEMLP.forward(hidden_states)
  Step3TextAttention.__init__(hidden_size, num_heads, num_kv_heads, head_dim, share_q_dim, layer_id, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, rms_norm_eps, prefix)
  Step3TextAttention.forward(positions, hidden_states, forward_batch)
  Step3TextDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  Step3TextDecoderLayer.moe_mlp_forward(hidden_states)
  Step3TextDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  Step3TextModel.__init__(config, quant_config, prefix)
  Step3TextModel.get_input_embeddings()
  Step3TextModel.forward(input_ids, positions, forward_batch, input_embeds)
get_abs_pos(abs_pos, tgt_size)
  Step3VisionMLP.__init__(dim, intermediate_size, bias, hidden_act, quant_config, prefix)
  Step3VisionMLP.forward(hidden_states)
  Step3VisionAttention.__init__(dim, num_heads, qkv_backend, quant_config, prefix)
  Step3VisionAttention.forward(hidden_states)
  Step3VisionEmbeddings.__init__(config)
  Step3VisionEmbeddings.forward(pixel_values)
  Step3VisionEncoderLayer.__init__(config, attn_implementation)
  Step3VisionEncoderLayer.forward(hidden_states)
  Step3VisionTransformer.__init__(config)
  Step3VisionTransformer.dtype()
  Step3VisionTransformer.forward(pixel_values)
  Step3VisionEncoder.__init__(config)
  Step3VisionEncoder.forward(inputs_embeds)
  Step3VLForConditionalGeneration.__init__(config, quant_config, prefix)
  Step3VLForConditionalGeneration.get_image_feature(items)
  Step3VLForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)
  Step3VLForConditionalGeneration.forward(input_ids, positions, forward_batch, input_embeds)
  Step3VLForConditionalGeneration.load_weights(weights, torch.Tensor]])
  Step3VLForConditionalGeneration.get_model_config_for_expert_location(cls, config)

# python/sglang/srt/models/torch_native_llama.py
gate_up_proj_weight_loader(param, loaded_weight, loaded_shard_id)
  LlamaMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  LlamaMLP.forward(x)
qkv_proj_weight_loader(param, loaded_weight, loaded_shard_id)
  LlamaAttention.__init__(config, hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], rope_is_neox_style, max_position_embeddings, quant_config, prefix)
  LlamaAttention.forward(positions, hidden_states, forward_batch)
  LlamaDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  LlamaDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  LlamaModel.__init__(config, quant_config)
  LlamaModel.forward(input_ids, positions, forward_batch, input_embeds)
  TorchNativeLlamaForCausalLM.__init__(config, quant_config)
  TorchNativeLlamaForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  TorchNativeLlamaForCausalLM.get_module_name_from_weight_name(name)
  TorchNativeLlamaForCausalLM.get_num_params()
  TorchNativeLlamaForCausalLM.load_weights_to_module(fqn, weights, torch.Tensor]])
  TorchNativeLlamaForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/transformers.py
maybe_prefix(prefix, name)
sglang_flash_attention_forward(module, query, key, value, attention_mask, forward_batch, scaling, attention_instances)
  HFColumnParallelLinear.forward(input)
  HFRowParallelLinear.forward(input)
replace_linear_class(linear, style, 'rowwise'], quant_config)
  TransformersForCausalLM.__init__(config, quant_config, prefix)
  TransformersForCausalLM.log_replacement(name, old_module, new_module)
  TransformersForCausalLM.tensor_parallel(tp_size)
  TransformersForCausalLM.replace_vocab_embed_class(module)
  TransformersForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  TransformersForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/vila.py
  VILAConfig.__init__(text_config, Any]], vision_config, Any]])
  DownSample3x3BlockFix.forward(x)
  MultimodalProjector.__init__(config)
  MultimodalProjector.device()
  MultimodalProjector.dtype()
  MultimodalProjector.forward(x)
  VILAForConditionalGeneration.__init__(config, quant_config, prefix)
  VILAForConditionalGeneration.dtype()
  VILAForConditionalGeneration.forward(input_ids, positions, forward_batch, get_embedding)
  VILAForConditionalGeneration.get_image_feature(mm_input)
  VILAForConditionalGeneration.load_weights(weights, Tensor]])
  VILAForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)

# python/sglang/srt/models/xverse.py
  XverseMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  XverseMLP.forward(x)
  XverseAttention.__init__(config, hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], rope_is_neox_style, max_position_embeddings, quant_config, prefix)
  XverseAttention.forward(positions, hidden_states, forward_batch)
  XverseDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  XverseDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  XverseModel.__init__(config, quant_config, prefix)
  XverseModel.forward(input_ids, positions, forward_batch, input_embeds)
  XverseForCausalLM.__init__(config, quant_config, prefix)
  XverseForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  XverseForCausalLM.load_weights(weights, torch.Tensor]], name, loaded_weight)

# python/sglang/srt/models/xverse_moe.py
  XverseMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, reduce_results, prefix)
  XverseMLP.forward(x)
  XverseMoE.__init__(config, quant_config, prefix)
  XverseMoE.pack_params()
  XverseMoE.forward(hidden_states)
  XverseAttention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, prefix)
  XverseAttention.forward(positions, hidden_states, forward_batch)
  XverseDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  XverseDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  XverseModel.__init__(config, quant_config, prefix)
  XverseModel.forward(input_ids, positions, forward_batch)
  XverseMoeForCausalLM.__init__(config, quant_config, prefix)
  XverseMoeForCausalLM.forward(input_ids, positions, forward_batch)
  XverseMoeForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/yivl.py
  YiVLForCausalLM.__init__(config, quant_config, prefix)
  YiVLForCausalLM.load_weights(weights, torch.Tensor]])
  YiVLMultiModalProjector.__init__(config)
  YiVLMultiModalProjector.forward(image_features)

# python/sglang/srt/multimodal/mm_utils.py
has_valid_data(data)
select_best_resolution(original_size, possible_resolutions)
resize_and_pad_image(image, target_resolution)
divide_to_patches(image, patch_size)
get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size)
process_anyres_image(image, processor, grid_pinpoints)
load_image_from_base64(image)
expand2square(pil_img, background_color)
unpad_image(tensor, original_size)
unpad_image_shape(current_height, current_width, original_size)
process_images(images, image_processor, model_cfg)

# python/sglang/srt/multimodal/processors/base_processor.py
  BaseMultiModalProcessorOutput.organize_results()
  MultimodalSpecialTokens.build(processor)
  MultimodalSpecialTokens.convert_to_str(token, int], processor)
  MultimodalSpecialTokens.convert_to_strs(processor)
  MultimodalSpecialTokens.get_modality_of_token(token)
  MultimodalSpecialTokens.get_token_id_by_modality(modality)
  MultimodalSpecialTokens.parse_regex()
  MultimodalSpecialTokens.get_combined_regex()
  BaseMultimodalProcessor.__init__(hf_config, server_args, _processor, transport_mode)
  BaseMultimodalProcessor.process_mm_data(input_text, images, videos, audios)
  BaseMultimodalProcessor.process_mm_data_async(image_data, audio_data, input_text, request_obj)
  BaseMultimodalProcessor.get_estimated_frames_list(image_data)
  BaseMultimodalProcessor.submit_data_loading_tasks(text_parts, multimodal_tokens, data_iterators, Iterator[Any]], discard_alpha_channel, image_estimated_frames_iter, image_scaling_factor, max_image_frames, audio_sample_rate)
  BaseMultimodalProcessor.load_mm_data(prompt, multimodal_tokens, image_data, video_data, audio_data, return_text, discard_alpha_channel, audio_sample_rate)
  BaseMultimodalProcessor.get_mm_items_offset(input_ids, mm_token_id)
  BaseMultimodalProcessor.get_mm_items_offset_by_pair(input_ids, mm_start_id, mm_end_id)
  BaseMultimodalProcessor.collect_mm_items_from_processor_output(data_dict)
  BaseMultimodalProcessor.process_and_combine_mm_data(base_output, mm_tokens)

# python/sglang/srt/multimodal/processors/clip.py
  ClipImageProcessor.__init__(hf_config, server_args, _processor)
  ClipImageProcessor.process_mm_data_async(image_data, bytes]], input_text)

# python/sglang/srt/multimodal/processors/deepseek_vl_v2.py
  DeepseekVL2ImageProcessor.__init__(hf_config, server_args, _processor)
  DeepseekVL2ImageProcessor.process_mm_data_async(image_data, bytes]], input_text, request_obj, max_req_input_len)

# python/sglang/srt/multimodal/processors/gemma3.py
  Gemma3SGLangImageProcessor.__init__(hf_config, server_args, _processor)
  Gemma3SGLangImageProcessor.process_mm_data_async(image_data, bytes, Dict]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/gemma3n.py
  Gemma3nSGLangProcessor.__init__(hf_config, server_args, _processor)
  Gemma3nSGLangProcessor.process_mm_data_async(image_data, bytes, Dict]]], audio_data, bytes, Dict]]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/glm4v.py
  Glm4vImageProcessor.__init__(hf_config, server_args, _processor)
  Glm4vImageProcessor.preprocess_video(vr)
  Glm4vImageProcessor.process_mm_data_async(image_data, bytes]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/internvl.py
  InternVLImageProcessor.__init__(hf_config, server_args, _image_processor)
  InternVLImageProcessor.build_transform(input_size)
  InternVLImageProcessor.dynamic_preprocess(image, min_num, max_num, image_size, use_thumbnail)
  InternVLImageProcessor.get_index(bound, fps, max_frame, first_idx, num_segments)
  InternVLImageProcessor.load_video(video_path, bound, input_size, max_num, num_segments)
  InternVLImageProcessor.process_mm_data_async(image_data, input_text, request_obj)

# python/sglang/srt/multimodal/processors/janus_pro.py
  JanusProImageProcessor.__init__(hf_config, server_args, _processor)
  JanusProImageProcessor.process_mm_data_async(image_data, bytes]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/kimi_vl.py
  KimiVLImageProcessor.__init__(hf_config, server_args, _processor)
  KimiVLImageProcessor.process_mm_data_async(image_data, bytes, Dict]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/llava.py
  LlavaImageProcessor.__init__(hf_config, server_args, _processor)
  LlavaImageProcessor.process_mm_data_async(image_data, bytes, ImageData]], input_text, request_obj)
  LlavaMultimodalProcessor.__init__(hf_config, server_args, _processor)
  LlavaMultimodalProcessor.process_mm_data_async()

# python/sglang/srt/multimodal/processors/minicpm.py
  MiniCPMMultimodalProcessor.__init__(hf_config, server_args, _processor)
  MiniCPMMultimodalProcessor.process_mm_data_async(image_data, bytes]], audio_data, bytes]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/mlama.py
  MllamaImageProcessor.__init__(hf_config, server_args, _processor)
  MllamaImageProcessor.process_mm_data_async(image_data, bytes]], input_text)

# python/sglang/srt/multimodal/processors/mllama4.py
  Mllama4ImageProcessor.__init__(hf_config, server_args, _processor)
  Mllama4ImageProcessor.process_mm_data_async(image_data, bytes]], input_text)

# python/sglang/srt/multimodal/processors/phi4mm.py
  Phi4MMProcessorAdapter.__init__(_processor)
  Phi4MMProcessorAdapter.__call__()
  Phi4MMMultimodalProcessor.__init__(hf_config, server_args, _processor)
  Phi4MMMultimodalProcessor.process_mm_data_async(image_data, bytes]], audio_data, input_text, request_obj)

# python/sglang/srt/multimodal/processors/pixtral.py
  PixtralProcessor.get_patch_grid_size()
  PixtralProcessor.__init__(hf_config, server_args, _processor)
  PixtralProcessor.process_mm_data_async(image_data, bytes]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/qwen_audio.py
  Qwen2AudioMultimodalProcessor.__init__(hf_config, server_args, _processor)
  Qwen2AudioMultimodalProcessor.process_mm_data_async(audio_data, input_text)

# python/sglang/srt/multimodal/processors/qwen_vl.py
smart_resize(height, width, factor, min_pixels, max_pixels)
resize_image(image, size_factor)
round_by_factor(number, factor)
ceil_by_factor(number, factor)
floor_by_factor(number, factor)
resize_image_async(image)
smart_nframes(ele, total_frames, video_fps)
preprocess_video(vr, image_factor)
  Qwen2_5VLImageProcessor.__init__(hf_config, server_args, _processor)
  Qwen2_5VLImageProcessor.process_mm_data_async(image_data, bytes]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/step3_vl.py
  GPUToTensor.forward(raw_image, Image.Image])
  Step3VisionProcessor.__init__(size, interpolation_mode, patch_size)
  Step3VisionProcessor.__call__(image, is_patch)
  ImagePatcher.determine_window_size(long, short)
  ImagePatcher.slide_window(width, height, sizes, int]], steps, int]], img_rate_thr)
  ImagePatcher.square_pad(img)
  ImagePatcher.get_image_size_for_padding(img_width, img_height)
  ImagePatcher.get_image_size_for_preprocess(img_width, img_height)
  ImagePatcher.get_image_size_for_crop(img_width, img_height, window_size)
  ImagePatcher.patch_crop(img, i, j, th, tw)
  ImagePatcher.get_num_patches(img_width, img_height)
  ImagePatcher.__call__(img)
  Step3VLProcessor.__init__(config, tokenizer)
  Step3VLProcessor.image_token_id()
  Step3VLProcessor.get_num_image_tokens(img_width, img_height)
  Step3VLProcessor.replace_placeholder(text, placeholder, repls)
  Step3VLProcessor.__call__(text, list[str]]], images, list[Image.Image]]], return_tensors, TensorType]])
  Step3VLImageProcessor.__init__(hf_config, server_args, _processor)
  Step3VLImageProcessor.preprocess(image)
  Step3VLImageProcessor.__call__(image)
  Step3VLImageProcessor.process_mm_data_async(image_data, bytes]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/vila.py
  VILAMultimodalProcessor.__init__(hf_config, server_args, _processor)
  VILAMultimodalProcessor.process_mm_data_async(image_data, input_text, request_obj)

# python/sglang/srt/offloader.py
  BaseOffloader.wrap_modules(all_modules_generator, None, None], submodule_accessor, whitelist_param_names_creator)
  BaseOffloader.post_init()
get_offloader()
set_offloader(instance)
create_offloader_from_server_args(server_args, dp_rank)
  OffloaderV1.__init__(cpu_offload_max_bytes)
  OffloaderV1.wrap_modules(all_modules_generator, None, None], submodule_accessor, whitelist_param_names_creator)
  OffloaderV1.maybe_offload_to_cpu(module)
  OffloaderV2.__init__(group_size, num_in_group, prefetch_step, mode, dp_rank, dp_size)
  OffloaderV2.wrap_modules(all_modules_generator, None, None], submodule_accessor, whitelist_param_names_creator)
  OffloaderV2.post_init()
  _ModuleOffloader.__init__(mode, module, alt_stream, whitelist_param_names)
  _ModuleOffloader.post_init()
  _ModuleOffloader.start_onload()
  _ModuleOffloader.offload()
  _ModuleOffloader.wait_and_get_device_tensors()
  _BaseParamOffloader.create(mode)
  _BaseParamOffloader.__init__(module, param_name)
  _BaseParamOffloader.post_init()
  _BaseParamOffloader.create_device_tensor()
  _MetaParamOffloader.__init__(module, param_name)
  _MetaParamOffloader.create_device_tensor()
  _CpuParamOffloader.__init__(module, param_name)
  _CpuParamOffloader.create_device_tensor()
  _ShmCpuParamOffloader.__init__(module, param_name)
  _ShmCpuParamOffloader.post_init()
  _ShmCpuParamOffloader.create_device_tensor()
  _ShardedGpuParamOffloader.__init__(module, param_name)
  _ShardedGpuParamOffloader.post_init()
  _ShardedGpuParamOffloader.create_device_tensor()

# python/sglang/srt/operations.py
execute_operations(inputs, operations)
execute_overlapped_operations(inputs_arr, operations_arr, delta_stages)
  _StageExecutor.__init__(debug_name, stages, inputs)
  _StageExecutor.next()
  _StageExecutor.output()
  _StageExecutor.done()
  _StageExecutor.num_stages()
  _StateDict.__init__()
  _StateDict.__setattr__(key, value)
  _StateDict.__getattr__(item)
  _StateDict.__delattr__(item)
  _StateDict.pop(item)
  _StateDict.update(values, Any])
  _StateDict.get(item)
  _StateDict.clear(expect_keys)

# python/sglang/srt/operations_strategy.py
  OperationsStrategy.concat(cls, items)
  OperationsStrategy.init_new_tbo(layers, forward_mode)

# python/sglang/srt/patch_torch.py
monkey_patch_torch_reductions()
monkey_patch_torch_compile()

# python/sglang/srt/poll_based_barrier.py
  PollBasedBarrier.__init__(noop)
  PollBasedBarrier.local_arrive()
  PollBasedBarrier.poll_global_arrived()

# python/sglang/srt/reasoning_parser.py
  StreamingParseResult.__init__(normal_text, reasoning_text)
  BaseReasoningFormatDetector.__init__(think_start_token, think_end_token, force_reasoning, stream_reasoning)
  BaseReasoningFormatDetector.detect_and_parse(text)
  BaseReasoningFormatDetector.parse_streaming_increment(new_text)
  DeepSeekR1Detector.__init__(stream_reasoning, force_reasoning)
  Qwen3Detector.__init__(stream_reasoning, force_reasoning)
  KimiDetector.__init__(stream_reasoning, force_reasoning)
  GptOssDetector.__init__(stream_reasoning, force_reasoning)
  GptOssDetector.detect_and_parse(text)
  GptOssDetector.parse_streaming_increment(new_text)
  ReasoningParser.__init__(model_type, stream_reasoning, force_reasoning)
  ReasoningParser.parse_non_stream(full_text)
  ReasoningParser.parse_stream_chunk(chunk_text)

# python/sglang/srt/sampling/custom_logit_processor.py
  CustomLogitProcessor.__call__(logits, custom_param_list, Any]]])
  CustomLogitProcessor.to_str(cls)
  CustomLogitProcessor.from_str(cls, json_str)
  DisallowedTokensLogitsProcessor.__call__(logits, custom_param_list, Any]]])

# python/sglang/srt/sampling/penaltylib/frequency_penalty.py
  BatchedFrequencyPenalizer.__init__(orchestrator)

# python/sglang/srt/sampling/penaltylib/min_new_tokens.py
  BatchedMinNewTokensPenalizer.__init__(orchestrator)

# python/sglang/srt/sampling/penaltylib/orchestrator.py
  BatchedPenalizerOrchestrator.__init__(vocab_size, batch, penalizers)
  BatchedPenalizerOrchestrator.batch()
  BatchedPenalizerOrchestrator.batch(value)
  BatchedPenalizerOrchestrator.reqs()
  BatchedPenalizerOrchestrator.cumulate_output_tokens(output_ids)
  BatchedPenalizerOrchestrator.apply(logits)
  BatchedPenalizerOrchestrator.filter(keep_indices)
  BatchedPenalizerOrchestrator.merge(their)
  _BatchedPenalizer.is_prepared()
  _BatchedPenalizer.is_required()
  _BatchedPenalizer.prepare()
  _BatchedPenalizer.prepare_if_required()
  _BatchedPenalizer.teardown()
  _BatchedPenalizer.cumulate_output_tokens(output_ids)
  _BatchedPenalizer.apply(logits)
  _BatchedPenalizer.filter(keep_indices)
  _BatchedPenalizer.merge(their)

# python/sglang/srt/sampling/penaltylib/presence_penalty.py
  BatchedPresencePenalizer.__init__(orchestrator)

# python/sglang/srt/sampling/sampling_batch_info.py
  SamplingBatchInfo.from_schedule_batch(cls, batch, vocab_size)
  SamplingBatchInfo.__len__()
  SamplingBatchInfo.update_regex_vocab_mask()
  SamplingBatchInfo.update_penalties()
  SamplingBatchInfo.apply_logits_bias(logits)
  SamplingBatchInfo.filter_batch(keep_indices, keep_indices_device)
  SamplingBatchInfo.merge_custom_logit_processor(lhs, Tuple[CustomLogitProcessor, torch.Tensor]]], rhs, Tuple[CustomLogitProcessor, torch.Tensor]]], bs1, bs2, device)
  SamplingBatchInfo.merge_batch(other)
merge_bias_tensor(lhs, rhs, bs1, bs2, device, default)

# python/sglang/srt/sampling/sampling_params.py
  SamplingParams.__init__(max_new_tokens, stop, List[str]]], stop_token_ids, temperature, top_p, top_k, min_p, frequency_penalty, presence_penalty, repetition_penalty, min_new_tokens, n, json_schema, regex, ebnf, structural_tag, ignore_eos, skip_special_tokens, spaces_between_special_tokens, no_stop_trim, custom_params, Any]], stream_interval, logit_bias, float]])
  SamplingParams.verify(vocab_size)
  SamplingParams.normalize(tokenizer)

# python/sglang/srt/server_args.py
  ServerArgs.__post_init__()
  ServerArgs.add_cli_args(parser)
  ServerArgs.from_cli_args(cls, args)
  ServerArgs.url()
  ServerArgs.get_hf_config()
  ServerArgs.check_server_args()
  ServerArgs.check_lora_server_args()
  ServerArgs.validate_disagg_tp_size(prefill_tp, decode_tp)
  ServerArgs.model_specific_adjustments()
  ServerArgs.adjust_mem_fraction_for_vlm(model_config)
prepare_server_args(argv)
  PortArgs.init_new(server_args, dp_rank)
  LoRAPathAction.__call__(parser, namespace, values, option_string)
  DeprecatedAction.__init__(option_strings, dest, nargs)
  DeprecatedAction.__call__(parser, namespace, values, option_string)
print_deprecated_warning(message)
auto_choose_speculative_params()

# python/sglang/srt/speculative/build_eagle_tree.py
build_tree_kernel_efficient_preprocess(verified_id, score_list, token_list, parents_list, num_verify_tokens)
build_tree_kernel_efficient(verified_id, score_list, token_list, parents_list, seq_lens, seq_lens_sum, topk, spec_steps, num_verify_tokens, tree_mask_mode, tree_mask_buf, position_buf)
test_build_tree_kernel_efficient()

# python/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py
  EAGLEDraftCudaGraphRunner.__init__(eagle_worker)
  EAGLEDraftCudaGraphRunner.can_run(forward_batch)
  EAGLEDraftCudaGraphRunner.capture()
  EAGLEDraftCudaGraphRunner.capture_one_batch_size(num_seqs, forward)
  EAGLEDraftCudaGraphRunner.replay(forward_batch)

# python/sglang/srt/speculative/eagle_draft_extend_cuda_graph_runner.py
  EAGLEDraftExtendCudaGraphRunner.__init__(eagle_worker)
  EAGLEDraftExtendCudaGraphRunner.can_run(forward_batch)
  EAGLEDraftExtendCudaGraphRunner.capture()
  EAGLEDraftExtendCudaGraphRunner.capture_one_batch_size(bs, forward)
  EAGLEDraftExtendCudaGraphRunner.replay(forward_batch)

# python/sglang/srt/speculative/eagle_utils.py
  EagleDraftInput.prepare_for_extend(batch)
  EagleDraftInput.create_idle_input(cls, device, hidden_size, dtype, topk, capture_hidden_mode)
  EagleDraftInput.prepare_extend_after_decode(batch, speculative_num_steps)
  EagleDraftInput.generate_attn_arg_prefill(req_pool_indices, paged_kernel_lens, paged_kernel_lens_sum, req_to_token)
  EagleDraftInput.filter_batch(new_indices, has_been_filtered)
  EagleDraftInput.merge_batch(spec_info)
  EagleVerifyInput.create_idle_input(cls, topk, spec_steps, num_verify_tokens)
  EagleVerifyInput.prepare_for_verify(batch, page_size)
  EagleVerifyInput.generate_attn_arg_prefill(req_pool_indices, paged_kernel_lens, paged_kernel_lens_sum, req_to_token)
  EagleVerifyInput.verify(batch, logits_output, token_to_kv_pool_allocator, page_size, vocab_mask)
create_extend_after_decode_spec_info(verified_id, seq_lens, accept_lens, positions, new_verified_id, bs_upper)
assign_req_to_token_pool(req_pool_indices, req_to_token, start_offset, end_offset, out_cache_loc, pool_len, bs_upper)
assign_draft_cache_locs(req_pool_indices, req_to_token, seq_lens, extend_lens, num_new_pages_per_topk, out_cache_loc, pool_len, topk, speculative_num_steps, page_size, bs_upper, iter_upper)
generate_draft_decode_kv_indices(req_pool_indices, req_to_token, paged_kernel_lens, kv_indices, kv_indptr, positions, pool_len, kv_indices_stride, kv_indptr_stride, bs_upper, iter_upper, num_tokens_upper, page_size)
align_evict_mask_to_page_size(seq_lens, evict_mask, page_size, num_draft_tokens, BLOCK_SIZE)
get_target_cache_loc(tgt_cache_loc, to_free_slots, accept_length, to_free_num_slots, out_cache_loc, num_verify_tokens, num_verify_tokens_upper, bs_upper)
get_src_tgt_cache_loc(seq_lens, out_cache_loc, accept_index, accept_length, draft_token_num, page_size)
filter_finished_cache_loc_kernel(out_cache_loc, tgt_cache_loc, accept_length, accept_length_filter, bs_upper, num_verify_tokens_upper)
create_accept_length_filter(accept_length, unfinished_index_device, seq_lens)
select_top_k_tokens(i, topk_p, topk_index, hidden_states, scores, topk)
traverse_tree(retrieve_next_token, retrieve_next_sibling, draft_tokens, grammar, allocate_token_bitmask)
generate_token_bitmask(reqs, verify_input, retrieve_next_token_cpu, retrieve_next_sibling_cpu, draft_tokens_cpu, vocab_size)

# python/sglang/srt/speculative/eagle_worker.py
draft_tp_context(tp_group)
  EAGLEWorker.__init__(server_args, gpu_id, tp_rank, dp_rank, moe_ep_rank, nccl_port, target_worker)
  EAGLEWorker.init_attention_backend()
  EAGLEWorker.init_cuda_graphs()
  EAGLEWorker.draft_model_runner()
  EAGLEWorker.forward_batch_speculative_generation(batch)
  EAGLEWorker.check_forward_draft_extend_after_decode(batch)
  EAGLEWorker.forward_target_extend(batch)
  EAGLEWorker.draft(batch)
  EAGLEWorker.draft_forward(forward_batch)
  EAGLEWorker.verify(batch, spec_info)
  EAGLEWorker.add_logprob_values(batch, res, logits_output)
  EAGLEWorker.forward_draft_extend(batch, hidden_states, next_token_ids, seq_lens_cpu)
  EAGLEWorker.forward_draft_extend_after_decode(batch)
  EAGLEWorker.capture_for_decode(logits_output, draft_input)
load_token_map(token_map_path)
get_last_loc_large_page_size_top_k_1(req_to_token, req_pool_indices, seq_lens, speculative_num_steps)
get_last_loc_large_page_size_large_top_k(req_to_token, req_pool_indices, seq_lens, speculative_num_steps, topk, page_size)

# python/sglang/srt/speculative/spec_info.py
  SpeculativeAlgorithm.is_none()
  SpeculativeAlgorithm.is_eagle()
  SpeculativeAlgorithm.is_eagle3()
  SpeculativeAlgorithm.from_string(name)

# python/sglang/srt/tokenizer/tiktoken_tokenizer.py
  TiktokenProcessor.__init__(name)
  TiktokenProcessor.image_processor(image)
  TiktokenTokenizer.__init__(tokenizer_path)
  TiktokenTokenizer.encode(x, add_special_tokens)
  TiktokenTokenizer.decode(x)
  TiktokenTokenizer.batch_decode(batch, skip_special_tokens, spaces_between_special_tokens)
  TiktokenTokenizer.apply_chat_template(messages, tokenize, add_generation_prompt, tools, reasoning_effort)
  TiktokenTokenizer.__call__(text)
  TiktokenTokenizer.init_xgrammar()

# python/sglang/srt/torch_memory_saver_adapter.py
  TorchMemorySaverAdapter.create(enable)
  TorchMemorySaverAdapter.check_validity(caller_name)
  TorchMemorySaverAdapter.configure_subprocess()
  TorchMemorySaverAdapter.region(tag)
  TorchMemorySaverAdapter.pause(tag)
  TorchMemorySaverAdapter.resume(tag)
  TorchMemorySaverAdapter.enabled()
  _TorchMemorySaverAdapterReal.configure_subprocess()
  _TorchMemorySaverAdapterReal.region(tag)
  _TorchMemorySaverAdapterReal.pause(tag)
  _TorchMemorySaverAdapterReal.resume(tag)
  _TorchMemorySaverAdapterReal.enabled()
  _TorchMemorySaverAdapterNoop.configure_subprocess()
  _TorchMemorySaverAdapterNoop.region(tag)
  _TorchMemorySaverAdapterNoop.pause(tag)
  _TorchMemorySaverAdapterNoop.resume(tag)
  _TorchMemorySaverAdapterNoop.enabled()

# python/sglang/srt/two_batch_overlap.py
get_token_num_per_seq(forward_mode, spec_info, EagleVerifyInput]])
compute_split_seq_index(forward_mode, num_tokens, extend_lens, token_num_per_seq)
split_spec_info(spec_info, start_seq_index, end_seq_index, start_token_index, end_token_index)
compute_split_token_index(split_seq_index, forward_mode, extend_seq_lens, token_num_per_seq)
compute_split_indices_for_cuda_graph_replay(forward_mode, cuda_graph_num_tokens, spec_info, EagleVerifyInput]])
  TboCudaGraphRunnerPlugin.__init__()
  TboCudaGraphRunnerPlugin.capture_one_batch_size(batch, num_tokens)
  TboCudaGraphRunnerPlugin.replay_prepare(forward_mode, bs, num_token_non_padded, spec_info, EagleVerifyInput]])
  TboDPAttentionPreparer.prepare_all_gather(local_batch)
  TboDPAttentionPreparer.compute_output(partial_global_info)
  TboForwardBatchPreparer.prepare(cls, batch, is_draft_worker)
  TboForwardBatchPreparer.prepare_raw(cls, batch, tbo_children_num_token_non_padded)
  TboForwardBatchPreparer.derive_fields_related_to_seq_len_for_two_chunk(cls, batch)
  TboForwardBatchPreparer.filter_batch(cls, batch)
  TboForwardBatchPreparer.compute_tbo_children_num_token_non_padded(cls, batch)
  TboForwardBatchPreparer.compute_tbo_children_num_token_non_padded_raw(cls, tbo_split_token_index, num_token_non_padded)
model_forward_maybe_tbo(layers, enable_tbo, positions, forward_batch, hidden_states, input_data_scatter_mode, residual, zero_allocator)
  MaybeTboDeepEPDispatcher.__init__()
  MaybeTboDeepEPDispatcher.dispatch()
  MaybeTboDeepEPDispatcher.dispatch_a()
  MaybeTboDeepEPDispatcher.dispatch_b()
  MaybeTboDeepEPDispatcher.combine()
  MaybeTboDeepEPDispatcher.combine_a()
  MaybeTboDeepEPDispatcher.combine_b()

# python/sglang/srt/utils.py
is_hip()
is_cuda()
is_cuda_alike()
is_hpu()
is_xpu()
is_npu()
is_host_cpu_x86()
is_cpu()
get_cuda_version()
is_blackwell()
is_sm100_supported(device)
is_sm90_supported(device)
get_bool_env_var(name, default)
get_int_env_var(name, default)
support_triton(backend)
cpu_has_amx_support()
use_intel_amx_backend(layer)
is_flashinfer_available()
random_uuid()
  DynamicGradMode.set_inference_mode(mode)
  DynamicGradMode.__init__(mode)
  DynamicGradMode.__new__(cls, mode_or_orig_func)
  DynamicGradMode.__enter__()
  DynamicGradMode.__exit__(exc_type, exc_value, traceback)
  DynamicGradMode.clone()
enable_show_time_cost()
  TimeInfo.__init__(name, interval, color, indent)
  TimeInfo.check()
  TimeInfo.pretty_print()
mark_start(name, interval, color, indent)
mark_end(name)
calculate_time(show, min_cost_ms)
get_available_gpu_memory(device, gpu_id, distributed, empty_cache, cpu_group)
is_pin_memory_available()
  LayerFn.__call__(layer_id, prefix)
make_layers(num_hidden_layers, layer_fn, pp_rank, pp_size, prefix, return_tuple, offloader_kwargs, Any])
set_random_seed(seed)
find_process_using_port(port)
wait_port_available(port, port_name, timeout_s, raise_exception)
is_port_available(port)
get_free_port()
decode_video_base64(video_base64)
load_audio(audio_file, sr, mono)
load_image(image_file, str, ImageData, bytes])
load_video(video_file, bytes], use_gpu)
suppress_other_loggers()
assert_pkg_version(pkg, min_version, message)
kill_process_tree(parent_pid, include_parent, skip_pid)
monkey_patch_p2p_access_check()
monkey_patch_vllm_gguf_config()
set_ulimit(target_soft_limit)
add_api_key_middleware(app, api_key)
prepare_model_and_tokenizer(model_path, tokenizer_path)
configure_logger(server_args, prefix)
replace_submodule(model, module_name, new_module)
set_weight_attrs(weight, weight_attrs, Any]])
broadcast_pyobj(data, rank, dist_group, src, force_cpu_device)
point_to_point_pyobj(data, rank, group, src, dst)
pytorch_profile(name, func)
get_zmq_socket(context, socket_type, endpoint, bind)
dump_to_file(dirpath, name, value)
is_triton_3()
maybe_torch_compile()
delete_directory(dirpath)
set_prometheus_multiproc_dir()
add_prometheus_middleware(app)
bind_port(port)
get_amdgpu_memory_capacity()
get_device_sm()
get_nvgpu_memory_capacity()
get_hpu_memory_capacity()
get_npu_memory_capacity()
get_device_memory_capacity(device)
init_custom_process_group(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)
crash_on_warnings()
print_warning_once(msg)
print_info_once(msg)
get_device_name(device_id)
is_habana_available()
get_device(device_id)
get_device_count()
get_device_core_count(device_id)
get_device_capability(device_id)
get_npu_compiler_config()
get_compiler_backend()
supports_custom_op()
direct_register_custom_op(op_name, op_func, mutates_args, fake_impl, target_lib)
set_gpu_proc_affinity(tp_size, nnodes, gpu_id)
disable_request_logging()
dataclass_to_string_truncated(data, max_length, skip_names)
permute_weight(x)
  MultiprocessingSerializer.serialize(obj, output_str)
  MultiprocessingSerializer.deserialize(data)
debug_timing(func)
nullable_str(val)
pyspy_dump_schedulers()
kill_itself_when_parent_died()
set_uvicorn_logging_configs()
get_ip()
get_open_port()
is_valid_ipv6_address(address)
maybe_wrap_ipv6_address(address)
format_tcp_address(ip, port)
configure_ipv6(dist_init_addr)
launch_dummy_health_check_server(host, port, enable_metrics)
create_checksum(directory)
set_cuda_arch()
next_power_of_2(n)
round_up(x, y)
  EmptyContextManager.__enter__()
  EmptyContextManager.__exit__(exc_type, exc_value, traceback)
empty_context()
add_prefix(name, prefix)
is_remote_url(url, Path])
parse_connector_type(url)
retry(fn, max_retry, initial_delay, max_delay, should_retry, bool])
flatten_nested_list(nested_list)
is_non_idle_and_non_empty(forward_mode, hidden_states)
fast_topk(values, topk, dim)
bind_or_assign(target, source)
get_local_ip_auto()
get_local_ip_by_nic(interface)
get_local_ip_by_remote()
is_page_size_one(server_args)
is_no_spec_infer_or_topk_one(server_args)
is_fa3_default_architecture(hf_config)
  BumpAllocator.__init__(buffer_size, dtype, device)
  BumpAllocator.allocate(size)
log_info_on_rank0(logger, msg)
load_json_config(data)
dispose_tensor(x)
  Withable.__init__()
  Withable.value()
  Withable.with_value(new_value)
require_mlp_tp_gather(server_args)
require_attn_tp_gather(server_args)
require_gathered_buffer(server_args)
require_mlp_sync(server_args)
find_local_repo_dir(repo_id, revision)
read_system_prompt_from_file(model_name)
bind_or_assign(target, source)
prepack_weight_if_needed(weight)
dim_is_supported(weight)
  PackWeightMethod.__init__(weight_names, transpose_dims)
  PackWeightMethod.process_weights_after_loading(module)
  LazyValue.__init__(creator)
  LazyValue.value()
dynamic_import(func_path)
gc_object_counts()
configure_gc_warning(warn_threshold_secs)
freeze_gc(context)
configure_gc_logger()
align(x, y)
ceil_div(x, y)
parse_lscpu_topology()
get_physical_cpus_by_numa()
get_cpu_ids_by_node()
is_shm_available(dtype, world_size, local_size)
lru_cache_frozenset(maxsize)
apply_module_patch(target_module, target_function, wrappers)
parse_module_path(module_path, function_name, create_dummy)
mxfp_supported()
  ConcurrentCounter.__init__(initial)
  ConcurrentCounter.value()
  ConcurrentCounter.__repr__()
  ConcurrentCounter.increment(n, notify_all)
  ConcurrentCounter.decrement(n, notify_all)
  ConcurrentCounter.wait_for(condition, bool])
  ConcurrentCounter.wait_for_zero()
is_triton_kernels_available()
check_cuda_result(raw_output)

# python/sglang/srt/warmup.py
warmup(name)
execute_warmups(disaggregation_mode, warmup_names, tokenizer_manager)
voice_chat(disaggregation_mode, tokenizer_manager)

# python/sglang/srt/weight_sync/tensor_bucket.py
  FlattenedTensorBucket.__init__(named_tensors, torch.Tensor]], flattened_tensor, metadata)
  FlattenedTensorBucket.get_flattened_tensor()
  FlattenedTensorBucket.get_metadata()
  FlattenedTensorBucket.reconstruct_tensors()

# python/sglang/srt/weight_sync/utils.py
update_weights(engine, params_batch, torch.Tensor]], device_mesh_key, device_mesh, load_format)