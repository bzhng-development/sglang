================================================================================
FUNCTION INDEX: SGLang SRT Complete
================================================================================
Total Functions: 5594
Documented: 914


============================================================
FILE: python/sglang/srt/_custom_ops.py
Functions: 26
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  35: def init_custom_ar(ipc_tensors: List[torch.Tensor],
        rank_data: torch.Tensor,
        rank: int,
        full_nvlink: bool)
         ‚Üí int

  L  43: def all_reduce(fa: int,
        inp: torch.Tensor,
        out: torch.Tensor,
        reg_buffer: int,
        reg_buffer_sz_bytes: int)
         ‚Üí None

  L  52: def dispose(fa: int)
         ‚Üí None

  L  55: def meta_size()
         ‚Üí int

  L  58: def register_buffer(fa: int, ipc_tensors: List[int])
         ‚Üí None

  L  61: def get_graph_buffer_ipc_meta(fa: int)
         ‚Üí Tuple[List[int], List[int]]

  L  64: def register_graph_buffers(fa: int,
        handles: List[List[int]],
        offsets: List[List[int]])
         ‚Üí None

  L  72: def init_custom_ar(meta: torch.Tensor,
        rank_data: torch.Tensor,
        handles: List[str],
        offsets: List[int],
        rank: int,
        full_nvlink: bool)
         ‚Üí int

  L  84: def all_reduce_reg(fa: int, inp: torch.Tensor, out: torch.Tensor)
         ‚Üí None

  L  87: def all_reduce_unreg(fa: int,
        inp: torch.Tensor,
        reg_buffer: torch.Tensor,
        out: torch.Tensor)
         ‚Üí None

  L  92: def dispose(fa: int)
         ‚Üí None

  L  95: def meta_size()
         ‚Üí int

  L  98: def register_buffer(fa: int,
        t: torch.Tensor,
        handles: List[str],
        offsets: List[int])
         ‚Üí None

  L 103: def get_graph_buffer_ipc_meta(fa: int)
         ‚Üí Tuple[torch.Tensor, List[int]]

  L 106: def register_graph_buffers(fa: int,
        handles: List[str],
        offsets: List[List[int]])
         ‚Üí None

  L 111: def allocate_meta_buffer(size: int)
         ‚Üí torch.Tensor

  L 114: def get_meta_buffer_ipc_handle(inp: torch.Tensor)
         ‚Üí torch.Tensor

  L 119: def init_custom_qr(rank: int, world_size: int, qr_max_size: Optional[int])
         ‚Üí int

  L 124: def qr_get_handle(fa: int)
         ‚Üí torch.Tensor

  L 127: def qr_open_handles(fa: int, handles: list[torch.Tensor])
         ‚Üí None

  L 130: def qr_all_reduce(fa: int,
        inp: torch.Tensor,
        out: torch.Tensor,
        quant_level: int,
        cast_bf2half: bool)
         ‚Üí None

  L 139: def qr_destroy(fa: int)
         ‚Üí None

  L 142: def qr_max_size()
         ‚Üí int

  L 146: def mscclpp_generate_unique_id()
         ‚Üí bytes

  L 150: def mscclpp_init_context(unique_id: bytes,
        rank: int,
        world_size: int,
        scratch: torch.Tensor,
        put_buffer: torch.Tensor,
        nranks_per_node: int,
        rank_to_node: List[int],
        rank_to_ib: List[int],
        context_selection: int)
         ‚Üí int

  L 174: def mscclpp_allreduce(context: int,
        inp: torch.Tensor,
        out: torch.Tensor,
        nthreads: int,
        nblocks: int)
         ‚Üí None


============================================================
FILE: python/sglang/srt/aio_rwlock.py
Functions: 13
============================================================


CLASS: RWLock
----------------------------------------
  L   5: __init__(self)

  L  22: reader_lock(self)
         üìù A context manager for acquiring a shared (reader) lock.

  L  33: writer_lock(self)
         üìù A context manager for acquiring an exclusive (writer) lock.

  L  43: acquire_reader(self)

  L  51: release_reader(self)

  L  59: acquire_writer(self)

  L  72: release_writer(self)


CLASS: _ReaderLock
----------------------------------------
  L  80: __init__(self, rwlock: RWLock)

  L  83: __aenter__(self)

  L  87: __aexit__(self, exc_type, exc_val, exc_tb)


CLASS: _WriterLock
----------------------------------------
  L  92: __init__(self, rwlock: RWLock)

  L  95: __aenter__(self)

  L  99: __aexit__(self, exc_type, exc_val, exc_tb)


============================================================
FILE: python/sglang/srt/bench_utils.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  45: def bench_kineto(fn,
        kernel_names,
        num_tests: int,
        suppress_kineto_output: bool,
        trace_path: str,
        flush_l2: bool,
        with_multiple_kernels: bool)


CLASS: suppress_stdout_stderr
----------------------------------------
  L  10: __enter__(self)

  L  30: __exit__(self)


============================================================
FILE: python/sglang/srt/code_completion_parser.py
Functions: 5
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  58: def register_completion_template(template: CompletionTemplate, override: bool)
         üìù Register a new completion template.

  L  68: def completion_template_exists(template_name: str)
         ‚Üí bool

  L  72: def is_completion_template_defined()
         ‚Üí bool

  L  77: def generate_completion_prompt_from_request(request: CompletionRequest)
         ‚Üí str

  L  87: def generate_completion_prompt(prompt: str, suffix: str, template_name: str)
         ‚Üí str


============================================================
FILE: python/sglang/srt/configs/chatglm.py
Functions: 1
============================================================


CLASS: ChatGLMConfig
----------------------------------------
  L  19: __init__(self, num_layers, padded_vocab_size, hidden_size, ffn_hidden_size, kv_channels, num_attention_heads, seq_length, hidden_dropout, attention_dropout, layernorm_epsilon, rmsnorm, apply_residual_connection_post_layernorm, post_layer_norm, add_bias_linear, add_qkv_bias, interleaved_qkv, bias_dropout_fusion, multi_query_attention, multi_query_group_num, apply_query_key_layer_scaling, attention_softmax_in_fp32, fp32_residual_connection, quantization_bit, pre_seq_len, prefix_projection)


============================================================
FILE: python/sglang/srt/configs/dbrx.py
Functions: 5
============================================================


CLASS: DbrxAttentionConfig
----------------------------------------
  L  34: __init__(self, attn_pdrop: float, clip_qkv: Optional[float], kv_n_heads: int, rope_theta: float)

  L  55: from_pretrained(cls, pretrained_model_name_or_path: str)
         ‚Üí 'PretrainedConfig'


CLASS: DbrxConfig
----------------------------------------
  L 229: __init__(self, d_model: int, n_heads: int, n_layers: int, max_seq_len: int, vocab_size: int, resid_pdrop: float, emb_pdrop: float, attn_config: Optional[DbrxAttentionConfig], ffn_config: Optional[DbrxFFNConfig], use_cache: bool, initializer_range: float, output_router_logits: bool, router_aux_loss_coef: float)


CLASS: DbrxFFNConfig
----------------------------------------
  L 106: __init__(self, ffn_act_fn: Optional[dict], ffn_hidden_size: int, moe_num_experts: int, moe_top_k: int, moe_jitter_eps: Optional[float], moe_loss_weight: float, moe_normalize_expert_weights: Optional[float], uniform_expert_assignment: bool)

  L 137: from_pretrained(cls, pretrained_model_name_or_path: str)
         ‚Üí 'PretrainedConfig'


============================================================
FILE: python/sglang/srt/configs/deepseekvl2.py
Functions: 24
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  16: def select_best_resolution(image_size, candidate_resolutions)


CLASS: DeepseekV2Config
----------------------------------------
  L 555: __init__(self, vocab_size, hidden_size, intermediate_size, moe_intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, n_shared_experts, n_routed_experts, ep_size, routed_scaling_factor, kv_lora_rank, q_lora_rank, qk_rope_head_dim, v_head_dim, qk_nope_head_dim, topk_method, n_group, topk_group, num_experts_per_tok, moe_layer_freq, first_k_dense_replace, norm_topk_prob, scoring_func, aux_loss_alpha, seq_aux, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, pretraining_tp, tie_word_embeddings, rope_theta, rope_scaling, attention_bias, attention_dropout, use_mla)


CLASS: DeepseekVL2Config
----------------------------------------
  L 661: __init__(self, tile_tag: str, global_view_pos: str, candidate_resolutions: Tuple[Tuple[int, int]])


CLASS: DeepseekVL2MlpProjectorConfig
----------------------------------------
  L 530: __init__(self, projector_type: str, input_dim: int, n_embed: int, depth: int, mlp_ratio: int, downsample_ratio: int)


CLASS: DeepseekVL2VisionEncoderConfig
----------------------------------------
  L 488: __init__(self, model_name: str, image_size: int, patch_size: int, width: int, layers: int, heads: int, mlp_ratio: int, global_pool: str, ignore_head: bool, class_token: bool, num_classes: int, use_checkpoint: bool)


CLASS: DeepseekVLV2Processor
----------------------------------------
  L 112: __init__(self, tokenizer: LlamaTokenizerFast, candidate_resolutions: Tuple[Tuple[int, int]], patch_size: int, downsample_ratio: int, image_mean: Tuple[float, float, float], image_std: Tuple[float, float, float], normalize: bool, image_token: str, pad_token: str, add_special_token: bool, sft_format: str, mask_prompt: bool, ignore_id: int)

  L 180: format_messages_v2(self, messages, pil_images, max_req_input_len)
         üìù play the role of format_messages_v2 and get_images_info in the last ve

  L 222: bos_id(self)

  L 226: eos_id(self)

  L 230: pad_id(self)

  L 233: encode(self, text: str, bos: bool, eos: bool)

  L 243: decode(self, t: List[int])
         ‚Üí str

  L 246: process_one(self, prompt: str, conversations: List[Dict[str, str]], images: List[Image.Image], apply_sft_format: bool, inference_mode: bool, system_prompt: str, max_req_input_len: int)
         üìù Args:

  L 334: __call__(self)

  L 358: find_all_indices(self, messages, target_value)

  L 365: tokenize_with_images(self, conversation: str, images: List[Image.Image], bos: bool, eos: bool, cropping: bool, max_req_input_len: int)
         üìù Tokenize text with <image> tags.


CLASS: DictOutput
----------------------------------------
  L  45: items(self)

  L  48: keys(self)

  L  51: __getitem__(self, item)

  L  54: __contains__(self, key)

  L  57: __setitem__(self, key, value)


CLASS: ImageTransform
----------------------------------------
  L  76: __init__(self, mean: Optional[Tuple[float, float, float]], std: Optional[Tuple[float, float, float]], normalize: bool)

  L 103: __call__(self, pil_img: Image.Image)


CLASS: VLChatProcessorOutput
----------------------------------------
  L  71: __len__(self)


============================================================
FILE: python/sglang/srt/configs/device_config.py
Functions: 1
============================================================


CLASS: DeviceConfig
----------------------------------------
  L  12: __init__(self, device: str)
         ‚Üí None


============================================================
FILE: python/sglang/srt/configs/exaone.py
Functions: 1
============================================================


CLASS: ExaoneConfig
----------------------------------------
  L 143: __init__(self, vocab_size, max_position_embeddings, hidden_size, num_layers, num_attention_heads, num_key_value_heads, intermediate_size, activation_function, rope_theta, rope_scaling, embed_dropout, attention_dropout, layer_norm_epsilon, initializer_range, use_cache, bos_token_id, eos_token_id, tie_word_embeddings)


============================================================
FILE: python/sglang/srt/configs/internvl.py
Functions: 16
============================================================


CLASS: InternLM2Config
----------------------------------------
  L  78: __init__(self, vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, tie_word_embeddings, bias, rope_theta, rope_scaling, attn_implementation)


CLASS: InternLM2Tokenizer
----------------------------------------
  L 494: __init__(self, vocab_file, unk_token, bos_token, eos_token, pad_token, sp_model_kwargs: Optional[Dict[str, Any]], add_bos_token, add_eos_token, decode_with_prefix_space, clean_up_tokenization_spaces)

  L 527: no_prefix_space_tokens(self)

  L 536: vocab_size(self)
         üìù Returns vocab size

  L 541: bos_token_id(self)
         ‚Üí Optional[int]

  L 545: eos_token_id(self)
         ‚Üí Optional[int]

  L 548: get_vocab(self)
         üìù Returns vocab as a dict

  L 573: convert_tokens_to_string(self, tokens)
         üìù Converts a sequence of tokens (string) in a single string.

  L 594: save_vocabulary(self, save_directory, filename_prefix: Optional[str])
         ‚Üí Tuple[str]
         üìù Save the vocabulary and special tokens file to a directory.

  L 627: build_inputs_with_special_tokens(self, token_ids_0, token_ids_1)

  L 643: get_special_tokens_mask(self, token_ids_0: List[int], token_ids_1: Optional[List[int]], already_has_special_tokens: bool)
         ‚Üí List[int]
         üìù Retrieve sequence ids from a token list that has no special tokens add

  L 675: create_token_type_ids_from_sequences(self, token_ids_0: List[int], token_ids_1: Optional[List[int]])
         ‚Üí List[int]
         üìù Create a mask from the two sequences passed to be used in a sequence-p


CLASS: InternVLChatConfig
----------------------------------------
  L 279: __init__(self, vision_config, llm_config, use_backbone_lora, use_llm_lora, pad2square, select_layer, force_image_size, downsample_ratio, template, dynamic_image_size, use_thumbnail, ps_version, min_dynamic_patch, max_dynamic_patch)

  L 345: to_dict(self)
         üìù Serializes this instance to a Python dictionary. Override the default 


CLASS: InternVisionConfig
----------------------------------------
  L 210: __init__(self, num_channels, patch_size, image_size, qkv_bias, hidden_size, num_attention_heads, intermediate_size, qk_normalization, num_hidden_layers, use_flash_attn, hidden_act, layer_norm_eps, dropout, drop_path_rate, attention_dropout, initializer_range, initializer_factor)

  L 252: from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike])
         ‚Üí 'PretrainedConfig'


============================================================
FILE: python/sglang/srt/configs/janus_pro.py
Functions: 30
============================================================


CLASS: AlignerConfig
----------------------------------------
  L  85: __init__(self)


CLASS: DictOutput
----------------------------------------
  L 287: items(self)

  L 290: keys(self)

  L 293: __getitem__(self, item)

  L 296: __contains__(self, key)

  L 299: __setitem__(self, key, value)


CLASS: DictToObject
----------------------------------------
  L  26: __init__(self, dictionary)


CLASS: GenAlignerConfig
----------------------------------------
  L  55: __init__(self)


CLASS: GenHeadConfig
----------------------------------------
  L  70: __init__(self)


CLASS: GenVisionConfig
----------------------------------------
  L 100: __init__(self)


CLASS: MultiModalityConfig
----------------------------------------
  L 135: __init__(self)


CLASS: VLChatProcessor
----------------------------------------
  L 332: __init__(self, image_processor: VLMImageProcessor, tokenizer: LlamaTokenizerFast, image_tag: str, image_start_tag: str, image_end_tag: str, pad_tag: str, num_image_tokens: int, add_special_token: bool, sft_format: str, mask_prompt: bool, ignore_id: int)

  L 374: image_token(self)

  L 378: image_id(self)
         ‚Üí int

  L 383: image_start_id(self)

  L 388: image_end_id(self)

  L 393: image_start_token(self)

  L 397: image_end_token(self)

  L 401: pad_id(self)

  L 405: add_image_token(self, image_indices: List[int], input_ids: torch.LongTensor)
         üìù Args:

  L 450: process_one(self, prompt: str, images: List[Image])
         üìù Args:

  L 497: __call__(self)
         üìù Args:

  L 532: batchify(self, prepare_list: List[VLChatProcessorOutput])
         ‚Üí BatchedVLChatProcessorOutput
         üìù Preprocesses the inputs for multimodal inference.


CLASS: VLChatProcessorOutput
----------------------------------------
  L 310: __len__(self)


CLASS: VLMImageProcessor
----------------------------------------
  L 162: __init__(self, image_size: int, min_size: int, image_mean: Union[Tuple[float, float, float], List[float]], image_std: Union[Tuple[float, float, float], List[float]], rescale_factor: float, do_normalize: bool)

  L 194: resize(self, pil_img: Image)
         ‚Üí np.ndarray
         üìù Args:

  L 249: preprocess(self, images, return_tensors: str)
         ‚Üí BatchFeature

  L 282: default_shape(self)


CLASS: VLMImageProcessorConfig
----------------------------------------
  L 605: __init__(self, image_size: int, min_size: int, image_mean: Union[Tuple[float, float, float], List[float]], image_std: Union[Tuple[float, float, float], List[float]], rescale_factor: float, do_normalize: bool)


CLASS: VisionConfig
----------------------------------------
  L  40: __init__(self)


============================================================
FILE: python/sglang/srt/configs/kimi_vl.py
Functions: 1
============================================================


CLASS: KimiVLConfig
----------------------------------------
  L  14: __init__(self, vision_config: Optional[Union[dict, MoonViTConfig]], text_config: Optional[Union[dict, DeepseekV2Config]], ignore_index: int, media_placeholder_token_id: int, pad_token_id: int)


============================================================
FILE: python/sglang/srt/configs/kimi_vl_moonvit.py
Functions: 1
============================================================


CLASS: MoonViTConfig
----------------------------------------
  L   9: __init__(self, patch_size: int, init_pos_emb_height: int, init_pos_emb_width: int, num_attention_heads: int, num_hidden_layers: int, hidden_size: int, intermediate_size: int, merge_kernel_size: tuple[int, int])


============================================================
FILE: python/sglang/srt/configs/load_config.py
Functions: 1
============================================================


CLASS: LoadConfig
----------------------------------------
  L  57: __post_init__(self)


============================================================
FILE: python/sglang/srt/configs/model_config.py
Functions: 18
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 647: def is_generation_model(model_architectures: List[str], is_embedding: bool)

  L 705: def is_multimodal_model(model_architectures: List[str])

  L 715: def is_multimodal_gen_model(model_architectures: List[str])

  L 719: def is_image_gen_model(model_architectures: List[str])

  L 723: def is_audio_model(model_architectures: List[str])

  L 727: def is_encoder_decoder_model(model_architectures: List[str])

  L 731: def is_multimodal_chunked_prefill_supported(model_architectures: List[str])
         üìù Check if chunked prefill is supported for a MultiModal model.

  L 746: def yarn_get_mscale(scale: float, mscale: float)
         ‚Üí float

  L 752: def is_hybrid_model(model_architectures: List[str],
        hybrid_kvcache_ratio: Optional[float],
        context_length: Optional[int],
        attention_chunk_size: Optional[int])

  L 770: def get_hybrid_layer_ids(model_architectures: List[str], num_hidden_layers: int)


CLASS: ModelConfig
----------------------------------------
  L  52: __init__(self, model_path: str, trust_remote_code: bool, revision: Optional[str], context_length: Optional[int], model_override_args: str, is_embedding: Optional[bool], enable_multimodal: Optional[bool], dtype: str, quantization: Optional[str], override_config_file: Optional[str], is_draft_model: bool, hybrid_kvcache_ratio: Optional[float], model_impl: Union[str, ModelImpl])
         ‚Üí None

  L 293: from_server_args(server_args: ServerArgs, model_path: str)

  L 309: get_total_num_attention_heads(self)
         ‚Üí int

  L 312: get_num_attention_heads(self, tensor_parallel_size)
         ‚Üí int

  L 317: get_total_num_kv_heads(self)
         ‚Üí int
         üìù Returns the total number of KV heads.

  L 380: get_num_kv_heads(self, tensor_parallel_size)
         ‚Üí int
         üìù Returns the number of KV heads per GPU.

  L 533: get_hf_eos_token_id(self)
         ‚Üí Optional[Set[int]]

  L 553: maybe_pull_model_tokenizer_from_remote(self)
         ‚Üí None
         üìù Pull the model config files to a temporary


============================================================
FILE: python/sglang/srt/configs/step3_vl.py
Functions: 3
============================================================


CLASS: Step3TextConfig
----------------------------------------
  L  40: __init__(self, hidden_size: int, intermediate_size: int, num_attention_heads: int, num_attention_groups: int, num_hidden_layers: int, max_seq_len: int, vocab_size: int, rms_norm_eps: float, moe_intermediate_size: int, moe_num_experts: int, moe_top_k: int, rope_theta: float, rope_scaling: Optional[dict[str, Any]], max_position_embedding: int, share_expert_dim: int, share_q_dim: int, head_dim: int, norm_expert_weight: bool, moe_layers_enum: tuple[int])
         ‚Üí None


CLASS: Step3VLConfig
----------------------------------------
  L 146: __init__(self, vision_config: Optional[Union[dict, Step3VisionEncoderConfig]], text_config: Optional[Union[dict, Step3TextConfig]], understand_projector_stride: int, projector_bias: bool, image_token_id: int)
         ‚Üí None


CLASS: Step3VisionEncoderConfig
----------------------------------------
  L   9: __init__(self, hidden_size, intermediate_size, output_hidden_size, num_hidden_layers, num_attention_heads, num_channels, image_size, patch_size, hidden_act, layer_norm_eps)


============================================================
FILE: python/sglang/srt/configs/update_config.py
Functions: 5
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  13: def may_get_weight_block_size(model_config, load_config)

  L  29: def get_moe_padding_size(weight_block_size)

  L  44: def get_num_heads_padding_size(tp_size, weight_block_size)

  L  51: def update_intermediate_size(model_config, attr_name, intermediate_padding_size)

  L  74: def adjust_config_with_unaligned_cpu_tp(model_config: ModelConfig,
        load_config: LoadConfig,
        tp_size: int)
         ‚Üí ModelConfig


============================================================
FILE: python/sglang/srt/configs/utils.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  12: def register_image_processor(config: Type[PretrainedConfig],
        image_processor: Type[BaseImageProcessor])
         üìù register customized hf image processor while removing hf impl

  L  21: def register_processor(config: Type[PretrainedConfig],
        processor: Type[ProcessorMixin])
         üìù register customized hf processor while removing hf impl


============================================================
FILE: python/sglang/srt/connector/__init__.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  23: def create_remote_connector(url, device)
         ‚Üí BaseConnector

  L  33: def get_connector_type(client: BaseConnector)
         ‚Üí ConnectorType


============================================================
FILE: python/sglang/srt/connector/base_connector.py
Functions: 14
============================================================


CLASS: BaseConnector
----------------------------------------
  L  23: __init__(self, url: str, device: torch.device)

  L  32: get_local_dir(self)

  L  36: weight_iterator(self, rank: int)
         ‚Üí Generator[Tuple[str, torch.Tensor], None, None]

  L  42: pull_files(self, allow_pattern: Optional[List[str]], ignore_pattern: Optional[List[str]])
         ‚Üí None

  L  49: close(self)

  L  57: __enter__(self)

  L  60: __exit__(self, exc_type, exc_value, traceback)

  L  63: __del__(self)


CLASS: BaseFileConnector
----------------------------------------
  L 111: glob(self, allow_pattern: str)
         ‚Üí List[str]


CLASS: BaseKVConnector
----------------------------------------
  L  79: get(self, key: str)
         ‚Üí Optional[torch.Tensor]

  L  83: getstr(self, key: str)
         ‚Üí Optional[str]

  L  87: set(self, key: str, obj: torch.Tensor)
         ‚Üí None

  L  91: setstr(self, key: str, obj: str)
         ‚Üí None

  L  95: list(self, prefix: str)
         ‚Üí List[str]


============================================================
FILE: python/sglang/srt/connector/redis.py
Functions: 9
============================================================


CLASS: RedisConnector
----------------------------------------
  L  18: __init__(self, url: str, device: torch.device)

  L  28: get(self, key: str)
         ‚Üí Optional[torch.Tensor]

  L  37: getstr(self, key: str)
         ‚Üí Optional[str]

  L  45: set(self, key: str, tensor: torch.Tensor)
         ‚Üí None

  L  49: setstr(self, key: str, obj: str)
         ‚Üí None

  L  52: list(self, prefix: str)
         ‚Üí List[str]

  L  67: weight_iterator(self, rank: int)
         ‚Üí Generator[Tuple[str, bytes], None, None]

  L  76: pull_files(self, allow_pattern: Optional[List[str]], ignore_pattern: Optional[List[str]])
         ‚Üí None

  L  83: close(self)


============================================================
FILE: python/sglang/srt/connector/s3.py
Functions: 6
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  29: def list_files(s3,
        path: str,
        allow_pattern: Optional[list[str]],
        ignore_pattern: Optional[list[str]])
         ‚Üí tuple[str, str, list[str]]
         üìù List files from S3 path and filter by pattern.


CLASS: S3Connector
----------------------------------------
  L  71: __init__(self, url: str)
         ‚Üí None

  L  77: glob(self, allow_pattern: Optional[list[str]])
         ‚Üí list[str]

  L  83: pull_files(self, allow_pattern: Optional[list[str]], ignore_pattern: Optional[list[str]])
         ‚Üí None
         üìù Pull files from S3 storage into the temporary directory.

  L 109: weight_iterator(self, rank: int)
         ‚Üí Generator[Tuple[str, torch.Tensor], None, None]

  L 120: close(self)


============================================================
FILE: python/sglang/srt/connector/serde/__init__.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  12: def create_serde(serde_type: str)
         ‚Üí Tuple[Serializer, Deserializer]


============================================================
FILE: python/sglang/srt/connector/serde/safe_serde.py
Functions: 5
============================================================


CLASS: SafeDeserializer
----------------------------------------
  L  22: __init__(self, dtype)

  L  25: from_bytes_normal(self, b: Union[bytearray, bytes])
         ‚Üí torch.Tensor

  L  28: from_bytes(self, b: Union[bytearray, bytes])
         ‚Üí torch.Tensor


CLASS: SafeSerializer
----------------------------------------
  L  13: __init__(self)

  L  16: to_bytes(self, t: torch.Tensor)
         ‚Üí bytes


============================================================
FILE: python/sglang/srt/connector/serde/serde.py
Functions: 3
============================================================


CLASS: Deserializer
----------------------------------------
  L  29: __init__(self, dtype)

  L  33: from_bytes(self, bs: bytes)
         ‚Üí torch.Tensor
         üìù Deserialize a pytorch tensor from bytes.


CLASS: Serializer
----------------------------------------
  L  12: to_bytes(self, t: torch.Tensor)
         ‚Üí bytes
         üìù Serialize a pytorch tensor to bytes. The serialized bytes should conta


============================================================
FILE: python/sglang/srt/connector/utils.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  11: def parse_model_name(url: str)
         ‚Üí str
         üìù Parse the model name from the url.

  L  20: def pull_files_from_db(connector: BaseConnector,
        model_name: str,
        allow_pattern: Optional[list[str]],
        ignore_pattern: Optional[list[str]])
         ‚Üí None


============================================================
FILE: python/sglang/srt/constrained/base_grammar_backend.py
Functions: 24
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 170: def create_grammar_backend(server_args: ServerArgs,
        tokenizer,
        vocab_size: int,
        eos_token_ids: Optional[set])
         ‚Üí Optional[BaseGrammarBackend]


CLASS: BaseGrammarBackend
----------------------------------------
  L 112: __init__(self)

  L 119: dispatch_fallback(self, key_type: str, key_string: str)
         ‚Üí Optional[BaseGrammarObject]
         üìù This function should not be reached in any case.

  L 127: dispatch_json(self, key_string: str)
         ‚Üí Optional[BaseGrammarObject]

  L 130: dispatch_regex(self, key_string: str)
         ‚Üí Optional[BaseGrammarObject]

  L 133: dispatch_ebnf(self, key_string: str)
         ‚Üí Optional[BaseGrammarObject]

  L 136: dispatch_structural_tag(self, key_string: str)
         ‚Üí Optional[BaseGrammarObject]

  L 154: get_cached_or_future_value(self, key: Tuple[str, str])
         ‚Üí Optional[BaseGrammarObject]

  L 163: set_cache(self, key: Tuple[str, str], value: BaseGrammarObject)

  L 166: reset(self)


CLASS: BaseGrammarObject
----------------------------------------
  L  31: __init__(self)

  L  34: accept_token(self, token: int)
         ‚Üí None
         üìù Accept a token in the grammar.

  L  40: rollback(self, k: int)

  L  43: is_terminated(self)

  L  46: allocate_vocab_mask(self, vocab_size: int, batch_size: int, device)
         ‚Üí torch.Tensor

  L  51: fill_vocab_mask(self, vocab_mask: torch.Tensor, idx: int)
         ‚Üí None

  L  55: move_vocab_mask(vocab_mask: torch.Tensor, device)
         ‚Üí torch.Tensor

  L  59: apply_vocab_mask(logits: torch.Tensor, vocab_mask: torch.Tensor)
         ‚Üí None

  L  62: copy(self)
         ‚Üí 'BaseGrammarObject'

  L  66: finished(self)

  L  70: finished(self, finished)

  L  73: try_jump_forward(self, tokenizer)
         ‚Üí Optional[Tuple[List[int], str]]
         üìù Try to jump forward in the grammar.

  L  83: jump_forward_str_state(self, helper: Tuple[List[int], str])
         ‚Üí Tuple[str, int]
         üìù Jump forward for the grammar.

  L  93: jump_and_retokenize(self, old_output_ids: List[int], new_output_ids: List[int], next_state: int)
         ‚Üí None
         üìù Jump forward occurs, and update the grammar state if needed.


============================================================
FILE: python/sglang/srt/constrained/llguidance_backend.py
Functions: 15
============================================================


CLASS: GuidanceBackend
----------------------------------------
  L 111: __init__(self, tokenizer, whitespace_pattern: Optional[str], n_vocab: Optional[int])

  L 133: dispatch_json(self, key_string: str)
         ‚Üí Optional[GuidanceGrammar]

  L 146: dispatch_regex(self, key_string: str)
         ‚Üí Optional[GuidanceGrammar]

  L 150: dispatch_ebnf(self, key_string: str)
         ‚Üí Optional[GuidanceGrammar]

  L 158: dispatch_structural_tag(self, key_string: str)
         ‚Üí Optional[GuidanceGrammar]


CLASS: GuidanceGrammar
----------------------------------------
  L  41: __init__(self, llguidance_tokenizer: LLTokenizer, serialized_grammar: str)

  L  54: accept_token(self, token: int)

  L  59: fill_vocab_mask(self, vocab_mask: torch.Tensor, idx: int)
         ‚Üí None

  L  65: allocate_vocab_mask(self, vocab_size: int, batch_size: int, device)
         ‚Üí torch.Tensor

  L  80: move_vocab_mask(vocab_mask: torch.Tensor, device)
         ‚Üí torch.Tensor

  L  84: apply_vocab_mask(logits: torch.Tensor, vocab_mask: torch.Tensor)
         ‚Üí None

  L  87: copy(self)

  L  93: try_jump_forward(self, tokenizer)
         ‚Üí Optional[Tuple[List[int], str]]

  L 100: jump_forward_str_state(self, helper: Tuple[List[int], str])
         ‚Üí Tuple[str, int]

  L 103: jump_and_retokenize(self, old_output_ids: List[int], new_output_ids: List[int], next_state: int)


============================================================
FILE: python/sglang/srt/constrained/outlines_backend.py
Functions: 16
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 182: def build_regex_from_object(object: Union[str,
        BaseModel,
        Dict],
        whitespace_pattern: Optional[str])


CLASS: OutlinesGrammar
----------------------------------------
  L  43: __init__(self, guide: RegexGuide, jump_forward_map: Union[OutlinesJumpForwardMap, None])
         ‚Üí None

  L  54: accept_token(self, token: int)

  L  57: allocate_vocab_mask(self, vocab_size: int, batch_size: int, device)
         ‚Üí torch.Tensor

  L  63: move_vocab_mask(vocab_mask: torch.Tensor, device)
         ‚Üí torch.Tensor

  L  66: fill_vocab_mask(self, vocab_mask: torch.Tensor, idx: int)
         ‚Üí None

  L  75: apply_vocab_mask(logits: torch.Tensor, vocab_mask: torch.Tensor)

  L  78: copy(self)

  L  81: try_jump_forward(self, tokenizer)
         ‚Üí Optional[Tuple]

  L 105: jump_forward_str_state(self, helper: Tuple[List[int], str])
         ‚Üí Tuple[str, int]

  L 109: jump_and_retokenize(self, old_output_ids: List[int], new_output_ids: List[int], next_state: int)


CLASS: OutlinesGrammarBackend
----------------------------------------
  L 116: __init__(self, tokenizer, whitespace_pattern: bool)

  L 161: dispatch_ebnf(self, key_string: str)

  L 164: dispatch_structural_tag(self, key_string: str)

  L 167: dispatch_json(self, key_string: str)

  L 178: dispatch_regex(self, key_string: str)


============================================================
FILE: python/sglang/srt/constrained/outlines_jump_forward.py
Functions: 7
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  54: def disk_cache(expire: Optional[float], typed, ignore)

  L  62: def init_state_to_jump_forward(regex_string)
         @disk_cache()

  L 181: def test_main(regex_string)


CLASS: OutlinesJumpForwardMap
----------------------------------------
  L 143: __init__(self, regex_string)

  L 146: jump_forward_symbol(self, state)

  L 159: jump_forward_byte(self, state)

  L 174: is_jump_forward_symbol_state(self, state)


============================================================
FILE: python/sglang/srt/constrained/reasoner_grammar_backend.py
Functions: 13
============================================================


CLASS: ReasonerGrammarBackend
----------------------------------------
  L  79: __init__(self, grammar_backend: BaseGrammarBackend, think_end_id)


CLASS: ReasonerGrammarObject
----------------------------------------
  L  24: __init__(self, grammar: BaseGrammarObject, think_end_id)

  L  30: accept_token(self, token: int)

  L  37: allocate_vocab_mask(self, vocab_size: int, batch_size: int, device)
         ‚Üí torch.Tensor

  L  42: fill_vocab_mask(self, vocab_mask: torch.Tensor, idx: int)
         ‚Üí None

  L  46: move_vocab_mask(self, vocab_mask: torch.Tensor, device)
         ‚Üí torch.Tensor

  L  50: apply_vocab_mask(self)

  L  53: copy(self)
         ‚Üí BaseGrammarObject

  L  57: finished(self)

  L  61: finished(self, finished)

  L  64: try_jump_forward(self, tokenizer)

  L  67: jump_forward_str_state(self, helper)

  L  70: jump_and_retokenize(self, old_output_ids: List[int], new_output_ids: List[int], next_state: int)


============================================================
FILE: python/sglang/srt/constrained/triton_ops/bitmask_ops.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  14: def apply_token_bitmask_inplace_kernel(logits_ptr,
        bitmask_ptr,
        indices_ptr,
        num_rows,
        vocab_size,
        logits_strides,
        bitmask_strides,
        NUM_SMS: tl.constexpr,
        BLOCK_SIZE: tl.constexpr)
         üìù Apply a bitmask to logits in-place using Triton. The bitmask is a 01 b
         @triton.jit

  L  84: def apply_token_bitmask_inplace_triton(logits: torch.Tensor,
        bitmask: torch.Tensor,
        indices: Optional[Union[List[int],
        torch.Tensor]])


============================================================
FILE: python/sglang/srt/constrained/xgrammar_backend.py
Functions: 19
============================================================


CLASS: XGrammarGrammar
----------------------------------------
  L  52: __init__(self, matcher: GrammarMatcher, vocab_size: int, ctx: CompiledGrammar, override_stop_tokens: Optional[Union[List[int], int]], key_string: Optional[str])
         ‚Üí None

  L  68: accept_token(self, token: int)

  L  81: rollback(self, k: int)

  L  85: is_terminated(self)

  L  88: allocate_vocab_mask(self, vocab_size: int, batch_size: int, device)
         ‚Üí torch.Tensor

  L  93: fill_vocab_mask(self, vocab_mask: torch.Tensor, idx: int)
         ‚Üí None

  L  97: move_vocab_mask(vocab_mask: torch.Tensor, device)
         ‚Üí torch.Tensor

  L 100: apply_vocab_mask(self, logits: torch.Tensor, vocab_mask: torch.Tensor)
         ‚Üí None

  L 111: copy(self)

  L 125: try_jump_forward(self, tokenizer)
         ‚Üí Optional[Tuple[List[int], str]]

  L 131: jump_forward_str_state(self, helper: Tuple[List[int], str])
         ‚Üí Tuple[str, int]

  L 135: jump_and_retokenize(self, old_output_ids: List[int], new_output_ids: List[int], next_state: int)

  L 152: __repr__(self)


CLASS: XGrammarGrammarBackend
----------------------------------------
  L 157: __init__(self, tokenizer, vocab_size: int, model_eos_token_ids: Optional[List[int]])

  L 190: dispatch_json(self, key_string: str)
         ‚Üí Optional[XGrammarGrammar]

  L 203: dispatch_ebnf(self, key_string: str)
         ‚Üí Optional[XGrammarGrammar]

  L 211: dispatch_regex(self, key_string: str)
         ‚Üí Optional[XGrammarGrammar]

  L 219: dispatch_structural_tag(self, key_string: str)
         ‚Üí Optional[XGrammarGrammar]

  L 238: reset(self)


============================================================
FILE: python/sglang/srt/conversation.py
Functions: 25
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 465: def register_conv_template(template: Conversation, override: bool)
         üìù Register a new conversation template.

  L 475: def register_conv_template_matching_function(func)

  L 479: def get_conv_template_by_model_path(model_path)

  L 487: def chat_template_exists(template_name: str)
         ‚Üí bool

  L 491: def generate_embedding_convs(texts: List[str],
        images: List[str],
        template_name: str)
         ‚Üí List[Conversation]

  L 559: def generate_chat_conv(request: ChatCompletionRequest, template_name: str)
         ‚Üí Conversation

  L 974: def get_model_type(model_path: str)
         ‚Üí Optional[str]

  L 987: def match_internvl(model_path: str)
         @register_conv_template_matching_function

  L 995: def match_deepseek_janus_pro(model_path: str)
         @register_conv_template_matching_function

  L1003: def match_vicuna(model_path: str)
         @register_conv_template_matching_function

  L1009: def match_deepseek_vl(model_path: str)
         @register_conv_template_matching_function

  L1017: def match_qwen_chat_ml(model_path: str)
         @register_conv_template_matching_function

  L1027: def match_minicpm(model_path: str)
         @register_conv_template_matching_function

  L1036: def match_phi_4_mm(model_path: str)
         @register_conv_template_matching_function


CLASS: Conversation
----------------------------------------
  L 105: get_prompt(self)
         ‚Üí str
         üìù Get the prompt for generation.

  L 380: set_system_message(self, system_message: str)
         üìù Set the system message.

  L 384: append_message(self, role: str, message: str)
         üìù Append a new message.

  L 388: append_image(self, image: str, detail: Literal['auto', 'low', 'high'])
         üìù Append a new image.

  L 392: append_video(self, video: str)
         üìù Append a new video.

  L 396: append_audio(self, audio: str)
         üìù Append a new audio.

  L 400: update_last_message(self, message: str)
         üìù Update the last output.

  L 408: to_gradio_chatbot(self)
         üìù Convert the conversation to gradio chatbot format.

  L 418: to_openai_api_messages(self)
         üìù Convert the conversation to OpenAI chat completion format.

  L 433: copy(self)

  L 450: dict(self)


============================================================
FILE: python/sglang/srt/custom_op.py
Functions: 12
============================================================


CLASS: CustomOp
----------------------------------------
  L  13: __init__(self)

  L  21: enter_torch_compile(self, num_tokens: int)

  L  48: leave_torch_compile(self)

  L  58: forward(self)

  L  61: forward_native(self)

  L  64: forward_cuda(self)

  L  67: forward_npu(self)

  L  70: forward_hip(self)

  L  73: forward_xpu(self)

  L  76: forward_hpu(self)

  L  79: forward_cpu(self)

  L  82: dispatch_forward(self)


============================================================
FILE: python/sglang/srt/debug_utils/dump_comparator.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  12: def main(args)

  L  53: def read_meta(directory)

  L  78: def check_tensor_pair(path_baseline, path_target)


============================================================
FILE: python/sglang/srt/debug_utils/dumper.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  89: def get_truncated_value(value)


CLASS: _Dumper
----------------------------------------
  L  27: __init__(self)

  L  38: on_forward_pass_start(self)

  L  44: dump(self, name, value)


============================================================
FILE: python/sglang/srt/debug_utils/text_comparator.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  15: def main(args)


============================================================
FILE: python/sglang/srt/disaggregation/ascend/conn.py
Functions: 2
============================================================


CLASS: AscendKVManager
----------------------------------------
  L  16: init_engine(self)

  L  25: register_buffer_to_engine(self)


============================================================
FILE: python/sglang/srt/disaggregation/ascend/transfer_engine.py
Functions: 3
============================================================


CLASS: AscendTransferEngine
----------------------------------------
  L  13: __init__(self, hostname: str, npu_id: int, disaggregation_mode: DisaggregationMode)

  L  39: initialize(self)
         ‚Üí None
         üìù Initialize the ascend transfer instance.

  L  51: batch_register(self, ptrs: List[int], lengths: List[int])


============================================================
FILE: python/sglang/srt/disaggregation/base/conn.py
Functions: 11
============================================================


CLASS: BaseKVBootstrapServer
----------------------------------------
  L 134: __init__(self, port: int)


CLASS: BaseKVManager
----------------------------------------
  L  50: __init__(self, args: KVArgs, disaggregation_mode: DisaggregationMode, server_args: ServerArgs, is_mla_backend: Optional[bool])


CLASS: BaseKVReceiver
----------------------------------------
  L 103: __init__(self, mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: Optional[int])

  L 111: init(self, kv_indices: npt.NDArray[np.int32], aux_index: Optional[int])
         üìù Notify the prefill server about the kv indices and aux index

  L 118: poll(self)
         ‚Üí KVPoll
         üìù Check the status of the kv cache transfer

  L 125: failure_exception(self)
         üìù Raise an exception if the kv cache transfer fails


CLASS: BaseKVSender
----------------------------------------
  L  62: __init__(self, mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: int, dest_tp_ranks: List[int], pp_rank: int)

  L  72: init(self, num_kv_indices: int, aux_index: Optional[int])
         üìù Notify the decoder server about the kv indices length and aux index

  L  79: send(self, kv_indices: npt.NDArray[np.int32])
         üìù Send the kv cache at the given kv indices to the decoder server

  L  86: poll(self)
         ‚Üí KVPoll
         üìù Check the status of the kv cache transfer

  L  93: failure_exception(self)
         üìù Raise an exception if the kv cache transfer fails


============================================================
FILE: python/sglang/srt/disaggregation/common/conn.py
Functions: 7
============================================================


CLASS: CommonKVBootstrapServer
----------------------------------------
  L 311: __init__(self, port: int)

  L 326: run(self)

  L 425: close(self)
         üìù Shutdown

  L 435: poll(self)
         ‚Üí KVPoll


CLASS: CommonKVManager
----------------------------------------
  L  39: __init__(self, args: KVArgs, disaggregation_mode: DisaggregationMode, server_args: ServerArgs, is_mla_backend: Optional[bool])


CLASS: CommonKVReceiver
----------------------------------------
  L 123: __init__(self, mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: Optional[int], data_parallel_rank: Optional[int])

  L 306: failure_exception(self)


============================================================
FILE: python/sglang/srt/disaggregation/common/utils.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  28: def group_concurrent_contiguous(src_indices: npt.NDArray[np.int32],
        dst_indices: npt.NDArray[np.int32])
         ‚Üí Tuple[List[npt.NDArray[np.int32]], List[npt.NDArray[np.int32]]]
         üìù Vectorised NumPy implementation.


CLASS: FastQueue
----------------------------------------
  L  10: __init__(self)

  L  14: put(self, item)

  L  20: get(self)


============================================================
FILE: python/sglang/srt/disaggregation/decode.py
Functions: 21
============================================================


CLASS: DecodePreallocQueue
----------------------------------------
  L 139: __init__(self, req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, draft_token_to_kv_pool: Optional[KVCache], req_to_metadata_buffer_idx_allocator: ReqToMetadataIdxAllocator, metadata_buffers: MetadataBuffers, scheduler: Scheduler, transfer_queue: DecodeTransferQueue, tree_cache: BasePrefixCache, gloo_group: ProcessGroup, tp_rank: int, tp_size: int, dp_size: int, gpu_id: int, bootstrap_port: int, max_total_num_tokens: int, prefill_pp_size: int, num_reserved_decode_tokens: int, transfer_backend: TransferBackend)

  L 230: add(self, req: Req, is_retracted: bool)
         ‚Üí None
         üìù Add a request to the pending queue.

  L 267: extend(self, reqs: List[Req], is_retracted: bool)
         ‚Üí None
         üìù Add a request to the pending queue.

  L 272: resume_retracted_reqs(self)
         ‚Üí List[Req]

  L 342: pop_preallocated(self)
         ‚Üí List[DecodeRequest]
         üìù Pop the preallocated requests from the pending queue (FIFO).

  L 432: num_tokens_pre_allocated(self)


CLASS: DecodeReqToTokenPool
----------------------------------------
  L  77: __init__(self, size: int, max_context_len: int, device: str, enable_memory_saver: bool, pre_alloc_size: int)

  L 102: write(self, indices, values)

  L 105: available_size(self)

  L 108: alloc(self, need_size: int)
         ‚Üí List[int]

  L 116: free(self, free_index: Union[int, List[int]])

  L 122: clear(self)


CLASS: DecodeTransferQueue
----------------------------------------
  L 548: __init__(self, gloo_group: ProcessGroup, req_to_metadata_buffer_idx_allocator: ReqToMetadataIdxAllocator, tp_rank: int, metadata_buffers: MetadataBuffers, scheduler: Scheduler, tree_cache: BasePrefixCache)

  L 566: add(self, decode_req: DecodeRequest)
         ‚Üí None

  L 569: extend(self, decode_reqs: List[DecodeRequest])
         ‚Üí None

  L 572: pop_transferred(self)
         ‚Üí List[Req]


CLASS: SchedulerDisaggregationDecodeMixin
----------------------------------------
  L 675: event_loop_normal_disagg_decode(self: Scheduler)
         üìù A normal scheduler loop for decode worker in disaggregation mode.

  L 716: event_loop_overlap_disagg_decode(self: Scheduler)

  L 799: get_next_disagg_decode_batch_to_run(self: Scheduler)
         ‚Üí Optional[Tuple[ScheduleBatch, bool]]
         üìù Create fake completed prefill if possible and merge with running batch

  L 831: get_new_prebuilt_batch(self: Scheduler)
         ‚Üí Optional[ScheduleBatch]
         üìù Create a schedulebatch for fake completed prefill

  L 879: process_decode_queue(self: Scheduler)


============================================================
FILE: python/sglang/srt/disaggregation/decode_schedule_batch_mixin.py
Functions: 2
============================================================


CLASS: ScheduleBatchDisaggregationDecodeMixin
----------------------------------------
  L  23: prepare_for_prebuilt_extend(self: ScheduleBatch)
         üìù Prepare a prebuilt extend by populate metadata

  L 102: process_prebuilt_extend(self: ScheduleBatch, server_args: ServerArgs, model_config: ModelConfig)
         üìù Assign the buffered last input id to schedule batch


============================================================
FILE: python/sglang/srt/disaggregation/fake/conn.py
Functions: 9
============================================================


CLASS: FakeKVReceiver
----------------------------------------
  L  60: __init__(self, mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: Optional[int], data_parallel_rank: Optional[int])

  L  69: poll(self)
         ‚Üí KVPoll

  L  78: init(self, kv_indices: list[int], aux_index: Optional[int])

  L  84: failure_exception(self)


CLASS: FakeKVSender
----------------------------------------
  L  19: __init__(self, mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: int, dest_tp_ranks: List[int], pp_rank: int)

  L  29: poll(self)
         ‚Üí KVPoll

  L  38: init(self, kv_indices: list[int], aux_index: Optional[int])

  L  48: send(self, kv_indices: npt.NDArray[np.int32])

  L  55: failure_exception(self)


============================================================
FILE: python/sglang/srt/disaggregation/kv_events.py
Functions: 12
============================================================


CLASS: EventPublisher
----------------------------------------
  L  93: __init__(self, attn_dp_rank: int)

  L  97: publish(self, events: EventBatch)
         ‚Üí None
         üìù Emit events in order.

  L 105: shutdown(self)
         ‚Üí None
         üìù Shutdown the publisher.


CLASS: EventPublisherFactory
----------------------------------------
  L 394: register_publisher(cls, name: str, ctor: Callable[..., EventPublisher])
         ‚Üí None

  L 400: create(cls, config: Optional[str], attn_dp_rank: int)
         ‚Üí EventPublisher
         üìù Create publisher from a config mapping.


CLASS: KVEventsConfig
----------------------------------------
  L 382: from_cli(cls, cli_value: str)
         ‚Üí 'KVEventsConfig'
         üìù Parse the CLI value for the event publisher config.


CLASS: NullEventPublisher
----------------------------------------
  L 112: publish(self, events)
         ‚Üí None

  L 115: shutdown(self)
         ‚Üí None


CLASS: ZmqEventPublisher
----------------------------------------
  L 146: __init__(self, attn_dp_rank: int, endpoint: str, replay_endpoint: Optional[str], buffer_steps: int, hwm: int, max_queue_size: int, topic: str)
         ‚Üí None

  L 188: publish(self, events: EventBatch)
         ‚Üí None

  L 195: shutdown(self)
         ‚Üí None
         üìù Stop the publisher thread and clean up resources.

  L 314: offset_endpoint_port(endpoint: Optional[str], data_parallel_rank: int)
         ‚Üí Optional[str]
         üìù Helper function to offset the port in an endpoint by


============================================================
FILE: python/sglang/srt/disaggregation/launch_lb.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 112: def main()


CLASS: LBArgs
----------------------------------------
  L  19: add_cli_args(parser: argparse.ArgumentParser)

  L  78: from_cli_args(cls, args: argparse.Namespace)
         ‚Üí 'LBArgs'

  L 105: __post_init__(self)


============================================================
FILE: python/sglang/srt/disaggregation/mini_lb.py
Functions: 18
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  27: def setup_logger()

  L 184: async def health_check()
         @app.get('/health')

  L 189: async def health_check()
         @app.get('/health_generate')

  L 205: async def flush_cache()
         @app.post('/flush_cache')

  L 221: async def get_server_info()
         @app.get('/get_server_info')

  L 264: async def get_model_info()
         @app.get('/get_model_info')

  L 276: async def handle_generate_request(request_data: dict)
         @app.post('/generate')

  L 346: async def handle_chat_completion_request(request_data: dict)
         @app.post('/v1/chat/completions')

  L 351: async def handle_completion_request(request_data: dict)
         @app.post('/v1/completions')

  L 369: async def get_models()
         @app.get('/v1/models')

  L 385: async def register(obj: PDRegistryRequest)
         @app.post('/register')

  L 410: def run(prefill_configs, decode_addrs, host, port, timeout)


CLASS: MiniLoadBalancer
----------------------------------------
  L  53: __init__(self, prefill_configs: List[PrefillConfig], decode_servers: List[str], timeout: int)

  L  64: add_prefill_server(self, new_prefill_config: PrefillConfig)

  L  68: add_decode_server(self, new_decode_server: str)

  L  71: select_pair(self)

  L  80: generate(self, modified_request, prefill_server, decode_server, endpoint)
         ‚Üí ORJSONResponse

  L 118: generate_stream(self, modified_request, prefill_server, decode_server, endpoint)


============================================================
FILE: python/sglang/srt/disaggregation/mooncake/conn.py
Functions: 40
============================================================


CLASS: AuxDataCodec
----------------------------------------
  L 146: serialize_data_from_buffer(src_addr, data_length)
         üìù Serialize data from memory buffer to bytes

  L 152: deserialize_data_to_buffer(kv_args, buffer_index, aux_index, data)
         üìù Deserialize bytes into target memory buffer


CLASS: KVArgsRegisterInfo
----------------------------------------
  L 128: from_zmq(cls, msg: List[bytes])


CLASS: KVTransferError
----------------------------------------
  L  61: __init__(self, bootstrap_room: int, failure_reason: str)

  L  66: __str__(self)


CLASS: MooncakeKVBootstrapServer
----------------------------------------
  L1548: __init__(self, port: int)

  L1565: run(self)

  L1686: close(self)
         üìù Shutdown

  L1696: poll(self)
         ‚Üí KVPoll


CLASS: MooncakeKVManager
----------------------------------------
  L 165: __init__(self, args: KVArgs, disaggregation_mode: DisaggregationMode, server_args: ServerArgs, is_mla_backend: Optional[bool])

  L 277: init_engine(self)

  L 284: register_buffer_to_engine(self)

  L 314: send_kvcache(self, mooncake_session_id: str, prefill_kv_indices: npt.NDArray[np.int32], dst_kv_ptrs: list[int], dst_kv_indices: npt.NDArray[np.int32], executor: concurrent.futures.ThreadPoolExecutor)

  L 419: send_kvcache_slice(self, mooncake_session_id: str, prefill_kv_indices: npt.NDArray[np.int64], dst_kv_ptrs: list[int], dst_kv_indices: npt.NDArray[np.int64], dst_tp_rank: int, dst_attn_tp_size: int, dst_kv_item_len: int, executor: concurrent.futures.ThreadPoolExecutor)
         üìù Sends KV cache slices from this Prefill rank to a target Decode rank,

  L 583: send_aux(self, req: TransferInfo, prefill_aux_index: int, dst_aux_ptrs: list[int])

  L 605: send_aux_tcp(self, req: TransferInfo, prefill_aux_index: int, dst_aux_ptrs: list[int])

  L 630: send_aux_data_to_endpoint(self, remote: str, dst_port: int, room: int, buffer_index: int, aux_index: int, data: bytes)

  L 654: sync_status_to_decode_endpoint(self, remote: str, dst_port: int, room: int, status: int, prefill_rank: int)

  L 667: transfer_worker(self, queue: FastQueue, executor: concurrent.futures.ThreadPoolExecutor)

  L 805: start_prefill_thread(self)

  L 864: start_decode_thread(self)

  L 955: add_transfer_request(self, bootstrap_room: int, kv_indices: npt.NDArray[np.int32], index_slice: slice, is_last: bool, aux_index: Optional[int])

  L 998: check_status(self, bootstrap_room: int)

  L1001: update_status(self, bootstrap_room: int, status: KVPoll)

  L1013: record_failure(self, bootstrap_room: int, failure_reason: str)

  L1017: get_session_id(self)


CLASS: MooncakeKVReceiver
----------------------------------------
  L1207: __init__(self, mgr: MooncakeKVManager, bootstrap_addr: str, bootstrap_room: Optional[int], data_parallel_rank: Optional[int])

  L1470: init(self, kv_indices: npt.NDArray[np.int32], aux_index: Optional[int])

  L1489: poll(self)
         ‚Üí KVPoll

  L1515: clear(self)
         ‚Üí None

  L1525: failure_exception(self)

  L1538: abort(self)


CLASS: MooncakeKVSender
----------------------------------------
  L1103: __init__(self, mgr: MooncakeKVManager, bootstrap_addr: str, bootstrap_room: int, dest_tp_ranks: List[int], pp_rank: int)

  L1121: init(self, num_kv_indices: int, aux_index: Optional[int])

  L1125: send(self, kv_indices: npt.NDArray[np.int32])

  L1149: poll(self)
         ‚Üí KVPoll

  L1175: clear(self)
         ‚Üí None

  L1179: failure_exception(self)

  L1192: abort(self)


CLASS: TransferInfo
----------------------------------------
  L  93: from_zmq(cls, msg: List[bytes])


============================================================
FILE: python/sglang/srt/disaggregation/mooncake/transfer_engine.py
Functions: 9
============================================================


CLASS: MooncakeTransferEngine
----------------------------------------
  L  11: __init__(self, hostname: str, gpu_id: int, ib_device: Optional[str])

  L  34: register(self, ptr, length)

  L  44: deregister(self, ptr)

  L  54: batch_register(self, ptrs: List[int], lengths: List[int])
         ‚Üí int
         üìù Batch register multiple memory regions.

  L  71: batch_deregister(self, ptrs: List[int])
         ‚Üí int
         üìù Batch deregister multiple memory regions.

  L  83: initialize(self, hostname: str, device_name: Optional[str])
         ‚Üí None
         üìù Initialize the mooncake instance.

  L 108: transfer_sync(self, session_id: str, buffer: int, peer_buffer_address: int, length: int)
         ‚Üí int
         üìù Synchronously transfer data to the specified address.

  L 133: batch_transfer_sync(self, session_id: str, buffers: List[int], peer_buffer_addresses: List[int], lengths: List[int])
         ‚Üí int
         üìù Synchronously transfer data to the specified addresses in batches.

  L 163: get_session_id(self)


============================================================
FILE: python/sglang/srt/disaggregation/nixl/conn.py
Functions: 22
============================================================


CLASS: KVArgsRegisterInfo
----------------------------------------
  L  83: from_zmq(cls, msg: List[bytes])


CLASS: NixlKVManager
----------------------------------------
  L 114: __init__(self, args: KVArgs, disaggregation_mode: DisaggregationMode, server_args: ServerArgs, is_mla_backend: Optional[bool])

  L 151: check_status(self, bootstrap_room: int)

  L 154: update_status(self, bootstrap_room: int, status: KVPoll)

  L 163: register_buffer_to_engine(self)

  L 191: send_kvcache(self, peer_name: str, prefill_kv_indices: npt.NDArray[np.int32], dst_kv_ptrs: list[int], dst_kv_indices: npt.NDArray[np.int32], dst_gpu_id: int, notif: str)

  L 242: send_aux(self, peer_name: str, prefill_aux_index: int, dst_aux_ptrs: list[int], dst_aux_index: int, notif: str)

  L 275: add_transfer_request(self, bootstrap_room: int, kv_indices: npt.NDArray[np.int32], index_slice: slice, is_last: bool, chunk_id: int, aux_index: Optional[int])

  L 323: update_transfer_status(self)

  L 342: check_transfer_done(self, room: int)


CLASS: NixlKVReceiver
----------------------------------------
  L 452: __init__(self, mgr: NixlKVManager, bootstrap_addr: str, bootstrap_room: Optional[int], data_parallel_rank: Optional[int])

  L 463: init(self, kv_indices: npt.NDArray[np.int32], aux_index: Optional[int])

  L 489: poll(self)
         ‚Üí KVPoll

  L 527: failure_exception(self)


CLASS: NixlKVSender
----------------------------------------
  L 392: __init__(self, mgr: NixlKVManager, bootstrap_addr: str, bootstrap_room: int, dest_tp_ranks: List[int], pp_rank: int)

  L 411: init(self, num_kv_indices: int, aux_index: Optional[int])

  L 415: send(self, kv_indices: npt.NDArray[np.int32])

  L 437: poll(self)
         ‚Üí KVPoll

  L 447: failure_exception(self)


CLASS: TransferInfo
----------------------------------------
  L  53: is_dummy(self)

  L  57: from_zmq(cls, msg: List[bytes])


CLASS: TransferStatus
----------------------------------------
  L 107: is_done(self)


============================================================
FILE: python/sglang/srt/disaggregation/prefill.py
Functions: 14
============================================================


CLASS: PrefillBootstrapQueue
----------------------------------------
  L  68: __init__(self, token_to_kv_pool: KVCache, draft_token_to_kv_pool: Optional[KVCache], req_to_metadata_buffer_idx_allocator: ReqToMetadataIdxAllocator, metadata_buffers: MetadataBuffers, tp_rank: int, tp_size: int, gpu_id: int, bootstrap_port: int, gloo_group: ProcessGroup, max_total_num_tokens: int, decode_tp_size: int, decode_dp_size: int, scheduler: Scheduler, pp_rank: int, pp_size: int, transfer_backend: TransferBackend)

  L 152: add(self, req: Req, num_kv_heads: int)
         ‚Üí None

  L 173: extend(self, reqs: List[Req], num_kv_heads: int)
         ‚Üí None

  L 192: pop_bootstrapped(self, return_failed_reqs: bool, rids_to_check: Optional[List[str]])
         ‚Üí List[Req]
         üìù pop the reqs which has finished bootstrapping


CLASS: SchedulerDisaggregationPrefillMixin
----------------------------------------
  L 276: event_loop_normal_disagg_prefill(self: Scheduler)
         ‚Üí None
         üìù A normal scheduler loop for prefill worker in disaggregation mode.

  L 308: event_loop_overlap_disagg_prefill(self: Scheduler)
         ‚Üí None

  L 355: process_batch_result_disagg_prefill(self: Scheduler, batch: ScheduleBatch, result: GenerationBatchResult, launch_done: Optional[threading.Event])
         ‚Üí None
         üìù Transfer kv for prefill completed requests and add it into disagg_pref

  L 478: process_disagg_prefill_inflight_queue(self: Scheduler, rids_to_check: Optional[List[str]])
         ‚Üí List[Req]
         üìù Poll the requests in the middle of transfer. If done, return the reque

  L 547: get_transferred_rids(self: Scheduler)
         ‚Üí List[str]
         üìù Used by PP, get the transferred rids but **do not pop**

  L 564: process_prefill_chunk(self: Scheduler)
         ‚Üí None

  L 583: send_kv_chunk(self: Scheduler, req: Req, last_chunk: bool, end_idx: Optional[int])
         ‚Üí None
         üìù Send a prefilled chunk to the decode server

  L 622: event_loop_pp_disagg_prefill(self: Scheduler)
         üìù An event loop for the prefill server in pipeline parallelism.

  L 837: send_pyobj_to_next_stage(self, data)

  L 848: recv_pyobj_from_prev_stage(self)


============================================================
FILE: python/sglang/srt/disaggregation/utils.py
Functions: 16
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  43: def poll_and_all_reduce(pollers, gloo_group)

  L 220: def get_kv_class(transfer_backend: TransferBackend, class_type: KVClassType)

  L 293: def kv_to_page_indices(kv_indices: np.ndarray, page_size: int)

  L 303: def kv_to_page_num(num_kv_indices: int, page_size: int)

  L 332: def register_disaggregation_server(mode: str,
        server_port: int,
        bootstrap_port: int,
        pdlb_url: str)

  L 356: def is_mla_backend(target_kv_pool)
         ‚Üí bool

  L 362: def prepare_abort(req: Req, error_message: str, status_code)


CLASS: MetadataBuffers
----------------------------------------
  L  88: __init__(self, size: int, hidden_size: int, dtype: torch.dtype, max_top_logprobs_num: int, custom_mem_pool: torch.cuda.MemPool)

  L 131: get_buf_infos(self)

  L 158: get_buf(self, idx: int)

  L 168: set_buf(self, req: Req)


CLASS: PDRegistryRequest
----------------------------------------
  L 321: __post_init__(self)


CLASS: ReqToMetadataIdxAllocator
----------------------------------------
  L  67: __init__(self, size: int)

  L  74: available_size(self)

  L  77: alloc(self)
         ‚Üí Optional[int]

  L  83: free(self, free_index: int)


============================================================
FILE: python/sglang/srt/distributed/communication_op.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  11: def tensor_model_parallel_all_reduce(input_: torch.Tensor)
         ‚Üí torch.Tensor
         üìù All-reduce the input tensor across model parallel group.

  L  16: def tensor_model_parallel_all_gather(input_: torch.Tensor, dim: int)
         ‚Üí torch.Tensor
         üìù All-gather the input tensor across model parallel group.

  L  23: def tensor_model_parallel_gather(input_: torch.Tensor, dst: int, dim: int)
         ‚Üí Optional[torch.Tensor]
         üìù Gather the input tensor across model parallel group.

  L  30: def broadcast_tensor_dict(tensor_dict: Optional[Dict[Any,
        Union[torch.Tensor,
        Any]]],
        src: int)


============================================================
FILE: python/sglang/srt/distributed/device_communicators/cuda_wrapper.py
Functions: 13
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  37: def find_loaded_library(lib_name)
         ‚Üí Optional[str]
         üìù According to according to https://man7.org/linux/man-pages/man5/proc_p


CLASS: CudaRTLibrary
----------------------------------------
  L 114: __init__(self, so_file: Optional[str])

  L 133: CUDART_CHECK(self, result: cudaError_t)
         ‚Üí None

  L 138: cudaGetErrorString(self, error: cudaError_t)
         ‚Üí str

  L 141: cudaSetDevice(self, device: int)
         ‚Üí None

  L 144: cudaDeviceSynchronize(self)
         ‚Üí None

  L 147: cudaDeviceReset(self)
         ‚Üí None

  L 150: cudaMalloc(self, size: int)
         ‚Üí ctypes.c_void_p

  L 155: cudaFree(self, devPtr: ctypes.c_void_p)
         ‚Üí None

  L 158: cudaMemset(self, devPtr: ctypes.c_void_p, value: int, count: int)
         ‚Üí None

  L 161: cudaMemcpy(self, dst: ctypes.c_void_p, src: ctypes.c_void_p, count: int)
         ‚Üí None

  L 168: cudaIpcGetMemHandle(self, devPtr: ctypes.c_void_p)
         ‚Üí cudaIpcMemHandle_t

  L 175: cudaIpcOpenMemHandle(self, handle: cudaIpcMemHandle_t)
         ‚Üí ctypes.c_void_p


============================================================
FILE: python/sglang/srt/distributed/device_communicators/custom_all_reduce.py
Functions: 13
============================================================


CLASS: CustomAllreduce
----------------------------------------
  L  66: __init__(self, group: ProcessGroup, device: Union[int, str, torch.device], max_size)
         ‚Üí None
         üìù Args:

  L 215: create_shared_buffer(size_in_bytes: int, group: Optional[ProcessGroup])
         ‚Üí List[int]
         üìù Creates a shared buffer and returns a list of pointers

  L 240: free_shared_buffer(pointers: List[int], group: Optional[ProcessGroup])
         ‚Üí None

  L 248: capture(self)
         üìù The main responsibility of this context manager is the

  L 296: register_buffer(self, inp: torch.Tensor)

  L 300: register_graph_buffers(self)

  L 326: should_custom_ar(self, inp: torch.Tensor)

  L 351: all_reduce_reg(self, inp: torch.Tensor, out: torch.Tensor)

  L 358: all_reduce_unreg(self, inp: torch.Tensor, out: torch.Tensor)

  L 364: all_reduce(self, inp: torch.Tensor)
         üìù Performs an out-of-place all reduce.

  L 387: custom_all_reduce(self, input: torch.Tensor)
         ‚Üí Optional[torch.Tensor]
         üìù The main allreduce API that provides support for cuda graph.

  L 412: close(self)

  L 420: __del__(self)


============================================================
FILE: python/sglang/srt/distributed/device_communicators/custom_all_reduce_utils.py
Functions: 8
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  50: def update_environment_variables(envs: Dict[str, str])

  L  62: def producer(batch_src: Sequence[int],
        producer_queue,
        consumer_queue,
        result_queue,
        cuda_visible_devices: Optional[str])

  L  96: def consumer(batch_tgt: Sequence[int],
        producer_queue,
        consumer_queue,
        result_queue,
        cuda_visible_devices: Optional[str])

  L 137: def can_actually_p2p(batch_src: Sequence[int], batch_tgt: Sequence[int])
         ‚Üí Sequence[bool]
         üìù Usually, checking if P2P access is enabled can be done by

  L 237: def gpu_p2p_access_check(src: int, tgt: int)
         ‚Üí bool
         üìù Check if GPU src can access GPU tgt.

  L 312: def with_nvml_context(fn: Callable[_P, _R])
         ‚Üí Callable[_P, _R]

  L 332: def is_full_nvlink(physical_device_ids: List[int], world_size: int)
         ‚Üí bool
         @with_nvml_context

  L 373: def is_weak_contiguous(inp: torch.Tensor)


============================================================
FILE: python/sglang/srt/distributed/device_communicators/hpu_communicator.py
Functions: 3
============================================================


CLASS: HpuCommunicator
----------------------------------------
  L  15: __init__(self, group: ProcessGroup)

  L  23: all_reduce(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  31: all_gather(self, x: torch.Tensor, dim: int)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/distributed/device_communicators/npu_communicator.py
Functions: 3
============================================================


CLASS: NpuCommunicator
----------------------------------------
  L  10: __init__(self, group: ProcessGroup)

  L  18: all_reduce(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  22: all_gather(self, x: torch.Tensor, dim: int)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/distributed/device_communicators/pymscclpp.py
Functions: 8
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  39: def mscclpp_is_weak_contiguous(inp: torch.Tensor)

  L  46: def mscclpp_convert_to_bytes(size_str)
         üìù Converts a human-readable size string (e.g., "1MB", "2.5kb", "3 GB")

  L  87: def mscclpp_bench_time(func, test_niter: int, warmup_niter: int)


CLASS: PyMscclppCommunicator
----------------------------------------
  L 111: __init__(self, group: ProcessGroup, device: Union[int, str, torch.device], max_bytes)
         ‚Üí None
         üìù Args:

  L 243: pre_tune_config(self, dtype)
         ‚Üí bool

  L 273: should_mscclpp_allreduce(self, inp: torch.Tensor, op: ReduceOp)
         ‚Üí bool

  L 289: all_reduce(self, tensor: torch.Tensor, op: ReduceOp)

  L 302: change_state(self, enable: Optional[bool])


============================================================
FILE: python/sglang/srt/distributed/device_communicators/pynccl.py
Functions: 12
============================================================


CLASS: PyNcclCommunicator
----------------------------------------
  L  28: __init__(self, group: Union[ProcessGroup, StatelessProcessGroup], device: Union[int, str, torch.device], library_path: Optional[str])
         üìù Args:

  L 126: all_reduce(self, tensor: torch.Tensor, op: ReduceOp, stream)

  L 150: all_gather(self, output_tensor: torch.Tensor, input_tensor: torch.Tensor, stream, sizes: Optional[list[int]])

  L 196: reduce_scatter(self, output_tensor: torch.Tensor, input_tensor: torch.Tensor, op: ReduceOp, stream, sizes: Optional[list[int]])

  L 245: send(self, tensor: torch.Tensor, dst: int, stream)

  L 263: recv(self, tensor: torch.Tensor, src: int, stream)

  L 281: broadcast(self, tensor: torch.Tensor, src: int, stream)

  L 307: register_comm_window_raw(self, ptr: int, size: int)

  L 310: deregister_comm_window(self, window)

  L 313: group_start(self)

  L 316: group_end(self)

  L 320: change_state(self, enable: Optional[bool], stream: Optional[torch.cuda.Stream])
         üìù A context manager to change the state of the communicator.


============================================================
FILE: python/sglang/srt/distributed/device_communicators/pynccl_allocator.py
Functions: 7
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  34: def is_symmetric_memory_enabled()

  L  38: def set_graph_pool_id(graph_pool_id)

  L  43: def get_nccl_mem_pool()


CLASS: use_symmetric_memory
----------------------------------------
  L  67: __init__(self, group_coordinator: GroupCoordinator)

  L  81: __enter__(self)

  L 104: tag(self, tensor: torch.Tensor)

  L 109: __exit__(self, exc_type, exc_val, exc_tb)


============================================================
FILE: python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py
Functions: 22
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  37: def find_nccl_library()
         ‚Üí str
         üìù We either use the library file specified by the `SGLANG_NCCL_SO_PATH`


CLASS: NCCLLibrary
----------------------------------------
  L 332: __init__(self, so_file: Optional[str])

  L 368: ncclGetErrorString(self, result: ncclResult_t)
         ‚Üí str

  L 371: NCCL_CHECK(self, result: ncclResult_t)
         ‚Üí None

  L 376: ncclGetRawVersion(self)
         ‚Üí int

  L 382: ncclGetVersion(self)
         ‚Üí str

  L 390: ncclGetUniqueId(self)
         ‚Üí ncclUniqueId

  L 395: ncclCommInitRank(self, world_size: int, unique_id: ncclUniqueId, rank: int)
         ‚Üí ncclComm_t

  L 406: ncclAllReduce(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 427: ncclReduce(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, root: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 449: ncclReduceScatter(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 470: ncclAllGather(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 489: ncclSend(self, sendbuff: buffer_type, count: int, datatype: int, dest: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 502: ncclRecv(self, recvbuff: buffer_type, count: int, datatype: int, src: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 515: ncclBroadcast(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, root: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 531: ncclCommDestroy(self, comm: ncclComm_t)
         ‚Üí None

  L 534: ncclCommWindowRegister(self, comm: ncclComm_t, buff: buffer_type, size: int, win_flags: int)
         ‚Üí ncclWindow_t

  L 545: ncclCommWindowDeregister(self, comm: ncclComm_t, window: ncclWindow_t)
         ‚Üí None

  L 548: ncclGroupStart(self)
         ‚Üí None

  L 551: ncclGroupEnd(self)
         ‚Üí None


CLASS: ncclDataTypeEnum
----------------------------------------
  L 102: from_torch(cls, dtype: torch.dtype)
         ‚Üí int


CLASS: ncclRedOpTypeEnum
----------------------------------------
  L 134: from_torch(cls, op: ReduceOp)
         ‚Üí int


============================================================
FILE: python/sglang/srt/distributed/device_communicators/quick_all_reduce.py
Functions: 8
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  34: def qr_rocm_arch_available()


CLASS: QuickAllReduce
----------------------------------------
  L  73: __init__(self, group: ProcessGroup, device: Union[int, str, torch.device])
         ‚Üí None
         üìù Custom allreduce provides non-destructive acceleration and is

  L 175: init_quick_all_reduce(self)

  L 219: create_shared_buffer(self)
         üìù Creates a shared buffer for quickreduce.

  L 230: should_quick_allreduce(self, inp: torch.Tensor)
         üìù Check if quickreduce is available

  L 254: quick_all_reduce(self, inp: torch.Tensor)
         üìù Performs an out-of-place custom quick all reduce.

  L 265: close(self)

  L 272: __del__(self)


============================================================
FILE: python/sglang/srt/distributed/device_communicators/shm_broadcast.py
Functions: 15
============================================================


CLASS: MessageQueue
----------------------------------------
  L 176: __init__(self, n_reader, n_local_reader, local_reader_ranks: Optional[List[int]], max_chunk_bytes: int, max_chunks: int, connect_ip: Optional[str])

  L 257: export_handle(self)
         ‚Üí Handle

  L 261: create_from_handle(handle: Handle, rank)
         ‚Üí 'MessageQueue'

  L 304: wait_until_ready(self)
         üìù This is a collective operation. All processes (including the

  L 338: acquire_write(self)

  L 391: acquire_read(self)

  L 434: enqueue(self, obj)

  L 449: dequeue(self)

  L 468: broadcast_object(self, obj)

  L 476: create_from_process_group(pg: ProcessGroup, max_chunk_bytes, max_chunks, writer_rank)
         ‚Üí 'MessageQueue'


CLASS: ShmRingBuffer
----------------------------------------
  L  36: __init__(self, n_reader: int, max_chunk_bytes: int, max_chunks: int, name: Optional[str])
         üìù A shared memory ring buffer implementation for broadcast communication

  L 132: __reduce__(self)

  L 143: __del__(self)

  L 150: get_data(self, current_idx: int)

  L 157: get_metadata(self, current_idx: int)


============================================================
FILE: python/sglang/srt/distributed/device_communicators/xpu_communicator.py
Functions: 3
============================================================


CLASS: XpuCommunicator
----------------------------------------
  L  12: __init__(self, group: ProcessGroup)

  L  20: all_reduce(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  24: gather(self, input_: torch.Tensor, rank_in_group: int, dst: int, dim: int)


============================================================
FILE: python/sglang/srt/distributed/naive_distributed.py
Functions: 8
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 104: def get_naive_distributed()

  L 109: def set_naive_distributed(instance: NaiveDistributed)


CLASS: NaiveDistributed
----------------------------------------
  L  14: __init__(self, rank: int, world_size: int, rendezvous: str)

  L  25: get_rank(self)

  L  28: get_world_size(self)

  L  31: scatter(self, tensor: torch.Tensor, scatter_list: List[torch.Tensor], src: int)

  L  69: all_gather_object(self, obj: Any)
         ‚Üí List[Any]

  L  95: barrier(self)


============================================================
FILE: python/sglang/srt/distributed/parallel_state.py
Functions: 62
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 115: def inplace_all_reduce(tensor: torch.Tensor, group_name: str)
         ‚Üí None

  L 122: def inplace_all_reduce_fake(tensor: torch.Tensor, group_name: str)
         ‚Üí None

  L 132: def outplace_all_reduce(tensor: torch.Tensor,
        group_name: str,
        outplace_all_reduce_method: str)
         ‚Üí torch.Tensor

  L 141: def outplace_all_reduce_fake(tensor: torch.Tensor,
        group_name: str,
        outplace_all_reduce_method: str)
         ‚Üí torch.Tensor

  L 153: def reg_all_gather_into_tensor(output: torch.Tensor,
        input: torch.Tensor,
        group_name: str)
         ‚Üí None

  L 162: def reg_all_gather_into_tensor_fake(output: torch.Tensor,
        input: torch.Tensor,
        group_name: str)
         ‚Üí None

  L1177: def get_world_group()
         ‚Üí GroupCoordinator

  L1182: def init_world_group(ranks: List[int], local_rank: int, backend: str)
         ‚Üí GroupCoordinator

  L1199: def init_model_parallel_group(group_ranks: List[List[int]],
        local_rank: int,
        backend: str,
        use_custom_allreduce: Optional[bool],
        use_message_queue_broadcaster: bool,
        group_name: Optional[str],
        use_mscclpp_allreduce: Optional[bool])
         ‚Üí GroupCoordinator

  L1235: def set_pdmux_status(enable_prefill_multiplexing: bool)

  L1240: def get_tp_group()
         ‚Üí GroupCoordinator

  L1254: def get_moe_ep_group()
         ‚Üí GroupCoordinator

  L1259: def get_moe_tp_group()
         ‚Üí GroupCoordinator

  L1270: def get_pp_group()
         ‚Üí GroupCoordinator

  L1280: def graph_capture()
         üìù `graph_capture` is a context manager which should surround the code th
         @contextmanager

  L1306: def set_custom_all_reduce(enable: bool)

  L1311: def set_mscclpp_all_reduce(enable: bool)

  L1316: def init_distributed_environment(world_size: int,
        rank: int,
        distributed_init_method: str,
        local_rank: int,
        backend: str,
        timeout: Optional[int])

  L1371: def initialize_model_parallel(tensor_model_parallel_size: int,
        expert_model_parallel_size: int,
        pipeline_model_parallel_size: int,
        backend: Optional[str],
        duplicate_tp_group: bool)
         ‚Üí None
         üìù Initialize model parallel groups.

  L1508: def ensure_model_parallel_initialized(tensor_model_parallel_size: int,
        expert_model_parallel_size: int,
        pipeline_model_parallel_size: int,
        backend: Optional[str])
         ‚Üí None
         üìù Helper to initialize model parallel groups if they are not initialized

  L1541: def model_parallel_is_initialized()
         üìù Check if tensor and pipeline parallel groups are initialized.

  L1550: def patch_tensor_parallel_group(tp_group: GroupCoordinator)
         üìù Patch the tp group temporarily until this function ends.
         @contextmanager

  L1574: def get_tensor_model_parallel_world_size()
         üìù Return world size for the tensor model parallel group.

  L1579: def get_tensor_model_parallel_rank()
         üìù Return my rank for the tensor model parallel group.

  L1584: def get_moe_expert_parallel_world_size()
         üìù Return world size for the moe expert parallel group.

  L1589: def get_moe_expert_parallel_rank()
         üìù Return my rank for the moe expert parallel group.

  L1594: def get_moe_tensor_parallel_world_size()
         üìù Return world size for the moe tensor parallel group.

  L1599: def get_moe_tensor_parallel_rank()
         üìù Return my rank for the moe tensor parallel group.

  L1604: def destroy_model_parallel()
         üìù Set the groups to none and destroy them.

  L1617: def destroy_distributed_environment()

  L1626: def cleanup_dist_env_and_memory(shutdown_ray: bool)

  L1651: def in_the_same_node_as(pg: ProcessGroup, source_rank: int)
         ‚Üí List[bool]
         üìù This is a collective operation that returns if each rank is in the sam

  L1722: def monkey_patch_vllm_parallel_state(reverse: bool)


CLASS: GroupCoordinator
----------------------------------------
  L 212: __init__(self, group_ranks: List[List[int]], local_rank: int, torch_distributed_backend: Union[str, Backend], use_pynccl: bool, use_pymscclpp: bool, use_custom_allreduce: bool, use_hpu_communicator: bool, use_xpu_communicator: bool, use_npu_communicator: bool, use_message_queue_broadcaster: bool, group_name: Optional[str])

  L 362: __repr__(self)

  L 370: first_rank(self)
         üìù Return the global rank of the first process in the group

  L 375: last_rank(self)
         üìù Return the global rank of the last process in the group

  L 380: is_first_rank(self)
         üìù Return whether the caller is the first process in the group

  L 385: is_last_rank(self)
         üìù Return whether the caller is the last process in the group

  L 390: next_rank(self)
         üìù Return the global rank of the process that follows the caller

  L 397: prev_rank(self)
         üìù Return the global rank of the process that precedes the caller

  L 404: graph_capture(self, graph_capture_context: Optional[GraphCaptureContext])

  L 464: all_reduce(self, input_: torch.Tensor)
         ‚Üí torch.Tensor
         üìù User-facing all-reduce function before we actually call the

  L 571: reduce_scatter_tensor(self, output: torch.Tensor, input: torch.Tensor)
         ‚Üí None

  L 580: reduce_scatter(self, output: torch.Tensor, input_list: List[torch.Tensor])
         ‚Üí None

  L 589: reduce_scatterv(self, input_: torch.Tensor, output: Optional[torch.Tensor], sizes: Optional[List[int]])
         ‚Üí torch.Tensor

  L 631: all_gather_into_tensor(self, output: torch.Tensor, input: torch.Tensor)

  L 639: all_gather(self, input_: torch.Tensor, dim: int, output_tensor_list: Optional[List[torch.Tensor]])
         ‚Üí torch.Tensor

  L 712: all_gatherv(self, input_: Union[torch.Tensor, List[torch.Tensor]], sizes: Optional[List[int]])
         ‚Üí Union[torch.Tensor, List[torch.Tensor]]
         üìù Supports varying sizes per rank and input tensor list.

  L 760: gather(self, input_: torch.Tensor, dst: int, dim: int)
         ‚Üí Optional[torch.Tensor]
         üìù NOTE: We assume that the input tensor is on the same device across

  L 795: broadcast(self, input_: torch.Tensor, src: int)
         üìù Broadcast the input tensor.

  L 810: broadcast_object(self, obj: Optional[Any], src: int)
         üìù Broadcast the input object.

  L 834: broadcast_object_list(self, obj_list: List[Any], src: int, group: Optional[ProcessGroup])
         üìù Broadcast the input object list.

  L 851: send_object(self, obj: Any, dst: int)
         ‚Üí None
         üìù Send the input object list to the destination rank.

  L 885: recv_object(self, src: int)
         ‚Üí Any
         üìù Receive the input object list from the source rank.

  L 923: broadcast_tensor_dict(self, tensor_dict: Optional[Dict[str, Union[torch.Tensor, Any]]], src: int, group: Optional[ProcessGroup], metadata_group: Optional[ProcessGroup])
         ‚Üí Optional[Dict[str, Union[torch.Tensor, Any]]]
         üìù Broadcast the input tensor dictionary.

  L1005: send_tensor_dict(self, tensor_dict: Dict[str, Union[torch.Tensor, Any]], dst: Optional[int], all_gather_group: Optional['GroupCoordinator'])
         ‚Üí Optional[Dict[str, Union[torch.Tensor, Any]]]
         üìù Send the input tensor dictionary.

  L1060: recv_tensor_dict(self, src: Optional[int], all_gather_group: Optional['GroupCoordinator'])
         ‚Üí Optional[Dict[str, Union[torch.Tensor, Any]]]
         üìù Recv the input tensor dictionary.

  L1122: barrier(self)
         üìù Barrier synchronization among the group.

  L1131: send(self, tensor: torch.Tensor, dst: Optional[int])
         ‚Üí None
         üìù Sends a tensor to the destination rank in a non-blocking way

  L1143: recv(self, size: torch.Size, dtype: torch.dtype, src: Optional[int])
         ‚Üí torch.Tensor
         üìù Receives a tensor from the source rank.

  L1159: destroy(self)


============================================================
FILE: python/sglang/srt/distributed/utils.py
Functions: 12
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  21: def ensure_divisibility(numerator, denominator)
         üìù Ensure that numerator is divisible by the denominator.

  L  28: def divide(numerator, denominator)
         üìù Ensure that numerator is divisible by the denominator and return

  L  35: def split_tensor_along_last_dim(tensor: torch.Tensor,
        num_partitions: int,
        contiguous_split_chunks: bool)
         ‚Üí Sequence[torch.Tensor]
         üìù Split a tensor along its last dimension.

  L  63: def get_pp_indices(num_hidden_layers: int, pp_rank: int, pp_size: int)
         ‚Üí Tuple[int, int]
         üìù Try to evenly distribute layers across partitions.


CLASS: StatelessProcessGroup
----------------------------------------
  L 118: __post_init__(self)

  L 124: send_obj(self, obj: Any, dst: int)
         üìù Send an object to a destination rank.

  L 132: expire_data(self)
         üìù Expire data that is older than `data_expiration_seconds` seconds.

  L 143: recv_obj(self, src: int)
         ‚Üí Any
         üìù Receive an object from a source rank.

  L 151: broadcast_obj(self, obj: Optional[Any], src: int)
         ‚Üí Any
         üìù Broadcast an object from a source rank to all other ranks.

  L 169: all_gather_obj(self, obj: Any)
         ‚Üí list[Any]
         üìù All gather an object from all ranks.

  L 181: barrier(self)
         üìù A barrier to synchronize all ranks.

  L 190: create(host: str, port: int, rank: int, world_size: int, data_expiration_seconds: int)
         ‚Üí 'StatelessProcessGroup'
         üìù A replacement for `torch.distributed.init_process_group` that does not


============================================================
FILE: python/sglang/srt/entrypoints/EngineBase.py
Functions: 8
============================================================


CLASS: EngineBase
----------------------------------------
  L  14: generate(self, prompt: Optional[Union[List[str], str]], sampling_params: Optional[Union[List[Dict], Dict]], input_ids: Optional[Union[List[List[int]], List[int]]], image_data: Optional[Union[List[str], str]], return_logprob: Optional[Union[List[bool], bool]], logprob_start_len: Optional[Union[List[int], int]], top_logprobs_num: Optional[Union[List[int], int]], token_ids_logprob: Optional[Union[List[List[int]], List[int]]], lora_path: Optional[Union[List[Optional[str]], Optional[str]]], custom_logit_processor: Optional[Union[List[str], str]], return_hidden_states: Optional[bool], stream: Optional[bool], bootstrap_host: Optional[Union[List[str], str]], bootstrap_port: Optional[Union[List[int], int]], bootstrap_room: Optional[Union[List[int], int]], data_parallel_rank: Optional[int])
         ‚Üí Union[Dict, Iterator[Dict]]
         üìù Generate outputs based on given inputs.

  L  37: flush_cache(self)
         üìù Flush the cache of the engine.

  L  42: update_weights_from_tensor(self, named_tensors: List[Tuple[str, torch.Tensor]], load_format: Optional[str], flush_cache: bool)
         üìù Update model weights with in-memory tensor data.

  L  51: load_lora_adapter(self, lora_name: str, lora_path: str)
         üìù Load a new LoRA adapter without re-launching the engine.

  L  55: unload_lora_adapter(self, lora_name: str)
         üìù Unload a LoRA adapter without re-launching the engine.

  L  60: release_memory_occupation(self)
         üìù Release GPU memory occupation temporarily.

  L  65: resume_memory_occupation(self)
         üìù Resume GPU memory occupation which is previously released.

  L  70: shutdown(self)
         üìù Shutdown the engine and clean up resources.


============================================================
FILE: python/sglang/srt/entrypoints/context.py
Functions: 23
============================================================


CLASS: ConversationContext
----------------------------------------
  L  28: append_output(self, output)
         ‚Üí None

  L  32: call_tool(self)
         ‚Üí list[Message]

  L  36: need_builtin_tool_call(self)
         ‚Üí bool

  L  40: render_for_completion(self)
         ‚Üí list[int]


CLASS: HarmonyContext
----------------------------------------
  L  64: __init__(self, messages: list, tool_sessions: dict[str, Union['ClientSession', Tool]])

  L  82: append_output(self, output)
         ‚Üí None

  L 114: messages(self)
         ‚Üí list

  L 117: need_builtin_tool_call(self)
         ‚Üí bool

  L 126: call_tool(self)
         ‚Üí list[Message]

  L 142: render_for_completion(self)
         ‚Üí list[int]

  L 145: call_search_tool(self, tool_session: Union['ClientSession', Tool], last_msg: Message)
         ‚Üí list[Message]

  L 158: call_python_tool(self, tool_session: Union['ClientSession', Tool], last_msg: Message)
         ‚Üí list[Message]


CLASS: SimpleContext
----------------------------------------
  L  46: __init__(self)

  L  49: append_output(self, output)
         ‚Üí None

  L  52: need_builtin_tool_call(self)
         ‚Üí bool

  L  55: call_tool(self)
         ‚Üí list[Message]

  L  58: render_for_completion(self)
         ‚Üí list[int]


CLASS: StreamingHarmonyContext
----------------------------------------
  L 184: __init__(self)

  L 193: messages(self)
         ‚Üí list

  L 196: append_output(self, output)
         ‚Üí None

  L 226: is_expecting_start(self)
         ‚Üí bool

  L 229: is_assistant_action_turn(self)
         ‚Üí bool

  L 232: render_for_completion(self)
         ‚Üí list[int]


============================================================
FILE: python/sglang/srt/entrypoints/engine.py
Functions: 31
============================================================


CLASS: Engine
----------------------------------------
  L 103: __init__(self)
         üìù The arguments of this function is the same as `sglang/srt/server_args.

  L 140: generate(self, prompt: Optional[Union[List[str], str]], sampling_params: Optional[Union[List[Dict], Dict]], input_ids: Optional[Union[List[List[int]], List[int]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat], return_logprob: Optional[Union[List[bool], bool]], logprob_start_len: Optional[Union[List[int], int]], top_logprobs_num: Optional[Union[List[int], int]], token_ids_logprob: Optional[Union[List[List[int]], List[int]]], lora_path: Optional[List[Optional[str]]], custom_logit_processor: Optional[Union[List[str], str]], return_hidden_states: bool, stream: bool, bootstrap_host: Optional[Union[List[str], str]], bootstrap_port: Optional[Union[List[int], int]], bootstrap_room: Optional[Union[List[int], int]], data_parallel_rank: Optional[int])
         ‚Üí Union[Dict, Iterator[Dict]]
         üìù The arguments of this function is the same as `sglang/srt/managers/io_

  L 221: async_generate(self, prompt: Optional[Union[List[str], str]], sampling_params: Optional[Union[List[Dict], Dict]], input_ids: Optional[Union[List[List[int]], List[int]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat], return_logprob: Optional[Union[List[bool], bool]], logprob_start_len: Optional[Union[List[int], int]], top_logprobs_num: Optional[Union[List[int], int]], token_ids_logprob: Optional[Union[List[List[int]], List[int]]], lora_path: Optional[List[Optional[str]]], custom_logit_processor: Optional[Union[List[str], str]], return_hidden_states: bool, stream: bool, bootstrap_host: Optional[Union[List[str], str]], bootstrap_port: Optional[Union[List[int], int]], bootstrap_room: Optional[Union[List[int], int]], data_parallel_rank: Optional[int])
         ‚Üí Union[Dict, AsyncIterator[Dict]]
         üìù The arguments of this function is the same as `sglang/srt/managers/io_

  L 293: encode(self, prompt: Union[str, List[str], List[Dict], List[List[Dict]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat])
         ‚Üí Dict
         üìù The arguments of this function is the same as `sglang/srt/managers/io_

  L 315: async_encode(self, prompt: Union[str, List[str], List[Dict], List[List[Dict]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat])
         ‚Üí Dict
         üìù Asynchronous version of encode method.

  L 337: rerank(self, prompt: Union[List[List[str]]])
         ‚Üí Dict
         üìù The arguments of this function is the same as `sglang/srt/managers/io_

  L 351: shutdown(self)
         üìù Shutdown the engine

  L 355: __enter__(self)

  L 358: __exit__(self, exc_type, exc_value, traceback)

  L 362: flush_cache(self)

  L 366: start_profile(self)

  L 370: stop_profile(self)

  L 374: start_expert_distribution_record(self)

  L 380: stop_expert_distribution_record(self)

  L 386: dump_expert_distribution_record(self)

  L 392: get_server_info(self)

  L 404: init_weights_update_group(self, master_address: str, master_port: int, rank_offset: int, world_size: int, group_name: str, backend: str)
         üìù Initialize parameter update group.

  L 427: update_weights_from_distributed(self, names: list[str], dtypes: list[str], shapes: list[list[int]], group_name: str, flush_cache: bool)
         üìù Update weights from distributed source.

  L 448: update_weights_from_tensor(self, named_tensors: List[Tuple[str, torch.Tensor]], load_format: Optional[str], flush_cache: bool)
         üìù Update weights from distributed source. If there are going to be more 

  L 474: update_weights_from_disk(self, model_path: str, load_format: Optional[str])
         üìù Update the weights from disk inplace without re-launching the engine.

  L 495: get_weights_by_name(self, name: str, truncate_size: int)
         üìù Get weights by parameter name.

  L 503: load_lora_adapter(self, lora_name: str, lora_path: str, pinned: bool)
         üìù Load a new LoRA adapter without re-launching the engine.

  L 517: unload_lora_adapter(self, lora_name: str)
         üìù Unload a LoRA adapter without re-launching the engine.

  L 527: release_memory_occupation(self, tags: Optional[List[str]])

  L 534: resume_memory_occupation(self, tags: Optional[List[str]])

  L 541: freeze_gc(self)
         üìù To maintain a high performance server with low latency, we want to red

  L 561: collective_rpc(self, method: str)

  L 568: save_remote_model(self)

  L 571: save_sharded_model(self)

  L 574: score(self, query: Optional[Union[str, List[int]]], items: Optional[Union[str, List[str], List[List[int]]]], label_token_ids: Optional[List[int]], apply_softmax: bool, item_first: bool)
         ‚Üí List[List[float]]
         üìù Score the probability of specified token IDs appearing after the given

  L 625: async_score(self, query: Optional[Union[str, List[int]]], items: Optional[Union[str, List[str], List[List[int]]]], label_token_ids: Optional[List[int]], apply_softmax: bool, item_first: bool)
         ‚Üí List[List[float]]
         üìù Asynchronous version of score method.


============================================================
FILE: python/sglang/srt/entrypoints/harmony_utils.py
Functions: 13
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  54: def get_encoding()

  L  61: def get_system_message(model_identity: Optional[str],
        reasoning_effort: Optional[Literal['high',
        'medium',
        'low']],
        start_date: Optional[str],
        browser_description: Optional[str],
        python_description: Optional[str])
         ‚Üí Message

  L  86: def get_developer_message(instructions: Optional[str],
        tools: Optional[list[Tool]])
         ‚Üí Message

  L 118: def get_user_message(content: str)
         ‚Üí Message

  L 122: def parse_response_input(response_msg: ResponseInputOutputItem,
        prev_responses: list[Union[ResponseOutputItem,
        ResponseReasoningItem]])
         ‚Üí Message

  L 174: def parse_response_output(output: ResponseOutputItem)
         ‚Üí Message

  L 190: def parse_chat_input(chat_msg)
         ‚Üí Message

  L 202: def render_for_completion(messages: list[Message])
         ‚Üí list[int]

  L 210: def get_stop_tokens_for_assistant_actions()
         ‚Üí list[int]

  L 214: def get_streamable_parser_for_assistant()
         ‚Üí StreamableParser

  L 218: def parse_output_message(message: Message)

  L 324: def parse_remaining_state(parser: StreamableParser)

  L 368: def parse_output_into_messages(token_ids: Iterable[int])


============================================================
FILE: python/sglang/srt/entrypoints/http_server.py
Functions: 55
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 128: def set_global_state(global_state: _GlobalState)

  L 134: async def lifespan(fast_api_app: FastAPI)
         @asynccontextmanager

  L 212: async def validation_exception_handler(request: Request, exc: HTTPException)
         üìù Enrich HTTP exception with status code and other details
         @app.exception_handler(HTTPException)

  L 225: async def validation_exception_handler(request: Request,
        exc: RequestValidationError)
         üìù Override FastAPI's default 422 validation error with 400
         @app.exception_handler(RequestValidationError)

  L 247: async def validate_json_request(raw_request: Request)
         üìù Validate that the request content-type is application/json.

  L 268: async def health_generate(request: Request)
         ‚Üí Response
         üìù Check the health of the inference server by sending a special request 
         @app.get('/health')
         @app.get('/health_generate')

  L 339: async def get_model_info()
         üìù Get the model information.
         @app.get('/get_model_info')

  L 352: async def get_weight_version()
         üìù Get the current weight version.
         @app.get('/get_weight_version')

  L 360: async def get_server_info()
         @app.get('/get_server_info')

  L 374: async def get_load()
         @app.get('/get_load')

  L 381: async def set_internal_state(obj: SetInternalStateReq, request: Request)
         @app.api_route('/set_internal_state', methods=['POST', 'PUT'])

  L 388: async def generate_request(obj: GenerateReqInput, request: Request)
         üìù Handle a generate request.
         @app.api_route('/generate', methods=['POST', 'PUT'])

  L 425: async def generate_from_file_request(file: UploadFile, request: Request)
         üìù Handle a generate request, this is purely to work with input_embeds.
         @app.api_route('/generate_from_file', methods=['POST'])

  L 449: async def encode_request(obj: EmbeddingReqInput, request: Request)
         üìù Handle an embedding request.
         @app.api_route('/encode', methods=['POST', 'PUT'])

  L 461: async def classify_request(obj: EmbeddingReqInput, request: Request)
         üìù Handle a reward model request. Now the arguments and return values are
         @app.api_route('/classify', methods=['POST', 'PUT'])

  L 473: async def flush_cache()
         üìù Flush the radix cache.
         @app.api_route('/flush_cache', methods=['GET', 'POST'])

  L 484: async def start_profile_async(obj: Optional[ProfileReqInput])
         üìù Start profiling.
         @app.api_route('/start_profile', methods=['GET', 'POST'])

  L 505: async def stop_profile_async()
         üìù Stop profiling.
         @app.api_route('/stop_profile', methods=['GET', 'POST'])

  L 515: async def freeze_gc_async()
         üìù See engine.freeze_gc for more details.
         @app.api_route('/freeze_gc', methods=['GET', 'POST'])

  L 527: async def start_expert_distribution_record_async()
         üìù Start recording the expert distribution. Clear the previous record if 
         @app.api_route('/start_expert_distribution_record', methods=['GET', 'POST'])

  L 537: async def stop_expert_distribution_record_async()
         üìù Stop recording the expert distribution.
         @app.api_route('/stop_expert_distribution_record', methods=['GET', 'POST'])

  L 547: async def dump_expert_distribution_record_async()
         üìù Dump expert distribution record.
         @app.api_route('/dump_expert_distribution_record', methods=['GET', 'POST'])

  L 557: async def update_weights_from_disk(obj: UpdateWeightFromDiskReqInput,
        request: Request)
         üìù Update the weights from disk inplace without re-launching the server.
         @app.post('/update_weights_from_disk')

  L 586: async def init_weights_update_group(obj: InitWeightsUpdateGroupReqInput,
        request: Request)
         üìù Initialize the parameter update group.
         @app.post('/init_weights_update_group')

  L 601: async def update_weights_from_tensor(obj: UpdateWeightsFromTensorReqInput,
        request: Request)
         üìù Update the weights from tensor inplace without re-launching the server
         @app.post('/update_weights_from_tensor')

  L 627: async def update_weights_from_distributed(obj: UpdateWeightsFromDistributedReqInput,
        request: Request)
         üìù Update model parameter from distributed online.
         @app.post('/update_weights_from_distributed')

  L 650: async def update_weight_version(obj: UpdateWeightVersionReqInput,
        request: Request)
         üìù Update the weight version. This operation requires no active requests.
         @app.post('/update_weight_version')

  L 680: async def get_weights_by_name(obj: GetWeightsByNameReqInput, request: Request)
         üìù Get model parameter by name.
         @app.api_route('/get_weights_by_name', methods=['GET', 'POST'])

  L 693: async def release_memory_occupation(obj: ReleaseMemoryOccupationReqInput,
        request: Request)
         üìù Release GPU memory occupation temporarily.
         @app.api_route('/release_memory_occupation', methods=['GET', 'POST'])

  L 704: async def resume_memory_occupation(obj: ResumeMemoryOccupationReqInput,
        request: Request)
         üìù Resume GPU memory occupation.
         @app.api_route('/resume_memory_occupation', methods=['GET', 'POST'])

  L 715: async def slow_down(obj: SlowDownReqInput, request: Request)
         üìù Slow down the system deliberately. Only for testing. Example scenario:
         @app.api_route('/slow_down', methods=['GET', 'POST'])

  L 728: async def load_lora_adapter(obj: LoadLoRAAdapterReqInput, request: Request)
         üìù Load a new LoRA adapter without re-launching the server.
         @app.api_route('/load_lora_adapter', methods=['POST'])

  L 745: async def unload_lora_adapter(obj: UnloadLoRAAdapterReqInput, request: Request)
         üìù Load a new LoRA adapter without re-launching the server.
         @app.api_route('/unload_lora_adapter', methods=['POST'])

  L 762: async def open_session(obj: OpenSessionReqInput, request: Request)
         üìù Open a session, and return its unique session id.
         @app.api_route('/open_session', methods=['GET', 'POST'])

  L 776: async def close_session(obj: CloseSessionReqInput, request: Request)
         üìù Close the session.
         @app.api_route('/close_session', methods=['GET', 'POST'])

  L 786: async def configure_logging(obj: ConfigureLoggingReq, request: Request)
         üìù Configure the request logging options.
         @app.api_route('/configure_logging', methods=['GET', 'POST'])

  L 793: async def abort_request(obj: AbortReq, request: Request)
         üìù Abort a request.
         @app.post('/abort_request')

  L 805: async def parse_function_call_request(obj: ParseFunctionCallReq,
        request: Request)
         üìù A native API endpoint to parse function calls from a text.
         @app.post('/parse_function_call')

  L 827: async def separate_reasoning_request(obj: SeparateReasoningReqInput,
        request: Request)
         üìù A native API endpoint to separate reasoning from a text.
         @app.post('/separate_reasoning')

  L 847: async def pause_generation(request: Request)
         üìù Pause generation.
         @app.post('/pause_generation')

  L 857: async def continue_generation(request: Request)
         üìù Continue generation.
         @app.post('/continue_generation')

  L 870: async def openai_v1_completions(request: CompletionRequest,
        raw_request: Request)
         üìù OpenAI-compatible text completion endpoint.
         @app.post('/v1/completions', dependencies=[Depends(validate_json_request)])

  L 878: async def openai_v1_chat_completions(request: ChatCompletionRequest,
        raw_request: Request)
         üìù OpenAI-compatible chat completion endpoint.
         @app.post('/v1/chat/completions', dependencies=[Depends(validate_json_request)])

  L 892: async def openai_v1_embeddings(request: EmbeddingRequest, raw_request: Request)
         üìù OpenAI-compatible embeddings endpoint.
         @app.post('/v1/embeddings', response_class=ORJSONResponse, dependencies=[Depends(validate_json_request)])

  L 900: async def available_models()
         üìù Show available models. OpenAI-compatible endpoint.
         @app.get('/v1/models', response_class=ORJSONResponse)

  L 916: async def retrieve_model(model: str)
         üìù Retrieves a model instance, providing basic information about the mode
         @app.get('/v1/models/{model:path}', response_class=ORJSONResponse)

  L 941: async def v1_score_request(request: ScoringRequest, raw_request: Request)
         üìù Endpoint for the decoder-only scoring API. See Engine.score() for deta
         @app.post('/v1/score', dependencies=[Depends(validate_json_request)])

  L 949: async def v1_responses_request(request: dict, raw_request: Request)
         üìù Endpoint for the responses API with reasoning support.
         @app.post('/v1/responses', dependencies=[Depends(validate_json_request)])

  L 969: async def v1_retrieve_responses(response_id: str, raw_request: Request)
         üìù Retrieve a response by ID.
         @app.get('/v1/responses/{response_id}')

  L 977: async def v1_cancel_responses(response_id: str, raw_request: Request)
         üìù Cancel a background response.
         @app.post('/v1/responses/{response_id}/cancel')

  L 987: async def v1_rerank_request(request: V1RerankReqInput, raw_request: Request)
         üìù Endpoint for reranking documents based on query relevance.
         @app.api_route('/v1/rerank', methods=['POST', 'PUT'], dependencies=[Depends(validate_json_request)])

  L 996: async def sagemaker_health()
         ‚Üí Response
         üìù Check the health of the http server.
         @app.get('/ping')

  L1002: async def sagemaker_chat_completions(request: ChatCompletionRequest,
        raw_request: Request)
         üìù OpenAI-compatible chat completion endpoint.
         @app.post('/invocations')

  L1013: async def vertex_generate(vertex_req: VertexGenerateReqInput,
        raw_request: Request)
         @app.post(os.environ.get('AIP_PREDICT_ROUTE', '/vertex_generate'))

  L1051: def launch_server(server_args: ServerArgs,
        pipe_finish_writer: Optional[multiprocessing.connection.Connection],
        launch_callback: Optional[Callable[[],
        None]])
         üìù Launch SRT (SGLang Runtime) Server.


============================================================
FILE: python/sglang/srt/entrypoints/http_server_engine.py
Functions: 8
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  20: def launch_server_process(server_args: ServerArgs)
         ‚Üí multiprocessing.Process


CLASS: HttpServerEngineAdapter
----------------------------------------
  L  58: __init__(self)

  L  78: update_weights_from_tensor(self, named_tensors: List[Tuple[str, torch.Tensor]], load_format: Optional[str], flush_cache: bool)
         üìù Update model weights from tensor data. The HTTP server will only post 

  L 102: shutdown(self)

  L 105: generate(self, prompt, sampling_params, input_ids, image_data, return_logprob, logprob_start_len, top_logprobs_num, token_ids_logprob, lora_path, custom_logit_processor)

  L 135: release_memory_occupation(self)

  L 138: resume_memory_occupation(self)

  L 141: flush_cache(self)


============================================================
FILE: python/sglang/srt/entrypoints/openai/protocol.py
Functions: 4
============================================================


CLASS: ChatCompletionRequest
----------------------------------------
  L 455: set_tool_choice_default(cls, values)


CLASS: CompletionRequest
----------------------------------------
  L 234: validate_max_tokens_positive(cls, v)


CLASS: ResponsesRequest
----------------------------------------
  L 730: to_sampling_params(self, default_max_tokens: int, default_params: Optional[Dict])
         ‚Üí Dict[str, Any]
         üìù Convert to sampling parameters for generation.


CLASS: ResponsesResponse
----------------------------------------
  L 801: from_request(cls, request: ResponsesRequest, sampling_params: Any, model_name: str, created_time: int, output: List[Union[ResponseOutputItem, ResponseReasoningItem, ResponseFunctionToolCall]], status: str, usage: Optional[UsageInfo])
         ‚Üí 'ResponsesResponse'
         üìù Create a response from a request.


============================================================
FILE: python/sglang/srt/entrypoints/openai/serving_base.py
Functions: 4
============================================================


CLASS: OpenAIServingBase
----------------------------------------
  L  21: __init__(self, tokenizer_manager: TokenizerManager)

  L  24: handle_request(self, request: OpenAIServingRequest, raw_request: Request)
         ‚Üí Union[Any, StreamingResponse, ErrorResponse]
         üìù Handle the specific request type with common pattern

  L 120: create_error_response(self, message: str, err_type: str, status_code: int, param: Optional[str])
         ‚Üí ORJSONResponse
         üìù Create an error response

  L 138: create_streaming_error_response(self, message: str, err_type: str, status_code: int)
         ‚Üí str
         üìù Create a streaming error response


============================================================
FILE: python/sglang/srt/entrypoints/openai/serving_chat.py
Functions: 1
============================================================


CLASS: OpenAIServingChat
----------------------------------------
  L  49: __init__(self, tokenizer_manager: TokenizerManager, template_manager: TemplateManager)


============================================================
FILE: python/sglang/srt/entrypoints/openai/serving_completions.py
Functions: 1
============================================================


CLASS: OpenAIServingCompletion
----------------------------------------
  L  34: __init__(self, tokenizer_manager: TokenizerManager, template_manager: TemplateManager)


============================================================
FILE: python/sglang/srt/entrypoints/openai/serving_embedding.py
Functions: 1
============================================================


CLASS: OpenAIServingEmbedding
----------------------------------------
  L  24: __init__(self, tokenizer_manager: TokenizerManager, template_manager: TemplateManager)


============================================================
FILE: python/sglang/srt/entrypoints/openai/serving_responses.py
Functions: 6
============================================================


CLASS: OpenAIServingResponses
----------------------------------------
  L  68: __init__(self, tokenizer_manager: TokenizerManager, template_manager: TemplateManager)
         ‚Üí None

  L 126: create_responses(self, request: ResponsesRequest, raw_request: Optional[Request])
         ‚Üí Union[AsyncGenerator[str, None], ResponsesResponse, ORJSONResponse]

  L 389: responses_full_generator(self, request: ResponsesRequest, sampling_params: Any, result_generator: AsyncIterator[Any], context: ConversationContext, model_name: str, tokenizer: Any, request_metadata: RequestResponseMetadata, created_time: Optional[int])
         ‚Üí Union[ResponsesResponse, ORJSONResponse]

  L 708: retrieve_responses(self, response_id: str)
         ‚Üí Union[ResponsesResponse, ORJSONResponse]

  L 722: cancel_responses(self, response_id: str)
         ‚Üí Union[ResponsesResponse, ORJSONResponse]

  L 771: responses_stream_generator(self, request: ResponsesRequest, sampling_params: Any, result_generator: AsyncIterator[StreamingHarmonyContext], context: StreamingHarmonyContext, model_name: str, tokenizer: Any, request_metadata: RequestResponseMetadata, created_time: Optional[int])
         ‚Üí AsyncGenerator[str, None]


============================================================
FILE: python/sglang/srt/entrypoints/openai/tool_server.py
Functions: 15
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  20: async def list_server_and_tools(server_url: str)

  L  30: def trim_schema(schema: dict)
         ‚Üí dict

  L  55: def post_process_tools_description(list_tools_result: 'ListToolsResult')
         ‚Üí 'ListToolsResult'


CLASS: DemoToolServer
----------------------------------------
  L 145: __init__(self)

  L 160: has_tool(self, tool_name: str)

  L 163: get_tool_description(self, tool_name: str)

  L 174: get_tool_session(self, tool_name: str)


CLASS: MCPToolServer
----------------------------------------
  L  89: __init__(self)

  L  92: add_tool_server(self, server_url: str)

  L 124: has_tool(self, tool_name: str)

  L 127: get_tool_description(self, tool_name: str)

  L 131: get_tool_session(self, tool_name: str)


CLASS: ToolServer
----------------------------------------
  L  76: has_tool(self, tool_name: str)

  L  80: get_tool_description(self, tool_name: str)

  L  84: get_tool_session(self, tool_name: str)
         ‚Üí AbstractAsyncContextManager[Any]


============================================================
FILE: python/sglang/srt/entrypoints/openai/usage_processor.py
Functions: 3
============================================================


CLASS: UsageProcessor
----------------------------------------
  L  18: calculate_response_usage(responses: List[Dict[str, Any]], n_choices: int, enable_cache_report: bool)
         ‚Üí UsageInfo

  L  44: calculate_streaming_usage(prompt_tokens: Mapping[int, int], completion_tokens: Mapping[int, int], cached_tokens: Mapping[int, int], n_choices: int, enable_cache_report: bool)
         ‚Üí UsageInfo

  L  70: calculate_token_usage(prompt_tokens: int, completion_tokens: int, cached_tokens: Optional[Dict[str, int]])
         ‚Üí UsageInfo
         üìù Calculate token usage information


============================================================
FILE: python/sglang/srt/entrypoints/openai/utils.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  13: def to_openai_style_logprobs(input_token_logprobs,
        output_token_logprobs,
        input_top_logprobs,
        output_top_logprobs)

  L  50: def process_hidden_states_from_ret(ret_item: Dict[str,
        Any],
        request: Union[ChatCompletionRequest,
        CompletionRequest])
         ‚Üí Optional[List]
         üìù Process hidden states from a ret item in non-streaming response.


============================================================
FILE: python/sglang/srt/entrypoints/tool.py
Functions: 7
============================================================


CLASS: HarmonyBrowserTool
----------------------------------------
  L  25: __init__(self)

  L  45: get_result(self, context: 'ConversationContext')
         ‚Üí Any

  L  56: tool_config(self)
         ‚Üí Any


CLASS: HarmonyPythonTool
----------------------------------------
  L  62: __init__(self)

  L  75: get_result(self, context: 'ConversationContext')
         ‚Üí Any

  L  86: tool_config(self)
         ‚Üí Any


CLASS: Tool
----------------------------------------
  L  19: get_result(self, context: 'ConversationContext')
         ‚Üí Any


============================================================
FILE: python/sglang/srt/eplb/eplb_algorithms/__init__.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  17: def rebalance_experts(tokens_per_expert: torch.Tensor,
        num_physical_experts: int,
        num_local_physical_experts: int,
        num_groups: Optional[int],
        num_nodes: int,
        algorithm: EplbAlgorithm)

  L  51: def compute_algorithm(raw_algorithm: str,
        num_groups: Optional[int],
        num_nodes: int)
         ‚Üí EplbAlgorithm


============================================================
FILE: python/sglang/srt/eplb/eplb_algorithms/deepseek.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L   9: def balanced_packing(weight: torch.Tensor, num_packs: int)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù Pack n weighted objects to m packs, such that each bin contains exactl

  L  54: def replicate_experts(weight: torch.Tensor, num_phy: int)
         ‚Üí Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
         üìù Replicate `num_log` experts to `num_phy` replicas, such that the maxim

  L  85: def rebalance_experts_hierarchical(weight: torch.Tensor,
        num_physical_experts: int,
        num_groups: int,
        num_nodes: int,
        num_gpus: int)
         üìù Parameters:

  L 170: def rebalance_experts(weight: torch.Tensor,
        num_replicas: int,
        num_groups: int,
        num_nodes: int,
        num_gpus: int,
        enable_hierarchical: bool)
         ‚Üí Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
         üìù Entry point for expert-parallelism load balancer.


============================================================
FILE: python/sglang/srt/eplb/eplb_algorithms/deepseek_vec.py
Functions: 5
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L   7: def pack_groups(tokens_per_group: torch.Tensor, num_nodes: int)
         ‚Üí torch.Tensor

  L  35: def make_redundant_experts_chunkwise(tokens_per_expert: torch.Tensor,
        num_physical_experts: int,
        num_local_physical_experts: int,
        num_physical_experts_per_chunk: int)
         ‚Üí Tuple[torch.Tensor, torch.Tensor, torch.Tensor]

  L 184: def decode_rebalance_experts(tokens_per_expert: torch.Tensor,
        num_physical_experts: int,
        num_local_physical_experts: int)

  L 197: def prefill_rebalance_experts(tokens_per_expert: torch.Tensor,
        num_physical_experts: int,
        num_local_physical_experts: int,
        num_groups: int,
        num_nodes: int)

  L 255: def rebalance_experts(tokens_per_expert: torch.Tensor,
        num_physical_experts: int,
        num_local_physical_experts: int,
        num_groups: Optional[int],
        num_nodes: int,
        enable_hierarchical: bool)


============================================================
FILE: python/sglang/srt/eplb/eplb_manager.py
Functions: 3
============================================================


CLASS: EPLBManager
----------------------------------------
  L  17: __init__(self, model_runner: 'ModelRunner')

  L  41: on_forward_pass_end(self)

  L  52: rebalance(self)


============================================================
FILE: python/sglang/srt/eplb/eplb_simulator/reader.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  16: def read_mode_per_pass(dir_data: Path)
         üìù Read data from ExpertDistributionRecorder when recorded with mode `per


============================================================
FILE: python/sglang/srt/eplb/expert_distribution.py
Functions: 91
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 265: def get_global_expert_distribution_recorder()

  L 269: def set_global_expert_distribution_recorder(value)

  L 898: def compute_gpu_physical_count(physical_count_of_whatever: torch.Tensor,
        num_gpu: int)
         üìù output: gpu_physical_count_of_batch (..., num_layer, num_gpu)

  L 911: def compute_utilization_rate(gpu_physical_count_of_batch: torch.Tensor)
         üìù output: utilization_rate (..., num_layer)


CLASS: ExpertDistributionRecorder
----------------------------------------
  L  43: init_new(server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)

  L  61: with_current_layer(self, layer_idx)

  L  65: with_debug_name(self, debug_name)

  L  69: disable_this_region(self)

  L  73: with_forward_pass(self, forward_pass_id: int, forward_batch: ForwardBatch)

  L  76: on_select_experts(self, topk_ids: torch.Tensor)

  L  79: on_deepep_dispatch_normal(self, local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)

  L  88: on_deepep_dispatch_low_latency(self, local_physical_count_of_layer: torch.Tensor)

  L  93: start_record(self)

  L  96: stop_record(self)

  L  99: dump_record(self, output_mode: _OutputMode)

  L 103: recording(self)


CLASS: _Accumulator
----------------------------------------
  L 561: init_new(server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)
         ‚Üí '_Accumulator'

  L 571: get_class(server_args: ServerArgs)
         ‚Üí Type['_Accumulator']

  L 579: __init__(self, server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)

  L 589: get_single_pass_gatherer_keys(self)

  L 592: get_single_pass_gatherer_key(self, debug_name: Optional[str])

  L 595: append(self, forward_pass_id: int, gatherer_key: str, single_pass_data: Dict)

  L 603: reset(self)

  L 606: dump(self, output_mode: _OutputMode)


CLASS: _Buffer
----------------------------------------
  L 812: init_new(item_shape: Tuple, buffer_size: int, dtype, device)

  L 818: append(self, value: torch.Tensor)

  L 821: get_all(self)
         ‚Üí torch.Tensor

  L 824: reset(self)


CLASS: _CircularBuffer
----------------------------------------
  L 829: __init__(self, item_shape: Tuple, buffer_size: int, dtype, device)

  L 835: append(self, value: torch.Tensor)

  L 839: get_all(self)
         ‚Üí torch.Tensor

  L 842: reset(self)


CLASS: _DeepepLowLatencySinglePassGatherer
----------------------------------------
  L 527: __init__(self)

  L 530: on_deepep_dispatch_low_latency(self, layer_idx: int, local_physical_count_of_layer: torch.Tensor)


CLASS: _DeepepNormalSinglePassGatherer
----------------------------------------
  L 494: __init__(self)

  L 502: on_deepep_dispatch_normal(self, layer_idx: int, local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)

  L 513: collect(self)
         ‚Üí Dict


CLASS: _DequeCollection
----------------------------------------
  L 668: __init__(self, maxlens: List[int])

  L 671: append(self, value)

  L 675: clear(self)

  L 679: mean(self)
         ‚Üí Dict[int, float]


CLASS: _DetailAccumulator
----------------------------------------
  L 684: __init__(self)

  L 688: get_single_pass_gatherer_keys(self)

  L 693: get_single_pass_gatherer_key(self, debug_name: Optional[str])

  L 698: append(self, forward_pass_id: int, gatherer_key: str, single_pass_data: Dict)

  L 724: reset(self)

  L 728: dump(self, output_mode: _OutputMode)


CLASS: _DetailSinglePassGatherer
----------------------------------------
  L 345: __init__(self, server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)

  L 369: on_forward_pass_start(self, forward_batch: ForwardBatch)

  L 380: on_select_experts(self, layer_idx: int, topk_ids: torch.Tensor)

  L 385: on_deepep_dispatch_normal(self, layer_idx: int, local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)

  L 402: reset(self)

  L 407: collect(self)
         ‚Üí Dict


CLASS: _ExpertDistributionRecorderReal
----------------------------------------
  L 117: __init__(self, server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)

  L 145: with_current_layer(self, layer_idx)

  L 148: with_debug_name(self, debug_name)

  L 152: with_forward_pass(self, forward_pass_id: int, forward_batch: ForwardBatch)

  L 161: disable_this_region(self)
         üìù Context manager to temporarily disable recording.

  L 184: on_select_experts(self, topk_ids: torch.Tensor)

  L 187: on_deepep_dispatch_normal(self, local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)

  L 202: on_deepep_dispatch_low_latency(self, local_physical_count_of_layer: torch.Tensor)

  L 232: start_record(self)
         üìù Start recording the expert distribution.

  L 241: stop_record(self)
         üìù Stop recording the expert distribution.

  L 249: dump_record(self, output_mode: _OutputMode)
         üìù Dump the expert distribution record and reset the recorder after dumpi

  L 256: recording(self)


CLASS: _InfiniteBuffer
----------------------------------------
  L 847: __init__(self, item_shape: Tuple, dtype, device)

  L 852: append(self, value: torch.Tensor)

  L 867: get_all(self)
         ‚Üí torch.Tensor

  L 870: reset(self)


CLASS: _LayerBasedCpuSinglePassGatherer
----------------------------------------
  L 417: __init__(self)

  L 430: reset(self)


CLASS: _LayerBasedGpuSinglePassGatherer
----------------------------------------
  L 446: __init__(self)

  L 462: reset(self)

  L 465: collect(self)
         ‚Üí Dict


CLASS: _SelectExpertsSinglePassGatherer
----------------------------------------
  L 481: __init__(self)

  L 485: on_select_experts(self, layer_idx: int, topk_ids: torch.Tensor)


CLASS: _SinglePassGatherer
----------------------------------------
  L 279: init_new(server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)
         ‚Üí '_SinglePassGatherer'

  L 309: __init__(self, expert_location_metadata: 'ExpertLocationMetadata', rank: int)

  L 313: on_forward_pass_start(self, forward_batch: ForwardBatch)

  L 316: on_select_experts(self, layer_idx: int, topk_ids: torch.Tensor)

  L 319: on_deepep_dispatch_normal(self, layer_idx: int, local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)

  L 329: on_deepep_dispatch_low_latency(self, layer_idx: int, local_physical_count_of_layer: torch.Tensor)

  L 334: reset(self)

  L 337: collect(self)
         ‚Üí Dict


CLASS: _StatAccumulator
----------------------------------------
  L 741: __init__(self)

  L 755: append(self, forward_pass_id: int, gatherer_key: str, single_pass_data: Dict)

  L 767: reset(self)

  L 771: dump(self, output_mode: _OutputMode)


CLASS: _UtilizationRateAccumulatorMixin
----------------------------------------
  L 611: __init__(self)

  L 621: append(self, forward_pass_id: int, gatherer_key: str, single_pass_data: Dict)

  L 633: reset(self)


============================================================
FILE: python/sglang/srt/eplb/expert_location.py
Functions: 16
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 289: def get_global_expert_location_metadata()

  L 293: def set_global_expert_location_metadata(value)

  L 337: def compute_logical_to_rank_dispatch_physical_map(logical_to_all_physical_map: torch.Tensor,
        num_gpus: int,
        num_physical_experts: int,
        ep_rank: int,
        seed: int)

  L 431: def compute_initial_expert_location_metadata(server_args: ServerArgs,
        model_config: ModelConfig)
         ‚Üí Optional[ExpertLocationMetadata]


CLASS: ExpertLocationMetadata
----------------------------------------
  L  46: num_layers(self)
         ‚Üí int

  L  50: num_physical_experts(self)
         ‚Üí int

  L  54: num_local_physical_experts(self)
         ‚Üí int

  L  60: num_logical_experts(self)
         ‚Üí int

  L  64: ep_size(self)

  L  68: __post_init__(self)

  L  83: init_trivial(server_args: ServerArgs, model_config: ModelConfig)
         üìù Trivial location - logical expert i corresponds to physical expert i

  L 107: init_by_mapping(server_args: ServerArgs, model_config: ModelConfig, physical_to_logical_map)

  L 135: init_by_eplb(server_args: ServerArgs, model_config: ModelConfig, logical_count: torch.Tensor)

  L 242: update(self, other: 'ExpertLocationMetadata', update_layer_ids: List[int])

  L 273: logical_to_all_physical(self, layer_id: int, logical_expert_id: int)
         ‚Üí List[int]


CLASS: ModelConfigForExpertLocation
----------------------------------------
  L 421: from_model_config(model_config: ModelConfig)


============================================================
FILE: python/sglang/srt/eplb/expert_location_dispatch.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  64: def transform_select_experts_inputs(router_logits: torch.Tensor,
        correction_bias: Optional[torch.Tensor],
        info: Optional[ExpertLocationDispatchInfo])

  L  76: def topk_ids_logical_to_physical(topk_ids: torch.Tensor,
        info: Optional[ExpertLocationDispatchInfo])
         ‚Üí torch.Tensor


CLASS: ExpertLocationDispatchInfo
----------------------------------------
  L  36: init_new(cls, layer_id: int)


============================================================
FILE: python/sglang/srt/eplb/expert_location_updater.py
Functions: 7
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 160: def create_temp_buffers(sample_tensors)

  L 164: def update_expert_weights_single_layer(routed_experts_weights: List[torch.Tensor],
        temp_buffers: List[torch.Tensor],
        old_physical_to_logical_map: List[int],
        new_physical_to_logical_map: List[int],
        num_local_physical_experts: int,
        num_gpu_per_node: int,
        rank: int,
        world_size: Optional[int],
        debug: bool,
        log_metrics: bool)


CLASS: ExpertLocationUpdater
----------------------------------------
  L  37: __init__(self)

  L  40: update(self, routed_experts_weights_of_layer: Dict[int, List[torch.Tensor]], new_expert_location_metadata: ExpertLocationMetadata, update_layer_ids: List[int], nnodes: int, rank: int)


CLASS: _ChunkUtils
----------------------------------------
  L 474: __init__(self)

  L 478: chunk_value_from_element_value(self, element_value)

  L 486: element_values_from_chunk_value(self, chunk_value)
         ‚Üí List


============================================================
FILE: python/sglang/srt/function_call/base_format_detector.py
Functions: 8
============================================================


CLASS: BaseFormatDetector
----------------------------------------
  L  27: __init__(self)

  L  69: parse_base_json(self, action: Any, tools: List[Tool])
         ‚Üí List[ToolCallItem]

  L  94: detect_and_parse(self, text: str, tools: List[Tool])
         ‚Üí StreamingParseResult
         üìù Parses the text in one go. Returns success=True if the format matches,

  L 115: parse_streaming_increment(self, new_text: str, tools: List[Tool])
         ‚Üí StreamingParseResult
         üìù        Streaming incremental parsing with tool validation.

  L 318: has_tool_call(self, text: str)
         ‚Üí bool
         üìù Check if the given text contains function call markers specific to thi

  L 324: supports_structural_tag(self)
         ‚Üí bool
         üìù Return True if this detector supports structural tag format.

  L 329: structure_info(self)
         ‚Üí _GetInfoFunc
         üìù Return a function that creates StructureInfo for constrained generatio

  L 343: build_ebnf(self, tools: List[Tool])
         ‚Üí str
         üìù Build an EBNF grammar for constrained generation of function calls.


============================================================
FILE: python/sglang/srt/function_call/deepseekv31_detector.py
Functions: 6
============================================================


CLASS: DeepSeekV31Detector
----------------------------------------
  L  46: __init__(self)

  L  57: has_tool_call(self, text: str)
         ‚Üí bool
         üìù Check if the text contains a deepseek format tool call.

  L  61: detect_and_parse(self, text: str, tools: List[Tool])
         ‚Üí StreamingParseResult
         üìù One-time parsing: Detects and parses tool calls in the provided text.

  L  91: parse_streaming_increment(self, new_text: str, tools: List[Tool])
         ‚Üí StreamingParseResult
         üìù Streaming incremental parsing tool calls for DeepSeekV3 format.

  L 207: structure_info(self)
         ‚Üí _GetInfoFunc

  L 214: build_ebnf(self, tools: List[Tool])


============================================================
FILE: python/sglang/srt/function_call/deepseekv3_detector.py
Functions: 6
============================================================


CLASS: DeepSeekV3Detector
----------------------------------------
  L  46: __init__(self)

  L  55: has_tool_call(self, text: str)
         ‚Üí bool
         üìù Check if the text contains a deepseek format tool call.

  L  59: detect_and_parse(self, text: str, tools: List[Tool])
         ‚Üí StreamingParseResult
         üìù One-time parsing: Detects and parses tool calls in the provided text.

  L  89: parse_streaming_increment(self, new_text: str, tools: List[Tool])
         ‚Üí StreamingParseResult
         üìù Streaming incremental parsing tool calls for DeepSeekV3 format.

  L 205: structure_info(self)
         ‚Üí _GetInfoFunc

  L 212: build_ebnf(self, tools: List[Tool])


============================================================
FILE: python/sglang/srt/function_call/ebnf_composer.py
Functions: 3
============================================================


CLASS: EBNFComposer
----------------------------------------
  L  92: get_value_rule(prop: dict, function_format: Literal['pythonic', 'json', 'xml'])
         ‚Üí str

  L 132: get_type_mapping(function_format: str)
         ‚Üí Dict[str, str]
         üìù Get the complete type mapping for a given format.

  L 155: build_ebnf(tools, function_format: Literal['pythonic', 'json', 'xml'], sequence_start_token: Optional[str], sequence_end_token: Optional[str], individual_call_start_token: Optional[str], individual_call_end_token: Optional[str], tool_call_separator: Optional[str], call_rule_fmt: Optional[str], key_value_rule_fmt: Optional[str], key_value_separator: str)
         üìù Generalized EBNF builder for all detectors.


============================================================
FILE: python/sglang/srt/function_call/function_call_parser.py
Functions: 7
============================================================


CLASS: FunctionCallParser
----------------------------------------
  L  50: __init__(self, tools: List[Tool], tool_call_parser: str)

  L  61: has_tool_call(self, text: str)
         ‚Üí bool
         üìù Check if the given text contains a tool call in the format supported b

  L  74: parse_non_stream(self, full_text: str)
         ‚Üí Tuple[str, list[ToolCallItem]]
         üìù One-time parsing of the full text to extract tool calls.

  L  93: parse_stream_chunk(self, chunk_text: str)
         ‚Üí Tuple[str, list[ToolCallItem]]
         üìù Streaming incremental parsing of chunks of text as they arrive.

  L 117: get_structure_tag(self)
         ‚Üí StructuralTagResponseFormat
         üìù Generate a structural tag response format for all available tools.

  L 151: get_structure_constraint(self, tool_choice: Union[ToolChoice, Literal['auto', 'required']])
         ‚Üí Optional[Tuple[str, Any]]
         üìù Returns the appropriate structure constraint for tool calls based on t

  L 178: get_ebnf(self, tool_choice: Union[ToolChoice, Literal['required']])
         ‚Üí Optional[str]
         üìù Get the EBNF grammar for the specified tool choice.


============================================================
FILE: python/sglang/srt/function_call/glm4_moe_detector.py
Functions: 9
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  19: def get_argument_type(func_name: str, arg_key: str, defined_tools: list)

  L  29: def parse_arguments(json_value)


CLASS: Glm4MoeDetector
----------------------------------------
  L  47: __init__(self)

  L  55: has_tool_call(self, text: str)
         ‚Üí bool
         üìù Check if the text contains a glm-4.5 format tool call.

  L  59: detect_and_parse(self, text: str, tools: List[Tool])
         ‚Üí StreamingParseResult
         üìù One-time parsing: Detects and parses tool calls in the provided text.

  L 101: parse_streaming_increment(self, new_text: str, tools: List[Tool])
         ‚Üí StreamingParseResult
         üìù Streaming incremental parsing tool calls for GLM-4.5 format.

  L 148: supports_structural_tag(self)
         ‚Üí bool

  L 151: structure_info(self)
         ‚Üí _GetInfoFunc

  L 154: build_ebnf(self, tools: List[Tool])


============================================================
FILE: python/sglang/srt/function_call/gpt_oss_detector.py
Functions: 6
============================================================


CLASS: GptOssDetector
----------------------------------------
  L  26: __init__(self)

  L  38: has_tool_call(self, text: str)
         ‚Üí bool
         üìù Check if text contains TypeScript-style function call markers.

  L  42: detect_and_parse(self, text: str, tools: List[Tool])
         ‚Üí StreamingParseResult
         üìù Parse TypeScript-style function calls from complete text.

  L  75: parse_streaming_increment(self, new_text: str, tools: List[Tool])
         ‚Üí StreamingParseResult
         üìù Parse incremental streaming text for TypeScript-style function calls.

  L 215: structure_info(self)
         ‚Üí _GetInfoFunc

  L 218: build_ebnf(self, tools: List[Tool])
         ‚Üí str


============================================================
FILE: python/sglang/srt/function_call/kimik2_detector.py
Functions: 6
============================================================


CLASS: KimiK2Detector
----------------------------------------
  L  34: __init__(self)

  L  53: has_tool_call(self, text: str)
         ‚Üí bool
         üìù Check if the text contains a KimiK2 format tool call.

  L  57: detect_and_parse(self, text: str, tools: List[Tool])
         ‚Üí StreamingParseResult
         üìù One-time parsing: Detects and parses tool calls in the provided text.

  L 100: parse_streaming_increment(self, new_text: str, tools: List[Tool])
         ‚Üí StreamingParseResult
         üìù Streaming incremental parsing tool calls for KimiK2 format.

  L 217: structure_info(self)
         ‚Üí _GetInfoFunc
         üìù Return function that creates StructureInfo for guided generation.

  L 229: build_ebnf(self, tools: List[Tool])
         ‚Üí str
         üìù Build EBNF grammar for KimiK2 tool call format.


============================================================
FILE: python/sglang/srt/function_call/llama32_detector.py
Functions: 5
============================================================


CLASS: Llama32Detector
----------------------------------------
  L  27: __init__(self)

  L  36: has_tool_call(self, text: str)
         ‚Üí bool
         üìù Check if the text contains a Llama 3.2 format tool call.

  L  42: detect_and_parse(self, text: str, tools: List[Tool])
         ‚Üí StreamingParseResult
         üìù Parse function calls from text, handling multiple JSON objects.

  L  84: structure_info(self)
         ‚Üí _GetInfoFunc

  L  91: build_ebnf(self, tools: List[Tool])


============================================================
FILE: python/sglang/srt/function_call/mistral_detector.py
Functions: 5
============================================================


CLASS: MistralDetector
----------------------------------------
  L  33: __init__(self)
         üìù Initializes the detector with necessary state variables.

  L  43: has_tool_call(self, text: str)
         ‚Üí bool
         üìù Check if the text contains a Mistral format tool call.

  L  47: detect_and_parse(self, text: str, tools: List[Tool])
         ‚Üí StreamingParseResult
         üìù One-time parsing: Detects and parses tool calls in the provided text.

  L 125: structure_info(self)
         ‚Üí _GetInfoFunc

  L 132: build_ebnf(self, tools: List[Tool])


============================================================
FILE: python/sglang/srt/function_call/pythonic_detector.py
Functions: 7
============================================================


CLASS: PythonicDetector
----------------------------------------
  L  34: __init__(self)

  L  49: has_tool_call(self, text: str)
         ‚Üí bool

  L  52: detect_and_parse(self, text: str, tools: List[Tool])
         ‚Üí StreamingParseResult

  L 157: parse_streaming_increment(self, new_text: str, tools: List[Tool])
         ‚Üí StreamingParseResult
         üìù Streaming incremental parsing for pythonic tool calls.

  L 218: supports_structural_tag(self)
         ‚Üí bool

  L 221: structure_info(self)
         ‚Üí _GetInfoFunc

  L 224: build_ebnf(self, tools: List[Tool])
         ‚Üí Optional[str]


============================================================
FILE: python/sglang/srt/function_call/qwen25_detector.py
Functions: 6
============================================================


CLASS: Qwen25Detector
----------------------------------------
  L  34: __init__(self)
         üìù Initializes the detector with necessary state variables.

  L  44: has_tool_call(self, text: str)
         ‚Üí bool
         üìù Check if the text contains a Qwen 2.5 format tool call.

  L  48: detect_and_parse(self, text: str, tools: List[Tool])
         ‚Üí StreamingParseResult
         üìù One-time parsing: Detects and parses tool calls in the provided text.

  L  76: parse_streaming_increment(self, new_text: str, tools: List[Tool])
         ‚Üí StreamingParseResult
         üìù Streaming incremental parsing for Qwen 2.5 tool calls.

  L 116: structure_info(self)
         ‚Üí _GetInfoFunc

  L 123: build_ebnf(self, tools: List[Tool])


============================================================
FILE: python/sglang/srt/function_call/qwen3_coder_detector.py
Functions: 7
============================================================


CLASS: Qwen3CoderDetector
----------------------------------------
  L  44: __init__(self)

  L  69: has_tool_call(self, text: str)
         ‚Üí bool

  L  72: detect_and_parse(self, text: str, tools: List[Tool])
         ‚Üí StreamingParseResult

  L  76: parse_streaming_increment(self, new_text: str, tools: List[Tool])
         ‚Üí StreamingParseResult

  L 346: supports_structural_tag(self)
         ‚Üí bool

  L 349: structure_info(self)
         ‚Üí _GetInfoFunc

  L 352: build_ebnf(self, tools: List[Tool])


============================================================
FILE: python/sglang/srt/function_call/step3_detector.py
Functions: 9
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  19: def get_argument_type(func_name: str, arg_key: str, defined_tools: List[Tool])
         ‚Üí str
         üìù Get the expected type for a function argument from tool schema.

  L  32: def parse_arguments(value: str)
         ‚Üí tuple[Any, bool]
         üìù Parse a string value to appropriate type. Returns (parsed_value, succe


CLASS: Step3Detector
----------------------------------------
  L  62: __init__(self)

  L  86: has_tool_call(self, text: str)
         ‚Üí bool
         üìù Check if the text contains a Step3 format tool call.

  L 121: detect_and_parse(self, text: str, tools: List[Tool])
         ‚Üí StreamingParseResult
         üìù One-time parsing: Detects and parses tool calls in the provided text.

  L 170: parse_streaming_increment(self, new_text: str, tools: List[Tool])
         ‚Üí StreamingParseResult
         üìù Streaming incremental parsing for Step3 format.

  L 403: supports_structural_tag(self)
         ‚Üí bool
         üìù Return True if this detector supports structural tag format.

  L 407: structure_info(self)
         ‚Üí _GetInfoFunc

  L 410: build_ebnf(self, tools: List[Tool])
         ‚Üí str
         üìù Build EBNF grammar for Step3 tool call format.


============================================================
FILE: python/sglang/srt/harmony_parser.py
Functions: 9
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  24: def prefix_hold(text: str, tokens: List[str])
         ‚Üí Tuple[str, str]
         üìù Holds back the longest suffix of `text` that could be a prefix of any 

  L  46: def iter_tokens(text: str, start_pos: int)
         ‚Üí Iterator[Token]
         üìù Iterate over structural tokens in left-to-right order.


CLASS: CanonicalStrategy
----------------------------------------
  L 126: __init__(self)

  L 137: parse(self, text: str)
         ‚Üí Tuple[List[Event], str]


CLASS: HarmonyParser
----------------------------------------
  L 504: __init__(self)

  L 514: parse(self, chunk: str)
         ‚Üí List[Event]


CLASS: TextStrategy
----------------------------------------
  L 422: __init__(self)

  L 438: set_buffer_context(self, buffer: str)

  L 441: parse(self, text: str)
         ‚Üí Tuple[List[Event], str]


============================================================
FILE: python/sglang/srt/hf_transformers_utils.py
Functions: 11
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  66: def download_from_hf(model_path: str,
        allow_patterns: Optional[Union[str,
        list]])

  L  79: def get_hf_text_config(config: PretrainedConfig)
         üìù Get the "sub" config relevant to llm for multi modal models.

  L 117: def get_config(model: str,
        trust_remote_code: bool,
        revision: Optional[str],
        model_override_args: Optional[dict])
         @lru_cache_frozenset(maxsize=32)

  L 186: def get_generation_config(model: str,
        trust_remote_code: bool,
        revision: Optional[str])
         @lru_cache_frozenset(maxsize=32)

  L 201: def get_sparse_attention_config(model: str,
        sparse_attention_config_filename: str)
         ‚Üí Dict[str, Any]

  L 233: def get_context_length(config)
         üìù Get the context length of a model from a huggingface model configs.

  L 257: def get_tokenizer(tokenizer_name: str)
         ‚Üí Union[PreTrainedTokenizer, PreTrainedTokenizerFast]
         üìù Gets a tokenizer for the given model name via Huggingface.

  L 338: def get_tokenizer_from_processor(processor)

  L 344: def get_processor(tokenizer_name: str)

  L 410: def attach_additional_stop_token_ids(tokenizer)

  L 420: def check_gguf_file(model: Union[str, os.PathLike])
         ‚Üí bool
         üìù Check if the file is a GGUF model.


============================================================
FILE: python/sglang/srt/host_shared_memory.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  75: def get_host_shared_memory_manager()

  L  80: def set_host_shared_memory_manager(instance: HostSharedMemoryManager)


CLASS: HostSharedMemoryManager
----------------------------------------
  L  18: __init__(self, base_name: str)

  L  23: malloc(self)


============================================================
FILE: python/sglang/srt/jinja_template_utils.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  81: def detect_jinja_template_content_format(chat_template: str)
         ‚Üí str
         üìù Detect whether a chat template expects 'string' or 'openai' content fo

  L 117: def process_content_for_template_format(msg_dict: dict,
        content_format: str,
        image_data: list,
        video_data: list,
        audio_data: list,
        modalities: list)
         ‚Üí dict
         üìù Process message content based on detected template format.


============================================================
FILE: python/sglang/srt/layers/activation.py
Functions: 18
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 191: def get_act_fn(act_fn_name: str,
        quant_config: Optional[QuantizationConfig],
        intermediate_size: Optional[int],
        input_is_parallel: bool,
        params_dtype: Optional[torch.dtype])
         ‚Üí nn.Module
         üìù Get an activation function by name.

  L 216: def get_cross_encoder_activation_function(config: PretrainedConfig)


CLASS: GeluAndMul
----------------------------------------
  L  86: __init__(self, approximate)

  L  90: forward_native(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  94: forward_cuda(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: NewGELU
----------------------------------------
  L 108: forward_native(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L 112: forward_cuda(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: QuickGELU
----------------------------------------
  L 129: forward_native(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L 132: forward_cuda(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L 135: forward_hip(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: ReLU2
----------------------------------------
  L 123: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: ScaledActivation
----------------------------------------
  L 147: __init__(self, act_module: nn.Module, intermediate_size: int, input_is_parallel: bool, params_dtype: Optional[torch.dtype])

  L 169: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L 172: weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor)


CLASS: SiluAndMul
----------------------------------------
  L  60: forward_native(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  64: forward_cuda(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  71: forward_cpu(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  80: forward_npu(self, x: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/amx_utils.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  10: def amx_process_weight_after_loading(weight)

  L  22: def dim_is_supported(weight)


CLASS: PackWeightMethod
----------------------------------------
  L  79: __init__(self, weight_names, transpose_dims)

  L  83: process_weights_after_loading(self, module)
         ‚Üí None


============================================================
FILE: python/sglang/srt/layers/attention/aiter_backend.py
Functions: 20
============================================================


CLASS: AiterAttnBackend
----------------------------------------
  L  65: __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor])

  L 157: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Init auxiliary variables for triton attention backend.

  L 341: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])

  L 364: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])

  L 499: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])

  L 572: get_cuda_graph_seq_len_fill_value(self)

  L 575: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)

  L 761: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)


CLASS: AiterIndicesUpdaterPrefill
----------------------------------------
  L 830: __init__(self, model_runner: ModelRunner, attn_backend: AttentionBackend)

  L 855: update(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, prefix_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], spec_info: Optional[SpecInfo])

  L 867: update_single_wrapper(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, prefix_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], spec_info: Optional[SpecInfo])


CLASS: AiterMlaIndicesUpdaterPrefill
----------------------------------------
  L 935: __init__(self, model_runner: ModelRunner, attn_backend: AttentionBackend)

  L 950: update(self, req_pool_indices: torch.Tensor, kv_lens: torch.Tensor, kv_lens_sum: int, extend_lens: torch.Tensor, max_q_len: int, max_kv_len: int, spec_info: Optional[SpecInfo])

  L 963: update_single_wrapper(self, req_pool_indices: torch.Tensor, kv_lens: torch.Tensor, kv_lens_sum: int, extend_lens: torch.Tensor, max_q_len: int, max_kv_len: int, spec_info: Optional[SpecInfo])


CLASS: AiterMultiStepDraftBackend
----------------------------------------
  L1022: __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)

  L1061: common_template(self, forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: int)

  L1093: init_forward_metadata(self, forward_batch: ForwardBatch)

  L1114: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L1125: init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)

  L1139: init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)


============================================================
FILE: python/sglang/srt/layers/attention/ascend_backend.py
Functions: 9
============================================================


CLASS: AscendAttnBackend
----------------------------------------
  L  40: gen_attention_mask(self, max_seq_len: int, dtype)

  L  58: __init__(self, model_runner: ModelRunner)

  L  84: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Init the metadata for a forward pass.

  L 105: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L 114: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 134: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])

  L 160: get_cuda_graph_seq_len_fill_value(self)

  L 163: forward_extend(self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)

  L 256: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor])


============================================================
FILE: python/sglang/srt/layers/attention/base_attn_backend.py
Functions: 9
============================================================


CLASS: AttentionBackend
----------------------------------------
  L  18: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Init the metadata for a forward pass.

  L  22: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)
         üìù Init the global shared states for cuda graph.

  L  26: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
         üìù Init the metadata for a forward pass for capturing a cuda graph.

  L  39: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])
         üìù Init the metadata for a forward pass for replaying a cuda graph.

  L  53: get_cuda_graph_seq_len_fill_value(self)
         üìù Get the fill value for padded seq lens. Typically, it is 0 or 1.

  L  57: forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)
         üìù Run forward on an attention layer.

  L  91: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)
         üìù Run a forward for decode.

  L 103: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)
         üìù Run a forward for extend.

  L 115: support_triton(self)
         üìù Check if the current backend supports triton.


============================================================
FILE: python/sglang/srt/layers/attention/cutlass_mla_backend.py
Functions: 8
============================================================


CLASS: CutlassMLABackend
----------------------------------------
  L  51: __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], kv_last_page_len_buf: Optional[torch.Tensor])

  L  82: init_forward_metadata(self, forward_batch: ForwardBatch)

  L 122: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, block_kv_indices: Optional[torch.Tensor])

  L 146: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])

  L 185: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])

  L 223: get_cuda_graph_seq_len_fill_value(self)

  L 226: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor])


CLASS: CutlassMLADecodeMetadata
----------------------------------------
  L  39: __init__(self, workspace: Optional[torch.Tensor], block_kv_indices: Optional[torch.Tensor])


============================================================
FILE: python/sglang/srt/layers/attention/double_sparsity_backend.py
Functions: 4
============================================================


CLASS: DoubleSparseAttnBackend
----------------------------------------
  L  17: __init__(self, model_runner: ModelRunner)

  L  52: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Init auxiliary variables for triton attention backend.

  L 113: forward_extend(self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)

  L 167: forward_decode(self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)


============================================================
FILE: python/sglang/srt/layers/attention/dual_chunk_flashattention_backend.py
Functions: 9
============================================================


CLASS: DualChunkFlashAttentionBackend
----------------------------------------
  L 102: __init__(self, model_runner: 'ModelRunner')
         ‚Üí None

  L 160: get_sparse_attention_config(self, layer_idx)
         ‚Üí List[Dict[str, Any]]

  L 168: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Initialize forward metadata hence all layers in the forward pass can r

  L 296: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: 'RadixAttention', forward_batch: ForwardBatch, save_kv_cache)

  L 409: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: 'RadixAttention', forward_batch: ForwardBatch, save_kv_cache)
         ‚Üí torch.Tensor

  L 486: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)
         üìù Initialize CUDA graph state for the attention backend.

  L 532: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[None])

  L 580: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[None], seq_lens_cpu: Optional[torch.Tensor], out_cache_loc: torch.Tensor)
         üìù Initialize forward metadata for replaying CUDA graph.

  L 670: get_cuda_graph_seq_len_fill_value(self)
         üìù Get the fill value for sequence length in CUDA graph.


============================================================
FILE: python/sglang/srt/layers/attention/flashattention_backend.py
Functions: 18
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 127: def make_local_attention_virtual_batches(attn_chunk_size: int,
        query_start_loc_np: np.ndarray,
        seq_lens_np: np.ndarray,
        block_table: torch.Tensor,
        page_size: int)
         ‚Üí tuple[np.ndarray, np.ndarray, np.ndarray, torch.Tensor]
         üìù Take in `query_start_loc_np` and `seq_lens_np` and break the sequences

  L 272: def cdiv(a: int, b: int)
         ‚Üí int
         üìù Ceiling division.

  L 279: def merge_state_v2_wrapper(o, s_a, o_exp, s_b)
         @torch._dynamo.disable()

  L2237: def prepare_swa_spec_page_table_triton(page_table_dst: torch.Tensor,
        page_table_a: torch.Tensor,
        page_table_b: torch.Tensor,
        seq_len_a: torch.Tensor,
        seq_len_b: torch.Tensor,
        speculative_num_draft_tokens: int)

  L2347: def normal_decode_set_metadata(cache_seqlens_int32: torch.Tensor,
        cu_seqlens_k: torch.Tensor,
        page_table: torch.Tensor,
        req_to_token: torch.Tensor,
        req_pool_indices: torch.Tensor,
        strided_indices: torch.Tensor,
        max_seq_pages: torch.Tensor,
        seq_lens: torch.Tensor,
        seq_len_delta: int,
        page_size: int)


CLASS: FlashAttentionBackend
----------------------------------------
  L 301: __init__(self, model_runner: ModelRunner, skip_prefill: bool, speculative_step_id, topk, speculative_num_steps)

  L 355: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Initialize forward metadata hence all layers in the forward pass can r

  L 639: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor], sinks: Optional[torch.Tensor])

  L 929: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor], sinks: Optional[torch.Tensor])
         ‚Üí torch.Tensor

  L1188: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)
         üìù Initialize CUDA graph state for the attention backend.

  L1448: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
         üìù Initialize forward metadata for capturing CUDA graph.

  L1683: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor], out_cache_loc: Optional[torch.Tensor])
         üìù Initialize forward metadata for replaying CUDA graph.

  L1939: get_cuda_graph_seq_len_fill_value(self)
         üìù Get the fill value for sequence length in CUDA graph.


CLASS: FlashAttentionMultiStepBackend
----------------------------------------
  L2279: __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)

  L2296: init_forward_metadata(self, forward_batch: ForwardBatch)

  L2300: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L2304: init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)

  L2322: init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)


============================================================
FILE: python/sglang/srt/layers/attention/flashinfer_backend.py
Functions: 28
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L1230: def should_use_tensor_core(kv_cache_dtype: torch.dtype,
        num_attention_heads: int,
        num_kv_heads: int)
         ‚Üí bool
         üìù Determine whether to use tensor cores for attention computation.

  L1284: def fast_decode_plan(self,
        indptr: torch.Tensor,
        indices: torch.Tensor,
        last_page_len: torch.Tensor,
        num_qo_heads: int,
        num_kv_heads: int,
        head_dim: int,
        page_size: int,
        pos_encoding_mode: str,
        window_left: int,
        logits_soft_cap: Optional[float],
        q_data_type: Optional[Union[str,
        torch.dtype]],
        kv_data_type: Optional[Union[str,
        torch.dtype]],
        data_type: Optional[Union[str,
        torch.dtype]],
        sm_scale: Optional[float],
        rope_scale: Optional[float],
        rope_theta: Optional[float],
        non_blocking: bool)
         ‚Üí None
         üìù A faster version of BatchDecodeWithPagedKVCacheWrapper::plan used for 


CLASS: FlashInferAttnBackend
----------------------------------------
  L  80: __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], kv_last_page_len_buf: Optional[torch.Tensor])

  L 211: init_forward_metadata(self, forward_batch: ForwardBatch)

  L 278: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])

  L 312: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 417: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])

  L 465: get_cuda_graph_seq_len_fill_value(self)

  L 468: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)

  L 552: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)


CLASS: FlashInferIndicesUpdaterDecode
----------------------------------------
  L 602: __init__(self, model_runner: ModelRunner, attn_backend: FlashInferAttnBackend)

  L 631: update(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 644: update_single_wrapper(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 666: update_sliding_window(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 714: update_cross_attention(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 746: call_begin_forward(self, wrapper: BatchDecodeWithPagedKVCacheWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, kv_indptr: torch.Tensor, kv_start_idx: torch.Tensor, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor], use_sliding_window_kv_pool: bool)


CLASS: FlashInferIndicesUpdaterPrefill
----------------------------------------
  L 818: __init__(self, model_runner: ModelRunner, attn_backend: FlashInferAttnBackend)

  L 849: update(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 864: update_single_wrapper(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 900: update_sliding_window(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 946: update_cross_attention(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 985: call_begin_forward(self, wrapper_ragged: BatchPrefillWithRaggedKVCacheWrapper, wrapper_paged: BatchPrefillWithPagedKVCacheWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, seq_lens: torch.Tensor, prefix_lens: torch.Tensor, kv_start_idx: torch.Tensor, kv_indptr: torch.Tensor, qo_indptr: torch.Tensor, use_ragged: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], use_sliding_window_kv_pool: bool)


CLASS: FlashInferMultiStepDraftBackend
----------------------------------------
  L1079: __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)

  L1120: common_template(self, forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: Callable)

  L1165: init_forward_metadata(self, forward_batch: ForwardBatch)

  L1186: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L1198: init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)

  L1212: init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)


============================================================
FILE: python/sglang/srt/layers/attention/flashinfer_mla_backend.py
Functions: 26
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L1038: def fast_mla_decode_plan(self,
        qo_indptr_cpu: torch.Tensor,
        kv_indptr_cpu: torch.Tensor,
        kv_indices: torch.Tensor,
        kv_len_arr_cpu: torch.Tensor,
        num_heads: int,
        head_dim_ckv: int,
        head_dim_kpe: int,
        page_size: int,
        causal: bool,
        sm_scale: float,
        q_data_type: torch.dtype,
        kv_data_type: torch.dtype)
         ‚Üí None
         üìù A faster version of BatchMLAPagedAttentionWrapper::plan,


CLASS: FlashInferMLAAttnBackend
----------------------------------------
  L 179: __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], q_indptr_decode_buf: Optional[torch.Tensor])

  L 271: init_forward_metadata(self, forward_batch: ForwardBatch)

  L 323: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])

  L 354: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])

  L 434: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])

  L 491: get_cuda_graph_seq_len_fill_value(self)

  L 494: init_mha_chunk_metadata(self, forward_batch: ForwardBatch)
         üìù Init the metadata for a forward pass.

  L 498: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor])

  L 576: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor])


CLASS: FlashInferMLAIndicesUpdaterDecode
----------------------------------------
  L 638: __init__(self, model_runner: ModelRunner, attn_backend: AttentionBackend)

  L 655: update(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, decode_wrapper: BatchMLAPagedAttentionWrapper, init_metadata_replay: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 678: call_begin_forward(self, wrapper: BatchMLAPagedAttentionWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, q_indptr: torch.Tensor, kv_indptr: torch.Tensor, init_metadata_replay: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])


CLASS: FlashInferMLAIndicesUpdaterPrefill
----------------------------------------
  L 747: __init__(self, model_runner: ModelRunner, attn_backend: AttentionBackend)

  L 767: update(self, req_pool_indices: torch.Tnesor, seq_lens: torch.Tensor, seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrapper_paged: BatchMLAPagedAttentionWrapper, use_ragged: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 798: call_begin_forward(self, wrapper_ragged: BatchPrefillWithRaggedKVCacheWrapper, wrapper_paged: BatchMLAPagedAttentionWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, seq_lens: torch.Tensor, prefix_lens: torch.Tensor, kv_indptr: torch.Tensor, qo_indptr: torch.Tensor, use_ragged: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])


CLASS: FlashInferMLAMultiStepDraftBackend
----------------------------------------
  L 887: __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)

  L 933: common_template(self, forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: Callable)

  L 971: init_forward_metadata(self, forward_batch: ForwardBatch)

  L 994: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L1006: init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)

  L1020: init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)


CLASS: FlashInferMhaChunkKVRunner
----------------------------------------
  L  68: __init__(self, model_runner: ModelRunner, attn_backend: 'FlashInferMlaAttnBackend')

  L  89: update_prefix_chunks(self, num_prefix_chunks: int)

  L  96: update_wrapper(self, forward_batch: ForwardBatch)

  L 142: forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch)


============================================================
FILE: python/sglang/srt/layers/attention/flashmla_backend.py
Functions: 15
============================================================


CLASS: FlashMLABackend
----------------------------------------
  L  51: __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], kv_last_page_len_buf: Optional[torch.Tensor])

  L  81: init_forward_metadata(self, forward_batch: ForwardBatch)

  L 148: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, block_kv_indices: Optional[torch.Tensor])

  L 182: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])

  L 252: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])

  L 327: get_cuda_graph_seq_len_fill_value(self)

  L 330: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)

  L 387: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)


CLASS: FlashMLADecodeMetadata
----------------------------------------
  L  37: __init__(self, flashmla_metadata: Optional[Tuple[torch.Tensor, torch.Tensor]], num_splits: Optional[torch.Tensor], block_kv_indices: Optional[torch.Tensor])


CLASS: FlashMLAMultiStepDraftBackend
----------------------------------------
  L 456: __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)

  L 489: common_template(self, forward_batch: ForwardBatch, call_fn: Callable)

  L 499: init_forward_metadata(self, forward_batch: ForwardBatch)

  L 506: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L 512: init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)

  L 526: init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)


============================================================
FILE: python/sglang/srt/layers/attention/hybrid_attn_backend.py
Functions: 8
============================================================


CLASS: HybridAttnBackend
----------------------------------------
  L  15: __init__(self, model_runner: ModelRunner, prefill_backend: AttentionBackend, decode_backend: AttentionBackend)

  L  25: init_forward_metadata(self, forward_batch: ForwardBatch)

  L  31: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L  38: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L  69: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])

  L 103: get_cuda_graph_seq_len_fill_value(self)

  L 106: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)

  L 120: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)


============================================================
FILE: python/sglang/srt/layers/attention/intel_amx_backend.py
Functions: 5
============================================================


CLASS: IntelAMXAttnBackend
----------------------------------------
  L  16: __init__(self, model_runner: ModelRunner)

  L  32: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Init the metadata for a forward pass.

  L  52: forward_extend(self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)

  L  91: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)

  L 127: support_triton(self)


============================================================
FILE: python/sglang/srt/layers/attention/merge_state.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  26: def merge_state(prefix_output: torch.Tensor,
        prefix_lse: torch.Tensor,
        suffix_output: torch.Tensor,
        suffix_lse: torch.Tensor,
        output: Optional[torch.Tensor],
        output_lse: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, Optional[torch.Tensor]]


============================================================
FILE: python/sglang/srt/layers/attention/tbo_backend.py
Functions: 9
============================================================


CLASS: TboAttnBackend
----------------------------------------
  L  14: __init__(self, primary: AttentionBackend, children: List[AttentionBackend])

  L  20: init_new(cls, creator: Callable[[], AttentionBackend])

  L  26: init_forward_metadata(self, forward_batch: 'ForwardBatch')

  L  35: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L  41: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: 'ForwardMode', spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L  72: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: 'ForwardMode', spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])

  L 176: get_cuda_graph_seq_len_fill_value(self)

  L 182: forward_extend(self)

  L 185: forward_decode(self)


============================================================
FILE: python/sglang/srt/layers/attention/torch_native_backend.py
Functions: 5
============================================================


CLASS: TorchNativeAttnBackend
----------------------------------------
  L  18: __init__(self, model_runner: ModelRunner)

  L  23: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Init the metadata for a forward pass.

  L 182: forward_extend(self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)

  L 226: forward_decode(self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)

  L 269: support_triton(self)


============================================================
FILE: python/sglang/srt/layers/attention/triton_backend.py
Functions: 19
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  23: def logit_capping_mod(logit_capping_method, logit_cap)

  L 965: def get_num_kv_splits_triton(num_kv_splits_ptr,
        seq_lens_ptr,
        num_seq,
        num_group,
        num_head,
        num_kv_head,
        max_kv_splits,
        device_core_count,
        MAX_NUM_SEQ: tl.constexpr)
         @triton.jit

  L1016: def update_sliding_window_buffer(window_kv_indptr,
        req_to_token,
        sliding_window_size,
        seq_lens,
        req_pool_indices,
        bs,
        device,
        token_to_kv_pool_allocator)

  L1056: def update_sliding_window_buffer_cuda_graph(window_kv_indptr,
        window_kv_indices,
        req_to_token,
        sliding_window_size,
        seq_lens,
        req_pool_indices,
        bs,
        token_to_kv_pool_allocator)


CLASS: TritonAttnBackend
----------------------------------------
  L  50: __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor])

  L 131: get_num_kv_splits(self, num_kv_splits: torch.Tensor, seq_lens: torch.Tensor)

  L 167: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Init auxiliary variables for triton attention backend.

  L 369: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])

  L 427: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 583: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])

  L 705: get_cuda_graph_seq_len_fill_value(self)

  L 708: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache, sinks)

  L 771: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache, sinks)


CLASS: TritonMultiStepDraftBackend
----------------------------------------
  L 830: __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)

  L 868: common_template(self, forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: int)

  L 900: init_forward_metadata(self, forward_batch: ForwardBatch)

  L 921: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L 932: init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)

  L 946: init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)


============================================================
FILE: python/sglang/srt/layers/attention/triton_ops/decode_attention.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  39: def tanh(x)
         @triton.jit

  L 633: def decode_attention_fwd_normal(q,
        k_buffer,
        v_buffer,
        o,
        kv_indptr,
        kv_indices,
        attn_logits,
        attn_lse,
        num_kv_splits,
        max_kv_splits,
        sm_scale,
        logit_cap,
        sinks,
        xai_temperature_len)

  L 676: def decode_attention_fwd_grouped(q,
        k_buffer,
        v_buffer,
        o,
        kv_indptr,
        kv_indices,
        attn_logits,
        attn_lse,
        num_kv_splits,
        max_kv_splits,
        sm_scale,
        logit_cap,
        sinks,
        xai_temperature_len)

  L 719: def decode_attention_fwd(q,
        k_buffer,
        v_buffer,
        o,
        kv_indptr,
        kv_indices,
        attn_logits,
        attn_lse,
        num_kv_splits,
        max_kv_splits,
        sm_scale,
        logit_cap,
        sinks,
        xai_temperature_len)


============================================================
FILE: python/sglang/srt/layers/attention/triton_ops/double_sparsity_attention.py
Functions: 9
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  23: def tanh(x)
         @triton.jit

  L 192: def flash_decode_stage1(q,
        k,
        v,
        Req_to_tokens,
        B_req_idx,
        B_Seqlen,
        max_len_in_batch,
        mid_out,
        mid_out_logsumexp,
        block_seq)
         @torch.no_grad()

  L 255: def flash_decode_stage2(mid_out, mid_out_logexpsum, B_Seqlen, O, block_seq)
         @torch.no_grad()

  L 284: def flash_decode_attention_fwd(q,
        k_buffer,
        v_buffer,
        o,
        req_to_token,
        b_req_idx,
        b_start_loc,
        b_seq_len,
        attn_logits,
        max_len_in_batch,
        sm_scale,
        logit_cap)

  L 561: def sparse_flash_decode_stage1(q_label,
        k_label_buffer,
        att_out,
        Req_to_tokens,
        B_Seqlen,
        max_len_in_batch,
        sm_scale,
        logit_cap)

  L 613: def sparse_flash_decode_stage2(q,
        k,
        v,
        Req_to_tokens,
        Topk_token_indices,
        heavy_token_num,
        mid_out,
        mid_out_logsumexp,
        block_seq,
        sm_scale)
         @torch.no_grad()

  L 674: def sparse_flash_decode_stage3(Seqlen, mid_out, mid_out_logexpsum, O, block_seq)
         @torch.no_grad()

  L 700: def flash_decode_sparse_attention_fwd(q,
        k_buffer,
        v_buffer,
        o,
        q_label,
        k_label_buffer,
        req_to_token,
        b_seq_len,
        max_len_in_batch,
        sm_scale,
        logit_cap,
        heavy_token_num,
        att_out_approx,
        mid_out,
        mid_o_logexpsum,
        BLOCK_SEQ)

  L 994: def extend_attention_fwd(q_extend,
        k_extend,
        v_extend,
        o_extend,
        k_buffer,
        v_buffer,
        req_to_tokens,
        b_req_idx,
        b_seq_len,
        b_seq_len_extend,
        b_start_loc_extend,
        max_len_extend,
        sm_scale,
        logit_cap)
         üìù q_extend, k_extend, v_extend, o_extend: contiguous tensors


============================================================
FILE: python/sglang/srt/layers/attention/triton_ops/extend_attention.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  36: def tanh(x)
         @triton.jit

  L 372: def extend_attention_fwd(q_extend,
        k_extend,
        v_extend,
        o_extend,
        k_buffer,
        v_buffer,
        qo_indptr,
        kv_indptr,
        kv_indices,
        custom_mask,
        is_causal,
        mask_indptr,
        max_len_extend,
        sm_scale,
        logit_cap,
        skip_prefix_custom_mask,
        sliding_window_size,
        sinks,
        window_kv_offsets,
        xai_temperature_len)
         üìù q_extend, k_extend, v_extend, o_extend: contiguous tensors

  L 516: def redundant_attention(q_extend,
        o_extend,
        k_buffer,
        v_buffer,
        b_req_idx,
        b_start_loc,
        b_seq_len,
        b_seq_len_prefix,
        max_len_in_batch)


============================================================
FILE: python/sglang/srt/layers/attention/triton_ops/merge_state.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L   9: def merge_state_kernel(output,
        output_lse,
        prefix_output,
        prefix_lse,
        suffix_output,
        suffix_lse,
        HEAD_SIZE: tl.constexpr,
        PADDED_HEAD_SIZE: tl.constexpr,
        OUTPUT_LSE: tl.constexpr)
         @triton.jit

  L  66: def merge_state_triton(prefix_output: torch.Tensor,
        prefix_lse: torch.Tensor,
        suffix_output: torch.Tensor,
        suffix_lse: torch.Tensor,
        output: Optional[torch.Tensor],
        output_lse: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, Optional[torch.Tensor]]


============================================================
FILE: python/sglang/srt/layers/attention/triton_ops/prefill_attention.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 170: def context_attention_fwd(q,
        k,
        v,
        o,
        b_start_loc,
        b_seq_len,
        max_input_len,
        is_causal)
         üìù q, k, v: [b * s, head, head_dim]


============================================================
FILE: python/sglang/srt/layers/attention/triton_ops/rocm_mla_decode_rope.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  31: def is_hip()

  L  39: def tanh(x)
         @triton.jit

  L 402: def decode_attention_fwd_grouped_rope(q,
        k_buffer,
        v_buffer,
        o,
        kv_indptr,
        kv_indices,
        k_pe_tokens,
        kv_lora_rank,
        rotary_dim,
        cos_sin_cache,
        positions,
        attn_logits,
        num_kv_splits,
        sm_scale,
        logit_cap,
        use_rope,
        is_neox_style)


============================================================
FILE: python/sglang/srt/layers/attention/trtllm_mha_backend.py
Functions: 13
============================================================


CLASS: TRTLLMHAAttnBackend
----------------------------------------
  L  58: __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], kv_last_page_len_buf: Optional[torch.Tensor], speculative_step_id: int)

  L 111: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])
         üìù Initialize CUDA graph state for TRTLLM MHA.

  L 196: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])
         üìù Initialize metadata for CUDA graph capture.

  L 309: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])
         üìù Replay CUDA graph with new inputs.

  L 411: get_cuda_graph_seq_len_fill_value(self)
         ‚Üí int
         üìù Get the fill value for sequence lengths in CUDA graph.

  L 415: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Initialize the metadata for a forward pass.

  L 517: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)
         ‚Üí torch.Tensor
         üìù Run forward for decode using TRTLLM MHA kernel.

  L 576: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)


CLASS: TRTLLMHAAttnMultiStepDraftBackend
----------------------------------------
  L 638: __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)

  L 651: init_forward_metadata(self, forward_batch: ForwardBatch)

  L 655: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)

  L 659: init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)

  L 677: init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)


============================================================
FILE: python/sglang/srt/layers/attention/trtllm_mla_backend.py
Functions: 9
============================================================


CLASS: TRTLLMMLABackend
----------------------------------------
  L  59: __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], q_indptr_decode_buf: Optional[torch.Tensor])

  L 166: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])
         üìù Initialize CUDA graph state for TRTLLM MLA.

  L 185: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])
         üìù Initialize metadata for CUDA graph capture.

  L 231: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])
         üìù Replay CUDA graph with new inputs.

  L 271: get_cuda_graph_seq_len_fill_value(self)
         ‚Üí int
         üìù Get the fill value for sequence lengths in CUDA graph.

  L 275: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Initialize the metadata for a forward pass.

  L 303: quantize_and_rope_for_fp8(self, q_nope: torch.Tensor, q_rope: torch.Tensor, k_nope: torch.Tensor, k_rope: torch.Tensor, forward_batch: ForwardBatch, cos_sin_cache: torch.Tensor, is_neox: bool)
         ‚Üí tuple[torch.Tensor, torch.Tensor, torch.Tensor]
         üìù Quantize and apply RoPE for FP8 attention path.

  L 381: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor], cos_sin_cache: Optional[torch.Tensor], is_neox: Optional[bool])
         ‚Üí torch.Tensor
         üìù Run forward for decode using TRTLLM MLA kernel.


CLASS: TRTLLMMLAMultiStepDraftBackend
----------------------------------------
  L 488: __init__(self, model_runner: 'ModelRunner', topk: int, speculative_num_steps: int)


============================================================
FILE: python/sglang/srt/layers/attention/utils.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  11: def create_flashinfer_kv_indices_triton(req_to_token_ptr,
        req_pool_indices_ptr,
        page_kernel_lens_ptr,
        kv_indptr,
        kv_start_idx,
        kv_indices_ptr,
        req_to_token_ptr_stride: tl.constexpr)
         @triton.jit

  L  50: def create_flashmla_kv_indices_triton(req_to_token_ptr,
        req_pool_indices_ptr,
        page_kernel_lens_ptr,
        kv_start_idx,
        kv_indices_ptr,
        req_to_token_ptr_stride: tl.constexpr,
        kv_indices_ptr_stride: tl.constexpr,
        NUM_PAGE_PER_BLOCK: tl.constexpr,
        PAGED_SIZE: tl.constexpr)
         @triton.jit


============================================================
FILE: python/sglang/srt/layers/attention/vision.py
Functions: 12
============================================================


CLASS: SingletonCache
----------------------------------------
  L  55: set_data(self, value: Any)
         ‚Üí None

  L  58: get_data(self)
         ‚Üí Optional[Any]

  L  61: empty(self)
         ‚Üí bool


CLASS: VisionAttention
----------------------------------------
  L 354: __init__(self, embed_dim: int, num_heads: int, projection_size: int, use_qkv_parallel: bool, qkv_backend: Optional[str], quant_config: Optional[QuantizationConfig], dropout: float, softmax_in_single_precision: bool, flatten_batch: bool, prefix: str, proj_bias: bool, num_dummy_heads: int, qkv_bias: bool, qk_normalization: bool, layer_norm_eps: float, customized_position_embedding_applier: Callable[[torch.Tensor, torch.Tensor, Any, Any], Tuple[torch.Tensor, torch.Tensor]])

  L 509: forward(self, x: torch.Tensor, cu_seqlens: Optional[torch.Tensor], position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]], attention_mask: Optional[torch.Tensor])
         ‚Üí torch.Tensor
         üìù Args:


CLASS: VisionFlash3Attention
----------------------------------------
  L 284: __init__(self)

  L 292: forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens: Optional[Union[SingletonCache, torch.Tensor]], bsz: int, seq_len: int)
         ‚Üí torch.Tensor
         üìù Args:


CLASS: VisionSdpaAttention
----------------------------------------
  L  88: __init__(self, head_dim: int, num_heads: int, num_kv_heads: int, dropout: float, flatten_batch: bool, softmax_in_single_precision: bool)

  L 141: generate_patch_attention_mask(self, s: int, cu_seqlens: Optional[torch.Tensor], flatten_batch: bool)
         ‚Üí Optional[torch.Tensor]
         üìù Creates a non-causal 4D mask of shape `(b, 1, s, s)` or `(1, 1, s, s)`

  L 163: forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, bsz: int, cu_seqlens: Optional[torch.Tensor], attention_mask: Optional[torch.Tensor])
         ‚Üí torch.Tensor
         üìù Args:


CLASS: VisionTritonAttention
----------------------------------------
  L 240: __init__(self)

  L 246: forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens: Optional[torch.Tensor], bsz: int, seq_len: int)
         ‚Üí torch.Tensor
         üìù Args:


============================================================
FILE: python/sglang/srt/layers/attention/vision_utils.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L   8: def update_vit_attn_dummy_heads_config(config)
         üìù Update HF config to ensure vision attention num_attention_heads is div

  L  26: def pad_vit_attn_dummy_heads(config, name: str, loaded_weight: torch.Tensor)
         üìù Pad attention qkv weights for dummy heads


============================================================
FILE: python/sglang/srt/layers/attention/wave_backend.py
Functions: 10
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  26: def get_num_kv_splits_triton(num_kv_splits_ptr,
        seq_lens_ptr,
        num_seq,
        num_group,
        num_head,
        num_kv_head,
        max_kv_splits,
        device_core_count,
        MAX_NUM_SEQ: tl.constexpr)
         @triton.jit


CLASS: WaveAttnBackend
----------------------------------------
  L  91: __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor])

  L 162: get_num_kv_splits(self, num_kv_splits: torch.Tensor, seq_lens: torch.Tensor)

  L 195: init_forward_metadata(self, forward_batch: ForwardBatch)
         üìù Init auxiliary variables for wave attention backend.

  L 344: init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])

  L 388: init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])

  L 472: init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])

  L 540: get_cuda_graph_seq_len_fill_value(self)

  L 543: forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)

  L 589: forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)


============================================================
FILE: python/sglang/srt/layers/attention/wave_ops/decode_attention.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  27: def get_wave_kernel(shape: paged_decode_attention_shape,
        max_kv_splits,
        input_dtype,
        output_dtype,
        logit_cap)
         @functools.lru_cache(maxsize=4096)

  L  92: def decode_attention_intermediate_arrays_shapes(num_seqs,
        head_size_kv,
        num_query_heads,
        max_kv_splits)

  L 107: def decode_attention_wave(q,
        k_buffer,
        v_buffer,
        o,
        b_req_idx,
        req_to_token,
        attn_logits,
        attn_logits_max,
        num_kv_splits,
        max_kv_splits,
        sm_scale,
        logit_cap)

  L 159: def decode_attention_fwd(q,
        k_buffer,
        v_buffer,
        o,
        b_req_idx,
        req_to_token,
        attn_logits,
        attn_logits_max,
        num_kv_splits,
        max_kv_splits,
        sm_scale,
        logit_cap)


============================================================
FILE: python/sglang/srt/layers/attention/wave_ops/extend_attention.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  23: def get_wave_kernel(shape: AttentionShape,
        q_shape: tuple[int],
        k_shape: tuple[int],
        v_shape: tuple[int],
        k_cache_shape: tuple[int],
        v_cache_shape: tuple[int],
        o_shape: tuple[int],
        input_dtype: torch.dtype,
        output_dtype: torch.dtype,
        size_dtype: torch.dtype,
        is_causal: bool,
        logit_cap: float,
        layer_scaling: float)
         @functools.lru_cache

  L  83: def extend_attention_wave(q_extend,
        k_extend,
        v_extend,
        k_buffer,
        v_buffer,
        qo_indptr,
        kv_indptr,
        kv_indices,
        custom_mask,
        mask_indptr,
        max_seq_len,
        output,
        is_causal,
        layer_scaling,
        logit_cap)


============================================================
FILE: python/sglang/srt/layers/attention/wave_ops/prefill_attention.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  22: def prefill_attention_wave(q,
        k,
        v,
        o,
        b_start_loc,
        b_seq_len,
        max_seq_len,
        is_causal)


============================================================
FILE: python/sglang/srt/layers/communicator.py
Functions: 16
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 155: def enable_moe_dense_fully_dp()


CLASS: CommunicateContext
----------------------------------------
  L 315: is_same_group_size(self, a: ScatterMode, b: ScatterMode)

  L 319: init_new(cls)


CLASS: CommunicateSimpleFn
----------------------------------------
  L 341: get_fn(input_mode: ScatterMode, output_mode: ScatterMode, context: CommunicateContext)


CLASS: CommunicateSummableTensorPairFn
----------------------------------------
  L 527: execute(cls, hidden_states_input_mode, residual_input_mode, output_mode, context)

  L 543: get_fn(hidden_states_input_mode: ScatterMode, residual_input_mode: ScatterMode, output_mode: ScatterMode, context: CommunicateContext)


CLASS: CommunicateWithAllReduceAndLayerNormFn
----------------------------------------
  L 388: get_fn(hidden_states_input_mode: ScatterMode, residual_input_mode: ScatterMode, hidden_states_output_mode: ScatterMode, residual_output_mode: ScatterMode, context: CommunicateContext)


CLASS: LayerCommunicator
----------------------------------------
  L 160: __init__(self, layer_scatter_modes: LayerScatterModes, input_layernorm: torch.nn.Module, post_attention_layernorm: torch.nn.Module, allow_reduce_scatter: bool, is_last_layer: bool)

  L 199: prepare_attn(self, hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch)

  L 235: prepare_mlp(self, hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch)

  L 249: postprocess_layer(self, hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch)

  L 263: should_use_reduce_scatter(self, forward_batch: ForwardBatch)

  L 271: should_fuse_mlp_allreduce_with_next_layer(self, forward_batch: ForwardBatch)
         ‚Üí bool


CLASS: LayerScatterModes
----------------------------------------
  L  99: init_new(cls)


CLASS: ScatterMode
----------------------------------------
  L  67: model_input_output()
         üìù The scatter mode for model forward pass input and output data


CLASS: _LayerModeComputationContext
----------------------------------------
  L  79: previous_layer(self)


============================================================
FILE: python/sglang/srt/layers/dp_attention.py
Functions: 40
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 123: def set_dp_buffer_len(global_dp_buffer_len: int,
        local_dp_buffer_len: int,
        global_num_tokens: Optional[List[int]])

  L 133: def get_global_dp_buffer()
         ‚Üí torch.Tensor

  L 137: def get_local_dp_buffer()
         ‚Üí torch.Tensor

  L 141: def get_global_dp_buffer_len()
         ‚Üí int

  L 145: def get_local_dp_buffer_len()
         ‚Üí int

  L 149: def get_dp_global_num_tokens()
         ‚Üí List[int]

  L 153: def compute_dp_attention_world_info(enable_dp_attention,
        tp_rank,
        tp_size,
        dp_size)

  L 164: def compute_dp_attention_local_info(enable_dp_attention,
        tp_rank,
        tp_size,
        dp_size,
        moe_dense_tp_size)

  L 181: def initialize_dp_attention(server_args: ServerArgs, model_config: ModelConfig)

  L 241: def is_dp_attention_enabled()
         ‚Üí bool

  L 245: def get_attention_tp_group()
         ‚Üí GroupCoordinator

  L 250: def get_attention_tp_rank()
         ‚Üí int

  L 255: def get_attention_tp_size()
         ‚Üí int

  L 260: def get_attention_dp_rank()
         ‚Üí int

  L 265: def get_attention_dp_size()
         ‚Üí int

  L 270: def get_local_attention_dp_rank()
         ‚Üí int

  L 275: def get_local_attention_dp_size()
         ‚Üí int

  L 281: def disable_dp_size()
         üìù Patch the tp group temporarily until this function ends.
         @contextmanager

  L 301: def get_dp_local_info(forward_batch: ForwardBatch)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 320: def memcpy_triton_kernel(dst_ptr,
        src_ptr,
        offset_ptr,
        sz_ptr,
        offset_src: tl.constexpr,
        chunk_size,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 345: def prod(x)

  L 349: def memcpy_triton(dst, src, dim, offset, sz, offset_src)

  L 431: def dp_gather_partial(global_tokens: torch.Tensor,
        local_tokens: torch.Tensor,
        forward_batch: ForwardBatch)

  L 439: def dp_gather_replicate(global_tokens: torch.Tensor,
        local_tokens: torch.Tensor,
        forward_batch: ForwardBatch)

  L 447: def dp_scatter(local_tokens: torch.Tensor,
        global_tokens: torch.Tensor,
        forward_batch: ForwardBatch)

  L 469: def dp_reduce_scatter_tensor(output: torch.Tensor, input: torch.Tensor)

  L 480: def attn_tp_reduce_scatter_tensor(output: torch.Tensor, input: torch.Tensor)

  L 484: def attn_tp_all_gather_into_tensor(output: torch.Tensor, input: torch.Tensor)

  L 488: def attn_tp_all_gather(output_list: List[torch.Tensor], input: torch.Tensor)


CLASS: DpPaddingMode
----------------------------------------
  L  47: is_max_len(self)

  L  50: is_sum_len(self)

  L  54: get_dp_padding_mode(cls, global_num_tokens: List[int])
         ‚Üí DpPaddingMode

  L  64: get_default_mode_in_cuda_graph(cls)
         ‚Üí DpPaddingMode


CLASS: _DpGatheredBufferWrapper
----------------------------------------
  L  78: set_metadata(cls, hidden_size: int, dtype: torch.dtype, device: torch.device)

  L  84: set_dp_buffer_len(cls, global_dp_buffer_len: int, local_dp_buffer_len: int, global_num_tokens: Optional[List[int]])

  L  95: get_global_dp_buffer(cls)
         ‚Üí torch.Tensor

  L 103: get_local_dp_buffer(cls)
         ‚Üí torch.Tensor

  L 111: get_global_dp_buffer_len(cls)
         ‚Üí int

  L 115: get_local_dp_buffer_len(cls)
         ‚Üí int

  L 119: get_dp_global_num_tokens(cls)
         ‚Üí List[int]


============================================================
FILE: python/sglang/srt/layers/elementwise.py
Functions: 23
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  37: def fused_softcap_kernel(output_ptr,
        input_ptr,
        n_ele,
        softcap_const: tl.constexpr,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L  61: def fused_softcap(x, softcap_const, autotune)

  L 138: def fused_dual_residual_rmsnorm_kernel(output_ptr,
        mid_ptr,
        activ_ptr,
        residual_ptr,
        weight1_ptr,
        weight2_ptr,
        eps: tl.constexpr,
        hidden_dim: tl.constexpr,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 188: def fused_dual_residual_rmsnorm(x, residual, weight1, weight2, eps, autotune)

  L 222: def fused_rmsnorm_kernel(output_ptr,
        activ_ptr,
        weight_ptr,
        eps: tl.constexpr,
        hidden_dim: tl.constexpr,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 252: def fused_rmsnorm(x, weight, eps, autotune, inplace)

  L 329: def experts_combine_kernel(out_hidden_states,
        moe_hidden_states,
        mlp_hidden_states,
        combine_k: tl.constexpr,
        hidden_dim: tl.constexpr,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 359: def experts_combine_triton(moe_hidden_states, mlp_hidden_states, output_buffer)

  L 399: def gelu_and_mul_kernel(out_hidden_states_ptr,
        out_scales_ptr,
        hidden_states_ptr,
        quant_max: tl.constexpr,
        static_scale: tl.constexpr,
        hidden_dim: tl.constexpr,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 436: def gelu_and_mul_triton(hidden_states, scales, quantize, out)

  L 493: def silu_and_mul_kernel(out_hidden_states_ptr,
        out_scales_ptr,
        hidden_states_ptr,
        quant_max: tl.constexpr,
        static_scale: tl.constexpr,
        hidden_dim: tl.constexpr,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 530: def silu_and_mul_triton(hidden_states, scales, quantize, out)


CLASS: FusedDualResidualRMSNorm
----------------------------------------
  L 279: __init__(self, rmsnorm1, rmsnorm2)
         ‚Üí None

  L 286: __call__(self)

  L 289: forward(self, x: torch.Tensor, residual: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 297: forward_cuda(self, x: torch.Tensor, residual: torch.Tensor, autotune)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 309: forward_flashinfer(self, x: torch.Tensor, residual: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 318: forward_native(self, x: torch.Tensor, residual: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: Softcap
----------------------------------------
  L  76: __init__(self, softcap_const: float)

  L  79: __call__(self)

  L  82: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  88: forward_native(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  91: forward_cuda(self, x: torch.Tensor, autotune)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/flashinfer_comm_fusion.py
Functions: 7
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  98: def ensure_workspace_initialized(max_token_num: int,
        hidden_dim: int,
        use_fp32_lamport: bool)
         üìù Ensure workspace is initialized

  L 126: def flashinfer_allreduce_residual_rmsnorm(input_tensor: torch.Tensor,
        residual: torch.Tensor,
        weight: torch.Tensor,
        eps: float,
        max_token_num: int,
        use_oneshot: Optional[bool],
        trigger_completion_at_end: bool,
        fp32_acc: bool)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù Use FlashInfer's fused allreduce + residual + RMS norm operation

  L 203: def fake_flashinfer_allreduce_residual_rmsnorm(input_tensor: torch.Tensor,
        residual: torch.Tensor,
        weight: torch.Tensor,
        eps: float,
        max_token_num: int,
        use_oneshot: Optional[bool],
        trigger_completion_at_end: bool,
        fp32_acc: bool)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 227: def cleanup_flashinfer_workspace()


CLASS: FlashInferWorkspaceManager
----------------------------------------
  L  32: __init__(self)

  L  39: initialize(self, world_size: int, rank: int, max_token_num: int, hidden_dim: int, group, use_fp32_lamport: bool)
         üìù Initialize workspace

  L  80: cleanup(self)
         üìù Clean up workspace


============================================================
FILE: python/sglang/srt/layers/layernorm.py
Functions: 14
============================================================


CLASS: Gemma3RMSNorm
----------------------------------------
  L 271: __init__(self, dim: int, eps: float)

  L 279: forward(self, x)

  L 286: extra_repr(self)


CLASS: GemmaRMSNorm
----------------------------------------
  L 226: __init__(self, hidden_size: int, eps: float)
         ‚Üí None

  L 239: forward_native(self, x: torch.Tensor, residual: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]

  L 256: forward_cuda(self, x: torch.Tensor, residual: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]


CLASS: RMSNorm
----------------------------------------
  L  61: __init__(self, hidden_size: int, eps: float, var_hidden_size: Optional[int])
         ‚Üí None

  L  77: forward_cuda(self, x: torch.Tensor, residual: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]

  L  90: forward_npu(self, x: torch.Tensor, residual: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]

  L 102: forward_aiter(self, x: torch.Tensor, residual: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]

  L 121: forward_hip(self, x: torch.Tensor, residual: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]

  L 136: forward_native(self, x: torch.Tensor, residual: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]

  L 175: forward_cpu(self, x: torch.Tensor, residual: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]

  L 192: forward_with_allreduce_fusion(self, x: torch.Tensor, residual: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
         üìù Forward method with allreduce fusion, prioritizing flashinfer fused op


============================================================
FILE: python/sglang/srt/layers/linear.py
Functions: 26
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  67: def adjust_marlin_shard(param, shard_size, shard_offset)

  L  75: def adjust_bitsandbytes_4bit_shard(param: Parameter,
        shard_offsets: Dict[str,
        Tuple[int,
        int]],
        loaded_shard_id: str)
         ‚Üí Tuple[int, int]
         üìù Adjust the quantization offsets and sizes for BitsAndBytes sharding.

  L  90: def adjust_scalar_to_fused_array(param, loaded_weight, shard_id)
         üìù For fused modules (QKV and MLP) we have an array of length

  L 113: def adjust_shard_offsets(shard_offsets, loaded_weight, dim)


CLASS: ColumnParallelLinear
----------------------------------------
  L 280: __init__(self, input_size: int, output_size: int, bias: bool, gather_output: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], output_sizes: Optional[List[int]], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], use_presharded_weights: bool)

  L 347: weight_loader(self, param: Parameter, loaded_weight: torch.Tensor)

  L 397: weight_loader_v2(self, param: Parameter, loaded_weight: torch.Tensor)

  L 415: forward(self, input_)

  L 429: extra_repr(self)
         ‚Üí str


CLASS: LinearBase
----------------------------------------
  L 139: __init__(self, input_size: int, output_size: int, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str)

  L 162: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: MergedColumnParallelLinear
----------------------------------------
  L 461: __init__(self, input_size: int, output_sizes: List[int], bias: bool, gather_output: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], use_presharded_weights: bool)

  L 498: weight_loader(self, param: Parameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[int])

  L 693: weight_loader_v2(self, param: BasevLLMParameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[int])


CLASS: QKVParallelLinear
----------------------------------------
  L 773: __init__(self, hidden_size: int, head_size: int, total_num_heads: int, total_num_kv_heads: Optional[int], bias: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], load_presharded_attn: bool)

  L 896: weight_loader_v2(self, param: BasevLLMParameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[str])

  L 934: weight_loader(self, param: Parameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[str])


CLASS: ReplicatedLinear
----------------------------------------
  L 180: __init__(self, input_size: int, output_size: int, bias: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str)

  L 225: weight_loader(self, param: Parameter, loaded_weight: torch.Tensor)

  L 242: forward(self, x: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, Optional[torch.Tensor]]

  L 249: extra_repr(self)
         ‚Üí str


CLASS: RowParallelLinear
----------------------------------------
  L1173: __init__(self, input_size: int, output_size: int, bias: bool, input_is_parallel: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], reduce_results: bool, quant_config: Optional[QuantizationConfig], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], use_presharded_weights: bool)

  L1231: weight_loader(self, param: Parameter, loaded_weight: torch.Tensor)

  L1283: weight_loader_v2(self, param: BasevLLMParameter, loaded_weight: torch.Tensor)

  L1304: forward(self, input_, skip_all_reduce)

  L1331: extra_repr(self)
         ‚Üí str


============================================================
FILE: python/sglang/srt/layers/logits_processor.py
Functions: 9
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 607: def fused_softcap_kernel(full_logits_ptr,
        softcapping_value,
        n_elements,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 634: def fused_softcap(full_logits, final_logit_softcapping)


CLASS: LogitsMetadata
----------------------------------------
  L 123: from_forward_batch(cls, forward_batch: ForwardBatch)

  L 173: compute_dp_attention_metadata(self)


CLASS: LogitsProcessor
----------------------------------------
  L 202: __init__(self, config, skip_all_gather: bool, logit_scale: Optional[float])

  L 235: forward(self, input_ids, hidden_states, lm_head: VocabParallelEmbedding, logits_metadata: Union[LogitsMetadata, ForwardBatch], aux_hidden_states: Optional[torch.Tensor])
         ‚Üí LogitsProcessorOutput

  L 525: get_top_logprobs(all_logprobs: torch.Tensor, logits_metadata: LogitsMetadata)

  L 554: get_token_ids_logprobs(all_logprobs: torch.Tensor, logits_metadata: LogitsMetadata)

  L 577: compute_temp_top_p_normalized_logprobs(last_logits: torch.Tensor, logits_metadata: LogitsMetadata)
         ‚Üí torch.Tensor
         üìù compute logprobs for the output token from the given logits.


============================================================
FILE: python/sglang/srt/layers/moe/cutlass_moe.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  21: def cutlass_fused_experts_fp8(a: torch.Tensor,
        w1_q: torch.Tensor,
        w2_q: torch.Tensor,
        w1_scale: torch.Tensor,
        w2_scale: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        a1_strides: torch.Tensor,
        c1_strides: torch.Tensor,
        a2_strides: torch.Tensor,
        c2_strides: torch.Tensor,
        workspace: torch.Tensor,
        a_ptrs: torch.Tensor,
        b_ptrs: torch.Tensor,
        out_ptrs: torch.Tensor,
        a_scales_ptrs: torch.Tensor,
        b_scales_ptrs: torch.Tensor,
        expert_offsets: torch.Tensor,
        problem_sizes1: torch.Tensor,
        problem_sizes2: torch.Tensor,
        use_fp8_blockscale: bool)
         ‚Üí torch.Tensor
         üìù Performs Fused MoE computation using CUTLASS-like kernels with FP8 wei

  L 214: def cutlass_moe_fp4(a: torch.Tensor,
        a1_gscale: torch.Tensor,
        w1_fp4: torch.Tensor,
        w1_blockscale: torch.Tensor,
        w1_alphas: torch.Tensor,
        a2_gscale: torch.Tensor,
        w2_fp4: torch.Tensor,
        w2_blockscale: torch.Tensor,
        w2_alphas: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        params: CutlassMoEParams,
        apply_router_weight_on_input: bool)
         üìù MoE implementation for FP4 Inputs


============================================================
FILE: python/sglang/srt/layers/moe/cutlass_moe_params.py
Functions: 3
============================================================


CLASS: CutlassMoEParams
----------------------------------------
  L  90: __init__(self, cutlass_moe_type: CutlassMoEType, device: torch.device, num_experts: int, intermediate_size_per_partition: int, hidden_size: int)

  L 143: to_gemm1_args(self)
         ‚Üí dict

  L 157: to_gemm2_args(self)
         ‚Üí dict


============================================================
FILE: python/sglang/srt/layers/moe/cutlass_w4a8_moe.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  20: def cutlass_w4a8_moe(start_expert_id: int,
        end_expert_id: int,
        total_num_experts: int,
        a: torch.Tensor,
        w1_q: torch.Tensor,
        w2_q: torch.Tensor,
        w1_scale: torch.Tensor,
        w2_scale: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids_: torch.Tensor,
        local_topk_ids: torch.Tensor,
        a_strides1: torch.Tensor,
        b_strides1: torch.Tensor,
        c_strides1: torch.Tensor,
        a_strides2: torch.Tensor,
        b_strides2: torch.Tensor,
        c_strides2: torch.Tensor,
        s_strides13: torch.Tensor,
        s_strides2: torch.Tensor,
        expert_offsets: torch.Tensor,
        problem_sizes1: torch.Tensor,
        problem_sizes2: torch.Tensor,
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        apply_router_weight_on_input: bool)
         ‚Üí torch.Tensor
         üìù This function computes a w4a8-quantized Mixture of Experts (MoE) layer


============================================================
FILE: python/sglang/srt/layers/moe/ep_moe/kernels.py
Functions: 28
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  23: def deepep_permute_triton_kernel(input_ptr,
        gateup_input_ptr,
        src2dst_ptr,
        topk_ids_ptr,
        a1_scales_ptr,
        topk,
        hidden_size,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L  54: def deepep_post_reorder_triton_kernel(down_output_ptr,
        output_ptr,
        src2dst_ptr,
        topk_ids_ptr,
        topk_weights_ptr,
        topk,
        hidden_size,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L  87: def compute_src2dst_triton_kernel(reorder_ids,
        src2dst,
        num_toks,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L  98: def deepep_compute_src2dst_triton_kernel(reorder_ids,
        src2dst,
        num_toks,
        num_minus_one,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 109: def deepep_run_moe_deep_preprocess(topk_ids: torch.Tensor, num_experts: int)

  L 132: def compute_seg_indptr_triton_kernel(reorder_topk_ids, seg_indptr, num_toks)
         @triton.jit

  L 148: def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int)

  L 167: def run_cutlass_moe_ep_preproess(local_topk_ids: torch.Tensor,
        local_num_experts: int)

  L 187: def pre_reorder_triton_kernel_for_cutlass_moe(input_ptr,
        gateup_input_ptr,
        src2dst_ptr,
        topk_ids_ptr,
        a1_scales_ptr,
        num_experts,
        topk,
        hidden_size,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 224: def pre_reorder_triton_kernel(input_ptr,
        gateup_input_ptr,
        src2dst_ptr,
        topk_ids_ptr,
        a1_scales_ptr,
        start_expert_id,
        end_expert_id,
        topk,
        hidden_size,
        BLOCK_SIZE: tl.constexpr,
        use_per_token_if_dynamic: tl.constexpr)
         @triton.jit

  L 271: def silu_and_mul_triton_kernel(gateup_output,
        down_input,
        hidden_size,
        reorder_topk_ids,
        scales,
        start_expert_id,
        end_expert_id,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 395: def silu_and_mul_masked_post_quant_fwd(input: torch.Tensor,
        output: torch.Tensor,
        output_scale: torch.Tensor,
        quant_group_size: int,
        masked_m: torch.Tensor,
        scale_ue8m0: bool)
         üìù input shape [expert_num, token_num_padded, hidden_dim]

  L 464: def tanh(x)
         @triton.jit

  L 469: def gelu_and_mul_triton_kernel(gateup_output,
        down_input,
        hidden_size,
        reorder_topk_ids,
        scales,
        start_expert_id,
        end_expert_id,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 531: def post_reorder_triton_kernel(down_output_ptr,
        output_ptr,
        src2dst_ptr,
        topk_ids_ptr,
        topk_weights_ptr,
        start_expert_id,
        end_expert_id,
        topk,
        hidden_size,
        dst_start,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 585: def post_reorder_triton_kernel_for_cutlass_moe(down_output_ptr,
        output_ptr,
        src2dst_ptr,
        topk_ids_ptr,
        topk_weights_ptr,
        num_experts,
        topk,
        hidden_size,
        dst_start,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 628: def compute_m_range(pid,
        batch_size,
        seg_indptr,
        weight_indices,
        m_num_tiles_indptr,
        BLOCK_SIZE_M: tl.constexpr)
         @triton.jit

  L 651: def grouped_gemm_triton_kernel(a,
        b,
        c,
        batch_size,
        N,
        K,
        seg_indptr,
        weight_indices,
        m_num_tiles_indptr,
        scale_a,
        scale_b,
        use_fp8_w8a8: tl.constexpr,
        group_n: tl.constexpr,
        group_k: tl.constexpr,
        a_stride_0: tl.constexpr,
        b_stride_0: tl.constexpr,
        b_stride_1: tl.constexpr,
        as_stride_0: tl.constexpr,
        as_stride_1: tl.constexpr,
        bs_stride_0: tl.constexpr,
        bs_stride_2: tl.constexpr,
        bs_stride_1: tl.constexpr,
        use_per_token_if_dynamic: tl.constexpr,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        BLOCK_SIZE_K: tl.constexpr)
         @triton.jit

  L 755: def compute_m_num_tiles_indptr(m_num_tiles_indptr,
        seg_indptr,
        batch_size: tl.constexpr,
        BLOCK_SIZE_M: tl.constexpr)
         @triton.jit

  L 765: def grouped_gemm_triton(a: torch.Tensor,
        b: torch.Tensor,
        c: torch.Tensor,
        batch_size: int,
        weight_column_major: bool,
        seg_indptr: Optional[torch.Tensor],
        weight_indices: Optional[torch.Tensor],
        use_fp8_w8a8: bool,
        scale_a: torch.Tensor,
        scale_b: torch.Tensor,
        block_shape: Optional[List[int]],
        c_dtype,
        use_per_token_if_dynamic: bool)

  L 960: def ep_scatter(recv_x: torch.Tensor,
        recv_x_scale: torch.Tensor,
        recv_topk: torch.Tensor,
        num_recv_tokens_per_expert: torch.Tensor,
        expert_start_loc: torch.Tensor,
        output_tensor: torch.Tensor,
        output_tensor_scale: torch.Tensor,
        m_indices: torch.Tensor,
        output_index: torch.Tensor,
        scale_ue8m0: bool)
         @torch.no_grad()

  L1100: def ep_gather(input_tensor: torch.Tensor,
        recv_topk_ids: torch.Tensor,
        recv_topk_weight: torch.Tensor,
        input_index: torch.Tensor,
        output_tensor: torch.Tensor)
         @torch.no_grad()

  L1139: def get_tma_aligned_size(x: int, element_size: int)
         ‚Üí int
         üìù Global memory address of TMA must be 16-byte aligned.

  L1189: def tma_align_input_scale(input_scale: torch.Tensor)

  L1215: def compute_masked_m_triton_kernel(seg_indptr, masked_m)
         @triton.jit

  L1223: def deepgemm_compute_src2dst_triton_kernel(topk_ids,
        reorder_ids,
        seg_indptr,
        src2dst,
        m_max,
        num_toks,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L1244: def fill_gateup_input_triton_kernel(input_ptr,
        scale_ptr,
        gateup_input_ptr,
        gateup_input_scale_ptr,
        src2dst_ptr,
        topk_ids_ptr,
        start_expert_id,
        end_expert_id,
        topk,
        m_max,
        hidden_size,
        scale_size,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L1288: def moe_ep_deepgemm_preprocess(topk_ids: torch.Tensor,
        num_experts: int,
        hidden_states: torch.Tensor,
        top_k: int,
        start_expert_id,
        end_expert_id,
        block_shape,
        output_dtype: torch.dtype)


============================================================
FILE: python/sglang/srt/layers/moe/ep_moe/layer.py
Functions: 13
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 779: def get_moe_impl_class(quant_config: Optional[QuantizationConfig])


CLASS: DeepEPMoE
----------------------------------------
  L 339: __init__(self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, num_fused_shared_experts: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str, activation: str, routed_scaling_factor: Optional[float])

  L 421: forward(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)

  L 440: dispatch(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)

  L 454: moe_impl(self, dispatch_output: DispatchOutput)

  L 475: combine(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)

  L 489: forward_aiter(self, dispatch_output: Union[DeepEPNormalOutput, DeepEPLLOutput])

  L 523: forward_deepgemm_contiguous(self, dispatch_output: DeepEPNormalOutput)

  L 647: forward_deepgemm_masked(self, dispatch_output: DeepEPLLOutput)

  L 724: forward_npu(self, dispatch_output: DeepEPLLOutput)


CLASS: EPMoE
----------------------------------------
  L  82: __init__(self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, num_fused_shared_experts: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str, activation: str, routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_clamp_limit: Optional[float], with_bias: bool)

  L 138: forward(self, hidden_states: torch.Tensor, topk_output: TopKOutput)

  L 144: forward_deepgemm(self, hidden_states: torch.Tensor, topk_output: TopKOutput)


============================================================
FILE: python/sglang/srt/layers/moe/fused_moe_native.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  14: def fused_moe_forward_native(layer: torch.nn.Module,
        x: torch.Tensor,
        topk_output: StandardTopKOutput,
        moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor

  L  41: def moe_forward_native(layer: torch.nn.Module,
        x: torch.Tensor,
        topk_output: StandardTopKOutput,
        moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/moe/fused_moe_triton/__init__.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  19: def override_config(config)
         @contextmanager

  L  27: def get_config()
         ‚Üí Optional[Dict[str, Any]]


============================================================
FILE: python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
Functions: 20
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  72: def write_zeros_to_output(c_ptr,
        stride_cm,
        stride_cn,
        pid_n,
        N,
        offs_token,
        token_mask,
        BLOCK_SIZE_M,
        BLOCK_SIZE_N,
        compute_type)
         @triton.jit

  L  92: def fused_moe_kernel_gptq_awq(a_ptr,
        b_ptr,
        c_ptr,
        b_scale_ptr,
        b_zp_ptr,
        topk_weights_ptr,
        sorted_token_ids_ptr,
        expert_ids_ptr,
        num_tokens_post_padded_ptr,
        N: tl.constexpr,
        K: tl.constexpr,
        EM,
        num_valid_tokens,
        stride_am,
        stride_ak,
        stride_be,
        stride_bk,
        stride_bn,
        stride_cm,
        stride_cn,
        stride_bse,
        stride_bsk,
        stride_bsn,
        stride_bze,
        stride_bzk,
        stride_bzn,
        group_size: tl.constexpr,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        BLOCK_SIZE_K: tl.constexpr,
        GROUP_SIZE_M: tl.constexpr,
        MUL_ROUTED_WEIGHT: tl.constexpr,
        top_k: tl.constexpr,
        compute_type: tl.constexpr,
        has_zp: tl.constexpr,
        use_int4_w4a16: tl.constexpr,
        use_int8_w8a16: tl.constexpr,
        even_Ks: tl.constexpr)
         üìù Implements the fused computation for a Mixture of Experts (MOE) using
         @triton.jit

  L 323: def fused_moe_kernel(a_ptr,
        b_ptr,
        bias_ptr,
        c_ptr,
        a_scale_ptr,
        b_scale_ptr,
        topk_weights_ptr,
        sorted_token_ids_ptr,
        expert_ids_ptr,
        num_tokens_post_padded_ptr,
        N,
        K,
        EM,
        num_valid_tokens,
        stride_am,
        stride_ak,
        stride_be,
        stride_bk,
        stride_bn,
        stride_bias_e,
        stride_bias_n,
        stride_cm,
        stride_cn,
        stride_asm,
        stride_ask,
        stride_bse,
        stride_bsk,
        stride_bsn,
        group_n: tl.constexpr,
        group_k: tl.constexpr,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        BLOCK_SIZE_K: tl.constexpr,
        GROUP_SIZE_M: tl.constexpr,
        MUL_ROUTED_WEIGHT: tl.constexpr,
        top_k: tl.constexpr,
        compute_type: tl.constexpr,
        use_fp8_w8a8: tl.constexpr,
        use_int8_w8a8: tl.constexpr,
        use_int8_w8a16: tl.constexpr,
        per_channel_quant: tl.constexpr,
        even_Ks: tl.constexpr)
         üìù Implements the fused computation for a Mixture of Experts (MOE) using
         @triton.jit

  L 563: def moe_align_block_size(topk_ids: torch.Tensor,
        block_size: int,
        num_experts: int)
         ‚Üí Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
         üìù Aligns the token distribution across experts to be compatible with blo

  L 636: def invoke_fused_moe_kernel(A: torch.Tensor,
        B: torch.Tensor,
        bias: Optional[torch.Tensor],
        C: torch.Tensor,
        A_scale: Optional[torch.Tensor],
        B_scale: Optional[torch.Tensor],
        B_zp: Optional[torch.Tensor],
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        sorted_token_ids: torch.Tensor,
        expert_ids: torch.Tensor,
        num_tokens_post_padded: torch.Tensor,
        mul_routed_weight: bool,
        top_k: int,
        config: Dict[str,
        Any],
        compute_type: tl.dtype,
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        block_shape: Optional[List[int]],
        no_combine: bool)
         ‚Üí None

  L 813: def get_config_file_name(E: int,
        N: int,
        dtype: Optional[str],
        block_shape: Optional[int])
         ‚Üí str

  L 825: def get_moe_configs(E: int,
        N: int,
        dtype: Optional[str],
        block_n: Optional[int],
        block_k: Optional[int])
         ‚Üí Optional[Dict[int, Any]]
         üìù Return optimized configurations for the fused MoE kernel.
         @functools.lru_cache

  L 898: def get_default_config(M: int,
        E: int,
        N: int,
        K: int,
        topk: int,
        dtype: Optional[str],
        is_marlin: bool,
        block_shape: Optional[List[int]])
         ‚Üí Dict[str, int]

  L 955: def try_get_optimal_moe_config(w1_shape: Tuple[int,
        ...],
        w2_shape: Tuple[int,
        ...],
        top_k: int,
        dtype: Optional[str],
        M: int,
        is_marlin: bool,
        block_shape: Optional[List[int]])

  L 988: def get_config_dtype_str(dtype: torch.dtype,
        use_int8_w8a16: Optional[bool],
        use_int4_w4a16: Optional[bool],
        use_fp8_w8a8: Optional[bool],
        use_int8_w8a8: Optional[bool])

  L1010: def inplace_fused_experts(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]],
        routed_scaling_factor: Optional[float],
        gemm1_alpha: Optional[float],
        gemm1_limit: Optional[float])
         ‚Üí None

  L1066: def inplace_fused_experts_fake(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]],
        routed_scaling_factor: Optional[float],
        gemm1_alpha: Optional[float],
        gemm1_limit: Optional[float])
         ‚Üí None

  L1103: def outplace_fused_experts(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]],
        no_combine: bool,
        routed_scaling_factor: Optional[float],
        gemm1_alpha: Optional[float],
        gemm1_limit: Optional[float])
         ‚Üí torch.Tensor

  L1160: def outplace_fused_experts_fake(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]],
        no_combine: bool,
        routed_scaling_factor: Optional[float],
        gemm1_alpha: Optional[float],
        gemm1_limit: Optional[float])
         ‚Üí torch.Tensor

  L1198: def fused_experts(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_output: StandardTopKOutput,
        moe_runner_config: MoeRunnerConfig,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]])

  L1329: def moe_sum_reduce_triton(input: torch.Tensor,
        output: torch.Tensor,
        routed_scaling_factor: float)

  L1366: def moe_sum_reduce_torch_compile(x, out, routed_scaling_factor)
         @torch.compile

  L1372: def swiglu_with_alpha_and_limit(x, gemm1_alpha, gemm1_limit)
         @torch.compile

  L1379: def fused_experts_impl(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        inplace: bool,
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]],
        no_combine: bool,
        routed_scaling_factor: Optional[float],
        gemm1_alpha: Optional[float],
        gemm1_limit: Optional[float])

  L1646: def fused_moe(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_output: StandardTopKOutput,
        moe_runner_config: MoeRunnerConfig,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]])
         ‚Üí torch.Tensor
         üìù This function computes a Mixture of Experts (MoE) layer using two sets


============================================================
FILE: python/sglang/srt/layers/moe/fused_moe_triton/layer.py
Functions: 14
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L1057: def get_fused_moe_impl_class()
         üìù Factory function to get the appropriate FusedMoE implementation class.


CLASS: FlashInferFP4MoE
----------------------------------------
  L 967: __init__(self)

  L1000: forward(self, hidden_states: torch.Tensor, topk_output: TopKOutput)
         üìù Forward pass using FP4 TRTLLM kernel.


CLASS: FlashInferFusedMoE
----------------------------------------
  L 931: __init__(self)

  L 935: forward(self, hidden_states: torch.Tensor, topk_output: TopKOutput)


CLASS: FusedMoE
----------------------------------------
  L 119: __init__(self, num_experts: int, hidden_size: int, intermediate_size: int, layer_id: int, top_k: Optional[int], num_fused_shared_experts: int, params_dtype: Optional[torch.dtype], reduce_results: bool, quant_config: Optional[QuantizationConfig], prefix: str, activation: str, apply_router_weight_on_input: bool, use_presharded_weights: bool, inplace: bool, no_combine: bool, routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_clamp_limit: Optional[float], use_weight_loader_fused: bool, with_bias)

  L 459: weight_loader(self, param: torch.nn.Parameter, loaded_weight: torch.Tensor, weight_name: str, shard_id: str, expert_id: Optional[int])
         ‚Üí None

  L 716: weight_loader_fused(self, param: torch.nn.Parameter, loaded_weight: torch.Tensor, weight_name: str, shard_id: str)
         ‚Üí None

  L 794: forward(self, hidden_states: torch.Tensor, topk_output: TopKOutput)

  L 832: make_expert_params_mapping(cls, ckpt_gate_proj_name: str, ckpt_down_proj_name: str, ckpt_up_proj_name: str, num_experts: int)
         ‚Üí List[Tuple[str, str, int, str]]

  L 861: make_expert_params_mapping_fused(cls, ckpt_gate_up_proj_name: str, ckpt_down_proj_name: str, ckpt_gate_up_proj_bias_name: str, ckpt_down_proj_bias_name: str)

  L 880: make_expert_params_mapping_fused_mxfp4(cls, ckpt_gate_up_proj_name: str, ckpt_down_proj_name: str, ckpt_gate_up_proj_bias_name: str, ckpt_down_proj_bias_name: str, ckpt_gate_up_proj_scale_name: str, ckpt_down_proj_scale_name: str)

  L 907: make_expert_input_scale_params_mapping(cls, num_experts: int)
         ‚Üí List[Tuple[str, str, int, str]]

  L 923: should_fuse_routed_scaling_factor_in_topk(self)


============================================================
FILE: python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py
Functions: 5
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  25: def quantize(w, dtype, dev)

  L  54: def triton_kernel_moe_forward(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_output: TopKOutput,
        moe_runner_config: MoeRunnerConfig,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        per_channel_quant: bool,
        global_num_experts: int,
        expert_map: Optional[torch.Tensor],
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[list[int]])
         ‚Üí torch.Tensor

  L 101: def triton_kernel_fused_experts(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        routing_data: RoutingData,
        gather_indx: GatherIndx,
        scatter_indx: ScatterIndx,
        inplace: bool,
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        per_channel_quant: bool,
        global_num_experts: int,
        expert_map: Optional[torch.Tensor],
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[list[int]])
         ‚Üí torch.Tensor

  L 189: def triton_kernel_moe_with_bias_forward(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w1_pcg,
        b1: torch.Tensor,
        w2: torch.Tensor,
        w2_pcg,
        b2: torch.Tensor,
        topk_output: TopKOutput,
        moe_runner_config: MoeRunnerConfig,
        use_fp8_w8a8: bool,
        per_channel_quant: bool,
        global_num_experts: int,
        expert_map: Optional[torch.Tensor],
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[list[int]])
         ‚Üí torch.Tensor

  L 242: def triton_kernel_fused_experts_with_bias(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w1_pcg,
        b1: torch.Tensor,
        w2: torch.Tensor,
        w2_pcg,
        b2: torch.Tensor,
        routing_data: RoutingData,
        gather_indx: GatherIndx,
        scatter_indx: ScatterIndx,
        inplace: bool,
        activation: str,
        use_fp8_w8a8: bool,
        per_channel_quant: bool,
        global_num_experts: int,
        expert_map: Optional[torch.Tensor],
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[list[int]],
        gemm1_alpha: Optional[float],
        gemm1_clamp_limit: Optional[float])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/moe/rocm_moe_utils.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  23: def rocm_aiter_asm_moe_tkw1_impl(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        fc1_scale: Optional[torch.Tensor],
        fc2_scale: Optional[torch.Tensor],
        fc1_smooth_scale: Optional[torch.Tensor],
        fc2_smooth_scale: Optional[torch.Tensor],
        a16: bool,
        per_tensor_quant_scale: Optional[torch.Tensor],
        expert_mask: Optional[torch.Tensor],
        activation_method: int)
         ‚Üí torch.Tensor

  L  61: def rocm_aiter_asm_moe_tkw1_fake(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        fc1_scale: Optional[torch.Tensor],
        fc2_scale: Optional[torch.Tensor],
        fc1_smooth_scale: Optional[torch.Tensor],
        fc2_smooth_scale: Optional[torch.Tensor],
        a16: bool,
        per_tensor_quant_scale: Optional[torch.Tensor],
        expert_mask: Optional[torch.Tensor],
        activation_method: int)
         ‚Üí torch.Tensor

  L  89: def rocm_fused_experts_tkw1(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[list[int]])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/moe/router.py
Functions: 10
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  14: def fused_moe_router_kernel(input_ptr,
        moe_router_weight_ptr,
        topk_weights_ptr,
        topk_ids_ptr,
        correction_bias_ptr,
        is_correction_bias: tl.constexpr,
        num_experts: tl.constexpr,
        topk: tl.constexpr,
        moe_softcapping: tl.constexpr,
        moe_renormalize: tl.constexpr,
        hidden_dim: tl.constexpr,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 117: def fused_moe_router_impl(x: torch.Tensor,
        router_weight: torch.Tensor,
        topk: int,
        moe_softcapping: float,
        correction_bias: Optional[torch.Tensor])

  L 160: def fused_moe_router_large_bs_kernel(a_ptr,
        b_ptr,
        topk_weights_ptr,
        topk_ids_ptr,
        bs,
        num_experts: tl.constexpr,
        topk: tl.constexpr,
        moe_softcapping: tl.constexpr,
        moe_renormalize: tl.constexpr,
        K: tl.constexpr,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        BLOCK_SIZE_K: tl.constexpr,
        stride_am: tl.constexpr,
        stride_bn: tl.constexpr)
         @triton.jit

  L 269: def fused_moe_router_large_bs_impl(x: torch.Tensor,
        router_weight: torch.Tensor,
        topk: int,
        moe_softcapping: float,
        BLOCK_SIZE_M: int,
        BLOCK_SIZE_N: int,
        BLOCK_SIZE_K: int)

  L 312: def fused_moe_router_shim(moe_softcapping,
        hidden_states,
        gating_output,
        topk,
        renormalize,
        correction_bias: Optional[torch.Tensor])


CLASS: FusedMoeRouter
----------------------------------------
  L 356: __init__(self, router_linear, topk, moe_softcapping)
         ‚Üí None

  L 361: __call__(self)

  L 364: forward(self, x: torch.Tensor, residual: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 372: forward_cuda(self, x: torch.Tensor, autotune)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 383: forward_vllm(self, x: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


============================================================
FILE: python/sglang/srt/layers/moe/token_dispatcher/base_dispatcher.py
Functions: 13
============================================================


CLASS: BaseDispatcher
----------------------------------------
  L  95: dispatch(self)
         ‚Üí DispatchOutput

  L  99: combine(self)
         ‚Üí torch.Tensor


CLASS: DispatchOutput
----------------------------------------
  L  82: format(self)
         ‚Üí DispatchOutputFormat


CLASS: DispatchOutputChecker
----------------------------------------
  L  21: format_is_standard(dispatch_output: DispatchOutput)
         ‚Üí TypeGuard[StandardDispatchOutput]

  L  27: format_is_deepep_normal(dispatch_output: DispatchOutput)
         ‚Üí TypeGuard[DeepEPNormalOutput]

  L  33: format_is_deepep_ll(dispatch_output: DispatchOutput)
         ‚Üí TypeGuard[DeepEPLLOutput]

  L  39: format_is_deepep(dispatch_output: DispatchOutput)
         ‚Üí TypeGuard[Union[DeepEPNormalOutput, DeepEPLLOutput]]

  L  45: format_is_ascent_ll(dispatch_output: DispatchOutput)
         ‚Üí TypeGuard[AscendDeepEPLLOutput]


CLASS: DispatchOutputFormat
----------------------------------------
  L  58: is_standard(self)
         ‚Üí bool

  L  61: is_deepep_normal(self)
         ‚Üí bool

  L  64: is_deepep_ll(self)
         ‚Üí bool

  L  67: is_deepep(self)
         ‚Üí bool

  L  73: is_ascent_ll(self)
         ‚Üí bool


============================================================
FILE: python/sglang/srt/layers/moe/token_dispatcher/deepep.py
Functions: 31
============================================================


CLASS: AscendDeepEPLLOutput
----------------------------------------
  L  93: format(self)
         ‚Üí DispatchOutputFormat


CLASS: DeepEPBuffer
----------------------------------------
  L 115: get_deepep_buffer(cls, group: dist.ProcessGroup, hidden_size: int, param_bytes: int, deepep_mode: DeepEPMode, num_max_dispatch_tokens_per_rank: int, num_experts: int)

  L 195: clean_buffer(cls)

  L 205: set_dispatch_mode_as_normal(cls)

  L 209: set_dispatch_mode_as_low_latency(cls)


CLASS: DeepEPConfig
----------------------------------------
  L 218: __init__(self)

  L 238: get_instance(cls)


CLASS: DeepEPDispatcher
----------------------------------------
  L 625: __init__(self, group: torch.distributed.ProcessGroup, router_topk: int, permute_fusion: bool, num_experts: int, num_local_experts: int, hidden_size: int, params_dtype: torch.dtype, deepep_mode: DeepEPMode, async_finish: bool, return_recv_hook: bool)

  L 664: dispatch(self)
         ‚Üí DispatchOutput

  L 669: dispatch_a(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)

  L 684: dispatch_b(self)

  L 690: combine(self)
         ‚Üí Tuple

  L 695: combine_a(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)

  L 710: combine_b(self)


CLASS: DeepEPLLOutput
----------------------------------------
  L  78: format(self)
         ‚Üí DispatchOutputFormat


CLASS: DeepEPNormalOutput
----------------------------------------
  L  64: format(self)
         ‚Üí DispatchOutputFormat


CLASS: _DeepEPDispatcherImplBase
----------------------------------------
  L 245: __init__(self, group: torch.distributed.ProcessGroup, router_topk: int, permute_fusion: bool, num_experts: int, num_local_experts: int, hidden_size: int, params_dtype: torch.dtype, deepep_mode: DeepEPMode)

  L 278: dispatch_a(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)

  L 286: dispatch_b(self)

  L 289: combine_a(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)

  L 297: combine_b(self)


CLASS: _DeepEPDispatcherImplLowLatency
----------------------------------------
  L 474: __init__(self, return_recv_hook: bool)

  L 483: dispatch_a(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)

  L 510: dispatch_b(self, hidden_states, topk_idx, topk_weights, masked_m, expected_m, event, hook)

  L 569: combine_a(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)

  L 582: combine_b(self, hidden_states, event, hook)


CLASS: _DeepEPDispatcherImplNormal
----------------------------------------
  L 305: __init__(self, async_finish: bool)

  L 311: dispatch_a(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)

  L 330: dispatch_b(self, hidden_states, topk_idx, topk_weights, previous_event)

  L 406: combine_a(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)

  L 441: combine_b(self, output, previous_event)


============================================================
FILE: python/sglang/srt/layers/moe/token_dispatcher/standard.py
Functions: 1
============================================================


CLASS: StandardDispatchOutput
----------------------------------------
  L  15: format(self)
         ‚Üí DispatchOutputFormat


============================================================
FILE: python/sglang/srt/layers/moe/topk.py
Functions: 27
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 345: def fused_topk_torch_native(hidden_states: torch.Tensor,
        gating_output: torch.Tensor,
        topk: int,
        renormalize: bool)

  L 366: def fused_topk_cpu(hidden_states: torch.Tensor,
        gating_output: torch.Tensor,
        topk: int,
        renormalize: bool,
        num_token_non_padded: Optional[torch.Tensor],
        expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo])

  L 385: def apply_topk_weights_cpu(need_apply, topk_weights, inputs)

  L 398: def fused_topk(hidden_states: torch.Tensor,
        gating_output: torch.Tensor,
        topk: int,
        renormalize: bool,
        num_token_non_padded: Optional[torch.Tensor],
        expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo])

  L 429: def grouped_topk_gpu(hidden_states: torch.Tensor,
        gating_output: torch.Tensor,
        topk: int,
        renormalize: bool,
        num_expert_group: Optional[int],
        topk_group: Optional[int],
        num_fused_shared_experts: int,
        routed_scaling_factor: Optional[float],
        num_token_non_padded: Optional[torch.Tensor],
        expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo],
        apply_routed_scaling_factor_on_output: Optional[bool])
         @torch.compile(dynamic=True, backend=get_compiler_backend())

  L 497: def grouped_topk_cpu(hidden_states: torch.Tensor,
        gating_output: torch.Tensor,
        topk: int,
        renormalize: bool,
        num_expert_group: Optional[int],
        topk_group: Optional[int],
        num_fused_shared_experts: int,
        routed_scaling_factor: Optional[float],
        num_token_non_padded: Optional[torch.Tensor],
        expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo],
        apply_routed_scaling_factor_on_output: Optional[bool])

  L 526: def biased_grouped_topk_impl(hidden_states: torch.Tensor,
        gating_output: torch.Tensor,
        correction_bias: torch.Tensor,
        topk: int,
        renormalize: bool,
        num_expert_group: Optional[int],
        topk_group: Optional[int],
        num_fused_shared_experts: int,
        routed_scaling_factor: Optional[float],
        num_token_non_padded: Optional[torch.Tensor],
        expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo],
        apply_routed_scaling_factor_on_output: Optional[bool])
         @torch.compile(dynamic=True, backend=get_compiler_backend(), disable=_is_npu)

  L 599: def is_power_of_two(n)

  L 622: def biased_grouped_topk_gpu(hidden_states: torch.Tensor,
        gating_output: torch.Tensor,
        correction_bias: torch.Tensor,
        topk: int,
        renormalize: bool,
        num_expert_group: Optional[int],
        topk_group: Optional[int],
        num_fused_shared_experts: int,
        routed_scaling_factor: Optional[float],
        num_token_non_padded: Optional[torch.Tensor],
        expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo],
        apply_routed_scaling_factor_on_output: Optional[bool])

  L 701: def biased_grouped_topk_cpu(hidden_states: torch.Tensor,
        gating_output: torch.Tensor,
        correction_bias: torch.Tensor,
        topk: int,
        renormalize: bool,
        num_expert_group: Optional[int],
        topk_group: Optional[int],
        compiled: bool,
        num_fused_shared_experts: int,
        routed_scaling_factor: Optional[float],
        num_token_non_padded: Optional[torch.Tensor],
        expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo],
        apply_routed_scaling_factor_on_output: Optional[bool])

  L 743: def select_experts(hidden_states: torch.Tensor,
        router_logits: torch.Tensor,
        topk_config: TopKConfig)
         ‚Üí StandardTopKOutput


CLASS: BypassedTopKOutput
----------------------------------------
  L 178: format(self)
         ‚Üí TopKOutputFormat


CLASS: StandardTopKOutput
----------------------------------------
  L 152: format(self)
         ‚Üí TopKOutputFormat


CLASS: TopK
----------------------------------------
  L 187: __init__(self, top_k: int)

  L 226: forward_native(self, hidden_states: torch.Tensor, router_logits: torch.Tensor)
         ‚Üí TopKOutput

  L 243: forward_cuda(self, hidden_states: torch.Tensor, router_logits: torch.Tensor)
         ‚Üí TopKOutput

  L 280: forward_cpu(self, hidden_states: torch.Tensor, router_logits: torch.Tensor)
         ‚Üí TopKOutput

  L 296: forward_npu(self, hidden_states: torch.Tensor, router_logits: torch.Tensor)
         ‚Üí TopKOutput

  L 334: empty_topk_output(self, device: torch.device)
         ‚Üí TopKOutput


CLASS: TopKOutput
----------------------------------------
  L 139: format(self)
         ‚Üí TopKOutputFormat
         üìù The format of the output.


CLASS: TopKOutputChecker
----------------------------------------
  L 105: format_is_standard(topk_output: TopKOutput)
         ‚Üí TypeGuard[StandardTopKOutput]

  L 109: format_is_triton_kernel(topk_output: TopKOutput)
         ‚Üí TypeGuard[TritonKernelTopKOutput]

  L 115: format_is_bypassed(topk_output: TopKOutput)
         ‚Üí TypeGuard[BypassedTopKOutput]


CLASS: TopKOutputFormat
----------------------------------------
  L 124: is_standard(self)
         ‚Üí bool

  L 127: is_triton_kernel(self)
         ‚Üí bool

  L 130: is_bypassed(self)
         ‚Üí bool


CLASS: TritonKernelTopKOutput
----------------------------------------
  L 164: format(self)
         ‚Üí TopKOutputFormat


============================================================
FILE: python/sglang/srt/layers/moe/utils.py
Functions: 23
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 110: def initialize_moe_config(server_args: ServerArgs)

  L 130: def get_moe_a2a_backend()
         ‚Üí MoeA2ABackend

  L 138: def get_moe_runner_backend()
         ‚Üí MoeRunnerBackend

  L 146: def get_deepep_mode()
         ‚Üí DeepEPMode

  L 154: def get_deepep_config()
         ‚Üí str

  L 162: def is_tbo_enabled()
         ‚Üí bool

  L 170: def get_tbo_token_distribution_threshold()
         ‚Üí float

  L 181: def should_use_flashinfer_trtllm_moe()
         @lru_cache(maxsize=1)

  L 191: def should_use_flashinfer_cutlass_moe_fp4_allgather()
         üìù Perform FP4 quantize before all-gather for flashinfer cutlass moe to r
         @lru_cache(maxsize=1)


CLASS: DeepEPMode
----------------------------------------
  L  76: enable_normal(self)
         ‚Üí bool

  L  79: enable_low_latency(self)
         ‚Üí bool

  L  82: resolve(self, is_extend_in_batch: bool)
         ‚Üí DeepEPMode

  L  91: is_normal(self)
         ‚Üí bool

  L  94: is_low_latency(self)
         ‚Üí bool

  L  97: is_auto(self)
         ‚Üí bool


CLASS: MoeA2ABackend
----------------------------------------
  L  35: is_none(self)

  L  38: is_deepep(self)


CLASS: MoeRunnerBackend
----------------------------------------
  L  51: is_auto(self)

  L  54: is_triton(self)

  L  57: is_triton_kernel(self)

  L  60: is_flashinfer_trtllm(self)

  L  63: is_flashinfer_cutlass(self)

  L  66: is_flashinfer_mxfp4(self)


============================================================
FILE: python/sglang/srt/layers/multimodal.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  44: def hash_tiles32_kernel_blocked(in_ptr,
        out_ptr,
        n_u32,
        seed1,
        seed2,
        FM_C1: tl.constexpr,
        FM_C2: tl.constexpr,
        POS_A: tl.constexpr,
        POS_B: tl.constexpr,
        TILE: tl.constexpr,
        BLOCK: tl.constexpr,
        USE_CG: tl.constexpr)
         @triton.jit

  L 108: def add_tree_reduce_u64_kernel(in_ptr, out_ptr, n_elems, CHUNK: tl.constexpr)
         @triton.jit

  L 145: def gpu_tensor_hash(tensor: torch.Tensor)
         ‚Üí int
         @torch.inference_mode()


============================================================
FILE: python/sglang/srt/layers/parameter.py
Functions: 31
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 460: def permute_param_layout_(param: BasevLLMParameter,
        input_dim: int,
        output_dim: int)
         ‚Üí BasevLLMParameter
         üìù Permute a parameter's layout to the specified input and output dimensi


CLASS: BasevLLMParameter
----------------------------------------
  L  36: __new__(cls, data: torch.Tensor)

  L  40: __init__(self, data: torch.Tensor, weight_loader: Callable)
         üìù Initialize the BasevLLMParameter

  L  53: weight_loader(self)

  L  60: load_column_parallel_weight(self, loaded_weight: torch.Tensor)

  L  63: load_row_parallel_weight(self, loaded_weight: torch.Tensor)

  L  66: load_merged_column_weight(self, loaded_weight: torch.Tensor)

  L  69: load_qkv_weight(self, loaded_weight: torch.Tensor)


CLASS: PackedColumnParameter
----------------------------------------
  L 383: __init__(self, packed_factor: Union[int, Fraction], packed_dim: int, marlin_tile_size: Optional[int])

  L 396: packed_dim(self)

  L 400: packed_factor(self)

  L 404: marlin_tile_size(self)

  L 407: adjust_shard_indexes_for_packing(self, shard_size, shard_offset)


CLASS: PackedvLLMParameter
----------------------------------------
  L 427: __init__(self, packed_factor: Union[int, Fraction], packed_dim: int, marlin_tile_size: Optional[int])

  L 440: packed_dim(self)

  L 444: packed_factor(self)

  L 448: marlin_tile_size(self)

  L 451: adjust_shard_indexes_for_packing(self, shard_size, shard_offset)


CLASS: PerTensorScaleParameter
----------------------------------------
  L 322: __init__(self)

  L 338: load_row_parallel_weight(self)

  L 343: load_merged_column_weight(self)

  L 346: load_qkv_weight(self)

  L 349: load_column_parallel_weight(self)


CLASS: RowvLLMParameter
----------------------------------------
  L 225: __init__(self, input_dim: int)

  L 230: input_dim(self)

  L 233: load_row_parallel_weight(self, loaded_weight: torch.Tensor, tp_rank: int, use_presharded_weights: bool)


CLASS: _ColumnvLLMParameter
----------------------------------------
  L  84: __init__(self, output_dim: int)

  L  89: output_dim(self)

  L  92: load_column_parallel_weight(self, loaded_weight: torch.Tensor, tp_rank: int, use_presharded_weights: bool)

  L 125: load_merged_column_weight(self, loaded_weight: torch.Tensor)

  L 166: load_qkv_weight(self, loaded_weight: torch.Tensor, tp_rank: int, use_presharded_weights: bool)


============================================================
FILE: python/sglang/srt/layers/pooler.py
Functions: 4
============================================================


CLASS: CrossEncodingPooler
----------------------------------------
  L  71: __init__(self, config: PretrainedConfig, classifier: nn.Module, pooler: Optional[nn.Module])

  L  82: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí EmbeddingPoolerOutput
         üìù Pools sentence pair scores from the hidden_states.


CLASS: Pooler
----------------------------------------
  L  37: __init__(self, pooling_type: PoolingType, normalize: bool)

  L  42: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí EmbeddingPoolerOutput


============================================================
FILE: python/sglang/srt/layers/quantization/__init__.py
Functions: 5
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 124: def get_quantization_config(quantization: str)
         ‚Üí Type[QuantizationConfig]

  L 143: def monkey_patch_isinstance_for_vllm_base_layer(reverse: bool)
         üìù Patch isinstance so that the `get_quant_method` in vllm's Quantization

  L 179: def monkey_patch_moe_apply(class_obj: 'FusedMoEMethodBase')
         üìù Monkey patch the apply function of vllm's FusedMoEMethodBase.

  L 215: def monkey_patch_quant_configs()
         üìù Apply all monkey patches in one place.


CLASS: DummyConfig
----------------------------------------
  L  34: override_quantization_method(self)


============================================================
FILE: python/sglang/srt/layers/quantization/awq.py
Functions: 33
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  67: def is_layer_skipped_awq(prefix: str, modules_to_not_convert: List[str])


CLASS: AWQConfig
----------------------------------------
  L  77: __init__(self, weight_bits: int, group_size: int, zero_point: bool, modules_to_not_convert: Optional[List[str]])
         ‚Üí None

  L  97: __repr__(self)
         ‚Üí str

  L 105: get_scaled_act_names(self)
         ‚Üí List[str]

  L 108: get_name(self)
         ‚Üí str

  L 111: get_supported_act_dtypes(self)
         ‚Üí List[torch.dtype]

  L 115: get_min_capability(cls)
         ‚Üí int

  L 120: get_config_filenames()
         ‚Üí List[str]

  L 128: from_config(cls, config: Dict[str, Any])
         ‚Üí AWQConfig

  L 137: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional[LinearMethodBase]


CLASS: AWQLinearMethod
----------------------------------------
  L 326: __init__(self, quant_config: AWQConfig)

  L 329: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)

  L 396: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 401: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor


CLASS: AWQMarlinConfig
----------------------------------------
  L 158: __init__(self, weight_bits: int, group_size: int, zero_point: bool, lm_head_quantized: bool, modules_to_not_convert: Optional[list[str]], full_config: dict[str, Any])
         ‚Üí None

  L 188: __repr__(self)
         ‚Üí str

  L 197: get_scaled_act_names(self)
         ‚Üí List[str]

  L 201: get_name(cls)
         ‚Üí str

  L 205: get_supported_act_dtypes(cls)
         ‚Üí list[torch.dtype]

  L 209: get_min_capability(cls)
         ‚Üí int

  L 213: get_config_filenames(cls)
         ‚Üí list[str]

  L 217: from_config(cls, config: dict[str, Any])
         ‚Üí AWQMarlinConfig

  L 235: override_quantization_method(cls, hf_quant_cfg, user_quant)
         ‚Üí Optional[str]

  L 258: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional[QuantizeMethodBase]

  L 294: is_awq_marlin_compatible(cls, quant_config: dict[str, Any])


CLASS: AWQMarlinLinearMethod
----------------------------------------
  L 428: __init__(self, quant_config: AWQMarlinConfig)
         ‚Üí None

  L 431: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype)
         ‚Üí None

  L 509: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 549: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor


CLASS: AWQMoEMethod
----------------------------------------
  L 572: __init__(self, quant_config: AWQMarlinConfig)

  L 578: create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)

  L 674: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 739: apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/quantization/awq_triton.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  14: def awq_dequantize_kernel(qweight_ptr,
        scales_ptr,
        zeros_ptr,
        group_size,
        result_ptr,
        num_cols,
        num_rows,
        BLOCK_SIZE_X: tl.constexpr,
        BLOCK_SIZE_Y: tl.constexpr)
         @triton.jit

  L 111: def awq_gemm_kernel(a_ptr,
        b_ptr,
        c_ptr,
        zeros_ptr,
        scales_ptr,
        M,
        N,
        K,
        group_size,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        BLOCK_SIZE_K: tl.constexpr,
        SPLIT_K: tl.constexpr)
         @triton.jit

  L 235: def awq_dequantize_triton(qweight: torch.Tensor,
        scales: torch.Tensor,
        zeros: torch.Tensor,
        block_size_x: int,
        block_size_y: int)
         ‚Üí torch.Tensor

  L 289: def awq_gemm_triton(input: torch.Tensor,
        qweight: torch.Tensor,
        scales: torch.Tensor,
        qzeros: torch.Tensor,
        split_k_iters: int,
        block_size_m: int,
        block_size_n: int,
        block_size_k: int)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/quantization/base_config.py
Functions: 19
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 202: def method_has_implemented_embedding(method_class: Type[QuantizeMethodBase])
         ‚Üí bool
         üìù Not all quant methods have embedding implemented, so we need to check 


CLASS: FusedMoEMethodBase
----------------------------------------
  L  87: create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)

  L  99: apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor


CLASS: LinearMethodBase
----------------------------------------
  L  47: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
         üìù Create weights for a linear layer.

  L  73: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor
         üìù Apply the weights in layer to the input tensor.


CLASS: QuantizationConfig
----------------------------------------
  L 112: __init__(self)

  L 118: get_name(self)
         ‚Üí str
         üìù Name of the quantization method.

  L 123: get_supported_act_dtypes(self)
         ‚Üí List[torch.dtype]
         üìù List of supported activation dtypes.

  L 129: get_min_capability(cls)
         ‚Üí int
         üìù Minimum GPU capability to support the quantization method.

  L 140: get_config_filenames()
         ‚Üí List[str]
         üìù List of filenames to search for in the model directory.

  L 146: from_config(cls, config: Dict[str, Any])
         ‚Üí 'QuantizationConfig'
         üìù Create a config class from the model's quantization config.

  L 151: override_quantization_method(cls, hf_quant_cfg, user_quant)
         ‚Üí Optional[str]
         üìù Detects if this quantization method can support a given checkpoint

  L 161: get_from_keys(config: Dict[str, Any], keys: List[str])
         ‚Üí Any
         üìù Get a value from the model's quantization config.

  L 171: get_from_keys_or(config: Dict[str, Any], keys: List[str], default: Any)
         ‚Üí Any
         üìù Get a optional value from the model's quantization config.

  L 179: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional[QuantizeMethodBase]
         üìù Get the quantize method to use for the quantized layer.

  L 194: get_scaled_act_names(self)
         ‚Üí List[str]
         üìù Returns the activation function names that should be post-scaled.


CLASS: QuantizeMethodBase
----------------------------------------
  L  20: create_weights(self, layer: torch.nn.Module)
         üìù Create weights for a layer.

  L  29: apply(self, layer: torch.nn.Module)
         ‚Üí torch.Tensor
         üìù Apply the weights in layer to the input tensor.

  L  35: process_weights_after_loading(self, layer: nn.Module)
         ‚Üí None
         üìù Process the weight after loading.


============================================================
FILE: python/sglang/srt/layers/quantization/blockwise_int8.py
Functions: 16
============================================================


CLASS: BlockInt8Config
----------------------------------------
  L  36: __init__(self, is_checkpoint_int8_serialized: bool, activation_scheme: str, ignored_layers: Optional[List[str]], weight_block_size: List[int])
         ‚Üí None

  L  69: get_name(cls)
         ‚Üí str

  L  73: get_supported_act_dtypes(cls)
         ‚Üí List[torch.dtype]

  L  77: get_min_capability(cls)
         ‚Üí int

  L  81: get_config_filenames(cls)
         ‚Üí List[str]

  L  85: from_config(cls, config: Dict[str, Any])
         ‚Üí BlockInt8Config

  L  98: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional[QuantizeMethodBase]

  L 112: get_scaled_act_names(self)
         ‚Üí List[str]


CLASS: BlockInt8LinearMethod
----------------------------------------
  L 128: __init__(self, quant_config: BlockInt8Config)

  L 133: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)

  L 214: process_weights_after_loading(self, layer: Module)
         ‚Üí None

  L 222: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor


CLASS: BlockInt8MoEMethod
----------------------------------------
  L 250: __init__(self, quant_config: BlockInt8Config)

  L 255: create_weights(self, layer: Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)

  L 343: process_weights_after_loading(self, layer: Module)
         ‚Üí None

  L 347: apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py
Functions: 18
============================================================


CLASS: CompressedTensorsConfig
----------------------------------------
  L  79: __init__(self, target_scheme_map: Dict[str, Any], ignore: List[str], quant_format: str, sparsity_scheme_map: Dict[str, SparsityCompressionConfig], sparsity_ignore_list: List[str], kv_cache_scheme: Optional[Dict[str, Any]], config: Optional[Dict[str, Any]], packed_modules_mapping: Dict[str, List[str]])

  L 101: get_linear_method(self)
         ‚Üí CompressedTensorsLinearMethod

  L 104: get_supported_act_dtypes(cls)
         ‚Üí List[torch.dtype]

  L 108: get_min_capability(cls)
         ‚Üí int

  L 111: get_name(self)
         ‚Üí str

  L 114: get_scaled_act_names(self)
         ‚Üí List[str]

  L 117: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional[QuantizeMethodBase]

  L 143: from_config(cls, config: Dict[str, Any])
         ‚Üí CompressedTensorsConfig

  L 234: get_config_filenames(cls)
         ‚Üí List[str]

  L 438: get_scheme(self, layer: torch.nn.Module, layer_name: Optional[str])
         ‚Üí Optional[CompressedTensorsScheme]
         üìù compressed-tensors supports non uniform in the following way:

  L 533: get_cache_scale(self, name: str)
         ‚Üí Optional[str]
         üìù Check whether the param name matches the format for k/v cache scales

  L 550: supports_cutlass_24(weight_quant: Optional[QuantizationArgs], input_quant: Optional[QuantizationArgs], sparsity_scheme: Optional[SparsityCompressionConfig])
         ‚Üí bool
         üìù Check if the layer is supported by the Cutlass 2:4 Kernel


CLASS: CompressedTensorsLinearMethod
----------------------------------------
  L 618: __init__(self, quantization_config: CompressedTensorsConfig)

  L 621: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 624: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
         üìù Use the CompressedTensorsScheme associated with each layer to create

  L 650: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         üìù Use the output of create_weights and the CompressedTensorsScheme


CLASS: DeviceCapability
----------------------------------------
  L  64: as_version_str(self)
         ‚Üí str

  L  67: to_int(self)
         ‚Üí int
         üìù Express device capability as an integer ``<major><minor>``.


============================================================
FILE: python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors_moe.py
Functions: 10
============================================================


CLASS: CompressedTensorsMoEMethod
----------------------------------------
  L  70: __new__(cls)

  L  76: get_moe_method(quant_config: CompressedTensorsConfig)
         ‚Üí 'CompressedTensorsMoEMethod'


CLASS: CompressedTensorsW8A8Fp8MoEMethod
----------------------------------------
  L 100: __init__(self, quant_config: CompressedTensorsConfig)

  L 109: create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)

  L 207: process_weights_after_loading(self, layer: FusedMoE)
         ‚Üí None

  L 296: apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor


CLASS: CompressedTensorsWNA16MoEMethod
----------------------------------------
  L 346: __init__(self, quant_config: CompressedTensorsConfig)

  L 369: create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)

  L 513: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 643: apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py
Functions: 4
============================================================


CLASS: CompressedTensorsScheme
----------------------------------------
  L  20: get_min_capability(cls)
         ‚Üí int
         üìù Get minimum device capability.

  L  27: create_weights(self)
         üìù Weight creation for the particular scheme. Inputs to this function

  L  35: apply_weights(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         üìù Run the forward pass for the particular scheme. This is where

  L  51: process_weights_after_loading(self, layer: torch.nn.Module)
         üìù Called after weight loading is complete for any cleanup that


============================================================
FILE: python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py
Functions: 7
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  29: def apply_fp8_marlin_linear()

  L  32: def prepare_fp8_layer_for_marlin()


CLASS: CompressedTensorsW8A16Fp8
----------------------------------------
  L  42: __init__(self, strategy: str, is_static_input_scheme: bool)

  L  52: get_min_capability(cls)
         ‚Üí int

  L  59: process_weights_after_loading(self, layer)
         ‚Üí None

  L  81: create_weights(self, layer: torch.nn.Module, input_size: int, output_partition_sizes: List[int], input_size_per_partition: int, params_dtype: torch.dtype, weight_loader: Callable)

  L 139: apply_weights(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
Functions: 5
============================================================


CLASS: CompressedTensorsW8A8Fp8
----------------------------------------
  L  30: __init__(self, strategy: str, is_static_input_scheme: bool)

  L  35: get_min_capability(cls)
         ‚Üí int

  L  39: process_weights_after_loading(self, layer)
         ‚Üí None

  L  92: create_weights(self, layer: torch.nn.Module, output_partition_sizes: List[int], input_size_per_partition: int, params_dtype: torch.dtype, weight_loader: Callable)

  L 146: apply_weights(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/quantization/compressed_tensors/utils.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  12: def is_activation_quantization_format(format: str)
         ‚Üí bool

  L  21: def should_ignore_layer(layer_name: Optional[str],
        ignore: Iterable[str],
        fused_mapping: Mapping[str,
        List[str]])
         ‚Üí bool

  L  76: def check_equal_or_regex_match(layer_name: str, targets: Iterable[str])
         ‚Üí bool
         üìù Checks whether a layer_name is exactly equal or a regex match for

  L  87: def find_matched_target(layer_name: Optional[str],
        module: Module,
        targets: Iterable[str],
        fused_mapping: Mapping[str,
        List[str]])
         ‚Üí str
         üìù Helper function to look up which "target" in the compressed-tensors


============================================================
FILE: python/sglang/srt/layers/quantization/deep_gemm_wrapper/compile_utils.py
Functions: 10
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  42: def update_deep_gemm_config(gpu_id: int, server_args: ServerArgs)

  L 229: def deep_gemm_execution_hook(m: int,
        n: int,
        k: int,
        num_groups: int,
        kernel_type: DeepGemmKernelType)
         @contextmanager


CLASS: _BaseWarmupExecutor
----------------------------------------
  L 142: create(kernel_type: DeepGemmKernelType)

  L 149: execute(self, m)


CLASS: _GroupedContWarmupExecutor
----------------------------------------
  L 193: __init__(self, max_m: int, n: int, k: int, num_groups: int)

  L 199: execute(self, m)


CLASS: _GroupedMaskedWarmupExecutor
----------------------------------------
  L 209: __init__(self, max_m: int, n: int, k: int, num_groups: int)

  L 217: execute(self, m)


CLASS: _NormalWarmupExecutor
----------------------------------------
  L 179: __init__(self, max_m: int, n: int, k: int, num_groups: int)

  L 184: execute(self, m)


============================================================
FILE: python/sglang/srt/layers/quantization/deep_gemm_wrapper/entrypoint.py
Functions: 5
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  23: def grouped_gemm_nt_f8f8bf16_masked(lhs: Tuple[torch.Tensor,
        torch.Tensor],
        rhs: Tuple[torch.Tensor,
        torch.Tensor],
        out: torch.Tensor,
        masked_m: torch.Tensor,
        expected_m: int)

  L  46: def grouped_gemm_nt_f8f8bf16_contig(lhs: Tuple[torch.Tensor,
        torch.Tensor],
        rhs: Tuple[torch.Tensor,
        torch.Tensor],
        out: torch.Tensor,
        m_indices: torch.Tensor)

  L  60: def gemm_nt_f8f8bf16(lhs: Tuple[torch.Tensor,
        torch.Tensor],
        rhs: Tuple[torch.Tensor,
        torch.Tensor],
        out: torch.Tensor)

  L  78: def update_deep_gemm_config(gpu_id: int, server_args: ServerArgs)

  L  83: def configure_deep_gemm_num_sms(num_sms)
         @contextmanager


============================================================
FILE: python/sglang/srt/layers/quantization/fp8.py
Functions: 23
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  23: def dummy_func()

  L 491: def get_tile_tokens_dim(num_tokens, top_k, num_experts)


CLASS: Fp8Config
----------------------------------------
  L 113: __init__(self, is_checkpoint_fp8_serialized: bool, activation_scheme: str, ignored_layers: Optional[List[str]], weight_block_size: List[int])
         ‚Üí None

  L 143: get_name(cls)
         ‚Üí str

  L 147: get_supported_act_dtypes(cls)
         ‚Üí List[torch.dtype]

  L 151: get_min_capability(cls)
         ‚Üí int

  L 155: get_config_filenames(cls)
         ‚Üí List[str]

  L 159: from_config(cls, config: Dict[str, Any])
         ‚Üí Fp8Config

  L 172: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional[QuantizeMethodBase]

  L 186: get_scaled_act_names(self)
         ‚Üí List[str]


CLASS: Fp8KVCacheMethod
----------------------------------------
  L1210: __init__(self, quant_config: Fp8Config)


CLASS: Fp8LinearMethod
----------------------------------------
  L 208: __init__(self, quant_config: Union[Fp8Config, W4AFp8Config])

  L 224: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)

  L 332: process_weights_after_loading(self, layer: Module)
         ‚Üí None

  L 442: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor


CLASS: Fp8MoEMethod
----------------------------------------
  L 514: __init__(self, quant_config: Fp8Config)

  L 525: create_weights(self, layer: Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)

  L 749: process_weights_after_loading(self, layer: Module)
         ‚Üí None

  L 911: process_weights_hip_int4(self, layer: Module)

  L 954: process_weights_hip_scale_padding(self, layer: Module)

  L 987: apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor

  L1082: apply_with_router_logits(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor

  L1141: maybe_apply_hip_fused_experts(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, activation: str, no_combine: bool)
         ‚Üí Optional[torch.Tensor]


============================================================
FILE: python/sglang/srt/layers/quantization/fp8_kernel.py
Functions: 26
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  72: def is_fp8_fnuz()
         ‚Üí bool
         @lru_cache()

  L  89: def deep_gemm_fp8_fp8_bf16_nt(A: torch.Tensor,
        As: torch.Tensor,
        B: torch.Tensor,
        Bs: torch.Tensor,
        C: torch.Tensor)
         ‚Üí None

  L  98: def deep_gemm_fp8_fp8_bf16_nt_fake(A: torch.Tensor,
        As: torch.Tensor,
        B: torch.Tensor,
        Bs: torch.Tensor,
        C: torch.Tensor)
         ‚Üí None

  L 394: def per_token_group_quant_8bit(x: torch.Tensor,
        group_size: int,
        dst_dtype: torch.dtype,
        eps: float,
        column_major_scales: bool,
        scale_tma_aligned: bool,
        scale_ue8m0: bool,
        fuse_silu_and_mul: bool,
        masked_m: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 427: def create_per_token_group_quant_fp8_output_scale(x_shape,
        device,
        group_size,
        column_major_scales: bool,
        scale_tma_aligned: bool,
        scale_ue8m0: bool)

  L 471: def sglang_per_token_group_quant_fp8(x: torch.Tensor,
        group_size: int,
        eps: float,
        column_major_scales: bool,
        scale_tma_aligned: bool,
        scale_ue8m0: bool,
        fuse_silu_and_mul: bool,
        masked_m: Optional[torch.Tensor])

  L 507: def sglang_per_token_group_quant_8bit(x: torch.Tensor,
        group_size: int,
        dst_dtype: torch.dtype,
        eps: float,
        column_major_scales: bool,
        scale_tma_aligned: bool,
        scale_ue8m0: bool,
        fuse_silu_and_mul: bool,
        masked_m: Optional[torch.Tensor])

  L 546: def sglang_per_token_quant_fp8(x: torch.Tensor, dtype: torch.dtype)

  L 608: def static_quant_fp8(x: torch.Tensor, x_s: torch.Tensor, repeat_scale: bool)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù Function to perform static quantization using the given scale on an in

  L 909: def get_w8a8_block_fp8_configs(N: int, K: int, block_n: int, block_k: int)
         ‚Üí Optional[Dict[int, Any]]
         üìù Return optimized configurations for the w8a8 block fp8 kernel.
         @functools.lru_cache

  L 950: def select_w8a8_block_fp8_matmul_kernel(M, N, META)

  L 956: def use_w8a8_block_fp8_matmul_unrolledx4(M, N, META)

  L 965: def select_w8a8_block_fp8_matmul_kernel(M, N, META)

  L 972: def prepare_block_fp8_matmul_inputs(A: torch.Tensor,
        B: torch.Tensor,
        As: torch.Tensor,
        Bs: torch.Tensor,
        block_size: List[int],
        output_dtype: torch.dtype)
         ‚Üí Tuple[int, int, int]

  L1020: def w8a8_block_fp8_matmul_deepgemm(A: torch.Tensor,
        B: torch.Tensor,
        As: torch.Tensor,
        Bs: torch.Tensor,
        block_size: List[int],
        output_dtype: torch.dtype)
         ‚Üí torch.Tensor

  L1041: def w8a8_block_fp8_matmul_triton(A: torch.Tensor,
        B: torch.Tensor,
        As: torch.Tensor,
        Bs: torch.Tensor,
        block_size: List[int],
        output_dtype: torch.dtype)
         ‚Üí torch.Tensor
         üìù This function performs matrix multiplication with block-wise quantizat

  L1122: def w8a8_block_fp8_matmul(A: torch.Tensor,
        B: torch.Tensor,
        As: torch.Tensor,
        Bs: torch.Tensor,
        block_size: List[int],
        output_dtype: torch.dtype)
         ‚Üí torch.Tensor

  L1192: def per_tensor_quant_mla_fp8(x: torch.Tensor, x_s_out: torch.Tensor, eps: float)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù This function quantizes input values to float8 values with tensor-wise

  L1288: def per_token_group_quant_mla_deep_gemm_masked_fp8(x: torch.Tensor,
        group_size: int,
        eps: float,
        dtype: torch.dtype)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù This function quantizes input values to float8 values with per-token-g

  L1358: def scaled_fp8_quant(input: torch.Tensor,
        scale: Optional[torch.Tensor],
        num_token_padding: Optional[int],
        use_per_token_if_dynamic: bool)
         ‚Üí tuple[torch.Tensor, torch.Tensor]

  L1402: def scaled_fp8_quant(input: torch.Tensor,
        scale: Optional[torch.Tensor],
        num_token_padding: Optional[int],
        use_per_token_if_dynamic: bool)
         ‚Üí tuple[torch.Tensor, torch.Tensor]

  L1500: def per_token_group_quant_fp8_hopper_moe_mn_major(A: torch.Tensor,
        expert_offsets: torch.Tensor,
        problem_sizes: torch.Tensor,
        group_size: int,
        expert_tokens_alignment: int)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L1568: def per_group_transpose(a: torch.Tensor,
        expert_offsets: torch.Tensor,
        M_ALIGNMENT: int)
         ‚Üí torch.Tensor

  L1591: def is_weak_contiguous(x: torch.Tensor)

  L1600: def scaled_mm_kernel(a_ptr,
        b_ptr,
        scale_a_ptr,
        scale_b_ptr,
        c_ptr,
        bias_ptr,
        M,
        N,
        K,
        stride_am,
        stride_ak,
        stride_bk,
        stride_bn,
        stride_cm,
        stride_cn,
        ACCUMULATOR_DTYPE: tl.constexpr,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        BLOCK_SIZE_K: tl.constexpr,
        BLOCK_SIZE_SCALE_A: tl.constexpr,
        BLOCK_SIZE_SCALE_B: tl.constexpr)
         @triton.jit

  L1723: def triton_scaled_mm(input: torch.Tensor,
        weight: torch.Tensor,
        scale_a: torch.Tensor,
        scale_b: torch.Tensor,
        out_dtype: type[torch.dtype],
        bias: Optional[torch.Tensor],
        block_size_m: int,
        block_size_n: int,
        block_size_k: int,
        use_heuristic)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/quantization/fp8_utils.py
Functions: 20
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  63: def use_rowwise_torch_scaled_mm()

  L  81: def cutlass_fp8_supported()

  L  93: def normalize_e4m3fn_to_e4m3fnuz(weight: torch.Tensor,
        weight_scale: torch.Tensor,
        input_scale: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]

  L 118: def cutlass_block_fp8_supported()
         ‚Üí bool

  L 140: def dispatch_w8a8_block_fp8_linear()
         ‚Üí Callable

  L 153: def flashinfer_gemm_w8a8_block_fp8_linear(input: torch.Tensor,
        weight: torch.Tensor,
        block_size: List[int],
        weight_scale: torch.Tensor,
        input_scale: Optional[torch.Tensor],
        bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor

  L 185: def cutlass_w8a8_block_fp8_linear_with_fallback(input: torch.Tensor,
        weight: torch.Tensor,
        block_size: List[int],
        weight_scale: torch.Tensor,
        input_scale: Optional[torch.Tensor],
        bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor

  L 218: def deepgemm_w8a8_block_fp8_linear_with_fallback(input: torch.Tensor,
        weight: torch.Tensor,
        block_size: List[int],
        weight_scale: torch.Tensor,
        input_scale: Optional[torch.Tensor],
        bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor

  L 269: def aiter_w8a8_block_fp8_linear(input: torch.Tensor,
        weight: torch.Tensor,
        block_size: List[int],
        weight_scale: torch.Tensor,
        input_scale: Optional[torch.Tensor],
        bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor

  L 292: def triton_w8a8_block_fp8_linear(input: torch.Tensor,
        weight: torch.Tensor,
        block_size: List[int],
        weight_scale: torch.Tensor,
        input_scale: Optional[torch.Tensor],
        bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor

  L 315: def dequant_mxfp4(w_block: torch.Tensor, w_scale: torch.Tensor, out_dtype)
         ‚Üí torch.Tensor
         üìù :param w_block: (batch, n, k, 16), uint8, pack two mxfp4 into one byte

  L 342: def input_to_float8(x: torch.Tensor, dtype: torch.dtype)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù This function quantizes input values to float8 values with tensor-wise

  L 361: def block_quant_to_tensor_quant(x_q_block: torch.Tensor,
        x_s: torch.Tensor,
        block_size: List[int])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù This function converts block-wise quantization to tensor-wise quantiza

  L 404: def block_quant_dequant(x_q_block: torch.Tensor,
        x_s: torch.Tensor,
        block_size: List[int],
        dtype: torch.dtype)
         ‚Üí torch.Tensor
         üìù This function converts block-wise quantization to unquantized.

  L 427: def requant_weight_ue8m0_inplace(weight, weight_scale_inv, weight_block_size)

  L 471: def per_block_cast_to_fp8(x: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 488: def ceil_to_ue8m0(x: torch.Tensor)

  L 492: def channel_quant_to_tensor_quant(x_q_channel: torch.Tensor, x_s: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 542: def apply_fp8_linear(input: torch.Tensor,
        weight: torch.Tensor,
        weight_scale: torch.Tensor,
        input_scale: Optional[torch.Tensor],
        input_scale_ub: Optional[torch.Tensor],
        bias: Optional[torch.Tensor],
        cutlass_fp8_supported: bool,
        use_per_token_if_dynamic: bool,
        pad_output: Optional[bool],
        compressed_tensor_quant: bool)
         ‚Üí torch.Tensor

  L 809: def can_auto_enable_marlin_fp8()
         ‚Üí bool


============================================================
FILE: python/sglang/srt/layers/quantization/fpgemm_fp8.py
Functions: 12
============================================================


CLASS: FBGEMMFp8Config
----------------------------------------
  L  43: __init__(self, ignore_list: list[str], input_scale_ub: float)

  L  58: get_name(cls)
         ‚Üí str

  L  62: get_supported_act_dtypes(cls)
         ‚Üí list[torch.dtype]

  L  66: get_min_capability(cls)
         ‚Üí int

  L  70: get_config_filenames(cls)
         ‚Üí list[str]

  L  74: from_config(cls, config: dict[str, Any])
         ‚Üí FBGEMMFp8Config

  L  79: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional[QuantizeMethodBase]

  L  92: get_scaled_act_names(self)
         ‚Üí List[str]


CLASS: FBGEMMFp8LinearMethod
----------------------------------------
  L  98: __init__(self, quant_config: FBGEMMFp8Config)

  L 105: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype)

  L 155: process_weights_after_loading(self, layer: Module)
         ‚Üí None

  L 176: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/quantization/gptq.py
Functions: 34
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  62: def check_marlin_format(hf_quant_cfg: Dict[str, Any])
         ‚Üí bool

  L  70: def gptq_marlin_moe_repack(b_q_weight: torch.Tensor,
        perm: torch.Tensor,
        size_k: int,
        size_n: int,
        num_bits: int)
         ‚Üí torch.Tensor


CLASS: GPTQConfig
----------------------------------------
  L 106: __init__(self, weight_bits: int, group_size: int, desc_act: bool, lm_head_quantized: bool, dynamic: Dict[str, Dict[str, Union[int, bool]]])
         ‚Üí None

  L 151: __repr__(self)
         ‚Üí str

  L 160: get_scaled_act_names(self)
         ‚Üí List[str]
         üìù Returns the activation function names that should be post-scaled.

  L 168: get_name(cls)
         ‚Üí str

  L 172: get_supported_act_dtypes(cls)
         ‚Üí List[torch.dtype]

  L 177: get_min_capability(cls)
         ‚Üí int

  L 181: get_config_filenames(cls)
         ‚Üí List[str]

  L 185: from_config(cls, config: Dict[str, Any])
         ‚Üí GPTQConfig

  L 195: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional[LinearMethodBase]


CLASS: GPTQLinearMethod
----------------------------------------
  L 399: __init__(self, quant_config: GPTQConfig)

  L 402: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype)

  L 513: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 531: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor


CLASS: GPTQMarlinConfig
----------------------------------------
  L 219: __init__(self, weight_bits: int, group_size: int, desc_act: bool, is_sym: bool, lm_head_quantized: bool, dynamic: Dict[str, Dict[str, Union[int, bool]]], full_config: Dict[str, Any])
         ‚Üí None

  L 277: __repr__(self)
         ‚Üí str

  L 286: get_scaled_act_names(self)
         ‚Üí List[str]
         üìù Returns the activation function names that should be post-scaled.

  L 294: get_name(cls)
         ‚Üí str

  L 298: get_supported_act_dtypes(cls)
         ‚Üí List[torch.dtype]

  L 302: get_min_capability(cls)
         ‚Üí int

  L 306: get_config_filenames(cls)
         ‚Üí List[str]

  L 310: from_config(cls, config: Dict[str, Any])
         ‚Üí GPTQMarlinConfig

  L 330: override_quantization_method(cls, hf_quant_cfg, user_quant)
         ‚Üí Optional[str]

  L 356: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional[QuantizeMethodBase]

  L 367: is_gptq_marlin_compatible(cls, quant_config: Dict[str, Any])


CLASS: GPTQMarlinLinearMethod
----------------------------------------
  L 563: __init__(self, quant_config: GPTQMarlinConfig)
         ‚Üí None

  L 572: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype)
         ‚Üí None

  L 685: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 779: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor


CLASS: GPTQMarlinMoEMethod
----------------------------------------
  L 825: __init__(self, quant_config: GPTQMarlinConfig)
         ‚Üí None

  L 828: create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)

  L 974: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L1055: apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/quantization/int8_kernel.py
Functions: 5
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  51: def per_token_quant_int8(x, scale_dtype, cal_sum)

  L 126: def per_token_group_quant_int8(x: torch.Tensor,
        group_size: int,
        eps: float,
        dtype: torch.dtype)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù Function to perform per-token-group quantization on an input tensor `x

  L 185: def sglang_per_token_group_quant_int8(x: torch.Tensor,
        group_size: int,
        eps: float,
        dtype: torch.dtype)

  L 298: def get_w8a8_block_int8_configs(N: int, K: int, block_n: int, block_k: int)
         ‚Üí Optional[Dict[int, Any]]
         üìù Return optimized configurations for the w8a8 block fp8 kernel.
         @functools.lru_cache

  L 339: def w8a8_block_int8_matmul(A: torch.Tensor,
        B: torch.Tensor,
        As: torch.Tensor,
        Bs: torch.Tensor,
        block_size: List[int],
        output_dtype: torch.dtype)
         ‚Üí torch.Tensor
         üìù This function performs matrix multiplication with block-wise quantizat


============================================================
FILE: python/sglang/srt/layers/quantization/int8_utils.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  11: def apply_w8a8_block_int8_linear(input: torch.Tensor,
        weight: torch.Tensor,
        block_size: List[int],
        weight_scale: torch.Tensor,
        input_scale: Optional[torch.Tensor],
        bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor

  L  34: def input_to_int8(x: torch.Tensor, dtype: torch.dtype)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù This function quantizes input values to int8 values with tensor-wise q

  L  47: def block_dequant(x_q_block: torch.Tensor,
        x_s: torch.Tensor,
        block_size: List[int])
         ‚Üí torch.Tensor
         üìù This function conducts block-wise dequantization.


============================================================
FILE: python/sglang/srt/layers/quantization/kv_cache.py
Functions: 4
============================================================


CLASS: BaseKVCacheMethod
----------------------------------------
  L  28: __init__(self, quant_config: QuantizationConfig)

  L  31: create_weights(self, layer: torch.nn.Module)
         üìù Create "weight" (aka k_scale and v_scale) for an attention layer.

  L  45: apply(self, layer: torch.nn.Module)
         ‚Üí torch.Tensor

  L  48: process_weights_after_loading(self, layer: RadixAttention)
         ‚Üí None


============================================================
FILE: python/sglang/srt/layers/quantization/marlin_utils.py
Functions: 38
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  63: def query_marlin_supported_quant_types(has_zp: Optional[bool],
        include_fp_type: bool,
        device_capability: Optional[int])

  L 134: def check_marlin_supported(quant_type: ScalarType,
        group_size: int,
        has_zp: bool,
        device_capability: Optional[int])
         ‚Üí bool

  L 144: def verify_marlin_supported(quant_type: ScalarType,
        group_size: int,
        has_zp: bool)
         ‚Üí None

  L 153: def verify_marlin_supports_shape(output_size_per_partition: int,
        input_size_per_partition: int,
        input_size: int,
        group_size: int)
         ‚Üí None

  L 189: def check_marlin_supports_shape(output_size_per_partition: int,
        input_size_per_partition: int,
        input_size: int,
        group_size: int)
         ‚Üí tuple[bool, Optional[str]]

  L 204: def check_marlin_supports_layer(layer: LinearBase, group_size: int)
         ‚Üí bool

  L 220: def check_moe_marlin_supports_layer(layer: FusedMoE, group_size: int)
         ‚Üí bool

  L 244: def marlin_make_workspace(device: torch.device, max_blocks_per_sm: int)
         ‚Üí torch.Tensor

  L 255: def marlin_is_k_full(act_order: bool, is_row_parallel: bool)
         ‚Üí bool

  L 259: def marlin_repeat_scales_on_all_ranks(act_order: bool,
        group_size: int,
        is_row_parallel: bool)
         ‚Üí bool

  L 268: def marlin_make_empty_g_idx(device: torch.device)
         ‚Üí torch.Tensor

  L 274: def marlin_make_empty_zp(device: torch.device)
         ‚Üí torch.Tensor

  L 280: def marlin_sort_g_idx(g_idx: torch.Tensor)
         ‚Üí tuple[torch.Tensor, torch.Tensor]

  L 285: def get_scale_perms()

  L 295: def marlin_permute_scales(s: torch.Tensor,
        size_k: int,
        size_n: int,
        group_size: int)
         ‚Üí torch.Tensor

  L 309: def marlin_permute_bias(s: torch.Tensor)
         ‚Üí torch.Tensor

  L 316: def marlin_moe_permute_scales(s: torch.Tensor,
        size_k: int,
        size_n: int,
        group_size: int)

  L 334: def marlin_zero_points(zp: torch.Tensor,
        size_k: int,
        size_n: int,
        num_bits: int)
         ‚Üí torch.Tensor

  L 357: def awq_to_marlin_zero_points(q_zp_packed: torch.Tensor,
        size_k: int,
        size_n: int,
        num_bits: int)
         ‚Üí torch.Tensor

  L 381: def moe_awq_to_marlin_zero_points(q_zp_packed: torch.Tensor,
        size_k: int,
        size_n: int,
        num_bits: int)

  L 395: def maybe_warn_marlin_atomic_add(device, dtype)

  L 407: def maybe_warn_marlin_atomic_add_env()

  L 423: def should_use_atomic_add_reduce(m: int,
        n: int,
        k: int,
        device: torch.device,
        dtype: torch.dtype)
         ‚Üí bool

  L 448: def apply_gptq_marlin_linear(input: torch.Tensor,
        weight: torch.Tensor,
        weight_scale: torch.Tensor,
        weight_zp: torch.Tensor,
        g_idx: torch.Tensor,
        g_idx_sort_indices: torch.Tensor,
        workspace: torch.Tensor,
        wtype: ScalarType,
        output_size_per_partition: int,
        input_size_per_partition: int,
        is_k_full: bool,
        bias: Optional[torch.Tensor],
        use_fp32_reduce: bool)
         ‚Üí torch.Tensor

  L 500: def apply_awq_marlin_linear(input: torch.Tensor,
        weight: torch.Tensor,
        weight_scale: torch.Tensor,
        weight_zp: torch.Tensor,
        g_idx: torch.Tensor,
        g_idx_sort_indices: torch.Tensor,
        workspace: torch.Tensor,
        quant_type: ScalarType,
        output_size_per_partition: int,
        input_size_per_partition: int,
        bias: Optional[torch.Tensor],
        use_fp32_reduce: bool)
         ‚Üí torch.Tensor


CLASS: MarlinConfig
----------------------------------------
  L 556: __init__(self, group_size: int, lm_head_quantized: bool)
         ‚Üí None

  L 592: __repr__(self)
         ‚Üí str

  L 599: get_name(cls)
         ‚Üí str

  L 603: get_supported_act_dtypes(cls)
         ‚Üí list[torch.dtype]

  L 608: get_min_capability(cls)
         ‚Üí int

  L 612: get_config_filenames(cls)
         ‚Üí list[str]

  L 616: from_config(cls, config: dict[str, Any])
         ‚Üí 'MarlinConfig'

  L 622: override_quantization_method(cls, hf_quant_cfg, user_quant)
         ‚Üí Optional[str]

  L 642: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional[MarlinLinearMethod]


CLASS: MarlinLinearMethod
----------------------------------------
  L 662: __init__(self, quant_config: MarlinConfig)

  L 665: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype)

  L 777: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 783: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/quantization/marlin_utils_fp8.py
Functions: 6
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  27: def fp8_fused_exponent_bias_into_scales(scales)

  L  41: def apply_fp8_marlin_linear(input: torch.Tensor,
        weight: torch.Tensor,
        weight_scale: torch.Tensor,
        workspace: torch.Tensor,
        size_n: int,
        size_k: int,
        bias: Optional[torch.Tensor],
        use_fp32_reduce: bool)
         ‚Üí torch.Tensor

  L  83: def prepare_fp8_layer_for_marlin(layer: torch.nn.Module, size_k_first: bool)
         ‚Üí None

  L 175: def prepare_moe_fp8_layer_for_marlin(layer: torch.nn.Module, size_k_first: bool)
         ‚Üí None

  L 305: def pack_fp8_to_int32(fp8_tensor: torch.Tensor, size_k_first: bool)
         ‚Üí torch.Tensor
         üìù Repack FP8 weights to gptq format (packed int32 elements)

  L 322: def marlin_quant_fp8_torch(weight, group_size)


============================================================
FILE: python/sglang/srt/layers/quantization/modelopt_quant.py
Functions: 38
============================================================


CLASS: ModelOptFp4Config
----------------------------------------
  L 487: __init__(self, is_checkpoint_nvfp4_serialized: bool, kv_cache_quant_algo: str, group_size: int, exclude_modules: List[str])
         ‚Üí None

  L 505: get_name(cls)
         ‚Üí str

  L 509: get_supported_act_dtypes(cls)
         ‚Üí List[torch.dtype]

  L 513: get_min_capability(cls)
         ‚Üí int

  L 517: get_config_filenames(cls)
         ‚Üí List[str]

  L 521: from_config(cls, config: Dict[str, Any])
         ‚Üí ModelOptFp4Config

  L 595: is_layer_excluded(self, prefix: str, exclude_modules: list)

  L 604: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional[QuantizeMethodBase]

  L 626: get_scaled_act_names(self)
         ‚Üí List[str]


CLASS: ModelOptFp4LinearMethod
----------------------------------------
  L 645: __init__(self, quant_config: ModelOptFp4Config)

  L 648: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)

  L 722: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 759: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor


CLASS: ModelOptFp8Config
----------------------------------------
  L  78: __init__(self, is_checkpoint_fp8_serialized: bool, kv_cache_quant_method: Optional[str], exclude_modules: Optional[List[str]])
         ‚Üí None
         üìù Args:

  L  97: get_name(cls)
         ‚Üí str

  L 101: get_supported_act_dtypes(cls)
         ‚Üí List[torch.dtype]

  L 105: get_min_capability(cls)
         ‚Üí int

  L 109: get_config_filenames(cls)
         ‚Üí List[str]

  L 113: from_config(cls, config: Dict[str, Any])
         ‚Üí ModelOptFp8Config

  L 168: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional[QuantizeMethodBase]

  L 195: get_scaled_act_names(self)
         ‚Üí List[str]


CLASS: ModelOptFp8KVCacheMethod
----------------------------------------
  L 304: __init__(self, quant_config: ModelOptFp8Config)


CLASS: ModelOptFp8LinearMethod
----------------------------------------
  L 213: __init__(self, quant_config: ModelOptFp8Config)

  L 218: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], params_dtype: torch.dtype)
         ‚Üí None
         üìù Creates and registers weights, weight scales, and input scales for FP8

  L 270: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None
         üìù Requantizes weights after loading using the maximum scale.

  L 282: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor
         üìù Applies FP8 linear transformation.


CLASS: ModelOptFp8MoEMethod
----------------------------------------
  L 316: __init__(self, quant_config: ModelOptFp8Config)

  L 320: create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)

  L 397: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None
         üìù Process FP8 MoE weights after loading from serialized checkpoint.

  L 460: apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor


CLASS: ModelOptNvFp4FusedMoEMethod
----------------------------------------
  L 804: __init__(self, quant_config: ModelOptFp4Config)

  L 816: enable_flashinfer_cutlass_moe(self)
         ‚Üí bool

  L 822: create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)

  L 944: swizzle_blockscale(self, scale: torch.Tensor)

  L 969: prepare_static_weights_for_kernel(self, gemm1_weights, gemm2_weights, gemm1_scales_linear_fp4_bytes, gemm2_scales_linear_fp4_bytes, hidden_size, intermediate_size, num_experts)

  L1102: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None
         üìù Process FP4 MoE weights after loading from serialized checkpoint.

  L1237: load_up_proj_weight_first(self)
         ‚Üí bool

  L1241: apply(self, layer: FusedMoE, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/quantization/moe_wna16.py
Functions: 16
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  29: def get_weight_perm(num_bits: int)

  L 216: def is_layer_skipped_quant(prefix: str, modules_to_not_convert: List[str])


CLASS: MoeWNA16Config
----------------------------------------
  L  62: __init__(self, linear_quant_method: str, weight_bits: int, group_size: int, has_zp: bool, lm_head_quantized: bool, modules_to_not_convert: Optional[List[str]], full_config: Dict[str, Any])
         ‚Üí None

  L 109: get_name(cls)
         ‚Üí str

  L 113: get_supported_act_dtypes(cls)
         ‚Üí List[torch.dtype]

  L 117: get_min_capability(cls)
         ‚Üí int

  L 121: get_config_filenames(cls)
         ‚Üí List[str]

  L 124: get_scaled_act_names(self)
         ‚Üí List[str]

  L 128: from_config(cls, config: Dict[str, Any])
         ‚Üí MoeWNA16Config

  L 155: override_quantization_method(cls, hf_quant_cfg, user_quant)
         ‚Üí Optional[str]

  L 161: is_moe_wna16_compatible(cls, quant_config: Dict[str, Any])

  L 185: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional[QuantizeMethodBase]


CLASS: MoeWNA16Method
----------------------------------------
  L 227: __init__(self, quant_config: MoeWNA16Config)

  L 230: create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)

  L 352: apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor

  L 385: get_weight_loader(layer, weight_loader)


============================================================
FILE: python/sglang/srt/layers/quantization/mxfp4.py
Functions: 17
============================================================


CLASS: Mxfp4Config
----------------------------------------
  L 172: __init__(self, ignored_layers: Optional[list[str]], is_checkpoint_mxfp4_serialized: bool)

  L 182: from_config(cls, config)

  L 202: get_min_capability(cls)
         ‚Üí int

  L 206: get_name(cls)
         ‚Üí str

  L 210: get_supported_act_dtypes(cls)
         ‚Üí list[torch.dtype]

  L 214: get_config_filenames(cls)
         ‚Üí list[str]

  L 217: is_static_cfg(self)

  L 220: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional['QuantizeMethodBase']

  L 247: get_scaled_act_names(self)
         ‚Üí List[str]


CLASS: Mxfp4DynamicQuantMoEMethod
----------------------------------------
  L 726: create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)

  L 783: mxfp4_quantize(self, w)

  L 801: process_weights_after_loading(self, layer: Module)
         ‚Üí None

  L 811: apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor


CLASS: Mxfp4MoEMethod
----------------------------------------
  L 253: __init__(self, prefix: str)

  L 281: create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype, with_bias: bool)

  L 389: process_weights_after_loading(self, layer)

  L 616: apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/quantization/mxfp4_tensor.py
Functions: 2
============================================================


CLASS: MXFP4QuantizeUtil
----------------------------------------
  L  29: quantize(cls, input: torch.Tensor, block_size: Optional[int])
         ‚Üí tuple
         üìù Converting a tensor to a quantized format based on MXFP4 quantization.

  L  77: dequantize(cls, quantized_data, dtype: torch.dtype, scale, block_sizes)
         üìù Dequantze MXFP4 packed tensor to a target dtype.


============================================================
FILE: python/sglang/srt/layers/quantization/petit.py
Functions: 15
============================================================


CLASS: PetitNvFp4Config
----------------------------------------
  L  36: __init__(self, is_checkpoint_nvfp4_serialized: bool, kv_cache_quant_algo: str, group_size: int, exclude_modules: List[str])
         ‚Üí None

  L  54: get_name(cls)
         ‚Üí str

  L  58: get_supported_act_dtypes(cls)
         ‚Üí List[torch.dtype]

  L  62: get_min_capability(cls)
         ‚Üí int

  L  67: get_config_filenames(cls)
         ‚Üí List[str]

  L  71: from_config(cls, config: Dict[str, Any])
         ‚Üí 'PetitNvFp4Config'

  L 101: override_quantization_method(cls, hf_quant_cfg, user_quant)
         ‚Üí Optional[str]

  L 108: is_petit_nvfp4_compatible(cls, quant_config: Dict[str, Any])
         ‚Üí bool

  L 112: is_layer_excluded(self, prefix: str, exclude_modules: list)

  L 119: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional['QuantizeMethodBase']

  L 130: get_scaled_act_names(self)
         ‚Üí List[str]


CLASS: PetitNvFp4LinearMethod
----------------------------------------
  L 149: __init__(self, quant_config: PetitNvFp4Config)

  L 152: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)

  L 226: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 238: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/quantization/petit_utils.py
Functions: 5
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  17: def prepare_nvfp4_layer_for_petit(layer: torch.nn.Module)
         ‚Üí None

  L  22: def apply_petit_nvfp4_linear(input: torch.Tensor,
        weight: torch.Tensor,
        weight_scale: torch.Tensor,
        weight_scale_2: torch.Tensor,
        size_n: int,
        size_k: int,
        bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor

  L  55: def verify_petit_nvfp4_supported(quant_method: str, group_size: Optional[int])
         ‚Üí None

  L  61: def prepare_nvfp4_layer_for_petit(layer: torch.nn.Module)
         ‚Üí None

  L  78: def apply_petit_nvfp4_linear(input: torch.Tensor,
        weight: torch.Tensor,
        weight_scale: torch.Tensor,
        weight_scale_2: torch.Tensor,
        size_n: int,
        size_k: int,
        bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/quantization/qoq.py
Functions: 13
============================================================


CLASS: QoQConfig
----------------------------------------
  L  40: __init__(self, weight_bits: int, group_size: int)
         ‚Üí None

  L  61: __repr__(self)
         ‚Üí str

  L  67: get_supported_act_dtypes(cls)
         ‚Üí List[torch.dtype]

  L  71: get_min_capability(cls)
         ‚Üí int

  L  75: get_name(cls)
         ‚Üí str

  L  79: get_config_filenames(cls)
         ‚Üí List[str]
         üìù List of filenames to search for in the model directory.

  L  87: from_config(cls, config: Dict[str, Any])
         ‚Üí QoQConfig

  L  92: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional[QuantizeMethodBase]

  L 103: get_scaled_act_names(self)
         ‚Üí List[str]


CLASS: QoQLinearMethod
----------------------------------------
  L 114: __init__(self, quant_config: QoQConfig)

  L 117: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)

  L 210: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 219: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])


============================================================
FILE: python/sglang/srt/layers/quantization/quark/quark.py
Functions: 16
============================================================


CLASS: QuarkConfig
----------------------------------------
  L  29: __init__(self, quant_config: dict[str, Any], kv_cache_group: Optional[list[str]], kv_cache_config: Optional[dict[str, Any]], pack_method: str)

  L  46: get_linear_method(self)
         ‚Üí 'QuarkLinearMethod'

  L  50: get_supported_act_dtypes(cls)
         ‚Üí list[torch.dtype]

  L  54: get_min_capability(cls)
         ‚Üí int

  L  57: get_name(self)
         ‚Üí str

  L  60: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional['QuantizeMethodBase']

  L  86: from_config(cls, config: dict[str, Any])
         ‚Üí 'QuarkConfig'

  L 153: get_config_filenames(cls)
         ‚Üí list[str]

  L 289: get_scheme(self, layer: torch.nn.Module, layer_name: str)
         ‚Üí 'QuarkScheme'

  L 302: get_scaled_act_names(self)
         ‚Üí List[str]


CLASS: QuarkKVCacheMethod
----------------------------------------
  L 363: __init__(self, quant_config: QuarkConfig)

  L 368: validate_kv_cache_config(kv_cache_config: Optional[dict[str, Any]])
         üìù Validator for the kv cache configuration. Useful for controlling the


CLASS: QuarkLinearMethod
----------------------------------------
  L 308: __init__(self, quantization_config: QuarkConfig)

  L 311: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 314: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype)
         üìù Use the CompressedTensorsScheme associated with each layer to create

  L 340: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         üìù Use the output of create_weights and the CompressedTensorsScheme


============================================================
FILE: python/sglang/srt/layers/quantization/quark/quark_moe.py
Functions: 6
============================================================


CLASS: QuarkMoEMethod
----------------------------------------
  L  26: __new__(cls)

  L  45: get_moe_method(quant_config: 'QuarkConfig', module: torch.nn.Module, layer_name: str)
         ‚Üí 'QuarkMoEMethod'


CLASS: QuarkW4A4MXFp4MoEMethod
----------------------------------------
  L  69: __init__(self, weight_config: dict[str, Any], input_config: dict[str, Any])

  L  85: create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)

  L 157: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 173: apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/quantization/quark/schemes/quark_scheme.py
Functions: 4
============================================================


CLASS: QuarkScheme
----------------------------------------
  L  19: get_min_capability(cls)
         ‚Üí int
         üìù Get minimum device capability.

  L  26: create_weights(self)
         üìù Weight creation for the particular scheme. Inputs to this function

  L  34: apply_weights(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         üìù Run the forward pass for the particular scheme. This is where

  L  50: process_weights_after_loading(self, layer: torch.nn.Module)
         üìù Called after weight loading is complete for any cleanup that


============================================================
FILE: python/sglang/srt/layers/quantization/quark/schemes/quark_w4a4_mxfp4.py
Functions: 5
============================================================


CLASS: QuarkW4A4MXFP4
----------------------------------------
  L  26: __init__(self, weight_quant_spec: dict[str, Any], input_quant_spec: dict[str, Any])

  L  35: get_min_capability(cls)
         ‚Üí int

  L  38: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L  50: create_weights(self, layer: torch.nn.Module, output_partition_sizes: list[int], input_size_per_partition: int, params_dtype: torch.dtype, weight_loader: Callable)

  L  90: apply_weights(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/quantization/quark/utils.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L   9: def deep_compare(dict1: Any, dict2: Any)
         ‚Üí bool

  L  22: def should_ignore_layer(layer_name: Optional[str],
        ignore: Iterable[str],
        fused_mapping: Mapping[str,
        list[str]])
         ‚Üí bool

  L  78: def check_equal_or_regex_match(layer_name: str, targets: Iterable[str])
         ‚Üí bool
         üìù Checks whether a layer_name is exactly equal or a regex match for


============================================================
FILE: python/sglang/srt/layers/quantization/unquant.py
Functions: 14
============================================================


CLASS: UnquantizedEmbeddingMethod
----------------------------------------
  L  47: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
         üìù Create weights for embedding layer.

  L  70: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor

  L  78: embedding(self, layer: torch.nn.Module, input_: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: UnquantizedFusedMoEMethod
----------------------------------------
  L 135: __init__(self, use_triton_kernels: bool)

  L 153: create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype, with_bias: bool)

  L 206: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 225: apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor

  L 240: forward_cuda(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor

  L 314: forward_cpu(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor

  L 361: forward_npu(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor

  L 377: forward_tpu(self)
         ‚Üí torch.Tensor


CLASS: UnquantizedLinearMethod
----------------------------------------
  L  85: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)

  L 107: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 111: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/quantization/utils.py
Functions: 20
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  20: def get_scalar_types()
         üìù Returns:

  L  47: def is_layer_skipped(prefix: str,
        ignored_layers: List[str],
        fused_mapping: Mapping[str,
        List[str]])
         ‚Üí bool

  L  85: def per_tensor_dequantize(tensor: torch.Tensor,
        inv_scale: Union[float,
        torch.Tensor])
         ‚Üí torch.Tensor

  L  93: def all_close_1d(x: torch.Tensor)
         ‚Üí bool

  L  98: def convert_to_channelwise(weight_scale: torch.Tensor,
        logical_widths: List[int])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 121: def requantize_with_max_scale(weight: torch.Tensor,
        weight_scale: torch.Tensor,
        logical_widths: List[int])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 149: def update_tensor_inplace(old: torch.Tensor, new: torch.Tensor)
         ‚Üí None

  L 156: def replace_parameter(mod: torch.nn.Module,
        name: str,
        new: Union[torch.Tensor,
        torch.nn.Parameter])
         ‚Üí None

  L 179: def assert_fp8_all_close(a: torch.Tensor, b: torch.Tensor)

  L 202: def override_config(config: QuantizationConfig, prefix: str)

  L 234: def get_dynamic_override(config: QuantizationConfig,
        layer_name: str,
        key: Optional[str],
        default_value: Union[int,
        bool,
        None])
         ‚Üí Union[Dict, int, bool, None]

  L 255: def get_linear_quant_method(config: QuantizationConfig,
        layer: torch.nn.Module,
        prefix: str,
        linear_method_cls: type)

  L 288: def get_pack_factor(num_bits)

  L 293: def permute_rows(q_w: torch.Tensor,
        w_ref: torch.Tensor,
        group_size: int,
        test_perm: Optional[torch.Tensor])

  L 323: def pack_cols(q_w: torch.Tensor, num_bits: int, size_k: int, size_n: int)

  L 349: def pack_rows(q_w: torch.Tensor, num_bits: int, size_k: int, size_n: int)

  L 373: def unpack_cols(packed_q_w: torch.Tensor,
        num_bits: int,
        size_k: int,
        size_n: int)

  L 406: def quantize_weights(w: torch.Tensor,
        quant_type: ScalarType,
        group_size: Optional[int],
        zero_points: bool,
        ref_zero_points_after_scales: bool)

  L 505: def gptq_quantize_weights(w: torch.Tensor,
        quant_type: ScalarType,
        group_size: int,
        act_order: bool,
        test_perm: Optional[torch.Tensor])

  L 539: def sort_weights(q_w: torch.Tensor, g_idx: torch.Tensor)


============================================================
FILE: python/sglang/srt/layers/quantization/w4afp8.py
Functions: 12
============================================================


CLASS: W4AFp8Config
----------------------------------------
  L  33: __init__(self, is_checkpoint_fp8_serialized: bool, is_checkpoint_w4afp8_serialized: bool, linear_activation_scheme: str, moe_activation_scheme: str, ignored_layers: Optional[List[str]], weight_block_size: Optional[List[int]], group_size: int)
         ‚Üí None

  L  57: get_name(cls)
         ‚Üí str

  L  61: get_supported_act_dtypes(cls)
         ‚Üí List[torch.dtype]

  L  65: get_min_capability(cls)
         ‚Üí int

  L  69: get_config_filenames(cls)
         ‚Üí List[str]

  L  73: from_config(cls, config: Dict[str, Any])
         ‚Üí W4AFp8Config

  L  88: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional[QuantizeMethodBase]

  L 103: get_scaled_act_names(self)
         ‚Üí List[str]


CLASS: W4AFp8MoEMethod
----------------------------------------
  L 109: __init__(self, quant_config: W4AFp8Config)

  L 112: create_weights(self, layer: EPMoE, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)

  L 252: process_weights_after_loading(self, layer: Module)
         ‚Üí None

  L 281: apply(self, layer: EPMoE, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/quantization/w8a8_fp8.py
Functions: 16
============================================================


CLASS: W8A8FP8MoEMethod
----------------------------------------
  L 204: __init__(self, quant_config: W8A8Fp8Config)

  L 207: create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)

  L 259: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 269: apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor


CLASS: W8A8Fp8Config
----------------------------------------
  L  54: __init__(self, is_checkpoint_fp8_serialized: bool)

  L  58: get_supported_act_dtypes(cls)
         ‚Üí List[torch.dtype]

  L  62: get_min_capability(cls)
         ‚Üí int

  L  66: get_name(self)
         ‚Üí str

  L  70: get_config_filenames(cls)
         ‚Üí List[str]

  L  74: from_config(cls, config: Dict[str, Any])
         ‚Üí W8A8Fp8Config

  L  81: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional[QuantizeMethodBase]

  L  95: get_scaled_act_names(self)
         ‚Üí List[str]


CLASS: W8A8Fp8LinearMethod
----------------------------------------
  L 101: __init__(self, quantization_config: W8A8Fp8Config)

  L 105: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 137: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)

  L 178: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])


============================================================
FILE: python/sglang/srt/layers/quantization/w8a8_int8.py
Functions: 50
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  75: def npu_wrapper_rmsnorm_init(func)

  L  86: def npu_wrapper_rmsnorm_forward(func)

  L 108: def npu_fused_experts(hidden_states: torch.Tensor,
        w13: torch.Tensor,
        w13_scale: torch.Tensor,
        w2: torch.Tensor,
        w2_scale: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        top_k: int)


CLASS: NPU_W8A8DynamicLinearMethod
----------------------------------------
  L 823: __init__(self, quantization_config: W8A8Int8Config)
         ‚Üí None

  L 827: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
         ‚Üí None

  L 867: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 871: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor


CLASS: NPU_W8A8DynamicLinearMethodImpl
----------------------------------------
  L 763: __init__(self)

  L 767: get_weight(input_size: int, output_size: int, params_dtype: torch.dtype)
         ‚Üí Dict[str, Any]

  L 774: get_pertensor_param(params_dtype: torch.dtype)
         ‚Üí Dict[str, Any]

  L 778: get_perchannel_param(output_size: int, params_dtype: torch.dtype)
         ‚Üí Dict[str, Any]

  L 788: apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor], tp_rank: Optional[int])
         ‚Üí torch.Tensor

  L 805: process_weights_after_loading(self, layer)


CLASS: NPU_W8A8LinearMethod
----------------------------------------
  L 699: __init__(self, quantization_config: W8A8Int8Config)
         ‚Üí None

  L 707: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
         ‚Üí None

  L 747: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 751: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor


CLASS: NPU_W8A8LinearMethodImpl
----------------------------------------
  L 537: __init__(self)
         ‚Üí None

  L 542: get_weight(input_size: int, output_size: int, params_dtype: torch.dtype)
         ‚Üí Dict[str, Any]

  L 551: get_pertensor_param(params_dtype: torch.dtype)
         ‚Üí Dict[str, Any]

  L 558: get_perchannel_param(output_size: int, params_dtype: torch.dtype)
         ‚Üí Dict[str, Any]

  L 573: apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor

  L 605: process_weights_after_loading(self, layer)


CLASS: NPU_W8A8LinearMethodMTImpl
----------------------------------------
  L 624: __init__(self)
         ‚Üí None

  L 628: get_weight(input_size: int, output_size: int, params_dtype: torch.dtype)
         ‚Üí Dict[str, Any]

  L 637: get_pertensor_param(params_dtype: torch.dtype)
         ‚Üí Dict[str, Any]

  L 644: get_perchannel_param(output_size: int, params_dtype: torch.dtype)
         ‚Üí Dict[str, Any]

  L 659: apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         ‚Üí torch.Tensor

  L 682: process_weights_after_loading(self, layer)


CLASS: NPU_W8A8MoEMethod
----------------------------------------
  L 890: __init__(self, quantization_config: W8A8Int8Config)
         ‚Üí None

  L 894: create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)
         ‚Üí None

  L 952: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 972: apply(self, layer, x, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor


CLASS: W8A8Int8Config
----------------------------------------
  L 191: __init__(self, quant_config: Dict[str, Any])

  L 218: get_supported_act_dtypes(cls)
         ‚Üí List[torch.dtype]

  L 226: get_min_capability(cls)
         ‚Üí int

  L 235: get_name(self)
         ‚Üí str

  L 239: get_config_filenames(cls)
         ‚Üí List[str]

  L 246: from_config(cls, config: Dict[str, Any])
         ‚Üí W8A8Int8Config

  L 249: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional[QuantizeMethodBase]

  L 296: is_layer_skipped(self, prefix: str, fused_mapping: Mapping[str, List[str]])

  L 327: get_scaled_act_names(self)
         ‚Üí List[str]


CLASS: W8A8Int8LinearMethod
----------------------------------------
  L 333: __init__(self, quantization_config: W8A8Int8Config)

  L 336: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 347: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)

  L 378: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])


CLASS: W8A8Int8MoEMethod
----------------------------------------
  L 412: __init__(self, quant_config: W8A8Int8Config)

  L 415: create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)

  L 469: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 486: apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/radix_attention.py
Functions: 2
============================================================


CLASS: RadixAttention
----------------------------------------
  L  44: __init__(self, num_heads: int, head_dim: int, scaling: float, num_kv_heads: int, layer_id: int, logit_cap: float, v_head_dim: int, sliding_window_size: int, is_cross_attention: bool, pos_encoding_mode: str, logit_capping_method: str, quant_config: Optional[QuantizationConfig], attn_type: AttentionType, use_irope: bool, prefix: str)

  L  90: forward(self, q, k, v, forward_batch: ForwardBatch, save_kv_cache: bool)


============================================================
FILE: python/sglang/srt/layers/rotary_embedding.py
Functions: 34
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 646: def yarn_get_mscale(scale: float, mscale: float)
         ‚Üí float

  L1654: def get_rope(head_size: int,
        rotary_dim: int,
        max_position: int,
        base: int,
        is_neox_style: bool,
        rope_scaling: Optional[Dict[str,
        Any]],
        dtype: Optional[torch.dtype],
        partial_rotary_factor: float,
        dual_chunk_attention_config: Optional[Dict[str,
        Any]])
         ‚Üí RotaryEmbedding

  L1872: def rotate_half(x)
         üìù Rotates half the hidden dims of the input.

  L1879: def apply_rotary_pos_emb(q: torch.Tensor,
        k: torch.Tensor,
        cos: torch.Tensor,
        sin: torch.Tensor,
        unsqueeze_dim)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L1902: def get_rope_cpu(head_size: int,
        rotary_dim: int,
        max_position: int,
        base: int,
        is_neox_style: bool,
        rope_scaling: Optional[Dict[str,
        Any]],
        dtype: Optional[torch.dtype],
        partial_rotary_factor: float,
        device: Optional[str])
         ‚Üí RotaryEmbedding

  L1974: def get_rope_wrapper(head_size: int,
        rotary_dim: int,
        max_position: int,
        base: int,
        is_neox_style: bool,
        rope_scaling: Optional[Dict[str,
        Any]],
        dtype: Optional[torch.dtype],
        partial_rotary_factor: float,
        device: Optional[str])


CLASS: DeepseekScalingRotaryEmbedding
----------------------------------------
  L 658: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype)
         ‚Üí None

  L 737: forward_native(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù PyTorch-native implementation equivalent to forward().

  L 778: forward_npu(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 818: forward_cpu(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: DualChunkRotaryEmbedding
----------------------------------------
  L1458: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, chunk_size: int, local_size: int)
         ‚Üí None

  L1560: forward(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L1643: extra_repr(self)
         ‚Üí str


CLASS: DynamicNTKAlphaRotaryEmbedding
----------------------------------------
  L 950: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_alpha: float, dtype: torch.dtype)
         ‚Üí None


CLASS: DynamicNTKScalingRotaryEmbedding
----------------------------------------
  L 357: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype)
         ‚Üí None


CLASS: LinearScalingRotaryEmbedding
----------------------------------------
  L 293: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factors: Union[List[float], float], dtype: torch.dtype)
         ‚Üí None

  L 347: scaling_factor_to_offset(self)
         ‚Üí Dict[float, int]


CLASS: Llama3RotaryEmbedding
----------------------------------------
  L 836: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, scaling_factor: float, low_freq_factor: float, high_freq_factor: float, orig_max_position: int)
         ‚Üí None


CLASS: Llama4VisionRotaryEmbedding
----------------------------------------
  L 883: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype)

  L 926: forward(self, query: torch.Tensor, key: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: MRotaryEmbedding
----------------------------------------
  L 984: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, mrope_section: Optional[List[int]])
         ‚Üí None

  L1033: forward(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù PyTorch-native implementation equivalent to forward().

  L1082: get_rope_index(spatial_merge_size: int, image_token_id: int, video_token_id: int, vision_start_token_id: int, model_type: str, tokens_per_second: Optional[int], input_ids: Optional[torch.LongTensor], image_grid_thw: Optional[torch.LongTensor], video_grid_thw: Optional[torch.LongTensor], second_per_grid_ts: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L1240: get_rope_index_glm4v(input_ids: torch.Tensor, hf_config: Any, image_grid_thw: Union[list[list[int]], torch.Tensor], video_grid_thw: Union[list[list[int]], torch.Tensor], attention_mask: torch.Tensor)
         ‚Üí tuple[torch.Tensor, torch.Tensor]
         üìù Get mrope input positions and delta value for GLM4V.

  L1437: get_next_input_positions(mrope_position_delta: int, context_len: int, seq_len: int)
         ‚Üí torch.Tensor


CLASS: Phi3LongRoPEScaledRotaryEmbedding
----------------------------------------
  L 513: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, original_max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, short_factor: List[float], long_factor: List[float], short_mscale: Optional[float], long_mscale: Optional[float])

  L 604: forward(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: RotaryEmbedding
----------------------------------------
  L  82: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype)
         ‚Üí None

  L 139: forward_native(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù A PyTorch-native implementation of forward().

  L 169: forward_npu(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù A PyTorch-npu implementation of forward().

  L 199: forward_cpu(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 219: forward_cuda(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor], fused_set_kv_buffer_arg)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 257: extra_repr(self)
         ‚Üí str


CLASS: YaRNScalingRotaryEmbedding
----------------------------------------
  L 444: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype)
         ‚Üí None


============================================================
FILE: python/sglang/srt/layers/sampler.py
Functions: 8
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 157: def top_k_top_p_min_p_sampling_from_probs_torch(probs: torch.Tensor,
        top_ks: torch.Tensor,
        top_ps: torch.Tensor,
        min_ps: torch.Tensor,
        need_min_p_sampling: bool)
         üìù A top-k, top-p and min-p sampling implementation with native pytorch o

  L 184: def sampling_from_probs_torch(probs: torch.Tensor)
         üìù A sampling implementation with native pytorch operations, without

  L 192: def top_p_normalize_probs_torch(probs: torch.Tensor, top_ps: torch.Tensor)

  L 204: def get_top_logprobs(logprobs: torch.Tensor, top_logprobs_nums: List[int])

  L 218: def get_token_ids_logprobs(logprobs: torch.Tensor,
        token_ids_logprobs: List[List[int]])

  L 232: def apply_custom_logit_processor(logits: torch.Tensor,
        sampling_batch_info: SamplingBatchInfo,
        num_tokens_in_batch: int)
         üìù Apply custom logit processors to the logits.


CLASS: Sampler
----------------------------------------
  L  33: __init__(self)

  L  41: forward(self, logits_output: LogitsProcessorOutput, sampling_info: SamplingBatchInfo, return_logprob: bool, top_logprobs_nums: List[int], token_ids_logprobs: List[List[int]])
         üìù Run a sampler & compute logprobs and update logits_output accordingly.


============================================================
FILE: python/sglang/srt/layers/torchao_utils.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  15: def get_gemlite_cache_path()
         ‚Üí str

  L  19: def save_gemlite_cache(print_error: bool)
         ‚Üí bool

  L  31: def proj_filter(module: torch.nn.Module, fqn: str)
         üìù Filter function for quantizing projection layers.

  L  39: def apply_torchao_config_to_model(model: torch.nn.Module,
        torchao_config: str,
        filter_fn: Optional[Callable])
         üìù Quantize a modelwith torchao quantization specified by torchao_config


============================================================
FILE: python/sglang/srt/layers/utils.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  10: def get_layer_id(weight_name)


CLASS: PPMissingLayer
----------------------------------------
  L  25: __init__(self)

  L  29: forward(self)
         üìù Return the first arg from args or the first value from kwargs.


============================================================
FILE: python/sglang/srt/layers/vocab_parallel_embedding.py
Functions: 20
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  44: def pad_vocab_size(vocab_size: int, pad_to: int)
         ‚Üí int
         üìù Pad the vocab size to the given value.

  L  49: def vocab_range_from_per_partition_vocab_size(per_partition_vocab_size: int,
        rank: int,
        offset: int)
         ‚Üí Sequence[int]

  L  57: def vocab_range_from_global_vocab_size(global_vocab_size: int,
        rank: int,
        world_size: int,
        offset: int)
         ‚Üí Sequence[int]

  L 126: def get_masked_input_and_mask(input_: torch.Tensor,
        org_vocab_start_index: int,
        org_vocab_end_index: int,
        num_org_vocab_padding: int,
        added_vocab_start_index: int,
        added_vocab_end_index: int)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         @torch.compile(dynamic=True, backend=get_compiler_backend())


CLASS: ParallelLMHead
----------------------------------------
  L 514: __init__(self, num_embeddings: int, embedding_dim: int)

  L 560: tie_weights(self, embed_tokens: VocabParallelEmbedding)
         üìù Tie the weights with word embeddings.

  L 569: forward(self, input_)


CLASS: VocabParallelEmbedding
----------------------------------------
  L 192: __init__(self, num_embeddings: int, embedding_dim: int)

  L 346: get_sharded_to_full_mapping(self)
         ‚Üí Optional[List[int]]
         üìù Get a mapping that can be used to reindex the gathered

  L 411: weight_loader(self, param: Parameter, loaded_weight: torch.Tensor)

  L 462: forward(self, input_)

  L 488: extra_repr(self)
         ‚Üí str


CLASS: VocabParallelEmbeddingShardIndices
----------------------------------------
  L  81: num_org_elements(self)
         ‚Üí int

  L  85: num_added_elements(self)
         ‚Üí int

  L  89: num_org_elements_padded(self)
         ‚Üí int

  L  93: num_added_elements_padded(self)
         ‚Üí int

  L  97: num_org_vocab_padding(self)
         ‚Üí int

  L 101: num_added_vocab_padding(self)
         ‚Üí int

  L 105: num_elements_padded(self)
         ‚Üí int

  L 108: __post_init__(self)


============================================================
FILE: python/sglang/srt/lora/backend/base_backend.py
Functions: 7
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 100: def get_backend_from_name(name: str)
         ‚Üí BaseLoRABackend
         üìù Get corresponding backend class from backend's name


CLASS: BaseLoRABackend
----------------------------------------
  L  17: __init__(self, name: str, batch_info: LoRABatchInfo)

  L  21: run_lora_a_sgemm(self, x: torch.Tensor, weights: torch.Tensor)
         ‚Üí torch.Tensor
         üìù Run segment Gemm of lora a modules with current backend.

  L  37: run_lora_b_sgemm(self, x: torch.Tensor, weights: torch.Tensor)
         ‚Üí torch.Tensor
         üìù Run segment Gemm of lora b modules with current backend.

  L  52: run_qkv_lora(self, x: torch.Tensor, qkv_lora_a: torch.Tensor, qkv_lora_b: Union[torch.Tensor, Tuple[torch.Tensor]])
         ‚Üí torch.Tensor
         üìù Run the lora pass for QKV Layer.

  L  75: run_gate_up_lora(self, x: torch.Tensor, gate_up_lora_a: torch.Tensor, gate_up_lora_b: Union[torch.Tensor, Tuple[torch.Tensor]])
         ‚Üí torch.Tensor
         üìù Run the lora pass for gate_up_proj, usually attached to MergedColumnPa

  L  96: set_batch_info(self, batch_info: LoRABatchInfo)


============================================================
FILE: python/sglang/srt/lora/backend/triton_backend.py
Functions: 5
============================================================


CLASS: TritonLoRABackend
----------------------------------------
  L  15: __init__(self, name: str, batch_info: LoRABatchInfo)

  L  18: run_lora_a_sgemm(self, x: torch.Tensor, weights: torch.Tensor)
         ‚Üí torch.Tensor

  L  23: run_lora_b_sgemm(self, x: torch.Tensor, weights: torch.Tensor, base_output: torch.Tensor)
         ‚Üí torch.Tensor

  L  33: run_qkv_lora(self, x: torch.Tensor, qkv_lora_a: torch.Tensor, qkv_lora_b: torch.Tensor, output_offset: torch.Tensor, max_qkv_out_dim: int, base_output: torch.Tensor)
         ‚Üí torch.Tensor

  L  61: run_gate_up_lora(self, x: torch.Tensor, gate_up_lora_a: torch.Tensor, gate_up_lora_b: torch.Tensor, base_output: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/lora/layers.py
Functions: 29
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 305: def get_lora_layer(layer: nn.Module, lora_backend: BaseLoRABackend)
         ‚Üí BaseLayerWithLoRA


CLASS: BaseLayerWithLoRA
----------------------------------------
  L  21: __init__(self, base_layer: nn.Module, lora_backend: BaseLoRABackend)

  L  31: forward(self, x: torch.Tensor)

  L  34: set_lora_info(self)

  L  37: slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int)

  L  40: slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int)


CLASS: ColumnParallelLinearWithLoRA
----------------------------------------
  L  63: __init__(self, base_layer: ColumnParallelLinear, lora_backend: BaseLoRABackend)
         ‚Üí None

  L  70: set_lora_info(self, A_buffer: torch.Tensor, B_buffer: torch.Tensor)

  L  79: apply_lora(self, base_output: torch.Tensor, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  88: forward(self, input_: torch.Tensor)

  L 105: slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int)

  L 108: slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int)


CLASS: MergedColumnParallelLinearWithLoRA
----------------------------------------
  L 117: __init__(self, base_layer: MergedColumnParallelLinear, lora_backend: BaseLoRABackend)
         ‚Üí None

  L 124: set_lora_info(self, A_buffer: torch.Tensor, B_buffer: torch.Tensor)

  L 133: apply_lora(self, base_output: torch.Tensor, x: torch.Tensor)
         ‚Üí torch.Tensor

  L 142: slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int)

  L 145: slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int)


CLASS: QKVParallelLinearWithLoRA
----------------------------------------
  L 161: __init__(self, base_layer: QKVParallelLinear, lora_backend: BaseLoRABackend)
         ‚Üí None

  L 183: set_lora_info(self, A_buffer_qkv: torch.Tensor, B_buffer_qkv: torch.Tensor)

  L 192: apply_lora(self, base_output: torch.Tensor, x: torch.Tensor)
         ‚Üí torch.Tensor

  L 203: slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int)

  L 206: slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int)
         ‚Üí torch.Tensor


CLASS: RowParallelLinearWithLoRA
----------------------------------------
  L 235: __init__(self, base_layer: RowParallelLinear, lora_backend: BaseLoRABackend)
         ‚Üí None

  L 242: set_lora_info(self, A_buffer: torch.Tensor, B_buffer: torch.Tensor)

  L 247: apply_lora(self, base_output: torch.Tensor, x: torch.Tensor)
         ‚Üí torch.Tensor

  L 256: forward(self, input_: torch.Tensor, skip_all_reduce)

  L 294: slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int)

  L 301: slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int)


CLASS: VocabParallelEmbeddingWithLoRA
----------------------------------------
  L  53: __init__(self, base_layer: VocabParallelEmbedding, lora_backend: BaseLoRABackend)
         ‚Üí None


============================================================
FILE: python/sglang/srt/lora/lora.py
Functions: 5
============================================================


CLASS: LoRAAdapter
----------------------------------------
  L  48: __init__(self, uid: str, config: LoRAConfig, base_hf_config: AutoConfig, load_config: LoadConfig, lora_backend: BaseLoRABackend)

  L  75: initialize_weights(self)

  L  97: normalize_qkv_proj(self, weight_names: List[str], weights: Dict[str, torch.Tensor])

  L 150: normalize_gate_up_proj(self, weight_names: List[str], weights: Dict[str, torch.Tensor])


CLASS: LoRALayer
----------------------------------------
  L  38: __init__(self, config: LoRAConfig, base_hf_config: AutoConfig)


============================================================
FILE: python/sglang/srt/lora/lora_config.py
Functions: 2
============================================================


CLASS: LoRAConfig
----------------------------------------
  L  22: __init__(self, path: str)
         ‚Üí None

  L  37: get_lora_config(self, dummy)


============================================================
FILE: python/sglang/srt/lora/lora_manager.py
Functions: 16
============================================================


CLASS: LoRAManager
----------------------------------------
  L  46: __init__(self, base_model: torch.nn.Module, base_hf_config: AutoConfig, max_loras_per_batch: int, load_config: LoadConfig, dtype: torch.dtype, lora_backend: str, tp_size: int, tp_rank: int, max_lora_rank: Optional[int], target_modules: Optional[Iterable[str]], lora_paths: Optional[List[LoRARef]])

  L  81: init_cuda_graph_batch_info(self, max_bs_in_cuda_graph: int)

  L 109: create_lora_update_result(self, success: bool, error_message: str)
         ‚Üí LoRAUpdateResult

  L 121: load_lora_adapter(self, lora_ref: LoRARef)
         ‚Üí LoRAUpdateResult
         üìù Load a single LoRA adapter from the specified path.

  L 155: validate_new_adapter(self, lora_config: LoRAConfig, lora_ref: LoRARef)
         üìù Validate if an adapter can be loaded into the current LoRA memory pool

  L 178: unload_lora_adapter(self, lora_ref: LoRARef)
         ‚Üí LoRAUpdateResult
         üìù Unload LoRA adapters by their names. This will remove the adapters fro

  L 203: validate_lora_batch(self, lora_ids: set[str])
         ‚Üí bool
         üìù Validate if the LoRA IDs in the batch can be loaded into the current L

  L 234: prepare_lora_batch(self, forward_batch: ForwardBatch)

  L 347: update_lora_info(self)
         üìù Update all LoRA modules to associate them with the latest memory buffe

  L 369: init_state(self, max_lora_rank: Optional[int], target_modules: Optional[Iterable[str]], lora_paths: Optional[List[LoRARef]])
         üìù Initialize the internal (mutable) state of the LoRAManager.

  L 395: init_lora_adapters(self, lora_paths: Optional[List[LoRARef]])

  L 416: init_lora_shapes(self, max_lora_rank: Optional[int], target_modules: Optional[Iterable[str]])
         üìù Infer LoRA target modules and max_lora_rank from loaded adapters if no

  L 463: load_lora_weights(self, lora_ref: LoRARef)
         üìù Load the weights of a LoRA adapter to CPU memory and conducts post-loa

  L 477: init_memory_pool(self)
         üìù (Re)initialize the LoRA memory pool based on the current configuration

  L 490: set_lora_module(self, module_name, module)

  L 495: init_lora_modules(self)


============================================================
FILE: python/sglang/srt/lora/lora_registry.py
Functions: 9
============================================================


CLASS: LoRARef
----------------------------------------
  L  40: __post_init__(self)

  L  44: __str__(self)
         ‚Üí str


CLASS: LoRARegistry
----------------------------------------
  L  62: __init__(self, lora_paths: Optional[List[LoRARef]])

  L  84: register(self, lora_ref: LoRARef)
         üìù Register a new LoRARef object in the registry.

  L  94: unregister(self, lora_name: str)
         ‚Üí str
         üìù Unregister a LoRARef object from the registry and returns the removed 

  L 111: acquire(self, lora_name: Union[str, List[str]])
         ‚Üí Union[str, List[str]]
         üìù Queries registry for LoRA IDs based on LoRA names and start tracking t

  L 151: release(self, lora_id: Union[str, List[str]])
         üìù Decrements the usage counter for a LoRA adapter, indicating that it is

  L 170: wait_for_unload(self, lora_id: str)
         üìù Waits until the usage counter for a LoRA adapter reaches zero, indicat

  L 203: num_registered_loras(self)
         ‚Üí int
         üìù Returns the total number of LoRA adapters currently registered.


============================================================
FILE: python/sglang/srt/lora/mem_pool.py
Functions: 11
============================================================


CLASS: EmptySlot
----------------------------------------
  L  32: __repr__(self)

  L  35: __new__(cls)


CLASS: LoRAMemoryPool
----------------------------------------
  L  47: __init__(self, base_hf_config: AutoConfig, max_loras_per_batch: int, dtype: torch.dtype, tp_size: int, tp_rank: int, max_lora_rank: int, target_modules: Set[str], base_model: torch.nn.Module)

  L  87: can_support(self, config: Union[LoRAConfig, Iterable[LoRAConfig]])
         ‚Üí bool
         üìù Check if the memory pool can support the given LoRA adapters.

  L 106: get_lora_A_shape(self, module_name: str, base_model: torch.nn.Module, max_lora_dim: int)
         ‚Üí Tuple[int]
         üìù Given a module_name (might be a stacked name), return the hidden dims 

  L 122: get_lora_B_shape(self, module_name: str, base_model: torch.nn.Module, max_lora_dim: int)
         ‚Üí Tuple[int]
         üìù Given a module_name (might be a stacked name), return the hidden dims 

  L 137: init_buffers(self, base_model: torch.nn.Module)

  L 170: prepare_lora_batch(self, cur_uids: Set[Optional[str]], lora_adapters: Dict[str, LoRAAdapter], lora_modules: List[Dict[str, BaseLayerWithLoRA]], lora_refs: Dict[str, LoRARef])

  L 214: load_lora_weight_to_buffer(self, uid: str, buffer_id: int, lora_adapter: LoRAAdapter, lora_modules: List[Dict[str, BaseLayerWithLoRA]])

  L 286: get_tensor(self, target_module: str, layer_id: int, lora_type: LoRAType)
         ‚Üí torch.Tensor

  L 294: get_buffer_id(self, lora_uid: str)


============================================================
FILE: python/sglang/srt/lora/triton_ops/gate_up_lora_b.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 126: def gate_up_lora_b_fwd(x: torch.Tensor,
        gate_up_lora_b: torch.Tensor,
        batch_info: LoRABatchInfo,
        output_dim: int,
        base_output: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/lora/triton_ops/qkv_lora_b.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 127: def qkv_lora_b_fwd(x: torch.Tensor,
        qkv_lora_b: torch.Tensor,
        batch_info: LoRABatchInfo,
        output_offset: torch.Tensor,
        max_qkv_out_dim: int,
        base_output: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/lora/triton_ops/sgemm_lora_a.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 114: def sgemm_lora_a_fwd(x: torch.Tensor,
        weights: torch.Tensor,
        batch_info: LoRABatchInfo,
        stack_num: int)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/lora/triton_ops/sgemm_lora_b.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 118: def sgemm_lora_b_fwd(x: torch.Tensor,
        weights: torch.Tensor,
        batch_info: LoRABatchInfo,
        base_output: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/lora/utils.py
Functions: 5
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  40: def get_layer_id(name: str)
         ‚Üí int
         üìù Extract integer id of layer from its name in string.

  L  50: def get_hidden_dim(module_name: str,
        config: AutoConfig,
        base_model: torch.nn.Module)
         ‚Üí Tuple[int]
         üìù Given a module_name (might be a stacked name), return the hidden dims 

  L  87: def get_normalized_target_modules(target_modules: Iterable[str])
         ‚Üí set[str]
         üìù Mapping a list of target module name to names of the normalized LoRA w

  L 108: def get_stacked_multiply(module_name: str)
         ‚Üí int
         üìù Mapping a lora module name to its magnification at output dimension

  L 119: def get_target_module_name(full_module_name: str, target_modules: Set[str])
         ‚Üí str
         üìù Get the target module name in target_modules that can match full_modul


============================================================
FILE: python/sglang/srt/managers/cache_controller.py
Functions: 40
============================================================


CLASS: CacheOperation
----------------------------------------
  L  84: __init__(self, host_indices: torch.Tensor, device_indices: torch.Tensor, node_id: int, priority: Optional[int])

  L 101: merge(self, other: 'CacheOperation')
         ‚Üí None

  L 108: split(self, factor)
         ‚Üí List['CacheOperation']

  L 129: __lt__(self, other: 'CacheOperation')


CLASS: HiCacheController
----------------------------------------
  L 233: __init__(self, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, mem_pool_host: HostKVCache, page_size: int, tp_group: torch.distributed.ProcessGroup, load_cache_event: threading.Event, write_policy: str, io_backend: str, storage_backend: Optional[str], prefetch_threshold: int, model_name: Optional[str], storage_backend_extra_config: Optional[str])

  L 421: reset(self)

  L 460: write(self, device_indices: torch.Tensor, priority: Optional[int], node_id: int)
         ‚Üí Optional[torch.Tensor]
         üìù Back up KV caches from device memory to host memory.

  L 479: load(self, host_indices: torch.Tensor, priority: Optional[int], node_id: int)
         ‚Üí Optional[torch.Tensor]
         üìù Load KV caches from host memory to device memory.

  L 499: move_indices(self, host_indices, device_indices)

  L 510: write_thread_func_direct(self)
         üìù Directly write through KV caches to host memory without buffering.

  L 534: load_thread_func_layer_by_layer(self)
         üìù Load KV caches from host memory to device memory layer by layer.

  L 577: evict_device(self, device_indices: torch.Tensor, host_indices: torch.Tensor)
         ‚Üí int

  L 589: evict_host(self, host_indices: torch.Tensor, backup_only: bool)
         ‚Üí int

  L 601: prefetch(self, request_id: str, host_indices: torch.Tensor, new_input_tokens: List[int], last_hash: Optional[str])
         ‚Üí PrefetchOperation
         üìù Prefetch KV caches from storage backend to host memory.

  L 617: terminate_prefetch(self, operation)

  L 709: is_mooncake_backend(self)

  L 712: prefetch_io_aux_func(self)
         üìù Auxiliary function conducting IO operations for prefetching.

  L 731: prefetch_rate_limit_check(self)
         ‚Üí bool
         üìù Rate limit the prefetching operations to avoid overwhelming the storag

  L 762: prefetch_thread_func(self)
         üìù Manage prefetching operations from storage backend to host memory.

  L 816: write_storage(self, host_indices: torch.Tensor, token_ids: List[int], hash_value: Optional[List[str]])
         ‚Üí int
         üìù Write KV caches from host memory to storage backend.

  L 889: backup_thread_func(self)
         üìù Manage backup operations from host memory to storage backend.


CLASS: LayerDoneCounter
----------------------------------------
  L  46: __init__(self, num_layers)

  L  55: next_producer(self)

  L  58: update_producer(self)

  L  62: set_consumer(self, index)

  L  65: increment(self)

  L  70: wait_until(self, threshold)

  L  75: reset(self)


CLASS: PrefetchOperation
----------------------------------------
  L 200: __init__(self, request_id: str, host_indices: torch.Tensor, token_ids: List[int], last_hash: Optional[str])

  L 216: increment(self, num_tokens: int)

  L 223: mark_done(self)

  L 227: is_done(self)
         ‚Üí bool


CLASS: StorageOperation
----------------------------------------
  L 179: __init__(self, host_indices: torch.Tensor, token_ids: List[int], last_hash: Optional[str], hash_value: Optional[List[str]])

  L 195: __lt__(self, other: 'StorageOperation')


CLASS: TransferBuffer
----------------------------------------
  L 138: __init__(self, stop_event, buffer_count: int, max_buffer_size: int)
         ‚Üí None

  L 146: full(self)
         ‚Üí bool

  L 149: empty(self)
         ‚Üí bool

  L 152: put(self, item, block, timeout)
         ‚Üí None

  L 164: get(self, block, timeout)
         ‚Üí Optional[CacheOperation]

  L 172: clear(self)


============================================================
FILE: python/sglang/srt/managers/data_parallel_controller.py
Functions: 11
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 341: def run_data_parallel_controller_process(server_args: ServerArgs,
        port_args: PortArgs,
        pipe_writer)


CLASS: DataParallelController
----------------------------------------
  L  67: __init__(self, server_args: ServerArgs, port_args: PortArgs, dp_balance_meta: DPBalanceMeta)
         ‚Üí None

  L 124: launch_dp_schedulers(self, server_args, port_args)

  L 164: launch_tensor_parallel_group_thread(self, server_args: ServerArgs, port_args: PortArgs, base_gpu_id: int, dp_rank: int, ready_event: threading.Event)

  L 180: launch_dp_attention_schedulers(self, server_args, port_args)

  L 187: launch_tensor_parallel_group(self, server_args: ServerArgs, port_args: PortArgs, base_gpu_id: int, dp_rank: int)

  L 269: round_robin_scheduler(self, req: Req)

  L 286: shortest_queue_scheduler(self, input_requests)

  L 289: minimum_tokens_scheduler(self, req)

  L 316: event_loop(self)


CLASS: LoadBalanceMethod
----------------------------------------
  L  56: from_str(cls, method: str)


============================================================
FILE: python/sglang/srt/managers/detokenizer_manager.py
Functions: 10
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 277: def run_detokenizer_process(server_args: ServerArgs, port_args: PortArgs)


CLASS: DetokenizerManager
----------------------------------------
  L  73: __init__(self, server_args: ServerArgs, port_args: PortArgs)

  L 111: event_loop(self)
         üìù The event loop that handles requests

  L 119: trim_matched_stop(self, output: Union[str, List[int]], finished_reason: Dict, no_stop_trim: bool)

  L 145: handle_batch_embedding_out(self, recv_obj: BatchEmbeddingOut)

  L 149: handle_batch_token_id_out(self, recv_obj: BatchTokenIDOut)

  L 248: handle_multimodal_decode_req(self, recv_obj: BatchMultimodalDecodeReq)

  L 259: handle_freeze_gc_req(self, recv_req: FreezeGCReq)


CLASS: LimitedCapacityDict
----------------------------------------
  L 265: __init__(self, capacity: int)

  L 269: __setitem__(self, key, value)


============================================================
FILE: python/sglang/srt/managers/io_struct.py
Functions: 16
============================================================


CLASS: BatchTokenizedEmbeddingReqInput
----------------------------------------
  L 691: __len__(self)

  L 694: __getitem__(self, i)

  L 697: __iter__(self)


CLASS: BatchTokenizedGenerateReqInput
----------------------------------------
  L 541: __len__(self)

  L 544: __getitem__(self, i)

  L 547: __iter__(self)


CLASS: EmbeddingReqInput
----------------------------------------
  L 584: normalize_batch_and_arguments(self)

  L 635: regenerate_rid(self)

  L 639: contains_mm_input(self)
         ‚Üí bool

  L 646: __getitem__(self, i)


CLASS: GenerateReqInput
----------------------------------------
  L 131: contains_mm_input(self)
         ‚Üí bool

  L 138: normalize_batch_and_arguments(self)
         üìù Normalize the batch size and arguments for the request.

  L 432: regenerate_rid(self)
         üìù Generate a new request ID and return it.

  L 437: __getitem__(self, i)


CLASS: LoadLoRAAdapterReqInput
----------------------------------------
  L1143: to_ref(self)
         ‚Üí LoRARef


CLASS: UnloadLoRAAdapterReqInput
----------------------------------------
  L1159: to_ref(self)
         ‚Üí LoRARef


============================================================
FILE: python/sglang/srt/managers/mm_utils.py
Functions: 20
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 288: def init_embedding_cache(max_size: int)

  L 293: def get_embedding_hash(embedding_items: List[MultimodalDataItem])
         ‚Üí int

  L 298: def get_embedding_chunk(embedding: torch.Tensor,
        extend_prefix_len: int,
        extend_seq_len: int,
        items_offset: List[Tuple[int,
        int]])
         ‚Üí Tuple[torch.Tensor, int, int]
         üìù Extract a chunk of embeddings based on the specified prefix length, se

  L 447: def get_embedding_and_mask(data_embedding_func: Callable[[List[MultimodalDataItem]],
        torch.Tensor],
        embedding_items: List[MultimodalDataItem],
        placeholder_tensor: torch.Tensor,
        input_ids: torch.Tensor,
        items_size: List[int],
        prefix_length: List[int],
        extend_length: List[int],
        items_offset_list: List[List[Tuple[int,
        int]]])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù Generate multimodal embeddings and create a mask for identifying their

  L 495: def embed_mm_inputs(mm_inputs_list: List[MultimodalInputs],
        extend_prefix_lens: List[int],
        extend_seq_lens: List[int],
        input_ids: torch.Tensor,
        input_embedding: nn.Embedding,
        multimodal_model: nn.Module,
        data_embedding_func_mapping: Dict[Modality,
        Callable[[List[MultimodalDataItem]],
        torch.Tensor]],
        placeholder_tokens: dict[Modality,
        List[int]])
         ‚Üí Optional[torch.Tensor]
         üìù Embed multimodal inputs and integrate them with text token embeddings.

  L 599: def general_mm_embed_routine(input_ids: torch.Tensor,
        forward_batch: ForwardBatch,
        language_model: nn.Module,
        multimodal_model: Optional[nn.Module],
        data_embedding_funcs: Dict[Modality,
        Callable[[List[MultimodalDataItem]],
        torch.Tensor]],
        placeholder_tokens: Optional[dict[Modality,
        List[int]]])
         ‚Üí torch.Tensor
         üìù Process multimodal inputs and forward through language model.

  L 668: def get_multimodal_data_bounds(input_ids: torch.Tensor,
        pad_values: List[int],
        token_pairs: List[Tuple[int,
        int]])
         ‚Üí torch.Tensor
         üìù Returns a tensor indicating the bounds of multimodal data (images, vid

  L 728: def data_hash(data)
         ‚Üí int

  L 733: def tensor_hash(tensor_list)
         ‚Üí int
         üìù hash a tensor or a tensor list

  L 759: def hash_feature(f)


CLASS: MultiModalityDataPaddingPattern
----------------------------------------
  L 162: pad_input_tokens(self, input_ids: List[int], mm_inputs: MultimodalInputs)
         ‚Üí List[int]
         üìù Pad the input ids sequence containing data tokens, and replace them wi


CLASS: MultiModalityDataPaddingPatternMultimodalTokens
----------------------------------------
  L 251: pad_input_tokens(self, input_ids: List[int], mm_inputs: MultimodalInputs)
         ‚Üí List[int]
         üìù Replaces multimodal tokens in input_ids with corresponding pad_values 


CLASS: MultiModalityDataPaddingPatternTokenPairs
----------------------------------------
  L 179: __init__(self, data_token_pairs: Optional[List[Tuple[int, int]]], data_start_token_ids: Optional[List[int]])
         ‚Üí None
         üìù Args:

  L 195: pad_input_tokens(self, input_ids: List[int], mm_inputs: MultimodalInputs)
         ‚Üí List[int]
         üìù This function will replace the data-tokens in between with pad_values 


CLASS: TransportProxyTensor
----------------------------------------
  L  43: __new__(cls, data: torch.Tensor, name: Optional[str], fields: Optional[Dict[str, Any]], transport_mode: TensorTransportMode)

  L  68: __getstate__(self)
         üìù Called during pickling. Implements the serialization logic.

  L 104: __setstate__(self, state: Dict[str, Any])
         üìù Called during unpickling. Implements the deserialization logic.

  L 142: name(self)
         ‚Üí Optional[str]

  L 146: fields(self)
         ‚Üí Dict[str, Any]

  L 150: transport_mode(self)
         ‚Üí TensorTransportMode


============================================================
FILE: python/sglang/srt/managers/multimodal_processor.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  15: def import_processors()

  L  39: def get_mm_processor(hf_config,
        server_args: ServerArgs,
        processor,
        transport_mode)
         ‚Üí BaseMultimodalProcessor


============================================================
FILE: python/sglang/srt/managers/schedule_batch.py
Functions: 72
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L1927: def write_req_to_token_pool_triton(req_to_token_ptr,
        req_pool_indices,
        pre_lens,
        seq_lens,
        extend_lens,
        out_cache_loc,
        req_to_token_ptr_stride: tl.constexpr)
         @triton.jit

  L1963: def get_last_loc(req_to_token: torch.Tensor,
        req_pool_indices_tensor: torch.Tensor,
        prefix_lens_tensor: torch.Tensor)
         ‚Üí torch.Tensor

  L1979: def get_last_loc_torch(req_to_token: torch.Tensor,
        req_pool_indices_tensor: torch.Tensor,
        prefix_lens_tensor: torch.Tensor)
         ‚Üí torch.Tensor

  L1992: def get_last_loc_kernel(req_to_token,
        req_pool_indices_tensor,
        prefix_lens_tensor,
        result,
        num_tokens,
        req_to_token_stride,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L2015: def get_last_loc_triton(req_to_token: torch.Tensor,
        req_pool_indices_tensor: torch.Tensor,
        prefix_lens_tensor: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: BaseFinishReason
----------------------------------------
  L 120: __init__(self, is_error: bool)

  L 123: to_json(self)


CLASS: FINISH_ABORT
----------------------------------------
  L 164: __init__(self, message, status_code, err_type)

  L 170: to_json(self)


CLASS: FINISH_LENGTH
----------------------------------------
  L 152: __init__(self, length: int)

  L 156: to_json(self)


CLASS: FINISH_MATCHED_STR
----------------------------------------
  L 140: __init__(self, matched: str)

  L 144: to_json(self)


CLASS: FINISH_MATCHED_TOKEN
----------------------------------------
  L 128: __init__(self, matched: Union[int, List[int]])

  L 132: to_json(self)


CLASS: Modality
----------------------------------------
  L 186: from_str(modality_str: str)

  L 195: all()


CLASS: MultimodalDataItem
----------------------------------------
  L 223: __getattr__(self, name: str)

  L 234: __setitem__(self, key: str, value: Any)

  L 240: set(self, key: str, value: Any)

  L 244: is_empty_list(l)

  L 249: set_pad_value(self)
         üìù Set the pad value after first hashing the data

  L 264: is_modality(self, modality: Modality)
         ‚Üí bool

  L 267: is_audio(self)

  L 270: is_image(self)

  L 273: is_video(self)

  L 276: is_valid(self)
         ‚Üí bool

  L 279: validate(self)

  L 284: from_dict(obj: dict)

  L 293: merge(self, other)


CLASS: MultimodalInputs
----------------------------------------
  L 329: from_dict(obj: dict)

  L 358: contains_image_inputs(self)
         ‚Üí bool

  L 361: contains_video_inputs(self)
         ‚Üí bool

  L 364: contains_audio_inputs(self)
         ‚Üí bool

  L 367: contains_mm_input(self)
         ‚Üí bool

  L 370: merge(self, other: MultimodalInputs)
         üìù merge image inputs when requests are being merged


CLASS: Req
----------------------------------------
  L 414: __init__(self, rid: str, origin_input_text: str, origin_input_ids: List[int], sampling_params: SamplingParams, return_logprob: bool, top_logprobs_num: int, token_ids_logprob: List[int], stream: bool, origin_input_ids_unpadded: Optional[Tuple[int]], lora_id: Optional[str], input_embeds: Optional[List[List[float]]], token_type_ids: List[int], session_id: Optional[str], custom_logit_processor: Optional[str], return_hidden_states: bool, eos_token_ids: Optional[Set[int]], bootstrap_host: Optional[str], bootstrap_port: Optional[int], bootstrap_room: Optional[int], data_parallel_rank: Optional[int], vocab_size: Optional[int])

  L 619: seqlen(self)

  L 622: extend_image_inputs(self, image_inputs)

  L 628: finished(self)
         ‚Üí bool

  L 632: init_next_round_input(self, tree_cache: Optional[BasePrefixCache])

  L 660: adjust_max_prefix_ids(self)

  L 679: init_incremental_detokenize(self)

  L 691: check_finished(self)

  L 751: reset_for_retract(self)

  L 765: offload_kv_cache(self, req_to_token_pool, token_to_kv_pool_allocator)

  L 771: load_kv_cache(self, req_to_token_pool, token_to_kv_pool_allocator)

  L 778: log_time_stats(self)

  L 790: set_finish_with_abort(self, error_msg: str)

  L 801: __repr__(self)


CLASS: ScheduleBatch
----------------------------------------
  L 917: init_new(cls, reqs: List[Req], req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, tree_cache: BasePrefixCache, model_config: ModelConfig, enable_overlap: bool, spec_algorithm: SpeculativeAlgorithm, chunked_req: Optional[Req])

  L 959: batch_size(self)

  L 962: is_empty(self)

  L 965: alloc_req_slots(self, num_reqs: int)

  L 976: alloc_token_slots(self, num_tokens: int, backup_state: bool)

  L1000: alloc_paged_token_slots_extend(self, prefix_lens: torch.Tensor, seq_lens: torch.Tensor, last_loc: torch.Tensor, extend_num_tokens: int, backup_state: bool)

  L1035: alloc_paged_token_slots_decode(self, seq_lens: torch.Tensor, last_loc: torch.Tensor, backup_state: bool)

  L1063: prepare_encoder_info_extend(self, input_ids: List[int], seq_lens: List[int])

  L1136: prepare_for_extend(self)

  L1340: prepare_for_split_prefill(self)

  L1345: mix_with_running(self, running_batch: 'ScheduleBatch')

  L1375: new_page_count_next_decode(self)

  L1387: check_decode_mem(self, buf_multiplier)

  L1397: retract_decode(self, server_args: ServerArgs)
         üìù Retract the decoding requests when there is not enough memory.

  L1521: prepare_encoder_info_decode(self)

  L1525: prepare_for_idle(self)

  L1539: prepare_for_decode(self)

  L1613: filter_batch(self, chunked_req_to_exclude: Optional[Union[Req, List[Req]]], keep_indices: Optional[List[int]])

  L1671: merge_batch(self, other: 'ScheduleBatch')

  L1711: get_model_worker_batch(self, seq_lens_cpu_cache: Optional[torch.Tensor])
         ‚Üí ModelWorkerBatch

  L1785: copy(self)

  L1846: __str__(self)


============================================================
FILE: python/sglang/srt/managers/schedule_policy.py
Functions: 10
============================================================


CLASS: PrefillAdder
----------------------------------------
  L 272: __init__(self, page_size: int, tree_cache: BasePrefixCache, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, running_batch: ScheduleBatch, new_token_ratio: float, rem_input_tokens: int, rem_chunk_tokens: Optional[int], mixed_with_decode_tokens: int)

  L 320: rem_total_tokens(self)

  L 337: cur_rem_tokens(self)

  L 353: ceil_paged_tokens(self, tokens: int)
         ‚Üí int

  L 356: budget_state(self)

  L 382: add_chunked_req(self, req: Req)

  L 415: add_one_req_ignore_eos(self, req: Req, has_chunked_req: bool)

  L 497: add_one_req(self, req: Req, has_chunked_req: bool)


CLASS: SchedulePolicy
----------------------------------------
  L  80: __init__(self, policy: str, tree_cache: BasePrefixCache, enable_hierarchical_cache: bool)

  L  98: calc_priority(self, waiting_queue: List[Req])
         ‚Üí bool


============================================================
FILE: python/sglang/srt/managers/scheduler.py
Functions: 53
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L2554: def is_health_check_generate_req(recv_req)

  L2558: def is_work_request(recv_req)

  L2570: def run_scheduler_process(server_args: ServerArgs,
        port_args: PortArgs,
        gpu_id: int,
        tp_rank: int,
        moe_ep_rank: int,
        pp_rank: int,
        dp_rank: Optional[int],
        pipe_writer,
        balance_meta: Optional[DPBalanceMeta])


CLASS: IdleSleeper
----------------------------------------
  L2537: __init__(self, sockets)

  L2543: maybe_sleep(self)


CLASS: Scheduler
----------------------------------------
  L 203: __init__(self, server_args: ServerArgs, port_args: PortArgs, gpu_id: int, tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int], dp_balance_meta: Optional[DPBalanceMeta])

  L 552: init_tokenizer(self)

  L 576: init_memory_pool_and_cache(self)

  L 684: init_disaggregation(self)

  L 777: init_moe_config(self)

  L 782: event_loop_normal(self)
         üìù A normal scheduler loop.

  L 801: event_loop_overlap(self)
         üìù A scheduler loop that overlaps the CPU processing and GPU computation.

  L 844: event_loop_pp(self)
         üìù A non-overlap scheduler loop for pipeline parallelism.

  L 976: recv_requests(self)
         ‚Üí List[Req]
         üìù Receive results at tp_rank = 0 and broadcast it to all other TP ranks.

  L1077: process_input_requests(self, recv_reqs: List)

  L1109: handle_generate_request(self, recv_req: TokenizedGenerateReqInput)

  L1274: handle_batch_generate_request(self, recv_req: BatchTokenizedGenerateReqInput)
         üìù Handle optimized batch generate request.

  L1322: handle_embedding_request(self, recv_req: TokenizedEmbeddingReqInput)

  L1368: handle_batch_embedding_request(self, recv_req: BatchTokenizedEmbeddingReqInput)
         üìù Handle optimized batch embedding request.

  L1381: self_check_during_idle(self)

  L1387: check_memory(self)

  L1464: check_tree_cache(self)

  L1499: get_next_batch_to_run(self)
         ‚Üí Optional[ScheduleBatch]

  L1564: get_num_allocatable_reqs(self, running_bs)

  L1570: get_new_batch_prefill(self)
         ‚Üí Optional[ScheduleBatch]

  L1722: update_running_batch(self, batch: ScheduleBatch)
         ‚Üí Optional[ScheduleBatch]
         üìù Update the current running decoding batch.

  L1762: run_batch(self, batch: ScheduleBatch)
         ‚Üí Union[GenerationBatchResult, EmbeddingBatchResult]
         üìù Run a batch.

  L1843: process_batch_result(self, batch: ScheduleBatch, result: Union[GenerationBatchResult, EmbeddingBatchResult], launch_done: Optional[threading.Event])

  L1862: maybe_send_health_check_signal(self)

  L1870: prepare_mlp_sync_batch(self, local_batch: ScheduleBatch)

  L1884: handle_dp_balance_data(self, local_batch: ScheduleBatch)

  L1965: prepare_mlp_sync_batch_raw(local_batch: ScheduleBatch, dp_size, attn_tp_size: int, tp_group, get_idle_batch, disable_cuda_graph: bool, spec_algorithm, speculative_num_draft_tokens, require_mlp_tp_gather: bool, disable_overlap_schedule: bool)

  L2068: get_idle_batch(self)

  L2081: move_ready_grammar_requests(self)
         üìù Move requests whose grammar objects are ready from grammar_queue to wa

  L2146: set_next_batch_sampling_info_done(self, batch: ScheduleBatch)

  L2153: watchdog_thread(self)
         üìù A watch dog thread that will try to kill the server itself if one forw

  L2206: flush_cache_wrapped(self, recv_req: FlushCacheReqInput)

  L2210: flush_cache(self)
         üìù Flush the memory pool and cache.

  L2247: get_load(self)

  L2281: get_internal_state(self, recv_req: GetInternalStateReq)

  L2310: set_internal_state(self, recv_req: SetInternalStateReq)

  L2348: handle_rpc_request(self, recv_req: RpcReqInput)

  L2367: abort_request(self, recv_req: AbortReq)

  L2444: load_lora_adapter(self, recv_req: LoadLoRAAdapterReqInput)
         ‚Üí LoadLoRAAdapterReqOutput
         üìù In-place loading a new lora adapter from disk or huggingface.

  L2452: unload_lora_adapter(self, recv_req: UnloadLoRAAdapterReqInput)
         ‚Üí UnloadLoRAAdapterReqOutput
         üìù Unload the lora adapter.

  L2460: slow_down(self, recv_req: SlowDownReqInput)

  L2467: expert_distribution_handle(self, recv_req: ExpertDistributionReq)

  L2478: open_session(self, recv_req: OpenSessionReqInput)

  L2493: close_session(self, recv_req: CloseSessionReqInput)

  L2501: get_print_prefix(self)

  L2511: current_scheduler_metrics_enabled(self)

  L2514: maybe_sleep_on_idle(self)

  L2518: handle_freeze_gc(self, recv_req: FreezeGCReq)
         üìù Handle freeze_gc request: freeze scheduler's GC and forward to detoken


============================================================
FILE: python/sglang/srt/managers/scheduler_input_blocker.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 101: def input_blocker_guard_region(send_to_scheduler)
         @contextmanager


CLASS: SchedulerInputBlocker
----------------------------------------
  L  26: __init__(self, noop: bool)

  L  32: handle(self, recv_reqs: Optional[List[Any]])


============================================================
FILE: python/sglang/srt/managers/scheduler_metrics_mixin.py
Functions: 5
============================================================


CLASS: KvMetrics
----------------------------------------
  L  19: __init__(self)


CLASS: SchedulerMetricsMixin
----------------------------------------
  L  31: init_metrics(self, tp_rank: int, pp_rank: int, dp_rank: Optional[int])

  L  53: init_kv_events(self, kv_events_config: Optional[str])

  L  59: log_prefill_stats(self, adder: PrefillAdder, can_run_list: List[Req], running_bs: int)

  L 140: log_decode_stats(self, can_run_cuda_graph: bool, running_batch: ScheduleBatch)


============================================================
FILE: python/sglang/srt/managers/scheduler_output_processor_mixin.py
Functions: 7
============================================================


CLASS: SchedulerOutputProcessorMixin
----------------------------------------
  L  32: process_batch_result_prefill(self: Scheduler, batch: ScheduleBatch, result: Union[GenerationBatchResult, EmbeddingBatchResult], launch_done: Optional[threading.Event])

  L 194: process_batch_result_decode(self: Scheduler, batch: ScheduleBatch, result: GenerationBatchResult, launch_done: Optional[threading.Event])

  L 298: add_input_logprob_return_values(self: Scheduler, i: int, req: Req, output: LogitsProcessorOutput, logprob_pt: int, num_input_logprobs: int, last_prefill_chunk: bool)
         üìù Incrementally add input logprobs to `req`.

  L 432: add_logprob_return_values(self: Scheduler, i: int, req: Req, pt: int, next_token_ids: List[int], num_input_logprobs: int, output: LogitsProcessorOutput)
         üìù Attach logprobs to the return values.

  L 463: stream_output(self: Scheduler, reqs: List[Req], return_logprob: bool, skip_req: Optional[Req])
         üìù Stream the output to detokenizer.

  L 475: stream_output_generation(self: Scheduler, reqs: List[Req], return_logprob: bool, skip_req: Optional[Req])

  L 704: stream_output_embedding(self: Scheduler, reqs: List[Req])


============================================================
FILE: python/sglang/srt/managers/scheduler_profiler_mixin.py
Functions: 5
============================================================


CLASS: SchedulerProfilerMixin
----------------------------------------
  L  29: init_profier(self)

  L  45: init_profile(self, output_dir: Optional[str], start_step: Optional[int], num_steps: Optional[int], activities: Optional[List[str]], with_stack: Optional[bool], record_shapes: Optional[bool], profile_by_stage: bool, profile_id: str)
         ‚Üí ProfileReqOutput

  L  97: start_profile(self, stage: Optional[ForwardMode])
         ‚Üí ProfileReqOutput | None

  L 172: stop_profile(self, stage: Optional[ForwardMode])
         ‚Üí ProfileReqOutput | None

  L 273: profile(self, recv_req: ProfileReq)


============================================================
FILE: python/sglang/srt/managers/scheduler_recv_skipper.py
Functions: 3
============================================================


CLASS: SchedulerRecvSkipper
----------------------------------------
  L   7: maybe_create(server_args: ServerArgs)

  L  12: __init__(self, server_args: ServerArgs)

  L  18: handle(self, last_forward_mode: ForwardMode)


============================================================
FILE: python/sglang/srt/managers/scheduler_update_weights_mixin.py
Functions: 9
============================================================


CLASS: SchedulerUpdateWeightsMixin
----------------------------------------
  L  29: update_weights_from_disk(self, recv_req: UpdateWeightFromDiskReqInput)
         üìù In-place update of the weights from disk.

  L  39: init_weights_update_group(self, recv_req: InitWeightsUpdateGroupReqInput)
         üìù Initialize the online model parameter update group.

  L  44: update_weights_from_distributed(self, recv_req: UpdateWeightsFromDistributedReqInput)
         ‚Üí Tuple[bool, str]
         üìù Update the online model parameter.

  L  58: update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput)
         üìù Update the online model parameter from tensors.

  L  71: get_weights_by_name(self, recv_req: GetWeightsByNameReqInput)

  L  75: release_memory_occupation(self, recv_req: ReleaseMemoryOccupationReqInput)

  L  97: resume_memory_occupation(self, recv_req: ResumeMemoryOccupationReqInput)

  L 120: save_remote_model(self, params)

  L 127: save_sharded_model(self, params)


============================================================
FILE: python/sglang/srt/managers/session_controller.py
Functions: 7
============================================================


CLASS: Session
----------------------------------------
  L  63: __init__(self, capacity_of_str_len: int, session_id: Optional[str])

  L  68: create_req(self, req: TokenizedGenerateReqInput, tokenizer)


CLASS: SessionReqNode
----------------------------------------
  L  22: __init__(self, req, parent, childs)

  L  29: clear_childs(self, req_dict)

  L  34: clear(self, req_dict)

  L  42: abort(self)

  L  46: __str__(self)


============================================================
FILE: python/sglang/srt/managers/template_manager.py
Functions: 9
============================================================


CLASS: TemplateManager
----------------------------------------
  L  54: __init__(self)

  L  61: chat_template_name(self)
         ‚Üí Optional[str]
         üìù Get the current chat template name.

  L  66: completion_template_name(self)
         ‚Üí Optional[str]
         üìù Get the current completion template name.

  L  71: jinja_template_content_format(self)
         ‚Üí Optional[str]
         üìù Get the detected template content format ('string' or 'openai' or None

  L  76: force_reasoning(self)
         ‚Üí bool
         üìù Check if the current chat template enforces reasoning/thinking.

  L 101: load_chat_template(self, tokenizer_manager, chat_template_arg: Optional[str], model_path: str)
         ‚Üí None
         üìù Load a chat template from various sources.

  L 166: guess_chat_template_from_model_path(self, model_path: str)
         ‚Üí None
         üìù Infer chat template name from model path.

  L 178: load_completion_template(self, completion_template_arg: str)
         ‚Üí None
         üìù Load completion template for code completion.

  L 198: initialize_templates(self, tokenizer_manager, model_path: str, chat_template: Optional[str], completion_template: Optional[str])
         ‚Üí None
         üìù Initialize all templates based on provided configuration.


============================================================
FILE: python/sglang/srt/managers/tokenizer_manager.py
Functions: 48
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L2065: async def print_exception_wrapper(func)
         üìù Sometimes an asyncio function does not print exception.


CLASS: SignalHandler
----------------------------------------
  L2082: __init__(self, tokenizer_manager: TokenizerManager)

  L2085: sigterm_handler(self, signum, frame)

  L2091: running_phase_sigquit_handler(self, signum, frame)


CLASS: TokenizerManager
----------------------------------------
  L 182: __init__(self, server_args: ServerArgs, port_args: PortArgs)

  L 482: generate_request(self, obj: Union[GenerateReqInput, EmbeddingReqInput], request: Optional[fastapi.Request])

  L 988: flush_cache(self)
         ‚Üí FlushCacheReqOutput

  L 991: abort_request(self, rid: str, abort_all: bool)

  L1000: start_profile(self, output_dir: Optional[str], start_step: Optional[int], num_steps: Optional[int], activities: Optional[List[str]], with_stack: Optional[bool], record_shapes: Optional[bool], profile_by_stage: bool)

  L1026: stop_profile(self)

  L1037: start_expert_distribution_record(self)

  L1041: stop_expert_distribution_record(self)

  L1045: dump_expert_distribution_record(self)

  L1049: pause_generation(self)

  L1054: continue_generation(self)

  L1059: update_weights_from_disk(self, obj: UpdateWeightFromDiskReqInput, request: Optional[fastapi.Request])
         ‚Üí Tuple[bool, str]

  L1107: init_weights_update_group(self, obj: InitWeightsUpdateGroupReqInput, request: Optional[fastapi.Request])
         ‚Üí Tuple[bool, str]

  L1119: update_weights_from_distributed(self, obj: UpdateWeightsFromDistributedReqInput, request: Optional[fastapi.Request])
         ‚Üí Tuple[bool, str]

  L1138: update_weights_from_tensor(self, obj: UpdateWeightsFromTensorReqInput, request: Optional[fastapi.Request])
         ‚Üí Tuple[bool, str]

  L1157: load_lora_adapter(self, obj: LoadLoRAAdapterReqInput, _: Optional[fastapi.Request])
         ‚Üí LoadLoRAAdapterReqOutput

  L1215: unload_lora_adapter(self, obj: UnloadLoRAAdapterReqInput, _: Optional[fastapi.Request])
         ‚Üí UnloadLoRAAdapterReqOutput

  L1257: get_weights_by_name(self, obj: GetWeightsByNameReqInput, request: Optional[fastapi.Request])

  L1268: release_memory_occupation(self, obj: ReleaseMemoryOccupationReqInput, request: Optional[fastapi.Request])

  L1276: resume_memory_occupation(self, obj: ResumeMemoryOccupationReqInput, request: Optional[fastapi.Request])

  L1284: slow_down(self, obj: SlowDownReqInput, request: Optional[fastapi.Request])

  L1292: open_session(self, obj: OpenSessionReqInput, request: Optional[fastapi.Request])

  L1309: close_session(self, obj: CloseSessionReqInput, request: Optional[fastapi.Request])

  L1314: get_internal_state(self)
         ‚Üí List[Dict[Any, Any]]

  L1322: set_internal_state(self, obj: SetInternalStateReq)
         ‚Üí SetInternalStateReqOutput

  L1330: get_load(self)
         ‚Üí dict

  L1338: get_log_request_metadata(self)

  L1392: configure_logging(self, obj: ConfigureLoggingReq)

  L1406: freeze_gc(self)
         üìù Send a freeze_gc message to the scheduler first, then freeze locally.

  L1412: create_abort_task(self, obj: GenerateReqInput)

  L1426: auto_create_handle_loop(self)

  L1457: dump_requests_before_crash(self)

  L1540: sigterm_watchdog(self)

  L1577: handle_loop(self)
         üìù The event loop that handles requests

  L1690: convert_logprob_style(self, meta_info: dict, state: ReqState, top_logprobs_num: int, token_ids_logprob: List[int], return_text_in_logprobs: bool, recv_obj: BatchStrOut, recv_obj_index: int)

  L1779: detokenize_logprob_tokens(self, token_logprobs_val: List[float], token_logprobs_idx: List[int], decode_to_text: bool)

  L1795: detokenize_top_logprobs_tokens(self, token_logprobs_val: List[float], token_logprobs_idx: List[int], decode_to_text: bool)

  L1815: collect_metrics(self, state: ReqState, recv_obj: BatchStrOut, i: int)

  L1858: dump_requests(self, state: ReqState, out_dict: dict)

  L1875: record_request_for_crash_dump(self, state: ReqState, out_dict: dict)

  L1945: score_request(self, query: Optional[Union[str, List[int]]], items: Optional[Union[str, List[str], List[List[int]]]], label_token_ids: Optional[List[int]], apply_softmax: bool, item_first: bool, request: Optional[Any])
         ‚Üí List[List[float]]
         üìù See Engine.score() for more details.


CLASS: _Communicator
----------------------------------------
  L2105: __init__(self, sender, fan_out: int)

  L2112: __call__(self, obj)

  L2134: handle_recv(self, recv_obj: T)


============================================================
FILE: python/sglang/srt/managers/tp_worker.py
Functions: 22
============================================================


CLASS: TpModelWorker
----------------------------------------
  L  54: __init__(self, server_args: ServerArgs, gpu_id: int, tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int], nccl_port: int, is_draft_worker: bool, req_to_token_pool: Optional[ReqToTokenPool], token_to_kv_pool_allocator: Optional[BaseTokenToKVPoolAllocator])

  L 165: register_hicache_layer_transfer_counter(self, counter)

  L 168: set_hicache_consumer(self, consumer_index)

  L 172: get_worker_info(self)

  L 189: sliding_window_size(self)
         ‚Üí Optional[int]

  L 193: is_hybrid(self)
         ‚Üí bool

  L 196: get_tokens_per_layer_info(self)

  L 202: get_pad_input_ids_func(self)

  L 205: get_tp_group(self)

  L 208: get_attention_tp_group(self)

  L 211: get_attention_tp_cpu_group(self)

  L 214: get_memory_pool(self)

  L 220: forward_batch_generation(self, model_worker_batch: ModelWorkerBatch, launch_done: Optional[threading.Event], skip_sample: bool)
         ‚Üí Tuple[Union[LogitsProcessorOutput, torch.Tensor], Optional[torch.Tensor], bool]

  L 260: forward_batch_embedding(self, model_worker_batch: ModelWorkerBatch)

  L 266: update_weights_from_disk(self, recv_req: UpdateWeightFromDiskReqInput)

  L 272: init_weights_update_group(self, recv_req: InitWeightsUpdateGroupReqInput)

  L 283: update_weights_from_distributed(self, recv_req: UpdateWeightsFromDistributedReqInput)

  L 291: update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput)

  L 302: get_weights_by_name(self, recv_req: GetWeightsByNameReqInput)

  L 308: load_lora_adapter(self, recv_req: LoadLoRAAdapterReqInput)

  L 312: unload_lora_adapter(self, recv_req: UnloadLoRAAdapterReqInput)

  L 316: can_run_lora_batch(self, lora_ids: list[str])
         ‚Üí bool


============================================================
FILE: python/sglang/srt/managers/tp_worker_overlap_thread.py
Functions: 27
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  45: def resolve_future_token_ids(input_ids, future_token_ids_map)
         @torch.compile(dynamic=True, backend=get_compiler_backend())


CLASS: TpModelWorkerClient
----------------------------------------
  L  56: __init__(self, server_args: ServerArgs, gpu_id: int, tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int], nccl_port: int)

  L  96: register_hicache_layer_transfer_counter(self, counter)

  L  99: set_hicache_consumer(self, consumer_index)

  L 103: get_worker_info(self)

  L 106: get_tokens_per_layer_info(self)

  L 110: sliding_window_size(self)
         ‚Üí Optional[int]

  L 114: is_hybrid(self)
         ‚Üí bool

  L 117: get_pad_input_ids_func(self)

  L 120: get_tp_group(self)

  L 123: get_attention_tp_group(self)

  L 126: get_attention_tp_cpu_group(self)

  L 129: get_memory_pool(self)

  L 135: get_kv_cache(self)

  L 138: forward_thread_func(self)

  L 148: forward_thread_func_(self)

  L 207: resolve_last_batch_result(self, launch_done: Optional[threading.Event])
         üìù This function is called to resolve the last batch result and

  L 231: forward_batch_generation(self, model_worker_batch: ModelWorkerBatch)
         ‚Üí Tuple[None, torch.Tensor, bool]

  L 264: update_weights_from_disk(self, recv_req: UpdateWeightFromDiskReqInput)

  L 268: init_weights_update_group(self, recv_req: InitWeightsUpdateGroupReqInput)

  L 272: update_weights_from_distributed(self, recv_req: UpdateWeightsFromDistributedReqInput)

  L 278: update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput)

  L 282: get_weights_by_name(self, recv_req: GetWeightsByNameReqInput)

  L 285: load_lora_adapter(self, recv_req: LoadLoRAAdapterReqInput)

  L 288: unload_lora_adapter(self, recv_req: UnloadLoRAAdapterReqInput)

  L 291: can_run_lora_batch(self, lora_ids: list[str])
         ‚Üí bool

  L 294: __delete__(self)


============================================================
FILE: python/sglang/srt/managers/utils.py
Functions: 11
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  18: def validate_input_length(req: Req,
        max_req_input_len: int,
        allow_auto_truncate: bool)
         ‚Üí Optional[str]
         üìù Validate and potentially truncate input length.

  L  51: def get_logprob_dict_from_result(result: GenerationBatchResult)
         ‚Üí dict

  L  72: def get_logprob_from_pp_outputs(next_pp_outputs: PPProxyTensors)
         ‚Üí tuple[LogitsProcessorOutput, list[int], list[int]]


CLASS: DPBalanceMeta
----------------------------------------
  L 107: __init__(self, num_workers: int)

  L 119: destructor(self)

  L 123: get_shared_onfly(self)
         ‚Üí List[Dict[int, int]]

  L 126: set_shared_onfly_info(self, data: List[Dict[int, int]])

  L 129: get_shared_local_tokens(self)
         ‚Üí List[int]

  L 132: set_shared_local_tokens(self, data: List[int])

  L 135: __getstate__(self)

  L 140: __setstate__(self, state)


============================================================
FILE: python/sglang/srt/mem_cache/allocator.py
Functions: 48
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 291: def alloc_extend_kernel(pre_lens_ptr,
        seq_lens_ptr,
        last_loc_ptr,
        free_page_ptr,
        out_indices,
        ret_values,
        bs_upper: tl.constexpr,
        page_size: tl.constexpr,
        max_num_extend_tokens: tl.constexpr)
         @triton.jit

  L 379: def alloc_decode_kernel(seq_lens_ptr,
        last_loc_ptr,
        free_page_ptr,
        out_indices,
        ret_values,
        bs_upper: tl.constexpr,
        page_size: tl.constexpr)
         @triton.jit


CLASS: BaseTokenToKVPoolAllocator
----------------------------------------
  L  38: __init__(self, size: int, page_size: int, dtype: torch.dtype, device: str, kvcache: KVCache, need_sort: bool)

  L  59: debug_print(self)
         ‚Üí str

  L  62: available_size(self)

  L  65: get_kvcache(self)

  L  68: restore_state(self, state)

  L  71: backup_state(self)

  L  74: free_group_begin(self)

  L  78: free_group_end(self)

  L  83: merge_and_sort_free(self)

  L  91: get_cpu_copy(self)

  L  95: load_cpu_copy(self)

  L  99: alloc_extend(self)

  L 102: alloc_decode(self)

  L 106: clear(self)

  L 110: alloc(self, need_size: int)

  L 114: free(self, free_index: torch.Tensor)


CLASS: PagedTokenToKVPoolAllocator
----------------------------------------
  L 429: __init__(self, size: int, page_size: int, dtype: torch.dtype, device: str, kvcache: KVCache, need_sort: bool)

  L 445: alloc(self, need_size: int)

  L 468: alloc_extend(self, prefix_lens: torch.Tensor, seq_lens: torch.Tensor, last_loc: torch.Tensor, extend_num_tokens: int)

  L 517: alloc_decode(self, seq_lens: torch.Tensor, last_loc: torch.Tensor)

  L 552: free(self, free_index: torch.Tensor)

  L 568: clear(self)

  L 577: get_cpu_copy(self, indices)

  L 580: load_cpu_copy(self, kv_cache_cpu, indices)


CLASS: SWATokenToKVPoolAllocator
----------------------------------------
  L 178: __init__(self, size: int, size_swa: int, dtype: torch.dtype, device: str, kvcache: SWAKVPool, need_sort: bool)

  L 214: available_size(self)

  L 217: full_available_size(self)

  L 220: swa_available_size(self)

  L 224: size_full(self)

  L 228: size_swa(self)

  L 231: debug_print(self)
         ‚Üí str

  L 239: get_kvcache(self)

  L 242: translate_loc_from_full_to_swa(self, kv_indices: torch.Tensor)

  L 246: alloc(self, need_size: int)

  L 257: free(self, free_index: torch.Tensor)

  L 270: free_swa(self, free_index: torch.Tensor)

  L 276: backup_state(self)

  L 279: restore_state(self, state)

  L 282: clear(self)


CLASS: TokenToKVPoolAllocator
----------------------------------------
  L 121: __init__(self, size: int, dtype: torch.dtype, device: str, kvcache: KVCache, need_sort: bool)

  L 132: clear(self)

  L 141: available_size(self)

  L 145: alloc(self, need_size: int)

  L 156: free(self, free_index: torch.Tensor)

  L 168: get_cpu_copy(self, indices)

  L 171: load_cpu_copy(self, kv_cache_cpu, indices)


============================================================
FILE: python/sglang/srt/mem_cache/allocator_ascend.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  13: def alloc_extend_kernel_ascend(prefix_lens,
        seq_lens,
        last_loc,
        free_pages,
        out_indices,
        page_size,
        device)


CLASS: AscendPagedTokenToKVPoolAllocator
----------------------------------------
  L  69: alloc_extend(self, prefix_lens: torch.Tensor, seq_lens: torch.Tensor, last_loc: torch.Tensor, extend_num_tokens: int)

  L 115: alloc_decode(self, seq_lens: torch.Tensor, last_loc: torch.Tensor)


============================================================
FILE: python/sglang/srt/mem_cache/base_prefix_cache.py
Functions: 19
============================================================


CLASS: BasePrefixCache
----------------------------------------
  L  35: reset(self)

  L  39: match_prefix(self, key: List[int])
         ‚Üí MatchResult

  L  43: cache_finished_req(self, req: Req)

  L  47: cache_unfinished_req(self, req: Req)

  L  51: evict(self, num_tokens: int)

  L  55: inc_lock_ref(self, node: Any)

  L  59: dec_lock_ref(self, node: Any, swa_uuid_for_lock: Optional[str])

  L  62: evictable_size(self)

  L  65: full_evictable_size(self)

  L  68: swa_evictable_size(self)

  L  71: protected_size(self)

  L  74: full_protected_size(self)

  L  77: swa_protected_size(self)

  L  80: total_size(self)

  L  83: pretty_print(self)

  L  86: init_load_back(self, last_host_node: Any, host_hit_length: int)
         ‚Üí Tuple[torch.Tensor, Any]
         üìù Preparing KV cache loading from host to device.

  L  96: ready_to_load_host_cache(self)
         ‚Üí Any
         üìù Notify the cache controller to start the KV cache loading

  L 102: check_hicache_events(self)
         ‚Üí Any
         üìù Check HiCache related activities to update radix tree and synchronize 

  L 108: take_events(self)


============================================================
FILE: python/sglang/srt/mem_cache/chunk_cache.py
Functions: 12
============================================================


CLASS: ChunkCache
----------------------------------------
  L  21: __init__(self, req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, page_size: int)

  L  31: reset(self)

  L  34: match_prefix(self)
         ‚Üí MatchResult

  L  41: cache_finished_req(self, req: Req)

  L  50: cache_unfinished_req(self, req: Req)

  L  58: evict(self, num_tokens: int)

  L  61: inc_lock_ref(self, node: Any)

  L  64: dec_lock_ref(self, node: Any, swa_uuid_for_lock: Optional[str])

  L  67: pretty_print(self)


CLASS: SWAChunkCache
----------------------------------------
  L  74: __init__(self, req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: SWATokenToKVPoolAllocator, page_size: int)

  L  83: evict_swa(self, req: Req, prelen: int, attention_chunk_size: int)

  L  99: evict(self, num_tokens: int)


============================================================
FILE: python/sglang/srt/mem_cache/cpp_radix_tree/radix_tree.py
Functions: 13
============================================================


CLASS: RadixTreeCpp
----------------------------------------
  L  33: __init__(self, disabled: bool, host_size: Optional[int], page_size: int, write_through_threshold: int)
         üìù Initializes the RadixTreeCpp instance.

  L  52: match_prefix(self, prefix: List[int])
         ‚Üí Tuple[List[torch.Tensor], int, TreeNodeCpp, TreeNodeCpp]
         üìù Matches a prefix in the radix tree.

  L  68: evict(self, num_tokens: int)
         ‚Üí List[torch.Tensor]
         üìù Evicts a number of tokens from the radix tree.

  L  78: lock_ref(self, handle: TreeNodeCpp, lock: bool)
         ‚Üí None
         üìù Locks or unlocks a reference to a tree node.

  L  88: writing_through(self, key: List[int], indices: torch.Tensor)
         ‚Üí Tuple[List[Tuple[IOHandle, torch.Tensor, torch.Tensor]], int]
         üìù Inserts a key-value pair into the radix tree and perform write-through

  L 104: loading_onboard(self, host_node: TreeNodeCpp, new_device_indices: torch.Tensor)
         ‚Üí Tuple[IOHandle, List[torch.Tensor]]
         üìù Updates the device indices of tree nodes within a range on the tree.

  L 122: commit_writing_through(self, handle: IOHandle, success: bool)
         ‚Üí None
         üìù Commits the write-through process for a tree node.

  L 131: commit_loading_onboard(self, handle: IOHandle, success: bool)
         ‚Üí None
         üìù Commits the load onboard process for tree nodes within a range on the 

  L 140: evictable_size(self)
         ‚Üí int
         üìù Returns the size of the evictable part of the radix tree.

  L 149: protected_size(self)
         ‚Üí int
         üìù Returns the size of the protected part of the radix tree.

  L 158: total_size(self)
         ‚Üí int
         üìù Returns the total size of the radix tree (including CPU nodes).

  L 166: reset(self)
         ‚Üí None
         üìù Resets the radix tree, clearing all nodes and indices.

  L 172: debug_print(self)
         ‚Üí None
         üìù Prints the internal state of the radix tree for debugging purposes.


============================================================
FILE: python/sglang/srt/mem_cache/hicache_storage.py
Functions: 15
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  13: def get_hash_str(token_ids: List[int], prior_hash: str)
         ‚Üí str


CLASS: HiCacheFile
----------------------------------------
  L 119: __init__(self, storage_config: HiCacheStorageConfig, file_path: str)

  L 137: get(self, key: str, target_location: torch.Tensor, target_sizes: Optional[Any])
         ‚Üí torch.Tensor | None

  L 158: batch_get(self, keys: List[str], target_locations: List[torch.Tensor], target_sizes: Optional[Any])
         ‚Üí List[torch.Tensor | None]

  L 171: set(self, key: str, value: Optional[Any], target_location: Optional[Any], target_sizes: Optional[Any])
         ‚Üí bool

  L 190: batch_set(self, keys: List[str], values: Optional[Any], target_locations: Optional[Any], target_sizes: Optional[Any])
         ‚Üí bool

  L 202: exists(self, key: str)
         ‚Üí bool

  L 207: delete(self, key: str)
         ‚Üí None

  L 216: clear(self)
         ‚Üí None


CLASS: HiCacheStorage
----------------------------------------
  L  44: get(self, key: str, target_location: Optional[Any], target_sizes: Optional[Any])
         ‚Üí torch.Tensor | None
         üìù Retrieve the value associated with the given key.

  L  57: batch_get(self, keys: List[str], target_locations: Optional[Any], target_sizes: Optional[Any])
         ‚Üí List[torch.Tensor | None] | int
         üìù Retrieve values for multiple keys.

  L  70: set(self, key: str, value: Optional[Any], target_location: Optional[Any], target_sizes: Optional[Any])
         ‚Üí bool
         üìù Store the value associated with the given key.

  L  84: batch_set(self, keys: List[str], values: Optional[Any], target_locations: Optional[Any], target_sizes: Optional[Any])
         ‚Üí bool
         üìù Store multiple key-value pairs.

  L  98: exists(self, key: str)
         ‚Üí bool
         üìù Check if the key exists in the storage.

  L 105: batch_exists(self, keys: List[str])
         ‚Üí int
         üìù Check if the keys exist in the storage.


============================================================
FILE: python/sglang/srt/mem_cache/hiradix_cache.py
Functions: 21
============================================================


CLASS: HiRadixCache
----------------------------------------
  L  29: __init__(self, req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, tp_cache_group: torch.distributed.ProcessGroup, page_size: int, hicache_ratio: float, hicache_size: int, hicache_write_policy: str, hicache_io_backend: str, hicache_mem_layout: str, hicache_storage_backend: Optional[str], hicache_storage_prefetch_policy: Optional[str], model_name: Optional[str], storage_backend_extra_config: Optional[str])

  L 115: reset(self)

  L 121: get_height(self, node: TreeNode)

  L 128: write_backup(self, node: TreeNode, write_back)

  L 151: write_backup_storage(self, node: TreeNode)

  L 158: inc_hit_count(self, node: TreeNode)

  L 176: writing_check(self, write_back)

  L 198: loading_check(self)

  L 213: evictable_size(self)

  L 216: evict(self, num_tokens: int)

  L 268: evict_host(self, num_tokens: int)

  L 295: load_back(self, node: TreeNode, mem_quota: Optional[int])
         ‚Üí Optional[torch.Tensor]

  L 347: init_load_back(self, last_node: TreeNode, host_hit_length: int, mem_quota: Optional[int])

  L 370: ready_to_load_host_cache(self)

  L 375: check_hicache_events(self)

  L 382: check_revoked_prefetch(self)

  L 404: check_backup_progress(self)

  L 431: can_terminate_prefetch(self, operation: PrefetchOperation)

  L 465: check_prefetch_progress(self, req_id: str)
         ‚Üí bool

  L 521: match_prefix(self, key: List[int])

  L 556: prefetch_from_storage(self, req_id: str, last_host_node: TreeNode, new_input_tokens: List[int], last_hash: Optional[str])


============================================================
FILE: python/sglang/srt/mem_cache/lora_radix_cache.py
Functions: 21
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  32: def get_child_key(key: LoRAKey)


CLASS: LoRAKey
----------------------------------------
  L  22: __init__(self, lora_id: str, token_ids: List[int])

  L  28: __len__(self)


CLASS: LoRARadixCache
----------------------------------------
  L  79: __init__(self, req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, page_size: int, disable: bool)

  L 104: reset(self)

  L 111: match_prefix(self, key: List[int])
         ‚Üí MatchResult

  L 116: match_prefix_with_lora_id(self, key: LoRAKey)
         ‚Üí MatchResult
         üìù Find the matching prefix from the lora radix tree.

  L 149: insert(self, key: LoRAKey, value)

  L 157: cache_finished_req(self, req: Req)
         üìù Cache request when it finishes.

  L 186: cache_unfinished_req(self, req: Req)
         üìù Cache request when it is unfinished.

  L 221: pretty_print(self)

  L 225: total_size(self)

  L 228: evict(self, num_tokens: int)

  L 251: inc_lock_ref(self, node: LoRATreeNode)

  L 265: dec_lock_ref(self, node: LoRATreeNode)

  L 279: evictable_size(self)

  L 282: protected_size(self)

  L 286: all_values_flatten(self)


CLASS: LoRATreeNode
----------------------------------------
  L  45: __init__(self, id: Optional[int])

  L  57: evicted(self)

  L  60: __lt__(self, other: 'LoRATreeNode')


============================================================
FILE: python/sglang/srt/mem_cache/memory_pool.py
Functions: 62
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 644: def set_mla_kv_buffer_kernel(kv_buffer_ptr,
        cache_k_nope_ptr,
        cache_k_rope_ptr,
        loc_ptr,
        buffer_stride: tl.constexpr,
        nope_stride: tl.constexpr,
        rope_stride: tl.constexpr,
        nope_dim: tl.constexpr,
        rope_dim: tl.constexpr,
        BLOCK: tl.constexpr)
         @triton.jit

  L 682: def set_mla_kv_buffer_triton(kv_buffer: torch.Tensor,
        loc: torch.Tensor,
        cache_k_nope: torch.Tensor,
        cache_k_rope: torch.Tensor)

  L1106: def copy_all_layer_kv_cache(data_ptrs,
        strides,
        tgt_loc_ptr,
        src_loc_ptr,
        num_locs,
        num_locs_upper: tl.constexpr)
         @triton.jit


CLASS: AscendMLAPagedTokenToKVPool
----------------------------------------
  L 885: __init__(self, size: int, page_size: int, dtype: torch.dtype, kv_lora_rank: int, qk_rope_head_dim: int, layer_num: int, device: str, enable_memory_saver: bool, start_layer: Optional[int], end_layer: Optional[int])

  L 945: get_kv_size_bytes(self)

  L 955: get_kv_buffer(self, layer_id: int)

  L 963: get_key_buffer(self, layer_id: int)

  L 971: get_value_buffer(self, layer_id: int)

  L 980: get_contiguous_buf_infos(self)

  L 993: set_kv_buffer(self, layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor)


CLASS: AscendTokenToKVPool
----------------------------------------
  L 582: get_contiguous_buf_infos(self)

  L 608: set_kv_buffer(self, layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor, k_scale: Optional[float], v_scale: Optional[float])


CLASS: DoubleSparseTokenToKVPool
----------------------------------------
  L1027: __init__(self, size: int, page_size: int, dtype: torch.dtype, head_num: int, head_dim: int, layer_num: int, device: str, heavy_channel_num: int, enable_memory_saver: bool, start_layer: Optional[int], end_layer: Optional[int])

  L1075: get_key_buffer(self, layer_id: int)

  L1078: get_value_buffer(self, layer_id: int)

  L1081: get_label_buffer(self, layer_id: int)

  L1084: get_kv_buffer(self, layer_id: int)

  L1090: set_kv_buffer(self, layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor, cache_label: torch.Tensor)


CLASS: KVCache
----------------------------------------
  L 102: __init__(self, size: int, page_size: int, dtype: torch.dtype, layer_num: int, device: str, enable_memory_saver: bool, start_layer: Optional[int], end_layer: Optional[int])

  L 134: get_key_buffer(self, layer_id: int)
         ‚Üí torch.Tensor

  L 138: get_value_buffer(self, layer_id: int)
         ‚Üí torch.Tensor

  L 142: get_kv_buffer(self, layer_id: int)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 146: set_kv_buffer(self, layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor)
         ‚Üí None

  L 155: register_layer_transfer_counter(self, layer_transfer_counter)

  L 158: get_cpu_copy(self, indices)

  L 161: load_cpu_copy(self, kv_cache_cpu, indices)


CLASS: MHATokenToKVPool
----------------------------------------
  L 167: __init__(self, size: int, page_size: int, dtype: torch.dtype, head_num: int, head_dim: int, layer_num: int, device: str, enable_memory_saver: bool, start_layer: Optional[int], end_layer: Optional[int])

  L 267: get_kv_size_bytes(self)

  L 279: get_contiguous_buf_infos(self)

  L 305: maybe_get_custom_mem_pool(self)

  L 308: get_cpu_copy(self, indices)

  L 326: load_cpu_copy(self, kv_cache_cpu, indices)

  L 349: get_key_buffer(self, layer_id: int)

  L 364: get_value_buffer(self, layer_id: int)

  L 369: get_kv_buffer(self, layer_id: int)

  L 372: set_kv_buffer(self, layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor, k_scale: Optional[float], v_scale: Optional[float], layer_id_override: Optional[int])

  L 412: move_kv_cache(self, tgt_loc: torch.Tensor, src_loc: torch.Tensor)


CLASS: MLATokenToKVPool
----------------------------------------
  L 710: __init__(self, size: int, page_size: int, dtype: torch.dtype, kv_lora_rank: int, qk_rope_head_dim: int, layer_num: int, device: str, enable_memory_saver: bool, start_layer: Optional[int], end_layer: Optional[int])

  L 779: get_kv_size_bytes(self)

  L 787: get_contiguous_buf_infos(self)

  L 796: maybe_get_custom_mem_pool(self)

  L 799: get_key_buffer(self, layer_id: int)

  L 807: get_value_buffer(self, layer_id: int)

  L 817: get_kv_buffer(self, layer_id: int)

  L 820: set_kv_buffer(self, layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor)

  L 837: set_mla_kv_buffer(self, layer: RadixAttention, loc: torch.Tensor, cache_k_nope: torch.Tensor, cache_k_rope: torch.Tensor)

  L 856: get_cpu_copy(self, indices)

  L 871: load_cpu_copy(self, kv_cache_cpu, indices)


CLASS: ReqToTokenPool
----------------------------------------
  L  53: __init__(self, size: int, max_context_len: int, device: str, enable_memory_saver: bool)

  L  75: write(self, indices, values)

  L  78: available_size(self)

  L  81: alloc(self, need_size: int)
         ‚Üí List[int]

  L  90: free(self, free_index: Union[int, List[int]])

  L  96: clear(self)


CLASS: SWAKVPool
----------------------------------------
  L 426: __init__(self, size: int, size_swa: int, dtype: torch.dtype, head_num: int, head_dim: int, swa_attention_layer_ids: List[int], full_attention_layer_ids: List[int], enable_kvcache_transpose: bool, device: str)

  L 478: get_kv_size_bytes(self)

  L 483: get_contiguous_buf_infos(self)

  L 497: get_key_buffer(self, layer_id: int)

  L 504: get_value_buffer(self, layer_id: int)

  L 511: get_kv_buffer(self, layer_id: int)

  L 518: translate_loc_from_full_to_swa(self, kv_indices: torch.Tensor)

  L 522: set_kv_buffer(self, layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor, k_scale: float, v_scale: float)


============================================================
FILE: python/sglang/srt/mem_cache/memory_pool_host.py
Functions: 48
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  38: def synchronized(debug_only)


CLASS: HostKVCache
----------------------------------------
  L  55: __init__(self, device_pool: KVCache, host_to_device_ratio: float, host_size: int, page_size: int, layout: str, pin_memory: bool, device: str)

  L 112: get_size_per_token(self)

  L 116: init_kv_buffer(self)

  L 120: load_to_device_per_layer(self, device_pool, host_indices, device_indices, layer_id, io_backend)
         ‚Üí None
         üìù Load KV data from the host memory pool to the device memory pool for a

  L 129: backup_from_device_all_layer(self, device_pool, host_indices, device_indices, io_backend)
         ‚Üí None
         üìù Backup KV data from the device memory pool to the host memory pool for

  L 138: get_flat_data_page(self, index)
         ‚Üí torch.Tensor
         üìù Get a flat data page from the host memory pool.

  L 145: get_dummy_flat_data_page(self)
         ‚Üí torch.Tensor
         üìù Get a dummy flat data page from the host memory pool.

  L 153: set_from_flat_data_page(self, index: int, data_page: torch.Tensor)
         ‚Üí None
         üìù Set a flat data page to the host memory pool.

  L 160: clear(self)

  L 167: available_size(self)

  L 171: alloc(self, need_size: int)
         ‚Üí torch.Tensor

  L 187: free(self, indices: torch.Tensor)
         ‚Üí int

  L 194: get_state(self, indices: torch.Tensor)
         ‚Üí MemoryStateInt

  L 203: is_reserved(self, indices: torch.Tensor)
         ‚Üí bool

  L 207: is_protected(self, indices: torch.Tensor)
         ‚Üí bool

  L 211: is_synced(self, indices: torch.Tensor)
         ‚Üí bool

  L 215: is_backup(self, indices: torch.Tensor)
         ‚Üí bool

  L 219: update_backup(self, indices: torch.Tensor)

  L 228: update_prefetch(self, indices: torch.Tensor)

  L 237: update_synced(self, indices: torch.Tensor)

  L 241: protect_write(self, indices: torch.Tensor)

  L 250: protect_load(self, indices: torch.Tensor)

  L 259: complete_io(self, indices: torch.Tensor)


CLASS: MHATokenToKVPoolHost
----------------------------------------
  L 271: __init__(self, device_pool: MHATokenToKVPool, host_to_device_ratio: float, host_size: int, page_size: int, layout: str, pin_memory: bool, device: str)

  L 303: get_size_per_token(self)

  L 310: get_ksize_per_token(self)

  L 313: init_kv_buffer(self)

  L 330: k_buffer(self)

  L 334: v_buffer(self)

  L 337: load_to_device_per_layer(self, device_pool, host_indices, device_indices, layer_id, io_backend)

  L 387: backup_from_device_all_layer(self, device_pool, host_indices, device_indices, io_backend)

  L 430: get_flat_data_page(self, index)
         ‚Üí torch.Tensor

  L 438: get_dummy_flat_data_page(self)
         ‚Üí torch.Tensor

  L 446: set_from_flat_data_page(self, index: int, data_page: torch.Tensor)
         ‚Üí None

  L 466: get_buffer_meta(self, keys, indices, local_rank)

  L 502: get_buffer_with_hash(self, keys, indices)


CLASS: MLATokenToKVPoolHost
----------------------------------------
  L 521: __init__(self, device_pool: MLATokenToKVPool, host_to_device_ratio: float, host_size: int, page_size: int, layout: str, pin_memory: bool, device: str)

  L 547: get_size_per_token(self)

  L 559: get_ksize_per_token(self)

  L 562: init_kv_buffer(self)

  L 591: load_to_device_per_layer(self, device_pool, host_indices, device_indices, layer_id, io_backend)

  L 627: backup_from_device_all_layer(self, device_pool, host_indices, device_indices, io_backend)

  L 666: get_flat_data_page(self, index)
         ‚Üí torch.Tensor

  L 674: get_dummy_flat_data_page(self)
         ‚Üí torch.Tensor

  L 687: set_from_flat_data_page(self, index: int, data_page: torch.Tensor)
         ‚Üí None

  L 705: get_buffer_meta(self, keys, indices, local_rank)

  L 729: get_buffer_with_hash(self, keys, indices)


============================================================
FILE: python/sglang/srt/mem_cache/multimodal_cache.py
Functions: 6
============================================================


CLASS: MultiModalCache
----------------------------------------
  L  14: __init__(self, max_size: int)

  L  40: put(self, mm_hash: int, embedding: torch.Tensor)
         ‚Üí bool

  L  49: has(self, mm_hash: int)
         ‚Üí bool

  L  52: get(self, mm_hash: int)
         ‚Üí torch.Tensor
         üìù Get embedding and update LRU order

  L  60: clear(self)

  L  67: __len__(self)


============================================================
FILE: python/sglang/srt/mem_cache/radix_cache.py
Functions: 22
============================================================


CLASS: RadixCache
----------------------------------------
  L 121: __init__(self, req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, page_size: int, disable: bool, enable_kv_cache_events: bool)

  L 151: reset(self)

  L 161: match_prefix(self, key: List[int])
         ‚Üí MatchResult
         üìù Find the matching prefix from the radix tree.

  L 198: insert(self, key: List, value)

  L 206: cache_finished_req(self, req: Req)
         üìù Cache request when it finishes.

  L 243: cache_unfinished_req(self, req: Req)
         üìù Cache request when it is unfinished.

  L 288: pretty_print(self)

  L 292: total_size(self)

  L 295: evict(self, num_tokens: int)

  L 320: inc_lock_ref(self, node: TreeNode)

  L 334: dec_lock_ref(self, node: TreeNode)

  L 348: evictable_size(self)

  L 351: protected_size(self)

  L 355: all_values_flatten(self)

  L 542: take_events(self)
         üìù Atomically takes all events and clears the queue.


CLASS: TreeNode
----------------------------------------
  L  47: __init__(self, id: Optional[int])

  L  71: evicted(self)

  L  75: backuped(self)

  L  78: protect_host(self)
         üìù Protect the host value from eviction.

  L  82: release_host(self)
         üìù Release the host value, allowing it to be evicted.

  L  89: get_last_hash_value(self)
         ‚Üí Optional[str]
         üìù Returns the hash value of the last page in this node.

  L  95: __lt__(self, other: 'TreeNode')


============================================================
FILE: python/sglang/srt/mem_cache/radix_cache_cpp.py
Functions: 12
============================================================


CLASS: RadixCacheCpp
----------------------------------------
  L  40: __init__(self, disable: bool, use_hicache: bool, req_to_token_pool: ReqToTokenPool, token_to_kv_pool: BaseTokenToKVPoolAllocator, tp_cache_group: torch.distributed.ProcessGroup, page_size: int, hicache_ratio: float, hicache_size: int, hicache_write_policy: str, enable_kv_cache_events: bool, hicache_oracle: bool, enable_write_cancel: bool)

  L  90: reset(self)

  L  96: match_prefix(self, key: List[int])
         ‚Üí MatchResult

  L 123: dec_lock_ref(self, node: TreeNodeCpp)
         üìù Decrement the reference count of a node to root of the radix tree.

  L 131: inc_lock_ref(self, node: TreeNodeCpp)
         üìù Increment the reference count of from a node to root of the radix tree

  L 139: evict(self, num_tokens: int)

  L 144: evictable_size(self)

  L 147: protected_size(self)

  L 150: total_size(self)

  L 153: cache_finished_req(self, req: Req)
         üìù Cache request when it finishes.

  L 184: cache_unfinished_req(self, req: Req)
         üìù Cache request when it is unfinished.

  L 228: pretty_print(self)


============================================================
FILE: python/sglang/srt/mem_cache/storage/hf3fs/client_hf3fs.py
Functions: 9
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  30: def rsynchronized()

  L  42: def wsynchronized()


CLASS: Hf3fsClient
----------------------------------------
  L  55: __init__(self, path: str, size: int, bytes_per_page: int, entries: int)

  L 106: batch_read(self, offsets: List[int], tensors: List[torch.Tensor])
         ‚Üí List[int]

  L 129: batch_write(self, offsets: List[int], tensors: List[torch.Tensor])
         ‚Üí List[int]

  L 151: check(self, offsets: List[int], tensors: List[torch.Tensor])
         ‚Üí None

  L 169: get_size(self)
         ‚Üí int

  L 172: close(self)
         ‚Üí None

  L 182: flush(self)
         ‚Üí None


============================================================
FILE: python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py
Functions: 39
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 405: def run_metadata_server(host: str,
        port: int,
        persistence_path: Optional[str],
        save_interval: int)
         üìù Run the HF3FS metadata server.


CLASS: GlobalMetadataState
----------------------------------------
  L 103: __init__(self, persistence_path: Optional[str], save_interval: int)

  L 111: load_from_disk(self)

  L 139: save_to_disk(self)

  L 163: schedule_save(self)

  L 170: shutdown(self)


CLASS: Hf3fsGlobalMetadataClient
----------------------------------------
  L 302: __init__(self, base_url: str, max_retries: int)

  L 324: initialize(self, rank: int, num_pages: int)
         ‚Üí None

  L 327: reserve_and_allocate_page_indices(self, rank: int, keys: List[Tuple[str, str]])
         ‚Üí List[Tuple[bool, int]]

  L 335: confirm_write(self, rank: int, written_keys_to_confirm: List[Tuple[str, int]], pages_to_release: List[int])
         ‚Üí None

  L 349: delete_keys(self, rank: int, keys: List[str])
         ‚Üí None

  L 352: exists(self, rank: int, keys: List[str])
         ‚Üí List[bool]

  L 356: clear(self, rank: int)
         ‚Üí None

  L 359: get_page_indices(self, rank: int, keys: List[str])
         ‚Üí List[Optional[int]]


CLASS: Hf3fsLocalMetadataClient
----------------------------------------
  L 367: __init__(self)

  L 370: initialize(self, rank: int, num_pages: int)
         ‚Üí None

  L 373: reserve_and_allocate_page_indices(self, rank: int, keys: List[Tuple[str, str]])
         ‚Üí List[Tuple[bool, int]]
         üìù Reserve and allocate page indices for keys.

  L 379: confirm_write(self, rank: int, written_keys_to_confirm: List[Tuple[str, int]], pages_to_release: List[int])
         ‚Üí None
         üìù Confirm write operations.

  L 388: delete_keys(self, rank: int, keys: List[str])
         ‚Üí None
         üìù Delete keys.

  L 392: exists(self, rank: int, keys: List[str])
         ‚Üí List[bool]
         üìù Check if keys exist.

  L 396: clear(self, rank: int)
         ‚Üí None
         üìù Clear all metadata for rank.

  L 400: get_page_indices(self, rank: int, keys: List[str])
         ‚Üí List[Optional[int]]
         üìù Get page indices for keys.


CLASS: Hf3fsMetadataServer
----------------------------------------
  L 183: __init__(self, persistence_path: Optional[str], save_interval: int)

  L 200: get_rank_metadata(self, rank: int)
         ‚Üí RankMetadata
         üìù Get rank metadata with proper error handling.

  L 210: initialize(self, rank: int, request: Request)
         üìù Initialize a rank with specified number of pages.

  L 228: exists(self, rank: int, request: Request)
         üìù Check if keys exist in metadata.

  L 236: reserve_and_allocate_page_indices(self, rank: int, request: Request)
         üìù Reserve and allocate page indices for keys.

  L 244: confirm_write(self, rank: int, request: Request)
         üìù Confirm write operations and release pages.

  L 257: delete_keys(self, rank: int, request: Request)
         üìù Delete keys from metadata.

  L 264: clear(self, rank: int)
         üìù Clear all metadata for a rank.

  L 270: get_page_indices(self, rank: int, request: Request)
         üìù Get page indices for keys.

  L 278: run(self, host: str, port: int)
         üìù Run the metadata server.


CLASS: RankMetadata
----------------------------------------
  L  26: __init__(self, num_pages: int)

  L  33: exists_keys(self, keys: List[str])
         ‚Üí List[bool]
         üìù Check if keys exist in metadata.

  L  38: reserve_and_allocate_page_indices(self, keys: List[Tuple[str, str]])
         ‚Üí List[Tuple[bool, int]]
         üìù Reserve and allocate page indices for keys.

  L  62: confirm_write(self, written_keys_to_confirm: List[Tuple[str, int]], pages_to_release: List[int])
         ‚Üí None
         üìù Confirm write operations and release pages.

  L  76: delete_keys(self, keys: List[str])
         ‚Üí int
         üìù Delete keys and return count of deleted keys.

  L  88: clear_all(self)
         ‚Üí None
         üìù Clear all metadata.

  L  94: get_page_indices(self, keys: List[str])
         ‚Üí List[Optional[int]]
         üìù Get page indices for keys.


============================================================
FILE: python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py
Functions: 20
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 103: def synchronized()


CLASS: AtomicCounter
----------------------------------------
  L  90: __init__(self, n: int)

  L  96: next(self)
         ‚Üí int


CLASS: Hf3fsMetadataInterface
----------------------------------------
  L  24: initialize(self, rank: int, num_pages: int)
         ‚Üí None
         üìù Initialize the metadata service with specified number of pages.

  L  29: reserve_and_allocate_page_indices(self, rank: int, keys: List[Tuple[str, str]])
         ‚Üí List[Tuple[bool, int]]
         üìù Reserve and allocate page indices for the specified keys.

  L  45: confirm_write(self, rank: int, written_keys_to_confirm: List[Tuple[str, int]], pages_to_release: List[int])
         ‚Üí None
         üìù Confirm that key-value pairs have been successfully written to storage

  L  61: get_page_indices(self, rank: int, keys: List[str])
         ‚Üí List[Optional[int]]
         üìù Get page indices for the specified keys.

  L  74: delete_keys(self, rank: int, keys: List[str])
         ‚Üí None
         üìù Delete specified keys and their associated pages.

  L  79: exists(self, rank: int, keys: List[str])
         ‚Üí List[bool]
         üìù Check if the specified keys exist.

  L  84: clear(self, rank: int)
         ‚Üí None
         üìù Clear all key-value pairs and page allocations for the specified rank.


CLASS: HiCacheHF3FS
----------------------------------------
  L 118: __init__(self, rank: int, file_path: str, file_size: int, numjobs: int, bytes_per_page: int, entries: int, dtype: torch.dtype, metadata_client: Hf3fsMetadataInterface)

  L 169: from_env_config(bytes_per_page: int, dtype: torch.dtype, storage_config: HiCacheStorageConfig)
         ‚Üí 'HiCacheHF3FS'

  L 234: get(self, key: str, target_location: Optional[Any], target_sizes: Optional[Any])
         ‚Üí torch.Tensor | None

  L 247: batch_get(self, keys: List[str], target_locations: Optional[Any], target_sizes: Optional[Any])
         ‚Üí List[torch.Tensor | None]

  L 294: set(self, key: str, value: Optional[Any], target_location: Optional[Any], target_sizes: Optional[Any])
         ‚Üí bool

  L 308: batch_set(self, keys: List[str], values: Optional[Any], target_locations: Optional[Any], target_sizes: Optional[Any])
         ‚Üí bool

  L 367: delete(self, key: str)
         ‚Üí None

  L 371: exists(self, key: str)
         ‚Üí bool

  L 376: clear(self)
         ‚Üí None

  L 379: close(self)
         ‚Üí None


============================================================
FILE: python/sglang/srt/mem_cache/storage/hf3fs/test_hf3fs_utils.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  15: def test_rw_shm()


============================================================
FILE: python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py
Functions: 15
============================================================


CLASS: MooncakeStore
----------------------------------------
  L  87: __init__(self, storage_config: HiCacheStorageConfig)

  L 132: warmup(self)

  L 139: register_buffer(self, buffer: torch.Tensor)
         ‚Üí None

  L 150: set(self, key, value: Optional[Any], target_location: Optional[List[int]], target_sizes: Optional[List[int]])
         ‚Üí bool

  L 159: batch_set(self, keys: List[str], target_location: Optional[List[int]], target_sizes: Optional[List[int]])
         ‚Üí bool

  L 200: get(self, key, target_location: Optional[Any], target_sizes: Optional[Any])
         ‚Üí bool

  L 208: batch_get(self, keys: List[str], target_location: Optional[Any], target_sizes: Optional[Any])
         ‚Üí int

  L 227: exists(self, key)
         ‚Üí bool

  L 230: batch_exists(self, keys)
         ‚Üí int

  L 247: delete(self, key)
         ‚Üí None

  L 250: close(self)

  L 255: clear(self)
         ‚Üí None


CLASS: MooncakeStoreConfig
----------------------------------------
  L  32: from_file()
         ‚Üí 'MooncakeStoreConfig'
         üìù Load the config from a JSON file.

  L  55: load_from_env()
         ‚Üí 'MooncakeStoreConfig'
         üìù Load config from a file specified in the environment variable.

  L  78: __post_init__(self)


============================================================
FILE: python/sglang/srt/mem_cache/storage/mooncake_store/unit_test.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L   5: def test_init_and_warmup()

  L  10: def test_register_buffer()

  L  16: def test_set_and_get()

  L  28: def test_exists()


============================================================
FILE: python/sglang/srt/mem_cache/storage/nixl/hicache_nixl.py
Functions: 9
============================================================


CLASS: HiCacheNixl
----------------------------------------
  L  29: __init__(self, file_path: str, plugin: str)
         üìù Initialize NIXL storage connector.

  L  49: register_buffers(self, buffers: Union[torch.Tensor, List[torch.Tensor], List[tuple]])
         ‚Üí Optional[Any]
         üìù Register tensor(s) or target locations in host memory (list of addr,le

  L  59: register_files(self, file_paths: List[str], open_file: Optional[bool])
         ‚Üí Optional[Any]
         üìù Register files with NIXL.

  L  66: register_objects(self, keys: List[str], sizes: Optional[List[int]])
         ‚Üí Optional[Any]
         üìù Register objects with NIXL.

  L 161: get(self, key: str, target_location: Optional[torch.Tensor | int], target_sizes: Optional[int])
         ‚Üí torch.Tensor | None

  L 176: batch_get(self, keys: List[str], target_locations: Optional[List[torch.Tensor | int]], target_sizes: Optional[List[int]])
         ‚Üí List[torch.Tensor | None]

  L 204: set(self, key: str, value: Optional[torch.Tensor], target_location: Optional[int], target_sizes: Optional[int])
         ‚Üí bool

  L 216: batch_set(self, keys: List[str], values: Optional[List[torch.Tensor]], target_locations: Optional[List[int]], target_sizes: Optional[List[int]])
         ‚Üí bool

  L 243: exists(self, key: str)
         ‚Üí bool


============================================================
FILE: python/sglang/srt/mem_cache/storage/nixl/nixl_utils.py
Functions: 11
============================================================


CLASS: NixlBackendSelection
----------------------------------------
  L  18: __init__(self, plugin: str)
         üìù Initialize backend selection.

  L  29: set_bucket(self, bucket_name: str)
         ‚Üí None
         üìù Set AWS bucket name in environment variable.

  L  34: create_backend(self, agent)
         ‚Üí bool
         üìù Create the appropriate NIXL backend based on configuration.


CLASS: NixlFileManager
----------------------------------------
  L 145: __init__(self, base_dir: str)
         üìù Initialize file manager.

  L 158: get_file_path(self, key: str)
         ‚Üí str
         üìù Get full file path for a given key.

  L 162: create_file(self, file_path: str)
         ‚Üí bool
         üìù Create a file if it doesn't exist.

  L 174: open_file(self, file_path: str)
         ‚Üí Optional[int]
         üìù Open a file and return its file descriptor.

  L 183: close_file(self, fd: int)
         ‚Üí bool
         üìù Close a file descriptor.

  L 192: files_to_nixl_tuples(self, file_paths: List[str])
         ‚Üí List[Tuple[int, int, int, str]]
         üìù Create NIXL tuples (offset, length, fd, file_path) for given files.


CLASS: NixlRegistration
----------------------------------------
  L  89: __init__(self, agent)

  L  92: create_query_tuples(self, key: str, mem_type: str, file_manager)
         ‚Üí List[Tuple]
         üìù Create NIXL tuples for querying memory.


============================================================
FILE: python/sglang/srt/mem_cache/storage/nixl/test_hicache_nixl_storage.py
Functions: 14
============================================================


CLASS: TestNixlUnified
----------------------------------------
  L  20: setUp(self)
         üìù Set up test environment.

  L  39: tearDown(self)
         üìù Clean up test directories.

  L  46: delete_test_file(self, file_path: str)
         ‚Üí bool
         üìù Helper method to delete a test file.

  L  62: verify_tensors_equal(self, expected: torch.Tensor, actual: torch.Tensor)
         üìù Helper to verify tensor equality.

  L  70: verify_tensor_lists_equal(self, expected: List[torch.Tensor], actual: List[torch.Tensor])
         üìù Helper to verify lists of tensors are equal.

  L  82: test_single_set_get(self)
         üìù Test single tensor set/get operations.

  L 115: test_batch_set_get(self)
         üìù Test batch tensor set/get operations.

  L 150: test_mixed_operations(self)
         üìù Test mixing single and batch operations.

  L 169: test_data_integrity(self)
         üìù Test data integrity across operations.

  L 195: test_basic_file_operations(self)
         üìù Test basic file operations.

  L 206: test_create_nixl_tuples(self)
         üìù Test creation of NIXL tuples.

  L 216: test_error_handling(self)
         üìù Test error handling in file operations.

  L 226: test_register_buffers(self)
         üìù Test registration of memory buffers.

  L 238: test_register_files_with_tuples(self)
         üìù Test registration of files using NIXL tuples.


============================================================
FILE: python/sglang/srt/mem_cache/swa_radix_cache.py
Functions: 38
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 113: def gen_swa_uuid()
         ‚Üí int


CLASS: LRUList
----------------------------------------
  L 119: __init__(self, swa: bool)

  L 168: reset_node_mru(self, node)
         üìù Move a (existing) node to most recently used position

  L 179: reset_node_and_parents_mru(self, node, root_node)
         üìù Move an (existing) node and its parents to most recently used position

  L 196: insert_mru(self, node)
         üìù Insert a (new) node as most recently used

  L 209: remove_node(self, node: TreeNode)
         üìù Remove node from lru list

  L 220: get_lru_no_lock(self)
         ‚Üí Optional[TreeNode]
         üìù Get the least recently used node that is not locked

  L 226: get_leaf_lru_no_lock(self)
         ‚Üí Optional[TreeNode]
         üìù Get the least recently used leaf node that is not locked

  L 232: get_prev_no_lock(self, node: TreeNode, check_id: bool)
         ‚Üí Optional[TreeNode]
         üìù Get the previous (i.e. more recently used) node that is not locked

  L 250: get_prev_leaf_no_lock(self, node: TreeNode, check_id: bool)
         üìù Get the previous (i.e. more recently used) leaf node that is not locke

  L 266: in_list(self, node: Optional[TreeNode])
         üìù Check if the node is in the lru list

  L 275: sanity_check_evictable_size(self)
         üìù Check the evictable size (i.e. the size of the nodes that are not lock

  L 287: sanity_check(self, tree_cache: 'SWARadixCache')
         üìù Check if the lru list is valid by rebuilding the lru list from the tre


CLASS: SWARadixCache
----------------------------------------
  L 340: __init__(self, req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: SWATokenToKVPoolAllocator, sliding_window_size: int, page_size: int, disable: bool)

  L 371: reset(self)
         ‚Üí None

  L 385: match_prefix(self, key: List[int])
         ‚Üí MatchResult
         üìù Find the matching prefix from the radix tree.

  L 422: insert(self, key: List, value, prev_prefix_len: int)
         ‚Üí int

  L 430: cache_finished_req(self, req: Req)
         ‚Üí None
         üìù Cache request when it finishes.

  L 467: cache_unfinished_req(self, req: Req)
         ‚Üí None
         üìù Cache request when it is unfinished.

  L 521: pretty_print(self)
         ‚Üí None

  L 526: total_size(self)
         ‚Üí Tuple[int, int]

  L 529: evict(self, full_num_tokens: int, swa_num_tokens: int)
         ‚Üí None

  L 612: inc_lock_ref(self, node: TreeNode)
         ‚Üí Optional[int]
         üìù Increment the lock reference count for the node. Returns the swa_uuid_

  L 653: dec_lock_ref(self, node: TreeNode, swa_uuid_for_lock: Optional[int])
         üìù Decrement the lock reference count for the node.

  L 690: sanity_check(self)

  L 694: evictable_size(self)
         ‚Üí Tuple[int, int]

  L 698: full_evictable_size(self)
         ‚Üí int

  L 701: swa_evictable_size(self)
         ‚Üí int

  L 705: full_lru_list_evictable_size(self)
         ‚Üí int

  L 709: swa_lru_list_evictable_size(self)
         ‚Üí int

  L 712: protected_size(self)
         ‚Üí Tuple[int, int]

  L 716: full_protected_size(self)
         ‚Üí int

  L 720: swa_protected_size(self)
         ‚Üí int

  L 724: all_values_flatten(self)
         ‚Üí torch.Tensor


CLASS: TreeNode
----------------------------------------
  L  47: __init__(self, id: Optional[int])

  L  81: evicted(self)

  L  85: backuped(self)

  L  88: __lt__(self, other: 'TreeNode')


============================================================
FILE: python/sglang/srt/metrics/collector.py
Functions: 12
============================================================


CLASS: SchedulerMetricsCollector
----------------------------------------
  L 153: __init__(self, labels: Dict[str, str])
         ‚Üí None

  L 275: increment_bootstrap_failed_reqs(self)
         ‚Üí None

  L 278: increment_transfer_failed_reqs(self)
         ‚Üí None

  L 281: log_stats(self, stats: SchedulerStats)
         ‚Üí None


CLASS: TimeStats
----------------------------------------
  L  51: __str__(self)
         ‚Üí str

  L 106: format_duration(self, duration: float)
         ‚Üí str

  L 109: get_type(self)
         ‚Üí RequestType
         üìù Determine the type of request based on timestamp values.


CLASS: TokenizerMetricsCollector
----------------------------------------
  L 310: __init__(self, labels: Dict[str, str], bucket_time_to_first_token: Optional[List[float]], bucket_inter_token_latency: Optional[List[float]], bucket_e2e_request_latency: Optional[List[float]], collect_tokens_histogram: bool)
         ‚Üí None

  L 516: observe_one_finished_request(self, prompt_tokens: int, generation_tokens: int, cached_tokens: int, e2e_latency: float, has_grammar: bool)

  L 536: observe_time_to_first_token(self, value: float)

  L 539: observe_inter_token_latency(self, internval: float, num_new_tokens: int)

  L 552: observe_one_aborted_request(self)


============================================================
FILE: python/sglang/srt/metrics/func_timer.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  26: def enable_func_timer()

  L  45: def exponential_buckets(start: float, width: float, length: int)
         ‚Üí List[float]

  L  52: def time_func_latency(func: Callable, name: Optional[str])
         ‚Üí Callable[..., Any]
         üìù A decorator to observe the latency of a function's execution. Supports


============================================================
FILE: python/sglang/srt/model_executor/cuda_graph_runner.py
Functions: 16
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  73: def get_is_capture_mode()

  L  78: def model_capture_mode()
         @contextmanager

  L  88: def freeze_gc(enable_cudagraph_gc: bool)
         üìù Optimize garbage collection during CUDA graph capture.
         @contextmanager

  L 117: def patch_model(model: torch.nn.Module,
        enable_compile: bool,
        num_tokens: int,
        tp_group: GroupCoordinator)
         üìù Patch the model to make it compatible with with torch.compile
         @contextmanager

  L 149: def set_torch_compile_config()

  L 165: def get_batch_sizes_to_capture(model_runner: ModelRunner)

  L 228: def get_global_graph_memory_pool()

  L 232: def set_global_graph_memory_pool(val)


CLASS: CudaGraphRunner
----------------------------------------
  L 240: __init__(self, model_runner: ModelRunner)

  L 393: can_run(self, forward_batch: ForwardBatch)

  L 445: capture(self)
         ‚Üí None

  L 520: capture_one_batch_size(self, bs: int, forward: Callable)

  L 675: recapture_if_needed(self, forward_batch: ForwardBatch)

  L 706: replay_prepare(self, forward_batch: ForwardBatch, pp_proxy_tensors: Optional[PPProxyTensors])

  L 796: replay(self, forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí Union[LogitsProcessorOutput, PPProxyTensors]

  L 826: get_spec_info(self, num_tokens: int)


============================================================
FILE: python/sglang/srt/model_executor/forward_batch_info.py
Functions: 44
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 875: def enable_num_token_non_padded(server_args)

  L 909: def compute_position(attn_backend: str,
        extend_prefix_lens: torch.Tensor,
        extend_seq_lens: torch.Tensor,
        extend_seq_lens_sum: int)

  L 928: def compute_position_triton(extend_prefix_lens: torch.Tensor,
        extend_seq_lens: torch.Tensor,
        extend_seq_lens_sum)
         üìù Compute positions. It is a fused version of `compute_position_torch`.

  L 955: def compute_position_kernel(positions,
        extend_start_loc,
        extend_prefix_lens,
        extend_seq_lens,
        has_prefix: tl.constexpr)
         @triton.jit

  L 984: def compute_position_torch(extend_prefix_lens: torch.Tensor,
        extend_seq_lens: torch.Tensor)

  L1002: def clamp_position(seq_lens)
         @torch.compile(dynamic=True, backend=get_compiler_backend(), disable=_is_npu)

  L1007: def create_chunked_prefix_cache_kv_indices(req_to_token_ptr,
        req_pool_indices_ptr,
        chunk_start_idx_ptr,
        chunk_seq_lens_ptr,
        chunk_cu_seq_lens_ptr,
        chunk_kv_indices_ptr,
        req_to_token_ptr_stride: tl.constexpr)
         @triton.jit


CLASS: CaptureHiddenMode
----------------------------------------
  L 151: need_capture(self)

  L 154: is_full(self)

  L 157: is_last(self)

  L 160: __lt__(self, other)


CLASS: ForwardBatch
----------------------------------------
  L 310: init_new(cls, batch: ModelWorkerBatch, model_runner: ModelRunner)

  L 456: merge_mm_inputs(self)
         ‚Üí Optional[MultimodalInputs]
         üìù Merge all multimodal inputs in the batch into a single MultiModalInput

  L 479: contains_image_inputs(self)
         ‚Üí bool

  L 487: contains_audio_inputs(self)
         ‚Üí bool

  L 495: contains_video_inputs(self)
         ‚Üí bool

  L 503: contains_mm_inputs(self)
         ‚Üí bool

  L 568: get_max_chunk_capacity(self)

  L 573: set_prefix_chunk_idx(self, idx: int)

  L 576: set_attn_attend_prefix_cache(self, attn_attend_prefix_cache: bool)

  L 579: prepare_chunked_kv_indices(self, device: torch.device)

  L 617: prepare_mlp_sync_batch(self, model_runner: ModelRunner)

  L 733: post_forward_mlp_sync_batch(self, logits_output: LogitsProcessorOutput)

  L 789: get_prefix_chunk_seq_lens(self, prefix_lens: torch.Tensor, num_prefix_chunks: int, prefix_chunk_len: int)

  L 812: prepare_chunked_prefix_cache_info(self, device: torch.device)

  L 871: can_run_tbo(self)


CLASS: ForwardMode
----------------------------------------
  L  92: is_prefill(self)

  L  95: is_extend(self)

  L 103: is_decode(self)

  L 106: is_mixed(self)

  L 109: is_idle(self)

  L 112: is_decode_or_idle(self)

  L 115: is_target_verify(self)

  L 118: is_draft_extend(self)

  L 121: is_extend_or_draft_extend_or_mixed(self)

  L 128: is_cuda_graph(self)

  L 135: is_dummy_first(self)

  L 138: is_split_prefill(self)


CLASS: PPProxyTensors
----------------------------------------
  L 883: __init__(self, tensors)

  L 890: __getitem__(self, key: Union[str, slice])

  L 896: __setitem__(self, key: str, value: torch.Tensor)

  L 899: __len__(self)

  L 902: __eq__(self, other: object)

  L 905: __repr__(self)
         ‚Üí str


============================================================
FILE: python/sglang/srt/model_executor/model_runner.py
Functions: 35
============================================================


CLASS: LocalSerializedTensor
----------------------------------------
  L1909: get(self, rank: int)


CLASS: ModelRunner
----------------------------------------
  L 162: __init__(self, model_config: ModelConfig, mem_fraction_static: float, gpu_id: int, tp_rank: int, tp_size: int, moe_ep_rank: int, moe_ep_size: int, pp_rank: int, pp_size: int, nccl_port: int, server_args: ServerArgs, dp_rank: Optional[int], is_draft_worker: bool, req_to_token_pool: Optional[ReqToTokenPool], token_to_kv_pool_allocator: Optional[BaseTokenToKVPoolAllocator])

  L 255: initialize(self, min_per_gpu_memory: float)

  L 382: model_specific_adjustment(self)

  L 551: init_torch_distributed(self)

  L 647: load_model(self)

  L 754: update_expert_location(self, new_expert_location_metadata: ExpertLocationMetadata, update_layer_ids: List[int])

  L 767: update_weights_from_disk(self, model_path: str, load_format: str)
         ‚Üí tuple[bool, str]
         üìù Update engine weights in-place from the disk.

  L 822: init_weights_update_group(self, master_address, master_port, rank_offset, world_size, group_name, backend)
         üìù Initialize the Torch process group for model parameter updates.

  L 867: update_weights_from_distributed(self, names, dtypes, shapes, group_name)
         üìù Update specific parameter in the model weights online

  L 915: update_weights_from_tensor(self, named_tensors: List[Tuple[str, Union[torch.Tensor, 'LocalSerializedTensor']]], load_format: Optional[str])

  L 978: get_weights_by_name(self, name: str, truncate_size: int)
         ‚Üí Optional[torch.Tensor]
         üìù Get the weights of the parameter by its name. Similar to `get_paramete

  L 995: init_lora_manager(self)

  L1010: load_lora_adapter(self, lora_ref: LoRARef)
         üìù Load a new lora adapter from disk or huggingface.

  L1027: unload_lora_adapter(self, lora_ref: LoRARef)
         üìù Unload a lora adapter that was previously loaded during initialization

  L1044: profile_max_num_token(self, total_gpu_memory: int)

  L1079: set_num_token_hybrid(self)

  L1161: init_memory_pool(self, total_gpu_memory: int, max_num_reqs: Optional[int], max_total_tokens: Optional[int])

  L1414: init_cublas(self)
         üìù We need to run a small matmul to init cublas. Otherwise, it will raise

  L1423: init_attention_backend(self)
         üìù Init attention kernel backend.

  L1585: init_double_sparsity_channel_config(self, selected_channel)

  L1602: init_device_graphs(self)
         üìù Capture cuda graphs.

  L1629: init_threads_binding(self)

  L1653: apply_torch_tp(self)

  L1660: forward_decode(self, forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors)
         ‚Üí LogitsProcessorOutput

  L1679: forward_extend(self, forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors)
         ‚Üí LogitsProcessorOutput

  L1702: forward_idle(self, forward_batch: ForwardBatch, pp_proxy_tensors)
         ‚Üí LogitsProcessorOutput

  L1715: forward_split_prefill(self, forward_batch: ForwardBatch, reinit_attn_backend: bool, forward_count: int)
         ‚Üí LogitsProcessorOutput

  L1736: forward(self, forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors: Optional[PPProxyTensors], reinit_attn_backend: bool, split_forward_count: int)
         ‚Üí Tuple[Union[LogitsProcessorOutput, PPProxyTensors], bool]

  L1830: sample(self, logits_output: LogitsProcessorOutput, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor
         üìù Sample and compute logprobs and update logits_output.

  L1864: model_is_mrope(self)
         ‚Üí bool
         üìù Detect if the model has "mrope" rope_scaling type.

  L1873: save_remote_model(self, url: str)

  L1879: save_sharded_model(self, path: str, pattern: Optional[str], max_size: Optional[int])


CLASS: RankZeroFilter
----------------------------------------
  L 149: __init__(self, is_rank_zero)

  L 153: filter(self, record)


============================================================
FILE: python/sglang/srt/model_executor/npu_graph_runner.py
Functions: 2
============================================================


CLASS: NPUGraphRunner
----------------------------------------
  L  38: __init__(self, model_runner: ModelRunner)

  L  62: replay(self, forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí Union[LogitsProcessorOutput, PPProxyTensors]


============================================================
FILE: python/sglang/srt/model_loader/__init__.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  15: def get_model()
         ‚Üí nn.Module


============================================================
FILE: python/sglang/srt/model_loader/loader.py
Functions: 30
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  76: def device_loading_context(module: torch.nn.Module, target_device: torch.device)
         @contextmanager

  L1523: def load_model_with_cpu_quantization(self)
         ‚Üí nn.Module

  L1555: def get_model_loader(load_config: LoadConfig)
         ‚Üí BaseModelLoader
         üìù Get a model loader based on the load format.


CLASS: BaseModelLoader
----------------------------------------
  L 213: __init__(self, load_config: LoadConfig)

  L 217: download_model(self, model_config: ModelConfig)
         ‚Üí None
         üìù Download a model so that it can be immediately loaded.

  L 222: load_model(self)
         ‚Üí nn.Module
         üìù Load a model with the given configurations.


CLASS: BitsAndBytesModelLoader
----------------------------------------
  L 818: __init__(self, load_config: LoadConfig)

  L1250: download_model(self, model_config: ModelConfig)
         ‚Üí None

  L1253: load_model(self)
         ‚Üí nn.Module


CLASS: DefaultModelLoader
----------------------------------------
  L 263: __init__(self, load_config: LoadConfig)

  L 448: download_model(self, model_config: ModelConfig)
         ‚Üí None

  L 453: load_model(self)
         ‚Üí nn.Module

  L 474: load_weights_and_postprocess(model, weights, target_device)


CLASS: DummyModelLoader
----------------------------------------
  L 564: __init__(self, load_config: LoadConfig)

  L 572: download_model(self, model_config: ModelConfig)
         ‚Üí None

  L 575: load_model(self)
         ‚Üí nn.Module


CLASS: GGUFModelLoader
----------------------------------------
  L1278: __init__(self, load_config: LoadConfig)

  L1342: download_model(self, model_config: ModelConfig)
         ‚Üí None

  L1345: load_model(self)
         ‚Üí nn.Module


CLASS: LayeredModelLoader
----------------------------------------
  L 493: __init__(self, load_config: LoadConfig)

  L 498: load_model(self)
         ‚Üí nn.Module


CLASS: RemoteModelLoader
----------------------------------------
  L1379: __init__(self, load_config: LoadConfig)

  L1401: download_model(self, model_config: ModelConfig)
         ‚Üí None

  L1405: save_model(model: torch.nn.Module, model_path: str, url: str)
         ‚Üí None

  L1482: load_model(self)
         ‚Üí nn.Module


CLASS: ShardedStateLoader
----------------------------------------
  L 629: __init__(self, load_config: LoadConfig)

  L 693: download_model(self, model_config: ModelConfig)
         ‚Üí None

  L 696: load_model(self)
         ‚Üí nn.Module

  L 757: save_model(model: torch.nn.Module, path: str, pattern: Optional[str], max_size: Optional[int])
         ‚Üí None


CLASS: Source
----------------------------------------
  L 255: init_new(cls, model_config: ModelConfig, model)


============================================================
FILE: python/sglang/srt/model_loader/utils.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  19: def set_default_torch_dtype(dtype: torch.dtype)
         üìù Sets the default torch dtype to the given dtype.
         @contextlib.contextmanager

  L  27: def resolve_transformers_arch(model_config: ModelConfig,
        architectures: list[str])

  L  82: def get_model_architecture(model_config: ModelConfig)
         ‚Üí Tuple[Type[nn.Module], str]

  L 106: def get_architecture_class_name(model_config: ModelConfig)
         ‚Üí str


============================================================
FILE: python/sglang/srt/model_loader/weight_utils.py
Functions: 35
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  51: def enable_hf_transfer()
         üìù automatically activates hf_transfer

  L  71: def get_lock(model_name_or_path: str, cache_dir: Optional[str])

  L  94: def convert_bin_to_safetensor_file(pt_filename: str, sf_filename: str)
         ‚Üí None

  L 134: def get_quant_config(model_config: ModelConfig,
        load_config: LoadConfig,
        packed_modules_mapping: Dict[str,
        List[str]])
         ‚Üí QuantizationConfig

  L 238: def download_weights_from_hf(model_name_or_path: str,
        cache_dir: Optional[str],
        allow_patterns: List[str],
        revision: Optional[str],
        ignore_patterns: Optional[Union[str,
        List[str]]])
         ‚Üí str
         üìù Download model weights from Hugging Face Hub.

  L 290: def download_safetensors_index_file_from_hf(model_name_or_path: str,
        index_file: str,
        cache_dir: Optional[str],
        revision: Optional[str])
         ‚Üí None
         üìù Download hf safetensors index file from Hugging Face Hub.

  L 329: def filter_duplicate_safetensors_files(hf_weights_files: List[str],
        hf_folder: str,
        index_file: str)
         ‚Üí List[str]

  L 350: def filter_files_not_needed_for_inference(hf_weights_files: List[str])
         ‚Üí List[str]
         üìù Exclude files that are not needed for inference.

  L 376: def np_cache_weights_iterator(model_name_or_path: str,
        cache_dir: Optional[str],
        hf_folder: str,
        hf_weights_files: List[str])
         ‚Üí Generator[Tuple[str, torch.Tensor], None, None]
         üìù Iterate over the weights in the model np files.

  L 424: def decrypt(fn, key)

  L 428: def safetensors_encrypted_weights_iterator(hf_weights_files: List[str],
        is_all_weights_sharded: bool,
        decryption_key: Optional[str])

  L 436: def safetensors_weights_iterator(hf_weights_files: List[str],
        is_all_weights_sharded: bool,
        decryption_key: Optional[str],
        disable_mmap: bool)
         ‚Üí Generator[Tuple[str, torch.Tensor], None, None]
         üìù Iterate over the weights in the model safetensor files.

  L 473: def multi_thread_safetensors_weights_iterator(hf_weights_files: List[str],
        is_all_weights_sharded: bool,
        decryption_key: Optional[str],
        max_workers: int,
        disable_mmap: bool)
         ‚Üí Generator[Tuple[str, torch.Tensor], None, None]
         üìù Multi-Thread iterate over the weights in the model safetensor files.

  L 528: def pt_weights_iterator(hf_weights_files: List[str])
         ‚Üí Generator[Tuple[str, torch.Tensor], None, None]
         üìù Iterate over the weights in the model bin/pt files.

  L 546: def multi_thread_pt_weights_iterator(hf_weights_files: List[str],
        max_workers: int)
         ‚Üí Generator[Tuple[str, torch.Tensor], None, None]
         üìù Multi-Thread iterate over the weights in the model bin/pt files.

  L 579: def get_gguf_extra_tensor_names(gguf_file: str,
        gguf_to_hf_name_map: Dict[str,
        str])
         ‚Üí List[str]

  L 591: def gguf_quant_weights_iterator(gguf_file: str,
        gguf_to_hf_name_map: Dict[str,
        str])
         ‚Üí Generator[Tuple[str, torch.Tensor], None, None]
         üìù Iterate over the quant weights in the model gguf files and convert

  L 625: def convert_pyslice_to_tensor(x: Any)
         ‚Üí torch.Tensor
         üìù convert PySafeSlice object from safetensors to torch.Tensor

  L 640: def default_weight_loader(param: torch.Tensor, loaded_weight: torch.Tensor)
         ‚Üí None
         üìù Default weight loader.

  L 661: def row_parallel_weight_loader(param: torch.Tensor, loaded_weight: torch.Tensor)
         ‚Üí None
         üìù Load weights that are row-parallelized.

  L 679: def sharded_weight_loader(shard_axis: int)
         ‚Üí LoaderFunction
         üìù Create a weight loader that shards the weights along the given axis

  L 694: def composed_weight_loader(loader: LoaderFunction,
        fn: Callable[[torch.Tensor],
        torch.Tensor])
         ‚Üí LoaderFunction
         üìù Create a weight loader that post-processes the weights after loading

  L 707: def runai_safetensors_weights_iterator(hf_weights_files: List[str])
         ‚Üí Generator[Tuple[str, torch.Tensor], None, None]
         üìù Iterate over the weights in the model safetensor files.

  L 728: def set_runai_streamer_env(load_config: LoadConfig)

  L 752: def initialize_dummy_weights(model: torch.nn.Module,
        low: float,
        high: float,
        seed: int)
         ‚Üí None
         üìù Initialize model weights with random values.

  L 784: def maybe_remap_kv_scale_name(name: str, params_dict: dict)
         ‚Üí Optional[str]
         üìù Remap the name of FP8 k/v_scale parameters.

  L 935: def kv_cache_scales_loader(filename: str,
        tp_rank: int,
        tp_size: int,
        num_hidden_layers: int,
        model_type: Optional[str])
         ‚Üí Iterable[Tuple[int, float]]
         üìù A simple utility to read in KV cache scaling factors that have been

  L 978: def get_actual_shard_size(shard_size, weight_start, weight_end)

  L 985: def reset_param_data_if_needed(param_data, dim, start, length)

  L 995: def narrow_padded_param_and_loaded_weight(param_data,
        loaded_weight,
        param_data_start,
        weight_start,
        dim,
        shard_size,
        narrow_weight)


CLASS: DisabledTqdm
----------------------------------------
  L  67: __init__(self)


CLASS: KVCacheQuantSchema
----------------------------------------
  L 870: check_is_fp8(self)
         ‚Üí 'KVCacheQuantSchema'

  L 878: check_tp_ranks(self, info: ValidationInfo)
         ‚Üí 'KVCacheQuantSchema'

  L 900: check_current_rank(self, info: ValidationInfo)
         ‚Üí 'KVCacheQuantSchema'


CLASS: QuantParamSchema
----------------------------------------
  L 922: check_model_type(self, info: ValidationInfo)
         ‚Üí 'QuantParamSchema'


============================================================
FILE: python/sglang/srt/model_parallel.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 121: def tensor_parallel(module: torch.nn.Module, device_mesh: Optional[DeviceMesh])
         üìù Tensor parallelize the model across the given device mesh.


============================================================
FILE: python/sglang/srt/models/arcee.py
Functions: 16
============================================================


CLASS: ArceeAttention
----------------------------------------
  L 105: __init__(self, config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str, bias: bool)
         ‚Üí None

  L 178: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: ArceeDecoderLayer
----------------------------------------
  L 193: __init__(self, config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 241: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: ArceeForCausalLM
----------------------------------------
  L 393: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 432: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí LogitsProcessorOutput

  L 468: start_layer(self)

  L 472: end_layer(self)

  L 475: get_input_embeddings(self)
         ‚Üí nn.Embedding

  L 478: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 528: load_kv_cache_scales(self, quantization_param_path: str)
         ‚Üí None


CLASS: ArceeMLP
----------------------------------------
  L  63: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str, reduce_results: bool)
         ‚Üí None

  L  97: forward(self, x, forward_batch)


CLASS: ArceeModel
----------------------------------------
  L 267: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 304: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]], PPProxyTensors]

  L 350: load_kv_cache_scales(self, quantization_param_path: str)
         ‚Üí None


============================================================
FILE: python/sglang/srt/models/baichuan.py
Functions: 12
============================================================


CLASS: BaiChuanAttention
----------------------------------------
  L 118: __init__(self, hidden_size: int, num_heads: int, position_embedding: str, rope_theta: float, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], layer_id: int, prefix: str)

  L 202: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: BaiChuanBaseForCausalLM
----------------------------------------
  L 348: __init__(self, config: PretrainedConfig, position_embedding: str, quant_config: Optional[QuantizationConfig], prefix: str)

  L 374: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 385: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: BaiChuanDecoderLayer
----------------------------------------
  L 219: __init__(self, config: PretrainedConfig, position_embedding: str, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 253: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: BaiChuanMLP
----------------------------------------
  L  78: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)

  L 108: forward(self, x)


CLASS: BaiChuanModel
----------------------------------------
  L 280: __init__(self, config: PretrainedConfig, position_embedding: str, quant_config: Optional[QuantizationConfig], prefix: str)

  L 310: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: BaichuanForCausalLM
----------------------------------------
  L 429: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)


============================================================
FILE: python/sglang/srt/models/bailing_moe.py
Functions: 13
============================================================


CLASS: BailingAttention
----------------------------------------
  L  41: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 103: forward(self, hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: BailingMLP
----------------------------------------
  L 119: __init__(self, intermediate_size: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], reduce_results: Optional[bool], prefix: str)
         ‚Üí None

  L 145: forward(self, x)


CLASS: BailingMoE
----------------------------------------
  L 154: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 201: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: BailingMoeBlock
----------------------------------------
  L 224: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 246: forward(self, hidden_states: torch.Tensor, position_ids: torch.Tensor, residual: Optional[torch.Tensor], forward_batch: ForwardBatch)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: BailingMoeForCausalLM
----------------------------------------
  L 338: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig])
         ‚Üí None

  L 356: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor])
         ‚Üí torch.Tensor

  L 368: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: BailingMoeModel
----------------------------------------
  L 279: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 311: forward(self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/bert.py
Functions: 24
============================================================


CLASS: BertAttention
----------------------------------------
  L 177: __init__(self, hidden_size: int, num_attention_heads: int, layer_norm_eps: float, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 203: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: BertEmbedding
----------------------------------------
  L  27: __init__(self, config: BertConfig)

  L  51: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: BertEncoder
----------------------------------------
  L 100: __init__(self, config: BertConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 121: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: BertForSequenceClassification
----------------------------------------
  L 439: __init__(self)

  L 458: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 478: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         ‚Üí torch.Tensor


CLASS: BertIntermediate
----------------------------------------
  L 295: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)

  L 313: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: BertLayer
----------------------------------------
  L 131: __init__(self, config: BertConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 167: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)


CLASS: BertModel
----------------------------------------
  L 351: __init__(self)

  L 375: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         ‚Üí torch.Tensor

  L 399: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])
         ‚Üí Set[str]


CLASS: BertOutput
----------------------------------------
  L 321: __init__(self, hidden_size: int, intermediate_size: int, layer_norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str)

  L 341: forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: BertPooler
----------------------------------------
  L  81: __init__(self, config: BertConfig)

  L  86: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: BertSelfAttention
----------------------------------------
  L 212: __init__(self, hidden_size: int, num_attention_heads: int, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 257: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: BertSelfOutput
----------------------------------------
  L 268: __init__(self, hidden_size: int, layer_norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str)

  L 285: forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/chatglm.py
Functions: 13
============================================================


CLASS: ChatGLMForCausalLM
----------------------------------------
  L 380: __init__(self, config: ChatGLMConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 397: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 408: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: ChatGLMM
----------------------------------------
  L 321: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 348: forward(self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: GLMAttention
----------------------------------------
  L  50: __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 120: forward(self, hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: GLMBlock
----------------------------------------
  L 193: __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 227: forward(self, hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: GLMMLP
----------------------------------------
  L 147: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 177: forward(self, hidden_states)


CLASS: GLMTransformer
----------------------------------------
  L 268: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 300: forward(self, hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/clip.py
Functions: 26
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 518: def monkey_patch_weight_loader()


CLASS: CLIPEncoder
----------------------------------------
  L 219: __init__(self, config: CLIPVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 244: forward(self, inputs_embeds: torch.Tensor, attention_mask: torch.Tensor, causal_attention_mask: torch.Tensor, return_all_hidden_states: bool)
         ‚Üí Union[torch.Tensor, list[torch.Tensor]]


CLASS: CLIPEncoderLayer
----------------------------------------
  L 139: __init__(self, config: CLIPVisionConfig, act_layer: Type[nn.Module], norm_layer: Type[nn.Module], attn_implementation: Optional[str], quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 180: forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, causal_attention_mask: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: CLIPMLP
----------------------------------------
  L 108: __init__(self, config, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)

  L 130: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: CLIPModel
----------------------------------------
  L 407: __init__(self, config: CLIPConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 454: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)

  L 486: pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)

  L 490: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: CLIPTextEmbeddings
----------------------------------------
  L  68: __init__(self, config: CLIPTextConfig)

  L  84: forward(self, input_ids: Optional[torch.LongTensor], position_ids: Optional[torch.LongTensor], inputs_embeds: Optional[torch.FloatTensor])
         ‚Üí torch.Tensor


CLASS: CLIPTextModel
----------------------------------------
  L 307: __init__(self, config: CLIPTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 321: forward(self, input_ids: torch.Tensor, position_ids: torch.Tensor)


CLASS: CLIPTextTransformer
----------------------------------------
  L 266: __init__(self, config: CLIPTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 284: device(self)
         ‚Üí torch.device

  L 287: forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor], position_ids: Optional[torch.Tensor])


CLASS: CLIPVisionEmbeddings
----------------------------------------
  L  25: __init__(self, config: CLIPVisionConfig)

  L  52: forward(self, pixel_values: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: CLIPVisionModel
----------------------------------------
  L 387: __init__(self, config: CLIPVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 399: device(self)
         ‚Üí torch.device

  L 402: forward(self, pixel_values: torch.Tensor)


CLASS: CLIPVisionTransformer
----------------------------------------
  L 331: __init__(self, config: CLIPVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 364: device(self)
         ‚Üí torch.device

  L 367: forward(self, pixel_values: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/commandr.py
Functions: 15
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  72: def layer_norm_func(hidden_states, weight, variance_epsilon)
         @torch.compile(backend=get_compiler_backend())


CLASS: CohereAttention
----------------------------------------
  L 143: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 228: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: CohereDecoderLayer
----------------------------------------
  L 245: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 271: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: CohereForCausalLM
----------------------------------------
  L 342: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 357: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 372: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: CohereMLP
----------------------------------------
  L 109: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 135: forward(self, x)


CLASS: CohereModel
----------------------------------------
  L 294: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 321: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: LayerNorm
----------------------------------------
  L  83: __init__(self, param_shape, eps)

  L  89: forward(self, hidden_states, residuals)

  L  95: weight_loader(self, param: Parameter, loaded_weight: torch.Tensor)


============================================================
FILE: python/sglang/srt/models/dbrx.py
Functions: 16
============================================================


CLASS: DbrxAttention
----------------------------------------
  L 195: __init__(self, config: DbrxConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 262: forward(self, position_ids: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: DbrxBlock
----------------------------------------
  L 317: __init__(self, config: DbrxConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 333: forward(self, position_ids: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: DbrxExperts
----------------------------------------
  L  90: __init__(self, config: DbrxConfig, quant_config: Optional[QuantizationConfig], params_dtype: Optional[torch.dtype], prefix: str)

  L 146: weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor, weight_name: str)

  L 174: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: DbrxForCausalLM
----------------------------------------
  L 397: __init__(self, config: DbrxConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 420: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 431: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: DbrxFusedNormAttention
----------------------------------------
  L 279: __init__(self, config: DbrxConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 297: forward(self, position_ids: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: DbrxModel
----------------------------------------
  L 350: __init__(self, config: DbrxConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 378: forward(self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: DbrxRouter
----------------------------------------
  L  59: __init__(self, config: DbrxConfig, params_dtype: Optional[torch.dtype], prefix: str)

  L  77: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/deepseek.py
Functions: 15
============================================================


CLASS: DeepseekAttention
----------------------------------------
  L 196: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 266: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: DeepseekDecoderLayer
----------------------------------------
  L 282: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 328: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí torch.Tensor


CLASS: DeepseekForCausalLM
----------------------------------------
  L 409: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 429: get_input_embeddings(self)
         ‚Üí nn.Embedding

  L 433: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor

  L 445: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: DeepseekMLP
----------------------------------------
  L  56: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str)
         ‚Üí None

  L  88: forward(self, x)


CLASS: DeepseekMoE
----------------------------------------
  L  97: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 152: pack_params(self)

  L 171: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: DeepseekModel
----------------------------------------
  L 357: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 384: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/deepseek_janus_pro.py
Functions: 84
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  82: def named_apply(fn: Callable,
        module: nn.Module,
        name,
        depth_first: bool,
        include_root: bool)
         ‚Üí nn.Module

  L 105: def VQ_16()

  L 176: def trunc_normal_tf_(tensor: torch.Tensor,
        mean: float,
        std: float,
        a: float,
        b: float)
         üìù Fills the input Tensor with values drawn from a truncated

  L 214: def nchw_to(x: torch.Tensor, fmt: Format)

  L 224: def resample_patch_embed(patch_embed,
        new_size: List[int],
        interpolation: str,
        antialias: bool,
        verbose: bool)
         üìù Resample the weights of the patch embedding kernel to target resolutio

  L 473: def drop_path(x, drop_prob: float, training: bool, scale_by_keep: bool)
         üìù Drop paths (Stochastic Depth) per sample (when applied in main path of

  L 625: def resample_abs_pos_embed(posemb: torch.Tensor,
        new_size: List[int],
        old_size: Optional[List[int]],
        num_prefix_tokens: int,
        interpolation: str,
        antialias: bool,
        verbose: bool)

  L 673: def init_weights(self)

  L 679: def init_weights_vit_timm(module: nn.Module, name: str)
         ‚Üí None
         üìù ViT weight initialization, original timm impl (for reproducibility)

  L 984: def model_name_to_cls(cls_name)

  L1054: def create_siglip_vit(model_name: str,
        image_size: int,
        select_layer: int,
        ckpt_path: str)

  L1328: def use_fused_attn(experimental: bool)
         ‚Üí bool

  L1790: def nonlinearity(x)

  L1795: def Normalize(in_channels, norm_type)

  L1847: def compute_entropy_loss(affinity, loss_type, temperature)


CLASS: AttentionPoolLatent
----------------------------------------
  L1342: __init__(self, in_features: int, out_features: int, embed_dim: int, num_heads: int, feat_size: Optional[int], mlp_ratio: float, qkv_bias: bool, qk_norm: bool, latent_len: int, latent_dim: int, pos_embed: str, pool_type: str, norm_layer: Optional[nn.Module], drop: float)

  L1394: init_weights(self)

  L1399: forward(self, x)


CLASS: AttnBlock
----------------------------------------
  L1753: __init__(self, in_channels, norm_type)

  L1763: forward(self, x)


CLASS: CLIPVisionTower
----------------------------------------
  L1138: __init__(self, model_name: str, image_size: Union[Tuple[int, int], int], select_feature: str, select_layer: int, select_layers: list, ckpt_path: str, pixel_mean: Optional[List[float]], pixel_std: Optional[List[float]])

  L1176: device(self)
         ‚Üí torch.device

  L1180: dtype(self)

  L1183: build_vision_tower(self, vision_tower_params)

  L1201: feature_select(self, image_forward_outs)

  L1220: forward(self, images)
         üìù Args:


CLASS: Decoder
----------------------------------------
  L1524: __init__(self, z_channels, ch, ch_mult, num_res_blocks, norm_type, dropout, resamp_with_conv, out_channels)

  L1586: last_layer(self)

  L1589: forward(self, z)


CLASS: Downsample
----------------------------------------
  L1828: __init__(self, in_channels, with_conv)

  L1837: forward(self, x)


CLASS: DropPath
----------------------------------------
  L 500: __init__(self, drop_prob: float, scale_by_keep: bool)

  L 505: forward(self, x)

  L 508: extra_repr(self)


CLASS: Encoder
----------------------------------------
  L1443: __init__(self, in_channels, ch, ch_mult, num_res_blocks, norm_type, dropout, resamp_with_conv, z_channels)

  L1501: forward(self, x)


CLASS: LayerScale
----------------------------------------
  L1301: __init__(self, dim: int, init_values: float, inplace: bool)
         ‚Üí None

  L1311: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Mlp
----------------------------------------
  L 436: __init__(self, in_features, hidden_features, out_features, act_layer, norm_layer, bias, drop, use_conv)

  L 463: forward(self, x)


CLASS: MlpProjector
----------------------------------------
  L1239: __init__(self, cfg)

  L1274: forward(self, x_or_tuple: Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor])
         üìù Args:


CLASS: MultiModalityCausalLM
----------------------------------------
  L1924: __init__(self, config: MultiModalityConfig, quant_config: Optional[QuantizationConfig])

  L1962: get_image_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor

  L1978: get_input_embeddings(self)
         ‚Üí nn.Embedding

  L1982: forward(self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)
         ‚Üí torch.Tensor

  L1999: prepare_gen_img_embeds(self, image_ids: torch.LongTensor)

  L2002: pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)

  L2011: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Normalize
----------------------------------------
  L1116: __init__(self, mean, std, inplace)

  L1123: forward(self, tensor: Tensor)
         ‚Üí Tensor
         üìù Args:

  L1133: __repr__(self)
         ‚Üí str


CLASS: PatchDropout
----------------------------------------
  L 573: __init__(self, prob: float, num_prefix_tokens: int, ordered: bool, return_indices: bool)

  L 589: forward(self, x)
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]]]


CLASS: PatchEmbed
----------------------------------------
  L 308: __init__(self, img_size: Optional[int], patch_size: int, in_chans: int, embed_dim: int, norm_layer: Optional[Callable], flatten: bool, output_fmt: Optional[str], bias: bool, strict_img_size: bool, dynamic_img_pad: bool)

  L 349: set_input_size(self, img_size: Optional[Union[int, Tuple[int, int]]], patch_size: Optional[Union[int, Tuple[int, int]]])

  L 379: feat_ratio(self, as_scalar)
         ‚Üí Union[Tuple[int, int], int]

  L 385: dynamic_feat_size(self, img_size: Tuple[int, int])
         ‚Üí Tuple[int, int]
         üìù Get grid (feature) size for given image size taking account of dynamic

  L 396: forward(self, x)


CLASS: ResnetBlock
----------------------------------------
  L1700: __init__(self, in_channels, out_channels, conv_shortcut, dropout, norm_type)

  L1734: forward(self, x)


CLASS: Upsample
----------------------------------------
  L1806: __init__(self, in_channels, with_conv)

  L1814: forward(self, x)


CLASS: VQModel
----------------------------------------
  L1864: __init__(self, config: ModelArgs)

  L1891: encode(self, x)

  L1897: decode(self, quant)

  L1902: decode_code(self, code_b, shape, channel_first)

  L1907: forward(self, input)


CLASS: VectorQuantizer
----------------------------------------
  L1614: __init__(self, n_e, e_dim, beta, entropy_loss_ratio, l2_norm, show_usage)

  L1633: forward(self, z)

  L1681: get_codebook_entry(self, indices, shape, channel_first)


CLASS: VisionTransformer
----------------------------------------
  L 698: __init__(self, img_size: Union[int, Tuple[int, int]], patch_size: Union[int, Tuple[int, int]], in_chans: int, num_classes: int, global_pool: Literal['', 'avg', 'token', 'map'], embed_dim: int, depth: int, num_heads: int, mlp_ratio: float, qkv_bias: bool, qk_norm: bool, init_values: Optional[float], class_token: bool, no_embed_class: bool, reg_tokens: int, pre_norm: bool, fc_norm: Optional[bool], dynamic_img_size: bool, dynamic_img_pad: bool, drop_rate: float, pos_drop_rate: float, patch_drop_rate: float, proj_drop_rate: float, attn_drop_rate: float, drop_path_rate: float, weight_init: Literal['skip', 'jax', 'jax_nlhb', 'moco', ''], embed_layer: Callable, _norm_layer: Optional[LayerType], _act_layer: Optional[LayerType], block_fn: Type[nn.Module], mlp_layer: Type[nn.Module], ignore_head: bool)
         ‚Üí None
         üìù Args:

  L 864: init_weights(self, mode: Literal['jax', 'jax_nlhb', 'moco', ''])
         ‚Üí None

  L 873: no_weight_decay(self)
         ‚Üí Set

  L 877: group_matcher(self, coarse: bool)
         ‚Üí Dict

  L 884: get_classifier(self)
         ‚Üí nn.Module

  L 887: reset_classifier(self, num_classes: int, global_pool)
         ‚Üí None

  L 957: forward_features(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L 966: forward_head(self, x: torch.Tensor, pre_logits: bool)
         ‚Üí torch.Tensor

  L 977: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: VisionTransformerBlock
----------------------------------------
  L 513: __init__(self, dim: int, num_heads: int, mlp_ratio: float, qkv_bias: bool, qk_norm: bool, proj_drop: float, attn_drop: float, init_values: Optional[float], drop_path: float, act_layer: nn.Module, norm_layer: nn.Module, mlp_layer: nn.Module)
         ‚Üí None

  L 557: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: vision_head
----------------------------------------
  L1003: __init__(self, params)

  L1013: forward(self, x)


============================================================
FILE: python/sglang/srt/models/deepseek_nextn.py
Functions: 5
============================================================


CLASS: DeepseekModelNextN
----------------------------------------
  L  42: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  80: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: DeepseekV3ForCausalLMNextN
----------------------------------------
  L 128: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 155: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 166: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/deepseek_v2.py
Functions: 59
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 736: def yarn_get_mscale(scale: float, mscale: float)
         ‚Üí float


CLASS: DeepseekV2AttentionMLA
----------------------------------------
  L 746: __init__(self, config: PretrainedConfig, hidden_size: int, num_heads: int, qk_nope_head_dim: int, qk_rope_head_dim: int, v_head_dim: int, q_lora_rank: int, kv_lora_rank: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], reduce_results: bool, layer_id: int, prefix: str, alt_stream: Optional[torch.cuda.Stream])
         ‚Üí None

  L 970: dispatch_attn_forward_method(self, forward_batch: ForwardBatch)
         ‚Üí AttnForwardMethod

  L1060: op_prepare(self, state)

  L1068: op_core(self, state)

  L1073: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)

  L1088: forward_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)

  L1130: forward_core(self, intermediate_state)

  L1150: forward_normal_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)

  L1194: forward_normal_core(self, q, k, v, forward_batch)

  L1210: forward_absorb_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)

  L1295: forward_absorb_core(self, q_pe, k_pe, q_nope_out, k_nope, forward_batch, zero_allocator)

  L1382: forward_absorb_fused_mla_rope_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)

  L1496: forward_absorb_fused_mla_rope_cpu_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)

  L1547: forward_absorb_fused_mla_rope_core(self, q_input, key_cache_buf, val_cache_buf, attn_output, kv_indptr, kv_indices, k_pe_output, cos_sin_cache, positions, attn_logits, num_kv_split, sm_scale, enable_rope_fusion, k_input, forward_batch, zero_allocator)

  L1620: forward_absorb_fused_mla_rope_cpu_core(self, q_input, k_input, v_input, forward_batch, zero_allocator)

  L1708: forward_normal_chunked_kv_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)

  L1726: forward_normal_chunked_kv_core(self, q, k, v, forward_batch)


CLASS: DeepseekV2DecoderLayer
----------------------------------------
  L1757: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], is_nextn: bool, prefix: str, alt_stream: Optional[torch.cuda.Stream])
         ‚Üí None

  L1852: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], zero_allocator: BumpAllocator)
         ‚Üí torch.Tensor

  L1900: op_comm_prepare_attn(self, state, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], zero_allocator: BumpAllocator, tbo_subbatch_index: Optional[int])

  L1922: op_comm_prepare_mlp(self, state)

  L1931: op_mlp(self, state)

  L1944: op_comm_postprocess_layer(self, state)


CLASS: DeepseekV2ForCausalLM
----------------------------------------
  L2113: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L2158: routed_experts_weights_of_layer(self)

  L2161: determine_num_fused_shared_experts(self, architecture: str)

  L2192: get_input_embeddings(self)
         ‚Üí nn.Embedding

  L2196: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí torch.Tensor

  L2216: start_layer(self)

  L2220: end_layer(self)

  L2223: post_load_weights(self, is_nextn, weight_names)

  L2454: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn)

  L2695: get_embed_and_head(self)

  L2698: set_embed_and_head(self, embed, head)

  L2707: get_model_config_for_expert_location(cls, config)


CLASS: DeepseekV2MLP
----------------------------------------
  L 179: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str, tp_rank: Optional[int], tp_size: Optional[int])
         ‚Üí None

  L 219: forward(self, x, forward_batch, should_allreduce_fusion: bool, use_reduce_scatter: bool)


CLASS: DeepseekV2MoE
----------------------------------------
  L 285: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream], is_nextn: bool)

  L 427: get_moe_weights(self)

  L 434: forward(self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch], should_allreduce_fusion: bool, use_reduce_scatter: bool)
         ‚Üí torch.Tensor

  L 463: forward_normal_dual_stream(self, hidden_states: torch.Tensor, should_allreduce_fusion: bool, use_reduce_scatter: bool)
         ‚Üí torch.Tensor

  L 498: forward_normal(self, hidden_states: torch.Tensor, should_allreduce_fusion: bool, use_reduce_scatter: bool)
         ‚Üí torch.Tensor

  L 537: forward_cpu(self, hidden_states: torch.Tensor, should_allreduce_fusion: bool)
         ‚Üí torch.Tensor

  L 595: forward_deepep(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 638: op_gate(self, state)

  L 647: op_shared_experts(self, state)

  L 656: op_select_experts(self, state)

  L 680: op_dispatch_a(self, state)

  L 690: op_dispatch_b(self, state)

  L 699: op_experts(self, state)

  L 704: op_combine_a(self, state)

  L 715: op_combine_b(self, state)

  L 723: op_output(self, state)


CLASS: DeepseekV2Model
----------------------------------------
  L1974: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L2031: get_input_embeddings(self)
         ‚Üí torch.Tensor

  L2034: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí Union[torch.Tensor, PPProxyTensors]


CLASS: MoEGate
----------------------------------------
  L 238: __init__(self, config, prefix: str, is_nextn: bool)

  L 258: forward(self, hidden_states)


============================================================
FILE: python/sglang/srt/models/deepseek_vl2.py
Functions: 7
============================================================


CLASS: DeepseekVL2ForCausalLM
----------------------------------------
  L 160: __init__(self, config: DeepseekVL2Config, quant_config: Optional[QuantizationConfig])

  L 219: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)

  L 236: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 256: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 260: get_image_feature(self, items: List[MultimodalDataItem])


CLASS: DeepseekVL2MlpProjector
----------------------------------------
  L  26: __init__(self, config: DeepseekVL2MlpProjectorConfig, quant_config: Optional[QuantizationConfig])

  L 111: forward(self, x)


============================================================
FILE: python/sglang/srt/models/ernie4.py
Functions: 14
============================================================


CLASS: Ernie4DecoderLayer
----------------------------------------
  L 148: __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, is_mtp: bool)

  L 212: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: Ernie4Model
----------------------------------------
  L 239: __init__(self, config: Ernie4_5_MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 264: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]]]


CLASS: Ernie4Moe
----------------------------------------
  L  68: __init__(self, config: Ernie4_5_MoeConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 119: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor

  L 122: forward_normal(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Ernie4_5_ForCausalLM
----------------------------------------
  L 302: __init__(self, config: Ernie4_5_MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 324: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 335: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 358: get_embed_and_head(self)


CLASS: Ernie4_5_MoeForCausalLM
----------------------------------------
  L 363: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: MoEGate
----------------------------------------
  L  49: __init__(self, config, prefix: str)

  L  62: forward(self, hidden_states)


============================================================
FILE: python/sglang/srt/models/ernie4_eagle.py
Functions: 7
============================================================


CLASS: Ernie4ModelMTP
----------------------------------------
  L  39: __init__(self, config: Ernie4_5_MoeConfig, layer_id: int, prefix: str, quant_config: Optional[QuantizationConfig])
         ‚Üí None

  L  67: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Ernie4_5_MoeForCausalLMMTP
----------------------------------------
  L 102: __init__(self, config: Ernie4_5_MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str, mtp_layer_id: int)
         ‚Üí None

  L 132: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 143: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 188: get_embed_and_head(self)

  L 191: set_embed_and_head(self, embed, head)


============================================================
FILE: python/sglang/srt/models/exaone.py
Functions: 11
============================================================


CLASS: ExaoneAttention
----------------------------------------
  L  84: __init__(self, config, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 161: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: ExaoneDecoderLayer
----------------------------------------
  L 176: __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 219: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: ExaoneForCausalLM
----------------------------------------
  L 298: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 321: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí LogitsProcessorOutput

  L 335: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: ExaoneGatedMLP
----------------------------------------
  L  46: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  76: forward(self, x)


CLASS: ExaoneModel
----------------------------------------
  L 245: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 273: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/gemma.py
Functions: 12
============================================================


CLASS: GemmaAttention
----------------------------------------
  L  76: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: int, layer_id: int, max_position_embeddings: int, rope_theta: float, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 144: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: GemmaDecoderLayer
----------------------------------------
  L 159: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 190: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: GemmaForCausalLM
----------------------------------------
  L 294: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 309: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor

  L 322: forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)

  L 369: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: GemmaMLP
----------------------------------------
  L  44: __init__(self, hidden_size: int, intermediate_size: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  68: forward(self, x)


CLASS: GemmaModel
----------------------------------------
  L 216: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 242: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/gemma2.py
Functions: 14
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  47: def get_attention_sliding_window_size(config)


CLASS: Gemma2Attention
----------------------------------------
  L  92: __init__(self, layer_id: int, config: PretrainedConfig, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: int, max_position_embeddings: int, rope_theta: float, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 170: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: Gemma2DecoderLayer
----------------------------------------
  L 185: __init__(self, layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 227: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: Gemma2ForCausalLM
----------------------------------------
  L 357: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 372: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor

  L 385: forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)

  L 435: get_attention_sliding_window_size(self)

  L 438: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Gemma2MLP
----------------------------------------
  L  52: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, hidden_activation: str, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  84: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Gemma2Model
----------------------------------------
  L 255: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 286: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/gemma2_reward.py
Functions: 3
============================================================


CLASS: Gemma2ForSequenceClassification
----------------------------------------
  L  29: __init__(self, config: Gemma2Config, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  48: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         ‚Üí EmbeddingPoolerOutput

  L  66: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/gemma3_causal.py
Functions: 22
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  52: def get_attention_sliding_window_size(config)

  L  58: def extract_layer_index(prefix: str)
         ‚Üí int
         üìù Extract the layer index from a prefix string.


CLASS: Gemma3Attention
----------------------------------------
  L 111: __init__(self, layer_id: int, config: Gemma3TextConfig, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 203: naive_attn_with_masks(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, out: torch.Tensor)
         ‚Üí torch.Tensor

  L 249: forward(self, hidden_states: torch.Tensor, position_embeddings: Tuple[torch.Tensor, torch.Tensor], forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: Gemma3DecoderLayer
----------------------------------------
  L 291: __init__(self, layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 330: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, position_embeddings_global: torch.Tensor, position_embeddings_local: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]


CLASS: Gemma3ForCausalLM
----------------------------------------
  L 599: __init__(self, config: Gemma3TextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 624: get_input_embeddings(self)
         ‚Üí nn.Embedding

  L 627: get_attention_sliding_window_size(self)

  L 630: dtype(self)
         ‚Üí torch.dtype

  L 634: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí LogitsProcessor

  L 651: forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)

  L 713: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Gemma3MLP
----------------------------------------
  L  72: __init__(self, hidden_size: int, intermediate_size: int, hidden_activation: str, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 103: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Gemma3RotaryEmbedding
----------------------------------------
  L 372: __init__(self, config: Gemma3TextConfig, device)

  L 418: forward(self, x, position_ids)


CLASS: Gemma3TextModel
----------------------------------------
  L 469: __init__(self, config: Gemma3TextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 513: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Gemma3TextScaledWordEmbedding
----------------------------------------
  L 454: __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: Optional[float])

  L 464: forward(self, input_ids: torch.Tensor)


============================================================
FILE: python/sglang/srt/models/gemma3_mm.py
Functions: 11
============================================================


CLASS: Gemma3ForConditionalGeneration
----------------------------------------
  L 158: __init__(self, config: Gemma3Config, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 188: pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)
         ‚Üí List[int]
         üìù Pad input IDs with image tokens.

  L 201: prepare_attn_masks(self, input_ids: torch.Tensor, positions: torch.Tensor, mask_dtype: torch.dtype)
         ‚Üí Dict
         üìù Prepare attention masks for multimodal inputs.

  L 269: get_input_embeddings(self)
         ‚Üí nn.Embedding

  L 272: get_attention_sliding_window_size(self)
         üìù This value is used to initialize attention backends in `ForwardBatch`.

  L 278: get_image_feature(self, items: List[MultimodalDataItem])
         üìù Projects the last hidden state from the vision model into language mod

  L 316: forward(self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí LogitsProcessor
         üìù     labels (`torch.LongTensor` of shape `(batch_size, sequence_length)

  L 383: tie_weights(self)

  L 386: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Gemma3MultiModalProjector
----------------------------------------
  L  61: __init__(self, config: Gemma3Config)

  L  83: forward(self, vision_outputs: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/gemma3n_audio.py
Functions: 20
============================================================


CLASS: Gemma3nAudioAttention
----------------------------------------
  L 280: __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 388: forward(self, x: torch.Tensor, mask: torch.BoolTensor)
         ‚Üí torch.Tensor


CLASS: Gemma3nAudioConformerAttention
----------------------------------------
  L 614: __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 646: forward(self, audio_encodings: torch.Tensor, audio_mel_mask: torch.BoolTensor)
         ‚Üí torch.Tensor


CLASS: Gemma3nAudioConformerBlock
----------------------------------------
  L 793: __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 821: forward(self, audio_encodings: torch.Tensor, audio_mel_mask: torch.BoolTensor)
         ‚Üí torch.Tensor


CLASS: Gemma3nAudioConformerFeedForward
----------------------------------------
  L 669: __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 702: forward(self, audio_encodings: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Gemma3nAudioConformerLightConv1d
----------------------------------------
  L 719: __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 766: forward(self, audio_encodings: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Gemma3nAudioEncoder
----------------------------------------
  L 846: __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 868: forward(self, audio_mel: torch.Tensor, audio_mel_mask: torch.BoolTensor)
         ‚Üí Tuple[torch.Tensor, torch.BoolTensor]
         üìù Encodes a batch of MELs.


CLASS: Gemma3nAudioRelativePositionEmbedding
----------------------------------------
  L 139: __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 227: forward(self, queries: torch.Tensor, keys: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Gemma3nAudioSSCPConvBlock
----------------------------------------
  L 490: __init__(self, config: Gemma3nAudioConfig, idx: int, input_freq_dim: int, manual_padding: Tuple[int, int, int, int], quant_config: Optional[QuantizationConfig], prefix: str)

  L 528: forward(self, audio_encodings: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Gemma3nAudioSubSampleConvProjection
----------------------------------------
  L 540: __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 602: forward(self, audio_encodings: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Gemma3nCumulativeGroupNorm
----------------------------------------
  L  36: __init__(self, num_channels: int, feature_dims: Sequence[int], eps: float)

  L  56: forward(self, x: torch.Tensor, mask: Optional[torch.Tensor])
         ‚Üí torch.Tensor
         üìù Applies cumulative group norm, optionally using a mask.


============================================================
FILE: python/sglang/srt/models/gemma3n_causal.py
Functions: 29
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  33: def get_attention_sliding_window_size(config)


CLASS: Gemma3nAltUp
----------------------------------------
  L 174: __init__(self, config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 219: compute_router_modalities(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L 230: predict(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor
         üìù Predicts the output of a layer using a trainable map.

  L 262: correct(self, predictions: torch.Tensor, activated: torch.Tensor)
         ‚Üí torch.Tensor
         üìù Corrects the predictions relative to the activated inputs.

  L 293: scale_corrected_output(self, corrected: torch.Tensor)
         ‚Üí torch.Tensor
         üìù Scales the provided 3D tensor.

  L 297: forward(self, hidden_states: torch.Tensor, activated: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù Predicts, correct, and optionally scales the output of a layer using t


CLASS: Gemma3nAttention
----------------------------------------
  L 316: __init__(self, layer_id: int, config: Gemma3nTextConfig, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 434: forward(self, hidden_states: torch.Tensor, positions: Tuple[torch.Tensor, torch.Tensor], forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: Gemma3nDecoderLayer
----------------------------------------
  L 496: __init__(self, layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 567: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, per_layer_input: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: Gemma3nForCausalLM
----------------------------------------
  L 900: __init__(self, config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 927: get_input_embeddings(self)
         ‚Üí nn.Embedding

  L 930: get_attention_sliding_window_size(self)

  L 933: dtype(self)
         ‚Üí torch.dtype

  L 937: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, per_layer_inputs: Optional[torch.Tensor])
         ‚Üí LogitsProcessor

  L 959: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Gemma3nLaurelBlock
----------------------------------------
  L 135: __init__(self, config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 163: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Gemma3nRMSNorm
----------------------------------------
  L  38: __init__(self, dim: int, eps: float, with_scale: bool)
         ‚Üí None

  L  53: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Gemma3nTextMLP
----------------------------------------
  L  66: __init__(self, hidden_size: int, intermediate_size: int, hidden_activation: str, activation_sparsity: float, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 105: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Gemma3nTextModel
----------------------------------------
  L 629: __init__(self, config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 724: get_input_embeddings(self)
         ‚Üí nn.Embedding

  L 727: dtype(self)
         ‚Üí torch.dtype

  L 730: get_per_layer_inputs(self, input_ids: torch.LongTensor)
         ‚Üí torch.Tensor

  L 738: project_per_layer_inputs(self, inputs_embeds: torch.Tensor, per_layer_inputs: Optional[torch.Tensor])
         ‚Üí torch.Tensor

  L 765: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, per_layer_inputs: Optional[torch.Tensor])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/gemma3n_mm.py
Functions: 15
============================================================


CLASS: Gemma3nForConditionalGeneration
----------------------------------------
  L 193: __init__(self, config: Gemma3nConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 245: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)
         ‚Üí List[int]
         üìù Pad input IDs with image and audio tokens.

  L 254: get_input_embeddings(self)
         ‚Üí nn.Embedding

  L 257: get_attention_sliding_window_size(self)

  L 260: get_image_feature(self, items: List[MultimodalDataItem])
         üìù Projects the last hidden state from the vision model into language mod

  L 308: get_audio_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor
         üìù Projects the last hidden state from the audio encoder into language mo

  L 388: get_per_layer_inputs(self, input_ids: torch.LongTensor)
         ‚Üí Optional[torch.Tensor]

  L 393: project_per_layer_inputs(self, inputs_embeds: torch.Tensor, per_layer_inputs: Optional[torch.Tensor])
         ‚Üí torch.Tensor

  L 403: forward(self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí LogitsProcessor
         üìù Forward pass for multimodal Gemma3n.

  L 449: tie_weights(self)

  L 452: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 499: should_apply_lora(self, module_name: str)
         ‚Üí bool

  L 502: get_hidden_dim(self, module_name)


CLASS: Gemma3nMultimodalEmbedder
----------------------------------------
  L  62: __init__(self, multimodal_config: Union[Gemma3nAudioConfig, Gemma3nVisionConfig], text_config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 108: forward(self, input_ids: Optional[torch.LongTensor], inputs_embeds: Optional[torch.Tensor])
         ‚Üí torch.Tensor
         üìù Embeds token ids or soft tokens for multimodal content into language m


============================================================
FILE: python/sglang/srt/models/glm4.py
Functions: 11
============================================================


CLASS: Glm4Attention
----------------------------------------
  L  44: __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 111: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: Glm4DecoderLayer
----------------------------------------
  L 137: __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 168: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: Glm4ForCausalLM
----------------------------------------
  L 253: __init__(self, config: Glm4Config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 275: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 286: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Glm4Model
----------------------------------------
  L 197: __init__(self, config: Glm4Config, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 221: get_input_embeddings(self)
         ‚Üí nn.Embedding

  L 224: dtype(self)
         ‚Üí torch.dtype

  L 228: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]]]


============================================================
FILE: python/sglang/srt/models/glm4_moe.py
Functions: 20
============================================================


CLASS: Glm4MoeAttention
----------------------------------------
  L 167: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, partial_rotary_factor: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, head_dim: Optional[int], rms_norm_eps: float, attention_bias: bool, quant_config: Optional[QuantizationConfig], use_qk_norm: bool, prefix: str, alt_stream: Optional[torch.cuda.Stream])
         ‚Üí None

  L 280: op_prepare(self, state)

  L 287: op_core(self, state)

  L 292: forward_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)

  L 308: forward_core(self, intermediate_state)

  L 316: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: Glm4MoeDecoderLayer
----------------------------------------
  L 578: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], is_nextn: bool, prefix: str, alt_stream: Optional[torch.cuda.Stream])
         ‚Üí None

  L 662: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], zero_allocator: BumpAllocator)
         ‚Üí torch.Tensor


CLASS: Glm4MoeForCausalLM
----------------------------------------
  L 731: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 764: determine_num_fused_shared_experts(self, architecture: str)

  L 794: get_input_embeddings(self)
         ‚Üí nn.Embedding

  L 797: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn)


CLASS: Glm4MoeGate
----------------------------------------
  L 331: __init__(self, config, prefix: str, is_nextn: bool)

  L 348: forward(self, hidden_states)


CLASS: Glm4MoeMLP
----------------------------------------
  L 116: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str, tp_rank: Optional[int], tp_size: Optional[int])
         ‚Üí None

  L 156: forward(self, x, forward_batch, should_allreduce_fusion)


CLASS: Glm4MoeModel
----------------------------------------
  L 694: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None


CLASS: Glm4MoeSparseMoeBlock
----------------------------------------
  L 376: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream], is_nextn: bool)

  L 499: forward_normal_dual_stream(self, hidden_states: torch.Tensor, should_allreduce_fusion: bool, use_reduce_scatter: bool)
         ‚Üí torch.Tensor

  L 541: forward_normal(self, hidden_states: torch.Tensor, should_allreduce_fusion: bool, use_reduce_scatter: bool)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/glm4_moe_nextn.py
Functions: 5
============================================================


CLASS: Glm4MoeForCausalLMNextN
----------------------------------------
  L 128: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 153: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 164: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Glm4MoeModelNextN
----------------------------------------
  L  42: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  80: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/glm4v.py
Functions: 22
============================================================


CLASS: Glm4vForConditionalGeneration
----------------------------------------
  L 465: __init__(self, config: Glm4vConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 501: get_image_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor

  L 516: get_video_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor

  L 587: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Glm4vPatchMerger
----------------------------------------
  L 145: __init__(self, d_model: int, context_dim: int, quant_config: Optional[QuantizationConfig], bias: bool, prefix: str)
         ‚Üí None

  L 180: forward(self, x: torch.Tensor)


CLASS: Glm4vRMSNorm
----------------------------------------
  L  38: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Glm4vVisionBlock
----------------------------------------
  L  80: __init__(self, config: Glm4vVisionConfig, norm_layer: Optional[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None


CLASS: Glm4vVisionEmbeddings
----------------------------------------
  L 191: __init__(self, config: Glm4vVisionConfig)

  L 207: forward(self, embeddings, lengths, image_shapes, h_coords, w_coords)
         ‚Üí torch.Tensor


CLASS: Glm4vVisionMLP
----------------------------------------
  L  47: __init__(self, in_features: int, hidden_features: int, bias: bool, quant_config: Optional[QuantizationConfig], prefix: str)

  L  72: forward(self, x: torch.Tensor)


CLASS: Glm4vVisionModel
----------------------------------------
  L 320: __init__(self, vision_config: Glm4vVisionConfig, norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 387: dtype(self)
         ‚Üí torch.dtype

  L 391: device(self)
         ‚Üí torch.device

  L 394: rot_pos_emb(self, grid_thw: torch.Tensor)
         ‚Üí torch.Tensor

  L 426: forward(self, x: torch.Tensor, grid_thw: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Glm4vVisionPatchEmbed
----------------------------------------
  L 110: __init__(self, patch_size: int, temporal_patch_size: int, in_channels: int, hidden_size: int)
         ‚Üí None

  L 132: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Glm4vVisionRotaryEmbedding
----------------------------------------
  L 282: __init__(self, dim: int, theta: float)
         ‚Üí None

  L 291: update_freqs_cache(self, seqlen: int)
         ‚Üí None

  L 314: forward(self, seqlen: int)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/glm4v_moe.py
Functions: 3
============================================================


CLASS: Glm4vMoeForConditionalGeneration
----------------------------------------
  L  34: __init__(self, config: Glm4vMoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  77: determine_num_fused_shared_experts(self, architecture: str)

  L 107: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn)


============================================================
FILE: python/sglang/srt/models/gpt2.py
Functions: 11
============================================================


CLASS: GPT2Attention
----------------------------------------
  L  44: __init__(self, layer_id: int, config: GPT2Config, quant_config: Optional[QuantizationConfig], prefix: str)

  L  84: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: GPT2Block
----------------------------------------
  L 136: __init__(self, layer_id: int, config: GPT2Config, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)

  L 161: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: GPT2LMHeadModel
----------------------------------------
  L 233: __init__(self, config: GPT2Config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 249: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 260: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: GPT2MLP
----------------------------------------
  L  98: __init__(self, intermediate_size: int, config: GPT2Config, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)

  L 124: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: GPT2Model
----------------------------------------
  L 185: __init__(self, config: GPT2Config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 213: forward(self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/gpt_bigcode.py
Functions: 11
============================================================


CLASS: GPTBigCodeAttention
----------------------------------------
  L  43: __init__(self, layer_id: int, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L  94: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: GPTBigCodeBlock
----------------------------------------
  L 151: __init__(self, layer_id: int, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 171: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: GPTBigCodeForCausalLM
----------------------------------------
  L 254: __init__(self, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 273: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 284: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: GPTBigCodeModel
----------------------------------------
  L 194: __init__(self, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 224: forward(self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: GPTBigMLP
----------------------------------------
  L 115: __init__(self, intermediate_size: int, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 142: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/gpt_oss.py
Functions: 27
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  99: def get_attention_sliding_window_size(config)


CLASS: GptOssAttention
----------------------------------------
  L 223: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, head_dim: Optional[int], rms_norm_eps: float, attention_bias: bool, quant_config: Optional[QuantizationConfig], prefix: str, sliding_window_size: int, layer_type: str, params_dtype: torch.dtype)
         ‚Üí None

  L 323: forward_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)

  L 351: forward_core(self, intermediate_state)

  L 363: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: GptOssConfig
----------------------------------------
  L  90: __init__(self)


CLASS: GptOssDecoderLayer
----------------------------------------
  L 378: __init__(self, config: GptOssConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, sliding_window_size: int | None)
         ‚Üí None

  L 463: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: GptOssForCausalLM
----------------------------------------
  L 598: __init__(self, config: GptOssConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 630: routed_experts_weights_of_layer(self)

  L 634: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí torch.Tensor

  L 666: start_layer(self)

  L 670: end_layer(self)

  L 741: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn: bool, weight_name_mapping: dict)

  L1125: get_embed_and_head(self)

  L1128: set_embed_and_head(self, embed, head)

  L1136: set_eagle3_layers_to_capture(self, layer_ids: Optional[List[int]])

  L1151: get_model_config_for_expert_location(cls, config)

  L1158: get_attention_sliding_window_size(self)


CLASS: GptOssModel
----------------------------------------
  L 505: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str, decoder_layer_type: type[nn.Module])
         ‚Üí None

  L 548: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí Union[torch.Tensor, PPProxyTensors]


CLASS: GptOssSparseMoeBlock
----------------------------------------
  L 104: __init__(self, layer_id: int, config: GptOssConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 160: forward(self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch], should_allreduce_fusion: bool)
         ‚Üí torch.Tensor

  L 171: get_moe_weights(self)

  L 178: forward_normal(self, hidden_states: torch.Tensor, should_allreduce_fusion: bool)
         ‚Üí torch.Tensor


CLASS: _WeightCreator
----------------------------------------
  L1204: __init__(self, fn)

  L1208: maybe_materialize(obj)


============================================================
FILE: python/sglang/srt/models/granite.py
Functions: 14
============================================================


CLASS: GraniteAttention
----------------------------------------
  L  90: __init__(self, config: GraniteConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 165: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: GraniteDecoderLayer
----------------------------------------
  L 180: __init__(self, config: GraniteConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 225: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: GraniteForCausalLM
----------------------------------------
  L 306: __init__(self, config: GraniteConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 349: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         ‚Üí LogitsProcessorOutput

  L 366: get_module_name_from_weight_name(self, name)

  L 375: get_num_params(self)

  L 379: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 431: get_weights_by_name(self, name: str, truncate_size: int, tp_size: int)
         ‚Üí Optional[torch.Tensor]
         üìù Get the weights of the parameter by its name. Similar to `get_paramete


CLASS: GraniteMLP
----------------------------------------
  L  52: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  82: forward(self, x)


CLASS: GraniteModel
----------------------------------------
  L 254: __init__(self, config: GraniteConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 280: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/granitemoe.py
Functions: 12
============================================================


CLASS: GraniteMoeAttention
----------------------------------------
  L  94: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, max_position: int, layer_id: int, rope_theta: float, quant_config: Optional[QuantizationConfig], attention_multiplier: Optional[float], prefix: str)
         ‚Üí None

  L 165: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: GraniteMoeDecoderLayer
----------------------------------------
  L 181: __init__(self, config: GraniteConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 219: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: GraniteMoeForCausalLM
----------------------------------------
  L 300: __init__(self, config: GraniteConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 331: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         ‚Üí LogitsProcessorOutput

  L 348: load_weights(self, weights: Iterable[tuple[str, torch.Tensor]])
         ‚Üí set[str]


CLASS: GraniteMoeMoE
----------------------------------------
  L  40: __init__(self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], tp_size: Optional[int], prefix: str)

  L  82: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: GraniteMoeModel
----------------------------------------
  L 244: __init__(self, config: GraniteConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 271: get_input_embeddings(self, input_ids: torch.Tensor)
         ‚Üí torch.Tensor

  L 274: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/grok.py
Functions: 18
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 218: def get_rope_scaling(config)


CLASS: Grok1Attention
----------------------------------------
  L 337: __init__(self, config: PretrainedConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, max_position: int, rope_theta: float, quant_config: Optional[QuantizationConfig], reduce_results: bool, alt_stream: Optional[torch.cuda.Stream], load_presharded_attn: bool, prefix: str)
         ‚Üí None

  L 457: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: Grok1DecoderLayer
----------------------------------------
  L 538: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], load_presharded_moe: bool, load_presharded_attn: bool, load_presharded_mlp: bool, alt_stream: Optional[torch.cuda.Stream], skip_moe: bool, prefix: str)
         ‚Üí None

  L 630: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], deferred_norm: Optional[RMSNorm])
         ‚Üí Tuple[torch.Tensor, torch.Tensor, RMSNorm]

  L 696: moe_with_rmoe(self, x)


CLASS: Grok1ForCausalLM
----------------------------------------
  L 808: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 893: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor

  L 908: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], ignore_parent_name: bool, check_hit_names: bool, model_config: PretrainedConfig | None)
         ‚Üí dict[str, torch.Tensor]

  L1025: get_num_params_analytical(self)

  L1073: get_num_params_torch(self)


CLASS: Grok1MLP
----------------------------------------
  L  88: __init__(self, hidden_size: int, intermediate_size: int, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, reduce_results, use_presharded_weights: bool, split_gate_up: bool)
         ‚Üí None

  L 121: forward(self, x)


CLASS: Grok1MoE
----------------------------------------
  L 137: __init__(self, config: PretrainedConfig, layer_id: int, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], tp_size: Optional[int], reduce_results: bool, use_presharded_weights: bool, inplace: bool, no_combine: bool, prefix: str)

  L 201: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Grok1Model
----------------------------------------
  L 708: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], load_presharded_moe: bool, load_presharded_embedding: bool, load_presharded_attn: bool, load_presharded_mlp: bool, replicate_embedding: bool, prefix: str)
         ‚Üí None

  L 749: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: ScalingRotaryEmbedding
----------------------------------------
  L 247: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype)
         ‚Üí None


============================================================
FILE: python/sglang/srt/models/hunyuan.py
Functions: 17
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 213: def get_head_dim(config)

  L 224: def check_head_dim(config)


CLASS: HunYuanAttention
----------------------------------------
  L 251: __init__(self, config: PretrainedConfig, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], bias: bool, prefix: str, attention_type: str, layer_id: int)
         ‚Üí None

  L 351: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, kv_states: Optional[Tuple[torch.Tensor]])
         ‚Üí torch.Tensor


CLASS: HunYuanDecoderLayer
----------------------------------------
  L 392: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str, layer_id: int)
         ‚Üí None

  L 462: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], kv_states: Optional[Tuple[torch.Tensor]])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: HunYuanMLP
----------------------------------------
  L  82: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], bias: bool, prefix: str, reduce_results: bool)
         ‚Üí None

  L 115: forward(self, x)


CLASS: HunYuanMoEV1ForCausalLM
----------------------------------------
  L 585: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig])
         ‚Üí None

  L 613: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor

  L 642: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 788: load_kv_cache_scales(self, quantization_param_path: str)
         ‚Üí None


CLASS: HunYuanModel
----------------------------------------
  L 491: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 522: get_input_embeddings(self, input_ids: torch.Tensor)
         ‚Üí torch.Tensor

  L 525: forward(self, input_ids: Optional[torch.Tensor], positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor])
         ‚Üí torch.Tensor


CLASS: HunYuanSparseMoeBlock
----------------------------------------
  L 124: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], layer_id: int)

  L 192: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/idefics2.py
Functions: 13
============================================================


CLASS: Idefics2Encoder
----------------------------------------
  L 131: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 151: forward(self, inputs_embeds: torch.Tensor, cu_seqlens: torch.Tensor)
         ‚Üí torch.Tensor
         üìù Args:


CLASS: Idefics2EncoderLayer
----------------------------------------
  L  69: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  98: forward(self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor)
         ‚Üí torch.Tensor
         üìù Args:


CLASS: Idefics2VisionEmbeddings
----------------------------------------
  L 189: __init__(self, config: PretrainedConfig)

  L 206: get_position_ids(self, pixel_values: torch.FloatTensor, patch_attention_mask: torch.BoolTensor, tgt_sizes: Optional[torch.IntTensor])

  L 248: forward(self, pixel_values: torch.FloatTensor, patch_attention_mask: torch.BoolTensor, tgt_sizes: Optional[torch.IntTensor])
         ‚Üí torch.Tensor


CLASS: Idefics2VisionMLP
----------------------------------------
  L  36: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  60: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Idefics2VisionTransformer
----------------------------------------
  L 270: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], require_post_norm: bool, prefix: str)
         ‚Üí None

  L 293: get_input_embeddings(self)
         ‚Üí nn.Embedding

  L 296: compute_cu_seqlens(self, tgt_sizes: Optional[torch.Tensor], input_embeds: Optional[torch.Tensor])
         ‚Üí torch.Tensor

  L 325: forward(self, pixel_values, patch_attention_mask: Optional[torch.BoolTensor], tgt_sizes: Optional[torch.IntTensor])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/internlm2.py
Functions: 12
============================================================


CLASS: InternLM2Attention
----------------------------------------
  L  83: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 152: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: InternLM2ForCausalLM
----------------------------------------
  L 276: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 293: get_input_embeddings(self)
         ‚Üí nn.Embedding

  L 297: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor

  L 309: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: InternLM2MLP
----------------------------------------
  L  45: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  75: forward(self, x)


CLASS: InternLM2Model
----------------------------------------
  L 226: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 251: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: InternLMDecoderLayer
----------------------------------------
  L 167: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 200: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


============================================================
FILE: python/sglang/srt/models/internlm2_reward.py
Functions: 3
============================================================


CLASS: InternLM2ForRewardModel
----------------------------------------
  L  29: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  46: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         ‚Üí EmbeddingPoolerOutput

  L  60: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/interns1.py
Functions: 7
============================================================


CLASS: InternS1ForConditionalGeneration
----------------------------------------
  L  30: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], use_flash_attn)
         ‚Üí None

  L  95: pixel_shuffle(self, x, scale_factor)

  L 117: extract_feature(self, pixel_values)

  L 135: get_image_feature(self, items: List[MultimodalDataItem])
         üìù Projects the last hidden state from the vision model into language mod

  L 147: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor

  L 167: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 209: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/internvl.py
Functions: 23
============================================================


CLASS: InternAttention
----------------------------------------
  L  36: __init__(self, config, quant_config: QuantizationConfig)

  L  66: forward(self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: InternMLP
----------------------------------------
  L 165: __init__(self, config: PretrainedConfig)

  L 172: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: InternRMSNorm
----------------------------------------
  L 151: __init__(self, hidden_size, eps)

  L 156: forward(self, hidden_states)


CLASS: InternVLChatModel
----------------------------------------
  L 406: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], use_flash_attn)
         ‚Üí None

  L 465: pixel_shuffle(self, x, scale_factor)

  L 487: extract_feature(self, pixel_values)

  L 505: get_image_feature(self, items: List[MultimodalDataItem])
         üìù Projects the last hidden state from the vision model into language mod

  L 517: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor

  L 537: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 547: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: InternVisionEmbeddings
----------------------------------------
  L  77: __init__(self, config: PretrainedConfig)

  L 130: forward(self, pixel_values: torch.FloatTensor)
         ‚Üí torch.Tensor


CLASS: InternVisionEncoder
----------------------------------------
  L 249: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig])

  L 268: forward(self, inputs_embeds, output_hidden_states: Optional[bool], return_dict: Optional[bool])
         ‚Üí Union[Tuple, BaseModelOutput]
         üìù Args:


CLASS: InternVisionEncoderLayer
----------------------------------------
  L 187: __init__(self, config: PretrainedConfig, drop_path_rate: float, quant_config: QuantizationConfig)

  L 211: forward(self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor)
         ‚Üí Tuple[torch.FloatTensor, Optional[torch.FloatTensor], Optional[Tuple[torch.FloatTensor]]]
         üìù Args:


CLASS: InternVisionModel
----------------------------------------
  L 320: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig])

  L 333: resize_pos_embeddings(self, old_size, new_size, patch_size)

  L 356: get_input_embeddings(self)

  L 359: forward(self, pixel_values: Optional[torch.FloatTensor], output_hidden_states: Optional[bool], return_dict: Optional[bool], pixel_embeds: Optional[torch.FloatTensor])
         ‚Üí Union[Tuple, BaseModelOutputWithPooling]


============================================================
FILE: python/sglang/srt/models/kimi_vl.py
Functions: 8
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 300: def get_spec_layer_idx_from_weight_name(config: DeepseekV2Config,
        weight_name: str)
         ‚Üí Optional[int]


CLASS: KimiVLForConditionalGeneration
----------------------------------------
  L 122: __init__(self, config: KimiVLConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 145: get_image_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor

  L 160: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 164: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)

  L 183: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: KimiVLMultiModalProjector
----------------------------------------
  L  96: __init__(self, config: KimiVLConfig)

  L 113: forward(self, image_features: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/kimi_vl_moonvit.py
Functions: 25
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  63: def multihead_attention(q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        q_cu_seqlens: Optional[torch.Tensor],
        k_cu_seqlens: Optional[torch.Tensor])
         üìù Multi-head attention using flash attention 2.

  L 115: def sdpa_attention(q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        q_cu_seqlens: Optional[torch.Tensor],
        k_cu_seqlens: Optional[torch.Tensor])
         ‚Üí torch.Tensor
         üìù Multi-head attention using torch scaled dot product attention.

  L 170: def apply_rope(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor)
         ‚Üí tuple[torch.Tensor, torch.Tensor]
         üìù Args: (The leading dimensions of all inputs should be the same)

  L 536: def patch_merger(x: torch.Tensor,
        grid_hw: torch.Tensor,
        merge_kernel_size: list[int,
        int])
         ‚Üí List[torch.Tensor]


CLASS: Learnable2DInterpPosEmb
----------------------------------------
  L 195: __init__(self, height: int, width: int, dim: int, interpolation_mode: str)
         ‚Üí None

  L 205: reset_parameters(self)

  L 208: forward(self, x: torch.Tensor, grid_hws: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: MLP2
----------------------------------------
  L 396: __init__(self, dims: list[int], activation, bias)

  L 407: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: MoonVisionPatchEmbed
----------------------------------------
  L 230: __init__(self, out_dim: int, in_dim: int, patch_size: Union[int, Tuple[int, int]], pos_emb_height: int, pos_emb_width: int)

  L 257: forward(self, x: torch.Tensor, grid_hw: torch.Tensor)
         ‚Üí torch.Tensor
         üìù Args:


CLASS: MoonVitEncoder
----------------------------------------
  L 497: __init__(self, hidden_dim: int, num_layers: int, block_cfg: dict)
         ‚Üí None

  L 513: forward(self, hidden_states: torch.Tensor, grid_hw: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: MoonVitEncoderLayer
----------------------------------------
  L 415: __init__(self, num_heads: int, hidden_dim: int, mlp_dim: int)

  L 437: attention_qkvpacked(self, x: torch.Tensor, cu_seqlens: torch.Tensor, rope_freqs_cis: Optional[torch.Tensor])
         üìù Args:

  L 469: forward(self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor, rope_freqs_cis: Union[torch.Tensor, None])
         ‚Üí torch.Tensor
         üìù Args:


CLASS: MoonVitPretrainedModel
----------------------------------------
  L 598: __init__(self, config: MoonViTConfig)

  L 623: forward(self, pixel_values: torch.Tensor, grid_hw: torch.Tensor)
         ‚Üí torch.Tensor
         üìù Args:


CLASS: MoonVitVLProjector
----------------------------------------
  L 567: __init__(self, in_channels: int, merge_kernel_size: list[int, int], hidden_act: str, ln_eps: float, out_dim: int)

  L 583: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Rope2DPosEmb
----------------------------------------
  L 294: __init__(self, dim: int, max_height: int, max_width: int, theta_base, device)

  L 305: extra_repr(self)

  L 309: precomputed_freqs_cis(self)
         ‚Üí torch.Tensor
         üìù Calculate the cis(freqs) for each position in the 2D grid.

  L 337: get_freqs_cis_by_seqlens(self, grid_hws: torch.Tensor)
         ‚Üí torch.Tensor
         üìù Args:

  L 361: get_freqs_cis_by_idx(self, pos_idx: torch.Tensor, pos_idx_mask: torch.Tensor)
         ‚Üí torch.Tensor
         üìù Args:


============================================================
FILE: python/sglang/srt/models/llama.py
Functions: 25
============================================================


CLASS: LlamaAttention
----------------------------------------
  L 110: __init__(self, config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str, bias: bool)
         ‚Üí None

  L 188: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: LlamaDecoderLayer
----------------------------------------
  L 203: __init__(self, config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 253: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: LlamaForCausalLM
----------------------------------------
  L 411: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 456: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí LogitsProcessorOutput

  L 492: forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)
         ‚Üí Optional[LogitsProcessorOutput]

  L 533: start_layer(self)

  L 537: end_layer(self)

  L 540: get_input_embeddings(self)
         ‚Üí nn.Embedding

  L 543: get_module_name_from_weight_name(self, name)

  L 552: get_num_params(self)

  L 556: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 624: get_weights_by_name(self, name: str, truncate_size: int, tp_size: int)
         ‚Üí Optional[torch.Tensor]
         üìù Get the weights of the parameter by its name. Similar to `get_paramete

  L 697: get_embed_and_head(self)

  L 700: set_embed_and_head(self, embed, head)

  L 708: get_embed(self)

  L 711: set_embed(self, embed)

  L 723: load_kv_cache_scales(self, quantization_param_path: str)
         ‚Üí None

  L 726: set_eagle3_layers_to_capture(self, layer_ids: Optional[List[int]])


CLASS: LlamaMLP
----------------------------------------
  L  62: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str, reduce_results: bool)
         ‚Üí None

  L  94: forward(self, x, forward_batch, use_reduce_scatter: bool)


CLASS: LlamaModel
----------------------------------------
  L 279: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 316: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]], PPProxyTensors]

  L 367: load_kv_cache_scales(self, quantization_param_path: str)
         ‚Üí None


============================================================
FILE: python/sglang/srt/models/llama4.py
Functions: 11
============================================================


CLASS: Llama4Attention
----------------------------------------
  L 193: __init__(self, config: Llama4TextConfig, layer_id: int, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], bias: bool, bias_o_proj: bool, prefix: str)
         ‚Üí None

  L 317: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: Llama4DecoderLayer
----------------------------------------
  L 353: __init__(self, config: Llama4TextConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 426: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: Llama4ForCausalLM
----------------------------------------
  L 532: __init__(self, config: Llama4TextConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 540: get_input_embeddings(self)


CLASS: Llama4MoE
----------------------------------------
  L  71: custom_routing_function(hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L  86: __init__(self, config: Llama4TextConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 133: forward(self, hidden_states, forward_batch: ForwardBatch, use_reduce_scatter: bool)


CLASS: Llama4Model
----------------------------------------
  L 465: __init__(self, config: Llama4TextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 493: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]]]


============================================================
FILE: python/sglang/srt/models/llama_classification.py
Functions: 3
============================================================


CLASS: LlamaForClassification
----------------------------------------
  L  30: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  49: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         ‚Üí EmbeddingPoolerOutput

  L  67: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/llama_eagle.py
Functions: 5
============================================================


CLASS: LlamaDecoderLayer
----------------------------------------
  L  40: __init__(self, config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None


CLASS: LlamaForCausalLMEagle
----------------------------------------
  L 114: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 142: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: LlamaModel
----------------------------------------
  L  57: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  84: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/llama_eagle3.py
Functions: 7
============================================================


CLASS: LlamaDecoderLayer
----------------------------------------
  L  43: __init__(self, config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  74: forward(self, positions: torch.Tensor, embeds: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: LlamaForCausalLMEagle3
----------------------------------------
  L 169: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 206: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])
         ‚Üí None

  L 249: get_hot_token_id(self)


CLASS: LlamaModel
----------------------------------------
  L 104: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 134: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/llama_embedding.py
Functions: 3
============================================================


CLASS: LlamaEmbeddingModel
----------------------------------------
  L  15: __init__(self, config: LlamaConfig, quant_config, prefix: str)
         ‚Üí None

  L  28: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         ‚Üí EmbeddingPoolerOutput

  L  42: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/llama_reward.py
Functions: 8
============================================================


CLASS: LlamaForSequenceClassification
----------------------------------------
  L  29: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  48: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         ‚Üí EmbeddingPoolerOutput

  L  66: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: LlamaForSequenceClassificationWithNormal_Weights
----------------------------------------
  L  85: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  95: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         ‚Üí EmbeddingPoolerOutput

  L 119: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Weights
----------------------------------------
  L  72: __init__(self, hidden_size, num_label)

  L  82: forward(self, x)


============================================================
FILE: python/sglang/srt/models/llava.py
Functions: 14
============================================================


CLASS: LlavaBaseForCausalLM
----------------------------------------
  L  58: pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)

  L 126: encode_images(self, pixel_values: Union[torch.Tensor, List[torch.Tensor]])
         ‚Üí torch.Tensor
         üìù encode images by vision tower and multimodal projector

  L 151: forward(self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 440: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 498: num_patches_per_side(self)


CLASS: LlavaForConditionalGeneration
----------------------------------------
  L 615: dtype(self)

  L 618: pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)

  L 670: __init__(self, config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 747: get_image_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor
         üìù Extract features from image inputs.

  L 782: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)

  L 803: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])
         üìù Load weights for LlavaForConditionalGeneration.


CLASS: LlavaLlamaForCausalLM
----------------------------------------
  L 503: __init__(self, config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None


CLASS: LlavaMistralForCausalLM
----------------------------------------
  L 566: __init__(self, config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None


CLASS: LlavaQwenForCausalLM
----------------------------------------
  L 529: __init__(self, config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None


============================================================
FILE: python/sglang/srt/models/llavavid.py
Functions: 6
============================================================


CLASS: LlavaVidForCausalLM
----------------------------------------
  L  33: __init__(self, config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  60: pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)

  L  77: encode_images(self, pixel_values: torch.Tensor)
         ‚Üí torch.Tensor

  L 109: forward(self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 221: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 279: num_patches_per_side(self)


============================================================
FILE: python/sglang/srt/models/mimo.py
Functions: 8
============================================================


CLASS: MiMoForCausalLM
----------------------------------------
  L  66: __init__(self, config: MiMoConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  90: get_input_embeddings(self, input_ids: torch.Tensor)
         ‚Üí torch.Tensor

  L  94: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         ‚Üí torch.Tensor

  L 110: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 156: get_embed_and_head(self)

  L 159: set_embed_and_head(self, embed, head)

  L 167: load_kv_cache_scales(self, quantization_param_path: str)
         ‚Üí None


CLASS: MiMoModel
----------------------------------------
  L  32: __init__(self, config: MiMoConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None


============================================================
FILE: python/sglang/srt/models/mimo_mtp.py
Functions: 8
============================================================


CLASS: MiMoMTP
----------------------------------------
  L  84: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 108: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 119: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 173: map_model_name_to_mtp_param_name(self, name: str)
         ‚Üí str

  L 192: get_embed_and_head(self)

  L 195: set_embed_and_head(self, embed, head)


CLASS: MiMoMultiTokenPredictorLayer
----------------------------------------
  L  25: __init__(self, config: PretrainedConfig, prefix: str, quant_config: Optional[QuantizationConfig])
         ‚Üí None

  L  47: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/minicpm.py
Functions: 11
============================================================


CLASS: MiniCPMAttention
----------------------------------------
  L  82: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 151: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: MiniCPMDecoderLayer
----------------------------------------
  L 169: __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 205: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: MiniCPMForCausalLM
----------------------------------------
  L 291: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 319: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor

  L 336: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: MiniCPMMLP
----------------------------------------
  L  44: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  74: forward(self, x)


CLASS: MiniCPMModel
----------------------------------------
  L 236: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 265: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/minicpm3.py
Functions: 12
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  86: def input_to_float8(x, dtype)


CLASS: MiniCPM3AttentionMLA
----------------------------------------
  L  97: __init__(self, config: PretrainedConfig, hidden_size: int, num_heads: int, qk_nope_head_dim: int, qk_rope_head_dim: int, v_head_dim: int, q_lora_rank: int, kv_lora_rank: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], layer_id, prefix: str)
         ‚Üí None

  L 202: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: MiniCPM3DecoderLayer
----------------------------------------
  L 271: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 315: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: MiniCPM3ForCausalLM
----------------------------------------
  L 401: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 429: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor

  L 446: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: MiniCPM3MLP
----------------------------------------
  L  49: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  79: forward(self, x)


CLASS: MiniCPM3Model
----------------------------------------
  L 346: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 375: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/minicpmo.py
Functions: 40
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  71: def apply_spk_emb(input_ids: torch.Tensor,
        spk_emb: torch.Tensor,
        input_embeds: torch.Tensor,
        spk_emb_token_id: int,
        num_spk_embs: int)
         üìù Replace consecutive `num_spk_embs` speaker embedding placeholders in i

  L 126: def make_streaming_chunk_mask_generation(inputs_embeds: torch.Tensor,
        past_seen_tokens: int,
        streaming_tts_text_mask: torch.Tensor,
        streaming_reserved_length: int,
        streaming_audio_chunk_size: int,
        streaming_text_chunk_size: int,
        num_spk_emb: int,
        use_spk_emb: bool)
         ‚Üí torch.Tensor
         üìù In streaming audio generation, determine which `text` positions the TT


CLASS: ConditionalChatTTS
----------------------------------------
  L 551: __init__(self, config: PretrainedConfig)

  L 610: merge_inputs_embeds(self, input_ids: torch.Tensor, lm_spk_emb_last_hidden_states: Optional[torch.Tensor])
         üìù Merge `input_ids` and `lm_spk_emb_last_hidden_states` to `inputs_embed

  L 656: prefill_text(self, input_ids: torch.Tensor, position_ids: torch.LongTensor, past_key_values: List[Tuple[torch.Tensor, torch.Tensor]], lm_spk_emb_last_hidden_states: Optional[torch.Tensor])
         üìù Prefill a chunk of new text tokens in streaming setting.

  L 728: prefill_audio_ids(self, input_ids: torch.Tensor, past_key_values: List[Tuple[torch.Tensor, torch.Tensor]], streaming_tts_text_mask, add_audio_bos: bool)
         üìù Prefill a chunk of audio ids to the model. Used in sliding-window long

  L 788: generate(self, input_ids: torch.Tensor, past_key_values: List[Tuple[torch.Tensor, torch.Tensor]], temperature: torch.Tensor, eos_token: Union[int, torch.Tensor], streaming_tts_text_mask, force_no_stop, min_new_token, max_new_token, logits_warpers: List[LogitsWarper], logits_processors: List[CustomRepetitionPenaltyLogitsProcessorRepeat], show_tqdm)
         üìù Generate audio codes in streaming setting or non-streaming setting.

  L1051: decode_to_mel_specs(self, result_list: List[torch.Tensor])
         üìù Decode discrete audio codes to mel spectrograms.


CLASS: ConvNeXtBlock
----------------------------------------
  L 203: __init__(self, dim: int, intermediate_dim: int, kernel: int, dilation: int, layer_scale_init_value: float)

  L 232: forward(self, x: torch.Tensor, cond)
         ‚Üí torch.Tensor


CLASS: CustomRepetitionPenaltyLogitsProcessorRepeat
----------------------------------------
  L 426: __init__(self, penalty: float, max_input_ids: int, past_window: int)

  L 436: __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor)
         ‚Üí torch.FloatTensor


CLASS: DVAE
----------------------------------------
  L 345: __init__(self)

  L 386: forward(self, inp: torch.Tensor, mode: Literal['encode', 'decode'])
         ‚Üí torch.Tensor


CLASS: DVAEDecoder
----------------------------------------
  L 257: __init__(self, idim: int, odim: int, n_layer, bn_dim, hidden, kernel, dilation, up)

  L 288: forward(self, x: torch.Tensor, conditioning)
         ‚Üí torch.Tensor


CLASS: GFSQ
----------------------------------------
  L 302: __init__(self, dim: int, levels: List[int], G: int, R: int, eps, transpose)

  L 331: __call__(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L 334: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: MiniCPMO
----------------------------------------
  L1417: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig])
         ‚Üí None

  L1459: init_tts_module(self)

  L1463: init_audio_module(self)

  L1467: init_llm(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí nn.Module

  L1475: init_vision_module(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L1496: init_resampler(self, embed_dim: int, vision_dim: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí nn.Module

  L1516: pad_input_ids(self, input_ids: List[int], mm_input: MultimodalInputs)

  L1547: get_audio_embedding_streaming(self, items: List[MultimodalDataItem])
         üìù Extract audio embeddings in a streaming manner using cached key-value 

  L1614: subsequent_chunk_mask(self, size: int, chunk_size: int, num_left_chunks: int, device: torch.device, num_lookhead: int)
         ‚Üí torch.Tensor
         üìù Create mask for subsequent steps (size, size) with chunk size,

  L1647: get_audio_embedding(self, items: List[MultimodalDataItem], chunk_length)
         üìù Extract full audio embeddings with optional chunk-based attention.

  L1746: get_audio_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor

  L1754: get_omni_embedding(self, items: List[MultimodalDataItem], chunk_length, stream_input)
         üìù Args:

  L1778: get_image_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor

  L1817: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L1834: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: MiniCPMWhisperEncoder
----------------------------------------
  L1186: __init__(self, config: WhisperConfig)

  L1195: forward(self, input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, past_key_values: Optional[EncoderDecoderCache], use_cache: Optional[bool])
         üìù Forward pass of the Whisper encoder.


CLASS: MiniCPMWhisperEncoderLayer
----------------------------------------
  L1090: __init__(self, config: WhisperConfig, layer_idx: int)

  L1108: forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, layer_head_mask: torch.Tensor, output_attentions: bool, past_key_values: Optional[EncoderDecoderCache], use_cache: Optional[bool])
         ‚Üí torch.Tensor
         üìù Args:


CLASS: MultiModalProjector
----------------------------------------
  L1404: __init__(self, in_dim, out_dim)

  L1410: forward(self, audio_features)


============================================================
FILE: python/sglang/srt/models/minicpmv.py
Functions: 27
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  65: def get_1d_sincos_pos_embed_from_grid(embed_dim: int,
        pos: np.ndarray,
        version: Tuple[int,
        int])
         ‚Üí torch.Tensor
         üìù embed_dim: output dimension for each position

  L  92: def get_2d_sincos_pos_embed_from_grid(embed_dim: int,
        grid: np.ndarray,
        version: Tuple[int,
        int])
         ‚Üí torch.Tensor

  L 112: def get_2d_sincos_pos_embed(embed_dim: int,
        grid_size: Union[int,
        Tuple[int,
        int]],
        cls_token: bool,
        version: Tuple[int,
        int])
         ‚Üí torch.Tensor
         üìù grid_size: int of the grid height and width

  L 358: def get_version_by_config(config: PretrainedConfig)
         ‚Üí Tuple[int, ...]


CLASS: BaseResampler
----------------------------------------
  L 201: __init__(self, num_queries: int, embed_dim: int, num_heads: int, kv_dim: Optional[int], norm_layer: Callable[[int], nn.LayerNorm], do_post_projection: bool, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None


CLASS: MiniCPMBaseModel
----------------------------------------
  L 378: __init__(self)

  L 527: get_embedding(self, input_ids: torch.Tensor, image_inputs: Optional[MiniCPMVImageInputs])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 563: get_input_embeddings(self)
         ‚Üí nn.Embedding

  L 566: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 582: init_llm(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí nn.Module

  L 590: init_vision_module(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí nn.Module

  L 598: init_resampler(self, embed_dim: int, vision_dim: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí nn.Module

  L 607: get_vision_embedding(self, pixel_values: List[torch.Tensor], patch_attn_mask: Optional[torch.Tensor], tgt_sizes: Optional[torch.Tensor])
         ‚Üí torch.Tensor

  L 615: get_image_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor


CLASS: MiniCPMV
----------------------------------------
  L 796: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 824: __getattr__(self, name)

  L 829: __call__(self)

  L 832: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: MiniCPMV2_6
----------------------------------------
  L 659: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 668: init_llm(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí nn.Module

  L 676: init_vision_module(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí nn.Module

  L 692: init_resampler(self, embed_dim: int, vision_dim: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí nn.Module

  L 712: get_vision_embedding(self, pixel_values: List[torch.Tensor], patch_attn_mask: Optional[torch.Tensor], tgt_sizes: Optional[torch.Tensor])
         ‚Üí torch.Tensor

  L 725: get_image_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor

  L 764: pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)


CLASS: Resampler2_5
----------------------------------------
  L 260: __init__(self, num_queries: int, embed_dim: int, num_heads: int, kv_dim: Optional[int], norm_layer: Callable[[int], nn.LayerNorm], max_size: Tuple[int, int], quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 309: forward(self, x: torch.Tensor, tgt_sizes: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/mistral.py
Functions: 5
============================================================


CLASS: Mistral3ForConditionalGeneration
----------------------------------------
  L  32: __init__(self)

  L  46: get_image_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor
         üìù Extract features from image inputs.

  L  83: __getattr__(self, name)

  L  86: __hasattr__(self, name)

  L  89: __call__(self)


============================================================
FILE: python/sglang/srt/models/mixtral.py
Functions: 13
============================================================


CLASS: MixtralAttention
----------------------------------------
  L 123: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, max_position: int, rope_theta: float, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 189: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: MixtralDecoderLayer
----------------------------------------
  L 204: __init__(self, config: MixtralConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 239: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí torch.Tensor


CLASS: MixtralForCausalLM
----------------------------------------
  L 341: __init__(self, config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 359: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí torch.Tensor

  L 383: start_layer(self)

  L 387: end_layer(self)

  L 390: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: MixtralMoE
----------------------------------------
  L  66: __init__(self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], tp_size: Optional[int], prefix: str)

  L 109: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: MixtralModel
----------------------------------------
  L 265: __init__(self, config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 301: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí Union[torch.Tensor, PPProxyTensors]


============================================================
FILE: python/sglang/srt/models/mixtral_quant.py
Functions: 13
============================================================


CLASS: MixtralAttention
----------------------------------------
  L 173: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, max_position: int, rope_theta: float, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 239: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: MixtralDecoderLayer
----------------------------------------
  L 254: __init__(self, config: MixtralConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 285: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí torch.Tensor


CLASS: MixtralMLP
----------------------------------------
  L  52: __init__(self, num_experts: int, hidden_size: int, intermediate_size: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  90: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: MixtralMoE
----------------------------------------
  L 100: __init__(self, config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 148: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: MixtralModel
----------------------------------------
  L 311: __init__(self, config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 339: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: QuantMixtralForCausalLM
----------------------------------------
  L 361: __init__(self, config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 379: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor

  L 391: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/mllama.py
Functions: 32
============================================================


CLASS: ColumnParallelConv2dPatch
----------------------------------------
  L  56: __init__(self, in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]], bias: bool)
         ‚Üí None

  L  74: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: MllamaCrossAttentionDecoderLayer
----------------------------------------
  L 591: __init__(self, config: config_mllama.MllamaTextConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 622: forward(self, hidden_states: torch.Tensor, cross_attention_states: torch.Tensor, cross_attention_mask: torch.Tensor, full_text_row_masked_out_mask: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: MllamaForCausalLM
----------------------------------------
  L 740: __init__(self, config: config_mllama.MllamaTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 760: forward(self, input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], cross_attention_states: Optional[torch.LongTensor], cross_attention_mask: Optional[torch.LongTensor], full_text_row_masked_out_mask: Optional[Tuple[torch.Tensor, torch.Tensor]], forward_batch: ForwardBatch, skip_cross_attention: bool)
         ‚Üí torch.Tensor


CLASS: MllamaForConditionalGeneration
----------------------------------------
  L 804: __init__(self, config: config_mllama.MllamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 840: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 913: flat_encoder_result(self, cross_attention_states: torch.Tensor, encoder_lens_need: List[int])

  L 939: get_full_text_row_masked_out_mask(self, forward_batch: ForwardBatch)

  L 963: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí Union[Tuple, CausalLMOutputWithPast]

  L1027: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: MllamaPrecomputedAspectRatioEmbedding
----------------------------------------
  L  83: __init__(self, config: config_mllama.MllamaVisionConfig, is_gated: bool)

  L  96: forward(self, hidden_state: torch.Tensor, aspect_ratio_ids: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: MllamaPrecomputedPositionEmbedding
----------------------------------------
  L 110: __init__(self, config: config_mllama.MllamaVisionConfig)

  L 130: forward(self, hidden_state: torch.Tensor, aspect_ratio_ids: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: MllamaTextCrossAttention
----------------------------------------
  L 499: __init__(self, config: Optional[config_mllama.MllamaTextConfig], layer_id: Optional[int], quant_config: Optional[QuantizationConfig], prefix: str)

  L 557: forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor], cross_attention_states: Optional[torch.Tensor], forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: MllamaTextModel
----------------------------------------
  L 654: __init__(self, config: config_mllama.MllamaTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 695: forward(self, input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], cross_attention_states: Optional[torch.LongTensor], cross_attention_mask: Optional[torch.LongTensor], full_text_row_masked_out_mask: Optional[Tuple[torch.Tensor, torch.Tensor]], forward_batch: ForwardBatch, skip_cross_attention: bool)
         ‚Üí torch.Tensor


CLASS: MllamaTextRMSNorm
----------------------------------------
  L 482: __init__(self, hidden_size, eps)

  L 487: forward(self, hidden_states)

  L 494: extra_repr(self)


CLASS: MllamaVisionEncoder
----------------------------------------
  L 248: __init__(self, config: config_mllama.MllamaVisionConfig, quant_config: Optional[QuantizationConfig], num_layers, is_gated, output_hidden_states, prefix: str)

  L 272: forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor])
         ‚Üí Union[Tuple, BaseModelOutput]


CLASS: MllamaVisionEncoderLayer
----------------------------------------
  L 185: __init__(self, config: config_mllama.MllamaVisionConfig, quant_config: Optional[QuantizationConfig], is_gated: bool, prefix: str)

  L 225: forward(self, hidden_state: torch.Tensor, attention_mask: Optional[torch.Tensor])


CLASS: MllamaVisionMLP
----------------------------------------
  L 152: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 176: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: MllamaVisionModel
----------------------------------------
  L 294: __init__(self, config: config_mllama.MllamaVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 350: apply_class_embedding(self, hidden_state: torch.Tensor)
         ‚Üí torch.Tensor

  L 356: forward(self, pixel_values: torch.Tensor, aspect_ratio_ids: torch.Tensor, aspect_ratio_mask: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/mllama4.py
Functions: 27
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  91: def pixel_shuffle(input_tensor, shuffle_ratio)

  L 143: def apply_position_embedding(q, k, freqs_ci, shape)


CLASS: Llama4ForConditionalGeneration
----------------------------------------
  L 425: __init__(self, config: Llama4Config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 524: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 527: get_image_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor

  L 547: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 570: permute_qk_weight_for_rotary(self, name: str, loaded_weight: torch.Tensor)
         ‚Üí Tuple[str, torch.Tensor]

  L 604: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])
         ‚Üí Set[str]

  L 935: set_eagle3_layers_to_capture(self, layer_ids: Optional[List[int]])

  L 939: get_embed_and_head(self)

  L 951: set_embed_and_head(self, embed, head)

  L 958: get_embed(self)

  L 961: set_embed(self, embed)


CLASS: Llama4UnfoldConvolution
----------------------------------------
  L 266: __init__(self, config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)

  L 292: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Llama4VisionEncoder
----------------------------------------
  L 220: __init__(self, config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)

  L 241: forward(self, hidden_states: torch.Tensor, freqs_ci: torch.Tensor)
         ‚Üí torch.Tensor
         üìù Args:


CLASS: Llama4VisionEncoderLayer
----------------------------------------
  L 156: __init__(self, config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)

  L 197: forward(self, hidden_state: torch.Tensor, freqs_ci: torch.Tensor)


CLASS: Llama4VisionMLP
----------------------------------------
  L  51: __init__(self, input_size: int, intermediate_size: int, output_size: int, bias: bool, output_activation: bool, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)

  L  82: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Llama4VisionModel
----------------------------------------
  L 332: __init__(self, config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 377: forward(self, pixel_values: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Llama4VisionPixelShuffleMLP
----------------------------------------
  L 118: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)

  L 138: forward(self, encoded_patches: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Llama4VisionRotaryEmbedding
----------------------------------------
  L 300: __init__(self, config)

  L 326: forward(self, hidden_states)


============================================================
FILE: python/sglang/srt/models/nemotron_nas.py
Functions: 9
============================================================


CLASS: DeciLMDecoderLayer
----------------------------------------
  L  59: __init__(self, config: LlamaConfig, layer_idx: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 127: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: DeciLMForCausalLM
----------------------------------------
  L 295: __init__(self)

  L 341: get_input_embeddings(self, input_ids: torch.Tensor)
         ‚Üí torch.Tensor

  L 345: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor], get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí LogitsProcessorOutput

  L 371: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])
         ‚Üí None


CLASS: DeciModel
----------------------------------------
  L 160: __init__(self)

  L 210: get_input_embeddings(self, input_ids: torch.Tensor)
         ‚Üí torch.Tensor

  L 213: forward(self, input_ids: Optional[torch.Tensor], positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor], pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí Union[torch.Tensor, PPProxyTensors]


============================================================
FILE: python/sglang/srt/models/olmo.py
Functions: 11
============================================================


CLASS: OlmoAttention
----------------------------------------
  L  51: __init__(self, config: OlmoConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 108: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: OlmoDecoderLayer
----------------------------------------
  L 180: __init__(self, config: OlmoConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 211: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]


CLASS: OlmoForCausalLM
----------------------------------------
  L 299: __init__(self, config: OlmoConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 321: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor

  L 338: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: OlmoMLP
----------------------------------------
  L 131: __init__(self, config: OlmoConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 163: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: OlmoModel
----------------------------------------
  L 233: __init__(self, config: OlmoConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 261: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor
         üìù :param input_ids: A tensor of shape `(batch_size, seq_len)`.


============================================================
FILE: python/sglang/srt/models/olmo2.py
Functions: 11
============================================================


CLASS: Olmo2Attention
----------------------------------------
  L  58: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 148: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: Olmo2DecoderLayer
----------------------------------------
  L 219: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 244: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: Olmo2ForCausalLM
----------------------------------------
  L 330: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 354: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor

  L 371: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Olmo2MLP
----------------------------------------
  L 170: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 202: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Olmo2Model
----------------------------------------
  L 266: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 292: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor
         üìù :param input_ids: A tensor of shape `(batch_size, seq_len)`.


============================================================
FILE: python/sglang/srt/models/olmoe.py
Functions: 11
============================================================


CLASS: OlmoeAttention
----------------------------------------
  L 109: __init__(self, layer_id: int, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 181: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: OlmoeDecoderLayer
----------------------------------------
  L 198: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 235: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí torch.Tensor


CLASS: OlmoeForCausalLM
----------------------------------------
  L 315: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 335: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor

  L 347: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: OlmoeMoE
----------------------------------------
  L  57: __init__(self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], tp_size: Optional[int], layer_id: int, prefix: str)

  L  96: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: OlmoeModel
----------------------------------------
  L 263: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 290: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/persimmon.py
Functions: 13
============================================================


CLASS: PersimmonAttention
----------------------------------------
  L  52: __init__(self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig], prefix: str, layer_id: int)

  L 120: forward(self, position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: PersimmonDecoderLayer
----------------------------------------
  L 147: __init__(self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig], prefix: str, idx: int)

  L 170: forward(self, position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: PersimmonForCausalLM
----------------------------------------
  L 262: __init__(self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 282: get_input_embeddings(self, input_ids: torch.Tensor)
         ‚Üí torch.Tensor

  L 285: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor])
         ‚Üí LogitsProcessorOutput

  L 303: load_weights(self, weights: Iterable[tuple[str, torch.Tensor]])


CLASS: PersimmonMLP
----------------------------------------
  L  31: __init__(self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig])

  L  43: forward(self, hidden_states)
         ‚Üí torch.Tensor


CLASS: PersimmonModel
----------------------------------------
  L 199: __init__(self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 233: get_input_embeddings(self, input_ids: torch.Tensor)
         ‚Üí torch.Tensor

  L 236: forward(self, input_ids: torch.Tensor, forward_batch: ForwardBatch, positions: torch.Tensor, inputs_embeds: Optional[torch.Tensor])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/phi.py
Functions: 13
============================================================


CLASS: PhiAttention
----------------------------------------
  L  30: __init__(self, config: PhiConfig, quant_config: Optional[QuantizationConfig], prefix: str, layer_id: int)

  L  84: forward(self, position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: PhiForCausalLM
----------------------------------------
  L 234: __init__(self, config: PhiConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 257: get_input_embeddings(self, input_ids: torch.Tensor)
         ‚Üí torch.Tensor

  L 260: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor])
         ‚Üí LogitsProcessorOutput

  L 279: load_weights(self, weights: Iterable[tuple[str, torch.Tensor]])


CLASS: PhiLayer
----------------------------------------
  L 129: __init__(self, config: PhiConfig, quant_config: Optional[QuantizationConfig], prefix: str, idx: int)

  L 148: forward(self, position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: PhiMLP
----------------------------------------
  L 100: __init__(self, config: PhiConfig, quant_config: Optional[QuantizationConfig])

  L 120: forward(self, hidden_states)


CLASS: PhiModel
----------------------------------------
  L 168: __init__(self, config: PhiConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 199: get_input_embeddings(self, input_ids: torch.Tensor)
         ‚Üí torch.Tensor

  L 202: forward(self, input_ids: torch.Tensor, forward_batch: ForwardBatch, positions: torch.Tensor, inputs_embeds: Optional[torch.Tensor])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/phi3_small.py
Functions: 21
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  32: def quick_gelu(x)
         @torch.jit.script

  L  37: def gegelu(input, limit: Optional[float])
         @torch.jit.script


CLASS: Phi3SmallDecoderLayer
----------------------------------------
  L 236: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 264: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: Phi3SmallForCausalLM
----------------------------------------
  L 364: __init__(self, config: Phi3Config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 406: get_input_embeddings(self, input_ids: torch.Tensor)
         ‚Üí torch.Tensor

  L 409: set_input_embeddings(self, value)

  L 412: get_output_embeddings(self)

  L 415: set_output_embeddings(self, value)

  L 418: set_decoder(self, decoder)

  L 421: get_decoder(self)

  L 424: compute_logits(self, input_ids: torch.LongTensor, hidden_states: torch.Tensor, sampling_metadata)
         ‚Üí Optional[torch.Tensor]

  L 437: forward(self, input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor], get_embedding: bool)
         ‚Üí LogitsProcessorOutput

  L 460: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Phi3SmallMLP
----------------------------------------
  L  54: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  84: forward(self, x)


CLASS: Phi3SmallModel
----------------------------------------
  L 289: __init__(self, config: Phi3Config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 332: get_input_embeddings(self, input_ids: torch.Tensor)
         ‚Üí torch.Tensor

  L 335: forward(self, input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor]


CLASS: Phi3SmallSelfAttention
----------------------------------------
  L  93: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 210: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]


============================================================
FILE: python/sglang/srt/models/phi4mm.py
Functions: 10
============================================================


CLASS: Phi4MMForCausalLM
----------------------------------------
  L 387: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 416: get_image_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor

  L 433: get_audio_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor

  L 454: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 474: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 478: should_apply_lora(self, module_name: str)
         ‚Üí bool

  L 481: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Phi4MMImageEncoder
----------------------------------------
  L  60: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str, model_dir: str)
         ‚Üí None

  L 136: get_img_features(self, img_embeds: torch.FloatTensor, attention_mask)
         ‚Üí torch.FloatTensor

  L 169: forward(self, pixel_values: torch.FloatTensor, image_sizes: torch.Tensor, image_attention_mask: torch.Tensor)
         ‚Üí list[torch.FloatTensor]
         üìù process image and return vision embeddings.


============================================================
FILE: python/sglang/srt/models/phi4mm_audio.py
Functions: 18
============================================================


CLASS: AudioEmbedding
----------------------------------------
  L1078: __init__(self, config: PretrainedConfig)
         ‚Üí None

  L1182: set_audio_embeds(self, input_embeds: torch.FloatTensor)
         ‚Üí None

  L1185: set_audio_embed_sizes(self, audio_embed_sizes: torch.LongTensor)
         ‚Üí None

  L1188: get_audio_features(self, input_embeds: torch.FloatTensor, audio_attention_mask: torch.Tensor, audio_projection_mode: str)
         ‚Üí torch.FloatTensor
         üìù arguments:

  L1242: forward(self, audio_features: torch.FloatTensor, audio_attention_mask: torch.Tensor, audio_projection_mode: str)
         ‚Üí torch.FloatTensor
         üìù arguments:


CLASS: ConformerEncoder
----------------------------------------
  L 778: __init__(self, input_size, chunk_size, left_chunk, num_lang, attention_dim, attention_heads, linear_units, num_blocks, dropout_rate, input_layer, causal, batch_norm, cnn_out, cnn_layer_norm, ext_pw_out_channel, ext_pw_kernel_size, depthwise_seperable_out_channel, depthwise_multiplier, chunk_se, kernel_size, activation, conv_activation, conv_glu_type, bias_in_glu, linear_glu_in_convm, attention_glu_type, export, extra_layer_output_idx, extra_multi_layer_output_idxs, activation_checkpointing, relative_attention_bias_args, time_reduction, use_pt_scaled_dot_product_attention, nemo_conv_settings, conv2d_extra_padding: Literal['feat', 'feat_time', 'none', True], replication_pad_for_subsample_embedding, attention_group_size, encoder_embedding_config)

  L 884: init_relative_attention_bias(self, input_tensor)

  L 888: calculate_hs_mask(self, xs_pad, device, mask)

  L 908: forward(self, xs_pad, masks)
         üìù Conformer Forward function


CLASS: ConformerEncoderLayer
----------------------------------------
  L 148: __init__(self, d_model, ext_pw_out_channel, depthwise_seperable_out_channel, depthwise_multiplier, n_head, d_ffn, ext_pw_kernel_size, kernel_size, dropout_rate, causal, batch_norm, activation, chunk_se, chunk_size, conv_activation, conv_glu_type, bias_in_glu, linear_glu_in_convm, attention_inner_dim, attention_glu_type, activation_checkpointing, export, use_pt_scaled_dot_product_attention, attn_group_sizes: int)

  L 225: forward(self, x, pos_k, pos_v, mask, relative_attention_bias: Optional[Tensor])
         üìù ConformerEncoder forward.


CLASS: TransformerEncoderBase
----------------------------------------
  L 339: __init__(self, input_size, chunk_size, left_chunk, attention_dim, attention_heads, input_layer, cnn_out, cnn_layer_norm, time_reduction, dropout_rate, padding_idx, relative_attention_bias_args, positional_dropout_rate, nemo_conv_settings, conv2d_extra_padding: Literal['feat', 'feat_time', 'none', True], attention_group_size, encoder_embedding_config)

  L 423: compute_lens_change(self, feature_lens)
         üìù feature_lens: int

  L 458: forward(self)
         üìù Abstract forward method implementation.

  L 534: forward_embeddings(self, xs_pad, masks, chunk_size_nc, left_chunk_nc)
         üìù Forwarding the inputs through the top embedding layers

  L 598: get_offset(self)
         üìù Returns offset used when retaining inputs for decoding.


CLASS: WindowQformer
----------------------------------------
  L1001: __init__(self, window_size: int, num_queries: int, num_blocks: int, attention_dim: int, attention_heads: int, linear_units: int, dropout_rate: float, normalize_before: bool)

  L1035: forward(self, audio_embed, mask, embed_len)
         üìù forward decoder


============================================================
FILE: python/sglang/srt/models/phi4mm_utils.py
Functions: 49
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  32: def get_activation(name)
         üìù Select an activation function by name

  L  53: def adaptive_enc_mask(x_len, chunk_start_idx, left_window, right_window)
         üìù The function is very important for Transformer Transducer Streaming mo

  L1586: def calc_length(lengths,
        all_paddings,
        kernel_size,
        stride,
        ceil_mode,
        repeat_num)
         üìù Calculates the output length of a Tensor passed through a convolution 

  L1639: def masked_softmax(scores, mask: Optional[Tensor])

  L1875: def get_offset(input_layer: str, time_reduction: int)
         üìù Get an offset. We will use the offset for determining #frames of a

  L1894: def unfold_tensor(xs_pad, max_seq_len)
         üìù For a given tensor with shape of (N, T, D), if sequence length T is


CLASS: AbsolutePositionalEncoding
----------------------------------------
  L 827: __init__(self, d_model, dropout_rate, max_len)
         üìù Construct an PositionalEncoding object.

  L 837: extend_pe(self, x)
         üìù Reset the positional encodings.

  L 858: forward(self, x: torch.Tensor)
         üìù Add positional encoding.


CLASS: AttBlock
----------------------------------------
  L1634: memory_dims(self, max_len)
         üìù memory dimensions


CLASS: AttModule
----------------------------------------
  L1601: __init__(self)

  L1605: set_export(self, mode)
         üìù set the export mode

  L1609: forward(self, x: Tensor, memory: Optional[Tensor], pos_emb: Optional[Tensor], att_mask: Optional[Tensor])
         ‚Üí tuple[Tensor, Tensor, Optional[Tensor], Optional[Tensor]]
         üìù AttModule forward


CLASS: BlockBase
----------------------------------------
  L  26: __init__(self, input_size, output_size)


CLASS: CausalConv1D
----------------------------------------
  L 919: __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: Union[str, int], dilation: int, groups: int, bias: bool, padding_mode: str, device, dtype)
         ‚Üí None

  L 969: update_cache(self, x, cache)

  L 983: forward(self, x, cache)


CLASS: CausalConv2D
----------------------------------------
  L1000: __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: Union[str, int], dilation: int, groups: int, bias: bool, padding_mode: str, device, dtype)
         ‚Üí None

  L1034: forward(self, x)


CLASS: ConvModule
----------------------------------------
  L 389: __init__(self, input_dim, ext_pw_out_channel, depthwise_seperable_out_channel, ext_pw_kernel_size, kernel_size, depthwise_multiplier, dropout_rate, causal, batch_norm, chunk_se, chunk_size, activation, glu_type, bias_in_glu, linear_glu_in_convm, export)

  L 512: forward(self, x)
         üìù ConvModule Forward.


CLASS: DepthWiseSeperableConv1d
----------------------------------------
  L 286: __init__(self, input_dim, depthwise_seperable_out_channel, kernel_size, depthwise_multiplier, padding)

  L 317: forward(self, x)
         üìù Args:


CLASS: FeedForward
----------------------------------------
  L 622: __init__(self, d_model, d_inner, dropout_rate, activation, bias_in_glu)

  L 643: forward(self, x)
         üìù FeedForward forward function.


CLASS: GLU
----------------------------------------
  L 125: __init__(self, dim: int, act_name: str)
         ‚Üí None

  L 141: forward(self, x: Tensor)
         ‚Üí Tensor
         üìù GLU forward


CLASS: GLULinear
----------------------------------------
  L 580: __init__(self, input_dim, output_dim, glu_type, bias_in_glu)

  L 591: forward(self, x)
         üìù GLULinear forward


CLASS: GLUPointWiseConv
----------------------------------------
  L 182: __init__(self, input_dim, output_dim, kernel_size, glu_type, bias_in_glu, causal)

  L 228: forward(self, x)
         üìù Args:


CLASS: MeanVarianceNormLayer
----------------------------------------
  L 886: __init__(self, input_size)

  L 892: forward(self, input_: Tensor)
         ‚Üí Tensor
         üìù MeanVarianceNormLayer Forward


CLASS: MultiHeadedAttention
----------------------------------------
  L1693: __init__(self, n_head, n_feat, dropout_rate, attention_inner_dim, glu_type, bias_in_glu, use_pt_scaled_dot_product_attention, n_value, group_size: int)

  L1741: forward(self, query: Tensor, key: Tensor, value: Tensor, pos_k: Tensor, pos_v: Tensor, mask: Optional[Tensor], relative_attention_bias: Optional[Tensor])
         üìù Compute 'Scaled Dot Product Attention'.


CLASS: MultiSequential
----------------------------------------
  L1868: forward(self)
         üìù Forward method implementation.


CLASS: NemoConvSubsampling
----------------------------------------
  L1080: __init__(self, feat_in, feat_out, subsampling_factor, subsampling, conv_channels, subsampling_conv_chunking_factor, activation, is_causal)

  L1369: get_sampling_frames(self)

  L1372: get_streaming_cache_size(self)

  L1375: forward(self, x, mask)
         üìù Forward method for NeMo subsampling.

  L1443: reset_parameters(self)

  L1468: conv_split_by_batch(self, x)
         üìù Tries to split input by batch, run conv and concat results

  L1494: conv_split_by_channel(self, x)
         üìù For dw convs, tries to split input by time, run conv and concat

  L1532: channel_chunked_conv(self, conv, chunk_size, x)
         üìù Performs channel chunked convolution

  L1572: change_subsampling_conv_chunking_factor(self, subsampling_conv_chunking_factor: int)


CLASS: Swish
----------------------------------------
  L 108: __init__(self)
         ‚Üí None

  L 112: forward(self, x: Tensor)
         ‚Üí Tensor
         üìù Apply Swish function


CLASS: T5RelativeAttentionLogitBias
----------------------------------------
  L 722: __init__(self, num_heads, num_buckets, max_distance, symmetric)

  L 739: forward(self, x)


============================================================
FILE: python/sglang/srt/models/phimoe.py
Functions: 14
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 107: def sparsemixer(scores, jitter_eps)

  L 159: def phimoe_routing_function(hidden_states: torch.Tensor,
        gating_output: torch.Tensor,
        topk: int,
        renormalize: bool)


CLASS: PhiMoE
----------------------------------------
  L 182: __init__(self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 221: forward(self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch])
         ‚Üí torch.Tensor


CLASS: PhiMoEAttention
----------------------------------------
  L 235: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: Optional[int], max_position: int, rope_theta: float, layer_id: int, attention_bias: bool, quant_config: Optional[QuantizationConfig], rope_scaling: Optional[dict], prefix: str)
         ‚Üí None

  L 315: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: PhiMoEConfig
----------------------------------------
  L  39: __init__(self, vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, head_dim, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, tie_word_embeddings, rope_theta, sliding_window, attention_dropout, num_experts_per_tok, num_local_experts, output_router_logits, router_aux_loss_coef, router_jitter_noise, attention_bias, lm_head_bias)


CLASS: PhiMoEDecoderLayer
----------------------------------------
  L 331: __init__(self, config: PhiMoEConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 372: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, residual: Optional[torch.Tensor], forward_batch: ForwardBatch)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: PhiMoEForCausalLM
----------------------------------------
  L 455: __init__(self, config: PhiMoEConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 484: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor], get_embedding: bool)
         ‚Üí LogitsProcessorOutput

  L 502: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: PhiMoEModel
----------------------------------------
  L 402: __init__(self, config: PhiMoEConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 431: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor]


============================================================
FILE: python/sglang/srt/models/pixtral.py
Functions: 13
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 225: def resolve_visual_encoder_outputs(outputs: Union[torch.Tensor,
        List[torch.Tensor]],
        feature_sample_layers: Optional[List[int]],
        post_norm: Optional[nn.Module],
        num_hidden_layers: int)
         ‚Üí torch.Tensor
         üìù Resolve outputs from visual encoder based on feature_sample_layers.


CLASS: PixtralHFMLP
----------------------------------------
  L  46: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig])
         ‚Üí None

  L  76: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: PixtralHFTransformer
----------------------------------------
  L 166: __init__(self, config: PixtralVisionConfig, quant_config: Optional[QuantizationConfig])
         ‚Üí None

  L 192: forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor], position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]], return_all_hidden_states: bool)
         ‚Üí Union[torch.Tensor, List[torch.Tensor]]
         üìù Forward pass through transformer layers.


CLASS: PixtralHFTransformerBlock
----------------------------------------
  L  90: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig])
         ‚Üí None

  L 123: forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor], position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]])
         ‚Üí torch.Tensor


CLASS: PixtralHFVisionModel
----------------------------------------
  L 271: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 274: __init__(self, config: PixtralVisionConfig, quant_config: Optional[QuantizationConfig])
         ‚Üí None

  L 320: dtype(self)

  L 324: device(self)

  L 327: forward(self, pixel_values: torch.Tensor, image_sizes: list[tuple[int, int]], output_hidden_states: bool, feature_sample_layers: Optional[list[int]])
         ‚Üí Union[torch.Tensor, tuple]
         üìù Args:

  L 419: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])
         ‚Üí Set[str]
         üìù Load weights from a HuggingFace checkpoint with proper parameter mappi


============================================================
FILE: python/sglang/srt/models/qwen.py
Functions: 12
============================================================


CLASS: QWenAttention
----------------------------------------
  L  87: __init__(self, hidden_size: int, num_heads: int, max_position_embeddings: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], quant_config: Optional[QuantizationConfig], prefix: str)

  L 141: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: QWenBlock
----------------------------------------
  L 156: __init__(self, config: PretrainedConfig, layer_id, quant_config: Optional[QuantizationConfig], prefix: str)

  L 188: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: QWenLMHeadModel
----------------------------------------
  L 261: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 279: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)

  L 291: forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int])

  L 326: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: QWenMLP
----------------------------------------
  L  47: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)

  L  79: forward(self, x)


CLASS: QWenModel
----------------------------------------
  L 213: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 242: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/qwen2.py
Functions: 22
============================================================


CLASS: Qwen2Attention
----------------------------------------
  L  99: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: Optional[int], layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], dual_chunk_attention_config: Optional[dict[str, Any]], prefix: str)
         ‚Üí None

  L 174: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: Qwen2DecoderLayer
----------------------------------------
  L 189: __init__(self, config: Qwen2Config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream])
         ‚Üí None

  L 231: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: Qwen2ForCausalLM
----------------------------------------
  L 409: __init__(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 456: get_input_embedding(self, input_ids: torch.Tensor)
         ‚Üí torch.Tensor

  L 459: get_input_embeddings(self)
         ‚Üí nn.Embedding

  L 463: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí torch.Tensor

  L 491: forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)

  L 532: start_layer(self)

  L 536: end_layer(self)

  L 539: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 608: get_embed_and_head(self)

  L 611: set_embed_and_head(self, embed, head)

  L 619: load_kv_cache_scales(self, quantization_param_path: str)
         ‚Üí None


CLASS: Qwen2MLP
----------------------------------------
  L  61: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  91: forward(self, x)


CLASS: Qwen2Model
----------------------------------------
  L 257: __init__(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str, decoder_layer_type: type[nn.Module], alt_stream: Optional[torch.cuda.Stream])
         ‚Üí None

  L 305: get_input_embedding(self, input_ids: torch.Tensor)
         ‚Üí torch.Tensor

  L 311: get_input_embeddings(self)
         ‚Üí nn.Embedding

  L 314: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí Union[torch.Tensor, PPProxyTensors]

  L 368: load_kv_cache_scales(self, quantization_param_path: str)
         ‚Üí None


============================================================
FILE: python/sglang/srt/models/qwen2_5_vl.py
Functions: 19
============================================================


CLASS: Qwen2_5_VLForConditionalGeneration
----------------------------------------
  L 462: __init__(self, config: Qwen2_5_VLConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 500: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 504: get_image_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor

  L 515: get_video_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor

  L 526: get_input_embeddings(self)

  L 530: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)
         üìù Run forward pass for Qwen2_5-VL.

  L 577: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Qwen2_5_VLMLP
----------------------------------------
  L  66: __init__(self, in_features: int, hidden_features: int, bias: bool, hidden_act, quant_config: Optional[QuantizationConfig], prefix: str)

  L  99: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Qwen2_5_VisionBlock
----------------------------------------
  L 110: __init__(self, dim: int, intermediate_dim: int, num_heads: int, hidden_act, norm_layer: Type[nn.Module], attn_implementation: Optional[str], quant_config: Optional[QuantizationConfig], prefix: str, num_dummy_heads: int)
         ‚Üí None

  L 171: forward(self, x: torch.Tensor, cu_seqlens: torch.Tensor, position_embeddings: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Qwen2_5_VisionPatchMerger
----------------------------------------
  L 194: __init__(self, dim: int, context_dim: int, spatial_merge_size: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 225: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Qwen2_5_VisionTransformer
----------------------------------------
  L 238: __init__(self, vision_config: Qwen2_5_VLVisionConfig, norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 292: get_window_index(self, grid_thw)

  L 338: dtype(self)
         ‚Üí torch.dtype

  L 342: device(self)
         ‚Üí torch.device

  L 345: rot_pos_emb(self, grid_thw: torch.Tensor)
         ‚Üí torch.Tensor

  L 377: forward(self, x: torch.Tensor, grid_thw: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/qwen2_audio.py
Functions: 5
============================================================


CLASS: Qwen2AudioForConditionalGeneration
----------------------------------------
  L  88: __init__(self, config: Qwen2AudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 115: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 118: get_audio_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor

  L 134: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 153: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/qwen2_eagle.py
Functions: 5
============================================================


CLASS: Qwen2DecoderLayer
----------------------------------------
  L  41: __init__(self, config: Qwen2Config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None


CLASS: Qwen2ForCausalLMEagle
----------------------------------------
  L 115: __init__(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 139: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Qwen2Model
----------------------------------------
  L  58: __init__(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  85: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/qwen2_moe.py
Functions: 17
============================================================


CLASS: Qwen2MoeAttention
----------------------------------------
  L 197: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, qkv_bias: int, quant_config: Optional[QuantizationConfig], dual_chunk_attention_config: Optional[dict[str, Any]], prefix: str)
         ‚Üí None

  L 278: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: Qwen2MoeDecoderLayer
----------------------------------------
  L 293: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream])
         ‚Üí None

  L 367: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: Qwen2MoeForCausalLM
----------------------------------------
  L 518: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 541: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí torch.Tensor

  L 564: forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)

  L 607: start_layer(self)

  L 611: end_layer(self)

  L 614: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 701: get_model_config_for_expert_location(cls, config)


CLASS: Qwen2MoeMLP
----------------------------------------
  L  74: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str)
         ‚Üí None

  L 105: forward(self, x, use_reduce_scatter: bool)


CLASS: Qwen2MoeModel
----------------------------------------
  L 405: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str, decoder_layer_type: type[nn.Module], alt_stream: Optional[torch.cuda.Stream])
         ‚Üí None

  L 452: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí Union[torch.Tensor, PPProxyTensors]


CLASS: Qwen2MoeSparseMoeBlock
----------------------------------------
  L 117: __init__(self, layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 168: forward(self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch], use_reduce_scatter: bool)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/qwen2_rm.py
Functions: 3
============================================================


CLASS: Qwen2ForRewardModel
----------------------------------------
  L  29: __init__(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  52: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         ‚Üí EmbeddingPoolerOutput

  L  68: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/qwen2_vl.py
Functions: 23
============================================================


CLASS: Qwen2VLForConditionalGeneration
----------------------------------------
  L 445: __init__(self, config: Qwen2VLConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 481: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 485: get_image_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor

  L 496: get_video_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor

  L 514: get_input_embeddings(self)

  L 517: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)
         üìù Run forward pass for Qwen2-VL.

  L 563: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Qwen2VisionBlock
----------------------------------------
  L 124: __init__(self, dim: int, num_heads: int, mlp_ratio: float, act_layer: Type[nn.Module], norm_layer: Type[nn.Module], attn_implementation: Optional[str], quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 170: forward(self, x: torch.Tensor, cu_seqlens: torch.Tensor, position_embeddings: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Qwen2VisionMLP
----------------------------------------
  L  92: __init__(self, in_features: int, hidden_features: int, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)

  L 115: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Qwen2VisionPatchEmbed
----------------------------------------
  L 191: __init__(self, patch_size: int, temporal_patch_size: int, in_chans: int, embed_dim: int)
         ‚Üí None

  L 208: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Qwen2VisionPatchMerger
----------------------------------------
  L 217: __init__(self, d_model: int, context_dim: int, norm_layer: Type[nn.Module], spatial_merge_size: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 251: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Qwen2VisionRotaryEmbedding
----------------------------------------
  L 264: __init__(self, dim: int, theta: float)
         ‚Üí None

  L 273: update_freqs_cache(self, seqlen: int)
         ‚Üí None

  L 292: forward(self, seqlen: int)
         ‚Üí torch.Tensor


CLASS: Qwen2VisionTransformer
----------------------------------------
  L 299: __init__(self, vision_config: Qwen2VLVisionConfig, norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 353: dtype(self)
         ‚Üí torch.dtype

  L 357: device(self)
         ‚Üí torch.device

  L 360: rot_pos_emb(self, grid_thw: torch.Tensor)
         ‚Üí torch.Tensor

  L 393: forward(self, x: torch.Tensor, grid_thw: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/qwen3.py
Functions: 16
============================================================


CLASS: Qwen3Attention
----------------------------------------
  L  39: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], head_dim: Optional[int], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], rms_norm_eps: float, attention_bias: bool, prefix: str, alt_stream: Optional[torch.cuda.Stream])
         ‚Üí None

  L 146: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: Qwen3DecoderLayer
----------------------------------------
  L 162: __init__(self, config: Qwen3Config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream])
         ‚Üí None

  L 215: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: Qwen3ForCausalLM
----------------------------------------
  L 281: __init__(self, config: Qwen3Config, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 330: get_input_embeddings(self)
         ‚Üí nn.Embedding

  L 334: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí torch.Tensor

  L 370: forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)

  L 411: start_layer(self)

  L 415: end_layer(self)

  L 418: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 487: get_embed_and_head(self)

  L 490: set_embed_and_head(self, embed, head)

  L 498: load_kv_cache_scales(self, quantization_param_path: str)
         ‚Üí None

  L 501: set_eagle3_layers_to_capture(self, layer_ids: Optional[List[int]])


CLASS: Qwen3Model
----------------------------------------
  L 245: __init__(self, config: Qwen3Config, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None


============================================================
FILE: python/sglang/srt/models/qwen3_classification.py
Functions: 3
============================================================


CLASS: Qwen3ForSequenceClassification
----------------------------------------
  L  29: __init__(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  56: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor], get_embedding: bool)
         ‚Üí EmbeddingPoolerOutput

  L  74: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/qwen3_moe.py
Functions: 36
============================================================


CLASS: Qwen3MoeAttention
----------------------------------------
  L 263: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, head_dim: Optional[int], rms_norm_eps: float, attention_bias: bool, quant_config: Optional[QuantizationConfig], prefix: str, dual_chunk_attention_config: Optional[dict[str, Any]], alt_stream: Optional[torch.cuda.Stream])
         ‚Üí None

  L 373: op_prepare(self, state)

  L 380: op_core(self, state)

  L 385: forward_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)

  L 400: forward_core(self, intermediate_state)

  L 408: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: Qwen3MoeDecoderLayer
----------------------------------------
  L 423: __init__(self, config: Qwen3MoeConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream])
         ‚Üí None

  L 505: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 541: op_comm_prepare_attn(self, state, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], tbo_subbatch_index: Optional[int])

  L 561: op_comm_prepare_mlp(self, state)

  L 570: op_mlp(self, state)

  L 574: op_comm_postprocess_layer(self, state)


CLASS: Qwen3MoeForCausalLM
----------------------------------------
  L 619: __init__(self, config: Qwen3MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 642: get_input_embeddings(self)
         ‚Üí nn.Embedding

  L 646: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí torch.Tensor

  L 674: forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)

  L 717: start_layer(self)

  L 721: end_layer(self)

  L 724: get_embed_and_head(self)

  L 727: set_eagle3_layers_to_capture(self, layer_ids: Optional[List[int]])

  L 742: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 857: get_model_config_for_expert_location(cls, config)


CLASS: Qwen3MoeModel
----------------------------------------
  L 600: __init__(self, config: Qwen3MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None


CLASS: Qwen3MoeSparseMoeBlock
----------------------------------------
  L  69: __init__(self, layer_id: int, config: Qwen3MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 118: forward(self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch], use_reduce_scatter: bool)
         ‚Üí torch.Tensor

  L 130: get_moe_weights(self)

  L 137: forward_normal(self, hidden_states: torch.Tensor, use_reduce_scatter: bool)
         ‚Üí torch.Tensor

  L 154: forward_deepep(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 183: op_gate(self, state)

  L 192: op_select_experts(self, state)

  L 215: op_dispatch_a(self, state)

  L 225: op_dispatch_b(self, state)

  L 234: op_experts(self, state)

  L 239: op_combine_a(self, state)

  L 250: op_combine_b(self, state)

  L 258: op_output(self, state)


============================================================
FILE: python/sglang/srt/models/registry.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  77: def import_model_classes()
         @lru_cache()


CLASS: _ModelRegistry
----------------------------------------
  L  20: get_supported_archs(self)
         ‚Üí AbstractSet[str]

  L  62: resolve_model_cls(self, architectures: Union[str, List[str]])
         ‚Üí Tuple[Type[nn.Module], str]


============================================================
FILE: python/sglang/srt/models/roberta.py
Functions: 14
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 192: def create_position_ids_from_input_ids(input_ids,
        padding_idx,
        past_key_values_length)


CLASS: RobertaClassificationHead
----------------------------------------
  L  23: __init__(self, config: RobertaConfig)

  L  28: forward(self, features)


CLASS: RobertaEmbedding
----------------------------------------
  L  38: __init__(self, config: RobertaConfig)

  L  66: forward(self, input_ids: torch.Tensor, seq_lens: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: XLMRobertaBaseModel
----------------------------------------
  L 115: __init__(self)

  L 135: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         ‚Üí torch.Tensor

  L 157: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: XLMRobertaForSequenceClassification
----------------------------------------
  L 234: __init__(self)

  L 248: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         ‚Üí torch.Tensor

  L 265: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: XLMRobertaModel
----------------------------------------
  L 203: __init__(self)

  L 216: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         ‚Üí torch.Tensor

  L 229: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/siglip.py
Functions: 14
============================================================


CLASS: SiglipEncoder
----------------------------------------
  L 176: __init__(self, config: SiglipVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 201: forward(self, inputs_embeds: torch.Tensor, attention_mask: torch.Tensor, causal_attention_mask: torch.Tensor, return_all_hidden_states: bool)
         ‚Üí Union[torch.Tensor, list[torch.Tensor]]


CLASS: SiglipEncoderLayer
----------------------------------------
  L  95: __init__(self, config: SiglipVisionConfig, act_layer: Type[nn.Module], norm_layer: Type[nn.Module], attn_implementation: Optional[str], quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 136: forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, causal_attention_mask: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: SiglipMLP
----------------------------------------
  L  63: __init__(self, config, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)

  L  85: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: SiglipVisionEmbeddings
----------------------------------------
  L  22: __init__(self, config: SiglipVisionConfig)

  L  48: forward(self, pixel_values: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: SiglipVisionModel
----------------------------------------
  L 278: __init__(self, config: SiglipVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 290: device(self)
         ‚Üí torch.device

  L 293: forward(self, pixel_values: torch.Tensor)


CLASS: SiglipVisionTransformer
----------------------------------------
  L 225: __init__(self, config: SiglipVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 255: device(self)
         ‚Üí torch.device

  L 258: forward(self, pixel_values: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/stablelm.py
Functions: 11
============================================================


CLASS: StableLMEpochModel
----------------------------------------
  L 215: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 241: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: StableLmForCausalLM
----------------------------------------
  L 264: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 282: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor

  L 294: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: StablelmAttention
----------------------------------------
  L  83: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 156: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: StablelmDecoderLayer
----------------------------------------
  L 171: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 189: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: StablelmMLP
----------------------------------------
  L  49: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  75: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/step3_vl.py
Functions: 32
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 496: def get_abs_pos(abs_pos, tgt_size)


CLASS: Step3TextAttention
----------------------------------------
  L 173: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: int, share_q_dim: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], rms_norm_eps, prefix: str)
         ‚Üí None

  L 267: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: Step3TextDecoderLayer
----------------------------------------
  L 284: __init__(self, config: Step3TextConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 388: moe_mlp_forward(self, hidden_states)

  L 397: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: Step3TextMLP
----------------------------------------
  L  75: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 105: forward(self, x)


CLASS: Step3TextMoEMLP
----------------------------------------
  L 114: __init__(self, layer_id: int, config: Step3TextConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 157: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Step3TextModel
----------------------------------------
  L 432: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 461: get_input_embeddings(self)

  L 464: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Step3VLForConditionalGeneration
----------------------------------------
  L 739: __init__(self, config: Step3VLConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 815: get_image_feature(self, items: List[MultimodalDataItem])
         ‚Üí torch.Tensor

  L 858: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 863: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor

  L 884: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 997: get_model_config_for_expert_location(cls, config: Step3VLConfig)


CLASS: Step3VisionAttention
----------------------------------------
  L 572: __init__(self, dim: int, num_heads: int, qkv_backend, quant_config, prefix: str)
         ‚Üí None

  L 605: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Step3VisionEmbeddings
----------------------------------------
  L 612: __init__(self, config: Step3VisionEncoderConfig)

  L 641: forward(self, pixel_values: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: Step3VisionEncoder
----------------------------------------
  L 716: __init__(self, config: Step3VisionEncoderConfig)

  L 723: forward(self, inputs_embeds)
         ‚Üí torch.Tensor


CLASS: Step3VisionEncoderLayer
----------------------------------------
  L 665: __init__(self, config, attn_implementation: str)
         ‚Üí None

  L 680: forward(self, hidden_states)
         ‚Üí torch.Tensor


CLASS: Step3VisionMLP
----------------------------------------
  L 529: __init__(self, dim: int, intermediate_size: int, bias: bool, hidden_act, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 564: forward(self, hidden_states)
         ‚Üí torch.Tensor


CLASS: Step3VisionTransformer
----------------------------------------
  L 687: __init__(self, config: Step3VisionEncoderConfig)

  L 695: dtype(self)
         ‚Üí torch.dtype

  L 698: forward(self, pixel_values: torch.Tensor)


============================================================
FILE: python/sglang/srt/models/torch_native_llama.py
Functions: 16
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  73: def gate_up_proj_weight_loader(self,
        param: Parameter,
        loaded_weight: torch.Tensor,
        loaded_shard_id: int)

  L 142: def qkv_proj_weight_loader(self,
        param: Parameter,
        loaded_weight: torch.Tensor,
        loaded_shard_id: str)


CLASS: LlamaAttention
----------------------------------------
  L 180: __init__(self, config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 253: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: LlamaDecoderLayer
----------------------------------------
  L 268: __init__(self, config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 312: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: LlamaMLP
----------------------------------------
  L 108: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 135: forward(self, x)


CLASS: LlamaModel
----------------------------------------
  L 338: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig])
         ‚Üí None

  L 361: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: TorchNativeLlamaForCausalLM
----------------------------------------
  L 386: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig])
         ‚Üí None

  L 407: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí LogitsProcessorOutput

  L 419: get_module_name_from_weight_name(self, name)

  L 436: get_num_params(self)

  L 440: load_weights_to_module(self, fqn: str, weights: Iterable[Tuple[str, torch.Tensor]])
         üìù Load weights onto submodule pointed by path `fqn`.

  L 488: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])
         üìù Load weights onto the full model.


============================================================
FILE: python/sglang/srt/models/transformers.py
Functions: 11
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  46: def maybe_prefix(prefix: str, name: str)
         ‚Üí str
         üìù Add a prefix to a name if the prefix is non-empty.

  L  59: def sglang_flash_attention_forward(module: torch.nn.Module,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        attention_mask: torch.Tensor,
        forward_batch: ForwardBatch,
        scaling: float,
        attention_instances: list[RadixAttention])

  L  97: def replace_linear_class(linear: nn.Linear,
        style: Literal['colwise',
        'rowwise'],
        quant_config: QuantizationConfig)
         ‚Üí Union[ColumnParallelLinear, RowParallelLinear]
         üìù Replace nn.Linear with one of vLLM's tensor parallel linear classes.


CLASS: HFColumnParallelLinear
----------------------------------------
  L  87: forward(self, input: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: HFRowParallelLinear
----------------------------------------
  L  93: forward(self, input: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: TransformersForCausalLM
----------------------------------------
  L 143: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 206: log_replacement(self, name: str, old_module: nn.Module, new_module: nn.Module)

  L 209: tensor_parallel(self, tp_size: int)
         üìù Apply the model's tensor parallelization plan.

  L 238: replace_vocab_embed_class(self, module: nn.Module)

  L 252: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         ‚Üí LogitsProcessorOutput

  L 277: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/vila.py
Functions: 12
============================================================


CLASS: DownSample3x3BlockFix
----------------------------------------
  L  94: forward(self, x: Tensor)
         ‚Üí Tensor
         üìù Args:


CLASS: MultimodalProjector
----------------------------------------
  L 130: __init__(self, config: VILAConfig)

  L 160: device(self)
         ‚Üí torch.device

  L 164: dtype(self)
         ‚Üí torch.dtype

  L 167: forward(self, x: Tensor)
         ‚Üí Tensor
         üìù Args:


CLASS: VILAConfig
----------------------------------------
  L  56: __init__(self, text_config: Optional[Dict[str, Any]], vision_config: Optional[Dict[str, Any]])


CLASS: VILAForConditionalGeneration
----------------------------------------
  L 193: __init__(self, config: VILAConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 216: dtype(self)
         ‚Üí torch.dtype

  L 219: forward(self, input_ids: Tensor, positions: Tensor, forward_batch: ForwardBatch, get_embedding: bool)
         ‚Üí LogitsProcessorOutput

  L 239: get_image_feature(self, mm_input: List[MultimodalDataItem])
         ‚Üí Tensor

  L 265: load_weights(self, weights: Iterable[Tuple[str, Tensor]])
         ‚Üí None

  L 278: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)
         ‚Üí List[int]


============================================================
FILE: python/sglang/srt/models/xverse.py
Functions: 11
============================================================


CLASS: XverseAttention
----------------------------------------
  L  85: __init__(self, config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 160: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: XverseDecoderLayer
----------------------------------------
  L 175: __init__(self, config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 222: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: XverseForCausalLM
----------------------------------------
  L 302: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 320: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor

  L 332: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], name, loaded_weight)


CLASS: XverseMLP
----------------------------------------
  L  47: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  77: forward(self, x)


CLASS: XverseModel
----------------------------------------
  L 248: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 276: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/models/xverse_moe.py
Functions: 14
============================================================


CLASS: XverseAttention
----------------------------------------
  L 195: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 265: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: XverseDecoderLayer
----------------------------------------
  L 281: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 326: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         ‚Üí torch.Tensor


CLASS: XverseMLP
----------------------------------------
  L  53: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str)
         ‚Üí None

  L  85: forward(self, x)


CLASS: XverseMoE
----------------------------------------
  L  94: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 151: pack_params(self)

  L 170: forward(self, hidden_states: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: XverseModel
----------------------------------------
  L 355: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 383: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor


CLASS: XverseMoeForCausalLM
----------------------------------------
  L 402: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L 423: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor

  L 434: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/yivl.py
Functions: 4
============================================================


CLASS: YiVLForCausalLM
----------------------------------------
  L  28: __init__(self, config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         ‚Üí None

  L  41: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: YiVLMultiModalProjector
----------------------------------------
  L  93: __init__(self, config: LlavaConfig)

  L 106: forward(self, image_features)


============================================================
FILE: python/sglang/srt/multimodal/mm_utils.py
Functions: 11
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  42: def has_valid_data(data)
         ‚Üí bool

  L  50: def select_best_resolution(original_size, possible_resolutions)
         üìù Selects the best resolution from a list of possible resolutions based 

  L  90: def resize_and_pad_image(image, target_resolution)
         üìù Resize and pad an image to a target resolution while maintaining aspec

  L 125: def divide_to_patches(image, patch_size)
         üìù Divides an image into patches of a specified size.

  L 147: def get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size)
         üìù Calculate the shape of the image patch grid after the preprocessing fo

  L 187: def process_anyres_image(image, processor, grid_pinpoints)
         üìù Process an image with variable resolutions.

  L 254: def load_image_from_base64(image)

  L 258: def expand2square(pil_img, background_color)

  L 274: def unpad_image(tensor, original_size)
         üìù Unpads a PyTorch tensor of a padded and resized image.

  L 305: def unpad_image_shape(current_height, current_width, original_size)
         üìù Unpads a PyTorch tensor of a padded and resized image

  L 329: def process_images(images, image_processor, model_cfg)


============================================================
FILE: python/sglang/srt/multimodal/processors/base_processor.py
Functions: 18
============================================================


CLASS: BaseMultiModalProcessorOutput
----------------------------------------
  L  39: organize_results(self)
         ‚Üí List[Tuple[Modality, Any]]
         üìù :return: a list of results, with their corresponding modalities


CLASS: BaseMultimodalProcessor
----------------------------------------
  L 151: __init__(self, hf_config, server_args, _processor, transport_mode)

  L 209: process_mm_data(self, input_text, images, videos, audios)
         ‚Üí dict
         üìù process multimodal data with transformers AutoProcessor

  L 252: process_mm_data_async(self, image_data, audio_data, input_text, request_obj)
         ‚Üí Optional[Dict[str, Any]]

  L 262: get_estimated_frames_list(self, image_data)
         üìù estimate the total frame count from all visual input

  L 314: submit_data_loading_tasks(self, text_parts: List[str], multimodal_tokens: MultimodalSpecialTokens, data_iterators: dict[Modality, Iterator[Any]], discard_alpha_channel: bool, image_estimated_frames_iter: Optional[iter], image_scaling_factor: float, max_image_frames: int, audio_sample_rate: Optional[int])
         ‚Üí Tuple[List, List]
         üìù load multimodal data parallelly using iterators.

  L 385: load_mm_data(self, prompt: str, multimodal_tokens: MultimodalSpecialTokens, image_data: Optional[list], video_data: Optional[list], audio_data: Optional[list], return_text: Optional[bool], discard_alpha_channel: bool, audio_sample_rate: Optional[int])
         ‚Üí BaseMultiModalProcessorOutput
         üìù Each frame of video/image will be replaced by a single image token

  L 498: get_mm_items_offset(input_ids: torch.Tensor, mm_token_id: int)
         ‚Üí List[Tuple[int, int]]
         üìù Get a set of range for mm_items from input_ids

  L 515: get_mm_items_offset_by_pair(input_ids: torch.Tensor, mm_start_id: int, mm_end_id: int)
         ‚Üí List[Tuple[int, int]]

  L 523: collect_mm_items_from_processor_output(self, data_dict: dict)
         ‚Üí List[MultimodalDataItem]
         üìù Create mm_items directly from processor output.

  L 574: process_and_combine_mm_data(self, base_output: BaseMultiModalProcessorOutput, mm_tokens: MultimodalSpecialTokens)
         ‚Üí Tuple[List[MultimodalDataItem], torch.Tensor, dict]
         üìù Process multimodal data and return the combined multimodal items and i


CLASS: MultimodalSpecialTokens
----------------------------------------
  L  67: build(self, processor)

  L  73: convert_to_str(self, token: Union[str, int], processor)
         ‚Üí str

  L  80: convert_to_strs(self, processor)

  L  88: get_modality_of_token(self, token: str)
         ‚Üí Optional[Modality]
         üìù :return: the modality associated with the given token, if the token is

  L 110: get_token_id_by_modality(self, modality: Modality)
         ‚Üí Optional[int]

  L 118: parse_regex(self)

  L 126: get_combined_regex(self)
         ‚Üí re.Pattern
         üìù Builds and returns a regex, used to split input str into tokens (with 


============================================================
FILE: python/sglang/srt/multimodal/processors/clip.py
Functions: 2
============================================================


CLASS: ClipImageProcessor
----------------------------------------
  L  13: __init__(self, hf_config, server_args, _processor)

  L  19: process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text)


============================================================
FILE: python/sglang/srt/multimodal/processors/deepseek_vl_v2.py
Functions: 2
============================================================


CLASS: DeepseekVL2ImageProcessor
----------------------------------------
  L  34: __init__(self, hf_config, server_args, _processor)

  L  40: process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text, request_obj, max_req_input_len)


============================================================
FILE: python/sglang/srt/multimodal/processors/gemma3.py
Functions: 2
============================================================


CLASS: Gemma3SGLangImageProcessor
----------------------------------------
  L  17: __init__(self, hf_config, server_args, _processor)

  L  31: process_mm_data_async(self, image_data: List[Union[str, bytes, Dict]], input_text, request_obj)


============================================================
FILE: python/sglang/srt/multimodal/processors/gemma3n.py
Functions: 2
============================================================


CLASS: Gemma3nSGLangProcessor
----------------------------------------
  L  29: __init__(self, hf_config, server_args, _processor)

  L  44: process_mm_data_async(self, image_data: Optional[List[Union[str, bytes, Dict]]], audio_data: Optional[List[Union[str, bytes, Dict]]], input_text: str, request_obj)
         üìù Process multimodal data including images and audio.


============================================================
FILE: python/sglang/srt/multimodal/processors/glm4v.py
Functions: 3
============================================================


CLASS: Glm4vImageProcessor
----------------------------------------
  L  22: __init__(self, hf_config, server_args, _processor)

  L  55: preprocess_video(self, vr: VideoReader)
         üìù Preprocess video using VideoReader from Decord backend.

  L  83: process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text, request_obj)


============================================================
FILE: python/sglang/srt/multimodal/processors/internvl.py
Functions: 6
============================================================


CLASS: InternVLImageProcessor
----------------------------------------
  L  20: __init__(self, hf_config, server_args, _image_processor)

  L  52: build_transform(input_size)

  L  81: dynamic_preprocess(image, min_num, max_num, image_size, use_thumbnail)

  L 145: get_index(bound, fps, max_frame, first_idx, num_segments)

  L 162: load_video(video_path, bound, input_size, max_num, num_segments)

  L 184: process_mm_data_async(self, image_data, input_text, request_obj)


============================================================
FILE: python/sglang/srt/multimodal/processors/janus_pro.py
Functions: 2
============================================================


CLASS: JanusProImageProcessor
----------------------------------------
  L  14: __init__(self, hf_config, server_args, _processor)

  L  22: process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text, request_obj)


============================================================
FILE: python/sglang/srt/multimodal/processors/kimi_vl.py
Functions: 2
============================================================


CLASS: KimiVLImageProcessor
----------------------------------------
  L  15: __init__(self, hf_config, server_args, _processor)

  L  24: process_mm_data_async(self, image_data: List[Union[str, bytes, Dict]], input_text, request_obj)


============================================================
FILE: python/sglang/srt/multimodal/processors/llava.py
Functions: 4
============================================================


CLASS: LlavaImageProcessor
----------------------------------------
  L  33: __init__(self, hf_config, server_args, _processor)

  L 109: process_mm_data_async(self, image_data: List[Union[str, bytes, ImageData]], input_text, request_obj)


CLASS: LlavaMultimodalProcessor
----------------------------------------
  L 194: __init__(self, hf_config, server_args, _processor)

  L 210: process_mm_data_async(self)


============================================================
FILE: python/sglang/srt/multimodal/processors/minicpm.py
Functions: 2
============================================================


CLASS: MiniCPMMultimodalProcessor
----------------------------------------
  L  18: __init__(self, hf_config, server_args, _processor)

  L  36: process_mm_data_async(self, image_data: List[Union[str, bytes]], audio_data: List[Union[str, bytes]], input_text, request_obj)


============================================================
FILE: python/sglang/srt/multimodal/processors/mlama.py
Functions: 2
============================================================


CLASS: MllamaImageProcessor
----------------------------------------
  L  13: __init__(self, hf_config, server_args, _processor)

  L  20: process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text)


============================================================
FILE: python/sglang/srt/multimodal/processors/mllama4.py
Functions: 2
============================================================


CLASS: Mllama4ImageProcessor
----------------------------------------
  L  21: __init__(self, hf_config, server_args, _processor)

  L  33: process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text)


============================================================
FILE: python/sglang/srt/multimodal/processors/phi4mm.py
Functions: 4
============================================================


CLASS: Phi4MMMultimodalProcessor
----------------------------------------
  L  50: __init__(self, hf_config, server_args, _processor)

  L  69: process_mm_data_async(self, image_data: List[Union[str, bytes]], audio_data, input_text, request_obj)


CLASS: Phi4MMProcessorAdapter
----------------------------------------
  L  19: __init__(self, _processor)
         ‚Üí None

  L  22: __call__(self)


============================================================
FILE: python/sglang/srt/multimodal/processors/pixtral.py
Functions: 3
============================================================


CLASS: PixtralProcessor
----------------------------------------
  L  23: get_patch_grid_size(self)
         ‚Üí tuple[int, int]

  L  45: __init__(self, hf_config, server_args, _processor)

  L  73: process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text, request_obj)


============================================================
FILE: python/sglang/srt/multimodal/processors/qwen_audio.py
Functions: 2
============================================================


CLASS: Qwen2AudioMultimodalProcessor
----------------------------------------
  L  14: __init__(self, hf_config, server_args, _processor)

  L  34: process_mm_data_async(self, audio_data, input_text)


============================================================
FILE: python/sglang/srt/multimodal/processors/qwen_vl.py
Functions: 10
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  37: def smart_resize(height: int,
        width: int,
        factor: int,
        min_pixels: int,
        max_pixels: int)
         ‚Üí tuple[int, int]
         üìù Rescales the image so that the following conditions are met:

  L  70: def resize_image(image, size_factor: int)
         ‚Üí Image.Image

  L  85: def round_by_factor(number: int, factor: int)
         ‚Üí int
         üìù Returns the closest integer to 'number' that is divisible by 'factor'.

  L  90: def ceil_by_factor(number: int, factor: int)
         ‚Üí int
         üìù Returns the smallest integer greater than or equal to 'number' that is

  L  95: def floor_by_factor(number: int, factor: int)
         ‚Üí int
         üìù Returns the largest integer less than or equal to 'number' that is div

  L 100: async def resize_image_async(image)

  L 104: def smart_nframes(ele: dict, total_frames: int, video_fps: int | float)
         ‚Üí int
         üìù calculate the number of frames for video used for model inputs.

  L 153: async def preprocess_video(vr, image_factor: int)
         ‚Üí torch.Tensor


CLASS: Qwen2_5VLImageProcessor
----------------------------------------
  L 204: __init__(self, hf_config, server_args, _processor)

  L 225: process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text, request_obj)


============================================================
FILE: python/sglang/srt/multimodal/processors/step3_vl.py
Functions: 21
============================================================


CLASS: GPUToTensor
----------------------------------------
  L  24: forward(self, raw_image: Union[np.ndarray, Image.Image])
         ‚Üí torch.Tensor


CLASS: ImagePatcher
----------------------------------------
  L  91: determine_window_size(self, long: int, short: int)
         ‚Üí int

  L  96: slide_window(self, width: int, height: int, sizes: list[tuple[int, int]], steps: list[tuple[int, int]], img_rate_thr: float)
         ‚Üí tuple[list[tuple[int, int, int, int]], tuple[int, int]]

  L 131: square_pad(self, img: Image.Image)
         ‚Üí Image.Image

  L 140: get_image_size_for_padding(self, img_width: int, img_height: int)
         ‚Üí tuple[int, int]

  L 149: get_image_size_for_preprocess(self, img_width: int, img_height: int)
         ‚Üí tuple[int, int]

  L 161: get_image_size_for_crop(self, img_width: int, img_height: int, window_size: int)

  L 181: patch_crop(self, img: Image.Image, i: int, j: int, th: int, tw: int)

  L 185: get_num_patches(self, img_width: int, img_height: int)
         ‚Üí tuple[int, int]

  L 210: __call__(self, img: Image.Image)
         ‚Üí tuple[Image.Image, list[Image.Image], list[bool] | None]


CLASS: Step3VLImageProcessor
----------------------------------------
  L 474: __init__(self, hf_config, server_args, _processor)

  L 488: preprocess(self, image)

  L 491: __call__(self, image)

  L 494: process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text: str | List[int], request_obj)


CLASS: Step3VLProcessor
----------------------------------------
  L 271: __init__(self, config, tokenizer)
         ‚Üí None

  L 298: image_token_id(self)
         ‚Üí int

  L 301: get_num_image_tokens(self, img_width: int, img_height: int)
         ‚Üí int

  L 377: replace_placeholder(self, text: str, placeholder: str, repls: list[str])
         ‚Üí str

  L 392: __call__(self, text: Optional[Union[str, list[str]]], images: Optional[Union[Image.Image, list[Image.Image]]], return_tensors: Optional[Union[str, TensorType]])
         ‚Üí BatchFeature


CLASS: Step3VisionProcessor
----------------------------------------
  L  41: __init__(self, size, interpolation_mode, patch_size)

  L  82: __call__(self, image, is_patch)


============================================================
FILE: python/sglang/srt/multimodal/processors/vila.py
Functions: 2
============================================================


CLASS: VILAMultimodalProcessor
----------------------------------------
  L  32: __init__(self, hf_config: PretrainedConfig, server_args: ServerArgs, _processor: VILAProcessor)
         ‚Üí None

  L  47: process_mm_data_async(self, image_data: Optional[ImageDataInputItem | List[ImageDataInputItem]], input_text: str | List[int], request_obj: GenerateReqInput | EmbeddingReqInput)
         ‚Üí Optional[Dict[str, Any]]


============================================================
FILE: python/sglang/srt/offloader.py
Functions: 30
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  50: def get_offloader()

  L  55: def set_offloader(instance: BaseOffloader)

  L  60: def create_offloader_from_server_args(server_args: ServerArgs, dp_rank: int)


CLASS: BaseOffloader
----------------------------------------
  L  30: wrap_modules(self, all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor], whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator])

  L  38: post_init(self)


CLASS: OffloaderV1
----------------------------------------
  L  81: __init__(self, cpu_offload_max_bytes: int)

  L  85: wrap_modules(self, all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor], whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator])

  L  93: maybe_offload_to_cpu(self, module: torch.nn.Module)
         ‚Üí torch.nn.Module


CLASS: OffloaderV2
----------------------------------------
  L 150: __init__(self, group_size: int, num_in_group: int, prefetch_step: int, mode: str, dp_rank: int, dp_size: int)

  L 189: wrap_modules(self, all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor], whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator])

  L 229: post_init(self)


CLASS: _BaseParamOffloader
----------------------------------------
  L 322: create(mode: str)
         ‚Üí '_BaseParamOffloader'

  L 330: __init__(self, module, param_name)

  L 338: post_init(self)

  L 341: create_device_tensor(self)


CLASS: _CpuParamOffloader
----------------------------------------
  L 357: __init__(self, module, param_name)

  L 361: create_device_tensor(self)


CLASS: _MetaParamOffloader
----------------------------------------
  L 348: __init__(self, module, param_name)

  L 352: create_device_tensor(self)


CLASS: _ModuleOffloader
----------------------------------------
  L 267: __init__(self, mode: str, module: torch.nn.Module, alt_stream: torch.cuda.Stream, whitelist_param_names: List[str])

  L 296: post_init(self)

  L 300: start_onload(self)

  L 307: offload(self)

  L 311: wait_and_get_device_tensors(self)


CLASS: _ShardedGpuParamOffloader
----------------------------------------
  L 453: __init__(self, module, param_name)

  L 472: post_init(self)

  L 501: create_device_tensor(self)


CLASS: _ShmCpuParamOffloader
----------------------------------------
  L 366: __init__(self, module, param_name)

  L 389: post_init(self)

  L 397: create_device_tensor(self)


============================================================
FILE: python/sglang/srt/operations.py
Functions: 15
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  21: def execute_operations(inputs, operations)

  L  30: def execute_overlapped_operations(inputs_arr: Sequence,
        operations_arr: Sequence,
        delta_stages: Sequence[int])
         ‚Üí Sequence


CLASS: _StageExecutor
----------------------------------------
  L  76: __init__(self, debug_name: str, stages: List[Stage], inputs: dict)

  L  89: next(self)

  L 114: output(self)

  L 119: done(self)

  L 123: num_stages(self)


CLASS: _StateDict
----------------------------------------
  L 138: __init__(self)

  L 141: __setattr__(self, key, value)

  L 150: __getattr__(self, item)

  L 153: __delattr__(self, item)

  L 156: pop(self, item)

  L 159: update(self, values: Dict[str, Any])

  L 163: get(self, item)

  L 166: clear(self, expect_keys: Sequence[str])


============================================================
FILE: python/sglang/srt/operations_strategy.py
Functions: 2
============================================================


CLASS: OperationsStrategy
----------------------------------------
  L  19: concat(cls, items: List['OperationsStrategy'])
         ‚Üí 'OperationsStrategy'

  L  31: init_new_tbo(layers: torch.nn.ModuleList, forward_mode: ForwardMode)
         ‚Üí 'OperationsStrategy'


============================================================
FILE: python/sglang/srt/patch_torch.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  21: def monkey_patch_torch_reductions()
         üìù Monkey patching before Torch https://github.com/pytorch/pytorch/pull/1

  L  75: def monkey_patch_torch_compile()


============================================================
FILE: python/sglang/srt/poll_based_barrier.py
Functions: 3
============================================================


CLASS: PollBasedBarrier
----------------------------------------
  L   7: __init__(self, noop: bool)

  L  11: local_arrive(self)

  L  15: poll_global_arrived(self)
         ‚Üí bool


============================================================
FILE: python/sglang/srt/reasoning_parser.py
Functions: 13
============================================================


CLASS: BaseReasoningFormatDetector
----------------------------------------
  L  22: __init__(self, think_start_token: str, think_end_token: str, force_reasoning: bool, stream_reasoning: bool)

  L  37: detect_and_parse(self, text: str)
         ‚Üí StreamingParseResult
         üìù One-time parsing: Detects and parses reasoning sections in the provide

  L  63: parse_streaming_increment(self, new_text: str)
         ‚Üí StreamingParseResult
         üìù Streaming incremental parsing for reasoning content.


CLASS: DeepSeekR1Detector
----------------------------------------
  L 141: __init__(self, stream_reasoning: bool, force_reasoning: bool)


CLASS: GptOssDetector
----------------------------------------
  L 200: __init__(self, stream_reasoning: bool, force_reasoning: bool)

  L 209: detect_and_parse(self, text: str)
         ‚Üí StreamingParseResult

  L 232: parse_streaming_increment(self, new_text: str)
         ‚Üí StreamingParseResult


CLASS: KimiDetector
----------------------------------------
  L 186: __init__(self, stream_reasoning: bool, force_reasoning: bool)


CLASS: Qwen3Detector
----------------------------------------
  L 168: __init__(self, stream_reasoning: bool, force_reasoning: bool)


CLASS: ReasoningParser
----------------------------------------
  L 275: __init__(self, model_type: Optional[str], stream_reasoning: bool, force_reasoning: Optional[bool])

  L 299: parse_non_stream(self, full_text: str)
         ‚Üí Tuple[Optional[str], Optional[str]]
         üìù Non-streaming call: one-time parsing

  L 304: parse_stream_chunk(self, chunk_text: str)
         ‚Üí Tuple[Optional[str], Optional[str]]
         üìù Streaming call: incremental parsing


CLASS: StreamingParseResult
----------------------------------------
  L  10: __init__(self, normal_text: Optional[str], reasoning_text: Optional[str])


============================================================
FILE: python/sglang/srt/sampling/custom_logit_processor.py
Functions: 4
============================================================


CLASS: CustomLogitProcessor
----------------------------------------
  L  23: __call__(self, logits: torch.Tensor, custom_param_list: Optional[List[Dict[str, Any]]])
         ‚Üí torch.Tensor
         üìù Define the callable behavior.

  L  32: to_str(cls)
         ‚Üí str
         üìù Serialize the callable function to a JSON-compatible string.

  L  37: from_str(cls, json_str: str)
         üìù Deserialize a callable function from a JSON string.


CLASS: DisallowedTokensLogitsProcessor
----------------------------------------
  L  43: __call__(self, logits: torch.Tensor, custom_param_list: Optional[List[Dict[str, Any]]])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/sampling/penaltylib/frequency_penalty.py
Functions: 1
============================================================


CLASS: BatchedFrequencyPenalizer
----------------------------------------
  L  14: __init__(self, orchestrator: BatchedPenalizerOrchestrator)


============================================================
FILE: python/sglang/srt/sampling/penaltylib/min_new_tokens.py
Functions: 1
============================================================


CLASS: BatchedMinNewTokensPenalizer
----------------------------------------
  L  14: __init__(self, orchestrator: BatchedPenalizerOrchestrator)


============================================================
FILE: python/sglang/srt/sampling/penaltylib/orchestrator.py
Functions: 17
============================================================


CLASS: BatchedPenalizerOrchestrator
----------------------------------------
  L  14: __init__(self, vocab_size: int, batch: ScheduleBatch, penalizers: Set[Type['_BatchedPenalizer']])

  L  32: batch(self)
         ‚Üí ScheduleBatch | None

  L  36: batch(self, value: Optional[ScheduleBatch])

  L  42: reqs(self)

  L  45: cumulate_output_tokens(self, output_ids: torch.Tensor)
         üìù Feed the output tokens to the penalizers.

  L  55: apply(self, logits: torch.Tensor)
         ‚Üí torch.Tensor
         üìù Apply the penalizers to the logits.

  L  69: filter(self, keep_indices: torch.Tensor)
         üìù Filter the penalizers based on the indices to keep in the batch.

  L  95: merge(self, their: 'BatchedPenalizerOrchestrator')
         üìù Merge the penalizers of another orchestrator into this one.


CLASS: _BatchedPenalizer
----------------------------------------
  L 119: is_prepared(self)
         ‚Üí bool

  L 122: is_required(self)
         ‚Üí bool

  L 125: prepare(self)

  L 130: prepare_if_required(self)

  L 137: teardown(self)

  L 140: cumulate_output_tokens(self, output_ids: torch.Tensor)

  L 146: apply(self, logits: torch.Tensor)
         ‚Üí torch.Tensor

  L 152: filter(self, keep_indices: torch.Tensor)

  L 158: merge(self, their: '_BatchedPenalizer')


============================================================
FILE: python/sglang/srt/sampling/penaltylib/presence_penalty.py
Functions: 1
============================================================


CLASS: BatchedPresencePenalizer
----------------------------------------
  L  14: __init__(self, orchestrator: BatchedPenalizerOrchestrator)


============================================================
FILE: python/sglang/srt/sampling/sampling_batch_info.py
Functions: 9
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 353: def merge_bias_tensor(lhs: Optional[torch.Tensor],
        rhs: Optional[torch.Tensor],
        bs1: int,
        bs2: int,
        device: str,
        default: float)
         üìù Merge two bias tensors for batch merging.


CLASS: SamplingBatchInfo
----------------------------------------
  L  70: from_schedule_batch(cls, batch: ScheduleBatch, vocab_size: int)

  L 171: __len__(self)

  L 174: update_regex_vocab_mask(self)

  L 201: update_penalties(self)

  L 212: apply_logits_bias(self, logits: torch.Tensor)

  L 227: filter_batch(self, keep_indices: List[int], keep_indices_device: torch.Tensor)

  L 266: merge_custom_logit_processor(lhs: Optional[Dict[int, Tuple[CustomLogitProcessor, torch.Tensor]]], rhs: Optional[Dict[int, Tuple[CustomLogitProcessor, torch.Tensor]]], bs1: int, bs2: int, device: str)

  L 305: merge_batch(self, other: 'SamplingBatchInfo')


============================================================
FILE: python/sglang/srt/sampling/sampling_params.py
Functions: 3
============================================================


CLASS: SamplingParams
----------------------------------------
  L  31: __init__(self, max_new_tokens: int, stop: Optional[Union[str, List[str]]], stop_token_ids: Optional[List[int]], temperature: float, top_p: float, top_k: int, min_p: float, frequency_penalty: float, presence_penalty: float, repetition_penalty: float, min_new_tokens: int, n: int, json_schema: Optional[str], regex: Optional[str], ebnf: Optional[str], structural_tag: Optional[str], ignore_eos: bool, skip_special_tokens: bool, spaces_between_special_tokens: bool, no_stop_trim: bool, custom_params: Optional[Dict[str, Any]], stream_interval: Optional[int], logit_bias: Optional[Dict[str, float]])
         ‚Üí None

  L  92: verify(self, vocab_size)

  L 149: normalize(self, tokenizer)


============================================================
FILE: python/sglang/srt/server_args.py
Functions: 17
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L2341: def prepare_server_args(argv: List[str])
         ‚Üí ServerArgs
         üìù Prepare the server arguments from the command line arguments.

  L2469: def print_deprecated_warning(message: str)

  L2473: def auto_choose_speculative_params(self: ServerArgs)
         üìù Automatically choose the parameters for speculative decoding.


CLASS: DeprecatedAction
----------------------------------------
  L2460: __init__(self, option_strings, dest, nargs)

  L2465: __call__(self, parser, namespace, values, option_string)


CLASS: LoRAPathAction
----------------------------------------
  L2440: __call__(self, parser, namespace, values, option_string)


CLASS: PortArgs
----------------------------------------
  L2381: init_new(server_args, dp_rank: Optional[int])
         ‚Üí 'PortArgs'


CLASS: ServerArgs
----------------------------------------
  L 310: __post_init__(self)

  L 731: add_cli_args(parser: argparse.ArgumentParser)

  L2071: from_cli_args(cls, args: argparse.Namespace)

  L2079: url(self)

  L2085: get_hf_config(self)

  L2096: check_server_args(self)

  L2137: check_lora_server_args(self)

  L2220: validate_disagg_tp_size(self, prefill_tp: int, decode_tp: int)

  L2228: model_specific_adjustments(self)

  L2302: adjust_mem_fraction_for_vlm(self, model_config)


============================================================
FILE: python/sglang/srt/speculative/build_eagle_tree.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  17: def build_tree_kernel_efficient_preprocess(verified_id: torch.Tensor,
        score_list: List[torch.Tensor],
        token_list: List[torch.Tensor],
        parents_list: List[torch.Tensor],
        num_verify_tokens: int)

  L  51: def build_tree_kernel_efficient(verified_id: torch.Tensor,
        score_list: List[torch.Tensor],
        token_list: List[torch.Tensor],
        parents_list: List[torch.Tensor],
        seq_lens: torch.Tensor,
        seq_lens_sum: int,
        topk: int,
        spec_steps: int,
        num_verify_tokens: int,
        tree_mask_mode: TreeMaskMode,
        tree_mask_buf: Optional[torch.Tensor],
        position_buf: Optional[torch.Tensor])

  L 154: def test_build_tree_kernel_efficient()


============================================================
FILE: python/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py
Functions: 5
============================================================


CLASS: EAGLEDraftCudaGraphRunner
----------------------------------------
  L  40: __init__(self, eagle_worker: EAGLEWorker)

  L 128: can_run(self, forward_batch: ForwardBatch)

  L 149: capture(self)

  L 152: capture_one_batch_size(self, num_seqs: int, forward: Callable)

  L 280: replay(self, forward_batch: ForwardBatch)


============================================================
FILE: python/sglang/srt/speculative/eagle_draft_extend_cuda_graph_runner.py
Functions: 5
============================================================


CLASS: EAGLEDraftExtendCudaGraphRunner
----------------------------------------
  L  37: __init__(self, eagle_worker: EAGLEWorker)

  L 155: can_run(self, forward_batch: ForwardBatch)

  L 176: capture(self)

  L 179: capture_one_batch_size(self, bs: int, forward: Callable)

  L 308: replay(self, forward_batch: ForwardBatch)


============================================================
FILE: python/sglang/srt/speculative/eagle_utils.py
Functions: 22
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 730: def create_extend_after_decode_spec_info(verified_id,
        seq_lens,
        accept_lens,
        positions,
        new_verified_id,
        bs_upper: tl.constexpr)
         @triton.jit

  L 756: def assign_req_to_token_pool(req_pool_indices,
        req_to_token,
        start_offset,
        end_offset,
        out_cache_loc,
        pool_len: tl.constexpr,
        bs_upper: tl.constexpr)
         @triton.jit

  L 791: def assign_draft_cache_locs(req_pool_indices,
        req_to_token,
        seq_lens,
        extend_lens,
        num_new_pages_per_topk,
        out_cache_loc,
        pool_len: tl.constexpr,
        topk: tl.constexpr,
        speculative_num_steps: tl.constexpr,
        page_size: tl.constexpr,
        bs_upper: tl.constexpr,
        iter_upper: tl.constexpr)
         @triton.jit

  L 867: def generate_draft_decode_kv_indices(req_pool_indices,
        req_to_token,
        paged_kernel_lens,
        kv_indices,
        kv_indptr,
        positions,
        pool_len: tl.constexpr,
        kv_indices_stride: tl.constexpr,
        kv_indptr_stride: tl.constexpr,
        bs_upper: tl.constexpr,
        iter_upper: tl.constexpr,
        num_tokens_upper: tl.constexpr,
        page_size: tl.constexpr)
         @triton.jit

  L 948: def align_evict_mask_to_page_size(seq_lens,
        evict_mask,
        page_size: tl.constexpr,
        num_draft_tokens: tl.constexpr,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 973: def get_target_cache_loc(tgt_cache_loc,
        to_free_slots,
        accept_length,
        to_free_num_slots,
        out_cache_loc,
        num_verify_tokens: tl.constexpr,
        num_verify_tokens_upper: tl.constexpr,
        bs_upper: tl.constexpr)
         @triton.jit

  L1019: def get_src_tgt_cache_loc(seq_lens: torch.Tensor,
        out_cache_loc: torch.Tensor,
        accept_index: torch.Tensor,
        accept_length: torch.Tensor,
        draft_token_num: int,
        page_size: int)
         @torch.compile(dynamic=True)

  L1039: def filter_finished_cache_loc_kernel(out_cache_loc,
        tgt_cache_loc,
        accept_length,
        accept_length_filter,
        bs_upper: tl.constexpr,
        num_verify_tokens_upper: tl.constexpr)
         @triton.jit

  L1069: def create_accept_length_filter(accept_length: torch.Tensor,
        unfinished_index_device: torch.Tensor,
        seq_lens: torch.Tensor)
         @torch.compile(dynamic=True)

  L1083: def select_top_k_tokens(i: int,
        topk_p: torch.Tensor,
        topk_index: torch.Tensor,
        hidden_states: torch.Tensor,
        scores: torch.Tensor,
        topk: int)
         @torch.compile(dynamic=True)

  L1183: def traverse_tree(retrieve_next_token: torch.Tensor,
        retrieve_next_sibling: torch.Tensor,
        draft_tokens: torch.Tensor,
        grammar: BaseGrammarObject,
        allocate_token_bitmask: torch.Tensor)
         üìù Traverse the tree constructed by the draft model to generate the logit

  L1249: def generate_token_bitmask(reqs: List[Req],
        verify_input: EagleVerifyInput,
        retrieve_next_token_cpu: torch.Tensor,
        retrieve_next_sibling_cpu: torch.Tensor,
        draft_tokens_cpu: torch.Tensor,
        vocab_size: int)
         üìù Generate the logit mask for structured output.


CLASS: EagleDraftInput
----------------------------------------
  L  85: prepare_for_extend(self, batch: ScheduleBatch)

  L 102: create_idle_input(cls, device: torch.device, hidden_size: int, dtype: torch.dtype, topk: int, capture_hidden_mode: CaptureHiddenMode)

  L 120: prepare_extend_after_decode(self, batch: ScheduleBatch, speculative_num_steps: int)

  L 151: generate_attn_arg_prefill(self, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, req_to_token: torch.Tensor)

  L 182: filter_batch(self, new_indices: torch.Tensor, has_been_filtered: bool)

  L 201: merge_batch(self, spec_info: EagleDraftInput)


CLASS: EagleVerifyInput
----------------------------------------
  L 250: create_idle_input(cls, topk: int, spec_steps: int, num_verify_tokens: int)

  L 273: prepare_for_verify(self, batch: ScheduleBatch, page_size: int)

  L 307: generate_attn_arg_prefill(self, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, req_to_token: torch.Tensor)

  L 345: verify(self, batch: ScheduleBatch, logits_output: LogitsProcessorOutput, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, page_size: int, vocab_mask: Optional[torch.Tensor])
         ‚Üí torch.Tensor
         üìù Verify and find accepted tokens based on logits output and batch


============================================================
FILE: python/sglang/srt/speculative/eagle_worker.py
Functions: 18
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  60: def draft_tp_context(tp_group: GroupCoordinator)
         @contextmanager

  L 999: def load_token_map(token_map_path: str)
         ‚Üí List[int]

  L1011: def get_last_loc_large_page_size_top_k_1(req_to_token: torch.Tensor,
        req_pool_indices: torch.Tensor,
        seq_lens,
        speculative_num_steps: int)
         @torch.compile(dynamic=True)

  L1030: def get_last_loc_large_page_size_large_top_k(req_to_token: torch.Tensor,
        req_pool_indices: torch.Tensor,
        seq_lens: torch.Tensor,
        speculative_num_steps: int,
        topk: int,
        page_size: int)


CLASS: EAGLEWorker
----------------------------------------
  L  69: __init__(self, server_args: ServerArgs, gpu_id: int, tp_rank: int, dp_rank: Optional[int], moe_ep_rank: int, nccl_port: int, target_worker: TpModelWorker)

  L 182: init_attention_backend(self)

  L 320: init_cuda_graphs(self)
         üìù Capture cuda graphs.

  L 356: draft_model_runner(self)

  L 359: forward_batch_speculative_generation(self, batch: ScheduleBatch)
         ‚Üí Tuple[LogitsProcessorOutput, torch.Tensor, int, int, bool]
         üìù Run speculative decoding forward.

  L 407: check_forward_draft_extend_after_decode(self, batch: ScheduleBatch)

  L 425: forward_target_extend(self, batch: ScheduleBatch)
         ‚Üí Tuple[LogitsProcessorOutput, torch.Tensor, int, Optional[torch.Tensor]]
         üìù Run the target extend.

  L 565: draft(self, batch: ScheduleBatch)

  L 643: draft_forward(self, forward_batch: ForwardBatch)

  L 702: verify(self, batch: ScheduleBatch, spec_info: EagleVerifyInput)

  L 779: add_logprob_values(self, batch: ScheduleBatch, res: EagleVerifyOutput, logits_output: LogitsProcessorOutput)

  L 847: forward_draft_extend(self, batch: ScheduleBatch, hidden_states: torch.Tensor, next_token_ids: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor])
         üìù Run draft model extend. This API modifies the states of the batch.

  L 898: forward_draft_extend_after_decode(self, batch: ScheduleBatch)

  L 984: capture_for_decode(self, logits_output: LogitsProcessorOutput, draft_input: EagleDraftInput)


============================================================
FILE: python/sglang/srt/speculative/spec_info.py
Functions: 4
============================================================


CLASS: SpeculativeAlgorithm
----------------------------------------
  L   9: is_none(self)

  L  12: is_eagle(self)

  L  15: is_eagle3(self)

  L  19: from_string(name: str)


============================================================
FILE: python/sglang/srt/tokenizer/tiktoken_tokenizer.py
Functions: 9
============================================================


CLASS: TiktokenProcessor
----------------------------------------
  L   7: __init__(self, name: str)

  L  10: image_processor(self, image)


CLASS: TiktokenTokenizer
----------------------------------------
  L  30: __init__(self, tokenizer_path)

  L 110: encode(self, x, add_special_tokens)

  L 113: decode(self, x)

  L 116: batch_decode(self, batch, skip_special_tokens, spaces_between_special_tokens)

  L 123: apply_chat_template(self, messages, tokenize, add_generation_prompt, tools, reasoning_effort)

  L 136: __call__(self, text)

  L 141: init_xgrammar(self)


============================================================
FILE: python/sglang/srt/torch_memory_saver_adapter.py
Functions: 17
============================================================


CLASS: TorchMemorySaverAdapter
----------------------------------------
  L  21: create(enable: bool)

  L  33: check_validity(self, caller_name)

  L  40: configure_subprocess(self)

  L  43: region(self, tag: str)

  L  46: pause(self, tag: str)

  L  49: resume(self, tag: str)

  L  53: enabled(self)


CLASS: _TorchMemorySaverAdapterNoop
----------------------------------------
  L  79: configure_subprocess(self)

  L  83: region(self, tag: str)

  L  86: pause(self, tag: str)

  L  89: resume(self, tag: str)

  L  93: enabled(self)


CLASS: _TorchMemorySaverAdapterReal
----------------------------------------
  L  60: configure_subprocess(self)

  L  63: region(self, tag: str)

  L  66: pause(self, tag: str)

  L  69: resume(self, tag: str)

  L  73: enabled(self)


============================================================
FILE: python/sglang/srt/two_batch_overlap.py
Functions: 24
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  49: def get_token_num_per_seq(forward_mode: ForwardMode,
        spec_info: Optional[Union[EagleDraftInput,
        EagleVerifyInput]])

  L  65: def compute_split_seq_index(forward_mode: 'ForwardMode',
        num_tokens: int,
        extend_lens: Optional[Sequence[int]],
        token_num_per_seq: Optional[int])
         ‚Üí Optional[int]

  L 180: def split_spec_info(spec_info: Optional[EagleVerifyInput],
        start_seq_index: int,
        end_seq_index: int,
        start_token_index: int,
        end_token_index: int)

  L 252: def compute_split_token_index(split_seq_index: int,
        forward_mode: 'ForwardMode',
        extend_seq_lens: Optional[Sequence[int]],
        token_num_per_seq: Optional[int])
         ‚Üí int

  L 273: def compute_split_indices_for_cuda_graph_replay(forward_mode: ForwardMode,
        cuda_graph_num_tokens: int,
        spec_info: Optional[Union[EagleDraftInput,
        EagleVerifyInput]])

  L 785: def model_forward_maybe_tbo(layers,
        enable_tbo: bool,
        positions: torch.Tensor,
        forward_batch: ForwardBatch,
        hidden_states: torch.Tensor,
        input_data_scatter_mode: ScatterMode,
        residual: Optional[torch.Tensor],
        zero_allocator: Optional[BumpAllocator])


CLASS: MaybeTboDeepEPDispatcher
----------------------------------------
  L 964: __init__(self)

  L 973: dispatch(self)
         ‚Üí DispatchOutput

  L 976: dispatch_a(self)

  L 979: dispatch_b(self)

  L 982: combine(self)
         ‚Üí torch.Tensor

  L 985: combine_a(self)

  L 988: combine_b(self)


CLASS: TboCudaGraphRunnerPlugin
----------------------------------------
  L 303: __init__(self)

  L 306: capture_one_batch_size(self, batch: ForwardBatch, num_tokens: int)

  L 331: replay_prepare(self, forward_mode: ForwardMode, bs: int, num_token_non_padded: int, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])


CLASS: TboDPAttentionPreparer
----------------------------------------
  L 358: prepare_all_gather(self, local_batch: ScheduleBatch)

  L 404: compute_output(self, partial_global_info)


CLASS: TboForwardBatchPreparer
----------------------------------------
  L 452: prepare(cls, batch: ForwardBatch, is_draft_worker: bool)

  L 464: prepare_raw(cls, batch: ForwardBatch, tbo_children_num_token_non_padded: torch.Tensor)

  L 529: derive_fields_related_to_seq_len_for_two_chunk(cls, batch: ForwardBatch)

  L 591: filter_batch(cls, batch: ForwardBatch)

  L 740: compute_tbo_children_num_token_non_padded(cls, batch: ForwardBatch)

  L 747: compute_tbo_children_num_token_non_padded_raw(cls, tbo_split_token_index: int, num_token_non_padded: int)


============================================================
FILE: python/sglang/srt/utils.py
Functions: 171
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 103: def is_hip()
         ‚Üí bool

  L 118: def is_cuda()

  L 122: def is_cuda_alike()

  L 126: def is_hpu()
         ‚Üí bool

  L 130: def is_xpu()
         ‚Üí bool

  L 134: def is_npu()
         ‚Üí bool

  L 138: def is_host_cpu_x86()
         ‚Üí bool

  L 147: def is_cpu()
         ‚Üí bool

  L 151: def get_cuda_version()

  L 169: def is_blackwell()

  L 176: def is_sm100_supported(device)
         ‚Üí bool
         @lru_cache(maxsize=1)

  L 183: def is_sm90_supported(device)
         ‚Üí bool
         @lru_cache(maxsize=1)

  L 192: def get_bool_env_var(name: str, default: str)
         ‚Üí bool

  L 209: def get_int_env_var(name: str, default: int)
         ‚Üí int

  L 219: def support_triton(backend: str)
         ‚Üí bool

  L 233: def cpu_has_amx_support()

  L 237: def use_intel_amx_backend(layer)

  L 241: def is_flashinfer_available()
         üìù Check whether flashinfer is available.

  L 251: def random_uuid()
         ‚Üí str

  L 312: def enable_show_time_cost()

  L 339: def mark_start(name, interval, color, indent)

  L 349: def mark_end(name)

  L 359: def calculate_time(show, min_cost_ms)

  L 378: def get_available_gpu_memory(device,
        gpu_id,
        distributed,
        empty_cache,
        cpu_group)
         üìù Get available memory for cuda:gpu_id device.

  L 451: def is_pin_memory_available()
         ‚Üí bool

  L 460: def make_layers(num_hidden_layers: int,
        layer_fn: LayerFn,
        pp_rank: Optional[int],
        pp_size: Optional[int],
        prefix: str,
        return_tuple: bool,
        offloader_kwargs: Dict[str,
        Any])
         ‚Üí Tuple[int, int, torch.nn.ModuleList]
         üìù Make a list of layers with the given layer function

  L 504: def set_random_seed(seed: int)
         ‚Üí None
         üìù Set the random seed for all libraries.

  L 513: def find_process_using_port(port: int)
         ‚Üí Optional[psutil.Process]

  L 525: def wait_port_available(port: int,
        port_name: str,
        timeout_s: int,
        raise_exception: bool)
         ‚Üí bool

  L 553: def is_port_available(port)
         üìù Return whether a port is available.

  L 567: def get_free_port()

  L 580: def decode_video_base64(video_base64)

  L 659: def load_audio(audio_file: str, sr: Optional[int], mono: bool)
         ‚Üí np.ndarray

  L 707: def load_image(image_file: Union[Image.Image, str, ImageData, bytes])
         ‚Üí tuple[Image.Image, tuple[int, int]]

  L 741: def load_video(video_file: Union[str, bytes], use_gpu: bool)

  L 796: def suppress_other_loggers()

  L 816: def assert_pkg_version(pkg: str, min_version: str, message: str)

  L 831: def kill_process_tree(parent_pid, include_parent: bool, skip_pid: int)
         üìù Kill the process and all its child processes.

  L 870: def monkey_patch_p2p_access_check()
         üìù Monkey patch the slow p2p access check.

  L 888: def monkey_patch_vllm_gguf_config()

  L 914: def set_ulimit(target_soft_limit)

  L 938: def add_api_key_middleware(app, api_key: str)

  L 952: def prepare_model_and_tokenizer(model_path: str, tokenizer_path: str)

  L 964: def configure_logger(server_args, prefix: str)

  L 986: def replace_submodule(model: nn.Module, module_name: str, new_module: nn.Module)
         ‚Üí nn.Module
         üìù Replace a submodule in a model with a new module.

  L 996: def set_weight_attrs(weight: torch.Tensor,
        weight_attrs: Optional[Dict[str,
        Any]])
         üìù Set attributes on a weight tensor.

  L1016: def broadcast_pyobj(data: List[Any],
        rank: int,
        dist_group: Optional[torch.distributed.ProcessGroup],
        src: int,
        force_cpu_device: bool)
         üìù Broadcast inputs from src rank to all other ranks with torch.dist back

  L1063: def point_to_point_pyobj(data: List[Any],
        rank: int,
        group: Optional[torch.distributed.ProcessGroup],
        src: int,
        dst: int)
         üìù Send data from src to dst in group using DeviceToDevice communication.

  L1122: def pytorch_profile(name, func)
         üìù Args:

  L1150: def get_zmq_socket(context: zmq.Context,
        socket_type: zmq.SocketType,
        endpoint: str,
        bind: bool)

  L1191: def dump_to_file(dirpath, name, value)

  L1206: def is_triton_3()

  L1210: def maybe_torch_compile()
         üìù torch.compile does not work for triton 2.2.0, which is needed in xlm1'

  L1224: def delete_directory(dirpath)

  L1237: def set_prometheus_multiproc_dir()

  L1255: def add_prometheus_middleware(app)

  L1268: def bind_port(port)
         üìù Bind to a specific port, assuming it's available.

  L1277: def get_amdgpu_memory_capacity()

  L1310: def get_device_sm()

  L1317: def get_nvgpu_memory_capacity()

  L1356: def get_hpu_memory_capacity()

  L1387: def get_npu_memory_capacity()

  L1396: def get_device_memory_capacity(device: str)

  L1415: def init_custom_process_group(backend,
        init_method,
        timeout,
        world_size,
        rank,
        store,
        group_name,
        pg_options)

  L1484: def crash_on_warnings()

  L1489: def print_warning_once(msg: str)
         ‚Üí None

  L1495: def print_info_once(msg: str)
         ‚Üí None
         @functools.lru_cache(None)

  L1499: def get_device_name(device_id: int)
         ‚Üí str

  L1514: def is_habana_available()
         ‚Üí bool
         @lru_cache(maxsize=1)

  L1519: def get_device(device_id: Optional[int])
         ‚Üí str
         @lru_cache(maxsize=8)

  L1561: def get_device_count()
         ‚Üí int
         @lru_cache(maxsize=1)

  L1586: def get_device_core_count(device_id: int)
         ‚Üí int

  L1593: def get_device_capability(device_id: int)
         ‚Üí Tuple[int, int]

  L1618: def get_npu_compiler_config()

  L1627: def get_compiler_backend()
         ‚Üí str

  L1657: def supports_custom_op()
         ‚Üí bool

  L1661: def direct_register_custom_op(op_name: str,
        op_func: Callable,
        mutates_args: List[str],
        fake_impl: Optional[Callable],
        target_lib: Optional[Library])
         üìù `torch.library.custom_op` can have significant overhead because it

  L1731: def set_gpu_proc_affinity(tp_size: int, nnodes: int, gpu_id: int)

  L1766: def disable_request_logging()
         ‚Üí bool
         @lru_cache(maxsize=2)

  L1770: def dataclass_to_string_truncated(data,
        max_length,
        skip_names: Optional[Set[str]])

  L1812: def permute_weight(x: torch.Tensor)
         ‚Üí torch.Tensor

  L1874: def debug_timing(func)

  L1898: def nullable_str(val: str)

  L1904: def pyspy_dump_schedulers()
         üìù py-spy dump on all scheduler in a local node.

  L1918: def kill_itself_when_parent_died()

  L1928: def set_uvicorn_logging_configs()

  L1941: def get_ip()
         ‚Üí str

  L1985: def get_open_port()
         ‚Üí int

  L2009: def is_valid_ipv6_address(address: str)
         ‚Üí bool

  L2017: def maybe_wrap_ipv6_address(address: str)
         ‚Üí str

  L2023: def format_tcp_address(ip: str, port: int)
         ‚Üí str

  L2027: def configure_ipv6(dist_init_addr)

  L2059: def launch_dummy_health_check_server(host, port, enable_metrics)

  L2105: def create_checksum(directory: str)

  L2109: def set_cuda_arch()

  L2116: def next_power_of_2(n: int)

  L2120: def round_up(x: int, y: int)
         ‚Üí int

  L2135: def empty_context()

  L2139: def add_prefix(name: str, prefix: str)
         ‚Üí str
         üìù Add a weight path prefix to a module name.

  L2152: def is_remote_url(url: Union[str, Path])
         ‚Üí bool
         üìù Check if the URL is a remote URL of the format:

  L2165: def parse_connector_type(url: str)
         ‚Üí str
         üìù Parse the connector type from the URL of the format:

  L2178: def retry(fn,
        max_retry: int,
        initial_delay: float,
        max_delay: float,
        should_retry: Callable[[Any],
        bool])

  L2207: def flatten_nested_list(nested_list)

  L2216: def is_non_idle_and_non_empty(forward_mode, hidden_states)

  L2224: def fast_topk(values, topk, dim)

  L2233: def bind_or_assign(target, source)

  L2241: def get_local_ip_auto()
         ‚Üí str

  L2250: def get_local_ip_by_nic(interface: str)
         ‚Üí str

  L2279: def get_local_ip_by_remote()
         ‚Üí str

  L2307: def is_page_size_one(server_args)

  L2313: def is_no_spec_infer_or_topk_one(server_args)

  L2321: def is_fa3_default_architecture(hf_config)

  L2353: def log_info_on_rank0(logger, msg)

  L2360: def load_json_config(data: str)

  L2367: def dispose_tensor(x: torch.Tensor)

  L2393: def require_mlp_tp_gather(server_args)
         üìù Check if the input of MLP is obtained by all-gather rather than all-re

  L2416: def require_attn_tp_gather(server_args)
         üìù Check if the input of attention is scattered.

  L2430: def require_gathered_buffer(server_args)

  L2434: def require_mlp_sync(server_args)

  L2438: def find_local_repo_dir(repo_id: str, revision: Optional[str])
         ‚Üí Optional[str]

  L2463: def read_system_prompt_from_file(model_name: str)
         ‚Üí str
         üìù Read system prompt from a file in the HuggingFace cache directory.

  L2486: def bind_or_assign(target, source)

  L2494: def prepack_weight_if_needed(weight)

  L2506: def dim_is_supported(weight)

  L2578: def dynamic_import(func_path: str)

  L2591: def gc_object_counts()

  L2600: def configure_gc_warning(warn_threshold_secs)

  L2621: def freeze_gc(context: str)

  L2635: def configure_gc_logger()

  L2661: def align(x: int, y: int)
         ‚Üí int

  L2666: def ceil_div(x: int, y: int)
         ‚Üí int

  L2670: def parse_lscpu_topology()

  L2690: def get_physical_cpus_by_numa()

  L2725: def get_cpu_ids_by_node()

  L2736: def is_shm_available(dtype, world_size, local_size)

  L2745: def lru_cache_frozenset(maxsize)

  L2790: def apply_module_patch(target_module, target_function, wrappers)

  L2812: def parse_module_path(module_path, function_name, create_dummy)

  L2888: def mxfp_supported()
         üìù Returns whether the current platform supports MX types.

  L3002: def is_triton_kernels_available()
         ‚Üí bool
         @lru_cache(maxsize=1)

  L3006: def check_cuda_result(raw_output)


CLASS: BumpAllocator
----------------------------------------
  L2342: __init__(self, buffer_size: int, dtype, device)

  L2346: allocate(self, size: int)


CLASS: ConcurrentCounter
----------------------------------------
  L2924: __init__(self, initial: int)
         üìù Initialize the counter with an optional initial value.

  L2934: value(self)
         ‚Üí int
         üìù Return the current value of the counter.

  L2947: __repr__(self)
         ‚Üí str
         üìù Return an informative string representation of the counter.

  L2951: increment(self, n: int, notify_all: bool)
         üìù Atomically increment the counter by a given amount and notify all wait

  L2964: decrement(self, n: int, notify_all: bool)
         üìù Atomically decrement the counter by a given amount and notify all wait

  L2977: wait_for(self, condition: Callable[[int], bool])
         üìù Asynchronously wait until the counter satisfies a given condition.

  L2991: wait_for_zero(self)
         üìù Asynchronously wait until the counter reaches zero.


CLASS: DynamicGradMode
----------------------------------------
  L 267: set_inference_mode(mode: bool)

  L 275: __init__(self, mode)

  L 283: __new__(cls, mode_or_orig_func)

  L 288: __enter__(self)
         ‚Üí None

  L 296: __exit__(self, exc_type: Any, exc_value: Any, traceback: Any)
         ‚Üí None

  L 302: clone(self)
         ‚Üí 'DynamicGradMode'
         üìù Create a copy of this class


CLASS: EmptyContextManager
----------------------------------------
  L2128: __enter__(self)

  L2131: __exit__(self, exc_type, exc_value, traceback)


CLASS: LayerFn
----------------------------------------
  L 457: __call__(self, layer_id: int, prefix: str)
         ‚Üí torch.nn.Module


CLASS: LazyValue
----------------------------------------
  L2566: __init__(self, creator: Callable)

  L2571: value(self)


CLASS: MultiprocessingSerializer
----------------------------------------
  L1834: serialize(obj, output_str: bool)
         üìù Serialize a Python object using ForkingPickler.

  L1857: deserialize(data)
         üìù Deserialize a previously serialized object.


CLASS: PackWeightMethod
----------------------------------------
  L2557: __init__(self, weight_names, transpose_dims)

  L2561: process_weights_after_loading(self, module)
         ‚Üí None


CLASS: TimeInfo
----------------------------------------
  L 318: __init__(self, name, interval, color, indent)

  L 327: check(self)

  L 333: pretty_print(self)


CLASS: Withable
----------------------------------------
  L2375: __init__(self)

  L2379: value(self)
         ‚Üí T

  L2383: with_value(self, new_value: T)


============================================================
FILE: python/sglang/srt/warmup.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  16: def warmup(name: str)
         ‚Üí callable

  L  24: async def execute_warmups(disaggregation_mode: str,
        warmup_names: List[str],
        tokenizer_manager: TokenizerManager)

  L  38: async def voice_chat(disaggregation_mode: str,
        tokenizer_manager: TokenizerManager)
         @warmup('voice_chat')


============================================================
FILE: python/sglang/srt/weight_sync/tensor_bucket.py
Functions: 4
============================================================


CLASS: FlattenedTensorBucket
----------------------------------------
  L  25: __init__(self, named_tensors: List[Tuple[str, torch.Tensor]], flattened_tensor: torch.Tensor, metadata: List[FlattenedTensorMetadata])
         üìù Initialize a tensor bucket from a list of named tensors OR from pre-fl

  L  79: get_flattened_tensor(self)
         ‚Üí torch.Tensor
         üìù Get the flattened tensor containing all bucket tensors

  L  83: get_metadata(self)
         ‚Üí List[FlattenedTensorMetadata]
         üìù Get metadata for all tensors in the bucket

  L  87: reconstruct_tensors(self)
         ‚Üí List[Tuple[str, torch.Tensor]]
         üìù Reconstruct original tensors from flattened tensor with optimized perf


============================================================
FILE: python/sglang/srt/weight_sync/utils.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  14: async def update_weights(engine: Engine,
        params_batch: list[tuple[str,
        torch.Tensor]],
        device_mesh_key: str,
        device_mesh: DeviceMesh,
        load_format: Optional[str])
         üìù Update weights for the inference engine.
