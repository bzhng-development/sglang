[
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 35,
    "qualname": "init_custom_ar",
    "signature": "def init_custom_ar(ipc_tensors: List[torch.Tensor], rank_data: torch.Tensor, rank: int, full_nvlink: bool)"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 43,
    "qualname": "all_reduce",
    "signature": "def all_reduce(fa: int, inp: torch.Tensor, out: torch.Tensor, reg_buffer: int, reg_buffer_sz_bytes: int)"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 52,
    "qualname": "dispose",
    "signature": "def dispose(fa: int)"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 55,
    "qualname": "meta_size",
    "signature": "def meta_size()"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 58,
    "qualname": "register_buffer",
    "signature": "def register_buffer(fa: int, ipc_tensors: List[int])"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 61,
    "qualname": "get_graph_buffer_ipc_meta",
    "signature": "def get_graph_buffer_ipc_meta(fa: int)"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 64,
    "qualname": "register_graph_buffers",
    "signature": "def register_graph_buffers(fa: int, handles: List[List[int]], offsets: List[List[int]])"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 72,
    "qualname": "init_custom_ar",
    "signature": "def init_custom_ar(meta: torch.Tensor, rank_data: torch.Tensor, handles: List[str], offsets: List[int], rank: int, full_nvlink: bool)"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 84,
    "qualname": "all_reduce_reg",
    "signature": "def all_reduce_reg(fa: int, inp: torch.Tensor, out: torch.Tensor)"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 87,
    "qualname": "all_reduce_unreg",
    "signature": "def all_reduce_unreg(fa: int, inp: torch.Tensor, reg_buffer: torch.Tensor, out: torch.Tensor)"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 92,
    "qualname": "dispose",
    "signature": "def dispose(fa: int)"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 95,
    "qualname": "meta_size",
    "signature": "def meta_size()"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 98,
    "qualname": "register_buffer",
    "signature": "def register_buffer(fa: int, t: torch.Tensor, handles: List[str], offsets: List[int])"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 103,
    "qualname": "get_graph_buffer_ipc_meta",
    "signature": "def get_graph_buffer_ipc_meta(fa: int)"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 106,
    "qualname": "register_graph_buffers",
    "signature": "def register_graph_buffers(fa: int, handles: List[str], offsets: List[List[int]])"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 111,
    "qualname": "allocate_meta_buffer",
    "signature": "def allocate_meta_buffer(size: int)"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 114,
    "qualname": "get_meta_buffer_ipc_handle",
    "signature": "def get_meta_buffer_ipc_handle(inp: torch.Tensor)"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 119,
    "qualname": "init_custom_qr",
    "signature": "def init_custom_qr(rank: int, world_size: int, qr_max_size: Optional[int])"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 124,
    "qualname": "qr_get_handle",
    "signature": "def qr_get_handle(fa: int)"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 127,
    "qualname": "qr_open_handles",
    "signature": "def qr_open_handles(fa: int, handles: list[torch.Tensor])"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 130,
    "qualname": "qr_all_reduce",
    "signature": "def qr_all_reduce(fa: int, inp: torch.Tensor, out: torch.Tensor, quant_level: int, cast_bf2half: bool)"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 139,
    "qualname": "qr_destroy",
    "signature": "def qr_destroy(fa: int)"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 142,
    "qualname": "qr_max_size",
    "signature": "def qr_max_size()"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 146,
    "qualname": "mscclpp_generate_unique_id",
    "signature": "def mscclpp_generate_unique_id()"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 150,
    "qualname": "mscclpp_init_context",
    "signature": "def mscclpp_init_context(unique_id: bytes, rank: int, world_size: int, scratch: torch.Tensor, put_buffer: torch.Tensor, nranks_per_node: int, rank_to_node: List[int], rank_to_ib: List[int], context_selection: int)"
  },
  {
    "module": "srt._custom_ops",
    "file": "python/sglang/srt/_custom_ops.py",
    "line": 174,
    "qualname": "mscclpp_allreduce",
    "signature": "def mscclpp_allreduce(context: int, inp: torch.Tensor, out: torch.Tensor, nthreads: int, nblocks: int)"
  },
  {
    "module": "srt.aio_rwlock",
    "file": "python/sglang/srt/aio_rwlock.py",
    "line": 5,
    "qualname": "RWLock.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.aio_rwlock",
    "file": "python/sglang/srt/aio_rwlock.py",
    "line": 22,
    "qualname": "RWLock.reader_lock",
    "signature": "def reader_lock(self)"
  },
  {
    "module": "srt.aio_rwlock",
    "file": "python/sglang/srt/aio_rwlock.py",
    "line": 33,
    "qualname": "RWLock.writer_lock",
    "signature": "def writer_lock(self)"
  },
  {
    "module": "srt.aio_rwlock",
    "file": "python/sglang/srt/aio_rwlock.py",
    "line": 43,
    "qualname": "RWLock.acquire_reader",
    "signature": "async def acquire_reader(self)"
  },
  {
    "module": "srt.aio_rwlock",
    "file": "python/sglang/srt/aio_rwlock.py",
    "line": 51,
    "qualname": "RWLock.release_reader",
    "signature": "async def release_reader(self)"
  },
  {
    "module": "srt.aio_rwlock",
    "file": "python/sglang/srt/aio_rwlock.py",
    "line": 59,
    "qualname": "RWLock.acquire_writer",
    "signature": "async def acquire_writer(self)"
  },
  {
    "module": "srt.aio_rwlock",
    "file": "python/sglang/srt/aio_rwlock.py",
    "line": 72,
    "qualname": "RWLock.release_writer",
    "signature": "async def release_writer(self)"
  },
  {
    "module": "srt.aio_rwlock",
    "file": "python/sglang/srt/aio_rwlock.py",
    "line": 80,
    "qualname": "_ReaderLock.__init__",
    "signature": "def __init__(self, rwlock: RWLock)"
  },
  {
    "module": "srt.aio_rwlock",
    "file": "python/sglang/srt/aio_rwlock.py",
    "line": 83,
    "qualname": "_ReaderLock.__aenter__",
    "signature": "async def __aenter__(self)"
  },
  {
    "module": "srt.aio_rwlock",
    "file": "python/sglang/srt/aio_rwlock.py",
    "line": 87,
    "qualname": "_ReaderLock.__aexit__",
    "signature": "async def __aexit__(self, exc_type, exc_val, exc_tb)"
  },
  {
    "module": "srt.aio_rwlock",
    "file": "python/sglang/srt/aio_rwlock.py",
    "line": 92,
    "qualname": "_WriterLock.__init__",
    "signature": "def __init__(self, rwlock: RWLock)"
  },
  {
    "module": "srt.aio_rwlock",
    "file": "python/sglang/srt/aio_rwlock.py",
    "line": 95,
    "qualname": "_WriterLock.__aenter__",
    "signature": "async def __aenter__(self)"
  },
  {
    "module": "srt.aio_rwlock",
    "file": "python/sglang/srt/aio_rwlock.py",
    "line": 99,
    "qualname": "_WriterLock.__aexit__",
    "signature": "async def __aexit__(self, exc_type, exc_val, exc_tb)"
  },
  {
    "module": "srt.bench_utils",
    "file": "python/sglang/srt/bench_utils.py",
    "line": 10,
    "qualname": "suppress_stdout_stderr.__enter__",
    "signature": "def __enter__(self)"
  },
  {
    "module": "srt.bench_utils",
    "file": "python/sglang/srt/bench_utils.py",
    "line": 30,
    "qualname": "suppress_stdout_stderr.__exit__",
    "signature": "def __exit__(self)"
  },
  {
    "module": "srt.bench_utils",
    "file": "python/sglang/srt/bench_utils.py",
    "line": 45,
    "qualname": "bench_kineto",
    "signature": "def bench_kineto(fn, kernel_names, num_tests: int, suppress_kineto_output: bool, trace_path: str, flush_l2: bool, with_multiple_kernels: bool)"
  },
  {
    "module": "srt.code_completion_parser",
    "file": "python/sglang/srt/code_completion_parser.py",
    "line": 58,
    "qualname": "register_completion_template",
    "signature": "def register_completion_template(template: CompletionTemplate, override: bool)"
  },
  {
    "module": "srt.code_completion_parser",
    "file": "python/sglang/srt/code_completion_parser.py",
    "line": 68,
    "qualname": "completion_template_exists",
    "signature": "def completion_template_exists(template_name: str)"
  },
  {
    "module": "srt.code_completion_parser",
    "file": "python/sglang/srt/code_completion_parser.py",
    "line": 72,
    "qualname": "is_completion_template_defined",
    "signature": "def is_completion_template_defined()"
  },
  {
    "module": "srt.code_completion_parser",
    "file": "python/sglang/srt/code_completion_parser.py",
    "line": 77,
    "qualname": "generate_completion_prompt_from_request",
    "signature": "def generate_completion_prompt_from_request(request: CompletionRequest)"
  },
  {
    "module": "srt.code_completion_parser",
    "file": "python/sglang/srt/code_completion_parser.py",
    "line": 87,
    "qualname": "generate_completion_prompt",
    "signature": "def generate_completion_prompt(prompt: str, suffix: str, template_name: str)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 105,
    "qualname": "Conversation.get_prompt",
    "signature": "def get_prompt(self)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 380,
    "qualname": "Conversation.set_system_message",
    "signature": "def set_system_message(self, system_message: str)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 384,
    "qualname": "Conversation.append_message",
    "signature": "def append_message(self, role: str, message: str)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 388,
    "qualname": "Conversation.append_image",
    "signature": "def append_image(self, image: str, detail: Literal['auto', 'low', 'high'])"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 392,
    "qualname": "Conversation.append_video",
    "signature": "def append_video(self, video: str)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 396,
    "qualname": "Conversation.append_audio",
    "signature": "def append_audio(self, audio: str)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 400,
    "qualname": "Conversation.update_last_message",
    "signature": "def update_last_message(self, message: str)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 408,
    "qualname": "Conversation.to_gradio_chatbot",
    "signature": "def to_gradio_chatbot(self)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 418,
    "qualname": "Conversation.to_openai_api_messages",
    "signature": "def to_openai_api_messages(self)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 433,
    "qualname": "Conversation.copy",
    "signature": "def copy(self)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 450,
    "qualname": "Conversation.dict",
    "signature": "def dict(self)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 465,
    "qualname": "register_conv_template",
    "signature": "def register_conv_template(template: Conversation, override: bool)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 475,
    "qualname": "register_conv_template_matching_function",
    "signature": "def register_conv_template_matching_function(func)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 479,
    "qualname": "get_conv_template_by_model_path",
    "signature": "def get_conv_template_by_model_path(model_path)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 487,
    "qualname": "chat_template_exists",
    "signature": "def chat_template_exists(template_name: str)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 491,
    "qualname": "generate_embedding_convs",
    "signature": "def generate_embedding_convs(texts: List[str], images: List[str], template_name: str)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 559,
    "qualname": "generate_chat_conv",
    "signature": "def generate_chat_conv(request: ChatCompletionRequest, template_name: str)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 974,
    "qualname": "get_model_type",
    "signature": "def get_model_type(model_path: str)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 987,
    "qualname": "match_internvl",
    "signature": "def match_internvl(model_path: str)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 995,
    "qualname": "match_deepseek_janus_pro",
    "signature": "def match_deepseek_janus_pro(model_path: str)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 1003,
    "qualname": "match_vicuna",
    "signature": "def match_vicuna(model_path: str)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 1009,
    "qualname": "match_deepseek_vl",
    "signature": "def match_deepseek_vl(model_path: str)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 1017,
    "qualname": "match_qwen_chat_ml",
    "signature": "def match_qwen_chat_ml(model_path: str)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 1027,
    "qualname": "match_minicpm",
    "signature": "def match_minicpm(model_path: str)"
  },
  {
    "module": "srt.conversation",
    "file": "python/sglang/srt/conversation.py",
    "line": 1036,
    "qualname": "match_phi_4_mm",
    "signature": "def match_phi_4_mm(model_path: str)"
  },
  {
    "module": "srt.custom_op",
    "file": "python/sglang/srt/custom_op.py",
    "line": 13,
    "qualname": "CustomOp.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.custom_op",
    "file": "python/sglang/srt/custom_op.py",
    "line": 21,
    "qualname": "CustomOp.enter_torch_compile",
    "signature": "def enter_torch_compile(self, num_tokens: int)"
  },
  {
    "module": "srt.custom_op",
    "file": "python/sglang/srt/custom_op.py",
    "line": 48,
    "qualname": "CustomOp.leave_torch_compile",
    "signature": "def leave_torch_compile(self)"
  },
  {
    "module": "srt.custom_op",
    "file": "python/sglang/srt/custom_op.py",
    "line": 58,
    "qualname": "CustomOp.forward",
    "signature": "def forward(self)"
  },
  {
    "module": "srt.custom_op",
    "file": "python/sglang/srt/custom_op.py",
    "line": 61,
    "qualname": "CustomOp.forward_native",
    "signature": "def forward_native(self)"
  },
  {
    "module": "srt.custom_op",
    "file": "python/sglang/srt/custom_op.py",
    "line": 64,
    "qualname": "CustomOp.forward_cuda",
    "signature": "def forward_cuda(self)"
  },
  {
    "module": "srt.custom_op",
    "file": "python/sglang/srt/custom_op.py",
    "line": 67,
    "qualname": "CustomOp.forward_npu",
    "signature": "def forward_npu(self)"
  },
  {
    "module": "srt.custom_op",
    "file": "python/sglang/srt/custom_op.py",
    "line": 70,
    "qualname": "CustomOp.forward_hip",
    "signature": "def forward_hip(self)"
  },
  {
    "module": "srt.custom_op",
    "file": "python/sglang/srt/custom_op.py",
    "line": 73,
    "qualname": "CustomOp.forward_xpu",
    "signature": "def forward_xpu(self)"
  },
  {
    "module": "srt.custom_op",
    "file": "python/sglang/srt/custom_op.py",
    "line": 76,
    "qualname": "CustomOp.forward_hpu",
    "signature": "def forward_hpu(self)"
  },
  {
    "module": "srt.custom_op",
    "file": "python/sglang/srt/custom_op.py",
    "line": 79,
    "qualname": "CustomOp.forward_cpu",
    "signature": "def forward_cpu(self)"
  },
  {
    "module": "srt.custom_op",
    "file": "python/sglang/srt/custom_op.py",
    "line": 82,
    "qualname": "CustomOp.dispatch_forward",
    "signature": "def dispatch_forward(self)"
  },
  {
    "module": "srt.harmony_parser",
    "file": "python/sglang/srt/harmony_parser.py",
    "line": 24,
    "qualname": "prefix_hold",
    "signature": "def prefix_hold(text: str, tokens: List[str])"
  },
  {
    "module": "srt.harmony_parser",
    "file": "python/sglang/srt/harmony_parser.py",
    "line": 46,
    "qualname": "iter_tokens",
    "signature": "def iter_tokens(text: str, start_pos: int)"
  },
  {
    "module": "srt.harmony_parser",
    "file": "python/sglang/srt/harmony_parser.py",
    "line": 126,
    "qualname": "CanonicalStrategy.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.harmony_parser",
    "file": "python/sglang/srt/harmony_parser.py",
    "line": 137,
    "qualname": "CanonicalStrategy.parse",
    "signature": "def parse(self, text: str)"
  },
  {
    "module": "srt.harmony_parser",
    "file": "python/sglang/srt/harmony_parser.py",
    "line": 422,
    "qualname": "TextStrategy.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.harmony_parser",
    "file": "python/sglang/srt/harmony_parser.py",
    "line": 438,
    "qualname": "TextStrategy.set_buffer_context",
    "signature": "def set_buffer_context(self, buffer: str)"
  },
  {
    "module": "srt.harmony_parser",
    "file": "python/sglang/srt/harmony_parser.py",
    "line": 441,
    "qualname": "TextStrategy.parse",
    "signature": "def parse(self, text: str)"
  },
  {
    "module": "srt.harmony_parser",
    "file": "python/sglang/srt/harmony_parser.py",
    "line": 504,
    "qualname": "HarmonyParser.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.harmony_parser",
    "file": "python/sglang/srt/harmony_parser.py",
    "line": 514,
    "qualname": "HarmonyParser.parse",
    "signature": "def parse(self, chunk: str)"
  },
  {
    "module": "srt.host_shared_memory",
    "file": "python/sglang/srt/host_shared_memory.py",
    "line": 18,
    "qualname": "HostSharedMemoryManager.__init__",
    "signature": "def __init__(self, base_name: str)"
  },
  {
    "module": "srt.host_shared_memory",
    "file": "python/sglang/srt/host_shared_memory.py",
    "line": 23,
    "qualname": "HostSharedMemoryManager.malloc",
    "signature": "def malloc(self)"
  },
  {
    "module": "srt.host_shared_memory",
    "file": "python/sglang/srt/host_shared_memory.py",
    "line": 75,
    "qualname": "get_host_shared_memory_manager",
    "signature": "def get_host_shared_memory_manager()"
  },
  {
    "module": "srt.host_shared_memory",
    "file": "python/sglang/srt/host_shared_memory.py",
    "line": 80,
    "qualname": "set_host_shared_memory_manager",
    "signature": "def set_host_shared_memory_manager(instance: HostSharedMemoryManager)"
  },
  {
    "module": "srt.jinja_template_utils",
    "file": "python/sglang/srt/jinja_template_utils.py",
    "line": 81,
    "qualname": "detect_jinja_template_content_format",
    "signature": "def detect_jinja_template_content_format(chat_template: str)"
  },
  {
    "module": "srt.jinja_template_utils",
    "file": "python/sglang/srt/jinja_template_utils.py",
    "line": 117,
    "qualname": "process_content_for_template_format",
    "signature": "def process_content_for_template_format(msg_dict: dict, content_format: str, image_data: list, video_data: list, audio_data: list, modalities: list)"
  },
  {
    "module": "srt.model_parallel",
    "file": "python/sglang/srt/model_parallel.py",
    "line": 121,
    "qualname": "tensor_parallel",
    "signature": "def tensor_parallel(module: torch.nn.Module, device_mesh: Optional[DeviceMesh])"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 30,
    "qualname": "BaseOffloader.wrap_modules",
    "signature": "def wrap_modules(self, all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor], whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator])"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 38,
    "qualname": "BaseOffloader.post_init",
    "signature": "def post_init(self)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 50,
    "qualname": "get_offloader",
    "signature": "def get_offloader()"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 55,
    "qualname": "set_offloader",
    "signature": "def set_offloader(instance: BaseOffloader)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 60,
    "qualname": "create_offloader_from_server_args",
    "signature": "def create_offloader_from_server_args(server_args: ServerArgs, dp_rank: int)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 81,
    "qualname": "OffloaderV1.__init__",
    "signature": "def __init__(self, cpu_offload_max_bytes: int)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 85,
    "qualname": "OffloaderV1.wrap_modules",
    "signature": "def wrap_modules(self, all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor], whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator])"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 93,
    "qualname": "OffloaderV1.maybe_offload_to_cpu",
    "signature": "def maybe_offload_to_cpu(self, module: torch.nn.Module)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 150,
    "qualname": "OffloaderV2.__init__",
    "signature": "def __init__(self, group_size: int, num_in_group: int, prefetch_step: int, mode: str, dp_rank: int, dp_size: int)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 189,
    "qualname": "OffloaderV2.wrap_modules",
    "signature": "def wrap_modules(self, all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor], whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator])"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 229,
    "qualname": "OffloaderV2.post_init",
    "signature": "def post_init(self)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 267,
    "qualname": "_ModuleOffloader.__init__",
    "signature": "def __init__(self, mode: str, module: torch.nn.Module, alt_stream: torch.cuda.Stream, whitelist_param_names: List[str])"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 296,
    "qualname": "_ModuleOffloader.post_init",
    "signature": "def post_init(self)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 300,
    "qualname": "_ModuleOffloader.start_onload",
    "signature": "def start_onload(self)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 307,
    "qualname": "_ModuleOffloader.offload",
    "signature": "def offload(self)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 311,
    "qualname": "_ModuleOffloader.wait_and_get_device_tensors",
    "signature": "def wait_and_get_device_tensors(self)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 322,
    "qualname": "_BaseParamOffloader.create",
    "signature": "def create(mode: str)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 330,
    "qualname": "_BaseParamOffloader.__init__",
    "signature": "def __init__(self, module, param_name)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 338,
    "qualname": "_BaseParamOffloader.post_init",
    "signature": "def post_init(self)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 341,
    "qualname": "_BaseParamOffloader.create_device_tensor",
    "signature": "def create_device_tensor(self)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 348,
    "qualname": "_MetaParamOffloader.__init__",
    "signature": "def __init__(self, module, param_name)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 352,
    "qualname": "_MetaParamOffloader.create_device_tensor",
    "signature": "def create_device_tensor(self)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 357,
    "qualname": "_CpuParamOffloader.__init__",
    "signature": "def __init__(self, module, param_name)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 361,
    "qualname": "_CpuParamOffloader.create_device_tensor",
    "signature": "def create_device_tensor(self)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 366,
    "qualname": "_ShmCpuParamOffloader.__init__",
    "signature": "def __init__(self, module, param_name)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 389,
    "qualname": "_ShmCpuParamOffloader.post_init",
    "signature": "def post_init(self)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 397,
    "qualname": "_ShmCpuParamOffloader.create_device_tensor",
    "signature": "def create_device_tensor(self)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 453,
    "qualname": "_ShardedGpuParamOffloader.__init__",
    "signature": "def __init__(self, module, param_name)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 472,
    "qualname": "_ShardedGpuParamOffloader.post_init",
    "signature": "def post_init(self)"
  },
  {
    "module": "srt.offloader",
    "file": "python/sglang/srt/offloader.py",
    "line": 501,
    "qualname": "_ShardedGpuParamOffloader.create_device_tensor",
    "signature": "def create_device_tensor(self)"
  },
  {
    "module": "srt.operations",
    "file": "python/sglang/srt/operations.py",
    "line": 21,
    "qualname": "execute_operations",
    "signature": "def execute_operations(inputs, operations)"
  },
  {
    "module": "srt.operations",
    "file": "python/sglang/srt/operations.py",
    "line": 30,
    "qualname": "execute_overlapped_operations",
    "signature": "def execute_overlapped_operations(inputs_arr: Sequence, operations_arr: Sequence, delta_stages: Sequence[int])"
  },
  {
    "module": "srt.operations",
    "file": "python/sglang/srt/operations.py",
    "line": 76,
    "qualname": "_StageExecutor.__init__",
    "signature": "def __init__(self, debug_name: str, stages: List[Stage], inputs: dict)"
  },
  {
    "module": "srt.operations",
    "file": "python/sglang/srt/operations.py",
    "line": 89,
    "qualname": "_StageExecutor.next",
    "signature": "def next(self)"
  },
  {
    "module": "srt.operations",
    "file": "python/sglang/srt/operations.py",
    "line": 114,
    "qualname": "_StageExecutor.output",
    "signature": "def output(self)"
  },
  {
    "module": "srt.operations",
    "file": "python/sglang/srt/operations.py",
    "line": 119,
    "qualname": "_StageExecutor.done",
    "signature": "def done(self)"
  },
  {
    "module": "srt.operations",
    "file": "python/sglang/srt/operations.py",
    "line": 123,
    "qualname": "_StageExecutor.num_stages",
    "signature": "def num_stages(self)"
  },
  {
    "module": "srt.operations",
    "file": "python/sglang/srt/operations.py",
    "line": 138,
    "qualname": "_StateDict.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.operations",
    "file": "python/sglang/srt/operations.py",
    "line": 141,
    "qualname": "_StateDict.__setattr__",
    "signature": "def __setattr__(self, key, value)"
  },
  {
    "module": "srt.operations",
    "file": "python/sglang/srt/operations.py",
    "line": 150,
    "qualname": "_StateDict.__getattr__",
    "signature": "def __getattr__(self, item)"
  },
  {
    "module": "srt.operations",
    "file": "python/sglang/srt/operations.py",
    "line": 153,
    "qualname": "_StateDict.__delattr__",
    "signature": "def __delattr__(self, item)"
  },
  {
    "module": "srt.operations",
    "file": "python/sglang/srt/operations.py",
    "line": 156,
    "qualname": "_StateDict.pop",
    "signature": "def pop(self, item)"
  },
  {
    "module": "srt.operations",
    "file": "python/sglang/srt/operations.py",
    "line": 159,
    "qualname": "_StateDict.update",
    "signature": "def update(self, values: Dict[str, Any])"
  },
  {
    "module": "srt.operations",
    "file": "python/sglang/srt/operations.py",
    "line": 163,
    "qualname": "_StateDict.get",
    "signature": "def get(self, item)"
  },
  {
    "module": "srt.operations",
    "file": "python/sglang/srt/operations.py",
    "line": 166,
    "qualname": "_StateDict.clear",
    "signature": "def clear(self, expect_keys: Sequence[str])"
  },
  {
    "module": "srt.operations_strategy",
    "file": "python/sglang/srt/operations_strategy.py",
    "line": 19,
    "qualname": "OperationsStrategy.concat",
    "signature": "def concat(cls, items: List['OperationsStrategy'])"
  },
  {
    "module": "srt.operations_strategy",
    "file": "python/sglang/srt/operations_strategy.py",
    "line": 31,
    "qualname": "OperationsStrategy.init_new_tbo",
    "signature": "def init_new_tbo(layers: torch.nn.ModuleList, forward_mode: ForwardMode)"
  },
  {
    "module": "srt.patch_torch",
    "file": "python/sglang/srt/patch_torch.py",
    "line": 21,
    "qualname": "monkey_patch_torch_reductions",
    "signature": "def monkey_patch_torch_reductions()"
  },
  {
    "module": "srt.patch_torch",
    "file": "python/sglang/srt/patch_torch.py",
    "line": 75,
    "qualname": "monkey_patch_torch_compile",
    "signature": "def monkey_patch_torch_compile()"
  },
  {
    "module": "srt.poll_based_barrier",
    "file": "python/sglang/srt/poll_based_barrier.py",
    "line": 7,
    "qualname": "PollBasedBarrier.__init__",
    "signature": "def __init__(self, noop: bool)"
  },
  {
    "module": "srt.poll_based_barrier",
    "file": "python/sglang/srt/poll_based_barrier.py",
    "line": 11,
    "qualname": "PollBasedBarrier.local_arrive",
    "signature": "def local_arrive(self)"
  },
  {
    "module": "srt.poll_based_barrier",
    "file": "python/sglang/srt/poll_based_barrier.py",
    "line": 15,
    "qualname": "PollBasedBarrier.poll_global_arrived",
    "signature": "def poll_global_arrived(self)"
  },
  {
    "module": "srt.reasoning_parser",
    "file": "python/sglang/srt/reasoning_parser.py",
    "line": 10,
    "qualname": "StreamingParseResult.__init__",
    "signature": "def __init__(self, normal_text: Optional[str], reasoning_text: Optional[str])"
  },
  {
    "module": "srt.reasoning_parser",
    "file": "python/sglang/srt/reasoning_parser.py",
    "line": 22,
    "qualname": "BaseReasoningFormatDetector.__init__",
    "signature": "def __init__(self, think_start_token: str, think_end_token: str, force_reasoning: bool, stream_reasoning: bool)"
  },
  {
    "module": "srt.reasoning_parser",
    "file": "python/sglang/srt/reasoning_parser.py",
    "line": 37,
    "qualname": "BaseReasoningFormatDetector.detect_and_parse",
    "signature": "def detect_and_parse(self, text: str)"
  },
  {
    "module": "srt.reasoning_parser",
    "file": "python/sglang/srt/reasoning_parser.py",
    "line": 63,
    "qualname": "BaseReasoningFormatDetector.parse_streaming_increment",
    "signature": "def parse_streaming_increment(self, new_text: str)"
  },
  {
    "module": "srt.reasoning_parser",
    "file": "python/sglang/srt/reasoning_parser.py",
    "line": 141,
    "qualname": "DeepSeekR1Detector.__init__",
    "signature": "def __init__(self, stream_reasoning: bool, force_reasoning: bool)"
  },
  {
    "module": "srt.reasoning_parser",
    "file": "python/sglang/srt/reasoning_parser.py",
    "line": 168,
    "qualname": "Qwen3Detector.__init__",
    "signature": "def __init__(self, stream_reasoning: bool, force_reasoning: bool)"
  },
  {
    "module": "srt.reasoning_parser",
    "file": "python/sglang/srt/reasoning_parser.py",
    "line": 186,
    "qualname": "KimiDetector.__init__",
    "signature": "def __init__(self, stream_reasoning: bool, force_reasoning: bool)"
  },
  {
    "module": "srt.reasoning_parser",
    "file": "python/sglang/srt/reasoning_parser.py",
    "line": 200,
    "qualname": "GptOssDetector.__init__",
    "signature": "def __init__(self, stream_reasoning: bool, force_reasoning: bool)"
  },
  {
    "module": "srt.reasoning_parser",
    "file": "python/sglang/srt/reasoning_parser.py",
    "line": 209,
    "qualname": "GptOssDetector.detect_and_parse",
    "signature": "def detect_and_parse(self, text: str)"
  },
  {
    "module": "srt.reasoning_parser",
    "file": "python/sglang/srt/reasoning_parser.py",
    "line": 232,
    "qualname": "GptOssDetector.parse_streaming_increment",
    "signature": "def parse_streaming_increment(self, new_text: str)"
  },
  {
    "module": "srt.reasoning_parser",
    "file": "python/sglang/srt/reasoning_parser.py",
    "line": 275,
    "qualname": "ReasoningParser.__init__",
    "signature": "def __init__(self, model_type: Optional[str], stream_reasoning: bool, force_reasoning: Optional[bool])"
  },
  {
    "module": "srt.reasoning_parser",
    "file": "python/sglang/srt/reasoning_parser.py",
    "line": 299,
    "qualname": "ReasoningParser.parse_non_stream",
    "signature": "def parse_non_stream(self, full_text: str)"
  },
  {
    "module": "srt.reasoning_parser",
    "file": "python/sglang/srt/reasoning_parser.py",
    "line": 304,
    "qualname": "ReasoningParser.parse_stream_chunk",
    "signature": "def parse_stream_chunk(self, chunk_text: str)"
  },
  {
    "module": "srt.torch_memory_saver_adapter",
    "file": "python/sglang/srt/torch_memory_saver_adapter.py",
    "line": 21,
    "qualname": "TorchMemorySaverAdapter.create",
    "signature": "def create(enable: bool)"
  },
  {
    "module": "srt.torch_memory_saver_adapter",
    "file": "python/sglang/srt/torch_memory_saver_adapter.py",
    "line": 33,
    "qualname": "TorchMemorySaverAdapter.check_validity",
    "signature": "def check_validity(self, caller_name)"
  },
  {
    "module": "srt.torch_memory_saver_adapter",
    "file": "python/sglang/srt/torch_memory_saver_adapter.py",
    "line": 40,
    "qualname": "TorchMemorySaverAdapter.configure_subprocess",
    "signature": "def configure_subprocess(self)"
  },
  {
    "module": "srt.torch_memory_saver_adapter",
    "file": "python/sglang/srt/torch_memory_saver_adapter.py",
    "line": 43,
    "qualname": "TorchMemorySaverAdapter.region",
    "signature": "def region(self, tag: str)"
  },
  {
    "module": "srt.torch_memory_saver_adapter",
    "file": "python/sglang/srt/torch_memory_saver_adapter.py",
    "line": 46,
    "qualname": "TorchMemorySaverAdapter.pause",
    "signature": "def pause(self, tag: str)"
  },
  {
    "module": "srt.torch_memory_saver_adapter",
    "file": "python/sglang/srt/torch_memory_saver_adapter.py",
    "line": 49,
    "qualname": "TorchMemorySaverAdapter.resume",
    "signature": "def resume(self, tag: str)"
  },
  {
    "module": "srt.torch_memory_saver_adapter",
    "file": "python/sglang/srt/torch_memory_saver_adapter.py",
    "line": 53,
    "qualname": "TorchMemorySaverAdapter.enabled",
    "signature": "def enabled(self)"
  },
  {
    "module": "srt.torch_memory_saver_adapter",
    "file": "python/sglang/srt/torch_memory_saver_adapter.py",
    "line": 60,
    "qualname": "_TorchMemorySaverAdapterReal.configure_subprocess",
    "signature": "def configure_subprocess(self)"
  },
  {
    "module": "srt.torch_memory_saver_adapter",
    "file": "python/sglang/srt/torch_memory_saver_adapter.py",
    "line": 63,
    "qualname": "_TorchMemorySaverAdapterReal.region",
    "signature": "def region(self, tag: str)"
  },
  {
    "module": "srt.torch_memory_saver_adapter",
    "file": "python/sglang/srt/torch_memory_saver_adapter.py",
    "line": 66,
    "qualname": "_TorchMemorySaverAdapterReal.pause",
    "signature": "def pause(self, tag: str)"
  },
  {
    "module": "srt.torch_memory_saver_adapter",
    "file": "python/sglang/srt/torch_memory_saver_adapter.py",
    "line": 69,
    "qualname": "_TorchMemorySaverAdapterReal.resume",
    "signature": "def resume(self, tag: str)"
  },
  {
    "module": "srt.torch_memory_saver_adapter",
    "file": "python/sglang/srt/torch_memory_saver_adapter.py",
    "line": 73,
    "qualname": "_TorchMemorySaverAdapterReal.enabled",
    "signature": "def enabled(self)"
  },
  {
    "module": "srt.torch_memory_saver_adapter",
    "file": "python/sglang/srt/torch_memory_saver_adapter.py",
    "line": 79,
    "qualname": "_TorchMemorySaverAdapterNoop.configure_subprocess",
    "signature": "def configure_subprocess(self)"
  },
  {
    "module": "srt.torch_memory_saver_adapter",
    "file": "python/sglang/srt/torch_memory_saver_adapter.py",
    "line": 83,
    "qualname": "_TorchMemorySaverAdapterNoop.region",
    "signature": "def region(self, tag: str)"
  },
  {
    "module": "srt.torch_memory_saver_adapter",
    "file": "python/sglang/srt/torch_memory_saver_adapter.py",
    "line": 86,
    "qualname": "_TorchMemorySaverAdapterNoop.pause",
    "signature": "def pause(self, tag: str)"
  },
  {
    "module": "srt.torch_memory_saver_adapter",
    "file": "python/sglang/srt/torch_memory_saver_adapter.py",
    "line": 89,
    "qualname": "_TorchMemorySaverAdapterNoop.resume",
    "signature": "def resume(self, tag: str)"
  },
  {
    "module": "srt.torch_memory_saver_adapter",
    "file": "python/sglang/srt/torch_memory_saver_adapter.py",
    "line": 93,
    "qualname": "_TorchMemorySaverAdapterNoop.enabled",
    "signature": "def enabled(self)"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 49,
    "qualname": "get_token_num_per_seq",
    "signature": "def get_token_num_per_seq(forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 65,
    "qualname": "compute_split_seq_index",
    "signature": "def compute_split_seq_index(forward_mode: 'ForwardMode', num_tokens: int, extend_lens: Optional[Sequence[int]], token_num_per_seq: Optional[int])"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 180,
    "qualname": "split_spec_info",
    "signature": "def split_spec_info(spec_info: Optional[EagleVerifyInput], start_seq_index: int, end_seq_index: int, start_token_index: int, end_token_index: int)"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 252,
    "qualname": "compute_split_token_index",
    "signature": "def compute_split_token_index(split_seq_index: int, forward_mode: 'ForwardMode', extend_seq_lens: Optional[Sequence[int]], token_num_per_seq: Optional[int])"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 273,
    "qualname": "compute_split_indices_for_cuda_graph_replay",
    "signature": "def compute_split_indices_for_cuda_graph_replay(forward_mode: ForwardMode, cuda_graph_num_tokens: int, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 303,
    "qualname": "TboCudaGraphRunnerPlugin.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 306,
    "qualname": "TboCudaGraphRunnerPlugin.capture_one_batch_size",
    "signature": "def capture_one_batch_size(self, batch: ForwardBatch, num_tokens: int)"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 331,
    "qualname": "TboCudaGraphRunnerPlugin.replay_prepare",
    "signature": "def replay_prepare(self, forward_mode: ForwardMode, bs: int, num_token_non_padded: int, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 358,
    "qualname": "TboDPAttentionPreparer.prepare_all_gather",
    "signature": "def prepare_all_gather(self, local_batch: ScheduleBatch)"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 404,
    "qualname": "TboDPAttentionPreparer.compute_output",
    "signature": "def compute_output(self, partial_global_info)"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 452,
    "qualname": "TboForwardBatchPreparer.prepare",
    "signature": "def prepare(cls, batch: ForwardBatch, is_draft_worker: bool)"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 464,
    "qualname": "TboForwardBatchPreparer.prepare_raw",
    "signature": "def prepare_raw(cls, batch: ForwardBatch, tbo_children_num_token_non_padded: torch.Tensor)"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 529,
    "qualname": "TboForwardBatchPreparer.derive_fields_related_to_seq_len_for_two_chunk",
    "signature": "def derive_fields_related_to_seq_len_for_two_chunk(cls, batch: ForwardBatch)"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 591,
    "qualname": "TboForwardBatchPreparer.filter_batch",
    "signature": "def filter_batch(cls, batch: ForwardBatch)"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 740,
    "qualname": "TboForwardBatchPreparer.compute_tbo_children_num_token_non_padded",
    "signature": "def compute_tbo_children_num_token_non_padded(cls, batch: ForwardBatch)"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 747,
    "qualname": "TboForwardBatchPreparer.compute_tbo_children_num_token_non_padded_raw",
    "signature": "def compute_tbo_children_num_token_non_padded_raw(cls, tbo_split_token_index: int, num_token_non_padded: int)"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 785,
    "qualname": "model_forward_maybe_tbo",
    "signature": "def model_forward_maybe_tbo(layers, enable_tbo: bool, positions: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor, input_data_scatter_mode: ScatterMode, residual: Optional[torch.Tensor], zero_allocator: Optional[BumpAllocator])"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 964,
    "qualname": "MaybeTboDeepEPDispatcher.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 973,
    "qualname": "MaybeTboDeepEPDispatcher.dispatch",
    "signature": "def dispatch(self)"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 976,
    "qualname": "MaybeTboDeepEPDispatcher.dispatch_a",
    "signature": "def dispatch_a(self)"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 979,
    "qualname": "MaybeTboDeepEPDispatcher.dispatch_b",
    "signature": "def dispatch_b(self)"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 982,
    "qualname": "MaybeTboDeepEPDispatcher.combine",
    "signature": "def combine(self)"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 985,
    "qualname": "MaybeTboDeepEPDispatcher.combine_a",
    "signature": "def combine_a(self)"
  },
  {
    "module": "srt.two_batch_overlap",
    "file": "python/sglang/srt/two_batch_overlap.py",
    "line": 988,
    "qualname": "MaybeTboDeepEPDispatcher.combine_b",
    "signature": "def combine_b(self)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 103,
    "qualname": "is_hip",
    "signature": "def is_hip()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 118,
    "qualname": "is_cuda",
    "signature": "def is_cuda()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 122,
    "qualname": "is_cuda_alike",
    "signature": "def is_cuda_alike()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 126,
    "qualname": "is_hpu",
    "signature": "def is_hpu()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 130,
    "qualname": "is_xpu",
    "signature": "def is_xpu()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 134,
    "qualname": "is_npu",
    "signature": "def is_npu()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 138,
    "qualname": "is_host_cpu_x86",
    "signature": "def is_host_cpu_x86()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 147,
    "qualname": "is_cpu",
    "signature": "def is_cpu()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 151,
    "qualname": "get_cuda_version",
    "signature": "def get_cuda_version()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 169,
    "qualname": "is_blackwell",
    "signature": "def is_blackwell()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 176,
    "qualname": "is_sm100_supported",
    "signature": "def is_sm100_supported(device)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 183,
    "qualname": "is_sm90_supported",
    "signature": "def is_sm90_supported(device)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 192,
    "qualname": "get_bool_env_var",
    "signature": "def get_bool_env_var(name: str, default: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 209,
    "qualname": "get_int_env_var",
    "signature": "def get_int_env_var(name: str, default: int)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 219,
    "qualname": "support_triton",
    "signature": "def support_triton(backend: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 233,
    "qualname": "cpu_has_amx_support",
    "signature": "def cpu_has_amx_support()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 237,
    "qualname": "use_intel_amx_backend",
    "signature": "def use_intel_amx_backend(layer)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 241,
    "qualname": "is_flashinfer_available",
    "signature": "def is_flashinfer_available()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 251,
    "qualname": "random_uuid",
    "signature": "def random_uuid()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 267,
    "qualname": "DynamicGradMode.set_inference_mode",
    "signature": "def set_inference_mode(mode: bool)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 275,
    "qualname": "DynamicGradMode.__init__",
    "signature": "def __init__(self, mode)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 283,
    "qualname": "DynamicGradMode.__new__",
    "signature": "def __new__(cls, mode_or_orig_func)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 288,
    "qualname": "DynamicGradMode.__enter__",
    "signature": "def __enter__(self)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 296,
    "qualname": "DynamicGradMode.__exit__",
    "signature": "def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 302,
    "qualname": "DynamicGradMode.clone",
    "signature": "def clone(self)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 312,
    "qualname": "enable_show_time_cost",
    "signature": "def enable_show_time_cost()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 318,
    "qualname": "TimeInfo.__init__",
    "signature": "def __init__(self, name, interval, color, indent)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 327,
    "qualname": "TimeInfo.check",
    "signature": "def check(self)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 333,
    "qualname": "TimeInfo.pretty_print",
    "signature": "def pretty_print(self)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 339,
    "qualname": "mark_start",
    "signature": "def mark_start(name, interval, color, indent)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 349,
    "qualname": "mark_end",
    "signature": "def mark_end(name)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 359,
    "qualname": "calculate_time",
    "signature": "def calculate_time(show, min_cost_ms)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 378,
    "qualname": "get_available_gpu_memory",
    "signature": "def get_available_gpu_memory(device, gpu_id, distributed, empty_cache, cpu_group)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 451,
    "qualname": "is_pin_memory_available",
    "signature": "def is_pin_memory_available()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 457,
    "qualname": "LayerFn.__call__",
    "signature": "def __call__(self, layer_id: int, prefix: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 460,
    "qualname": "make_layers",
    "signature": "def make_layers(num_hidden_layers: int, layer_fn: LayerFn, pp_rank: Optional[int], pp_size: Optional[int], prefix: str, return_tuple: bool, offloader_kwargs: Dict[str, Any])"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 504,
    "qualname": "set_random_seed",
    "signature": "def set_random_seed(seed: int)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 513,
    "qualname": "find_process_using_port",
    "signature": "def find_process_using_port(port: int)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 525,
    "qualname": "wait_port_available",
    "signature": "def wait_port_available(port: int, port_name: str, timeout_s: int, raise_exception: bool)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 553,
    "qualname": "is_port_available",
    "signature": "def is_port_available(port)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 567,
    "qualname": "get_free_port",
    "signature": "def get_free_port()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 580,
    "qualname": "decode_video_base64",
    "signature": "def decode_video_base64(video_base64)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 659,
    "qualname": "load_audio",
    "signature": "def load_audio(audio_file: str, sr: Optional[int], mono: bool)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 707,
    "qualname": "load_image",
    "signature": "def load_image(image_file: Union[Image.Image, str, ImageData, bytes])"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 741,
    "qualname": "load_video",
    "signature": "def load_video(video_file: Union[str, bytes], use_gpu: bool)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 796,
    "qualname": "suppress_other_loggers",
    "signature": "def suppress_other_loggers()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 816,
    "qualname": "assert_pkg_version",
    "signature": "def assert_pkg_version(pkg: str, min_version: str, message: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 831,
    "qualname": "kill_process_tree",
    "signature": "def kill_process_tree(parent_pid, include_parent: bool, skip_pid: int)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 870,
    "qualname": "monkey_patch_p2p_access_check",
    "signature": "def monkey_patch_p2p_access_check()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 888,
    "qualname": "monkey_patch_vllm_gguf_config",
    "signature": "def monkey_patch_vllm_gguf_config()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 914,
    "qualname": "set_ulimit",
    "signature": "def set_ulimit(target_soft_limit)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 938,
    "qualname": "add_api_key_middleware",
    "signature": "def add_api_key_middleware(app, api_key: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 952,
    "qualname": "prepare_model_and_tokenizer",
    "signature": "def prepare_model_and_tokenizer(model_path: str, tokenizer_path: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 964,
    "qualname": "configure_logger",
    "signature": "def configure_logger(server_args, prefix: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 986,
    "qualname": "replace_submodule",
    "signature": "def replace_submodule(model: nn.Module, module_name: str, new_module: nn.Module)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 996,
    "qualname": "set_weight_attrs",
    "signature": "def set_weight_attrs(weight: torch.Tensor, weight_attrs: Optional[Dict[str, Any]])"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1016,
    "qualname": "broadcast_pyobj",
    "signature": "def broadcast_pyobj(data: List[Any], rank: int, dist_group: Optional[torch.distributed.ProcessGroup], src: int, force_cpu_device: bool)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1063,
    "qualname": "point_to_point_pyobj",
    "signature": "def point_to_point_pyobj(data: List[Any], rank: int, group: Optional[torch.distributed.ProcessGroup], src: int, dst: int)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1122,
    "qualname": "pytorch_profile",
    "signature": "def pytorch_profile(name, func)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1150,
    "qualname": "get_zmq_socket",
    "signature": "def get_zmq_socket(context: zmq.Context, socket_type: zmq.SocketType, endpoint: str, bind: bool)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1191,
    "qualname": "dump_to_file",
    "signature": "def dump_to_file(dirpath, name, value)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1206,
    "qualname": "is_triton_3",
    "signature": "def is_triton_3()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1210,
    "qualname": "maybe_torch_compile",
    "signature": "def maybe_torch_compile()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1224,
    "qualname": "delete_directory",
    "signature": "def delete_directory(dirpath)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1237,
    "qualname": "set_prometheus_multiproc_dir",
    "signature": "def set_prometheus_multiproc_dir()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1255,
    "qualname": "add_prometheus_middleware",
    "signature": "def add_prometheus_middleware(app)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1268,
    "qualname": "bind_port",
    "signature": "def bind_port(port)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1277,
    "qualname": "get_amdgpu_memory_capacity",
    "signature": "def get_amdgpu_memory_capacity()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1310,
    "qualname": "get_device_sm",
    "signature": "def get_device_sm()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1317,
    "qualname": "get_nvgpu_memory_capacity",
    "signature": "def get_nvgpu_memory_capacity()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1356,
    "qualname": "get_hpu_memory_capacity",
    "signature": "def get_hpu_memory_capacity()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1387,
    "qualname": "get_npu_memory_capacity",
    "signature": "def get_npu_memory_capacity()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1396,
    "qualname": "get_device_memory_capacity",
    "signature": "def get_device_memory_capacity(device: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1415,
    "qualname": "init_custom_process_group",
    "signature": "def init_custom_process_group(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1484,
    "qualname": "crash_on_warnings",
    "signature": "def crash_on_warnings()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1489,
    "qualname": "print_warning_once",
    "signature": "def print_warning_once(msg: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1495,
    "qualname": "print_info_once",
    "signature": "def print_info_once(msg: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1499,
    "qualname": "get_device_name",
    "signature": "def get_device_name(device_id: int)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1514,
    "qualname": "is_habana_available",
    "signature": "def is_habana_available()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1519,
    "qualname": "get_device",
    "signature": "def get_device(device_id: Optional[int])"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1561,
    "qualname": "get_device_count",
    "signature": "def get_device_count()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1586,
    "qualname": "get_device_core_count",
    "signature": "def get_device_core_count(device_id: int)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1593,
    "qualname": "get_device_capability",
    "signature": "def get_device_capability(device_id: int)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1618,
    "qualname": "get_npu_compiler_config",
    "signature": "def get_npu_compiler_config()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1627,
    "qualname": "get_compiler_backend",
    "signature": "def get_compiler_backend()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1657,
    "qualname": "supports_custom_op",
    "signature": "def supports_custom_op()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1661,
    "qualname": "direct_register_custom_op",
    "signature": "def direct_register_custom_op(op_name: str, op_func: Callable, mutates_args: List[str], fake_impl: Optional[Callable], target_lib: Optional[Library])"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1731,
    "qualname": "set_gpu_proc_affinity",
    "signature": "def set_gpu_proc_affinity(tp_size: int, nnodes: int, gpu_id: int)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1766,
    "qualname": "disable_request_logging",
    "signature": "def disable_request_logging()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1770,
    "qualname": "dataclass_to_string_truncated",
    "signature": "def dataclass_to_string_truncated(data, max_length, skip_names: Optional[Set[str]])"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1812,
    "qualname": "permute_weight",
    "signature": "def permute_weight(x: torch.Tensor)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1834,
    "qualname": "MultiprocessingSerializer.serialize",
    "signature": "def serialize(obj, output_str: bool)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1857,
    "qualname": "MultiprocessingSerializer.deserialize",
    "signature": "def deserialize(data)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1874,
    "qualname": "debug_timing",
    "signature": "def debug_timing(func)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1898,
    "qualname": "nullable_str",
    "signature": "def nullable_str(val: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1904,
    "qualname": "pyspy_dump_schedulers",
    "signature": "def pyspy_dump_schedulers()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1918,
    "qualname": "kill_itself_when_parent_died",
    "signature": "def kill_itself_when_parent_died()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1928,
    "qualname": "set_uvicorn_logging_configs",
    "signature": "def set_uvicorn_logging_configs()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1941,
    "qualname": "get_ip",
    "signature": "def get_ip()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 1985,
    "qualname": "get_open_port",
    "signature": "def get_open_port()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2009,
    "qualname": "is_valid_ipv6_address",
    "signature": "def is_valid_ipv6_address(address: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2017,
    "qualname": "maybe_wrap_ipv6_address",
    "signature": "def maybe_wrap_ipv6_address(address: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2023,
    "qualname": "format_tcp_address",
    "signature": "def format_tcp_address(ip: str, port: int)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2027,
    "qualname": "configure_ipv6",
    "signature": "def configure_ipv6(dist_init_addr)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2059,
    "qualname": "launch_dummy_health_check_server",
    "signature": "def launch_dummy_health_check_server(host, port, enable_metrics)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2105,
    "qualname": "create_checksum",
    "signature": "def create_checksum(directory: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2109,
    "qualname": "set_cuda_arch",
    "signature": "def set_cuda_arch()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2116,
    "qualname": "next_power_of_2",
    "signature": "def next_power_of_2(n: int)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2120,
    "qualname": "round_up",
    "signature": "def round_up(x: int, y: int)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2128,
    "qualname": "EmptyContextManager.__enter__",
    "signature": "def __enter__(self)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2131,
    "qualname": "EmptyContextManager.__exit__",
    "signature": "def __exit__(self, exc_type, exc_value, traceback)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2135,
    "qualname": "empty_context",
    "signature": "def empty_context()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2139,
    "qualname": "add_prefix",
    "signature": "def add_prefix(name: str, prefix: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2152,
    "qualname": "is_remote_url",
    "signature": "def is_remote_url(url: Union[str, Path])"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2165,
    "qualname": "parse_connector_type",
    "signature": "def parse_connector_type(url: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2178,
    "qualname": "retry",
    "signature": "def retry(fn, max_retry: int, initial_delay: float, max_delay: float, should_retry: Callable[[Any], bool])"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2207,
    "qualname": "flatten_nested_list",
    "signature": "def flatten_nested_list(nested_list)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2216,
    "qualname": "is_non_idle_and_non_empty",
    "signature": "def is_non_idle_and_non_empty(forward_mode, hidden_states)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2224,
    "qualname": "fast_topk",
    "signature": "def fast_topk(values, topk, dim)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2233,
    "qualname": "bind_or_assign",
    "signature": "def bind_or_assign(target, source)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2241,
    "qualname": "get_local_ip_auto",
    "signature": "def get_local_ip_auto()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2250,
    "qualname": "get_local_ip_by_nic",
    "signature": "def get_local_ip_by_nic(interface: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2279,
    "qualname": "get_local_ip_by_remote",
    "signature": "def get_local_ip_by_remote()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2307,
    "qualname": "is_page_size_one",
    "signature": "def is_page_size_one(server_args)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2313,
    "qualname": "is_no_spec_infer_or_topk_one",
    "signature": "def is_no_spec_infer_or_topk_one(server_args)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2321,
    "qualname": "is_fa3_default_architecture",
    "signature": "def is_fa3_default_architecture(hf_config)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2342,
    "qualname": "BumpAllocator.__init__",
    "signature": "def __init__(self, buffer_size: int, dtype, device)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2346,
    "qualname": "BumpAllocator.allocate",
    "signature": "def allocate(self, size: int)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2353,
    "qualname": "log_info_on_rank0",
    "signature": "def log_info_on_rank0(logger, msg)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2360,
    "qualname": "load_json_config",
    "signature": "def load_json_config(data: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2367,
    "qualname": "dispose_tensor",
    "signature": "def dispose_tensor(x: torch.Tensor)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2375,
    "qualname": "Withable.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2379,
    "qualname": "Withable.value",
    "signature": "def value(self)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2383,
    "qualname": "Withable.with_value",
    "signature": "def with_value(self, new_value: T)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2393,
    "qualname": "require_mlp_tp_gather",
    "signature": "def require_mlp_tp_gather(server_args)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2416,
    "qualname": "require_attn_tp_gather",
    "signature": "def require_attn_tp_gather(server_args)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2430,
    "qualname": "require_gathered_buffer",
    "signature": "def require_gathered_buffer(server_args)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2434,
    "qualname": "require_mlp_sync",
    "signature": "def require_mlp_sync(server_args)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2438,
    "qualname": "find_local_repo_dir",
    "signature": "def find_local_repo_dir(repo_id: str, revision: Optional[str])"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2463,
    "qualname": "read_system_prompt_from_file",
    "signature": "def read_system_prompt_from_file(model_name: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2486,
    "qualname": "bind_or_assign",
    "signature": "def bind_or_assign(target, source)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2494,
    "qualname": "prepack_weight_if_needed",
    "signature": "def prepack_weight_if_needed(weight)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2506,
    "qualname": "dim_is_supported",
    "signature": "def dim_is_supported(weight)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2557,
    "qualname": "PackWeightMethod.__init__",
    "signature": "def __init__(self, weight_names, transpose_dims)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2561,
    "qualname": "PackWeightMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, module)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2566,
    "qualname": "LazyValue.__init__",
    "signature": "def __init__(self, creator: Callable)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2571,
    "qualname": "LazyValue.value",
    "signature": "def value(self)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2578,
    "qualname": "dynamic_import",
    "signature": "def dynamic_import(func_path: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2591,
    "qualname": "gc_object_counts",
    "signature": "def gc_object_counts()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2600,
    "qualname": "configure_gc_warning",
    "signature": "def configure_gc_warning(warn_threshold_secs)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2621,
    "qualname": "freeze_gc",
    "signature": "def freeze_gc(context: str)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2635,
    "qualname": "configure_gc_logger",
    "signature": "def configure_gc_logger()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2661,
    "qualname": "align",
    "signature": "def align(x: int, y: int)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2666,
    "qualname": "ceil_div",
    "signature": "def ceil_div(x: int, y: int)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2670,
    "qualname": "parse_lscpu_topology",
    "signature": "def parse_lscpu_topology()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2690,
    "qualname": "get_physical_cpus_by_numa",
    "signature": "def get_physical_cpus_by_numa()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2725,
    "qualname": "get_cpu_ids_by_node",
    "signature": "def get_cpu_ids_by_node()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2736,
    "qualname": "is_shm_available",
    "signature": "def is_shm_available(dtype, world_size, local_size)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2745,
    "qualname": "lru_cache_frozenset",
    "signature": "def lru_cache_frozenset(maxsize)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2790,
    "qualname": "apply_module_patch",
    "signature": "def apply_module_patch(target_module, target_function, wrappers)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2812,
    "qualname": "parse_module_path",
    "signature": "def parse_module_path(module_path, function_name, create_dummy)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2888,
    "qualname": "mxfp_supported",
    "signature": "def mxfp_supported()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2924,
    "qualname": "ConcurrentCounter.__init__",
    "signature": "def __init__(self, initial: int)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2934,
    "qualname": "ConcurrentCounter.value",
    "signature": "def value(self)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2947,
    "qualname": "ConcurrentCounter.__repr__",
    "signature": "def __repr__(self)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2951,
    "qualname": "ConcurrentCounter.increment",
    "signature": "async def increment(self, n: int, notify_all: bool)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2964,
    "qualname": "ConcurrentCounter.decrement",
    "signature": "async def decrement(self, n: int, notify_all: bool)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2977,
    "qualname": "ConcurrentCounter.wait_for",
    "signature": "async def wait_for(self, condition: Callable[[int], bool])"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 2991,
    "qualname": "ConcurrentCounter.wait_for_zero",
    "signature": "async def wait_for_zero(self)"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 3002,
    "qualname": "is_triton_kernels_available",
    "signature": "def is_triton_kernels_available()"
  },
  {
    "module": "srt.utils",
    "file": "python/sglang/srt/utils.py",
    "line": 3006,
    "qualname": "check_cuda_result",
    "signature": "def check_cuda_result(raw_output)"
  },
  {
    "module": "srt.warmup",
    "file": "python/sglang/srt/warmup.py",
    "line": 16,
    "qualname": "warmup",
    "signature": "def warmup(name: str)"
  },
  {
    "module": "srt.warmup",
    "file": "python/sglang/srt/warmup.py",
    "line": 24,
    "qualname": "execute_warmups",
    "signature": "async def execute_warmups(disaggregation_mode: str, warmup_names: List[str], tokenizer_manager: TokenizerManager)"
  },
  {
    "module": "srt.warmup",
    "file": "python/sglang/srt/warmup.py",
    "line": 38,
    "qualname": "voice_chat",
    "signature": "async def voice_chat(disaggregation_mode: str, tokenizer_manager: TokenizerManager)"
  },
  {
    "module": "srt.hf_transformers_utils",
    "file": "python/sglang/srt/hf_transformers_utils.py",
    "line": 66,
    "qualname": "download_from_hf",
    "signature": "def download_from_hf(model_path: str, allow_patterns: Optional[Union[str, list]])"
  },
  {
    "module": "srt.hf_transformers_utils",
    "file": "python/sglang/srt/hf_transformers_utils.py",
    "line": 79,
    "qualname": "get_hf_text_config",
    "signature": "def get_hf_text_config(config: PretrainedConfig)"
  },
  {
    "module": "srt.hf_transformers_utils",
    "file": "python/sglang/srt/hf_transformers_utils.py",
    "line": 117,
    "qualname": "get_config",
    "signature": "def get_config(model: str, trust_remote_code: bool, revision: Optional[str], model_override_args: Optional[dict])"
  },
  {
    "module": "srt.hf_transformers_utils",
    "file": "python/sglang/srt/hf_transformers_utils.py",
    "line": 186,
    "qualname": "get_generation_config",
    "signature": "def get_generation_config(model: str, trust_remote_code: bool, revision: Optional[str])"
  },
  {
    "module": "srt.hf_transformers_utils",
    "file": "python/sglang/srt/hf_transformers_utils.py",
    "line": 201,
    "qualname": "get_sparse_attention_config",
    "signature": "def get_sparse_attention_config(model: str, sparse_attention_config_filename: str)"
  },
  {
    "module": "srt.hf_transformers_utils",
    "file": "python/sglang/srt/hf_transformers_utils.py",
    "line": 233,
    "qualname": "get_context_length",
    "signature": "def get_context_length(config)"
  },
  {
    "module": "srt.hf_transformers_utils",
    "file": "python/sglang/srt/hf_transformers_utils.py",
    "line": 257,
    "qualname": "get_tokenizer",
    "signature": "def get_tokenizer(tokenizer_name: str)"
  },
  {
    "module": "srt.hf_transformers_utils",
    "file": "python/sglang/srt/hf_transformers_utils.py",
    "line": 338,
    "qualname": "get_tokenizer_from_processor",
    "signature": "def get_tokenizer_from_processor(processor)"
  },
  {
    "module": "srt.hf_transformers_utils",
    "file": "python/sglang/srt/hf_transformers_utils.py",
    "line": 344,
    "qualname": "get_processor",
    "signature": "def get_processor(tokenizer_name: str)"
  },
  {
    "module": "srt.hf_transformers_utils",
    "file": "python/sglang/srt/hf_transformers_utils.py",
    "line": 410,
    "qualname": "attach_additional_stop_token_ids",
    "signature": "def attach_additional_stop_token_ids(tokenizer)"
  },
  {
    "module": "srt.hf_transformers_utils",
    "file": "python/sglang/srt/hf_transformers_utils.py",
    "line": 420,
    "qualname": "check_gguf_file",
    "signature": "def check_gguf_file(model: Union[str, os.PathLike])"
  },
  {
    "module": "srt.server_args",
    "file": "python/sglang/srt/server_args.py",
    "line": 310,
    "qualname": "ServerArgs.__post_init__",
    "signature": "def __post_init__(self)"
  },
  {
    "module": "srt.server_args",
    "file": "python/sglang/srt/server_args.py",
    "line": 731,
    "qualname": "ServerArgs.add_cli_args",
    "signature": "def add_cli_args(parser: argparse.ArgumentParser)"
  },
  {
    "module": "srt.server_args",
    "file": "python/sglang/srt/server_args.py",
    "line": 2071,
    "qualname": "ServerArgs.from_cli_args",
    "signature": "def from_cli_args(cls, args: argparse.Namespace)"
  },
  {
    "module": "srt.server_args",
    "file": "python/sglang/srt/server_args.py",
    "line": 2079,
    "qualname": "ServerArgs.url",
    "signature": "def url(self)"
  },
  {
    "module": "srt.server_args",
    "file": "python/sglang/srt/server_args.py",
    "line": 2085,
    "qualname": "ServerArgs.get_hf_config",
    "signature": "def get_hf_config(self)"
  },
  {
    "module": "srt.server_args",
    "file": "python/sglang/srt/server_args.py",
    "line": 2096,
    "qualname": "ServerArgs.check_server_args",
    "signature": "def check_server_args(self)"
  },
  {
    "module": "srt.server_args",
    "file": "python/sglang/srt/server_args.py",
    "line": 2137,
    "qualname": "ServerArgs.check_lora_server_args",
    "signature": "def check_lora_server_args(self)"
  },
  {
    "module": "srt.server_args",
    "file": "python/sglang/srt/server_args.py",
    "line": 2220,
    "qualname": "ServerArgs.validate_disagg_tp_size",
    "signature": "def validate_disagg_tp_size(self, prefill_tp: int, decode_tp: int)"
  },
  {
    "module": "srt.server_args",
    "file": "python/sglang/srt/server_args.py",
    "line": 2228,
    "qualname": "ServerArgs.model_specific_adjustments",
    "signature": "def model_specific_adjustments(self)"
  },
  {
    "module": "srt.server_args",
    "file": "python/sglang/srt/server_args.py",
    "line": 2302,
    "qualname": "ServerArgs.adjust_mem_fraction_for_vlm",
    "signature": "def adjust_mem_fraction_for_vlm(self, model_config)"
  },
  {
    "module": "srt.server_args",
    "file": "python/sglang/srt/server_args.py",
    "line": 2341,
    "qualname": "prepare_server_args",
    "signature": "def prepare_server_args(argv: List[str])"
  },
  {
    "module": "srt.server_args",
    "file": "python/sglang/srt/server_args.py",
    "line": 2381,
    "qualname": "PortArgs.init_new",
    "signature": "def init_new(server_args, dp_rank: Optional[int])"
  },
  {
    "module": "srt.server_args",
    "file": "python/sglang/srt/server_args.py",
    "line": 2440,
    "qualname": "LoRAPathAction.__call__",
    "signature": "def __call__(self, parser, namespace, values, option_string)"
  },
  {
    "module": "srt.server_args",
    "file": "python/sglang/srt/server_args.py",
    "line": 2460,
    "qualname": "DeprecatedAction.__init__",
    "signature": "def __init__(self, option_strings, dest, nargs)"
  },
  {
    "module": "srt.server_args",
    "file": "python/sglang/srt/server_args.py",
    "line": 2465,
    "qualname": "DeprecatedAction.__call__",
    "signature": "def __call__(self, parser, namespace, values, option_string)"
  },
  {
    "module": "srt.server_args",
    "file": "python/sglang/srt/server_args.py",
    "line": 2469,
    "qualname": "print_deprecated_warning",
    "signature": "def print_deprecated_warning(message: str)"
  },
  {
    "module": "srt.server_args",
    "file": "python/sglang/srt/server_args.py",
    "line": 2473,
    "qualname": "auto_choose_speculative_params",
    "signature": "def auto_choose_speculative_params(self: ServerArgs)"
  },
  {
    "module": "srt.configs.chatglm",
    "file": "python/sglang/srt/configs/chatglm.py",
    "line": 19,
    "qualname": "ChatGLMConfig.__init__",
    "signature": "def __init__(self, num_layers, padded_vocab_size, hidden_size, ffn_hidden_size, kv_channels, num_attention_heads, seq_length, hidden_dropout, attention_dropout, layernorm_epsilon, rmsnorm, apply_residual_connection_post_layernorm, post_layer_norm, add_bias_linear, add_qkv_bias, interleaved_qkv, bias_dropout_fusion, multi_query_attention, multi_query_group_num, apply_query_key_layer_scaling, attention_softmax_in_fp32, fp32_residual_connection, quantization_bit, pre_seq_len, prefix_projection)"
  },
  {
    "module": "srt.configs.dbrx",
    "file": "python/sglang/srt/configs/dbrx.py",
    "line": 34,
    "qualname": "DbrxAttentionConfig.__init__",
    "signature": "def __init__(self, attn_pdrop: float, clip_qkv: Optional[float], kv_n_heads: int, rope_theta: float)"
  },
  {
    "module": "srt.configs.dbrx",
    "file": "python/sglang/srt/configs/dbrx.py",
    "line": 55,
    "qualname": "DbrxAttentionConfig.from_pretrained",
    "signature": "def from_pretrained(cls, pretrained_model_name_or_path: str)"
  },
  {
    "module": "srt.configs.dbrx",
    "file": "python/sglang/srt/configs/dbrx.py",
    "line": 106,
    "qualname": "DbrxFFNConfig.__init__",
    "signature": "def __init__(self, ffn_act_fn: Optional[dict], ffn_hidden_size: int, moe_num_experts: int, moe_top_k: int, moe_jitter_eps: Optional[float], moe_loss_weight: float, moe_normalize_expert_weights: Optional[float], uniform_expert_assignment: bool)"
  },
  {
    "module": "srt.configs.dbrx",
    "file": "python/sglang/srt/configs/dbrx.py",
    "line": 137,
    "qualname": "DbrxFFNConfig.from_pretrained",
    "signature": "def from_pretrained(cls, pretrained_model_name_or_path: str)"
  },
  {
    "module": "srt.configs.dbrx",
    "file": "python/sglang/srt/configs/dbrx.py",
    "line": 229,
    "qualname": "DbrxConfig.__init__",
    "signature": "def __init__(self, d_model: int, n_heads: int, n_layers: int, max_seq_len: int, vocab_size: int, resid_pdrop: float, emb_pdrop: float, attn_config: Optional[DbrxAttentionConfig], ffn_config: Optional[DbrxFFNConfig], use_cache: bool, initializer_range: float, output_router_logits: bool, router_aux_loss_coef: float)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 16,
    "qualname": "select_best_resolution",
    "signature": "def select_best_resolution(image_size, candidate_resolutions)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 45,
    "qualname": "DictOutput.items",
    "signature": "def items(self)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 48,
    "qualname": "DictOutput.keys",
    "signature": "def keys(self)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 51,
    "qualname": "DictOutput.__getitem__",
    "signature": "def __getitem__(self, item)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 54,
    "qualname": "DictOutput.__contains__",
    "signature": "def __contains__(self, key)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 57,
    "qualname": "DictOutput.__setitem__",
    "signature": "def __setitem__(self, key, value)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 71,
    "qualname": "VLChatProcessorOutput.__len__",
    "signature": "def __len__(self)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 76,
    "qualname": "ImageTransform.__init__",
    "signature": "def __init__(self, mean: Optional[Tuple[float, float, float]], std: Optional[Tuple[float, float, float]], normalize: bool)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 103,
    "qualname": "ImageTransform.__call__",
    "signature": "def __call__(self, pil_img: Image.Image)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 112,
    "qualname": "DeepseekVLV2Processor.__init__",
    "signature": "def __init__(self, tokenizer: LlamaTokenizerFast, candidate_resolutions: Tuple[Tuple[int, int]], patch_size: int, downsample_ratio: int, image_mean: Tuple[float, float, float], image_std: Tuple[float, float, float], normalize: bool, image_token: str, pad_token: str, add_special_token: bool, sft_format: str, mask_prompt: bool, ignore_id: int)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 180,
    "qualname": "DeepseekVLV2Processor.format_messages_v2",
    "signature": "def format_messages_v2(self, messages, pil_images, max_req_input_len)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 222,
    "qualname": "DeepseekVLV2Processor.bos_id",
    "signature": "def bos_id(self)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 226,
    "qualname": "DeepseekVLV2Processor.eos_id",
    "signature": "def eos_id(self)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 230,
    "qualname": "DeepseekVLV2Processor.pad_id",
    "signature": "def pad_id(self)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 233,
    "qualname": "DeepseekVLV2Processor.encode",
    "signature": "def encode(self, text: str, bos: bool, eos: bool)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 243,
    "qualname": "DeepseekVLV2Processor.decode",
    "signature": "def decode(self, t: List[int])"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 246,
    "qualname": "DeepseekVLV2Processor.process_one",
    "signature": "def process_one(self, prompt: str, conversations: List[Dict[str, str]], images: List[Image.Image], apply_sft_format: bool, inference_mode: bool, system_prompt: str, max_req_input_len: int)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 334,
    "qualname": "DeepseekVLV2Processor.__call__",
    "signature": "def __call__(self)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 358,
    "qualname": "DeepseekVLV2Processor.find_all_indices",
    "signature": "def find_all_indices(self, messages, target_value)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 365,
    "qualname": "DeepseekVLV2Processor.tokenize_with_images",
    "signature": "def tokenize_with_images(self, conversation: str, images: List[Image.Image], bos: bool, eos: bool, cropping: bool, max_req_input_len: int)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 488,
    "qualname": "DeepseekVL2VisionEncoderConfig.__init__",
    "signature": "def __init__(self, model_name: str, image_size: int, patch_size: int, width: int, layers: int, heads: int, mlp_ratio: int, global_pool: str, ignore_head: bool, class_token: bool, num_classes: int, use_checkpoint: bool)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 530,
    "qualname": "DeepseekVL2MlpProjectorConfig.__init__",
    "signature": "def __init__(self, projector_type: str, input_dim: int, n_embed: int, depth: int, mlp_ratio: int, downsample_ratio: int)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 555,
    "qualname": "DeepseekV2Config.__init__",
    "signature": "def __init__(self, vocab_size, hidden_size, intermediate_size, moe_intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, n_shared_experts, n_routed_experts, ep_size, routed_scaling_factor, kv_lora_rank, q_lora_rank, qk_rope_head_dim, v_head_dim, qk_nope_head_dim, topk_method, n_group, topk_group, num_experts_per_tok, moe_layer_freq, first_k_dense_replace, norm_topk_prob, scoring_func, aux_loss_alpha, seq_aux, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, pretraining_tp, tie_word_embeddings, rope_theta, rope_scaling, attention_bias, attention_dropout, use_mla)"
  },
  {
    "module": "srt.configs.deepseekvl2",
    "file": "python/sglang/srt/configs/deepseekvl2.py",
    "line": 661,
    "qualname": "DeepseekVL2Config.__init__",
    "signature": "def __init__(self, tile_tag: str, global_view_pos: str, candidate_resolutions: Tuple[Tuple[int, int]])"
  },
  {
    "module": "srt.configs.device_config",
    "file": "python/sglang/srt/configs/device_config.py",
    "line": 12,
    "qualname": "DeviceConfig.__init__",
    "signature": "def __init__(self, device: str)"
  },
  {
    "module": "srt.configs.exaone",
    "file": "python/sglang/srt/configs/exaone.py",
    "line": 143,
    "qualname": "ExaoneConfig.__init__",
    "signature": "def __init__(self, vocab_size, max_position_embeddings, hidden_size, num_layers, num_attention_heads, num_key_value_heads, intermediate_size, activation_function, rope_theta, rope_scaling, embed_dropout, attention_dropout, layer_norm_epsilon, initializer_range, use_cache, bos_token_id, eos_token_id, tie_word_embeddings)"
  },
  {
    "module": "srt.configs.internvl",
    "file": "python/sglang/srt/configs/internvl.py",
    "line": 78,
    "qualname": "InternLM2Config.__init__",
    "signature": "def __init__(self, vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, tie_word_embeddings, bias, rope_theta, rope_scaling, attn_implementation)"
  },
  {
    "module": "srt.configs.internvl",
    "file": "python/sglang/srt/configs/internvl.py",
    "line": 210,
    "qualname": "InternVisionConfig.__init__",
    "signature": "def __init__(self, num_channels, patch_size, image_size, qkv_bias, hidden_size, num_attention_heads, intermediate_size, qk_normalization, num_hidden_layers, use_flash_attn, hidden_act, layer_norm_eps, dropout, drop_path_rate, attention_dropout, initializer_range, initializer_factor)"
  },
  {
    "module": "srt.configs.internvl",
    "file": "python/sglang/srt/configs/internvl.py",
    "line": 252,
    "qualname": "InternVisionConfig.from_pretrained",
    "signature": "def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike])"
  },
  {
    "module": "srt.configs.internvl",
    "file": "python/sglang/srt/configs/internvl.py",
    "line": 279,
    "qualname": "InternVLChatConfig.__init__",
    "signature": "def __init__(self, vision_config, llm_config, use_backbone_lora, use_llm_lora, pad2square, select_layer, force_image_size, downsample_ratio, template, dynamic_image_size, use_thumbnail, ps_version, min_dynamic_patch, max_dynamic_patch)"
  },
  {
    "module": "srt.configs.internvl",
    "file": "python/sglang/srt/configs/internvl.py",
    "line": 345,
    "qualname": "InternVLChatConfig.to_dict",
    "signature": "def to_dict(self)"
  },
  {
    "module": "srt.configs.internvl",
    "file": "python/sglang/srt/configs/internvl.py",
    "line": 494,
    "qualname": "InternLM2Tokenizer.__init__",
    "signature": "def __init__(self, vocab_file, unk_token, bos_token, eos_token, pad_token, sp_model_kwargs: Optional[Dict[str, Any]], add_bos_token, add_eos_token, decode_with_prefix_space, clean_up_tokenization_spaces)"
  },
  {
    "module": "srt.configs.internvl",
    "file": "python/sglang/srt/configs/internvl.py",
    "line": 527,
    "qualname": "InternLM2Tokenizer.no_prefix_space_tokens",
    "signature": "def no_prefix_space_tokens(self)"
  },
  {
    "module": "srt.configs.internvl",
    "file": "python/sglang/srt/configs/internvl.py",
    "line": 536,
    "qualname": "InternLM2Tokenizer.vocab_size",
    "signature": "def vocab_size(self)"
  },
  {
    "module": "srt.configs.internvl",
    "file": "python/sglang/srt/configs/internvl.py",
    "line": 541,
    "qualname": "InternLM2Tokenizer.bos_token_id",
    "signature": "def bos_token_id(self)"
  },
  {
    "module": "srt.configs.internvl",
    "file": "python/sglang/srt/configs/internvl.py",
    "line": 545,
    "qualname": "InternLM2Tokenizer.eos_token_id",
    "signature": "def eos_token_id(self)"
  },
  {
    "module": "srt.configs.internvl",
    "file": "python/sglang/srt/configs/internvl.py",
    "line": 548,
    "qualname": "InternLM2Tokenizer.get_vocab",
    "signature": "def get_vocab(self)"
  },
  {
    "module": "srt.configs.internvl",
    "file": "python/sglang/srt/configs/internvl.py",
    "line": 573,
    "qualname": "InternLM2Tokenizer.convert_tokens_to_string",
    "signature": "def convert_tokens_to_string(self, tokens)"
  },
  {
    "module": "srt.configs.internvl",
    "file": "python/sglang/srt/configs/internvl.py",
    "line": 594,
    "qualname": "InternLM2Tokenizer.save_vocabulary",
    "signature": "def save_vocabulary(self, save_directory, filename_prefix: Optional[str])"
  },
  {
    "module": "srt.configs.internvl",
    "file": "python/sglang/srt/configs/internvl.py",
    "line": 627,
    "qualname": "InternLM2Tokenizer.build_inputs_with_special_tokens",
    "signature": "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1)"
  },
  {
    "module": "srt.configs.internvl",
    "file": "python/sglang/srt/configs/internvl.py",
    "line": 643,
    "qualname": "InternLM2Tokenizer.get_special_tokens_mask",
    "signature": "def get_special_tokens_mask(self, token_ids_0: List[int], token_ids_1: Optional[List[int]], already_has_special_tokens: bool)"
  },
  {
    "module": "srt.configs.internvl",
    "file": "python/sglang/srt/configs/internvl.py",
    "line": 675,
    "qualname": "InternLM2Tokenizer.create_token_type_ids_from_sequences",
    "signature": "def create_token_type_ids_from_sequences(self, token_ids_0: List[int], token_ids_1: Optional[List[int]])"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 26,
    "qualname": "DictToObject.__init__",
    "signature": "def __init__(self, dictionary)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 40,
    "qualname": "VisionConfig.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 55,
    "qualname": "GenAlignerConfig.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 70,
    "qualname": "GenHeadConfig.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 85,
    "qualname": "AlignerConfig.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 100,
    "qualname": "GenVisionConfig.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 135,
    "qualname": "MultiModalityConfig.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 162,
    "qualname": "VLMImageProcessor.__init__",
    "signature": "def __init__(self, image_size: int, min_size: int, image_mean: Union[Tuple[float, float, float], List[float]], image_std: Union[Tuple[float, float, float], List[float]], rescale_factor: float, do_normalize: bool)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 194,
    "qualname": "VLMImageProcessor.resize",
    "signature": "def resize(self, pil_img: Image)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 249,
    "qualname": "VLMImageProcessor.preprocess",
    "signature": "def preprocess(self, images, return_tensors: str)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 282,
    "qualname": "VLMImageProcessor.default_shape",
    "signature": "def default_shape(self)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 287,
    "qualname": "DictOutput.items",
    "signature": "def items(self)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 290,
    "qualname": "DictOutput.keys",
    "signature": "def keys(self)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 293,
    "qualname": "DictOutput.__getitem__",
    "signature": "def __getitem__(self, item)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 296,
    "qualname": "DictOutput.__contains__",
    "signature": "def __contains__(self, key)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 299,
    "qualname": "DictOutput.__setitem__",
    "signature": "def __setitem__(self, key, value)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 310,
    "qualname": "VLChatProcessorOutput.__len__",
    "signature": "def __len__(self)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 332,
    "qualname": "VLChatProcessor.__init__",
    "signature": "def __init__(self, image_processor: VLMImageProcessor, tokenizer: LlamaTokenizerFast, image_tag: str, image_start_tag: str, image_end_tag: str, pad_tag: str, num_image_tokens: int, add_special_token: bool, sft_format: str, mask_prompt: bool, ignore_id: int)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 374,
    "qualname": "VLChatProcessor.image_token",
    "signature": "def image_token(self)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 378,
    "qualname": "VLChatProcessor.image_id",
    "signature": "def image_id(self)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 383,
    "qualname": "VLChatProcessor.image_start_id",
    "signature": "def image_start_id(self)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 388,
    "qualname": "VLChatProcessor.image_end_id",
    "signature": "def image_end_id(self)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 393,
    "qualname": "VLChatProcessor.image_start_token",
    "signature": "def image_start_token(self)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 397,
    "qualname": "VLChatProcessor.image_end_token",
    "signature": "def image_end_token(self)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 401,
    "qualname": "VLChatProcessor.pad_id",
    "signature": "def pad_id(self)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 405,
    "qualname": "VLChatProcessor.add_image_token",
    "signature": "def add_image_token(self, image_indices: List[int], input_ids: torch.LongTensor)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 450,
    "qualname": "VLChatProcessor.process_one",
    "signature": "def process_one(self, prompt: str, images: List[Image])"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 497,
    "qualname": "VLChatProcessor.__call__",
    "signature": "def __call__(self)"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 532,
    "qualname": "VLChatProcessor.batchify",
    "signature": "def batchify(self, prepare_list: List[VLChatProcessorOutput])"
  },
  {
    "module": "srt.configs.janus_pro",
    "file": "python/sglang/srt/configs/janus_pro.py",
    "line": 605,
    "qualname": "VLMImageProcessorConfig.__init__",
    "signature": "def __init__(self, image_size: int, min_size: int, image_mean: Union[Tuple[float, float, float], List[float]], image_std: Union[Tuple[float, float, float], List[float]], rescale_factor: float, do_normalize: bool)"
  },
  {
    "module": "srt.configs.kimi_vl",
    "file": "python/sglang/srt/configs/kimi_vl.py",
    "line": 14,
    "qualname": "KimiVLConfig.__init__",
    "signature": "def __init__(self, vision_config: Optional[Union[dict, MoonViTConfig]], text_config: Optional[Union[dict, DeepseekV2Config]], ignore_index: int, media_placeholder_token_id: int, pad_token_id: int)"
  },
  {
    "module": "srt.configs.kimi_vl_moonvit",
    "file": "python/sglang/srt/configs/kimi_vl_moonvit.py",
    "line": 9,
    "qualname": "MoonViTConfig.__init__",
    "signature": "def __init__(self, patch_size: int, init_pos_emb_height: int, init_pos_emb_width: int, num_attention_heads: int, num_hidden_layers: int, hidden_size: int, intermediate_size: int, merge_kernel_size: tuple[int, int])"
  },
  {
    "module": "srt.configs.load_config",
    "file": "python/sglang/srt/configs/load_config.py",
    "line": 57,
    "qualname": "LoadConfig.__post_init__",
    "signature": "def __post_init__(self)"
  },
  {
    "module": "srt.configs.step3_vl",
    "file": "python/sglang/srt/configs/step3_vl.py",
    "line": 9,
    "qualname": "Step3VisionEncoderConfig.__init__",
    "signature": "def __init__(self, hidden_size, intermediate_size, output_hidden_size, num_hidden_layers, num_attention_heads, num_channels, image_size, patch_size, hidden_act, layer_norm_eps)"
  },
  {
    "module": "srt.configs.step3_vl",
    "file": "python/sglang/srt/configs/step3_vl.py",
    "line": 40,
    "qualname": "Step3TextConfig.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, num_attention_heads: int, num_attention_groups: int, num_hidden_layers: int, max_seq_len: int, vocab_size: int, rms_norm_eps: float, moe_intermediate_size: int, moe_num_experts: int, moe_top_k: int, rope_theta: float, rope_scaling: Optional[dict[str, Any]], max_position_embedding: int, share_expert_dim: int, share_q_dim: int, head_dim: int, norm_expert_weight: bool, moe_layers_enum: tuple[int])"
  },
  {
    "module": "srt.configs.step3_vl",
    "file": "python/sglang/srt/configs/step3_vl.py",
    "line": 146,
    "qualname": "Step3VLConfig.__init__",
    "signature": "def __init__(self, vision_config: Optional[Union[dict, Step3VisionEncoderConfig]], text_config: Optional[Union[dict, Step3TextConfig]], understand_projector_stride: int, projector_bias: bool, image_token_id: int)"
  },
  {
    "module": "srt.configs.update_config",
    "file": "python/sglang/srt/configs/update_config.py",
    "line": 13,
    "qualname": "may_get_weight_block_size",
    "signature": "def may_get_weight_block_size(model_config, load_config)"
  },
  {
    "module": "srt.configs.update_config",
    "file": "python/sglang/srt/configs/update_config.py",
    "line": 29,
    "qualname": "get_moe_padding_size",
    "signature": "def get_moe_padding_size(weight_block_size)"
  },
  {
    "module": "srt.configs.update_config",
    "file": "python/sglang/srt/configs/update_config.py",
    "line": 44,
    "qualname": "get_num_heads_padding_size",
    "signature": "def get_num_heads_padding_size(tp_size, weight_block_size)"
  },
  {
    "module": "srt.configs.update_config",
    "file": "python/sglang/srt/configs/update_config.py",
    "line": 51,
    "qualname": "update_intermediate_size",
    "signature": "def update_intermediate_size(model_config, attr_name, intermediate_padding_size)"
  },
  {
    "module": "srt.configs.update_config",
    "file": "python/sglang/srt/configs/update_config.py",
    "line": 74,
    "qualname": "adjust_config_with_unaligned_cpu_tp",
    "signature": "def adjust_config_with_unaligned_cpu_tp(model_config: ModelConfig, load_config: LoadConfig, tp_size: int)"
  },
  {
    "module": "srt.configs.utils",
    "file": "python/sglang/srt/configs/utils.py",
    "line": 12,
    "qualname": "register_image_processor",
    "signature": "def register_image_processor(config: Type[PretrainedConfig], image_processor: Type[BaseImageProcessor])"
  },
  {
    "module": "srt.configs.utils",
    "file": "python/sglang/srt/configs/utils.py",
    "line": 21,
    "qualname": "register_processor",
    "signature": "def register_processor(config: Type[PretrainedConfig], processor: Type[ProcessorMixin])"
  },
  {
    "module": "srt.configs.model_config",
    "file": "python/sglang/srt/configs/model_config.py",
    "line": 52,
    "qualname": "ModelConfig.__init__",
    "signature": "def __init__(self, model_path: str, trust_remote_code: bool, revision: Optional[str], context_length: Optional[int], model_override_args: str, is_embedding: Optional[bool], enable_multimodal: Optional[bool], dtype: str, quantization: Optional[str], override_config_file: Optional[str], is_draft_model: bool, hybrid_kvcache_ratio: Optional[float], model_impl: Union[str, ModelImpl])"
  },
  {
    "module": "srt.configs.model_config",
    "file": "python/sglang/srt/configs/model_config.py",
    "line": 293,
    "qualname": "ModelConfig.from_server_args",
    "signature": "def from_server_args(server_args: ServerArgs, model_path: str)"
  },
  {
    "module": "srt.configs.model_config",
    "file": "python/sglang/srt/configs/model_config.py",
    "line": 309,
    "qualname": "ModelConfig.get_total_num_attention_heads",
    "signature": "def get_total_num_attention_heads(self)"
  },
  {
    "module": "srt.configs.model_config",
    "file": "python/sglang/srt/configs/model_config.py",
    "line": 312,
    "qualname": "ModelConfig.get_num_attention_heads",
    "signature": "def get_num_attention_heads(self, tensor_parallel_size)"
  },
  {
    "module": "srt.configs.model_config",
    "file": "python/sglang/srt/configs/model_config.py",
    "line": 317,
    "qualname": "ModelConfig.get_total_num_kv_heads",
    "signature": "def get_total_num_kv_heads(self)"
  },
  {
    "module": "srt.configs.model_config",
    "file": "python/sglang/srt/configs/model_config.py",
    "line": 380,
    "qualname": "ModelConfig.get_num_kv_heads",
    "signature": "def get_num_kv_heads(self, tensor_parallel_size)"
  },
  {
    "module": "srt.configs.model_config",
    "file": "python/sglang/srt/configs/model_config.py",
    "line": 533,
    "qualname": "ModelConfig.get_hf_eos_token_id",
    "signature": "def get_hf_eos_token_id(self)"
  },
  {
    "module": "srt.configs.model_config",
    "file": "python/sglang/srt/configs/model_config.py",
    "line": 553,
    "qualname": "ModelConfig.maybe_pull_model_tokenizer_from_remote",
    "signature": "def maybe_pull_model_tokenizer_from_remote(self)"
  },
  {
    "module": "srt.configs.model_config",
    "file": "python/sglang/srt/configs/model_config.py",
    "line": 647,
    "qualname": "is_generation_model",
    "signature": "def is_generation_model(model_architectures: List[str], is_embedding: bool)"
  },
  {
    "module": "srt.configs.model_config",
    "file": "python/sglang/srt/configs/model_config.py",
    "line": 705,
    "qualname": "is_multimodal_model",
    "signature": "def is_multimodal_model(model_architectures: List[str])"
  },
  {
    "module": "srt.configs.model_config",
    "file": "python/sglang/srt/configs/model_config.py",
    "line": 715,
    "qualname": "is_multimodal_gen_model",
    "signature": "def is_multimodal_gen_model(model_architectures: List[str])"
  },
  {
    "module": "srt.configs.model_config",
    "file": "python/sglang/srt/configs/model_config.py",
    "line": 719,
    "qualname": "is_image_gen_model",
    "signature": "def is_image_gen_model(model_architectures: List[str])"
  },
  {
    "module": "srt.configs.model_config",
    "file": "python/sglang/srt/configs/model_config.py",
    "line": 723,
    "qualname": "is_audio_model",
    "signature": "def is_audio_model(model_architectures: List[str])"
  },
  {
    "module": "srt.configs.model_config",
    "file": "python/sglang/srt/configs/model_config.py",
    "line": 727,
    "qualname": "is_encoder_decoder_model",
    "signature": "def is_encoder_decoder_model(model_architectures: List[str])"
  },
  {
    "module": "srt.configs.model_config",
    "file": "python/sglang/srt/configs/model_config.py",
    "line": 731,
    "qualname": "is_multimodal_chunked_prefill_supported",
    "signature": "def is_multimodal_chunked_prefill_supported(model_architectures: List[str])"
  },
  {
    "module": "srt.configs.model_config",
    "file": "python/sglang/srt/configs/model_config.py",
    "line": 746,
    "qualname": "yarn_get_mscale",
    "signature": "def yarn_get_mscale(scale: float, mscale: float)"
  },
  {
    "module": "srt.configs.model_config",
    "file": "python/sglang/srt/configs/model_config.py",
    "line": 752,
    "qualname": "is_hybrid_model",
    "signature": "def is_hybrid_model(model_architectures: List[str], hybrid_kvcache_ratio: Optional[float], context_length: Optional[int], attention_chunk_size: Optional[int])"
  },
  {
    "module": "srt.configs.model_config",
    "file": "python/sglang/srt/configs/model_config.py",
    "line": 770,
    "qualname": "get_hybrid_layer_ids",
    "signature": "def get_hybrid_layer_ids(model_architectures: List[str], num_hidden_layers: int)"
  },
  {
    "module": "srt.connector.s3",
    "file": "python/sglang/srt/connector/s3.py",
    "line": 29,
    "qualname": "list_files",
    "signature": "def list_files(s3, path: str, allow_pattern: Optional[list[str]], ignore_pattern: Optional[list[str]])"
  },
  {
    "module": "srt.connector.s3",
    "file": "python/sglang/srt/connector/s3.py",
    "line": 71,
    "qualname": "S3Connector.__init__",
    "signature": "def __init__(self, url: str)"
  },
  {
    "module": "srt.connector.s3",
    "file": "python/sglang/srt/connector/s3.py",
    "line": 77,
    "qualname": "S3Connector.glob",
    "signature": "def glob(self, allow_pattern: Optional[list[str]])"
  },
  {
    "module": "srt.connector.s3",
    "file": "python/sglang/srt/connector/s3.py",
    "line": 83,
    "qualname": "S3Connector.pull_files",
    "signature": "def pull_files(self, allow_pattern: Optional[list[str]], ignore_pattern: Optional[list[str]])"
  },
  {
    "module": "srt.connector.s3",
    "file": "python/sglang/srt/connector/s3.py",
    "line": 109,
    "qualname": "S3Connector.weight_iterator",
    "signature": "def weight_iterator(self, rank: int)"
  },
  {
    "module": "srt.connector.s3",
    "file": "python/sglang/srt/connector/s3.py",
    "line": 120,
    "qualname": "S3Connector.close",
    "signature": "def close(self)"
  },
  {
    "module": "srt.connector.utils",
    "file": "python/sglang/srt/connector/utils.py",
    "line": 11,
    "qualname": "parse_model_name",
    "signature": "def parse_model_name(url: str)"
  },
  {
    "module": "srt.connector.utils",
    "file": "python/sglang/srt/connector/utils.py",
    "line": 20,
    "qualname": "pull_files_from_db",
    "signature": "def pull_files_from_db(connector: BaseConnector, model_name: str, allow_pattern: Optional[list[str]], ignore_pattern: Optional[list[str]])"
  },
  {
    "module": "srt.connector.__init__",
    "file": "python/sglang/srt/connector/__init__.py",
    "line": 23,
    "qualname": "create_remote_connector",
    "signature": "def create_remote_connector(url, device)"
  },
  {
    "module": "srt.connector.__init__",
    "file": "python/sglang/srt/connector/__init__.py",
    "line": 33,
    "qualname": "get_connector_type",
    "signature": "def get_connector_type(client: BaseConnector)"
  },
  {
    "module": "srt.connector.base_connector",
    "file": "python/sglang/srt/connector/base_connector.py",
    "line": 23,
    "qualname": "BaseConnector.__init__",
    "signature": "def __init__(self, url: str, device: torch.device)"
  },
  {
    "module": "srt.connector.base_connector",
    "file": "python/sglang/srt/connector/base_connector.py",
    "line": 32,
    "qualname": "BaseConnector.get_local_dir",
    "signature": "def get_local_dir(self)"
  },
  {
    "module": "srt.connector.base_connector",
    "file": "python/sglang/srt/connector/base_connector.py",
    "line": 36,
    "qualname": "BaseConnector.weight_iterator",
    "signature": "def weight_iterator(self, rank: int)"
  },
  {
    "module": "srt.connector.base_connector",
    "file": "python/sglang/srt/connector/base_connector.py",
    "line": 42,
    "qualname": "BaseConnector.pull_files",
    "signature": "def pull_files(self, allow_pattern: Optional[List[str]], ignore_pattern: Optional[List[str]])"
  },
  {
    "module": "srt.connector.base_connector",
    "file": "python/sglang/srt/connector/base_connector.py",
    "line": 49,
    "qualname": "BaseConnector.close",
    "signature": "def close(self)"
  },
  {
    "module": "srt.connector.base_connector",
    "file": "python/sglang/srt/connector/base_connector.py",
    "line": 57,
    "qualname": "BaseConnector.__enter__",
    "signature": "def __enter__(self)"
  },
  {
    "module": "srt.connector.base_connector",
    "file": "python/sglang/srt/connector/base_connector.py",
    "line": 60,
    "qualname": "BaseConnector.__exit__",
    "signature": "def __exit__(self, exc_type, exc_value, traceback)"
  },
  {
    "module": "srt.connector.base_connector",
    "file": "python/sglang/srt/connector/base_connector.py",
    "line": 63,
    "qualname": "BaseConnector.__del__",
    "signature": "def __del__(self)"
  },
  {
    "module": "srt.connector.base_connector",
    "file": "python/sglang/srt/connector/base_connector.py",
    "line": 79,
    "qualname": "BaseKVConnector.get",
    "signature": "def get(self, key: str)"
  },
  {
    "module": "srt.connector.base_connector",
    "file": "python/sglang/srt/connector/base_connector.py",
    "line": 83,
    "qualname": "BaseKVConnector.getstr",
    "signature": "def getstr(self, key: str)"
  },
  {
    "module": "srt.connector.base_connector",
    "file": "python/sglang/srt/connector/base_connector.py",
    "line": 87,
    "qualname": "BaseKVConnector.set",
    "signature": "def set(self, key: str, obj: torch.Tensor)"
  },
  {
    "module": "srt.connector.base_connector",
    "file": "python/sglang/srt/connector/base_connector.py",
    "line": 91,
    "qualname": "BaseKVConnector.setstr",
    "signature": "def setstr(self, key: str, obj: str)"
  },
  {
    "module": "srt.connector.base_connector",
    "file": "python/sglang/srt/connector/base_connector.py",
    "line": 95,
    "qualname": "BaseKVConnector.list",
    "signature": "def list(self, prefix: str)"
  },
  {
    "module": "srt.connector.base_connector",
    "file": "python/sglang/srt/connector/base_connector.py",
    "line": 111,
    "qualname": "BaseFileConnector.glob",
    "signature": "def glob(self, allow_pattern: str)"
  },
  {
    "module": "srt.connector.redis",
    "file": "python/sglang/srt/connector/redis.py",
    "line": 18,
    "qualname": "RedisConnector.__init__",
    "signature": "def __init__(self, url: str, device: torch.device)"
  },
  {
    "module": "srt.connector.redis",
    "file": "python/sglang/srt/connector/redis.py",
    "line": 28,
    "qualname": "RedisConnector.get",
    "signature": "def get(self, key: str)"
  },
  {
    "module": "srt.connector.redis",
    "file": "python/sglang/srt/connector/redis.py",
    "line": 37,
    "qualname": "RedisConnector.getstr",
    "signature": "def getstr(self, key: str)"
  },
  {
    "module": "srt.connector.redis",
    "file": "python/sglang/srt/connector/redis.py",
    "line": 45,
    "qualname": "RedisConnector.set",
    "signature": "def set(self, key: str, tensor: torch.Tensor)"
  },
  {
    "module": "srt.connector.redis",
    "file": "python/sglang/srt/connector/redis.py",
    "line": 49,
    "qualname": "RedisConnector.setstr",
    "signature": "def setstr(self, key: str, obj: str)"
  },
  {
    "module": "srt.connector.redis",
    "file": "python/sglang/srt/connector/redis.py",
    "line": 52,
    "qualname": "RedisConnector.list",
    "signature": "def list(self, prefix: str)"
  },
  {
    "module": "srt.connector.redis",
    "file": "python/sglang/srt/connector/redis.py",
    "line": 67,
    "qualname": "RedisConnector.weight_iterator",
    "signature": "def weight_iterator(self, rank: int)"
  },
  {
    "module": "srt.connector.redis",
    "file": "python/sglang/srt/connector/redis.py",
    "line": 76,
    "qualname": "RedisConnector.pull_files",
    "signature": "def pull_files(self, allow_pattern: Optional[List[str]], ignore_pattern: Optional[List[str]])"
  },
  {
    "module": "srt.connector.redis",
    "file": "python/sglang/srt/connector/redis.py",
    "line": 83,
    "qualname": "RedisConnector.close",
    "signature": "def close(self)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 31,
    "qualname": "BaseGrammarObject.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 34,
    "qualname": "BaseGrammarObject.accept_token",
    "signature": "def accept_token(self, token: int)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 40,
    "qualname": "BaseGrammarObject.rollback",
    "signature": "def rollback(self, k: int)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 43,
    "qualname": "BaseGrammarObject.is_terminated",
    "signature": "def is_terminated(self)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 46,
    "qualname": "BaseGrammarObject.allocate_vocab_mask",
    "signature": "def allocate_vocab_mask(self, vocab_size: int, batch_size: int, device)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 51,
    "qualname": "BaseGrammarObject.fill_vocab_mask",
    "signature": "def fill_vocab_mask(self, vocab_mask: torch.Tensor, idx: int)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 55,
    "qualname": "BaseGrammarObject.move_vocab_mask",
    "signature": "def move_vocab_mask(vocab_mask: torch.Tensor, device)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 59,
    "qualname": "BaseGrammarObject.apply_vocab_mask",
    "signature": "def apply_vocab_mask(logits: torch.Tensor, vocab_mask: torch.Tensor)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 62,
    "qualname": "BaseGrammarObject.copy",
    "signature": "def copy(self)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 66,
    "qualname": "BaseGrammarObject.finished",
    "signature": "def finished(self)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 70,
    "qualname": "BaseGrammarObject.finished",
    "signature": "def finished(self, finished)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 73,
    "qualname": "BaseGrammarObject.try_jump_forward",
    "signature": "def try_jump_forward(self, tokenizer)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 83,
    "qualname": "BaseGrammarObject.jump_forward_str_state",
    "signature": "def jump_forward_str_state(self, helper: Tuple[List[int], str])"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 93,
    "qualname": "BaseGrammarObject.jump_and_retokenize",
    "signature": "def jump_and_retokenize(self, old_output_ids: List[int], new_output_ids: List[int], next_state: int)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 112,
    "qualname": "BaseGrammarBackend.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 119,
    "qualname": "BaseGrammarBackend.dispatch_fallback",
    "signature": "def dispatch_fallback(self, key_type: str, key_string: str)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 127,
    "qualname": "BaseGrammarBackend.dispatch_json",
    "signature": "def dispatch_json(self, key_string: str)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 130,
    "qualname": "BaseGrammarBackend.dispatch_regex",
    "signature": "def dispatch_regex(self, key_string: str)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 133,
    "qualname": "BaseGrammarBackend.dispatch_ebnf",
    "signature": "def dispatch_ebnf(self, key_string: str)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 136,
    "qualname": "BaseGrammarBackend.dispatch_structural_tag",
    "signature": "def dispatch_structural_tag(self, key_string: str)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 154,
    "qualname": "BaseGrammarBackend.get_cached_or_future_value",
    "signature": "def get_cached_or_future_value(self, key: Tuple[str, str])"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 163,
    "qualname": "BaseGrammarBackend.set_cache",
    "signature": "def set_cache(self, key: Tuple[str, str], value: BaseGrammarObject)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 166,
    "qualname": "BaseGrammarBackend.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.constrained.base_grammar_backend",
    "file": "python/sglang/srt/constrained/base_grammar_backend.py",
    "line": 170,
    "qualname": "create_grammar_backend",
    "signature": "def create_grammar_backend(server_args: ServerArgs, tokenizer, vocab_size: int, eos_token_ids: Optional[set])"
  },
  {
    "module": "srt.constrained.llguidance_backend",
    "file": "python/sglang/srt/constrained/llguidance_backend.py",
    "line": 41,
    "qualname": "GuidanceGrammar.__init__",
    "signature": "def __init__(self, llguidance_tokenizer: LLTokenizer, serialized_grammar: str)"
  },
  {
    "module": "srt.constrained.llguidance_backend",
    "file": "python/sglang/srt/constrained/llguidance_backend.py",
    "line": 54,
    "qualname": "GuidanceGrammar.accept_token",
    "signature": "def accept_token(self, token: int)"
  },
  {
    "module": "srt.constrained.llguidance_backend",
    "file": "python/sglang/srt/constrained/llguidance_backend.py",
    "line": 59,
    "qualname": "GuidanceGrammar.fill_vocab_mask",
    "signature": "def fill_vocab_mask(self, vocab_mask: torch.Tensor, idx: int)"
  },
  {
    "module": "srt.constrained.llguidance_backend",
    "file": "python/sglang/srt/constrained/llguidance_backend.py",
    "line": 65,
    "qualname": "GuidanceGrammar.allocate_vocab_mask",
    "signature": "def allocate_vocab_mask(self, vocab_size: int, batch_size: int, device)"
  },
  {
    "module": "srt.constrained.llguidance_backend",
    "file": "python/sglang/srt/constrained/llguidance_backend.py",
    "line": 80,
    "qualname": "GuidanceGrammar.move_vocab_mask",
    "signature": "def move_vocab_mask(vocab_mask: torch.Tensor, device)"
  },
  {
    "module": "srt.constrained.llguidance_backend",
    "file": "python/sglang/srt/constrained/llguidance_backend.py",
    "line": 84,
    "qualname": "GuidanceGrammar.apply_vocab_mask",
    "signature": "def apply_vocab_mask(logits: torch.Tensor, vocab_mask: torch.Tensor)"
  },
  {
    "module": "srt.constrained.llguidance_backend",
    "file": "python/sglang/srt/constrained/llguidance_backend.py",
    "line": 87,
    "qualname": "GuidanceGrammar.copy",
    "signature": "def copy(self)"
  },
  {
    "module": "srt.constrained.llguidance_backend",
    "file": "python/sglang/srt/constrained/llguidance_backend.py",
    "line": 93,
    "qualname": "GuidanceGrammar.try_jump_forward",
    "signature": "def try_jump_forward(self, tokenizer)"
  },
  {
    "module": "srt.constrained.llguidance_backend",
    "file": "python/sglang/srt/constrained/llguidance_backend.py",
    "line": 100,
    "qualname": "GuidanceGrammar.jump_forward_str_state",
    "signature": "def jump_forward_str_state(self, helper: Tuple[List[int], str])"
  },
  {
    "module": "srt.constrained.llguidance_backend",
    "file": "python/sglang/srt/constrained/llguidance_backend.py",
    "line": 103,
    "qualname": "GuidanceGrammar.jump_and_retokenize",
    "signature": "def jump_and_retokenize(self, old_output_ids: List[int], new_output_ids: List[int], next_state: int)"
  },
  {
    "module": "srt.constrained.llguidance_backend",
    "file": "python/sglang/srt/constrained/llguidance_backend.py",
    "line": 111,
    "qualname": "GuidanceBackend.__init__",
    "signature": "def __init__(self, tokenizer, whitespace_pattern: Optional[str], n_vocab: Optional[int])"
  },
  {
    "module": "srt.constrained.llguidance_backend",
    "file": "python/sglang/srt/constrained/llguidance_backend.py",
    "line": 133,
    "qualname": "GuidanceBackend.dispatch_json",
    "signature": "def dispatch_json(self, key_string: str)"
  },
  {
    "module": "srt.constrained.llguidance_backend",
    "file": "python/sglang/srt/constrained/llguidance_backend.py",
    "line": 146,
    "qualname": "GuidanceBackend.dispatch_regex",
    "signature": "def dispatch_regex(self, key_string: str)"
  },
  {
    "module": "srt.constrained.llguidance_backend",
    "file": "python/sglang/srt/constrained/llguidance_backend.py",
    "line": 150,
    "qualname": "GuidanceBackend.dispatch_ebnf",
    "signature": "def dispatch_ebnf(self, key_string: str)"
  },
  {
    "module": "srt.constrained.llguidance_backend",
    "file": "python/sglang/srt/constrained/llguidance_backend.py",
    "line": 158,
    "qualname": "GuidanceBackend.dispatch_structural_tag",
    "signature": "def dispatch_structural_tag(self, key_string: str)"
  },
  {
    "module": "srt.constrained.outlines_backend",
    "file": "python/sglang/srt/constrained/outlines_backend.py",
    "line": 43,
    "qualname": "OutlinesGrammar.__init__",
    "signature": "def __init__(self, guide: RegexGuide, jump_forward_map: Union[OutlinesJumpForwardMap, None])"
  },
  {
    "module": "srt.constrained.outlines_backend",
    "file": "python/sglang/srt/constrained/outlines_backend.py",
    "line": 54,
    "qualname": "OutlinesGrammar.accept_token",
    "signature": "def accept_token(self, token: int)"
  },
  {
    "module": "srt.constrained.outlines_backend",
    "file": "python/sglang/srt/constrained/outlines_backend.py",
    "line": 57,
    "qualname": "OutlinesGrammar.allocate_vocab_mask",
    "signature": "def allocate_vocab_mask(self, vocab_size: int, batch_size: int, device)"
  },
  {
    "module": "srt.constrained.outlines_backend",
    "file": "python/sglang/srt/constrained/outlines_backend.py",
    "line": 63,
    "qualname": "OutlinesGrammar.move_vocab_mask",
    "signature": "def move_vocab_mask(vocab_mask: torch.Tensor, device)"
  },
  {
    "module": "srt.constrained.outlines_backend",
    "file": "python/sglang/srt/constrained/outlines_backend.py",
    "line": 66,
    "qualname": "OutlinesGrammar.fill_vocab_mask",
    "signature": "def fill_vocab_mask(self, vocab_mask: torch.Tensor, idx: int)"
  },
  {
    "module": "srt.constrained.outlines_backend",
    "file": "python/sglang/srt/constrained/outlines_backend.py",
    "line": 75,
    "qualname": "OutlinesGrammar.apply_vocab_mask",
    "signature": "def apply_vocab_mask(logits: torch.Tensor, vocab_mask: torch.Tensor)"
  },
  {
    "module": "srt.constrained.outlines_backend",
    "file": "python/sglang/srt/constrained/outlines_backend.py",
    "line": 78,
    "qualname": "OutlinesGrammar.copy",
    "signature": "def copy(self)"
  },
  {
    "module": "srt.constrained.outlines_backend",
    "file": "python/sglang/srt/constrained/outlines_backend.py",
    "line": 81,
    "qualname": "OutlinesGrammar.try_jump_forward",
    "signature": "def try_jump_forward(self, tokenizer)"
  },
  {
    "module": "srt.constrained.outlines_backend",
    "file": "python/sglang/srt/constrained/outlines_backend.py",
    "line": 105,
    "qualname": "OutlinesGrammar.jump_forward_str_state",
    "signature": "def jump_forward_str_state(self, helper: Tuple[List[int], str])"
  },
  {
    "module": "srt.constrained.outlines_backend",
    "file": "python/sglang/srt/constrained/outlines_backend.py",
    "line": 109,
    "qualname": "OutlinesGrammar.jump_and_retokenize",
    "signature": "def jump_and_retokenize(self, old_output_ids: List[int], new_output_ids: List[int], next_state: int)"
  },
  {
    "module": "srt.constrained.outlines_backend",
    "file": "python/sglang/srt/constrained/outlines_backend.py",
    "line": 116,
    "qualname": "OutlinesGrammarBackend.__init__",
    "signature": "def __init__(self, tokenizer, whitespace_pattern: bool)"
  },
  {
    "module": "srt.constrained.outlines_backend",
    "file": "python/sglang/srt/constrained/outlines_backend.py",
    "line": 161,
    "qualname": "OutlinesGrammarBackend.dispatch_ebnf",
    "signature": "def dispatch_ebnf(self, key_string: str)"
  },
  {
    "module": "srt.constrained.outlines_backend",
    "file": "python/sglang/srt/constrained/outlines_backend.py",
    "line": 164,
    "qualname": "OutlinesGrammarBackend.dispatch_structural_tag",
    "signature": "def dispatch_structural_tag(self, key_string: str)"
  },
  {
    "module": "srt.constrained.outlines_backend",
    "file": "python/sglang/srt/constrained/outlines_backend.py",
    "line": 167,
    "qualname": "OutlinesGrammarBackend.dispatch_json",
    "signature": "def dispatch_json(self, key_string: str)"
  },
  {
    "module": "srt.constrained.outlines_backend",
    "file": "python/sglang/srt/constrained/outlines_backend.py",
    "line": 178,
    "qualname": "OutlinesGrammarBackend.dispatch_regex",
    "signature": "def dispatch_regex(self, key_string: str)"
  },
  {
    "module": "srt.constrained.outlines_backend",
    "file": "python/sglang/srt/constrained/outlines_backend.py",
    "line": 182,
    "qualname": "build_regex_from_object",
    "signature": "def build_regex_from_object(object: Union[str, BaseModel, Dict], whitespace_pattern: Optional[str])"
  },
  {
    "module": "srt.constrained.outlines_jump_forward",
    "file": "python/sglang/srt/constrained/outlines_jump_forward.py",
    "line": 54,
    "qualname": "disk_cache",
    "signature": "def disk_cache(expire: Optional[float], typed, ignore)"
  },
  {
    "module": "srt.constrained.outlines_jump_forward",
    "file": "python/sglang/srt/constrained/outlines_jump_forward.py",
    "line": 62,
    "qualname": "init_state_to_jump_forward",
    "signature": "def init_state_to_jump_forward(regex_string)"
  },
  {
    "module": "srt.constrained.outlines_jump_forward",
    "file": "python/sglang/srt/constrained/outlines_jump_forward.py",
    "line": 143,
    "qualname": "OutlinesJumpForwardMap.__init__",
    "signature": "def __init__(self, regex_string)"
  },
  {
    "module": "srt.constrained.outlines_jump_forward",
    "file": "python/sglang/srt/constrained/outlines_jump_forward.py",
    "line": 146,
    "qualname": "OutlinesJumpForwardMap.jump_forward_symbol",
    "signature": "def jump_forward_symbol(self, state)"
  },
  {
    "module": "srt.constrained.outlines_jump_forward",
    "file": "python/sglang/srt/constrained/outlines_jump_forward.py",
    "line": 159,
    "qualname": "OutlinesJumpForwardMap.jump_forward_byte",
    "signature": "def jump_forward_byte(self, state)"
  },
  {
    "module": "srt.constrained.outlines_jump_forward",
    "file": "python/sglang/srt/constrained/outlines_jump_forward.py",
    "line": 174,
    "qualname": "OutlinesJumpForwardMap.is_jump_forward_symbol_state",
    "signature": "def is_jump_forward_symbol_state(self, state)"
  },
  {
    "module": "srt.constrained.outlines_jump_forward",
    "file": "python/sglang/srt/constrained/outlines_jump_forward.py",
    "line": 181,
    "qualname": "test_main",
    "signature": "def test_main(regex_string)"
  },
  {
    "module": "srt.constrained.reasoner_grammar_backend",
    "file": "python/sglang/srt/constrained/reasoner_grammar_backend.py",
    "line": 24,
    "qualname": "ReasonerGrammarObject.__init__",
    "signature": "def __init__(self, grammar: BaseGrammarObject, think_end_id)"
  },
  {
    "module": "srt.constrained.reasoner_grammar_backend",
    "file": "python/sglang/srt/constrained/reasoner_grammar_backend.py",
    "line": 30,
    "qualname": "ReasonerGrammarObject.accept_token",
    "signature": "def accept_token(self, token: int)"
  },
  {
    "module": "srt.constrained.reasoner_grammar_backend",
    "file": "python/sglang/srt/constrained/reasoner_grammar_backend.py",
    "line": 37,
    "qualname": "ReasonerGrammarObject.allocate_vocab_mask",
    "signature": "def allocate_vocab_mask(self, vocab_size: int, batch_size: int, device)"
  },
  {
    "module": "srt.constrained.reasoner_grammar_backend",
    "file": "python/sglang/srt/constrained/reasoner_grammar_backend.py",
    "line": 42,
    "qualname": "ReasonerGrammarObject.fill_vocab_mask",
    "signature": "def fill_vocab_mask(self, vocab_mask: torch.Tensor, idx: int)"
  },
  {
    "module": "srt.constrained.reasoner_grammar_backend",
    "file": "python/sglang/srt/constrained/reasoner_grammar_backend.py",
    "line": 46,
    "qualname": "ReasonerGrammarObject.move_vocab_mask",
    "signature": "def move_vocab_mask(self, vocab_mask: torch.Tensor, device)"
  },
  {
    "module": "srt.constrained.reasoner_grammar_backend",
    "file": "python/sglang/srt/constrained/reasoner_grammar_backend.py",
    "line": 50,
    "qualname": "ReasonerGrammarObject.apply_vocab_mask",
    "signature": "def apply_vocab_mask(self)"
  },
  {
    "module": "srt.constrained.reasoner_grammar_backend",
    "file": "python/sglang/srt/constrained/reasoner_grammar_backend.py",
    "line": 53,
    "qualname": "ReasonerGrammarObject.copy",
    "signature": "def copy(self)"
  },
  {
    "module": "srt.constrained.reasoner_grammar_backend",
    "file": "python/sglang/srt/constrained/reasoner_grammar_backend.py",
    "line": 57,
    "qualname": "ReasonerGrammarObject.finished",
    "signature": "def finished(self)"
  },
  {
    "module": "srt.constrained.reasoner_grammar_backend",
    "file": "python/sglang/srt/constrained/reasoner_grammar_backend.py",
    "line": 61,
    "qualname": "ReasonerGrammarObject.finished",
    "signature": "def finished(self, finished)"
  },
  {
    "module": "srt.constrained.reasoner_grammar_backend",
    "file": "python/sglang/srt/constrained/reasoner_grammar_backend.py",
    "line": 64,
    "qualname": "ReasonerGrammarObject.try_jump_forward",
    "signature": "def try_jump_forward(self, tokenizer)"
  },
  {
    "module": "srt.constrained.reasoner_grammar_backend",
    "file": "python/sglang/srt/constrained/reasoner_grammar_backend.py",
    "line": 67,
    "qualname": "ReasonerGrammarObject.jump_forward_str_state",
    "signature": "def jump_forward_str_state(self, helper)"
  },
  {
    "module": "srt.constrained.reasoner_grammar_backend",
    "file": "python/sglang/srt/constrained/reasoner_grammar_backend.py",
    "line": 70,
    "qualname": "ReasonerGrammarObject.jump_and_retokenize",
    "signature": "def jump_and_retokenize(self, old_output_ids: List[int], new_output_ids: List[int], next_state: int)"
  },
  {
    "module": "srt.constrained.reasoner_grammar_backend",
    "file": "python/sglang/srt/constrained/reasoner_grammar_backend.py",
    "line": 79,
    "qualname": "ReasonerGrammarBackend.__init__",
    "signature": "def __init__(self, grammar_backend: BaseGrammarBackend, think_end_id)"
  },
  {
    "module": "srt.constrained.xgrammar_backend",
    "file": "python/sglang/srt/constrained/xgrammar_backend.py",
    "line": 52,
    "qualname": "XGrammarGrammar.__init__",
    "signature": "def __init__(self, matcher: GrammarMatcher, vocab_size: int, ctx: CompiledGrammar, override_stop_tokens: Optional[Union[List[int], int]], key_string: Optional[str])"
  },
  {
    "module": "srt.constrained.xgrammar_backend",
    "file": "python/sglang/srt/constrained/xgrammar_backend.py",
    "line": 68,
    "qualname": "XGrammarGrammar.accept_token",
    "signature": "def accept_token(self, token: int)"
  },
  {
    "module": "srt.constrained.xgrammar_backend",
    "file": "python/sglang/srt/constrained/xgrammar_backend.py",
    "line": 81,
    "qualname": "XGrammarGrammar.rollback",
    "signature": "def rollback(self, k: int)"
  },
  {
    "module": "srt.constrained.xgrammar_backend",
    "file": "python/sglang/srt/constrained/xgrammar_backend.py",
    "line": 85,
    "qualname": "XGrammarGrammar.is_terminated",
    "signature": "def is_terminated(self)"
  },
  {
    "module": "srt.constrained.xgrammar_backend",
    "file": "python/sglang/srt/constrained/xgrammar_backend.py",
    "line": 88,
    "qualname": "XGrammarGrammar.allocate_vocab_mask",
    "signature": "def allocate_vocab_mask(self, vocab_size: int, batch_size: int, device)"
  },
  {
    "module": "srt.constrained.xgrammar_backend",
    "file": "python/sglang/srt/constrained/xgrammar_backend.py",
    "line": 93,
    "qualname": "XGrammarGrammar.fill_vocab_mask",
    "signature": "def fill_vocab_mask(self, vocab_mask: torch.Tensor, idx: int)"
  },
  {
    "module": "srt.constrained.xgrammar_backend",
    "file": "python/sglang/srt/constrained/xgrammar_backend.py",
    "line": 97,
    "qualname": "XGrammarGrammar.move_vocab_mask",
    "signature": "def move_vocab_mask(vocab_mask: torch.Tensor, device)"
  },
  {
    "module": "srt.constrained.xgrammar_backend",
    "file": "python/sglang/srt/constrained/xgrammar_backend.py",
    "line": 100,
    "qualname": "XGrammarGrammar.apply_vocab_mask",
    "signature": "def apply_vocab_mask(self, logits: torch.Tensor, vocab_mask: torch.Tensor)"
  },
  {
    "module": "srt.constrained.xgrammar_backend",
    "file": "python/sglang/srt/constrained/xgrammar_backend.py",
    "line": 111,
    "qualname": "XGrammarGrammar.copy",
    "signature": "def copy(self)"
  },
  {
    "module": "srt.constrained.xgrammar_backend",
    "file": "python/sglang/srt/constrained/xgrammar_backend.py",
    "line": 125,
    "qualname": "XGrammarGrammar.try_jump_forward",
    "signature": "def try_jump_forward(self, tokenizer)"
  },
  {
    "module": "srt.constrained.xgrammar_backend",
    "file": "python/sglang/srt/constrained/xgrammar_backend.py",
    "line": 131,
    "qualname": "XGrammarGrammar.jump_forward_str_state",
    "signature": "def jump_forward_str_state(self, helper: Tuple[List[int], str])"
  },
  {
    "module": "srt.constrained.xgrammar_backend",
    "file": "python/sglang/srt/constrained/xgrammar_backend.py",
    "line": 135,
    "qualname": "XGrammarGrammar.jump_and_retokenize",
    "signature": "def jump_and_retokenize(self, old_output_ids: List[int], new_output_ids: List[int], next_state: int)"
  },
  {
    "module": "srt.constrained.xgrammar_backend",
    "file": "python/sglang/srt/constrained/xgrammar_backend.py",
    "line": 152,
    "qualname": "XGrammarGrammar.__repr__",
    "signature": "def __repr__(self)"
  },
  {
    "module": "srt.constrained.xgrammar_backend",
    "file": "python/sglang/srt/constrained/xgrammar_backend.py",
    "line": 157,
    "qualname": "XGrammarGrammarBackend.__init__",
    "signature": "def __init__(self, tokenizer, vocab_size: int, model_eos_token_ids: Optional[List[int]])"
  },
  {
    "module": "srt.constrained.xgrammar_backend",
    "file": "python/sglang/srt/constrained/xgrammar_backend.py",
    "line": 190,
    "qualname": "XGrammarGrammarBackend.dispatch_json",
    "signature": "def dispatch_json(self, key_string: str)"
  },
  {
    "module": "srt.constrained.xgrammar_backend",
    "file": "python/sglang/srt/constrained/xgrammar_backend.py",
    "line": 203,
    "qualname": "XGrammarGrammarBackend.dispatch_ebnf",
    "signature": "def dispatch_ebnf(self, key_string: str)"
  },
  {
    "module": "srt.constrained.xgrammar_backend",
    "file": "python/sglang/srt/constrained/xgrammar_backend.py",
    "line": 211,
    "qualname": "XGrammarGrammarBackend.dispatch_regex",
    "signature": "def dispatch_regex(self, key_string: str)"
  },
  {
    "module": "srt.constrained.xgrammar_backend",
    "file": "python/sglang/srt/constrained/xgrammar_backend.py",
    "line": 219,
    "qualname": "XGrammarGrammarBackend.dispatch_structural_tag",
    "signature": "def dispatch_structural_tag(self, key_string: str)"
  },
  {
    "module": "srt.constrained.xgrammar_backend",
    "file": "python/sglang/srt/constrained/xgrammar_backend.py",
    "line": 238,
    "qualname": "XGrammarGrammarBackend.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.debug_utils.dump_comparator",
    "file": "python/sglang/srt/debug_utils/dump_comparator.py",
    "line": 12,
    "qualname": "main",
    "signature": "def main(args)"
  },
  {
    "module": "srt.debug_utils.dump_comparator",
    "file": "python/sglang/srt/debug_utils/dump_comparator.py",
    "line": 53,
    "qualname": "read_meta",
    "signature": "def read_meta(directory)"
  },
  {
    "module": "srt.debug_utils.dump_comparator",
    "file": "python/sglang/srt/debug_utils/dump_comparator.py",
    "line": 78,
    "qualname": "check_tensor_pair",
    "signature": "def check_tensor_pair(path_baseline, path_target)"
  },
  {
    "module": "srt.debug_utils.dumper",
    "file": "python/sglang/srt/debug_utils/dumper.py",
    "line": 27,
    "qualname": "_Dumper.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.debug_utils.dumper",
    "file": "python/sglang/srt/debug_utils/dumper.py",
    "line": 38,
    "qualname": "_Dumper.on_forward_pass_start",
    "signature": "def on_forward_pass_start(self)"
  },
  {
    "module": "srt.debug_utils.dumper",
    "file": "python/sglang/srt/debug_utils/dumper.py",
    "line": 44,
    "qualname": "_Dumper.dump",
    "signature": "def dump(self, name, value)"
  },
  {
    "module": "srt.debug_utils.dumper",
    "file": "python/sglang/srt/debug_utils/dumper.py",
    "line": 89,
    "qualname": "get_truncated_value",
    "signature": "def get_truncated_value(value)"
  },
  {
    "module": "srt.debug_utils.text_comparator",
    "file": "python/sglang/srt/debug_utils/text_comparator.py",
    "line": 15,
    "qualname": "main",
    "signature": "def main(args)"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 77,
    "qualname": "DecodeReqToTokenPool.__init__",
    "signature": "def __init__(self, size: int, max_context_len: int, device: str, enable_memory_saver: bool, pre_alloc_size: int)"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 102,
    "qualname": "DecodeReqToTokenPool.write",
    "signature": "def write(self, indices, values)"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 105,
    "qualname": "DecodeReqToTokenPool.available_size",
    "signature": "def available_size(self)"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 108,
    "qualname": "DecodeReqToTokenPool.alloc",
    "signature": "def alloc(self, need_size: int)"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 116,
    "qualname": "DecodeReqToTokenPool.free",
    "signature": "def free(self, free_index: Union[int, List[int]])"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 122,
    "qualname": "DecodeReqToTokenPool.clear",
    "signature": "def clear(self)"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 139,
    "qualname": "DecodePreallocQueue.__init__",
    "signature": "def __init__(self, req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, draft_token_to_kv_pool: Optional[KVCache], req_to_metadata_buffer_idx_allocator: ReqToMetadataIdxAllocator, metadata_buffers: MetadataBuffers, scheduler: Scheduler, transfer_queue: DecodeTransferQueue, tree_cache: BasePrefixCache, gloo_group: ProcessGroup, tp_rank: int, tp_size: int, dp_size: int, gpu_id: int, bootstrap_port: int, max_total_num_tokens: int, prefill_pp_size: int, num_reserved_decode_tokens: int, transfer_backend: TransferBackend)"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 230,
    "qualname": "DecodePreallocQueue.add",
    "signature": "def add(self, req: Req, is_retracted: bool)"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 267,
    "qualname": "DecodePreallocQueue.extend",
    "signature": "def extend(self, reqs: List[Req], is_retracted: bool)"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 272,
    "qualname": "DecodePreallocQueue.resume_retracted_reqs",
    "signature": "def resume_retracted_reqs(self)"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 342,
    "qualname": "DecodePreallocQueue.pop_preallocated",
    "signature": "def pop_preallocated(self)"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 432,
    "qualname": "DecodePreallocQueue.num_tokens_pre_allocated",
    "signature": "def num_tokens_pre_allocated(self)"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 548,
    "qualname": "DecodeTransferQueue.__init__",
    "signature": "def __init__(self, gloo_group: ProcessGroup, req_to_metadata_buffer_idx_allocator: ReqToMetadataIdxAllocator, tp_rank: int, metadata_buffers: MetadataBuffers, scheduler: Scheduler, tree_cache: BasePrefixCache)"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 566,
    "qualname": "DecodeTransferQueue.add",
    "signature": "def add(self, decode_req: DecodeRequest)"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 569,
    "qualname": "DecodeTransferQueue.extend",
    "signature": "def extend(self, decode_reqs: List[DecodeRequest])"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 572,
    "qualname": "DecodeTransferQueue.pop_transferred",
    "signature": "def pop_transferred(self)"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 675,
    "qualname": "SchedulerDisaggregationDecodeMixin.event_loop_normal_disagg_decode",
    "signature": "def event_loop_normal_disagg_decode(self: Scheduler)"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 716,
    "qualname": "SchedulerDisaggregationDecodeMixin.event_loop_overlap_disagg_decode",
    "signature": "def event_loop_overlap_disagg_decode(self: Scheduler)"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 799,
    "qualname": "SchedulerDisaggregationDecodeMixin.get_next_disagg_decode_batch_to_run",
    "signature": "def get_next_disagg_decode_batch_to_run(self: Scheduler)"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 831,
    "qualname": "SchedulerDisaggregationDecodeMixin.get_new_prebuilt_batch",
    "signature": "def get_new_prebuilt_batch(self: Scheduler)"
  },
  {
    "module": "srt.disaggregation.decode",
    "file": "python/sglang/srt/disaggregation/decode.py",
    "line": 879,
    "qualname": "SchedulerDisaggregationDecodeMixin.process_decode_queue",
    "signature": "def process_decode_queue(self: Scheduler)"
  },
  {
    "module": "srt.disaggregation.decode_schedule_batch_mixin",
    "file": "python/sglang/srt/disaggregation/decode_schedule_batch_mixin.py",
    "line": 23,
    "qualname": "ScheduleBatchDisaggregationDecodeMixin.prepare_for_prebuilt_extend",
    "signature": "def prepare_for_prebuilt_extend(self: ScheduleBatch)"
  },
  {
    "module": "srt.disaggregation.decode_schedule_batch_mixin",
    "file": "python/sglang/srt/disaggregation/decode_schedule_batch_mixin.py",
    "line": 102,
    "qualname": "ScheduleBatchDisaggregationDecodeMixin.process_prebuilt_extend",
    "signature": "def process_prebuilt_extend(self: ScheduleBatch, server_args: ServerArgs, model_config: ModelConfig)"
  },
  {
    "module": "srt.disaggregation.kv_events",
    "file": "python/sglang/srt/disaggregation/kv_events.py",
    "line": 93,
    "qualname": "EventPublisher.__init__",
    "signature": "def __init__(self, attn_dp_rank: int)"
  },
  {
    "module": "srt.disaggregation.kv_events",
    "file": "python/sglang/srt/disaggregation/kv_events.py",
    "line": 97,
    "qualname": "EventPublisher.publish",
    "signature": "def publish(self, events: EventBatch)"
  },
  {
    "module": "srt.disaggregation.kv_events",
    "file": "python/sglang/srt/disaggregation/kv_events.py",
    "line": 105,
    "qualname": "EventPublisher.shutdown",
    "signature": "def shutdown(self)"
  },
  {
    "module": "srt.disaggregation.kv_events",
    "file": "python/sglang/srt/disaggregation/kv_events.py",
    "line": 112,
    "qualname": "NullEventPublisher.publish",
    "signature": "def publish(self, events)"
  },
  {
    "module": "srt.disaggregation.kv_events",
    "file": "python/sglang/srt/disaggregation/kv_events.py",
    "line": 115,
    "qualname": "NullEventPublisher.shutdown",
    "signature": "def shutdown(self)"
  },
  {
    "module": "srt.disaggregation.kv_events",
    "file": "python/sglang/srt/disaggregation/kv_events.py",
    "line": 146,
    "qualname": "ZmqEventPublisher.__init__",
    "signature": "def __init__(self, attn_dp_rank: int, endpoint: str, replay_endpoint: Optional[str], buffer_steps: int, hwm: int, max_queue_size: int, topic: str)"
  },
  {
    "module": "srt.disaggregation.kv_events",
    "file": "python/sglang/srt/disaggregation/kv_events.py",
    "line": 188,
    "qualname": "ZmqEventPublisher.publish",
    "signature": "def publish(self, events: EventBatch)"
  },
  {
    "module": "srt.disaggregation.kv_events",
    "file": "python/sglang/srt/disaggregation/kv_events.py",
    "line": 195,
    "qualname": "ZmqEventPublisher.shutdown",
    "signature": "def shutdown(self)"
  },
  {
    "module": "srt.disaggregation.kv_events",
    "file": "python/sglang/srt/disaggregation/kv_events.py",
    "line": 314,
    "qualname": "ZmqEventPublisher.offset_endpoint_port",
    "signature": "def offset_endpoint_port(endpoint: Optional[str], data_parallel_rank: int)"
  },
  {
    "module": "srt.disaggregation.kv_events",
    "file": "python/sglang/srt/disaggregation/kv_events.py",
    "line": 382,
    "qualname": "KVEventsConfig.from_cli",
    "signature": "def from_cli(cls, cli_value: str)"
  },
  {
    "module": "srt.disaggregation.kv_events",
    "file": "python/sglang/srt/disaggregation/kv_events.py",
    "line": 394,
    "qualname": "EventPublisherFactory.register_publisher",
    "signature": "def register_publisher(cls, name: str, ctor: Callable[..., EventPublisher])"
  },
  {
    "module": "srt.disaggregation.kv_events",
    "file": "python/sglang/srt/disaggregation/kv_events.py",
    "line": 400,
    "qualname": "EventPublisherFactory.create",
    "signature": "def create(cls, config: Optional[str], attn_dp_rank: int)"
  },
  {
    "module": "srt.disaggregation.utils",
    "file": "python/sglang/srt/disaggregation/utils.py",
    "line": 43,
    "qualname": "poll_and_all_reduce",
    "signature": "def poll_and_all_reduce(pollers, gloo_group)"
  },
  {
    "module": "srt.disaggregation.utils",
    "file": "python/sglang/srt/disaggregation/utils.py",
    "line": 67,
    "qualname": "ReqToMetadataIdxAllocator.__init__",
    "signature": "def __init__(self, size: int)"
  },
  {
    "module": "srt.disaggregation.utils",
    "file": "python/sglang/srt/disaggregation/utils.py",
    "line": 74,
    "qualname": "ReqToMetadataIdxAllocator.available_size",
    "signature": "def available_size(self)"
  },
  {
    "module": "srt.disaggregation.utils",
    "file": "python/sglang/srt/disaggregation/utils.py",
    "line": 77,
    "qualname": "ReqToMetadataIdxAllocator.alloc",
    "signature": "def alloc(self)"
  },
  {
    "module": "srt.disaggregation.utils",
    "file": "python/sglang/srt/disaggregation/utils.py",
    "line": 83,
    "qualname": "ReqToMetadataIdxAllocator.free",
    "signature": "def free(self, free_index: int)"
  },
  {
    "module": "srt.disaggregation.utils",
    "file": "python/sglang/srt/disaggregation/utils.py",
    "line": 88,
    "qualname": "MetadataBuffers.__init__",
    "signature": "def __init__(self, size: int, hidden_size: int, dtype: torch.dtype, max_top_logprobs_num: int, custom_mem_pool: torch.cuda.MemPool)"
  },
  {
    "module": "srt.disaggregation.utils",
    "file": "python/sglang/srt/disaggregation/utils.py",
    "line": 131,
    "qualname": "MetadataBuffers.get_buf_infos",
    "signature": "def get_buf_infos(self)"
  },
  {
    "module": "srt.disaggregation.utils",
    "file": "python/sglang/srt/disaggregation/utils.py",
    "line": 158,
    "qualname": "MetadataBuffers.get_buf",
    "signature": "def get_buf(self, idx: int)"
  },
  {
    "module": "srt.disaggregation.utils",
    "file": "python/sglang/srt/disaggregation/utils.py",
    "line": 168,
    "qualname": "MetadataBuffers.set_buf",
    "signature": "def set_buf(self, req: Req)"
  },
  {
    "module": "srt.disaggregation.utils",
    "file": "python/sglang/srt/disaggregation/utils.py",
    "line": 220,
    "qualname": "get_kv_class",
    "signature": "def get_kv_class(transfer_backend: TransferBackend, class_type: KVClassType)"
  },
  {
    "module": "srt.disaggregation.utils",
    "file": "python/sglang/srt/disaggregation/utils.py",
    "line": 293,
    "qualname": "kv_to_page_indices",
    "signature": "def kv_to_page_indices(kv_indices: np.ndarray, page_size: int)"
  },
  {
    "module": "srt.disaggregation.utils",
    "file": "python/sglang/srt/disaggregation/utils.py",
    "line": 303,
    "qualname": "kv_to_page_num",
    "signature": "def kv_to_page_num(num_kv_indices: int, page_size: int)"
  },
  {
    "module": "srt.disaggregation.utils",
    "file": "python/sglang/srt/disaggregation/utils.py",
    "line": 321,
    "qualname": "PDRegistryRequest.__post_init__",
    "signature": "def __post_init__(self)"
  },
  {
    "module": "srt.disaggregation.utils",
    "file": "python/sglang/srt/disaggregation/utils.py",
    "line": 332,
    "qualname": "register_disaggregation_server",
    "signature": "def register_disaggregation_server(mode: str, server_port: int, bootstrap_port: int, pdlb_url: str)"
  },
  {
    "module": "srt.disaggregation.utils",
    "file": "python/sglang/srt/disaggregation/utils.py",
    "line": 356,
    "qualname": "is_mla_backend",
    "signature": "def is_mla_backend(target_kv_pool)"
  },
  {
    "module": "srt.disaggregation.utils",
    "file": "python/sglang/srt/disaggregation/utils.py",
    "line": 362,
    "qualname": "prepare_abort",
    "signature": "def prepare_abort(req: Req, error_message: str, status_code)"
  },
  {
    "module": "srt.disaggregation.launch_lb",
    "file": "python/sglang/srt/disaggregation/launch_lb.py",
    "line": 19,
    "qualname": "LBArgs.add_cli_args",
    "signature": "def add_cli_args(parser: argparse.ArgumentParser)"
  },
  {
    "module": "srt.disaggregation.launch_lb",
    "file": "python/sglang/srt/disaggregation/launch_lb.py",
    "line": 78,
    "qualname": "LBArgs.from_cli_args",
    "signature": "def from_cli_args(cls, args: argparse.Namespace)"
  },
  {
    "module": "srt.disaggregation.launch_lb",
    "file": "python/sglang/srt/disaggregation/launch_lb.py",
    "line": 105,
    "qualname": "LBArgs.__post_init__",
    "signature": "def __post_init__(self)"
  },
  {
    "module": "srt.disaggregation.launch_lb",
    "file": "python/sglang/srt/disaggregation/launch_lb.py",
    "line": 112,
    "qualname": "main",
    "signature": "def main()"
  },
  {
    "module": "srt.disaggregation.mini_lb",
    "file": "python/sglang/srt/disaggregation/mini_lb.py",
    "line": 27,
    "qualname": "setup_logger",
    "signature": "def setup_logger()"
  },
  {
    "module": "srt.disaggregation.mini_lb",
    "file": "python/sglang/srt/disaggregation/mini_lb.py",
    "line": 53,
    "qualname": "MiniLoadBalancer.__init__",
    "signature": "def __init__(self, prefill_configs: List[PrefillConfig], decode_servers: List[str], timeout: int)"
  },
  {
    "module": "srt.disaggregation.mini_lb",
    "file": "python/sglang/srt/disaggregation/mini_lb.py",
    "line": 64,
    "qualname": "MiniLoadBalancer.add_prefill_server",
    "signature": "def add_prefill_server(self, new_prefill_config: PrefillConfig)"
  },
  {
    "module": "srt.disaggregation.mini_lb",
    "file": "python/sglang/srt/disaggregation/mini_lb.py",
    "line": 68,
    "qualname": "MiniLoadBalancer.add_decode_server",
    "signature": "def add_decode_server(self, new_decode_server: str)"
  },
  {
    "module": "srt.disaggregation.mini_lb",
    "file": "python/sglang/srt/disaggregation/mini_lb.py",
    "line": 71,
    "qualname": "MiniLoadBalancer.select_pair",
    "signature": "def select_pair(self)"
  },
  {
    "module": "srt.disaggregation.mini_lb",
    "file": "python/sglang/srt/disaggregation/mini_lb.py",
    "line": 80,
    "qualname": "MiniLoadBalancer.generate",
    "signature": "async def generate(self, modified_request, prefill_server, decode_server, endpoint)"
  },
  {
    "module": "srt.disaggregation.mini_lb",
    "file": "python/sglang/srt/disaggregation/mini_lb.py",
    "line": 118,
    "qualname": "MiniLoadBalancer.generate_stream",
    "signature": "async def generate_stream(self, modified_request, prefill_server, decode_server, endpoint)"
  },
  {
    "module": "srt.disaggregation.mini_lb",
    "file": "python/sglang/srt/disaggregation/mini_lb.py",
    "line": 184,
    "qualname": "health_check",
    "signature": "async def health_check()"
  },
  {
    "module": "srt.disaggregation.mini_lb",
    "file": "python/sglang/srt/disaggregation/mini_lb.py",
    "line": 189,
    "qualname": "health_check",
    "signature": "async def health_check()"
  },
  {
    "module": "srt.disaggregation.mini_lb",
    "file": "python/sglang/srt/disaggregation/mini_lb.py",
    "line": 205,
    "qualname": "flush_cache",
    "signature": "async def flush_cache()"
  },
  {
    "module": "srt.disaggregation.mini_lb",
    "file": "python/sglang/srt/disaggregation/mini_lb.py",
    "line": 221,
    "qualname": "get_server_info",
    "signature": "async def get_server_info()"
  },
  {
    "module": "srt.disaggregation.mini_lb",
    "file": "python/sglang/srt/disaggregation/mini_lb.py",
    "line": 264,
    "qualname": "get_model_info",
    "signature": "async def get_model_info()"
  },
  {
    "module": "srt.disaggregation.mini_lb",
    "file": "python/sglang/srt/disaggregation/mini_lb.py",
    "line": 276,
    "qualname": "handle_generate_request",
    "signature": "async def handle_generate_request(request_data: dict)"
  },
  {
    "module": "srt.disaggregation.mini_lb",
    "file": "python/sglang/srt/disaggregation/mini_lb.py",
    "line": 346,
    "qualname": "handle_chat_completion_request",
    "signature": "async def handle_chat_completion_request(request_data: dict)"
  },
  {
    "module": "srt.disaggregation.mini_lb",
    "file": "python/sglang/srt/disaggregation/mini_lb.py",
    "line": 351,
    "qualname": "handle_completion_request",
    "signature": "async def handle_completion_request(request_data: dict)"
  },
  {
    "module": "srt.disaggregation.mini_lb",
    "file": "python/sglang/srt/disaggregation/mini_lb.py",
    "line": 369,
    "qualname": "get_models",
    "signature": "async def get_models()"
  },
  {
    "module": "srt.disaggregation.mini_lb",
    "file": "python/sglang/srt/disaggregation/mini_lb.py",
    "line": 385,
    "qualname": "register",
    "signature": "async def register(obj: PDRegistryRequest)"
  },
  {
    "module": "srt.disaggregation.mini_lb",
    "file": "python/sglang/srt/disaggregation/mini_lb.py",
    "line": 410,
    "qualname": "run",
    "signature": "def run(prefill_configs, decode_addrs, host, port, timeout)"
  },
  {
    "module": "srt.disaggregation.prefill",
    "file": "python/sglang/srt/disaggregation/prefill.py",
    "line": 68,
    "qualname": "PrefillBootstrapQueue.__init__",
    "signature": "def __init__(self, token_to_kv_pool: KVCache, draft_token_to_kv_pool: Optional[KVCache], req_to_metadata_buffer_idx_allocator: ReqToMetadataIdxAllocator, metadata_buffers: MetadataBuffers, tp_rank: int, tp_size: int, gpu_id: int, bootstrap_port: int, gloo_group: ProcessGroup, max_total_num_tokens: int, decode_tp_size: int, decode_dp_size: int, scheduler: Scheduler, pp_rank: int, pp_size: int, transfer_backend: TransferBackend)"
  },
  {
    "module": "srt.disaggregation.prefill",
    "file": "python/sglang/srt/disaggregation/prefill.py",
    "line": 152,
    "qualname": "PrefillBootstrapQueue.add",
    "signature": "def add(self, req: Req, num_kv_heads: int)"
  },
  {
    "module": "srt.disaggregation.prefill",
    "file": "python/sglang/srt/disaggregation/prefill.py",
    "line": 173,
    "qualname": "PrefillBootstrapQueue.extend",
    "signature": "def extend(self, reqs: List[Req], num_kv_heads: int)"
  },
  {
    "module": "srt.disaggregation.prefill",
    "file": "python/sglang/srt/disaggregation/prefill.py",
    "line": 192,
    "qualname": "PrefillBootstrapQueue.pop_bootstrapped",
    "signature": "def pop_bootstrapped(self, return_failed_reqs: bool, rids_to_check: Optional[List[str]])"
  },
  {
    "module": "srt.disaggregation.prefill",
    "file": "python/sglang/srt/disaggregation/prefill.py",
    "line": 276,
    "qualname": "SchedulerDisaggregationPrefillMixin.event_loop_normal_disagg_prefill",
    "signature": "def event_loop_normal_disagg_prefill(self: Scheduler)"
  },
  {
    "module": "srt.disaggregation.prefill",
    "file": "python/sglang/srt/disaggregation/prefill.py",
    "line": 308,
    "qualname": "SchedulerDisaggregationPrefillMixin.event_loop_overlap_disagg_prefill",
    "signature": "def event_loop_overlap_disagg_prefill(self: Scheduler)"
  },
  {
    "module": "srt.disaggregation.prefill",
    "file": "python/sglang/srt/disaggregation/prefill.py",
    "line": 355,
    "qualname": "SchedulerDisaggregationPrefillMixin.process_batch_result_disagg_prefill",
    "signature": "def process_batch_result_disagg_prefill(self: Scheduler, batch: ScheduleBatch, result: GenerationBatchResult, launch_done: Optional[threading.Event])"
  },
  {
    "module": "srt.disaggregation.prefill",
    "file": "python/sglang/srt/disaggregation/prefill.py",
    "line": 478,
    "qualname": "SchedulerDisaggregationPrefillMixin.process_disagg_prefill_inflight_queue",
    "signature": "def process_disagg_prefill_inflight_queue(self: Scheduler, rids_to_check: Optional[List[str]])"
  },
  {
    "module": "srt.disaggregation.prefill",
    "file": "python/sglang/srt/disaggregation/prefill.py",
    "line": 547,
    "qualname": "SchedulerDisaggregationPrefillMixin.get_transferred_rids",
    "signature": "def get_transferred_rids(self: Scheduler)"
  },
  {
    "module": "srt.disaggregation.prefill",
    "file": "python/sglang/srt/disaggregation/prefill.py",
    "line": 564,
    "qualname": "SchedulerDisaggregationPrefillMixin.process_prefill_chunk",
    "signature": "def process_prefill_chunk(self: Scheduler)"
  },
  {
    "module": "srt.disaggregation.prefill",
    "file": "python/sglang/srt/disaggregation/prefill.py",
    "line": 583,
    "qualname": "SchedulerDisaggregationPrefillMixin.send_kv_chunk",
    "signature": "def send_kv_chunk(self: Scheduler, req: Req, last_chunk: bool, end_idx: Optional[int])"
  },
  {
    "module": "srt.disaggregation.prefill",
    "file": "python/sglang/srt/disaggregation/prefill.py",
    "line": 622,
    "qualname": "SchedulerDisaggregationPrefillMixin.event_loop_pp_disagg_prefill",
    "signature": "def event_loop_pp_disagg_prefill(self: Scheduler)"
  },
  {
    "module": "srt.disaggregation.prefill",
    "file": "python/sglang/srt/disaggregation/prefill.py",
    "line": 837,
    "qualname": "SchedulerDisaggregationPrefillMixin.send_pyobj_to_next_stage",
    "signature": "def send_pyobj_to_next_stage(self, data)"
  },
  {
    "module": "srt.disaggregation.prefill",
    "file": "python/sglang/srt/disaggregation/prefill.py",
    "line": 848,
    "qualname": "SchedulerDisaggregationPrefillMixin.recv_pyobj_from_prev_stage",
    "signature": "def recv_pyobj_from_prev_stage(self)"
  },
  {
    "module": "srt.distributed.communication_op",
    "file": "python/sglang/srt/distributed/communication_op.py",
    "line": 11,
    "qualname": "tensor_model_parallel_all_reduce",
    "signature": "def tensor_model_parallel_all_reduce(input_: torch.Tensor)"
  },
  {
    "module": "srt.distributed.communication_op",
    "file": "python/sglang/srt/distributed/communication_op.py",
    "line": 16,
    "qualname": "tensor_model_parallel_all_gather",
    "signature": "def tensor_model_parallel_all_gather(input_: torch.Tensor, dim: int)"
  },
  {
    "module": "srt.distributed.communication_op",
    "file": "python/sglang/srt/distributed/communication_op.py",
    "line": 23,
    "qualname": "tensor_model_parallel_gather",
    "signature": "def tensor_model_parallel_gather(input_: torch.Tensor, dst: int, dim: int)"
  },
  {
    "module": "srt.distributed.communication_op",
    "file": "python/sglang/srt/distributed/communication_op.py",
    "line": 30,
    "qualname": "broadcast_tensor_dict",
    "signature": "def broadcast_tensor_dict(tensor_dict: Optional[Dict[Any, Union[torch.Tensor, Any]]], src: int)"
  },
  {
    "module": "srt.distributed.naive_distributed",
    "file": "python/sglang/srt/distributed/naive_distributed.py",
    "line": 14,
    "qualname": "NaiveDistributed.__init__",
    "signature": "def __init__(self, rank: int, world_size: int, rendezvous: str)"
  },
  {
    "module": "srt.distributed.naive_distributed",
    "file": "python/sglang/srt/distributed/naive_distributed.py",
    "line": 25,
    "qualname": "NaiveDistributed.get_rank",
    "signature": "def get_rank(self)"
  },
  {
    "module": "srt.distributed.naive_distributed",
    "file": "python/sglang/srt/distributed/naive_distributed.py",
    "line": 28,
    "qualname": "NaiveDistributed.get_world_size",
    "signature": "def get_world_size(self)"
  },
  {
    "module": "srt.distributed.naive_distributed",
    "file": "python/sglang/srt/distributed/naive_distributed.py",
    "line": 31,
    "qualname": "NaiveDistributed.scatter",
    "signature": "def scatter(self, tensor: torch.Tensor, scatter_list: List[torch.Tensor], src: int)"
  },
  {
    "module": "srt.distributed.naive_distributed",
    "file": "python/sglang/srt/distributed/naive_distributed.py",
    "line": 69,
    "qualname": "NaiveDistributed.all_gather_object",
    "signature": "def all_gather_object(self, obj: Any)"
  },
  {
    "module": "srt.distributed.naive_distributed",
    "file": "python/sglang/srt/distributed/naive_distributed.py",
    "line": 95,
    "qualname": "NaiveDistributed.barrier",
    "signature": "def barrier(self)"
  },
  {
    "module": "srt.distributed.naive_distributed",
    "file": "python/sglang/srt/distributed/naive_distributed.py",
    "line": 104,
    "qualname": "get_naive_distributed",
    "signature": "def get_naive_distributed()"
  },
  {
    "module": "srt.distributed.naive_distributed",
    "file": "python/sglang/srt/distributed/naive_distributed.py",
    "line": 109,
    "qualname": "set_naive_distributed",
    "signature": "def set_naive_distributed(instance: NaiveDistributed)"
  },
  {
    "module": "srt.distributed.utils",
    "file": "python/sglang/srt/distributed/utils.py",
    "line": 21,
    "qualname": "ensure_divisibility",
    "signature": "def ensure_divisibility(numerator, denominator)"
  },
  {
    "module": "srt.distributed.utils",
    "file": "python/sglang/srt/distributed/utils.py",
    "line": 28,
    "qualname": "divide",
    "signature": "def divide(numerator, denominator)"
  },
  {
    "module": "srt.distributed.utils",
    "file": "python/sglang/srt/distributed/utils.py",
    "line": 35,
    "qualname": "split_tensor_along_last_dim",
    "signature": "def split_tensor_along_last_dim(tensor: torch.Tensor, num_partitions: int, contiguous_split_chunks: bool)"
  },
  {
    "module": "srt.distributed.utils",
    "file": "python/sglang/srt/distributed/utils.py",
    "line": 63,
    "qualname": "get_pp_indices",
    "signature": "def get_pp_indices(num_hidden_layers: int, pp_rank: int, pp_size: int)"
  },
  {
    "module": "srt.distributed.utils",
    "file": "python/sglang/srt/distributed/utils.py",
    "line": 118,
    "qualname": "StatelessProcessGroup.__post_init__",
    "signature": "def __post_init__(self)"
  },
  {
    "module": "srt.distributed.utils",
    "file": "python/sglang/srt/distributed/utils.py",
    "line": 124,
    "qualname": "StatelessProcessGroup.send_obj",
    "signature": "def send_obj(self, obj: Any, dst: int)"
  },
  {
    "module": "srt.distributed.utils",
    "file": "python/sglang/srt/distributed/utils.py",
    "line": 132,
    "qualname": "StatelessProcessGroup.expire_data",
    "signature": "def expire_data(self)"
  },
  {
    "module": "srt.distributed.utils",
    "file": "python/sglang/srt/distributed/utils.py",
    "line": 143,
    "qualname": "StatelessProcessGroup.recv_obj",
    "signature": "def recv_obj(self, src: int)"
  },
  {
    "module": "srt.distributed.utils",
    "file": "python/sglang/srt/distributed/utils.py",
    "line": 151,
    "qualname": "StatelessProcessGroup.broadcast_obj",
    "signature": "def broadcast_obj(self, obj: Optional[Any], src: int)"
  },
  {
    "module": "srt.distributed.utils",
    "file": "python/sglang/srt/distributed/utils.py",
    "line": 169,
    "qualname": "StatelessProcessGroup.all_gather_obj",
    "signature": "def all_gather_obj(self, obj: Any)"
  },
  {
    "module": "srt.distributed.utils",
    "file": "python/sglang/srt/distributed/utils.py",
    "line": 181,
    "qualname": "StatelessProcessGroup.barrier",
    "signature": "def barrier(self)"
  },
  {
    "module": "srt.distributed.utils",
    "file": "python/sglang/srt/distributed/utils.py",
    "line": 190,
    "qualname": "StatelessProcessGroup.create",
    "signature": "def create(host: str, port: int, rank: int, world_size: int, data_expiration_seconds: int)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 115,
    "qualname": "inplace_all_reduce",
    "signature": "def inplace_all_reduce(tensor: torch.Tensor, group_name: str)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 122,
    "qualname": "inplace_all_reduce_fake",
    "signature": "def inplace_all_reduce_fake(tensor: torch.Tensor, group_name: str)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 132,
    "qualname": "outplace_all_reduce",
    "signature": "def outplace_all_reduce(tensor: torch.Tensor, group_name: str, outplace_all_reduce_method: str)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 141,
    "qualname": "outplace_all_reduce_fake",
    "signature": "def outplace_all_reduce_fake(tensor: torch.Tensor, group_name: str, outplace_all_reduce_method: str)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 153,
    "qualname": "reg_all_gather_into_tensor",
    "signature": "def reg_all_gather_into_tensor(output: torch.Tensor, input: torch.Tensor, group_name: str)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 162,
    "qualname": "reg_all_gather_into_tensor_fake",
    "signature": "def reg_all_gather_into_tensor_fake(output: torch.Tensor, input: torch.Tensor, group_name: str)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 212,
    "qualname": "GroupCoordinator.__init__",
    "signature": "def __init__(self, group_ranks: List[List[int]], local_rank: int, torch_distributed_backend: Union[str, Backend], use_pynccl: bool, use_pymscclpp: bool, use_custom_allreduce: bool, use_hpu_communicator: bool, use_xpu_communicator: bool, use_npu_communicator: bool, use_message_queue_broadcaster: bool, group_name: Optional[str])"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 362,
    "qualname": "GroupCoordinator.__repr__",
    "signature": "def __repr__(self)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 370,
    "qualname": "GroupCoordinator.first_rank",
    "signature": "def first_rank(self)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 375,
    "qualname": "GroupCoordinator.last_rank",
    "signature": "def last_rank(self)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 380,
    "qualname": "GroupCoordinator.is_first_rank",
    "signature": "def is_first_rank(self)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 385,
    "qualname": "GroupCoordinator.is_last_rank",
    "signature": "def is_last_rank(self)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 390,
    "qualname": "GroupCoordinator.next_rank",
    "signature": "def next_rank(self)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 397,
    "qualname": "GroupCoordinator.prev_rank",
    "signature": "def prev_rank(self)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 404,
    "qualname": "GroupCoordinator.graph_capture",
    "signature": "def graph_capture(self, graph_capture_context: Optional[GraphCaptureContext])"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 464,
    "qualname": "GroupCoordinator.all_reduce",
    "signature": "def all_reduce(self, input_: torch.Tensor)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 571,
    "qualname": "GroupCoordinator.reduce_scatter_tensor",
    "signature": "def reduce_scatter_tensor(self, output: torch.Tensor, input: torch.Tensor)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 580,
    "qualname": "GroupCoordinator.reduce_scatter",
    "signature": "def reduce_scatter(self, output: torch.Tensor, input_list: List[torch.Tensor])"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 589,
    "qualname": "GroupCoordinator.reduce_scatterv",
    "signature": "def reduce_scatterv(self, input_: torch.Tensor, output: Optional[torch.Tensor], sizes: Optional[List[int]])"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 631,
    "qualname": "GroupCoordinator.all_gather_into_tensor",
    "signature": "def all_gather_into_tensor(self, output: torch.Tensor, input: torch.Tensor)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 639,
    "qualname": "GroupCoordinator.all_gather",
    "signature": "def all_gather(self, input_: torch.Tensor, dim: int, output_tensor_list: Optional[List[torch.Tensor]])"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 712,
    "qualname": "GroupCoordinator.all_gatherv",
    "signature": "def all_gatherv(self, input_: Union[torch.Tensor, List[torch.Tensor]], sizes: Optional[List[int]])"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 760,
    "qualname": "GroupCoordinator.gather",
    "signature": "def gather(self, input_: torch.Tensor, dst: int, dim: int)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 795,
    "qualname": "GroupCoordinator.broadcast",
    "signature": "def broadcast(self, input_: torch.Tensor, src: int)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 810,
    "qualname": "GroupCoordinator.broadcast_object",
    "signature": "def broadcast_object(self, obj: Optional[Any], src: int)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 834,
    "qualname": "GroupCoordinator.broadcast_object_list",
    "signature": "def broadcast_object_list(self, obj_list: List[Any], src: int, group: Optional[ProcessGroup])"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 851,
    "qualname": "GroupCoordinator.send_object",
    "signature": "def send_object(self, obj: Any, dst: int)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 885,
    "qualname": "GroupCoordinator.recv_object",
    "signature": "def recv_object(self, src: int)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 923,
    "qualname": "GroupCoordinator.broadcast_tensor_dict",
    "signature": "def broadcast_tensor_dict(self, tensor_dict: Optional[Dict[str, Union[torch.Tensor, Any]]], src: int, group: Optional[ProcessGroup], metadata_group: Optional[ProcessGroup])"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1005,
    "qualname": "GroupCoordinator.send_tensor_dict",
    "signature": "def send_tensor_dict(self, tensor_dict: Dict[str, Union[torch.Tensor, Any]], dst: Optional[int], all_gather_group: Optional['GroupCoordinator'])"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1060,
    "qualname": "GroupCoordinator.recv_tensor_dict",
    "signature": "def recv_tensor_dict(self, src: Optional[int], all_gather_group: Optional['GroupCoordinator'])"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1122,
    "qualname": "GroupCoordinator.barrier",
    "signature": "def barrier(self)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1131,
    "qualname": "GroupCoordinator.send",
    "signature": "def send(self, tensor: torch.Tensor, dst: Optional[int])"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1143,
    "qualname": "GroupCoordinator.recv",
    "signature": "def recv(self, size: torch.Size, dtype: torch.dtype, src: Optional[int])"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1159,
    "qualname": "GroupCoordinator.destroy",
    "signature": "def destroy(self)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1177,
    "qualname": "get_world_group",
    "signature": "def get_world_group()"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1182,
    "qualname": "init_world_group",
    "signature": "def init_world_group(ranks: List[int], local_rank: int, backend: str)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1199,
    "qualname": "init_model_parallel_group",
    "signature": "def init_model_parallel_group(group_ranks: List[List[int]], local_rank: int, backend: str, use_custom_allreduce: Optional[bool], use_message_queue_broadcaster: bool, group_name: Optional[str], use_mscclpp_allreduce: Optional[bool])"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1235,
    "qualname": "set_pdmux_status",
    "signature": "def set_pdmux_status(enable_prefill_multiplexing: bool)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1240,
    "qualname": "get_tp_group",
    "signature": "def get_tp_group()"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1254,
    "qualname": "get_moe_ep_group",
    "signature": "def get_moe_ep_group()"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1259,
    "qualname": "get_moe_tp_group",
    "signature": "def get_moe_tp_group()"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1270,
    "qualname": "get_pp_group",
    "signature": "def get_pp_group()"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1280,
    "qualname": "graph_capture",
    "signature": "def graph_capture()"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1306,
    "qualname": "set_custom_all_reduce",
    "signature": "def set_custom_all_reduce(enable: bool)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1311,
    "qualname": "set_mscclpp_all_reduce",
    "signature": "def set_mscclpp_all_reduce(enable: bool)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1316,
    "qualname": "init_distributed_environment",
    "signature": "def init_distributed_environment(world_size: int, rank: int, distributed_init_method: str, local_rank: int, backend: str, timeout: Optional[int])"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1371,
    "qualname": "initialize_model_parallel",
    "signature": "def initialize_model_parallel(tensor_model_parallel_size: int, expert_model_parallel_size: int, pipeline_model_parallel_size: int, backend: Optional[str], duplicate_tp_group: bool)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1508,
    "qualname": "ensure_model_parallel_initialized",
    "signature": "def ensure_model_parallel_initialized(tensor_model_parallel_size: int, expert_model_parallel_size: int, pipeline_model_parallel_size: int, backend: Optional[str])"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1541,
    "qualname": "model_parallel_is_initialized",
    "signature": "def model_parallel_is_initialized()"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1550,
    "qualname": "patch_tensor_parallel_group",
    "signature": "def patch_tensor_parallel_group(tp_group: GroupCoordinator)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1574,
    "qualname": "get_tensor_model_parallel_world_size",
    "signature": "def get_tensor_model_parallel_world_size()"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1579,
    "qualname": "get_tensor_model_parallel_rank",
    "signature": "def get_tensor_model_parallel_rank()"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1584,
    "qualname": "get_moe_expert_parallel_world_size",
    "signature": "def get_moe_expert_parallel_world_size()"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1589,
    "qualname": "get_moe_expert_parallel_rank",
    "signature": "def get_moe_expert_parallel_rank()"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1594,
    "qualname": "get_moe_tensor_parallel_world_size",
    "signature": "def get_moe_tensor_parallel_world_size()"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1599,
    "qualname": "get_moe_tensor_parallel_rank",
    "signature": "def get_moe_tensor_parallel_rank()"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1604,
    "qualname": "destroy_model_parallel",
    "signature": "def destroy_model_parallel()"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1617,
    "qualname": "destroy_distributed_environment",
    "signature": "def destroy_distributed_environment()"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1626,
    "qualname": "cleanup_dist_env_and_memory",
    "signature": "def cleanup_dist_env_and_memory(shutdown_ray: bool)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1651,
    "qualname": "in_the_same_node_as",
    "signature": "def in_the_same_node_as(pg: ProcessGroup, source_rank: int)"
  },
  {
    "module": "srt.distributed.parallel_state",
    "file": "python/sglang/srt/distributed/parallel_state.py",
    "line": 1722,
    "qualname": "monkey_patch_vllm_parallel_state",
    "signature": "def monkey_patch_vllm_parallel_state(reverse: bool)"
  },
  {
    "module": "srt.entrypoints.EngineBase",
    "file": "python/sglang/srt/entrypoints/EngineBase.py",
    "line": 14,
    "qualname": "EngineBase.generate",
    "signature": "def generate(self, prompt: Optional[Union[List[str], str]], sampling_params: Optional[Union[List[Dict], Dict]], input_ids: Optional[Union[List[List[int]], List[int]]], image_data: Optional[Union[List[str], str]], return_logprob: Optional[Union[List[bool], bool]], logprob_start_len: Optional[Union[List[int], int]], top_logprobs_num: Optional[Union[List[int], int]], token_ids_logprob: Optional[Union[List[List[int]], List[int]]], lora_path: Optional[Union[List[Optional[str]], Optional[str]]], custom_logit_processor: Optional[Union[List[str], str]], return_hidden_states: Optional[bool], stream: Optional[bool], bootstrap_host: Optional[Union[List[str], str]], bootstrap_port: Optional[Union[List[int], int]], bootstrap_room: Optional[Union[List[int], int]], data_parallel_rank: Optional[int])"
  },
  {
    "module": "srt.entrypoints.EngineBase",
    "file": "python/sglang/srt/entrypoints/EngineBase.py",
    "line": 37,
    "qualname": "EngineBase.flush_cache",
    "signature": "def flush_cache(self)"
  },
  {
    "module": "srt.entrypoints.EngineBase",
    "file": "python/sglang/srt/entrypoints/EngineBase.py",
    "line": 42,
    "qualname": "EngineBase.update_weights_from_tensor",
    "signature": "def update_weights_from_tensor(self, named_tensors: List[Tuple[str, torch.Tensor]], load_format: Optional[str], flush_cache: bool)"
  },
  {
    "module": "srt.entrypoints.EngineBase",
    "file": "python/sglang/srt/entrypoints/EngineBase.py",
    "line": 51,
    "qualname": "EngineBase.load_lora_adapter",
    "signature": "def load_lora_adapter(self, lora_name: str, lora_path: str)"
  },
  {
    "module": "srt.entrypoints.EngineBase",
    "file": "python/sglang/srt/entrypoints/EngineBase.py",
    "line": 55,
    "qualname": "EngineBase.unload_lora_adapter",
    "signature": "def unload_lora_adapter(self, lora_name: str)"
  },
  {
    "module": "srt.entrypoints.EngineBase",
    "file": "python/sglang/srt/entrypoints/EngineBase.py",
    "line": 60,
    "qualname": "EngineBase.release_memory_occupation",
    "signature": "def release_memory_occupation(self)"
  },
  {
    "module": "srt.entrypoints.EngineBase",
    "file": "python/sglang/srt/entrypoints/EngineBase.py",
    "line": 65,
    "qualname": "EngineBase.resume_memory_occupation",
    "signature": "def resume_memory_occupation(self)"
  },
  {
    "module": "srt.entrypoints.EngineBase",
    "file": "python/sglang/srt/entrypoints/EngineBase.py",
    "line": 70,
    "qualname": "EngineBase.shutdown",
    "signature": "def shutdown(self)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 28,
    "qualname": "ConversationContext.append_output",
    "signature": "def append_output(self, output)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 32,
    "qualname": "ConversationContext.call_tool",
    "signature": "async def call_tool(self)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 36,
    "qualname": "ConversationContext.need_builtin_tool_call",
    "signature": "def need_builtin_tool_call(self)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 40,
    "qualname": "ConversationContext.render_for_completion",
    "signature": "def render_for_completion(self)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 46,
    "qualname": "SimpleContext.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 49,
    "qualname": "SimpleContext.append_output",
    "signature": "def append_output(self, output)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 52,
    "qualname": "SimpleContext.need_builtin_tool_call",
    "signature": "def need_builtin_tool_call(self)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 55,
    "qualname": "SimpleContext.call_tool",
    "signature": "async def call_tool(self)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 58,
    "qualname": "SimpleContext.render_for_completion",
    "signature": "def render_for_completion(self)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 64,
    "qualname": "HarmonyContext.__init__",
    "signature": "def __init__(self, messages: list, tool_sessions: dict[str, Union['ClientSession', Tool]])"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 82,
    "qualname": "HarmonyContext.append_output",
    "signature": "def append_output(self, output)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 114,
    "qualname": "HarmonyContext.messages",
    "signature": "def messages(self)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 117,
    "qualname": "HarmonyContext.need_builtin_tool_call",
    "signature": "def need_builtin_tool_call(self)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 126,
    "qualname": "HarmonyContext.call_tool",
    "signature": "async def call_tool(self)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 142,
    "qualname": "HarmonyContext.render_for_completion",
    "signature": "def render_for_completion(self)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 145,
    "qualname": "HarmonyContext.call_search_tool",
    "signature": "async def call_search_tool(self, tool_session: Union['ClientSession', Tool], last_msg: Message)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 158,
    "qualname": "HarmonyContext.call_python_tool",
    "signature": "async def call_python_tool(self, tool_session: Union['ClientSession', Tool], last_msg: Message)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 184,
    "qualname": "StreamingHarmonyContext.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 193,
    "qualname": "StreamingHarmonyContext.messages",
    "signature": "def messages(self)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 196,
    "qualname": "StreamingHarmonyContext.append_output",
    "signature": "def append_output(self, output)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 226,
    "qualname": "StreamingHarmonyContext.is_expecting_start",
    "signature": "def is_expecting_start(self)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 229,
    "qualname": "StreamingHarmonyContext.is_assistant_action_turn",
    "signature": "def is_assistant_action_turn(self)"
  },
  {
    "module": "srt.entrypoints.context",
    "file": "python/sglang/srt/entrypoints/context.py",
    "line": 232,
    "qualname": "StreamingHarmonyContext.render_for_completion",
    "signature": "def render_for_completion(self)"
  },
  {
    "module": "srt.entrypoints.harmony_utils",
    "file": "python/sglang/srt/entrypoints/harmony_utils.py",
    "line": 54,
    "qualname": "get_encoding",
    "signature": "def get_encoding()"
  },
  {
    "module": "srt.entrypoints.harmony_utils",
    "file": "python/sglang/srt/entrypoints/harmony_utils.py",
    "line": 61,
    "qualname": "get_system_message",
    "signature": "def get_system_message(model_identity: Optional[str], reasoning_effort: Optional[Literal['high', 'medium', 'low']], start_date: Optional[str], browser_description: Optional[str], python_description: Optional[str])"
  },
  {
    "module": "srt.entrypoints.harmony_utils",
    "file": "python/sglang/srt/entrypoints/harmony_utils.py",
    "line": 86,
    "qualname": "get_developer_message",
    "signature": "def get_developer_message(instructions: Optional[str], tools: Optional[list[Tool]])"
  },
  {
    "module": "srt.entrypoints.harmony_utils",
    "file": "python/sglang/srt/entrypoints/harmony_utils.py",
    "line": 118,
    "qualname": "get_user_message",
    "signature": "def get_user_message(content: str)"
  },
  {
    "module": "srt.entrypoints.harmony_utils",
    "file": "python/sglang/srt/entrypoints/harmony_utils.py",
    "line": 122,
    "qualname": "parse_response_input",
    "signature": "def parse_response_input(response_msg: ResponseInputOutputItem, prev_responses: list[Union[ResponseOutputItem, ResponseReasoningItem]])"
  },
  {
    "module": "srt.entrypoints.harmony_utils",
    "file": "python/sglang/srt/entrypoints/harmony_utils.py",
    "line": 174,
    "qualname": "parse_response_output",
    "signature": "def parse_response_output(output: ResponseOutputItem)"
  },
  {
    "module": "srt.entrypoints.harmony_utils",
    "file": "python/sglang/srt/entrypoints/harmony_utils.py",
    "line": 190,
    "qualname": "parse_chat_input",
    "signature": "def parse_chat_input(chat_msg)"
  },
  {
    "module": "srt.entrypoints.harmony_utils",
    "file": "python/sglang/srt/entrypoints/harmony_utils.py",
    "line": 202,
    "qualname": "render_for_completion",
    "signature": "def render_for_completion(messages: list[Message])"
  },
  {
    "module": "srt.entrypoints.harmony_utils",
    "file": "python/sglang/srt/entrypoints/harmony_utils.py",
    "line": 210,
    "qualname": "get_stop_tokens_for_assistant_actions",
    "signature": "def get_stop_tokens_for_assistant_actions()"
  },
  {
    "module": "srt.entrypoints.harmony_utils",
    "file": "python/sglang/srt/entrypoints/harmony_utils.py",
    "line": 214,
    "qualname": "get_streamable_parser_for_assistant",
    "signature": "def get_streamable_parser_for_assistant()"
  },
  {
    "module": "srt.entrypoints.harmony_utils",
    "file": "python/sglang/srt/entrypoints/harmony_utils.py",
    "line": 218,
    "qualname": "parse_output_message",
    "signature": "def parse_output_message(message: Message)"
  },
  {
    "module": "srt.entrypoints.harmony_utils",
    "file": "python/sglang/srt/entrypoints/harmony_utils.py",
    "line": 324,
    "qualname": "parse_remaining_state",
    "signature": "def parse_remaining_state(parser: StreamableParser)"
  },
  {
    "module": "srt.entrypoints.harmony_utils",
    "file": "python/sglang/srt/entrypoints/harmony_utils.py",
    "line": 368,
    "qualname": "parse_output_into_messages",
    "signature": "def parse_output_into_messages(token_ids: Iterable[int])"
  },
  {
    "module": "srt.entrypoints.http_server_engine",
    "file": "python/sglang/srt/entrypoints/http_server_engine.py",
    "line": 20,
    "qualname": "launch_server_process",
    "signature": "def launch_server_process(server_args: ServerArgs)"
  },
  {
    "module": "srt.entrypoints.http_server_engine",
    "file": "python/sglang/srt/entrypoints/http_server_engine.py",
    "line": 58,
    "qualname": "HttpServerEngineAdapter.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.entrypoints.http_server_engine",
    "file": "python/sglang/srt/entrypoints/http_server_engine.py",
    "line": 78,
    "qualname": "HttpServerEngineAdapter.update_weights_from_tensor",
    "signature": "def update_weights_from_tensor(self, named_tensors: List[Tuple[str, torch.Tensor]], load_format: Optional[str], flush_cache: bool)"
  },
  {
    "module": "srt.entrypoints.http_server_engine",
    "file": "python/sglang/srt/entrypoints/http_server_engine.py",
    "line": 102,
    "qualname": "HttpServerEngineAdapter.shutdown",
    "signature": "def shutdown(self)"
  },
  {
    "module": "srt.entrypoints.http_server_engine",
    "file": "python/sglang/srt/entrypoints/http_server_engine.py",
    "line": 105,
    "qualname": "HttpServerEngineAdapter.generate",
    "signature": "def generate(self, prompt, sampling_params, input_ids, image_data, return_logprob, logprob_start_len, top_logprobs_num, token_ids_logprob, lora_path, custom_logit_processor)"
  },
  {
    "module": "srt.entrypoints.http_server_engine",
    "file": "python/sglang/srt/entrypoints/http_server_engine.py",
    "line": 135,
    "qualname": "HttpServerEngineAdapter.release_memory_occupation",
    "signature": "def release_memory_occupation(self)"
  },
  {
    "module": "srt.entrypoints.http_server_engine",
    "file": "python/sglang/srt/entrypoints/http_server_engine.py",
    "line": 138,
    "qualname": "HttpServerEngineAdapter.resume_memory_occupation",
    "signature": "def resume_memory_occupation(self)"
  },
  {
    "module": "srt.entrypoints.http_server_engine",
    "file": "python/sglang/srt/entrypoints/http_server_engine.py",
    "line": 141,
    "qualname": "HttpServerEngineAdapter.flush_cache",
    "signature": "def flush_cache(self)"
  },
  {
    "module": "srt.entrypoints.tool",
    "file": "python/sglang/srt/entrypoints/tool.py",
    "line": 19,
    "qualname": "Tool.get_result",
    "signature": "async def get_result(self, context: 'ConversationContext')"
  },
  {
    "module": "srt.entrypoints.tool",
    "file": "python/sglang/srt/entrypoints/tool.py",
    "line": 25,
    "qualname": "HarmonyBrowserTool.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.entrypoints.tool",
    "file": "python/sglang/srt/entrypoints/tool.py",
    "line": 45,
    "qualname": "HarmonyBrowserTool.get_result",
    "signature": "async def get_result(self, context: 'ConversationContext')"
  },
  {
    "module": "srt.entrypoints.tool",
    "file": "python/sglang/srt/entrypoints/tool.py",
    "line": 56,
    "qualname": "HarmonyBrowserTool.tool_config",
    "signature": "def tool_config(self)"
  },
  {
    "module": "srt.entrypoints.tool",
    "file": "python/sglang/srt/entrypoints/tool.py",
    "line": 62,
    "qualname": "HarmonyPythonTool.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.entrypoints.tool",
    "file": "python/sglang/srt/entrypoints/tool.py",
    "line": 75,
    "qualname": "HarmonyPythonTool.get_result",
    "signature": "async def get_result(self, context: 'ConversationContext')"
  },
  {
    "module": "srt.entrypoints.tool",
    "file": "python/sglang/srt/entrypoints/tool.py",
    "line": 86,
    "qualname": "HarmonyPythonTool.tool_config",
    "signature": "def tool_config(self)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 103,
    "qualname": "Engine.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 140,
    "qualname": "Engine.generate",
    "signature": "def generate(self, prompt: Optional[Union[List[str], str]], sampling_params: Optional[Union[List[Dict], Dict]], input_ids: Optional[Union[List[List[int]], List[int]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat], return_logprob: Optional[Union[List[bool], bool]], logprob_start_len: Optional[Union[List[int], int]], top_logprobs_num: Optional[Union[List[int], int]], token_ids_logprob: Optional[Union[List[List[int]], List[int]]], lora_path: Optional[List[Optional[str]]], custom_logit_processor: Optional[Union[List[str], str]], return_hidden_states: bool, stream: bool, bootstrap_host: Optional[Union[List[str], str]], bootstrap_port: Optional[Union[List[int], int]], bootstrap_room: Optional[Union[List[int], int]], data_parallel_rank: Optional[int])"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 221,
    "qualname": "Engine.async_generate",
    "signature": "async def async_generate(self, prompt: Optional[Union[List[str], str]], sampling_params: Optional[Union[List[Dict], Dict]], input_ids: Optional[Union[List[List[int]], List[int]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat], return_logprob: Optional[Union[List[bool], bool]], logprob_start_len: Optional[Union[List[int], int]], top_logprobs_num: Optional[Union[List[int], int]], token_ids_logprob: Optional[Union[List[List[int]], List[int]]], lora_path: Optional[List[Optional[str]]], custom_logit_processor: Optional[Union[List[str], str]], return_hidden_states: bool, stream: bool, bootstrap_host: Optional[Union[List[str], str]], bootstrap_port: Optional[Union[List[int], int]], bootstrap_room: Optional[Union[List[int], int]], data_parallel_rank: Optional[int])"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 293,
    "qualname": "Engine.encode",
    "signature": "def encode(self, prompt: Union[str, List[str], List[Dict], List[List[Dict]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat])"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 315,
    "qualname": "Engine.async_encode",
    "signature": "async def async_encode(self, prompt: Union[str, List[str], List[Dict], List[List[Dict]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat])"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 337,
    "qualname": "Engine.rerank",
    "signature": "def rerank(self, prompt: Union[List[List[str]]])"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 351,
    "qualname": "Engine.shutdown",
    "signature": "def shutdown(self)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 355,
    "qualname": "Engine.__enter__",
    "signature": "def __enter__(self)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 358,
    "qualname": "Engine.__exit__",
    "signature": "def __exit__(self, exc_type, exc_value, traceback)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 362,
    "qualname": "Engine.flush_cache",
    "signature": "def flush_cache(self)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 366,
    "qualname": "Engine.start_profile",
    "signature": "def start_profile(self)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 370,
    "qualname": "Engine.stop_profile",
    "signature": "def stop_profile(self)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 374,
    "qualname": "Engine.start_expert_distribution_record",
    "signature": "def start_expert_distribution_record(self)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 380,
    "qualname": "Engine.stop_expert_distribution_record",
    "signature": "def stop_expert_distribution_record(self)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 386,
    "qualname": "Engine.dump_expert_distribution_record",
    "signature": "def dump_expert_distribution_record(self)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 392,
    "qualname": "Engine.get_server_info",
    "signature": "def get_server_info(self)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 404,
    "qualname": "Engine.init_weights_update_group",
    "signature": "def init_weights_update_group(self, master_address: str, master_port: int, rank_offset: int, world_size: int, group_name: str, backend: str)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 427,
    "qualname": "Engine.update_weights_from_distributed",
    "signature": "def update_weights_from_distributed(self, names: list[str], dtypes: list[str], shapes: list[list[int]], group_name: str, flush_cache: bool)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 448,
    "qualname": "Engine.update_weights_from_tensor",
    "signature": "def update_weights_from_tensor(self, named_tensors: List[Tuple[str, torch.Tensor]], load_format: Optional[str], flush_cache: bool)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 474,
    "qualname": "Engine.update_weights_from_disk",
    "signature": "def update_weights_from_disk(self, model_path: str, load_format: Optional[str])"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 495,
    "qualname": "Engine.get_weights_by_name",
    "signature": "def get_weights_by_name(self, name: str, truncate_size: int)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 503,
    "qualname": "Engine.load_lora_adapter",
    "signature": "def load_lora_adapter(self, lora_name: str, lora_path: str, pinned: bool)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 517,
    "qualname": "Engine.unload_lora_adapter",
    "signature": "def unload_lora_adapter(self, lora_name: str)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 527,
    "qualname": "Engine.release_memory_occupation",
    "signature": "def release_memory_occupation(self, tags: Optional[List[str]])"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 534,
    "qualname": "Engine.resume_memory_occupation",
    "signature": "def resume_memory_occupation(self, tags: Optional[List[str]])"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 541,
    "qualname": "Engine.freeze_gc",
    "signature": "def freeze_gc(self)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 561,
    "qualname": "Engine.collective_rpc",
    "signature": "def collective_rpc(self, method: str)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 568,
    "qualname": "Engine.save_remote_model",
    "signature": "def save_remote_model(self)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 571,
    "qualname": "Engine.save_sharded_model",
    "signature": "def save_sharded_model(self)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 574,
    "qualname": "Engine.score",
    "signature": "def score(self, query: Optional[Union[str, List[int]]], items: Optional[Union[str, List[str], List[List[int]]]], label_token_ids: Optional[List[int]], apply_softmax: bool, item_first: bool)"
  },
  {
    "module": "srt.entrypoints.engine",
    "file": "python/sglang/srt/entrypoints/engine.py",
    "line": 625,
    "qualname": "Engine.async_score",
    "signature": "async def async_score(self, query: Optional[Union[str, List[int]]], items: Optional[Union[str, List[str], List[List[int]]]], label_token_ids: Optional[List[int]], apply_softmax: bool, item_first: bool)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 128,
    "qualname": "set_global_state",
    "signature": "def set_global_state(global_state: _GlobalState)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 134,
    "qualname": "lifespan",
    "signature": "async def lifespan(fast_api_app: FastAPI)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 212,
    "qualname": "validation_exception_handler",
    "signature": "async def validation_exception_handler(request: Request, exc: HTTPException)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 225,
    "qualname": "validation_exception_handler",
    "signature": "async def validation_exception_handler(request: Request, exc: RequestValidationError)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 247,
    "qualname": "validate_json_request",
    "signature": "async def validate_json_request(raw_request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 268,
    "qualname": "health_generate",
    "signature": "async def health_generate(request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 339,
    "qualname": "get_model_info",
    "signature": "async def get_model_info()"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 352,
    "qualname": "get_weight_version",
    "signature": "async def get_weight_version()"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 360,
    "qualname": "get_server_info",
    "signature": "async def get_server_info()"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 374,
    "qualname": "get_load",
    "signature": "async def get_load()"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 381,
    "qualname": "set_internal_state",
    "signature": "async def set_internal_state(obj: SetInternalStateReq, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 388,
    "qualname": "generate_request",
    "signature": "async def generate_request(obj: GenerateReqInput, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 425,
    "qualname": "generate_from_file_request",
    "signature": "async def generate_from_file_request(file: UploadFile, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 449,
    "qualname": "encode_request",
    "signature": "async def encode_request(obj: EmbeddingReqInput, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 461,
    "qualname": "classify_request",
    "signature": "async def classify_request(obj: EmbeddingReqInput, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 473,
    "qualname": "flush_cache",
    "signature": "async def flush_cache()"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 484,
    "qualname": "start_profile_async",
    "signature": "async def start_profile_async(obj: Optional[ProfileReqInput])"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 505,
    "qualname": "stop_profile_async",
    "signature": "async def stop_profile_async()"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 515,
    "qualname": "freeze_gc_async",
    "signature": "async def freeze_gc_async()"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 527,
    "qualname": "start_expert_distribution_record_async",
    "signature": "async def start_expert_distribution_record_async()"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 537,
    "qualname": "stop_expert_distribution_record_async",
    "signature": "async def stop_expert_distribution_record_async()"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 547,
    "qualname": "dump_expert_distribution_record_async",
    "signature": "async def dump_expert_distribution_record_async()"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 557,
    "qualname": "update_weights_from_disk",
    "signature": "async def update_weights_from_disk(obj: UpdateWeightFromDiskReqInput, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 586,
    "qualname": "init_weights_update_group",
    "signature": "async def init_weights_update_group(obj: InitWeightsUpdateGroupReqInput, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 601,
    "qualname": "update_weights_from_tensor",
    "signature": "async def update_weights_from_tensor(obj: UpdateWeightsFromTensorReqInput, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 627,
    "qualname": "update_weights_from_distributed",
    "signature": "async def update_weights_from_distributed(obj: UpdateWeightsFromDistributedReqInput, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 650,
    "qualname": "update_weight_version",
    "signature": "async def update_weight_version(obj: UpdateWeightVersionReqInput, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 680,
    "qualname": "get_weights_by_name",
    "signature": "async def get_weights_by_name(obj: GetWeightsByNameReqInput, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 693,
    "qualname": "release_memory_occupation",
    "signature": "async def release_memory_occupation(obj: ReleaseMemoryOccupationReqInput, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 704,
    "qualname": "resume_memory_occupation",
    "signature": "async def resume_memory_occupation(obj: ResumeMemoryOccupationReqInput, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 715,
    "qualname": "slow_down",
    "signature": "async def slow_down(obj: SlowDownReqInput, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 728,
    "qualname": "load_lora_adapter",
    "signature": "async def load_lora_adapter(obj: LoadLoRAAdapterReqInput, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 745,
    "qualname": "unload_lora_adapter",
    "signature": "async def unload_lora_adapter(obj: UnloadLoRAAdapterReqInput, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 762,
    "qualname": "open_session",
    "signature": "async def open_session(obj: OpenSessionReqInput, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 776,
    "qualname": "close_session",
    "signature": "async def close_session(obj: CloseSessionReqInput, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 786,
    "qualname": "configure_logging",
    "signature": "async def configure_logging(obj: ConfigureLoggingReq, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 793,
    "qualname": "abort_request",
    "signature": "async def abort_request(obj: AbortReq, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 805,
    "qualname": "parse_function_call_request",
    "signature": "async def parse_function_call_request(obj: ParseFunctionCallReq, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 827,
    "qualname": "separate_reasoning_request",
    "signature": "async def separate_reasoning_request(obj: SeparateReasoningReqInput, request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 847,
    "qualname": "pause_generation",
    "signature": "async def pause_generation(request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 857,
    "qualname": "continue_generation",
    "signature": "async def continue_generation(request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 870,
    "qualname": "openai_v1_completions",
    "signature": "async def openai_v1_completions(request: CompletionRequest, raw_request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 878,
    "qualname": "openai_v1_chat_completions",
    "signature": "async def openai_v1_chat_completions(request: ChatCompletionRequest, raw_request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 892,
    "qualname": "openai_v1_embeddings",
    "signature": "async def openai_v1_embeddings(request: EmbeddingRequest, raw_request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 900,
    "qualname": "available_models",
    "signature": "async def available_models()"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 916,
    "qualname": "retrieve_model",
    "signature": "async def retrieve_model(model: str)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 941,
    "qualname": "v1_score_request",
    "signature": "async def v1_score_request(request: ScoringRequest, raw_request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 949,
    "qualname": "v1_responses_request",
    "signature": "async def v1_responses_request(request: dict, raw_request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 969,
    "qualname": "v1_retrieve_responses",
    "signature": "async def v1_retrieve_responses(response_id: str, raw_request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 977,
    "qualname": "v1_cancel_responses",
    "signature": "async def v1_cancel_responses(response_id: str, raw_request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 987,
    "qualname": "v1_rerank_request",
    "signature": "async def v1_rerank_request(request: V1RerankReqInput, raw_request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 996,
    "qualname": "sagemaker_health",
    "signature": "async def sagemaker_health()"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 1002,
    "qualname": "sagemaker_chat_completions",
    "signature": "async def sagemaker_chat_completions(request: ChatCompletionRequest, raw_request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 1013,
    "qualname": "vertex_generate",
    "signature": "async def vertex_generate(vertex_req: VertexGenerateReqInput, raw_request: Request)"
  },
  {
    "module": "srt.entrypoints.http_server",
    "file": "python/sglang/srt/entrypoints/http_server.py",
    "line": 1051,
    "qualname": "launch_server",
    "signature": "def launch_server(server_args: ServerArgs, pipe_finish_writer: Optional[multiprocessing.connection.Connection], launch_callback: Optional[Callable[[], None]])"
  },
  {
    "module": "srt.eplb.expert_location",
    "file": "python/sglang/srt/eplb/expert_location.py",
    "line": 46,
    "qualname": "ExpertLocationMetadata.num_layers",
    "signature": "def num_layers(self)"
  },
  {
    "module": "srt.eplb.expert_location",
    "file": "python/sglang/srt/eplb/expert_location.py",
    "line": 50,
    "qualname": "ExpertLocationMetadata.num_physical_experts",
    "signature": "def num_physical_experts(self)"
  },
  {
    "module": "srt.eplb.expert_location",
    "file": "python/sglang/srt/eplb/expert_location.py",
    "line": 54,
    "qualname": "ExpertLocationMetadata.num_local_physical_experts",
    "signature": "def num_local_physical_experts(self)"
  },
  {
    "module": "srt.eplb.expert_location",
    "file": "python/sglang/srt/eplb/expert_location.py",
    "line": 60,
    "qualname": "ExpertLocationMetadata.num_logical_experts",
    "signature": "def num_logical_experts(self)"
  },
  {
    "module": "srt.eplb.expert_location",
    "file": "python/sglang/srt/eplb/expert_location.py",
    "line": 64,
    "qualname": "ExpertLocationMetadata.ep_size",
    "signature": "def ep_size(self)"
  },
  {
    "module": "srt.eplb.expert_location",
    "file": "python/sglang/srt/eplb/expert_location.py",
    "line": 68,
    "qualname": "ExpertLocationMetadata.__post_init__",
    "signature": "def __post_init__(self)"
  },
  {
    "module": "srt.eplb.expert_location",
    "file": "python/sglang/srt/eplb/expert_location.py",
    "line": 83,
    "qualname": "ExpertLocationMetadata.init_trivial",
    "signature": "def init_trivial(server_args: ServerArgs, model_config: ModelConfig)"
  },
  {
    "module": "srt.eplb.expert_location",
    "file": "python/sglang/srt/eplb/expert_location.py",
    "line": 107,
    "qualname": "ExpertLocationMetadata.init_by_mapping",
    "signature": "def init_by_mapping(server_args: ServerArgs, model_config: ModelConfig, physical_to_logical_map)"
  },
  {
    "module": "srt.eplb.expert_location",
    "file": "python/sglang/srt/eplb/expert_location.py",
    "line": 135,
    "qualname": "ExpertLocationMetadata.init_by_eplb",
    "signature": "def init_by_eplb(server_args: ServerArgs, model_config: ModelConfig, logical_count: torch.Tensor)"
  },
  {
    "module": "srt.eplb.expert_location",
    "file": "python/sglang/srt/eplb/expert_location.py",
    "line": 242,
    "qualname": "ExpertLocationMetadata.update",
    "signature": "def update(self, other: 'ExpertLocationMetadata', update_layer_ids: List[int])"
  },
  {
    "module": "srt.eplb.expert_location",
    "file": "python/sglang/srt/eplb/expert_location.py",
    "line": 273,
    "qualname": "ExpertLocationMetadata.logical_to_all_physical",
    "signature": "def logical_to_all_physical(self, layer_id: int, logical_expert_id: int)"
  },
  {
    "module": "srt.eplb.expert_location",
    "file": "python/sglang/srt/eplb/expert_location.py",
    "line": 289,
    "qualname": "get_global_expert_location_metadata",
    "signature": "def get_global_expert_location_metadata()"
  },
  {
    "module": "srt.eplb.expert_location",
    "file": "python/sglang/srt/eplb/expert_location.py",
    "line": 293,
    "qualname": "set_global_expert_location_metadata",
    "signature": "def set_global_expert_location_metadata(value)"
  },
  {
    "module": "srt.eplb.expert_location",
    "file": "python/sglang/srt/eplb/expert_location.py",
    "line": 337,
    "qualname": "compute_logical_to_rank_dispatch_physical_map",
    "signature": "def compute_logical_to_rank_dispatch_physical_map(logical_to_all_physical_map: torch.Tensor, num_gpus: int, num_physical_experts: int, ep_rank: int, seed: int)"
  },
  {
    "module": "srt.eplb.expert_location",
    "file": "python/sglang/srt/eplb/expert_location.py",
    "line": 421,
    "qualname": "ModelConfigForExpertLocation.from_model_config",
    "signature": "def from_model_config(model_config: ModelConfig)"
  },
  {
    "module": "srt.eplb.expert_location",
    "file": "python/sglang/srt/eplb/expert_location.py",
    "line": 431,
    "qualname": "compute_initial_expert_location_metadata",
    "signature": "def compute_initial_expert_location_metadata(server_args: ServerArgs, model_config: ModelConfig)"
  },
  {
    "module": "srt.eplb.expert_location_dispatch",
    "file": "python/sglang/srt/eplb/expert_location_dispatch.py",
    "line": 36,
    "qualname": "ExpertLocationDispatchInfo.init_new",
    "signature": "def init_new(cls, layer_id: int)"
  },
  {
    "module": "srt.eplb.expert_location_dispatch",
    "file": "python/sglang/srt/eplb/expert_location_dispatch.py",
    "line": 64,
    "qualname": "transform_select_experts_inputs",
    "signature": "def transform_select_experts_inputs(router_logits: torch.Tensor, correction_bias: Optional[torch.Tensor], info: Optional[ExpertLocationDispatchInfo])"
  },
  {
    "module": "srt.eplb.expert_location_dispatch",
    "file": "python/sglang/srt/eplb/expert_location_dispatch.py",
    "line": 76,
    "qualname": "topk_ids_logical_to_physical",
    "signature": "def topk_ids_logical_to_physical(topk_ids: torch.Tensor, info: Optional[ExpertLocationDispatchInfo])"
  },
  {
    "module": "srt.eplb.expert_location_updater",
    "file": "python/sglang/srt/eplb/expert_location_updater.py",
    "line": 37,
    "qualname": "ExpertLocationUpdater.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.eplb.expert_location_updater",
    "file": "python/sglang/srt/eplb/expert_location_updater.py",
    "line": 40,
    "qualname": "ExpertLocationUpdater.update",
    "signature": "def update(self, routed_experts_weights_of_layer: Dict[int, List[torch.Tensor]], new_expert_location_metadata: ExpertLocationMetadata, update_layer_ids: List[int], nnodes: int, rank: int)"
  },
  {
    "module": "srt.eplb.expert_location_updater",
    "file": "python/sglang/srt/eplb/expert_location_updater.py",
    "line": 160,
    "qualname": "create_temp_buffers",
    "signature": "def create_temp_buffers(sample_tensors)"
  },
  {
    "module": "srt.eplb.expert_location_updater",
    "file": "python/sglang/srt/eplb/expert_location_updater.py",
    "line": 164,
    "qualname": "update_expert_weights_single_layer",
    "signature": "def update_expert_weights_single_layer(routed_experts_weights: List[torch.Tensor], temp_buffers: List[torch.Tensor], old_physical_to_logical_map: List[int], new_physical_to_logical_map: List[int], num_local_physical_experts: int, num_gpu_per_node: int, rank: int, world_size: Optional[int], debug: bool, log_metrics: bool)"
  },
  {
    "module": "srt.eplb.expert_location_updater",
    "file": "python/sglang/srt/eplb/expert_location_updater.py",
    "line": 474,
    "qualname": "_ChunkUtils.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.eplb.expert_location_updater",
    "file": "python/sglang/srt/eplb/expert_location_updater.py",
    "line": 478,
    "qualname": "_ChunkUtils.chunk_value_from_element_value",
    "signature": "def chunk_value_from_element_value(self, element_value)"
  },
  {
    "module": "srt.eplb.expert_location_updater",
    "file": "python/sglang/srt/eplb/expert_location_updater.py",
    "line": 486,
    "qualname": "_ChunkUtils.element_values_from_chunk_value",
    "signature": "def element_values_from_chunk_value(self, chunk_value)"
  },
  {
    "module": "srt.eplb.eplb_manager",
    "file": "python/sglang/srt/eplb/eplb_manager.py",
    "line": 17,
    "qualname": "EPLBManager.__init__",
    "signature": "def __init__(self, model_runner: 'ModelRunner')"
  },
  {
    "module": "srt.eplb.eplb_manager",
    "file": "python/sglang/srt/eplb/eplb_manager.py",
    "line": 41,
    "qualname": "EPLBManager.on_forward_pass_end",
    "signature": "def on_forward_pass_end(self)"
  },
  {
    "module": "srt.eplb.eplb_manager",
    "file": "python/sglang/srt/eplb/eplb_manager.py",
    "line": 52,
    "qualname": "EPLBManager.rebalance",
    "signature": "def rebalance(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 43,
    "qualname": "ExpertDistributionRecorder.init_new",
    "signature": "def init_new(server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 61,
    "qualname": "ExpertDistributionRecorder.with_current_layer",
    "signature": "def with_current_layer(self, layer_idx)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 65,
    "qualname": "ExpertDistributionRecorder.with_debug_name",
    "signature": "def with_debug_name(self, debug_name)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 69,
    "qualname": "ExpertDistributionRecorder.disable_this_region",
    "signature": "def disable_this_region(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 73,
    "qualname": "ExpertDistributionRecorder.with_forward_pass",
    "signature": "def with_forward_pass(self, forward_pass_id: int, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 76,
    "qualname": "ExpertDistributionRecorder.on_select_experts",
    "signature": "def on_select_experts(self, topk_ids: torch.Tensor)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 79,
    "qualname": "ExpertDistributionRecorder.on_deepep_dispatch_normal",
    "signature": "def on_deepep_dispatch_normal(self, local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 88,
    "qualname": "ExpertDistributionRecorder.on_deepep_dispatch_low_latency",
    "signature": "def on_deepep_dispatch_low_latency(self, local_physical_count_of_layer: torch.Tensor)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 93,
    "qualname": "ExpertDistributionRecorder.start_record",
    "signature": "def start_record(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 96,
    "qualname": "ExpertDistributionRecorder.stop_record",
    "signature": "def stop_record(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 99,
    "qualname": "ExpertDistributionRecorder.dump_record",
    "signature": "def dump_record(self, output_mode: _OutputMode)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 103,
    "qualname": "ExpertDistributionRecorder.recording",
    "signature": "def recording(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 117,
    "qualname": "_ExpertDistributionRecorderReal.__init__",
    "signature": "def __init__(self, server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 145,
    "qualname": "_ExpertDistributionRecorderReal.with_current_layer",
    "signature": "def with_current_layer(self, layer_idx)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 148,
    "qualname": "_ExpertDistributionRecorderReal.with_debug_name",
    "signature": "def with_debug_name(self, debug_name)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 152,
    "qualname": "_ExpertDistributionRecorderReal.with_forward_pass",
    "signature": "def with_forward_pass(self, forward_pass_id: int, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 161,
    "qualname": "_ExpertDistributionRecorderReal.disable_this_region",
    "signature": "def disable_this_region(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 184,
    "qualname": "_ExpertDistributionRecorderReal.on_select_experts",
    "signature": "def on_select_experts(self, topk_ids: torch.Tensor)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 187,
    "qualname": "_ExpertDistributionRecorderReal.on_deepep_dispatch_normal",
    "signature": "def on_deepep_dispatch_normal(self, local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 202,
    "qualname": "_ExpertDistributionRecorderReal.on_deepep_dispatch_low_latency",
    "signature": "def on_deepep_dispatch_low_latency(self, local_physical_count_of_layer: torch.Tensor)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 232,
    "qualname": "_ExpertDistributionRecorderReal.start_record",
    "signature": "def start_record(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 241,
    "qualname": "_ExpertDistributionRecorderReal.stop_record",
    "signature": "def stop_record(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 249,
    "qualname": "_ExpertDistributionRecorderReal.dump_record",
    "signature": "def dump_record(self, output_mode: _OutputMode)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 256,
    "qualname": "_ExpertDistributionRecorderReal.recording",
    "signature": "def recording(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 265,
    "qualname": "get_global_expert_distribution_recorder",
    "signature": "def get_global_expert_distribution_recorder()"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 269,
    "qualname": "set_global_expert_distribution_recorder",
    "signature": "def set_global_expert_distribution_recorder(value)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 279,
    "qualname": "_SinglePassGatherer.init_new",
    "signature": "def init_new(server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 309,
    "qualname": "_SinglePassGatherer.__init__",
    "signature": "def __init__(self, expert_location_metadata: 'ExpertLocationMetadata', rank: int)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 313,
    "qualname": "_SinglePassGatherer.on_forward_pass_start",
    "signature": "def on_forward_pass_start(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 316,
    "qualname": "_SinglePassGatherer.on_select_experts",
    "signature": "def on_select_experts(self, layer_idx: int, topk_ids: torch.Tensor)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 319,
    "qualname": "_SinglePassGatherer.on_deepep_dispatch_normal",
    "signature": "def on_deepep_dispatch_normal(self, layer_idx: int, local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 329,
    "qualname": "_SinglePassGatherer.on_deepep_dispatch_low_latency",
    "signature": "def on_deepep_dispatch_low_latency(self, layer_idx: int, local_physical_count_of_layer: torch.Tensor)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 334,
    "qualname": "_SinglePassGatherer.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 337,
    "qualname": "_SinglePassGatherer.collect",
    "signature": "def collect(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 345,
    "qualname": "_DetailSinglePassGatherer.__init__",
    "signature": "def __init__(self, server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 369,
    "qualname": "_DetailSinglePassGatherer.on_forward_pass_start",
    "signature": "def on_forward_pass_start(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 380,
    "qualname": "_DetailSinglePassGatherer.on_select_experts",
    "signature": "def on_select_experts(self, layer_idx: int, topk_ids: torch.Tensor)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 385,
    "qualname": "_DetailSinglePassGatherer.on_deepep_dispatch_normal",
    "signature": "def on_deepep_dispatch_normal(self, layer_idx: int, local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 402,
    "qualname": "_DetailSinglePassGatherer.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 407,
    "qualname": "_DetailSinglePassGatherer.collect",
    "signature": "def collect(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 417,
    "qualname": "_LayerBasedCpuSinglePassGatherer.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 430,
    "qualname": "_LayerBasedCpuSinglePassGatherer.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 446,
    "qualname": "_LayerBasedGpuSinglePassGatherer.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 462,
    "qualname": "_LayerBasedGpuSinglePassGatherer.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 465,
    "qualname": "_LayerBasedGpuSinglePassGatherer.collect",
    "signature": "def collect(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 481,
    "qualname": "_SelectExpertsSinglePassGatherer.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 485,
    "qualname": "_SelectExpertsSinglePassGatherer.on_select_experts",
    "signature": "def on_select_experts(self, layer_idx: int, topk_ids: torch.Tensor)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 494,
    "qualname": "_DeepepNormalSinglePassGatherer.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 502,
    "qualname": "_DeepepNormalSinglePassGatherer.on_deepep_dispatch_normal",
    "signature": "def on_deepep_dispatch_normal(self, layer_idx: int, local_physical_count_of_layer: List[int], num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 513,
    "qualname": "_DeepepNormalSinglePassGatherer.collect",
    "signature": "def collect(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 527,
    "qualname": "_DeepepLowLatencySinglePassGatherer.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 530,
    "qualname": "_DeepepLowLatencySinglePassGatherer.on_deepep_dispatch_low_latency",
    "signature": "def on_deepep_dispatch_low_latency(self, layer_idx: int, local_physical_count_of_layer: torch.Tensor)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 561,
    "qualname": "_Accumulator.init_new",
    "signature": "def init_new(server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 571,
    "qualname": "_Accumulator.get_class",
    "signature": "def get_class(server_args: ServerArgs)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 579,
    "qualname": "_Accumulator.__init__",
    "signature": "def __init__(self, server_args: ServerArgs, expert_location_metadata: 'ExpertLocationMetadata', rank: int)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 589,
    "qualname": "_Accumulator.get_single_pass_gatherer_keys",
    "signature": "def get_single_pass_gatherer_keys(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 592,
    "qualname": "_Accumulator.get_single_pass_gatherer_key",
    "signature": "def get_single_pass_gatherer_key(self, debug_name: Optional[str])"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 595,
    "qualname": "_Accumulator.append",
    "signature": "def append(self, forward_pass_id: int, gatherer_key: str, single_pass_data: Dict)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 603,
    "qualname": "_Accumulator.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 606,
    "qualname": "_Accumulator.dump",
    "signature": "def dump(self, output_mode: _OutputMode)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 611,
    "qualname": "_UtilizationRateAccumulatorMixin.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 621,
    "qualname": "_UtilizationRateAccumulatorMixin.append",
    "signature": "def append(self, forward_pass_id: int, gatherer_key: str, single_pass_data: Dict)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 633,
    "qualname": "_UtilizationRateAccumulatorMixin.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 668,
    "qualname": "_DequeCollection.__init__",
    "signature": "def __init__(self, maxlens: List[int])"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 671,
    "qualname": "_DequeCollection.append",
    "signature": "def append(self, value)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 675,
    "qualname": "_DequeCollection.clear",
    "signature": "def clear(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 679,
    "qualname": "_DequeCollection.mean",
    "signature": "def mean(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 684,
    "qualname": "_DetailAccumulator.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 688,
    "qualname": "_DetailAccumulator.get_single_pass_gatherer_keys",
    "signature": "def get_single_pass_gatherer_keys(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 693,
    "qualname": "_DetailAccumulator.get_single_pass_gatherer_key",
    "signature": "def get_single_pass_gatherer_key(self, debug_name: Optional[str])"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 698,
    "qualname": "_DetailAccumulator.append",
    "signature": "def append(self, forward_pass_id: int, gatherer_key: str, single_pass_data: Dict)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 724,
    "qualname": "_DetailAccumulator.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 728,
    "qualname": "_DetailAccumulator.dump",
    "signature": "def dump(self, output_mode: _OutputMode)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 741,
    "qualname": "_StatAccumulator.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 755,
    "qualname": "_StatAccumulator.append",
    "signature": "def append(self, forward_pass_id: int, gatherer_key: str, single_pass_data: Dict)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 767,
    "qualname": "_StatAccumulator.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 771,
    "qualname": "_StatAccumulator.dump",
    "signature": "def dump(self, output_mode: _OutputMode)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 812,
    "qualname": "_Buffer.init_new",
    "signature": "def init_new(item_shape: Tuple, buffer_size: int, dtype, device)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 818,
    "qualname": "_Buffer.append",
    "signature": "def append(self, value: torch.Tensor)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 821,
    "qualname": "_Buffer.get_all",
    "signature": "def get_all(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 824,
    "qualname": "_Buffer.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 829,
    "qualname": "_CircularBuffer.__init__",
    "signature": "def __init__(self, item_shape: Tuple, buffer_size: int, dtype, device)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 835,
    "qualname": "_CircularBuffer.append",
    "signature": "def append(self, value: torch.Tensor)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 839,
    "qualname": "_CircularBuffer.get_all",
    "signature": "def get_all(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 842,
    "qualname": "_CircularBuffer.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 847,
    "qualname": "_InfiniteBuffer.__init__",
    "signature": "def __init__(self, item_shape: Tuple, dtype, device)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 852,
    "qualname": "_InfiniteBuffer.append",
    "signature": "def append(self, value: torch.Tensor)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 867,
    "qualname": "_InfiniteBuffer.get_all",
    "signature": "def get_all(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 870,
    "qualname": "_InfiniteBuffer.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 898,
    "qualname": "compute_gpu_physical_count",
    "signature": "def compute_gpu_physical_count(physical_count_of_whatever: torch.Tensor, num_gpu: int)"
  },
  {
    "module": "srt.eplb.expert_distribution",
    "file": "python/sglang/srt/eplb/expert_distribution.py",
    "line": 911,
    "qualname": "compute_utilization_rate",
    "signature": "def compute_utilization_rate(gpu_physical_count_of_batch: torch.Tensor)"
  },
  {
    "module": "srt.function_call.base_format_detector",
    "file": "python/sglang/srt/function_call/base_format_detector.py",
    "line": 27,
    "qualname": "BaseFormatDetector.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.function_call.base_format_detector",
    "file": "python/sglang/srt/function_call/base_format_detector.py",
    "line": 69,
    "qualname": "BaseFormatDetector.parse_base_json",
    "signature": "def parse_base_json(self, action: Any, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.base_format_detector",
    "file": "python/sglang/srt/function_call/base_format_detector.py",
    "line": 94,
    "qualname": "BaseFormatDetector.detect_and_parse",
    "signature": "def detect_and_parse(self, text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.base_format_detector",
    "file": "python/sglang/srt/function_call/base_format_detector.py",
    "line": 115,
    "qualname": "BaseFormatDetector.parse_streaming_increment",
    "signature": "def parse_streaming_increment(self, new_text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.base_format_detector",
    "file": "python/sglang/srt/function_call/base_format_detector.py",
    "line": 318,
    "qualname": "BaseFormatDetector.has_tool_call",
    "signature": "def has_tool_call(self, text: str)"
  },
  {
    "module": "srt.function_call.base_format_detector",
    "file": "python/sglang/srt/function_call/base_format_detector.py",
    "line": 324,
    "qualname": "BaseFormatDetector.supports_structural_tag",
    "signature": "def supports_structural_tag(self)"
  },
  {
    "module": "srt.function_call.base_format_detector",
    "file": "python/sglang/srt/function_call/base_format_detector.py",
    "line": 329,
    "qualname": "BaseFormatDetector.structure_info",
    "signature": "def structure_info(self)"
  },
  {
    "module": "srt.function_call.base_format_detector",
    "file": "python/sglang/srt/function_call/base_format_detector.py",
    "line": 343,
    "qualname": "BaseFormatDetector.build_ebnf",
    "signature": "def build_ebnf(self, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.deepseekv31_detector",
    "file": "python/sglang/srt/function_call/deepseekv31_detector.py",
    "line": 46,
    "qualname": "DeepSeekV31Detector.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.function_call.deepseekv31_detector",
    "file": "python/sglang/srt/function_call/deepseekv31_detector.py",
    "line": 57,
    "qualname": "DeepSeekV31Detector.has_tool_call",
    "signature": "def has_tool_call(self, text: str)"
  },
  {
    "module": "srt.function_call.deepseekv31_detector",
    "file": "python/sglang/srt/function_call/deepseekv31_detector.py",
    "line": 61,
    "qualname": "DeepSeekV31Detector.detect_and_parse",
    "signature": "def detect_and_parse(self, text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.deepseekv31_detector",
    "file": "python/sglang/srt/function_call/deepseekv31_detector.py",
    "line": 91,
    "qualname": "DeepSeekV31Detector.parse_streaming_increment",
    "signature": "def parse_streaming_increment(self, new_text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.deepseekv31_detector",
    "file": "python/sglang/srt/function_call/deepseekv31_detector.py",
    "line": 207,
    "qualname": "DeepSeekV31Detector.structure_info",
    "signature": "def structure_info(self)"
  },
  {
    "module": "srt.function_call.deepseekv31_detector",
    "file": "python/sglang/srt/function_call/deepseekv31_detector.py",
    "line": 214,
    "qualname": "DeepSeekV31Detector.build_ebnf",
    "signature": "def build_ebnf(self, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.deepseekv3_detector",
    "file": "python/sglang/srt/function_call/deepseekv3_detector.py",
    "line": 46,
    "qualname": "DeepSeekV3Detector.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.function_call.deepseekv3_detector",
    "file": "python/sglang/srt/function_call/deepseekv3_detector.py",
    "line": 55,
    "qualname": "DeepSeekV3Detector.has_tool_call",
    "signature": "def has_tool_call(self, text: str)"
  },
  {
    "module": "srt.function_call.deepseekv3_detector",
    "file": "python/sglang/srt/function_call/deepseekv3_detector.py",
    "line": 59,
    "qualname": "DeepSeekV3Detector.detect_and_parse",
    "signature": "def detect_and_parse(self, text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.deepseekv3_detector",
    "file": "python/sglang/srt/function_call/deepseekv3_detector.py",
    "line": 89,
    "qualname": "DeepSeekV3Detector.parse_streaming_increment",
    "signature": "def parse_streaming_increment(self, new_text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.deepseekv3_detector",
    "file": "python/sglang/srt/function_call/deepseekv3_detector.py",
    "line": 205,
    "qualname": "DeepSeekV3Detector.structure_info",
    "signature": "def structure_info(self)"
  },
  {
    "module": "srt.function_call.deepseekv3_detector",
    "file": "python/sglang/srt/function_call/deepseekv3_detector.py",
    "line": 212,
    "qualname": "DeepSeekV3Detector.build_ebnf",
    "signature": "def build_ebnf(self, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.ebnf_composer",
    "file": "python/sglang/srt/function_call/ebnf_composer.py",
    "line": 92,
    "qualname": "EBNFComposer.get_value_rule",
    "signature": "def get_value_rule(prop: dict, function_format: Literal['pythonic', 'json', 'xml'])"
  },
  {
    "module": "srt.function_call.ebnf_composer",
    "file": "python/sglang/srt/function_call/ebnf_composer.py",
    "line": 132,
    "qualname": "EBNFComposer.get_type_mapping",
    "signature": "def get_type_mapping(function_format: str)"
  },
  {
    "module": "srt.function_call.ebnf_composer",
    "file": "python/sglang/srt/function_call/ebnf_composer.py",
    "line": 155,
    "qualname": "EBNFComposer.build_ebnf",
    "signature": "def build_ebnf(tools, function_format: Literal['pythonic', 'json', 'xml'], sequence_start_token: Optional[str], sequence_end_token: Optional[str], individual_call_start_token: Optional[str], individual_call_end_token: Optional[str], tool_call_separator: Optional[str], call_rule_fmt: Optional[str], key_value_rule_fmt: Optional[str], key_value_separator: str)"
  },
  {
    "module": "srt.function_call.function_call_parser",
    "file": "python/sglang/srt/function_call/function_call_parser.py",
    "line": 50,
    "qualname": "FunctionCallParser.__init__",
    "signature": "def __init__(self, tools: List[Tool], tool_call_parser: str)"
  },
  {
    "module": "srt.function_call.function_call_parser",
    "file": "python/sglang/srt/function_call/function_call_parser.py",
    "line": 61,
    "qualname": "FunctionCallParser.has_tool_call",
    "signature": "def has_tool_call(self, text: str)"
  },
  {
    "module": "srt.function_call.function_call_parser",
    "file": "python/sglang/srt/function_call/function_call_parser.py",
    "line": 74,
    "qualname": "FunctionCallParser.parse_non_stream",
    "signature": "def parse_non_stream(self, full_text: str)"
  },
  {
    "module": "srt.function_call.function_call_parser",
    "file": "python/sglang/srt/function_call/function_call_parser.py",
    "line": 93,
    "qualname": "FunctionCallParser.parse_stream_chunk",
    "signature": "def parse_stream_chunk(self, chunk_text: str)"
  },
  {
    "module": "srt.function_call.function_call_parser",
    "file": "python/sglang/srt/function_call/function_call_parser.py",
    "line": 117,
    "qualname": "FunctionCallParser.get_structure_tag",
    "signature": "def get_structure_tag(self)"
  },
  {
    "module": "srt.function_call.function_call_parser",
    "file": "python/sglang/srt/function_call/function_call_parser.py",
    "line": 151,
    "qualname": "FunctionCallParser.get_structure_constraint",
    "signature": "def get_structure_constraint(self, tool_choice: Union[ToolChoice, Literal['auto', 'required']])"
  },
  {
    "module": "srt.function_call.function_call_parser",
    "file": "python/sglang/srt/function_call/function_call_parser.py",
    "line": 178,
    "qualname": "FunctionCallParser.get_ebnf",
    "signature": "def get_ebnf(self, tool_choice: Union[ToolChoice, Literal['required']])"
  },
  {
    "module": "srt.function_call.glm4_moe_detector",
    "file": "python/sglang/srt/function_call/glm4_moe_detector.py",
    "line": 19,
    "qualname": "get_argument_type",
    "signature": "def get_argument_type(func_name: str, arg_key: str, defined_tools: list)"
  },
  {
    "module": "srt.function_call.glm4_moe_detector",
    "file": "python/sglang/srt/function_call/glm4_moe_detector.py",
    "line": 29,
    "qualname": "parse_arguments",
    "signature": "def parse_arguments(json_value)"
  },
  {
    "module": "srt.function_call.glm4_moe_detector",
    "file": "python/sglang/srt/function_call/glm4_moe_detector.py",
    "line": 47,
    "qualname": "Glm4MoeDetector.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.function_call.glm4_moe_detector",
    "file": "python/sglang/srt/function_call/glm4_moe_detector.py",
    "line": 55,
    "qualname": "Glm4MoeDetector.has_tool_call",
    "signature": "def has_tool_call(self, text: str)"
  },
  {
    "module": "srt.function_call.glm4_moe_detector",
    "file": "python/sglang/srt/function_call/glm4_moe_detector.py",
    "line": 59,
    "qualname": "Glm4MoeDetector.detect_and_parse",
    "signature": "def detect_and_parse(self, text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.glm4_moe_detector",
    "file": "python/sglang/srt/function_call/glm4_moe_detector.py",
    "line": 101,
    "qualname": "Glm4MoeDetector.parse_streaming_increment",
    "signature": "def parse_streaming_increment(self, new_text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.glm4_moe_detector",
    "file": "python/sglang/srt/function_call/glm4_moe_detector.py",
    "line": 148,
    "qualname": "Glm4MoeDetector.supports_structural_tag",
    "signature": "def supports_structural_tag(self)"
  },
  {
    "module": "srt.function_call.glm4_moe_detector",
    "file": "python/sglang/srt/function_call/glm4_moe_detector.py",
    "line": 151,
    "qualname": "Glm4MoeDetector.structure_info",
    "signature": "def structure_info(self)"
  },
  {
    "module": "srt.function_call.glm4_moe_detector",
    "file": "python/sglang/srt/function_call/glm4_moe_detector.py",
    "line": 154,
    "qualname": "Glm4MoeDetector.build_ebnf",
    "signature": "def build_ebnf(self, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.gpt_oss_detector",
    "file": "python/sglang/srt/function_call/gpt_oss_detector.py",
    "line": 26,
    "qualname": "GptOssDetector.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.function_call.gpt_oss_detector",
    "file": "python/sglang/srt/function_call/gpt_oss_detector.py",
    "line": 38,
    "qualname": "GptOssDetector.has_tool_call",
    "signature": "def has_tool_call(self, text: str)"
  },
  {
    "module": "srt.function_call.gpt_oss_detector",
    "file": "python/sglang/srt/function_call/gpt_oss_detector.py",
    "line": 42,
    "qualname": "GptOssDetector.detect_and_parse",
    "signature": "def detect_and_parse(self, text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.gpt_oss_detector",
    "file": "python/sglang/srt/function_call/gpt_oss_detector.py",
    "line": 75,
    "qualname": "GptOssDetector.parse_streaming_increment",
    "signature": "def parse_streaming_increment(self, new_text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.gpt_oss_detector",
    "file": "python/sglang/srt/function_call/gpt_oss_detector.py",
    "line": 215,
    "qualname": "GptOssDetector.structure_info",
    "signature": "def structure_info(self)"
  },
  {
    "module": "srt.function_call.gpt_oss_detector",
    "file": "python/sglang/srt/function_call/gpt_oss_detector.py",
    "line": 218,
    "qualname": "GptOssDetector.build_ebnf",
    "signature": "def build_ebnf(self, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.kimik2_detector",
    "file": "python/sglang/srt/function_call/kimik2_detector.py",
    "line": 34,
    "qualname": "KimiK2Detector.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.function_call.kimik2_detector",
    "file": "python/sglang/srt/function_call/kimik2_detector.py",
    "line": 53,
    "qualname": "KimiK2Detector.has_tool_call",
    "signature": "def has_tool_call(self, text: str)"
  },
  {
    "module": "srt.function_call.kimik2_detector",
    "file": "python/sglang/srt/function_call/kimik2_detector.py",
    "line": 57,
    "qualname": "KimiK2Detector.detect_and_parse",
    "signature": "def detect_and_parse(self, text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.kimik2_detector",
    "file": "python/sglang/srt/function_call/kimik2_detector.py",
    "line": 100,
    "qualname": "KimiK2Detector.parse_streaming_increment",
    "signature": "def parse_streaming_increment(self, new_text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.kimik2_detector",
    "file": "python/sglang/srt/function_call/kimik2_detector.py",
    "line": 217,
    "qualname": "KimiK2Detector.structure_info",
    "signature": "def structure_info(self)"
  },
  {
    "module": "srt.function_call.kimik2_detector",
    "file": "python/sglang/srt/function_call/kimik2_detector.py",
    "line": 229,
    "qualname": "KimiK2Detector.build_ebnf",
    "signature": "def build_ebnf(self, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.llama32_detector",
    "file": "python/sglang/srt/function_call/llama32_detector.py",
    "line": 27,
    "qualname": "Llama32Detector.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.function_call.llama32_detector",
    "file": "python/sglang/srt/function_call/llama32_detector.py",
    "line": 36,
    "qualname": "Llama32Detector.has_tool_call",
    "signature": "def has_tool_call(self, text: str)"
  },
  {
    "module": "srt.function_call.llama32_detector",
    "file": "python/sglang/srt/function_call/llama32_detector.py",
    "line": 42,
    "qualname": "Llama32Detector.detect_and_parse",
    "signature": "def detect_and_parse(self, text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.llama32_detector",
    "file": "python/sglang/srt/function_call/llama32_detector.py",
    "line": 84,
    "qualname": "Llama32Detector.structure_info",
    "signature": "def structure_info(self)"
  },
  {
    "module": "srt.function_call.llama32_detector",
    "file": "python/sglang/srt/function_call/llama32_detector.py",
    "line": 91,
    "qualname": "Llama32Detector.build_ebnf",
    "signature": "def build_ebnf(self, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.mistral_detector",
    "file": "python/sglang/srt/function_call/mistral_detector.py",
    "line": 33,
    "qualname": "MistralDetector.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.function_call.mistral_detector",
    "file": "python/sglang/srt/function_call/mistral_detector.py",
    "line": 43,
    "qualname": "MistralDetector.has_tool_call",
    "signature": "def has_tool_call(self, text: str)"
  },
  {
    "module": "srt.function_call.mistral_detector",
    "file": "python/sglang/srt/function_call/mistral_detector.py",
    "line": 47,
    "qualname": "MistralDetector.detect_and_parse",
    "signature": "def detect_and_parse(self, text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.mistral_detector",
    "file": "python/sglang/srt/function_call/mistral_detector.py",
    "line": 125,
    "qualname": "MistralDetector.structure_info",
    "signature": "def structure_info(self)"
  },
  {
    "module": "srt.function_call.mistral_detector",
    "file": "python/sglang/srt/function_call/mistral_detector.py",
    "line": 132,
    "qualname": "MistralDetector.build_ebnf",
    "signature": "def build_ebnf(self, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.pythonic_detector",
    "file": "python/sglang/srt/function_call/pythonic_detector.py",
    "line": 34,
    "qualname": "PythonicDetector.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.function_call.pythonic_detector",
    "file": "python/sglang/srt/function_call/pythonic_detector.py",
    "line": 49,
    "qualname": "PythonicDetector.has_tool_call",
    "signature": "def has_tool_call(self, text: str)"
  },
  {
    "module": "srt.function_call.pythonic_detector",
    "file": "python/sglang/srt/function_call/pythonic_detector.py",
    "line": 52,
    "qualname": "PythonicDetector.detect_and_parse",
    "signature": "def detect_and_parse(self, text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.pythonic_detector",
    "file": "python/sglang/srt/function_call/pythonic_detector.py",
    "line": 157,
    "qualname": "PythonicDetector.parse_streaming_increment",
    "signature": "def parse_streaming_increment(self, new_text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.pythonic_detector",
    "file": "python/sglang/srt/function_call/pythonic_detector.py",
    "line": 218,
    "qualname": "PythonicDetector.supports_structural_tag",
    "signature": "def supports_structural_tag(self)"
  },
  {
    "module": "srt.function_call.pythonic_detector",
    "file": "python/sglang/srt/function_call/pythonic_detector.py",
    "line": 221,
    "qualname": "PythonicDetector.structure_info",
    "signature": "def structure_info(self)"
  },
  {
    "module": "srt.function_call.pythonic_detector",
    "file": "python/sglang/srt/function_call/pythonic_detector.py",
    "line": 224,
    "qualname": "PythonicDetector.build_ebnf",
    "signature": "def build_ebnf(self, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.qwen25_detector",
    "file": "python/sglang/srt/function_call/qwen25_detector.py",
    "line": 34,
    "qualname": "Qwen25Detector.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.function_call.qwen25_detector",
    "file": "python/sglang/srt/function_call/qwen25_detector.py",
    "line": 44,
    "qualname": "Qwen25Detector.has_tool_call",
    "signature": "def has_tool_call(self, text: str)"
  },
  {
    "module": "srt.function_call.qwen25_detector",
    "file": "python/sglang/srt/function_call/qwen25_detector.py",
    "line": 48,
    "qualname": "Qwen25Detector.detect_and_parse",
    "signature": "def detect_and_parse(self, text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.qwen25_detector",
    "file": "python/sglang/srt/function_call/qwen25_detector.py",
    "line": 76,
    "qualname": "Qwen25Detector.parse_streaming_increment",
    "signature": "def parse_streaming_increment(self, new_text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.qwen25_detector",
    "file": "python/sglang/srt/function_call/qwen25_detector.py",
    "line": 116,
    "qualname": "Qwen25Detector.structure_info",
    "signature": "def structure_info(self)"
  },
  {
    "module": "srt.function_call.qwen25_detector",
    "file": "python/sglang/srt/function_call/qwen25_detector.py",
    "line": 123,
    "qualname": "Qwen25Detector.build_ebnf",
    "signature": "def build_ebnf(self, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.qwen3_coder_detector",
    "file": "python/sglang/srt/function_call/qwen3_coder_detector.py",
    "line": 44,
    "qualname": "Qwen3CoderDetector.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.function_call.qwen3_coder_detector",
    "file": "python/sglang/srt/function_call/qwen3_coder_detector.py",
    "line": 69,
    "qualname": "Qwen3CoderDetector.has_tool_call",
    "signature": "def has_tool_call(self, text: str)"
  },
  {
    "module": "srt.function_call.qwen3_coder_detector",
    "file": "python/sglang/srt/function_call/qwen3_coder_detector.py",
    "line": 72,
    "qualname": "Qwen3CoderDetector.detect_and_parse",
    "signature": "def detect_and_parse(self, text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.qwen3_coder_detector",
    "file": "python/sglang/srt/function_call/qwen3_coder_detector.py",
    "line": 76,
    "qualname": "Qwen3CoderDetector.parse_streaming_increment",
    "signature": "def parse_streaming_increment(self, new_text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.qwen3_coder_detector",
    "file": "python/sglang/srt/function_call/qwen3_coder_detector.py",
    "line": 346,
    "qualname": "Qwen3CoderDetector.supports_structural_tag",
    "signature": "def supports_structural_tag(self)"
  },
  {
    "module": "srt.function_call.qwen3_coder_detector",
    "file": "python/sglang/srt/function_call/qwen3_coder_detector.py",
    "line": 349,
    "qualname": "Qwen3CoderDetector.structure_info",
    "signature": "def structure_info(self)"
  },
  {
    "module": "srt.function_call.qwen3_coder_detector",
    "file": "python/sglang/srt/function_call/qwen3_coder_detector.py",
    "line": 352,
    "qualname": "Qwen3CoderDetector.build_ebnf",
    "signature": "def build_ebnf(self, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.step3_detector",
    "file": "python/sglang/srt/function_call/step3_detector.py",
    "line": 19,
    "qualname": "get_argument_type",
    "signature": "def get_argument_type(func_name: str, arg_key: str, defined_tools: List[Tool])"
  },
  {
    "module": "srt.function_call.step3_detector",
    "file": "python/sglang/srt/function_call/step3_detector.py",
    "line": 32,
    "qualname": "parse_arguments",
    "signature": "def parse_arguments(value: str)"
  },
  {
    "module": "srt.function_call.step3_detector",
    "file": "python/sglang/srt/function_call/step3_detector.py",
    "line": 62,
    "qualname": "Step3Detector.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.function_call.step3_detector",
    "file": "python/sglang/srt/function_call/step3_detector.py",
    "line": 86,
    "qualname": "Step3Detector.has_tool_call",
    "signature": "def has_tool_call(self, text: str)"
  },
  {
    "module": "srt.function_call.step3_detector",
    "file": "python/sglang/srt/function_call/step3_detector.py",
    "line": 121,
    "qualname": "Step3Detector.detect_and_parse",
    "signature": "def detect_and_parse(self, text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.step3_detector",
    "file": "python/sglang/srt/function_call/step3_detector.py",
    "line": 170,
    "qualname": "Step3Detector.parse_streaming_increment",
    "signature": "def parse_streaming_increment(self, new_text: str, tools: List[Tool])"
  },
  {
    "module": "srt.function_call.step3_detector",
    "file": "python/sglang/srt/function_call/step3_detector.py",
    "line": 403,
    "qualname": "Step3Detector.supports_structural_tag",
    "signature": "def supports_structural_tag(self)"
  },
  {
    "module": "srt.function_call.step3_detector",
    "file": "python/sglang/srt/function_call/step3_detector.py",
    "line": 407,
    "qualname": "Step3Detector.structure_info",
    "signature": "def structure_info(self)"
  },
  {
    "module": "srt.function_call.step3_detector",
    "file": "python/sglang/srt/function_call/step3_detector.py",
    "line": 410,
    "qualname": "Step3Detector.build_ebnf",
    "signature": "def build_ebnf(self, tools: List[Tool])"
  },
  {
    "module": "srt.layers.amx_utils",
    "file": "python/sglang/srt/layers/amx_utils.py",
    "line": 10,
    "qualname": "amx_process_weight_after_loading",
    "signature": "def amx_process_weight_after_loading(weight)"
  },
  {
    "module": "srt.layers.amx_utils",
    "file": "python/sglang/srt/layers/amx_utils.py",
    "line": 22,
    "qualname": "dim_is_supported",
    "signature": "def dim_is_supported(weight)"
  },
  {
    "module": "srt.layers.amx_utils",
    "file": "python/sglang/srt/layers/amx_utils.py",
    "line": 79,
    "qualname": "PackWeightMethod.__init__",
    "signature": "def __init__(self, weight_names, transpose_dims)"
  },
  {
    "module": "srt.layers.amx_utils",
    "file": "python/sglang/srt/layers/amx_utils.py",
    "line": 83,
    "qualname": "PackWeightMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, module)"
  },
  {
    "module": "srt.layers.communicator",
    "file": "python/sglang/srt/layers/communicator.py",
    "line": 67,
    "qualname": "ScatterMode.model_input_output",
    "signature": "def model_input_output()"
  },
  {
    "module": "srt.layers.communicator",
    "file": "python/sglang/srt/layers/communicator.py",
    "line": 79,
    "qualname": "_LayerModeComputationContext.previous_layer",
    "signature": "def previous_layer(self)"
  },
  {
    "module": "srt.layers.communicator",
    "file": "python/sglang/srt/layers/communicator.py",
    "line": 99,
    "qualname": "LayerScatterModes.init_new",
    "signature": "def init_new(cls)"
  },
  {
    "module": "srt.layers.communicator",
    "file": "python/sglang/srt/layers/communicator.py",
    "line": 155,
    "qualname": "enable_moe_dense_fully_dp",
    "signature": "def enable_moe_dense_fully_dp()"
  },
  {
    "module": "srt.layers.communicator",
    "file": "python/sglang/srt/layers/communicator.py",
    "line": 160,
    "qualname": "LayerCommunicator.__init__",
    "signature": "def __init__(self, layer_scatter_modes: LayerScatterModes, input_layernorm: torch.nn.Module, post_attention_layernorm: torch.nn.Module, allow_reduce_scatter: bool, is_last_layer: bool)"
  },
  {
    "module": "srt.layers.communicator",
    "file": "python/sglang/srt/layers/communicator.py",
    "line": 199,
    "qualname": "LayerCommunicator.prepare_attn",
    "signature": "def prepare_attn(self, hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.communicator",
    "file": "python/sglang/srt/layers/communicator.py",
    "line": 235,
    "qualname": "LayerCommunicator.prepare_mlp",
    "signature": "def prepare_mlp(self, hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.communicator",
    "file": "python/sglang/srt/layers/communicator.py",
    "line": 249,
    "qualname": "LayerCommunicator.postprocess_layer",
    "signature": "def postprocess_layer(self, hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.communicator",
    "file": "python/sglang/srt/layers/communicator.py",
    "line": 263,
    "qualname": "LayerCommunicator.should_use_reduce_scatter",
    "signature": "def should_use_reduce_scatter(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.communicator",
    "file": "python/sglang/srt/layers/communicator.py",
    "line": 271,
    "qualname": "LayerCommunicator.should_fuse_mlp_allreduce_with_next_layer",
    "signature": "def should_fuse_mlp_allreduce_with_next_layer(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.communicator",
    "file": "python/sglang/srt/layers/communicator.py",
    "line": 315,
    "qualname": "CommunicateContext.is_same_group_size",
    "signature": "def is_same_group_size(self, a: ScatterMode, b: ScatterMode)"
  },
  {
    "module": "srt.layers.communicator",
    "file": "python/sglang/srt/layers/communicator.py",
    "line": 319,
    "qualname": "CommunicateContext.init_new",
    "signature": "def init_new(cls)"
  },
  {
    "module": "srt.layers.communicator",
    "file": "python/sglang/srt/layers/communicator.py",
    "line": 341,
    "qualname": "CommunicateSimpleFn.get_fn",
    "signature": "def get_fn(input_mode: ScatterMode, output_mode: ScatterMode, context: CommunicateContext)"
  },
  {
    "module": "srt.layers.communicator",
    "file": "python/sglang/srt/layers/communicator.py",
    "line": 388,
    "qualname": "CommunicateWithAllReduceAndLayerNormFn.get_fn",
    "signature": "def get_fn(hidden_states_input_mode: ScatterMode, residual_input_mode: ScatterMode, hidden_states_output_mode: ScatterMode, residual_output_mode: ScatterMode, context: CommunicateContext)"
  },
  {
    "module": "srt.layers.communicator",
    "file": "python/sglang/srt/layers/communicator.py",
    "line": 527,
    "qualname": "CommunicateSummableTensorPairFn.execute",
    "signature": "def execute(cls, hidden_states_input_mode, residual_input_mode, output_mode, context)"
  },
  {
    "module": "srt.layers.communicator",
    "file": "python/sglang/srt/layers/communicator.py",
    "line": 543,
    "qualname": "CommunicateSummableTensorPairFn.get_fn",
    "signature": "def get_fn(hidden_states_input_mode: ScatterMode, residual_input_mode: ScatterMode, output_mode: ScatterMode, context: CommunicateContext)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 47,
    "qualname": "DpPaddingMode.is_max_len",
    "signature": "def is_max_len(self)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 50,
    "qualname": "DpPaddingMode.is_sum_len",
    "signature": "def is_sum_len(self)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 54,
    "qualname": "DpPaddingMode.get_dp_padding_mode",
    "signature": "def get_dp_padding_mode(cls, global_num_tokens: List[int])"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 64,
    "qualname": "DpPaddingMode.get_default_mode_in_cuda_graph",
    "signature": "def get_default_mode_in_cuda_graph(cls)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 78,
    "qualname": "_DpGatheredBufferWrapper.set_metadata",
    "signature": "def set_metadata(cls, hidden_size: int, dtype: torch.dtype, device: torch.device)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 84,
    "qualname": "_DpGatheredBufferWrapper.set_dp_buffer_len",
    "signature": "def set_dp_buffer_len(cls, global_dp_buffer_len: int, local_dp_buffer_len: int, global_num_tokens: Optional[List[int]])"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 95,
    "qualname": "_DpGatheredBufferWrapper.get_global_dp_buffer",
    "signature": "def get_global_dp_buffer(cls)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 103,
    "qualname": "_DpGatheredBufferWrapper.get_local_dp_buffer",
    "signature": "def get_local_dp_buffer(cls)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 111,
    "qualname": "_DpGatheredBufferWrapper.get_global_dp_buffer_len",
    "signature": "def get_global_dp_buffer_len(cls)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 115,
    "qualname": "_DpGatheredBufferWrapper.get_local_dp_buffer_len",
    "signature": "def get_local_dp_buffer_len(cls)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 119,
    "qualname": "_DpGatheredBufferWrapper.get_dp_global_num_tokens",
    "signature": "def get_dp_global_num_tokens(cls)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 123,
    "qualname": "set_dp_buffer_len",
    "signature": "def set_dp_buffer_len(global_dp_buffer_len: int, local_dp_buffer_len: int, global_num_tokens: Optional[List[int]])"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 133,
    "qualname": "get_global_dp_buffer",
    "signature": "def get_global_dp_buffer()"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 137,
    "qualname": "get_local_dp_buffer",
    "signature": "def get_local_dp_buffer()"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 141,
    "qualname": "get_global_dp_buffer_len",
    "signature": "def get_global_dp_buffer_len()"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 145,
    "qualname": "get_local_dp_buffer_len",
    "signature": "def get_local_dp_buffer_len()"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 149,
    "qualname": "get_dp_global_num_tokens",
    "signature": "def get_dp_global_num_tokens()"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 153,
    "qualname": "compute_dp_attention_world_info",
    "signature": "def compute_dp_attention_world_info(enable_dp_attention, tp_rank, tp_size, dp_size)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 164,
    "qualname": "compute_dp_attention_local_info",
    "signature": "def compute_dp_attention_local_info(enable_dp_attention, tp_rank, tp_size, dp_size, moe_dense_tp_size)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 181,
    "qualname": "initialize_dp_attention",
    "signature": "def initialize_dp_attention(server_args: ServerArgs, model_config: ModelConfig)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 241,
    "qualname": "is_dp_attention_enabled",
    "signature": "def is_dp_attention_enabled()"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 245,
    "qualname": "get_attention_tp_group",
    "signature": "def get_attention_tp_group()"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 250,
    "qualname": "get_attention_tp_rank",
    "signature": "def get_attention_tp_rank()"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 255,
    "qualname": "get_attention_tp_size",
    "signature": "def get_attention_tp_size()"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 260,
    "qualname": "get_attention_dp_rank",
    "signature": "def get_attention_dp_rank()"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 265,
    "qualname": "get_attention_dp_size",
    "signature": "def get_attention_dp_size()"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 270,
    "qualname": "get_local_attention_dp_rank",
    "signature": "def get_local_attention_dp_rank()"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 275,
    "qualname": "get_local_attention_dp_size",
    "signature": "def get_local_attention_dp_size()"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 281,
    "qualname": "disable_dp_size",
    "signature": "def disable_dp_size()"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 301,
    "qualname": "get_dp_local_info",
    "signature": "def get_dp_local_info(forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 320,
    "qualname": "memcpy_triton_kernel",
    "signature": "def memcpy_triton_kernel(dst_ptr, src_ptr, offset_ptr, sz_ptr, offset_src: tl.constexpr, chunk_size, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 345,
    "qualname": "prod",
    "signature": "def prod(x)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 349,
    "qualname": "memcpy_triton",
    "signature": "def memcpy_triton(dst, src, dim, offset, sz, offset_src)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 431,
    "qualname": "dp_gather_partial",
    "signature": "def dp_gather_partial(global_tokens: torch.Tensor, local_tokens: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 439,
    "qualname": "dp_gather_replicate",
    "signature": "def dp_gather_replicate(global_tokens: torch.Tensor, local_tokens: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 447,
    "qualname": "dp_scatter",
    "signature": "def dp_scatter(local_tokens: torch.Tensor, global_tokens: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 469,
    "qualname": "dp_reduce_scatter_tensor",
    "signature": "def dp_reduce_scatter_tensor(output: torch.Tensor, input: torch.Tensor)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 480,
    "qualname": "attn_tp_reduce_scatter_tensor",
    "signature": "def attn_tp_reduce_scatter_tensor(output: torch.Tensor, input: torch.Tensor)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 484,
    "qualname": "attn_tp_all_gather_into_tensor",
    "signature": "def attn_tp_all_gather_into_tensor(output: torch.Tensor, input: torch.Tensor)"
  },
  {
    "module": "srt.layers.dp_attention",
    "file": "python/sglang/srt/layers/dp_attention.py",
    "line": 488,
    "qualname": "attn_tp_all_gather",
    "signature": "def attn_tp_all_gather(output_list: List[torch.Tensor], input: torch.Tensor)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 37,
    "qualname": "fused_softcap_kernel",
    "signature": "def fused_softcap_kernel(output_ptr, input_ptr, n_ele, softcap_const: tl.constexpr, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 61,
    "qualname": "fused_softcap",
    "signature": "def fused_softcap(x, softcap_const, autotune)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 76,
    "qualname": "Softcap.__init__",
    "signature": "def __init__(self, softcap_const: float)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 79,
    "qualname": "Softcap.__call__",
    "signature": "def __call__(self)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 82,
    "qualname": "Softcap.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 88,
    "qualname": "Softcap.forward_native",
    "signature": "def forward_native(self, x: torch.Tensor)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 91,
    "qualname": "Softcap.forward_cuda",
    "signature": "def forward_cuda(self, x: torch.Tensor, autotune)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 138,
    "qualname": "fused_dual_residual_rmsnorm_kernel",
    "signature": "def fused_dual_residual_rmsnorm_kernel(output_ptr, mid_ptr, activ_ptr, residual_ptr, weight1_ptr, weight2_ptr, eps: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 188,
    "qualname": "fused_dual_residual_rmsnorm",
    "signature": "def fused_dual_residual_rmsnorm(x, residual, weight1, weight2, eps, autotune)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 222,
    "qualname": "fused_rmsnorm_kernel",
    "signature": "def fused_rmsnorm_kernel(output_ptr, activ_ptr, weight_ptr, eps: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 252,
    "qualname": "fused_rmsnorm",
    "signature": "def fused_rmsnorm(x, weight, eps, autotune, inplace)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 279,
    "qualname": "FusedDualResidualRMSNorm.__init__",
    "signature": "def __init__(self, rmsnorm1, rmsnorm2)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 286,
    "qualname": "FusedDualResidualRMSNorm.__call__",
    "signature": "def __call__(self)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 289,
    "qualname": "FusedDualResidualRMSNorm.forward",
    "signature": "def forward(self, x: torch.Tensor, residual: torch.Tensor)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 297,
    "qualname": "FusedDualResidualRMSNorm.forward_cuda",
    "signature": "def forward_cuda(self, x: torch.Tensor, residual: torch.Tensor, autotune)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 309,
    "qualname": "FusedDualResidualRMSNorm.forward_flashinfer",
    "signature": "def forward_flashinfer(self, x: torch.Tensor, residual: torch.Tensor)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 318,
    "qualname": "FusedDualResidualRMSNorm.forward_native",
    "signature": "def forward_native(self, x: torch.Tensor, residual: torch.Tensor)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 329,
    "qualname": "experts_combine_kernel",
    "signature": "def experts_combine_kernel(out_hidden_states, moe_hidden_states, mlp_hidden_states, combine_k: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 359,
    "qualname": "experts_combine_triton",
    "signature": "def experts_combine_triton(moe_hidden_states, mlp_hidden_states, output_buffer)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 399,
    "qualname": "gelu_and_mul_kernel",
    "signature": "def gelu_and_mul_kernel(out_hidden_states_ptr, out_scales_ptr, hidden_states_ptr, quant_max: tl.constexpr, static_scale: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 436,
    "qualname": "gelu_and_mul_triton",
    "signature": "def gelu_and_mul_triton(hidden_states, scales, quantize, out)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 493,
    "qualname": "silu_and_mul_kernel",
    "signature": "def silu_and_mul_kernel(out_hidden_states_ptr, out_scales_ptr, hidden_states_ptr, quant_max: tl.constexpr, static_scale: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.elementwise",
    "file": "python/sglang/srt/layers/elementwise.py",
    "line": 530,
    "qualname": "silu_and_mul_triton",
    "signature": "def silu_and_mul_triton(hidden_states, scales, quantize, out)"
  },
  {
    "module": "srt.layers.flashinfer_comm_fusion",
    "file": "python/sglang/srt/layers/flashinfer_comm_fusion.py",
    "line": 32,
    "qualname": "FlashInferWorkspaceManager.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.layers.flashinfer_comm_fusion",
    "file": "python/sglang/srt/layers/flashinfer_comm_fusion.py",
    "line": 39,
    "qualname": "FlashInferWorkspaceManager.initialize",
    "signature": "def initialize(self, world_size: int, rank: int, max_token_num: int, hidden_dim: int, group, use_fp32_lamport: bool)"
  },
  {
    "module": "srt.layers.flashinfer_comm_fusion",
    "file": "python/sglang/srt/layers/flashinfer_comm_fusion.py",
    "line": 80,
    "qualname": "FlashInferWorkspaceManager.cleanup",
    "signature": "def cleanup(self)"
  },
  {
    "module": "srt.layers.flashinfer_comm_fusion",
    "file": "python/sglang/srt/layers/flashinfer_comm_fusion.py",
    "line": 98,
    "qualname": "ensure_workspace_initialized",
    "signature": "def ensure_workspace_initialized(max_token_num: int, hidden_dim: int, use_fp32_lamport: bool)"
  },
  {
    "module": "srt.layers.flashinfer_comm_fusion",
    "file": "python/sglang/srt/layers/flashinfer_comm_fusion.py",
    "line": 126,
    "qualname": "flashinfer_allreduce_residual_rmsnorm",
    "signature": "def flashinfer_allreduce_residual_rmsnorm(input_tensor: torch.Tensor, residual: torch.Tensor, weight: torch.Tensor, eps: float, max_token_num: int, use_oneshot: Optional[bool], trigger_completion_at_end: bool, fp32_acc: bool)"
  },
  {
    "module": "srt.layers.flashinfer_comm_fusion",
    "file": "python/sglang/srt/layers/flashinfer_comm_fusion.py",
    "line": 203,
    "qualname": "fake_flashinfer_allreduce_residual_rmsnorm",
    "signature": "def fake_flashinfer_allreduce_residual_rmsnorm(input_tensor: torch.Tensor, residual: torch.Tensor, weight: torch.Tensor, eps: float, max_token_num: int, use_oneshot: Optional[bool], trigger_completion_at_end: bool, fp32_acc: bool)"
  },
  {
    "module": "srt.layers.flashinfer_comm_fusion",
    "file": "python/sglang/srt/layers/flashinfer_comm_fusion.py",
    "line": 227,
    "qualname": "cleanup_flashinfer_workspace",
    "signature": "def cleanup_flashinfer_workspace()"
  },
  {
    "module": "srt.layers.multimodal",
    "file": "python/sglang/srt/layers/multimodal.py",
    "line": 44,
    "qualname": "hash_tiles32_kernel_blocked",
    "signature": "def hash_tiles32_kernel_blocked(in_ptr, out_ptr, n_u32, seed1, seed2, FM_C1: tl.constexpr, FM_C2: tl.constexpr, POS_A: tl.constexpr, POS_B: tl.constexpr, TILE: tl.constexpr, BLOCK: tl.constexpr, USE_CG: tl.constexpr)"
  },
  {
    "module": "srt.layers.multimodal",
    "file": "python/sglang/srt/layers/multimodal.py",
    "line": 108,
    "qualname": "add_tree_reduce_u64_kernel",
    "signature": "def add_tree_reduce_u64_kernel(in_ptr, out_ptr, n_elems, CHUNK: tl.constexpr)"
  },
  {
    "module": "srt.layers.multimodal",
    "file": "python/sglang/srt/layers/multimodal.py",
    "line": 145,
    "qualname": "gpu_tensor_hash",
    "signature": "def gpu_tensor_hash(tensor: torch.Tensor)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 36,
    "qualname": "BasevLLMParameter.__new__",
    "signature": "def __new__(cls, data: torch.Tensor)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 40,
    "qualname": "BasevLLMParameter.__init__",
    "signature": "def __init__(self, data: torch.Tensor, weight_loader: Callable)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 53,
    "qualname": "BasevLLMParameter.weight_loader",
    "signature": "def weight_loader(self)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 60,
    "qualname": "BasevLLMParameter.load_column_parallel_weight",
    "signature": "def load_column_parallel_weight(self, loaded_weight: torch.Tensor)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 63,
    "qualname": "BasevLLMParameter.load_row_parallel_weight",
    "signature": "def load_row_parallel_weight(self, loaded_weight: torch.Tensor)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 66,
    "qualname": "BasevLLMParameter.load_merged_column_weight",
    "signature": "def load_merged_column_weight(self, loaded_weight: torch.Tensor)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 69,
    "qualname": "BasevLLMParameter.load_qkv_weight",
    "signature": "def load_qkv_weight(self, loaded_weight: torch.Tensor)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 84,
    "qualname": "_ColumnvLLMParameter.__init__",
    "signature": "def __init__(self, output_dim: int)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 89,
    "qualname": "_ColumnvLLMParameter.output_dim",
    "signature": "def output_dim(self)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 92,
    "qualname": "_ColumnvLLMParameter.load_column_parallel_weight",
    "signature": "def load_column_parallel_weight(self, loaded_weight: torch.Tensor, tp_rank: int, use_presharded_weights: bool)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 125,
    "qualname": "_ColumnvLLMParameter.load_merged_column_weight",
    "signature": "def load_merged_column_weight(self, loaded_weight: torch.Tensor)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 166,
    "qualname": "_ColumnvLLMParameter.load_qkv_weight",
    "signature": "def load_qkv_weight(self, loaded_weight: torch.Tensor, tp_rank: int, use_presharded_weights: bool)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 225,
    "qualname": "RowvLLMParameter.__init__",
    "signature": "def __init__(self, input_dim: int)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 230,
    "qualname": "RowvLLMParameter.input_dim",
    "signature": "def input_dim(self)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 233,
    "qualname": "RowvLLMParameter.load_row_parallel_weight",
    "signature": "def load_row_parallel_weight(self, loaded_weight: torch.Tensor, tp_rank: int, use_presharded_weights: bool)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 322,
    "qualname": "PerTensorScaleParameter.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 338,
    "qualname": "PerTensorScaleParameter.load_row_parallel_weight",
    "signature": "def load_row_parallel_weight(self)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 343,
    "qualname": "PerTensorScaleParameter.load_merged_column_weight",
    "signature": "def load_merged_column_weight(self)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 346,
    "qualname": "PerTensorScaleParameter.load_qkv_weight",
    "signature": "def load_qkv_weight(self)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 349,
    "qualname": "PerTensorScaleParameter.load_column_parallel_weight",
    "signature": "def load_column_parallel_weight(self)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 383,
    "qualname": "PackedColumnParameter.__init__",
    "signature": "def __init__(self, packed_factor: Union[int, Fraction], packed_dim: int, marlin_tile_size: Optional[int])"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 396,
    "qualname": "PackedColumnParameter.packed_dim",
    "signature": "def packed_dim(self)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 400,
    "qualname": "PackedColumnParameter.packed_factor",
    "signature": "def packed_factor(self)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 404,
    "qualname": "PackedColumnParameter.marlin_tile_size",
    "signature": "def marlin_tile_size(self)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 407,
    "qualname": "PackedColumnParameter.adjust_shard_indexes_for_packing",
    "signature": "def adjust_shard_indexes_for_packing(self, shard_size, shard_offset)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 427,
    "qualname": "PackedvLLMParameter.__init__",
    "signature": "def __init__(self, packed_factor: Union[int, Fraction], packed_dim: int, marlin_tile_size: Optional[int])"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 440,
    "qualname": "PackedvLLMParameter.packed_dim",
    "signature": "def packed_dim(self)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 444,
    "qualname": "PackedvLLMParameter.packed_factor",
    "signature": "def packed_factor(self)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 448,
    "qualname": "PackedvLLMParameter.marlin_tile_size",
    "signature": "def marlin_tile_size(self)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 451,
    "qualname": "PackedvLLMParameter.adjust_shard_indexes_for_packing",
    "signature": "def adjust_shard_indexes_for_packing(self, shard_size, shard_offset)"
  },
  {
    "module": "srt.layers.parameter",
    "file": "python/sglang/srt/layers/parameter.py",
    "line": 460,
    "qualname": "permute_param_layout_",
    "signature": "def permute_param_layout_(param: BasevLLMParameter, input_dim: int, output_dim: int)"
  },
  {
    "module": "srt.layers.pooler",
    "file": "python/sglang/srt/layers/pooler.py",
    "line": 37,
    "qualname": "Pooler.__init__",
    "signature": "def __init__(self, pooling_type: PoolingType, normalize: bool)"
  },
  {
    "module": "srt.layers.pooler",
    "file": "python/sglang/srt/layers/pooler.py",
    "line": 42,
    "qualname": "Pooler.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.pooler",
    "file": "python/sglang/srt/layers/pooler.py",
    "line": 71,
    "qualname": "CrossEncodingPooler.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, classifier: nn.Module, pooler: Optional[nn.Module])"
  },
  {
    "module": "srt.layers.pooler",
    "file": "python/sglang/srt/layers/pooler.py",
    "line": 82,
    "qualname": "CrossEncodingPooler.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.radix_attention",
    "file": "python/sglang/srt/layers/radix_attention.py",
    "line": 44,
    "qualname": "RadixAttention.__init__",
    "signature": "def __init__(self, num_heads: int, head_dim: int, scaling: float, num_kv_heads: int, layer_id: int, logit_cap: float, v_head_dim: int, sliding_window_size: int, is_cross_attention: bool, pos_encoding_mode: str, logit_capping_method: str, quant_config: Optional[QuantizationConfig], attn_type: AttentionType, use_irope: bool, prefix: str)"
  },
  {
    "module": "srt.layers.radix_attention",
    "file": "python/sglang/srt/layers/radix_attention.py",
    "line": 90,
    "qualname": "RadixAttention.forward",
    "signature": "def forward(self, q, k, v, forward_batch: ForwardBatch, save_kv_cache: bool)"
  },
  {
    "module": "srt.layers.torchao_utils",
    "file": "python/sglang/srt/layers/torchao_utils.py",
    "line": 15,
    "qualname": "get_gemlite_cache_path",
    "signature": "def get_gemlite_cache_path()"
  },
  {
    "module": "srt.layers.torchao_utils",
    "file": "python/sglang/srt/layers/torchao_utils.py",
    "line": 19,
    "qualname": "save_gemlite_cache",
    "signature": "def save_gemlite_cache(print_error: bool)"
  },
  {
    "module": "srt.layers.torchao_utils",
    "file": "python/sglang/srt/layers/torchao_utils.py",
    "line": 31,
    "qualname": "proj_filter",
    "signature": "def proj_filter(module: torch.nn.Module, fqn: str)"
  },
  {
    "module": "srt.layers.torchao_utils",
    "file": "python/sglang/srt/layers/torchao_utils.py",
    "line": 39,
    "qualname": "apply_torchao_config_to_model",
    "signature": "def apply_torchao_config_to_model(model: torch.nn.Module, torchao_config: str, filter_fn: Optional[Callable])"
  },
  {
    "module": "srt.layers.utils",
    "file": "python/sglang/srt/layers/utils.py",
    "line": 10,
    "qualname": "get_layer_id",
    "signature": "def get_layer_id(weight_name)"
  },
  {
    "module": "srt.layers.utils",
    "file": "python/sglang/srt/layers/utils.py",
    "line": 25,
    "qualname": "PPMissingLayer.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.layers.utils",
    "file": "python/sglang/srt/layers/utils.py",
    "line": 29,
    "qualname": "PPMissingLayer.forward",
    "signature": "def forward(self)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 44,
    "qualname": "pad_vocab_size",
    "signature": "def pad_vocab_size(vocab_size: int, pad_to: int)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 49,
    "qualname": "vocab_range_from_per_partition_vocab_size",
    "signature": "def vocab_range_from_per_partition_vocab_size(per_partition_vocab_size: int, rank: int, offset: int)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 57,
    "qualname": "vocab_range_from_global_vocab_size",
    "signature": "def vocab_range_from_global_vocab_size(global_vocab_size: int, rank: int, world_size: int, offset: int)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 81,
    "qualname": "VocabParallelEmbeddingShardIndices.num_org_elements",
    "signature": "def num_org_elements(self)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 85,
    "qualname": "VocabParallelEmbeddingShardIndices.num_added_elements",
    "signature": "def num_added_elements(self)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 89,
    "qualname": "VocabParallelEmbeddingShardIndices.num_org_elements_padded",
    "signature": "def num_org_elements_padded(self)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 93,
    "qualname": "VocabParallelEmbeddingShardIndices.num_added_elements_padded",
    "signature": "def num_added_elements_padded(self)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 97,
    "qualname": "VocabParallelEmbeddingShardIndices.num_org_vocab_padding",
    "signature": "def num_org_vocab_padding(self)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 101,
    "qualname": "VocabParallelEmbeddingShardIndices.num_added_vocab_padding",
    "signature": "def num_added_vocab_padding(self)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 105,
    "qualname": "VocabParallelEmbeddingShardIndices.num_elements_padded",
    "signature": "def num_elements_padded(self)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 108,
    "qualname": "VocabParallelEmbeddingShardIndices.__post_init__",
    "signature": "def __post_init__(self)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 126,
    "qualname": "get_masked_input_and_mask",
    "signature": "def get_masked_input_and_mask(input_: torch.Tensor, org_vocab_start_index: int, org_vocab_end_index: int, num_org_vocab_padding: int, added_vocab_start_index: int, added_vocab_end_index: int)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 192,
    "qualname": "VocabParallelEmbedding.__init__",
    "signature": "def __init__(self, num_embeddings: int, embedding_dim: int)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 346,
    "qualname": "VocabParallelEmbedding.get_sharded_to_full_mapping",
    "signature": "def get_sharded_to_full_mapping(self)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 411,
    "qualname": "VocabParallelEmbedding.weight_loader",
    "signature": "def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 462,
    "qualname": "VocabParallelEmbedding.forward",
    "signature": "def forward(self, input_)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 488,
    "qualname": "VocabParallelEmbedding.extra_repr",
    "signature": "def extra_repr(self)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 514,
    "qualname": "ParallelLMHead.__init__",
    "signature": "def __init__(self, num_embeddings: int, embedding_dim: int)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 560,
    "qualname": "ParallelLMHead.tie_weights",
    "signature": "def tie_weights(self, embed_tokens: VocabParallelEmbedding)"
  },
  {
    "module": "srt.layers.vocab_parallel_embedding",
    "file": "python/sglang/srt/layers/vocab_parallel_embedding.py",
    "line": 569,
    "qualname": "ParallelLMHead.forward",
    "signature": "def forward(self, input_)"
  },
  {
    "module": "srt.layers.activation",
    "file": "python/sglang/srt/layers/activation.py",
    "line": 60,
    "qualname": "SiluAndMul.forward_native",
    "signature": "def forward_native(self, x: torch.Tensor)"
  },
  {
    "module": "srt.layers.activation",
    "file": "python/sglang/srt/layers/activation.py",
    "line": 64,
    "qualname": "SiluAndMul.forward_cuda",
    "signature": "def forward_cuda(self, x: torch.Tensor)"
  },
  {
    "module": "srt.layers.activation",
    "file": "python/sglang/srt/layers/activation.py",
    "line": 71,
    "qualname": "SiluAndMul.forward_cpu",
    "signature": "def forward_cpu(self, x: torch.Tensor)"
  },
  {
    "module": "srt.layers.activation",
    "file": "python/sglang/srt/layers/activation.py",
    "line": 80,
    "qualname": "SiluAndMul.forward_npu",
    "signature": "def forward_npu(self, x: torch.Tensor)"
  },
  {
    "module": "srt.layers.activation",
    "file": "python/sglang/srt/layers/activation.py",
    "line": 86,
    "qualname": "GeluAndMul.__init__",
    "signature": "def __init__(self, approximate)"
  },
  {
    "module": "srt.layers.activation",
    "file": "python/sglang/srt/layers/activation.py",
    "line": 90,
    "qualname": "GeluAndMul.forward_native",
    "signature": "def forward_native(self, x: torch.Tensor)"
  },
  {
    "module": "srt.layers.activation",
    "file": "python/sglang/srt/layers/activation.py",
    "line": 94,
    "qualname": "GeluAndMul.forward_cuda",
    "signature": "def forward_cuda(self, x: torch.Tensor)"
  },
  {
    "module": "srt.layers.activation",
    "file": "python/sglang/srt/layers/activation.py",
    "line": 108,
    "qualname": "NewGELU.forward_native",
    "signature": "def forward_native(self, x: torch.Tensor)"
  },
  {
    "module": "srt.layers.activation",
    "file": "python/sglang/srt/layers/activation.py",
    "line": 112,
    "qualname": "NewGELU.forward_cuda",
    "signature": "def forward_cuda(self, x: torch.Tensor)"
  },
  {
    "module": "srt.layers.activation",
    "file": "python/sglang/srt/layers/activation.py",
    "line": 123,
    "qualname": "ReLU2.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.layers.activation",
    "file": "python/sglang/srt/layers/activation.py",
    "line": 129,
    "qualname": "QuickGELU.forward_native",
    "signature": "def forward_native(self, x: torch.Tensor)"
  },
  {
    "module": "srt.layers.activation",
    "file": "python/sglang/srt/layers/activation.py",
    "line": 132,
    "qualname": "QuickGELU.forward_cuda",
    "signature": "def forward_cuda(self, x: torch.Tensor)"
  },
  {
    "module": "srt.layers.activation",
    "file": "python/sglang/srt/layers/activation.py",
    "line": 135,
    "qualname": "QuickGELU.forward_hip",
    "signature": "def forward_hip(self, x: torch.Tensor)"
  },
  {
    "module": "srt.layers.activation",
    "file": "python/sglang/srt/layers/activation.py",
    "line": 147,
    "qualname": "ScaledActivation.__init__",
    "signature": "def __init__(self, act_module: nn.Module, intermediate_size: int, input_is_parallel: bool, params_dtype: Optional[torch.dtype])"
  },
  {
    "module": "srt.layers.activation",
    "file": "python/sglang/srt/layers/activation.py",
    "line": 169,
    "qualname": "ScaledActivation.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.layers.activation",
    "file": "python/sglang/srt/layers/activation.py",
    "line": 172,
    "qualname": "ScaledActivation.weight_loader",
    "signature": "def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor)"
  },
  {
    "module": "srt.layers.activation",
    "file": "python/sglang/srt/layers/activation.py",
    "line": 191,
    "qualname": "get_act_fn",
    "signature": "def get_act_fn(act_fn_name: str, quant_config: Optional[QuantizationConfig], intermediate_size: Optional[int], input_is_parallel: bool, params_dtype: Optional[torch.dtype])"
  },
  {
    "module": "srt.layers.activation",
    "file": "python/sglang/srt/layers/activation.py",
    "line": 216,
    "qualname": "get_cross_encoder_activation_function",
    "signature": "def get_cross_encoder_activation_function(config: PretrainedConfig)"
  },
  {
    "module": "srt.layers.layernorm",
    "file": "python/sglang/srt/layers/layernorm.py",
    "line": 61,
    "qualname": "RMSNorm.__init__",
    "signature": "def __init__(self, hidden_size: int, eps: float, var_hidden_size: Optional[int])"
  },
  {
    "module": "srt.layers.layernorm",
    "file": "python/sglang/srt/layers/layernorm.py",
    "line": 77,
    "qualname": "RMSNorm.forward_cuda",
    "signature": "def forward_cuda(self, x: torch.Tensor, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.layernorm",
    "file": "python/sglang/srt/layers/layernorm.py",
    "line": 90,
    "qualname": "RMSNorm.forward_npu",
    "signature": "def forward_npu(self, x: torch.Tensor, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.layernorm",
    "file": "python/sglang/srt/layers/layernorm.py",
    "line": 102,
    "qualname": "RMSNorm.forward_aiter",
    "signature": "def forward_aiter(self, x: torch.Tensor, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.layernorm",
    "file": "python/sglang/srt/layers/layernorm.py",
    "line": 121,
    "qualname": "RMSNorm.forward_hip",
    "signature": "def forward_hip(self, x: torch.Tensor, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.layernorm",
    "file": "python/sglang/srt/layers/layernorm.py",
    "line": 136,
    "qualname": "RMSNorm.forward_native",
    "signature": "def forward_native(self, x: torch.Tensor, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.layernorm",
    "file": "python/sglang/srt/layers/layernorm.py",
    "line": 175,
    "qualname": "RMSNorm.forward_cpu",
    "signature": "def forward_cpu(self, x: torch.Tensor, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.layernorm",
    "file": "python/sglang/srt/layers/layernorm.py",
    "line": 192,
    "qualname": "RMSNorm.forward_with_allreduce_fusion",
    "signature": "def forward_with_allreduce_fusion(self, x: torch.Tensor, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.layernorm",
    "file": "python/sglang/srt/layers/layernorm.py",
    "line": 226,
    "qualname": "GemmaRMSNorm.__init__",
    "signature": "def __init__(self, hidden_size: int, eps: float)"
  },
  {
    "module": "srt.layers.layernorm",
    "file": "python/sglang/srt/layers/layernorm.py",
    "line": 239,
    "qualname": "GemmaRMSNorm.forward_native",
    "signature": "def forward_native(self, x: torch.Tensor, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.layernorm",
    "file": "python/sglang/srt/layers/layernorm.py",
    "line": 256,
    "qualname": "GemmaRMSNorm.forward_cuda",
    "signature": "def forward_cuda(self, x: torch.Tensor, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.layernorm",
    "file": "python/sglang/srt/layers/layernorm.py",
    "line": 271,
    "qualname": "Gemma3RMSNorm.__init__",
    "signature": "def __init__(self, dim: int, eps: float)"
  },
  {
    "module": "srt.layers.layernorm",
    "file": "python/sglang/srt/layers/layernorm.py",
    "line": 279,
    "qualname": "Gemma3RMSNorm.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.layers.layernorm",
    "file": "python/sglang/srt/layers/layernorm.py",
    "line": 286,
    "qualname": "Gemma3RMSNorm.extra_repr",
    "signature": "def extra_repr(self)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 67,
    "qualname": "adjust_marlin_shard",
    "signature": "def adjust_marlin_shard(param, shard_size, shard_offset)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 75,
    "qualname": "adjust_bitsandbytes_4bit_shard",
    "signature": "def adjust_bitsandbytes_4bit_shard(param: Parameter, shard_offsets: Dict[str, Tuple[int, int]], loaded_shard_id: str)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 90,
    "qualname": "adjust_scalar_to_fused_array",
    "signature": "def adjust_scalar_to_fused_array(param, loaded_weight, shard_id)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 113,
    "qualname": "adjust_shard_offsets",
    "signature": "def adjust_shard_offsets(shard_offsets, loaded_weight, dim)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 139,
    "qualname": "LinearBase.__init__",
    "signature": "def __init__(self, input_size: int, output_size: int, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 162,
    "qualname": "LinearBase.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 180,
    "qualname": "ReplicatedLinear.__init__",
    "signature": "def __init__(self, input_size: int, output_size: int, bias: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 225,
    "qualname": "ReplicatedLinear.weight_loader",
    "signature": "def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 242,
    "qualname": "ReplicatedLinear.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 249,
    "qualname": "ReplicatedLinear.extra_repr",
    "signature": "def extra_repr(self)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 280,
    "qualname": "ColumnParallelLinear.__init__",
    "signature": "def __init__(self, input_size: int, output_size: int, bias: bool, gather_output: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], output_sizes: Optional[List[int]], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], use_presharded_weights: bool)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 347,
    "qualname": "ColumnParallelLinear.weight_loader",
    "signature": "def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 397,
    "qualname": "ColumnParallelLinear.weight_loader_v2",
    "signature": "def weight_loader_v2(self, param: Parameter, loaded_weight: torch.Tensor)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 415,
    "qualname": "ColumnParallelLinear.forward",
    "signature": "def forward(self, input_)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 429,
    "qualname": "ColumnParallelLinear.extra_repr",
    "signature": "def extra_repr(self)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 461,
    "qualname": "MergedColumnParallelLinear.__init__",
    "signature": "def __init__(self, input_size: int, output_sizes: List[int], bias: bool, gather_output: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], use_presharded_weights: bool)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 498,
    "qualname": "MergedColumnParallelLinear.weight_loader",
    "signature": "def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[int])"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 693,
    "qualname": "MergedColumnParallelLinear.weight_loader_v2",
    "signature": "def weight_loader_v2(self, param: BasevLLMParameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[int])"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 773,
    "qualname": "QKVParallelLinear.__init__",
    "signature": "def __init__(self, hidden_size: int, head_size: int, total_num_heads: int, total_num_kv_heads: Optional[int], bias: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], load_presharded_attn: bool)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 896,
    "qualname": "QKVParallelLinear.weight_loader_v2",
    "signature": "def weight_loader_v2(self, param: BasevLLMParameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[str])"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 934,
    "qualname": "QKVParallelLinear.weight_loader",
    "signature": "def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[str])"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 1173,
    "qualname": "RowParallelLinear.__init__",
    "signature": "def __init__(self, input_size: int, output_size: int, bias: bool, input_is_parallel: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], reduce_results: bool, quant_config: Optional[QuantizationConfig], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], use_presharded_weights: bool)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 1231,
    "qualname": "RowParallelLinear.weight_loader",
    "signature": "def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 1283,
    "qualname": "RowParallelLinear.weight_loader_v2",
    "signature": "def weight_loader_v2(self, param: BasevLLMParameter, loaded_weight: torch.Tensor)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 1304,
    "qualname": "RowParallelLinear.forward",
    "signature": "def forward(self, input_, skip_all_reduce)"
  },
  {
    "module": "srt.layers.linear",
    "file": "python/sglang/srt/layers/linear.py",
    "line": 1331,
    "qualname": "RowParallelLinear.extra_repr",
    "signature": "def extra_repr(self)"
  },
  {
    "module": "srt.layers.logits_processor",
    "file": "python/sglang/srt/layers/logits_processor.py",
    "line": 123,
    "qualname": "LogitsMetadata.from_forward_batch",
    "signature": "def from_forward_batch(cls, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.logits_processor",
    "file": "python/sglang/srt/layers/logits_processor.py",
    "line": 173,
    "qualname": "LogitsMetadata.compute_dp_attention_metadata",
    "signature": "def compute_dp_attention_metadata(self)"
  },
  {
    "module": "srt.layers.logits_processor",
    "file": "python/sglang/srt/layers/logits_processor.py",
    "line": 202,
    "qualname": "LogitsProcessor.__init__",
    "signature": "def __init__(self, config, skip_all_gather: bool, logit_scale: Optional[float])"
  },
  {
    "module": "srt.layers.logits_processor",
    "file": "python/sglang/srt/layers/logits_processor.py",
    "line": 235,
    "qualname": "LogitsProcessor.forward",
    "signature": "def forward(self, input_ids, hidden_states, lm_head: VocabParallelEmbedding, logits_metadata: Union[LogitsMetadata, ForwardBatch], aux_hidden_states: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.logits_processor",
    "file": "python/sglang/srt/layers/logits_processor.py",
    "line": 525,
    "qualname": "LogitsProcessor.get_top_logprobs",
    "signature": "def get_top_logprobs(all_logprobs: torch.Tensor, logits_metadata: LogitsMetadata)"
  },
  {
    "module": "srt.layers.logits_processor",
    "file": "python/sglang/srt/layers/logits_processor.py",
    "line": 554,
    "qualname": "LogitsProcessor.get_token_ids_logprobs",
    "signature": "def get_token_ids_logprobs(all_logprobs: torch.Tensor, logits_metadata: LogitsMetadata)"
  },
  {
    "module": "srt.layers.logits_processor",
    "file": "python/sglang/srt/layers/logits_processor.py",
    "line": 577,
    "qualname": "LogitsProcessor.compute_temp_top_p_normalized_logprobs",
    "signature": "def compute_temp_top_p_normalized_logprobs(last_logits: torch.Tensor, logits_metadata: LogitsMetadata)"
  },
  {
    "module": "srt.layers.logits_processor",
    "file": "python/sglang/srt/layers/logits_processor.py",
    "line": 607,
    "qualname": "fused_softcap_kernel",
    "signature": "def fused_softcap_kernel(full_logits_ptr, softcapping_value, n_elements, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.logits_processor",
    "file": "python/sglang/srt/layers/logits_processor.py",
    "line": 634,
    "qualname": "fused_softcap",
    "signature": "def fused_softcap(full_logits, final_logit_softcapping)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 82,
    "qualname": "RotaryEmbedding.__init__",
    "signature": "def __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 139,
    "qualname": "RotaryEmbedding.forward_native",
    "signature": "def forward_native(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 169,
    "qualname": "RotaryEmbedding.forward_npu",
    "signature": "def forward_npu(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 199,
    "qualname": "RotaryEmbedding.forward_cpu",
    "signature": "def forward_cpu(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 219,
    "qualname": "RotaryEmbedding.forward_cuda",
    "signature": "def forward_cuda(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor], fused_set_kv_buffer_arg)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 257,
    "qualname": "RotaryEmbedding.extra_repr",
    "signature": "def extra_repr(self)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 293,
    "qualname": "LinearScalingRotaryEmbedding.__init__",
    "signature": "def __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factors: Union[List[float], float], dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 347,
    "qualname": "LinearScalingRotaryEmbedding.scaling_factor_to_offset",
    "signature": "def scaling_factor_to_offset(self)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 357,
    "qualname": "DynamicNTKScalingRotaryEmbedding.__init__",
    "signature": "def __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 444,
    "qualname": "YaRNScalingRotaryEmbedding.__init__",
    "signature": "def __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 513,
    "qualname": "Phi3LongRoPEScaledRotaryEmbedding.__init__",
    "signature": "def __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, original_max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, short_factor: List[float], long_factor: List[float], short_mscale: Optional[float], long_mscale: Optional[float])"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 604,
    "qualname": "Phi3LongRoPEScaledRotaryEmbedding.forward",
    "signature": "def forward(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 646,
    "qualname": "yarn_get_mscale",
    "signature": "def yarn_get_mscale(scale: float, mscale: float)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 658,
    "qualname": "DeepseekScalingRotaryEmbedding.__init__",
    "signature": "def __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 737,
    "qualname": "DeepseekScalingRotaryEmbedding.forward_native",
    "signature": "def forward_native(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 778,
    "qualname": "DeepseekScalingRotaryEmbedding.forward_npu",
    "signature": "def forward_npu(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 818,
    "qualname": "DeepseekScalingRotaryEmbedding.forward_cpu",
    "signature": "def forward_cpu(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 836,
    "qualname": "Llama3RotaryEmbedding.__init__",
    "signature": "def __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, scaling_factor: float, low_freq_factor: float, high_freq_factor: float, orig_max_position: int)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 883,
    "qualname": "Llama4VisionRotaryEmbedding.__init__",
    "signature": "def __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 926,
    "qualname": "Llama4VisionRotaryEmbedding.forward",
    "signature": "def forward(self, query: torch.Tensor, key: torch.Tensor)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 950,
    "qualname": "DynamicNTKAlphaRotaryEmbedding.__init__",
    "signature": "def __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_alpha: float, dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 984,
    "qualname": "MRotaryEmbedding.__init__",
    "signature": "def __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, mrope_section: Optional[List[int]])"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 1033,
    "qualname": "MRotaryEmbedding.forward",
    "signature": "def forward(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 1082,
    "qualname": "MRotaryEmbedding.get_rope_index",
    "signature": "def get_rope_index(spatial_merge_size: int, image_token_id: int, video_token_id: int, vision_start_token_id: int, model_type: str, tokens_per_second: Optional[int], input_ids: Optional[torch.LongTensor], image_grid_thw: Optional[torch.LongTensor], video_grid_thw: Optional[torch.LongTensor], second_per_grid_ts: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 1240,
    "qualname": "MRotaryEmbedding.get_rope_index_glm4v",
    "signature": "def get_rope_index_glm4v(input_ids: torch.Tensor, hf_config: Any, image_grid_thw: Union[list[list[int]], torch.Tensor], video_grid_thw: Union[list[list[int]], torch.Tensor], attention_mask: torch.Tensor)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 1437,
    "qualname": "MRotaryEmbedding.get_next_input_positions",
    "signature": "def get_next_input_positions(mrope_position_delta: int, context_len: int, seq_len: int)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 1458,
    "qualname": "DualChunkRotaryEmbedding.__init__",
    "signature": "def __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, chunk_size: int, local_size: int)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 1560,
    "qualname": "DualChunkRotaryEmbedding.forward",
    "signature": "def forward(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 1643,
    "qualname": "DualChunkRotaryEmbedding.extra_repr",
    "signature": "def extra_repr(self)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 1654,
    "qualname": "get_rope",
    "signature": "def get_rope(head_size: int, rotary_dim: int, max_position: int, base: int, is_neox_style: bool, rope_scaling: Optional[Dict[str, Any]], dtype: Optional[torch.dtype], partial_rotary_factor: float, dual_chunk_attention_config: Optional[Dict[str, Any]])"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 1872,
    "qualname": "rotate_half",
    "signature": "def rotate_half(x)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 1879,
    "qualname": "apply_rotary_pos_emb",
    "signature": "def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor, unsqueeze_dim)"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 1902,
    "qualname": "get_rope_cpu",
    "signature": "def get_rope_cpu(head_size: int, rotary_dim: int, max_position: int, base: int, is_neox_style: bool, rope_scaling: Optional[Dict[str, Any]], dtype: Optional[torch.dtype], partial_rotary_factor: float, device: Optional[str])"
  },
  {
    "module": "srt.layers.rotary_embedding",
    "file": "python/sglang/srt/layers/rotary_embedding.py",
    "line": 1974,
    "qualname": "get_rope_wrapper",
    "signature": "def get_rope_wrapper(head_size: int, rotary_dim: int, max_position: int, base: int, is_neox_style: bool, rope_scaling: Optional[Dict[str, Any]], dtype: Optional[torch.dtype], partial_rotary_factor: float, device: Optional[str])"
  },
  {
    "module": "srt.layers.sampler",
    "file": "python/sglang/srt/layers/sampler.py",
    "line": 33,
    "qualname": "Sampler.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.layers.sampler",
    "file": "python/sglang/srt/layers/sampler.py",
    "line": 41,
    "qualname": "Sampler.forward",
    "signature": "def forward(self, logits_output: LogitsProcessorOutput, sampling_info: SamplingBatchInfo, return_logprob: bool, top_logprobs_nums: List[int], token_ids_logprobs: List[List[int]])"
  },
  {
    "module": "srt.layers.sampler",
    "file": "python/sglang/srt/layers/sampler.py",
    "line": 157,
    "qualname": "top_k_top_p_min_p_sampling_from_probs_torch",
    "signature": "def top_k_top_p_min_p_sampling_from_probs_torch(probs: torch.Tensor, top_ks: torch.Tensor, top_ps: torch.Tensor, min_ps: torch.Tensor, need_min_p_sampling: bool)"
  },
  {
    "module": "srt.layers.sampler",
    "file": "python/sglang/srt/layers/sampler.py",
    "line": 184,
    "qualname": "sampling_from_probs_torch",
    "signature": "def sampling_from_probs_torch(probs: torch.Tensor)"
  },
  {
    "module": "srt.layers.sampler",
    "file": "python/sglang/srt/layers/sampler.py",
    "line": 192,
    "qualname": "top_p_normalize_probs_torch",
    "signature": "def top_p_normalize_probs_torch(probs: torch.Tensor, top_ps: torch.Tensor)"
  },
  {
    "module": "srt.layers.sampler",
    "file": "python/sglang/srt/layers/sampler.py",
    "line": 204,
    "qualname": "get_top_logprobs",
    "signature": "def get_top_logprobs(logprobs: torch.Tensor, top_logprobs_nums: List[int])"
  },
  {
    "module": "srt.layers.sampler",
    "file": "python/sglang/srt/layers/sampler.py",
    "line": 218,
    "qualname": "get_token_ids_logprobs",
    "signature": "def get_token_ids_logprobs(logprobs: torch.Tensor, token_ids_logprobs: List[List[int]])"
  },
  {
    "module": "srt.layers.sampler",
    "file": "python/sglang/srt/layers/sampler.py",
    "line": 232,
    "qualname": "apply_custom_logit_processor",
    "signature": "def apply_custom_logit_processor(logits: torch.Tensor, sampling_batch_info: SamplingBatchInfo, num_tokens_in_batch: int)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 21,
    "qualname": "BaseLayerWithLoRA.__init__",
    "signature": "def __init__(self, base_layer: nn.Module, lora_backend: BaseLoRABackend)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 31,
    "qualname": "BaseLayerWithLoRA.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 34,
    "qualname": "BaseLayerWithLoRA.set_lora_info",
    "signature": "def set_lora_info(self)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 37,
    "qualname": "BaseLayerWithLoRA.slice_lora_a_weights",
    "signature": "def slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 40,
    "qualname": "BaseLayerWithLoRA.slice_lora_b_weights",
    "signature": "def slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 53,
    "qualname": "VocabParallelEmbeddingWithLoRA.__init__",
    "signature": "def __init__(self, base_layer: VocabParallelEmbedding, lora_backend: BaseLoRABackend)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 63,
    "qualname": "ColumnParallelLinearWithLoRA.__init__",
    "signature": "def __init__(self, base_layer: ColumnParallelLinear, lora_backend: BaseLoRABackend)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 70,
    "qualname": "ColumnParallelLinearWithLoRA.set_lora_info",
    "signature": "def set_lora_info(self, A_buffer: torch.Tensor, B_buffer: torch.Tensor)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 79,
    "qualname": "ColumnParallelLinearWithLoRA.apply_lora",
    "signature": "def apply_lora(self, base_output: torch.Tensor, x: torch.Tensor)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 88,
    "qualname": "ColumnParallelLinearWithLoRA.forward",
    "signature": "def forward(self, input_: torch.Tensor)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 105,
    "qualname": "ColumnParallelLinearWithLoRA.slice_lora_a_weights",
    "signature": "def slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 108,
    "qualname": "ColumnParallelLinearWithLoRA.slice_lora_b_weights",
    "signature": "def slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 117,
    "qualname": "MergedColumnParallelLinearWithLoRA.__init__",
    "signature": "def __init__(self, base_layer: MergedColumnParallelLinear, lora_backend: BaseLoRABackend)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 124,
    "qualname": "MergedColumnParallelLinearWithLoRA.set_lora_info",
    "signature": "def set_lora_info(self, A_buffer: torch.Tensor, B_buffer: torch.Tensor)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 133,
    "qualname": "MergedColumnParallelLinearWithLoRA.apply_lora",
    "signature": "def apply_lora(self, base_output: torch.Tensor, x: torch.Tensor)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 142,
    "qualname": "MergedColumnParallelLinearWithLoRA.slice_lora_a_weights",
    "signature": "def slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 145,
    "qualname": "MergedColumnParallelLinearWithLoRA.slice_lora_b_weights",
    "signature": "def slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 161,
    "qualname": "QKVParallelLinearWithLoRA.__init__",
    "signature": "def __init__(self, base_layer: QKVParallelLinear, lora_backend: BaseLoRABackend)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 183,
    "qualname": "QKVParallelLinearWithLoRA.set_lora_info",
    "signature": "def set_lora_info(self, A_buffer_qkv: torch.Tensor, B_buffer_qkv: torch.Tensor)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 192,
    "qualname": "QKVParallelLinearWithLoRA.apply_lora",
    "signature": "def apply_lora(self, base_output: torch.Tensor, x: torch.Tensor)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 203,
    "qualname": "QKVParallelLinearWithLoRA.slice_lora_a_weights",
    "signature": "def slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 206,
    "qualname": "QKVParallelLinearWithLoRA.slice_lora_b_weights",
    "signature": "def slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 235,
    "qualname": "RowParallelLinearWithLoRA.__init__",
    "signature": "def __init__(self, base_layer: RowParallelLinear, lora_backend: BaseLoRABackend)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 242,
    "qualname": "RowParallelLinearWithLoRA.set_lora_info",
    "signature": "def set_lora_info(self, A_buffer: torch.Tensor, B_buffer: torch.Tensor)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 247,
    "qualname": "RowParallelLinearWithLoRA.apply_lora",
    "signature": "def apply_lora(self, base_output: torch.Tensor, x: torch.Tensor)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 256,
    "qualname": "RowParallelLinearWithLoRA.forward",
    "signature": "def forward(self, input_: torch.Tensor, skip_all_reduce)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 294,
    "qualname": "RowParallelLinearWithLoRA.slice_lora_a_weights",
    "signature": "def slice_lora_a_weights(self, A: torch.Tensor, tp_rank: int)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 301,
    "qualname": "RowParallelLinearWithLoRA.slice_lora_b_weights",
    "signature": "def slice_lora_b_weights(self, B: torch.Tensor, tp_rank: int)"
  },
  {
    "module": "srt.lora.layers",
    "file": "python/sglang/srt/lora/layers.py",
    "line": 305,
    "qualname": "get_lora_layer",
    "signature": "def get_lora_layer(layer: nn.Module, lora_backend: BaseLoRABackend)"
  },
  {
    "module": "srt.lora.lora",
    "file": "python/sglang/srt/lora/lora.py",
    "line": 38,
    "qualname": "LoRALayer.__init__",
    "signature": "def __init__(self, config: LoRAConfig, base_hf_config: AutoConfig)"
  },
  {
    "module": "srt.lora.lora",
    "file": "python/sglang/srt/lora/lora.py",
    "line": 48,
    "qualname": "LoRAAdapter.__init__",
    "signature": "def __init__(self, uid: str, config: LoRAConfig, base_hf_config: AutoConfig, load_config: LoadConfig, lora_backend: BaseLoRABackend)"
  },
  {
    "module": "srt.lora.lora",
    "file": "python/sglang/srt/lora/lora.py",
    "line": 75,
    "qualname": "LoRAAdapter.initialize_weights",
    "signature": "def initialize_weights(self)"
  },
  {
    "module": "srt.lora.lora",
    "file": "python/sglang/srt/lora/lora.py",
    "line": 97,
    "qualname": "LoRAAdapter.normalize_qkv_proj",
    "signature": "def normalize_qkv_proj(self, weight_names: List[str], weights: Dict[str, torch.Tensor])"
  },
  {
    "module": "srt.lora.lora",
    "file": "python/sglang/srt/lora/lora.py",
    "line": 150,
    "qualname": "LoRAAdapter.normalize_gate_up_proj",
    "signature": "def normalize_gate_up_proj(self, weight_names: List[str], weights: Dict[str, torch.Tensor])"
  },
  {
    "module": "srt.lora.lora_config",
    "file": "python/sglang/srt/lora/lora_config.py",
    "line": 22,
    "qualname": "LoRAConfig.__init__",
    "signature": "def __init__(self, path: str)"
  },
  {
    "module": "srt.lora.lora_config",
    "file": "python/sglang/srt/lora/lora_config.py",
    "line": 37,
    "qualname": "LoRAConfig.get_lora_config",
    "signature": "def get_lora_config(self, dummy)"
  },
  {
    "module": "srt.lora.lora_manager",
    "file": "python/sglang/srt/lora/lora_manager.py",
    "line": 46,
    "qualname": "LoRAManager.__init__",
    "signature": "def __init__(self, base_model: torch.nn.Module, base_hf_config: AutoConfig, max_loras_per_batch: int, load_config: LoadConfig, dtype: torch.dtype, lora_backend: str, tp_size: int, tp_rank: int, max_lora_rank: Optional[int], target_modules: Optional[Iterable[str]], lora_paths: Optional[List[LoRARef]])"
  },
  {
    "module": "srt.lora.lora_manager",
    "file": "python/sglang/srt/lora/lora_manager.py",
    "line": 81,
    "qualname": "LoRAManager.init_cuda_graph_batch_info",
    "signature": "def init_cuda_graph_batch_info(self, max_bs_in_cuda_graph: int)"
  },
  {
    "module": "srt.lora.lora_manager",
    "file": "python/sglang/srt/lora/lora_manager.py",
    "line": 109,
    "qualname": "LoRAManager.create_lora_update_result",
    "signature": "def create_lora_update_result(self, success: bool, error_message: str)"
  },
  {
    "module": "srt.lora.lora_manager",
    "file": "python/sglang/srt/lora/lora_manager.py",
    "line": 121,
    "qualname": "LoRAManager.load_lora_adapter",
    "signature": "def load_lora_adapter(self, lora_ref: LoRARef)"
  },
  {
    "module": "srt.lora.lora_manager",
    "file": "python/sglang/srt/lora/lora_manager.py",
    "line": 155,
    "qualname": "LoRAManager.validate_new_adapter",
    "signature": "def validate_new_adapter(self, lora_config: LoRAConfig, lora_ref: LoRARef)"
  },
  {
    "module": "srt.lora.lora_manager",
    "file": "python/sglang/srt/lora/lora_manager.py",
    "line": 178,
    "qualname": "LoRAManager.unload_lora_adapter",
    "signature": "def unload_lora_adapter(self, lora_ref: LoRARef)"
  },
  {
    "module": "srt.lora.lora_manager",
    "file": "python/sglang/srt/lora/lora_manager.py",
    "line": 203,
    "qualname": "LoRAManager.validate_lora_batch",
    "signature": "def validate_lora_batch(self, lora_ids: set[str])"
  },
  {
    "module": "srt.lora.lora_manager",
    "file": "python/sglang/srt/lora/lora_manager.py",
    "line": 234,
    "qualname": "LoRAManager.prepare_lora_batch",
    "signature": "def prepare_lora_batch(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.lora.lora_manager",
    "file": "python/sglang/srt/lora/lora_manager.py",
    "line": 347,
    "qualname": "LoRAManager.update_lora_info",
    "signature": "def update_lora_info(self)"
  },
  {
    "module": "srt.lora.lora_manager",
    "file": "python/sglang/srt/lora/lora_manager.py",
    "line": 369,
    "qualname": "LoRAManager.init_state",
    "signature": "def init_state(self, max_lora_rank: Optional[int], target_modules: Optional[Iterable[str]], lora_paths: Optional[List[LoRARef]])"
  },
  {
    "module": "srt.lora.lora_manager",
    "file": "python/sglang/srt/lora/lora_manager.py",
    "line": 395,
    "qualname": "LoRAManager.init_lora_adapters",
    "signature": "def init_lora_adapters(self, lora_paths: Optional[List[LoRARef]])"
  },
  {
    "module": "srt.lora.lora_manager",
    "file": "python/sglang/srt/lora/lora_manager.py",
    "line": 416,
    "qualname": "LoRAManager.init_lora_shapes",
    "signature": "def init_lora_shapes(self, max_lora_rank: Optional[int], target_modules: Optional[Iterable[str]])"
  },
  {
    "module": "srt.lora.lora_manager",
    "file": "python/sglang/srt/lora/lora_manager.py",
    "line": 463,
    "qualname": "LoRAManager.load_lora_weights",
    "signature": "def load_lora_weights(self, lora_ref: LoRARef)"
  },
  {
    "module": "srt.lora.lora_manager",
    "file": "python/sglang/srt/lora/lora_manager.py",
    "line": 477,
    "qualname": "LoRAManager.init_memory_pool",
    "signature": "def init_memory_pool(self)"
  },
  {
    "module": "srt.lora.lora_manager",
    "file": "python/sglang/srt/lora/lora_manager.py",
    "line": 490,
    "qualname": "LoRAManager.set_lora_module",
    "signature": "def set_lora_module(self, module_name, module)"
  },
  {
    "module": "srt.lora.lora_manager",
    "file": "python/sglang/srt/lora/lora_manager.py",
    "line": 495,
    "qualname": "LoRAManager.init_lora_modules",
    "signature": "def init_lora_modules(self)"
  },
  {
    "module": "srt.lora.lora_registry",
    "file": "python/sglang/srt/lora/lora_registry.py",
    "line": 40,
    "qualname": "LoRARef.__post_init__",
    "signature": "def __post_init__(self)"
  },
  {
    "module": "srt.lora.lora_registry",
    "file": "python/sglang/srt/lora/lora_registry.py",
    "line": 44,
    "qualname": "LoRARef.__str__",
    "signature": "def __str__(self)"
  },
  {
    "module": "srt.lora.lora_registry",
    "file": "python/sglang/srt/lora/lora_registry.py",
    "line": 62,
    "qualname": "LoRARegistry.__init__",
    "signature": "def __init__(self, lora_paths: Optional[List[LoRARef]])"
  },
  {
    "module": "srt.lora.lora_registry",
    "file": "python/sglang/srt/lora/lora_registry.py",
    "line": 84,
    "qualname": "LoRARegistry.register",
    "signature": "async def register(self, lora_ref: LoRARef)"
  },
  {
    "module": "srt.lora.lora_registry",
    "file": "python/sglang/srt/lora/lora_registry.py",
    "line": 94,
    "qualname": "LoRARegistry.unregister",
    "signature": "async def unregister(self, lora_name: str)"
  },
  {
    "module": "srt.lora.lora_registry",
    "file": "python/sglang/srt/lora/lora_registry.py",
    "line": 111,
    "qualname": "LoRARegistry.acquire",
    "signature": "async def acquire(self, lora_name: Union[str, List[str]])"
  },
  {
    "module": "srt.lora.lora_registry",
    "file": "python/sglang/srt/lora/lora_registry.py",
    "line": 151,
    "qualname": "LoRARegistry.release",
    "signature": "async def release(self, lora_id: Union[str, List[str]])"
  },
  {
    "module": "srt.lora.lora_registry",
    "file": "python/sglang/srt/lora/lora_registry.py",
    "line": 170,
    "qualname": "LoRARegistry.wait_for_unload",
    "signature": "async def wait_for_unload(self, lora_id: str)"
  },
  {
    "module": "srt.lora.lora_registry",
    "file": "python/sglang/srt/lora/lora_registry.py",
    "line": 203,
    "qualname": "LoRARegistry.num_registered_loras",
    "signature": "def num_registered_loras(self)"
  },
  {
    "module": "srt.lora.mem_pool",
    "file": "python/sglang/srt/lora/mem_pool.py",
    "line": 32,
    "qualname": "EmptySlot.__repr__",
    "signature": "def __repr__(self)"
  },
  {
    "module": "srt.lora.mem_pool",
    "file": "python/sglang/srt/lora/mem_pool.py",
    "line": 35,
    "qualname": "EmptySlot.__new__",
    "signature": "def __new__(cls)"
  },
  {
    "module": "srt.lora.mem_pool",
    "file": "python/sglang/srt/lora/mem_pool.py",
    "line": 47,
    "qualname": "LoRAMemoryPool.__init__",
    "signature": "def __init__(self, base_hf_config: AutoConfig, max_loras_per_batch: int, dtype: torch.dtype, tp_size: int, tp_rank: int, max_lora_rank: int, target_modules: Set[str], base_model: torch.nn.Module)"
  },
  {
    "module": "srt.lora.mem_pool",
    "file": "python/sglang/srt/lora/mem_pool.py",
    "line": 87,
    "qualname": "LoRAMemoryPool.can_support",
    "signature": "def can_support(self, config: Union[LoRAConfig, Iterable[LoRAConfig]])"
  },
  {
    "module": "srt.lora.mem_pool",
    "file": "python/sglang/srt/lora/mem_pool.py",
    "line": 106,
    "qualname": "LoRAMemoryPool.get_lora_A_shape",
    "signature": "def get_lora_A_shape(self, module_name: str, base_model: torch.nn.Module, max_lora_dim: int)"
  },
  {
    "module": "srt.lora.mem_pool",
    "file": "python/sglang/srt/lora/mem_pool.py",
    "line": 122,
    "qualname": "LoRAMemoryPool.get_lora_B_shape",
    "signature": "def get_lora_B_shape(self, module_name: str, base_model: torch.nn.Module, max_lora_dim: int)"
  },
  {
    "module": "srt.lora.mem_pool",
    "file": "python/sglang/srt/lora/mem_pool.py",
    "line": 137,
    "qualname": "LoRAMemoryPool.init_buffers",
    "signature": "def init_buffers(self, base_model: torch.nn.Module)"
  },
  {
    "module": "srt.lora.mem_pool",
    "file": "python/sglang/srt/lora/mem_pool.py",
    "line": 170,
    "qualname": "LoRAMemoryPool.prepare_lora_batch",
    "signature": "def prepare_lora_batch(self, cur_uids: Set[Optional[str]], lora_adapters: Dict[str, LoRAAdapter], lora_modules: List[Dict[str, BaseLayerWithLoRA]], lora_refs: Dict[str, LoRARef])"
  },
  {
    "module": "srt.lora.mem_pool",
    "file": "python/sglang/srt/lora/mem_pool.py",
    "line": 214,
    "qualname": "LoRAMemoryPool.load_lora_weight_to_buffer",
    "signature": "def load_lora_weight_to_buffer(self, uid: str, buffer_id: int, lora_adapter: LoRAAdapter, lora_modules: List[Dict[str, BaseLayerWithLoRA]])"
  },
  {
    "module": "srt.lora.mem_pool",
    "file": "python/sglang/srt/lora/mem_pool.py",
    "line": 286,
    "qualname": "LoRAMemoryPool.get_tensor",
    "signature": "def get_tensor(self, target_module: str, layer_id: int, lora_type: LoRAType)"
  },
  {
    "module": "srt.lora.mem_pool",
    "file": "python/sglang/srt/lora/mem_pool.py",
    "line": 294,
    "qualname": "LoRAMemoryPool.get_buffer_id",
    "signature": "def get_buffer_id(self, lora_uid: str)"
  },
  {
    "module": "srt.lora.utils",
    "file": "python/sglang/srt/lora/utils.py",
    "line": 40,
    "qualname": "get_layer_id",
    "signature": "def get_layer_id(name: str)"
  },
  {
    "module": "srt.lora.utils",
    "file": "python/sglang/srt/lora/utils.py",
    "line": 50,
    "qualname": "get_hidden_dim",
    "signature": "def get_hidden_dim(module_name: str, config: AutoConfig, base_model: torch.nn.Module)"
  },
  {
    "module": "srt.lora.utils",
    "file": "python/sglang/srt/lora/utils.py",
    "line": 87,
    "qualname": "get_normalized_target_modules",
    "signature": "def get_normalized_target_modules(target_modules: Iterable[str])"
  },
  {
    "module": "srt.lora.utils",
    "file": "python/sglang/srt/lora/utils.py",
    "line": 108,
    "qualname": "get_stacked_multiply",
    "signature": "def get_stacked_multiply(module_name: str)"
  },
  {
    "module": "srt.lora.utils",
    "file": "python/sglang/srt/lora/utils.py",
    "line": 119,
    "qualname": "get_target_module_name",
    "signature": "def get_target_module_name(full_module_name: str, target_modules: Set[str])"
  },
  {
    "module": "srt.managers.data_parallel_controller",
    "file": "python/sglang/srt/managers/data_parallel_controller.py",
    "line": 56,
    "qualname": "LoadBalanceMethod.from_str",
    "signature": "def from_str(cls, method: str)"
  },
  {
    "module": "srt.managers.data_parallel_controller",
    "file": "python/sglang/srt/managers/data_parallel_controller.py",
    "line": 67,
    "qualname": "DataParallelController.__init__",
    "signature": "def __init__(self, server_args: ServerArgs, port_args: PortArgs, dp_balance_meta: DPBalanceMeta)"
  },
  {
    "module": "srt.managers.data_parallel_controller",
    "file": "python/sglang/srt/managers/data_parallel_controller.py",
    "line": 124,
    "qualname": "DataParallelController.launch_dp_schedulers",
    "signature": "def launch_dp_schedulers(self, server_args, port_args)"
  },
  {
    "module": "srt.managers.data_parallel_controller",
    "file": "python/sglang/srt/managers/data_parallel_controller.py",
    "line": 164,
    "qualname": "DataParallelController.launch_tensor_parallel_group_thread",
    "signature": "def launch_tensor_parallel_group_thread(self, server_args: ServerArgs, port_args: PortArgs, base_gpu_id: int, dp_rank: int, ready_event: threading.Event)"
  },
  {
    "module": "srt.managers.data_parallel_controller",
    "file": "python/sglang/srt/managers/data_parallel_controller.py",
    "line": 180,
    "qualname": "DataParallelController.launch_dp_attention_schedulers",
    "signature": "def launch_dp_attention_schedulers(self, server_args, port_args)"
  },
  {
    "module": "srt.managers.data_parallel_controller",
    "file": "python/sglang/srt/managers/data_parallel_controller.py",
    "line": 187,
    "qualname": "DataParallelController.launch_tensor_parallel_group",
    "signature": "def launch_tensor_parallel_group(self, server_args: ServerArgs, port_args: PortArgs, base_gpu_id: int, dp_rank: int)"
  },
  {
    "module": "srt.managers.data_parallel_controller",
    "file": "python/sglang/srt/managers/data_parallel_controller.py",
    "line": 269,
    "qualname": "DataParallelController.round_robin_scheduler",
    "signature": "def round_robin_scheduler(self, req: Req)"
  },
  {
    "module": "srt.managers.data_parallel_controller",
    "file": "python/sglang/srt/managers/data_parallel_controller.py",
    "line": 286,
    "qualname": "DataParallelController.shortest_queue_scheduler",
    "signature": "def shortest_queue_scheduler(self, input_requests)"
  },
  {
    "module": "srt.managers.data_parallel_controller",
    "file": "python/sglang/srt/managers/data_parallel_controller.py",
    "line": 289,
    "qualname": "DataParallelController.minimum_tokens_scheduler",
    "signature": "def minimum_tokens_scheduler(self, req)"
  },
  {
    "module": "srt.managers.data_parallel_controller",
    "file": "python/sglang/srt/managers/data_parallel_controller.py",
    "line": 316,
    "qualname": "DataParallelController.event_loop",
    "signature": "def event_loop(self)"
  },
  {
    "module": "srt.managers.data_parallel_controller",
    "file": "python/sglang/srt/managers/data_parallel_controller.py",
    "line": 341,
    "qualname": "run_data_parallel_controller_process",
    "signature": "def run_data_parallel_controller_process(server_args: ServerArgs, port_args: PortArgs, pipe_writer)"
  },
  {
    "module": "srt.managers.detokenizer_manager",
    "file": "python/sglang/srt/managers/detokenizer_manager.py",
    "line": 73,
    "qualname": "DetokenizerManager.__init__",
    "signature": "def __init__(self, server_args: ServerArgs, port_args: PortArgs)"
  },
  {
    "module": "srt.managers.detokenizer_manager",
    "file": "python/sglang/srt/managers/detokenizer_manager.py",
    "line": 111,
    "qualname": "DetokenizerManager.event_loop",
    "signature": "def event_loop(self)"
  },
  {
    "module": "srt.managers.detokenizer_manager",
    "file": "python/sglang/srt/managers/detokenizer_manager.py",
    "line": 119,
    "qualname": "DetokenizerManager.trim_matched_stop",
    "signature": "def trim_matched_stop(self, output: Union[str, List[int]], finished_reason: Dict, no_stop_trim: bool)"
  },
  {
    "module": "srt.managers.detokenizer_manager",
    "file": "python/sglang/srt/managers/detokenizer_manager.py",
    "line": 145,
    "qualname": "DetokenizerManager.handle_batch_embedding_out",
    "signature": "def handle_batch_embedding_out(self, recv_obj: BatchEmbeddingOut)"
  },
  {
    "module": "srt.managers.detokenizer_manager",
    "file": "python/sglang/srt/managers/detokenizer_manager.py",
    "line": 149,
    "qualname": "DetokenizerManager.handle_batch_token_id_out",
    "signature": "def handle_batch_token_id_out(self, recv_obj: BatchTokenIDOut)"
  },
  {
    "module": "srt.managers.detokenizer_manager",
    "file": "python/sglang/srt/managers/detokenizer_manager.py",
    "line": 248,
    "qualname": "DetokenizerManager.handle_multimodal_decode_req",
    "signature": "def handle_multimodal_decode_req(self, recv_obj: BatchMultimodalDecodeReq)"
  },
  {
    "module": "srt.managers.detokenizer_manager",
    "file": "python/sglang/srt/managers/detokenizer_manager.py",
    "line": 259,
    "qualname": "DetokenizerManager.handle_freeze_gc_req",
    "signature": "def handle_freeze_gc_req(self, recv_req: FreezeGCReq)"
  },
  {
    "module": "srt.managers.detokenizer_manager",
    "file": "python/sglang/srt/managers/detokenizer_manager.py",
    "line": 265,
    "qualname": "LimitedCapacityDict.__init__",
    "signature": "def __init__(self, capacity: int)"
  },
  {
    "module": "srt.managers.detokenizer_manager",
    "file": "python/sglang/srt/managers/detokenizer_manager.py",
    "line": 269,
    "qualname": "LimitedCapacityDict.__setitem__",
    "signature": "def __setitem__(self, key, value)"
  },
  {
    "module": "srt.managers.detokenizer_manager",
    "file": "python/sglang/srt/managers/detokenizer_manager.py",
    "line": 277,
    "qualname": "run_detokenizer_process",
    "signature": "def run_detokenizer_process(server_args: ServerArgs, port_args: PortArgs)"
  },
  {
    "module": "srt.managers.multimodal_processor",
    "file": "python/sglang/srt/managers/multimodal_processor.py",
    "line": 15,
    "qualname": "import_processors",
    "signature": "def import_processors()"
  },
  {
    "module": "srt.managers.multimodal_processor",
    "file": "python/sglang/srt/managers/multimodal_processor.py",
    "line": 39,
    "qualname": "get_mm_processor",
    "signature": "def get_mm_processor(hf_config, server_args: ServerArgs, processor, transport_mode)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 120,
    "qualname": "BaseFinishReason.__init__",
    "signature": "def __init__(self, is_error: bool)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 123,
    "qualname": "BaseFinishReason.to_json",
    "signature": "def to_json(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 128,
    "qualname": "FINISH_MATCHED_TOKEN.__init__",
    "signature": "def __init__(self, matched: Union[int, List[int]])"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 132,
    "qualname": "FINISH_MATCHED_TOKEN.to_json",
    "signature": "def to_json(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 140,
    "qualname": "FINISH_MATCHED_STR.__init__",
    "signature": "def __init__(self, matched: str)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 144,
    "qualname": "FINISH_MATCHED_STR.to_json",
    "signature": "def to_json(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 152,
    "qualname": "FINISH_LENGTH.__init__",
    "signature": "def __init__(self, length: int)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 156,
    "qualname": "FINISH_LENGTH.to_json",
    "signature": "def to_json(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 164,
    "qualname": "FINISH_ABORT.__init__",
    "signature": "def __init__(self, message, status_code, err_type)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 170,
    "qualname": "FINISH_ABORT.to_json",
    "signature": "def to_json(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 186,
    "qualname": "Modality.from_str",
    "signature": "def from_str(modality_str: str)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 195,
    "qualname": "Modality.all",
    "signature": "def all()"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 223,
    "qualname": "MultimodalDataItem.__getattr__",
    "signature": "def __getattr__(self, name: str)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 234,
    "qualname": "MultimodalDataItem.__setitem__",
    "signature": "def __setitem__(self, key: str, value: Any)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 240,
    "qualname": "MultimodalDataItem.set",
    "signature": "def set(self, key: str, value: Any)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 244,
    "qualname": "MultimodalDataItem.is_empty_list",
    "signature": "def is_empty_list(l)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 249,
    "qualname": "MultimodalDataItem.set_pad_value",
    "signature": "def set_pad_value(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 264,
    "qualname": "MultimodalDataItem.is_modality",
    "signature": "def is_modality(self, modality: Modality)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 267,
    "qualname": "MultimodalDataItem.is_audio",
    "signature": "def is_audio(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 270,
    "qualname": "MultimodalDataItem.is_image",
    "signature": "def is_image(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 273,
    "qualname": "MultimodalDataItem.is_video",
    "signature": "def is_video(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 276,
    "qualname": "MultimodalDataItem.is_valid",
    "signature": "def is_valid(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 279,
    "qualname": "MultimodalDataItem.validate",
    "signature": "def validate(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 284,
    "qualname": "MultimodalDataItem.from_dict",
    "signature": "def from_dict(obj: dict)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 293,
    "qualname": "MultimodalDataItem.merge",
    "signature": "def merge(self, other)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 329,
    "qualname": "MultimodalInputs.from_dict",
    "signature": "def from_dict(obj: dict)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 358,
    "qualname": "MultimodalInputs.contains_image_inputs",
    "signature": "def contains_image_inputs(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 361,
    "qualname": "MultimodalInputs.contains_video_inputs",
    "signature": "def contains_video_inputs(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 364,
    "qualname": "MultimodalInputs.contains_audio_inputs",
    "signature": "def contains_audio_inputs(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 367,
    "qualname": "MultimodalInputs.contains_mm_input",
    "signature": "def contains_mm_input(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 370,
    "qualname": "MultimodalInputs.merge",
    "signature": "def merge(self, other: MultimodalInputs)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 414,
    "qualname": "Req.__init__",
    "signature": "def __init__(self, rid: str, origin_input_text: str, origin_input_ids: List[int], sampling_params: SamplingParams, return_logprob: bool, top_logprobs_num: int, token_ids_logprob: List[int], stream: bool, origin_input_ids_unpadded: Optional[Tuple[int]], lora_id: Optional[str], input_embeds: Optional[List[List[float]]], token_type_ids: List[int], session_id: Optional[str], custom_logit_processor: Optional[str], return_hidden_states: bool, eos_token_ids: Optional[Set[int]], bootstrap_host: Optional[str], bootstrap_port: Optional[int], bootstrap_room: Optional[int], data_parallel_rank: Optional[int], vocab_size: Optional[int])"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 619,
    "qualname": "Req.seqlen",
    "signature": "def seqlen(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 622,
    "qualname": "Req.extend_image_inputs",
    "signature": "def extend_image_inputs(self, image_inputs)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 628,
    "qualname": "Req.finished",
    "signature": "def finished(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 632,
    "qualname": "Req.init_next_round_input",
    "signature": "def init_next_round_input(self, tree_cache: Optional[BasePrefixCache])"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 660,
    "qualname": "Req.adjust_max_prefix_ids",
    "signature": "def adjust_max_prefix_ids(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 679,
    "qualname": "Req.init_incremental_detokenize",
    "signature": "def init_incremental_detokenize(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 691,
    "qualname": "Req.check_finished",
    "signature": "def check_finished(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 751,
    "qualname": "Req.reset_for_retract",
    "signature": "def reset_for_retract(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 765,
    "qualname": "Req.offload_kv_cache",
    "signature": "def offload_kv_cache(self, req_to_token_pool, token_to_kv_pool_allocator)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 771,
    "qualname": "Req.load_kv_cache",
    "signature": "def load_kv_cache(self, req_to_token_pool, token_to_kv_pool_allocator)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 778,
    "qualname": "Req.log_time_stats",
    "signature": "def log_time_stats(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 790,
    "qualname": "Req.set_finish_with_abort",
    "signature": "def set_finish_with_abort(self, error_msg: str)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 801,
    "qualname": "Req.__repr__",
    "signature": "def __repr__(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 917,
    "qualname": "ScheduleBatch.init_new",
    "signature": "def init_new(cls, reqs: List[Req], req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, tree_cache: BasePrefixCache, model_config: ModelConfig, enable_overlap: bool, spec_algorithm: SpeculativeAlgorithm, chunked_req: Optional[Req])"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 959,
    "qualname": "ScheduleBatch.batch_size",
    "signature": "def batch_size(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 962,
    "qualname": "ScheduleBatch.is_empty",
    "signature": "def is_empty(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 965,
    "qualname": "ScheduleBatch.alloc_req_slots",
    "signature": "def alloc_req_slots(self, num_reqs: int)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 976,
    "qualname": "ScheduleBatch.alloc_token_slots",
    "signature": "def alloc_token_slots(self, num_tokens: int, backup_state: bool)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1000,
    "qualname": "ScheduleBatch.alloc_paged_token_slots_extend",
    "signature": "def alloc_paged_token_slots_extend(self, prefix_lens: torch.Tensor, seq_lens: torch.Tensor, last_loc: torch.Tensor, extend_num_tokens: int, backup_state: bool)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1035,
    "qualname": "ScheduleBatch.alloc_paged_token_slots_decode",
    "signature": "def alloc_paged_token_slots_decode(self, seq_lens: torch.Tensor, last_loc: torch.Tensor, backup_state: bool)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1063,
    "qualname": "ScheduleBatch.prepare_encoder_info_extend",
    "signature": "def prepare_encoder_info_extend(self, input_ids: List[int], seq_lens: List[int])"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1136,
    "qualname": "ScheduleBatch.prepare_for_extend",
    "signature": "def prepare_for_extend(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1340,
    "qualname": "ScheduleBatch.prepare_for_split_prefill",
    "signature": "def prepare_for_split_prefill(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1345,
    "qualname": "ScheduleBatch.mix_with_running",
    "signature": "def mix_with_running(self, running_batch: 'ScheduleBatch')"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1375,
    "qualname": "ScheduleBatch.new_page_count_next_decode",
    "signature": "def new_page_count_next_decode(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1387,
    "qualname": "ScheduleBatch.check_decode_mem",
    "signature": "def check_decode_mem(self, buf_multiplier)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1397,
    "qualname": "ScheduleBatch.retract_decode",
    "signature": "def retract_decode(self, server_args: ServerArgs)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1521,
    "qualname": "ScheduleBatch.prepare_encoder_info_decode",
    "signature": "def prepare_encoder_info_decode(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1525,
    "qualname": "ScheduleBatch.prepare_for_idle",
    "signature": "def prepare_for_idle(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1539,
    "qualname": "ScheduleBatch.prepare_for_decode",
    "signature": "def prepare_for_decode(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1613,
    "qualname": "ScheduleBatch.filter_batch",
    "signature": "def filter_batch(self, chunked_req_to_exclude: Optional[Union[Req, List[Req]]], keep_indices: Optional[List[int]])"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1671,
    "qualname": "ScheduleBatch.merge_batch",
    "signature": "def merge_batch(self, other: 'ScheduleBatch')"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1711,
    "qualname": "ScheduleBatch.get_model_worker_batch",
    "signature": "def get_model_worker_batch(self, seq_lens_cpu_cache: Optional[torch.Tensor])"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1785,
    "qualname": "ScheduleBatch.copy",
    "signature": "def copy(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1846,
    "qualname": "ScheduleBatch.__str__",
    "signature": "def __str__(self)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1927,
    "qualname": "write_req_to_token_pool_triton",
    "signature": "def write_req_to_token_pool_triton(req_to_token_ptr, req_pool_indices, pre_lens, seq_lens, extend_lens, out_cache_loc, req_to_token_ptr_stride: tl.constexpr)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1963,
    "qualname": "get_last_loc",
    "signature": "def get_last_loc(req_to_token: torch.Tensor, req_pool_indices_tensor: torch.Tensor, prefix_lens_tensor: torch.Tensor)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1979,
    "qualname": "get_last_loc_torch",
    "signature": "def get_last_loc_torch(req_to_token: torch.Tensor, req_pool_indices_tensor: torch.Tensor, prefix_lens_tensor: torch.Tensor)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 1992,
    "qualname": "get_last_loc_kernel",
    "signature": "def get_last_loc_kernel(req_to_token, req_pool_indices_tensor, prefix_lens_tensor, result, num_tokens, req_to_token_stride, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.managers.schedule_batch",
    "file": "python/sglang/srt/managers/schedule_batch.py",
    "line": 2015,
    "qualname": "get_last_loc_triton",
    "signature": "def get_last_loc_triton(req_to_token: torch.Tensor, req_pool_indices_tensor: torch.Tensor, prefix_lens_tensor: torch.Tensor)"
  },
  {
    "module": "srt.managers.schedule_policy",
    "file": "python/sglang/srt/managers/schedule_policy.py",
    "line": 80,
    "qualname": "SchedulePolicy.__init__",
    "signature": "def __init__(self, policy: str, tree_cache: BasePrefixCache, enable_hierarchical_cache: bool)"
  },
  {
    "module": "srt.managers.schedule_policy",
    "file": "python/sglang/srt/managers/schedule_policy.py",
    "line": 98,
    "qualname": "SchedulePolicy.calc_priority",
    "signature": "def calc_priority(self, waiting_queue: List[Req])"
  },
  {
    "module": "srt.managers.schedule_policy",
    "file": "python/sglang/srt/managers/schedule_policy.py",
    "line": 272,
    "qualname": "PrefillAdder.__init__",
    "signature": "def __init__(self, page_size: int, tree_cache: BasePrefixCache, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, running_batch: ScheduleBatch, new_token_ratio: float, rem_input_tokens: int, rem_chunk_tokens: Optional[int], mixed_with_decode_tokens: int)"
  },
  {
    "module": "srt.managers.schedule_policy",
    "file": "python/sglang/srt/managers/schedule_policy.py",
    "line": 320,
    "qualname": "PrefillAdder.rem_total_tokens",
    "signature": "def rem_total_tokens(self)"
  },
  {
    "module": "srt.managers.schedule_policy",
    "file": "python/sglang/srt/managers/schedule_policy.py",
    "line": 337,
    "qualname": "PrefillAdder.cur_rem_tokens",
    "signature": "def cur_rem_tokens(self)"
  },
  {
    "module": "srt.managers.schedule_policy",
    "file": "python/sglang/srt/managers/schedule_policy.py",
    "line": 353,
    "qualname": "PrefillAdder.ceil_paged_tokens",
    "signature": "def ceil_paged_tokens(self, tokens: int)"
  },
  {
    "module": "srt.managers.schedule_policy",
    "file": "python/sglang/srt/managers/schedule_policy.py",
    "line": 356,
    "qualname": "PrefillAdder.budget_state",
    "signature": "def budget_state(self)"
  },
  {
    "module": "srt.managers.schedule_policy",
    "file": "python/sglang/srt/managers/schedule_policy.py",
    "line": 382,
    "qualname": "PrefillAdder.add_chunked_req",
    "signature": "def add_chunked_req(self, req: Req)"
  },
  {
    "module": "srt.managers.schedule_policy",
    "file": "python/sglang/srt/managers/schedule_policy.py",
    "line": 415,
    "qualname": "PrefillAdder.add_one_req_ignore_eos",
    "signature": "def add_one_req_ignore_eos(self, req: Req, has_chunked_req: bool)"
  },
  {
    "module": "srt.managers.schedule_policy",
    "file": "python/sglang/srt/managers/schedule_policy.py",
    "line": 497,
    "qualname": "PrefillAdder.add_one_req",
    "signature": "def add_one_req(self, req: Req, has_chunked_req: bool)"
  },
  {
    "module": "srt.managers.scheduler_input_blocker",
    "file": "python/sglang/srt/managers/scheduler_input_blocker.py",
    "line": 26,
    "qualname": "SchedulerInputBlocker.__init__",
    "signature": "def __init__(self, noop: bool)"
  },
  {
    "module": "srt.managers.scheduler_input_blocker",
    "file": "python/sglang/srt/managers/scheduler_input_blocker.py",
    "line": 32,
    "qualname": "SchedulerInputBlocker.handle",
    "signature": "def handle(self, recv_reqs: Optional[List[Any]])"
  },
  {
    "module": "srt.managers.scheduler_input_blocker",
    "file": "python/sglang/srt/managers/scheduler_input_blocker.py",
    "line": 101,
    "qualname": "input_blocker_guard_region",
    "signature": "def input_blocker_guard_region(send_to_scheduler)"
  },
  {
    "module": "srt.managers.scheduler_metrics_mixin",
    "file": "python/sglang/srt/managers/scheduler_metrics_mixin.py",
    "line": 19,
    "qualname": "KvMetrics.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.managers.scheduler_metrics_mixin",
    "file": "python/sglang/srt/managers/scheduler_metrics_mixin.py",
    "line": 31,
    "qualname": "SchedulerMetricsMixin.init_metrics",
    "signature": "def init_metrics(self, tp_rank: int, pp_rank: int, dp_rank: Optional[int])"
  },
  {
    "module": "srt.managers.scheduler_metrics_mixin",
    "file": "python/sglang/srt/managers/scheduler_metrics_mixin.py",
    "line": 53,
    "qualname": "SchedulerMetricsMixin.init_kv_events",
    "signature": "def init_kv_events(self, kv_events_config: Optional[str])"
  },
  {
    "module": "srt.managers.scheduler_metrics_mixin",
    "file": "python/sglang/srt/managers/scheduler_metrics_mixin.py",
    "line": 59,
    "qualname": "SchedulerMetricsMixin.log_prefill_stats",
    "signature": "def log_prefill_stats(self, adder: PrefillAdder, can_run_list: List[Req], running_bs: int)"
  },
  {
    "module": "srt.managers.scheduler_metrics_mixin",
    "file": "python/sglang/srt/managers/scheduler_metrics_mixin.py",
    "line": 140,
    "qualname": "SchedulerMetricsMixin.log_decode_stats",
    "signature": "def log_decode_stats(self, can_run_cuda_graph: bool, running_batch: ScheduleBatch)"
  },
  {
    "module": "srt.managers.scheduler_profiler_mixin",
    "file": "python/sglang/srt/managers/scheduler_profiler_mixin.py",
    "line": 29,
    "qualname": "SchedulerProfilerMixin.init_profier",
    "signature": "def init_profier(self)"
  },
  {
    "module": "srt.managers.scheduler_profiler_mixin",
    "file": "python/sglang/srt/managers/scheduler_profiler_mixin.py",
    "line": 45,
    "qualname": "SchedulerProfilerMixin.init_profile",
    "signature": "def init_profile(self, output_dir: Optional[str], start_step: Optional[int], num_steps: Optional[int], activities: Optional[List[str]], with_stack: Optional[bool], record_shapes: Optional[bool], profile_by_stage: bool, profile_id: str)"
  },
  {
    "module": "srt.managers.scheduler_profiler_mixin",
    "file": "python/sglang/srt/managers/scheduler_profiler_mixin.py",
    "line": 97,
    "qualname": "SchedulerProfilerMixin.start_profile",
    "signature": "def start_profile(self, stage: Optional[ForwardMode])"
  },
  {
    "module": "srt.managers.scheduler_profiler_mixin",
    "file": "python/sglang/srt/managers/scheduler_profiler_mixin.py",
    "line": 172,
    "qualname": "SchedulerProfilerMixin.stop_profile",
    "signature": "def stop_profile(self, stage: Optional[ForwardMode])"
  },
  {
    "module": "srt.managers.scheduler_profiler_mixin",
    "file": "python/sglang/srt/managers/scheduler_profiler_mixin.py",
    "line": 273,
    "qualname": "SchedulerProfilerMixin.profile",
    "signature": "def profile(self, recv_req: ProfileReq)"
  },
  {
    "module": "srt.managers.scheduler_recv_skipper",
    "file": "python/sglang/srt/managers/scheduler_recv_skipper.py",
    "line": 7,
    "qualname": "SchedulerRecvSkipper.maybe_create",
    "signature": "def maybe_create(server_args: ServerArgs)"
  },
  {
    "module": "srt.managers.scheduler_recv_skipper",
    "file": "python/sglang/srt/managers/scheduler_recv_skipper.py",
    "line": 12,
    "qualname": "SchedulerRecvSkipper.__init__",
    "signature": "def __init__(self, server_args: ServerArgs)"
  },
  {
    "module": "srt.managers.scheduler_recv_skipper",
    "file": "python/sglang/srt/managers/scheduler_recv_skipper.py",
    "line": 18,
    "qualname": "SchedulerRecvSkipper.handle",
    "signature": "def handle(self, last_forward_mode: ForwardMode)"
  },
  {
    "module": "srt.managers.session_controller",
    "file": "python/sglang/srt/managers/session_controller.py",
    "line": 22,
    "qualname": "SessionReqNode.__init__",
    "signature": "def __init__(self, req, parent, childs)"
  },
  {
    "module": "srt.managers.session_controller",
    "file": "python/sglang/srt/managers/session_controller.py",
    "line": 29,
    "qualname": "SessionReqNode.clear_childs",
    "signature": "def clear_childs(self, req_dict)"
  },
  {
    "module": "srt.managers.session_controller",
    "file": "python/sglang/srt/managers/session_controller.py",
    "line": 34,
    "qualname": "SessionReqNode.clear",
    "signature": "def clear(self, req_dict)"
  },
  {
    "module": "srt.managers.session_controller",
    "file": "python/sglang/srt/managers/session_controller.py",
    "line": 42,
    "qualname": "SessionReqNode.abort",
    "signature": "def abort(self)"
  },
  {
    "module": "srt.managers.session_controller",
    "file": "python/sglang/srt/managers/session_controller.py",
    "line": 46,
    "qualname": "SessionReqNode.__str__",
    "signature": "def __str__(self)"
  },
  {
    "module": "srt.managers.session_controller",
    "file": "python/sglang/srt/managers/session_controller.py",
    "line": 63,
    "qualname": "Session.__init__",
    "signature": "def __init__(self, capacity_of_str_len: int, session_id: Optional[str])"
  },
  {
    "module": "srt.managers.session_controller",
    "file": "python/sglang/srt/managers/session_controller.py",
    "line": 68,
    "qualname": "Session.create_req",
    "signature": "def create_req(self, req: TokenizedGenerateReqInput, tokenizer)"
  },
  {
    "module": "srt.managers.template_manager",
    "file": "python/sglang/srt/managers/template_manager.py",
    "line": 54,
    "qualname": "TemplateManager.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.managers.template_manager",
    "file": "python/sglang/srt/managers/template_manager.py",
    "line": 61,
    "qualname": "TemplateManager.chat_template_name",
    "signature": "def chat_template_name(self)"
  },
  {
    "module": "srt.managers.template_manager",
    "file": "python/sglang/srt/managers/template_manager.py",
    "line": 66,
    "qualname": "TemplateManager.completion_template_name",
    "signature": "def completion_template_name(self)"
  },
  {
    "module": "srt.managers.template_manager",
    "file": "python/sglang/srt/managers/template_manager.py",
    "line": 71,
    "qualname": "TemplateManager.jinja_template_content_format",
    "signature": "def jinja_template_content_format(self)"
  },
  {
    "module": "srt.managers.template_manager",
    "file": "python/sglang/srt/managers/template_manager.py",
    "line": 76,
    "qualname": "TemplateManager.force_reasoning",
    "signature": "def force_reasoning(self)"
  },
  {
    "module": "srt.managers.template_manager",
    "file": "python/sglang/srt/managers/template_manager.py",
    "line": 101,
    "qualname": "TemplateManager.load_chat_template",
    "signature": "def load_chat_template(self, tokenizer_manager, chat_template_arg: Optional[str], model_path: str)"
  },
  {
    "module": "srt.managers.template_manager",
    "file": "python/sglang/srt/managers/template_manager.py",
    "line": 166,
    "qualname": "TemplateManager.guess_chat_template_from_model_path",
    "signature": "def guess_chat_template_from_model_path(self, model_path: str)"
  },
  {
    "module": "srt.managers.template_manager",
    "file": "python/sglang/srt/managers/template_manager.py",
    "line": 178,
    "qualname": "TemplateManager.load_completion_template",
    "signature": "def load_completion_template(self, completion_template_arg: str)"
  },
  {
    "module": "srt.managers.template_manager",
    "file": "python/sglang/srt/managers/template_manager.py",
    "line": 198,
    "qualname": "TemplateManager.initialize_templates",
    "signature": "def initialize_templates(self, tokenizer_manager, model_path: str, chat_template: Optional[str], completion_template: Optional[str])"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 54,
    "qualname": "TpModelWorker.__init__",
    "signature": "def __init__(self, server_args: ServerArgs, gpu_id: int, tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int], nccl_port: int, is_draft_worker: bool, req_to_token_pool: Optional[ReqToTokenPool], token_to_kv_pool_allocator: Optional[BaseTokenToKVPoolAllocator])"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 165,
    "qualname": "TpModelWorker.register_hicache_layer_transfer_counter",
    "signature": "def register_hicache_layer_transfer_counter(self, counter)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 168,
    "qualname": "TpModelWorker.set_hicache_consumer",
    "signature": "def set_hicache_consumer(self, consumer_index)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 172,
    "qualname": "TpModelWorker.get_worker_info",
    "signature": "def get_worker_info(self)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 189,
    "qualname": "TpModelWorker.sliding_window_size",
    "signature": "def sliding_window_size(self)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 193,
    "qualname": "TpModelWorker.is_hybrid",
    "signature": "def is_hybrid(self)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 196,
    "qualname": "TpModelWorker.get_tokens_per_layer_info",
    "signature": "def get_tokens_per_layer_info(self)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 202,
    "qualname": "TpModelWorker.get_pad_input_ids_func",
    "signature": "def get_pad_input_ids_func(self)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 205,
    "qualname": "TpModelWorker.get_tp_group",
    "signature": "def get_tp_group(self)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 208,
    "qualname": "TpModelWorker.get_attention_tp_group",
    "signature": "def get_attention_tp_group(self)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 211,
    "qualname": "TpModelWorker.get_attention_tp_cpu_group",
    "signature": "def get_attention_tp_cpu_group(self)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 214,
    "qualname": "TpModelWorker.get_memory_pool",
    "signature": "def get_memory_pool(self)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 220,
    "qualname": "TpModelWorker.forward_batch_generation",
    "signature": "def forward_batch_generation(self, model_worker_batch: ModelWorkerBatch, launch_done: Optional[threading.Event], skip_sample: bool)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 260,
    "qualname": "TpModelWorker.forward_batch_embedding",
    "signature": "def forward_batch_embedding(self, model_worker_batch: ModelWorkerBatch)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 266,
    "qualname": "TpModelWorker.update_weights_from_disk",
    "signature": "def update_weights_from_disk(self, recv_req: UpdateWeightFromDiskReqInput)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 272,
    "qualname": "TpModelWorker.init_weights_update_group",
    "signature": "def init_weights_update_group(self, recv_req: InitWeightsUpdateGroupReqInput)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 283,
    "qualname": "TpModelWorker.update_weights_from_distributed",
    "signature": "def update_weights_from_distributed(self, recv_req: UpdateWeightsFromDistributedReqInput)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 291,
    "qualname": "TpModelWorker.update_weights_from_tensor",
    "signature": "def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 302,
    "qualname": "TpModelWorker.get_weights_by_name",
    "signature": "def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 308,
    "qualname": "TpModelWorker.load_lora_adapter",
    "signature": "def load_lora_adapter(self, recv_req: LoadLoRAAdapterReqInput)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 312,
    "qualname": "TpModelWorker.unload_lora_adapter",
    "signature": "def unload_lora_adapter(self, recv_req: UnloadLoRAAdapterReqInput)"
  },
  {
    "module": "srt.managers.tp_worker",
    "file": "python/sglang/srt/managers/tp_worker.py",
    "line": 316,
    "qualname": "TpModelWorker.can_run_lora_batch",
    "signature": "def can_run_lora_batch(self, lora_ids: list[str])"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 45,
    "qualname": "resolve_future_token_ids",
    "signature": "def resolve_future_token_ids(input_ids, future_token_ids_map)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 56,
    "qualname": "TpModelWorkerClient.__init__",
    "signature": "def __init__(self, server_args: ServerArgs, gpu_id: int, tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int], nccl_port: int)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 96,
    "qualname": "TpModelWorkerClient.register_hicache_layer_transfer_counter",
    "signature": "def register_hicache_layer_transfer_counter(self, counter)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 99,
    "qualname": "TpModelWorkerClient.set_hicache_consumer",
    "signature": "def set_hicache_consumer(self, consumer_index)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 103,
    "qualname": "TpModelWorkerClient.get_worker_info",
    "signature": "def get_worker_info(self)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 106,
    "qualname": "TpModelWorkerClient.get_tokens_per_layer_info",
    "signature": "def get_tokens_per_layer_info(self)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 110,
    "qualname": "TpModelWorkerClient.sliding_window_size",
    "signature": "def sliding_window_size(self)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 114,
    "qualname": "TpModelWorkerClient.is_hybrid",
    "signature": "def is_hybrid(self)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 117,
    "qualname": "TpModelWorkerClient.get_pad_input_ids_func",
    "signature": "def get_pad_input_ids_func(self)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 120,
    "qualname": "TpModelWorkerClient.get_tp_group",
    "signature": "def get_tp_group(self)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 123,
    "qualname": "TpModelWorkerClient.get_attention_tp_group",
    "signature": "def get_attention_tp_group(self)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 126,
    "qualname": "TpModelWorkerClient.get_attention_tp_cpu_group",
    "signature": "def get_attention_tp_cpu_group(self)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 129,
    "qualname": "TpModelWorkerClient.get_memory_pool",
    "signature": "def get_memory_pool(self)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 135,
    "qualname": "TpModelWorkerClient.get_kv_cache",
    "signature": "def get_kv_cache(self)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 138,
    "qualname": "TpModelWorkerClient.forward_thread_func",
    "signature": "def forward_thread_func(self)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 148,
    "qualname": "TpModelWorkerClient.forward_thread_func_",
    "signature": "def forward_thread_func_(self)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 207,
    "qualname": "TpModelWorkerClient.resolve_last_batch_result",
    "signature": "def resolve_last_batch_result(self, launch_done: Optional[threading.Event])"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 231,
    "qualname": "TpModelWorkerClient.forward_batch_generation",
    "signature": "def forward_batch_generation(self, model_worker_batch: ModelWorkerBatch)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 264,
    "qualname": "TpModelWorkerClient.update_weights_from_disk",
    "signature": "def update_weights_from_disk(self, recv_req: UpdateWeightFromDiskReqInput)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 268,
    "qualname": "TpModelWorkerClient.init_weights_update_group",
    "signature": "def init_weights_update_group(self, recv_req: InitWeightsUpdateGroupReqInput)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 272,
    "qualname": "TpModelWorkerClient.update_weights_from_distributed",
    "signature": "def update_weights_from_distributed(self, recv_req: UpdateWeightsFromDistributedReqInput)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 278,
    "qualname": "TpModelWorkerClient.update_weights_from_tensor",
    "signature": "def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 282,
    "qualname": "TpModelWorkerClient.get_weights_by_name",
    "signature": "def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 285,
    "qualname": "TpModelWorkerClient.load_lora_adapter",
    "signature": "def load_lora_adapter(self, recv_req: LoadLoRAAdapterReqInput)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 288,
    "qualname": "TpModelWorkerClient.unload_lora_adapter",
    "signature": "def unload_lora_adapter(self, recv_req: UnloadLoRAAdapterReqInput)"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 291,
    "qualname": "TpModelWorkerClient.can_run_lora_batch",
    "signature": "def can_run_lora_batch(self, lora_ids: list[str])"
  },
  {
    "module": "srt.managers.tp_worker_overlap_thread",
    "file": "python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "line": 294,
    "qualname": "TpModelWorkerClient.__delete__",
    "signature": "def __delete__(self)"
  },
  {
    "module": "srt.managers.utils",
    "file": "python/sglang/srt/managers/utils.py",
    "line": 18,
    "qualname": "validate_input_length",
    "signature": "def validate_input_length(req: Req, max_req_input_len: int, allow_auto_truncate: bool)"
  },
  {
    "module": "srt.managers.utils",
    "file": "python/sglang/srt/managers/utils.py",
    "line": 51,
    "qualname": "get_logprob_dict_from_result",
    "signature": "def get_logprob_dict_from_result(result: GenerationBatchResult)"
  },
  {
    "module": "srt.managers.utils",
    "file": "python/sglang/srt/managers/utils.py",
    "line": 72,
    "qualname": "get_logprob_from_pp_outputs",
    "signature": "def get_logprob_from_pp_outputs(next_pp_outputs: PPProxyTensors)"
  },
  {
    "module": "srt.managers.utils",
    "file": "python/sglang/srt/managers/utils.py",
    "line": 107,
    "qualname": "DPBalanceMeta.__init__",
    "signature": "def __init__(self, num_workers: int)"
  },
  {
    "module": "srt.managers.utils",
    "file": "python/sglang/srt/managers/utils.py",
    "line": 119,
    "qualname": "DPBalanceMeta.destructor",
    "signature": "def destructor(self)"
  },
  {
    "module": "srt.managers.utils",
    "file": "python/sglang/srt/managers/utils.py",
    "line": 123,
    "qualname": "DPBalanceMeta.get_shared_onfly",
    "signature": "def get_shared_onfly(self)"
  },
  {
    "module": "srt.managers.utils",
    "file": "python/sglang/srt/managers/utils.py",
    "line": 126,
    "qualname": "DPBalanceMeta.set_shared_onfly_info",
    "signature": "def set_shared_onfly_info(self, data: List[Dict[int, int]])"
  },
  {
    "module": "srt.managers.utils",
    "file": "python/sglang/srt/managers/utils.py",
    "line": 129,
    "qualname": "DPBalanceMeta.get_shared_local_tokens",
    "signature": "def get_shared_local_tokens(self)"
  },
  {
    "module": "srt.managers.utils",
    "file": "python/sglang/srt/managers/utils.py",
    "line": 132,
    "qualname": "DPBalanceMeta.set_shared_local_tokens",
    "signature": "def set_shared_local_tokens(self, data: List[int])"
  },
  {
    "module": "srt.managers.utils",
    "file": "python/sglang/srt/managers/utils.py",
    "line": 135,
    "qualname": "DPBalanceMeta.__getstate__",
    "signature": "def __getstate__(self)"
  },
  {
    "module": "srt.managers.utils",
    "file": "python/sglang/srt/managers/utils.py",
    "line": 140,
    "qualname": "DPBalanceMeta.__setstate__",
    "signature": "def __setstate__(self, state)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 46,
    "qualname": "LayerDoneCounter.__init__",
    "signature": "def __init__(self, num_layers)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 55,
    "qualname": "LayerDoneCounter.next_producer",
    "signature": "def next_producer(self)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 58,
    "qualname": "LayerDoneCounter.update_producer",
    "signature": "def update_producer(self)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 62,
    "qualname": "LayerDoneCounter.set_consumer",
    "signature": "def set_consumer(self, index)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 65,
    "qualname": "LayerDoneCounter.increment",
    "signature": "def increment(self)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 70,
    "qualname": "LayerDoneCounter.wait_until",
    "signature": "def wait_until(self, threshold)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 75,
    "qualname": "LayerDoneCounter.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 84,
    "qualname": "CacheOperation.__init__",
    "signature": "def __init__(self, host_indices: torch.Tensor, device_indices: torch.Tensor, node_id: int, priority: Optional[int])"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 101,
    "qualname": "CacheOperation.merge",
    "signature": "def merge(self, other: 'CacheOperation')"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 108,
    "qualname": "CacheOperation.split",
    "signature": "def split(self, factor)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 129,
    "qualname": "CacheOperation.__lt__",
    "signature": "def __lt__(self, other: 'CacheOperation')"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 138,
    "qualname": "TransferBuffer.__init__",
    "signature": "def __init__(self, stop_event, buffer_count: int, max_buffer_size: int)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 146,
    "qualname": "TransferBuffer.full",
    "signature": "def full(self)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 149,
    "qualname": "TransferBuffer.empty",
    "signature": "def empty(self)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 152,
    "qualname": "TransferBuffer.put",
    "signature": "def put(self, item, block, timeout)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 164,
    "qualname": "TransferBuffer.get",
    "signature": "def get(self, block, timeout)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 172,
    "qualname": "TransferBuffer.clear",
    "signature": "def clear(self)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 179,
    "qualname": "StorageOperation.__init__",
    "signature": "def __init__(self, host_indices: torch.Tensor, token_ids: List[int], last_hash: Optional[str], hash_value: Optional[List[str]])"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 195,
    "qualname": "StorageOperation.__lt__",
    "signature": "def __lt__(self, other: 'StorageOperation')"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 200,
    "qualname": "PrefetchOperation.__init__",
    "signature": "def __init__(self, request_id: str, host_indices: torch.Tensor, token_ids: List[int], last_hash: Optional[str])"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 216,
    "qualname": "PrefetchOperation.increment",
    "signature": "def increment(self, num_tokens: int)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 223,
    "qualname": "PrefetchOperation.mark_done",
    "signature": "def mark_done(self)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 227,
    "qualname": "PrefetchOperation.is_done",
    "signature": "def is_done(self)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 233,
    "qualname": "HiCacheController.__init__",
    "signature": "def __init__(self, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, mem_pool_host: HostKVCache, page_size: int, tp_group: torch.distributed.ProcessGroup, load_cache_event: threading.Event, write_policy: str, io_backend: str, storage_backend: Optional[str], prefetch_threshold: int, model_name: Optional[str], storage_backend_extra_config: Optional[str])"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 421,
    "qualname": "HiCacheController.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 460,
    "qualname": "HiCacheController.write",
    "signature": "def write(self, device_indices: torch.Tensor, priority: Optional[int], node_id: int)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 479,
    "qualname": "HiCacheController.load",
    "signature": "def load(self, host_indices: torch.Tensor, priority: Optional[int], node_id: int)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 499,
    "qualname": "HiCacheController.move_indices",
    "signature": "def move_indices(self, host_indices, device_indices)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 510,
    "qualname": "HiCacheController.write_thread_func_direct",
    "signature": "def write_thread_func_direct(self)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 534,
    "qualname": "HiCacheController.load_thread_func_layer_by_layer",
    "signature": "def load_thread_func_layer_by_layer(self)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 577,
    "qualname": "HiCacheController.evict_device",
    "signature": "def evict_device(self, device_indices: torch.Tensor, host_indices: torch.Tensor)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 589,
    "qualname": "HiCacheController.evict_host",
    "signature": "def evict_host(self, host_indices: torch.Tensor, backup_only: bool)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 601,
    "qualname": "HiCacheController.prefetch",
    "signature": "def prefetch(self, request_id: str, host_indices: torch.Tensor, new_input_tokens: List[int], last_hash: Optional[str])"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 617,
    "qualname": "HiCacheController.terminate_prefetch",
    "signature": "def terminate_prefetch(self, operation)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 709,
    "qualname": "HiCacheController.is_mooncake_backend",
    "signature": "def is_mooncake_backend(self)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 712,
    "qualname": "HiCacheController.prefetch_io_aux_func",
    "signature": "def prefetch_io_aux_func(self)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 731,
    "qualname": "HiCacheController.prefetch_rate_limit_check",
    "signature": "def prefetch_rate_limit_check(self)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 762,
    "qualname": "HiCacheController.prefetch_thread_func",
    "signature": "def prefetch_thread_func(self)"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 816,
    "qualname": "HiCacheController.write_storage",
    "signature": "def write_storage(self, host_indices: torch.Tensor, token_ids: List[int], hash_value: Optional[List[str]])"
  },
  {
    "module": "srt.managers.cache_controller",
    "file": "python/sglang/srt/managers/cache_controller.py",
    "line": 889,
    "qualname": "HiCacheController.backup_thread_func",
    "signature": "def backup_thread_func(self)"
  },
  {
    "module": "srt.managers.io_struct",
    "file": "python/sglang/srt/managers/io_struct.py",
    "line": 131,
    "qualname": "GenerateReqInput.contains_mm_input",
    "signature": "def contains_mm_input(self)"
  },
  {
    "module": "srt.managers.io_struct",
    "file": "python/sglang/srt/managers/io_struct.py",
    "line": 138,
    "qualname": "GenerateReqInput.normalize_batch_and_arguments",
    "signature": "def normalize_batch_and_arguments(self)"
  },
  {
    "module": "srt.managers.io_struct",
    "file": "python/sglang/srt/managers/io_struct.py",
    "line": 432,
    "qualname": "GenerateReqInput.regenerate_rid",
    "signature": "def regenerate_rid(self)"
  },
  {
    "module": "srt.managers.io_struct",
    "file": "python/sglang/srt/managers/io_struct.py",
    "line": 437,
    "qualname": "GenerateReqInput.__getitem__",
    "signature": "def __getitem__(self, i)"
  },
  {
    "module": "srt.managers.io_struct",
    "file": "python/sglang/srt/managers/io_struct.py",
    "line": 541,
    "qualname": "BatchTokenizedGenerateReqInput.__len__",
    "signature": "def __len__(self)"
  },
  {
    "module": "srt.managers.io_struct",
    "file": "python/sglang/srt/managers/io_struct.py",
    "line": 544,
    "qualname": "BatchTokenizedGenerateReqInput.__getitem__",
    "signature": "def __getitem__(self, i)"
  },
  {
    "module": "srt.managers.io_struct",
    "file": "python/sglang/srt/managers/io_struct.py",
    "line": 547,
    "qualname": "BatchTokenizedGenerateReqInput.__iter__",
    "signature": "def __iter__(self)"
  },
  {
    "module": "srt.managers.io_struct",
    "file": "python/sglang/srt/managers/io_struct.py",
    "line": 584,
    "qualname": "EmbeddingReqInput.normalize_batch_and_arguments",
    "signature": "def normalize_batch_and_arguments(self)"
  },
  {
    "module": "srt.managers.io_struct",
    "file": "python/sglang/srt/managers/io_struct.py",
    "line": 635,
    "qualname": "EmbeddingReqInput.regenerate_rid",
    "signature": "def regenerate_rid(self)"
  },
  {
    "module": "srt.managers.io_struct",
    "file": "python/sglang/srt/managers/io_struct.py",
    "line": 639,
    "qualname": "EmbeddingReqInput.contains_mm_input",
    "signature": "def contains_mm_input(self)"
  },
  {
    "module": "srt.managers.io_struct",
    "file": "python/sglang/srt/managers/io_struct.py",
    "line": 646,
    "qualname": "EmbeddingReqInput.__getitem__",
    "signature": "def __getitem__(self, i)"
  },
  {
    "module": "srt.managers.io_struct",
    "file": "python/sglang/srt/managers/io_struct.py",
    "line": 691,
    "qualname": "BatchTokenizedEmbeddingReqInput.__len__",
    "signature": "def __len__(self)"
  },
  {
    "module": "srt.managers.io_struct",
    "file": "python/sglang/srt/managers/io_struct.py",
    "line": 694,
    "qualname": "BatchTokenizedEmbeddingReqInput.__getitem__",
    "signature": "def __getitem__(self, i)"
  },
  {
    "module": "srt.managers.io_struct",
    "file": "python/sglang/srt/managers/io_struct.py",
    "line": 697,
    "qualname": "BatchTokenizedEmbeddingReqInput.__iter__",
    "signature": "def __iter__(self)"
  },
  {
    "module": "srt.managers.io_struct",
    "file": "python/sglang/srt/managers/io_struct.py",
    "line": 1143,
    "qualname": "LoadLoRAAdapterReqInput.to_ref",
    "signature": "def to_ref(self)"
  },
  {
    "module": "srt.managers.io_struct",
    "file": "python/sglang/srt/managers/io_struct.py",
    "line": 1159,
    "qualname": "UnloadLoRAAdapterReqInput.to_ref",
    "signature": "def to_ref(self)"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 43,
    "qualname": "TransportProxyTensor.__new__",
    "signature": "def __new__(cls, data: torch.Tensor, name: Optional[str], fields: Optional[Dict[str, Any]], transport_mode: TensorTransportMode)"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 68,
    "qualname": "TransportProxyTensor.__getstate__",
    "signature": "def __getstate__(self)"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 104,
    "qualname": "TransportProxyTensor.__setstate__",
    "signature": "def __setstate__(self, state: Dict[str, Any])"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 142,
    "qualname": "TransportProxyTensor.name",
    "signature": "def name(self)"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 146,
    "qualname": "TransportProxyTensor.fields",
    "signature": "def fields(self)"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 150,
    "qualname": "TransportProxyTensor.transport_mode",
    "signature": "def transport_mode(self)"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 162,
    "qualname": "MultiModalityDataPaddingPattern.pad_input_tokens",
    "signature": "def pad_input_tokens(self, input_ids: List[int], mm_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 179,
    "qualname": "MultiModalityDataPaddingPatternTokenPairs.__init__",
    "signature": "def __init__(self, data_token_pairs: Optional[List[Tuple[int, int]]], data_start_token_ids: Optional[List[int]])"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 195,
    "qualname": "MultiModalityDataPaddingPatternTokenPairs.pad_input_tokens",
    "signature": "def pad_input_tokens(self, input_ids: List[int], mm_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 251,
    "qualname": "MultiModalityDataPaddingPatternMultimodalTokens.pad_input_tokens",
    "signature": "def pad_input_tokens(self, input_ids: List[int], mm_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 288,
    "qualname": "init_embedding_cache",
    "signature": "def init_embedding_cache(max_size: int)"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 293,
    "qualname": "get_embedding_hash",
    "signature": "def get_embedding_hash(embedding_items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 298,
    "qualname": "get_embedding_chunk",
    "signature": "def get_embedding_chunk(embedding: torch.Tensor, extend_prefix_len: int, extend_seq_len: int, items_offset: List[Tuple[int, int]])"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 447,
    "qualname": "get_embedding_and_mask",
    "signature": "def get_embedding_and_mask(data_embedding_func: Callable[[List[MultimodalDataItem]], torch.Tensor], embedding_items: List[MultimodalDataItem], placeholder_tensor: torch.Tensor, input_ids: torch.Tensor, items_size: List[int], prefix_length: List[int], extend_length: List[int], items_offset_list: List[List[Tuple[int, int]]])"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 495,
    "qualname": "embed_mm_inputs",
    "signature": "def embed_mm_inputs(mm_inputs_list: List[MultimodalInputs], extend_prefix_lens: List[int], extend_seq_lens: List[int], input_ids: torch.Tensor, input_embedding: nn.Embedding, multimodal_model: nn.Module, data_embedding_func_mapping: Dict[Modality, Callable[[List[MultimodalDataItem]], torch.Tensor]], placeholder_tokens: dict[Modality, List[int]])"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 599,
    "qualname": "general_mm_embed_routine",
    "signature": "def general_mm_embed_routine(input_ids: torch.Tensor, forward_batch: ForwardBatch, language_model: nn.Module, multimodal_model: Optional[nn.Module], data_embedding_funcs: Dict[Modality, Callable[[List[MultimodalDataItem]], torch.Tensor]], placeholder_tokens: Optional[dict[Modality, List[int]]])"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 668,
    "qualname": "get_multimodal_data_bounds",
    "signature": "def get_multimodal_data_bounds(input_ids: torch.Tensor, pad_values: List[int], token_pairs: List[Tuple[int, int]])"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 728,
    "qualname": "data_hash",
    "signature": "def data_hash(data)"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 733,
    "qualname": "tensor_hash",
    "signature": "def tensor_hash(tensor_list)"
  },
  {
    "module": "srt.managers.mm_utils",
    "file": "python/sglang/srt/managers/mm_utils.py",
    "line": 759,
    "qualname": "hash_feature",
    "signature": "def hash_feature(f)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 203,
    "qualname": "Scheduler.__init__",
    "signature": "def __init__(self, server_args: ServerArgs, port_args: PortArgs, gpu_id: int, tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int], dp_balance_meta: Optional[DPBalanceMeta])"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 552,
    "qualname": "Scheduler.init_tokenizer",
    "signature": "def init_tokenizer(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 576,
    "qualname": "Scheduler.init_memory_pool_and_cache",
    "signature": "def init_memory_pool_and_cache(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 684,
    "qualname": "Scheduler.init_disaggregation",
    "signature": "def init_disaggregation(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 777,
    "qualname": "Scheduler.init_moe_config",
    "signature": "def init_moe_config(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 782,
    "qualname": "Scheduler.event_loop_normal",
    "signature": "def event_loop_normal(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 801,
    "qualname": "Scheduler.event_loop_overlap",
    "signature": "def event_loop_overlap(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 844,
    "qualname": "Scheduler.event_loop_pp",
    "signature": "def event_loop_pp(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 976,
    "qualname": "Scheduler.recv_requests",
    "signature": "def recv_requests(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 1077,
    "qualname": "Scheduler.process_input_requests",
    "signature": "def process_input_requests(self, recv_reqs: List)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 1109,
    "qualname": "Scheduler.handle_generate_request",
    "signature": "def handle_generate_request(self, recv_req: TokenizedGenerateReqInput)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 1274,
    "qualname": "Scheduler.handle_batch_generate_request",
    "signature": "def handle_batch_generate_request(self, recv_req: BatchTokenizedGenerateReqInput)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 1322,
    "qualname": "Scheduler.handle_embedding_request",
    "signature": "def handle_embedding_request(self, recv_req: TokenizedEmbeddingReqInput)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 1368,
    "qualname": "Scheduler.handle_batch_embedding_request",
    "signature": "def handle_batch_embedding_request(self, recv_req: BatchTokenizedEmbeddingReqInput)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 1381,
    "qualname": "Scheduler.self_check_during_idle",
    "signature": "def self_check_during_idle(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 1387,
    "qualname": "Scheduler.check_memory",
    "signature": "def check_memory(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 1464,
    "qualname": "Scheduler.check_tree_cache",
    "signature": "def check_tree_cache(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 1499,
    "qualname": "Scheduler.get_next_batch_to_run",
    "signature": "def get_next_batch_to_run(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 1564,
    "qualname": "Scheduler.get_num_allocatable_reqs",
    "signature": "def get_num_allocatable_reqs(self, running_bs)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 1570,
    "qualname": "Scheduler.get_new_batch_prefill",
    "signature": "def get_new_batch_prefill(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 1722,
    "qualname": "Scheduler.update_running_batch",
    "signature": "def update_running_batch(self, batch: ScheduleBatch)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 1762,
    "qualname": "Scheduler.run_batch",
    "signature": "def run_batch(self, batch: ScheduleBatch)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 1843,
    "qualname": "Scheduler.process_batch_result",
    "signature": "def process_batch_result(self, batch: ScheduleBatch, result: Union[GenerationBatchResult, EmbeddingBatchResult], launch_done: Optional[threading.Event])"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 1862,
    "qualname": "Scheduler.maybe_send_health_check_signal",
    "signature": "def maybe_send_health_check_signal(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 1870,
    "qualname": "Scheduler.prepare_mlp_sync_batch",
    "signature": "def prepare_mlp_sync_batch(self, local_batch: ScheduleBatch)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 1884,
    "qualname": "Scheduler.handle_dp_balance_data",
    "signature": "def handle_dp_balance_data(self, local_batch: ScheduleBatch)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 1965,
    "qualname": "Scheduler.prepare_mlp_sync_batch_raw",
    "signature": "def prepare_mlp_sync_batch_raw(local_batch: ScheduleBatch, dp_size, attn_tp_size: int, tp_group, get_idle_batch, disable_cuda_graph: bool, spec_algorithm, speculative_num_draft_tokens, require_mlp_tp_gather: bool, disable_overlap_schedule: bool)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2068,
    "qualname": "Scheduler.get_idle_batch",
    "signature": "def get_idle_batch(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2081,
    "qualname": "Scheduler.move_ready_grammar_requests",
    "signature": "def move_ready_grammar_requests(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2146,
    "qualname": "Scheduler.set_next_batch_sampling_info_done",
    "signature": "def set_next_batch_sampling_info_done(self, batch: ScheduleBatch)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2153,
    "qualname": "Scheduler.watchdog_thread",
    "signature": "def watchdog_thread(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2206,
    "qualname": "Scheduler.flush_cache_wrapped",
    "signature": "def flush_cache_wrapped(self, recv_req: FlushCacheReqInput)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2210,
    "qualname": "Scheduler.flush_cache",
    "signature": "def flush_cache(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2247,
    "qualname": "Scheduler.get_load",
    "signature": "def get_load(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2281,
    "qualname": "Scheduler.get_internal_state",
    "signature": "def get_internal_state(self, recv_req: GetInternalStateReq)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2310,
    "qualname": "Scheduler.set_internal_state",
    "signature": "def set_internal_state(self, recv_req: SetInternalStateReq)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2348,
    "qualname": "Scheduler.handle_rpc_request",
    "signature": "def handle_rpc_request(self, recv_req: RpcReqInput)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2367,
    "qualname": "Scheduler.abort_request",
    "signature": "def abort_request(self, recv_req: AbortReq)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2444,
    "qualname": "Scheduler.load_lora_adapter",
    "signature": "def load_lora_adapter(self, recv_req: LoadLoRAAdapterReqInput)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2452,
    "qualname": "Scheduler.unload_lora_adapter",
    "signature": "def unload_lora_adapter(self, recv_req: UnloadLoRAAdapterReqInput)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2460,
    "qualname": "Scheduler.slow_down",
    "signature": "def slow_down(self, recv_req: SlowDownReqInput)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2467,
    "qualname": "Scheduler.expert_distribution_handle",
    "signature": "def expert_distribution_handle(self, recv_req: ExpertDistributionReq)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2478,
    "qualname": "Scheduler.open_session",
    "signature": "def open_session(self, recv_req: OpenSessionReqInput)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2493,
    "qualname": "Scheduler.close_session",
    "signature": "def close_session(self, recv_req: CloseSessionReqInput)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2501,
    "qualname": "Scheduler.get_print_prefix",
    "signature": "def get_print_prefix(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2511,
    "qualname": "Scheduler.current_scheduler_metrics_enabled",
    "signature": "def current_scheduler_metrics_enabled(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2514,
    "qualname": "Scheduler.maybe_sleep_on_idle",
    "signature": "def maybe_sleep_on_idle(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2518,
    "qualname": "Scheduler.handle_freeze_gc",
    "signature": "def handle_freeze_gc(self, recv_req: FreezeGCReq)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2537,
    "qualname": "IdleSleeper.__init__",
    "signature": "def __init__(self, sockets)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2543,
    "qualname": "IdleSleeper.maybe_sleep",
    "signature": "def maybe_sleep(self)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2554,
    "qualname": "is_health_check_generate_req",
    "signature": "def is_health_check_generate_req(recv_req)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2558,
    "qualname": "is_work_request",
    "signature": "def is_work_request(recv_req)"
  },
  {
    "module": "srt.managers.scheduler",
    "file": "python/sglang/srt/managers/scheduler.py",
    "line": 2570,
    "qualname": "run_scheduler_process",
    "signature": "def run_scheduler_process(server_args: ServerArgs, port_args: PortArgs, gpu_id: int, tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int], pipe_writer, balance_meta: Optional[DPBalanceMeta])"
  },
  {
    "module": "srt.managers.scheduler_output_processor_mixin",
    "file": "python/sglang/srt/managers/scheduler_output_processor_mixin.py",
    "line": 32,
    "qualname": "SchedulerOutputProcessorMixin.process_batch_result_prefill",
    "signature": "def process_batch_result_prefill(self: Scheduler, batch: ScheduleBatch, result: Union[GenerationBatchResult, EmbeddingBatchResult], launch_done: Optional[threading.Event])"
  },
  {
    "module": "srt.managers.scheduler_output_processor_mixin",
    "file": "python/sglang/srt/managers/scheduler_output_processor_mixin.py",
    "line": 194,
    "qualname": "SchedulerOutputProcessorMixin.process_batch_result_decode",
    "signature": "def process_batch_result_decode(self: Scheduler, batch: ScheduleBatch, result: GenerationBatchResult, launch_done: Optional[threading.Event])"
  },
  {
    "module": "srt.managers.scheduler_output_processor_mixin",
    "file": "python/sglang/srt/managers/scheduler_output_processor_mixin.py",
    "line": 298,
    "qualname": "SchedulerOutputProcessorMixin.add_input_logprob_return_values",
    "signature": "def add_input_logprob_return_values(self: Scheduler, i: int, req: Req, output: LogitsProcessorOutput, logprob_pt: int, num_input_logprobs: int, last_prefill_chunk: bool)"
  },
  {
    "module": "srt.managers.scheduler_output_processor_mixin",
    "file": "python/sglang/srt/managers/scheduler_output_processor_mixin.py",
    "line": 432,
    "qualname": "SchedulerOutputProcessorMixin.add_logprob_return_values",
    "signature": "def add_logprob_return_values(self: Scheduler, i: int, req: Req, pt: int, next_token_ids: List[int], num_input_logprobs: int, output: LogitsProcessorOutput)"
  },
  {
    "module": "srt.managers.scheduler_output_processor_mixin",
    "file": "python/sglang/srt/managers/scheduler_output_processor_mixin.py",
    "line": 463,
    "qualname": "SchedulerOutputProcessorMixin.stream_output",
    "signature": "def stream_output(self: Scheduler, reqs: List[Req], return_logprob: bool, skip_req: Optional[Req])"
  },
  {
    "module": "srt.managers.scheduler_output_processor_mixin",
    "file": "python/sglang/srt/managers/scheduler_output_processor_mixin.py",
    "line": 475,
    "qualname": "SchedulerOutputProcessorMixin.stream_output_generation",
    "signature": "def stream_output_generation(self: Scheduler, reqs: List[Req], return_logprob: bool, skip_req: Optional[Req])"
  },
  {
    "module": "srt.managers.scheduler_output_processor_mixin",
    "file": "python/sglang/srt/managers/scheduler_output_processor_mixin.py",
    "line": 704,
    "qualname": "SchedulerOutputProcessorMixin.stream_output_embedding",
    "signature": "def stream_output_embedding(self: Scheduler, reqs: List[Req])"
  },
  {
    "module": "srt.managers.scheduler_update_weights_mixin",
    "file": "python/sglang/srt/managers/scheduler_update_weights_mixin.py",
    "line": 29,
    "qualname": "SchedulerUpdateWeightsMixin.update_weights_from_disk",
    "signature": "def update_weights_from_disk(self, recv_req: UpdateWeightFromDiskReqInput)"
  },
  {
    "module": "srt.managers.scheduler_update_weights_mixin",
    "file": "python/sglang/srt/managers/scheduler_update_weights_mixin.py",
    "line": 39,
    "qualname": "SchedulerUpdateWeightsMixin.init_weights_update_group",
    "signature": "def init_weights_update_group(self, recv_req: InitWeightsUpdateGroupReqInput)"
  },
  {
    "module": "srt.managers.scheduler_update_weights_mixin",
    "file": "python/sglang/srt/managers/scheduler_update_weights_mixin.py",
    "line": 44,
    "qualname": "SchedulerUpdateWeightsMixin.update_weights_from_distributed",
    "signature": "def update_weights_from_distributed(self, recv_req: UpdateWeightsFromDistributedReqInput)"
  },
  {
    "module": "srt.managers.scheduler_update_weights_mixin",
    "file": "python/sglang/srt/managers/scheduler_update_weights_mixin.py",
    "line": 58,
    "qualname": "SchedulerUpdateWeightsMixin.update_weights_from_tensor",
    "signature": "def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput)"
  },
  {
    "module": "srt.managers.scheduler_update_weights_mixin",
    "file": "python/sglang/srt/managers/scheduler_update_weights_mixin.py",
    "line": 71,
    "qualname": "SchedulerUpdateWeightsMixin.get_weights_by_name",
    "signature": "def get_weights_by_name(self, recv_req: GetWeightsByNameReqInput)"
  },
  {
    "module": "srt.managers.scheduler_update_weights_mixin",
    "file": "python/sglang/srt/managers/scheduler_update_weights_mixin.py",
    "line": 75,
    "qualname": "SchedulerUpdateWeightsMixin.release_memory_occupation",
    "signature": "def release_memory_occupation(self, recv_req: ReleaseMemoryOccupationReqInput)"
  },
  {
    "module": "srt.managers.scheduler_update_weights_mixin",
    "file": "python/sglang/srt/managers/scheduler_update_weights_mixin.py",
    "line": 97,
    "qualname": "SchedulerUpdateWeightsMixin.resume_memory_occupation",
    "signature": "def resume_memory_occupation(self, recv_req: ResumeMemoryOccupationReqInput)"
  },
  {
    "module": "srt.managers.scheduler_update_weights_mixin",
    "file": "python/sglang/srt/managers/scheduler_update_weights_mixin.py",
    "line": 120,
    "qualname": "SchedulerUpdateWeightsMixin.save_remote_model",
    "signature": "def save_remote_model(self, params)"
  },
  {
    "module": "srt.managers.scheduler_update_weights_mixin",
    "file": "python/sglang/srt/managers/scheduler_update_weights_mixin.py",
    "line": 127,
    "qualname": "SchedulerUpdateWeightsMixin.save_sharded_model",
    "signature": "def save_sharded_model(self, params)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 182,
    "qualname": "TokenizerManager.__init__",
    "signature": "def __init__(self, server_args: ServerArgs, port_args: PortArgs)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 482,
    "qualname": "TokenizerManager.generate_request",
    "signature": "async def generate_request(self, obj: Union[GenerateReqInput, EmbeddingReqInput], request: Optional[fastapi.Request])"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 988,
    "qualname": "TokenizerManager.flush_cache",
    "signature": "async def flush_cache(self)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 991,
    "qualname": "TokenizerManager.abort_request",
    "signature": "def abort_request(self, rid: str, abort_all: bool)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1000,
    "qualname": "TokenizerManager.start_profile",
    "signature": "async def start_profile(self, output_dir: Optional[str], start_step: Optional[int], num_steps: Optional[int], activities: Optional[List[str]], with_stack: Optional[bool], record_shapes: Optional[bool], profile_by_stage: bool)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1026,
    "qualname": "TokenizerManager.stop_profile",
    "signature": "async def stop_profile(self)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1037,
    "qualname": "TokenizerManager.start_expert_distribution_record",
    "signature": "async def start_expert_distribution_record(self)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1041,
    "qualname": "TokenizerManager.stop_expert_distribution_record",
    "signature": "async def stop_expert_distribution_record(self)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1045,
    "qualname": "TokenizerManager.dump_expert_distribution_record",
    "signature": "async def dump_expert_distribution_record(self)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1049,
    "qualname": "TokenizerManager.pause_generation",
    "signature": "async def pause_generation(self)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1054,
    "qualname": "TokenizerManager.continue_generation",
    "signature": "async def continue_generation(self)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1059,
    "qualname": "TokenizerManager.update_weights_from_disk",
    "signature": "async def update_weights_from_disk(self, obj: UpdateWeightFromDiskReqInput, request: Optional[fastapi.Request])"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1107,
    "qualname": "TokenizerManager.init_weights_update_group",
    "signature": "async def init_weights_update_group(self, obj: InitWeightsUpdateGroupReqInput, request: Optional[fastapi.Request])"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1119,
    "qualname": "TokenizerManager.update_weights_from_distributed",
    "signature": "async def update_weights_from_distributed(self, obj: UpdateWeightsFromDistributedReqInput, request: Optional[fastapi.Request])"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1138,
    "qualname": "TokenizerManager.update_weights_from_tensor",
    "signature": "async def update_weights_from_tensor(self, obj: UpdateWeightsFromTensorReqInput, request: Optional[fastapi.Request])"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1157,
    "qualname": "TokenizerManager.load_lora_adapter",
    "signature": "async def load_lora_adapter(self, obj: LoadLoRAAdapterReqInput, _: Optional[fastapi.Request])"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1215,
    "qualname": "TokenizerManager.unload_lora_adapter",
    "signature": "async def unload_lora_adapter(self, obj: UnloadLoRAAdapterReqInput, _: Optional[fastapi.Request])"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1257,
    "qualname": "TokenizerManager.get_weights_by_name",
    "signature": "async def get_weights_by_name(self, obj: GetWeightsByNameReqInput, request: Optional[fastapi.Request])"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1268,
    "qualname": "TokenizerManager.release_memory_occupation",
    "signature": "async def release_memory_occupation(self, obj: ReleaseMemoryOccupationReqInput, request: Optional[fastapi.Request])"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1276,
    "qualname": "TokenizerManager.resume_memory_occupation",
    "signature": "async def resume_memory_occupation(self, obj: ResumeMemoryOccupationReqInput, request: Optional[fastapi.Request])"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1284,
    "qualname": "TokenizerManager.slow_down",
    "signature": "async def slow_down(self, obj: SlowDownReqInput, request: Optional[fastapi.Request])"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1292,
    "qualname": "TokenizerManager.open_session",
    "signature": "async def open_session(self, obj: OpenSessionReqInput, request: Optional[fastapi.Request])"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1309,
    "qualname": "TokenizerManager.close_session",
    "signature": "async def close_session(self, obj: CloseSessionReqInput, request: Optional[fastapi.Request])"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1314,
    "qualname": "TokenizerManager.get_internal_state",
    "signature": "async def get_internal_state(self)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1322,
    "qualname": "TokenizerManager.set_internal_state",
    "signature": "async def set_internal_state(self, obj: SetInternalStateReq)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1330,
    "qualname": "TokenizerManager.get_load",
    "signature": "async def get_load(self)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1338,
    "qualname": "TokenizerManager.get_log_request_metadata",
    "signature": "def get_log_request_metadata(self)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1392,
    "qualname": "TokenizerManager.configure_logging",
    "signature": "def configure_logging(self, obj: ConfigureLoggingReq)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1406,
    "qualname": "TokenizerManager.freeze_gc",
    "signature": "async def freeze_gc(self)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1412,
    "qualname": "TokenizerManager.create_abort_task",
    "signature": "def create_abort_task(self, obj: GenerateReqInput)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1426,
    "qualname": "TokenizerManager.auto_create_handle_loop",
    "signature": "def auto_create_handle_loop(self)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1457,
    "qualname": "TokenizerManager.dump_requests_before_crash",
    "signature": "def dump_requests_before_crash(self)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1540,
    "qualname": "TokenizerManager.sigterm_watchdog",
    "signature": "async def sigterm_watchdog(self)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1577,
    "qualname": "TokenizerManager.handle_loop",
    "signature": "async def handle_loop(self)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1690,
    "qualname": "TokenizerManager.convert_logprob_style",
    "signature": "def convert_logprob_style(self, meta_info: dict, state: ReqState, top_logprobs_num: int, token_ids_logprob: List[int], return_text_in_logprobs: bool, recv_obj: BatchStrOut, recv_obj_index: int)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1779,
    "qualname": "TokenizerManager.detokenize_logprob_tokens",
    "signature": "def detokenize_logprob_tokens(self, token_logprobs_val: List[float], token_logprobs_idx: List[int], decode_to_text: bool)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1795,
    "qualname": "TokenizerManager.detokenize_top_logprobs_tokens",
    "signature": "def detokenize_top_logprobs_tokens(self, token_logprobs_val: List[float], token_logprobs_idx: List[int], decode_to_text: bool)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1815,
    "qualname": "TokenizerManager.collect_metrics",
    "signature": "def collect_metrics(self, state: ReqState, recv_obj: BatchStrOut, i: int)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1858,
    "qualname": "TokenizerManager.dump_requests",
    "signature": "def dump_requests(self, state: ReqState, out_dict: dict)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1875,
    "qualname": "TokenizerManager.record_request_for_crash_dump",
    "signature": "def record_request_for_crash_dump(self, state: ReqState, out_dict: dict)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 1945,
    "qualname": "TokenizerManager.score_request",
    "signature": "async def score_request(self, query: Optional[Union[str, List[int]]], items: Optional[Union[str, List[str], List[List[int]]]], label_token_ids: Optional[List[int]], apply_softmax: bool, item_first: bool, request: Optional[Any])"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 2065,
    "qualname": "print_exception_wrapper",
    "signature": "async def print_exception_wrapper(func)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 2082,
    "qualname": "SignalHandler.__init__",
    "signature": "def __init__(self, tokenizer_manager: TokenizerManager)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 2085,
    "qualname": "SignalHandler.sigterm_handler",
    "signature": "def sigterm_handler(self, signum, frame)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 2091,
    "qualname": "SignalHandler.running_phase_sigquit_handler",
    "signature": "def running_phase_sigquit_handler(self, signum, frame)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 2105,
    "qualname": "_Communicator.__init__",
    "signature": "def __init__(self, sender, fan_out: int)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 2112,
    "qualname": "_Communicator.__call__",
    "signature": "async def __call__(self, obj)"
  },
  {
    "module": "srt.managers.tokenizer_manager",
    "file": "python/sglang/srt/managers/tokenizer_manager.py",
    "line": 2134,
    "qualname": "_Communicator.handle_recv",
    "signature": "def handle_recv(self, recv_obj: T)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 38,
    "qualname": "BaseTokenToKVPoolAllocator.__init__",
    "signature": "def __init__(self, size: int, page_size: int, dtype: torch.dtype, device: str, kvcache: KVCache, need_sort: bool)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 59,
    "qualname": "BaseTokenToKVPoolAllocator.debug_print",
    "signature": "def debug_print(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 62,
    "qualname": "BaseTokenToKVPoolAllocator.available_size",
    "signature": "def available_size(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 65,
    "qualname": "BaseTokenToKVPoolAllocator.get_kvcache",
    "signature": "def get_kvcache(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 68,
    "qualname": "BaseTokenToKVPoolAllocator.restore_state",
    "signature": "def restore_state(self, state)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 71,
    "qualname": "BaseTokenToKVPoolAllocator.backup_state",
    "signature": "def backup_state(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 74,
    "qualname": "BaseTokenToKVPoolAllocator.free_group_begin",
    "signature": "def free_group_begin(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 78,
    "qualname": "BaseTokenToKVPoolAllocator.free_group_end",
    "signature": "def free_group_end(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 83,
    "qualname": "BaseTokenToKVPoolAllocator.merge_and_sort_free",
    "signature": "def merge_and_sort_free(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 91,
    "qualname": "BaseTokenToKVPoolAllocator.get_cpu_copy",
    "signature": "def get_cpu_copy(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 95,
    "qualname": "BaseTokenToKVPoolAllocator.load_cpu_copy",
    "signature": "def load_cpu_copy(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 99,
    "qualname": "BaseTokenToKVPoolAllocator.alloc_extend",
    "signature": "def alloc_extend(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 102,
    "qualname": "BaseTokenToKVPoolAllocator.alloc_decode",
    "signature": "def alloc_decode(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 106,
    "qualname": "BaseTokenToKVPoolAllocator.clear",
    "signature": "def clear(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 110,
    "qualname": "BaseTokenToKVPoolAllocator.alloc",
    "signature": "def alloc(self, need_size: int)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 114,
    "qualname": "BaseTokenToKVPoolAllocator.free",
    "signature": "def free(self, free_index: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 121,
    "qualname": "TokenToKVPoolAllocator.__init__",
    "signature": "def __init__(self, size: int, dtype: torch.dtype, device: str, kvcache: KVCache, need_sort: bool)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 132,
    "qualname": "TokenToKVPoolAllocator.clear",
    "signature": "def clear(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 141,
    "qualname": "TokenToKVPoolAllocator.available_size",
    "signature": "def available_size(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 145,
    "qualname": "TokenToKVPoolAllocator.alloc",
    "signature": "def alloc(self, need_size: int)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 156,
    "qualname": "TokenToKVPoolAllocator.free",
    "signature": "def free(self, free_index: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 168,
    "qualname": "TokenToKVPoolAllocator.get_cpu_copy",
    "signature": "def get_cpu_copy(self, indices)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 171,
    "qualname": "TokenToKVPoolAllocator.load_cpu_copy",
    "signature": "def load_cpu_copy(self, kv_cache_cpu, indices)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 178,
    "qualname": "SWATokenToKVPoolAllocator.__init__",
    "signature": "def __init__(self, size: int, size_swa: int, dtype: torch.dtype, device: str, kvcache: SWAKVPool, need_sort: bool)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 214,
    "qualname": "SWATokenToKVPoolAllocator.available_size",
    "signature": "def available_size(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 217,
    "qualname": "SWATokenToKVPoolAllocator.full_available_size",
    "signature": "def full_available_size(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 220,
    "qualname": "SWATokenToKVPoolAllocator.swa_available_size",
    "signature": "def swa_available_size(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 224,
    "qualname": "SWATokenToKVPoolAllocator.size_full",
    "signature": "def size_full(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 228,
    "qualname": "SWATokenToKVPoolAllocator.size_swa",
    "signature": "def size_swa(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 231,
    "qualname": "SWATokenToKVPoolAllocator.debug_print",
    "signature": "def debug_print(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 239,
    "qualname": "SWATokenToKVPoolAllocator.get_kvcache",
    "signature": "def get_kvcache(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 242,
    "qualname": "SWATokenToKVPoolAllocator.translate_loc_from_full_to_swa",
    "signature": "def translate_loc_from_full_to_swa(self, kv_indices: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 246,
    "qualname": "SWATokenToKVPoolAllocator.alloc",
    "signature": "def alloc(self, need_size: int)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 257,
    "qualname": "SWATokenToKVPoolAllocator.free",
    "signature": "def free(self, free_index: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 270,
    "qualname": "SWATokenToKVPoolAllocator.free_swa",
    "signature": "def free_swa(self, free_index: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 276,
    "qualname": "SWATokenToKVPoolAllocator.backup_state",
    "signature": "def backup_state(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 279,
    "qualname": "SWATokenToKVPoolAllocator.restore_state",
    "signature": "def restore_state(self, state)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 282,
    "qualname": "SWATokenToKVPoolAllocator.clear",
    "signature": "def clear(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 291,
    "qualname": "alloc_extend_kernel",
    "signature": "def alloc_extend_kernel(pre_lens_ptr, seq_lens_ptr, last_loc_ptr, free_page_ptr, out_indices, ret_values, bs_upper: tl.constexpr, page_size: tl.constexpr, max_num_extend_tokens: tl.constexpr)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 379,
    "qualname": "alloc_decode_kernel",
    "signature": "def alloc_decode_kernel(seq_lens_ptr, last_loc_ptr, free_page_ptr, out_indices, ret_values, bs_upper: tl.constexpr, page_size: tl.constexpr)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 429,
    "qualname": "PagedTokenToKVPoolAllocator.__init__",
    "signature": "def __init__(self, size: int, page_size: int, dtype: torch.dtype, device: str, kvcache: KVCache, need_sort: bool)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 445,
    "qualname": "PagedTokenToKVPoolAllocator.alloc",
    "signature": "def alloc(self, need_size: int)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 468,
    "qualname": "PagedTokenToKVPoolAllocator.alloc_extend",
    "signature": "def alloc_extend(self, prefix_lens: torch.Tensor, seq_lens: torch.Tensor, last_loc: torch.Tensor, extend_num_tokens: int)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 517,
    "qualname": "PagedTokenToKVPoolAllocator.alloc_decode",
    "signature": "def alloc_decode(self, seq_lens: torch.Tensor, last_loc: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 552,
    "qualname": "PagedTokenToKVPoolAllocator.free",
    "signature": "def free(self, free_index: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 568,
    "qualname": "PagedTokenToKVPoolAllocator.clear",
    "signature": "def clear(self)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 577,
    "qualname": "PagedTokenToKVPoolAllocator.get_cpu_copy",
    "signature": "def get_cpu_copy(self, indices)"
  },
  {
    "module": "srt.mem_cache.allocator",
    "file": "python/sglang/srt/mem_cache/allocator.py",
    "line": 580,
    "qualname": "PagedTokenToKVPoolAllocator.load_cpu_copy",
    "signature": "def load_cpu_copy(self, kv_cache_cpu, indices)"
  },
  {
    "module": "srt.mem_cache.allocator_ascend",
    "file": "python/sglang/srt/mem_cache/allocator_ascend.py",
    "line": 13,
    "qualname": "alloc_extend_kernel_ascend",
    "signature": "def alloc_extend_kernel_ascend(prefix_lens, seq_lens, last_loc, free_pages, out_indices, page_size, device)"
  },
  {
    "module": "srt.mem_cache.allocator_ascend",
    "file": "python/sglang/srt/mem_cache/allocator_ascend.py",
    "line": 69,
    "qualname": "AscendPagedTokenToKVPoolAllocator.alloc_extend",
    "signature": "def alloc_extend(self, prefix_lens: torch.Tensor, seq_lens: torch.Tensor, last_loc: torch.Tensor, extend_num_tokens: int)"
  },
  {
    "module": "srt.mem_cache.allocator_ascend",
    "file": "python/sglang/srt/mem_cache/allocator_ascend.py",
    "line": 115,
    "qualname": "AscendPagedTokenToKVPoolAllocator.alloc_decode",
    "signature": "def alloc_decode(self, seq_lens: torch.Tensor, last_loc: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.base_prefix_cache",
    "file": "python/sglang/srt/mem_cache/base_prefix_cache.py",
    "line": 35,
    "qualname": "BasePrefixCache.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.mem_cache.base_prefix_cache",
    "file": "python/sglang/srt/mem_cache/base_prefix_cache.py",
    "line": 39,
    "qualname": "BasePrefixCache.match_prefix",
    "signature": "def match_prefix(self, key: List[int])"
  },
  {
    "module": "srt.mem_cache.base_prefix_cache",
    "file": "python/sglang/srt/mem_cache/base_prefix_cache.py",
    "line": 43,
    "qualname": "BasePrefixCache.cache_finished_req",
    "signature": "def cache_finished_req(self, req: Req)"
  },
  {
    "module": "srt.mem_cache.base_prefix_cache",
    "file": "python/sglang/srt/mem_cache/base_prefix_cache.py",
    "line": 47,
    "qualname": "BasePrefixCache.cache_unfinished_req",
    "signature": "def cache_unfinished_req(self, req: Req)"
  },
  {
    "module": "srt.mem_cache.base_prefix_cache",
    "file": "python/sglang/srt/mem_cache/base_prefix_cache.py",
    "line": 51,
    "qualname": "BasePrefixCache.evict",
    "signature": "def evict(self, num_tokens: int)"
  },
  {
    "module": "srt.mem_cache.base_prefix_cache",
    "file": "python/sglang/srt/mem_cache/base_prefix_cache.py",
    "line": 55,
    "qualname": "BasePrefixCache.inc_lock_ref",
    "signature": "def inc_lock_ref(self, node: Any)"
  },
  {
    "module": "srt.mem_cache.base_prefix_cache",
    "file": "python/sglang/srt/mem_cache/base_prefix_cache.py",
    "line": 59,
    "qualname": "BasePrefixCache.dec_lock_ref",
    "signature": "def dec_lock_ref(self, node: Any, swa_uuid_for_lock: Optional[str])"
  },
  {
    "module": "srt.mem_cache.base_prefix_cache",
    "file": "python/sglang/srt/mem_cache/base_prefix_cache.py",
    "line": 62,
    "qualname": "BasePrefixCache.evictable_size",
    "signature": "def evictable_size(self)"
  },
  {
    "module": "srt.mem_cache.base_prefix_cache",
    "file": "python/sglang/srt/mem_cache/base_prefix_cache.py",
    "line": 65,
    "qualname": "BasePrefixCache.full_evictable_size",
    "signature": "def full_evictable_size(self)"
  },
  {
    "module": "srt.mem_cache.base_prefix_cache",
    "file": "python/sglang/srt/mem_cache/base_prefix_cache.py",
    "line": 68,
    "qualname": "BasePrefixCache.swa_evictable_size",
    "signature": "def swa_evictable_size(self)"
  },
  {
    "module": "srt.mem_cache.base_prefix_cache",
    "file": "python/sglang/srt/mem_cache/base_prefix_cache.py",
    "line": 71,
    "qualname": "BasePrefixCache.protected_size",
    "signature": "def protected_size(self)"
  },
  {
    "module": "srt.mem_cache.base_prefix_cache",
    "file": "python/sglang/srt/mem_cache/base_prefix_cache.py",
    "line": 74,
    "qualname": "BasePrefixCache.full_protected_size",
    "signature": "def full_protected_size(self)"
  },
  {
    "module": "srt.mem_cache.base_prefix_cache",
    "file": "python/sglang/srt/mem_cache/base_prefix_cache.py",
    "line": 77,
    "qualname": "BasePrefixCache.swa_protected_size",
    "signature": "def swa_protected_size(self)"
  },
  {
    "module": "srt.mem_cache.base_prefix_cache",
    "file": "python/sglang/srt/mem_cache/base_prefix_cache.py",
    "line": 80,
    "qualname": "BasePrefixCache.total_size",
    "signature": "def total_size(self)"
  },
  {
    "module": "srt.mem_cache.base_prefix_cache",
    "file": "python/sglang/srt/mem_cache/base_prefix_cache.py",
    "line": 83,
    "qualname": "BasePrefixCache.pretty_print",
    "signature": "def pretty_print(self)"
  },
  {
    "module": "srt.mem_cache.base_prefix_cache",
    "file": "python/sglang/srt/mem_cache/base_prefix_cache.py",
    "line": 86,
    "qualname": "BasePrefixCache.init_load_back",
    "signature": "def init_load_back(self, last_host_node: Any, host_hit_length: int)"
  },
  {
    "module": "srt.mem_cache.base_prefix_cache",
    "file": "python/sglang/srt/mem_cache/base_prefix_cache.py",
    "line": 96,
    "qualname": "BasePrefixCache.ready_to_load_host_cache",
    "signature": "def ready_to_load_host_cache(self)"
  },
  {
    "module": "srt.mem_cache.base_prefix_cache",
    "file": "python/sglang/srt/mem_cache/base_prefix_cache.py",
    "line": 102,
    "qualname": "BasePrefixCache.check_hicache_events",
    "signature": "def check_hicache_events(self)"
  },
  {
    "module": "srt.mem_cache.base_prefix_cache",
    "file": "python/sglang/srt/mem_cache/base_prefix_cache.py",
    "line": 108,
    "qualname": "BasePrefixCache.take_events",
    "signature": "def take_events(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 38,
    "qualname": "synchronized",
    "signature": "def synchronized(debug_only)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 55,
    "qualname": "HostKVCache.__init__",
    "signature": "def __init__(self, device_pool: KVCache, host_to_device_ratio: float, host_size: int, page_size: int, layout: str, pin_memory: bool, device: str)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 112,
    "qualname": "HostKVCache.get_size_per_token",
    "signature": "def get_size_per_token(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 116,
    "qualname": "HostKVCache.init_kv_buffer",
    "signature": "def init_kv_buffer(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 120,
    "qualname": "HostKVCache.load_to_device_per_layer",
    "signature": "def load_to_device_per_layer(self, device_pool, host_indices, device_indices, layer_id, io_backend)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 129,
    "qualname": "HostKVCache.backup_from_device_all_layer",
    "signature": "def backup_from_device_all_layer(self, device_pool, host_indices, device_indices, io_backend)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 138,
    "qualname": "HostKVCache.get_flat_data_page",
    "signature": "def get_flat_data_page(self, index)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 145,
    "qualname": "HostKVCache.get_dummy_flat_data_page",
    "signature": "def get_dummy_flat_data_page(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 153,
    "qualname": "HostKVCache.set_from_flat_data_page",
    "signature": "def set_from_flat_data_page(self, index: int, data_page: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 160,
    "qualname": "HostKVCache.clear",
    "signature": "def clear(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 167,
    "qualname": "HostKVCache.available_size",
    "signature": "def available_size(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 171,
    "qualname": "HostKVCache.alloc",
    "signature": "def alloc(self, need_size: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 187,
    "qualname": "HostKVCache.free",
    "signature": "def free(self, indices: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 194,
    "qualname": "HostKVCache.get_state",
    "signature": "def get_state(self, indices: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 203,
    "qualname": "HostKVCache.is_reserved",
    "signature": "def is_reserved(self, indices: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 207,
    "qualname": "HostKVCache.is_protected",
    "signature": "def is_protected(self, indices: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 211,
    "qualname": "HostKVCache.is_synced",
    "signature": "def is_synced(self, indices: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 215,
    "qualname": "HostKVCache.is_backup",
    "signature": "def is_backup(self, indices: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 219,
    "qualname": "HostKVCache.update_backup",
    "signature": "def update_backup(self, indices: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 228,
    "qualname": "HostKVCache.update_prefetch",
    "signature": "def update_prefetch(self, indices: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 237,
    "qualname": "HostKVCache.update_synced",
    "signature": "def update_synced(self, indices: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 241,
    "qualname": "HostKVCache.protect_write",
    "signature": "def protect_write(self, indices: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 250,
    "qualname": "HostKVCache.protect_load",
    "signature": "def protect_load(self, indices: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 259,
    "qualname": "HostKVCache.complete_io",
    "signature": "def complete_io(self, indices: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 271,
    "qualname": "MHATokenToKVPoolHost.__init__",
    "signature": "def __init__(self, device_pool: MHATokenToKVPool, host_to_device_ratio: float, host_size: int, page_size: int, layout: str, pin_memory: bool, device: str)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 303,
    "qualname": "MHATokenToKVPoolHost.get_size_per_token",
    "signature": "def get_size_per_token(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 310,
    "qualname": "MHATokenToKVPoolHost.get_ksize_per_token",
    "signature": "def get_ksize_per_token(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 313,
    "qualname": "MHATokenToKVPoolHost.init_kv_buffer",
    "signature": "def init_kv_buffer(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 330,
    "qualname": "MHATokenToKVPoolHost.k_buffer",
    "signature": "def k_buffer(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 334,
    "qualname": "MHATokenToKVPoolHost.v_buffer",
    "signature": "def v_buffer(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 337,
    "qualname": "MHATokenToKVPoolHost.load_to_device_per_layer",
    "signature": "def load_to_device_per_layer(self, device_pool, host_indices, device_indices, layer_id, io_backend)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 387,
    "qualname": "MHATokenToKVPoolHost.backup_from_device_all_layer",
    "signature": "def backup_from_device_all_layer(self, device_pool, host_indices, device_indices, io_backend)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 430,
    "qualname": "MHATokenToKVPoolHost.get_flat_data_page",
    "signature": "def get_flat_data_page(self, index)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 438,
    "qualname": "MHATokenToKVPoolHost.get_dummy_flat_data_page",
    "signature": "def get_dummy_flat_data_page(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 446,
    "qualname": "MHATokenToKVPoolHost.set_from_flat_data_page",
    "signature": "def set_from_flat_data_page(self, index: int, data_page: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 466,
    "qualname": "MHATokenToKVPoolHost.get_buffer_meta",
    "signature": "def get_buffer_meta(self, keys, indices, local_rank)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 502,
    "qualname": "MHATokenToKVPoolHost.get_buffer_with_hash",
    "signature": "def get_buffer_with_hash(self, keys, indices)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 521,
    "qualname": "MLATokenToKVPoolHost.__init__",
    "signature": "def __init__(self, device_pool: MLATokenToKVPool, host_to_device_ratio: float, host_size: int, page_size: int, layout: str, pin_memory: bool, device: str)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 547,
    "qualname": "MLATokenToKVPoolHost.get_size_per_token",
    "signature": "def get_size_per_token(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 559,
    "qualname": "MLATokenToKVPoolHost.get_ksize_per_token",
    "signature": "def get_ksize_per_token(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 562,
    "qualname": "MLATokenToKVPoolHost.init_kv_buffer",
    "signature": "def init_kv_buffer(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 591,
    "qualname": "MLATokenToKVPoolHost.load_to_device_per_layer",
    "signature": "def load_to_device_per_layer(self, device_pool, host_indices, device_indices, layer_id, io_backend)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 627,
    "qualname": "MLATokenToKVPoolHost.backup_from_device_all_layer",
    "signature": "def backup_from_device_all_layer(self, device_pool, host_indices, device_indices, io_backend)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 666,
    "qualname": "MLATokenToKVPoolHost.get_flat_data_page",
    "signature": "def get_flat_data_page(self, index)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 674,
    "qualname": "MLATokenToKVPoolHost.get_dummy_flat_data_page",
    "signature": "def get_dummy_flat_data_page(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 687,
    "qualname": "MLATokenToKVPoolHost.set_from_flat_data_page",
    "signature": "def set_from_flat_data_page(self, index: int, data_page: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 705,
    "qualname": "MLATokenToKVPoolHost.get_buffer_meta",
    "signature": "def get_buffer_meta(self, keys, indices, local_rank)"
  },
  {
    "module": "srt.mem_cache.memory_pool_host",
    "file": "python/sglang/srt/mem_cache/memory_pool_host.py",
    "line": 729,
    "qualname": "MLATokenToKVPoolHost.get_buffer_with_hash",
    "signature": "def get_buffer_with_hash(self, keys, indices)"
  },
  {
    "module": "srt.mem_cache.multimodal_cache",
    "file": "python/sglang/srt/mem_cache/multimodal_cache.py",
    "line": 14,
    "qualname": "MultiModalCache.__init__",
    "signature": "def __init__(self, max_size: int)"
  },
  {
    "module": "srt.mem_cache.multimodal_cache",
    "file": "python/sglang/srt/mem_cache/multimodal_cache.py",
    "line": 40,
    "qualname": "MultiModalCache.put",
    "signature": "def put(self, mm_hash: int, embedding: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.multimodal_cache",
    "file": "python/sglang/srt/mem_cache/multimodal_cache.py",
    "line": 49,
    "qualname": "MultiModalCache.has",
    "signature": "def has(self, mm_hash: int)"
  },
  {
    "module": "srt.mem_cache.multimodal_cache",
    "file": "python/sglang/srt/mem_cache/multimodal_cache.py",
    "line": 52,
    "qualname": "MultiModalCache.get",
    "signature": "def get(self, mm_hash: int)"
  },
  {
    "module": "srt.mem_cache.multimodal_cache",
    "file": "python/sglang/srt/mem_cache/multimodal_cache.py",
    "line": 60,
    "qualname": "MultiModalCache.clear",
    "signature": "def clear(self)"
  },
  {
    "module": "srt.mem_cache.multimodal_cache",
    "file": "python/sglang/srt/mem_cache/multimodal_cache.py",
    "line": 67,
    "qualname": "MultiModalCache.__len__",
    "signature": "def __len__(self)"
  },
  {
    "module": "srt.mem_cache.chunk_cache",
    "file": "python/sglang/srt/mem_cache/chunk_cache.py",
    "line": 21,
    "qualname": "ChunkCache.__init__",
    "signature": "def __init__(self, req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, page_size: int)"
  },
  {
    "module": "srt.mem_cache.chunk_cache",
    "file": "python/sglang/srt/mem_cache/chunk_cache.py",
    "line": 31,
    "qualname": "ChunkCache.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.mem_cache.chunk_cache",
    "file": "python/sglang/srt/mem_cache/chunk_cache.py",
    "line": 34,
    "qualname": "ChunkCache.match_prefix",
    "signature": "def match_prefix(self)"
  },
  {
    "module": "srt.mem_cache.chunk_cache",
    "file": "python/sglang/srt/mem_cache/chunk_cache.py",
    "line": 41,
    "qualname": "ChunkCache.cache_finished_req",
    "signature": "def cache_finished_req(self, req: Req)"
  },
  {
    "module": "srt.mem_cache.chunk_cache",
    "file": "python/sglang/srt/mem_cache/chunk_cache.py",
    "line": 50,
    "qualname": "ChunkCache.cache_unfinished_req",
    "signature": "def cache_unfinished_req(self, req: Req)"
  },
  {
    "module": "srt.mem_cache.chunk_cache",
    "file": "python/sglang/srt/mem_cache/chunk_cache.py",
    "line": 58,
    "qualname": "ChunkCache.evict",
    "signature": "def evict(self, num_tokens: int)"
  },
  {
    "module": "srt.mem_cache.chunk_cache",
    "file": "python/sglang/srt/mem_cache/chunk_cache.py",
    "line": 61,
    "qualname": "ChunkCache.inc_lock_ref",
    "signature": "def inc_lock_ref(self, node: Any)"
  },
  {
    "module": "srt.mem_cache.chunk_cache",
    "file": "python/sglang/srt/mem_cache/chunk_cache.py",
    "line": 64,
    "qualname": "ChunkCache.dec_lock_ref",
    "signature": "def dec_lock_ref(self, node: Any, swa_uuid_for_lock: Optional[str])"
  },
  {
    "module": "srt.mem_cache.chunk_cache",
    "file": "python/sglang/srt/mem_cache/chunk_cache.py",
    "line": 67,
    "qualname": "ChunkCache.pretty_print",
    "signature": "def pretty_print(self)"
  },
  {
    "module": "srt.mem_cache.chunk_cache",
    "file": "python/sglang/srt/mem_cache/chunk_cache.py",
    "line": 74,
    "qualname": "SWAChunkCache.__init__",
    "signature": "def __init__(self, req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: SWATokenToKVPoolAllocator, page_size: int)"
  },
  {
    "module": "srt.mem_cache.chunk_cache",
    "file": "python/sglang/srt/mem_cache/chunk_cache.py",
    "line": 83,
    "qualname": "SWAChunkCache.evict_swa",
    "signature": "def evict_swa(self, req: Req, prelen: int, attention_chunk_size: int)"
  },
  {
    "module": "srt.mem_cache.chunk_cache",
    "file": "python/sglang/srt/mem_cache/chunk_cache.py",
    "line": 99,
    "qualname": "SWAChunkCache.evict",
    "signature": "def evict(self, num_tokens: int)"
  },
  {
    "module": "srt.mem_cache.hicache_storage",
    "file": "python/sglang/srt/mem_cache/hicache_storage.py",
    "line": 13,
    "qualname": "get_hash_str",
    "signature": "def get_hash_str(token_ids: List[int], prior_hash: str)"
  },
  {
    "module": "srt.mem_cache.hicache_storage",
    "file": "python/sglang/srt/mem_cache/hicache_storage.py",
    "line": 44,
    "qualname": "HiCacheStorage.get",
    "signature": "def get(self, key: str, target_location: Optional[Any], target_sizes: Optional[Any])"
  },
  {
    "module": "srt.mem_cache.hicache_storage",
    "file": "python/sglang/srt/mem_cache/hicache_storage.py",
    "line": 57,
    "qualname": "HiCacheStorage.batch_get",
    "signature": "def batch_get(self, keys: List[str], target_locations: Optional[Any], target_sizes: Optional[Any])"
  },
  {
    "module": "srt.mem_cache.hicache_storage",
    "file": "python/sglang/srt/mem_cache/hicache_storage.py",
    "line": 70,
    "qualname": "HiCacheStorage.set",
    "signature": "def set(self, key: str, value: Optional[Any], target_location: Optional[Any], target_sizes: Optional[Any])"
  },
  {
    "module": "srt.mem_cache.hicache_storage",
    "file": "python/sglang/srt/mem_cache/hicache_storage.py",
    "line": 84,
    "qualname": "HiCacheStorage.batch_set",
    "signature": "def batch_set(self, keys: List[str], values: Optional[Any], target_locations: Optional[Any], target_sizes: Optional[Any])"
  },
  {
    "module": "srt.mem_cache.hicache_storage",
    "file": "python/sglang/srt/mem_cache/hicache_storage.py",
    "line": 98,
    "qualname": "HiCacheStorage.exists",
    "signature": "def exists(self, key: str)"
  },
  {
    "module": "srt.mem_cache.hicache_storage",
    "file": "python/sglang/srt/mem_cache/hicache_storage.py",
    "line": 105,
    "qualname": "HiCacheStorage.batch_exists",
    "signature": "def batch_exists(self, keys: List[str])"
  },
  {
    "module": "srt.mem_cache.hicache_storage",
    "file": "python/sglang/srt/mem_cache/hicache_storage.py",
    "line": 119,
    "qualname": "HiCacheFile.__init__",
    "signature": "def __init__(self, storage_config: HiCacheStorageConfig, file_path: str)"
  },
  {
    "module": "srt.mem_cache.hicache_storage",
    "file": "python/sglang/srt/mem_cache/hicache_storage.py",
    "line": 137,
    "qualname": "HiCacheFile.get",
    "signature": "def get(self, key: str, target_location: torch.Tensor, target_sizes: Optional[Any])"
  },
  {
    "module": "srt.mem_cache.hicache_storage",
    "file": "python/sglang/srt/mem_cache/hicache_storage.py",
    "line": 158,
    "qualname": "HiCacheFile.batch_get",
    "signature": "def batch_get(self, keys: List[str], target_locations: List[torch.Tensor], target_sizes: Optional[Any])"
  },
  {
    "module": "srt.mem_cache.hicache_storage",
    "file": "python/sglang/srt/mem_cache/hicache_storage.py",
    "line": 171,
    "qualname": "HiCacheFile.set",
    "signature": "def set(self, key: str, value: Optional[Any], target_location: Optional[Any], target_sizes: Optional[Any])"
  },
  {
    "module": "srt.mem_cache.hicache_storage",
    "file": "python/sglang/srt/mem_cache/hicache_storage.py",
    "line": 190,
    "qualname": "HiCacheFile.batch_set",
    "signature": "def batch_set(self, keys: List[str], values: Optional[Any], target_locations: Optional[Any], target_sizes: Optional[Any])"
  },
  {
    "module": "srt.mem_cache.hicache_storage",
    "file": "python/sglang/srt/mem_cache/hicache_storage.py",
    "line": 202,
    "qualname": "HiCacheFile.exists",
    "signature": "def exists(self, key: str)"
  },
  {
    "module": "srt.mem_cache.hicache_storage",
    "file": "python/sglang/srt/mem_cache/hicache_storage.py",
    "line": 207,
    "qualname": "HiCacheFile.delete",
    "signature": "def delete(self, key: str)"
  },
  {
    "module": "srt.mem_cache.hicache_storage",
    "file": "python/sglang/srt/mem_cache/hicache_storage.py",
    "line": 216,
    "qualname": "HiCacheFile.clear",
    "signature": "def clear(self)"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 29,
    "qualname": "HiRadixCache.__init__",
    "signature": "def __init__(self, req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, tp_cache_group: torch.distributed.ProcessGroup, page_size: int, hicache_ratio: float, hicache_size: int, hicache_write_policy: str, hicache_io_backend: str, hicache_mem_layout: str, hicache_storage_backend: Optional[str], hicache_storage_prefetch_policy: Optional[str], model_name: Optional[str], storage_backend_extra_config: Optional[str])"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 115,
    "qualname": "HiRadixCache.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 121,
    "qualname": "HiRadixCache.get_height",
    "signature": "def get_height(self, node: TreeNode)"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 128,
    "qualname": "HiRadixCache.write_backup",
    "signature": "def write_backup(self, node: TreeNode, write_back)"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 151,
    "qualname": "HiRadixCache.write_backup_storage",
    "signature": "def write_backup_storage(self, node: TreeNode)"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 158,
    "qualname": "HiRadixCache.inc_hit_count",
    "signature": "def inc_hit_count(self, node: TreeNode)"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 176,
    "qualname": "HiRadixCache.writing_check",
    "signature": "def writing_check(self, write_back)"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 198,
    "qualname": "HiRadixCache.loading_check",
    "signature": "def loading_check(self)"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 213,
    "qualname": "HiRadixCache.evictable_size",
    "signature": "def evictable_size(self)"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 216,
    "qualname": "HiRadixCache.evict",
    "signature": "def evict(self, num_tokens: int)"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 268,
    "qualname": "HiRadixCache.evict_host",
    "signature": "def evict_host(self, num_tokens: int)"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 295,
    "qualname": "HiRadixCache.load_back",
    "signature": "def load_back(self, node: TreeNode, mem_quota: Optional[int])"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 347,
    "qualname": "HiRadixCache.init_load_back",
    "signature": "def init_load_back(self, last_node: TreeNode, host_hit_length: int, mem_quota: Optional[int])"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 370,
    "qualname": "HiRadixCache.ready_to_load_host_cache",
    "signature": "def ready_to_load_host_cache(self)"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 375,
    "qualname": "HiRadixCache.check_hicache_events",
    "signature": "def check_hicache_events(self)"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 382,
    "qualname": "HiRadixCache.check_revoked_prefetch",
    "signature": "def check_revoked_prefetch(self)"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 404,
    "qualname": "HiRadixCache.check_backup_progress",
    "signature": "def check_backup_progress(self)"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 431,
    "qualname": "HiRadixCache.can_terminate_prefetch",
    "signature": "def can_terminate_prefetch(self, operation: PrefetchOperation)"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 465,
    "qualname": "HiRadixCache.check_prefetch_progress",
    "signature": "def check_prefetch_progress(self, req_id: str)"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 521,
    "qualname": "HiRadixCache.match_prefix",
    "signature": "def match_prefix(self, key: List[int])"
  },
  {
    "module": "srt.mem_cache.hiradix_cache",
    "file": "python/sglang/srt/mem_cache/hiradix_cache.py",
    "line": 556,
    "qualname": "HiRadixCache.prefetch_from_storage",
    "signature": "def prefetch_from_storage(self, req_id: str, last_host_node: TreeNode, new_input_tokens: List[int], last_hash: Optional[str])"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 22,
    "qualname": "LoRAKey.__init__",
    "signature": "def __init__(self, lora_id: str, token_ids: List[int])"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 28,
    "qualname": "LoRAKey.__len__",
    "signature": "def __len__(self)"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 32,
    "qualname": "get_child_key",
    "signature": "def get_child_key(key: LoRAKey)"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 45,
    "qualname": "LoRATreeNode.__init__",
    "signature": "def __init__(self, id: Optional[int])"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 57,
    "qualname": "LoRATreeNode.evicted",
    "signature": "def evicted(self)"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 60,
    "qualname": "LoRATreeNode.__lt__",
    "signature": "def __lt__(self, other: 'LoRATreeNode')"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 79,
    "qualname": "LoRARadixCache.__init__",
    "signature": "def __init__(self, req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, page_size: int, disable: bool)"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 104,
    "qualname": "LoRARadixCache.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 111,
    "qualname": "LoRARadixCache.match_prefix",
    "signature": "def match_prefix(self, key: List[int])"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 116,
    "qualname": "LoRARadixCache.match_prefix_with_lora_id",
    "signature": "def match_prefix_with_lora_id(self, key: LoRAKey)"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 149,
    "qualname": "LoRARadixCache.insert",
    "signature": "def insert(self, key: LoRAKey, value)"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 157,
    "qualname": "LoRARadixCache.cache_finished_req",
    "signature": "def cache_finished_req(self, req: Req)"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 186,
    "qualname": "LoRARadixCache.cache_unfinished_req",
    "signature": "def cache_unfinished_req(self, req: Req)"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 221,
    "qualname": "LoRARadixCache.pretty_print",
    "signature": "def pretty_print(self)"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 225,
    "qualname": "LoRARadixCache.total_size",
    "signature": "def total_size(self)"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 228,
    "qualname": "LoRARadixCache.evict",
    "signature": "def evict(self, num_tokens: int)"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 251,
    "qualname": "LoRARadixCache.inc_lock_ref",
    "signature": "def inc_lock_ref(self, node: LoRATreeNode)"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 265,
    "qualname": "LoRARadixCache.dec_lock_ref",
    "signature": "def dec_lock_ref(self, node: LoRATreeNode)"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 279,
    "qualname": "LoRARadixCache.evictable_size",
    "signature": "def evictable_size(self)"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 282,
    "qualname": "LoRARadixCache.protected_size",
    "signature": "def protected_size(self)"
  },
  {
    "module": "srt.mem_cache.lora_radix_cache",
    "file": "python/sglang/srt/mem_cache/lora_radix_cache.py",
    "line": 286,
    "qualname": "LoRARadixCache.all_values_flatten",
    "signature": "def all_values_flatten(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 53,
    "qualname": "ReqToTokenPool.__init__",
    "signature": "def __init__(self, size: int, max_context_len: int, device: str, enable_memory_saver: bool)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 75,
    "qualname": "ReqToTokenPool.write",
    "signature": "def write(self, indices, values)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 78,
    "qualname": "ReqToTokenPool.available_size",
    "signature": "def available_size(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 81,
    "qualname": "ReqToTokenPool.alloc",
    "signature": "def alloc(self, need_size: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 90,
    "qualname": "ReqToTokenPool.free",
    "signature": "def free(self, free_index: Union[int, List[int]])"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 96,
    "qualname": "ReqToTokenPool.clear",
    "signature": "def clear(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 102,
    "qualname": "KVCache.__init__",
    "signature": "def __init__(self, size: int, page_size: int, dtype: torch.dtype, layer_num: int, device: str, enable_memory_saver: bool, start_layer: Optional[int], end_layer: Optional[int])"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 134,
    "qualname": "KVCache.get_key_buffer",
    "signature": "def get_key_buffer(self, layer_id: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 138,
    "qualname": "KVCache.get_value_buffer",
    "signature": "def get_value_buffer(self, layer_id: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 142,
    "qualname": "KVCache.get_kv_buffer",
    "signature": "def get_kv_buffer(self, layer_id: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 146,
    "qualname": "KVCache.set_kv_buffer",
    "signature": "def set_kv_buffer(self, layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 155,
    "qualname": "KVCache.register_layer_transfer_counter",
    "signature": "def register_layer_transfer_counter(self, layer_transfer_counter)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 158,
    "qualname": "KVCache.get_cpu_copy",
    "signature": "def get_cpu_copy(self, indices)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 161,
    "qualname": "KVCache.load_cpu_copy",
    "signature": "def load_cpu_copy(self, kv_cache_cpu, indices)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 167,
    "qualname": "MHATokenToKVPool.__init__",
    "signature": "def __init__(self, size: int, page_size: int, dtype: torch.dtype, head_num: int, head_dim: int, layer_num: int, device: str, enable_memory_saver: bool, start_layer: Optional[int], end_layer: Optional[int])"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 267,
    "qualname": "MHATokenToKVPool.get_kv_size_bytes",
    "signature": "def get_kv_size_bytes(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 279,
    "qualname": "MHATokenToKVPool.get_contiguous_buf_infos",
    "signature": "def get_contiguous_buf_infos(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 305,
    "qualname": "MHATokenToKVPool.maybe_get_custom_mem_pool",
    "signature": "def maybe_get_custom_mem_pool(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 308,
    "qualname": "MHATokenToKVPool.get_cpu_copy",
    "signature": "def get_cpu_copy(self, indices)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 326,
    "qualname": "MHATokenToKVPool.load_cpu_copy",
    "signature": "def load_cpu_copy(self, kv_cache_cpu, indices)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 349,
    "qualname": "MHATokenToKVPool.get_key_buffer",
    "signature": "def get_key_buffer(self, layer_id: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 364,
    "qualname": "MHATokenToKVPool.get_value_buffer",
    "signature": "def get_value_buffer(self, layer_id: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 369,
    "qualname": "MHATokenToKVPool.get_kv_buffer",
    "signature": "def get_kv_buffer(self, layer_id: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 372,
    "qualname": "MHATokenToKVPool.set_kv_buffer",
    "signature": "def set_kv_buffer(self, layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor, k_scale: Optional[float], v_scale: Optional[float], layer_id_override: Optional[int])"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 412,
    "qualname": "MHATokenToKVPool.move_kv_cache",
    "signature": "def move_kv_cache(self, tgt_loc: torch.Tensor, src_loc: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 426,
    "qualname": "SWAKVPool.__init__",
    "signature": "def __init__(self, size: int, size_swa: int, dtype: torch.dtype, head_num: int, head_dim: int, swa_attention_layer_ids: List[int], full_attention_layer_ids: List[int], enable_kvcache_transpose: bool, device: str)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 478,
    "qualname": "SWAKVPool.get_kv_size_bytes",
    "signature": "def get_kv_size_bytes(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 483,
    "qualname": "SWAKVPool.get_contiguous_buf_infos",
    "signature": "def get_contiguous_buf_infos(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 497,
    "qualname": "SWAKVPool.get_key_buffer",
    "signature": "def get_key_buffer(self, layer_id: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 504,
    "qualname": "SWAKVPool.get_value_buffer",
    "signature": "def get_value_buffer(self, layer_id: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 511,
    "qualname": "SWAKVPool.get_kv_buffer",
    "signature": "def get_kv_buffer(self, layer_id: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 518,
    "qualname": "SWAKVPool.translate_loc_from_full_to_swa",
    "signature": "def translate_loc_from_full_to_swa(self, kv_indices: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 522,
    "qualname": "SWAKVPool.set_kv_buffer",
    "signature": "def set_kv_buffer(self, layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor, k_scale: float, v_scale: float)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 582,
    "qualname": "AscendTokenToKVPool.get_contiguous_buf_infos",
    "signature": "def get_contiguous_buf_infos(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 608,
    "qualname": "AscendTokenToKVPool.set_kv_buffer",
    "signature": "def set_kv_buffer(self, layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor, k_scale: Optional[float], v_scale: Optional[float])"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 644,
    "qualname": "set_mla_kv_buffer_kernel",
    "signature": "def set_mla_kv_buffer_kernel(kv_buffer_ptr, cache_k_nope_ptr, cache_k_rope_ptr, loc_ptr, buffer_stride: tl.constexpr, nope_stride: tl.constexpr, rope_stride: tl.constexpr, nope_dim: tl.constexpr, rope_dim: tl.constexpr, BLOCK: tl.constexpr)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 682,
    "qualname": "set_mla_kv_buffer_triton",
    "signature": "def set_mla_kv_buffer_triton(kv_buffer: torch.Tensor, loc: torch.Tensor, cache_k_nope: torch.Tensor, cache_k_rope: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 710,
    "qualname": "MLATokenToKVPool.__init__",
    "signature": "def __init__(self, size: int, page_size: int, dtype: torch.dtype, kv_lora_rank: int, qk_rope_head_dim: int, layer_num: int, device: str, enable_memory_saver: bool, start_layer: Optional[int], end_layer: Optional[int])"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 779,
    "qualname": "MLATokenToKVPool.get_kv_size_bytes",
    "signature": "def get_kv_size_bytes(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 787,
    "qualname": "MLATokenToKVPool.get_contiguous_buf_infos",
    "signature": "def get_contiguous_buf_infos(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 796,
    "qualname": "MLATokenToKVPool.maybe_get_custom_mem_pool",
    "signature": "def maybe_get_custom_mem_pool(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 799,
    "qualname": "MLATokenToKVPool.get_key_buffer",
    "signature": "def get_key_buffer(self, layer_id: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 807,
    "qualname": "MLATokenToKVPool.get_value_buffer",
    "signature": "def get_value_buffer(self, layer_id: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 817,
    "qualname": "MLATokenToKVPool.get_kv_buffer",
    "signature": "def get_kv_buffer(self, layer_id: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 820,
    "qualname": "MLATokenToKVPool.set_kv_buffer",
    "signature": "def set_kv_buffer(self, layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 837,
    "qualname": "MLATokenToKVPool.set_mla_kv_buffer",
    "signature": "def set_mla_kv_buffer(self, layer: RadixAttention, loc: torch.Tensor, cache_k_nope: torch.Tensor, cache_k_rope: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 856,
    "qualname": "MLATokenToKVPool.get_cpu_copy",
    "signature": "def get_cpu_copy(self, indices)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 871,
    "qualname": "MLATokenToKVPool.load_cpu_copy",
    "signature": "def load_cpu_copy(self, kv_cache_cpu, indices)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 885,
    "qualname": "AscendMLAPagedTokenToKVPool.__init__",
    "signature": "def __init__(self, size: int, page_size: int, dtype: torch.dtype, kv_lora_rank: int, qk_rope_head_dim: int, layer_num: int, device: str, enable_memory_saver: bool, start_layer: Optional[int], end_layer: Optional[int])"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 945,
    "qualname": "AscendMLAPagedTokenToKVPool.get_kv_size_bytes",
    "signature": "def get_kv_size_bytes(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 955,
    "qualname": "AscendMLAPagedTokenToKVPool.get_kv_buffer",
    "signature": "def get_kv_buffer(self, layer_id: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 963,
    "qualname": "AscendMLAPagedTokenToKVPool.get_key_buffer",
    "signature": "def get_key_buffer(self, layer_id: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 971,
    "qualname": "AscendMLAPagedTokenToKVPool.get_value_buffer",
    "signature": "def get_value_buffer(self, layer_id: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 980,
    "qualname": "AscendMLAPagedTokenToKVPool.get_contiguous_buf_infos",
    "signature": "def get_contiguous_buf_infos(self)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 993,
    "qualname": "AscendMLAPagedTokenToKVPool.set_kv_buffer",
    "signature": "def set_kv_buffer(self, layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 1027,
    "qualname": "DoubleSparseTokenToKVPool.__init__",
    "signature": "def __init__(self, size: int, page_size: int, dtype: torch.dtype, head_num: int, head_dim: int, layer_num: int, device: str, heavy_channel_num: int, enable_memory_saver: bool, start_layer: Optional[int], end_layer: Optional[int])"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 1075,
    "qualname": "DoubleSparseTokenToKVPool.get_key_buffer",
    "signature": "def get_key_buffer(self, layer_id: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 1078,
    "qualname": "DoubleSparseTokenToKVPool.get_value_buffer",
    "signature": "def get_value_buffer(self, layer_id: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 1081,
    "qualname": "DoubleSparseTokenToKVPool.get_label_buffer",
    "signature": "def get_label_buffer(self, layer_id: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 1084,
    "qualname": "DoubleSparseTokenToKVPool.get_kv_buffer",
    "signature": "def get_kv_buffer(self, layer_id: int)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 1090,
    "qualname": "DoubleSparseTokenToKVPool.set_kv_buffer",
    "signature": "def set_kv_buffer(self, layer: RadixAttention, loc: torch.Tensor, cache_k: torch.Tensor, cache_v: torch.Tensor, cache_label: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.memory_pool",
    "file": "python/sglang/srt/mem_cache/memory_pool.py",
    "line": 1106,
    "qualname": "copy_all_layer_kv_cache",
    "signature": "def copy_all_layer_kv_cache(data_ptrs, strides, tgt_loc_ptr, src_loc_ptr, num_locs, num_locs_upper: tl.constexpr)"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 47,
    "qualname": "TreeNode.__init__",
    "signature": "def __init__(self, id: Optional[int])"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 71,
    "qualname": "TreeNode.evicted",
    "signature": "def evicted(self)"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 75,
    "qualname": "TreeNode.backuped",
    "signature": "def backuped(self)"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 78,
    "qualname": "TreeNode.protect_host",
    "signature": "def protect_host(self)"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 82,
    "qualname": "TreeNode.release_host",
    "signature": "def release_host(self)"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 89,
    "qualname": "TreeNode.get_last_hash_value",
    "signature": "def get_last_hash_value(self)"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 95,
    "qualname": "TreeNode.__lt__",
    "signature": "def __lt__(self, other: 'TreeNode')"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 121,
    "qualname": "RadixCache.__init__",
    "signature": "def __init__(self, req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, page_size: int, disable: bool, enable_kv_cache_events: bool)"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 151,
    "qualname": "RadixCache.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 161,
    "qualname": "RadixCache.match_prefix",
    "signature": "def match_prefix(self, key: List[int])"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 198,
    "qualname": "RadixCache.insert",
    "signature": "def insert(self, key: List, value)"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 206,
    "qualname": "RadixCache.cache_finished_req",
    "signature": "def cache_finished_req(self, req: Req)"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 243,
    "qualname": "RadixCache.cache_unfinished_req",
    "signature": "def cache_unfinished_req(self, req: Req)"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 288,
    "qualname": "RadixCache.pretty_print",
    "signature": "def pretty_print(self)"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 292,
    "qualname": "RadixCache.total_size",
    "signature": "def total_size(self)"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 295,
    "qualname": "RadixCache.evict",
    "signature": "def evict(self, num_tokens: int)"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 320,
    "qualname": "RadixCache.inc_lock_ref",
    "signature": "def inc_lock_ref(self, node: TreeNode)"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 334,
    "qualname": "RadixCache.dec_lock_ref",
    "signature": "def dec_lock_ref(self, node: TreeNode)"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 348,
    "qualname": "RadixCache.evictable_size",
    "signature": "def evictable_size(self)"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 351,
    "qualname": "RadixCache.protected_size",
    "signature": "def protected_size(self)"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 355,
    "qualname": "RadixCache.all_values_flatten",
    "signature": "def all_values_flatten(self)"
  },
  {
    "module": "srt.mem_cache.radix_cache",
    "file": "python/sglang/srt/mem_cache/radix_cache.py",
    "line": 542,
    "qualname": "RadixCache.take_events",
    "signature": "def take_events(self)"
  },
  {
    "module": "srt.mem_cache.radix_cache_cpp",
    "file": "python/sglang/srt/mem_cache/radix_cache_cpp.py",
    "line": 40,
    "qualname": "RadixCacheCpp.__init__",
    "signature": "def __init__(self, disable: bool, use_hicache: bool, req_to_token_pool: ReqToTokenPool, token_to_kv_pool: BaseTokenToKVPoolAllocator, tp_cache_group: torch.distributed.ProcessGroup, page_size: int, hicache_ratio: float, hicache_size: int, hicache_write_policy: str, enable_kv_cache_events: bool, hicache_oracle: bool, enable_write_cancel: bool)"
  },
  {
    "module": "srt.mem_cache.radix_cache_cpp",
    "file": "python/sglang/srt/mem_cache/radix_cache_cpp.py",
    "line": 90,
    "qualname": "RadixCacheCpp.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.mem_cache.radix_cache_cpp",
    "file": "python/sglang/srt/mem_cache/radix_cache_cpp.py",
    "line": 96,
    "qualname": "RadixCacheCpp.match_prefix",
    "signature": "def match_prefix(self, key: List[int])"
  },
  {
    "module": "srt.mem_cache.radix_cache_cpp",
    "file": "python/sglang/srt/mem_cache/radix_cache_cpp.py",
    "line": 123,
    "qualname": "RadixCacheCpp.dec_lock_ref",
    "signature": "def dec_lock_ref(self, node: TreeNodeCpp)"
  },
  {
    "module": "srt.mem_cache.radix_cache_cpp",
    "file": "python/sglang/srt/mem_cache/radix_cache_cpp.py",
    "line": 131,
    "qualname": "RadixCacheCpp.inc_lock_ref",
    "signature": "def inc_lock_ref(self, node: TreeNodeCpp)"
  },
  {
    "module": "srt.mem_cache.radix_cache_cpp",
    "file": "python/sglang/srt/mem_cache/radix_cache_cpp.py",
    "line": 139,
    "qualname": "RadixCacheCpp.evict",
    "signature": "def evict(self, num_tokens: int)"
  },
  {
    "module": "srt.mem_cache.radix_cache_cpp",
    "file": "python/sglang/srt/mem_cache/radix_cache_cpp.py",
    "line": 144,
    "qualname": "RadixCacheCpp.evictable_size",
    "signature": "def evictable_size(self)"
  },
  {
    "module": "srt.mem_cache.radix_cache_cpp",
    "file": "python/sglang/srt/mem_cache/radix_cache_cpp.py",
    "line": 147,
    "qualname": "RadixCacheCpp.protected_size",
    "signature": "def protected_size(self)"
  },
  {
    "module": "srt.mem_cache.radix_cache_cpp",
    "file": "python/sglang/srt/mem_cache/radix_cache_cpp.py",
    "line": 150,
    "qualname": "RadixCacheCpp.total_size",
    "signature": "def total_size(self)"
  },
  {
    "module": "srt.mem_cache.radix_cache_cpp",
    "file": "python/sglang/srt/mem_cache/radix_cache_cpp.py",
    "line": 153,
    "qualname": "RadixCacheCpp.cache_finished_req",
    "signature": "def cache_finished_req(self, req: Req)"
  },
  {
    "module": "srt.mem_cache.radix_cache_cpp",
    "file": "python/sglang/srt/mem_cache/radix_cache_cpp.py",
    "line": 184,
    "qualname": "RadixCacheCpp.cache_unfinished_req",
    "signature": "def cache_unfinished_req(self, req: Req)"
  },
  {
    "module": "srt.mem_cache.radix_cache_cpp",
    "file": "python/sglang/srt/mem_cache/radix_cache_cpp.py",
    "line": 228,
    "qualname": "RadixCacheCpp.pretty_print",
    "signature": "def pretty_print(self)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 47,
    "qualname": "TreeNode.__init__",
    "signature": "def __init__(self, id: Optional[int])"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 81,
    "qualname": "TreeNode.evicted",
    "signature": "def evicted(self)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 85,
    "qualname": "TreeNode.backuped",
    "signature": "def backuped(self)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 88,
    "qualname": "TreeNode.__lt__",
    "signature": "def __lt__(self, other: 'TreeNode')"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 113,
    "qualname": "gen_swa_uuid",
    "signature": "def gen_swa_uuid()"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 119,
    "qualname": "LRUList.__init__",
    "signature": "def __init__(self, swa: bool)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 168,
    "qualname": "LRUList.reset_node_mru",
    "signature": "def reset_node_mru(self, node)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 179,
    "qualname": "LRUList.reset_node_and_parents_mru",
    "signature": "def reset_node_and_parents_mru(self, node, root_node)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 196,
    "qualname": "LRUList.insert_mru",
    "signature": "def insert_mru(self, node)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 209,
    "qualname": "LRUList.remove_node",
    "signature": "def remove_node(self, node: TreeNode)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 220,
    "qualname": "LRUList.get_lru_no_lock",
    "signature": "def get_lru_no_lock(self)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 226,
    "qualname": "LRUList.get_leaf_lru_no_lock",
    "signature": "def get_leaf_lru_no_lock(self)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 232,
    "qualname": "LRUList.get_prev_no_lock",
    "signature": "def get_prev_no_lock(self, node: TreeNode, check_id: bool)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 250,
    "qualname": "LRUList.get_prev_leaf_no_lock",
    "signature": "def get_prev_leaf_no_lock(self, node: TreeNode, check_id: bool)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 266,
    "qualname": "LRUList.in_list",
    "signature": "def in_list(self, node: Optional[TreeNode])"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 275,
    "qualname": "LRUList.sanity_check_evictable_size",
    "signature": "def sanity_check_evictable_size(self)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 287,
    "qualname": "LRUList.sanity_check",
    "signature": "def sanity_check(self, tree_cache: 'SWARadixCache')"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 340,
    "qualname": "SWARadixCache.__init__",
    "signature": "def __init__(self, req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: SWATokenToKVPoolAllocator, sliding_window_size: int, page_size: int, disable: bool)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 371,
    "qualname": "SWARadixCache.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 385,
    "qualname": "SWARadixCache.match_prefix",
    "signature": "def match_prefix(self, key: List[int])"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 422,
    "qualname": "SWARadixCache.insert",
    "signature": "def insert(self, key: List, value, prev_prefix_len: int)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 430,
    "qualname": "SWARadixCache.cache_finished_req",
    "signature": "def cache_finished_req(self, req: Req)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 467,
    "qualname": "SWARadixCache.cache_unfinished_req",
    "signature": "def cache_unfinished_req(self, req: Req)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 521,
    "qualname": "SWARadixCache.pretty_print",
    "signature": "def pretty_print(self)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 526,
    "qualname": "SWARadixCache.total_size",
    "signature": "def total_size(self)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 529,
    "qualname": "SWARadixCache.evict",
    "signature": "def evict(self, full_num_tokens: int, swa_num_tokens: int)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 612,
    "qualname": "SWARadixCache.inc_lock_ref",
    "signature": "def inc_lock_ref(self, node: TreeNode)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 653,
    "qualname": "SWARadixCache.dec_lock_ref",
    "signature": "def dec_lock_ref(self, node: TreeNode, swa_uuid_for_lock: Optional[int])"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 690,
    "qualname": "SWARadixCache.sanity_check",
    "signature": "def sanity_check(self)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 694,
    "qualname": "SWARadixCache.evictable_size",
    "signature": "def evictable_size(self)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 698,
    "qualname": "SWARadixCache.full_evictable_size",
    "signature": "def full_evictable_size(self)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 701,
    "qualname": "SWARadixCache.swa_evictable_size",
    "signature": "def swa_evictable_size(self)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 705,
    "qualname": "SWARadixCache.full_lru_list_evictable_size",
    "signature": "def full_lru_list_evictable_size(self)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 709,
    "qualname": "SWARadixCache.swa_lru_list_evictable_size",
    "signature": "def swa_lru_list_evictable_size(self)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 712,
    "qualname": "SWARadixCache.protected_size",
    "signature": "def protected_size(self)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 716,
    "qualname": "SWARadixCache.full_protected_size",
    "signature": "def full_protected_size(self)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 720,
    "qualname": "SWARadixCache.swa_protected_size",
    "signature": "def swa_protected_size(self)"
  },
  {
    "module": "srt.mem_cache.swa_radix_cache",
    "file": "python/sglang/srt/mem_cache/swa_radix_cache.py",
    "line": 724,
    "qualname": "SWARadixCache.all_values_flatten",
    "signature": "def all_values_flatten(self)"
  },
  {
    "module": "srt.metrics.collector",
    "file": "python/sglang/srt/metrics/collector.py",
    "line": 51,
    "qualname": "TimeStats.__str__",
    "signature": "def __str__(self)"
  },
  {
    "module": "srt.metrics.collector",
    "file": "python/sglang/srt/metrics/collector.py",
    "line": 106,
    "qualname": "TimeStats.format_duration",
    "signature": "def format_duration(self, duration: float)"
  },
  {
    "module": "srt.metrics.collector",
    "file": "python/sglang/srt/metrics/collector.py",
    "line": 109,
    "qualname": "TimeStats.get_type",
    "signature": "def get_type(self)"
  },
  {
    "module": "srt.metrics.collector",
    "file": "python/sglang/srt/metrics/collector.py",
    "line": 153,
    "qualname": "SchedulerMetricsCollector.__init__",
    "signature": "def __init__(self, labels: Dict[str, str])"
  },
  {
    "module": "srt.metrics.collector",
    "file": "python/sglang/srt/metrics/collector.py",
    "line": 275,
    "qualname": "SchedulerMetricsCollector.increment_bootstrap_failed_reqs",
    "signature": "def increment_bootstrap_failed_reqs(self)"
  },
  {
    "module": "srt.metrics.collector",
    "file": "python/sglang/srt/metrics/collector.py",
    "line": 278,
    "qualname": "SchedulerMetricsCollector.increment_transfer_failed_reqs",
    "signature": "def increment_transfer_failed_reqs(self)"
  },
  {
    "module": "srt.metrics.collector",
    "file": "python/sglang/srt/metrics/collector.py",
    "line": 281,
    "qualname": "SchedulerMetricsCollector.log_stats",
    "signature": "def log_stats(self, stats: SchedulerStats)"
  },
  {
    "module": "srt.metrics.collector",
    "file": "python/sglang/srt/metrics/collector.py",
    "line": 310,
    "qualname": "TokenizerMetricsCollector.__init__",
    "signature": "def __init__(self, labels: Dict[str, str], bucket_time_to_first_token: Optional[List[float]], bucket_inter_token_latency: Optional[List[float]], bucket_e2e_request_latency: Optional[List[float]], collect_tokens_histogram: bool)"
  },
  {
    "module": "srt.metrics.collector",
    "file": "python/sglang/srt/metrics/collector.py",
    "line": 516,
    "qualname": "TokenizerMetricsCollector.observe_one_finished_request",
    "signature": "def observe_one_finished_request(self, prompt_tokens: int, generation_tokens: int, cached_tokens: int, e2e_latency: float, has_grammar: bool)"
  },
  {
    "module": "srt.metrics.collector",
    "file": "python/sglang/srt/metrics/collector.py",
    "line": 536,
    "qualname": "TokenizerMetricsCollector.observe_time_to_first_token",
    "signature": "def observe_time_to_first_token(self, value: float)"
  },
  {
    "module": "srt.metrics.collector",
    "file": "python/sglang/srt/metrics/collector.py",
    "line": 539,
    "qualname": "TokenizerMetricsCollector.observe_inter_token_latency",
    "signature": "def observe_inter_token_latency(self, internval: float, num_new_tokens: int)"
  },
  {
    "module": "srt.metrics.collector",
    "file": "python/sglang/srt/metrics/collector.py",
    "line": 552,
    "qualname": "TokenizerMetricsCollector.observe_one_aborted_request",
    "signature": "def observe_one_aborted_request(self)"
  },
  {
    "module": "srt.metrics.func_timer",
    "file": "python/sglang/srt/metrics/func_timer.py",
    "line": 26,
    "qualname": "enable_func_timer",
    "signature": "def enable_func_timer()"
  },
  {
    "module": "srt.metrics.func_timer",
    "file": "python/sglang/srt/metrics/func_timer.py",
    "line": 45,
    "qualname": "exponential_buckets",
    "signature": "def exponential_buckets(start: float, width: float, length: int)"
  },
  {
    "module": "srt.metrics.func_timer",
    "file": "python/sglang/srt/metrics/func_timer.py",
    "line": 52,
    "qualname": "time_func_latency",
    "signature": "def time_func_latency(func: Callable, name: Optional[str])"
  },
  {
    "module": "srt.model_executor.cuda_graph_runner",
    "file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
    "line": 73,
    "qualname": "get_is_capture_mode",
    "signature": "def get_is_capture_mode()"
  },
  {
    "module": "srt.model_executor.cuda_graph_runner",
    "file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
    "line": 78,
    "qualname": "model_capture_mode",
    "signature": "def model_capture_mode()"
  },
  {
    "module": "srt.model_executor.cuda_graph_runner",
    "file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
    "line": 88,
    "qualname": "freeze_gc",
    "signature": "def freeze_gc(enable_cudagraph_gc: bool)"
  },
  {
    "module": "srt.model_executor.cuda_graph_runner",
    "file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
    "line": 117,
    "qualname": "patch_model",
    "signature": "def patch_model(model: torch.nn.Module, enable_compile: bool, num_tokens: int, tp_group: GroupCoordinator)"
  },
  {
    "module": "srt.model_executor.cuda_graph_runner",
    "file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
    "line": 149,
    "qualname": "set_torch_compile_config",
    "signature": "def set_torch_compile_config()"
  },
  {
    "module": "srt.model_executor.cuda_graph_runner",
    "file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
    "line": 165,
    "qualname": "get_batch_sizes_to_capture",
    "signature": "def get_batch_sizes_to_capture(model_runner: ModelRunner)"
  },
  {
    "module": "srt.model_executor.cuda_graph_runner",
    "file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
    "line": 228,
    "qualname": "get_global_graph_memory_pool",
    "signature": "def get_global_graph_memory_pool()"
  },
  {
    "module": "srt.model_executor.cuda_graph_runner",
    "file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
    "line": 232,
    "qualname": "set_global_graph_memory_pool",
    "signature": "def set_global_graph_memory_pool(val)"
  },
  {
    "module": "srt.model_executor.cuda_graph_runner",
    "file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
    "line": 240,
    "qualname": "CudaGraphRunner.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner)"
  },
  {
    "module": "srt.model_executor.cuda_graph_runner",
    "file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
    "line": 393,
    "qualname": "CudaGraphRunner.can_run",
    "signature": "def can_run(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.model_executor.cuda_graph_runner",
    "file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
    "line": 445,
    "qualname": "CudaGraphRunner.capture",
    "signature": "def capture(self)"
  },
  {
    "module": "srt.model_executor.cuda_graph_runner",
    "file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
    "line": 520,
    "qualname": "CudaGraphRunner.capture_one_batch_size",
    "signature": "def capture_one_batch_size(self, bs: int, forward: Callable)"
  },
  {
    "module": "srt.model_executor.cuda_graph_runner",
    "file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
    "line": 675,
    "qualname": "CudaGraphRunner.recapture_if_needed",
    "signature": "def recapture_if_needed(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.model_executor.cuda_graph_runner",
    "file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
    "line": 706,
    "qualname": "CudaGraphRunner.replay_prepare",
    "signature": "def replay_prepare(self, forward_batch: ForwardBatch, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.model_executor.cuda_graph_runner",
    "file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
    "line": 796,
    "qualname": "CudaGraphRunner.replay",
    "signature": "def replay(self, forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.model_executor.cuda_graph_runner",
    "file": "python/sglang/srt/model_executor/cuda_graph_runner.py",
    "line": 826,
    "qualname": "CudaGraphRunner.get_spec_info",
    "signature": "def get_spec_info(self, num_tokens: int)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 92,
    "qualname": "ForwardMode.is_prefill",
    "signature": "def is_prefill(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 95,
    "qualname": "ForwardMode.is_extend",
    "signature": "def is_extend(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 103,
    "qualname": "ForwardMode.is_decode",
    "signature": "def is_decode(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 106,
    "qualname": "ForwardMode.is_mixed",
    "signature": "def is_mixed(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 109,
    "qualname": "ForwardMode.is_idle",
    "signature": "def is_idle(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 112,
    "qualname": "ForwardMode.is_decode_or_idle",
    "signature": "def is_decode_or_idle(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 115,
    "qualname": "ForwardMode.is_target_verify",
    "signature": "def is_target_verify(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 118,
    "qualname": "ForwardMode.is_draft_extend",
    "signature": "def is_draft_extend(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 121,
    "qualname": "ForwardMode.is_extend_or_draft_extend_or_mixed",
    "signature": "def is_extend_or_draft_extend_or_mixed(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 128,
    "qualname": "ForwardMode.is_cuda_graph",
    "signature": "def is_cuda_graph(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 135,
    "qualname": "ForwardMode.is_dummy_first",
    "signature": "def is_dummy_first(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 138,
    "qualname": "ForwardMode.is_split_prefill",
    "signature": "def is_split_prefill(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 151,
    "qualname": "CaptureHiddenMode.need_capture",
    "signature": "def need_capture(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 154,
    "qualname": "CaptureHiddenMode.is_full",
    "signature": "def is_full(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 157,
    "qualname": "CaptureHiddenMode.is_last",
    "signature": "def is_last(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 160,
    "qualname": "CaptureHiddenMode.__lt__",
    "signature": "def __lt__(self, other)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 310,
    "qualname": "ForwardBatch.init_new",
    "signature": "def init_new(cls, batch: ModelWorkerBatch, model_runner: ModelRunner)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 456,
    "qualname": "ForwardBatch.merge_mm_inputs",
    "signature": "def merge_mm_inputs(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 479,
    "qualname": "ForwardBatch.contains_image_inputs",
    "signature": "def contains_image_inputs(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 487,
    "qualname": "ForwardBatch.contains_audio_inputs",
    "signature": "def contains_audio_inputs(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 495,
    "qualname": "ForwardBatch.contains_video_inputs",
    "signature": "def contains_video_inputs(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 503,
    "qualname": "ForwardBatch.contains_mm_inputs",
    "signature": "def contains_mm_inputs(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 568,
    "qualname": "ForwardBatch.get_max_chunk_capacity",
    "signature": "def get_max_chunk_capacity(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 573,
    "qualname": "ForwardBatch.set_prefix_chunk_idx",
    "signature": "def set_prefix_chunk_idx(self, idx: int)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 576,
    "qualname": "ForwardBatch.set_attn_attend_prefix_cache",
    "signature": "def set_attn_attend_prefix_cache(self, attn_attend_prefix_cache: bool)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 579,
    "qualname": "ForwardBatch.prepare_chunked_kv_indices",
    "signature": "def prepare_chunked_kv_indices(self, device: torch.device)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 617,
    "qualname": "ForwardBatch.prepare_mlp_sync_batch",
    "signature": "def prepare_mlp_sync_batch(self, model_runner: ModelRunner)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 733,
    "qualname": "ForwardBatch.post_forward_mlp_sync_batch",
    "signature": "def post_forward_mlp_sync_batch(self, logits_output: LogitsProcessorOutput)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 789,
    "qualname": "ForwardBatch.get_prefix_chunk_seq_lens",
    "signature": "def get_prefix_chunk_seq_lens(self, prefix_lens: torch.Tensor, num_prefix_chunks: int, prefix_chunk_len: int)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 812,
    "qualname": "ForwardBatch.prepare_chunked_prefix_cache_info",
    "signature": "def prepare_chunked_prefix_cache_info(self, device: torch.device)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 871,
    "qualname": "ForwardBatch.can_run_tbo",
    "signature": "def can_run_tbo(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 875,
    "qualname": "enable_num_token_non_padded",
    "signature": "def enable_num_token_non_padded(server_args)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 883,
    "qualname": "PPProxyTensors.__init__",
    "signature": "def __init__(self, tensors)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 890,
    "qualname": "PPProxyTensors.__getitem__",
    "signature": "def __getitem__(self, key: Union[str, slice])"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 896,
    "qualname": "PPProxyTensors.__setitem__",
    "signature": "def __setitem__(self, key: str, value: torch.Tensor)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 899,
    "qualname": "PPProxyTensors.__len__",
    "signature": "def __len__(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 902,
    "qualname": "PPProxyTensors.__eq__",
    "signature": "def __eq__(self, other: object)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 905,
    "qualname": "PPProxyTensors.__repr__",
    "signature": "def __repr__(self)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 909,
    "qualname": "compute_position",
    "signature": "def compute_position(attn_backend: str, extend_prefix_lens: torch.Tensor, extend_seq_lens: torch.Tensor, extend_seq_lens_sum: int)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 928,
    "qualname": "compute_position_triton",
    "signature": "def compute_position_triton(extend_prefix_lens: torch.Tensor, extend_seq_lens: torch.Tensor, extend_seq_lens_sum)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 955,
    "qualname": "compute_position_kernel",
    "signature": "def compute_position_kernel(positions, extend_start_loc, extend_prefix_lens, extend_seq_lens, has_prefix: tl.constexpr)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 984,
    "qualname": "compute_position_torch",
    "signature": "def compute_position_torch(extend_prefix_lens: torch.Tensor, extend_seq_lens: torch.Tensor)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 1002,
    "qualname": "clamp_position",
    "signature": "def clamp_position(seq_lens)"
  },
  {
    "module": "srt.model_executor.forward_batch_info",
    "file": "python/sglang/srt/model_executor/forward_batch_info.py",
    "line": 1007,
    "qualname": "create_chunked_prefix_cache_kv_indices",
    "signature": "def create_chunked_prefix_cache_kv_indices(req_to_token_ptr, req_pool_indices_ptr, chunk_start_idx_ptr, chunk_seq_lens_ptr, chunk_cu_seq_lens_ptr, chunk_kv_indices_ptr, req_to_token_ptr_stride: tl.constexpr)"
  },
  {
    "module": "srt.model_executor.npu_graph_runner",
    "file": "python/sglang/srt/model_executor/npu_graph_runner.py",
    "line": 38,
    "qualname": "NPUGraphRunner.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner)"
  },
  {
    "module": "srt.model_executor.npu_graph_runner",
    "file": "python/sglang/srt/model_executor/npu_graph_runner.py",
    "line": 62,
    "qualname": "NPUGraphRunner.replay",
    "signature": "def replay(self, forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 149,
    "qualname": "RankZeroFilter.__init__",
    "signature": "def __init__(self, is_rank_zero)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 153,
    "qualname": "RankZeroFilter.filter",
    "signature": "def filter(self, record)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 162,
    "qualname": "ModelRunner.__init__",
    "signature": "def __init__(self, model_config: ModelConfig, mem_fraction_static: float, gpu_id: int, tp_rank: int, tp_size: int, moe_ep_rank: int, moe_ep_size: int, pp_rank: int, pp_size: int, nccl_port: int, server_args: ServerArgs, dp_rank: Optional[int], is_draft_worker: bool, req_to_token_pool: Optional[ReqToTokenPool], token_to_kv_pool_allocator: Optional[BaseTokenToKVPoolAllocator])"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 255,
    "qualname": "ModelRunner.initialize",
    "signature": "def initialize(self, min_per_gpu_memory: float)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 382,
    "qualname": "ModelRunner.model_specific_adjustment",
    "signature": "def model_specific_adjustment(self)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 551,
    "qualname": "ModelRunner.init_torch_distributed",
    "signature": "def init_torch_distributed(self)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 647,
    "qualname": "ModelRunner.load_model",
    "signature": "def load_model(self)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 754,
    "qualname": "ModelRunner.update_expert_location",
    "signature": "def update_expert_location(self, new_expert_location_metadata: ExpertLocationMetadata, update_layer_ids: List[int])"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 767,
    "qualname": "ModelRunner.update_weights_from_disk",
    "signature": "def update_weights_from_disk(self, model_path: str, load_format: str)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 822,
    "qualname": "ModelRunner.init_weights_update_group",
    "signature": "def init_weights_update_group(self, master_address, master_port, rank_offset, world_size, group_name, backend)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 867,
    "qualname": "ModelRunner.update_weights_from_distributed",
    "signature": "def update_weights_from_distributed(self, names, dtypes, shapes, group_name)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 915,
    "qualname": "ModelRunner.update_weights_from_tensor",
    "signature": "def update_weights_from_tensor(self, named_tensors: List[Tuple[str, Union[torch.Tensor, 'LocalSerializedTensor']]], load_format: Optional[str])"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 978,
    "qualname": "ModelRunner.get_weights_by_name",
    "signature": "def get_weights_by_name(self, name: str, truncate_size: int)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 995,
    "qualname": "ModelRunner.init_lora_manager",
    "signature": "def init_lora_manager(self)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1010,
    "qualname": "ModelRunner.load_lora_adapter",
    "signature": "def load_lora_adapter(self, lora_ref: LoRARef)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1027,
    "qualname": "ModelRunner.unload_lora_adapter",
    "signature": "def unload_lora_adapter(self, lora_ref: LoRARef)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1044,
    "qualname": "ModelRunner.profile_max_num_token",
    "signature": "def profile_max_num_token(self, total_gpu_memory: int)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1079,
    "qualname": "ModelRunner.set_num_token_hybrid",
    "signature": "def set_num_token_hybrid(self)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1161,
    "qualname": "ModelRunner.init_memory_pool",
    "signature": "def init_memory_pool(self, total_gpu_memory: int, max_num_reqs: Optional[int], max_total_tokens: Optional[int])"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1414,
    "qualname": "ModelRunner.init_cublas",
    "signature": "def init_cublas(self)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1423,
    "qualname": "ModelRunner.init_attention_backend",
    "signature": "def init_attention_backend(self)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1585,
    "qualname": "ModelRunner.init_double_sparsity_channel_config",
    "signature": "def init_double_sparsity_channel_config(self, selected_channel)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1602,
    "qualname": "ModelRunner.init_device_graphs",
    "signature": "def init_device_graphs(self)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1629,
    "qualname": "ModelRunner.init_threads_binding",
    "signature": "def init_threads_binding(self)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1653,
    "qualname": "ModelRunner.apply_torch_tp",
    "signature": "def apply_torch_tp(self)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1660,
    "qualname": "ModelRunner.forward_decode",
    "signature": "def forward_decode(self, forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1679,
    "qualname": "ModelRunner.forward_extend",
    "signature": "def forward_extend(self, forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1702,
    "qualname": "ModelRunner.forward_idle",
    "signature": "def forward_idle(self, forward_batch: ForwardBatch, pp_proxy_tensors)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1715,
    "qualname": "ModelRunner.forward_split_prefill",
    "signature": "def forward_split_prefill(self, forward_batch: ForwardBatch, reinit_attn_backend: bool, forward_count: int)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1736,
    "qualname": "ModelRunner.forward",
    "signature": "def forward(self, forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors: Optional[PPProxyTensors], reinit_attn_backend: bool, split_forward_count: int)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1830,
    "qualname": "ModelRunner.sample",
    "signature": "def sample(self, logits_output: LogitsProcessorOutput, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1864,
    "qualname": "ModelRunner.model_is_mrope",
    "signature": "def model_is_mrope(self)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1873,
    "qualname": "ModelRunner.save_remote_model",
    "signature": "def save_remote_model(self, url: str)"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1879,
    "qualname": "ModelRunner.save_sharded_model",
    "signature": "def save_sharded_model(self, path: str, pattern: Optional[str], max_size: Optional[int])"
  },
  {
    "module": "srt.model_executor.model_runner",
    "file": "python/sglang/srt/model_executor/model_runner.py",
    "line": 1909,
    "qualname": "LocalSerializedTensor.get",
    "signature": "def get(self, rank: int)"
  },
  {
    "module": "srt.model_loader.__init__",
    "file": "python/sglang/srt/model_loader/__init__.py",
    "line": 15,
    "qualname": "get_model",
    "signature": "def get_model()"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 51,
    "qualname": "enable_hf_transfer",
    "signature": "def enable_hf_transfer()"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 67,
    "qualname": "DisabledTqdm.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 71,
    "qualname": "get_lock",
    "signature": "def get_lock(model_name_or_path: str, cache_dir: Optional[str])"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 94,
    "qualname": "convert_bin_to_safetensor_file",
    "signature": "def convert_bin_to_safetensor_file(pt_filename: str, sf_filename: str)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 134,
    "qualname": "get_quant_config",
    "signature": "def get_quant_config(model_config: ModelConfig, load_config: LoadConfig, packed_modules_mapping: Dict[str, List[str]])"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 238,
    "qualname": "download_weights_from_hf",
    "signature": "def download_weights_from_hf(model_name_or_path: str, cache_dir: Optional[str], allow_patterns: List[str], revision: Optional[str], ignore_patterns: Optional[Union[str, List[str]]])"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 290,
    "qualname": "download_safetensors_index_file_from_hf",
    "signature": "def download_safetensors_index_file_from_hf(model_name_or_path: str, index_file: str, cache_dir: Optional[str], revision: Optional[str])"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 329,
    "qualname": "filter_duplicate_safetensors_files",
    "signature": "def filter_duplicate_safetensors_files(hf_weights_files: List[str], hf_folder: str, index_file: str)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 350,
    "qualname": "filter_files_not_needed_for_inference",
    "signature": "def filter_files_not_needed_for_inference(hf_weights_files: List[str])"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 376,
    "qualname": "np_cache_weights_iterator",
    "signature": "def np_cache_weights_iterator(model_name_or_path: str, cache_dir: Optional[str], hf_folder: str, hf_weights_files: List[str])"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 424,
    "qualname": "decrypt",
    "signature": "def decrypt(fn, key)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 428,
    "qualname": "safetensors_encrypted_weights_iterator",
    "signature": "def safetensors_encrypted_weights_iterator(hf_weights_files: List[str], is_all_weights_sharded: bool, decryption_key: Optional[str])"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 436,
    "qualname": "safetensors_weights_iterator",
    "signature": "def safetensors_weights_iterator(hf_weights_files: List[str], is_all_weights_sharded: bool, decryption_key: Optional[str], disable_mmap: bool)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 473,
    "qualname": "multi_thread_safetensors_weights_iterator",
    "signature": "def multi_thread_safetensors_weights_iterator(hf_weights_files: List[str], is_all_weights_sharded: bool, decryption_key: Optional[str], max_workers: int, disable_mmap: bool)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 528,
    "qualname": "pt_weights_iterator",
    "signature": "def pt_weights_iterator(hf_weights_files: List[str])"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 546,
    "qualname": "multi_thread_pt_weights_iterator",
    "signature": "def multi_thread_pt_weights_iterator(hf_weights_files: List[str], max_workers: int)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 579,
    "qualname": "get_gguf_extra_tensor_names",
    "signature": "def get_gguf_extra_tensor_names(gguf_file: str, gguf_to_hf_name_map: Dict[str, str])"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 591,
    "qualname": "gguf_quant_weights_iterator",
    "signature": "def gguf_quant_weights_iterator(gguf_file: str, gguf_to_hf_name_map: Dict[str, str])"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 625,
    "qualname": "convert_pyslice_to_tensor",
    "signature": "def convert_pyslice_to_tensor(x: Any)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 640,
    "qualname": "default_weight_loader",
    "signature": "def default_weight_loader(param: torch.Tensor, loaded_weight: torch.Tensor)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 661,
    "qualname": "row_parallel_weight_loader",
    "signature": "def row_parallel_weight_loader(param: torch.Tensor, loaded_weight: torch.Tensor)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 679,
    "qualname": "sharded_weight_loader",
    "signature": "def sharded_weight_loader(shard_axis: int)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 694,
    "qualname": "composed_weight_loader",
    "signature": "def composed_weight_loader(loader: LoaderFunction, fn: Callable[[torch.Tensor], torch.Tensor])"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 707,
    "qualname": "runai_safetensors_weights_iterator",
    "signature": "def runai_safetensors_weights_iterator(hf_weights_files: List[str])"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 728,
    "qualname": "set_runai_streamer_env",
    "signature": "def set_runai_streamer_env(load_config: LoadConfig)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 752,
    "qualname": "initialize_dummy_weights",
    "signature": "def initialize_dummy_weights(model: torch.nn.Module, low: float, high: float, seed: int)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 784,
    "qualname": "maybe_remap_kv_scale_name",
    "signature": "def maybe_remap_kv_scale_name(name: str, params_dict: dict)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 870,
    "qualname": "KVCacheQuantSchema.check_is_fp8",
    "signature": "def check_is_fp8(self)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 878,
    "qualname": "KVCacheQuantSchema.check_tp_ranks",
    "signature": "def check_tp_ranks(self, info: ValidationInfo)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 900,
    "qualname": "KVCacheQuantSchema.check_current_rank",
    "signature": "def check_current_rank(self, info: ValidationInfo)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 922,
    "qualname": "QuantParamSchema.check_model_type",
    "signature": "def check_model_type(self, info: ValidationInfo)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 935,
    "qualname": "kv_cache_scales_loader",
    "signature": "def kv_cache_scales_loader(filename: str, tp_rank: int, tp_size: int, num_hidden_layers: int, model_type: Optional[str])"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 978,
    "qualname": "get_actual_shard_size",
    "signature": "def get_actual_shard_size(shard_size, weight_start, weight_end)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 985,
    "qualname": "reset_param_data_if_needed",
    "signature": "def reset_param_data_if_needed(param_data, dim, start, length)"
  },
  {
    "module": "srt.model_loader.weight_utils",
    "file": "python/sglang/srt/model_loader/weight_utils.py",
    "line": 995,
    "qualname": "narrow_padded_param_and_loaded_weight",
    "signature": "def narrow_padded_param_and_loaded_weight(param_data, loaded_weight, param_data_start, weight_start, dim, shard_size, narrow_weight)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 76,
    "qualname": "device_loading_context",
    "signature": "def device_loading_context(module: torch.nn.Module, target_device: torch.device)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 213,
    "qualname": "BaseModelLoader.__init__",
    "signature": "def __init__(self, load_config: LoadConfig)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 217,
    "qualname": "BaseModelLoader.download_model",
    "signature": "def download_model(self, model_config: ModelConfig)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 222,
    "qualname": "BaseModelLoader.load_model",
    "signature": "def load_model(self)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 255,
    "qualname": "Source.init_new",
    "signature": "def init_new(cls, model_config: ModelConfig, model)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 263,
    "qualname": "DefaultModelLoader.__init__",
    "signature": "def __init__(self, load_config: LoadConfig)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 448,
    "qualname": "DefaultModelLoader.download_model",
    "signature": "def download_model(self, model_config: ModelConfig)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 453,
    "qualname": "DefaultModelLoader.load_model",
    "signature": "def load_model(self)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 474,
    "qualname": "DefaultModelLoader.load_weights_and_postprocess",
    "signature": "def load_weights_and_postprocess(model, weights, target_device)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 493,
    "qualname": "LayeredModelLoader.__init__",
    "signature": "def __init__(self, load_config: LoadConfig)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 498,
    "qualname": "LayeredModelLoader.load_model",
    "signature": "def load_model(self)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 564,
    "qualname": "DummyModelLoader.__init__",
    "signature": "def __init__(self, load_config: LoadConfig)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 572,
    "qualname": "DummyModelLoader.download_model",
    "signature": "def download_model(self, model_config: ModelConfig)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 575,
    "qualname": "DummyModelLoader.load_model",
    "signature": "def load_model(self)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 629,
    "qualname": "ShardedStateLoader.__init__",
    "signature": "def __init__(self, load_config: LoadConfig)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 693,
    "qualname": "ShardedStateLoader.download_model",
    "signature": "def download_model(self, model_config: ModelConfig)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 696,
    "qualname": "ShardedStateLoader.load_model",
    "signature": "def load_model(self)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 757,
    "qualname": "ShardedStateLoader.save_model",
    "signature": "def save_model(model: torch.nn.Module, path: str, pattern: Optional[str], max_size: Optional[int])"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 818,
    "qualname": "BitsAndBytesModelLoader.__init__",
    "signature": "def __init__(self, load_config: LoadConfig)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 1250,
    "qualname": "BitsAndBytesModelLoader.download_model",
    "signature": "def download_model(self, model_config: ModelConfig)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 1253,
    "qualname": "BitsAndBytesModelLoader.load_model",
    "signature": "def load_model(self)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 1278,
    "qualname": "GGUFModelLoader.__init__",
    "signature": "def __init__(self, load_config: LoadConfig)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 1342,
    "qualname": "GGUFModelLoader.download_model",
    "signature": "def download_model(self, model_config: ModelConfig)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 1345,
    "qualname": "GGUFModelLoader.load_model",
    "signature": "def load_model(self)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 1379,
    "qualname": "RemoteModelLoader.__init__",
    "signature": "def __init__(self, load_config: LoadConfig)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 1401,
    "qualname": "RemoteModelLoader.download_model",
    "signature": "def download_model(self, model_config: ModelConfig)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 1405,
    "qualname": "RemoteModelLoader.save_model",
    "signature": "def save_model(model: torch.nn.Module, model_path: str, url: str)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 1482,
    "qualname": "RemoteModelLoader.load_model",
    "signature": "def load_model(self)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 1523,
    "qualname": "load_model_with_cpu_quantization",
    "signature": "def load_model_with_cpu_quantization(self)"
  },
  {
    "module": "srt.model_loader.loader",
    "file": "python/sglang/srt/model_loader/loader.py",
    "line": 1555,
    "qualname": "get_model_loader",
    "signature": "def get_model_loader(load_config: LoadConfig)"
  },
  {
    "module": "srt.model_loader.utils",
    "file": "python/sglang/srt/model_loader/utils.py",
    "line": 19,
    "qualname": "set_default_torch_dtype",
    "signature": "def set_default_torch_dtype(dtype: torch.dtype)"
  },
  {
    "module": "srt.model_loader.utils",
    "file": "python/sglang/srt/model_loader/utils.py",
    "line": 27,
    "qualname": "resolve_transformers_arch",
    "signature": "def resolve_transformers_arch(model_config: ModelConfig, architectures: list[str])"
  },
  {
    "module": "srt.model_loader.utils",
    "file": "python/sglang/srt/model_loader/utils.py",
    "line": 82,
    "qualname": "get_model_architecture",
    "signature": "def get_model_architecture(model_config: ModelConfig)"
  },
  {
    "module": "srt.model_loader.utils",
    "file": "python/sglang/srt/model_loader/utils.py",
    "line": 106,
    "qualname": "get_architecture_class_name",
    "signature": "def get_architecture_class_name(model_config: ModelConfig)"
  },
  {
    "module": "srt.models.arcee",
    "file": "python/sglang/srt/models/arcee.py",
    "line": 63,
    "qualname": "ArceeMLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str, reduce_results: bool)"
  },
  {
    "module": "srt.models.arcee",
    "file": "python/sglang/srt/models/arcee.py",
    "line": 97,
    "qualname": "ArceeMLP.forward",
    "signature": "def forward(self, x, forward_batch)"
  },
  {
    "module": "srt.models.arcee",
    "file": "python/sglang/srt/models/arcee.py",
    "line": 105,
    "qualname": "ArceeAttention.__init__",
    "signature": "def __init__(self, config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str, bias: bool)"
  },
  {
    "module": "srt.models.arcee",
    "file": "python/sglang/srt/models/arcee.py",
    "line": 178,
    "qualname": "ArceeAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.arcee",
    "file": "python/sglang/srt/models/arcee.py",
    "line": 193,
    "qualname": "ArceeDecoderLayer.__init__",
    "signature": "def __init__(self, config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.arcee",
    "file": "python/sglang/srt/models/arcee.py",
    "line": 241,
    "qualname": "ArceeDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.arcee",
    "file": "python/sglang/srt/models/arcee.py",
    "line": 267,
    "qualname": "ArceeModel.__init__",
    "signature": "def __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.arcee",
    "file": "python/sglang/srt/models/arcee.py",
    "line": 304,
    "qualname": "ArceeModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.arcee",
    "file": "python/sglang/srt/models/arcee.py",
    "line": 350,
    "qualname": "ArceeModel.load_kv_cache_scales",
    "signature": "def load_kv_cache_scales(self, quantization_param_path: str)"
  },
  {
    "module": "srt.models.arcee",
    "file": "python/sglang/srt/models/arcee.py",
    "line": 393,
    "qualname": "ArceeForCausalLM.__init__",
    "signature": "def __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.arcee",
    "file": "python/sglang/srt/models/arcee.py",
    "line": 432,
    "qualname": "ArceeForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.arcee",
    "file": "python/sglang/srt/models/arcee.py",
    "line": 468,
    "qualname": "ArceeForCausalLM.start_layer",
    "signature": "def start_layer(self)"
  },
  {
    "module": "srt.models.arcee",
    "file": "python/sglang/srt/models/arcee.py",
    "line": 472,
    "qualname": "ArceeForCausalLM.end_layer",
    "signature": "def end_layer(self)"
  },
  {
    "module": "srt.models.arcee",
    "file": "python/sglang/srt/models/arcee.py",
    "line": 475,
    "qualname": "ArceeForCausalLM.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.arcee",
    "file": "python/sglang/srt/models/arcee.py",
    "line": 478,
    "qualname": "ArceeForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.arcee",
    "file": "python/sglang/srt/models/arcee.py",
    "line": 528,
    "qualname": "ArceeForCausalLM.load_kv_cache_scales",
    "signature": "def load_kv_cache_scales(self, quantization_param_path: str)"
  },
  {
    "module": "srt.models.baichuan",
    "file": "python/sglang/srt/models/baichuan.py",
    "line": 78,
    "qualname": "BaiChuanMLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.baichuan",
    "file": "python/sglang/srt/models/baichuan.py",
    "line": 108,
    "qualname": "BaiChuanMLP.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.baichuan",
    "file": "python/sglang/srt/models/baichuan.py",
    "line": 118,
    "qualname": "BaiChuanAttention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_heads: int, position_embedding: str, rope_theta: float, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], layer_id: int, prefix: str)"
  },
  {
    "module": "srt.models.baichuan",
    "file": "python/sglang/srt/models/baichuan.py",
    "line": 202,
    "qualname": "BaiChuanAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.baichuan",
    "file": "python/sglang/srt/models/baichuan.py",
    "line": 219,
    "qualname": "BaiChuanDecoderLayer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, position_embedding: str, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.baichuan",
    "file": "python/sglang/srt/models/baichuan.py",
    "line": 253,
    "qualname": "BaiChuanDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.baichuan",
    "file": "python/sglang/srt/models/baichuan.py",
    "line": 280,
    "qualname": "BaiChuanModel.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, position_embedding: str, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.baichuan",
    "file": "python/sglang/srt/models/baichuan.py",
    "line": 310,
    "qualname": "BaiChuanModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.baichuan",
    "file": "python/sglang/srt/models/baichuan.py",
    "line": 348,
    "qualname": "BaiChuanBaseForCausalLM.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, position_embedding: str, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.baichuan",
    "file": "python/sglang/srt/models/baichuan.py",
    "line": 374,
    "qualname": "BaiChuanBaseForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.baichuan",
    "file": "python/sglang/srt/models/baichuan.py",
    "line": 385,
    "qualname": "BaiChuanBaseForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.baichuan",
    "file": "python/sglang/srt/models/baichuan.py",
    "line": 429,
    "qualname": "BaichuanForCausalLM.__init__",
    "signature": "def __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.bailing_moe",
    "file": "python/sglang/srt/models/bailing_moe.py",
    "line": 41,
    "qualname": "BailingAttention.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.bailing_moe",
    "file": "python/sglang/srt/models/bailing_moe.py",
    "line": 103,
    "qualname": "BailingAttention.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.bailing_moe",
    "file": "python/sglang/srt/models/bailing_moe.py",
    "line": 119,
    "qualname": "BailingMLP.__init__",
    "signature": "def __init__(self, intermediate_size: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], reduce_results: Optional[bool], prefix: str)"
  },
  {
    "module": "srt.models.bailing_moe",
    "file": "python/sglang/srt/models/bailing_moe.py",
    "line": 145,
    "qualname": "BailingMLP.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.bailing_moe",
    "file": "python/sglang/srt/models/bailing_moe.py",
    "line": 154,
    "qualname": "BailingMoE.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.bailing_moe",
    "file": "python/sglang/srt/models/bailing_moe.py",
    "line": 201,
    "qualname": "BailingMoE.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.bailing_moe",
    "file": "python/sglang/srt/models/bailing_moe.py",
    "line": 224,
    "qualname": "BailingMoeBlock.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.bailing_moe",
    "file": "python/sglang/srt/models/bailing_moe.py",
    "line": 246,
    "qualname": "BailingMoeBlock.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, position_ids: torch.Tensor, residual: Optional[torch.Tensor], forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.bailing_moe",
    "file": "python/sglang/srt/models/bailing_moe.py",
    "line": 279,
    "qualname": "BailingMoeModel.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.bailing_moe",
    "file": "python/sglang/srt/models/bailing_moe.py",
    "line": 311,
    "qualname": "BailingMoeModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.bailing_moe",
    "file": "python/sglang/srt/models/bailing_moe.py",
    "line": 338,
    "qualname": "BailingMoeForCausalLM.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig])"
  },
  {
    "module": "srt.models.bailing_moe",
    "file": "python/sglang/srt/models/bailing_moe.py",
    "line": 356,
    "qualname": "BailingMoeForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.bailing_moe",
    "file": "python/sglang/srt/models/bailing_moe.py",
    "line": 368,
    "qualname": "BailingMoeForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 27,
    "qualname": "BertEmbedding.__init__",
    "signature": "def __init__(self, config: BertConfig)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 51,
    "qualname": "BertEmbedding.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 81,
    "qualname": "BertPooler.__init__",
    "signature": "def __init__(self, config: BertConfig)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 86,
    "qualname": "BertPooler.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 100,
    "qualname": "BertEncoder.__init__",
    "signature": "def __init__(self, config: BertConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 121,
    "qualname": "BertEncoder.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 131,
    "qualname": "BertLayer.__init__",
    "signature": "def __init__(self, config: BertConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 167,
    "qualname": "BertLayer.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 177,
    "qualname": "BertAttention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_attention_heads: int, layer_norm_eps: float, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 203,
    "qualname": "BertAttention.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 212,
    "qualname": "BertSelfAttention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_attention_heads: int, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 257,
    "qualname": "BertSelfAttention.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 268,
    "qualname": "BertSelfOutput.__init__",
    "signature": "def __init__(self, hidden_size: int, layer_norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 285,
    "qualname": "BertSelfOutput.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 295,
    "qualname": "BertIntermediate.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 313,
    "qualname": "BertIntermediate.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 321,
    "qualname": "BertOutput.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, layer_norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 341,
    "qualname": "BertOutput.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 351,
    "qualname": "BertModel.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 375,
    "qualname": "BertModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 399,
    "qualname": "BertModel.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 439,
    "qualname": "BertForSequenceClassification.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 458,
    "qualname": "BertForSequenceClassification.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.bert",
    "file": "python/sglang/srt/models/bert.py",
    "line": 478,
    "qualname": "BertForSequenceClassification.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)"
  },
  {
    "module": "srt.models.chatglm",
    "file": "python/sglang/srt/models/chatglm.py",
    "line": 50,
    "qualname": "GLMAttention.__init__",
    "signature": "def __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.chatglm",
    "file": "python/sglang/srt/models/chatglm.py",
    "line": 120,
    "qualname": "GLMAttention.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.chatglm",
    "file": "python/sglang/srt/models/chatglm.py",
    "line": 147,
    "qualname": "GLMMLP.__init__",
    "signature": "def __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.chatglm",
    "file": "python/sglang/srt/models/chatglm.py",
    "line": 177,
    "qualname": "GLMMLP.forward",
    "signature": "def forward(self, hidden_states)"
  },
  {
    "module": "srt.models.chatglm",
    "file": "python/sglang/srt/models/chatglm.py",
    "line": 193,
    "qualname": "GLMBlock.__init__",
    "signature": "def __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.chatglm",
    "file": "python/sglang/srt/models/chatglm.py",
    "line": 227,
    "qualname": "GLMBlock.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.chatglm",
    "file": "python/sglang/srt/models/chatglm.py",
    "line": 268,
    "qualname": "GLMTransformer.__init__",
    "signature": "def __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.chatglm",
    "file": "python/sglang/srt/models/chatglm.py",
    "line": 300,
    "qualname": "GLMTransformer.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.chatglm",
    "file": "python/sglang/srt/models/chatglm.py",
    "line": 321,
    "qualname": "ChatGLMM.__init__",
    "signature": "def __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.chatglm",
    "file": "python/sglang/srt/models/chatglm.py",
    "line": 348,
    "qualname": "ChatGLMM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.chatglm",
    "file": "python/sglang/srt/models/chatglm.py",
    "line": 380,
    "qualname": "ChatGLMForCausalLM.__init__",
    "signature": "def __init__(self, config: ChatGLMConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.chatglm",
    "file": "python/sglang/srt/models/chatglm.py",
    "line": 397,
    "qualname": "ChatGLMForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.chatglm",
    "file": "python/sglang/srt/models/chatglm.py",
    "line": 408,
    "qualname": "ChatGLMForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 25,
    "qualname": "CLIPVisionEmbeddings.__init__",
    "signature": "def __init__(self, config: CLIPVisionConfig)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 52,
    "qualname": "CLIPVisionEmbeddings.forward",
    "signature": "def forward(self, pixel_values: torch.Tensor)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 68,
    "qualname": "CLIPTextEmbeddings.__init__",
    "signature": "def __init__(self, config: CLIPTextConfig)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 84,
    "qualname": "CLIPTextEmbeddings.forward",
    "signature": "def forward(self, input_ids: Optional[torch.LongTensor], position_ids: Optional[torch.LongTensor], inputs_embeds: Optional[torch.FloatTensor])"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 108,
    "qualname": "CLIPMLP.__init__",
    "signature": "def __init__(self, config, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 130,
    "qualname": "CLIPMLP.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 139,
    "qualname": "CLIPEncoderLayer.__init__",
    "signature": "def __init__(self, config: CLIPVisionConfig, act_layer: Type[nn.Module], norm_layer: Type[nn.Module], attn_implementation: Optional[str], quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 180,
    "qualname": "CLIPEncoderLayer.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, causal_attention_mask: torch.Tensor)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 219,
    "qualname": "CLIPEncoder.__init__",
    "signature": "def __init__(self, config: CLIPVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 244,
    "qualname": "CLIPEncoder.forward",
    "signature": "def forward(self, inputs_embeds: torch.Tensor, attention_mask: torch.Tensor, causal_attention_mask: torch.Tensor, return_all_hidden_states: bool)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 266,
    "qualname": "CLIPTextTransformer.__init__",
    "signature": "def __init__(self, config: CLIPTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 284,
    "qualname": "CLIPTextTransformer.device",
    "signature": "def device(self)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 287,
    "qualname": "CLIPTextTransformer.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor], position_ids: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 307,
    "qualname": "CLIPTextModel.__init__",
    "signature": "def __init__(self, config: CLIPTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 321,
    "qualname": "CLIPTextModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, position_ids: torch.Tensor)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 331,
    "qualname": "CLIPVisionTransformer.__init__",
    "signature": "def __init__(self, config: CLIPVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 364,
    "qualname": "CLIPVisionTransformer.device",
    "signature": "def device(self)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 367,
    "qualname": "CLIPVisionTransformer.forward",
    "signature": "def forward(self, pixel_values: torch.Tensor)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 387,
    "qualname": "CLIPVisionModel.__init__",
    "signature": "def __init__(self, config: CLIPVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 399,
    "qualname": "CLIPVisionModel.device",
    "signature": "def device(self)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 402,
    "qualname": "CLIPVisionModel.forward",
    "signature": "def forward(self, pixel_values: torch.Tensor)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 407,
    "qualname": "CLIPModel.__init__",
    "signature": "def __init__(self, config: CLIPConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 454,
    "qualname": "CLIPModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 486,
    "qualname": "CLIPModel.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 490,
    "qualname": "CLIPModel.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.clip",
    "file": "python/sglang/srt/models/clip.py",
    "line": 518,
    "qualname": "monkey_patch_weight_loader",
    "signature": "def monkey_patch_weight_loader()"
  },
  {
    "module": "srt.models.commandr",
    "file": "python/sglang/srt/models/commandr.py",
    "line": 72,
    "qualname": "layer_norm_func",
    "signature": "def layer_norm_func(hidden_states, weight, variance_epsilon)"
  },
  {
    "module": "srt.models.commandr",
    "file": "python/sglang/srt/models/commandr.py",
    "line": 83,
    "qualname": "LayerNorm.__init__",
    "signature": "def __init__(self, param_shape, eps)"
  },
  {
    "module": "srt.models.commandr",
    "file": "python/sglang/srt/models/commandr.py",
    "line": 89,
    "qualname": "LayerNorm.forward",
    "signature": "def forward(self, hidden_states, residuals)"
  },
  {
    "module": "srt.models.commandr",
    "file": "python/sglang/srt/models/commandr.py",
    "line": 95,
    "qualname": "LayerNorm.weight_loader",
    "signature": "def weight_loader(self, param: Parameter, loaded_weight: torch.Tensor)"
  },
  {
    "module": "srt.models.commandr",
    "file": "python/sglang/srt/models/commandr.py",
    "line": 109,
    "qualname": "CohereMLP.__init__",
    "signature": "def __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.commandr",
    "file": "python/sglang/srt/models/commandr.py",
    "line": 135,
    "qualname": "CohereMLP.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.commandr",
    "file": "python/sglang/srt/models/commandr.py",
    "line": 143,
    "qualname": "CohereAttention.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.commandr",
    "file": "python/sglang/srt/models/commandr.py",
    "line": 228,
    "qualname": "CohereAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.commandr",
    "file": "python/sglang/srt/models/commandr.py",
    "line": 245,
    "qualname": "CohereDecoderLayer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.commandr",
    "file": "python/sglang/srt/models/commandr.py",
    "line": 271,
    "qualname": "CohereDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.commandr",
    "file": "python/sglang/srt/models/commandr.py",
    "line": 294,
    "qualname": "CohereModel.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.commandr",
    "file": "python/sglang/srt/models/commandr.py",
    "line": 321,
    "qualname": "CohereModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.commandr",
    "file": "python/sglang/srt/models/commandr.py",
    "line": 342,
    "qualname": "CohereForCausalLM.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.commandr",
    "file": "python/sglang/srt/models/commandr.py",
    "line": 357,
    "qualname": "CohereForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.commandr",
    "file": "python/sglang/srt/models/commandr.py",
    "line": 372,
    "qualname": "CohereForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.dbrx",
    "file": "python/sglang/srt/models/dbrx.py",
    "line": 59,
    "qualname": "DbrxRouter.__init__",
    "signature": "def __init__(self, config: DbrxConfig, params_dtype: Optional[torch.dtype], prefix: str)"
  },
  {
    "module": "srt.models.dbrx",
    "file": "python/sglang/srt/models/dbrx.py",
    "line": 77,
    "qualname": "DbrxRouter.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.dbrx",
    "file": "python/sglang/srt/models/dbrx.py",
    "line": 90,
    "qualname": "DbrxExperts.__init__",
    "signature": "def __init__(self, config: DbrxConfig, quant_config: Optional[QuantizationConfig], params_dtype: Optional[torch.dtype], prefix: str)"
  },
  {
    "module": "srt.models.dbrx",
    "file": "python/sglang/srt/models/dbrx.py",
    "line": 146,
    "qualname": "DbrxExperts.weight_loader",
    "signature": "def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor, weight_name: str)"
  },
  {
    "module": "srt.models.dbrx",
    "file": "python/sglang/srt/models/dbrx.py",
    "line": 174,
    "qualname": "DbrxExperts.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.dbrx",
    "file": "python/sglang/srt/models/dbrx.py",
    "line": 195,
    "qualname": "DbrxAttention.__init__",
    "signature": "def __init__(self, config: DbrxConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.dbrx",
    "file": "python/sglang/srt/models/dbrx.py",
    "line": 262,
    "qualname": "DbrxAttention.forward",
    "signature": "def forward(self, position_ids: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.dbrx",
    "file": "python/sglang/srt/models/dbrx.py",
    "line": 279,
    "qualname": "DbrxFusedNormAttention.__init__",
    "signature": "def __init__(self, config: DbrxConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.dbrx",
    "file": "python/sglang/srt/models/dbrx.py",
    "line": 297,
    "qualname": "DbrxFusedNormAttention.forward",
    "signature": "def forward(self, position_ids: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.dbrx",
    "file": "python/sglang/srt/models/dbrx.py",
    "line": 317,
    "qualname": "DbrxBlock.__init__",
    "signature": "def __init__(self, config: DbrxConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.dbrx",
    "file": "python/sglang/srt/models/dbrx.py",
    "line": 333,
    "qualname": "DbrxBlock.forward",
    "signature": "def forward(self, position_ids: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.dbrx",
    "file": "python/sglang/srt/models/dbrx.py",
    "line": 350,
    "qualname": "DbrxModel.__init__",
    "signature": "def __init__(self, config: DbrxConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.dbrx",
    "file": "python/sglang/srt/models/dbrx.py",
    "line": 378,
    "qualname": "DbrxModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.dbrx",
    "file": "python/sglang/srt/models/dbrx.py",
    "line": 397,
    "qualname": "DbrxForCausalLM.__init__",
    "signature": "def __init__(self, config: DbrxConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.dbrx",
    "file": "python/sglang/srt/models/dbrx.py",
    "line": 420,
    "qualname": "DbrxForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.dbrx",
    "file": "python/sglang/srt/models/dbrx.py",
    "line": 431,
    "qualname": "DbrxForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.deepseek",
    "file": "python/sglang/srt/models/deepseek.py",
    "line": 56,
    "qualname": "DeepseekMLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str)"
  },
  {
    "module": "srt.models.deepseek",
    "file": "python/sglang/srt/models/deepseek.py",
    "line": 88,
    "qualname": "DeepseekMLP.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.deepseek",
    "file": "python/sglang/srt/models/deepseek.py",
    "line": 97,
    "qualname": "DeepseekMoE.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.deepseek",
    "file": "python/sglang/srt/models/deepseek.py",
    "line": 152,
    "qualname": "DeepseekMoE.pack_params",
    "signature": "def pack_params(self)"
  },
  {
    "module": "srt.models.deepseek",
    "file": "python/sglang/srt/models/deepseek.py",
    "line": 171,
    "qualname": "DeepseekMoE.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.deepseek",
    "file": "python/sglang/srt/models/deepseek.py",
    "line": 196,
    "qualname": "DeepseekAttention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.deepseek",
    "file": "python/sglang/srt/models/deepseek.py",
    "line": 266,
    "qualname": "DeepseekAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.deepseek",
    "file": "python/sglang/srt/models/deepseek.py",
    "line": 282,
    "qualname": "DeepseekDecoderLayer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.deepseek",
    "file": "python/sglang/srt/models/deepseek.py",
    "line": 328,
    "qualname": "DeepseekDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.deepseek",
    "file": "python/sglang/srt/models/deepseek.py",
    "line": 357,
    "qualname": "DeepseekModel.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.deepseek",
    "file": "python/sglang/srt/models/deepseek.py",
    "line": 384,
    "qualname": "DeepseekModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.deepseek",
    "file": "python/sglang/srt/models/deepseek.py",
    "line": 409,
    "qualname": "DeepseekForCausalLM.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.deepseek",
    "file": "python/sglang/srt/models/deepseek.py",
    "line": 429,
    "qualname": "DeepseekForCausalLM.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.deepseek",
    "file": "python/sglang/srt/models/deepseek.py",
    "line": 433,
    "qualname": "DeepseekForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.deepseek",
    "file": "python/sglang/srt/models/deepseek.py",
    "line": 445,
    "qualname": "DeepseekForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 82,
    "qualname": "named_apply",
    "signature": "def named_apply(fn: Callable, module: nn.Module, name, depth_first: bool, include_root: bool)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 105,
    "qualname": "VQ_16",
    "signature": "def VQ_16()"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 176,
    "qualname": "trunc_normal_tf_",
    "signature": "def trunc_normal_tf_(tensor: torch.Tensor, mean: float, std: float, a: float, b: float)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 214,
    "qualname": "nchw_to",
    "signature": "def nchw_to(x: torch.Tensor, fmt: Format)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 224,
    "qualname": "resample_patch_embed",
    "signature": "def resample_patch_embed(patch_embed, new_size: List[int], interpolation: str, antialias: bool, verbose: bool)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 308,
    "qualname": "PatchEmbed.__init__",
    "signature": "def __init__(self, img_size: Optional[int], patch_size: int, in_chans: int, embed_dim: int, norm_layer: Optional[Callable], flatten: bool, output_fmt: Optional[str], bias: bool, strict_img_size: bool, dynamic_img_pad: bool)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 349,
    "qualname": "PatchEmbed.set_input_size",
    "signature": "def set_input_size(self, img_size: Optional[Union[int, Tuple[int, int]]], patch_size: Optional[Union[int, Tuple[int, int]]])"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 379,
    "qualname": "PatchEmbed.feat_ratio",
    "signature": "def feat_ratio(self, as_scalar)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 385,
    "qualname": "PatchEmbed.dynamic_feat_size",
    "signature": "def dynamic_feat_size(self, img_size: Tuple[int, int])"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 396,
    "qualname": "PatchEmbed.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 436,
    "qualname": "Mlp.__init__",
    "signature": "def __init__(self, in_features, hidden_features, out_features, act_layer, norm_layer, bias, drop, use_conv)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 463,
    "qualname": "Mlp.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 473,
    "qualname": "drop_path",
    "signature": "def drop_path(x, drop_prob: float, training: bool, scale_by_keep: bool)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 500,
    "qualname": "DropPath.__init__",
    "signature": "def __init__(self, drop_prob: float, scale_by_keep: bool)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 505,
    "qualname": "DropPath.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 508,
    "qualname": "DropPath.extra_repr",
    "signature": "def extra_repr(self)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 513,
    "qualname": "VisionTransformerBlock.__init__",
    "signature": "def __init__(self, dim: int, num_heads: int, mlp_ratio: float, qkv_bias: bool, qk_norm: bool, proj_drop: float, attn_drop: float, init_values: Optional[float], drop_path: float, act_layer: nn.Module, norm_layer: nn.Module, mlp_layer: nn.Module)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 557,
    "qualname": "VisionTransformerBlock.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 573,
    "qualname": "PatchDropout.__init__",
    "signature": "def __init__(self, prob: float, num_prefix_tokens: int, ordered: bool, return_indices: bool)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 589,
    "qualname": "PatchDropout.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 625,
    "qualname": "resample_abs_pos_embed",
    "signature": "def resample_abs_pos_embed(posemb: torch.Tensor, new_size: List[int], old_size: Optional[List[int]], num_prefix_tokens: int, interpolation: str, antialias: bool, verbose: bool)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 673,
    "qualname": "init_weights",
    "signature": "def init_weights(self)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 679,
    "qualname": "init_weights_vit_timm",
    "signature": "def init_weights_vit_timm(module: nn.Module, name: str)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 698,
    "qualname": "VisionTransformer.__init__",
    "signature": "def __init__(self, img_size: Union[int, Tuple[int, int]], patch_size: Union[int, Tuple[int, int]], in_chans: int, num_classes: int, global_pool: Literal['', 'avg', 'token', 'map'], embed_dim: int, depth: int, num_heads: int, mlp_ratio: float, qkv_bias: bool, qk_norm: bool, init_values: Optional[float], class_token: bool, no_embed_class: bool, reg_tokens: int, pre_norm: bool, fc_norm: Optional[bool], dynamic_img_size: bool, dynamic_img_pad: bool, drop_rate: float, pos_drop_rate: float, patch_drop_rate: float, proj_drop_rate: float, attn_drop_rate: float, drop_path_rate: float, weight_init: Literal['skip', 'jax', 'jax_nlhb', 'moco', ''], embed_layer: Callable, _norm_layer: Optional[LayerType], _act_layer: Optional[LayerType], block_fn: Type[nn.Module], mlp_layer: Type[nn.Module], ignore_head: bool)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 864,
    "qualname": "VisionTransformer.init_weights",
    "signature": "def init_weights(self, mode: Literal['jax', 'jax_nlhb', 'moco', ''])"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 873,
    "qualname": "VisionTransformer.no_weight_decay",
    "signature": "def no_weight_decay(self)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 877,
    "qualname": "VisionTransformer.group_matcher",
    "signature": "def group_matcher(self, coarse: bool)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 884,
    "qualname": "VisionTransformer.get_classifier",
    "signature": "def get_classifier(self)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 887,
    "qualname": "VisionTransformer.reset_classifier",
    "signature": "def reset_classifier(self, num_classes: int, global_pool)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 957,
    "qualname": "VisionTransformer.forward_features",
    "signature": "def forward_features(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 966,
    "qualname": "VisionTransformer.forward_head",
    "signature": "def forward_head(self, x: torch.Tensor, pre_logits: bool)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 977,
    "qualname": "VisionTransformer.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 984,
    "qualname": "model_name_to_cls",
    "signature": "def model_name_to_cls(cls_name)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1003,
    "qualname": "vision_head.__init__",
    "signature": "def __init__(self, params)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1013,
    "qualname": "vision_head.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1054,
    "qualname": "create_siglip_vit",
    "signature": "def create_siglip_vit(model_name: str, image_size: int, select_layer: int, ckpt_path: str)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1116,
    "qualname": "Normalize.__init__",
    "signature": "def __init__(self, mean, std, inplace)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1123,
    "qualname": "Normalize.forward",
    "signature": "def forward(self, tensor: Tensor)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1133,
    "qualname": "Normalize.__repr__",
    "signature": "def __repr__(self)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1138,
    "qualname": "CLIPVisionTower.__init__",
    "signature": "def __init__(self, model_name: str, image_size: Union[Tuple[int, int], int], select_feature: str, select_layer: int, select_layers: list, ckpt_path: str, pixel_mean: Optional[List[float]], pixel_std: Optional[List[float]])"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1176,
    "qualname": "CLIPVisionTower.device",
    "signature": "def device(self)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1180,
    "qualname": "CLIPVisionTower.dtype",
    "signature": "def dtype(self)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1183,
    "qualname": "CLIPVisionTower.build_vision_tower",
    "signature": "def build_vision_tower(self, vision_tower_params)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1201,
    "qualname": "CLIPVisionTower.feature_select",
    "signature": "def feature_select(self, image_forward_outs)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1220,
    "qualname": "CLIPVisionTower.forward",
    "signature": "def forward(self, images)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1239,
    "qualname": "MlpProjector.__init__",
    "signature": "def __init__(self, cfg)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1274,
    "qualname": "MlpProjector.forward",
    "signature": "def forward(self, x_or_tuple: Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor])"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1301,
    "qualname": "LayerScale.__init__",
    "signature": "def __init__(self, dim: int, init_values: float, inplace: bool)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1311,
    "qualname": "LayerScale.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1328,
    "qualname": "use_fused_attn",
    "signature": "def use_fused_attn(experimental: bool)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1342,
    "qualname": "AttentionPoolLatent.__init__",
    "signature": "def __init__(self, in_features: int, out_features: int, embed_dim: int, num_heads: int, feat_size: Optional[int], mlp_ratio: float, qkv_bias: bool, qk_norm: bool, latent_len: int, latent_dim: int, pos_embed: str, pool_type: str, norm_layer: Optional[nn.Module], drop: float)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1394,
    "qualname": "AttentionPoolLatent.init_weights",
    "signature": "def init_weights(self)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1399,
    "qualname": "AttentionPoolLatent.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1443,
    "qualname": "Encoder.__init__",
    "signature": "def __init__(self, in_channels, ch, ch_mult, num_res_blocks, norm_type, dropout, resamp_with_conv, z_channels)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1501,
    "qualname": "Encoder.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1524,
    "qualname": "Decoder.__init__",
    "signature": "def __init__(self, z_channels, ch, ch_mult, num_res_blocks, norm_type, dropout, resamp_with_conv, out_channels)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1586,
    "qualname": "Decoder.last_layer",
    "signature": "def last_layer(self)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1589,
    "qualname": "Decoder.forward",
    "signature": "def forward(self, z)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1614,
    "qualname": "VectorQuantizer.__init__",
    "signature": "def __init__(self, n_e, e_dim, beta, entropy_loss_ratio, l2_norm, show_usage)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1633,
    "qualname": "VectorQuantizer.forward",
    "signature": "def forward(self, z)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1681,
    "qualname": "VectorQuantizer.get_codebook_entry",
    "signature": "def get_codebook_entry(self, indices, shape, channel_first)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1700,
    "qualname": "ResnetBlock.__init__",
    "signature": "def __init__(self, in_channels, out_channels, conv_shortcut, dropout, norm_type)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1734,
    "qualname": "ResnetBlock.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1753,
    "qualname": "AttnBlock.__init__",
    "signature": "def __init__(self, in_channels, norm_type)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1763,
    "qualname": "AttnBlock.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1790,
    "qualname": "nonlinearity",
    "signature": "def nonlinearity(x)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1795,
    "qualname": "Normalize",
    "signature": "def Normalize(in_channels, norm_type)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1806,
    "qualname": "Upsample.__init__",
    "signature": "def __init__(self, in_channels, with_conv)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1814,
    "qualname": "Upsample.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1828,
    "qualname": "Downsample.__init__",
    "signature": "def __init__(self, in_channels, with_conv)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1837,
    "qualname": "Downsample.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1847,
    "qualname": "compute_entropy_loss",
    "signature": "def compute_entropy_loss(affinity, loss_type, temperature)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1864,
    "qualname": "VQModel.__init__",
    "signature": "def __init__(self, config: ModelArgs)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1891,
    "qualname": "VQModel.encode",
    "signature": "def encode(self, x)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1897,
    "qualname": "VQModel.decode",
    "signature": "def decode(self, quant)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1902,
    "qualname": "VQModel.decode_code",
    "signature": "def decode_code(self, code_b, shape, channel_first)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1907,
    "qualname": "VQModel.forward",
    "signature": "def forward(self, input)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1924,
    "qualname": "MultiModalityCausalLM.__init__",
    "signature": "def __init__(self, config: MultiModalityConfig, quant_config: Optional[QuantizationConfig])"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1962,
    "qualname": "MultiModalityCausalLM.get_image_feature",
    "signature": "def get_image_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1978,
    "qualname": "MultiModalityCausalLM.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1982,
    "qualname": "MultiModalityCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 1999,
    "qualname": "MultiModalityCausalLM.prepare_gen_img_embeds",
    "signature": "def prepare_gen_img_embeds(self, image_ids: torch.LongTensor)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 2002,
    "qualname": "MultiModalityCausalLM.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.deepseek_janus_pro",
    "file": "python/sglang/srt/models/deepseek_janus_pro.py",
    "line": 2011,
    "qualname": "MultiModalityCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.deepseek_nextn",
    "file": "python/sglang/srt/models/deepseek_nextn.py",
    "line": 42,
    "qualname": "DeepseekModelNextN.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.deepseek_nextn",
    "file": "python/sglang/srt/models/deepseek_nextn.py",
    "line": 80,
    "qualname": "DeepseekModelNextN.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.deepseek_nextn",
    "file": "python/sglang/srt/models/deepseek_nextn.py",
    "line": 128,
    "qualname": "DeepseekV3ForCausalLMNextN.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.deepseek_nextn",
    "file": "python/sglang/srt/models/deepseek_nextn.py",
    "line": 155,
    "qualname": "DeepseekV3ForCausalLMNextN.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.deepseek_nextn",
    "file": "python/sglang/srt/models/deepseek_nextn.py",
    "line": 166,
    "qualname": "DeepseekV3ForCausalLMNextN.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.deepseek_vl2",
    "file": "python/sglang/srt/models/deepseek_vl2.py",
    "line": 26,
    "qualname": "DeepseekVL2MlpProjector.__init__",
    "signature": "def __init__(self, config: DeepseekVL2MlpProjectorConfig, quant_config: Optional[QuantizationConfig])"
  },
  {
    "module": "srt.models.deepseek_vl2",
    "file": "python/sglang/srt/models/deepseek_vl2.py",
    "line": 111,
    "qualname": "DeepseekVL2MlpProjector.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.deepseek_vl2",
    "file": "python/sglang/srt/models/deepseek_vl2.py",
    "line": 160,
    "qualname": "DeepseekVL2ForCausalLM.__init__",
    "signature": "def __init__(self, config: DeepseekVL2Config, quant_config: Optional[QuantizationConfig])"
  },
  {
    "module": "srt.models.deepseek_vl2",
    "file": "python/sglang/srt/models/deepseek_vl2.py",
    "line": 219,
    "qualname": "DeepseekVL2ForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.deepseek_vl2",
    "file": "python/sglang/srt/models/deepseek_vl2.py",
    "line": 236,
    "qualname": "DeepseekVL2ForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.deepseek_vl2",
    "file": "python/sglang/srt/models/deepseek_vl2.py",
    "line": 256,
    "qualname": "DeepseekVL2ForCausalLM.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.deepseek_vl2",
    "file": "python/sglang/srt/models/deepseek_vl2.py",
    "line": 260,
    "qualname": "DeepseekVL2ForCausalLM.get_image_feature",
    "signature": "def get_image_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.ernie4",
    "file": "python/sglang/srt/models/ernie4.py",
    "line": 49,
    "qualname": "MoEGate.__init__",
    "signature": "def __init__(self, config, prefix: str)"
  },
  {
    "module": "srt.models.ernie4",
    "file": "python/sglang/srt/models/ernie4.py",
    "line": 62,
    "qualname": "MoEGate.forward",
    "signature": "def forward(self, hidden_states)"
  },
  {
    "module": "srt.models.ernie4",
    "file": "python/sglang/srt/models/ernie4.py",
    "line": 68,
    "qualname": "Ernie4Moe.__init__",
    "signature": "def __init__(self, config: Ernie4_5_MoeConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.ernie4",
    "file": "python/sglang/srt/models/ernie4.py",
    "line": 119,
    "qualname": "Ernie4Moe.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.ernie4",
    "file": "python/sglang/srt/models/ernie4.py",
    "line": 122,
    "qualname": "Ernie4Moe.forward_normal",
    "signature": "def forward_normal(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.ernie4",
    "file": "python/sglang/srt/models/ernie4.py",
    "line": 148,
    "qualname": "Ernie4DecoderLayer.__init__",
    "signature": "def __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, is_mtp: bool)"
  },
  {
    "module": "srt.models.ernie4",
    "file": "python/sglang/srt/models/ernie4.py",
    "line": 212,
    "qualname": "Ernie4DecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.ernie4",
    "file": "python/sglang/srt/models/ernie4.py",
    "line": 239,
    "qualname": "Ernie4Model.__init__",
    "signature": "def __init__(self, config: Ernie4_5_MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.ernie4",
    "file": "python/sglang/srt/models/ernie4.py",
    "line": 264,
    "qualname": "Ernie4Model.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.ernie4",
    "file": "python/sglang/srt/models/ernie4.py",
    "line": 302,
    "qualname": "Ernie4_5_ForCausalLM.__init__",
    "signature": "def __init__(self, config: Ernie4_5_MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.ernie4",
    "file": "python/sglang/srt/models/ernie4.py",
    "line": 324,
    "qualname": "Ernie4_5_ForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.ernie4",
    "file": "python/sglang/srt/models/ernie4.py",
    "line": 335,
    "qualname": "Ernie4_5_ForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.ernie4",
    "file": "python/sglang/srt/models/ernie4.py",
    "line": 358,
    "qualname": "Ernie4_5_ForCausalLM.get_embed_and_head",
    "signature": "def get_embed_and_head(self)"
  },
  {
    "module": "srt.models.ernie4",
    "file": "python/sglang/srt/models/ernie4.py",
    "line": 363,
    "qualname": "Ernie4_5_MoeForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.ernie4_eagle",
    "file": "python/sglang/srt/models/ernie4_eagle.py",
    "line": 39,
    "qualname": "Ernie4ModelMTP.__init__",
    "signature": "def __init__(self, config: Ernie4_5_MoeConfig, layer_id: int, prefix: str, quant_config: Optional[QuantizationConfig])"
  },
  {
    "module": "srt.models.ernie4_eagle",
    "file": "python/sglang/srt/models/ernie4_eagle.py",
    "line": 67,
    "qualname": "Ernie4ModelMTP.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.ernie4_eagle",
    "file": "python/sglang/srt/models/ernie4_eagle.py",
    "line": 102,
    "qualname": "Ernie4_5_MoeForCausalLMMTP.__init__",
    "signature": "def __init__(self, config: Ernie4_5_MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str, mtp_layer_id: int)"
  },
  {
    "module": "srt.models.ernie4_eagle",
    "file": "python/sglang/srt/models/ernie4_eagle.py",
    "line": 132,
    "qualname": "Ernie4_5_MoeForCausalLMMTP.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.ernie4_eagle",
    "file": "python/sglang/srt/models/ernie4_eagle.py",
    "line": 143,
    "qualname": "Ernie4_5_MoeForCausalLMMTP.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.ernie4_eagle",
    "file": "python/sglang/srt/models/ernie4_eagle.py",
    "line": 188,
    "qualname": "Ernie4_5_MoeForCausalLMMTP.get_embed_and_head",
    "signature": "def get_embed_and_head(self)"
  },
  {
    "module": "srt.models.ernie4_eagle",
    "file": "python/sglang/srt/models/ernie4_eagle.py",
    "line": 191,
    "qualname": "Ernie4_5_MoeForCausalLMMTP.set_embed_and_head",
    "signature": "def set_embed_and_head(self, embed, head)"
  },
  {
    "module": "srt.models.exaone",
    "file": "python/sglang/srt/models/exaone.py",
    "line": 46,
    "qualname": "ExaoneGatedMLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.exaone",
    "file": "python/sglang/srt/models/exaone.py",
    "line": 76,
    "qualname": "ExaoneGatedMLP.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.exaone",
    "file": "python/sglang/srt/models/exaone.py",
    "line": 84,
    "qualname": "ExaoneAttention.__init__",
    "signature": "def __init__(self, config, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.exaone",
    "file": "python/sglang/srt/models/exaone.py",
    "line": 161,
    "qualname": "ExaoneAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.exaone",
    "file": "python/sglang/srt/models/exaone.py",
    "line": 176,
    "qualname": "ExaoneDecoderLayer.__init__",
    "signature": "def __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.exaone",
    "file": "python/sglang/srt/models/exaone.py",
    "line": 219,
    "qualname": "ExaoneDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.exaone",
    "file": "python/sglang/srt/models/exaone.py",
    "line": 245,
    "qualname": "ExaoneModel.__init__",
    "signature": "def __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.exaone",
    "file": "python/sglang/srt/models/exaone.py",
    "line": 273,
    "qualname": "ExaoneModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.exaone",
    "file": "python/sglang/srt/models/exaone.py",
    "line": 298,
    "qualname": "ExaoneForCausalLM.__init__",
    "signature": "def __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.exaone",
    "file": "python/sglang/srt/models/exaone.py",
    "line": 321,
    "qualname": "ExaoneForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.exaone",
    "file": "python/sglang/srt/models/exaone.py",
    "line": 335,
    "qualname": "ExaoneForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.gemma",
    "file": "python/sglang/srt/models/gemma.py",
    "line": 44,
    "qualname": "GemmaMLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma",
    "file": "python/sglang/srt/models/gemma.py",
    "line": 68,
    "qualname": "GemmaMLP.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.gemma",
    "file": "python/sglang/srt/models/gemma.py",
    "line": 76,
    "qualname": "GemmaAttention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: int, layer_id: int, max_position_embeddings: int, rope_theta: float, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma",
    "file": "python/sglang/srt/models/gemma.py",
    "line": 144,
    "qualname": "GemmaAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.gemma",
    "file": "python/sglang/srt/models/gemma.py",
    "line": 159,
    "qualname": "GemmaDecoderLayer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma",
    "file": "python/sglang/srt/models/gemma.py",
    "line": 190,
    "qualname": "GemmaDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.gemma",
    "file": "python/sglang/srt/models/gemma.py",
    "line": 216,
    "qualname": "GemmaModel.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma",
    "file": "python/sglang/srt/models/gemma.py",
    "line": 242,
    "qualname": "GemmaModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma",
    "file": "python/sglang/srt/models/gemma.py",
    "line": 294,
    "qualname": "GemmaForCausalLM.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma",
    "file": "python/sglang/srt/models/gemma.py",
    "line": 309,
    "qualname": "GemmaForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma",
    "file": "python/sglang/srt/models/gemma.py",
    "line": 322,
    "qualname": "GemmaForCausalLM.forward_split_prefill",
    "signature": "def forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma",
    "file": "python/sglang/srt/models/gemma.py",
    "line": 369,
    "qualname": "GemmaForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.gemma2",
    "file": "python/sglang/srt/models/gemma2.py",
    "line": 47,
    "qualname": "get_attention_sliding_window_size",
    "signature": "def get_attention_sliding_window_size(config)"
  },
  {
    "module": "srt.models.gemma2",
    "file": "python/sglang/srt/models/gemma2.py",
    "line": 52,
    "qualname": "Gemma2MLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, hidden_activation: str, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma2",
    "file": "python/sglang/srt/models/gemma2.py",
    "line": 84,
    "qualname": "Gemma2MLP.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma2",
    "file": "python/sglang/srt/models/gemma2.py",
    "line": 92,
    "qualname": "Gemma2Attention.__init__",
    "signature": "def __init__(self, layer_id: int, config: PretrainedConfig, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: int, max_position_embeddings: int, rope_theta: float, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma2",
    "file": "python/sglang/srt/models/gemma2.py",
    "line": 170,
    "qualname": "Gemma2Attention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.gemma2",
    "file": "python/sglang/srt/models/gemma2.py",
    "line": 185,
    "qualname": "Gemma2DecoderLayer.__init__",
    "signature": "def __init__(self, layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma2",
    "file": "python/sglang/srt/models/gemma2.py",
    "line": 227,
    "qualname": "Gemma2DecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.gemma2",
    "file": "python/sglang/srt/models/gemma2.py",
    "line": 255,
    "qualname": "Gemma2Model.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma2",
    "file": "python/sglang/srt/models/gemma2.py",
    "line": 286,
    "qualname": "Gemma2Model.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma2",
    "file": "python/sglang/srt/models/gemma2.py",
    "line": 357,
    "qualname": "Gemma2ForCausalLM.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma2",
    "file": "python/sglang/srt/models/gemma2.py",
    "line": 372,
    "qualname": "Gemma2ForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma2",
    "file": "python/sglang/srt/models/gemma2.py",
    "line": 385,
    "qualname": "Gemma2ForCausalLM.forward_split_prefill",
    "signature": "def forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma2",
    "file": "python/sglang/srt/models/gemma2.py",
    "line": 435,
    "qualname": "Gemma2ForCausalLM.get_attention_sliding_window_size",
    "signature": "def get_attention_sliding_window_size(self)"
  },
  {
    "module": "srt.models.gemma2",
    "file": "python/sglang/srt/models/gemma2.py",
    "line": 438,
    "qualname": "Gemma2ForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.gemma2_reward",
    "file": "python/sglang/srt/models/gemma2_reward.py",
    "line": 29,
    "qualname": "Gemma2ForSequenceClassification.__init__",
    "signature": "def __init__(self, config: Gemma2Config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma2_reward",
    "file": "python/sglang/srt/models/gemma2_reward.py",
    "line": 48,
    "qualname": "Gemma2ForSequenceClassification.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)"
  },
  {
    "module": "srt.models.gemma2_reward",
    "file": "python/sglang/srt/models/gemma2_reward.py",
    "line": 66,
    "qualname": "Gemma2ForSequenceClassification.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 52,
    "qualname": "get_attention_sliding_window_size",
    "signature": "def get_attention_sliding_window_size(config)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 58,
    "qualname": "extract_layer_index",
    "signature": "def extract_layer_index(prefix: str)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 72,
    "qualname": "Gemma3MLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_activation: str, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 103,
    "qualname": "Gemma3MLP.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 111,
    "qualname": "Gemma3Attention.__init__",
    "signature": "def __init__(self, layer_id: int, config: Gemma3TextConfig, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 203,
    "qualname": "Gemma3Attention.naive_attn_with_masks",
    "signature": "def naive_attn_with_masks(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, out: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 249,
    "qualname": "Gemma3Attention.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, position_embeddings: Tuple[torch.Tensor, torch.Tensor], forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 291,
    "qualname": "Gemma3DecoderLayer.__init__",
    "signature": "def __init__(self, layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 330,
    "qualname": "Gemma3DecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, position_embeddings_global: torch.Tensor, position_embeddings_local: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 372,
    "qualname": "Gemma3RotaryEmbedding.__init__",
    "signature": "def __init__(self, config: Gemma3TextConfig, device)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 418,
    "qualname": "Gemma3RotaryEmbedding.forward",
    "signature": "def forward(self, x, position_ids)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 454,
    "qualname": "Gemma3TextScaledWordEmbedding.__init__",
    "signature": "def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: Optional[float])"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 464,
    "qualname": "Gemma3TextScaledWordEmbedding.forward",
    "signature": "def forward(self, input_ids: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 469,
    "qualname": "Gemma3TextModel.__init__",
    "signature": "def __init__(self, config: Gemma3TextConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 513,
    "qualname": "Gemma3TextModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 599,
    "qualname": "Gemma3ForCausalLM.__init__",
    "signature": "def __init__(self, config: Gemma3TextConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 624,
    "qualname": "Gemma3ForCausalLM.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 627,
    "qualname": "Gemma3ForCausalLM.get_attention_sliding_window_size",
    "signature": "def get_attention_sliding_window_size(self)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 630,
    "qualname": "Gemma3ForCausalLM.dtype",
    "signature": "def dtype(self)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 634,
    "qualname": "Gemma3ForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 651,
    "qualname": "Gemma3ForCausalLM.forward_split_prefill",
    "signature": "def forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3_causal",
    "file": "python/sglang/srt/models/gemma3_causal.py",
    "line": 713,
    "qualname": "Gemma3ForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.gemma3_mm",
    "file": "python/sglang/srt/models/gemma3_mm.py",
    "line": 61,
    "qualname": "Gemma3MultiModalProjector.__init__",
    "signature": "def __init__(self, config: Gemma3Config)"
  },
  {
    "module": "srt.models.gemma3_mm",
    "file": "python/sglang/srt/models/gemma3_mm.py",
    "line": 83,
    "qualname": "Gemma3MultiModalProjector.forward",
    "signature": "def forward(self, vision_outputs: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3_mm",
    "file": "python/sglang/srt/models/gemma3_mm.py",
    "line": 158,
    "qualname": "Gemma3ForConditionalGeneration.__init__",
    "signature": "def __init__(self, config: Gemma3Config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3_mm",
    "file": "python/sglang/srt/models/gemma3_mm.py",
    "line": 188,
    "qualname": "Gemma3ForConditionalGeneration.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.gemma3_mm",
    "file": "python/sglang/srt/models/gemma3_mm.py",
    "line": 201,
    "qualname": "Gemma3ForConditionalGeneration.prepare_attn_masks",
    "signature": "def prepare_attn_masks(self, input_ids: torch.Tensor, positions: torch.Tensor, mask_dtype: torch.dtype)"
  },
  {
    "module": "srt.models.gemma3_mm",
    "file": "python/sglang/srt/models/gemma3_mm.py",
    "line": 269,
    "qualname": "Gemma3ForConditionalGeneration.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.gemma3_mm",
    "file": "python/sglang/srt/models/gemma3_mm.py",
    "line": 272,
    "qualname": "Gemma3ForConditionalGeneration.get_attention_sliding_window_size",
    "signature": "def get_attention_sliding_window_size(self)"
  },
  {
    "module": "srt.models.gemma3_mm",
    "file": "python/sglang/srt/models/gemma3_mm.py",
    "line": 278,
    "qualname": "Gemma3ForConditionalGeneration.get_image_feature",
    "signature": "def get_image_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.gemma3_mm",
    "file": "python/sglang/srt/models/gemma3_mm.py",
    "line": 316,
    "qualname": "Gemma3ForConditionalGeneration.forward",
    "signature": "def forward(self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3_mm",
    "file": "python/sglang/srt/models/gemma3_mm.py",
    "line": 383,
    "qualname": "Gemma3ForConditionalGeneration.tie_weights",
    "signature": "def tie_weights(self)"
  },
  {
    "module": "srt.models.gemma3_mm",
    "file": "python/sglang/srt/models/gemma3_mm.py",
    "line": 386,
    "qualname": "Gemma3ForConditionalGeneration.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 36,
    "qualname": "Gemma3nCumulativeGroupNorm.__init__",
    "signature": "def __init__(self, num_channels: int, feature_dims: Sequence[int], eps: float)"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 56,
    "qualname": "Gemma3nCumulativeGroupNorm.forward",
    "signature": "def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 139,
    "qualname": "Gemma3nAudioRelativePositionEmbedding.__init__",
    "signature": "def __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 227,
    "qualname": "Gemma3nAudioRelativePositionEmbedding.forward",
    "signature": "def forward(self, queries: torch.Tensor, keys: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 280,
    "qualname": "Gemma3nAudioAttention.__init__",
    "signature": "def __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 388,
    "qualname": "Gemma3nAudioAttention.forward",
    "signature": "def forward(self, x: torch.Tensor, mask: torch.BoolTensor)"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 490,
    "qualname": "Gemma3nAudioSSCPConvBlock.__init__",
    "signature": "def __init__(self, config: Gemma3nAudioConfig, idx: int, input_freq_dim: int, manual_padding: Tuple[int, int, int, int], quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 528,
    "qualname": "Gemma3nAudioSSCPConvBlock.forward",
    "signature": "def forward(self, audio_encodings: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 540,
    "qualname": "Gemma3nAudioSubSampleConvProjection.__init__",
    "signature": "def __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 602,
    "qualname": "Gemma3nAudioSubSampleConvProjection.forward",
    "signature": "def forward(self, audio_encodings: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 614,
    "qualname": "Gemma3nAudioConformerAttention.__init__",
    "signature": "def __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 646,
    "qualname": "Gemma3nAudioConformerAttention.forward",
    "signature": "def forward(self, audio_encodings: torch.Tensor, audio_mel_mask: torch.BoolTensor)"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 669,
    "qualname": "Gemma3nAudioConformerFeedForward.__init__",
    "signature": "def __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 702,
    "qualname": "Gemma3nAudioConformerFeedForward.forward",
    "signature": "def forward(self, audio_encodings: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 719,
    "qualname": "Gemma3nAudioConformerLightConv1d.__init__",
    "signature": "def __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 766,
    "qualname": "Gemma3nAudioConformerLightConv1d.forward",
    "signature": "def forward(self, audio_encodings: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 793,
    "qualname": "Gemma3nAudioConformerBlock.__init__",
    "signature": "def __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 821,
    "qualname": "Gemma3nAudioConformerBlock.forward",
    "signature": "def forward(self, audio_encodings: torch.Tensor, audio_mel_mask: torch.BoolTensor)"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 846,
    "qualname": "Gemma3nAudioEncoder.__init__",
    "signature": "def __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3n_audio",
    "file": "python/sglang/srt/models/gemma3n_audio.py",
    "line": 868,
    "qualname": "Gemma3nAudioEncoder.forward",
    "signature": "def forward(self, audio_mel: torch.Tensor, audio_mel_mask: torch.BoolTensor)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 33,
    "qualname": "get_attention_sliding_window_size",
    "signature": "def get_attention_sliding_window_size(config)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 38,
    "qualname": "Gemma3nRMSNorm.__init__",
    "signature": "def __init__(self, dim: int, eps: float, with_scale: bool)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 53,
    "qualname": "Gemma3nRMSNorm.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 66,
    "qualname": "Gemma3nTextMLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_activation: str, activation_sparsity: float, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 105,
    "qualname": "Gemma3nTextMLP.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 135,
    "qualname": "Gemma3nLaurelBlock.__init__",
    "signature": "def __init__(self, config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 163,
    "qualname": "Gemma3nLaurelBlock.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 174,
    "qualname": "Gemma3nAltUp.__init__",
    "signature": "def __init__(self, config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 219,
    "qualname": "Gemma3nAltUp.compute_router_modalities",
    "signature": "def compute_router_modalities(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 230,
    "qualname": "Gemma3nAltUp.predict",
    "signature": "def predict(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 262,
    "qualname": "Gemma3nAltUp.correct",
    "signature": "def correct(self, predictions: torch.Tensor, activated: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 293,
    "qualname": "Gemma3nAltUp.scale_corrected_output",
    "signature": "def scale_corrected_output(self, corrected: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 297,
    "qualname": "Gemma3nAltUp.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, activated: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 316,
    "qualname": "Gemma3nAttention.__init__",
    "signature": "def __init__(self, layer_id: int, config: Gemma3nTextConfig, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 434,
    "qualname": "Gemma3nAttention.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, positions: Tuple[torch.Tensor, torch.Tensor], forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 496,
    "qualname": "Gemma3nDecoderLayer.__init__",
    "signature": "def __init__(self, layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 567,
    "qualname": "Gemma3nDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, per_layer_input: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 629,
    "qualname": "Gemma3nTextModel.__init__",
    "signature": "def __init__(self, config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 724,
    "qualname": "Gemma3nTextModel.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 727,
    "qualname": "Gemma3nTextModel.dtype",
    "signature": "def dtype(self)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 730,
    "qualname": "Gemma3nTextModel.get_per_layer_inputs",
    "signature": "def get_per_layer_inputs(self, input_ids: torch.LongTensor)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 738,
    "qualname": "Gemma3nTextModel.project_per_layer_inputs",
    "signature": "def project_per_layer_inputs(self, inputs_embeds: torch.Tensor, per_layer_inputs: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 765,
    "qualname": "Gemma3nTextModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, per_layer_inputs: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 900,
    "qualname": "Gemma3nForCausalLM.__init__",
    "signature": "def __init__(self, config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 927,
    "qualname": "Gemma3nForCausalLM.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 930,
    "qualname": "Gemma3nForCausalLM.get_attention_sliding_window_size",
    "signature": "def get_attention_sliding_window_size(self)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 933,
    "qualname": "Gemma3nForCausalLM.dtype",
    "signature": "def dtype(self)"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 937,
    "qualname": "Gemma3nForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, per_layer_inputs: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.gemma3n_causal",
    "file": "python/sglang/srt/models/gemma3n_causal.py",
    "line": 959,
    "qualname": "Gemma3nForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.gemma3n_mm",
    "file": "python/sglang/srt/models/gemma3n_mm.py",
    "line": 62,
    "qualname": "Gemma3nMultimodalEmbedder.__init__",
    "signature": "def __init__(self, multimodal_config: Union[Gemma3nAudioConfig, Gemma3nVisionConfig], text_config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3n_mm",
    "file": "python/sglang/srt/models/gemma3n_mm.py",
    "line": 108,
    "qualname": "Gemma3nMultimodalEmbedder.forward",
    "signature": "def forward(self, input_ids: Optional[torch.LongTensor], inputs_embeds: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.gemma3n_mm",
    "file": "python/sglang/srt/models/gemma3n_mm.py",
    "line": 193,
    "qualname": "Gemma3nForConditionalGeneration.__init__",
    "signature": "def __init__(self, config: Gemma3nConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gemma3n_mm",
    "file": "python/sglang/srt/models/gemma3n_mm.py",
    "line": 245,
    "qualname": "Gemma3nForConditionalGeneration.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.gemma3n_mm",
    "file": "python/sglang/srt/models/gemma3n_mm.py",
    "line": 254,
    "qualname": "Gemma3nForConditionalGeneration.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.gemma3n_mm",
    "file": "python/sglang/srt/models/gemma3n_mm.py",
    "line": 257,
    "qualname": "Gemma3nForConditionalGeneration.get_attention_sliding_window_size",
    "signature": "def get_attention_sliding_window_size(self)"
  },
  {
    "module": "srt.models.gemma3n_mm",
    "file": "python/sglang/srt/models/gemma3n_mm.py",
    "line": 260,
    "qualname": "Gemma3nForConditionalGeneration.get_image_feature",
    "signature": "def get_image_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.gemma3n_mm",
    "file": "python/sglang/srt/models/gemma3n_mm.py",
    "line": 308,
    "qualname": "Gemma3nForConditionalGeneration.get_audio_feature",
    "signature": "def get_audio_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.gemma3n_mm",
    "file": "python/sglang/srt/models/gemma3n_mm.py",
    "line": 388,
    "qualname": "Gemma3nForConditionalGeneration.get_per_layer_inputs",
    "signature": "def get_per_layer_inputs(self, input_ids: torch.LongTensor)"
  },
  {
    "module": "srt.models.gemma3n_mm",
    "file": "python/sglang/srt/models/gemma3n_mm.py",
    "line": 393,
    "qualname": "Gemma3nForConditionalGeneration.project_per_layer_inputs",
    "signature": "def project_per_layer_inputs(self, inputs_embeds: torch.Tensor, per_layer_inputs: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.gemma3n_mm",
    "file": "python/sglang/srt/models/gemma3n_mm.py",
    "line": 403,
    "qualname": "Gemma3nForConditionalGeneration.forward",
    "signature": "def forward(self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.gemma3n_mm",
    "file": "python/sglang/srt/models/gemma3n_mm.py",
    "line": 449,
    "qualname": "Gemma3nForConditionalGeneration.tie_weights",
    "signature": "def tie_weights(self)"
  },
  {
    "module": "srt.models.gemma3n_mm",
    "file": "python/sglang/srt/models/gemma3n_mm.py",
    "line": 452,
    "qualname": "Gemma3nForConditionalGeneration.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.gemma3n_mm",
    "file": "python/sglang/srt/models/gemma3n_mm.py",
    "line": 499,
    "qualname": "Gemma3nForConditionalGeneration.should_apply_lora",
    "signature": "def should_apply_lora(self, module_name: str)"
  },
  {
    "module": "srt.models.gemma3n_mm",
    "file": "python/sglang/srt/models/gemma3n_mm.py",
    "line": 502,
    "qualname": "Gemma3nForConditionalGeneration.get_hidden_dim",
    "signature": "def get_hidden_dim(self, module_name)"
  },
  {
    "module": "srt.models.glm4",
    "file": "python/sglang/srt/models/glm4.py",
    "line": 44,
    "qualname": "Glm4Attention.__init__",
    "signature": "def __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.glm4",
    "file": "python/sglang/srt/models/glm4.py",
    "line": 111,
    "qualname": "Glm4Attention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.glm4",
    "file": "python/sglang/srt/models/glm4.py",
    "line": 137,
    "qualname": "Glm4DecoderLayer.__init__",
    "signature": "def __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.glm4",
    "file": "python/sglang/srt/models/glm4.py",
    "line": 168,
    "qualname": "Glm4DecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.glm4",
    "file": "python/sglang/srt/models/glm4.py",
    "line": 197,
    "qualname": "Glm4Model.__init__",
    "signature": "def __init__(self, config: Glm4Config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.glm4",
    "file": "python/sglang/srt/models/glm4.py",
    "line": 221,
    "qualname": "Glm4Model.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.glm4",
    "file": "python/sglang/srt/models/glm4.py",
    "line": 224,
    "qualname": "Glm4Model.dtype",
    "signature": "def dtype(self)"
  },
  {
    "module": "srt.models.glm4",
    "file": "python/sglang/srt/models/glm4.py",
    "line": 228,
    "qualname": "Glm4Model.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.glm4",
    "file": "python/sglang/srt/models/glm4.py",
    "line": 253,
    "qualname": "Glm4ForCausalLM.__init__",
    "signature": "def __init__(self, config: Glm4Config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.glm4",
    "file": "python/sglang/srt/models/glm4.py",
    "line": 275,
    "qualname": "Glm4ForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.glm4",
    "file": "python/sglang/srt/models/glm4.py",
    "line": 286,
    "qualname": "Glm4ForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 116,
    "qualname": "Glm4MoeMLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str, tp_rank: Optional[int], tp_size: Optional[int])"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 156,
    "qualname": "Glm4MoeMLP.forward",
    "signature": "def forward(self, x, forward_batch, should_allreduce_fusion)"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 167,
    "qualname": "Glm4MoeAttention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, partial_rotary_factor: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, head_dim: Optional[int], rms_norm_eps: float, attention_bias: bool, quant_config: Optional[QuantizationConfig], use_qk_norm: bool, prefix: str, alt_stream: Optional[torch.cuda.Stream])"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 280,
    "qualname": "Glm4MoeAttention.op_prepare",
    "signature": "def op_prepare(self, state)"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 287,
    "qualname": "Glm4MoeAttention.op_core",
    "signature": "def op_core(self, state)"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 292,
    "qualname": "Glm4MoeAttention.forward_prepare",
    "signature": "def forward_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 308,
    "qualname": "Glm4MoeAttention.forward_core",
    "signature": "def forward_core(self, intermediate_state)"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 316,
    "qualname": "Glm4MoeAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 331,
    "qualname": "Glm4MoeGate.__init__",
    "signature": "def __init__(self, config, prefix: str, is_nextn: bool)"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 348,
    "qualname": "Glm4MoeGate.forward",
    "signature": "def forward(self, hidden_states)"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 376,
    "qualname": "Glm4MoeSparseMoeBlock.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream], is_nextn: bool)"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 499,
    "qualname": "Glm4MoeSparseMoeBlock.forward_normal_dual_stream",
    "signature": "def forward_normal_dual_stream(self, hidden_states: torch.Tensor, should_allreduce_fusion: bool, use_reduce_scatter: bool)"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 541,
    "qualname": "Glm4MoeSparseMoeBlock.forward_normal",
    "signature": "def forward_normal(self, hidden_states: torch.Tensor, should_allreduce_fusion: bool, use_reduce_scatter: bool)"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 578,
    "qualname": "Glm4MoeDecoderLayer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], is_nextn: bool, prefix: str, alt_stream: Optional[torch.cuda.Stream])"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 662,
    "qualname": "Glm4MoeDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], zero_allocator: BumpAllocator)"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 694,
    "qualname": "Glm4MoeModel.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 731,
    "qualname": "Glm4MoeForCausalLM.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 764,
    "qualname": "Glm4MoeForCausalLM.determine_num_fused_shared_experts",
    "signature": "def determine_num_fused_shared_experts(self, architecture: str)"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 794,
    "qualname": "Glm4MoeForCausalLM.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.glm4_moe",
    "file": "python/sglang/srt/models/glm4_moe.py",
    "line": 797,
    "qualname": "Glm4MoeForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn)"
  },
  {
    "module": "srt.models.glm4_moe_nextn",
    "file": "python/sglang/srt/models/glm4_moe_nextn.py",
    "line": 42,
    "qualname": "Glm4MoeModelNextN.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.glm4_moe_nextn",
    "file": "python/sglang/srt/models/glm4_moe_nextn.py",
    "line": 80,
    "qualname": "Glm4MoeModelNextN.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.glm4_moe_nextn",
    "file": "python/sglang/srt/models/glm4_moe_nextn.py",
    "line": 128,
    "qualname": "Glm4MoeForCausalLMNextN.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.glm4_moe_nextn",
    "file": "python/sglang/srt/models/glm4_moe_nextn.py",
    "line": 153,
    "qualname": "Glm4MoeForCausalLMNextN.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.glm4_moe_nextn",
    "file": "python/sglang/srt/models/glm4_moe_nextn.py",
    "line": 164,
    "qualname": "Glm4MoeForCausalLMNextN.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 38,
    "qualname": "Glm4vRMSNorm.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 47,
    "qualname": "Glm4vVisionMLP.__init__",
    "signature": "def __init__(self, in_features: int, hidden_features: int, bias: bool, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 72,
    "qualname": "Glm4vVisionMLP.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 80,
    "qualname": "Glm4vVisionBlock.__init__",
    "signature": "def __init__(self, config: Glm4vVisionConfig, norm_layer: Optional[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 110,
    "qualname": "Glm4vVisionPatchEmbed.__init__",
    "signature": "def __init__(self, patch_size: int, temporal_patch_size: int, in_channels: int, hidden_size: int)"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 132,
    "qualname": "Glm4vVisionPatchEmbed.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 145,
    "qualname": "Glm4vPatchMerger.__init__",
    "signature": "def __init__(self, d_model: int, context_dim: int, quant_config: Optional[QuantizationConfig], bias: bool, prefix: str)"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 180,
    "qualname": "Glm4vPatchMerger.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 191,
    "qualname": "Glm4vVisionEmbeddings.__init__",
    "signature": "def __init__(self, config: Glm4vVisionConfig)"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 207,
    "qualname": "Glm4vVisionEmbeddings.forward",
    "signature": "def forward(self, embeddings, lengths, image_shapes, h_coords, w_coords)"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 282,
    "qualname": "Glm4vVisionRotaryEmbedding.__init__",
    "signature": "def __init__(self, dim: int, theta: float)"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 291,
    "qualname": "Glm4vVisionRotaryEmbedding.update_freqs_cache",
    "signature": "def update_freqs_cache(self, seqlen: int)"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 314,
    "qualname": "Glm4vVisionRotaryEmbedding.forward",
    "signature": "def forward(self, seqlen: int)"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 320,
    "qualname": "Glm4vVisionModel.__init__",
    "signature": "def __init__(self, vision_config: Glm4vVisionConfig, norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 387,
    "qualname": "Glm4vVisionModel.dtype",
    "signature": "def dtype(self)"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 391,
    "qualname": "Glm4vVisionModel.device",
    "signature": "def device(self)"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 394,
    "qualname": "Glm4vVisionModel.rot_pos_emb",
    "signature": "def rot_pos_emb(self, grid_thw: torch.Tensor)"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 426,
    "qualname": "Glm4vVisionModel.forward",
    "signature": "def forward(self, x: torch.Tensor, grid_thw: torch.Tensor)"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 465,
    "qualname": "Glm4vForConditionalGeneration.__init__",
    "signature": "def __init__(self, config: Glm4vConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 501,
    "qualname": "Glm4vForConditionalGeneration.get_image_feature",
    "signature": "def get_image_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 516,
    "qualname": "Glm4vForConditionalGeneration.get_video_feature",
    "signature": "def get_video_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.glm4v",
    "file": "python/sglang/srt/models/glm4v.py",
    "line": 587,
    "qualname": "Glm4vForConditionalGeneration.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.glm4v_moe",
    "file": "python/sglang/srt/models/glm4v_moe.py",
    "line": 34,
    "qualname": "Glm4vMoeForConditionalGeneration.__init__",
    "signature": "def __init__(self, config: Glm4vMoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.glm4v_moe",
    "file": "python/sglang/srt/models/glm4v_moe.py",
    "line": 77,
    "qualname": "Glm4vMoeForConditionalGeneration.determine_num_fused_shared_experts",
    "signature": "def determine_num_fused_shared_experts(self, architecture: str)"
  },
  {
    "module": "srt.models.glm4v_moe",
    "file": "python/sglang/srt/models/glm4v_moe.py",
    "line": 107,
    "qualname": "Glm4vMoeForConditionalGeneration.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn)"
  },
  {
    "module": "srt.models.gpt2",
    "file": "python/sglang/srt/models/gpt2.py",
    "line": 44,
    "qualname": "GPT2Attention.__init__",
    "signature": "def __init__(self, layer_id: int, config: GPT2Config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gpt2",
    "file": "python/sglang/srt/models/gpt2.py",
    "line": 84,
    "qualname": "GPT2Attention.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.gpt2",
    "file": "python/sglang/srt/models/gpt2.py",
    "line": 98,
    "qualname": "GPT2MLP.__init__",
    "signature": "def __init__(self, intermediate_size: int, config: GPT2Config, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gpt2",
    "file": "python/sglang/srt/models/gpt2.py",
    "line": 124,
    "qualname": "GPT2MLP.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.gpt2",
    "file": "python/sglang/srt/models/gpt2.py",
    "line": 136,
    "qualname": "GPT2Block.__init__",
    "signature": "def __init__(self, layer_id: int, config: GPT2Config, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gpt2",
    "file": "python/sglang/srt/models/gpt2.py",
    "line": 161,
    "qualname": "GPT2Block.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.gpt2",
    "file": "python/sglang/srt/models/gpt2.py",
    "line": 185,
    "qualname": "GPT2Model.__init__",
    "signature": "def __init__(self, config: GPT2Config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gpt2",
    "file": "python/sglang/srt/models/gpt2.py",
    "line": 213,
    "qualname": "GPT2Model.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.gpt2",
    "file": "python/sglang/srt/models/gpt2.py",
    "line": 233,
    "qualname": "GPT2LMHeadModel.__init__",
    "signature": "def __init__(self, config: GPT2Config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gpt2",
    "file": "python/sglang/srt/models/gpt2.py",
    "line": 249,
    "qualname": "GPT2LMHeadModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.gpt2",
    "file": "python/sglang/srt/models/gpt2.py",
    "line": 260,
    "qualname": "GPT2LMHeadModel.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.gpt_bigcode",
    "file": "python/sglang/srt/models/gpt_bigcode.py",
    "line": 43,
    "qualname": "GPTBigCodeAttention.__init__",
    "signature": "def __init__(self, layer_id: int, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gpt_bigcode",
    "file": "python/sglang/srt/models/gpt_bigcode.py",
    "line": 94,
    "qualname": "GPTBigCodeAttention.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.gpt_bigcode",
    "file": "python/sglang/srt/models/gpt_bigcode.py",
    "line": 115,
    "qualname": "GPTBigMLP.__init__",
    "signature": "def __init__(self, intermediate_size: int, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gpt_bigcode",
    "file": "python/sglang/srt/models/gpt_bigcode.py",
    "line": 142,
    "qualname": "GPTBigMLP.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.gpt_bigcode",
    "file": "python/sglang/srt/models/gpt_bigcode.py",
    "line": 151,
    "qualname": "GPTBigCodeBlock.__init__",
    "signature": "def __init__(self, layer_id: int, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gpt_bigcode",
    "file": "python/sglang/srt/models/gpt_bigcode.py",
    "line": 171,
    "qualname": "GPTBigCodeBlock.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.gpt_bigcode",
    "file": "python/sglang/srt/models/gpt_bigcode.py",
    "line": 194,
    "qualname": "GPTBigCodeModel.__init__",
    "signature": "def __init__(self, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gpt_bigcode",
    "file": "python/sglang/srt/models/gpt_bigcode.py",
    "line": 224,
    "qualname": "GPTBigCodeModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.gpt_bigcode",
    "file": "python/sglang/srt/models/gpt_bigcode.py",
    "line": 254,
    "qualname": "GPTBigCodeForCausalLM.__init__",
    "signature": "def __init__(self, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gpt_bigcode",
    "file": "python/sglang/srt/models/gpt_bigcode.py",
    "line": 273,
    "qualname": "GPTBigCodeForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.gpt_bigcode",
    "file": "python/sglang/srt/models/gpt_bigcode.py",
    "line": 284,
    "qualname": "GPTBigCodeForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.granite",
    "file": "python/sglang/srt/models/granite.py",
    "line": 52,
    "qualname": "GraniteMLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.granite",
    "file": "python/sglang/srt/models/granite.py",
    "line": 82,
    "qualname": "GraniteMLP.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.granite",
    "file": "python/sglang/srt/models/granite.py",
    "line": 90,
    "qualname": "GraniteAttention.__init__",
    "signature": "def __init__(self, config: GraniteConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.granite",
    "file": "python/sglang/srt/models/granite.py",
    "line": 165,
    "qualname": "GraniteAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.granite",
    "file": "python/sglang/srt/models/granite.py",
    "line": 180,
    "qualname": "GraniteDecoderLayer.__init__",
    "signature": "def __init__(self, config: GraniteConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.granite",
    "file": "python/sglang/srt/models/granite.py",
    "line": 225,
    "qualname": "GraniteDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.granite",
    "file": "python/sglang/srt/models/granite.py",
    "line": 254,
    "qualname": "GraniteModel.__init__",
    "signature": "def __init__(self, config: GraniteConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.granite",
    "file": "python/sglang/srt/models/granite.py",
    "line": 280,
    "qualname": "GraniteModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.granite",
    "file": "python/sglang/srt/models/granite.py",
    "line": 306,
    "qualname": "GraniteForCausalLM.__init__",
    "signature": "def __init__(self, config: GraniteConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.granite",
    "file": "python/sglang/srt/models/granite.py",
    "line": 349,
    "qualname": "GraniteForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)"
  },
  {
    "module": "srt.models.granite",
    "file": "python/sglang/srt/models/granite.py",
    "line": 366,
    "qualname": "GraniteForCausalLM.get_module_name_from_weight_name",
    "signature": "def get_module_name_from_weight_name(self, name)"
  },
  {
    "module": "srt.models.granite",
    "file": "python/sglang/srt/models/granite.py",
    "line": 375,
    "qualname": "GraniteForCausalLM.get_num_params",
    "signature": "def get_num_params(self)"
  },
  {
    "module": "srt.models.granite",
    "file": "python/sglang/srt/models/granite.py",
    "line": 379,
    "qualname": "GraniteForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.granite",
    "file": "python/sglang/srt/models/granite.py",
    "line": 431,
    "qualname": "GraniteForCausalLM.get_weights_by_name",
    "signature": "def get_weights_by_name(self, name: str, truncate_size: int, tp_size: int)"
  },
  {
    "module": "srt.models.granitemoe",
    "file": "python/sglang/srt/models/granitemoe.py",
    "line": 40,
    "qualname": "GraniteMoeMoE.__init__",
    "signature": "def __init__(self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], tp_size: Optional[int], prefix: str)"
  },
  {
    "module": "srt.models.granitemoe",
    "file": "python/sglang/srt/models/granitemoe.py",
    "line": 82,
    "qualname": "GraniteMoeMoE.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.granitemoe",
    "file": "python/sglang/srt/models/granitemoe.py",
    "line": 94,
    "qualname": "GraniteMoeAttention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, max_position: int, layer_id: int, rope_theta: float, quant_config: Optional[QuantizationConfig], attention_multiplier: Optional[float], prefix: str)"
  },
  {
    "module": "srt.models.granitemoe",
    "file": "python/sglang/srt/models/granitemoe.py",
    "line": 165,
    "qualname": "GraniteMoeAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.granitemoe",
    "file": "python/sglang/srt/models/granitemoe.py",
    "line": 181,
    "qualname": "GraniteMoeDecoderLayer.__init__",
    "signature": "def __init__(self, config: GraniteConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.granitemoe",
    "file": "python/sglang/srt/models/granitemoe.py",
    "line": 219,
    "qualname": "GraniteMoeDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.granitemoe",
    "file": "python/sglang/srt/models/granitemoe.py",
    "line": 244,
    "qualname": "GraniteMoeModel.__init__",
    "signature": "def __init__(self, config: GraniteConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.granitemoe",
    "file": "python/sglang/srt/models/granitemoe.py",
    "line": 271,
    "qualname": "GraniteMoeModel.get_input_embeddings",
    "signature": "def get_input_embeddings(self, input_ids: torch.Tensor)"
  },
  {
    "module": "srt.models.granitemoe",
    "file": "python/sglang/srt/models/granitemoe.py",
    "line": 274,
    "qualname": "GraniteMoeModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.granitemoe",
    "file": "python/sglang/srt/models/granitemoe.py",
    "line": 300,
    "qualname": "GraniteMoeForCausalLM.__init__",
    "signature": "def __init__(self, config: GraniteConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.granitemoe",
    "file": "python/sglang/srt/models/granitemoe.py",
    "line": 331,
    "qualname": "GraniteMoeForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)"
  },
  {
    "module": "srt.models.granitemoe",
    "file": "python/sglang/srt/models/granitemoe.py",
    "line": 348,
    "qualname": "GraniteMoeForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.grok",
    "file": "python/sglang/srt/models/grok.py",
    "line": 88,
    "qualname": "Grok1MLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, reduce_results, use_presharded_weights: bool, split_gate_up: bool)"
  },
  {
    "module": "srt.models.grok",
    "file": "python/sglang/srt/models/grok.py",
    "line": 121,
    "qualname": "Grok1MLP.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.grok",
    "file": "python/sglang/srt/models/grok.py",
    "line": 137,
    "qualname": "Grok1MoE.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], tp_size: Optional[int], reduce_results: bool, use_presharded_weights: bool, inplace: bool, no_combine: bool, prefix: str)"
  },
  {
    "module": "srt.models.grok",
    "file": "python/sglang/srt/models/grok.py",
    "line": 201,
    "qualname": "Grok1MoE.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.grok",
    "file": "python/sglang/srt/models/grok.py",
    "line": 218,
    "qualname": "get_rope_scaling",
    "signature": "def get_rope_scaling(config)"
  },
  {
    "module": "srt.models.grok",
    "file": "python/sglang/srt/models/grok.py",
    "line": 247,
    "qualname": "ScalingRotaryEmbedding.__init__",
    "signature": "def __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype)"
  },
  {
    "module": "srt.models.grok",
    "file": "python/sglang/srt/models/grok.py",
    "line": 337,
    "qualname": "Grok1Attention.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, max_position: int, rope_theta: float, quant_config: Optional[QuantizationConfig], reduce_results: bool, alt_stream: Optional[torch.cuda.Stream], load_presharded_attn: bool, prefix: str)"
  },
  {
    "module": "srt.models.grok",
    "file": "python/sglang/srt/models/grok.py",
    "line": 457,
    "qualname": "Grok1Attention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.grok",
    "file": "python/sglang/srt/models/grok.py",
    "line": 538,
    "qualname": "Grok1DecoderLayer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], load_presharded_moe: bool, load_presharded_attn: bool, load_presharded_mlp: bool, alt_stream: Optional[torch.cuda.Stream], skip_moe: bool, prefix: str)"
  },
  {
    "module": "srt.models.grok",
    "file": "python/sglang/srt/models/grok.py",
    "line": 630,
    "qualname": "Grok1DecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], deferred_norm: Optional[RMSNorm])"
  },
  {
    "module": "srt.models.grok",
    "file": "python/sglang/srt/models/grok.py",
    "line": 696,
    "qualname": "Grok1DecoderLayer.moe_with_rmoe",
    "signature": "def moe_with_rmoe(self, x)"
  },
  {
    "module": "srt.models.grok",
    "file": "python/sglang/srt/models/grok.py",
    "line": 708,
    "qualname": "Grok1Model.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], load_presharded_moe: bool, load_presharded_embedding: bool, load_presharded_attn: bool, load_presharded_mlp: bool, replicate_embedding: bool, prefix: str)"
  },
  {
    "module": "srt.models.grok",
    "file": "python/sglang/srt/models/grok.py",
    "line": 749,
    "qualname": "Grok1Model.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.grok",
    "file": "python/sglang/srt/models/grok.py",
    "line": 808,
    "qualname": "Grok1ForCausalLM.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.grok",
    "file": "python/sglang/srt/models/grok.py",
    "line": 893,
    "qualname": "Grok1ForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.grok",
    "file": "python/sglang/srt/models/grok.py",
    "line": 908,
    "qualname": "Grok1ForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], ignore_parent_name: bool, check_hit_names: bool, model_config: PretrainedConfig | None)"
  },
  {
    "module": "srt.models.grok",
    "file": "python/sglang/srt/models/grok.py",
    "line": 1025,
    "qualname": "Grok1ForCausalLM.get_num_params_analytical",
    "signature": "def get_num_params_analytical(self)"
  },
  {
    "module": "srt.models.grok",
    "file": "python/sglang/srt/models/grok.py",
    "line": 1073,
    "qualname": "Grok1ForCausalLM.get_num_params_torch",
    "signature": "def get_num_params_torch(self)"
  },
  {
    "module": "srt.models.hunyuan",
    "file": "python/sglang/srt/models/hunyuan.py",
    "line": 82,
    "qualname": "HunYuanMLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], bias: bool, prefix: str, reduce_results: bool)"
  },
  {
    "module": "srt.models.hunyuan",
    "file": "python/sglang/srt/models/hunyuan.py",
    "line": 115,
    "qualname": "HunYuanMLP.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.hunyuan",
    "file": "python/sglang/srt/models/hunyuan.py",
    "line": 124,
    "qualname": "HunYuanSparseMoeBlock.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], layer_id: int)"
  },
  {
    "module": "srt.models.hunyuan",
    "file": "python/sglang/srt/models/hunyuan.py",
    "line": 192,
    "qualname": "HunYuanSparseMoeBlock.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.hunyuan",
    "file": "python/sglang/srt/models/hunyuan.py",
    "line": 213,
    "qualname": "get_head_dim",
    "signature": "def get_head_dim(config)"
  },
  {
    "module": "srt.models.hunyuan",
    "file": "python/sglang/srt/models/hunyuan.py",
    "line": 224,
    "qualname": "check_head_dim",
    "signature": "def check_head_dim(config)"
  },
  {
    "module": "srt.models.hunyuan",
    "file": "python/sglang/srt/models/hunyuan.py",
    "line": 251,
    "qualname": "HunYuanAttention.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], bias: bool, prefix: str, attention_type: str, layer_id: int)"
  },
  {
    "module": "srt.models.hunyuan",
    "file": "python/sglang/srt/models/hunyuan.py",
    "line": 351,
    "qualname": "HunYuanAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, kv_states: Optional[Tuple[torch.Tensor]])"
  },
  {
    "module": "srt.models.hunyuan",
    "file": "python/sglang/srt/models/hunyuan.py",
    "line": 392,
    "qualname": "HunYuanDecoderLayer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str, layer_id: int)"
  },
  {
    "module": "srt.models.hunyuan",
    "file": "python/sglang/srt/models/hunyuan.py",
    "line": 462,
    "qualname": "HunYuanDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], kv_states: Optional[Tuple[torch.Tensor]])"
  },
  {
    "module": "srt.models.hunyuan",
    "file": "python/sglang/srt/models/hunyuan.py",
    "line": 491,
    "qualname": "HunYuanModel.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.hunyuan",
    "file": "python/sglang/srt/models/hunyuan.py",
    "line": 522,
    "qualname": "HunYuanModel.get_input_embeddings",
    "signature": "def get_input_embeddings(self, input_ids: torch.Tensor)"
  },
  {
    "module": "srt.models.hunyuan",
    "file": "python/sglang/srt/models/hunyuan.py",
    "line": 525,
    "qualname": "HunYuanModel.forward",
    "signature": "def forward(self, input_ids: Optional[torch.Tensor], positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.hunyuan",
    "file": "python/sglang/srt/models/hunyuan.py",
    "line": 585,
    "qualname": "HunYuanMoEV1ForCausalLM.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig])"
  },
  {
    "module": "srt.models.hunyuan",
    "file": "python/sglang/srt/models/hunyuan.py",
    "line": 613,
    "qualname": "HunYuanMoEV1ForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.hunyuan",
    "file": "python/sglang/srt/models/hunyuan.py",
    "line": 642,
    "qualname": "HunYuanMoEV1ForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.hunyuan",
    "file": "python/sglang/srt/models/hunyuan.py",
    "line": 788,
    "qualname": "HunYuanMoEV1ForCausalLM.load_kv_cache_scales",
    "signature": "def load_kv_cache_scales(self, quantization_param_path: str)"
  },
  {
    "module": "srt.models.idefics2",
    "file": "python/sglang/srt/models/idefics2.py",
    "line": 36,
    "qualname": "Idefics2VisionMLP.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.idefics2",
    "file": "python/sglang/srt/models/idefics2.py",
    "line": 60,
    "qualname": "Idefics2VisionMLP.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.idefics2",
    "file": "python/sglang/srt/models/idefics2.py",
    "line": 69,
    "qualname": "Idefics2EncoderLayer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.idefics2",
    "file": "python/sglang/srt/models/idefics2.py",
    "line": 98,
    "qualname": "Idefics2EncoderLayer.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor)"
  },
  {
    "module": "srt.models.idefics2",
    "file": "python/sglang/srt/models/idefics2.py",
    "line": 131,
    "qualname": "Idefics2Encoder.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.idefics2",
    "file": "python/sglang/srt/models/idefics2.py",
    "line": 151,
    "qualname": "Idefics2Encoder.forward",
    "signature": "def forward(self, inputs_embeds: torch.Tensor, cu_seqlens: torch.Tensor)"
  },
  {
    "module": "srt.models.idefics2",
    "file": "python/sglang/srt/models/idefics2.py",
    "line": 189,
    "qualname": "Idefics2VisionEmbeddings.__init__",
    "signature": "def __init__(self, config: PretrainedConfig)"
  },
  {
    "module": "srt.models.idefics2",
    "file": "python/sglang/srt/models/idefics2.py",
    "line": 206,
    "qualname": "Idefics2VisionEmbeddings.get_position_ids",
    "signature": "def get_position_ids(self, pixel_values: torch.FloatTensor, patch_attention_mask: torch.BoolTensor, tgt_sizes: Optional[torch.IntTensor])"
  },
  {
    "module": "srt.models.idefics2",
    "file": "python/sglang/srt/models/idefics2.py",
    "line": 248,
    "qualname": "Idefics2VisionEmbeddings.forward",
    "signature": "def forward(self, pixel_values: torch.FloatTensor, patch_attention_mask: torch.BoolTensor, tgt_sizes: Optional[torch.IntTensor])"
  },
  {
    "module": "srt.models.idefics2",
    "file": "python/sglang/srt/models/idefics2.py",
    "line": 270,
    "qualname": "Idefics2VisionTransformer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], require_post_norm: bool, prefix: str)"
  },
  {
    "module": "srt.models.idefics2",
    "file": "python/sglang/srt/models/idefics2.py",
    "line": 293,
    "qualname": "Idefics2VisionTransformer.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.idefics2",
    "file": "python/sglang/srt/models/idefics2.py",
    "line": 296,
    "qualname": "Idefics2VisionTransformer.compute_cu_seqlens",
    "signature": "def compute_cu_seqlens(self, tgt_sizes: Optional[torch.Tensor], input_embeds: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.idefics2",
    "file": "python/sglang/srt/models/idefics2.py",
    "line": 325,
    "qualname": "Idefics2VisionTransformer.forward",
    "signature": "def forward(self, pixel_values, patch_attention_mask: Optional[torch.BoolTensor], tgt_sizes: Optional[torch.IntTensor])"
  },
  {
    "module": "srt.models.internlm2",
    "file": "python/sglang/srt/models/internlm2.py",
    "line": 45,
    "qualname": "InternLM2MLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.internlm2",
    "file": "python/sglang/srt/models/internlm2.py",
    "line": 75,
    "qualname": "InternLM2MLP.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.internlm2",
    "file": "python/sglang/srt/models/internlm2.py",
    "line": 83,
    "qualname": "InternLM2Attention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.internlm2",
    "file": "python/sglang/srt/models/internlm2.py",
    "line": 152,
    "qualname": "InternLM2Attention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.internlm2",
    "file": "python/sglang/srt/models/internlm2.py",
    "line": 167,
    "qualname": "InternLMDecoderLayer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.internlm2",
    "file": "python/sglang/srt/models/internlm2.py",
    "line": 200,
    "qualname": "InternLMDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.internlm2",
    "file": "python/sglang/srt/models/internlm2.py",
    "line": 226,
    "qualname": "InternLM2Model.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.internlm2",
    "file": "python/sglang/srt/models/internlm2.py",
    "line": 251,
    "qualname": "InternLM2Model.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.internlm2",
    "file": "python/sglang/srt/models/internlm2.py",
    "line": 276,
    "qualname": "InternLM2ForCausalLM.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.internlm2",
    "file": "python/sglang/srt/models/internlm2.py",
    "line": 293,
    "qualname": "InternLM2ForCausalLM.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.internlm2",
    "file": "python/sglang/srt/models/internlm2.py",
    "line": 297,
    "qualname": "InternLM2ForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.internlm2",
    "file": "python/sglang/srt/models/internlm2.py",
    "line": 309,
    "qualname": "InternLM2ForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.internlm2_reward",
    "file": "python/sglang/srt/models/internlm2_reward.py",
    "line": 29,
    "qualname": "InternLM2ForRewardModel.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.internlm2_reward",
    "file": "python/sglang/srt/models/internlm2_reward.py",
    "line": 46,
    "qualname": "InternLM2ForRewardModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)"
  },
  {
    "module": "srt.models.internlm2_reward",
    "file": "python/sglang/srt/models/internlm2_reward.py",
    "line": 60,
    "qualname": "InternLM2ForRewardModel.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.interns1",
    "file": "python/sglang/srt/models/interns1.py",
    "line": 30,
    "qualname": "InternS1ForConditionalGeneration.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], use_flash_attn)"
  },
  {
    "module": "srt.models.interns1",
    "file": "python/sglang/srt/models/interns1.py",
    "line": 95,
    "qualname": "InternS1ForConditionalGeneration.pixel_shuffle",
    "signature": "def pixel_shuffle(self, x, scale_factor)"
  },
  {
    "module": "srt.models.interns1",
    "file": "python/sglang/srt/models/interns1.py",
    "line": 117,
    "qualname": "InternS1ForConditionalGeneration.extract_feature",
    "signature": "def extract_feature(self, pixel_values)"
  },
  {
    "module": "srt.models.interns1",
    "file": "python/sglang/srt/models/interns1.py",
    "line": 135,
    "qualname": "InternS1ForConditionalGeneration.get_image_feature",
    "signature": "def get_image_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.interns1",
    "file": "python/sglang/srt/models/interns1.py",
    "line": 147,
    "qualname": "InternS1ForConditionalGeneration.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.interns1",
    "file": "python/sglang/srt/models/interns1.py",
    "line": 167,
    "qualname": "InternS1ForConditionalGeneration.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.interns1",
    "file": "python/sglang/srt/models/interns1.py",
    "line": 209,
    "qualname": "InternS1ForConditionalGeneration.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 36,
    "qualname": "InternAttention.__init__",
    "signature": "def __init__(self, config, quant_config: QuantizationConfig)"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 66,
    "qualname": "InternAttention.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor)"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 77,
    "qualname": "InternVisionEmbeddings.__init__",
    "signature": "def __init__(self, config: PretrainedConfig)"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 130,
    "qualname": "InternVisionEmbeddings.forward",
    "signature": "def forward(self, pixel_values: torch.FloatTensor)"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 151,
    "qualname": "InternRMSNorm.__init__",
    "signature": "def __init__(self, hidden_size, eps)"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 156,
    "qualname": "InternRMSNorm.forward",
    "signature": "def forward(self, hidden_states)"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 165,
    "qualname": "InternMLP.__init__",
    "signature": "def __init__(self, config: PretrainedConfig)"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 172,
    "qualname": "InternMLP.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 187,
    "qualname": "InternVisionEncoderLayer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, drop_path_rate: float, quant_config: QuantizationConfig)"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 211,
    "qualname": "InternVisionEncoderLayer.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor)"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 249,
    "qualname": "InternVisionEncoder.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig])"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 268,
    "qualname": "InternVisionEncoder.forward",
    "signature": "def forward(self, inputs_embeds, output_hidden_states: Optional[bool], return_dict: Optional[bool])"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 320,
    "qualname": "InternVisionModel.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig])"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 333,
    "qualname": "InternVisionModel.resize_pos_embeddings",
    "signature": "def resize_pos_embeddings(self, old_size, new_size, patch_size)"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 356,
    "qualname": "InternVisionModel.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 359,
    "qualname": "InternVisionModel.forward",
    "signature": "def forward(self, pixel_values: Optional[torch.FloatTensor], output_hidden_states: Optional[bool], return_dict: Optional[bool], pixel_embeds: Optional[torch.FloatTensor])"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 406,
    "qualname": "InternVLChatModel.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], use_flash_attn)"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 465,
    "qualname": "InternVLChatModel.pixel_shuffle",
    "signature": "def pixel_shuffle(self, x, scale_factor)"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 487,
    "qualname": "InternVLChatModel.extract_feature",
    "signature": "def extract_feature(self, pixel_values)"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 505,
    "qualname": "InternVLChatModel.get_image_feature",
    "signature": "def get_image_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 517,
    "qualname": "InternVLChatModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 537,
    "qualname": "InternVLChatModel.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.internvl",
    "file": "python/sglang/srt/models/internvl.py",
    "line": 547,
    "qualname": "InternVLChatModel.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.kimi_vl",
    "file": "python/sglang/srt/models/kimi_vl.py",
    "line": 96,
    "qualname": "KimiVLMultiModalProjector.__init__",
    "signature": "def __init__(self, config: KimiVLConfig)"
  },
  {
    "module": "srt.models.kimi_vl",
    "file": "python/sglang/srt/models/kimi_vl.py",
    "line": 113,
    "qualname": "KimiVLMultiModalProjector.forward",
    "signature": "def forward(self, image_features: torch.Tensor)"
  },
  {
    "module": "srt.models.kimi_vl",
    "file": "python/sglang/srt/models/kimi_vl.py",
    "line": 122,
    "qualname": "KimiVLForConditionalGeneration.__init__",
    "signature": "def __init__(self, config: KimiVLConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.kimi_vl",
    "file": "python/sglang/srt/models/kimi_vl.py",
    "line": 145,
    "qualname": "KimiVLForConditionalGeneration.get_image_feature",
    "signature": "def get_image_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.kimi_vl",
    "file": "python/sglang/srt/models/kimi_vl.py",
    "line": 160,
    "qualname": "KimiVLForConditionalGeneration.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.kimi_vl",
    "file": "python/sglang/srt/models/kimi_vl.py",
    "line": 164,
    "qualname": "KimiVLForConditionalGeneration.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)"
  },
  {
    "module": "srt.models.kimi_vl",
    "file": "python/sglang/srt/models/kimi_vl.py",
    "line": 183,
    "qualname": "KimiVLForConditionalGeneration.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.kimi_vl",
    "file": "python/sglang/srt/models/kimi_vl.py",
    "line": 300,
    "qualname": "get_spec_layer_idx_from_weight_name",
    "signature": "def get_spec_layer_idx_from_weight_name(config: DeepseekV2Config, weight_name: str)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 63,
    "qualname": "multihead_attention",
    "signature": "def multihead_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, q_cu_seqlens: Optional[torch.Tensor], k_cu_seqlens: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 115,
    "qualname": "sdpa_attention",
    "signature": "def sdpa_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, q_cu_seqlens: Optional[torch.Tensor], k_cu_seqlens: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 170,
    "qualname": "apply_rope",
    "signature": "def apply_rope(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 195,
    "qualname": "Learnable2DInterpPosEmb.__init__",
    "signature": "def __init__(self, height: int, width: int, dim: int, interpolation_mode: str)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 205,
    "qualname": "Learnable2DInterpPosEmb.reset_parameters",
    "signature": "def reset_parameters(self)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 208,
    "qualname": "Learnable2DInterpPosEmb.forward",
    "signature": "def forward(self, x: torch.Tensor, grid_hws: torch.Tensor)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 230,
    "qualname": "MoonVisionPatchEmbed.__init__",
    "signature": "def __init__(self, out_dim: int, in_dim: int, patch_size: Union[int, Tuple[int, int]], pos_emb_height: int, pos_emb_width: int)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 257,
    "qualname": "MoonVisionPatchEmbed.forward",
    "signature": "def forward(self, x: torch.Tensor, grid_hw: torch.Tensor)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 294,
    "qualname": "Rope2DPosEmb.__init__",
    "signature": "def __init__(self, dim: int, max_height: int, max_width: int, theta_base, device)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 305,
    "qualname": "Rope2DPosEmb.extra_repr",
    "signature": "def extra_repr(self)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 309,
    "qualname": "Rope2DPosEmb.precomputed_freqs_cis",
    "signature": "def precomputed_freqs_cis(self)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 337,
    "qualname": "Rope2DPosEmb.get_freqs_cis_by_seqlens",
    "signature": "def get_freqs_cis_by_seqlens(self, grid_hws: torch.Tensor)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 361,
    "qualname": "Rope2DPosEmb.get_freqs_cis_by_idx",
    "signature": "def get_freqs_cis_by_idx(self, pos_idx: torch.Tensor, pos_idx_mask: torch.Tensor)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 396,
    "qualname": "MLP2.__init__",
    "signature": "def __init__(self, dims: list[int], activation, bias)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 407,
    "qualname": "MLP2.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 415,
    "qualname": "MoonVitEncoderLayer.__init__",
    "signature": "def __init__(self, num_heads: int, hidden_dim: int, mlp_dim: int)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 437,
    "qualname": "MoonVitEncoderLayer.attention_qkvpacked",
    "signature": "def attention_qkvpacked(self, x: torch.Tensor, cu_seqlens: torch.Tensor, rope_freqs_cis: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 469,
    "qualname": "MoonVitEncoderLayer.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor, rope_freqs_cis: Union[torch.Tensor, None])"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 497,
    "qualname": "MoonVitEncoder.__init__",
    "signature": "def __init__(self, hidden_dim: int, num_layers: int, block_cfg: dict)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 513,
    "qualname": "MoonVitEncoder.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, grid_hw: torch.Tensor)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 536,
    "qualname": "patch_merger",
    "signature": "def patch_merger(x: torch.Tensor, grid_hw: torch.Tensor, merge_kernel_size: list[int, int])"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 567,
    "qualname": "MoonVitVLProjector.__init__",
    "signature": "def __init__(self, in_channels: int, merge_kernel_size: list[int, int], hidden_act: str, ln_eps: float, out_dim: int)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 583,
    "qualname": "MoonVitVLProjector.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 598,
    "qualname": "MoonVitPretrainedModel.__init__",
    "signature": "def __init__(self, config: MoonViTConfig)"
  },
  {
    "module": "srt.models.kimi_vl_moonvit",
    "file": "python/sglang/srt/models/kimi_vl_moonvit.py",
    "line": 623,
    "qualname": "MoonVitPretrainedModel.forward",
    "signature": "def forward(self, pixel_values: torch.Tensor, grid_hw: torch.Tensor)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 62,
    "qualname": "LlamaMLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str, reduce_results: bool)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 94,
    "qualname": "LlamaMLP.forward",
    "signature": "def forward(self, x, forward_batch, use_reduce_scatter: bool)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 110,
    "qualname": "LlamaAttention.__init__",
    "signature": "def __init__(self, config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str, bias: bool)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 188,
    "qualname": "LlamaAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 203,
    "qualname": "LlamaDecoderLayer.__init__",
    "signature": "def __init__(self, config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 253,
    "qualname": "LlamaDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 279,
    "qualname": "LlamaModel.__init__",
    "signature": "def __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 316,
    "qualname": "LlamaModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 367,
    "qualname": "LlamaModel.load_kv_cache_scales",
    "signature": "def load_kv_cache_scales(self, quantization_param_path: str)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 411,
    "qualname": "LlamaForCausalLM.__init__",
    "signature": "def __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 456,
    "qualname": "LlamaForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 492,
    "qualname": "LlamaForCausalLM.forward_split_prefill",
    "signature": "def forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 533,
    "qualname": "LlamaForCausalLM.start_layer",
    "signature": "def start_layer(self)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 537,
    "qualname": "LlamaForCausalLM.end_layer",
    "signature": "def end_layer(self)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 540,
    "qualname": "LlamaForCausalLM.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 543,
    "qualname": "LlamaForCausalLM.get_module_name_from_weight_name",
    "signature": "def get_module_name_from_weight_name(self, name)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 552,
    "qualname": "LlamaForCausalLM.get_num_params",
    "signature": "def get_num_params(self)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 556,
    "qualname": "LlamaForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 624,
    "qualname": "LlamaForCausalLM.get_weights_by_name",
    "signature": "def get_weights_by_name(self, name: str, truncate_size: int, tp_size: int)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 697,
    "qualname": "LlamaForCausalLM.get_embed_and_head",
    "signature": "def get_embed_and_head(self)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 700,
    "qualname": "LlamaForCausalLM.set_embed_and_head",
    "signature": "def set_embed_and_head(self, embed, head)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 708,
    "qualname": "LlamaForCausalLM.get_embed",
    "signature": "def get_embed(self)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 711,
    "qualname": "LlamaForCausalLM.set_embed",
    "signature": "def set_embed(self, embed)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 723,
    "qualname": "LlamaForCausalLM.load_kv_cache_scales",
    "signature": "def load_kv_cache_scales(self, quantization_param_path: str)"
  },
  {
    "module": "srt.models.llama",
    "file": "python/sglang/srt/models/llama.py",
    "line": 726,
    "qualname": "LlamaForCausalLM.set_eagle3_layers_to_capture",
    "signature": "def set_eagle3_layers_to_capture(self, layer_ids: Optional[List[int]])"
  },
  {
    "module": "srt.models.llama4",
    "file": "python/sglang/srt/models/llama4.py",
    "line": 71,
    "qualname": "Llama4MoE.custom_routing_function",
    "signature": "def custom_routing_function(hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool)"
  },
  {
    "module": "srt.models.llama4",
    "file": "python/sglang/srt/models/llama4.py",
    "line": 86,
    "qualname": "Llama4MoE.__init__",
    "signature": "def __init__(self, config: Llama4TextConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llama4",
    "file": "python/sglang/srt/models/llama4.py",
    "line": 133,
    "qualname": "Llama4MoE.forward",
    "signature": "def forward(self, hidden_states, forward_batch: ForwardBatch, use_reduce_scatter: bool)"
  },
  {
    "module": "srt.models.llama4",
    "file": "python/sglang/srt/models/llama4.py",
    "line": 193,
    "qualname": "Llama4Attention.__init__",
    "signature": "def __init__(self, config: Llama4TextConfig, layer_id: int, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], bias: bool, bias_o_proj: bool, prefix: str)"
  },
  {
    "module": "srt.models.llama4",
    "file": "python/sglang/srt/models/llama4.py",
    "line": 317,
    "qualname": "Llama4Attention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.llama4",
    "file": "python/sglang/srt/models/llama4.py",
    "line": 353,
    "qualname": "Llama4DecoderLayer.__init__",
    "signature": "def __init__(self, config: Llama4TextConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llama4",
    "file": "python/sglang/srt/models/llama4.py",
    "line": 426,
    "qualname": "Llama4DecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.llama4",
    "file": "python/sglang/srt/models/llama4.py",
    "line": 465,
    "qualname": "Llama4Model.__init__",
    "signature": "def __init__(self, config: Llama4TextConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llama4",
    "file": "python/sglang/srt/models/llama4.py",
    "line": 493,
    "qualname": "Llama4Model.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.llama4",
    "file": "python/sglang/srt/models/llama4.py",
    "line": 532,
    "qualname": "Llama4ForCausalLM.__init__",
    "signature": "def __init__(self, config: Llama4TextConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llama4",
    "file": "python/sglang/srt/models/llama4.py",
    "line": 540,
    "qualname": "Llama4ForCausalLM.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.llama_classification",
    "file": "python/sglang/srt/models/llama_classification.py",
    "line": 30,
    "qualname": "LlamaForClassification.__init__",
    "signature": "def __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llama_classification",
    "file": "python/sglang/srt/models/llama_classification.py",
    "line": 49,
    "qualname": "LlamaForClassification.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)"
  },
  {
    "module": "srt.models.llama_classification",
    "file": "python/sglang/srt/models/llama_classification.py",
    "line": 67,
    "qualname": "LlamaForClassification.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.llama_eagle",
    "file": "python/sglang/srt/models/llama_eagle.py",
    "line": 40,
    "qualname": "LlamaDecoderLayer.__init__",
    "signature": "def __init__(self, config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llama_eagle",
    "file": "python/sglang/srt/models/llama_eagle.py",
    "line": 57,
    "qualname": "LlamaModel.__init__",
    "signature": "def __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llama_eagle",
    "file": "python/sglang/srt/models/llama_eagle.py",
    "line": 84,
    "qualname": "LlamaModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.llama_eagle",
    "file": "python/sglang/srt/models/llama_eagle.py",
    "line": 114,
    "qualname": "LlamaForCausalLMEagle.__init__",
    "signature": "def __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llama_eagle",
    "file": "python/sglang/srt/models/llama_eagle.py",
    "line": 142,
    "qualname": "LlamaForCausalLMEagle.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.llama_eagle3",
    "file": "python/sglang/srt/models/llama_eagle3.py",
    "line": 43,
    "qualname": "LlamaDecoderLayer.__init__",
    "signature": "def __init__(self, config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llama_eagle3",
    "file": "python/sglang/srt/models/llama_eagle3.py",
    "line": 74,
    "qualname": "LlamaDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, embeds: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.llama_eagle3",
    "file": "python/sglang/srt/models/llama_eagle3.py",
    "line": 104,
    "qualname": "LlamaModel.__init__",
    "signature": "def __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llama_eagle3",
    "file": "python/sglang/srt/models/llama_eagle3.py",
    "line": 134,
    "qualname": "LlamaModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.llama_eagle3",
    "file": "python/sglang/srt/models/llama_eagle3.py",
    "line": 169,
    "qualname": "LlamaForCausalLMEagle3.__init__",
    "signature": "def __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llama_eagle3",
    "file": "python/sglang/srt/models/llama_eagle3.py",
    "line": 206,
    "qualname": "LlamaForCausalLMEagle3.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.llama_eagle3",
    "file": "python/sglang/srt/models/llama_eagle3.py",
    "line": 249,
    "qualname": "LlamaForCausalLMEagle3.get_hot_token_id",
    "signature": "def get_hot_token_id(self)"
  },
  {
    "module": "srt.models.llama_embedding",
    "file": "python/sglang/srt/models/llama_embedding.py",
    "line": 15,
    "qualname": "LlamaEmbeddingModel.__init__",
    "signature": "def __init__(self, config: LlamaConfig, quant_config, prefix: str)"
  },
  {
    "module": "srt.models.llama_embedding",
    "file": "python/sglang/srt/models/llama_embedding.py",
    "line": 28,
    "qualname": "LlamaEmbeddingModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)"
  },
  {
    "module": "srt.models.llama_embedding",
    "file": "python/sglang/srt/models/llama_embedding.py",
    "line": 42,
    "qualname": "LlamaEmbeddingModel.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.llama_reward",
    "file": "python/sglang/srt/models/llama_reward.py",
    "line": 29,
    "qualname": "LlamaForSequenceClassification.__init__",
    "signature": "def __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llama_reward",
    "file": "python/sglang/srt/models/llama_reward.py",
    "line": 48,
    "qualname": "LlamaForSequenceClassification.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)"
  },
  {
    "module": "srt.models.llama_reward",
    "file": "python/sglang/srt/models/llama_reward.py",
    "line": 66,
    "qualname": "LlamaForSequenceClassification.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.llama_reward",
    "file": "python/sglang/srt/models/llama_reward.py",
    "line": 72,
    "qualname": "Weights.__init__",
    "signature": "def __init__(self, hidden_size, num_label)"
  },
  {
    "module": "srt.models.llama_reward",
    "file": "python/sglang/srt/models/llama_reward.py",
    "line": 82,
    "qualname": "Weights.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.llama_reward",
    "file": "python/sglang/srt/models/llama_reward.py",
    "line": 85,
    "qualname": "LlamaForSequenceClassificationWithNormal_Weights.__init__",
    "signature": "def __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llama_reward",
    "file": "python/sglang/srt/models/llama_reward.py",
    "line": 95,
    "qualname": "LlamaForSequenceClassificationWithNormal_Weights.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)"
  },
  {
    "module": "srt.models.llama_reward",
    "file": "python/sglang/srt/models/llama_reward.py",
    "line": 119,
    "qualname": "LlamaForSequenceClassificationWithNormal_Weights.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.llava",
    "file": "python/sglang/srt/models/llava.py",
    "line": 58,
    "qualname": "LlavaBaseForCausalLM.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.llava",
    "file": "python/sglang/srt/models/llava.py",
    "line": 126,
    "qualname": "LlavaBaseForCausalLM.encode_images",
    "signature": "def encode_images(self, pixel_values: Union[torch.Tensor, List[torch.Tensor]])"
  },
  {
    "module": "srt.models.llava",
    "file": "python/sglang/srt/models/llava.py",
    "line": 151,
    "qualname": "LlavaBaseForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.llava",
    "file": "python/sglang/srt/models/llava.py",
    "line": 440,
    "qualname": "LlavaBaseForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.llava",
    "file": "python/sglang/srt/models/llava.py",
    "line": 498,
    "qualname": "LlavaBaseForCausalLM.num_patches_per_side",
    "signature": "def num_patches_per_side(self)"
  },
  {
    "module": "srt.models.llava",
    "file": "python/sglang/srt/models/llava.py",
    "line": 503,
    "qualname": "LlavaLlamaForCausalLM.__init__",
    "signature": "def __init__(self, config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llava",
    "file": "python/sglang/srt/models/llava.py",
    "line": 529,
    "qualname": "LlavaQwenForCausalLM.__init__",
    "signature": "def __init__(self, config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llava",
    "file": "python/sglang/srt/models/llava.py",
    "line": 566,
    "qualname": "LlavaMistralForCausalLM.__init__",
    "signature": "def __init__(self, config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llava",
    "file": "python/sglang/srt/models/llava.py",
    "line": 615,
    "qualname": "LlavaForConditionalGeneration.dtype",
    "signature": "def dtype(self)"
  },
  {
    "module": "srt.models.llava",
    "file": "python/sglang/srt/models/llava.py",
    "line": 618,
    "qualname": "LlavaForConditionalGeneration.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.llava",
    "file": "python/sglang/srt/models/llava.py",
    "line": 670,
    "qualname": "LlavaForConditionalGeneration.__init__",
    "signature": "def __init__(self, config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llava",
    "file": "python/sglang/srt/models/llava.py",
    "line": 747,
    "qualname": "LlavaForConditionalGeneration.get_image_feature",
    "signature": "def get_image_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.llava",
    "file": "python/sglang/srt/models/llava.py",
    "line": 782,
    "qualname": "LlavaForConditionalGeneration.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)"
  },
  {
    "module": "srt.models.llava",
    "file": "python/sglang/srt/models/llava.py",
    "line": 803,
    "qualname": "LlavaForConditionalGeneration.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.llavavid",
    "file": "python/sglang/srt/models/llavavid.py",
    "line": 33,
    "qualname": "LlavaVidForCausalLM.__init__",
    "signature": "def __init__(self, config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.llavavid",
    "file": "python/sglang/srt/models/llavavid.py",
    "line": 60,
    "qualname": "LlavaVidForCausalLM.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.llavavid",
    "file": "python/sglang/srt/models/llavavid.py",
    "line": 77,
    "qualname": "LlavaVidForCausalLM.encode_images",
    "signature": "def encode_images(self, pixel_values: torch.Tensor)"
  },
  {
    "module": "srt.models.llavavid",
    "file": "python/sglang/srt/models/llavavid.py",
    "line": 109,
    "qualname": "LlavaVidForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.llavavid",
    "file": "python/sglang/srt/models/llavavid.py",
    "line": 221,
    "qualname": "LlavaVidForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.llavavid",
    "file": "python/sglang/srt/models/llavavid.py",
    "line": 279,
    "qualname": "LlavaVidForCausalLM.num_patches_per_side",
    "signature": "def num_patches_per_side(self)"
  },
  {
    "module": "srt.models.mimo",
    "file": "python/sglang/srt/models/mimo.py",
    "line": 32,
    "qualname": "MiMoModel.__init__",
    "signature": "def __init__(self, config: MiMoConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mimo",
    "file": "python/sglang/srt/models/mimo.py",
    "line": 66,
    "qualname": "MiMoForCausalLM.__init__",
    "signature": "def __init__(self, config: MiMoConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mimo",
    "file": "python/sglang/srt/models/mimo.py",
    "line": 90,
    "qualname": "MiMoForCausalLM.get_input_embeddings",
    "signature": "def get_input_embeddings(self, input_ids: torch.Tensor)"
  },
  {
    "module": "srt.models.mimo",
    "file": "python/sglang/srt/models/mimo.py",
    "line": 94,
    "qualname": "MiMoForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)"
  },
  {
    "module": "srt.models.mimo",
    "file": "python/sglang/srt/models/mimo.py",
    "line": 110,
    "qualname": "MiMoForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.mimo",
    "file": "python/sglang/srt/models/mimo.py",
    "line": 156,
    "qualname": "MiMoForCausalLM.get_embed_and_head",
    "signature": "def get_embed_and_head(self)"
  },
  {
    "module": "srt.models.mimo",
    "file": "python/sglang/srt/models/mimo.py",
    "line": 159,
    "qualname": "MiMoForCausalLM.set_embed_and_head",
    "signature": "def set_embed_and_head(self, embed, head)"
  },
  {
    "module": "srt.models.mimo",
    "file": "python/sglang/srt/models/mimo.py",
    "line": 167,
    "qualname": "MiMoForCausalLM.load_kv_cache_scales",
    "signature": "def load_kv_cache_scales(self, quantization_param_path: str)"
  },
  {
    "module": "srt.models.mimo_mtp",
    "file": "python/sglang/srt/models/mimo_mtp.py",
    "line": 25,
    "qualname": "MiMoMultiTokenPredictorLayer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, prefix: str, quant_config: Optional[QuantizationConfig])"
  },
  {
    "module": "srt.models.mimo_mtp",
    "file": "python/sglang/srt/models/mimo_mtp.py",
    "line": 47,
    "qualname": "MiMoMultiTokenPredictorLayer.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.mimo_mtp",
    "file": "python/sglang/srt/models/mimo_mtp.py",
    "line": 84,
    "qualname": "MiMoMTP.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mimo_mtp",
    "file": "python/sglang/srt/models/mimo_mtp.py",
    "line": 108,
    "qualname": "MiMoMTP.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.mimo_mtp",
    "file": "python/sglang/srt/models/mimo_mtp.py",
    "line": 119,
    "qualname": "MiMoMTP.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.mimo_mtp",
    "file": "python/sglang/srt/models/mimo_mtp.py",
    "line": 173,
    "qualname": "MiMoMTP.map_model_name_to_mtp_param_name",
    "signature": "def map_model_name_to_mtp_param_name(self, name: str)"
  },
  {
    "module": "srt.models.mimo_mtp",
    "file": "python/sglang/srt/models/mimo_mtp.py",
    "line": 192,
    "qualname": "MiMoMTP.get_embed_and_head",
    "signature": "def get_embed_and_head(self)"
  },
  {
    "module": "srt.models.mimo_mtp",
    "file": "python/sglang/srt/models/mimo_mtp.py",
    "line": 195,
    "qualname": "MiMoMTP.set_embed_and_head",
    "signature": "def set_embed_and_head(self, embed, head)"
  },
  {
    "module": "srt.models.minicpm",
    "file": "python/sglang/srt/models/minicpm.py",
    "line": 44,
    "qualname": "MiniCPMMLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpm",
    "file": "python/sglang/srt/models/minicpm.py",
    "line": 74,
    "qualname": "MiniCPMMLP.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.minicpm",
    "file": "python/sglang/srt/models/minicpm.py",
    "line": 82,
    "qualname": "MiniCPMAttention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpm",
    "file": "python/sglang/srt/models/minicpm.py",
    "line": 151,
    "qualname": "MiniCPMAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.minicpm",
    "file": "python/sglang/srt/models/minicpm.py",
    "line": 169,
    "qualname": "MiniCPMDecoderLayer.__init__",
    "signature": "def __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpm",
    "file": "python/sglang/srt/models/minicpm.py",
    "line": 205,
    "qualname": "MiniCPMDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.minicpm",
    "file": "python/sglang/srt/models/minicpm.py",
    "line": 236,
    "qualname": "MiniCPMModel.__init__",
    "signature": "def __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpm",
    "file": "python/sglang/srt/models/minicpm.py",
    "line": 265,
    "qualname": "MiniCPMModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.minicpm",
    "file": "python/sglang/srt/models/minicpm.py",
    "line": 291,
    "qualname": "MiniCPMForCausalLM.__init__",
    "signature": "def __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpm",
    "file": "python/sglang/srt/models/minicpm.py",
    "line": 319,
    "qualname": "MiniCPMForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.minicpm",
    "file": "python/sglang/srt/models/minicpm.py",
    "line": 336,
    "qualname": "MiniCPMForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.minicpm3",
    "file": "python/sglang/srt/models/minicpm3.py",
    "line": 49,
    "qualname": "MiniCPM3MLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpm3",
    "file": "python/sglang/srt/models/minicpm3.py",
    "line": 79,
    "qualname": "MiniCPM3MLP.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.minicpm3",
    "file": "python/sglang/srt/models/minicpm3.py",
    "line": 86,
    "qualname": "input_to_float8",
    "signature": "def input_to_float8(x, dtype)"
  },
  {
    "module": "srt.models.minicpm3",
    "file": "python/sglang/srt/models/minicpm3.py",
    "line": 97,
    "qualname": "MiniCPM3AttentionMLA.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, hidden_size: int, num_heads: int, qk_nope_head_dim: int, qk_rope_head_dim: int, v_head_dim: int, q_lora_rank: int, kv_lora_rank: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], layer_id, prefix: str)"
  },
  {
    "module": "srt.models.minicpm3",
    "file": "python/sglang/srt/models/minicpm3.py",
    "line": 202,
    "qualname": "MiniCPM3AttentionMLA.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.minicpm3",
    "file": "python/sglang/srt/models/minicpm3.py",
    "line": 271,
    "qualname": "MiniCPM3DecoderLayer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpm3",
    "file": "python/sglang/srt/models/minicpm3.py",
    "line": 315,
    "qualname": "MiniCPM3DecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.minicpm3",
    "file": "python/sglang/srt/models/minicpm3.py",
    "line": 346,
    "qualname": "MiniCPM3Model.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpm3",
    "file": "python/sglang/srt/models/minicpm3.py",
    "line": 375,
    "qualname": "MiniCPM3Model.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.minicpm3",
    "file": "python/sglang/srt/models/minicpm3.py",
    "line": 401,
    "qualname": "MiniCPM3ForCausalLM.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpm3",
    "file": "python/sglang/srt/models/minicpm3.py",
    "line": 429,
    "qualname": "MiniCPM3ForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.minicpm3",
    "file": "python/sglang/srt/models/minicpm3.py",
    "line": 446,
    "qualname": "MiniCPM3ForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 71,
    "qualname": "apply_spk_emb",
    "signature": "def apply_spk_emb(input_ids: torch.Tensor, spk_emb: torch.Tensor, input_embeds: torch.Tensor, spk_emb_token_id: int, num_spk_embs: int)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 126,
    "qualname": "make_streaming_chunk_mask_generation",
    "signature": "def make_streaming_chunk_mask_generation(inputs_embeds: torch.Tensor, past_seen_tokens: int, streaming_tts_text_mask: torch.Tensor, streaming_reserved_length: int, streaming_audio_chunk_size: int, streaming_text_chunk_size: int, num_spk_emb: int, use_spk_emb: bool)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 203,
    "qualname": "ConvNeXtBlock.__init__",
    "signature": "def __init__(self, dim: int, intermediate_dim: int, kernel: int, dilation: int, layer_scale_init_value: float)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 232,
    "qualname": "ConvNeXtBlock.forward",
    "signature": "def forward(self, x: torch.Tensor, cond)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 257,
    "qualname": "DVAEDecoder.__init__",
    "signature": "def __init__(self, idim: int, odim: int, n_layer, bn_dim, hidden, kernel, dilation, up)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 288,
    "qualname": "DVAEDecoder.forward",
    "signature": "def forward(self, x: torch.Tensor, conditioning)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 302,
    "qualname": "GFSQ.__init__",
    "signature": "def __init__(self, dim: int, levels: List[int], G: int, R: int, eps, transpose)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 331,
    "qualname": "GFSQ.__call__",
    "signature": "def __call__(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 334,
    "qualname": "GFSQ.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 345,
    "qualname": "DVAE.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 386,
    "qualname": "DVAE.forward",
    "signature": "def forward(self, inp: torch.Tensor, mode: Literal['encode', 'decode'])"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 426,
    "qualname": "CustomRepetitionPenaltyLogitsProcessorRepeat.__init__",
    "signature": "def __init__(self, penalty: float, max_input_ids: int, past_window: int)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 436,
    "qualname": "CustomRepetitionPenaltyLogitsProcessorRepeat.__call__",
    "signature": "def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 551,
    "qualname": "ConditionalChatTTS.__init__",
    "signature": "def __init__(self, config: PretrainedConfig)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 610,
    "qualname": "ConditionalChatTTS.merge_inputs_embeds",
    "signature": "def merge_inputs_embeds(self, input_ids: torch.Tensor, lm_spk_emb_last_hidden_states: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 656,
    "qualname": "ConditionalChatTTS.prefill_text",
    "signature": "def prefill_text(self, input_ids: torch.Tensor, position_ids: torch.LongTensor, past_key_values: List[Tuple[torch.Tensor, torch.Tensor]], lm_spk_emb_last_hidden_states: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 728,
    "qualname": "ConditionalChatTTS.prefill_audio_ids",
    "signature": "def prefill_audio_ids(self, input_ids: torch.Tensor, past_key_values: List[Tuple[torch.Tensor, torch.Tensor]], streaming_tts_text_mask, add_audio_bos: bool)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 788,
    "qualname": "ConditionalChatTTS.generate",
    "signature": "def generate(self, input_ids: torch.Tensor, past_key_values: List[Tuple[torch.Tensor, torch.Tensor]], temperature: torch.Tensor, eos_token: Union[int, torch.Tensor], streaming_tts_text_mask, force_no_stop, min_new_token, max_new_token, logits_warpers: List[LogitsWarper], logits_processors: List[CustomRepetitionPenaltyLogitsProcessorRepeat], show_tqdm)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1051,
    "qualname": "ConditionalChatTTS.decode_to_mel_specs",
    "signature": "def decode_to_mel_specs(self, result_list: List[torch.Tensor])"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1090,
    "qualname": "MiniCPMWhisperEncoderLayer.__init__",
    "signature": "def __init__(self, config: WhisperConfig, layer_idx: int)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1108,
    "qualname": "MiniCPMWhisperEncoderLayer.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, layer_head_mask: torch.Tensor, output_attentions: bool, past_key_values: Optional[EncoderDecoderCache], use_cache: Optional[bool])"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1186,
    "qualname": "MiniCPMWhisperEncoder.__init__",
    "signature": "def __init__(self, config: WhisperConfig)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1195,
    "qualname": "MiniCPMWhisperEncoder.forward",
    "signature": "def forward(self, input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, past_key_values: Optional[EncoderDecoderCache], use_cache: Optional[bool])"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1404,
    "qualname": "MultiModalProjector.__init__",
    "signature": "def __init__(self, in_dim, out_dim)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1410,
    "qualname": "MultiModalProjector.forward",
    "signature": "def forward(self, audio_features)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1417,
    "qualname": "MiniCPMO.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig])"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1459,
    "qualname": "MiniCPMO.init_tts_module",
    "signature": "def init_tts_module(self)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1463,
    "qualname": "MiniCPMO.init_audio_module",
    "signature": "def init_audio_module(self)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1467,
    "qualname": "MiniCPMO.init_llm",
    "signature": "def init_llm(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1475,
    "qualname": "MiniCPMO.init_vision_module",
    "signature": "def init_vision_module(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1496,
    "qualname": "MiniCPMO.init_resampler",
    "signature": "def init_resampler(self, embed_dim: int, vision_dim: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1516,
    "qualname": "MiniCPMO.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], mm_input: MultimodalInputs)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1547,
    "qualname": "MiniCPMO.get_audio_embedding_streaming",
    "signature": "def get_audio_embedding_streaming(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1614,
    "qualname": "MiniCPMO.subsequent_chunk_mask",
    "signature": "def subsequent_chunk_mask(self, size: int, chunk_size: int, num_left_chunks: int, device: torch.device, num_lookhead: int)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1647,
    "qualname": "MiniCPMO.get_audio_embedding",
    "signature": "def get_audio_embedding(self, items: List[MultimodalDataItem], chunk_length)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1746,
    "qualname": "MiniCPMO.get_audio_feature",
    "signature": "def get_audio_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1754,
    "qualname": "MiniCPMO.get_omni_embedding",
    "signature": "def get_omni_embedding(self, items: List[MultimodalDataItem], chunk_length, stream_input)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1778,
    "qualname": "MiniCPMO.get_image_feature",
    "signature": "def get_image_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1817,
    "qualname": "MiniCPMO.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.minicpmo",
    "file": "python/sglang/srt/models/minicpmo.py",
    "line": 1834,
    "qualname": "MiniCPMO.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 65,
    "qualname": "get_1d_sincos_pos_embed_from_grid",
    "signature": "def get_1d_sincos_pos_embed_from_grid(embed_dim: int, pos: np.ndarray, version: Tuple[int, int])"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 92,
    "qualname": "get_2d_sincos_pos_embed_from_grid",
    "signature": "def get_2d_sincos_pos_embed_from_grid(embed_dim: int, grid: np.ndarray, version: Tuple[int, int])"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 112,
    "qualname": "get_2d_sincos_pos_embed",
    "signature": "def get_2d_sincos_pos_embed(embed_dim: int, grid_size: Union[int, Tuple[int, int]], cls_token: bool, version: Tuple[int, int])"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 201,
    "qualname": "BaseResampler.__init__",
    "signature": "def __init__(self, num_queries: int, embed_dim: int, num_heads: int, kv_dim: Optional[int], norm_layer: Callable[[int], nn.LayerNorm], do_post_projection: bool, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 260,
    "qualname": "Resampler2_5.__init__",
    "signature": "def __init__(self, num_queries: int, embed_dim: int, num_heads: int, kv_dim: Optional[int], norm_layer: Callable[[int], nn.LayerNorm], max_size: Tuple[int, int], quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 309,
    "qualname": "Resampler2_5.forward",
    "signature": "def forward(self, x: torch.Tensor, tgt_sizes: torch.Tensor)"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 358,
    "qualname": "get_version_by_config",
    "signature": "def get_version_by_config(config: PretrainedConfig)"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 378,
    "qualname": "MiniCPMBaseModel.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 527,
    "qualname": "MiniCPMBaseModel.get_embedding",
    "signature": "def get_embedding(self, input_ids: torch.Tensor, image_inputs: Optional[MiniCPMVImageInputs])"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 563,
    "qualname": "MiniCPMBaseModel.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 566,
    "qualname": "MiniCPMBaseModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 582,
    "qualname": "MiniCPMBaseModel.init_llm",
    "signature": "def init_llm(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 590,
    "qualname": "MiniCPMBaseModel.init_vision_module",
    "signature": "def init_vision_module(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 598,
    "qualname": "MiniCPMBaseModel.init_resampler",
    "signature": "def init_resampler(self, embed_dim: int, vision_dim: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 607,
    "qualname": "MiniCPMBaseModel.get_vision_embedding",
    "signature": "def get_vision_embedding(self, pixel_values: List[torch.Tensor], patch_attn_mask: Optional[torch.Tensor], tgt_sizes: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 615,
    "qualname": "MiniCPMBaseModel.get_image_feature",
    "signature": "def get_image_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 659,
    "qualname": "MiniCPMV2_6.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 668,
    "qualname": "MiniCPMV2_6.init_llm",
    "signature": "def init_llm(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 676,
    "qualname": "MiniCPMV2_6.init_vision_module",
    "signature": "def init_vision_module(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 692,
    "qualname": "MiniCPMV2_6.init_resampler",
    "signature": "def init_resampler(self, embed_dim: int, vision_dim: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 712,
    "qualname": "MiniCPMV2_6.get_vision_embedding",
    "signature": "def get_vision_embedding(self, pixel_values: List[torch.Tensor], patch_attn_mask: Optional[torch.Tensor], tgt_sizes: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 725,
    "qualname": "MiniCPMV2_6.get_image_feature",
    "signature": "def get_image_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 764,
    "qualname": "MiniCPMV2_6.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 796,
    "qualname": "MiniCPMV.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 824,
    "qualname": "MiniCPMV.__getattr__",
    "signature": "def __getattr__(self, name)"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 829,
    "qualname": "MiniCPMV.__call__",
    "signature": "def __call__(self)"
  },
  {
    "module": "srt.models.minicpmv",
    "file": "python/sglang/srt/models/minicpmv.py",
    "line": 832,
    "qualname": "MiniCPMV.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.mistral",
    "file": "python/sglang/srt/models/mistral.py",
    "line": 32,
    "qualname": "Mistral3ForConditionalGeneration.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.models.mistral",
    "file": "python/sglang/srt/models/mistral.py",
    "line": 46,
    "qualname": "Mistral3ForConditionalGeneration.get_image_feature",
    "signature": "def get_image_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.mistral",
    "file": "python/sglang/srt/models/mistral.py",
    "line": 83,
    "qualname": "Mistral3ForConditionalGeneration.__getattr__",
    "signature": "def __getattr__(self, name)"
  },
  {
    "module": "srt.models.mistral",
    "file": "python/sglang/srt/models/mistral.py",
    "line": 86,
    "qualname": "Mistral3ForConditionalGeneration.__hasattr__",
    "signature": "def __hasattr__(self, name)"
  },
  {
    "module": "srt.models.mistral",
    "file": "python/sglang/srt/models/mistral.py",
    "line": 89,
    "qualname": "Mistral3ForConditionalGeneration.__call__",
    "signature": "def __call__(self)"
  },
  {
    "module": "srt.models.mixtral",
    "file": "python/sglang/srt/models/mixtral.py",
    "line": 66,
    "qualname": "MixtralMoE.__init__",
    "signature": "def __init__(self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], tp_size: Optional[int], prefix: str)"
  },
  {
    "module": "srt.models.mixtral",
    "file": "python/sglang/srt/models/mixtral.py",
    "line": 109,
    "qualname": "MixtralMoE.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.mixtral",
    "file": "python/sglang/srt/models/mixtral.py",
    "line": 123,
    "qualname": "MixtralAttention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, max_position: int, rope_theta: float, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mixtral",
    "file": "python/sglang/srt/models/mixtral.py",
    "line": 189,
    "qualname": "MixtralAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.mixtral",
    "file": "python/sglang/srt/models/mixtral.py",
    "line": 204,
    "qualname": "MixtralDecoderLayer.__init__",
    "signature": "def __init__(self, config: MixtralConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mixtral",
    "file": "python/sglang/srt/models/mixtral.py",
    "line": 239,
    "qualname": "MixtralDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.mixtral",
    "file": "python/sglang/srt/models/mixtral.py",
    "line": 265,
    "qualname": "MixtralModel.__init__",
    "signature": "def __init__(self, config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mixtral",
    "file": "python/sglang/srt/models/mixtral.py",
    "line": 301,
    "qualname": "MixtralModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.mixtral",
    "file": "python/sglang/srt/models/mixtral.py",
    "line": 341,
    "qualname": "MixtralForCausalLM.__init__",
    "signature": "def __init__(self, config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mixtral",
    "file": "python/sglang/srt/models/mixtral.py",
    "line": 359,
    "qualname": "MixtralForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.mixtral",
    "file": "python/sglang/srt/models/mixtral.py",
    "line": 383,
    "qualname": "MixtralForCausalLM.start_layer",
    "signature": "def start_layer(self)"
  },
  {
    "module": "srt.models.mixtral",
    "file": "python/sglang/srt/models/mixtral.py",
    "line": 387,
    "qualname": "MixtralForCausalLM.end_layer",
    "signature": "def end_layer(self)"
  },
  {
    "module": "srt.models.mixtral",
    "file": "python/sglang/srt/models/mixtral.py",
    "line": 390,
    "qualname": "MixtralForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.mixtral_quant",
    "file": "python/sglang/srt/models/mixtral_quant.py",
    "line": 52,
    "qualname": "MixtralMLP.__init__",
    "signature": "def __init__(self, num_experts: int, hidden_size: int, intermediate_size: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mixtral_quant",
    "file": "python/sglang/srt/models/mixtral_quant.py",
    "line": 90,
    "qualname": "MixtralMLP.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.mixtral_quant",
    "file": "python/sglang/srt/models/mixtral_quant.py",
    "line": 100,
    "qualname": "MixtralMoE.__init__",
    "signature": "def __init__(self, config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mixtral_quant",
    "file": "python/sglang/srt/models/mixtral_quant.py",
    "line": 148,
    "qualname": "MixtralMoE.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.mixtral_quant",
    "file": "python/sglang/srt/models/mixtral_quant.py",
    "line": 173,
    "qualname": "MixtralAttention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, max_position: int, rope_theta: float, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mixtral_quant",
    "file": "python/sglang/srt/models/mixtral_quant.py",
    "line": 239,
    "qualname": "MixtralAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.mixtral_quant",
    "file": "python/sglang/srt/models/mixtral_quant.py",
    "line": 254,
    "qualname": "MixtralDecoderLayer.__init__",
    "signature": "def __init__(self, config: MixtralConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mixtral_quant",
    "file": "python/sglang/srt/models/mixtral_quant.py",
    "line": 285,
    "qualname": "MixtralDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.mixtral_quant",
    "file": "python/sglang/srt/models/mixtral_quant.py",
    "line": 311,
    "qualname": "MixtralModel.__init__",
    "signature": "def __init__(self, config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mixtral_quant",
    "file": "python/sglang/srt/models/mixtral_quant.py",
    "line": 339,
    "qualname": "MixtralModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.mixtral_quant",
    "file": "python/sglang/srt/models/mixtral_quant.py",
    "line": 361,
    "qualname": "QuantMixtralForCausalLM.__init__",
    "signature": "def __init__(self, config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mixtral_quant",
    "file": "python/sglang/srt/models/mixtral_quant.py",
    "line": 379,
    "qualname": "QuantMixtralForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.mixtral_quant",
    "file": "python/sglang/srt/models/mixtral_quant.py",
    "line": 391,
    "qualname": "QuantMixtralForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 56,
    "qualname": "ColumnParallelConv2dPatch.__init__",
    "signature": "def __init__(self, in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]], bias: bool)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 74,
    "qualname": "ColumnParallelConv2dPatch.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 83,
    "qualname": "MllamaPrecomputedAspectRatioEmbedding.__init__",
    "signature": "def __init__(self, config: config_mllama.MllamaVisionConfig, is_gated: bool)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 96,
    "qualname": "MllamaPrecomputedAspectRatioEmbedding.forward",
    "signature": "def forward(self, hidden_state: torch.Tensor, aspect_ratio_ids: torch.Tensor)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 110,
    "qualname": "MllamaPrecomputedPositionEmbedding.__init__",
    "signature": "def __init__(self, config: config_mllama.MllamaVisionConfig)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 130,
    "qualname": "MllamaPrecomputedPositionEmbedding.forward",
    "signature": "def forward(self, hidden_state: torch.Tensor, aspect_ratio_ids: torch.Tensor)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 152,
    "qualname": "MllamaVisionMLP.__init__",
    "signature": "def __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 176,
    "qualname": "MllamaVisionMLP.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 185,
    "qualname": "MllamaVisionEncoderLayer.__init__",
    "signature": "def __init__(self, config: config_mllama.MllamaVisionConfig, quant_config: Optional[QuantizationConfig], is_gated: bool, prefix: str)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 225,
    "qualname": "MllamaVisionEncoderLayer.forward",
    "signature": "def forward(self, hidden_state: torch.Tensor, attention_mask: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 248,
    "qualname": "MllamaVisionEncoder.__init__",
    "signature": "def __init__(self, config: config_mllama.MllamaVisionConfig, quant_config: Optional[QuantizationConfig], num_layers, is_gated, output_hidden_states, prefix: str)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 272,
    "qualname": "MllamaVisionEncoder.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 294,
    "qualname": "MllamaVisionModel.__init__",
    "signature": "def __init__(self, config: config_mllama.MllamaVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 350,
    "qualname": "MllamaVisionModel.apply_class_embedding",
    "signature": "def apply_class_embedding(self, hidden_state: torch.Tensor)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 356,
    "qualname": "MllamaVisionModel.forward",
    "signature": "def forward(self, pixel_values: torch.Tensor, aspect_ratio_ids: torch.Tensor, aspect_ratio_mask: torch.Tensor)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 482,
    "qualname": "MllamaTextRMSNorm.__init__",
    "signature": "def __init__(self, hidden_size, eps)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 487,
    "qualname": "MllamaTextRMSNorm.forward",
    "signature": "def forward(self, hidden_states)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 494,
    "qualname": "MllamaTextRMSNorm.extra_repr",
    "signature": "def extra_repr(self)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 499,
    "qualname": "MllamaTextCrossAttention.__init__",
    "signature": "def __init__(self, config: Optional[config_mllama.MllamaTextConfig], layer_id: Optional[int], quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 557,
    "qualname": "MllamaTextCrossAttention.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor], cross_attention_states: Optional[torch.Tensor], forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 591,
    "qualname": "MllamaCrossAttentionDecoderLayer.__init__",
    "signature": "def __init__(self, config: config_mllama.MllamaTextConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 622,
    "qualname": "MllamaCrossAttentionDecoderLayer.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, cross_attention_states: torch.Tensor, cross_attention_mask: torch.Tensor, full_text_row_masked_out_mask: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 654,
    "qualname": "MllamaTextModel.__init__",
    "signature": "def __init__(self, config: config_mllama.MllamaTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 695,
    "qualname": "MllamaTextModel.forward",
    "signature": "def forward(self, input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], cross_attention_states: Optional[torch.LongTensor], cross_attention_mask: Optional[torch.LongTensor], full_text_row_masked_out_mask: Optional[Tuple[torch.Tensor, torch.Tensor]], forward_batch: ForwardBatch, skip_cross_attention: bool)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 740,
    "qualname": "MllamaForCausalLM.__init__",
    "signature": "def __init__(self, config: config_mllama.MllamaTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 760,
    "qualname": "MllamaForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], cross_attention_states: Optional[torch.LongTensor], cross_attention_mask: Optional[torch.LongTensor], full_text_row_masked_out_mask: Optional[Tuple[torch.Tensor, torch.Tensor]], forward_batch: ForwardBatch, skip_cross_attention: bool)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 804,
    "qualname": "MllamaForConditionalGeneration.__init__",
    "signature": "def __init__(self, config: config_mllama.MllamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 840,
    "qualname": "MllamaForConditionalGeneration.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 913,
    "qualname": "MllamaForConditionalGeneration.flat_encoder_result",
    "signature": "def flat_encoder_result(self, cross_attention_states: torch.Tensor, encoder_lens_need: List[int])"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 939,
    "qualname": "MllamaForConditionalGeneration.get_full_text_row_masked_out_mask",
    "signature": "def get_full_text_row_masked_out_mask(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 963,
    "qualname": "MllamaForConditionalGeneration.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.mllama",
    "file": "python/sglang/srt/models/mllama.py",
    "line": 1027,
    "qualname": "MllamaForConditionalGeneration.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 51,
    "qualname": "Llama4VisionMLP.__init__",
    "signature": "def __init__(self, input_size: int, intermediate_size: int, output_size: int, bias: bool, output_activation: bool, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 82,
    "qualname": "Llama4VisionMLP.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 91,
    "qualname": "pixel_shuffle",
    "signature": "def pixel_shuffle(input_tensor, shuffle_ratio)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 118,
    "qualname": "Llama4VisionPixelShuffleMLP.__init__",
    "signature": "def __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 138,
    "qualname": "Llama4VisionPixelShuffleMLP.forward",
    "signature": "def forward(self, encoded_patches: torch.Tensor)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 143,
    "qualname": "apply_position_embedding",
    "signature": "def apply_position_embedding(q, k, freqs_ci, shape)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 156,
    "qualname": "Llama4VisionEncoderLayer.__init__",
    "signature": "def __init__(self, config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 197,
    "qualname": "Llama4VisionEncoderLayer.forward",
    "signature": "def forward(self, hidden_state: torch.Tensor, freqs_ci: torch.Tensor)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 220,
    "qualname": "Llama4VisionEncoder.__init__",
    "signature": "def __init__(self, config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 241,
    "qualname": "Llama4VisionEncoder.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, freqs_ci: torch.Tensor)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 266,
    "qualname": "Llama4UnfoldConvolution.__init__",
    "signature": "def __init__(self, config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 292,
    "qualname": "Llama4UnfoldConvolution.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 300,
    "qualname": "Llama4VisionRotaryEmbedding.__init__",
    "signature": "def __init__(self, config)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 326,
    "qualname": "Llama4VisionRotaryEmbedding.forward",
    "signature": "def forward(self, hidden_states)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 332,
    "qualname": "Llama4VisionModel.__init__",
    "signature": "def __init__(self, config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 377,
    "qualname": "Llama4VisionModel.forward",
    "signature": "def forward(self, pixel_values: torch.Tensor)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 425,
    "qualname": "Llama4ForConditionalGeneration.__init__",
    "signature": "def __init__(self, config: Llama4Config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 524,
    "qualname": "Llama4ForConditionalGeneration.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 527,
    "qualname": "Llama4ForConditionalGeneration.get_image_feature",
    "signature": "def get_image_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 547,
    "qualname": "Llama4ForConditionalGeneration.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 570,
    "qualname": "Llama4ForConditionalGeneration.permute_qk_weight_for_rotary",
    "signature": "def permute_qk_weight_for_rotary(self, name: str, loaded_weight: torch.Tensor)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 604,
    "qualname": "Llama4ForConditionalGeneration.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 935,
    "qualname": "Llama4ForConditionalGeneration.set_eagle3_layers_to_capture",
    "signature": "def set_eagle3_layers_to_capture(self, layer_ids: Optional[List[int]])"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 939,
    "qualname": "Llama4ForConditionalGeneration.get_embed_and_head",
    "signature": "def get_embed_and_head(self)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 951,
    "qualname": "Llama4ForConditionalGeneration.set_embed_and_head",
    "signature": "def set_embed_and_head(self, embed, head)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 958,
    "qualname": "Llama4ForConditionalGeneration.get_embed",
    "signature": "def get_embed(self)"
  },
  {
    "module": "srt.models.mllama4",
    "file": "python/sglang/srt/models/mllama4.py",
    "line": 961,
    "qualname": "Llama4ForConditionalGeneration.set_embed",
    "signature": "def set_embed(self, embed)"
  },
  {
    "module": "srt.models.nemotron_nas",
    "file": "python/sglang/srt/models/nemotron_nas.py",
    "line": 59,
    "qualname": "DeciLMDecoderLayer.__init__",
    "signature": "def __init__(self, config: LlamaConfig, layer_idx: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.nemotron_nas",
    "file": "python/sglang/srt/models/nemotron_nas.py",
    "line": 127,
    "qualname": "DeciLMDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.nemotron_nas",
    "file": "python/sglang/srt/models/nemotron_nas.py",
    "line": 160,
    "qualname": "DeciModel.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.models.nemotron_nas",
    "file": "python/sglang/srt/models/nemotron_nas.py",
    "line": 210,
    "qualname": "DeciModel.get_input_embeddings",
    "signature": "def get_input_embeddings(self, input_ids: torch.Tensor)"
  },
  {
    "module": "srt.models.nemotron_nas",
    "file": "python/sglang/srt/models/nemotron_nas.py",
    "line": 213,
    "qualname": "DeciModel.forward",
    "signature": "def forward(self, input_ids: Optional[torch.Tensor], positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor], pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.nemotron_nas",
    "file": "python/sglang/srt/models/nemotron_nas.py",
    "line": 295,
    "qualname": "DeciLMForCausalLM.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.models.nemotron_nas",
    "file": "python/sglang/srt/models/nemotron_nas.py",
    "line": 341,
    "qualname": "DeciLMForCausalLM.get_input_embeddings",
    "signature": "def get_input_embeddings(self, input_ids: torch.Tensor)"
  },
  {
    "module": "srt.models.nemotron_nas",
    "file": "python/sglang/srt/models/nemotron_nas.py",
    "line": 345,
    "qualname": "DeciLMForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor], get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.nemotron_nas",
    "file": "python/sglang/srt/models/nemotron_nas.py",
    "line": 371,
    "qualname": "DeciLMForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.olmo",
    "file": "python/sglang/srt/models/olmo.py",
    "line": 51,
    "qualname": "OlmoAttention.__init__",
    "signature": "def __init__(self, config: OlmoConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.olmo",
    "file": "python/sglang/srt/models/olmo.py",
    "line": 108,
    "qualname": "OlmoAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.olmo",
    "file": "python/sglang/srt/models/olmo.py",
    "line": 131,
    "qualname": "OlmoMLP.__init__",
    "signature": "def __init__(self, config: OlmoConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.olmo",
    "file": "python/sglang/srt/models/olmo.py",
    "line": 163,
    "qualname": "OlmoMLP.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.olmo",
    "file": "python/sglang/srt/models/olmo.py",
    "line": 180,
    "qualname": "OlmoDecoderLayer.__init__",
    "signature": "def __init__(self, config: OlmoConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.olmo",
    "file": "python/sglang/srt/models/olmo.py",
    "line": 211,
    "qualname": "OlmoDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.olmo",
    "file": "python/sglang/srt/models/olmo.py",
    "line": 233,
    "qualname": "OlmoModel.__init__",
    "signature": "def __init__(self, config: OlmoConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.olmo",
    "file": "python/sglang/srt/models/olmo.py",
    "line": 261,
    "qualname": "OlmoModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.olmo",
    "file": "python/sglang/srt/models/olmo.py",
    "line": 299,
    "qualname": "OlmoForCausalLM.__init__",
    "signature": "def __init__(self, config: OlmoConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.olmo",
    "file": "python/sglang/srt/models/olmo.py",
    "line": 321,
    "qualname": "OlmoForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.olmo",
    "file": "python/sglang/srt/models/olmo.py",
    "line": 338,
    "qualname": "OlmoForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.olmo2",
    "file": "python/sglang/srt/models/olmo2.py",
    "line": 58,
    "qualname": "Olmo2Attention.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.olmo2",
    "file": "python/sglang/srt/models/olmo2.py",
    "line": 148,
    "qualname": "Olmo2Attention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.olmo2",
    "file": "python/sglang/srt/models/olmo2.py",
    "line": 170,
    "qualname": "Olmo2MLP.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.olmo2",
    "file": "python/sglang/srt/models/olmo2.py",
    "line": 202,
    "qualname": "Olmo2MLP.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.olmo2",
    "file": "python/sglang/srt/models/olmo2.py",
    "line": 219,
    "qualname": "Olmo2DecoderLayer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.olmo2",
    "file": "python/sglang/srt/models/olmo2.py",
    "line": 244,
    "qualname": "Olmo2DecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.olmo2",
    "file": "python/sglang/srt/models/olmo2.py",
    "line": 266,
    "qualname": "Olmo2Model.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.olmo2",
    "file": "python/sglang/srt/models/olmo2.py",
    "line": 292,
    "qualname": "Olmo2Model.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.olmo2",
    "file": "python/sglang/srt/models/olmo2.py",
    "line": 330,
    "qualname": "Olmo2ForCausalLM.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.olmo2",
    "file": "python/sglang/srt/models/olmo2.py",
    "line": 354,
    "qualname": "Olmo2ForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.olmo2",
    "file": "python/sglang/srt/models/olmo2.py",
    "line": 371,
    "qualname": "Olmo2ForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.olmoe",
    "file": "python/sglang/srt/models/olmoe.py",
    "line": 57,
    "qualname": "OlmoeMoE.__init__",
    "signature": "def __init__(self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], tp_size: Optional[int], layer_id: int, prefix: str)"
  },
  {
    "module": "srt.models.olmoe",
    "file": "python/sglang/srt/models/olmoe.py",
    "line": 96,
    "qualname": "OlmoeMoE.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.olmoe",
    "file": "python/sglang/srt/models/olmoe.py",
    "line": 109,
    "qualname": "OlmoeAttention.__init__",
    "signature": "def __init__(self, layer_id: int, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.olmoe",
    "file": "python/sglang/srt/models/olmoe.py",
    "line": 181,
    "qualname": "OlmoeAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.olmoe",
    "file": "python/sglang/srt/models/olmoe.py",
    "line": 198,
    "qualname": "OlmoeDecoderLayer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.olmoe",
    "file": "python/sglang/srt/models/olmoe.py",
    "line": 235,
    "qualname": "OlmoeDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.olmoe",
    "file": "python/sglang/srt/models/olmoe.py",
    "line": 263,
    "qualname": "OlmoeModel.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.olmoe",
    "file": "python/sglang/srt/models/olmoe.py",
    "line": 290,
    "qualname": "OlmoeModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.olmoe",
    "file": "python/sglang/srt/models/olmoe.py",
    "line": 315,
    "qualname": "OlmoeForCausalLM.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.olmoe",
    "file": "python/sglang/srt/models/olmoe.py",
    "line": 335,
    "qualname": "OlmoeForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.olmoe",
    "file": "python/sglang/srt/models/olmoe.py",
    "line": 347,
    "qualname": "OlmoeForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.persimmon",
    "file": "python/sglang/srt/models/persimmon.py",
    "line": 31,
    "qualname": "PersimmonMLP.__init__",
    "signature": "def __init__(self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig])"
  },
  {
    "module": "srt.models.persimmon",
    "file": "python/sglang/srt/models/persimmon.py",
    "line": 43,
    "qualname": "PersimmonMLP.forward",
    "signature": "def forward(self, hidden_states)"
  },
  {
    "module": "srt.models.persimmon",
    "file": "python/sglang/srt/models/persimmon.py",
    "line": 52,
    "qualname": "PersimmonAttention.__init__",
    "signature": "def __init__(self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig], prefix: str, layer_id: int)"
  },
  {
    "module": "srt.models.persimmon",
    "file": "python/sglang/srt/models/persimmon.py",
    "line": 120,
    "qualname": "PersimmonAttention.forward",
    "signature": "def forward(self, position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.persimmon",
    "file": "python/sglang/srt/models/persimmon.py",
    "line": 147,
    "qualname": "PersimmonDecoderLayer.__init__",
    "signature": "def __init__(self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig], prefix: str, idx: int)"
  },
  {
    "module": "srt.models.persimmon",
    "file": "python/sglang/srt/models/persimmon.py",
    "line": 170,
    "qualname": "PersimmonDecoderLayer.forward",
    "signature": "def forward(self, position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.persimmon",
    "file": "python/sglang/srt/models/persimmon.py",
    "line": 199,
    "qualname": "PersimmonModel.__init__",
    "signature": "def __init__(self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.persimmon",
    "file": "python/sglang/srt/models/persimmon.py",
    "line": 233,
    "qualname": "PersimmonModel.get_input_embeddings",
    "signature": "def get_input_embeddings(self, input_ids: torch.Tensor)"
  },
  {
    "module": "srt.models.persimmon",
    "file": "python/sglang/srt/models/persimmon.py",
    "line": 236,
    "qualname": "PersimmonModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, forward_batch: ForwardBatch, positions: torch.Tensor, inputs_embeds: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.persimmon",
    "file": "python/sglang/srt/models/persimmon.py",
    "line": 262,
    "qualname": "PersimmonForCausalLM.__init__",
    "signature": "def __init__(self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.persimmon",
    "file": "python/sglang/srt/models/persimmon.py",
    "line": 282,
    "qualname": "PersimmonForCausalLM.get_input_embeddings",
    "signature": "def get_input_embeddings(self, input_ids: torch.Tensor)"
  },
  {
    "module": "srt.models.persimmon",
    "file": "python/sglang/srt/models/persimmon.py",
    "line": 285,
    "qualname": "PersimmonForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.persimmon",
    "file": "python/sglang/srt/models/persimmon.py",
    "line": 303,
    "qualname": "PersimmonForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.phi",
    "file": "python/sglang/srt/models/phi.py",
    "line": 30,
    "qualname": "PhiAttention.__init__",
    "signature": "def __init__(self, config: PhiConfig, quant_config: Optional[QuantizationConfig], prefix: str, layer_id: int)"
  },
  {
    "module": "srt.models.phi",
    "file": "python/sglang/srt/models/phi.py",
    "line": 84,
    "qualname": "PhiAttention.forward",
    "signature": "def forward(self, position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.phi",
    "file": "python/sglang/srt/models/phi.py",
    "line": 100,
    "qualname": "PhiMLP.__init__",
    "signature": "def __init__(self, config: PhiConfig, quant_config: Optional[QuantizationConfig])"
  },
  {
    "module": "srt.models.phi",
    "file": "python/sglang/srt/models/phi.py",
    "line": 120,
    "qualname": "PhiMLP.forward",
    "signature": "def forward(self, hidden_states)"
  },
  {
    "module": "srt.models.phi",
    "file": "python/sglang/srt/models/phi.py",
    "line": 129,
    "qualname": "PhiLayer.__init__",
    "signature": "def __init__(self, config: PhiConfig, quant_config: Optional[QuantizationConfig], prefix: str, idx: int)"
  },
  {
    "module": "srt.models.phi",
    "file": "python/sglang/srt/models/phi.py",
    "line": 148,
    "qualname": "PhiLayer.forward",
    "signature": "def forward(self, position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.phi",
    "file": "python/sglang/srt/models/phi.py",
    "line": 168,
    "qualname": "PhiModel.__init__",
    "signature": "def __init__(self, config: PhiConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.phi",
    "file": "python/sglang/srt/models/phi.py",
    "line": 199,
    "qualname": "PhiModel.get_input_embeddings",
    "signature": "def get_input_embeddings(self, input_ids: torch.Tensor)"
  },
  {
    "module": "srt.models.phi",
    "file": "python/sglang/srt/models/phi.py",
    "line": 202,
    "qualname": "PhiModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, forward_batch: ForwardBatch, positions: torch.Tensor, inputs_embeds: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.phi",
    "file": "python/sglang/srt/models/phi.py",
    "line": 234,
    "qualname": "PhiForCausalLM.__init__",
    "signature": "def __init__(self, config: PhiConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.phi",
    "file": "python/sglang/srt/models/phi.py",
    "line": 257,
    "qualname": "PhiForCausalLM.get_input_embeddings",
    "signature": "def get_input_embeddings(self, input_ids: torch.Tensor)"
  },
  {
    "module": "srt.models.phi",
    "file": "python/sglang/srt/models/phi.py",
    "line": 260,
    "qualname": "PhiForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.phi",
    "file": "python/sglang/srt/models/phi.py",
    "line": 279,
    "qualname": "PhiForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 32,
    "qualname": "quick_gelu",
    "signature": "def quick_gelu(x)"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 37,
    "qualname": "gegelu",
    "signature": "def gegelu(input, limit: Optional[float])"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 54,
    "qualname": "Phi3SmallMLP.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 84,
    "qualname": "Phi3SmallMLP.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 93,
    "qualname": "Phi3SmallSelfAttention.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 210,
    "qualname": "Phi3SmallSelfAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 236,
    "qualname": "Phi3SmallDecoderLayer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 264,
    "qualname": "Phi3SmallDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 289,
    "qualname": "Phi3SmallModel.__init__",
    "signature": "def __init__(self, config: Phi3Config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 332,
    "qualname": "Phi3SmallModel.get_input_embeddings",
    "signature": "def get_input_embeddings(self, input_ids: torch.Tensor)"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 335,
    "qualname": "Phi3SmallModel.forward",
    "signature": "def forward(self, input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 364,
    "qualname": "Phi3SmallForCausalLM.__init__",
    "signature": "def __init__(self, config: Phi3Config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 406,
    "qualname": "Phi3SmallForCausalLM.get_input_embeddings",
    "signature": "def get_input_embeddings(self, input_ids: torch.Tensor)"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 409,
    "qualname": "Phi3SmallForCausalLM.set_input_embeddings",
    "signature": "def set_input_embeddings(self, value)"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 412,
    "qualname": "Phi3SmallForCausalLM.get_output_embeddings",
    "signature": "def get_output_embeddings(self)"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 415,
    "qualname": "Phi3SmallForCausalLM.set_output_embeddings",
    "signature": "def set_output_embeddings(self, value)"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 418,
    "qualname": "Phi3SmallForCausalLM.set_decoder",
    "signature": "def set_decoder(self, decoder)"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 421,
    "qualname": "Phi3SmallForCausalLM.get_decoder",
    "signature": "def get_decoder(self)"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 424,
    "qualname": "Phi3SmallForCausalLM.compute_logits",
    "signature": "def compute_logits(self, input_ids: torch.LongTensor, hidden_states: torch.Tensor, sampling_metadata)"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 437,
    "qualname": "Phi3SmallForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor], get_embedding: bool)"
  },
  {
    "module": "srt.models.phi3_small",
    "file": "python/sglang/srt/models/phi3_small.py",
    "line": 460,
    "qualname": "Phi3SmallForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.phi4mm",
    "file": "python/sglang/srt/models/phi4mm.py",
    "line": 60,
    "qualname": "Phi4MMImageEncoder.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str, model_dir: str)"
  },
  {
    "module": "srt.models.phi4mm",
    "file": "python/sglang/srt/models/phi4mm.py",
    "line": 136,
    "qualname": "Phi4MMImageEncoder.get_img_features",
    "signature": "def get_img_features(self, img_embeds: torch.FloatTensor, attention_mask)"
  },
  {
    "module": "srt.models.phi4mm",
    "file": "python/sglang/srt/models/phi4mm.py",
    "line": 169,
    "qualname": "Phi4MMImageEncoder.forward",
    "signature": "def forward(self, pixel_values: torch.FloatTensor, image_sizes: torch.Tensor, image_attention_mask: torch.Tensor)"
  },
  {
    "module": "srt.models.phi4mm",
    "file": "python/sglang/srt/models/phi4mm.py",
    "line": 387,
    "qualname": "Phi4MMForCausalLM.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.phi4mm",
    "file": "python/sglang/srt/models/phi4mm.py",
    "line": 416,
    "qualname": "Phi4MMForCausalLM.get_image_feature",
    "signature": "def get_image_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.phi4mm",
    "file": "python/sglang/srt/models/phi4mm.py",
    "line": 433,
    "qualname": "Phi4MMForCausalLM.get_audio_feature",
    "signature": "def get_audio_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.phi4mm",
    "file": "python/sglang/srt/models/phi4mm.py",
    "line": 454,
    "qualname": "Phi4MMForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.phi4mm",
    "file": "python/sglang/srt/models/phi4mm.py",
    "line": 474,
    "qualname": "Phi4MMForCausalLM.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.phi4mm",
    "file": "python/sglang/srt/models/phi4mm.py",
    "line": 478,
    "qualname": "Phi4MMForCausalLM.should_apply_lora",
    "signature": "def should_apply_lora(self, module_name: str)"
  },
  {
    "module": "srt.models.phi4mm",
    "file": "python/sglang/srt/models/phi4mm.py",
    "line": 481,
    "qualname": "Phi4MMForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.phi4mm_audio",
    "file": "python/sglang/srt/models/phi4mm_audio.py",
    "line": 148,
    "qualname": "ConformerEncoderLayer.__init__",
    "signature": "def __init__(self, d_model, ext_pw_out_channel, depthwise_seperable_out_channel, depthwise_multiplier, n_head, d_ffn, ext_pw_kernel_size, kernel_size, dropout_rate, causal, batch_norm, activation, chunk_se, chunk_size, conv_activation, conv_glu_type, bias_in_glu, linear_glu_in_convm, attention_inner_dim, attention_glu_type, activation_checkpointing, export, use_pt_scaled_dot_product_attention, attn_group_sizes: int)"
  },
  {
    "module": "srt.models.phi4mm_audio",
    "file": "python/sglang/srt/models/phi4mm_audio.py",
    "line": 225,
    "qualname": "ConformerEncoderLayer.forward",
    "signature": "def forward(self, x, pos_k, pos_v, mask, relative_attention_bias: Optional[Tensor])"
  },
  {
    "module": "srt.models.phi4mm_audio",
    "file": "python/sglang/srt/models/phi4mm_audio.py",
    "line": 339,
    "qualname": "TransformerEncoderBase.__init__",
    "signature": "def __init__(self, input_size, chunk_size, left_chunk, attention_dim, attention_heads, input_layer, cnn_out, cnn_layer_norm, time_reduction, dropout_rate, padding_idx, relative_attention_bias_args, positional_dropout_rate, nemo_conv_settings, conv2d_extra_padding: Literal['feat', 'feat_time', 'none', True], attention_group_size, encoder_embedding_config)"
  },
  {
    "module": "srt.models.phi4mm_audio",
    "file": "python/sglang/srt/models/phi4mm_audio.py",
    "line": 423,
    "qualname": "TransformerEncoderBase.compute_lens_change",
    "signature": "def compute_lens_change(self, feature_lens)"
  },
  {
    "module": "srt.models.phi4mm_audio",
    "file": "python/sglang/srt/models/phi4mm_audio.py",
    "line": 458,
    "qualname": "TransformerEncoderBase.forward",
    "signature": "def forward(self)"
  },
  {
    "module": "srt.models.phi4mm_audio",
    "file": "python/sglang/srt/models/phi4mm_audio.py",
    "line": 534,
    "qualname": "TransformerEncoderBase.forward_embeddings",
    "signature": "def forward_embeddings(self, xs_pad, masks, chunk_size_nc, left_chunk_nc)"
  },
  {
    "module": "srt.models.phi4mm_audio",
    "file": "python/sglang/srt/models/phi4mm_audio.py",
    "line": 598,
    "qualname": "TransformerEncoderBase.get_offset",
    "signature": "def get_offset(self)"
  },
  {
    "module": "srt.models.phi4mm_audio",
    "file": "python/sglang/srt/models/phi4mm_audio.py",
    "line": 778,
    "qualname": "ConformerEncoder.__init__",
    "signature": "def __init__(self, input_size, chunk_size, left_chunk, num_lang, attention_dim, attention_heads, linear_units, num_blocks, dropout_rate, input_layer, causal, batch_norm, cnn_out, cnn_layer_norm, ext_pw_out_channel, ext_pw_kernel_size, depthwise_seperable_out_channel, depthwise_multiplier, chunk_se, kernel_size, activation, conv_activation, conv_glu_type, bias_in_glu, linear_glu_in_convm, attention_glu_type, export, extra_layer_output_idx, extra_multi_layer_output_idxs, activation_checkpointing, relative_attention_bias_args, time_reduction, use_pt_scaled_dot_product_attention, nemo_conv_settings, conv2d_extra_padding: Literal['feat', 'feat_time', 'none', True], replication_pad_for_subsample_embedding, attention_group_size, encoder_embedding_config)"
  },
  {
    "module": "srt.models.phi4mm_audio",
    "file": "python/sglang/srt/models/phi4mm_audio.py",
    "line": 884,
    "qualname": "ConformerEncoder.init_relative_attention_bias",
    "signature": "def init_relative_attention_bias(self, input_tensor)"
  },
  {
    "module": "srt.models.phi4mm_audio",
    "file": "python/sglang/srt/models/phi4mm_audio.py",
    "line": 888,
    "qualname": "ConformerEncoder.calculate_hs_mask",
    "signature": "def calculate_hs_mask(self, xs_pad, device, mask)"
  },
  {
    "module": "srt.models.phi4mm_audio",
    "file": "python/sglang/srt/models/phi4mm_audio.py",
    "line": 908,
    "qualname": "ConformerEncoder.forward",
    "signature": "def forward(self, xs_pad, masks)"
  },
  {
    "module": "srt.models.phi4mm_audio",
    "file": "python/sglang/srt/models/phi4mm_audio.py",
    "line": 1001,
    "qualname": "WindowQformer.__init__",
    "signature": "def __init__(self, window_size: int, num_queries: int, num_blocks: int, attention_dim: int, attention_heads: int, linear_units: int, dropout_rate: float, normalize_before: bool)"
  },
  {
    "module": "srt.models.phi4mm_audio",
    "file": "python/sglang/srt/models/phi4mm_audio.py",
    "line": 1035,
    "qualname": "WindowQformer.forward",
    "signature": "def forward(self, audio_embed, mask, embed_len)"
  },
  {
    "module": "srt.models.phi4mm_audio",
    "file": "python/sglang/srt/models/phi4mm_audio.py",
    "line": 1078,
    "qualname": "AudioEmbedding.__init__",
    "signature": "def __init__(self, config: PretrainedConfig)"
  },
  {
    "module": "srt.models.phi4mm_audio",
    "file": "python/sglang/srt/models/phi4mm_audio.py",
    "line": 1182,
    "qualname": "AudioEmbedding.set_audio_embeds",
    "signature": "def set_audio_embeds(self, input_embeds: torch.FloatTensor)"
  },
  {
    "module": "srt.models.phi4mm_audio",
    "file": "python/sglang/srt/models/phi4mm_audio.py",
    "line": 1185,
    "qualname": "AudioEmbedding.set_audio_embed_sizes",
    "signature": "def set_audio_embed_sizes(self, audio_embed_sizes: torch.LongTensor)"
  },
  {
    "module": "srt.models.phi4mm_audio",
    "file": "python/sglang/srt/models/phi4mm_audio.py",
    "line": 1188,
    "qualname": "AudioEmbedding.get_audio_features",
    "signature": "def get_audio_features(self, input_embeds: torch.FloatTensor, audio_attention_mask: torch.Tensor, audio_projection_mode: str)"
  },
  {
    "module": "srt.models.phi4mm_audio",
    "file": "python/sglang/srt/models/phi4mm_audio.py",
    "line": 1242,
    "qualname": "AudioEmbedding.forward",
    "signature": "def forward(self, audio_features: torch.FloatTensor, audio_attention_mask: torch.Tensor, audio_projection_mode: str)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 26,
    "qualname": "BlockBase.__init__",
    "signature": "def __init__(self, input_size, output_size)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 32,
    "qualname": "get_activation",
    "signature": "def get_activation(name)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 53,
    "qualname": "adaptive_enc_mask",
    "signature": "def adaptive_enc_mask(x_len, chunk_start_idx, left_window, right_window)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 108,
    "qualname": "Swish.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 112,
    "qualname": "Swish.forward",
    "signature": "def forward(self, x: Tensor)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 125,
    "qualname": "GLU.__init__",
    "signature": "def __init__(self, dim: int, act_name: str)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 141,
    "qualname": "GLU.forward",
    "signature": "def forward(self, x: Tensor)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 182,
    "qualname": "GLUPointWiseConv.__init__",
    "signature": "def __init__(self, input_dim, output_dim, kernel_size, glu_type, bias_in_glu, causal)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 228,
    "qualname": "GLUPointWiseConv.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 286,
    "qualname": "DepthWiseSeperableConv1d.__init__",
    "signature": "def __init__(self, input_dim, depthwise_seperable_out_channel, kernel_size, depthwise_multiplier, padding)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 317,
    "qualname": "DepthWiseSeperableConv1d.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 389,
    "qualname": "ConvModule.__init__",
    "signature": "def __init__(self, input_dim, ext_pw_out_channel, depthwise_seperable_out_channel, ext_pw_kernel_size, kernel_size, depthwise_multiplier, dropout_rate, causal, batch_norm, chunk_se, chunk_size, activation, glu_type, bias_in_glu, linear_glu_in_convm, export)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 512,
    "qualname": "ConvModule.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 580,
    "qualname": "GLULinear.__init__",
    "signature": "def __init__(self, input_dim, output_dim, glu_type, bias_in_glu)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 591,
    "qualname": "GLULinear.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 622,
    "qualname": "FeedForward.__init__",
    "signature": "def __init__(self, d_model, d_inner, dropout_rate, activation, bias_in_glu)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 643,
    "qualname": "FeedForward.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 722,
    "qualname": "T5RelativeAttentionLogitBias.__init__",
    "signature": "def __init__(self, num_heads, num_buckets, max_distance, symmetric)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 739,
    "qualname": "T5RelativeAttentionLogitBias.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 827,
    "qualname": "AbsolutePositionalEncoding.__init__",
    "signature": "def __init__(self, d_model, dropout_rate, max_len)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 837,
    "qualname": "AbsolutePositionalEncoding.extend_pe",
    "signature": "def extend_pe(self, x)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 858,
    "qualname": "AbsolutePositionalEncoding.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 886,
    "qualname": "MeanVarianceNormLayer.__init__",
    "signature": "def __init__(self, input_size)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 892,
    "qualname": "MeanVarianceNormLayer.forward",
    "signature": "def forward(self, input_: Tensor)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 919,
    "qualname": "CausalConv1D.__init__",
    "signature": "def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: Union[str, int], dilation: int, groups: int, bias: bool, padding_mode: str, device, dtype)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 969,
    "qualname": "CausalConv1D.update_cache",
    "signature": "def update_cache(self, x, cache)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 983,
    "qualname": "CausalConv1D.forward",
    "signature": "def forward(self, x, cache)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1000,
    "qualname": "CausalConv2D.__init__",
    "signature": "def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: Union[str, int], dilation: int, groups: int, bias: bool, padding_mode: str, device, dtype)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1034,
    "qualname": "CausalConv2D.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1080,
    "qualname": "NemoConvSubsampling.__init__",
    "signature": "def __init__(self, feat_in, feat_out, subsampling_factor, subsampling, conv_channels, subsampling_conv_chunking_factor, activation, is_causal)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1369,
    "qualname": "NemoConvSubsampling.get_sampling_frames",
    "signature": "def get_sampling_frames(self)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1372,
    "qualname": "NemoConvSubsampling.get_streaming_cache_size",
    "signature": "def get_streaming_cache_size(self)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1375,
    "qualname": "NemoConvSubsampling.forward",
    "signature": "def forward(self, x, mask)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1443,
    "qualname": "NemoConvSubsampling.reset_parameters",
    "signature": "def reset_parameters(self)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1468,
    "qualname": "NemoConvSubsampling.conv_split_by_batch",
    "signature": "def conv_split_by_batch(self, x)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1494,
    "qualname": "NemoConvSubsampling.conv_split_by_channel",
    "signature": "def conv_split_by_channel(self, x)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1532,
    "qualname": "NemoConvSubsampling.channel_chunked_conv",
    "signature": "def channel_chunked_conv(self, conv, chunk_size, x)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1572,
    "qualname": "NemoConvSubsampling.change_subsampling_conv_chunking_factor",
    "signature": "def change_subsampling_conv_chunking_factor(self, subsampling_conv_chunking_factor: int)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1586,
    "qualname": "calc_length",
    "signature": "def calc_length(lengths, all_paddings, kernel_size, stride, ceil_mode, repeat_num)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1601,
    "qualname": "AttModule.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1605,
    "qualname": "AttModule.set_export",
    "signature": "def set_export(self, mode)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1609,
    "qualname": "AttModule.forward",
    "signature": "def forward(self, x: Tensor, memory: Optional[Tensor], pos_emb: Optional[Tensor], att_mask: Optional[Tensor])"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1634,
    "qualname": "AttBlock.memory_dims",
    "signature": "def memory_dims(self, max_len)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1639,
    "qualname": "masked_softmax",
    "signature": "def masked_softmax(scores, mask: Optional[Tensor])"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1693,
    "qualname": "MultiHeadedAttention.__init__",
    "signature": "def __init__(self, n_head, n_feat, dropout_rate, attention_inner_dim, glu_type, bias_in_glu, use_pt_scaled_dot_product_attention, n_value, group_size: int)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1741,
    "qualname": "MultiHeadedAttention.forward",
    "signature": "def forward(self, query: Tensor, key: Tensor, value: Tensor, pos_k: Tensor, pos_v: Tensor, mask: Optional[Tensor], relative_attention_bias: Optional[Tensor])"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1868,
    "qualname": "MultiSequential.forward",
    "signature": "def forward(self)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1875,
    "qualname": "get_offset",
    "signature": "def get_offset(input_layer: str, time_reduction: int)"
  },
  {
    "module": "srt.models.phi4mm_utils",
    "file": "python/sglang/srt/models/phi4mm_utils.py",
    "line": 1894,
    "qualname": "unfold_tensor",
    "signature": "def unfold_tensor(xs_pad, max_seq_len)"
  },
  {
    "module": "srt.models.phimoe",
    "file": "python/sglang/srt/models/phimoe.py",
    "line": 39,
    "qualname": "PhiMoEConfig.__init__",
    "signature": "def __init__(self, vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, head_dim, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, tie_word_embeddings, rope_theta, sliding_window, attention_dropout, num_experts_per_tok, num_local_experts, output_router_logits, router_aux_loss_coef, router_jitter_noise, attention_bias, lm_head_bias)"
  },
  {
    "module": "srt.models.phimoe",
    "file": "python/sglang/srt/models/phimoe.py",
    "line": 107,
    "qualname": "sparsemixer",
    "signature": "def sparsemixer(scores, jitter_eps)"
  },
  {
    "module": "srt.models.phimoe",
    "file": "python/sglang/srt/models/phimoe.py",
    "line": 159,
    "qualname": "phimoe_routing_function",
    "signature": "def phimoe_routing_function(hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool)"
  },
  {
    "module": "srt.models.phimoe",
    "file": "python/sglang/srt/models/phimoe.py",
    "line": 182,
    "qualname": "PhiMoE.__init__",
    "signature": "def __init__(self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.phimoe",
    "file": "python/sglang/srt/models/phimoe.py",
    "line": 221,
    "qualname": "PhiMoE.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch])"
  },
  {
    "module": "srt.models.phimoe",
    "file": "python/sglang/srt/models/phimoe.py",
    "line": 235,
    "qualname": "PhiMoEAttention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: Optional[int], max_position: int, rope_theta: float, layer_id: int, attention_bias: bool, quant_config: Optional[QuantizationConfig], rope_scaling: Optional[dict], prefix: str)"
  },
  {
    "module": "srt.models.phimoe",
    "file": "python/sglang/srt/models/phimoe.py",
    "line": 315,
    "qualname": "PhiMoEAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.phimoe",
    "file": "python/sglang/srt/models/phimoe.py",
    "line": 331,
    "qualname": "PhiMoEDecoderLayer.__init__",
    "signature": "def __init__(self, config: PhiMoEConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.phimoe",
    "file": "python/sglang/srt/models/phimoe.py",
    "line": 372,
    "qualname": "PhiMoEDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, residual: Optional[torch.Tensor], forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.phimoe",
    "file": "python/sglang/srt/models/phimoe.py",
    "line": 402,
    "qualname": "PhiMoEModel.__init__",
    "signature": "def __init__(self, config: PhiMoEConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.phimoe",
    "file": "python/sglang/srt/models/phimoe.py",
    "line": 431,
    "qualname": "PhiMoEModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.phimoe",
    "file": "python/sglang/srt/models/phimoe.py",
    "line": 455,
    "qualname": "PhiMoEForCausalLM.__init__",
    "signature": "def __init__(self, config: PhiMoEConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.phimoe",
    "file": "python/sglang/srt/models/phimoe.py",
    "line": 484,
    "qualname": "PhiMoEForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor], get_embedding: bool)"
  },
  {
    "module": "srt.models.phimoe",
    "file": "python/sglang/srt/models/phimoe.py",
    "line": 502,
    "qualname": "PhiMoEForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.pixtral",
    "file": "python/sglang/srt/models/pixtral.py",
    "line": 46,
    "qualname": "PixtralHFMLP.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig])"
  },
  {
    "module": "srt.models.pixtral",
    "file": "python/sglang/srt/models/pixtral.py",
    "line": 76,
    "qualname": "PixtralHFMLP.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.pixtral",
    "file": "python/sglang/srt/models/pixtral.py",
    "line": 90,
    "qualname": "PixtralHFTransformerBlock.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig])"
  },
  {
    "module": "srt.models.pixtral",
    "file": "python/sglang/srt/models/pixtral.py",
    "line": 123,
    "qualname": "PixtralHFTransformerBlock.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor], position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]])"
  },
  {
    "module": "srt.models.pixtral",
    "file": "python/sglang/srt/models/pixtral.py",
    "line": 166,
    "qualname": "PixtralHFTransformer.__init__",
    "signature": "def __init__(self, config: PixtralVisionConfig, quant_config: Optional[QuantizationConfig])"
  },
  {
    "module": "srt.models.pixtral",
    "file": "python/sglang/srt/models/pixtral.py",
    "line": 192,
    "qualname": "PixtralHFTransformer.forward",
    "signature": "def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor], position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]], return_all_hidden_states: bool)"
  },
  {
    "module": "srt.models.pixtral",
    "file": "python/sglang/srt/models/pixtral.py",
    "line": 225,
    "qualname": "resolve_visual_encoder_outputs",
    "signature": "def resolve_visual_encoder_outputs(outputs: Union[torch.Tensor, List[torch.Tensor]], feature_sample_layers: Optional[List[int]], post_norm: Optional[nn.Module], num_hidden_layers: int)"
  },
  {
    "module": "srt.models.pixtral",
    "file": "python/sglang/srt/models/pixtral.py",
    "line": 271,
    "qualname": "PixtralHFVisionModel.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.pixtral",
    "file": "python/sglang/srt/models/pixtral.py",
    "line": 274,
    "qualname": "PixtralHFVisionModel.__init__",
    "signature": "def __init__(self, config: PixtralVisionConfig, quant_config: Optional[QuantizationConfig])"
  },
  {
    "module": "srt.models.pixtral",
    "file": "python/sglang/srt/models/pixtral.py",
    "line": 320,
    "qualname": "PixtralHFVisionModel.dtype",
    "signature": "def dtype(self)"
  },
  {
    "module": "srt.models.pixtral",
    "file": "python/sglang/srt/models/pixtral.py",
    "line": 324,
    "qualname": "PixtralHFVisionModel.device",
    "signature": "def device(self)"
  },
  {
    "module": "srt.models.pixtral",
    "file": "python/sglang/srt/models/pixtral.py",
    "line": 327,
    "qualname": "PixtralHFVisionModel.forward",
    "signature": "def forward(self, pixel_values: torch.Tensor, image_sizes: list[tuple[int, int]], output_hidden_states: bool, feature_sample_layers: Optional[list[int]])"
  },
  {
    "module": "srt.models.pixtral",
    "file": "python/sglang/srt/models/pixtral.py",
    "line": 419,
    "qualname": "PixtralHFVisionModel.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.qwen",
    "file": "python/sglang/srt/models/qwen.py",
    "line": 47,
    "qualname": "QWenMLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen",
    "file": "python/sglang/srt/models/qwen.py",
    "line": 79,
    "qualname": "QWenMLP.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.qwen",
    "file": "python/sglang/srt/models/qwen.py",
    "line": 87,
    "qualname": "QWenAttention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_heads: int, max_position_embeddings: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen",
    "file": "python/sglang/srt/models/qwen.py",
    "line": 141,
    "qualname": "QWenAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.qwen",
    "file": "python/sglang/srt/models/qwen.py",
    "line": 156,
    "qualname": "QWenBlock.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen",
    "file": "python/sglang/srt/models/qwen.py",
    "line": 188,
    "qualname": "QWenBlock.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.qwen",
    "file": "python/sglang/srt/models/qwen.py",
    "line": 213,
    "qualname": "QWenModel.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen",
    "file": "python/sglang/srt/models/qwen.py",
    "line": 242,
    "qualname": "QWenModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.qwen",
    "file": "python/sglang/srt/models/qwen.py",
    "line": 261,
    "qualname": "QWenLMHeadModel.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen",
    "file": "python/sglang/srt/models/qwen.py",
    "line": 279,
    "qualname": "QWenLMHeadModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.qwen",
    "file": "python/sglang/srt/models/qwen.py",
    "line": 291,
    "qualname": "QWenLMHeadModel.forward_split_prefill",
    "signature": "def forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int])"
  },
  {
    "module": "srt.models.qwen",
    "file": "python/sglang/srt/models/qwen.py",
    "line": 326,
    "qualname": "QWenLMHeadModel.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.qwen2_audio",
    "file": "python/sglang/srt/models/qwen2_audio.py",
    "line": 88,
    "qualname": "Qwen2AudioForConditionalGeneration.__init__",
    "signature": "def __init__(self, config: Qwen2AudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen2_audio",
    "file": "python/sglang/srt/models/qwen2_audio.py",
    "line": 115,
    "qualname": "Qwen2AudioForConditionalGeneration.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.qwen2_audio",
    "file": "python/sglang/srt/models/qwen2_audio.py",
    "line": 118,
    "qualname": "Qwen2AudioForConditionalGeneration.get_audio_feature",
    "signature": "def get_audio_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.qwen2_audio",
    "file": "python/sglang/srt/models/qwen2_audio.py",
    "line": 134,
    "qualname": "Qwen2AudioForConditionalGeneration.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.qwen2_audio",
    "file": "python/sglang/srt/models/qwen2_audio.py",
    "line": 153,
    "qualname": "Qwen2AudioForConditionalGeneration.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.qwen2_eagle",
    "file": "python/sglang/srt/models/qwen2_eagle.py",
    "line": 41,
    "qualname": "Qwen2DecoderLayer.__init__",
    "signature": "def __init__(self, config: Qwen2Config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen2_eagle",
    "file": "python/sglang/srt/models/qwen2_eagle.py",
    "line": 58,
    "qualname": "Qwen2Model.__init__",
    "signature": "def __init__(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen2_eagle",
    "file": "python/sglang/srt/models/qwen2_eagle.py",
    "line": 85,
    "qualname": "Qwen2Model.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.qwen2_eagle",
    "file": "python/sglang/srt/models/qwen2_eagle.py",
    "line": 115,
    "qualname": "Qwen2ForCausalLMEagle.__init__",
    "signature": "def __init__(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen2_eagle",
    "file": "python/sglang/srt/models/qwen2_eagle.py",
    "line": 139,
    "qualname": "Qwen2ForCausalLMEagle.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.qwen2_rm",
    "file": "python/sglang/srt/models/qwen2_rm.py",
    "line": 29,
    "qualname": "Qwen2ForRewardModel.__init__",
    "signature": "def __init__(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen2_rm",
    "file": "python/sglang/srt/models/qwen2_rm.py",
    "line": 52,
    "qualname": "Qwen2ForRewardModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)"
  },
  {
    "module": "srt.models.qwen2_rm",
    "file": "python/sglang/srt/models/qwen2_rm.py",
    "line": 68,
    "qualname": "Qwen2ForRewardModel.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 92,
    "qualname": "Qwen2VisionMLP.__init__",
    "signature": "def __init__(self, in_features: int, hidden_features: int, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 115,
    "qualname": "Qwen2VisionMLP.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 124,
    "qualname": "Qwen2VisionBlock.__init__",
    "signature": "def __init__(self, dim: int, num_heads: int, mlp_ratio: float, act_layer: Type[nn.Module], norm_layer: Type[nn.Module], attn_implementation: Optional[str], quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 170,
    "qualname": "Qwen2VisionBlock.forward",
    "signature": "def forward(self, x: torch.Tensor, cu_seqlens: torch.Tensor, position_embeddings: torch.Tensor)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 191,
    "qualname": "Qwen2VisionPatchEmbed.__init__",
    "signature": "def __init__(self, patch_size: int, temporal_patch_size: int, in_chans: int, embed_dim: int)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 208,
    "qualname": "Qwen2VisionPatchEmbed.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 217,
    "qualname": "Qwen2VisionPatchMerger.__init__",
    "signature": "def __init__(self, d_model: int, context_dim: int, norm_layer: Type[nn.Module], spatial_merge_size: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 251,
    "qualname": "Qwen2VisionPatchMerger.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 264,
    "qualname": "Qwen2VisionRotaryEmbedding.__init__",
    "signature": "def __init__(self, dim: int, theta: float)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 273,
    "qualname": "Qwen2VisionRotaryEmbedding.update_freqs_cache",
    "signature": "def update_freqs_cache(self, seqlen: int)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 292,
    "qualname": "Qwen2VisionRotaryEmbedding.forward",
    "signature": "def forward(self, seqlen: int)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 299,
    "qualname": "Qwen2VisionTransformer.__init__",
    "signature": "def __init__(self, vision_config: Qwen2VLVisionConfig, norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 353,
    "qualname": "Qwen2VisionTransformer.dtype",
    "signature": "def dtype(self)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 357,
    "qualname": "Qwen2VisionTransformer.device",
    "signature": "def device(self)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 360,
    "qualname": "Qwen2VisionTransformer.rot_pos_emb",
    "signature": "def rot_pos_emb(self, grid_thw: torch.Tensor)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 393,
    "qualname": "Qwen2VisionTransformer.forward",
    "signature": "def forward(self, x: torch.Tensor, grid_thw: torch.Tensor)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 445,
    "qualname": "Qwen2VLForConditionalGeneration.__init__",
    "signature": "def __init__(self, config: Qwen2VLConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 481,
    "qualname": "Qwen2VLForConditionalGeneration.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 485,
    "qualname": "Qwen2VLForConditionalGeneration.get_image_feature",
    "signature": "def get_image_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 496,
    "qualname": "Qwen2VLForConditionalGeneration.get_video_feature",
    "signature": "def get_video_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 514,
    "qualname": "Qwen2VLForConditionalGeneration.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 517,
    "qualname": "Qwen2VLForConditionalGeneration.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)"
  },
  {
    "module": "srt.models.qwen2_vl",
    "file": "python/sglang/srt/models/qwen2_vl.py",
    "line": 563,
    "qualname": "Qwen2VLForConditionalGeneration.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.qwen3",
    "file": "python/sglang/srt/models/qwen3.py",
    "line": 39,
    "qualname": "Qwen3Attention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], head_dim: Optional[int], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], rms_norm_eps: float, attention_bias: bool, prefix: str, alt_stream: Optional[torch.cuda.Stream])"
  },
  {
    "module": "srt.models.qwen3",
    "file": "python/sglang/srt/models/qwen3.py",
    "line": 146,
    "qualname": "Qwen3Attention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.qwen3",
    "file": "python/sglang/srt/models/qwen3.py",
    "line": 162,
    "qualname": "Qwen3DecoderLayer.__init__",
    "signature": "def __init__(self, config: Qwen3Config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream])"
  },
  {
    "module": "srt.models.qwen3",
    "file": "python/sglang/srt/models/qwen3.py",
    "line": 215,
    "qualname": "Qwen3DecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.qwen3",
    "file": "python/sglang/srt/models/qwen3.py",
    "line": 245,
    "qualname": "Qwen3Model.__init__",
    "signature": "def __init__(self, config: Qwen3Config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen3",
    "file": "python/sglang/srt/models/qwen3.py",
    "line": 281,
    "qualname": "Qwen3ForCausalLM.__init__",
    "signature": "def __init__(self, config: Qwen3Config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen3",
    "file": "python/sglang/srt/models/qwen3.py",
    "line": 330,
    "qualname": "Qwen3ForCausalLM.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.qwen3",
    "file": "python/sglang/srt/models/qwen3.py",
    "line": 334,
    "qualname": "Qwen3ForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.qwen3",
    "file": "python/sglang/srt/models/qwen3.py",
    "line": 370,
    "qualname": "Qwen3ForCausalLM.forward_split_prefill",
    "signature": "def forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.qwen3",
    "file": "python/sglang/srt/models/qwen3.py",
    "line": 411,
    "qualname": "Qwen3ForCausalLM.start_layer",
    "signature": "def start_layer(self)"
  },
  {
    "module": "srt.models.qwen3",
    "file": "python/sglang/srt/models/qwen3.py",
    "line": 415,
    "qualname": "Qwen3ForCausalLM.end_layer",
    "signature": "def end_layer(self)"
  },
  {
    "module": "srt.models.qwen3",
    "file": "python/sglang/srt/models/qwen3.py",
    "line": 418,
    "qualname": "Qwen3ForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.qwen3",
    "file": "python/sglang/srt/models/qwen3.py",
    "line": 487,
    "qualname": "Qwen3ForCausalLM.get_embed_and_head",
    "signature": "def get_embed_and_head(self)"
  },
  {
    "module": "srt.models.qwen3",
    "file": "python/sglang/srt/models/qwen3.py",
    "line": 490,
    "qualname": "Qwen3ForCausalLM.set_embed_and_head",
    "signature": "def set_embed_and_head(self, embed, head)"
  },
  {
    "module": "srt.models.qwen3",
    "file": "python/sglang/srt/models/qwen3.py",
    "line": 498,
    "qualname": "Qwen3ForCausalLM.load_kv_cache_scales",
    "signature": "def load_kv_cache_scales(self, quantization_param_path: str)"
  },
  {
    "module": "srt.models.qwen3",
    "file": "python/sglang/srt/models/qwen3.py",
    "line": 501,
    "qualname": "Qwen3ForCausalLM.set_eagle3_layers_to_capture",
    "signature": "def set_eagle3_layers_to_capture(self, layer_ids: Optional[List[int]])"
  },
  {
    "module": "srt.models.qwen3_classification",
    "file": "python/sglang/srt/models/qwen3_classification.py",
    "line": 29,
    "qualname": "Qwen3ForSequenceClassification.__init__",
    "signature": "def __init__(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen3_classification",
    "file": "python/sglang/srt/models/qwen3_classification.py",
    "line": 56,
    "qualname": "Qwen3ForSequenceClassification.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor], get_embedding: bool)"
  },
  {
    "module": "srt.models.qwen3_classification",
    "file": "python/sglang/srt/models/qwen3_classification.py",
    "line": 74,
    "qualname": "Qwen3ForSequenceClassification.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 69,
    "qualname": "Qwen3MoeSparseMoeBlock.__init__",
    "signature": "def __init__(self, layer_id: int, config: Qwen3MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 118,
    "qualname": "Qwen3MoeSparseMoeBlock.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch], use_reduce_scatter: bool)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 130,
    "qualname": "Qwen3MoeSparseMoeBlock.get_moe_weights",
    "signature": "def get_moe_weights(self)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 137,
    "qualname": "Qwen3MoeSparseMoeBlock.forward_normal",
    "signature": "def forward_normal(self, hidden_states: torch.Tensor, use_reduce_scatter: bool)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 154,
    "qualname": "Qwen3MoeSparseMoeBlock.forward_deepep",
    "signature": "def forward_deepep(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 183,
    "qualname": "Qwen3MoeSparseMoeBlock.op_gate",
    "signature": "def op_gate(self, state)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 192,
    "qualname": "Qwen3MoeSparseMoeBlock.op_select_experts",
    "signature": "def op_select_experts(self, state)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 215,
    "qualname": "Qwen3MoeSparseMoeBlock.op_dispatch_a",
    "signature": "def op_dispatch_a(self, state)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 225,
    "qualname": "Qwen3MoeSparseMoeBlock.op_dispatch_b",
    "signature": "def op_dispatch_b(self, state)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 234,
    "qualname": "Qwen3MoeSparseMoeBlock.op_experts",
    "signature": "def op_experts(self, state)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 239,
    "qualname": "Qwen3MoeSparseMoeBlock.op_combine_a",
    "signature": "def op_combine_a(self, state)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 250,
    "qualname": "Qwen3MoeSparseMoeBlock.op_combine_b",
    "signature": "def op_combine_b(self, state)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 258,
    "qualname": "Qwen3MoeSparseMoeBlock.op_output",
    "signature": "def op_output(self, state)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 263,
    "qualname": "Qwen3MoeAttention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, head_dim: Optional[int], rms_norm_eps: float, attention_bias: bool, quant_config: Optional[QuantizationConfig], prefix: str, dual_chunk_attention_config: Optional[dict[str, Any]], alt_stream: Optional[torch.cuda.Stream])"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 373,
    "qualname": "Qwen3MoeAttention.op_prepare",
    "signature": "def op_prepare(self, state)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 380,
    "qualname": "Qwen3MoeAttention.op_core",
    "signature": "def op_core(self, state)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 385,
    "qualname": "Qwen3MoeAttention.forward_prepare",
    "signature": "def forward_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 400,
    "qualname": "Qwen3MoeAttention.forward_core",
    "signature": "def forward_core(self, intermediate_state)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 408,
    "qualname": "Qwen3MoeAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 423,
    "qualname": "Qwen3MoeDecoderLayer.__init__",
    "signature": "def __init__(self, config: Qwen3MoeConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream])"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 505,
    "qualname": "Qwen3MoeDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 541,
    "qualname": "Qwen3MoeDecoderLayer.op_comm_prepare_attn",
    "signature": "def op_comm_prepare_attn(self, state, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], tbo_subbatch_index: Optional[int])"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 561,
    "qualname": "Qwen3MoeDecoderLayer.op_comm_prepare_mlp",
    "signature": "def op_comm_prepare_mlp(self, state)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 570,
    "qualname": "Qwen3MoeDecoderLayer.op_mlp",
    "signature": "def op_mlp(self, state)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 574,
    "qualname": "Qwen3MoeDecoderLayer.op_comm_postprocess_layer",
    "signature": "def op_comm_postprocess_layer(self, state)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 600,
    "qualname": "Qwen3MoeModel.__init__",
    "signature": "def __init__(self, config: Qwen3MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 619,
    "qualname": "Qwen3MoeForCausalLM.__init__",
    "signature": "def __init__(self, config: Qwen3MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 642,
    "qualname": "Qwen3MoeForCausalLM.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 646,
    "qualname": "Qwen3MoeForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 674,
    "qualname": "Qwen3MoeForCausalLM.forward_split_prefill",
    "signature": "def forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 717,
    "qualname": "Qwen3MoeForCausalLM.start_layer",
    "signature": "def start_layer(self)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 721,
    "qualname": "Qwen3MoeForCausalLM.end_layer",
    "signature": "def end_layer(self)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 724,
    "qualname": "Qwen3MoeForCausalLM.get_embed_and_head",
    "signature": "def get_embed_and_head(self)"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 727,
    "qualname": "Qwen3MoeForCausalLM.set_eagle3_layers_to_capture",
    "signature": "def set_eagle3_layers_to_capture(self, layer_ids: Optional[List[int]])"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 742,
    "qualname": "Qwen3MoeForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.qwen3_moe",
    "file": "python/sglang/srt/models/qwen3_moe.py",
    "line": 857,
    "qualname": "Qwen3MoeForCausalLM.get_model_config_for_expert_location",
    "signature": "def get_model_config_for_expert_location(cls, config)"
  },
  {
    "module": "srt.models.registry",
    "file": "python/sglang/srt/models/registry.py",
    "line": 20,
    "qualname": "_ModelRegistry.get_supported_archs",
    "signature": "def get_supported_archs(self)"
  },
  {
    "module": "srt.models.registry",
    "file": "python/sglang/srt/models/registry.py",
    "line": 62,
    "qualname": "_ModelRegistry.resolve_model_cls",
    "signature": "def resolve_model_cls(self, architectures: Union[str, List[str]])"
  },
  {
    "module": "srt.models.registry",
    "file": "python/sglang/srt/models/registry.py",
    "line": 77,
    "qualname": "import_model_classes",
    "signature": "def import_model_classes()"
  },
  {
    "module": "srt.models.roberta",
    "file": "python/sglang/srt/models/roberta.py",
    "line": 23,
    "qualname": "RobertaClassificationHead.__init__",
    "signature": "def __init__(self, config: RobertaConfig)"
  },
  {
    "module": "srt.models.roberta",
    "file": "python/sglang/srt/models/roberta.py",
    "line": 28,
    "qualname": "RobertaClassificationHead.forward",
    "signature": "def forward(self, features)"
  },
  {
    "module": "srt.models.roberta",
    "file": "python/sglang/srt/models/roberta.py",
    "line": 38,
    "qualname": "RobertaEmbedding.__init__",
    "signature": "def __init__(self, config: RobertaConfig)"
  },
  {
    "module": "srt.models.roberta",
    "file": "python/sglang/srt/models/roberta.py",
    "line": 66,
    "qualname": "RobertaEmbedding.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, seq_lens: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.roberta",
    "file": "python/sglang/srt/models/roberta.py",
    "line": 115,
    "qualname": "XLMRobertaBaseModel.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.models.roberta",
    "file": "python/sglang/srt/models/roberta.py",
    "line": 135,
    "qualname": "XLMRobertaBaseModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)"
  },
  {
    "module": "srt.models.roberta",
    "file": "python/sglang/srt/models/roberta.py",
    "line": 157,
    "qualname": "XLMRobertaBaseModel.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.roberta",
    "file": "python/sglang/srt/models/roberta.py",
    "line": 192,
    "qualname": "create_position_ids_from_input_ids",
    "signature": "def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length)"
  },
  {
    "module": "srt.models.roberta",
    "file": "python/sglang/srt/models/roberta.py",
    "line": 203,
    "qualname": "XLMRobertaModel.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.models.roberta",
    "file": "python/sglang/srt/models/roberta.py",
    "line": 216,
    "qualname": "XLMRobertaModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)"
  },
  {
    "module": "srt.models.roberta",
    "file": "python/sglang/srt/models/roberta.py",
    "line": 229,
    "qualname": "XLMRobertaModel.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.roberta",
    "file": "python/sglang/srt/models/roberta.py",
    "line": 234,
    "qualname": "XLMRobertaForSequenceClassification.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.models.roberta",
    "file": "python/sglang/srt/models/roberta.py",
    "line": 248,
    "qualname": "XLMRobertaForSequenceClassification.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)"
  },
  {
    "module": "srt.models.roberta",
    "file": "python/sglang/srt/models/roberta.py",
    "line": 265,
    "qualname": "XLMRobertaForSequenceClassification.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.siglip",
    "file": "python/sglang/srt/models/siglip.py",
    "line": 22,
    "qualname": "SiglipVisionEmbeddings.__init__",
    "signature": "def __init__(self, config: SiglipVisionConfig)"
  },
  {
    "module": "srt.models.siglip",
    "file": "python/sglang/srt/models/siglip.py",
    "line": 48,
    "qualname": "SiglipVisionEmbeddings.forward",
    "signature": "def forward(self, pixel_values: torch.Tensor)"
  },
  {
    "module": "srt.models.siglip",
    "file": "python/sglang/srt/models/siglip.py",
    "line": 63,
    "qualname": "SiglipMLP.__init__",
    "signature": "def __init__(self, config, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.siglip",
    "file": "python/sglang/srt/models/siglip.py",
    "line": 85,
    "qualname": "SiglipMLP.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.siglip",
    "file": "python/sglang/srt/models/siglip.py",
    "line": 95,
    "qualname": "SiglipEncoderLayer.__init__",
    "signature": "def __init__(self, config: SiglipVisionConfig, act_layer: Type[nn.Module], norm_layer: Type[nn.Module], attn_implementation: Optional[str], quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.siglip",
    "file": "python/sglang/srt/models/siglip.py",
    "line": 136,
    "qualname": "SiglipEncoderLayer.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, causal_attention_mask: torch.Tensor)"
  },
  {
    "module": "srt.models.siglip",
    "file": "python/sglang/srt/models/siglip.py",
    "line": 176,
    "qualname": "SiglipEncoder.__init__",
    "signature": "def __init__(self, config: SiglipVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.siglip",
    "file": "python/sglang/srt/models/siglip.py",
    "line": 201,
    "qualname": "SiglipEncoder.forward",
    "signature": "def forward(self, inputs_embeds: torch.Tensor, attention_mask: torch.Tensor, causal_attention_mask: torch.Tensor, return_all_hidden_states: bool)"
  },
  {
    "module": "srt.models.siglip",
    "file": "python/sglang/srt/models/siglip.py",
    "line": 225,
    "qualname": "SiglipVisionTransformer.__init__",
    "signature": "def __init__(self, config: SiglipVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.siglip",
    "file": "python/sglang/srt/models/siglip.py",
    "line": 255,
    "qualname": "SiglipVisionTransformer.device",
    "signature": "def device(self)"
  },
  {
    "module": "srt.models.siglip",
    "file": "python/sglang/srt/models/siglip.py",
    "line": 258,
    "qualname": "SiglipVisionTransformer.forward",
    "signature": "def forward(self, pixel_values: torch.Tensor)"
  },
  {
    "module": "srt.models.siglip",
    "file": "python/sglang/srt/models/siglip.py",
    "line": 278,
    "qualname": "SiglipVisionModel.__init__",
    "signature": "def __init__(self, config: SiglipVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.siglip",
    "file": "python/sglang/srt/models/siglip.py",
    "line": 290,
    "qualname": "SiglipVisionModel.device",
    "signature": "def device(self)"
  },
  {
    "module": "srt.models.siglip",
    "file": "python/sglang/srt/models/siglip.py",
    "line": 293,
    "qualname": "SiglipVisionModel.forward",
    "signature": "def forward(self, pixel_values: torch.Tensor)"
  },
  {
    "module": "srt.models.stablelm",
    "file": "python/sglang/srt/models/stablelm.py",
    "line": 49,
    "qualname": "StablelmMLP.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.stablelm",
    "file": "python/sglang/srt/models/stablelm.py",
    "line": 75,
    "qualname": "StablelmMLP.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.stablelm",
    "file": "python/sglang/srt/models/stablelm.py",
    "line": 83,
    "qualname": "StablelmAttention.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.stablelm",
    "file": "python/sglang/srt/models/stablelm.py",
    "line": 156,
    "qualname": "StablelmAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.stablelm",
    "file": "python/sglang/srt/models/stablelm.py",
    "line": 171,
    "qualname": "StablelmDecoderLayer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.stablelm",
    "file": "python/sglang/srt/models/stablelm.py",
    "line": 189,
    "qualname": "StablelmDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.stablelm",
    "file": "python/sglang/srt/models/stablelm.py",
    "line": 215,
    "qualname": "StableLMEpochModel.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.stablelm",
    "file": "python/sglang/srt/models/stablelm.py",
    "line": 241,
    "qualname": "StableLMEpochModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.stablelm",
    "file": "python/sglang/srt/models/stablelm.py",
    "line": 264,
    "qualname": "StableLmForCausalLM.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.stablelm",
    "file": "python/sglang/srt/models/stablelm.py",
    "line": 282,
    "qualname": "StableLmForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.stablelm",
    "file": "python/sglang/srt/models/stablelm.py",
    "line": 294,
    "qualname": "StableLmForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 75,
    "qualname": "Step3TextMLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 105,
    "qualname": "Step3TextMLP.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 114,
    "qualname": "Step3TextMoEMLP.__init__",
    "signature": "def __init__(self, layer_id: int, config: Step3TextConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 157,
    "qualname": "Step3TextMoEMLP.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 173,
    "qualname": "Step3TextAttention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: int, share_q_dim: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], rms_norm_eps, prefix: str)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 267,
    "qualname": "Step3TextAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 284,
    "qualname": "Step3TextDecoderLayer.__init__",
    "signature": "def __init__(self, config: Step3TextConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 388,
    "qualname": "Step3TextDecoderLayer.moe_mlp_forward",
    "signature": "def moe_mlp_forward(self, hidden_states)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 397,
    "qualname": "Step3TextDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 432,
    "qualname": "Step3TextModel.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 461,
    "qualname": "Step3TextModel.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 464,
    "qualname": "Step3TextModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 496,
    "qualname": "get_abs_pos",
    "signature": "def get_abs_pos(abs_pos, tgt_size)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 529,
    "qualname": "Step3VisionMLP.__init__",
    "signature": "def __init__(self, dim: int, intermediate_size: int, bias: bool, hidden_act, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 564,
    "qualname": "Step3VisionMLP.forward",
    "signature": "def forward(self, hidden_states)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 572,
    "qualname": "Step3VisionAttention.__init__",
    "signature": "def __init__(self, dim: int, num_heads: int, qkv_backend, quant_config, prefix: str)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 605,
    "qualname": "Step3VisionAttention.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 612,
    "qualname": "Step3VisionEmbeddings.__init__",
    "signature": "def __init__(self, config: Step3VisionEncoderConfig)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 641,
    "qualname": "Step3VisionEmbeddings.forward",
    "signature": "def forward(self, pixel_values: torch.Tensor)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 665,
    "qualname": "Step3VisionEncoderLayer.__init__",
    "signature": "def __init__(self, config, attn_implementation: str)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 680,
    "qualname": "Step3VisionEncoderLayer.forward",
    "signature": "def forward(self, hidden_states)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 687,
    "qualname": "Step3VisionTransformer.__init__",
    "signature": "def __init__(self, config: Step3VisionEncoderConfig)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 695,
    "qualname": "Step3VisionTransformer.dtype",
    "signature": "def dtype(self)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 698,
    "qualname": "Step3VisionTransformer.forward",
    "signature": "def forward(self, pixel_values: torch.Tensor)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 716,
    "qualname": "Step3VisionEncoder.__init__",
    "signature": "def __init__(self, config: Step3VisionEncoderConfig)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 723,
    "qualname": "Step3VisionEncoder.forward",
    "signature": "def forward(self, inputs_embeds)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 739,
    "qualname": "Step3VLForConditionalGeneration.__init__",
    "signature": "def __init__(self, config: Step3VLConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 815,
    "qualname": "Step3VLForConditionalGeneration.get_image_feature",
    "signature": "def get_image_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 858,
    "qualname": "Step3VLForConditionalGeneration.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 863,
    "qualname": "Step3VLForConditionalGeneration.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 884,
    "qualname": "Step3VLForConditionalGeneration.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.step3_vl",
    "file": "python/sglang/srt/models/step3_vl.py",
    "line": 997,
    "qualname": "Step3VLForConditionalGeneration.get_model_config_for_expert_location",
    "signature": "def get_model_config_for_expert_location(cls, config: Step3VLConfig)"
  },
  {
    "module": "srt.models.torch_native_llama",
    "file": "python/sglang/srt/models/torch_native_llama.py",
    "line": 73,
    "qualname": "gate_up_proj_weight_loader",
    "signature": "def gate_up_proj_weight_loader(self, param: Parameter, loaded_weight: torch.Tensor, loaded_shard_id: int)"
  },
  {
    "module": "srt.models.torch_native_llama",
    "file": "python/sglang/srt/models/torch_native_llama.py",
    "line": 108,
    "qualname": "LlamaMLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.torch_native_llama",
    "file": "python/sglang/srt/models/torch_native_llama.py",
    "line": 135,
    "qualname": "LlamaMLP.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.torch_native_llama",
    "file": "python/sglang/srt/models/torch_native_llama.py",
    "line": 142,
    "qualname": "qkv_proj_weight_loader",
    "signature": "def qkv_proj_weight_loader(self, param: Parameter, loaded_weight: torch.Tensor, loaded_shard_id: str)"
  },
  {
    "module": "srt.models.torch_native_llama",
    "file": "python/sglang/srt/models/torch_native_llama.py",
    "line": 180,
    "qualname": "LlamaAttention.__init__",
    "signature": "def __init__(self, config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.torch_native_llama",
    "file": "python/sglang/srt/models/torch_native_llama.py",
    "line": 253,
    "qualname": "LlamaAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.torch_native_llama",
    "file": "python/sglang/srt/models/torch_native_llama.py",
    "line": 268,
    "qualname": "LlamaDecoderLayer.__init__",
    "signature": "def __init__(self, config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.torch_native_llama",
    "file": "python/sglang/srt/models/torch_native_llama.py",
    "line": 312,
    "qualname": "LlamaDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.torch_native_llama",
    "file": "python/sglang/srt/models/torch_native_llama.py",
    "line": 338,
    "qualname": "LlamaModel.__init__",
    "signature": "def __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig])"
  },
  {
    "module": "srt.models.torch_native_llama",
    "file": "python/sglang/srt/models/torch_native_llama.py",
    "line": 361,
    "qualname": "LlamaModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.torch_native_llama",
    "file": "python/sglang/srt/models/torch_native_llama.py",
    "line": 386,
    "qualname": "TorchNativeLlamaForCausalLM.__init__",
    "signature": "def __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig])"
  },
  {
    "module": "srt.models.torch_native_llama",
    "file": "python/sglang/srt/models/torch_native_llama.py",
    "line": 407,
    "qualname": "TorchNativeLlamaForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.torch_native_llama",
    "file": "python/sglang/srt/models/torch_native_llama.py",
    "line": 419,
    "qualname": "TorchNativeLlamaForCausalLM.get_module_name_from_weight_name",
    "signature": "def get_module_name_from_weight_name(self, name)"
  },
  {
    "module": "srt.models.torch_native_llama",
    "file": "python/sglang/srt/models/torch_native_llama.py",
    "line": 436,
    "qualname": "TorchNativeLlamaForCausalLM.get_num_params",
    "signature": "def get_num_params(self)"
  },
  {
    "module": "srt.models.torch_native_llama",
    "file": "python/sglang/srt/models/torch_native_llama.py",
    "line": 440,
    "qualname": "TorchNativeLlamaForCausalLM.load_weights_to_module",
    "signature": "def load_weights_to_module(self, fqn: str, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.torch_native_llama",
    "file": "python/sglang/srt/models/torch_native_llama.py",
    "line": 488,
    "qualname": "TorchNativeLlamaForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.vila",
    "file": "python/sglang/srt/models/vila.py",
    "line": 56,
    "qualname": "VILAConfig.__init__",
    "signature": "def __init__(self, text_config: Optional[Dict[str, Any]], vision_config: Optional[Dict[str, Any]])"
  },
  {
    "module": "srt.models.vila",
    "file": "python/sglang/srt/models/vila.py",
    "line": 94,
    "qualname": "DownSample3x3BlockFix.forward",
    "signature": "def forward(self, x: Tensor)"
  },
  {
    "module": "srt.models.vila",
    "file": "python/sglang/srt/models/vila.py",
    "line": 130,
    "qualname": "MultimodalProjector.__init__",
    "signature": "def __init__(self, config: VILAConfig)"
  },
  {
    "module": "srt.models.vila",
    "file": "python/sglang/srt/models/vila.py",
    "line": 160,
    "qualname": "MultimodalProjector.device",
    "signature": "def device(self)"
  },
  {
    "module": "srt.models.vila",
    "file": "python/sglang/srt/models/vila.py",
    "line": 164,
    "qualname": "MultimodalProjector.dtype",
    "signature": "def dtype(self)"
  },
  {
    "module": "srt.models.vila",
    "file": "python/sglang/srt/models/vila.py",
    "line": 167,
    "qualname": "MultimodalProjector.forward",
    "signature": "def forward(self, x: Tensor)"
  },
  {
    "module": "srt.models.vila",
    "file": "python/sglang/srt/models/vila.py",
    "line": 193,
    "qualname": "VILAForConditionalGeneration.__init__",
    "signature": "def __init__(self, config: VILAConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.vila",
    "file": "python/sglang/srt/models/vila.py",
    "line": 216,
    "qualname": "VILAForConditionalGeneration.dtype",
    "signature": "def dtype(self)"
  },
  {
    "module": "srt.models.vila",
    "file": "python/sglang/srt/models/vila.py",
    "line": 219,
    "qualname": "VILAForConditionalGeneration.forward",
    "signature": "def forward(self, input_ids: Tensor, positions: Tensor, forward_batch: ForwardBatch, get_embedding: bool)"
  },
  {
    "module": "srt.models.vila",
    "file": "python/sglang/srt/models/vila.py",
    "line": 239,
    "qualname": "VILAForConditionalGeneration.get_image_feature",
    "signature": "def get_image_feature(self, mm_input: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.vila",
    "file": "python/sglang/srt/models/vila.py",
    "line": 265,
    "qualname": "VILAForConditionalGeneration.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, Tensor]])"
  },
  {
    "module": "srt.models.vila",
    "file": "python/sglang/srt/models/vila.py",
    "line": 278,
    "qualname": "VILAForConditionalGeneration.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.xverse",
    "file": "python/sglang/srt/models/xverse.py",
    "line": 47,
    "qualname": "XverseMLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.xverse",
    "file": "python/sglang/srt/models/xverse.py",
    "line": 77,
    "qualname": "XverseMLP.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.xverse",
    "file": "python/sglang/srt/models/xverse.py",
    "line": 85,
    "qualname": "XverseAttention.__init__",
    "signature": "def __init__(self, config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.xverse",
    "file": "python/sglang/srt/models/xverse.py",
    "line": 160,
    "qualname": "XverseAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.xverse",
    "file": "python/sglang/srt/models/xverse.py",
    "line": 175,
    "qualname": "XverseDecoderLayer.__init__",
    "signature": "def __init__(self, config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.xverse",
    "file": "python/sglang/srt/models/xverse.py",
    "line": 222,
    "qualname": "XverseDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.xverse",
    "file": "python/sglang/srt/models/xverse.py",
    "line": 248,
    "qualname": "XverseModel.__init__",
    "signature": "def __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.xverse",
    "file": "python/sglang/srt/models/xverse.py",
    "line": 276,
    "qualname": "XverseModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.xverse",
    "file": "python/sglang/srt/models/xverse.py",
    "line": 302,
    "qualname": "XverseForCausalLM.__init__",
    "signature": "def __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.xverse",
    "file": "python/sglang/srt/models/xverse.py",
    "line": 320,
    "qualname": "XverseForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.xverse",
    "file": "python/sglang/srt/models/xverse.py",
    "line": 332,
    "qualname": "XverseForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], name, loaded_weight)"
  },
  {
    "module": "srt.models.xverse_moe",
    "file": "python/sglang/srt/models/xverse_moe.py",
    "line": 53,
    "qualname": "XverseMLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str)"
  },
  {
    "module": "srt.models.xverse_moe",
    "file": "python/sglang/srt/models/xverse_moe.py",
    "line": 85,
    "qualname": "XverseMLP.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.xverse_moe",
    "file": "python/sglang/srt/models/xverse_moe.py",
    "line": 94,
    "qualname": "XverseMoE.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.xverse_moe",
    "file": "python/sglang/srt/models/xverse_moe.py",
    "line": 151,
    "qualname": "XverseMoE.pack_params",
    "signature": "def pack_params(self)"
  },
  {
    "module": "srt.models.xverse_moe",
    "file": "python/sglang/srt/models/xverse_moe.py",
    "line": 170,
    "qualname": "XverseMoE.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor)"
  },
  {
    "module": "srt.models.xverse_moe",
    "file": "python/sglang/srt/models/xverse_moe.py",
    "line": 195,
    "qualname": "XverseAttention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.xverse_moe",
    "file": "python/sglang/srt/models/xverse_moe.py",
    "line": 265,
    "qualname": "XverseAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.xverse_moe",
    "file": "python/sglang/srt/models/xverse_moe.py",
    "line": 281,
    "qualname": "XverseDecoderLayer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.xverse_moe",
    "file": "python/sglang/srt/models/xverse_moe.py",
    "line": 326,
    "qualname": "XverseDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.xverse_moe",
    "file": "python/sglang/srt/models/xverse_moe.py",
    "line": 355,
    "qualname": "XverseModel.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.xverse_moe",
    "file": "python/sglang/srt/models/xverse_moe.py",
    "line": 383,
    "qualname": "XverseModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.xverse_moe",
    "file": "python/sglang/srt/models/xverse_moe.py",
    "line": 402,
    "qualname": "XverseMoeForCausalLM.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.xverse_moe",
    "file": "python/sglang/srt/models/xverse_moe.py",
    "line": 423,
    "qualname": "XverseMoeForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.xverse_moe",
    "file": "python/sglang/srt/models/xverse_moe.py",
    "line": 434,
    "qualname": "XverseMoeForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.yivl",
    "file": "python/sglang/srt/models/yivl.py",
    "line": 28,
    "qualname": "YiVLForCausalLM.__init__",
    "signature": "def __init__(self, config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.yivl",
    "file": "python/sglang/srt/models/yivl.py",
    "line": 41,
    "qualname": "YiVLForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.yivl",
    "file": "python/sglang/srt/models/yivl.py",
    "line": 93,
    "qualname": "YiVLMultiModalProjector.__init__",
    "signature": "def __init__(self, config: LlavaConfig)"
  },
  {
    "module": "srt.models.yivl",
    "file": "python/sglang/srt/models/yivl.py",
    "line": 106,
    "qualname": "YiVLMultiModalProjector.forward",
    "signature": "def forward(self, image_features)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 179,
    "qualname": "DeepseekV2MLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str, tp_rank: Optional[int], tp_size: Optional[int])"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 219,
    "qualname": "DeepseekV2MLP.forward",
    "signature": "def forward(self, x, forward_batch, should_allreduce_fusion: bool, use_reduce_scatter: bool)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 238,
    "qualname": "MoEGate.__init__",
    "signature": "def __init__(self, config, prefix: str, is_nextn: bool)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 258,
    "qualname": "MoEGate.forward",
    "signature": "def forward(self, hidden_states)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 285,
    "qualname": "DeepseekV2MoE.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream], is_nextn: bool)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 427,
    "qualname": "DeepseekV2MoE.get_moe_weights",
    "signature": "def get_moe_weights(self)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 434,
    "qualname": "DeepseekV2MoE.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch], should_allreduce_fusion: bool, use_reduce_scatter: bool)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 463,
    "qualname": "DeepseekV2MoE.forward_normal_dual_stream",
    "signature": "def forward_normal_dual_stream(self, hidden_states: torch.Tensor, should_allreduce_fusion: bool, use_reduce_scatter: bool)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 498,
    "qualname": "DeepseekV2MoE.forward_normal",
    "signature": "def forward_normal(self, hidden_states: torch.Tensor, should_allreduce_fusion: bool, use_reduce_scatter: bool)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 537,
    "qualname": "DeepseekV2MoE.forward_cpu",
    "signature": "def forward_cpu(self, hidden_states: torch.Tensor, should_allreduce_fusion: bool)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 595,
    "qualname": "DeepseekV2MoE.forward_deepep",
    "signature": "def forward_deepep(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 638,
    "qualname": "DeepseekV2MoE.op_gate",
    "signature": "def op_gate(self, state)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 647,
    "qualname": "DeepseekV2MoE.op_shared_experts",
    "signature": "def op_shared_experts(self, state)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 656,
    "qualname": "DeepseekV2MoE.op_select_experts",
    "signature": "def op_select_experts(self, state)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 680,
    "qualname": "DeepseekV2MoE.op_dispatch_a",
    "signature": "def op_dispatch_a(self, state)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 690,
    "qualname": "DeepseekV2MoE.op_dispatch_b",
    "signature": "def op_dispatch_b(self, state)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 699,
    "qualname": "DeepseekV2MoE.op_experts",
    "signature": "def op_experts(self, state)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 704,
    "qualname": "DeepseekV2MoE.op_combine_a",
    "signature": "def op_combine_a(self, state)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 715,
    "qualname": "DeepseekV2MoE.op_combine_b",
    "signature": "def op_combine_b(self, state)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 723,
    "qualname": "DeepseekV2MoE.op_output",
    "signature": "def op_output(self, state)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 736,
    "qualname": "yarn_get_mscale",
    "signature": "def yarn_get_mscale(scale: float, mscale: float)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 746,
    "qualname": "DeepseekV2AttentionMLA.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, hidden_size: int, num_heads: int, qk_nope_head_dim: int, qk_rope_head_dim: int, v_head_dim: int, q_lora_rank: int, kv_lora_rank: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], reduce_results: bool, layer_id: int, prefix: str, alt_stream: Optional[torch.cuda.Stream])"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 970,
    "qualname": "DeepseekV2AttentionMLA.dispatch_attn_forward_method",
    "signature": "def dispatch_attn_forward_method(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1060,
    "qualname": "DeepseekV2AttentionMLA.op_prepare",
    "signature": "def op_prepare(self, state)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1068,
    "qualname": "DeepseekV2AttentionMLA.op_core",
    "signature": "def op_core(self, state)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1073,
    "qualname": "DeepseekV2AttentionMLA.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1088,
    "qualname": "DeepseekV2AttentionMLA.forward_prepare",
    "signature": "def forward_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1130,
    "qualname": "DeepseekV2AttentionMLA.forward_core",
    "signature": "def forward_core(self, intermediate_state)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1150,
    "qualname": "DeepseekV2AttentionMLA.forward_normal_prepare",
    "signature": "def forward_normal_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1194,
    "qualname": "DeepseekV2AttentionMLA.forward_normal_core",
    "signature": "def forward_normal_core(self, q, k, v, forward_batch)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1210,
    "qualname": "DeepseekV2AttentionMLA.forward_absorb_prepare",
    "signature": "def forward_absorb_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1295,
    "qualname": "DeepseekV2AttentionMLA.forward_absorb_core",
    "signature": "def forward_absorb_core(self, q_pe, k_pe, q_nope_out, k_nope, forward_batch, zero_allocator)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1382,
    "qualname": "DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_prepare",
    "signature": "def forward_absorb_fused_mla_rope_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1496,
    "qualname": "DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_cpu_prepare",
    "signature": "def forward_absorb_fused_mla_rope_cpu_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1547,
    "qualname": "DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_core",
    "signature": "def forward_absorb_fused_mla_rope_core(self, q_input, key_cache_buf, val_cache_buf, attn_output, kv_indptr, kv_indices, k_pe_output, cos_sin_cache, positions, attn_logits, num_kv_split, sm_scale, enable_rope_fusion, k_input, forward_batch, zero_allocator)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1620,
    "qualname": "DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_cpu_core",
    "signature": "def forward_absorb_fused_mla_rope_cpu_core(self, q_input, k_input, v_input, forward_batch, zero_allocator)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1708,
    "qualname": "DeepseekV2AttentionMLA.forward_normal_chunked_kv_prepare",
    "signature": "def forward_normal_chunked_kv_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1726,
    "qualname": "DeepseekV2AttentionMLA.forward_normal_chunked_kv_core",
    "signature": "def forward_normal_chunked_kv_core(self, q, k, v, forward_batch)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1757,
    "qualname": "DeepseekV2DecoderLayer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], is_nextn: bool, prefix: str, alt_stream: Optional[torch.cuda.Stream])"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1852,
    "qualname": "DeepseekV2DecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], zero_allocator: BumpAllocator)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1900,
    "qualname": "DeepseekV2DecoderLayer.op_comm_prepare_attn",
    "signature": "def op_comm_prepare_attn(self, state, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], zero_allocator: BumpAllocator, tbo_subbatch_index: Optional[int])"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1922,
    "qualname": "DeepseekV2DecoderLayer.op_comm_prepare_mlp",
    "signature": "def op_comm_prepare_mlp(self, state)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1931,
    "qualname": "DeepseekV2DecoderLayer.op_mlp",
    "signature": "def op_mlp(self, state)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1944,
    "qualname": "DeepseekV2DecoderLayer.op_comm_postprocess_layer",
    "signature": "def op_comm_postprocess_layer(self, state)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 1974,
    "qualname": "DeepseekV2Model.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 2031,
    "qualname": "DeepseekV2Model.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 2034,
    "qualname": "DeepseekV2Model.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 2113,
    "qualname": "DeepseekV2ForCausalLM.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 2158,
    "qualname": "DeepseekV2ForCausalLM.routed_experts_weights_of_layer",
    "signature": "def routed_experts_weights_of_layer(self)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 2161,
    "qualname": "DeepseekV2ForCausalLM.determine_num_fused_shared_experts",
    "signature": "def determine_num_fused_shared_experts(self, architecture: str)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 2192,
    "qualname": "DeepseekV2ForCausalLM.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 2196,
    "qualname": "DeepseekV2ForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 2216,
    "qualname": "DeepseekV2ForCausalLM.start_layer",
    "signature": "def start_layer(self)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 2220,
    "qualname": "DeepseekV2ForCausalLM.end_layer",
    "signature": "def end_layer(self)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 2223,
    "qualname": "DeepseekV2ForCausalLM.post_load_weights",
    "signature": "def post_load_weights(self, is_nextn, weight_names)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 2454,
    "qualname": "DeepseekV2ForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 2695,
    "qualname": "DeepseekV2ForCausalLM.get_embed_and_head",
    "signature": "def get_embed_and_head(self)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 2698,
    "qualname": "DeepseekV2ForCausalLM.set_embed_and_head",
    "signature": "def set_embed_and_head(self, embed, head)"
  },
  {
    "module": "srt.models.deepseek_v2",
    "file": "python/sglang/srt/models/deepseek_v2.py",
    "line": 2707,
    "qualname": "DeepseekV2ForCausalLM.get_model_config_for_expert_location",
    "signature": "def get_model_config_for_expert_location(cls, config)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 90,
    "qualname": "GptOssConfig.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 99,
    "qualname": "get_attention_sliding_window_size",
    "signature": "def get_attention_sliding_window_size(config)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 104,
    "qualname": "GptOssSparseMoeBlock.__init__",
    "signature": "def __init__(self, layer_id: int, config: GptOssConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 160,
    "qualname": "GptOssSparseMoeBlock.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch], should_allreduce_fusion: bool)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 171,
    "qualname": "GptOssSparseMoeBlock.get_moe_weights",
    "signature": "def get_moe_weights(self)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 178,
    "qualname": "GptOssSparseMoeBlock.forward_normal",
    "signature": "def forward_normal(self, hidden_states: torch.Tensor, should_allreduce_fusion: bool)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 223,
    "qualname": "GptOssAttention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, head_dim: Optional[int], rms_norm_eps: float, attention_bias: bool, quant_config: Optional[QuantizationConfig], prefix: str, sliding_window_size: int, layer_type: str, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 323,
    "qualname": "GptOssAttention.forward_prepare",
    "signature": "def forward_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 351,
    "qualname": "GptOssAttention.forward_core",
    "signature": "def forward_core(self, intermediate_state)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 363,
    "qualname": "GptOssAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 378,
    "qualname": "GptOssDecoderLayer.__init__",
    "signature": "def __init__(self, config: GptOssConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, sliding_window_size: int | None)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 463,
    "qualname": "GptOssDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 505,
    "qualname": "GptOssModel.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str, decoder_layer_type: type[nn.Module])"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 548,
    "qualname": "GptOssModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 598,
    "qualname": "GptOssForCausalLM.__init__",
    "signature": "def __init__(self, config: GptOssConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 630,
    "qualname": "GptOssForCausalLM.routed_experts_weights_of_layer",
    "signature": "def routed_experts_weights_of_layer(self)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 634,
    "qualname": "GptOssForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 666,
    "qualname": "GptOssForCausalLM.start_layer",
    "signature": "def start_layer(self)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 670,
    "qualname": "GptOssForCausalLM.end_layer",
    "signature": "def end_layer(self)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 741,
    "qualname": "GptOssForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn: bool, weight_name_mapping: dict)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 1125,
    "qualname": "GptOssForCausalLM.get_embed_and_head",
    "signature": "def get_embed_and_head(self)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 1128,
    "qualname": "GptOssForCausalLM.set_embed_and_head",
    "signature": "def set_embed_and_head(self, embed, head)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 1136,
    "qualname": "GptOssForCausalLM.set_eagle3_layers_to_capture",
    "signature": "def set_eagle3_layers_to_capture(self, layer_ids: Optional[List[int]])"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 1151,
    "qualname": "GptOssForCausalLM.get_model_config_for_expert_location",
    "signature": "def get_model_config_for_expert_location(cls, config)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 1158,
    "qualname": "GptOssForCausalLM.get_attention_sliding_window_size",
    "signature": "def get_attention_sliding_window_size(self)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 1204,
    "qualname": "_WeightCreator.__init__",
    "signature": "def __init__(self, fn)"
  },
  {
    "module": "srt.models.gpt_oss",
    "file": "python/sglang/srt/models/gpt_oss.py",
    "line": 1208,
    "qualname": "_WeightCreator.maybe_materialize",
    "signature": "def maybe_materialize(obj)"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 61,
    "qualname": "Qwen2MLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 91,
    "qualname": "Qwen2MLP.forward",
    "signature": "def forward(self, x)"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 99,
    "qualname": "Qwen2Attention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: Optional[int], layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], dual_chunk_attention_config: Optional[dict[str, Any]], prefix: str)"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 174,
    "qualname": "Qwen2Attention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 189,
    "qualname": "Qwen2DecoderLayer.__init__",
    "signature": "def __init__(self, config: Qwen2Config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream])"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 231,
    "qualname": "Qwen2DecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 257,
    "qualname": "Qwen2Model.__init__",
    "signature": "def __init__(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str, decoder_layer_type: type[nn.Module], alt_stream: Optional[torch.cuda.Stream])"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 305,
    "qualname": "Qwen2Model.get_input_embedding",
    "signature": "def get_input_embedding(self, input_ids: torch.Tensor)"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 311,
    "qualname": "Qwen2Model.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 314,
    "qualname": "Qwen2Model.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 368,
    "qualname": "Qwen2Model.load_kv_cache_scales",
    "signature": "def load_kv_cache_scales(self, quantization_param_path: str)"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 409,
    "qualname": "Qwen2ForCausalLM.__init__",
    "signature": "def __init__(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 456,
    "qualname": "Qwen2ForCausalLM.get_input_embedding",
    "signature": "def get_input_embedding(self, input_ids: torch.Tensor)"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 459,
    "qualname": "Qwen2ForCausalLM.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 463,
    "qualname": "Qwen2ForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 491,
    "qualname": "Qwen2ForCausalLM.forward_split_prefill",
    "signature": "def forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 532,
    "qualname": "Qwen2ForCausalLM.start_layer",
    "signature": "def start_layer(self)"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 536,
    "qualname": "Qwen2ForCausalLM.end_layer",
    "signature": "def end_layer(self)"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 539,
    "qualname": "Qwen2ForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 608,
    "qualname": "Qwen2ForCausalLM.get_embed_and_head",
    "signature": "def get_embed_and_head(self)"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 611,
    "qualname": "Qwen2ForCausalLM.set_embed_and_head",
    "signature": "def set_embed_and_head(self, embed, head)"
  },
  {
    "module": "srt.models.qwen2",
    "file": "python/sglang/srt/models/qwen2.py",
    "line": 619,
    "qualname": "Qwen2ForCausalLM.load_kv_cache_scales",
    "signature": "def load_kv_cache_scales(self, quantization_param_path: str)"
  },
  {
    "module": "srt.models.qwen2_5_vl",
    "file": "python/sglang/srt/models/qwen2_5_vl.py",
    "line": 66,
    "qualname": "Qwen2_5_VLMLP.__init__",
    "signature": "def __init__(self, in_features: int, hidden_features: int, bias: bool, hidden_act, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen2_5_vl",
    "file": "python/sglang/srt/models/qwen2_5_vl.py",
    "line": 99,
    "qualname": "Qwen2_5_VLMLP.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.qwen2_5_vl",
    "file": "python/sglang/srt/models/qwen2_5_vl.py",
    "line": 110,
    "qualname": "Qwen2_5_VisionBlock.__init__",
    "signature": "def __init__(self, dim: int, intermediate_dim: int, num_heads: int, hidden_act, norm_layer: Type[nn.Module], attn_implementation: Optional[str], quant_config: Optional[QuantizationConfig], prefix: str, num_dummy_heads: int)"
  },
  {
    "module": "srt.models.qwen2_5_vl",
    "file": "python/sglang/srt/models/qwen2_5_vl.py",
    "line": 171,
    "qualname": "Qwen2_5_VisionBlock.forward",
    "signature": "def forward(self, x: torch.Tensor, cu_seqlens: torch.Tensor, position_embeddings: torch.Tensor)"
  },
  {
    "module": "srt.models.qwen2_5_vl",
    "file": "python/sglang/srt/models/qwen2_5_vl.py",
    "line": 194,
    "qualname": "Qwen2_5_VisionPatchMerger.__init__",
    "signature": "def __init__(self, dim: int, context_dim: int, spatial_merge_size: int, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen2_5_vl",
    "file": "python/sglang/srt/models/qwen2_5_vl.py",
    "line": 225,
    "qualname": "Qwen2_5_VisionPatchMerger.forward",
    "signature": "def forward(self, x: torch.Tensor)"
  },
  {
    "module": "srt.models.qwen2_5_vl",
    "file": "python/sglang/srt/models/qwen2_5_vl.py",
    "line": 238,
    "qualname": "Qwen2_5_VisionTransformer.__init__",
    "signature": "def __init__(self, vision_config: Qwen2_5_VLVisionConfig, norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen2_5_vl",
    "file": "python/sglang/srt/models/qwen2_5_vl.py",
    "line": 292,
    "qualname": "Qwen2_5_VisionTransformer.get_window_index",
    "signature": "def get_window_index(self, grid_thw)"
  },
  {
    "module": "srt.models.qwen2_5_vl",
    "file": "python/sglang/srt/models/qwen2_5_vl.py",
    "line": 338,
    "qualname": "Qwen2_5_VisionTransformer.dtype",
    "signature": "def dtype(self)"
  },
  {
    "module": "srt.models.qwen2_5_vl",
    "file": "python/sglang/srt/models/qwen2_5_vl.py",
    "line": 342,
    "qualname": "Qwen2_5_VisionTransformer.device",
    "signature": "def device(self)"
  },
  {
    "module": "srt.models.qwen2_5_vl",
    "file": "python/sglang/srt/models/qwen2_5_vl.py",
    "line": 345,
    "qualname": "Qwen2_5_VisionTransformer.rot_pos_emb",
    "signature": "def rot_pos_emb(self, grid_thw: torch.Tensor)"
  },
  {
    "module": "srt.models.qwen2_5_vl",
    "file": "python/sglang/srt/models/qwen2_5_vl.py",
    "line": 377,
    "qualname": "Qwen2_5_VisionTransformer.forward",
    "signature": "def forward(self, x: torch.Tensor, grid_thw: torch.Tensor)"
  },
  {
    "module": "srt.models.qwen2_5_vl",
    "file": "python/sglang/srt/models/qwen2_5_vl.py",
    "line": 462,
    "qualname": "Qwen2_5_VLForConditionalGeneration.__init__",
    "signature": "def __init__(self, config: Qwen2_5_VLConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen2_5_vl",
    "file": "python/sglang/srt/models/qwen2_5_vl.py",
    "line": 500,
    "qualname": "Qwen2_5_VLForConditionalGeneration.pad_input_ids",
    "signature": "def pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)"
  },
  {
    "module": "srt.models.qwen2_5_vl",
    "file": "python/sglang/srt/models/qwen2_5_vl.py",
    "line": 504,
    "qualname": "Qwen2_5_VLForConditionalGeneration.get_image_feature",
    "signature": "def get_image_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.qwen2_5_vl",
    "file": "python/sglang/srt/models/qwen2_5_vl.py",
    "line": 515,
    "qualname": "Qwen2_5_VLForConditionalGeneration.get_video_feature",
    "signature": "def get_video_feature(self, items: List[MultimodalDataItem])"
  },
  {
    "module": "srt.models.qwen2_5_vl",
    "file": "python/sglang/srt/models/qwen2_5_vl.py",
    "line": 526,
    "qualname": "Qwen2_5_VLForConditionalGeneration.get_input_embeddings",
    "signature": "def get_input_embeddings(self)"
  },
  {
    "module": "srt.models.qwen2_5_vl",
    "file": "python/sglang/srt/models/qwen2_5_vl.py",
    "line": 530,
    "qualname": "Qwen2_5_VLForConditionalGeneration.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)"
  },
  {
    "module": "srt.models.qwen2_5_vl",
    "file": "python/sglang/srt/models/qwen2_5_vl.py",
    "line": 577,
    "qualname": "Qwen2_5_VLForConditionalGeneration.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.qwen2_moe",
    "file": "python/sglang/srt/models/qwen2_moe.py",
    "line": 74,
    "qualname": "Qwen2MoeMLP.__init__",
    "signature": "def __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str)"
  },
  {
    "module": "srt.models.qwen2_moe",
    "file": "python/sglang/srt/models/qwen2_moe.py",
    "line": 105,
    "qualname": "Qwen2MoeMLP.forward",
    "signature": "def forward(self, x, use_reduce_scatter: bool)"
  },
  {
    "module": "srt.models.qwen2_moe",
    "file": "python/sglang/srt/models/qwen2_moe.py",
    "line": 117,
    "qualname": "Qwen2MoeSparseMoeBlock.__init__",
    "signature": "def __init__(self, layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen2_moe",
    "file": "python/sglang/srt/models/qwen2_moe.py",
    "line": 168,
    "qualname": "Qwen2MoeSparseMoeBlock.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch], use_reduce_scatter: bool)"
  },
  {
    "module": "srt.models.qwen2_moe",
    "file": "python/sglang/srt/models/qwen2_moe.py",
    "line": 197,
    "qualname": "Qwen2MoeAttention.__init__",
    "signature": "def __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, qkv_bias: int, quant_config: Optional[QuantizationConfig], dual_chunk_attention_config: Optional[dict[str, Any]], prefix: str)"
  },
  {
    "module": "srt.models.qwen2_moe",
    "file": "python/sglang/srt/models/qwen2_moe.py",
    "line": 278,
    "qualname": "Qwen2MoeAttention.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.models.qwen2_moe",
    "file": "python/sglang/srt/models/qwen2_moe.py",
    "line": 293,
    "qualname": "Qwen2MoeDecoderLayer.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream])"
  },
  {
    "module": "srt.models.qwen2_moe",
    "file": "python/sglang/srt/models/qwen2_moe.py",
    "line": 367,
    "qualname": "Qwen2MoeDecoderLayer.forward",
    "signature": "def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])"
  },
  {
    "module": "srt.models.qwen2_moe",
    "file": "python/sglang/srt/models/qwen2_moe.py",
    "line": 405,
    "qualname": "Qwen2MoeModel.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str, decoder_layer_type: type[nn.Module], alt_stream: Optional[torch.cuda.Stream])"
  },
  {
    "module": "srt.models.qwen2_moe",
    "file": "python/sglang/srt/models/qwen2_moe.py",
    "line": 452,
    "qualname": "Qwen2MoeModel.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.qwen2_moe",
    "file": "python/sglang/srt/models/qwen2_moe.py",
    "line": 518,
    "qualname": "Qwen2MoeForCausalLM.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.qwen2_moe",
    "file": "python/sglang/srt/models/qwen2_moe.py",
    "line": 541,
    "qualname": "Qwen2MoeForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])"
  },
  {
    "module": "srt.models.qwen2_moe",
    "file": "python/sglang/srt/models/qwen2_moe.py",
    "line": 564,
    "qualname": "Qwen2MoeForCausalLM.forward_split_prefill",
    "signature": "def forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)"
  },
  {
    "module": "srt.models.qwen2_moe",
    "file": "python/sglang/srt/models/qwen2_moe.py",
    "line": 607,
    "qualname": "Qwen2MoeForCausalLM.start_layer",
    "signature": "def start_layer(self)"
  },
  {
    "module": "srt.models.qwen2_moe",
    "file": "python/sglang/srt/models/qwen2_moe.py",
    "line": 611,
    "qualname": "Qwen2MoeForCausalLM.end_layer",
    "signature": "def end_layer(self)"
  },
  {
    "module": "srt.models.qwen2_moe",
    "file": "python/sglang/srt/models/qwen2_moe.py",
    "line": 614,
    "qualname": "Qwen2MoeForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.models.qwen2_moe",
    "file": "python/sglang/srt/models/qwen2_moe.py",
    "line": 701,
    "qualname": "Qwen2MoeForCausalLM.get_model_config_for_expert_location",
    "signature": "def get_model_config_for_expert_location(cls, config)"
  },
  {
    "module": "srt.models.transformers",
    "file": "python/sglang/srt/models/transformers.py",
    "line": 46,
    "qualname": "maybe_prefix",
    "signature": "def maybe_prefix(prefix: str, name: str)"
  },
  {
    "module": "srt.models.transformers",
    "file": "python/sglang/srt/models/transformers.py",
    "line": 59,
    "qualname": "sglang_flash_attention_forward",
    "signature": "def sglang_flash_attention_forward(module: torch.nn.Module, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attention_mask: torch.Tensor, forward_batch: ForwardBatch, scaling: float, attention_instances: list[RadixAttention])"
  },
  {
    "module": "srt.models.transformers",
    "file": "python/sglang/srt/models/transformers.py",
    "line": 87,
    "qualname": "HFColumnParallelLinear.forward",
    "signature": "def forward(self, input: torch.Tensor)"
  },
  {
    "module": "srt.models.transformers",
    "file": "python/sglang/srt/models/transformers.py",
    "line": 93,
    "qualname": "HFRowParallelLinear.forward",
    "signature": "def forward(self, input: torch.Tensor)"
  },
  {
    "module": "srt.models.transformers",
    "file": "python/sglang/srt/models/transformers.py",
    "line": 97,
    "qualname": "replace_linear_class",
    "signature": "def replace_linear_class(linear: nn.Linear, style: Literal['colwise', 'rowwise'], quant_config: QuantizationConfig)"
  },
  {
    "module": "srt.models.transformers",
    "file": "python/sglang/srt/models/transformers.py",
    "line": 143,
    "qualname": "TransformersForCausalLM.__init__",
    "signature": "def __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)"
  },
  {
    "module": "srt.models.transformers",
    "file": "python/sglang/srt/models/transformers.py",
    "line": 206,
    "qualname": "TransformersForCausalLM.log_replacement",
    "signature": "def log_replacement(self, name: str, old_module: nn.Module, new_module: nn.Module)"
  },
  {
    "module": "srt.models.transformers",
    "file": "python/sglang/srt/models/transformers.py",
    "line": 209,
    "qualname": "TransformersForCausalLM.tensor_parallel",
    "signature": "def tensor_parallel(self, tp_size: int)"
  },
  {
    "module": "srt.models.transformers",
    "file": "python/sglang/srt/models/transformers.py",
    "line": 238,
    "qualname": "TransformersForCausalLM.replace_vocab_embed_class",
    "signature": "def replace_vocab_embed_class(self, module: nn.Module)"
  },
  {
    "module": "srt.models.transformers",
    "file": "python/sglang/srt/models/transformers.py",
    "line": 252,
    "qualname": "TransformersForCausalLM.forward",
    "signature": "def forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)"
  },
  {
    "module": "srt.models.transformers",
    "file": "python/sglang/srt/models/transformers.py",
    "line": 277,
    "qualname": "TransformersForCausalLM.load_weights",
    "signature": "def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])"
  },
  {
    "module": "srt.multimodal.mm_utils",
    "file": "python/sglang/srt/multimodal/mm_utils.py",
    "line": 42,
    "qualname": "has_valid_data",
    "signature": "def has_valid_data(data)"
  },
  {
    "module": "srt.multimodal.mm_utils",
    "file": "python/sglang/srt/multimodal/mm_utils.py",
    "line": 50,
    "qualname": "select_best_resolution",
    "signature": "def select_best_resolution(original_size, possible_resolutions)"
  },
  {
    "module": "srt.multimodal.mm_utils",
    "file": "python/sglang/srt/multimodal/mm_utils.py",
    "line": 90,
    "qualname": "resize_and_pad_image",
    "signature": "def resize_and_pad_image(image, target_resolution)"
  },
  {
    "module": "srt.multimodal.mm_utils",
    "file": "python/sglang/srt/multimodal/mm_utils.py",
    "line": 125,
    "qualname": "divide_to_patches",
    "signature": "def divide_to_patches(image, patch_size)"
  },
  {
    "module": "srt.multimodal.mm_utils",
    "file": "python/sglang/srt/multimodal/mm_utils.py",
    "line": 147,
    "qualname": "get_anyres_image_grid_shape",
    "signature": "def get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size)"
  },
  {
    "module": "srt.multimodal.mm_utils",
    "file": "python/sglang/srt/multimodal/mm_utils.py",
    "line": 187,
    "qualname": "process_anyres_image",
    "signature": "def process_anyres_image(image, processor, grid_pinpoints)"
  },
  {
    "module": "srt.multimodal.mm_utils",
    "file": "python/sglang/srt/multimodal/mm_utils.py",
    "line": 254,
    "qualname": "load_image_from_base64",
    "signature": "def load_image_from_base64(image)"
  },
  {
    "module": "srt.multimodal.mm_utils",
    "file": "python/sglang/srt/multimodal/mm_utils.py",
    "line": 258,
    "qualname": "expand2square",
    "signature": "def expand2square(pil_img, background_color)"
  },
  {
    "module": "srt.multimodal.mm_utils",
    "file": "python/sglang/srt/multimodal/mm_utils.py",
    "line": 274,
    "qualname": "unpad_image",
    "signature": "def unpad_image(tensor, original_size)"
  },
  {
    "module": "srt.multimodal.mm_utils",
    "file": "python/sglang/srt/multimodal/mm_utils.py",
    "line": 305,
    "qualname": "unpad_image_shape",
    "signature": "def unpad_image_shape(current_height, current_width, original_size)"
  },
  {
    "module": "srt.multimodal.mm_utils",
    "file": "python/sglang/srt/multimodal/mm_utils.py",
    "line": 329,
    "qualname": "process_images",
    "signature": "def process_images(images, image_processor, model_cfg)"
  },
  {
    "module": "srt.sampling.custom_logit_processor",
    "file": "python/sglang/srt/sampling/custom_logit_processor.py",
    "line": 23,
    "qualname": "CustomLogitProcessor.__call__",
    "signature": "def __call__(self, logits: torch.Tensor, custom_param_list: Optional[List[Dict[str, Any]]])"
  },
  {
    "module": "srt.sampling.custom_logit_processor",
    "file": "python/sglang/srt/sampling/custom_logit_processor.py",
    "line": 32,
    "qualname": "CustomLogitProcessor.to_str",
    "signature": "def to_str(cls)"
  },
  {
    "module": "srt.sampling.custom_logit_processor",
    "file": "python/sglang/srt/sampling/custom_logit_processor.py",
    "line": 37,
    "qualname": "CustomLogitProcessor.from_str",
    "signature": "def from_str(cls, json_str: str)"
  },
  {
    "module": "srt.sampling.custom_logit_processor",
    "file": "python/sglang/srt/sampling/custom_logit_processor.py",
    "line": 43,
    "qualname": "DisallowedTokensLogitsProcessor.__call__",
    "signature": "def __call__(self, logits: torch.Tensor, custom_param_list: Optional[List[Dict[str, Any]]])"
  },
  {
    "module": "srt.sampling.sampling_batch_info",
    "file": "python/sglang/srt/sampling/sampling_batch_info.py",
    "line": 70,
    "qualname": "SamplingBatchInfo.from_schedule_batch",
    "signature": "def from_schedule_batch(cls, batch: ScheduleBatch, vocab_size: int)"
  },
  {
    "module": "srt.sampling.sampling_batch_info",
    "file": "python/sglang/srt/sampling/sampling_batch_info.py",
    "line": 171,
    "qualname": "SamplingBatchInfo.__len__",
    "signature": "def __len__(self)"
  },
  {
    "module": "srt.sampling.sampling_batch_info",
    "file": "python/sglang/srt/sampling/sampling_batch_info.py",
    "line": 174,
    "qualname": "SamplingBatchInfo.update_regex_vocab_mask",
    "signature": "def update_regex_vocab_mask(self)"
  },
  {
    "module": "srt.sampling.sampling_batch_info",
    "file": "python/sglang/srt/sampling/sampling_batch_info.py",
    "line": 201,
    "qualname": "SamplingBatchInfo.update_penalties",
    "signature": "def update_penalties(self)"
  },
  {
    "module": "srt.sampling.sampling_batch_info",
    "file": "python/sglang/srt/sampling/sampling_batch_info.py",
    "line": 212,
    "qualname": "SamplingBatchInfo.apply_logits_bias",
    "signature": "def apply_logits_bias(self, logits: torch.Tensor)"
  },
  {
    "module": "srt.sampling.sampling_batch_info",
    "file": "python/sglang/srt/sampling/sampling_batch_info.py",
    "line": 227,
    "qualname": "SamplingBatchInfo.filter_batch",
    "signature": "def filter_batch(self, keep_indices: List[int], keep_indices_device: torch.Tensor)"
  },
  {
    "module": "srt.sampling.sampling_batch_info",
    "file": "python/sglang/srt/sampling/sampling_batch_info.py",
    "line": 266,
    "qualname": "SamplingBatchInfo.merge_custom_logit_processor",
    "signature": "def merge_custom_logit_processor(lhs: Optional[Dict[int, Tuple[CustomLogitProcessor, torch.Tensor]]], rhs: Optional[Dict[int, Tuple[CustomLogitProcessor, torch.Tensor]]], bs1: int, bs2: int, device: str)"
  },
  {
    "module": "srt.sampling.sampling_batch_info",
    "file": "python/sglang/srt/sampling/sampling_batch_info.py",
    "line": 305,
    "qualname": "SamplingBatchInfo.merge_batch",
    "signature": "def merge_batch(self, other: 'SamplingBatchInfo')"
  },
  {
    "module": "srt.sampling.sampling_batch_info",
    "file": "python/sglang/srt/sampling/sampling_batch_info.py",
    "line": 353,
    "qualname": "merge_bias_tensor",
    "signature": "def merge_bias_tensor(lhs: Optional[torch.Tensor], rhs: Optional[torch.Tensor], bs1: int, bs2: int, device: str, default: float)"
  },
  {
    "module": "srt.sampling.sampling_params",
    "file": "python/sglang/srt/sampling/sampling_params.py",
    "line": 31,
    "qualname": "SamplingParams.__init__",
    "signature": "def __init__(self, max_new_tokens: int, stop: Optional[Union[str, List[str]]], stop_token_ids: Optional[List[int]], temperature: float, top_p: float, top_k: int, min_p: float, frequency_penalty: float, presence_penalty: float, repetition_penalty: float, min_new_tokens: int, n: int, json_schema: Optional[str], regex: Optional[str], ebnf: Optional[str], structural_tag: Optional[str], ignore_eos: bool, skip_special_tokens: bool, spaces_between_special_tokens: bool, no_stop_trim: bool, custom_params: Optional[Dict[str, Any]], stream_interval: Optional[int], logit_bias: Optional[Dict[str, float]])"
  },
  {
    "module": "srt.sampling.sampling_params",
    "file": "python/sglang/srt/sampling/sampling_params.py",
    "line": 92,
    "qualname": "SamplingParams.verify",
    "signature": "def verify(self, vocab_size)"
  },
  {
    "module": "srt.sampling.sampling_params",
    "file": "python/sglang/srt/sampling/sampling_params.py",
    "line": 149,
    "qualname": "SamplingParams.normalize",
    "signature": "def normalize(self, tokenizer)"
  },
  {
    "module": "srt.speculative.build_eagle_tree",
    "file": "python/sglang/srt/speculative/build_eagle_tree.py",
    "line": 17,
    "qualname": "build_tree_kernel_efficient_preprocess",
    "signature": "def build_tree_kernel_efficient_preprocess(verified_id: torch.Tensor, score_list: List[torch.Tensor], token_list: List[torch.Tensor], parents_list: List[torch.Tensor], num_verify_tokens: int)"
  },
  {
    "module": "srt.speculative.build_eagle_tree",
    "file": "python/sglang/srt/speculative/build_eagle_tree.py",
    "line": 51,
    "qualname": "build_tree_kernel_efficient",
    "signature": "def build_tree_kernel_efficient(verified_id: torch.Tensor, score_list: List[torch.Tensor], token_list: List[torch.Tensor], parents_list: List[torch.Tensor], seq_lens: torch.Tensor, seq_lens_sum: int, topk: int, spec_steps: int, num_verify_tokens: int, tree_mask_mode: TreeMaskMode, tree_mask_buf: Optional[torch.Tensor], position_buf: Optional[torch.Tensor])"
  },
  {
    "module": "srt.speculative.build_eagle_tree",
    "file": "python/sglang/srt/speculative/build_eagle_tree.py",
    "line": 154,
    "qualname": "test_build_tree_kernel_efficient",
    "signature": "def test_build_tree_kernel_efficient()"
  },
  {
    "module": "srt.speculative.eagle_draft_cuda_graph_runner",
    "file": "python/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py",
    "line": 40,
    "qualname": "EAGLEDraftCudaGraphRunner.__init__",
    "signature": "def __init__(self, eagle_worker: EAGLEWorker)"
  },
  {
    "module": "srt.speculative.eagle_draft_cuda_graph_runner",
    "file": "python/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py",
    "line": 128,
    "qualname": "EAGLEDraftCudaGraphRunner.can_run",
    "signature": "def can_run(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.speculative.eagle_draft_cuda_graph_runner",
    "file": "python/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py",
    "line": 149,
    "qualname": "EAGLEDraftCudaGraphRunner.capture",
    "signature": "def capture(self)"
  },
  {
    "module": "srt.speculative.eagle_draft_cuda_graph_runner",
    "file": "python/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py",
    "line": 152,
    "qualname": "EAGLEDraftCudaGraphRunner.capture_one_batch_size",
    "signature": "def capture_one_batch_size(self, num_seqs: int, forward: Callable)"
  },
  {
    "module": "srt.speculative.eagle_draft_cuda_graph_runner",
    "file": "python/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py",
    "line": 280,
    "qualname": "EAGLEDraftCudaGraphRunner.replay",
    "signature": "def replay(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.speculative.eagle_draft_extend_cuda_graph_runner",
    "file": "python/sglang/srt/speculative/eagle_draft_extend_cuda_graph_runner.py",
    "line": 37,
    "qualname": "EAGLEDraftExtendCudaGraphRunner.__init__",
    "signature": "def __init__(self, eagle_worker: EAGLEWorker)"
  },
  {
    "module": "srt.speculative.eagle_draft_extend_cuda_graph_runner",
    "file": "python/sglang/srt/speculative/eagle_draft_extend_cuda_graph_runner.py",
    "line": 155,
    "qualname": "EAGLEDraftExtendCudaGraphRunner.can_run",
    "signature": "def can_run(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.speculative.eagle_draft_extend_cuda_graph_runner",
    "file": "python/sglang/srt/speculative/eagle_draft_extend_cuda_graph_runner.py",
    "line": 176,
    "qualname": "EAGLEDraftExtendCudaGraphRunner.capture",
    "signature": "def capture(self)"
  },
  {
    "module": "srt.speculative.eagle_draft_extend_cuda_graph_runner",
    "file": "python/sglang/srt/speculative/eagle_draft_extend_cuda_graph_runner.py",
    "line": 179,
    "qualname": "EAGLEDraftExtendCudaGraphRunner.capture_one_batch_size",
    "signature": "def capture_one_batch_size(self, bs: int, forward: Callable)"
  },
  {
    "module": "srt.speculative.eagle_draft_extend_cuda_graph_runner",
    "file": "python/sglang/srt/speculative/eagle_draft_extend_cuda_graph_runner.py",
    "line": 308,
    "qualname": "EAGLEDraftExtendCudaGraphRunner.replay",
    "signature": "def replay(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 85,
    "qualname": "EagleDraftInput.prepare_for_extend",
    "signature": "def prepare_for_extend(self, batch: ScheduleBatch)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 102,
    "qualname": "EagleDraftInput.create_idle_input",
    "signature": "def create_idle_input(cls, device: torch.device, hidden_size: int, dtype: torch.dtype, topk: int, capture_hidden_mode: CaptureHiddenMode)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 120,
    "qualname": "EagleDraftInput.prepare_extend_after_decode",
    "signature": "def prepare_extend_after_decode(self, batch: ScheduleBatch, speculative_num_steps: int)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 151,
    "qualname": "EagleDraftInput.generate_attn_arg_prefill",
    "signature": "def generate_attn_arg_prefill(self, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, req_to_token: torch.Tensor)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 182,
    "qualname": "EagleDraftInput.filter_batch",
    "signature": "def filter_batch(self, new_indices: torch.Tensor, has_been_filtered: bool)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 201,
    "qualname": "EagleDraftInput.merge_batch",
    "signature": "def merge_batch(self, spec_info: EagleDraftInput)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 250,
    "qualname": "EagleVerifyInput.create_idle_input",
    "signature": "def create_idle_input(cls, topk: int, spec_steps: int, num_verify_tokens: int)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 273,
    "qualname": "EagleVerifyInput.prepare_for_verify",
    "signature": "def prepare_for_verify(self, batch: ScheduleBatch, page_size: int)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 307,
    "qualname": "EagleVerifyInput.generate_attn_arg_prefill",
    "signature": "def generate_attn_arg_prefill(self, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, req_to_token: torch.Tensor)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 345,
    "qualname": "EagleVerifyInput.verify",
    "signature": "def verify(self, batch: ScheduleBatch, logits_output: LogitsProcessorOutput, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, page_size: int, vocab_mask: Optional[torch.Tensor])"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 730,
    "qualname": "create_extend_after_decode_spec_info",
    "signature": "def create_extend_after_decode_spec_info(verified_id, seq_lens, accept_lens, positions, new_verified_id, bs_upper: tl.constexpr)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 756,
    "qualname": "assign_req_to_token_pool",
    "signature": "def assign_req_to_token_pool(req_pool_indices, req_to_token, start_offset, end_offset, out_cache_loc, pool_len: tl.constexpr, bs_upper: tl.constexpr)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 791,
    "qualname": "assign_draft_cache_locs",
    "signature": "def assign_draft_cache_locs(req_pool_indices, req_to_token, seq_lens, extend_lens, num_new_pages_per_topk, out_cache_loc, pool_len: tl.constexpr, topk: tl.constexpr, speculative_num_steps: tl.constexpr, page_size: tl.constexpr, bs_upper: tl.constexpr, iter_upper: tl.constexpr)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 867,
    "qualname": "generate_draft_decode_kv_indices",
    "signature": "def generate_draft_decode_kv_indices(req_pool_indices, req_to_token, paged_kernel_lens, kv_indices, kv_indptr, positions, pool_len: tl.constexpr, kv_indices_stride: tl.constexpr, kv_indptr_stride: tl.constexpr, bs_upper: tl.constexpr, iter_upper: tl.constexpr, num_tokens_upper: tl.constexpr, page_size: tl.constexpr)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 948,
    "qualname": "align_evict_mask_to_page_size",
    "signature": "def align_evict_mask_to_page_size(seq_lens, evict_mask, page_size: tl.constexpr, num_draft_tokens: tl.constexpr, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 973,
    "qualname": "get_target_cache_loc",
    "signature": "def get_target_cache_loc(tgt_cache_loc, to_free_slots, accept_length, to_free_num_slots, out_cache_loc, num_verify_tokens: tl.constexpr, num_verify_tokens_upper: tl.constexpr, bs_upper: tl.constexpr)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 1019,
    "qualname": "get_src_tgt_cache_loc",
    "signature": "def get_src_tgt_cache_loc(seq_lens: torch.Tensor, out_cache_loc: torch.Tensor, accept_index: torch.Tensor, accept_length: torch.Tensor, draft_token_num: int, page_size: int)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 1039,
    "qualname": "filter_finished_cache_loc_kernel",
    "signature": "def filter_finished_cache_loc_kernel(out_cache_loc, tgt_cache_loc, accept_length, accept_length_filter, bs_upper: tl.constexpr, num_verify_tokens_upper: tl.constexpr)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 1069,
    "qualname": "create_accept_length_filter",
    "signature": "def create_accept_length_filter(accept_length: torch.Tensor, unfinished_index_device: torch.Tensor, seq_lens: torch.Tensor)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 1083,
    "qualname": "select_top_k_tokens",
    "signature": "def select_top_k_tokens(i: int, topk_p: torch.Tensor, topk_index: torch.Tensor, hidden_states: torch.Tensor, scores: torch.Tensor, topk: int)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 1183,
    "qualname": "traverse_tree",
    "signature": "def traverse_tree(retrieve_next_token: torch.Tensor, retrieve_next_sibling: torch.Tensor, draft_tokens: torch.Tensor, grammar: BaseGrammarObject, allocate_token_bitmask: torch.Tensor)"
  },
  {
    "module": "srt.speculative.eagle_utils",
    "file": "python/sglang/srt/speculative/eagle_utils.py",
    "line": 1249,
    "qualname": "generate_token_bitmask",
    "signature": "def generate_token_bitmask(reqs: List[Req], verify_input: EagleVerifyInput, retrieve_next_token_cpu: torch.Tensor, retrieve_next_sibling_cpu: torch.Tensor, draft_tokens_cpu: torch.Tensor, vocab_size: int)"
  },
  {
    "module": "srt.speculative.spec_info",
    "file": "python/sglang/srt/speculative/spec_info.py",
    "line": 9,
    "qualname": "SpeculativeAlgorithm.is_none",
    "signature": "def is_none(self)"
  },
  {
    "module": "srt.speculative.spec_info",
    "file": "python/sglang/srt/speculative/spec_info.py",
    "line": 12,
    "qualname": "SpeculativeAlgorithm.is_eagle",
    "signature": "def is_eagle(self)"
  },
  {
    "module": "srt.speculative.spec_info",
    "file": "python/sglang/srt/speculative/spec_info.py",
    "line": 15,
    "qualname": "SpeculativeAlgorithm.is_eagle3",
    "signature": "def is_eagle3(self)"
  },
  {
    "module": "srt.speculative.spec_info",
    "file": "python/sglang/srt/speculative/spec_info.py",
    "line": 19,
    "qualname": "SpeculativeAlgorithm.from_string",
    "signature": "def from_string(name: str)"
  },
  {
    "module": "srt.speculative.eagle_worker",
    "file": "python/sglang/srt/speculative/eagle_worker.py",
    "line": 60,
    "qualname": "draft_tp_context",
    "signature": "def draft_tp_context(tp_group: GroupCoordinator)"
  },
  {
    "module": "srt.speculative.eagle_worker",
    "file": "python/sglang/srt/speculative/eagle_worker.py",
    "line": 69,
    "qualname": "EAGLEWorker.__init__",
    "signature": "def __init__(self, server_args: ServerArgs, gpu_id: int, tp_rank: int, dp_rank: Optional[int], moe_ep_rank: int, nccl_port: int, target_worker: TpModelWorker)"
  },
  {
    "module": "srt.speculative.eagle_worker",
    "file": "python/sglang/srt/speculative/eagle_worker.py",
    "line": 182,
    "qualname": "EAGLEWorker.init_attention_backend",
    "signature": "def init_attention_backend(self)"
  },
  {
    "module": "srt.speculative.eagle_worker",
    "file": "python/sglang/srt/speculative/eagle_worker.py",
    "line": 320,
    "qualname": "EAGLEWorker.init_cuda_graphs",
    "signature": "def init_cuda_graphs(self)"
  },
  {
    "module": "srt.speculative.eagle_worker",
    "file": "python/sglang/srt/speculative/eagle_worker.py",
    "line": 356,
    "qualname": "EAGLEWorker.draft_model_runner",
    "signature": "def draft_model_runner(self)"
  },
  {
    "module": "srt.speculative.eagle_worker",
    "file": "python/sglang/srt/speculative/eagle_worker.py",
    "line": 359,
    "qualname": "EAGLEWorker.forward_batch_speculative_generation",
    "signature": "def forward_batch_speculative_generation(self, batch: ScheduleBatch)"
  },
  {
    "module": "srt.speculative.eagle_worker",
    "file": "python/sglang/srt/speculative/eagle_worker.py",
    "line": 407,
    "qualname": "EAGLEWorker.check_forward_draft_extend_after_decode",
    "signature": "def check_forward_draft_extend_after_decode(self, batch: ScheduleBatch)"
  },
  {
    "module": "srt.speculative.eagle_worker",
    "file": "python/sglang/srt/speculative/eagle_worker.py",
    "line": 425,
    "qualname": "EAGLEWorker.forward_target_extend",
    "signature": "def forward_target_extend(self, batch: ScheduleBatch)"
  },
  {
    "module": "srt.speculative.eagle_worker",
    "file": "python/sglang/srt/speculative/eagle_worker.py",
    "line": 565,
    "qualname": "EAGLEWorker.draft",
    "signature": "def draft(self, batch: ScheduleBatch)"
  },
  {
    "module": "srt.speculative.eagle_worker",
    "file": "python/sglang/srt/speculative/eagle_worker.py",
    "line": 643,
    "qualname": "EAGLEWorker.draft_forward",
    "signature": "def draft_forward(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.speculative.eagle_worker",
    "file": "python/sglang/srt/speculative/eagle_worker.py",
    "line": 702,
    "qualname": "EAGLEWorker.verify",
    "signature": "def verify(self, batch: ScheduleBatch, spec_info: EagleVerifyInput)"
  },
  {
    "module": "srt.speculative.eagle_worker",
    "file": "python/sglang/srt/speculative/eagle_worker.py",
    "line": 779,
    "qualname": "EAGLEWorker.add_logprob_values",
    "signature": "def add_logprob_values(self, batch: ScheduleBatch, res: EagleVerifyOutput, logits_output: LogitsProcessorOutput)"
  },
  {
    "module": "srt.speculative.eagle_worker",
    "file": "python/sglang/srt/speculative/eagle_worker.py",
    "line": 847,
    "qualname": "EAGLEWorker.forward_draft_extend",
    "signature": "def forward_draft_extend(self, batch: ScheduleBatch, hidden_states: torch.Tensor, next_token_ids: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor])"
  },
  {
    "module": "srt.speculative.eagle_worker",
    "file": "python/sglang/srt/speculative/eagle_worker.py",
    "line": 898,
    "qualname": "EAGLEWorker.forward_draft_extend_after_decode",
    "signature": "def forward_draft_extend_after_decode(self, batch: ScheduleBatch)"
  },
  {
    "module": "srt.speculative.eagle_worker",
    "file": "python/sglang/srt/speculative/eagle_worker.py",
    "line": 984,
    "qualname": "EAGLEWorker.capture_for_decode",
    "signature": "def capture_for_decode(self, logits_output: LogitsProcessorOutput, draft_input: EagleDraftInput)"
  },
  {
    "module": "srt.speculative.eagle_worker",
    "file": "python/sglang/srt/speculative/eagle_worker.py",
    "line": 999,
    "qualname": "load_token_map",
    "signature": "def load_token_map(token_map_path: str)"
  },
  {
    "module": "srt.speculative.eagle_worker",
    "file": "python/sglang/srt/speculative/eagle_worker.py",
    "line": 1011,
    "qualname": "get_last_loc_large_page_size_top_k_1",
    "signature": "def get_last_loc_large_page_size_top_k_1(req_to_token: torch.Tensor, req_pool_indices: torch.Tensor, seq_lens, speculative_num_steps: int)"
  },
  {
    "module": "srt.speculative.eagle_worker",
    "file": "python/sglang/srt/speculative/eagle_worker.py",
    "line": 1030,
    "qualname": "get_last_loc_large_page_size_large_top_k",
    "signature": "def get_last_loc_large_page_size_large_top_k(req_to_token: torch.Tensor, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, speculative_num_steps: int, topk: int, page_size: int)"
  },
  {
    "module": "srt.tokenizer.tiktoken_tokenizer",
    "file": "python/sglang/srt/tokenizer/tiktoken_tokenizer.py",
    "line": 7,
    "qualname": "TiktokenProcessor.__init__",
    "signature": "def __init__(self, name: str)"
  },
  {
    "module": "srt.tokenizer.tiktoken_tokenizer",
    "file": "python/sglang/srt/tokenizer/tiktoken_tokenizer.py",
    "line": 10,
    "qualname": "TiktokenProcessor.image_processor",
    "signature": "def image_processor(self, image)"
  },
  {
    "module": "srt.tokenizer.tiktoken_tokenizer",
    "file": "python/sglang/srt/tokenizer/tiktoken_tokenizer.py",
    "line": 30,
    "qualname": "TiktokenTokenizer.__init__",
    "signature": "def __init__(self, tokenizer_path)"
  },
  {
    "module": "srt.tokenizer.tiktoken_tokenizer",
    "file": "python/sglang/srt/tokenizer/tiktoken_tokenizer.py",
    "line": 110,
    "qualname": "TiktokenTokenizer.encode",
    "signature": "def encode(self, x, add_special_tokens)"
  },
  {
    "module": "srt.tokenizer.tiktoken_tokenizer",
    "file": "python/sglang/srt/tokenizer/tiktoken_tokenizer.py",
    "line": 113,
    "qualname": "TiktokenTokenizer.decode",
    "signature": "def decode(self, x)"
  },
  {
    "module": "srt.tokenizer.tiktoken_tokenizer",
    "file": "python/sglang/srt/tokenizer/tiktoken_tokenizer.py",
    "line": 116,
    "qualname": "TiktokenTokenizer.batch_decode",
    "signature": "def batch_decode(self, batch, skip_special_tokens, spaces_between_special_tokens)"
  },
  {
    "module": "srt.tokenizer.tiktoken_tokenizer",
    "file": "python/sglang/srt/tokenizer/tiktoken_tokenizer.py",
    "line": 123,
    "qualname": "TiktokenTokenizer.apply_chat_template",
    "signature": "def apply_chat_template(self, messages, tokenize, add_generation_prompt, tools, reasoning_effort)"
  },
  {
    "module": "srt.tokenizer.tiktoken_tokenizer",
    "file": "python/sglang/srt/tokenizer/tiktoken_tokenizer.py",
    "line": 136,
    "qualname": "TiktokenTokenizer.__call__",
    "signature": "def __call__(self, text)"
  },
  {
    "module": "srt.tokenizer.tiktoken_tokenizer",
    "file": "python/sglang/srt/tokenizer/tiktoken_tokenizer.py",
    "line": 141,
    "qualname": "TiktokenTokenizer.init_xgrammar",
    "signature": "def init_xgrammar(self)"
  },
  {
    "module": "srt.weight_sync.tensor_bucket",
    "file": "python/sglang/srt/weight_sync/tensor_bucket.py",
    "line": 25,
    "qualname": "FlattenedTensorBucket.__init__",
    "signature": "def __init__(self, named_tensors: List[Tuple[str, torch.Tensor]], flattened_tensor: torch.Tensor, metadata: List[FlattenedTensorMetadata])"
  },
  {
    "module": "srt.weight_sync.tensor_bucket",
    "file": "python/sglang/srt/weight_sync/tensor_bucket.py",
    "line": 79,
    "qualname": "FlattenedTensorBucket.get_flattened_tensor",
    "signature": "def get_flattened_tensor(self)"
  },
  {
    "module": "srt.weight_sync.tensor_bucket",
    "file": "python/sglang/srt/weight_sync/tensor_bucket.py",
    "line": 83,
    "qualname": "FlattenedTensorBucket.get_metadata",
    "signature": "def get_metadata(self)"
  },
  {
    "module": "srt.weight_sync.tensor_bucket",
    "file": "python/sglang/srt/weight_sync/tensor_bucket.py",
    "line": 87,
    "qualname": "FlattenedTensorBucket.reconstruct_tensors",
    "signature": "def reconstruct_tensors(self)"
  },
  {
    "module": "srt.weight_sync.utils",
    "file": "python/sglang/srt/weight_sync/utils.py",
    "line": 14,
    "qualname": "update_weights",
    "signature": "async def update_weights(engine: Engine, params_batch: list[tuple[str, torch.Tensor]], device_mesh_key: str, device_mesh: DeviceMesh, load_format: Optional[str])"
  },
  {
    "module": "srt.connector.serde.serde",
    "file": "python/sglang/srt/connector/serde/serde.py",
    "line": 12,
    "qualname": "Serializer.to_bytes",
    "signature": "def to_bytes(self, t: torch.Tensor)"
  },
  {
    "module": "srt.connector.serde.serde",
    "file": "python/sglang/srt/connector/serde/serde.py",
    "line": 29,
    "qualname": "Deserializer.__init__",
    "signature": "def __init__(self, dtype)"
  },
  {
    "module": "srt.connector.serde.serde",
    "file": "python/sglang/srt/connector/serde/serde.py",
    "line": 33,
    "qualname": "Deserializer.from_bytes",
    "signature": "def from_bytes(self, bs: bytes)"
  },
  {
    "module": "srt.connector.serde.__init__",
    "file": "python/sglang/srt/connector/serde/__init__.py",
    "line": 12,
    "qualname": "create_serde",
    "signature": "def create_serde(serde_type: str)"
  },
  {
    "module": "srt.connector.serde.safe_serde",
    "file": "python/sglang/srt/connector/serde/safe_serde.py",
    "line": 13,
    "qualname": "SafeSerializer.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.connector.serde.safe_serde",
    "file": "python/sglang/srt/connector/serde/safe_serde.py",
    "line": 16,
    "qualname": "SafeSerializer.to_bytes",
    "signature": "def to_bytes(self, t: torch.Tensor)"
  },
  {
    "module": "srt.connector.serde.safe_serde",
    "file": "python/sglang/srt/connector/serde/safe_serde.py",
    "line": 22,
    "qualname": "SafeDeserializer.__init__",
    "signature": "def __init__(self, dtype)"
  },
  {
    "module": "srt.connector.serde.safe_serde",
    "file": "python/sglang/srt/connector/serde/safe_serde.py",
    "line": 25,
    "qualname": "SafeDeserializer.from_bytes_normal",
    "signature": "def from_bytes_normal(self, b: Union[bytearray, bytes])"
  },
  {
    "module": "srt.connector.serde.safe_serde",
    "file": "python/sglang/srt/connector/serde/safe_serde.py",
    "line": 28,
    "qualname": "SafeDeserializer.from_bytes",
    "signature": "def from_bytes(self, b: Union[bytearray, bytes])"
  },
  {
    "module": "srt.constrained.triton_ops.bitmask_ops",
    "file": "python/sglang/srt/constrained/triton_ops/bitmask_ops.py",
    "line": 14,
    "qualname": "apply_token_bitmask_inplace_kernel",
    "signature": "def apply_token_bitmask_inplace_kernel(logits_ptr, bitmask_ptr, indices_ptr, num_rows, vocab_size, logits_strides, bitmask_strides, NUM_SMS: tl.constexpr, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.constrained.triton_ops.bitmask_ops",
    "file": "python/sglang/srt/constrained/triton_ops/bitmask_ops.py",
    "line": 84,
    "qualname": "apply_token_bitmask_inplace_triton",
    "signature": "def apply_token_bitmask_inplace_triton(logits: torch.Tensor, bitmask: torch.Tensor, indices: Optional[Union[List[int], torch.Tensor]])"
  },
  {
    "module": "srt.disaggregation.ascend.transfer_engine",
    "file": "python/sglang/srt/disaggregation/ascend/transfer_engine.py",
    "line": 13,
    "qualname": "AscendTransferEngine.__init__",
    "signature": "def __init__(self, hostname: str, npu_id: int, disaggregation_mode: DisaggregationMode)"
  },
  {
    "module": "srt.disaggregation.ascend.transfer_engine",
    "file": "python/sglang/srt/disaggregation/ascend/transfer_engine.py",
    "line": 39,
    "qualname": "AscendTransferEngine.initialize",
    "signature": "def initialize(self)"
  },
  {
    "module": "srt.disaggregation.ascend.transfer_engine",
    "file": "python/sglang/srt/disaggregation/ascend/transfer_engine.py",
    "line": 51,
    "qualname": "AscendTransferEngine.batch_register",
    "signature": "def batch_register(self, ptrs: List[int], lengths: List[int])"
  },
  {
    "module": "srt.disaggregation.ascend.conn",
    "file": "python/sglang/srt/disaggregation/ascend/conn.py",
    "line": 16,
    "qualname": "AscendKVManager.init_engine",
    "signature": "def init_engine(self)"
  },
  {
    "module": "srt.disaggregation.ascend.conn",
    "file": "python/sglang/srt/disaggregation/ascend/conn.py",
    "line": 25,
    "qualname": "AscendKVManager.register_buffer_to_engine",
    "signature": "def register_buffer_to_engine(self)"
  },
  {
    "module": "srt.disaggregation.base.conn",
    "file": "python/sglang/srt/disaggregation/base/conn.py",
    "line": 50,
    "qualname": "BaseKVManager.__init__",
    "signature": "def __init__(self, args: KVArgs, disaggregation_mode: DisaggregationMode, server_args: ServerArgs, is_mla_backend: Optional[bool])"
  },
  {
    "module": "srt.disaggregation.base.conn",
    "file": "python/sglang/srt/disaggregation/base/conn.py",
    "line": 62,
    "qualname": "BaseKVSender.__init__",
    "signature": "def __init__(self, mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: int, dest_tp_ranks: List[int], pp_rank: int)"
  },
  {
    "module": "srt.disaggregation.base.conn",
    "file": "python/sglang/srt/disaggregation/base/conn.py",
    "line": 72,
    "qualname": "BaseKVSender.init",
    "signature": "def init(self, num_kv_indices: int, aux_index: Optional[int])"
  },
  {
    "module": "srt.disaggregation.base.conn",
    "file": "python/sglang/srt/disaggregation/base/conn.py",
    "line": 79,
    "qualname": "BaseKVSender.send",
    "signature": "def send(self, kv_indices: npt.NDArray[np.int32])"
  },
  {
    "module": "srt.disaggregation.base.conn",
    "file": "python/sglang/srt/disaggregation/base/conn.py",
    "line": 86,
    "qualname": "BaseKVSender.poll",
    "signature": "def poll(self)"
  },
  {
    "module": "srt.disaggregation.base.conn",
    "file": "python/sglang/srt/disaggregation/base/conn.py",
    "line": 93,
    "qualname": "BaseKVSender.failure_exception",
    "signature": "def failure_exception(self)"
  },
  {
    "module": "srt.disaggregation.base.conn",
    "file": "python/sglang/srt/disaggregation/base/conn.py",
    "line": 103,
    "qualname": "BaseKVReceiver.__init__",
    "signature": "def __init__(self, mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: Optional[int])"
  },
  {
    "module": "srt.disaggregation.base.conn",
    "file": "python/sglang/srt/disaggregation/base/conn.py",
    "line": 111,
    "qualname": "BaseKVReceiver.init",
    "signature": "def init(self, kv_indices: npt.NDArray[np.int32], aux_index: Optional[int])"
  },
  {
    "module": "srt.disaggregation.base.conn",
    "file": "python/sglang/srt/disaggregation/base/conn.py",
    "line": 118,
    "qualname": "BaseKVReceiver.poll",
    "signature": "def poll(self)"
  },
  {
    "module": "srt.disaggregation.base.conn",
    "file": "python/sglang/srt/disaggregation/base/conn.py",
    "line": 125,
    "qualname": "BaseKVReceiver.failure_exception",
    "signature": "def failure_exception(self)"
  },
  {
    "module": "srt.disaggregation.base.conn",
    "file": "python/sglang/srt/disaggregation/base/conn.py",
    "line": 134,
    "qualname": "BaseKVBootstrapServer.__init__",
    "signature": "def __init__(self, port: int)"
  },
  {
    "module": "srt.disaggregation.common.conn",
    "file": "python/sglang/srt/disaggregation/common/conn.py",
    "line": 39,
    "qualname": "CommonKVManager.__init__",
    "signature": "def __init__(self, args: KVArgs, disaggregation_mode: DisaggregationMode, server_args: ServerArgs, is_mla_backend: Optional[bool])"
  },
  {
    "module": "srt.disaggregation.common.conn",
    "file": "python/sglang/srt/disaggregation/common/conn.py",
    "line": 123,
    "qualname": "CommonKVReceiver.__init__",
    "signature": "def __init__(self, mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: Optional[int], data_parallel_rank: Optional[int])"
  },
  {
    "module": "srt.disaggregation.common.conn",
    "file": "python/sglang/srt/disaggregation/common/conn.py",
    "line": 306,
    "qualname": "CommonKVReceiver.failure_exception",
    "signature": "def failure_exception(self)"
  },
  {
    "module": "srt.disaggregation.common.conn",
    "file": "python/sglang/srt/disaggregation/common/conn.py",
    "line": 311,
    "qualname": "CommonKVBootstrapServer.__init__",
    "signature": "def __init__(self, port: int)"
  },
  {
    "module": "srt.disaggregation.common.conn",
    "file": "python/sglang/srt/disaggregation/common/conn.py",
    "line": 326,
    "qualname": "CommonKVBootstrapServer.run",
    "signature": "def run(self)"
  },
  {
    "module": "srt.disaggregation.common.conn",
    "file": "python/sglang/srt/disaggregation/common/conn.py",
    "line": 425,
    "qualname": "CommonKVBootstrapServer.close",
    "signature": "def close(self)"
  },
  {
    "module": "srt.disaggregation.common.conn",
    "file": "python/sglang/srt/disaggregation/common/conn.py",
    "line": 435,
    "qualname": "CommonKVBootstrapServer.poll",
    "signature": "def poll(self)"
  },
  {
    "module": "srt.disaggregation.common.utils",
    "file": "python/sglang/srt/disaggregation/common/utils.py",
    "line": 10,
    "qualname": "FastQueue.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.disaggregation.common.utils",
    "file": "python/sglang/srt/disaggregation/common/utils.py",
    "line": 14,
    "qualname": "FastQueue.put",
    "signature": "def put(self, item)"
  },
  {
    "module": "srt.disaggregation.common.utils",
    "file": "python/sglang/srt/disaggregation/common/utils.py",
    "line": 20,
    "qualname": "FastQueue.get",
    "signature": "def get(self)"
  },
  {
    "module": "srt.disaggregation.common.utils",
    "file": "python/sglang/srt/disaggregation/common/utils.py",
    "line": 28,
    "qualname": "group_concurrent_contiguous",
    "signature": "def group_concurrent_contiguous(src_indices: npt.NDArray[np.int32], dst_indices: npt.NDArray[np.int32])"
  },
  {
    "module": "srt.disaggregation.fake.conn",
    "file": "python/sglang/srt/disaggregation/fake/conn.py",
    "line": 19,
    "qualname": "FakeKVSender.__init__",
    "signature": "def __init__(self, mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: int, dest_tp_ranks: List[int], pp_rank: int)"
  },
  {
    "module": "srt.disaggregation.fake.conn",
    "file": "python/sglang/srt/disaggregation/fake/conn.py",
    "line": 29,
    "qualname": "FakeKVSender.poll",
    "signature": "def poll(self)"
  },
  {
    "module": "srt.disaggregation.fake.conn",
    "file": "python/sglang/srt/disaggregation/fake/conn.py",
    "line": 38,
    "qualname": "FakeKVSender.init",
    "signature": "def init(self, kv_indices: list[int], aux_index: Optional[int])"
  },
  {
    "module": "srt.disaggregation.fake.conn",
    "file": "python/sglang/srt/disaggregation/fake/conn.py",
    "line": 48,
    "qualname": "FakeKVSender.send",
    "signature": "def send(self, kv_indices: npt.NDArray[np.int32])"
  },
  {
    "module": "srt.disaggregation.fake.conn",
    "file": "python/sglang/srt/disaggregation/fake/conn.py",
    "line": 55,
    "qualname": "FakeKVSender.failure_exception",
    "signature": "def failure_exception(self)"
  },
  {
    "module": "srt.disaggregation.fake.conn",
    "file": "python/sglang/srt/disaggregation/fake/conn.py",
    "line": 60,
    "qualname": "FakeKVReceiver.__init__",
    "signature": "def __init__(self, mgr: BaseKVManager, bootstrap_addr: str, bootstrap_room: Optional[int], data_parallel_rank: Optional[int])"
  },
  {
    "module": "srt.disaggregation.fake.conn",
    "file": "python/sglang/srt/disaggregation/fake/conn.py",
    "line": 69,
    "qualname": "FakeKVReceiver.poll",
    "signature": "def poll(self)"
  },
  {
    "module": "srt.disaggregation.fake.conn",
    "file": "python/sglang/srt/disaggregation/fake/conn.py",
    "line": 78,
    "qualname": "FakeKVReceiver.init",
    "signature": "def init(self, kv_indices: list[int], aux_index: Optional[int])"
  },
  {
    "module": "srt.disaggregation.fake.conn",
    "file": "python/sglang/srt/disaggregation/fake/conn.py",
    "line": 84,
    "qualname": "FakeKVReceiver.failure_exception",
    "signature": "def failure_exception(self)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 61,
    "qualname": "KVTransferError.__init__",
    "signature": "def __init__(self, bootstrap_room: int, failure_reason: str)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 66,
    "qualname": "KVTransferError.__str__",
    "signature": "def __str__(self)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 93,
    "qualname": "TransferInfo.from_zmq",
    "signature": "def from_zmq(cls, msg: List[bytes])"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 128,
    "qualname": "KVArgsRegisterInfo.from_zmq",
    "signature": "def from_zmq(cls, msg: List[bytes])"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 146,
    "qualname": "AuxDataCodec.serialize_data_from_buffer",
    "signature": "def serialize_data_from_buffer(src_addr, data_length)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 152,
    "qualname": "AuxDataCodec.deserialize_data_to_buffer",
    "signature": "def deserialize_data_to_buffer(kv_args, buffer_index, aux_index, data)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 165,
    "qualname": "MooncakeKVManager.__init__",
    "signature": "def __init__(self, args: KVArgs, disaggregation_mode: DisaggregationMode, server_args: ServerArgs, is_mla_backend: Optional[bool])"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 277,
    "qualname": "MooncakeKVManager.init_engine",
    "signature": "def init_engine(self)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 284,
    "qualname": "MooncakeKVManager.register_buffer_to_engine",
    "signature": "def register_buffer_to_engine(self)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 314,
    "qualname": "MooncakeKVManager.send_kvcache",
    "signature": "def send_kvcache(self, mooncake_session_id: str, prefill_kv_indices: npt.NDArray[np.int32], dst_kv_ptrs: list[int], dst_kv_indices: npt.NDArray[np.int32], executor: concurrent.futures.ThreadPoolExecutor)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 419,
    "qualname": "MooncakeKVManager.send_kvcache_slice",
    "signature": "def send_kvcache_slice(self, mooncake_session_id: str, prefill_kv_indices: npt.NDArray[np.int64], dst_kv_ptrs: list[int], dst_kv_indices: npt.NDArray[np.int64], dst_tp_rank: int, dst_attn_tp_size: int, dst_kv_item_len: int, executor: concurrent.futures.ThreadPoolExecutor)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 583,
    "qualname": "MooncakeKVManager.send_aux",
    "signature": "def send_aux(self, req: TransferInfo, prefill_aux_index: int, dst_aux_ptrs: list[int])"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 605,
    "qualname": "MooncakeKVManager.send_aux_tcp",
    "signature": "def send_aux_tcp(self, req: TransferInfo, prefill_aux_index: int, dst_aux_ptrs: list[int])"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 630,
    "qualname": "MooncakeKVManager.send_aux_data_to_endpoint",
    "signature": "def send_aux_data_to_endpoint(self, remote: str, dst_port: int, room: int, buffer_index: int, aux_index: int, data: bytes)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 654,
    "qualname": "MooncakeKVManager.sync_status_to_decode_endpoint",
    "signature": "def sync_status_to_decode_endpoint(self, remote: str, dst_port: int, room: int, status: int, prefill_rank: int)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 667,
    "qualname": "MooncakeKVManager.transfer_worker",
    "signature": "def transfer_worker(self, queue: FastQueue, executor: concurrent.futures.ThreadPoolExecutor)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 805,
    "qualname": "MooncakeKVManager.start_prefill_thread",
    "signature": "def start_prefill_thread(self)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 864,
    "qualname": "MooncakeKVManager.start_decode_thread",
    "signature": "def start_decode_thread(self)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 955,
    "qualname": "MooncakeKVManager.add_transfer_request",
    "signature": "def add_transfer_request(self, bootstrap_room: int, kv_indices: npt.NDArray[np.int32], index_slice: slice, is_last: bool, aux_index: Optional[int])"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 998,
    "qualname": "MooncakeKVManager.check_status",
    "signature": "def check_status(self, bootstrap_room: int)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1001,
    "qualname": "MooncakeKVManager.update_status",
    "signature": "def update_status(self, bootstrap_room: int, status: KVPoll)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1013,
    "qualname": "MooncakeKVManager.record_failure",
    "signature": "def record_failure(self, bootstrap_room: int, failure_reason: str)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1017,
    "qualname": "MooncakeKVManager.get_session_id",
    "signature": "def get_session_id(self)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1103,
    "qualname": "MooncakeKVSender.__init__",
    "signature": "def __init__(self, mgr: MooncakeKVManager, bootstrap_addr: str, bootstrap_room: int, dest_tp_ranks: List[int], pp_rank: int)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1121,
    "qualname": "MooncakeKVSender.init",
    "signature": "def init(self, num_kv_indices: int, aux_index: Optional[int])"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1125,
    "qualname": "MooncakeKVSender.send",
    "signature": "def send(self, kv_indices: npt.NDArray[np.int32])"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1149,
    "qualname": "MooncakeKVSender.poll",
    "signature": "def poll(self)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1175,
    "qualname": "MooncakeKVSender.clear",
    "signature": "def clear(self)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1179,
    "qualname": "MooncakeKVSender.failure_exception",
    "signature": "def failure_exception(self)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1192,
    "qualname": "MooncakeKVSender.abort",
    "signature": "def abort(self)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1207,
    "qualname": "MooncakeKVReceiver.__init__",
    "signature": "def __init__(self, mgr: MooncakeKVManager, bootstrap_addr: str, bootstrap_room: Optional[int], data_parallel_rank: Optional[int])"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1470,
    "qualname": "MooncakeKVReceiver.init",
    "signature": "def init(self, kv_indices: npt.NDArray[np.int32], aux_index: Optional[int])"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1489,
    "qualname": "MooncakeKVReceiver.poll",
    "signature": "def poll(self)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1515,
    "qualname": "MooncakeKVReceiver.clear",
    "signature": "def clear(self)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1525,
    "qualname": "MooncakeKVReceiver.failure_exception",
    "signature": "def failure_exception(self)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1538,
    "qualname": "MooncakeKVReceiver.abort",
    "signature": "def abort(self)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1548,
    "qualname": "MooncakeKVBootstrapServer.__init__",
    "signature": "def __init__(self, port: int)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1565,
    "qualname": "MooncakeKVBootstrapServer.run",
    "signature": "def run(self)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1686,
    "qualname": "MooncakeKVBootstrapServer.close",
    "signature": "def close(self)"
  },
  {
    "module": "srt.disaggregation.mooncake.conn",
    "file": "python/sglang/srt/disaggregation/mooncake/conn.py",
    "line": 1696,
    "qualname": "MooncakeKVBootstrapServer.poll",
    "signature": "def poll(self)"
  },
  {
    "module": "srt.disaggregation.mooncake.transfer_engine",
    "file": "python/sglang/srt/disaggregation/mooncake/transfer_engine.py",
    "line": 11,
    "qualname": "MooncakeTransferEngine.__init__",
    "signature": "def __init__(self, hostname: str, gpu_id: int, ib_device: Optional[str])"
  },
  {
    "module": "srt.disaggregation.mooncake.transfer_engine",
    "file": "python/sglang/srt/disaggregation/mooncake/transfer_engine.py",
    "line": 34,
    "qualname": "MooncakeTransferEngine.register",
    "signature": "def register(self, ptr, length)"
  },
  {
    "module": "srt.disaggregation.mooncake.transfer_engine",
    "file": "python/sglang/srt/disaggregation/mooncake/transfer_engine.py",
    "line": 44,
    "qualname": "MooncakeTransferEngine.deregister",
    "signature": "def deregister(self, ptr)"
  },
  {
    "module": "srt.disaggregation.mooncake.transfer_engine",
    "file": "python/sglang/srt/disaggregation/mooncake/transfer_engine.py",
    "line": 54,
    "qualname": "MooncakeTransferEngine.batch_register",
    "signature": "def batch_register(self, ptrs: List[int], lengths: List[int])"
  },
  {
    "module": "srt.disaggregation.mooncake.transfer_engine",
    "file": "python/sglang/srt/disaggregation/mooncake/transfer_engine.py",
    "line": 71,
    "qualname": "MooncakeTransferEngine.batch_deregister",
    "signature": "def batch_deregister(self, ptrs: List[int])"
  },
  {
    "module": "srt.disaggregation.mooncake.transfer_engine",
    "file": "python/sglang/srt/disaggregation/mooncake/transfer_engine.py",
    "line": 83,
    "qualname": "MooncakeTransferEngine.initialize",
    "signature": "def initialize(self, hostname: str, device_name: Optional[str])"
  },
  {
    "module": "srt.disaggregation.mooncake.transfer_engine",
    "file": "python/sglang/srt/disaggregation/mooncake/transfer_engine.py",
    "line": 108,
    "qualname": "MooncakeTransferEngine.transfer_sync",
    "signature": "def transfer_sync(self, session_id: str, buffer: int, peer_buffer_address: int, length: int)"
  },
  {
    "module": "srt.disaggregation.mooncake.transfer_engine",
    "file": "python/sglang/srt/disaggregation/mooncake/transfer_engine.py",
    "line": 133,
    "qualname": "MooncakeTransferEngine.batch_transfer_sync",
    "signature": "def batch_transfer_sync(self, session_id: str, buffers: List[int], peer_buffer_addresses: List[int], lengths: List[int])"
  },
  {
    "module": "srt.disaggregation.mooncake.transfer_engine",
    "file": "python/sglang/srt/disaggregation/mooncake/transfer_engine.py",
    "line": 163,
    "qualname": "MooncakeTransferEngine.get_session_id",
    "signature": "def get_session_id(self)"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 53,
    "qualname": "TransferInfo.is_dummy",
    "signature": "def is_dummy(self)"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 57,
    "qualname": "TransferInfo.from_zmq",
    "signature": "def from_zmq(cls, msg: List[bytes])"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 83,
    "qualname": "KVArgsRegisterInfo.from_zmq",
    "signature": "def from_zmq(cls, msg: List[bytes])"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 107,
    "qualname": "TransferStatus.is_done",
    "signature": "def is_done(self)"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 114,
    "qualname": "NixlKVManager.__init__",
    "signature": "def __init__(self, args: KVArgs, disaggregation_mode: DisaggregationMode, server_args: ServerArgs, is_mla_backend: Optional[bool])"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 151,
    "qualname": "NixlKVManager.check_status",
    "signature": "def check_status(self, bootstrap_room: int)"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 154,
    "qualname": "NixlKVManager.update_status",
    "signature": "def update_status(self, bootstrap_room: int, status: KVPoll)"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 163,
    "qualname": "NixlKVManager.register_buffer_to_engine",
    "signature": "def register_buffer_to_engine(self)"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 191,
    "qualname": "NixlKVManager.send_kvcache",
    "signature": "def send_kvcache(self, peer_name: str, prefill_kv_indices: npt.NDArray[np.int32], dst_kv_ptrs: list[int], dst_kv_indices: npt.NDArray[np.int32], dst_gpu_id: int, notif: str)"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 242,
    "qualname": "NixlKVManager.send_aux",
    "signature": "def send_aux(self, peer_name: str, prefill_aux_index: int, dst_aux_ptrs: list[int], dst_aux_index: int, notif: str)"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 275,
    "qualname": "NixlKVManager.add_transfer_request",
    "signature": "def add_transfer_request(self, bootstrap_room: int, kv_indices: npt.NDArray[np.int32], index_slice: slice, is_last: bool, chunk_id: int, aux_index: Optional[int])"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 323,
    "qualname": "NixlKVManager.update_transfer_status",
    "signature": "def update_transfer_status(self)"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 342,
    "qualname": "NixlKVManager.check_transfer_done",
    "signature": "def check_transfer_done(self, room: int)"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 392,
    "qualname": "NixlKVSender.__init__",
    "signature": "def __init__(self, mgr: NixlKVManager, bootstrap_addr: str, bootstrap_room: int, dest_tp_ranks: List[int], pp_rank: int)"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 411,
    "qualname": "NixlKVSender.init",
    "signature": "def init(self, num_kv_indices: int, aux_index: Optional[int])"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 415,
    "qualname": "NixlKVSender.send",
    "signature": "def send(self, kv_indices: npt.NDArray[np.int32])"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 437,
    "qualname": "NixlKVSender.poll",
    "signature": "def poll(self)"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 447,
    "qualname": "NixlKVSender.failure_exception",
    "signature": "def failure_exception(self)"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 452,
    "qualname": "NixlKVReceiver.__init__",
    "signature": "def __init__(self, mgr: NixlKVManager, bootstrap_addr: str, bootstrap_room: Optional[int], data_parallel_rank: Optional[int])"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 463,
    "qualname": "NixlKVReceiver.init",
    "signature": "def init(self, kv_indices: npt.NDArray[np.int32], aux_index: Optional[int])"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 489,
    "qualname": "NixlKVReceiver.poll",
    "signature": "def poll(self)"
  },
  {
    "module": "srt.disaggregation.nixl.conn",
    "file": "python/sglang/srt/disaggregation/nixl/conn.py",
    "line": 527,
    "qualname": "NixlKVReceiver.failure_exception",
    "signature": "def failure_exception(self)"
  },
  {
    "module": "srt.distributed.device_communicators.cuda_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/cuda_wrapper.py",
    "line": 37,
    "qualname": "find_loaded_library",
    "signature": "def find_loaded_library(lib_name)"
  },
  {
    "module": "srt.distributed.device_communicators.cuda_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/cuda_wrapper.py",
    "line": 114,
    "qualname": "CudaRTLibrary.__init__",
    "signature": "def __init__(self, so_file: Optional[str])"
  },
  {
    "module": "srt.distributed.device_communicators.cuda_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/cuda_wrapper.py",
    "line": 133,
    "qualname": "CudaRTLibrary.CUDART_CHECK",
    "signature": "def CUDART_CHECK(self, result: cudaError_t)"
  },
  {
    "module": "srt.distributed.device_communicators.cuda_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/cuda_wrapper.py",
    "line": 138,
    "qualname": "CudaRTLibrary.cudaGetErrorString",
    "signature": "def cudaGetErrorString(self, error: cudaError_t)"
  },
  {
    "module": "srt.distributed.device_communicators.cuda_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/cuda_wrapper.py",
    "line": 141,
    "qualname": "CudaRTLibrary.cudaSetDevice",
    "signature": "def cudaSetDevice(self, device: int)"
  },
  {
    "module": "srt.distributed.device_communicators.cuda_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/cuda_wrapper.py",
    "line": 144,
    "qualname": "CudaRTLibrary.cudaDeviceSynchronize",
    "signature": "def cudaDeviceSynchronize(self)"
  },
  {
    "module": "srt.distributed.device_communicators.cuda_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/cuda_wrapper.py",
    "line": 147,
    "qualname": "CudaRTLibrary.cudaDeviceReset",
    "signature": "def cudaDeviceReset(self)"
  },
  {
    "module": "srt.distributed.device_communicators.cuda_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/cuda_wrapper.py",
    "line": 150,
    "qualname": "CudaRTLibrary.cudaMalloc",
    "signature": "def cudaMalloc(self, size: int)"
  },
  {
    "module": "srt.distributed.device_communicators.cuda_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/cuda_wrapper.py",
    "line": 155,
    "qualname": "CudaRTLibrary.cudaFree",
    "signature": "def cudaFree(self, devPtr: ctypes.c_void_p)"
  },
  {
    "module": "srt.distributed.device_communicators.cuda_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/cuda_wrapper.py",
    "line": 158,
    "qualname": "CudaRTLibrary.cudaMemset",
    "signature": "def cudaMemset(self, devPtr: ctypes.c_void_p, value: int, count: int)"
  },
  {
    "module": "srt.distributed.device_communicators.cuda_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/cuda_wrapper.py",
    "line": 161,
    "qualname": "CudaRTLibrary.cudaMemcpy",
    "signature": "def cudaMemcpy(self, dst: ctypes.c_void_p, src: ctypes.c_void_p, count: int)"
  },
  {
    "module": "srt.distributed.device_communicators.cuda_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/cuda_wrapper.py",
    "line": 168,
    "qualname": "CudaRTLibrary.cudaIpcGetMemHandle",
    "signature": "def cudaIpcGetMemHandle(self, devPtr: ctypes.c_void_p)"
  },
  {
    "module": "srt.distributed.device_communicators.cuda_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/cuda_wrapper.py",
    "line": 175,
    "qualname": "CudaRTLibrary.cudaIpcOpenMemHandle",
    "signature": "def cudaIpcOpenMemHandle(self, handle: cudaIpcMemHandle_t)"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce.py",
    "line": 66,
    "qualname": "CustomAllreduce.__init__",
    "signature": "def __init__(self, group: ProcessGroup, device: Union[int, str, torch.device], max_size)"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce.py",
    "line": 215,
    "qualname": "CustomAllreduce.create_shared_buffer",
    "signature": "def create_shared_buffer(size_in_bytes: int, group: Optional[ProcessGroup])"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce.py",
    "line": 240,
    "qualname": "CustomAllreduce.free_shared_buffer",
    "signature": "def free_shared_buffer(pointers: List[int], group: Optional[ProcessGroup])"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce.py",
    "line": 248,
    "qualname": "CustomAllreduce.capture",
    "signature": "def capture(self)"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce.py",
    "line": 296,
    "qualname": "CustomAllreduce.register_buffer",
    "signature": "def register_buffer(self, inp: torch.Tensor)"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce.py",
    "line": 300,
    "qualname": "CustomAllreduce.register_graph_buffers",
    "signature": "def register_graph_buffers(self)"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce.py",
    "line": 326,
    "qualname": "CustomAllreduce.should_custom_ar",
    "signature": "def should_custom_ar(self, inp: torch.Tensor)"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce.py",
    "line": 351,
    "qualname": "CustomAllreduce.all_reduce_reg",
    "signature": "def all_reduce_reg(self, inp: torch.Tensor, out: torch.Tensor)"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce.py",
    "line": 358,
    "qualname": "CustomAllreduce.all_reduce_unreg",
    "signature": "def all_reduce_unreg(self, inp: torch.Tensor, out: torch.Tensor)"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce.py",
    "line": 364,
    "qualname": "CustomAllreduce.all_reduce",
    "signature": "def all_reduce(self, inp: torch.Tensor)"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce.py",
    "line": 387,
    "qualname": "CustomAllreduce.custom_all_reduce",
    "signature": "def custom_all_reduce(self, input: torch.Tensor)"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce.py",
    "line": 412,
    "qualname": "CustomAllreduce.close",
    "signature": "def close(self)"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce.py",
    "line": 420,
    "qualname": "CustomAllreduce.__del__",
    "signature": "def __del__(self)"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce_utils",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce_utils.py",
    "line": 50,
    "qualname": "update_environment_variables",
    "signature": "def update_environment_variables(envs: Dict[str, str])"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce_utils",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce_utils.py",
    "line": 62,
    "qualname": "producer",
    "signature": "def producer(batch_src: Sequence[int], producer_queue, consumer_queue, result_queue, cuda_visible_devices: Optional[str])"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce_utils",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce_utils.py",
    "line": 96,
    "qualname": "consumer",
    "signature": "def consumer(batch_tgt: Sequence[int], producer_queue, consumer_queue, result_queue, cuda_visible_devices: Optional[str])"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce_utils",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce_utils.py",
    "line": 137,
    "qualname": "can_actually_p2p",
    "signature": "def can_actually_p2p(batch_src: Sequence[int], batch_tgt: Sequence[int])"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce_utils",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce_utils.py",
    "line": 237,
    "qualname": "gpu_p2p_access_check",
    "signature": "def gpu_p2p_access_check(src: int, tgt: int)"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce_utils",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce_utils.py",
    "line": 312,
    "qualname": "with_nvml_context",
    "signature": "def with_nvml_context(fn: Callable[_P, _R])"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce_utils",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce_utils.py",
    "line": 332,
    "qualname": "is_full_nvlink",
    "signature": "def is_full_nvlink(physical_device_ids: List[int], world_size: int)"
  },
  {
    "module": "srt.distributed.device_communicators.custom_all_reduce_utils",
    "file": "python/sglang/srt/distributed/device_communicators/custom_all_reduce_utils.py",
    "line": 373,
    "qualname": "is_weak_contiguous",
    "signature": "def is_weak_contiguous(inp: torch.Tensor)"
  },
  {
    "module": "srt.distributed.device_communicators.hpu_communicator",
    "file": "python/sglang/srt/distributed/device_communicators/hpu_communicator.py",
    "line": 15,
    "qualname": "HpuCommunicator.__init__",
    "signature": "def __init__(self, group: ProcessGroup)"
  },
  {
    "module": "srt.distributed.device_communicators.hpu_communicator",
    "file": "python/sglang/srt/distributed/device_communicators/hpu_communicator.py",
    "line": 23,
    "qualname": "HpuCommunicator.all_reduce",
    "signature": "def all_reduce(self, x: torch.Tensor)"
  },
  {
    "module": "srt.distributed.device_communicators.hpu_communicator",
    "file": "python/sglang/srt/distributed/device_communicators/hpu_communicator.py",
    "line": 31,
    "qualname": "HpuCommunicator.all_gather",
    "signature": "def all_gather(self, x: torch.Tensor, dim: int)"
  },
  {
    "module": "srt.distributed.device_communicators.npu_communicator",
    "file": "python/sglang/srt/distributed/device_communicators/npu_communicator.py",
    "line": 10,
    "qualname": "NpuCommunicator.__init__",
    "signature": "def __init__(self, group: ProcessGroup)"
  },
  {
    "module": "srt.distributed.device_communicators.npu_communicator",
    "file": "python/sglang/srt/distributed/device_communicators/npu_communicator.py",
    "line": 18,
    "qualname": "NpuCommunicator.all_reduce",
    "signature": "def all_reduce(self, x: torch.Tensor)"
  },
  {
    "module": "srt.distributed.device_communicators.npu_communicator",
    "file": "python/sglang/srt/distributed/device_communicators/npu_communicator.py",
    "line": 22,
    "qualname": "NpuCommunicator.all_gather",
    "signature": "def all_gather(self, x: torch.Tensor, dim: int)"
  },
  {
    "module": "srt.distributed.device_communicators.pymscclpp",
    "file": "python/sglang/srt/distributed/device_communicators/pymscclpp.py",
    "line": 39,
    "qualname": "mscclpp_is_weak_contiguous",
    "signature": "def mscclpp_is_weak_contiguous(inp: torch.Tensor)"
  },
  {
    "module": "srt.distributed.device_communicators.pymscclpp",
    "file": "python/sglang/srt/distributed/device_communicators/pymscclpp.py",
    "line": 46,
    "qualname": "mscclpp_convert_to_bytes",
    "signature": "def mscclpp_convert_to_bytes(size_str)"
  },
  {
    "module": "srt.distributed.device_communicators.pymscclpp",
    "file": "python/sglang/srt/distributed/device_communicators/pymscclpp.py",
    "line": 87,
    "qualname": "mscclpp_bench_time",
    "signature": "def mscclpp_bench_time(func, test_niter: int, warmup_niter: int)"
  },
  {
    "module": "srt.distributed.device_communicators.pymscclpp",
    "file": "python/sglang/srt/distributed/device_communicators/pymscclpp.py",
    "line": 111,
    "qualname": "PyMscclppCommunicator.__init__",
    "signature": "def __init__(self, group: ProcessGroup, device: Union[int, str, torch.device], max_bytes)"
  },
  {
    "module": "srt.distributed.device_communicators.pymscclpp",
    "file": "python/sglang/srt/distributed/device_communicators/pymscclpp.py",
    "line": 243,
    "qualname": "PyMscclppCommunicator.pre_tune_config",
    "signature": "def pre_tune_config(self, dtype)"
  },
  {
    "module": "srt.distributed.device_communicators.pymscclpp",
    "file": "python/sglang/srt/distributed/device_communicators/pymscclpp.py",
    "line": 273,
    "qualname": "PyMscclppCommunicator.should_mscclpp_allreduce",
    "signature": "def should_mscclpp_allreduce(self, inp: torch.Tensor, op: ReduceOp)"
  },
  {
    "module": "srt.distributed.device_communicators.pymscclpp",
    "file": "python/sglang/srt/distributed/device_communicators/pymscclpp.py",
    "line": 289,
    "qualname": "PyMscclppCommunicator.all_reduce",
    "signature": "def all_reduce(self, tensor: torch.Tensor, op: ReduceOp)"
  },
  {
    "module": "srt.distributed.device_communicators.pymscclpp",
    "file": "python/sglang/srt/distributed/device_communicators/pymscclpp.py",
    "line": 302,
    "qualname": "PyMscclppCommunicator.change_state",
    "signature": "def change_state(self, enable: Optional[bool])"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl.py",
    "line": 28,
    "qualname": "PyNcclCommunicator.__init__",
    "signature": "def __init__(self, group: Union[ProcessGroup, StatelessProcessGroup], device: Union[int, str, torch.device], library_path: Optional[str])"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl.py",
    "line": 126,
    "qualname": "PyNcclCommunicator.all_reduce",
    "signature": "def all_reduce(self, tensor: torch.Tensor, op: ReduceOp, stream)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl.py",
    "line": 150,
    "qualname": "PyNcclCommunicator.all_gather",
    "signature": "def all_gather(self, output_tensor: torch.Tensor, input_tensor: torch.Tensor, stream, sizes: Optional[list[int]])"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl.py",
    "line": 196,
    "qualname": "PyNcclCommunicator.reduce_scatter",
    "signature": "def reduce_scatter(self, output_tensor: torch.Tensor, input_tensor: torch.Tensor, op: ReduceOp, stream, sizes: Optional[list[int]])"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl.py",
    "line": 245,
    "qualname": "PyNcclCommunicator.send",
    "signature": "def send(self, tensor: torch.Tensor, dst: int, stream)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl.py",
    "line": 263,
    "qualname": "PyNcclCommunicator.recv",
    "signature": "def recv(self, tensor: torch.Tensor, src: int, stream)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl.py",
    "line": 281,
    "qualname": "PyNcclCommunicator.broadcast",
    "signature": "def broadcast(self, tensor: torch.Tensor, src: int, stream)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl.py",
    "line": 307,
    "qualname": "PyNcclCommunicator.register_comm_window_raw",
    "signature": "def register_comm_window_raw(self, ptr: int, size: int)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl.py",
    "line": 310,
    "qualname": "PyNcclCommunicator.deregister_comm_window",
    "signature": "def deregister_comm_window(self, window)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl.py",
    "line": 313,
    "qualname": "PyNcclCommunicator.group_start",
    "signature": "def group_start(self)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl.py",
    "line": 316,
    "qualname": "PyNcclCommunicator.group_end",
    "signature": "def group_end(self)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl.py",
    "line": 320,
    "qualname": "PyNcclCommunicator.change_state",
    "signature": "def change_state(self, enable: Optional[bool], stream: Optional[torch.cuda.Stream])"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_allocator",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_allocator.py",
    "line": 34,
    "qualname": "is_symmetric_memory_enabled",
    "signature": "def is_symmetric_memory_enabled()"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_allocator",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_allocator.py",
    "line": 38,
    "qualname": "set_graph_pool_id",
    "signature": "def set_graph_pool_id(graph_pool_id)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_allocator",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_allocator.py",
    "line": 43,
    "qualname": "get_nccl_mem_pool",
    "signature": "def get_nccl_mem_pool()"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_allocator",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_allocator.py",
    "line": 67,
    "qualname": "use_symmetric_memory.__init__",
    "signature": "def __init__(self, group_coordinator: GroupCoordinator)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_allocator",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_allocator.py",
    "line": 81,
    "qualname": "use_symmetric_memory.__enter__",
    "signature": "def __enter__(self)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_allocator",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_allocator.py",
    "line": 104,
    "qualname": "use_symmetric_memory.tag",
    "signature": "def tag(self, tensor: torch.Tensor)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_allocator",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_allocator.py",
    "line": 109,
    "qualname": "use_symmetric_memory.__exit__",
    "signature": "def __exit__(self, exc_type, exc_val, exc_tb)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 37,
    "qualname": "find_nccl_library",
    "signature": "def find_nccl_library()"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 102,
    "qualname": "ncclDataTypeEnum.from_torch",
    "signature": "def from_torch(cls, dtype: torch.dtype)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 134,
    "qualname": "ncclRedOpTypeEnum.from_torch",
    "signature": "def from_torch(cls, op: ReduceOp)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 332,
    "qualname": "NCCLLibrary.__init__",
    "signature": "def __init__(self, so_file: Optional[str])"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 368,
    "qualname": "NCCLLibrary.ncclGetErrorString",
    "signature": "def ncclGetErrorString(self, result: ncclResult_t)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 371,
    "qualname": "NCCLLibrary.NCCL_CHECK",
    "signature": "def NCCL_CHECK(self, result: ncclResult_t)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 376,
    "qualname": "NCCLLibrary.ncclGetRawVersion",
    "signature": "def ncclGetRawVersion(self)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 382,
    "qualname": "NCCLLibrary.ncclGetVersion",
    "signature": "def ncclGetVersion(self)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 390,
    "qualname": "NCCLLibrary.ncclGetUniqueId",
    "signature": "def ncclGetUniqueId(self)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 395,
    "qualname": "NCCLLibrary.ncclCommInitRank",
    "signature": "def ncclCommInitRank(self, world_size: int, unique_id: ncclUniqueId, rank: int)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 406,
    "qualname": "NCCLLibrary.ncclAllReduce",
    "signature": "def ncclAllReduce(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, comm: ncclComm_t, stream: cudaStream_t)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 427,
    "qualname": "NCCLLibrary.ncclReduce",
    "signature": "def ncclReduce(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, root: int, comm: ncclComm_t, stream: cudaStream_t)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 449,
    "qualname": "NCCLLibrary.ncclReduceScatter",
    "signature": "def ncclReduceScatter(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, comm: ncclComm_t, stream: cudaStream_t)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 470,
    "qualname": "NCCLLibrary.ncclAllGather",
    "signature": "def ncclAllGather(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, comm: ncclComm_t, stream: cudaStream_t)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 489,
    "qualname": "NCCLLibrary.ncclSend",
    "signature": "def ncclSend(self, sendbuff: buffer_type, count: int, datatype: int, dest: int, comm: ncclComm_t, stream: cudaStream_t)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 502,
    "qualname": "NCCLLibrary.ncclRecv",
    "signature": "def ncclRecv(self, recvbuff: buffer_type, count: int, datatype: int, src: int, comm: ncclComm_t, stream: cudaStream_t)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 515,
    "qualname": "NCCLLibrary.ncclBroadcast",
    "signature": "def ncclBroadcast(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, root: int, comm: ncclComm_t, stream: cudaStream_t)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 531,
    "qualname": "NCCLLibrary.ncclCommDestroy",
    "signature": "def ncclCommDestroy(self, comm: ncclComm_t)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 534,
    "qualname": "NCCLLibrary.ncclCommWindowRegister",
    "signature": "def ncclCommWindowRegister(self, comm: ncclComm_t, buff: buffer_type, size: int, win_flags: int)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 545,
    "qualname": "NCCLLibrary.ncclCommWindowDeregister",
    "signature": "def ncclCommWindowDeregister(self, comm: ncclComm_t, window: ncclWindow_t)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 548,
    "qualname": "NCCLLibrary.ncclGroupStart",
    "signature": "def ncclGroupStart(self)"
  },
  {
    "module": "srt.distributed.device_communicators.pynccl_wrapper",
    "file": "python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py",
    "line": 551,
    "qualname": "NCCLLibrary.ncclGroupEnd",
    "signature": "def ncclGroupEnd(self)"
  },
  {
    "module": "srt.distributed.device_communicators.quick_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/quick_all_reduce.py",
    "line": 34,
    "qualname": "qr_rocm_arch_available",
    "signature": "def qr_rocm_arch_available()"
  },
  {
    "module": "srt.distributed.device_communicators.quick_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/quick_all_reduce.py",
    "line": 73,
    "qualname": "QuickAllReduce.__init__",
    "signature": "def __init__(self, group: ProcessGroup, device: Union[int, str, torch.device])"
  },
  {
    "module": "srt.distributed.device_communicators.quick_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/quick_all_reduce.py",
    "line": 175,
    "qualname": "QuickAllReduce.init_quick_all_reduce",
    "signature": "def init_quick_all_reduce(self)"
  },
  {
    "module": "srt.distributed.device_communicators.quick_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/quick_all_reduce.py",
    "line": 219,
    "qualname": "QuickAllReduce.create_shared_buffer",
    "signature": "def create_shared_buffer(self)"
  },
  {
    "module": "srt.distributed.device_communicators.quick_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/quick_all_reduce.py",
    "line": 230,
    "qualname": "QuickAllReduce.should_quick_allreduce",
    "signature": "def should_quick_allreduce(self, inp: torch.Tensor)"
  },
  {
    "module": "srt.distributed.device_communicators.quick_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/quick_all_reduce.py",
    "line": 254,
    "qualname": "QuickAllReduce.quick_all_reduce",
    "signature": "def quick_all_reduce(self, inp: torch.Tensor)"
  },
  {
    "module": "srt.distributed.device_communicators.quick_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/quick_all_reduce.py",
    "line": 265,
    "qualname": "QuickAllReduce.close",
    "signature": "def close(self)"
  },
  {
    "module": "srt.distributed.device_communicators.quick_all_reduce",
    "file": "python/sglang/srt/distributed/device_communicators/quick_all_reduce.py",
    "line": 272,
    "qualname": "QuickAllReduce.__del__",
    "signature": "def __del__(self)"
  },
  {
    "module": "srt.distributed.device_communicators.shm_broadcast",
    "file": "python/sglang/srt/distributed/device_communicators/shm_broadcast.py",
    "line": 36,
    "qualname": "ShmRingBuffer.__init__",
    "signature": "def __init__(self, n_reader: int, max_chunk_bytes: int, max_chunks: int, name: Optional[str])"
  },
  {
    "module": "srt.distributed.device_communicators.shm_broadcast",
    "file": "python/sglang/srt/distributed/device_communicators/shm_broadcast.py",
    "line": 132,
    "qualname": "ShmRingBuffer.__reduce__",
    "signature": "def __reduce__(self)"
  },
  {
    "module": "srt.distributed.device_communicators.shm_broadcast",
    "file": "python/sglang/srt/distributed/device_communicators/shm_broadcast.py",
    "line": 143,
    "qualname": "ShmRingBuffer.__del__",
    "signature": "def __del__(self)"
  },
  {
    "module": "srt.distributed.device_communicators.shm_broadcast",
    "file": "python/sglang/srt/distributed/device_communicators/shm_broadcast.py",
    "line": 150,
    "qualname": "ShmRingBuffer.get_data",
    "signature": "def get_data(self, current_idx: int)"
  },
  {
    "module": "srt.distributed.device_communicators.shm_broadcast",
    "file": "python/sglang/srt/distributed/device_communicators/shm_broadcast.py",
    "line": 157,
    "qualname": "ShmRingBuffer.get_metadata",
    "signature": "def get_metadata(self, current_idx: int)"
  },
  {
    "module": "srt.distributed.device_communicators.shm_broadcast",
    "file": "python/sglang/srt/distributed/device_communicators/shm_broadcast.py",
    "line": 176,
    "qualname": "MessageQueue.__init__",
    "signature": "def __init__(self, n_reader, n_local_reader, local_reader_ranks: Optional[List[int]], max_chunk_bytes: int, max_chunks: int, connect_ip: Optional[str])"
  },
  {
    "module": "srt.distributed.device_communicators.shm_broadcast",
    "file": "python/sglang/srt/distributed/device_communicators/shm_broadcast.py",
    "line": 257,
    "qualname": "MessageQueue.export_handle",
    "signature": "def export_handle(self)"
  },
  {
    "module": "srt.distributed.device_communicators.shm_broadcast",
    "file": "python/sglang/srt/distributed/device_communicators/shm_broadcast.py",
    "line": 261,
    "qualname": "MessageQueue.create_from_handle",
    "signature": "def create_from_handle(handle: Handle, rank)"
  },
  {
    "module": "srt.distributed.device_communicators.shm_broadcast",
    "file": "python/sglang/srt/distributed/device_communicators/shm_broadcast.py",
    "line": 304,
    "qualname": "MessageQueue.wait_until_ready",
    "signature": "def wait_until_ready(self)"
  },
  {
    "module": "srt.distributed.device_communicators.shm_broadcast",
    "file": "python/sglang/srt/distributed/device_communicators/shm_broadcast.py",
    "line": 338,
    "qualname": "MessageQueue.acquire_write",
    "signature": "def acquire_write(self)"
  },
  {
    "module": "srt.distributed.device_communicators.shm_broadcast",
    "file": "python/sglang/srt/distributed/device_communicators/shm_broadcast.py",
    "line": 391,
    "qualname": "MessageQueue.acquire_read",
    "signature": "def acquire_read(self)"
  },
  {
    "module": "srt.distributed.device_communicators.shm_broadcast",
    "file": "python/sglang/srt/distributed/device_communicators/shm_broadcast.py",
    "line": 434,
    "qualname": "MessageQueue.enqueue",
    "signature": "def enqueue(self, obj)"
  },
  {
    "module": "srt.distributed.device_communicators.shm_broadcast",
    "file": "python/sglang/srt/distributed/device_communicators/shm_broadcast.py",
    "line": 449,
    "qualname": "MessageQueue.dequeue",
    "signature": "def dequeue(self)"
  },
  {
    "module": "srt.distributed.device_communicators.shm_broadcast",
    "file": "python/sglang/srt/distributed/device_communicators/shm_broadcast.py",
    "line": 468,
    "qualname": "MessageQueue.broadcast_object",
    "signature": "def broadcast_object(self, obj)"
  },
  {
    "module": "srt.distributed.device_communicators.shm_broadcast",
    "file": "python/sglang/srt/distributed/device_communicators/shm_broadcast.py",
    "line": 476,
    "qualname": "MessageQueue.create_from_process_group",
    "signature": "def create_from_process_group(pg: ProcessGroup, max_chunk_bytes, max_chunks, writer_rank)"
  },
  {
    "module": "srt.distributed.device_communicators.xpu_communicator",
    "file": "python/sglang/srt/distributed/device_communicators/xpu_communicator.py",
    "line": 12,
    "qualname": "XpuCommunicator.__init__",
    "signature": "def __init__(self, group: ProcessGroup)"
  },
  {
    "module": "srt.distributed.device_communicators.xpu_communicator",
    "file": "python/sglang/srt/distributed/device_communicators/xpu_communicator.py",
    "line": 20,
    "qualname": "XpuCommunicator.all_reduce",
    "signature": "def all_reduce(self, x: torch.Tensor)"
  },
  {
    "module": "srt.distributed.device_communicators.xpu_communicator",
    "file": "python/sglang/srt/distributed/device_communicators/xpu_communicator.py",
    "line": 24,
    "qualname": "XpuCommunicator.gather",
    "signature": "def gather(self, input_: torch.Tensor, rank_in_group: int, dst: int, dim: int)"
  },
  {
    "module": "srt.entrypoints.openai.serving_base",
    "file": "python/sglang/srt/entrypoints/openai/serving_base.py",
    "line": 21,
    "qualname": "OpenAIServingBase.__init__",
    "signature": "def __init__(self, tokenizer_manager: TokenizerManager)"
  },
  {
    "module": "srt.entrypoints.openai.serving_base",
    "file": "python/sglang/srt/entrypoints/openai/serving_base.py",
    "line": 24,
    "qualname": "OpenAIServingBase.handle_request",
    "signature": "async def handle_request(self, request: OpenAIServingRequest, raw_request: Request)"
  },
  {
    "module": "srt.entrypoints.openai.serving_base",
    "file": "python/sglang/srt/entrypoints/openai/serving_base.py",
    "line": 120,
    "qualname": "OpenAIServingBase.create_error_response",
    "signature": "def create_error_response(self, message: str, err_type: str, status_code: int, param: Optional[str])"
  },
  {
    "module": "srt.entrypoints.openai.serving_base",
    "file": "python/sglang/srt/entrypoints/openai/serving_base.py",
    "line": 138,
    "qualname": "OpenAIServingBase.create_streaming_error_response",
    "signature": "def create_streaming_error_response(self, message: str, err_type: str, status_code: int)"
  },
  {
    "module": "srt.entrypoints.openai.serving_chat",
    "file": "python/sglang/srt/entrypoints/openai/serving_chat.py",
    "line": 49,
    "qualname": "OpenAIServingChat.__init__",
    "signature": "def __init__(self, tokenizer_manager: TokenizerManager, template_manager: TemplateManager)"
  },
  {
    "module": "srt.entrypoints.openai.serving_completions",
    "file": "python/sglang/srt/entrypoints/openai/serving_completions.py",
    "line": 34,
    "qualname": "OpenAIServingCompletion.__init__",
    "signature": "def __init__(self, tokenizer_manager: TokenizerManager, template_manager: TemplateManager)"
  },
  {
    "module": "srt.entrypoints.openai.serving_embedding",
    "file": "python/sglang/srt/entrypoints/openai/serving_embedding.py",
    "line": 24,
    "qualname": "OpenAIServingEmbedding.__init__",
    "signature": "def __init__(self, tokenizer_manager: TokenizerManager, template_manager: TemplateManager)"
  },
  {
    "module": "srt.entrypoints.openai.serving_responses",
    "file": "python/sglang/srt/entrypoints/openai/serving_responses.py",
    "line": 68,
    "qualname": "OpenAIServingResponses.__init__",
    "signature": "def __init__(self, tokenizer_manager: TokenizerManager, template_manager: TemplateManager)"
  },
  {
    "module": "srt.entrypoints.openai.serving_responses",
    "file": "python/sglang/srt/entrypoints/openai/serving_responses.py",
    "line": 126,
    "qualname": "OpenAIServingResponses.create_responses",
    "signature": "async def create_responses(self, request: ResponsesRequest, raw_request: Optional[Request])"
  },
  {
    "module": "srt.entrypoints.openai.serving_responses",
    "file": "python/sglang/srt/entrypoints/openai/serving_responses.py",
    "line": 389,
    "qualname": "OpenAIServingResponses.responses_full_generator",
    "signature": "async def responses_full_generator(self, request: ResponsesRequest, sampling_params: Any, result_generator: AsyncIterator[Any], context: ConversationContext, model_name: str, tokenizer: Any, request_metadata: RequestResponseMetadata, created_time: Optional[int])"
  },
  {
    "module": "srt.entrypoints.openai.serving_responses",
    "file": "python/sglang/srt/entrypoints/openai/serving_responses.py",
    "line": 708,
    "qualname": "OpenAIServingResponses.retrieve_responses",
    "signature": "async def retrieve_responses(self, response_id: str)"
  },
  {
    "module": "srt.entrypoints.openai.serving_responses",
    "file": "python/sglang/srt/entrypoints/openai/serving_responses.py",
    "line": 722,
    "qualname": "OpenAIServingResponses.cancel_responses",
    "signature": "async def cancel_responses(self, response_id: str)"
  },
  {
    "module": "srt.entrypoints.openai.serving_responses",
    "file": "python/sglang/srt/entrypoints/openai/serving_responses.py",
    "line": 771,
    "qualname": "OpenAIServingResponses.responses_stream_generator",
    "signature": "async def responses_stream_generator(self, request: ResponsesRequest, sampling_params: Any, result_generator: AsyncIterator[StreamingHarmonyContext], context: StreamingHarmonyContext, model_name: str, tokenizer: Any, request_metadata: RequestResponseMetadata, created_time: Optional[int])"
  },
  {
    "module": "srt.entrypoints.openai.tool_server",
    "file": "python/sglang/srt/entrypoints/openai/tool_server.py",
    "line": 20,
    "qualname": "list_server_and_tools",
    "signature": "async def list_server_and_tools(server_url: str)"
  },
  {
    "module": "srt.entrypoints.openai.tool_server",
    "file": "python/sglang/srt/entrypoints/openai/tool_server.py",
    "line": 30,
    "qualname": "trim_schema",
    "signature": "def trim_schema(schema: dict)"
  },
  {
    "module": "srt.entrypoints.openai.tool_server",
    "file": "python/sglang/srt/entrypoints/openai/tool_server.py",
    "line": 55,
    "qualname": "post_process_tools_description",
    "signature": "def post_process_tools_description(list_tools_result: 'ListToolsResult')"
  },
  {
    "module": "srt.entrypoints.openai.tool_server",
    "file": "python/sglang/srt/entrypoints/openai/tool_server.py",
    "line": 76,
    "qualname": "ToolServer.has_tool",
    "signature": "def has_tool(self, tool_name: str)"
  },
  {
    "module": "srt.entrypoints.openai.tool_server",
    "file": "python/sglang/srt/entrypoints/openai/tool_server.py",
    "line": 80,
    "qualname": "ToolServer.get_tool_description",
    "signature": "def get_tool_description(self, tool_name: str)"
  },
  {
    "module": "srt.entrypoints.openai.tool_server",
    "file": "python/sglang/srt/entrypoints/openai/tool_server.py",
    "line": 84,
    "qualname": "ToolServer.get_tool_session",
    "signature": "def get_tool_session(self, tool_name: str)"
  },
  {
    "module": "srt.entrypoints.openai.tool_server",
    "file": "python/sglang/srt/entrypoints/openai/tool_server.py",
    "line": 89,
    "qualname": "MCPToolServer.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.entrypoints.openai.tool_server",
    "file": "python/sglang/srt/entrypoints/openai/tool_server.py",
    "line": 92,
    "qualname": "MCPToolServer.add_tool_server",
    "signature": "async def add_tool_server(self, server_url: str)"
  },
  {
    "module": "srt.entrypoints.openai.tool_server",
    "file": "python/sglang/srt/entrypoints/openai/tool_server.py",
    "line": 124,
    "qualname": "MCPToolServer.has_tool",
    "signature": "def has_tool(self, tool_name: str)"
  },
  {
    "module": "srt.entrypoints.openai.tool_server",
    "file": "python/sglang/srt/entrypoints/openai/tool_server.py",
    "line": 127,
    "qualname": "MCPToolServer.get_tool_description",
    "signature": "def get_tool_description(self, tool_name: str)"
  },
  {
    "module": "srt.entrypoints.openai.tool_server",
    "file": "python/sglang/srt/entrypoints/openai/tool_server.py",
    "line": 131,
    "qualname": "MCPToolServer.get_tool_session",
    "signature": "async def get_tool_session(self, tool_name: str)"
  },
  {
    "module": "srt.entrypoints.openai.tool_server",
    "file": "python/sglang/srt/entrypoints/openai/tool_server.py",
    "line": 145,
    "qualname": "DemoToolServer.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.entrypoints.openai.tool_server",
    "file": "python/sglang/srt/entrypoints/openai/tool_server.py",
    "line": 160,
    "qualname": "DemoToolServer.has_tool",
    "signature": "def has_tool(self, tool_name: str)"
  },
  {
    "module": "srt.entrypoints.openai.tool_server",
    "file": "python/sglang/srt/entrypoints/openai/tool_server.py",
    "line": 163,
    "qualname": "DemoToolServer.get_tool_description",
    "signature": "def get_tool_description(self, tool_name: str)"
  },
  {
    "module": "srt.entrypoints.openai.tool_server",
    "file": "python/sglang/srt/entrypoints/openai/tool_server.py",
    "line": 174,
    "qualname": "DemoToolServer.get_tool_session",
    "signature": "async def get_tool_session(self, tool_name: str)"
  },
  {
    "module": "srt.entrypoints.openai.usage_processor",
    "file": "python/sglang/srt/entrypoints/openai/usage_processor.py",
    "line": 18,
    "qualname": "UsageProcessor.calculate_response_usage",
    "signature": "def calculate_response_usage(responses: List[Dict[str, Any]], n_choices: int, enable_cache_report: bool)"
  },
  {
    "module": "srt.entrypoints.openai.usage_processor",
    "file": "python/sglang/srt/entrypoints/openai/usage_processor.py",
    "line": 44,
    "qualname": "UsageProcessor.calculate_streaming_usage",
    "signature": "def calculate_streaming_usage(prompt_tokens: Mapping[int, int], completion_tokens: Mapping[int, int], cached_tokens: Mapping[int, int], n_choices: int, enable_cache_report: bool)"
  },
  {
    "module": "srt.entrypoints.openai.usage_processor",
    "file": "python/sglang/srt/entrypoints/openai/usage_processor.py",
    "line": 70,
    "qualname": "UsageProcessor.calculate_token_usage",
    "signature": "def calculate_token_usage(prompt_tokens: int, completion_tokens: int, cached_tokens: Optional[Dict[str, int]])"
  },
  {
    "module": "srt.entrypoints.openai.utils",
    "file": "python/sglang/srt/entrypoints/openai/utils.py",
    "line": 13,
    "qualname": "to_openai_style_logprobs",
    "signature": "def to_openai_style_logprobs(input_token_logprobs, output_token_logprobs, input_top_logprobs, output_top_logprobs)"
  },
  {
    "module": "srt.entrypoints.openai.utils",
    "file": "python/sglang/srt/entrypoints/openai/utils.py",
    "line": 50,
    "qualname": "process_hidden_states_from_ret",
    "signature": "def process_hidden_states_from_ret(ret_item: Dict[str, Any], request: Union[ChatCompletionRequest, CompletionRequest])"
  },
  {
    "module": "srt.entrypoints.openai.protocol",
    "file": "python/sglang/srt/entrypoints/openai/protocol.py",
    "line": 234,
    "qualname": "CompletionRequest.validate_max_tokens_positive",
    "signature": "def validate_max_tokens_positive(cls, v)"
  },
  {
    "module": "srt.entrypoints.openai.protocol",
    "file": "python/sglang/srt/entrypoints/openai/protocol.py",
    "line": 455,
    "qualname": "ChatCompletionRequest.set_tool_choice_default",
    "signature": "def set_tool_choice_default(cls, values)"
  },
  {
    "module": "srt.entrypoints.openai.protocol",
    "file": "python/sglang/srt/entrypoints/openai/protocol.py",
    "line": 730,
    "qualname": "ResponsesRequest.to_sampling_params",
    "signature": "def to_sampling_params(self, default_max_tokens: int, default_params: Optional[Dict])"
  },
  {
    "module": "srt.entrypoints.openai.protocol",
    "file": "python/sglang/srt/entrypoints/openai/protocol.py",
    "line": 801,
    "qualname": "ResponsesResponse.from_request",
    "signature": "def from_request(cls, request: ResponsesRequest, sampling_params: Any, model_name: str, created_time: int, output: List[Union[ResponseOutputItem, ResponseReasoningItem, ResponseFunctionToolCall]], status: str, usage: Optional[UsageInfo])"
  },
  {
    "module": "srt.eplb.eplb_algorithms.__init__",
    "file": "python/sglang/srt/eplb/eplb_algorithms/__init__.py",
    "line": 17,
    "qualname": "rebalance_experts",
    "signature": "def rebalance_experts(tokens_per_expert: torch.Tensor, num_physical_experts: int, num_local_physical_experts: int, num_groups: Optional[int], num_nodes: int, algorithm: EplbAlgorithm)"
  },
  {
    "module": "srt.eplb.eplb_algorithms.__init__",
    "file": "python/sglang/srt/eplb/eplb_algorithms/__init__.py",
    "line": 51,
    "qualname": "compute_algorithm",
    "signature": "def compute_algorithm(raw_algorithm: str, num_groups: Optional[int], num_nodes: int)"
  },
  {
    "module": "srt.eplb.eplb_algorithms.deepseek",
    "file": "python/sglang/srt/eplb/eplb_algorithms/deepseek.py",
    "line": 9,
    "qualname": "balanced_packing",
    "signature": "def balanced_packing(weight: torch.Tensor, num_packs: int)"
  },
  {
    "module": "srt.eplb.eplb_algorithms.deepseek",
    "file": "python/sglang/srt/eplb/eplb_algorithms/deepseek.py",
    "line": 54,
    "qualname": "replicate_experts",
    "signature": "def replicate_experts(weight: torch.Tensor, num_phy: int)"
  },
  {
    "module": "srt.eplb.eplb_algorithms.deepseek",
    "file": "python/sglang/srt/eplb/eplb_algorithms/deepseek.py",
    "line": 85,
    "qualname": "rebalance_experts_hierarchical",
    "signature": "def rebalance_experts_hierarchical(weight: torch.Tensor, num_physical_experts: int, num_groups: int, num_nodes: int, num_gpus: int)"
  },
  {
    "module": "srt.eplb.eplb_algorithms.deepseek",
    "file": "python/sglang/srt/eplb/eplb_algorithms/deepseek.py",
    "line": 170,
    "qualname": "rebalance_experts",
    "signature": "def rebalance_experts(weight: torch.Tensor, num_replicas: int, num_groups: int, num_nodes: int, num_gpus: int, enable_hierarchical: bool)"
  },
  {
    "module": "srt.eplb.eplb_algorithms.deepseek_vec",
    "file": "python/sglang/srt/eplb/eplb_algorithms/deepseek_vec.py",
    "line": 7,
    "qualname": "pack_groups",
    "signature": "def pack_groups(tokens_per_group: torch.Tensor, num_nodes: int)"
  },
  {
    "module": "srt.eplb.eplb_algorithms.deepseek_vec",
    "file": "python/sglang/srt/eplb/eplb_algorithms/deepseek_vec.py",
    "line": 35,
    "qualname": "make_redundant_experts_chunkwise",
    "signature": "def make_redundant_experts_chunkwise(tokens_per_expert: torch.Tensor, num_physical_experts: int, num_local_physical_experts: int, num_physical_experts_per_chunk: int)"
  },
  {
    "module": "srt.eplb.eplb_algorithms.deepseek_vec",
    "file": "python/sglang/srt/eplb/eplb_algorithms/deepseek_vec.py",
    "line": 184,
    "qualname": "decode_rebalance_experts",
    "signature": "def decode_rebalance_experts(tokens_per_expert: torch.Tensor, num_physical_experts: int, num_local_physical_experts: int)"
  },
  {
    "module": "srt.eplb.eplb_algorithms.deepseek_vec",
    "file": "python/sglang/srt/eplb/eplb_algorithms/deepseek_vec.py",
    "line": 197,
    "qualname": "prefill_rebalance_experts",
    "signature": "def prefill_rebalance_experts(tokens_per_expert: torch.Tensor, num_physical_experts: int, num_local_physical_experts: int, num_groups: int, num_nodes: int)"
  },
  {
    "module": "srt.eplb.eplb_algorithms.deepseek_vec",
    "file": "python/sglang/srt/eplb/eplb_algorithms/deepseek_vec.py",
    "line": 255,
    "qualname": "rebalance_experts",
    "signature": "def rebalance_experts(tokens_per_expert: torch.Tensor, num_physical_experts: int, num_local_physical_experts: int, num_groups: Optional[int], num_nodes: int, enable_hierarchical: bool)"
  },
  {
    "module": "srt.eplb.eplb_simulator.reader",
    "file": "python/sglang/srt/eplb/eplb_simulator/reader.py",
    "line": 16,
    "qualname": "read_mode_per_pass",
    "signature": "def read_mode_per_pass(dir_data: Path)"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 65,
    "qualname": "AiterAttnBackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 157,
    "qualname": "AiterAttnBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 341,
    "qualname": "AiterAttnBackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 364,
    "qualname": "AiterAttnBackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 499,
    "qualname": "AiterAttnBackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 572,
    "qualname": "AiterAttnBackend.get_cuda_graph_seq_len_fill_value",
    "signature": "def get_cuda_graph_seq_len_fill_value(self)"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 575,
    "qualname": "AiterAttnBackend.forward_extend",
    "signature": "def forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 761,
    "qualname": "AiterAttnBackend.forward_decode",
    "signature": "def forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 830,
    "qualname": "AiterIndicesUpdaterPrefill.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, attn_backend: AttentionBackend)"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 855,
    "qualname": "AiterIndicesUpdaterPrefill.update",
    "signature": "def update(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, prefix_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], spec_info: Optional[SpecInfo])"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 867,
    "qualname": "AiterIndicesUpdaterPrefill.update_single_wrapper",
    "signature": "def update_single_wrapper(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, prefix_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], spec_info: Optional[SpecInfo])"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 935,
    "qualname": "AiterMlaIndicesUpdaterPrefill.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, attn_backend: AttentionBackend)"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 950,
    "qualname": "AiterMlaIndicesUpdaterPrefill.update",
    "signature": "def update(self, req_pool_indices: torch.Tensor, kv_lens: torch.Tensor, kv_lens_sum: int, extend_lens: torch.Tensor, max_q_len: int, max_kv_len: int, spec_info: Optional[SpecInfo])"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 963,
    "qualname": "AiterMlaIndicesUpdaterPrefill.update_single_wrapper",
    "signature": "def update_single_wrapper(self, req_pool_indices: torch.Tensor, kv_lens: torch.Tensor, kv_lens_sum: int, extend_lens: torch.Tensor, max_q_len: int, max_kv_len: int, spec_info: Optional[SpecInfo])"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 1022,
    "qualname": "AiterMultiStepDraftBackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 1061,
    "qualname": "AiterMultiStepDraftBackend.common_template",
    "signature": "def common_template(self, forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: int)"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 1093,
    "qualname": "AiterMultiStepDraftBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 1114,
    "qualname": "AiterMultiStepDraftBackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 1125,
    "qualname": "AiterMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.aiter_backend",
    "file": "python/sglang/srt/layers/attention/aiter_backend.py",
    "line": 1139,
    "qualname": "AiterMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)"
  },
  {
    "module": "srt.layers.attention.base_attn_backend",
    "file": "python/sglang/srt/layers/attention/base_attn_backend.py",
    "line": 18,
    "qualname": "AttentionBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.base_attn_backend",
    "file": "python/sglang/srt/layers/attention/base_attn_backend.py",
    "line": 22,
    "qualname": "AttentionBackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)"
  },
  {
    "module": "srt.layers.attention.base_attn_backend",
    "file": "python/sglang/srt/layers/attention/base_attn_backend.py",
    "line": 26,
    "qualname": "AttentionBackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.base_attn_backend",
    "file": "python/sglang/srt/layers/attention/base_attn_backend.py",
    "line": 39,
    "qualname": "AttentionBackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.base_attn_backend",
    "file": "python/sglang/srt/layers/attention/base_attn_backend.py",
    "line": 53,
    "qualname": "AttentionBackend.get_cuda_graph_seq_len_fill_value",
    "signature": "def get_cuda_graph_seq_len_fill_value(self)"
  },
  {
    "module": "srt.layers.attention.base_attn_backend",
    "file": "python/sglang/srt/layers/attention/base_attn_backend.py",
    "line": 57,
    "qualname": "AttentionBackend.forward",
    "signature": "def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)"
  },
  {
    "module": "srt.layers.attention.base_attn_backend",
    "file": "python/sglang/srt/layers/attention/base_attn_backend.py",
    "line": 91,
    "qualname": "AttentionBackend.forward_decode",
    "signature": "def forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)"
  },
  {
    "module": "srt.layers.attention.base_attn_backend",
    "file": "python/sglang/srt/layers/attention/base_attn_backend.py",
    "line": 103,
    "qualname": "AttentionBackend.forward_extend",
    "signature": "def forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)"
  },
  {
    "module": "srt.layers.attention.base_attn_backend",
    "file": "python/sglang/srt/layers/attention/base_attn_backend.py",
    "line": 115,
    "qualname": "AttentionBackend.support_triton",
    "signature": "def support_triton(self)"
  },
  {
    "module": "srt.layers.attention.cutlass_mla_backend",
    "file": "python/sglang/srt/layers/attention/cutlass_mla_backend.py",
    "line": 39,
    "qualname": "CutlassMLADecodeMetadata.__init__",
    "signature": "def __init__(self, workspace: Optional[torch.Tensor], block_kv_indices: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.cutlass_mla_backend",
    "file": "python/sglang/srt/layers/attention/cutlass_mla_backend.py",
    "line": 51,
    "qualname": "CutlassMLABackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], kv_last_page_len_buf: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.cutlass_mla_backend",
    "file": "python/sglang/srt/layers/attention/cutlass_mla_backend.py",
    "line": 82,
    "qualname": "CutlassMLABackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.cutlass_mla_backend",
    "file": "python/sglang/srt/layers/attention/cutlass_mla_backend.py",
    "line": 122,
    "qualname": "CutlassMLABackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, block_kv_indices: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.cutlass_mla_backend",
    "file": "python/sglang/srt/layers/attention/cutlass_mla_backend.py",
    "line": 146,
    "qualname": "CutlassMLABackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])"
  },
  {
    "module": "srt.layers.attention.cutlass_mla_backend",
    "file": "python/sglang/srt/layers/attention/cutlass_mla_backend.py",
    "line": 185,
    "qualname": "CutlassMLABackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.cutlass_mla_backend",
    "file": "python/sglang/srt/layers/attention/cutlass_mla_backend.py",
    "line": 223,
    "qualname": "CutlassMLABackend.get_cuda_graph_seq_len_fill_value",
    "signature": "def get_cuda_graph_seq_len_fill_value(self)"
  },
  {
    "module": "srt.layers.attention.cutlass_mla_backend",
    "file": "python/sglang/srt/layers/attention/cutlass_mla_backend.py",
    "line": 226,
    "qualname": "CutlassMLABackend.forward_decode",
    "signature": "def forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.double_sparsity_backend",
    "file": "python/sglang/srt/layers/attention/double_sparsity_backend.py",
    "line": 17,
    "qualname": "DoubleSparseAttnBackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner)"
  },
  {
    "module": "srt.layers.attention.double_sparsity_backend",
    "file": "python/sglang/srt/layers/attention/double_sparsity_backend.py",
    "line": 52,
    "qualname": "DoubleSparseAttnBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.double_sparsity_backend",
    "file": "python/sglang/srt/layers/attention/double_sparsity_backend.py",
    "line": 113,
    "qualname": "DoubleSparseAttnBackend.forward_extend",
    "signature": "def forward_extend(self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)"
  },
  {
    "module": "srt.layers.attention.double_sparsity_backend",
    "file": "python/sglang/srt/layers/attention/double_sparsity_backend.py",
    "line": 167,
    "qualname": "DoubleSparseAttnBackend.forward_decode",
    "signature": "def forward_decode(self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)"
  },
  {
    "module": "srt.layers.attention.dual_chunk_flashattention_backend",
    "file": "python/sglang/srt/layers/attention/dual_chunk_flashattention_backend.py",
    "line": 102,
    "qualname": "DualChunkFlashAttentionBackend.__init__",
    "signature": "def __init__(self, model_runner: 'ModelRunner')"
  },
  {
    "module": "srt.layers.attention.dual_chunk_flashattention_backend",
    "file": "python/sglang/srt/layers/attention/dual_chunk_flashattention_backend.py",
    "line": 160,
    "qualname": "DualChunkFlashAttentionBackend.get_sparse_attention_config",
    "signature": "def get_sparse_attention_config(self, layer_idx)"
  },
  {
    "module": "srt.layers.attention.dual_chunk_flashattention_backend",
    "file": "python/sglang/srt/layers/attention/dual_chunk_flashattention_backend.py",
    "line": 168,
    "qualname": "DualChunkFlashAttentionBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.dual_chunk_flashattention_backend",
    "file": "python/sglang/srt/layers/attention/dual_chunk_flashattention_backend.py",
    "line": 296,
    "qualname": "DualChunkFlashAttentionBackend.forward_extend",
    "signature": "def forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: 'RadixAttention', forward_batch: ForwardBatch, save_kv_cache)"
  },
  {
    "module": "srt.layers.attention.dual_chunk_flashattention_backend",
    "file": "python/sglang/srt/layers/attention/dual_chunk_flashattention_backend.py",
    "line": 409,
    "qualname": "DualChunkFlashAttentionBackend.forward_decode",
    "signature": "def forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: 'RadixAttention', forward_batch: ForwardBatch, save_kv_cache)"
  },
  {
    "module": "srt.layers.attention.dual_chunk_flashattention_backend",
    "file": "python/sglang/srt/layers/attention/dual_chunk_flashattention_backend.py",
    "line": 486,
    "qualname": "DualChunkFlashAttentionBackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)"
  },
  {
    "module": "srt.layers.attention.dual_chunk_flashattention_backend",
    "file": "python/sglang/srt/layers/attention/dual_chunk_flashattention_backend.py",
    "line": 532,
    "qualname": "DualChunkFlashAttentionBackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[None])"
  },
  {
    "module": "srt.layers.attention.dual_chunk_flashattention_backend",
    "file": "python/sglang/srt/layers/attention/dual_chunk_flashattention_backend.py",
    "line": 580,
    "qualname": "DualChunkFlashAttentionBackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[None], seq_lens_cpu: Optional[torch.Tensor], out_cache_loc: torch.Tensor)"
  },
  {
    "module": "srt.layers.attention.dual_chunk_flashattention_backend",
    "file": "python/sglang/srt/layers/attention/dual_chunk_flashattention_backend.py",
    "line": 670,
    "qualname": "DualChunkFlashAttentionBackend.get_cuda_graph_seq_len_fill_value",
    "signature": "def get_cuda_graph_seq_len_fill_value(self)"
  },
  {
    "module": "srt.layers.attention.flashattention_backend",
    "file": "python/sglang/srt/layers/attention/flashattention_backend.py",
    "line": 127,
    "qualname": "make_local_attention_virtual_batches",
    "signature": "def make_local_attention_virtual_batches(attn_chunk_size: int, query_start_loc_np: np.ndarray, seq_lens_np: np.ndarray, block_table: torch.Tensor, page_size: int)"
  },
  {
    "module": "srt.layers.attention.flashattention_backend",
    "file": "python/sglang/srt/layers/attention/flashattention_backend.py",
    "line": 272,
    "qualname": "cdiv",
    "signature": "def cdiv(a: int, b: int)"
  },
  {
    "module": "srt.layers.attention.flashattention_backend",
    "file": "python/sglang/srt/layers/attention/flashattention_backend.py",
    "line": 279,
    "qualname": "merge_state_v2_wrapper",
    "signature": "def merge_state_v2_wrapper(o, s_a, o_exp, s_b)"
  },
  {
    "module": "srt.layers.attention.flashattention_backend",
    "file": "python/sglang/srt/layers/attention/flashattention_backend.py",
    "line": 301,
    "qualname": "FlashAttentionBackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, skip_prefill: bool, speculative_step_id, topk, speculative_num_steps)"
  },
  {
    "module": "srt.layers.attention.flashattention_backend",
    "file": "python/sglang/srt/layers/attention/flashattention_backend.py",
    "line": 355,
    "qualname": "FlashAttentionBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.flashattention_backend",
    "file": "python/sglang/srt/layers/attention/flashattention_backend.py",
    "line": 639,
    "qualname": "FlashAttentionBackend.forward_extend",
    "signature": "def forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor], sinks: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.flashattention_backend",
    "file": "python/sglang/srt/layers/attention/flashattention_backend.py",
    "line": 929,
    "qualname": "FlashAttentionBackend.forward_decode",
    "signature": "def forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor], sinks: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.flashattention_backend",
    "file": "python/sglang/srt/layers/attention/flashattention_backend.py",
    "line": 1188,
    "qualname": "FlashAttentionBackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)"
  },
  {
    "module": "srt.layers.attention.flashattention_backend",
    "file": "python/sglang/srt/layers/attention/flashattention_backend.py",
    "line": 1448,
    "qualname": "FlashAttentionBackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.flashattention_backend",
    "file": "python/sglang/srt/layers/attention/flashattention_backend.py",
    "line": 1683,
    "qualname": "FlashAttentionBackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor], out_cache_loc: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.flashattention_backend",
    "file": "python/sglang/srt/layers/attention/flashattention_backend.py",
    "line": 1939,
    "qualname": "FlashAttentionBackend.get_cuda_graph_seq_len_fill_value",
    "signature": "def get_cuda_graph_seq_len_fill_value(self)"
  },
  {
    "module": "srt.layers.attention.flashattention_backend",
    "file": "python/sglang/srt/layers/attention/flashattention_backend.py",
    "line": 2237,
    "qualname": "prepare_swa_spec_page_table_triton",
    "signature": "def prepare_swa_spec_page_table_triton(page_table_dst: torch.Tensor, page_table_a: torch.Tensor, page_table_b: torch.Tensor, seq_len_a: torch.Tensor, seq_len_b: torch.Tensor, speculative_num_draft_tokens: int)"
  },
  {
    "module": "srt.layers.attention.flashattention_backend",
    "file": "python/sglang/srt/layers/attention/flashattention_backend.py",
    "line": 2279,
    "qualname": "FlashAttentionMultiStepBackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)"
  },
  {
    "module": "srt.layers.attention.flashattention_backend",
    "file": "python/sglang/srt/layers/attention/flashattention_backend.py",
    "line": 2296,
    "qualname": "FlashAttentionMultiStepBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.flashattention_backend",
    "file": "python/sglang/srt/layers/attention/flashattention_backend.py",
    "line": 2300,
    "qualname": "FlashAttentionMultiStepBackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)"
  },
  {
    "module": "srt.layers.attention.flashattention_backend",
    "file": "python/sglang/srt/layers/attention/flashattention_backend.py",
    "line": 2304,
    "qualname": "FlashAttentionMultiStepBackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.flashattention_backend",
    "file": "python/sglang/srt/layers/attention/flashattention_backend.py",
    "line": 2322,
    "qualname": "FlashAttentionMultiStepBackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)"
  },
  {
    "module": "srt.layers.attention.flashattention_backend",
    "file": "python/sglang/srt/layers/attention/flashattention_backend.py",
    "line": 2347,
    "qualname": "normal_decode_set_metadata",
    "signature": "def normal_decode_set_metadata(cache_seqlens_int32: torch.Tensor, cu_seqlens_k: torch.Tensor, page_table: torch.Tensor, req_to_token: torch.Tensor, req_pool_indices: torch.Tensor, strided_indices: torch.Tensor, max_seq_pages: torch.Tensor, seq_lens: torch.Tensor, seq_len_delta: int, page_size: int)"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 80,
    "qualname": "FlashInferAttnBackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], kv_last_page_len_buf: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 211,
    "qualname": "FlashInferAttnBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 278,
    "qualname": "FlashInferAttnBackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 312,
    "qualname": "FlashInferAttnBackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 417,
    "qualname": "FlashInferAttnBackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 465,
    "qualname": "FlashInferAttnBackend.get_cuda_graph_seq_len_fill_value",
    "signature": "def get_cuda_graph_seq_len_fill_value(self)"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 468,
    "qualname": "FlashInferAttnBackend.forward_extend",
    "signature": "def forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 552,
    "qualname": "FlashInferAttnBackend.forward_decode",
    "signature": "def forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 602,
    "qualname": "FlashInferIndicesUpdaterDecode.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, attn_backend: FlashInferAttnBackend)"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 631,
    "qualname": "FlashInferIndicesUpdaterDecode.update",
    "signature": "def update(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 644,
    "qualname": "FlashInferIndicesUpdaterDecode.update_single_wrapper",
    "signature": "def update_single_wrapper(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 666,
    "qualname": "FlashInferIndicesUpdaterDecode.update_sliding_window",
    "signature": "def update_sliding_window(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 714,
    "qualname": "FlashInferIndicesUpdaterDecode.update_cross_attention",
    "signature": "def update_cross_attention(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, decode_wrappers: List[BatchDecodeWithPagedKVCacheWrapper], encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 746,
    "qualname": "FlashInferIndicesUpdaterDecode.call_begin_forward",
    "signature": "def call_begin_forward(self, wrapper: BatchDecodeWithPagedKVCacheWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, kv_indptr: torch.Tensor, kv_start_idx: torch.Tensor, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor], use_sliding_window_kv_pool: bool)"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 818,
    "qualname": "FlashInferIndicesUpdaterPrefill.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, attn_backend: FlashInferAttnBackend)"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 849,
    "qualname": "FlashInferIndicesUpdaterPrefill.update",
    "signature": "def update(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 864,
    "qualname": "FlashInferIndicesUpdaterPrefill.update_single_wrapper",
    "signature": "def update_single_wrapper(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 900,
    "qualname": "FlashInferIndicesUpdaterPrefill.update_sliding_window",
    "signature": "def update_sliding_window(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 946,
    "qualname": "FlashInferIndicesUpdaterPrefill.update_cross_attention",
    "signature": "def update_cross_attention(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor], seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrappers: List[BatchPrefillWithPagedKVCacheWrapper], use_ragged: bool, encoder_lens: Optional[torch.Tensor], spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 985,
    "qualname": "FlashInferIndicesUpdaterPrefill.call_begin_forward",
    "signature": "def call_begin_forward(self, wrapper_ragged: BatchPrefillWithRaggedKVCacheWrapper, wrapper_paged: BatchPrefillWithPagedKVCacheWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, seq_lens: torch.Tensor, prefix_lens: torch.Tensor, kv_start_idx: torch.Tensor, kv_indptr: torch.Tensor, qo_indptr: torch.Tensor, use_ragged: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], use_sliding_window_kv_pool: bool)"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 1079,
    "qualname": "FlashInferMultiStepDraftBackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 1120,
    "qualname": "FlashInferMultiStepDraftBackend.common_template",
    "signature": "def common_template(self, forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: Callable)"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 1165,
    "qualname": "FlashInferMultiStepDraftBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 1186,
    "qualname": "FlashInferMultiStepDraftBackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 1198,
    "qualname": "FlashInferMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 1212,
    "qualname": "FlashInferMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 1230,
    "qualname": "should_use_tensor_core",
    "signature": "def should_use_tensor_core(kv_cache_dtype: torch.dtype, num_attention_heads: int, num_kv_heads: int)"
  },
  {
    "module": "srt.layers.attention.flashinfer_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_backend.py",
    "line": 1284,
    "qualname": "fast_decode_plan",
    "signature": "def fast_decode_plan(self, indptr: torch.Tensor, indices: torch.Tensor, last_page_len: torch.Tensor, num_qo_heads: int, num_kv_heads: int, head_dim: int, page_size: int, pos_encoding_mode: str, window_left: int, logits_soft_cap: Optional[float], q_data_type: Optional[Union[str, torch.dtype]], kv_data_type: Optional[Union[str, torch.dtype]], data_type: Optional[Union[str, torch.dtype]], sm_scale: Optional[float], rope_scale: Optional[float], rope_theta: Optional[float], non_blocking: bool)"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 68,
    "qualname": "FlashInferMhaChunkKVRunner.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, attn_backend: 'FlashInferMlaAttnBackend')"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 89,
    "qualname": "FlashInferMhaChunkKVRunner.update_prefix_chunks",
    "signature": "def update_prefix_chunks(self, num_prefix_chunks: int)"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 96,
    "qualname": "FlashInferMhaChunkKVRunner.update_wrapper",
    "signature": "def update_wrapper(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 142,
    "qualname": "FlashInferMhaChunkKVRunner.forward",
    "signature": "def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 179,
    "qualname": "FlashInferMLAAttnBackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], q_indptr_decode_buf: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 271,
    "qualname": "FlashInferMLAAttnBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 323,
    "qualname": "FlashInferMLAAttnBackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 354,
    "qualname": "FlashInferMLAAttnBackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 434,
    "qualname": "FlashInferMLAAttnBackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 491,
    "qualname": "FlashInferMLAAttnBackend.get_cuda_graph_seq_len_fill_value",
    "signature": "def get_cuda_graph_seq_len_fill_value(self)"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 494,
    "qualname": "FlashInferMLAAttnBackend.init_mha_chunk_metadata",
    "signature": "def init_mha_chunk_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 498,
    "qualname": "FlashInferMLAAttnBackend.forward_extend",
    "signature": "def forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 576,
    "qualname": "FlashInferMLAAttnBackend.forward_decode",
    "signature": "def forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 638,
    "qualname": "FlashInferMLAIndicesUpdaterDecode.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, attn_backend: AttentionBackend)"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 655,
    "qualname": "FlashInferMLAIndicesUpdaterDecode.update",
    "signature": "def update(self, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, decode_wrapper: BatchMLAPagedAttentionWrapper, init_metadata_replay: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 678,
    "qualname": "FlashInferMLAIndicesUpdaterDecode.call_begin_forward",
    "signature": "def call_begin_forward(self, wrapper: BatchMLAPagedAttentionWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, q_indptr: torch.Tensor, kv_indptr: torch.Tensor, init_metadata_replay: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 747,
    "qualname": "FlashInferMLAIndicesUpdaterPrefill.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, attn_backend: AttentionBackend)"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 767,
    "qualname": "FlashInferMLAIndicesUpdaterPrefill.update",
    "signature": "def update(self, req_pool_indices: torch.Tnesor, seq_lens: torch.Tensor, seq_lens_sum: int, prefix_lens: torch.Tensor, prefill_wrapper_paged: BatchMLAPagedAttentionWrapper, use_ragged: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 798,
    "qualname": "FlashInferMLAIndicesUpdaterPrefill.call_begin_forward",
    "signature": "def call_begin_forward(self, wrapper_ragged: BatchPrefillWithRaggedKVCacheWrapper, wrapper_paged: BatchMLAPagedAttentionWrapper, req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, seq_lens: torch.Tensor, prefix_lens: torch.Tensor, kv_indptr: torch.Tensor, qo_indptr: torch.Tensor, use_ragged: bool, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 887,
    "qualname": "FlashInferMLAMultiStepDraftBackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 933,
    "qualname": "FlashInferMLAMultiStepDraftBackend.common_template",
    "signature": "def common_template(self, forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: Callable)"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 971,
    "qualname": "FlashInferMLAMultiStepDraftBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 994,
    "qualname": "FlashInferMLAMultiStepDraftBackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 1006,
    "qualname": "FlashInferMLAMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 1020,
    "qualname": "FlashInferMLAMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)"
  },
  {
    "module": "srt.layers.attention.flashinfer_mla_backend",
    "file": "python/sglang/srt/layers/attention/flashinfer_mla_backend.py",
    "line": 1038,
    "qualname": "fast_mla_decode_plan",
    "signature": "def fast_mla_decode_plan(self, qo_indptr_cpu: torch.Tensor, kv_indptr_cpu: torch.Tensor, kv_indices: torch.Tensor, kv_len_arr_cpu: torch.Tensor, num_heads: int, head_dim_ckv: int, head_dim_kpe: int, page_size: int, causal: bool, sm_scale: float, q_data_type: torch.dtype, kv_data_type: torch.dtype)"
  },
  {
    "module": "srt.layers.attention.flashmla_backend",
    "file": "python/sglang/srt/layers/attention/flashmla_backend.py",
    "line": 37,
    "qualname": "FlashMLADecodeMetadata.__init__",
    "signature": "def __init__(self, flashmla_metadata: Optional[Tuple[torch.Tensor, torch.Tensor]], num_splits: Optional[torch.Tensor], block_kv_indices: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.flashmla_backend",
    "file": "python/sglang/srt/layers/attention/flashmla_backend.py",
    "line": 51,
    "qualname": "FlashMLABackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], kv_last_page_len_buf: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.flashmla_backend",
    "file": "python/sglang/srt/layers/attention/flashmla_backend.py",
    "line": 81,
    "qualname": "FlashMLABackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.flashmla_backend",
    "file": "python/sglang/srt/layers/attention/flashmla_backend.py",
    "line": 148,
    "qualname": "FlashMLABackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, block_kv_indices: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.flashmla_backend",
    "file": "python/sglang/srt/layers/attention/flashmla_backend.py",
    "line": 182,
    "qualname": "FlashMLABackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])"
  },
  {
    "module": "srt.layers.attention.flashmla_backend",
    "file": "python/sglang/srt/layers/attention/flashmla_backend.py",
    "line": 252,
    "qualname": "FlashMLABackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.flashmla_backend",
    "file": "python/sglang/srt/layers/attention/flashmla_backend.py",
    "line": 327,
    "qualname": "FlashMLABackend.get_cuda_graph_seq_len_fill_value",
    "signature": "def get_cuda_graph_seq_len_fill_value(self)"
  },
  {
    "module": "srt.layers.attention.flashmla_backend",
    "file": "python/sglang/srt/layers/attention/flashmla_backend.py",
    "line": 330,
    "qualname": "FlashMLABackend.forward_decode",
    "signature": "def forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)"
  },
  {
    "module": "srt.layers.attention.flashmla_backend",
    "file": "python/sglang/srt/layers/attention/flashmla_backend.py",
    "line": 387,
    "qualname": "FlashMLABackend.forward_extend",
    "signature": "def forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)"
  },
  {
    "module": "srt.layers.attention.flashmla_backend",
    "file": "python/sglang/srt/layers/attention/flashmla_backend.py",
    "line": 456,
    "qualname": "FlashMLAMultiStepDraftBackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)"
  },
  {
    "module": "srt.layers.attention.flashmla_backend",
    "file": "python/sglang/srt/layers/attention/flashmla_backend.py",
    "line": 489,
    "qualname": "FlashMLAMultiStepDraftBackend.common_template",
    "signature": "def common_template(self, forward_batch: ForwardBatch, call_fn: Callable)"
  },
  {
    "module": "srt.layers.attention.flashmla_backend",
    "file": "python/sglang/srt/layers/attention/flashmla_backend.py",
    "line": 499,
    "qualname": "FlashMLAMultiStepDraftBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.flashmla_backend",
    "file": "python/sglang/srt/layers/attention/flashmla_backend.py",
    "line": 506,
    "qualname": "FlashMLAMultiStepDraftBackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)"
  },
  {
    "module": "srt.layers.attention.flashmla_backend",
    "file": "python/sglang/srt/layers/attention/flashmla_backend.py",
    "line": 512,
    "qualname": "FlashMLAMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.flashmla_backend",
    "file": "python/sglang/srt/layers/attention/flashmla_backend.py",
    "line": 526,
    "qualname": "FlashMLAMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)"
  },
  {
    "module": "srt.layers.attention.hybrid_attn_backend",
    "file": "python/sglang/srt/layers/attention/hybrid_attn_backend.py",
    "line": 15,
    "qualname": "HybridAttnBackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, prefill_backend: AttentionBackend, decode_backend: AttentionBackend)"
  },
  {
    "module": "srt.layers.attention.hybrid_attn_backend",
    "file": "python/sglang/srt/layers/attention/hybrid_attn_backend.py",
    "line": 25,
    "qualname": "HybridAttnBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.hybrid_attn_backend",
    "file": "python/sglang/srt/layers/attention/hybrid_attn_backend.py",
    "line": 31,
    "qualname": "HybridAttnBackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)"
  },
  {
    "module": "srt.layers.attention.hybrid_attn_backend",
    "file": "python/sglang/srt/layers/attention/hybrid_attn_backend.py",
    "line": 38,
    "qualname": "HybridAttnBackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.hybrid_attn_backend",
    "file": "python/sglang/srt/layers/attention/hybrid_attn_backend.py",
    "line": 69,
    "qualname": "HybridAttnBackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.hybrid_attn_backend",
    "file": "python/sglang/srt/layers/attention/hybrid_attn_backend.py",
    "line": 103,
    "qualname": "HybridAttnBackend.get_cuda_graph_seq_len_fill_value",
    "signature": "def get_cuda_graph_seq_len_fill_value(self)"
  },
  {
    "module": "srt.layers.attention.hybrid_attn_backend",
    "file": "python/sglang/srt/layers/attention/hybrid_attn_backend.py",
    "line": 106,
    "qualname": "HybridAttnBackend.forward_decode",
    "signature": "def forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)"
  },
  {
    "module": "srt.layers.attention.hybrid_attn_backend",
    "file": "python/sglang/srt/layers/attention/hybrid_attn_backend.py",
    "line": 120,
    "qualname": "HybridAttnBackend.forward_extend",
    "signature": "def forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)"
  },
  {
    "module": "srt.layers.attention.intel_amx_backend",
    "file": "python/sglang/srt/layers/attention/intel_amx_backend.py",
    "line": 16,
    "qualname": "IntelAMXAttnBackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner)"
  },
  {
    "module": "srt.layers.attention.intel_amx_backend",
    "file": "python/sglang/srt/layers/attention/intel_amx_backend.py",
    "line": 32,
    "qualname": "IntelAMXAttnBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.intel_amx_backend",
    "file": "python/sglang/srt/layers/attention/intel_amx_backend.py",
    "line": 52,
    "qualname": "IntelAMXAttnBackend.forward_extend",
    "signature": "def forward_extend(self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)"
  },
  {
    "module": "srt.layers.attention.intel_amx_backend",
    "file": "python/sglang/srt/layers/attention/intel_amx_backend.py",
    "line": 91,
    "qualname": "IntelAMXAttnBackend.forward_decode",
    "signature": "def forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)"
  },
  {
    "module": "srt.layers.attention.intel_amx_backend",
    "file": "python/sglang/srt/layers/attention/intel_amx_backend.py",
    "line": 127,
    "qualname": "IntelAMXAttnBackend.support_triton",
    "signature": "def support_triton(self)"
  },
  {
    "module": "srt.layers.attention.merge_state",
    "file": "python/sglang/srt/layers/attention/merge_state.py",
    "line": 26,
    "qualname": "merge_state",
    "signature": "def merge_state(prefix_output: torch.Tensor, prefix_lse: torch.Tensor, suffix_output: torch.Tensor, suffix_lse: torch.Tensor, output: Optional[torch.Tensor], output_lse: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.tbo_backend",
    "file": "python/sglang/srt/layers/attention/tbo_backend.py",
    "line": 14,
    "qualname": "TboAttnBackend.__init__",
    "signature": "def __init__(self, primary: AttentionBackend, children: List[AttentionBackend])"
  },
  {
    "module": "srt.layers.attention.tbo_backend",
    "file": "python/sglang/srt/layers/attention/tbo_backend.py",
    "line": 20,
    "qualname": "TboAttnBackend.init_new",
    "signature": "def init_new(cls, creator: Callable[[], AttentionBackend])"
  },
  {
    "module": "srt.layers.attention.tbo_backend",
    "file": "python/sglang/srt/layers/attention/tbo_backend.py",
    "line": 26,
    "qualname": "TboAttnBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: 'ForwardBatch')"
  },
  {
    "module": "srt.layers.attention.tbo_backend",
    "file": "python/sglang/srt/layers/attention/tbo_backend.py",
    "line": 35,
    "qualname": "TboAttnBackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)"
  },
  {
    "module": "srt.layers.attention.tbo_backend",
    "file": "python/sglang/srt/layers/attention/tbo_backend.py",
    "line": 41,
    "qualname": "TboAttnBackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: 'ForwardMode', spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.tbo_backend",
    "file": "python/sglang/srt/layers/attention/tbo_backend.py",
    "line": 72,
    "qualname": "TboAttnBackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: 'ForwardMode', spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.tbo_backend",
    "file": "python/sglang/srt/layers/attention/tbo_backend.py",
    "line": 176,
    "qualname": "TboAttnBackend.get_cuda_graph_seq_len_fill_value",
    "signature": "def get_cuda_graph_seq_len_fill_value(self)"
  },
  {
    "module": "srt.layers.attention.tbo_backend",
    "file": "python/sglang/srt/layers/attention/tbo_backend.py",
    "line": 182,
    "qualname": "TboAttnBackend.forward_extend",
    "signature": "def forward_extend(self)"
  },
  {
    "module": "srt.layers.attention.tbo_backend",
    "file": "python/sglang/srt/layers/attention/tbo_backend.py",
    "line": 185,
    "qualname": "TboAttnBackend.forward_decode",
    "signature": "def forward_decode(self)"
  },
  {
    "module": "srt.layers.attention.torch_native_backend",
    "file": "python/sglang/srt/layers/attention/torch_native_backend.py",
    "line": 18,
    "qualname": "TorchNativeAttnBackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner)"
  },
  {
    "module": "srt.layers.attention.torch_native_backend",
    "file": "python/sglang/srt/layers/attention/torch_native_backend.py",
    "line": 23,
    "qualname": "TorchNativeAttnBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.torch_native_backend",
    "file": "python/sglang/srt/layers/attention/torch_native_backend.py",
    "line": 182,
    "qualname": "TorchNativeAttnBackend.forward_extend",
    "signature": "def forward_extend(self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)"
  },
  {
    "module": "srt.layers.attention.torch_native_backend",
    "file": "python/sglang/srt/layers/attention/torch_native_backend.py",
    "line": 226,
    "qualname": "TorchNativeAttnBackend.forward_decode",
    "signature": "def forward_decode(self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)"
  },
  {
    "module": "srt.layers.attention.torch_native_backend",
    "file": "python/sglang/srt/layers/attention/torch_native_backend.py",
    "line": 269,
    "qualname": "TorchNativeAttnBackend.support_triton",
    "signature": "def support_triton(self)"
  },
  {
    "module": "srt.layers.attention.triton_backend",
    "file": "python/sglang/srt/layers/attention/triton_backend.py",
    "line": 23,
    "qualname": "logit_capping_mod",
    "signature": "def logit_capping_mod(logit_capping_method, logit_cap)"
  },
  {
    "module": "srt.layers.attention.triton_backend",
    "file": "python/sglang/srt/layers/attention/triton_backend.py",
    "line": 50,
    "qualname": "TritonAttnBackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.triton_backend",
    "file": "python/sglang/srt/layers/attention/triton_backend.py",
    "line": 131,
    "qualname": "TritonAttnBackend.get_num_kv_splits",
    "signature": "def get_num_kv_splits(self, num_kv_splits: torch.Tensor, seq_lens: torch.Tensor)"
  },
  {
    "module": "srt.layers.attention.triton_backend",
    "file": "python/sglang/srt/layers/attention/triton_backend.py",
    "line": 167,
    "qualname": "TritonAttnBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.triton_backend",
    "file": "python/sglang/srt/layers/attention/triton_backend.py",
    "line": 369,
    "qualname": "TritonAttnBackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.triton_backend",
    "file": "python/sglang/srt/layers/attention/triton_backend.py",
    "line": 427,
    "qualname": "TritonAttnBackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.triton_backend",
    "file": "python/sglang/srt/layers/attention/triton_backend.py",
    "line": 583,
    "qualname": "TritonAttnBackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.triton_backend",
    "file": "python/sglang/srt/layers/attention/triton_backend.py",
    "line": 705,
    "qualname": "TritonAttnBackend.get_cuda_graph_seq_len_fill_value",
    "signature": "def get_cuda_graph_seq_len_fill_value(self)"
  },
  {
    "module": "srt.layers.attention.triton_backend",
    "file": "python/sglang/srt/layers/attention/triton_backend.py",
    "line": 708,
    "qualname": "TritonAttnBackend.forward_extend",
    "signature": "def forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache, sinks)"
  },
  {
    "module": "srt.layers.attention.triton_backend",
    "file": "python/sglang/srt/layers/attention/triton_backend.py",
    "line": 771,
    "qualname": "TritonAttnBackend.forward_decode",
    "signature": "def forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache, sinks)"
  },
  {
    "module": "srt.layers.attention.triton_backend",
    "file": "python/sglang/srt/layers/attention/triton_backend.py",
    "line": 830,
    "qualname": "TritonMultiStepDraftBackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)"
  },
  {
    "module": "srt.layers.attention.triton_backend",
    "file": "python/sglang/srt/layers/attention/triton_backend.py",
    "line": 868,
    "qualname": "TritonMultiStepDraftBackend.common_template",
    "signature": "def common_template(self, forward_batch: ForwardBatch, kv_indices_buffer: torch.Tensor, call_fn: int)"
  },
  {
    "module": "srt.layers.attention.triton_backend",
    "file": "python/sglang/srt/layers/attention/triton_backend.py",
    "line": 900,
    "qualname": "TritonMultiStepDraftBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.triton_backend",
    "file": "python/sglang/srt/layers/attention/triton_backend.py",
    "line": 921,
    "qualname": "TritonMultiStepDraftBackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)"
  },
  {
    "module": "srt.layers.attention.triton_backend",
    "file": "python/sglang/srt/layers/attention/triton_backend.py",
    "line": 932,
    "qualname": "TritonMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.triton_backend",
    "file": "python/sglang/srt/layers/attention/triton_backend.py",
    "line": 946,
    "qualname": "TritonMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)"
  },
  {
    "module": "srt.layers.attention.triton_backend",
    "file": "python/sglang/srt/layers/attention/triton_backend.py",
    "line": 965,
    "qualname": "get_num_kv_splits_triton",
    "signature": "def get_num_kv_splits_triton(num_kv_splits_ptr, seq_lens_ptr, num_seq, num_group, num_head, num_kv_head, max_kv_splits, device_core_count, MAX_NUM_SEQ: tl.constexpr)"
  },
  {
    "module": "srt.layers.attention.triton_backend",
    "file": "python/sglang/srt/layers/attention/triton_backend.py",
    "line": 1016,
    "qualname": "update_sliding_window_buffer",
    "signature": "def update_sliding_window_buffer(window_kv_indptr, req_to_token, sliding_window_size, seq_lens, req_pool_indices, bs, device, token_to_kv_pool_allocator)"
  },
  {
    "module": "srt.layers.attention.triton_backend",
    "file": "python/sglang/srt/layers/attention/triton_backend.py",
    "line": 1056,
    "qualname": "update_sliding_window_buffer_cuda_graph",
    "signature": "def update_sliding_window_buffer_cuda_graph(window_kv_indptr, window_kv_indices, req_to_token, sliding_window_size, seq_lens, req_pool_indices, bs, token_to_kv_pool_allocator)"
  },
  {
    "module": "srt.layers.attention.trtllm_mha_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mha_backend.py",
    "line": 58,
    "qualname": "TRTLLMHAAttnBackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], kv_last_page_len_buf: Optional[torch.Tensor], speculative_step_id: int)"
  },
  {
    "module": "srt.layers.attention.trtllm_mha_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mha_backend.py",
    "line": 111,
    "qualname": "TRTLLMHAAttnBackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.trtllm_mha_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mha_backend.py",
    "line": 196,
    "qualname": "TRTLLMHAAttnBackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])"
  },
  {
    "module": "srt.layers.attention.trtllm_mha_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mha_backend.py",
    "line": 309,
    "qualname": "TRTLLMHAAttnBackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.trtllm_mha_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mha_backend.py",
    "line": 411,
    "qualname": "TRTLLMHAAttnBackend.get_cuda_graph_seq_len_fill_value",
    "signature": "def get_cuda_graph_seq_len_fill_value(self)"
  },
  {
    "module": "srt.layers.attention.trtllm_mha_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mha_backend.py",
    "line": 415,
    "qualname": "TRTLLMHAAttnBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.trtllm_mha_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mha_backend.py",
    "line": 517,
    "qualname": "TRTLLMHAAttnBackend.forward_decode",
    "signature": "def forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool)"
  },
  {
    "module": "srt.layers.attention.trtllm_mha_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mha_backend.py",
    "line": 576,
    "qualname": "TRTLLMHAAttnBackend.forward_extend",
    "signature": "def forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)"
  },
  {
    "module": "srt.layers.attention.trtllm_mha_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mha_backend.py",
    "line": 638,
    "qualname": "TRTLLMHAAttnMultiStepDraftBackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, topk: int, speculative_num_steps: int)"
  },
  {
    "module": "srt.layers.attention.trtllm_mha_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mha_backend.py",
    "line": 651,
    "qualname": "TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.trtllm_mha_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mha_backend.py",
    "line": 655,
    "qualname": "TRTLLMHAAttnMultiStepDraftBackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)"
  },
  {
    "module": "srt.layers.attention.trtllm_mha_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mha_backend.py",
    "line": 659,
    "qualname": "TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.trtllm_mha_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mha_backend.py",
    "line": 677,
    "qualname": "TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, forward_batch: ForwardBatch, bs: int)"
  },
  {
    "module": "srt.layers.attention.utils",
    "file": "python/sglang/srt/layers/attention/utils.py",
    "line": 11,
    "qualname": "create_flashinfer_kv_indices_triton",
    "signature": "def create_flashinfer_kv_indices_triton(req_to_token_ptr, req_pool_indices_ptr, page_kernel_lens_ptr, kv_indptr, kv_start_idx, kv_indices_ptr, req_to_token_ptr_stride: tl.constexpr)"
  },
  {
    "module": "srt.layers.attention.utils",
    "file": "python/sglang/srt/layers/attention/utils.py",
    "line": 50,
    "qualname": "create_flashmla_kv_indices_triton",
    "signature": "def create_flashmla_kv_indices_triton(req_to_token_ptr, req_pool_indices_ptr, page_kernel_lens_ptr, kv_start_idx, kv_indices_ptr, req_to_token_ptr_stride: tl.constexpr, kv_indices_ptr_stride: tl.constexpr, NUM_PAGE_PER_BLOCK: tl.constexpr, PAGED_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.attention.vision",
    "file": "python/sglang/srt/layers/attention/vision.py",
    "line": 55,
    "qualname": "SingletonCache.set_data",
    "signature": "def set_data(self, value: Any)"
  },
  {
    "module": "srt.layers.attention.vision",
    "file": "python/sglang/srt/layers/attention/vision.py",
    "line": 58,
    "qualname": "SingletonCache.get_data",
    "signature": "def get_data(self)"
  },
  {
    "module": "srt.layers.attention.vision",
    "file": "python/sglang/srt/layers/attention/vision.py",
    "line": 61,
    "qualname": "SingletonCache.empty",
    "signature": "def empty(self)"
  },
  {
    "module": "srt.layers.attention.vision",
    "file": "python/sglang/srt/layers/attention/vision.py",
    "line": 88,
    "qualname": "VisionSdpaAttention.__init__",
    "signature": "def __init__(self, head_dim: int, num_heads: int, num_kv_heads: int, dropout: float, flatten_batch: bool, softmax_in_single_precision: bool)"
  },
  {
    "module": "srt.layers.attention.vision",
    "file": "python/sglang/srt/layers/attention/vision.py",
    "line": 141,
    "qualname": "VisionSdpaAttention.generate_patch_attention_mask",
    "signature": "def generate_patch_attention_mask(self, s: int, cu_seqlens: Optional[torch.Tensor], flatten_batch: bool)"
  },
  {
    "module": "srt.layers.attention.vision",
    "file": "python/sglang/srt/layers/attention/vision.py",
    "line": 163,
    "qualname": "VisionSdpaAttention.forward",
    "signature": "def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, bsz: int, cu_seqlens: Optional[torch.Tensor], attention_mask: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.vision",
    "file": "python/sglang/srt/layers/attention/vision.py",
    "line": 240,
    "qualname": "VisionTritonAttention.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.layers.attention.vision",
    "file": "python/sglang/srt/layers/attention/vision.py",
    "line": 246,
    "qualname": "VisionTritonAttention.forward",
    "signature": "def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens: Optional[torch.Tensor], bsz: int, seq_len: int)"
  },
  {
    "module": "srt.layers.attention.vision",
    "file": "python/sglang/srt/layers/attention/vision.py",
    "line": 284,
    "qualname": "VisionFlash3Attention.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.layers.attention.vision",
    "file": "python/sglang/srt/layers/attention/vision.py",
    "line": 292,
    "qualname": "VisionFlash3Attention.forward",
    "signature": "def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens: Optional[Union[SingletonCache, torch.Tensor]], bsz: int, seq_len: int)"
  },
  {
    "module": "srt.layers.attention.vision",
    "file": "python/sglang/srt/layers/attention/vision.py",
    "line": 354,
    "qualname": "VisionAttention.__init__",
    "signature": "def __init__(self, embed_dim: int, num_heads: int, projection_size: int, use_qkv_parallel: bool, qkv_backend: Optional[str], quant_config: Optional[QuantizationConfig], dropout: float, softmax_in_single_precision: bool, flatten_batch: bool, prefix: str, proj_bias: bool, num_dummy_heads: int, qkv_bias: bool, qk_normalization: bool, layer_norm_eps: float, customized_position_embedding_applier: Callable[[torch.Tensor, torch.Tensor, Any, Any], Tuple[torch.Tensor, torch.Tensor]])"
  },
  {
    "module": "srt.layers.attention.vision",
    "file": "python/sglang/srt/layers/attention/vision.py",
    "line": 509,
    "qualname": "VisionAttention.forward",
    "signature": "def forward(self, x: torch.Tensor, cu_seqlens: Optional[torch.Tensor], position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]], attention_mask: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.vision_utils",
    "file": "python/sglang/srt/layers/attention/vision_utils.py",
    "line": 8,
    "qualname": "update_vit_attn_dummy_heads_config",
    "signature": "def update_vit_attn_dummy_heads_config(config)"
  },
  {
    "module": "srt.layers.attention.vision_utils",
    "file": "python/sglang/srt/layers/attention/vision_utils.py",
    "line": 26,
    "qualname": "pad_vit_attn_dummy_heads",
    "signature": "def pad_vit_attn_dummy_heads(config, name: str, loaded_weight: torch.Tensor)"
  },
  {
    "module": "srt.layers.attention.wave_backend",
    "file": "python/sglang/srt/layers/attention/wave_backend.py",
    "line": 26,
    "qualname": "get_num_kv_splits_triton",
    "signature": "def get_num_kv_splits_triton(num_kv_splits_ptr, seq_lens_ptr, num_seq, num_group, num_head, num_kv_head, max_kv_splits, device_core_count, MAX_NUM_SEQ: tl.constexpr)"
  },
  {
    "module": "srt.layers.attention.wave_backend",
    "file": "python/sglang/srt/layers/attention/wave_backend.py",
    "line": 91,
    "qualname": "WaveAttnBackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.wave_backend",
    "file": "python/sglang/srt/layers/attention/wave_backend.py",
    "line": 162,
    "qualname": "WaveAttnBackend.get_num_kv_splits",
    "signature": "def get_num_kv_splits(self, num_kv_splits: torch.Tensor, seq_lens: torch.Tensor)"
  },
  {
    "module": "srt.layers.attention.wave_backend",
    "file": "python/sglang/srt/layers/attention/wave_backend.py",
    "line": 195,
    "qualname": "WaveAttnBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.wave_backend",
    "file": "python/sglang/srt/layers/attention/wave_backend.py",
    "line": 344,
    "qualname": "WaveAttnBackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.wave_backend",
    "file": "python/sglang/srt/layers/attention/wave_backend.py",
    "line": 388,
    "qualname": "WaveAttnBackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.wave_backend",
    "file": "python/sglang/srt/layers/attention/wave_backend.py",
    "line": 472,
    "qualname": "WaveAttnBackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.wave_backend",
    "file": "python/sglang/srt/layers/attention/wave_backend.py",
    "line": 540,
    "qualname": "WaveAttnBackend.get_cuda_graph_seq_len_fill_value",
    "signature": "def get_cuda_graph_seq_len_fill_value(self)"
  },
  {
    "module": "srt.layers.attention.wave_backend",
    "file": "python/sglang/srt/layers/attention/wave_backend.py",
    "line": 543,
    "qualname": "WaveAttnBackend.forward_extend",
    "signature": "def forward_extend(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)"
  },
  {
    "module": "srt.layers.attention.wave_backend",
    "file": "python/sglang/srt/layers/attention/wave_backend.py",
    "line": 589,
    "qualname": "WaveAttnBackend.forward_decode",
    "signature": "def forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)"
  },
  {
    "module": "srt.layers.attention.ascend_backend",
    "file": "python/sglang/srt/layers/attention/ascend_backend.py",
    "line": 40,
    "qualname": "AscendAttnBackend.gen_attention_mask",
    "signature": "def gen_attention_mask(self, max_seq_len: int, dtype)"
  },
  {
    "module": "srt.layers.attention.ascend_backend",
    "file": "python/sglang/srt/layers/attention/ascend_backend.py",
    "line": 58,
    "qualname": "AscendAttnBackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner)"
  },
  {
    "module": "srt.layers.attention.ascend_backend",
    "file": "python/sglang/srt/layers/attention/ascend_backend.py",
    "line": 84,
    "qualname": "AscendAttnBackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.ascend_backend",
    "file": "python/sglang/srt/layers/attention/ascend_backend.py",
    "line": 105,
    "qualname": "AscendAttnBackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int)"
  },
  {
    "module": "srt.layers.attention.ascend_backend",
    "file": "python/sglang/srt/layers/attention/ascend_backend.py",
    "line": 114,
    "qualname": "AscendAttnBackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])"
  },
  {
    "module": "srt.layers.attention.ascend_backend",
    "file": "python/sglang/srt/layers/attention/ascend_backend.py",
    "line": 134,
    "qualname": "AscendAttnBackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]], seq_lens_cpu: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.ascend_backend",
    "file": "python/sglang/srt/layers/attention/ascend_backend.py",
    "line": 160,
    "qualname": "AscendAttnBackend.get_cuda_graph_seq_len_fill_value",
    "signature": "def get_cuda_graph_seq_len_fill_value(self)"
  },
  {
    "module": "srt.layers.attention.ascend_backend",
    "file": "python/sglang/srt/layers/attention/ascend_backend.py",
    "line": 163,
    "qualname": "AscendAttnBackend.forward_extend",
    "signature": "def forward_extend(self, q, k, v, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache)"
  },
  {
    "module": "srt.layers.attention.ascend_backend",
    "file": "python/sglang/srt/layers/attention/ascend_backend.py",
    "line": 256,
    "qualname": "AscendAttnBackend.forward_decode",
    "signature": "def forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.trtllm_mla_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mla_backend.py",
    "line": 59,
    "qualname": "TRTLLMMLABackend.__init__",
    "signature": "def __init__(self, model_runner: ModelRunner, skip_prefill: bool, kv_indptr_buf: Optional[torch.Tensor], q_indptr_decode_buf: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.trtllm_mla_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mla_backend.py",
    "line": 166,
    "qualname": "TRTLLMMLABackend.init_cuda_graph_state",
    "signature": "def init_cuda_graph_state(self, max_bs: int, max_num_tokens: int, kv_indices_buf: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.trtllm_mla_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mla_backend.py",
    "line": 185,
    "qualname": "TRTLLMMLABackend.init_forward_metadata_capture_cuda_graph",
    "signature": "def init_forward_metadata_capture_cuda_graph(self, bs: int, num_tokens: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo])"
  },
  {
    "module": "srt.layers.attention.trtllm_mla_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mla_backend.py",
    "line": 231,
    "qualname": "TRTLLMMLABackend.init_forward_metadata_replay_cuda_graph",
    "signature": "def init_forward_metadata_replay_cuda_graph(self, bs: int, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, seq_lens_sum: int, encoder_lens: Optional[torch.Tensor], forward_mode: ForwardMode, spec_info: Optional[SpecInfo], seq_lens_cpu: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.trtllm_mla_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mla_backend.py",
    "line": 271,
    "qualname": "TRTLLMMLABackend.get_cuda_graph_seq_len_fill_value",
    "signature": "def get_cuda_graph_seq_len_fill_value(self)"
  },
  {
    "module": "srt.layers.attention.trtllm_mla_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mla_backend.py",
    "line": 275,
    "qualname": "TRTLLMMLABackend.init_forward_metadata",
    "signature": "def init_forward_metadata(self, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.attention.trtllm_mla_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mla_backend.py",
    "line": 303,
    "qualname": "TRTLLMMLABackend.quantize_and_rope_for_fp8",
    "signature": "def quantize_and_rope_for_fp8(self, q_nope: torch.Tensor, q_rope: torch.Tensor, k_nope: torch.Tensor, k_rope: torch.Tensor, forward_batch: ForwardBatch, cos_sin_cache: torch.Tensor, is_neox: bool)"
  },
  {
    "module": "srt.layers.attention.trtllm_mla_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mla_backend.py",
    "line": 381,
    "qualname": "TRTLLMMLABackend.forward_decode",
    "signature": "def forward_decode(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, layer: RadixAttention, forward_batch: ForwardBatch, save_kv_cache: bool, q_rope: Optional[torch.Tensor], k_rope: Optional[torch.Tensor], cos_sin_cache: Optional[torch.Tensor], is_neox: Optional[bool])"
  },
  {
    "module": "srt.layers.attention.trtllm_mla_backend",
    "file": "python/sglang/srt/layers/attention/trtllm_mla_backend.py",
    "line": 488,
    "qualname": "TRTLLMMLAMultiStepDraftBackend.__init__",
    "signature": "def __init__(self, model_runner: 'ModelRunner', topk: int, speculative_num_steps: int)"
  },
  {
    "module": "srt.layers.moe.cutlass_moe",
    "file": "python/sglang/srt/layers/moe/cutlass_moe.py",
    "line": 21,
    "qualname": "cutlass_fused_experts_fp8",
    "signature": "def cutlass_fused_experts_fp8(a: torch.Tensor, w1_q: torch.Tensor, w2_q: torch.Tensor, w1_scale: torch.Tensor, w2_scale: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, a1_strides: torch.Tensor, c1_strides: torch.Tensor, a2_strides: torch.Tensor, c2_strides: torch.Tensor, workspace: torch.Tensor, a_ptrs: torch.Tensor, b_ptrs: torch.Tensor, out_ptrs: torch.Tensor, a_scales_ptrs: torch.Tensor, b_scales_ptrs: torch.Tensor, expert_offsets: torch.Tensor, problem_sizes1: torch.Tensor, problem_sizes2: torch.Tensor, use_fp8_blockscale: bool)"
  },
  {
    "module": "srt.layers.moe.cutlass_moe",
    "file": "python/sglang/srt/layers/moe/cutlass_moe.py",
    "line": 214,
    "qualname": "cutlass_moe_fp4",
    "signature": "def cutlass_moe_fp4(a: torch.Tensor, a1_gscale: torch.Tensor, w1_fp4: torch.Tensor, w1_blockscale: torch.Tensor, w1_alphas: torch.Tensor, a2_gscale: torch.Tensor, w2_fp4: torch.Tensor, w2_blockscale: torch.Tensor, w2_alphas: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, params: CutlassMoEParams, apply_router_weight_on_input: bool)"
  },
  {
    "module": "srt.layers.moe.cutlass_moe_params",
    "file": "python/sglang/srt/layers/moe/cutlass_moe_params.py",
    "line": 90,
    "qualname": "CutlassMoEParams.__init__",
    "signature": "def __init__(self, cutlass_moe_type: CutlassMoEType, device: torch.device, num_experts: int, intermediate_size_per_partition: int, hidden_size: int)"
  },
  {
    "module": "srt.layers.moe.cutlass_moe_params",
    "file": "python/sglang/srt/layers/moe/cutlass_moe_params.py",
    "line": 143,
    "qualname": "CutlassMoEParams.to_gemm1_args",
    "signature": "def to_gemm1_args(self)"
  },
  {
    "module": "srt.layers.moe.cutlass_moe_params",
    "file": "python/sglang/srt/layers/moe/cutlass_moe_params.py",
    "line": 157,
    "qualname": "CutlassMoEParams.to_gemm2_args",
    "signature": "def to_gemm2_args(self)"
  },
  {
    "module": "srt.layers.moe.cutlass_w4a8_moe",
    "file": "python/sglang/srt/layers/moe/cutlass_w4a8_moe.py",
    "line": 20,
    "qualname": "cutlass_w4a8_moe",
    "signature": "def cutlass_w4a8_moe(start_expert_id: int, end_expert_id: int, total_num_experts: int, a: torch.Tensor, w1_q: torch.Tensor, w2_q: torch.Tensor, w1_scale: torch.Tensor, w2_scale: torch.Tensor, topk_weights: torch.Tensor, topk_ids_: torch.Tensor, local_topk_ids: torch.Tensor, a_strides1: torch.Tensor, b_strides1: torch.Tensor, c_strides1: torch.Tensor, a_strides2: torch.Tensor, b_strides2: torch.Tensor, c_strides2: torch.Tensor, s_strides13: torch.Tensor, s_strides2: torch.Tensor, expert_offsets: torch.Tensor, problem_sizes1: torch.Tensor, problem_sizes2: torch.Tensor, a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], apply_router_weight_on_input: bool)"
  },
  {
    "module": "srt.layers.moe.fused_moe_native",
    "file": "python/sglang/srt/layers/moe/fused_moe_native.py",
    "line": 14,
    "qualname": "fused_moe_forward_native",
    "signature": "def fused_moe_forward_native(layer: torch.nn.Module, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.moe.fused_moe_native",
    "file": "python/sglang/srt/layers/moe/fused_moe_native.py",
    "line": 41,
    "qualname": "moe_forward_native",
    "signature": "def moe_forward_native(layer: torch.nn.Module, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.moe.rocm_moe_utils",
    "file": "python/sglang/srt/layers/moe/rocm_moe_utils.py",
    "line": 23,
    "qualname": "rocm_aiter_asm_moe_tkw1_impl",
    "signature": "def rocm_aiter_asm_moe_tkw1_impl(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, fc1_scale: Optional[torch.Tensor], fc2_scale: Optional[torch.Tensor], fc1_smooth_scale: Optional[torch.Tensor], fc2_smooth_scale: Optional[torch.Tensor], a16: bool, per_tensor_quant_scale: Optional[torch.Tensor], expert_mask: Optional[torch.Tensor], activation_method: int)"
  },
  {
    "module": "srt.layers.moe.rocm_moe_utils",
    "file": "python/sglang/srt/layers/moe/rocm_moe_utils.py",
    "line": 61,
    "qualname": "rocm_aiter_asm_moe_tkw1_fake",
    "signature": "def rocm_aiter_asm_moe_tkw1_fake(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, fc1_scale: Optional[torch.Tensor], fc2_scale: Optional[torch.Tensor], fc1_smooth_scale: Optional[torch.Tensor], fc2_smooth_scale: Optional[torch.Tensor], a16: bool, per_tensor_quant_scale: Optional[torch.Tensor], expert_mask: Optional[torch.Tensor], activation_method: int)"
  },
  {
    "module": "srt.layers.moe.rocm_moe_utils",
    "file": "python/sglang/srt/layers/moe/rocm_moe_utils.py",
    "line": 89,
    "qualname": "rocm_fused_experts_tkw1",
    "signature": "def rocm_fused_experts_tkw1(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[list[int]])"
  },
  {
    "module": "srt.layers.moe.router",
    "file": "python/sglang/srt/layers/moe/router.py",
    "line": 14,
    "qualname": "fused_moe_router_kernel",
    "signature": "def fused_moe_router_kernel(input_ptr, moe_router_weight_ptr, topk_weights_ptr, topk_ids_ptr, correction_bias_ptr, is_correction_bias: tl.constexpr, num_experts: tl.constexpr, topk: tl.constexpr, moe_softcapping: tl.constexpr, moe_renormalize: tl.constexpr, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.moe.router",
    "file": "python/sglang/srt/layers/moe/router.py",
    "line": 117,
    "qualname": "fused_moe_router_impl",
    "signature": "def fused_moe_router_impl(x: torch.Tensor, router_weight: torch.Tensor, topk: int, moe_softcapping: float, correction_bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.moe.router",
    "file": "python/sglang/srt/layers/moe/router.py",
    "line": 160,
    "qualname": "fused_moe_router_large_bs_kernel",
    "signature": "def fused_moe_router_large_bs_kernel(a_ptr, b_ptr, topk_weights_ptr, topk_ids_ptr, bs, num_experts: tl.constexpr, topk: tl.constexpr, moe_softcapping: tl.constexpr, moe_renormalize: tl.constexpr, K: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, stride_am: tl.constexpr, stride_bn: tl.constexpr)"
  },
  {
    "module": "srt.layers.moe.router",
    "file": "python/sglang/srt/layers/moe/router.py",
    "line": 269,
    "qualname": "fused_moe_router_large_bs_impl",
    "signature": "def fused_moe_router_large_bs_impl(x: torch.Tensor, router_weight: torch.Tensor, topk: int, moe_softcapping: float, BLOCK_SIZE_M: int, BLOCK_SIZE_N: int, BLOCK_SIZE_K: int)"
  },
  {
    "module": "srt.layers.moe.router",
    "file": "python/sglang/srt/layers/moe/router.py",
    "line": 312,
    "qualname": "fused_moe_router_shim",
    "signature": "def fused_moe_router_shim(moe_softcapping, hidden_states, gating_output, topk, renormalize, correction_bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.moe.router",
    "file": "python/sglang/srt/layers/moe/router.py",
    "line": 356,
    "qualname": "FusedMoeRouter.__init__",
    "signature": "def __init__(self, router_linear, topk, moe_softcapping)"
  },
  {
    "module": "srt.layers.moe.router",
    "file": "python/sglang/srt/layers/moe/router.py",
    "line": 361,
    "qualname": "FusedMoeRouter.__call__",
    "signature": "def __call__(self)"
  },
  {
    "module": "srt.layers.moe.router",
    "file": "python/sglang/srt/layers/moe/router.py",
    "line": 364,
    "qualname": "FusedMoeRouter.forward",
    "signature": "def forward(self, x: torch.Tensor, residual: torch.Tensor)"
  },
  {
    "module": "srt.layers.moe.router",
    "file": "python/sglang/srt/layers/moe/router.py",
    "line": 372,
    "qualname": "FusedMoeRouter.forward_cuda",
    "signature": "def forward_cuda(self, x: torch.Tensor, autotune)"
  },
  {
    "module": "srt.layers.moe.router",
    "file": "python/sglang/srt/layers/moe/router.py",
    "line": 383,
    "qualname": "FusedMoeRouter.forward_vllm",
    "signature": "def forward_vllm(self, x: torch.Tensor)"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 35,
    "qualname": "MoeA2ABackend.is_none",
    "signature": "def is_none(self)"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 38,
    "qualname": "MoeA2ABackend.is_deepep",
    "signature": "def is_deepep(self)"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 51,
    "qualname": "MoeRunnerBackend.is_auto",
    "signature": "def is_auto(self)"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 54,
    "qualname": "MoeRunnerBackend.is_triton",
    "signature": "def is_triton(self)"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 57,
    "qualname": "MoeRunnerBackend.is_triton_kernel",
    "signature": "def is_triton_kernel(self)"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 60,
    "qualname": "MoeRunnerBackend.is_flashinfer_trtllm",
    "signature": "def is_flashinfer_trtllm(self)"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 63,
    "qualname": "MoeRunnerBackend.is_flashinfer_cutlass",
    "signature": "def is_flashinfer_cutlass(self)"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 66,
    "qualname": "MoeRunnerBackend.is_flashinfer_mxfp4",
    "signature": "def is_flashinfer_mxfp4(self)"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 76,
    "qualname": "DeepEPMode.enable_normal",
    "signature": "def enable_normal(self)"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 79,
    "qualname": "DeepEPMode.enable_low_latency",
    "signature": "def enable_low_latency(self)"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 82,
    "qualname": "DeepEPMode.resolve",
    "signature": "def resolve(self, is_extend_in_batch: bool)"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 91,
    "qualname": "DeepEPMode.is_normal",
    "signature": "def is_normal(self)"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 94,
    "qualname": "DeepEPMode.is_low_latency",
    "signature": "def is_low_latency(self)"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 97,
    "qualname": "DeepEPMode.is_auto",
    "signature": "def is_auto(self)"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 110,
    "qualname": "initialize_moe_config",
    "signature": "def initialize_moe_config(server_args: ServerArgs)"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 130,
    "qualname": "get_moe_a2a_backend",
    "signature": "def get_moe_a2a_backend()"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 138,
    "qualname": "get_moe_runner_backend",
    "signature": "def get_moe_runner_backend()"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 146,
    "qualname": "get_deepep_mode",
    "signature": "def get_deepep_mode()"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 154,
    "qualname": "get_deepep_config",
    "signature": "def get_deepep_config()"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 162,
    "qualname": "is_tbo_enabled",
    "signature": "def is_tbo_enabled()"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 170,
    "qualname": "get_tbo_token_distribution_threshold",
    "signature": "def get_tbo_token_distribution_threshold()"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 181,
    "qualname": "should_use_flashinfer_trtllm_moe",
    "signature": "def should_use_flashinfer_trtllm_moe()"
  },
  {
    "module": "srt.layers.moe.utils",
    "file": "python/sglang/srt/layers/moe/utils.py",
    "line": 191,
    "qualname": "should_use_flashinfer_cutlass_moe_fp4_allgather",
    "signature": "def should_use_flashinfer_cutlass_moe_fp4_allgather()"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 105,
    "qualname": "TopKOutputChecker.format_is_standard",
    "signature": "def format_is_standard(topk_output: TopKOutput)"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 109,
    "qualname": "TopKOutputChecker.format_is_triton_kernel",
    "signature": "def format_is_triton_kernel(topk_output: TopKOutput)"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 115,
    "qualname": "TopKOutputChecker.format_is_bypassed",
    "signature": "def format_is_bypassed(topk_output: TopKOutput)"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 124,
    "qualname": "TopKOutputFormat.is_standard",
    "signature": "def is_standard(self)"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 127,
    "qualname": "TopKOutputFormat.is_triton_kernel",
    "signature": "def is_triton_kernel(self)"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 130,
    "qualname": "TopKOutputFormat.is_bypassed",
    "signature": "def is_bypassed(self)"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 139,
    "qualname": "TopKOutput.format",
    "signature": "def format(self)"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 152,
    "qualname": "StandardTopKOutput.format",
    "signature": "def format(self)"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 164,
    "qualname": "TritonKernelTopKOutput.format",
    "signature": "def format(self)"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 178,
    "qualname": "BypassedTopKOutput.format",
    "signature": "def format(self)"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 187,
    "qualname": "TopK.__init__",
    "signature": "def __init__(self, top_k: int)"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 226,
    "qualname": "TopK.forward_native",
    "signature": "def forward_native(self, hidden_states: torch.Tensor, router_logits: torch.Tensor)"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 243,
    "qualname": "TopK.forward_cuda",
    "signature": "def forward_cuda(self, hidden_states: torch.Tensor, router_logits: torch.Tensor)"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 280,
    "qualname": "TopK.forward_cpu",
    "signature": "def forward_cpu(self, hidden_states: torch.Tensor, router_logits: torch.Tensor)"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 296,
    "qualname": "TopK.forward_npu",
    "signature": "def forward_npu(self, hidden_states: torch.Tensor, router_logits: torch.Tensor)"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 334,
    "qualname": "TopK.empty_topk_output",
    "signature": "def empty_topk_output(self, device: torch.device)"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 345,
    "qualname": "fused_topk_torch_native",
    "signature": "def fused_topk_torch_native(hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool)"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 366,
    "qualname": "fused_topk_cpu",
    "signature": "def fused_topk_cpu(hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool, num_token_non_padded: Optional[torch.Tensor], expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo])"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 385,
    "qualname": "apply_topk_weights_cpu",
    "signature": "def apply_topk_weights_cpu(need_apply, topk_weights, inputs)"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 398,
    "qualname": "fused_topk",
    "signature": "def fused_topk(hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool, num_token_non_padded: Optional[torch.Tensor], expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo])"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 429,
    "qualname": "grouped_topk_gpu",
    "signature": "def grouped_topk_gpu(hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool, num_expert_group: Optional[int], topk_group: Optional[int], num_fused_shared_experts: int, routed_scaling_factor: Optional[float], num_token_non_padded: Optional[torch.Tensor], expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo], apply_routed_scaling_factor_on_output: Optional[bool])"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 497,
    "qualname": "grouped_topk_cpu",
    "signature": "def grouped_topk_cpu(hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool, num_expert_group: Optional[int], topk_group: Optional[int], num_fused_shared_experts: int, routed_scaling_factor: Optional[float], num_token_non_padded: Optional[torch.Tensor], expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo], apply_routed_scaling_factor_on_output: Optional[bool])"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 526,
    "qualname": "biased_grouped_topk_impl",
    "signature": "def biased_grouped_topk_impl(hidden_states: torch.Tensor, gating_output: torch.Tensor, correction_bias: torch.Tensor, topk: int, renormalize: bool, num_expert_group: Optional[int], topk_group: Optional[int], num_fused_shared_experts: int, routed_scaling_factor: Optional[float], num_token_non_padded: Optional[torch.Tensor], expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo], apply_routed_scaling_factor_on_output: Optional[bool])"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 599,
    "qualname": "is_power_of_two",
    "signature": "def is_power_of_two(n)"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 622,
    "qualname": "biased_grouped_topk_gpu",
    "signature": "def biased_grouped_topk_gpu(hidden_states: torch.Tensor, gating_output: torch.Tensor, correction_bias: torch.Tensor, topk: int, renormalize: bool, num_expert_group: Optional[int], topk_group: Optional[int], num_fused_shared_experts: int, routed_scaling_factor: Optional[float], num_token_non_padded: Optional[torch.Tensor], expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo], apply_routed_scaling_factor_on_output: Optional[bool])"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 701,
    "qualname": "biased_grouped_topk_cpu",
    "signature": "def biased_grouped_topk_cpu(hidden_states: torch.Tensor, gating_output: torch.Tensor, correction_bias: torch.Tensor, topk: int, renormalize: bool, num_expert_group: Optional[int], topk_group: Optional[int], compiled: bool, num_fused_shared_experts: int, routed_scaling_factor: Optional[float], num_token_non_padded: Optional[torch.Tensor], expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo], apply_routed_scaling_factor_on_output: Optional[bool])"
  },
  {
    "module": "srt.layers.moe.topk",
    "file": "python/sglang/srt/layers/moe/topk.py",
    "line": 743,
    "qualname": "select_experts",
    "signature": "def select_experts(hidden_states: torch.Tensor, router_logits: torch.Tensor, topk_config: TopKConfig)"
  },
  {
    "module": "srt.layers.quantization.__init__",
    "file": "python/sglang/srt/layers/quantization/__init__.py",
    "line": 34,
    "qualname": "DummyConfig.override_quantization_method",
    "signature": "def override_quantization_method(self)"
  },
  {
    "module": "srt.layers.quantization.__init__",
    "file": "python/sglang/srt/layers/quantization/__init__.py",
    "line": 124,
    "qualname": "get_quantization_config",
    "signature": "def get_quantization_config(quantization: str)"
  },
  {
    "module": "srt.layers.quantization.__init__",
    "file": "python/sglang/srt/layers/quantization/__init__.py",
    "line": 143,
    "qualname": "monkey_patch_isinstance_for_vllm_base_layer",
    "signature": "def monkey_patch_isinstance_for_vllm_base_layer(reverse: bool)"
  },
  {
    "module": "srt.layers.quantization.__init__",
    "file": "python/sglang/srt/layers/quantization/__init__.py",
    "line": 179,
    "qualname": "monkey_patch_moe_apply",
    "signature": "def monkey_patch_moe_apply(class_obj: 'FusedMoEMethodBase')"
  },
  {
    "module": "srt.layers.quantization.__init__",
    "file": "python/sglang/srt/layers/quantization/__init__.py",
    "line": 215,
    "qualname": "monkey_patch_quant_configs",
    "signature": "def monkey_patch_quant_configs()"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 67,
    "qualname": "is_layer_skipped_awq",
    "signature": "def is_layer_skipped_awq(prefix: str, modules_to_not_convert: List[str])"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 77,
    "qualname": "AWQConfig.__init__",
    "signature": "def __init__(self, weight_bits: int, group_size: int, zero_point: bool, modules_to_not_convert: Optional[List[str]])"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 97,
    "qualname": "AWQConfig.__repr__",
    "signature": "def __repr__(self)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 105,
    "qualname": "AWQConfig.get_scaled_act_names",
    "signature": "def get_scaled_act_names(self)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 108,
    "qualname": "AWQConfig.get_name",
    "signature": "def get_name(self)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 111,
    "qualname": "AWQConfig.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(self)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 115,
    "qualname": "AWQConfig.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 120,
    "qualname": "AWQConfig.get_config_filenames",
    "signature": "def get_config_filenames()"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 128,
    "qualname": "AWQConfig.from_config",
    "signature": "def from_config(cls, config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 137,
    "qualname": "AWQConfig.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 158,
    "qualname": "AWQMarlinConfig.__init__",
    "signature": "def __init__(self, weight_bits: int, group_size: int, zero_point: bool, lm_head_quantized: bool, modules_to_not_convert: Optional[list[str]], full_config: dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 188,
    "qualname": "AWQMarlinConfig.__repr__",
    "signature": "def __repr__(self)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 197,
    "qualname": "AWQMarlinConfig.get_scaled_act_names",
    "signature": "def get_scaled_act_names(self)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 201,
    "qualname": "AWQMarlinConfig.get_name",
    "signature": "def get_name(cls)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 205,
    "qualname": "AWQMarlinConfig.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(cls)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 209,
    "qualname": "AWQMarlinConfig.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 213,
    "qualname": "AWQMarlinConfig.get_config_filenames",
    "signature": "def get_config_filenames(cls)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 217,
    "qualname": "AWQMarlinConfig.from_config",
    "signature": "def from_config(cls, config: dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 235,
    "qualname": "AWQMarlinConfig.override_quantization_method",
    "signature": "def override_quantization_method(cls, hf_quant_cfg, user_quant)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 258,
    "qualname": "AWQMarlinConfig.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 294,
    "qualname": "AWQMarlinConfig.is_awq_marlin_compatible",
    "signature": "def is_awq_marlin_compatible(cls, quant_config: dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 326,
    "qualname": "AWQLinearMethod.__init__",
    "signature": "def __init__(self, quant_config: AWQConfig)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 329,
    "qualname": "AWQLinearMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 396,
    "qualname": "AWQLinearMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 401,
    "qualname": "AWQLinearMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 428,
    "qualname": "AWQMarlinLinearMethod.__init__",
    "signature": "def __init__(self, quant_config: AWQMarlinConfig)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 431,
    "qualname": "AWQMarlinLinearMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 509,
    "qualname": "AWQMarlinLinearMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 549,
    "qualname": "AWQMarlinLinearMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 572,
    "qualname": "AWQMoEMethod.__init__",
    "signature": "def __init__(self, quant_config: AWQMarlinConfig)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 578,
    "qualname": "AWQMoEMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 674,
    "qualname": "AWQMoEMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.awq",
    "file": "python/sglang/srt/layers/quantization/awq.py",
    "line": 739,
    "qualname": "AWQMoEMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.awq_triton",
    "file": "python/sglang/srt/layers/quantization/awq_triton.py",
    "line": 14,
    "qualname": "awq_dequantize_kernel",
    "signature": "def awq_dequantize_kernel(qweight_ptr, scales_ptr, zeros_ptr, group_size, result_ptr, num_cols, num_rows, BLOCK_SIZE_X: tl.constexpr, BLOCK_SIZE_Y: tl.constexpr)"
  },
  {
    "module": "srt.layers.quantization.awq_triton",
    "file": "python/sglang/srt/layers/quantization/awq_triton.py",
    "line": 111,
    "qualname": "awq_gemm_kernel",
    "signature": "def awq_gemm_kernel(a_ptr, b_ptr, c_ptr, zeros_ptr, scales_ptr, M, N, K, group_size, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, SPLIT_K: tl.constexpr)"
  },
  {
    "module": "srt.layers.quantization.awq_triton",
    "file": "python/sglang/srt/layers/quantization/awq_triton.py",
    "line": 235,
    "qualname": "awq_dequantize_triton",
    "signature": "def awq_dequantize_triton(qweight: torch.Tensor, scales: torch.Tensor, zeros: torch.Tensor, block_size_x: int, block_size_y: int)"
  },
  {
    "module": "srt.layers.quantization.awq_triton",
    "file": "python/sglang/srt/layers/quantization/awq_triton.py",
    "line": 289,
    "qualname": "awq_gemm_triton",
    "signature": "def awq_gemm_triton(input: torch.Tensor, qweight: torch.Tensor, scales: torch.Tensor, qzeros: torch.Tensor, split_k_iters: int, block_size_m: int, block_size_n: int, block_size_k: int)"
  },
  {
    "module": "srt.layers.quantization.base_config",
    "file": "python/sglang/srt/layers/quantization/base_config.py",
    "line": 20,
    "qualname": "QuantizeMethodBase.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.base_config",
    "file": "python/sglang/srt/layers/quantization/base_config.py",
    "line": 29,
    "qualname": "QuantizeMethodBase.apply",
    "signature": "def apply(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.base_config",
    "file": "python/sglang/srt/layers/quantization/base_config.py",
    "line": 35,
    "qualname": "QuantizeMethodBase.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: nn.Module)"
  },
  {
    "module": "srt.layers.quantization.base_config",
    "file": "python/sglang/srt/layers/quantization/base_config.py",
    "line": 47,
    "qualname": "LinearMethodBase.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.base_config",
    "file": "python/sglang/srt/layers/quantization/base_config.py",
    "line": 73,
    "qualname": "LinearMethodBase.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.base_config",
    "file": "python/sglang/srt/layers/quantization/base_config.py",
    "line": 87,
    "qualname": "FusedMoEMethodBase.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.base_config",
    "file": "python/sglang/srt/layers/quantization/base_config.py",
    "line": 99,
    "qualname": "FusedMoEMethodBase.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.base_config",
    "file": "python/sglang/srt/layers/quantization/base_config.py",
    "line": 112,
    "qualname": "QuantizationConfig.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.layers.quantization.base_config",
    "file": "python/sglang/srt/layers/quantization/base_config.py",
    "line": 118,
    "qualname": "QuantizationConfig.get_name",
    "signature": "def get_name(self)"
  },
  {
    "module": "srt.layers.quantization.base_config",
    "file": "python/sglang/srt/layers/quantization/base_config.py",
    "line": 123,
    "qualname": "QuantizationConfig.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(self)"
  },
  {
    "module": "srt.layers.quantization.base_config",
    "file": "python/sglang/srt/layers/quantization/base_config.py",
    "line": 129,
    "qualname": "QuantizationConfig.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.base_config",
    "file": "python/sglang/srt/layers/quantization/base_config.py",
    "line": 140,
    "qualname": "QuantizationConfig.get_config_filenames",
    "signature": "def get_config_filenames()"
  },
  {
    "module": "srt.layers.quantization.base_config",
    "file": "python/sglang/srt/layers/quantization/base_config.py",
    "line": 146,
    "qualname": "QuantizationConfig.from_config",
    "signature": "def from_config(cls, config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.base_config",
    "file": "python/sglang/srt/layers/quantization/base_config.py",
    "line": 151,
    "qualname": "QuantizationConfig.override_quantization_method",
    "signature": "def override_quantization_method(cls, hf_quant_cfg, user_quant)"
  },
  {
    "module": "srt.layers.quantization.base_config",
    "file": "python/sglang/srt/layers/quantization/base_config.py",
    "line": 161,
    "qualname": "QuantizationConfig.get_from_keys",
    "signature": "def get_from_keys(config: Dict[str, Any], keys: List[str])"
  },
  {
    "module": "srt.layers.quantization.base_config",
    "file": "python/sglang/srt/layers/quantization/base_config.py",
    "line": 171,
    "qualname": "QuantizationConfig.get_from_keys_or",
    "signature": "def get_from_keys_or(config: Dict[str, Any], keys: List[str], default: Any)"
  },
  {
    "module": "srt.layers.quantization.base_config",
    "file": "python/sglang/srt/layers/quantization/base_config.py",
    "line": 179,
    "qualname": "QuantizationConfig.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.base_config",
    "file": "python/sglang/srt/layers/quantization/base_config.py",
    "line": 194,
    "qualname": "QuantizationConfig.get_scaled_act_names",
    "signature": "def get_scaled_act_names(self)"
  },
  {
    "module": "srt.layers.quantization.base_config",
    "file": "python/sglang/srt/layers/quantization/base_config.py",
    "line": 202,
    "qualname": "method_has_implemented_embedding",
    "signature": "def method_has_implemented_embedding(method_class: Type[QuantizeMethodBase])"
  },
  {
    "module": "srt.layers.quantization.blockwise_int8",
    "file": "python/sglang/srt/layers/quantization/blockwise_int8.py",
    "line": 36,
    "qualname": "BlockInt8Config.__init__",
    "signature": "def __init__(self, is_checkpoint_int8_serialized: bool, activation_scheme: str, ignored_layers: Optional[List[str]], weight_block_size: List[int])"
  },
  {
    "module": "srt.layers.quantization.blockwise_int8",
    "file": "python/sglang/srt/layers/quantization/blockwise_int8.py",
    "line": 69,
    "qualname": "BlockInt8Config.get_name",
    "signature": "def get_name(cls)"
  },
  {
    "module": "srt.layers.quantization.blockwise_int8",
    "file": "python/sglang/srt/layers/quantization/blockwise_int8.py",
    "line": 73,
    "qualname": "BlockInt8Config.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(cls)"
  },
  {
    "module": "srt.layers.quantization.blockwise_int8",
    "file": "python/sglang/srt/layers/quantization/blockwise_int8.py",
    "line": 77,
    "qualname": "BlockInt8Config.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.blockwise_int8",
    "file": "python/sglang/srt/layers/quantization/blockwise_int8.py",
    "line": 81,
    "qualname": "BlockInt8Config.get_config_filenames",
    "signature": "def get_config_filenames(cls)"
  },
  {
    "module": "srt.layers.quantization.blockwise_int8",
    "file": "python/sglang/srt/layers/quantization/blockwise_int8.py",
    "line": 85,
    "qualname": "BlockInt8Config.from_config",
    "signature": "def from_config(cls, config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.blockwise_int8",
    "file": "python/sglang/srt/layers/quantization/blockwise_int8.py",
    "line": 98,
    "qualname": "BlockInt8Config.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.blockwise_int8",
    "file": "python/sglang/srt/layers/quantization/blockwise_int8.py",
    "line": 112,
    "qualname": "BlockInt8Config.get_scaled_act_names",
    "signature": "def get_scaled_act_names(self)"
  },
  {
    "module": "srt.layers.quantization.blockwise_int8",
    "file": "python/sglang/srt/layers/quantization/blockwise_int8.py",
    "line": 128,
    "qualname": "BlockInt8LinearMethod.__init__",
    "signature": "def __init__(self, quant_config: BlockInt8Config)"
  },
  {
    "module": "srt.layers.quantization.blockwise_int8",
    "file": "python/sglang/srt/layers/quantization/blockwise_int8.py",
    "line": 133,
    "qualname": "BlockInt8LinearMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.blockwise_int8",
    "file": "python/sglang/srt/layers/quantization/blockwise_int8.py",
    "line": 214,
    "qualname": "BlockInt8LinearMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: Module)"
  },
  {
    "module": "srt.layers.quantization.blockwise_int8",
    "file": "python/sglang/srt/layers/quantization/blockwise_int8.py",
    "line": 222,
    "qualname": "BlockInt8LinearMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.blockwise_int8",
    "file": "python/sglang/srt/layers/quantization/blockwise_int8.py",
    "line": 250,
    "qualname": "BlockInt8MoEMethod.__init__",
    "signature": "def __init__(self, quant_config: BlockInt8Config)"
  },
  {
    "module": "srt.layers.quantization.blockwise_int8",
    "file": "python/sglang/srt/layers/quantization/blockwise_int8.py",
    "line": 255,
    "qualname": "BlockInt8MoEMethod.create_weights",
    "signature": "def create_weights(self, layer: Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.blockwise_int8",
    "file": "python/sglang/srt/layers/quantization/blockwise_int8.py",
    "line": 343,
    "qualname": "BlockInt8MoEMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: Module)"
  },
  {
    "module": "srt.layers.quantization.blockwise_int8",
    "file": "python/sglang/srt/layers/quantization/blockwise_int8.py",
    "line": 347,
    "qualname": "BlockInt8MoEMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 23,
    "qualname": "dummy_func",
    "signature": "def dummy_func()"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 113,
    "qualname": "Fp8Config.__init__",
    "signature": "def __init__(self, is_checkpoint_fp8_serialized: bool, activation_scheme: str, ignored_layers: Optional[List[str]], weight_block_size: List[int])"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 143,
    "qualname": "Fp8Config.get_name",
    "signature": "def get_name(cls)"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 147,
    "qualname": "Fp8Config.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(cls)"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 151,
    "qualname": "Fp8Config.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 155,
    "qualname": "Fp8Config.get_config_filenames",
    "signature": "def get_config_filenames(cls)"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 159,
    "qualname": "Fp8Config.from_config",
    "signature": "def from_config(cls, config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 172,
    "qualname": "Fp8Config.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 186,
    "qualname": "Fp8Config.get_scaled_act_names",
    "signature": "def get_scaled_act_names(self)"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 208,
    "qualname": "Fp8LinearMethod.__init__",
    "signature": "def __init__(self, quant_config: Union[Fp8Config, W4AFp8Config])"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 224,
    "qualname": "Fp8LinearMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 332,
    "qualname": "Fp8LinearMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: Module)"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 442,
    "qualname": "Fp8LinearMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 491,
    "qualname": "get_tile_tokens_dim",
    "signature": "def get_tile_tokens_dim(num_tokens, top_k, num_experts)"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 514,
    "qualname": "Fp8MoEMethod.__init__",
    "signature": "def __init__(self, quant_config: Fp8Config)"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 525,
    "qualname": "Fp8MoEMethod.create_weights",
    "signature": "def create_weights(self, layer: Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 749,
    "qualname": "Fp8MoEMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: Module)"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 911,
    "qualname": "Fp8MoEMethod.process_weights_hip_int4",
    "signature": "def process_weights_hip_int4(self, layer: Module)"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 954,
    "qualname": "Fp8MoEMethod.process_weights_hip_scale_padding",
    "signature": "def process_weights_hip_scale_padding(self, layer: Module)"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 987,
    "qualname": "Fp8MoEMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 1082,
    "qualname": "Fp8MoEMethod.apply_with_router_logits",
    "signature": "def apply_with_router_logits(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 1141,
    "qualname": "Fp8MoEMethod.maybe_apply_hip_fused_experts",
    "signature": "def maybe_apply_hip_fused_experts(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, activation: str, no_combine: bool)"
  },
  {
    "module": "srt.layers.quantization.fp8",
    "file": "python/sglang/srt/layers/quantization/fp8.py",
    "line": 1210,
    "qualname": "Fp8KVCacheMethod.__init__",
    "signature": "def __init__(self, quant_config: Fp8Config)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 72,
    "qualname": "is_fp8_fnuz",
    "signature": "def is_fp8_fnuz()"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 89,
    "qualname": "deep_gemm_fp8_fp8_bf16_nt",
    "signature": "def deep_gemm_fp8_fp8_bf16_nt(A: torch.Tensor, As: torch.Tensor, B: torch.Tensor, Bs: torch.Tensor, C: torch.Tensor)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 98,
    "qualname": "deep_gemm_fp8_fp8_bf16_nt_fake",
    "signature": "def deep_gemm_fp8_fp8_bf16_nt_fake(A: torch.Tensor, As: torch.Tensor, B: torch.Tensor, Bs: torch.Tensor, C: torch.Tensor)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 394,
    "qualname": "per_token_group_quant_8bit",
    "signature": "def per_token_group_quant_8bit(x: torch.Tensor, group_size: int, dst_dtype: torch.dtype, eps: float, column_major_scales: bool, scale_tma_aligned: bool, scale_ue8m0: bool, fuse_silu_and_mul: bool, masked_m: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 427,
    "qualname": "create_per_token_group_quant_fp8_output_scale",
    "signature": "def create_per_token_group_quant_fp8_output_scale(x_shape, device, group_size, column_major_scales: bool, scale_tma_aligned: bool, scale_ue8m0: bool)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 471,
    "qualname": "sglang_per_token_group_quant_fp8",
    "signature": "def sglang_per_token_group_quant_fp8(x: torch.Tensor, group_size: int, eps: float, column_major_scales: bool, scale_tma_aligned: bool, scale_ue8m0: bool, fuse_silu_and_mul: bool, masked_m: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 507,
    "qualname": "sglang_per_token_group_quant_8bit",
    "signature": "def sglang_per_token_group_quant_8bit(x: torch.Tensor, group_size: int, dst_dtype: torch.dtype, eps: float, column_major_scales: bool, scale_tma_aligned: bool, scale_ue8m0: bool, fuse_silu_and_mul: bool, masked_m: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 546,
    "qualname": "sglang_per_token_quant_fp8",
    "signature": "def sglang_per_token_quant_fp8(x: torch.Tensor, dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 608,
    "qualname": "static_quant_fp8",
    "signature": "def static_quant_fp8(x: torch.Tensor, x_s: torch.Tensor, repeat_scale: bool)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 909,
    "qualname": "get_w8a8_block_fp8_configs",
    "signature": "def get_w8a8_block_fp8_configs(N: int, K: int, block_n: int, block_k: int)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 950,
    "qualname": "select_w8a8_block_fp8_matmul_kernel",
    "signature": "def select_w8a8_block_fp8_matmul_kernel(M, N, META)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 956,
    "qualname": "use_w8a8_block_fp8_matmul_unrolledx4",
    "signature": "def use_w8a8_block_fp8_matmul_unrolledx4(M, N, META)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 965,
    "qualname": "select_w8a8_block_fp8_matmul_kernel",
    "signature": "def select_w8a8_block_fp8_matmul_kernel(M, N, META)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 972,
    "qualname": "prepare_block_fp8_matmul_inputs",
    "signature": "def prepare_block_fp8_matmul_inputs(A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 1020,
    "qualname": "w8a8_block_fp8_matmul_deepgemm",
    "signature": "def w8a8_block_fp8_matmul_deepgemm(A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 1041,
    "qualname": "w8a8_block_fp8_matmul_triton",
    "signature": "def w8a8_block_fp8_matmul_triton(A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 1122,
    "qualname": "w8a8_block_fp8_matmul",
    "signature": "def w8a8_block_fp8_matmul(A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 1192,
    "qualname": "per_tensor_quant_mla_fp8",
    "signature": "def per_tensor_quant_mla_fp8(x: torch.Tensor, x_s_out: torch.Tensor, eps: float)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 1288,
    "qualname": "per_token_group_quant_mla_deep_gemm_masked_fp8",
    "signature": "def per_token_group_quant_mla_deep_gemm_masked_fp8(x: torch.Tensor, group_size: int, eps: float, dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 1358,
    "qualname": "scaled_fp8_quant",
    "signature": "def scaled_fp8_quant(input: torch.Tensor, scale: Optional[torch.Tensor], num_token_padding: Optional[int], use_per_token_if_dynamic: bool)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 1402,
    "qualname": "scaled_fp8_quant",
    "signature": "def scaled_fp8_quant(input: torch.Tensor, scale: Optional[torch.Tensor], num_token_padding: Optional[int], use_per_token_if_dynamic: bool)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 1500,
    "qualname": "per_token_group_quant_fp8_hopper_moe_mn_major",
    "signature": "def per_token_group_quant_fp8_hopper_moe_mn_major(A: torch.Tensor, expert_offsets: torch.Tensor, problem_sizes: torch.Tensor, group_size: int, expert_tokens_alignment: int)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 1568,
    "qualname": "per_group_transpose",
    "signature": "def per_group_transpose(a: torch.Tensor, expert_offsets: torch.Tensor, M_ALIGNMENT: int)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 1591,
    "qualname": "is_weak_contiguous",
    "signature": "def is_weak_contiguous(x: torch.Tensor)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 1600,
    "qualname": "scaled_mm_kernel",
    "signature": "def scaled_mm_kernel(a_ptr, b_ptr, scale_a_ptr, scale_b_ptr, c_ptr, bias_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, ACCUMULATOR_DTYPE: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_SCALE_A: tl.constexpr, BLOCK_SIZE_SCALE_B: tl.constexpr)"
  },
  {
    "module": "srt.layers.quantization.fp8_kernel",
    "file": "python/sglang/srt/layers/quantization/fp8_kernel.py",
    "line": 1723,
    "qualname": "triton_scaled_mm",
    "signature": "def triton_scaled_mm(input: torch.Tensor, weight: torch.Tensor, scale_a: torch.Tensor, scale_b: torch.Tensor, out_dtype: type[torch.dtype], bias: Optional[torch.Tensor], block_size_m: int, block_size_n: int, block_size_k: int, use_heuristic)"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 63,
    "qualname": "use_rowwise_torch_scaled_mm",
    "signature": "def use_rowwise_torch_scaled_mm()"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 81,
    "qualname": "cutlass_fp8_supported",
    "signature": "def cutlass_fp8_supported()"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 93,
    "qualname": "normalize_e4m3fn_to_e4m3fnuz",
    "signature": "def normalize_e4m3fn_to_e4m3fnuz(weight: torch.Tensor, weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 118,
    "qualname": "cutlass_block_fp8_supported",
    "signature": "def cutlass_block_fp8_supported()"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 140,
    "qualname": "dispatch_w8a8_block_fp8_linear",
    "signature": "def dispatch_w8a8_block_fp8_linear()"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 153,
    "qualname": "flashinfer_gemm_w8a8_block_fp8_linear",
    "signature": "def flashinfer_gemm_w8a8_block_fp8_linear(input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 185,
    "qualname": "cutlass_w8a8_block_fp8_linear_with_fallback",
    "signature": "def cutlass_w8a8_block_fp8_linear_with_fallback(input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 218,
    "qualname": "deepgemm_w8a8_block_fp8_linear_with_fallback",
    "signature": "def deepgemm_w8a8_block_fp8_linear_with_fallback(input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 269,
    "qualname": "aiter_w8a8_block_fp8_linear",
    "signature": "def aiter_w8a8_block_fp8_linear(input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 292,
    "qualname": "triton_w8a8_block_fp8_linear",
    "signature": "def triton_w8a8_block_fp8_linear(input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 315,
    "qualname": "dequant_mxfp4",
    "signature": "def dequant_mxfp4(w_block: torch.Tensor, w_scale: torch.Tensor, out_dtype)"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 342,
    "qualname": "input_to_float8",
    "signature": "def input_to_float8(x: torch.Tensor, dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 361,
    "qualname": "block_quant_to_tensor_quant",
    "signature": "def block_quant_to_tensor_quant(x_q_block: torch.Tensor, x_s: torch.Tensor, block_size: List[int])"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 404,
    "qualname": "block_quant_dequant",
    "signature": "def block_quant_dequant(x_q_block: torch.Tensor, x_s: torch.Tensor, block_size: List[int], dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 427,
    "qualname": "requant_weight_ue8m0_inplace",
    "signature": "def requant_weight_ue8m0_inplace(weight, weight_scale_inv, weight_block_size)"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 471,
    "qualname": "per_block_cast_to_fp8",
    "signature": "def per_block_cast_to_fp8(x: torch.Tensor)"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 488,
    "qualname": "ceil_to_ue8m0",
    "signature": "def ceil_to_ue8m0(x: torch.Tensor)"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 492,
    "qualname": "channel_quant_to_tensor_quant",
    "signature": "def channel_quant_to_tensor_quant(x_q_channel: torch.Tensor, x_s: torch.Tensor)"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 542,
    "qualname": "apply_fp8_linear",
    "signature": "def apply_fp8_linear(input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], input_scale_ub: Optional[torch.Tensor], bias: Optional[torch.Tensor], cutlass_fp8_supported: bool, use_per_token_if_dynamic: bool, pad_output: Optional[bool], compressed_tensor_quant: bool)"
  },
  {
    "module": "srt.layers.quantization.fp8_utils",
    "file": "python/sglang/srt/layers/quantization/fp8_utils.py",
    "line": 809,
    "qualname": "can_auto_enable_marlin_fp8",
    "signature": "def can_auto_enable_marlin_fp8()"
  },
  {
    "module": "srt.layers.quantization.fpgemm_fp8",
    "file": "python/sglang/srt/layers/quantization/fpgemm_fp8.py",
    "line": 43,
    "qualname": "FBGEMMFp8Config.__init__",
    "signature": "def __init__(self, ignore_list: list[str], input_scale_ub: float)"
  },
  {
    "module": "srt.layers.quantization.fpgemm_fp8",
    "file": "python/sglang/srt/layers/quantization/fpgemm_fp8.py",
    "line": 58,
    "qualname": "FBGEMMFp8Config.get_name",
    "signature": "def get_name(cls)"
  },
  {
    "module": "srt.layers.quantization.fpgemm_fp8",
    "file": "python/sglang/srt/layers/quantization/fpgemm_fp8.py",
    "line": 62,
    "qualname": "FBGEMMFp8Config.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(cls)"
  },
  {
    "module": "srt.layers.quantization.fpgemm_fp8",
    "file": "python/sglang/srt/layers/quantization/fpgemm_fp8.py",
    "line": 66,
    "qualname": "FBGEMMFp8Config.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.fpgemm_fp8",
    "file": "python/sglang/srt/layers/quantization/fpgemm_fp8.py",
    "line": 70,
    "qualname": "FBGEMMFp8Config.get_config_filenames",
    "signature": "def get_config_filenames(cls)"
  },
  {
    "module": "srt.layers.quantization.fpgemm_fp8",
    "file": "python/sglang/srt/layers/quantization/fpgemm_fp8.py",
    "line": 74,
    "qualname": "FBGEMMFp8Config.from_config",
    "signature": "def from_config(cls, config: dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.fpgemm_fp8",
    "file": "python/sglang/srt/layers/quantization/fpgemm_fp8.py",
    "line": 79,
    "qualname": "FBGEMMFp8Config.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.fpgemm_fp8",
    "file": "python/sglang/srt/layers/quantization/fpgemm_fp8.py",
    "line": 92,
    "qualname": "FBGEMMFp8Config.get_scaled_act_names",
    "signature": "def get_scaled_act_names(self)"
  },
  {
    "module": "srt.layers.quantization.fpgemm_fp8",
    "file": "python/sglang/srt/layers/quantization/fpgemm_fp8.py",
    "line": 98,
    "qualname": "FBGEMMFp8LinearMethod.__init__",
    "signature": "def __init__(self, quant_config: FBGEMMFp8Config)"
  },
  {
    "module": "srt.layers.quantization.fpgemm_fp8",
    "file": "python/sglang/srt/layers/quantization/fpgemm_fp8.py",
    "line": 105,
    "qualname": "FBGEMMFp8LinearMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.fpgemm_fp8",
    "file": "python/sglang/srt/layers/quantization/fpgemm_fp8.py",
    "line": 155,
    "qualname": "FBGEMMFp8LinearMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: Module)"
  },
  {
    "module": "srt.layers.quantization.fpgemm_fp8",
    "file": "python/sglang/srt/layers/quantization/fpgemm_fp8.py",
    "line": 176,
    "qualname": "FBGEMMFp8LinearMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 62,
    "qualname": "check_marlin_format",
    "signature": "def check_marlin_format(hf_quant_cfg: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 70,
    "qualname": "gptq_marlin_moe_repack",
    "signature": "def gptq_marlin_moe_repack(b_q_weight: torch.Tensor, perm: torch.Tensor, size_k: int, size_n: int, num_bits: int)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 106,
    "qualname": "GPTQConfig.__init__",
    "signature": "def __init__(self, weight_bits: int, group_size: int, desc_act: bool, lm_head_quantized: bool, dynamic: Dict[str, Dict[str, Union[int, bool]]])"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 151,
    "qualname": "GPTQConfig.__repr__",
    "signature": "def __repr__(self)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 160,
    "qualname": "GPTQConfig.get_scaled_act_names",
    "signature": "def get_scaled_act_names(self)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 168,
    "qualname": "GPTQConfig.get_name",
    "signature": "def get_name(cls)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 172,
    "qualname": "GPTQConfig.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(cls)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 177,
    "qualname": "GPTQConfig.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 181,
    "qualname": "GPTQConfig.get_config_filenames",
    "signature": "def get_config_filenames(cls)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 185,
    "qualname": "GPTQConfig.from_config",
    "signature": "def from_config(cls, config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 195,
    "qualname": "GPTQConfig.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 219,
    "qualname": "GPTQMarlinConfig.__init__",
    "signature": "def __init__(self, weight_bits: int, group_size: int, desc_act: bool, is_sym: bool, lm_head_quantized: bool, dynamic: Dict[str, Dict[str, Union[int, bool]]], full_config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 277,
    "qualname": "GPTQMarlinConfig.__repr__",
    "signature": "def __repr__(self)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 286,
    "qualname": "GPTQMarlinConfig.get_scaled_act_names",
    "signature": "def get_scaled_act_names(self)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 294,
    "qualname": "GPTQMarlinConfig.get_name",
    "signature": "def get_name(cls)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 298,
    "qualname": "GPTQMarlinConfig.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(cls)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 302,
    "qualname": "GPTQMarlinConfig.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 306,
    "qualname": "GPTQMarlinConfig.get_config_filenames",
    "signature": "def get_config_filenames(cls)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 310,
    "qualname": "GPTQMarlinConfig.from_config",
    "signature": "def from_config(cls, config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 330,
    "qualname": "GPTQMarlinConfig.override_quantization_method",
    "signature": "def override_quantization_method(cls, hf_quant_cfg, user_quant)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 356,
    "qualname": "GPTQMarlinConfig.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 367,
    "qualname": "GPTQMarlinConfig.is_gptq_marlin_compatible",
    "signature": "def is_gptq_marlin_compatible(cls, quant_config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 399,
    "qualname": "GPTQLinearMethod.__init__",
    "signature": "def __init__(self, quant_config: GPTQConfig)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 402,
    "qualname": "GPTQLinearMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 513,
    "qualname": "GPTQLinearMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 531,
    "qualname": "GPTQLinearMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 563,
    "qualname": "GPTQMarlinLinearMethod.__init__",
    "signature": "def __init__(self, quant_config: GPTQMarlinConfig)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 572,
    "qualname": "GPTQMarlinLinearMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 685,
    "qualname": "GPTQMarlinLinearMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 779,
    "qualname": "GPTQMarlinLinearMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 825,
    "qualname": "GPTQMarlinMoEMethod.__init__",
    "signature": "def __init__(self, quant_config: GPTQMarlinConfig)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 828,
    "qualname": "GPTQMarlinMoEMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 974,
    "qualname": "GPTQMarlinMoEMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.gptq",
    "file": "python/sglang/srt/layers/quantization/gptq.py",
    "line": 1055,
    "qualname": "GPTQMarlinMoEMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.int8_kernel",
    "file": "python/sglang/srt/layers/quantization/int8_kernel.py",
    "line": 51,
    "qualname": "per_token_quant_int8",
    "signature": "def per_token_quant_int8(x, scale_dtype, cal_sum)"
  },
  {
    "module": "srt.layers.quantization.int8_kernel",
    "file": "python/sglang/srt/layers/quantization/int8_kernel.py",
    "line": 126,
    "qualname": "per_token_group_quant_int8",
    "signature": "def per_token_group_quant_int8(x: torch.Tensor, group_size: int, eps: float, dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.int8_kernel",
    "file": "python/sglang/srt/layers/quantization/int8_kernel.py",
    "line": 185,
    "qualname": "sglang_per_token_group_quant_int8",
    "signature": "def sglang_per_token_group_quant_int8(x: torch.Tensor, group_size: int, eps: float, dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.int8_kernel",
    "file": "python/sglang/srt/layers/quantization/int8_kernel.py",
    "line": 298,
    "qualname": "get_w8a8_block_int8_configs",
    "signature": "def get_w8a8_block_int8_configs(N: int, K: int, block_n: int, block_k: int)"
  },
  {
    "module": "srt.layers.quantization.int8_kernel",
    "file": "python/sglang/srt/layers/quantization/int8_kernel.py",
    "line": 339,
    "qualname": "w8a8_block_int8_matmul",
    "signature": "def w8a8_block_int8_matmul(A: torch.Tensor, B: torch.Tensor, As: torch.Tensor, Bs: torch.Tensor, block_size: List[int], output_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.int8_utils",
    "file": "python/sglang/srt/layers/quantization/int8_utils.py",
    "line": 11,
    "qualname": "apply_w8a8_block_int8_linear",
    "signature": "def apply_w8a8_block_int8_linear(input: torch.Tensor, weight: torch.Tensor, block_size: List[int], weight_scale: torch.Tensor, input_scale: Optional[torch.Tensor], bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.int8_utils",
    "file": "python/sglang/srt/layers/quantization/int8_utils.py",
    "line": 34,
    "qualname": "input_to_int8",
    "signature": "def input_to_int8(x: torch.Tensor, dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.int8_utils",
    "file": "python/sglang/srt/layers/quantization/int8_utils.py",
    "line": 47,
    "qualname": "block_dequant",
    "signature": "def block_dequant(x_q_block: torch.Tensor, x_s: torch.Tensor, block_size: List[int])"
  },
  {
    "module": "srt.layers.quantization.kv_cache",
    "file": "python/sglang/srt/layers/quantization/kv_cache.py",
    "line": 28,
    "qualname": "BaseKVCacheMethod.__init__",
    "signature": "def __init__(self, quant_config: QuantizationConfig)"
  },
  {
    "module": "srt.layers.quantization.kv_cache",
    "file": "python/sglang/srt/layers/quantization/kv_cache.py",
    "line": 31,
    "qualname": "BaseKVCacheMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.kv_cache",
    "file": "python/sglang/srt/layers/quantization/kv_cache.py",
    "line": 45,
    "qualname": "BaseKVCacheMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.kv_cache",
    "file": "python/sglang/srt/layers/quantization/kv_cache.py",
    "line": 48,
    "qualname": "BaseKVCacheMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: RadixAttention)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 63,
    "qualname": "query_marlin_supported_quant_types",
    "signature": "def query_marlin_supported_quant_types(has_zp: Optional[bool], include_fp_type: bool, device_capability: Optional[int])"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 134,
    "qualname": "check_marlin_supported",
    "signature": "def check_marlin_supported(quant_type: ScalarType, group_size: int, has_zp: bool, device_capability: Optional[int])"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 144,
    "qualname": "verify_marlin_supported",
    "signature": "def verify_marlin_supported(quant_type: ScalarType, group_size: int, has_zp: bool)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 153,
    "qualname": "verify_marlin_supports_shape",
    "signature": "def verify_marlin_supports_shape(output_size_per_partition: int, input_size_per_partition: int, input_size: int, group_size: int)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 189,
    "qualname": "check_marlin_supports_shape",
    "signature": "def check_marlin_supports_shape(output_size_per_partition: int, input_size_per_partition: int, input_size: int, group_size: int)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 204,
    "qualname": "check_marlin_supports_layer",
    "signature": "def check_marlin_supports_layer(layer: LinearBase, group_size: int)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 220,
    "qualname": "check_moe_marlin_supports_layer",
    "signature": "def check_moe_marlin_supports_layer(layer: FusedMoE, group_size: int)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 244,
    "qualname": "marlin_make_workspace",
    "signature": "def marlin_make_workspace(device: torch.device, max_blocks_per_sm: int)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 255,
    "qualname": "marlin_is_k_full",
    "signature": "def marlin_is_k_full(act_order: bool, is_row_parallel: bool)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 259,
    "qualname": "marlin_repeat_scales_on_all_ranks",
    "signature": "def marlin_repeat_scales_on_all_ranks(act_order: bool, group_size: int, is_row_parallel: bool)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 268,
    "qualname": "marlin_make_empty_g_idx",
    "signature": "def marlin_make_empty_g_idx(device: torch.device)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 274,
    "qualname": "marlin_make_empty_zp",
    "signature": "def marlin_make_empty_zp(device: torch.device)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 280,
    "qualname": "marlin_sort_g_idx",
    "signature": "def marlin_sort_g_idx(g_idx: torch.Tensor)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 285,
    "qualname": "get_scale_perms",
    "signature": "def get_scale_perms()"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 295,
    "qualname": "marlin_permute_scales",
    "signature": "def marlin_permute_scales(s: torch.Tensor, size_k: int, size_n: int, group_size: int)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 309,
    "qualname": "marlin_permute_bias",
    "signature": "def marlin_permute_bias(s: torch.Tensor)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 316,
    "qualname": "marlin_moe_permute_scales",
    "signature": "def marlin_moe_permute_scales(s: torch.Tensor, size_k: int, size_n: int, group_size: int)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 334,
    "qualname": "marlin_zero_points",
    "signature": "def marlin_zero_points(zp: torch.Tensor, size_k: int, size_n: int, num_bits: int)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 357,
    "qualname": "awq_to_marlin_zero_points",
    "signature": "def awq_to_marlin_zero_points(q_zp_packed: torch.Tensor, size_k: int, size_n: int, num_bits: int)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 381,
    "qualname": "moe_awq_to_marlin_zero_points",
    "signature": "def moe_awq_to_marlin_zero_points(q_zp_packed: torch.Tensor, size_k: int, size_n: int, num_bits: int)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 395,
    "qualname": "maybe_warn_marlin_atomic_add",
    "signature": "def maybe_warn_marlin_atomic_add(device, dtype)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 407,
    "qualname": "maybe_warn_marlin_atomic_add_env",
    "signature": "def maybe_warn_marlin_atomic_add_env()"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 423,
    "qualname": "should_use_atomic_add_reduce",
    "signature": "def should_use_atomic_add_reduce(m: int, n: int, k: int, device: torch.device, dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 448,
    "qualname": "apply_gptq_marlin_linear",
    "signature": "def apply_gptq_marlin_linear(input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, weight_zp: torch.Tensor, g_idx: torch.Tensor, g_idx_sort_indices: torch.Tensor, workspace: torch.Tensor, wtype: ScalarType, output_size_per_partition: int, input_size_per_partition: int, is_k_full: bool, bias: Optional[torch.Tensor], use_fp32_reduce: bool)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 500,
    "qualname": "apply_awq_marlin_linear",
    "signature": "def apply_awq_marlin_linear(input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, weight_zp: torch.Tensor, g_idx: torch.Tensor, g_idx_sort_indices: torch.Tensor, workspace: torch.Tensor, quant_type: ScalarType, output_size_per_partition: int, input_size_per_partition: int, bias: Optional[torch.Tensor], use_fp32_reduce: bool)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 556,
    "qualname": "MarlinConfig.__init__",
    "signature": "def __init__(self, group_size: int, lm_head_quantized: bool)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 592,
    "qualname": "MarlinConfig.__repr__",
    "signature": "def __repr__(self)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 599,
    "qualname": "MarlinConfig.get_name",
    "signature": "def get_name(cls)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 603,
    "qualname": "MarlinConfig.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(cls)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 608,
    "qualname": "MarlinConfig.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 612,
    "qualname": "MarlinConfig.get_config_filenames",
    "signature": "def get_config_filenames(cls)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 616,
    "qualname": "MarlinConfig.from_config",
    "signature": "def from_config(cls, config: dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 622,
    "qualname": "MarlinConfig.override_quantization_method",
    "signature": "def override_quantization_method(cls, hf_quant_cfg, user_quant)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 642,
    "qualname": "MarlinConfig.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 662,
    "qualname": "MarlinLinearMethod.__init__",
    "signature": "def __init__(self, quant_config: MarlinConfig)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 665,
    "qualname": "MarlinLinearMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 777,
    "qualname": "MarlinLinearMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils",
    "file": "python/sglang/srt/layers/quantization/marlin_utils.py",
    "line": 783,
    "qualname": "MarlinLinearMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.marlin_utils_fp8",
    "file": "python/sglang/srt/layers/quantization/marlin_utils_fp8.py",
    "line": 27,
    "qualname": "fp8_fused_exponent_bias_into_scales",
    "signature": "def fp8_fused_exponent_bias_into_scales(scales)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils_fp8",
    "file": "python/sglang/srt/layers/quantization/marlin_utils_fp8.py",
    "line": 41,
    "qualname": "apply_fp8_marlin_linear",
    "signature": "def apply_fp8_marlin_linear(input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, workspace: torch.Tensor, size_n: int, size_k: int, bias: Optional[torch.Tensor], use_fp32_reduce: bool)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils_fp8",
    "file": "python/sglang/srt/layers/quantization/marlin_utils_fp8.py",
    "line": 83,
    "qualname": "prepare_fp8_layer_for_marlin",
    "signature": "def prepare_fp8_layer_for_marlin(layer: torch.nn.Module, size_k_first: bool)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils_fp8",
    "file": "python/sglang/srt/layers/quantization/marlin_utils_fp8.py",
    "line": 175,
    "qualname": "prepare_moe_fp8_layer_for_marlin",
    "signature": "def prepare_moe_fp8_layer_for_marlin(layer: torch.nn.Module, size_k_first: bool)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils_fp8",
    "file": "python/sglang/srt/layers/quantization/marlin_utils_fp8.py",
    "line": 305,
    "qualname": "pack_fp8_to_int32",
    "signature": "def pack_fp8_to_int32(fp8_tensor: torch.Tensor, size_k_first: bool)"
  },
  {
    "module": "srt.layers.quantization.marlin_utils_fp8",
    "file": "python/sglang/srt/layers/quantization/marlin_utils_fp8.py",
    "line": 322,
    "qualname": "marlin_quant_fp8_torch",
    "signature": "def marlin_quant_fp8_torch(weight, group_size)"
  },
  {
    "module": "srt.layers.quantization.moe_wna16",
    "file": "python/sglang/srt/layers/quantization/moe_wna16.py",
    "line": 29,
    "qualname": "get_weight_perm",
    "signature": "def get_weight_perm(num_bits: int)"
  },
  {
    "module": "srt.layers.quantization.moe_wna16",
    "file": "python/sglang/srt/layers/quantization/moe_wna16.py",
    "line": 62,
    "qualname": "MoeWNA16Config.__init__",
    "signature": "def __init__(self, linear_quant_method: str, weight_bits: int, group_size: int, has_zp: bool, lm_head_quantized: bool, modules_to_not_convert: Optional[List[str]], full_config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.moe_wna16",
    "file": "python/sglang/srt/layers/quantization/moe_wna16.py",
    "line": 109,
    "qualname": "MoeWNA16Config.get_name",
    "signature": "def get_name(cls)"
  },
  {
    "module": "srt.layers.quantization.moe_wna16",
    "file": "python/sglang/srt/layers/quantization/moe_wna16.py",
    "line": 113,
    "qualname": "MoeWNA16Config.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(cls)"
  },
  {
    "module": "srt.layers.quantization.moe_wna16",
    "file": "python/sglang/srt/layers/quantization/moe_wna16.py",
    "line": 117,
    "qualname": "MoeWNA16Config.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.moe_wna16",
    "file": "python/sglang/srt/layers/quantization/moe_wna16.py",
    "line": 121,
    "qualname": "MoeWNA16Config.get_config_filenames",
    "signature": "def get_config_filenames(cls)"
  },
  {
    "module": "srt.layers.quantization.moe_wna16",
    "file": "python/sglang/srt/layers/quantization/moe_wna16.py",
    "line": 124,
    "qualname": "MoeWNA16Config.get_scaled_act_names",
    "signature": "def get_scaled_act_names(self)"
  },
  {
    "module": "srt.layers.quantization.moe_wna16",
    "file": "python/sglang/srt/layers/quantization/moe_wna16.py",
    "line": 128,
    "qualname": "MoeWNA16Config.from_config",
    "signature": "def from_config(cls, config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.moe_wna16",
    "file": "python/sglang/srt/layers/quantization/moe_wna16.py",
    "line": 155,
    "qualname": "MoeWNA16Config.override_quantization_method",
    "signature": "def override_quantization_method(cls, hf_quant_cfg, user_quant)"
  },
  {
    "module": "srt.layers.quantization.moe_wna16",
    "file": "python/sglang/srt/layers/quantization/moe_wna16.py",
    "line": 161,
    "qualname": "MoeWNA16Config.is_moe_wna16_compatible",
    "signature": "def is_moe_wna16_compatible(cls, quant_config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.moe_wna16",
    "file": "python/sglang/srt/layers/quantization/moe_wna16.py",
    "line": 185,
    "qualname": "MoeWNA16Config.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.moe_wna16",
    "file": "python/sglang/srt/layers/quantization/moe_wna16.py",
    "line": 216,
    "qualname": "is_layer_skipped_quant",
    "signature": "def is_layer_skipped_quant(prefix: str, modules_to_not_convert: List[str])"
  },
  {
    "module": "srt.layers.quantization.moe_wna16",
    "file": "python/sglang/srt/layers/quantization/moe_wna16.py",
    "line": 227,
    "qualname": "MoeWNA16Method.__init__",
    "signature": "def __init__(self, quant_config: MoeWNA16Config)"
  },
  {
    "module": "srt.layers.quantization.moe_wna16",
    "file": "python/sglang/srt/layers/quantization/moe_wna16.py",
    "line": 230,
    "qualname": "MoeWNA16Method.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.moe_wna16",
    "file": "python/sglang/srt/layers/quantization/moe_wna16.py",
    "line": 352,
    "qualname": "MoeWNA16Method.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.moe_wna16",
    "file": "python/sglang/srt/layers/quantization/moe_wna16.py",
    "line": 385,
    "qualname": "MoeWNA16Method.get_weight_loader",
    "signature": "def get_weight_loader(layer, weight_loader)"
  },
  {
    "module": "srt.layers.quantization.mxfp4",
    "file": "python/sglang/srt/layers/quantization/mxfp4.py",
    "line": 172,
    "qualname": "Mxfp4Config.__init__",
    "signature": "def __init__(self, ignored_layers: Optional[list[str]], is_checkpoint_mxfp4_serialized: bool)"
  },
  {
    "module": "srt.layers.quantization.mxfp4",
    "file": "python/sglang/srt/layers/quantization/mxfp4.py",
    "line": 182,
    "qualname": "Mxfp4Config.from_config",
    "signature": "def from_config(cls, config)"
  },
  {
    "module": "srt.layers.quantization.mxfp4",
    "file": "python/sglang/srt/layers/quantization/mxfp4.py",
    "line": 202,
    "qualname": "Mxfp4Config.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.mxfp4",
    "file": "python/sglang/srt/layers/quantization/mxfp4.py",
    "line": 206,
    "qualname": "Mxfp4Config.get_name",
    "signature": "def get_name(cls)"
  },
  {
    "module": "srt.layers.quantization.mxfp4",
    "file": "python/sglang/srt/layers/quantization/mxfp4.py",
    "line": 210,
    "qualname": "Mxfp4Config.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(cls)"
  },
  {
    "module": "srt.layers.quantization.mxfp4",
    "file": "python/sglang/srt/layers/quantization/mxfp4.py",
    "line": 214,
    "qualname": "Mxfp4Config.get_config_filenames",
    "signature": "def get_config_filenames(cls)"
  },
  {
    "module": "srt.layers.quantization.mxfp4",
    "file": "python/sglang/srt/layers/quantization/mxfp4.py",
    "line": 217,
    "qualname": "Mxfp4Config.is_static_cfg",
    "signature": "def is_static_cfg(self)"
  },
  {
    "module": "srt.layers.quantization.mxfp4",
    "file": "python/sglang/srt/layers/quantization/mxfp4.py",
    "line": 220,
    "qualname": "Mxfp4Config.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.mxfp4",
    "file": "python/sglang/srt/layers/quantization/mxfp4.py",
    "line": 247,
    "qualname": "Mxfp4Config.get_scaled_act_names",
    "signature": "def get_scaled_act_names(self)"
  },
  {
    "module": "srt.layers.quantization.mxfp4",
    "file": "python/sglang/srt/layers/quantization/mxfp4.py",
    "line": 253,
    "qualname": "Mxfp4MoEMethod.__init__",
    "signature": "def __init__(self, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.mxfp4",
    "file": "python/sglang/srt/layers/quantization/mxfp4.py",
    "line": 281,
    "qualname": "Mxfp4MoEMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype, with_bias: bool)"
  },
  {
    "module": "srt.layers.quantization.mxfp4",
    "file": "python/sglang/srt/layers/quantization/mxfp4.py",
    "line": 389,
    "qualname": "Mxfp4MoEMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer)"
  },
  {
    "module": "srt.layers.quantization.mxfp4",
    "file": "python/sglang/srt/layers/quantization/mxfp4.py",
    "line": 616,
    "qualname": "Mxfp4MoEMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.mxfp4",
    "file": "python/sglang/srt/layers/quantization/mxfp4.py",
    "line": 726,
    "qualname": "Mxfp4DynamicQuantMoEMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.mxfp4",
    "file": "python/sglang/srt/layers/quantization/mxfp4.py",
    "line": 783,
    "qualname": "Mxfp4DynamicQuantMoEMethod.mxfp4_quantize",
    "signature": "def mxfp4_quantize(self, w)"
  },
  {
    "module": "srt.layers.quantization.mxfp4",
    "file": "python/sglang/srt/layers/quantization/mxfp4.py",
    "line": 801,
    "qualname": "Mxfp4DynamicQuantMoEMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: Module)"
  },
  {
    "module": "srt.layers.quantization.mxfp4",
    "file": "python/sglang/srt/layers/quantization/mxfp4.py",
    "line": 811,
    "qualname": "Mxfp4DynamicQuantMoEMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.mxfp4_tensor",
    "file": "python/sglang/srt/layers/quantization/mxfp4_tensor.py",
    "line": 29,
    "qualname": "MXFP4QuantizeUtil.quantize",
    "signature": "def quantize(cls, input: torch.Tensor, block_size: Optional[int])"
  },
  {
    "module": "srt.layers.quantization.mxfp4_tensor",
    "file": "python/sglang/srt/layers/quantization/mxfp4_tensor.py",
    "line": 77,
    "qualname": "MXFP4QuantizeUtil.dequantize",
    "signature": "def dequantize(cls, quantized_data, dtype: torch.dtype, scale, block_sizes)"
  },
  {
    "module": "srt.layers.quantization.petit",
    "file": "python/sglang/srt/layers/quantization/petit.py",
    "line": 36,
    "qualname": "PetitNvFp4Config.__init__",
    "signature": "def __init__(self, is_checkpoint_nvfp4_serialized: bool, kv_cache_quant_algo: str, group_size: int, exclude_modules: List[str])"
  },
  {
    "module": "srt.layers.quantization.petit",
    "file": "python/sglang/srt/layers/quantization/petit.py",
    "line": 54,
    "qualname": "PetitNvFp4Config.get_name",
    "signature": "def get_name(cls)"
  },
  {
    "module": "srt.layers.quantization.petit",
    "file": "python/sglang/srt/layers/quantization/petit.py",
    "line": 58,
    "qualname": "PetitNvFp4Config.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(cls)"
  },
  {
    "module": "srt.layers.quantization.petit",
    "file": "python/sglang/srt/layers/quantization/petit.py",
    "line": 62,
    "qualname": "PetitNvFp4Config.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.petit",
    "file": "python/sglang/srt/layers/quantization/petit.py",
    "line": 67,
    "qualname": "PetitNvFp4Config.get_config_filenames",
    "signature": "def get_config_filenames(cls)"
  },
  {
    "module": "srt.layers.quantization.petit",
    "file": "python/sglang/srt/layers/quantization/petit.py",
    "line": 71,
    "qualname": "PetitNvFp4Config.from_config",
    "signature": "def from_config(cls, config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.petit",
    "file": "python/sglang/srt/layers/quantization/petit.py",
    "line": 101,
    "qualname": "PetitNvFp4Config.override_quantization_method",
    "signature": "def override_quantization_method(cls, hf_quant_cfg, user_quant)"
  },
  {
    "module": "srt.layers.quantization.petit",
    "file": "python/sglang/srt/layers/quantization/petit.py",
    "line": 108,
    "qualname": "PetitNvFp4Config.is_petit_nvfp4_compatible",
    "signature": "def is_petit_nvfp4_compatible(cls, quant_config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.petit",
    "file": "python/sglang/srt/layers/quantization/petit.py",
    "line": 112,
    "qualname": "PetitNvFp4Config.is_layer_excluded",
    "signature": "def is_layer_excluded(self, prefix: str, exclude_modules: list)"
  },
  {
    "module": "srt.layers.quantization.petit",
    "file": "python/sglang/srt/layers/quantization/petit.py",
    "line": 119,
    "qualname": "PetitNvFp4Config.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.petit",
    "file": "python/sglang/srt/layers/quantization/petit.py",
    "line": 130,
    "qualname": "PetitNvFp4Config.get_scaled_act_names",
    "signature": "def get_scaled_act_names(self)"
  },
  {
    "module": "srt.layers.quantization.petit",
    "file": "python/sglang/srt/layers/quantization/petit.py",
    "line": 149,
    "qualname": "PetitNvFp4LinearMethod.__init__",
    "signature": "def __init__(self, quant_config: PetitNvFp4Config)"
  },
  {
    "module": "srt.layers.quantization.petit",
    "file": "python/sglang/srt/layers/quantization/petit.py",
    "line": 152,
    "qualname": "PetitNvFp4LinearMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.petit",
    "file": "python/sglang/srt/layers/quantization/petit.py",
    "line": 226,
    "qualname": "PetitNvFp4LinearMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.petit",
    "file": "python/sglang/srt/layers/quantization/petit.py",
    "line": 238,
    "qualname": "PetitNvFp4LinearMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.petit_utils",
    "file": "python/sglang/srt/layers/quantization/petit_utils.py",
    "line": 17,
    "qualname": "prepare_nvfp4_layer_for_petit",
    "signature": "def prepare_nvfp4_layer_for_petit(layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.petit_utils",
    "file": "python/sglang/srt/layers/quantization/petit_utils.py",
    "line": 22,
    "qualname": "apply_petit_nvfp4_linear",
    "signature": "def apply_petit_nvfp4_linear(input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, weight_scale_2: torch.Tensor, size_n: int, size_k: int, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.petit_utils",
    "file": "python/sglang/srt/layers/quantization/petit_utils.py",
    "line": 55,
    "qualname": "verify_petit_nvfp4_supported",
    "signature": "def verify_petit_nvfp4_supported(quant_method: str, group_size: Optional[int])"
  },
  {
    "module": "srt.layers.quantization.petit_utils",
    "file": "python/sglang/srt/layers/quantization/petit_utils.py",
    "line": 61,
    "qualname": "prepare_nvfp4_layer_for_petit",
    "signature": "def prepare_nvfp4_layer_for_petit(layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.petit_utils",
    "file": "python/sglang/srt/layers/quantization/petit_utils.py",
    "line": 78,
    "qualname": "apply_petit_nvfp4_linear",
    "signature": "def apply_petit_nvfp4_linear(input: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, weight_scale_2: torch.Tensor, size_n: int, size_k: int, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.qoq",
    "file": "python/sglang/srt/layers/quantization/qoq.py",
    "line": 40,
    "qualname": "QoQConfig.__init__",
    "signature": "def __init__(self, weight_bits: int, group_size: int)"
  },
  {
    "module": "srt.layers.quantization.qoq",
    "file": "python/sglang/srt/layers/quantization/qoq.py",
    "line": 61,
    "qualname": "QoQConfig.__repr__",
    "signature": "def __repr__(self)"
  },
  {
    "module": "srt.layers.quantization.qoq",
    "file": "python/sglang/srt/layers/quantization/qoq.py",
    "line": 67,
    "qualname": "QoQConfig.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(cls)"
  },
  {
    "module": "srt.layers.quantization.qoq",
    "file": "python/sglang/srt/layers/quantization/qoq.py",
    "line": 71,
    "qualname": "QoQConfig.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.qoq",
    "file": "python/sglang/srt/layers/quantization/qoq.py",
    "line": 75,
    "qualname": "QoQConfig.get_name",
    "signature": "def get_name(cls)"
  },
  {
    "module": "srt.layers.quantization.qoq",
    "file": "python/sglang/srt/layers/quantization/qoq.py",
    "line": 79,
    "qualname": "QoQConfig.get_config_filenames",
    "signature": "def get_config_filenames(cls)"
  },
  {
    "module": "srt.layers.quantization.qoq",
    "file": "python/sglang/srt/layers/quantization/qoq.py",
    "line": 87,
    "qualname": "QoQConfig.from_config",
    "signature": "def from_config(cls, config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.qoq",
    "file": "python/sglang/srt/layers/quantization/qoq.py",
    "line": 92,
    "qualname": "QoQConfig.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.qoq",
    "file": "python/sglang/srt/layers/quantization/qoq.py",
    "line": 103,
    "qualname": "QoQConfig.get_scaled_act_names",
    "signature": "def get_scaled_act_names(self)"
  },
  {
    "module": "srt.layers.quantization.qoq",
    "file": "python/sglang/srt/layers/quantization/qoq.py",
    "line": 114,
    "qualname": "QoQLinearMethod.__init__",
    "signature": "def __init__(self, quant_config: QoQConfig)"
  },
  {
    "module": "srt.layers.quantization.qoq",
    "file": "python/sglang/srt/layers/quantization/qoq.py",
    "line": 117,
    "qualname": "QoQLinearMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.qoq",
    "file": "python/sglang/srt/layers/quantization/qoq.py",
    "line": 210,
    "qualname": "QoQLinearMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.qoq",
    "file": "python/sglang/srt/layers/quantization/qoq.py",
    "line": 219,
    "qualname": "QoQLinearMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.unquant",
    "file": "python/sglang/srt/layers/quantization/unquant.py",
    "line": 47,
    "qualname": "UnquantizedEmbeddingMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.unquant",
    "file": "python/sglang/srt/layers/quantization/unquant.py",
    "line": 70,
    "qualname": "UnquantizedEmbeddingMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.unquant",
    "file": "python/sglang/srt/layers/quantization/unquant.py",
    "line": 78,
    "qualname": "UnquantizedEmbeddingMethod.embedding",
    "signature": "def embedding(self, layer: torch.nn.Module, input_: torch.Tensor)"
  },
  {
    "module": "srt.layers.quantization.unquant",
    "file": "python/sglang/srt/layers/quantization/unquant.py",
    "line": 85,
    "qualname": "UnquantizedLinearMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.unquant",
    "file": "python/sglang/srt/layers/quantization/unquant.py",
    "line": 107,
    "qualname": "UnquantizedLinearMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.unquant",
    "file": "python/sglang/srt/layers/quantization/unquant.py",
    "line": 111,
    "qualname": "UnquantizedLinearMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.unquant",
    "file": "python/sglang/srt/layers/quantization/unquant.py",
    "line": 135,
    "qualname": "UnquantizedFusedMoEMethod.__init__",
    "signature": "def __init__(self, use_triton_kernels: bool)"
  },
  {
    "module": "srt.layers.quantization.unquant",
    "file": "python/sglang/srt/layers/quantization/unquant.py",
    "line": 153,
    "qualname": "UnquantizedFusedMoEMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype, with_bias: bool)"
  },
  {
    "module": "srt.layers.quantization.unquant",
    "file": "python/sglang/srt/layers/quantization/unquant.py",
    "line": 206,
    "qualname": "UnquantizedFusedMoEMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.unquant",
    "file": "python/sglang/srt/layers/quantization/unquant.py",
    "line": 225,
    "qualname": "UnquantizedFusedMoEMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.unquant",
    "file": "python/sglang/srt/layers/quantization/unquant.py",
    "line": 240,
    "qualname": "UnquantizedFusedMoEMethod.forward_cuda",
    "signature": "def forward_cuda(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.unquant",
    "file": "python/sglang/srt/layers/quantization/unquant.py",
    "line": 314,
    "qualname": "UnquantizedFusedMoEMethod.forward_cpu",
    "signature": "def forward_cpu(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.unquant",
    "file": "python/sglang/srt/layers/quantization/unquant.py",
    "line": 361,
    "qualname": "UnquantizedFusedMoEMethod.forward_npu",
    "signature": "def forward_npu(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.unquant",
    "file": "python/sglang/srt/layers/quantization/unquant.py",
    "line": 377,
    "qualname": "UnquantizedFusedMoEMethod.forward_tpu",
    "signature": "def forward_tpu(self)"
  },
  {
    "module": "srt.layers.quantization.w4afp8",
    "file": "python/sglang/srt/layers/quantization/w4afp8.py",
    "line": 33,
    "qualname": "W4AFp8Config.__init__",
    "signature": "def __init__(self, is_checkpoint_fp8_serialized: bool, is_checkpoint_w4afp8_serialized: bool, linear_activation_scheme: str, moe_activation_scheme: str, ignored_layers: Optional[List[str]], weight_block_size: Optional[List[int]], group_size: int)"
  },
  {
    "module": "srt.layers.quantization.w4afp8",
    "file": "python/sglang/srt/layers/quantization/w4afp8.py",
    "line": 57,
    "qualname": "W4AFp8Config.get_name",
    "signature": "def get_name(cls)"
  },
  {
    "module": "srt.layers.quantization.w4afp8",
    "file": "python/sglang/srt/layers/quantization/w4afp8.py",
    "line": 61,
    "qualname": "W4AFp8Config.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(cls)"
  },
  {
    "module": "srt.layers.quantization.w4afp8",
    "file": "python/sglang/srt/layers/quantization/w4afp8.py",
    "line": 65,
    "qualname": "W4AFp8Config.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.w4afp8",
    "file": "python/sglang/srt/layers/quantization/w4afp8.py",
    "line": 69,
    "qualname": "W4AFp8Config.get_config_filenames",
    "signature": "def get_config_filenames(cls)"
  },
  {
    "module": "srt.layers.quantization.w4afp8",
    "file": "python/sglang/srt/layers/quantization/w4afp8.py",
    "line": 73,
    "qualname": "W4AFp8Config.from_config",
    "signature": "def from_config(cls, config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.w4afp8",
    "file": "python/sglang/srt/layers/quantization/w4afp8.py",
    "line": 88,
    "qualname": "W4AFp8Config.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.w4afp8",
    "file": "python/sglang/srt/layers/quantization/w4afp8.py",
    "line": 103,
    "qualname": "W4AFp8Config.get_scaled_act_names",
    "signature": "def get_scaled_act_names(self)"
  },
  {
    "module": "srt.layers.quantization.w4afp8",
    "file": "python/sglang/srt/layers/quantization/w4afp8.py",
    "line": 109,
    "qualname": "W4AFp8MoEMethod.__init__",
    "signature": "def __init__(self, quant_config: W4AFp8Config)"
  },
  {
    "module": "srt.layers.quantization.w4afp8",
    "file": "python/sglang/srt/layers/quantization/w4afp8.py",
    "line": 112,
    "qualname": "W4AFp8MoEMethod.create_weights",
    "signature": "def create_weights(self, layer: EPMoE, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.w4afp8",
    "file": "python/sglang/srt/layers/quantization/w4afp8.py",
    "line": 252,
    "qualname": "W4AFp8MoEMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: Module)"
  },
  {
    "module": "srt.layers.quantization.w4afp8",
    "file": "python/sglang/srt/layers/quantization/w4afp8.py",
    "line": 281,
    "qualname": "W4AFp8MoEMethod.apply",
    "signature": "def apply(self, layer: EPMoE, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/w8a8_fp8.py",
    "line": 54,
    "qualname": "W8A8Fp8Config.__init__",
    "signature": "def __init__(self, is_checkpoint_fp8_serialized: bool)"
  },
  {
    "module": "srt.layers.quantization.w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/w8a8_fp8.py",
    "line": 58,
    "qualname": "W8A8Fp8Config.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(cls)"
  },
  {
    "module": "srt.layers.quantization.w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/w8a8_fp8.py",
    "line": 62,
    "qualname": "W8A8Fp8Config.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/w8a8_fp8.py",
    "line": 66,
    "qualname": "W8A8Fp8Config.get_name",
    "signature": "def get_name(self)"
  },
  {
    "module": "srt.layers.quantization.w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/w8a8_fp8.py",
    "line": 70,
    "qualname": "W8A8Fp8Config.get_config_filenames",
    "signature": "def get_config_filenames(cls)"
  },
  {
    "module": "srt.layers.quantization.w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/w8a8_fp8.py",
    "line": 74,
    "qualname": "W8A8Fp8Config.from_config",
    "signature": "def from_config(cls, config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/w8a8_fp8.py",
    "line": 81,
    "qualname": "W8A8Fp8Config.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/w8a8_fp8.py",
    "line": 95,
    "qualname": "W8A8Fp8Config.get_scaled_act_names",
    "signature": "def get_scaled_act_names(self)"
  },
  {
    "module": "srt.layers.quantization.w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/w8a8_fp8.py",
    "line": 101,
    "qualname": "W8A8Fp8LinearMethod.__init__",
    "signature": "def __init__(self, quantization_config: W8A8Fp8Config)"
  },
  {
    "module": "srt.layers.quantization.w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/w8a8_fp8.py",
    "line": 105,
    "qualname": "W8A8Fp8LinearMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/w8a8_fp8.py",
    "line": 137,
    "qualname": "W8A8Fp8LinearMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/w8a8_fp8.py",
    "line": 178,
    "qualname": "W8A8Fp8LinearMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/w8a8_fp8.py",
    "line": 204,
    "qualname": "W8A8FP8MoEMethod.__init__",
    "signature": "def __init__(self, quant_config: W8A8Fp8Config)"
  },
  {
    "module": "srt.layers.quantization.w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/w8a8_fp8.py",
    "line": 207,
    "qualname": "W8A8FP8MoEMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/w8a8_fp8.py",
    "line": 259,
    "qualname": "W8A8FP8MoEMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/w8a8_fp8.py",
    "line": 269,
    "qualname": "W8A8FP8MoEMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 78,
    "qualname": "ModelOptFp8Config.__init__",
    "signature": "def __init__(self, is_checkpoint_fp8_serialized: bool, kv_cache_quant_method: Optional[str], exclude_modules: Optional[List[str]])"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 97,
    "qualname": "ModelOptFp8Config.get_name",
    "signature": "def get_name(cls)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 101,
    "qualname": "ModelOptFp8Config.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(cls)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 105,
    "qualname": "ModelOptFp8Config.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 109,
    "qualname": "ModelOptFp8Config.get_config_filenames",
    "signature": "def get_config_filenames(cls)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 113,
    "qualname": "ModelOptFp8Config.from_config",
    "signature": "def from_config(cls, config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 168,
    "qualname": "ModelOptFp8Config.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 195,
    "qualname": "ModelOptFp8Config.get_scaled_act_names",
    "signature": "def get_scaled_act_names(self)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 213,
    "qualname": "ModelOptFp8LinearMethod.__init__",
    "signature": "def __init__(self, quant_config: ModelOptFp8Config)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 218,
    "qualname": "ModelOptFp8LinearMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 270,
    "qualname": "ModelOptFp8LinearMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 282,
    "qualname": "ModelOptFp8LinearMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 304,
    "qualname": "ModelOptFp8KVCacheMethod.__init__",
    "signature": "def __init__(self, quant_config: ModelOptFp8Config)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 316,
    "qualname": "ModelOptFp8MoEMethod.__init__",
    "signature": "def __init__(self, quant_config: ModelOptFp8Config)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 320,
    "qualname": "ModelOptFp8MoEMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 397,
    "qualname": "ModelOptFp8MoEMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 460,
    "qualname": "ModelOptFp8MoEMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 487,
    "qualname": "ModelOptFp4Config.__init__",
    "signature": "def __init__(self, is_checkpoint_nvfp4_serialized: bool, kv_cache_quant_algo: str, group_size: int, exclude_modules: List[str])"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 505,
    "qualname": "ModelOptFp4Config.get_name",
    "signature": "def get_name(cls)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 509,
    "qualname": "ModelOptFp4Config.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(cls)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 513,
    "qualname": "ModelOptFp4Config.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 517,
    "qualname": "ModelOptFp4Config.get_config_filenames",
    "signature": "def get_config_filenames(cls)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 521,
    "qualname": "ModelOptFp4Config.from_config",
    "signature": "def from_config(cls, config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 595,
    "qualname": "ModelOptFp4Config.is_layer_excluded",
    "signature": "def is_layer_excluded(self, prefix: str, exclude_modules: list)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 604,
    "qualname": "ModelOptFp4Config.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 626,
    "qualname": "ModelOptFp4Config.get_scaled_act_names",
    "signature": "def get_scaled_act_names(self)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 645,
    "qualname": "ModelOptFp4LinearMethod.__init__",
    "signature": "def __init__(self, quant_config: ModelOptFp4Config)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 648,
    "qualname": "ModelOptFp4LinearMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 722,
    "qualname": "ModelOptFp4LinearMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 759,
    "qualname": "ModelOptFp4LinearMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 804,
    "qualname": "ModelOptNvFp4FusedMoEMethod.__init__",
    "signature": "def __init__(self, quant_config: ModelOptFp4Config)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 816,
    "qualname": "ModelOptNvFp4FusedMoEMethod.enable_flashinfer_cutlass_moe",
    "signature": "def enable_flashinfer_cutlass_moe(self)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 822,
    "qualname": "ModelOptNvFp4FusedMoEMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 944,
    "qualname": "ModelOptNvFp4FusedMoEMethod.swizzle_blockscale",
    "signature": "def swizzle_blockscale(self, scale: torch.Tensor)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 969,
    "qualname": "ModelOptNvFp4FusedMoEMethod.prepare_static_weights_for_kernel",
    "signature": "def prepare_static_weights_for_kernel(self, gemm1_weights, gemm2_weights, gemm1_scales_linear_fp4_bytes, gemm2_scales_linear_fp4_bytes, hidden_size, intermediate_size, num_experts)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 1102,
    "qualname": "ModelOptNvFp4FusedMoEMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 1237,
    "qualname": "ModelOptNvFp4FusedMoEMethod.load_up_proj_weight_first",
    "signature": "def load_up_proj_weight_first(self)"
  },
  {
    "module": "srt.layers.quantization.modelopt_quant",
    "file": "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "line": 1241,
    "qualname": "ModelOptNvFp4FusedMoEMethod.apply",
    "signature": "def apply(self, layer: FusedMoE, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 20,
    "qualname": "get_scalar_types",
    "signature": "def get_scalar_types()"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 47,
    "qualname": "is_layer_skipped",
    "signature": "def is_layer_skipped(prefix: str, ignored_layers: List[str], fused_mapping: Mapping[str, List[str]])"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 85,
    "qualname": "per_tensor_dequantize",
    "signature": "def per_tensor_dequantize(tensor: torch.Tensor, inv_scale: Union[float, torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 93,
    "qualname": "all_close_1d",
    "signature": "def all_close_1d(x: torch.Tensor)"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 98,
    "qualname": "convert_to_channelwise",
    "signature": "def convert_to_channelwise(weight_scale: torch.Tensor, logical_widths: List[int])"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 121,
    "qualname": "requantize_with_max_scale",
    "signature": "def requantize_with_max_scale(weight: torch.Tensor, weight_scale: torch.Tensor, logical_widths: List[int])"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 149,
    "qualname": "update_tensor_inplace",
    "signature": "def update_tensor_inplace(old: torch.Tensor, new: torch.Tensor)"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 156,
    "qualname": "replace_parameter",
    "signature": "def replace_parameter(mod: torch.nn.Module, name: str, new: Union[torch.Tensor, torch.nn.Parameter])"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 179,
    "qualname": "assert_fp8_all_close",
    "signature": "def assert_fp8_all_close(a: torch.Tensor, b: torch.Tensor)"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 202,
    "qualname": "override_config",
    "signature": "def override_config(config: QuantizationConfig, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 234,
    "qualname": "get_dynamic_override",
    "signature": "def get_dynamic_override(config: QuantizationConfig, layer_name: str, key: Optional[str], default_value: Union[int, bool, None])"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 255,
    "qualname": "get_linear_quant_method",
    "signature": "def get_linear_quant_method(config: QuantizationConfig, layer: torch.nn.Module, prefix: str, linear_method_cls: type)"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 288,
    "qualname": "get_pack_factor",
    "signature": "def get_pack_factor(num_bits)"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 293,
    "qualname": "permute_rows",
    "signature": "def permute_rows(q_w: torch.Tensor, w_ref: torch.Tensor, group_size: int, test_perm: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 323,
    "qualname": "pack_cols",
    "signature": "def pack_cols(q_w: torch.Tensor, num_bits: int, size_k: int, size_n: int)"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 349,
    "qualname": "pack_rows",
    "signature": "def pack_rows(q_w: torch.Tensor, num_bits: int, size_k: int, size_n: int)"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 373,
    "qualname": "unpack_cols",
    "signature": "def unpack_cols(packed_q_w: torch.Tensor, num_bits: int, size_k: int, size_n: int)"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 406,
    "qualname": "quantize_weights",
    "signature": "def quantize_weights(w: torch.Tensor, quant_type: ScalarType, group_size: Optional[int], zero_points: bool, ref_zero_points_after_scales: bool)"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 505,
    "qualname": "gptq_quantize_weights",
    "signature": "def gptq_quantize_weights(w: torch.Tensor, quant_type: ScalarType, group_size: int, act_order: bool, test_perm: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.utils",
    "file": "python/sglang/srt/layers/quantization/utils.py",
    "line": 539,
    "qualname": "sort_weights",
    "signature": "def sort_weights(q_w: torch.Tensor, g_idx: torch.Tensor)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 75,
    "qualname": "npu_wrapper_rmsnorm_init",
    "signature": "def npu_wrapper_rmsnorm_init(func)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 86,
    "qualname": "npu_wrapper_rmsnorm_forward",
    "signature": "def npu_wrapper_rmsnorm_forward(func)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 108,
    "qualname": "npu_fused_experts",
    "signature": "def npu_fused_experts(hidden_states: torch.Tensor, w13: torch.Tensor, w13_scale: torch.Tensor, w2: torch.Tensor, w2_scale: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, top_k: int)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 191,
    "qualname": "W8A8Int8Config.__init__",
    "signature": "def __init__(self, quant_config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 218,
    "qualname": "W8A8Int8Config.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(cls)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 226,
    "qualname": "W8A8Int8Config.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 235,
    "qualname": "W8A8Int8Config.get_name",
    "signature": "def get_name(self)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 239,
    "qualname": "W8A8Int8Config.get_config_filenames",
    "signature": "def get_config_filenames(cls)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 246,
    "qualname": "W8A8Int8Config.from_config",
    "signature": "def from_config(cls, config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 249,
    "qualname": "W8A8Int8Config.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 296,
    "qualname": "W8A8Int8Config.is_layer_skipped",
    "signature": "def is_layer_skipped(self, prefix: str, fused_mapping: Mapping[str, List[str]])"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 327,
    "qualname": "W8A8Int8Config.get_scaled_act_names",
    "signature": "def get_scaled_act_names(self)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 333,
    "qualname": "W8A8Int8LinearMethod.__init__",
    "signature": "def __init__(self, quantization_config: W8A8Int8Config)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 336,
    "qualname": "W8A8Int8LinearMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 347,
    "qualname": "W8A8Int8LinearMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 378,
    "qualname": "W8A8Int8LinearMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 412,
    "qualname": "W8A8Int8MoEMethod.__init__",
    "signature": "def __init__(self, quant_config: W8A8Int8Config)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 415,
    "qualname": "W8A8Int8MoEMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 469,
    "qualname": "W8A8Int8MoEMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 486,
    "qualname": "W8A8Int8MoEMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 537,
    "qualname": "NPU_W8A8LinearMethodImpl.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 542,
    "qualname": "NPU_W8A8LinearMethodImpl.get_weight",
    "signature": "def get_weight(input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 551,
    "qualname": "NPU_W8A8LinearMethodImpl.get_pertensor_param",
    "signature": "def get_pertensor_param(params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 558,
    "qualname": "NPU_W8A8LinearMethodImpl.get_perchannel_param",
    "signature": "def get_perchannel_param(output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 573,
    "qualname": "NPU_W8A8LinearMethodImpl.apply",
    "signature": "def apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 605,
    "qualname": "NPU_W8A8LinearMethodImpl.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 624,
    "qualname": "NPU_W8A8LinearMethodMTImpl.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 628,
    "qualname": "NPU_W8A8LinearMethodMTImpl.get_weight",
    "signature": "def get_weight(input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 637,
    "qualname": "NPU_W8A8LinearMethodMTImpl.get_pertensor_param",
    "signature": "def get_pertensor_param(params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 644,
    "qualname": "NPU_W8A8LinearMethodMTImpl.get_perchannel_param",
    "signature": "def get_perchannel_param(output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 659,
    "qualname": "NPU_W8A8LinearMethodMTImpl.apply",
    "signature": "def apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 682,
    "qualname": "NPU_W8A8LinearMethodMTImpl.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 699,
    "qualname": "NPU_W8A8LinearMethod.__init__",
    "signature": "def __init__(self, quantization_config: W8A8Int8Config)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 707,
    "qualname": "NPU_W8A8LinearMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 747,
    "qualname": "NPU_W8A8LinearMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 751,
    "qualname": "NPU_W8A8LinearMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 763,
    "qualname": "NPU_W8A8DynamicLinearMethodImpl.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 767,
    "qualname": "NPU_W8A8DynamicLinearMethodImpl.get_weight",
    "signature": "def get_weight(input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 774,
    "qualname": "NPU_W8A8DynamicLinearMethodImpl.get_pertensor_param",
    "signature": "def get_pertensor_param(params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 778,
    "qualname": "NPU_W8A8DynamicLinearMethodImpl.get_perchannel_param",
    "signature": "def get_perchannel_param(output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 788,
    "qualname": "NPU_W8A8DynamicLinearMethodImpl.apply",
    "signature": "def apply(layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor], tp_rank: Optional[int])"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 805,
    "qualname": "NPU_W8A8DynamicLinearMethodImpl.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 823,
    "qualname": "NPU_W8A8DynamicLinearMethod.__init__",
    "signature": "def __init__(self, quantization_config: W8A8Int8Config)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 827,
    "qualname": "NPU_W8A8DynamicLinearMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 867,
    "qualname": "NPU_W8A8DynamicLinearMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 871,
    "qualname": "NPU_W8A8DynamicLinearMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 890,
    "qualname": "NPU_W8A8MoEMethod.__init__",
    "signature": "def __init__(self, quantization_config: W8A8Int8Config)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 894,
    "qualname": "NPU_W8A8MoEMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 952,
    "qualname": "NPU_W8A8MoEMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.w8a8_int8",
    "file": "python/sglang/srt/layers/quantization/w8a8_int8.py",
    "line": 972,
    "qualname": "NPU_W8A8MoEMethod.apply",
    "signature": "def apply(self, layer, x, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.attention.triton_ops.decode_attention",
    "file": "python/sglang/srt/layers/attention/triton_ops/decode_attention.py",
    "line": 39,
    "qualname": "tanh",
    "signature": "def tanh(x)"
  },
  {
    "module": "srt.layers.attention.triton_ops.decode_attention",
    "file": "python/sglang/srt/layers/attention/triton_ops/decode_attention.py",
    "line": 633,
    "qualname": "decode_attention_fwd_normal",
    "signature": "def decode_attention_fwd_normal(q, k_buffer, v_buffer, o, kv_indptr, kv_indices, attn_logits, attn_lse, num_kv_splits, max_kv_splits, sm_scale, logit_cap, sinks, xai_temperature_len)"
  },
  {
    "module": "srt.layers.attention.triton_ops.decode_attention",
    "file": "python/sglang/srt/layers/attention/triton_ops/decode_attention.py",
    "line": 676,
    "qualname": "decode_attention_fwd_grouped",
    "signature": "def decode_attention_fwd_grouped(q, k_buffer, v_buffer, o, kv_indptr, kv_indices, attn_logits, attn_lse, num_kv_splits, max_kv_splits, sm_scale, logit_cap, sinks, xai_temperature_len)"
  },
  {
    "module": "srt.layers.attention.triton_ops.decode_attention",
    "file": "python/sglang/srt/layers/attention/triton_ops/decode_attention.py",
    "line": 719,
    "qualname": "decode_attention_fwd",
    "signature": "def decode_attention_fwd(q, k_buffer, v_buffer, o, kv_indptr, kv_indices, attn_logits, attn_lse, num_kv_splits, max_kv_splits, sm_scale, logit_cap, sinks, xai_temperature_len)"
  },
  {
    "module": "srt.layers.attention.triton_ops.double_sparsity_attention",
    "file": "python/sglang/srt/layers/attention/triton_ops/double_sparsity_attention.py",
    "line": 23,
    "qualname": "tanh",
    "signature": "def tanh(x)"
  },
  {
    "module": "srt.layers.attention.triton_ops.double_sparsity_attention",
    "file": "python/sglang/srt/layers/attention/triton_ops/double_sparsity_attention.py",
    "line": 192,
    "qualname": "flash_decode_stage1",
    "signature": "def flash_decode_stage1(q, k, v, Req_to_tokens, B_req_idx, B_Seqlen, max_len_in_batch, mid_out, mid_out_logsumexp, block_seq)"
  },
  {
    "module": "srt.layers.attention.triton_ops.double_sparsity_attention",
    "file": "python/sglang/srt/layers/attention/triton_ops/double_sparsity_attention.py",
    "line": 255,
    "qualname": "flash_decode_stage2",
    "signature": "def flash_decode_stage2(mid_out, mid_out_logexpsum, B_Seqlen, O, block_seq)"
  },
  {
    "module": "srt.layers.attention.triton_ops.double_sparsity_attention",
    "file": "python/sglang/srt/layers/attention/triton_ops/double_sparsity_attention.py",
    "line": 284,
    "qualname": "flash_decode_attention_fwd",
    "signature": "def flash_decode_attention_fwd(q, k_buffer, v_buffer, o, req_to_token, b_req_idx, b_start_loc, b_seq_len, attn_logits, max_len_in_batch, sm_scale, logit_cap)"
  },
  {
    "module": "srt.layers.attention.triton_ops.double_sparsity_attention",
    "file": "python/sglang/srt/layers/attention/triton_ops/double_sparsity_attention.py",
    "line": 561,
    "qualname": "sparse_flash_decode_stage1",
    "signature": "def sparse_flash_decode_stage1(q_label, k_label_buffer, att_out, Req_to_tokens, B_Seqlen, max_len_in_batch, sm_scale, logit_cap)"
  },
  {
    "module": "srt.layers.attention.triton_ops.double_sparsity_attention",
    "file": "python/sglang/srt/layers/attention/triton_ops/double_sparsity_attention.py",
    "line": 613,
    "qualname": "sparse_flash_decode_stage2",
    "signature": "def sparse_flash_decode_stage2(q, k, v, Req_to_tokens, Topk_token_indices, heavy_token_num, mid_out, mid_out_logsumexp, block_seq, sm_scale)"
  },
  {
    "module": "srt.layers.attention.triton_ops.double_sparsity_attention",
    "file": "python/sglang/srt/layers/attention/triton_ops/double_sparsity_attention.py",
    "line": 674,
    "qualname": "sparse_flash_decode_stage3",
    "signature": "def sparse_flash_decode_stage3(Seqlen, mid_out, mid_out_logexpsum, O, block_seq)"
  },
  {
    "module": "srt.layers.attention.triton_ops.double_sparsity_attention",
    "file": "python/sglang/srt/layers/attention/triton_ops/double_sparsity_attention.py",
    "line": 700,
    "qualname": "flash_decode_sparse_attention_fwd",
    "signature": "def flash_decode_sparse_attention_fwd(q, k_buffer, v_buffer, o, q_label, k_label_buffer, req_to_token, b_seq_len, max_len_in_batch, sm_scale, logit_cap, heavy_token_num, att_out_approx, mid_out, mid_o_logexpsum, BLOCK_SEQ)"
  },
  {
    "module": "srt.layers.attention.triton_ops.double_sparsity_attention",
    "file": "python/sglang/srt/layers/attention/triton_ops/double_sparsity_attention.py",
    "line": 994,
    "qualname": "extend_attention_fwd",
    "signature": "def extend_attention_fwd(q_extend, k_extend, v_extend, o_extend, k_buffer, v_buffer, req_to_tokens, b_req_idx, b_seq_len, b_seq_len_extend, b_start_loc_extend, max_len_extend, sm_scale, logit_cap)"
  },
  {
    "module": "srt.layers.attention.triton_ops.extend_attention",
    "file": "python/sglang/srt/layers/attention/triton_ops/extend_attention.py",
    "line": 36,
    "qualname": "tanh",
    "signature": "def tanh(x)"
  },
  {
    "module": "srt.layers.attention.triton_ops.extend_attention",
    "file": "python/sglang/srt/layers/attention/triton_ops/extend_attention.py",
    "line": 372,
    "qualname": "extend_attention_fwd",
    "signature": "def extend_attention_fwd(q_extend, k_extend, v_extend, o_extend, k_buffer, v_buffer, qo_indptr, kv_indptr, kv_indices, custom_mask, is_causal, mask_indptr, max_len_extend, sm_scale, logit_cap, skip_prefix_custom_mask, sliding_window_size, sinks, window_kv_offsets, xai_temperature_len)"
  },
  {
    "module": "srt.layers.attention.triton_ops.extend_attention",
    "file": "python/sglang/srt/layers/attention/triton_ops/extend_attention.py",
    "line": 516,
    "qualname": "redundant_attention",
    "signature": "def redundant_attention(q_extend, o_extend, k_buffer, v_buffer, b_req_idx, b_start_loc, b_seq_len, b_seq_len_prefix, max_len_in_batch)"
  },
  {
    "module": "srt.layers.attention.triton_ops.merge_state",
    "file": "python/sglang/srt/layers/attention/triton_ops/merge_state.py",
    "line": 9,
    "qualname": "merge_state_kernel",
    "signature": "def merge_state_kernel(output, output_lse, prefix_output, prefix_lse, suffix_output, suffix_lse, HEAD_SIZE: tl.constexpr, PADDED_HEAD_SIZE: tl.constexpr, OUTPUT_LSE: tl.constexpr)"
  },
  {
    "module": "srt.layers.attention.triton_ops.merge_state",
    "file": "python/sglang/srt/layers/attention/triton_ops/merge_state.py",
    "line": 66,
    "qualname": "merge_state_triton",
    "signature": "def merge_state_triton(prefix_output: torch.Tensor, prefix_lse: torch.Tensor, suffix_output: torch.Tensor, suffix_lse: torch.Tensor, output: Optional[torch.Tensor], output_lse: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.attention.triton_ops.prefill_attention",
    "file": "python/sglang/srt/layers/attention/triton_ops/prefill_attention.py",
    "line": 170,
    "qualname": "context_attention_fwd",
    "signature": "def context_attention_fwd(q, k, v, o, b_start_loc, b_seq_len, max_input_len, is_causal)"
  },
  {
    "module": "srt.layers.attention.triton_ops.rocm_mla_decode_rope",
    "file": "python/sglang/srt/layers/attention/triton_ops/rocm_mla_decode_rope.py",
    "line": 31,
    "qualname": "is_hip",
    "signature": "def is_hip()"
  },
  {
    "module": "srt.layers.attention.triton_ops.rocm_mla_decode_rope",
    "file": "python/sglang/srt/layers/attention/triton_ops/rocm_mla_decode_rope.py",
    "line": 39,
    "qualname": "tanh",
    "signature": "def tanh(x)"
  },
  {
    "module": "srt.layers.attention.triton_ops.rocm_mla_decode_rope",
    "file": "python/sglang/srt/layers/attention/triton_ops/rocm_mla_decode_rope.py",
    "line": 402,
    "qualname": "decode_attention_fwd_grouped_rope",
    "signature": "def decode_attention_fwd_grouped_rope(q, k_buffer, v_buffer, o, kv_indptr, kv_indices, k_pe_tokens, kv_lora_rank, rotary_dim, cos_sin_cache, positions, attn_logits, num_kv_splits, sm_scale, logit_cap, use_rope, is_neox_style)"
  },
  {
    "module": "srt.layers.attention.wave_ops.decode_attention",
    "file": "python/sglang/srt/layers/attention/wave_ops/decode_attention.py",
    "line": 27,
    "qualname": "get_wave_kernel",
    "signature": "def get_wave_kernel(shape: paged_decode_attention_shape, max_kv_splits, input_dtype, output_dtype, logit_cap)"
  },
  {
    "module": "srt.layers.attention.wave_ops.decode_attention",
    "file": "python/sglang/srt/layers/attention/wave_ops/decode_attention.py",
    "line": 92,
    "qualname": "decode_attention_intermediate_arrays_shapes",
    "signature": "def decode_attention_intermediate_arrays_shapes(num_seqs, head_size_kv, num_query_heads, max_kv_splits)"
  },
  {
    "module": "srt.layers.attention.wave_ops.decode_attention",
    "file": "python/sglang/srt/layers/attention/wave_ops/decode_attention.py",
    "line": 107,
    "qualname": "decode_attention_wave",
    "signature": "def decode_attention_wave(q, k_buffer, v_buffer, o, b_req_idx, req_to_token, attn_logits, attn_logits_max, num_kv_splits, max_kv_splits, sm_scale, logit_cap)"
  },
  {
    "module": "srt.layers.attention.wave_ops.decode_attention",
    "file": "python/sglang/srt/layers/attention/wave_ops/decode_attention.py",
    "line": 159,
    "qualname": "decode_attention_fwd",
    "signature": "def decode_attention_fwd(q, k_buffer, v_buffer, o, b_req_idx, req_to_token, attn_logits, attn_logits_max, num_kv_splits, max_kv_splits, sm_scale, logit_cap)"
  },
  {
    "module": "srt.layers.attention.wave_ops.extend_attention",
    "file": "python/sglang/srt/layers/attention/wave_ops/extend_attention.py",
    "line": 23,
    "qualname": "get_wave_kernel",
    "signature": "def get_wave_kernel(shape: AttentionShape, q_shape: tuple[int], k_shape: tuple[int], v_shape: tuple[int], k_cache_shape: tuple[int], v_cache_shape: tuple[int], o_shape: tuple[int], input_dtype: torch.dtype, output_dtype: torch.dtype, size_dtype: torch.dtype, is_causal: bool, logit_cap: float, layer_scaling: float)"
  },
  {
    "module": "srt.layers.attention.wave_ops.extend_attention",
    "file": "python/sglang/srt/layers/attention/wave_ops/extend_attention.py",
    "line": 83,
    "qualname": "extend_attention_wave",
    "signature": "def extend_attention_wave(q_extend, k_extend, v_extend, k_buffer, v_buffer, qo_indptr, kv_indptr, kv_indices, custom_mask, mask_indptr, max_seq_len, output, is_causal, layer_scaling, logit_cap)"
  },
  {
    "module": "srt.layers.attention.wave_ops.prefill_attention",
    "file": "python/sglang/srt/layers/attention/wave_ops/prefill_attention.py",
    "line": 22,
    "qualname": "prefill_attention_wave",
    "signature": "def prefill_attention_wave(q, k, v, o, b_start_loc, b_seq_len, max_seq_len, is_causal)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 23,
    "qualname": "deepep_permute_triton_kernel",
    "signature": "def deepep_permute_triton_kernel(input_ptr, gateup_input_ptr, src2dst_ptr, topk_ids_ptr, a1_scales_ptr, topk, hidden_size, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 54,
    "qualname": "deepep_post_reorder_triton_kernel",
    "signature": "def deepep_post_reorder_triton_kernel(down_output_ptr, output_ptr, src2dst_ptr, topk_ids_ptr, topk_weights_ptr, topk, hidden_size, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 87,
    "qualname": "compute_src2dst_triton_kernel",
    "signature": "def compute_src2dst_triton_kernel(reorder_ids, src2dst, num_toks, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 98,
    "qualname": "deepep_compute_src2dst_triton_kernel",
    "signature": "def deepep_compute_src2dst_triton_kernel(reorder_ids, src2dst, num_toks, num_minus_one, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 109,
    "qualname": "deepep_run_moe_deep_preprocess",
    "signature": "def deepep_run_moe_deep_preprocess(topk_ids: torch.Tensor, num_experts: int)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 132,
    "qualname": "compute_seg_indptr_triton_kernel",
    "signature": "def compute_seg_indptr_triton_kernel(reorder_topk_ids, seg_indptr, num_toks)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 148,
    "qualname": "run_moe_ep_preproess",
    "signature": "def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 167,
    "qualname": "run_cutlass_moe_ep_preproess",
    "signature": "def run_cutlass_moe_ep_preproess(local_topk_ids: torch.Tensor, local_num_experts: int)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 187,
    "qualname": "pre_reorder_triton_kernel_for_cutlass_moe",
    "signature": "def pre_reorder_triton_kernel_for_cutlass_moe(input_ptr, gateup_input_ptr, src2dst_ptr, topk_ids_ptr, a1_scales_ptr, num_experts, topk, hidden_size, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 224,
    "qualname": "pre_reorder_triton_kernel",
    "signature": "def pre_reorder_triton_kernel(input_ptr, gateup_input_ptr, src2dst_ptr, topk_ids_ptr, a1_scales_ptr, start_expert_id, end_expert_id, topk, hidden_size, BLOCK_SIZE: tl.constexpr, use_per_token_if_dynamic: tl.constexpr)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 271,
    "qualname": "silu_and_mul_triton_kernel",
    "signature": "def silu_and_mul_triton_kernel(gateup_output, down_input, hidden_size, reorder_topk_ids, scales, start_expert_id, end_expert_id, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 395,
    "qualname": "silu_and_mul_masked_post_quant_fwd",
    "signature": "def silu_and_mul_masked_post_quant_fwd(input: torch.Tensor, output: torch.Tensor, output_scale: torch.Tensor, quant_group_size: int, masked_m: torch.Tensor, scale_ue8m0: bool)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 464,
    "qualname": "tanh",
    "signature": "def tanh(x)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 469,
    "qualname": "gelu_and_mul_triton_kernel",
    "signature": "def gelu_and_mul_triton_kernel(gateup_output, down_input, hidden_size, reorder_topk_ids, scales, start_expert_id, end_expert_id, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 531,
    "qualname": "post_reorder_triton_kernel",
    "signature": "def post_reorder_triton_kernel(down_output_ptr, output_ptr, src2dst_ptr, topk_ids_ptr, topk_weights_ptr, start_expert_id, end_expert_id, topk, hidden_size, dst_start, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 585,
    "qualname": "post_reorder_triton_kernel_for_cutlass_moe",
    "signature": "def post_reorder_triton_kernel_for_cutlass_moe(down_output_ptr, output_ptr, src2dst_ptr, topk_ids_ptr, topk_weights_ptr, num_experts, topk, hidden_size, dst_start, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 628,
    "qualname": "compute_m_range",
    "signature": "def compute_m_range(pid, batch_size, seg_indptr, weight_indices, m_num_tiles_indptr, BLOCK_SIZE_M: tl.constexpr)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 651,
    "qualname": "grouped_gemm_triton_kernel",
    "signature": "def grouped_gemm_triton_kernel(a, b, c, batch_size, N, K, seg_indptr, weight_indices, m_num_tiles_indptr, scale_a, scale_b, use_fp8_w8a8: tl.constexpr, group_n: tl.constexpr, group_k: tl.constexpr, a_stride_0: tl.constexpr, b_stride_0: tl.constexpr, b_stride_1: tl.constexpr, as_stride_0: tl.constexpr, as_stride_1: tl.constexpr, bs_stride_0: tl.constexpr, bs_stride_2: tl.constexpr, bs_stride_1: tl.constexpr, use_per_token_if_dynamic: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 755,
    "qualname": "compute_m_num_tiles_indptr",
    "signature": "def compute_m_num_tiles_indptr(m_num_tiles_indptr, seg_indptr, batch_size: tl.constexpr, BLOCK_SIZE_M: tl.constexpr)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 765,
    "qualname": "grouped_gemm_triton",
    "signature": "def grouped_gemm_triton(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, batch_size: int, weight_column_major: bool, seg_indptr: Optional[torch.Tensor], weight_indices: Optional[torch.Tensor], use_fp8_w8a8: bool, scale_a: torch.Tensor, scale_b: torch.Tensor, block_shape: Optional[List[int]], c_dtype, use_per_token_if_dynamic: bool)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 960,
    "qualname": "ep_scatter",
    "signature": "def ep_scatter(recv_x: torch.Tensor, recv_x_scale: torch.Tensor, recv_topk: torch.Tensor, num_recv_tokens_per_expert: torch.Tensor, expert_start_loc: torch.Tensor, output_tensor: torch.Tensor, output_tensor_scale: torch.Tensor, m_indices: torch.Tensor, output_index: torch.Tensor, scale_ue8m0: bool)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 1100,
    "qualname": "ep_gather",
    "signature": "def ep_gather(input_tensor: torch.Tensor, recv_topk_ids: torch.Tensor, recv_topk_weight: torch.Tensor, input_index: torch.Tensor, output_tensor: torch.Tensor)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 1139,
    "qualname": "get_tma_aligned_size",
    "signature": "def get_tma_aligned_size(x: int, element_size: int)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 1189,
    "qualname": "tma_align_input_scale",
    "signature": "def tma_align_input_scale(input_scale: torch.Tensor)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 1215,
    "qualname": "compute_masked_m_triton_kernel",
    "signature": "def compute_masked_m_triton_kernel(seg_indptr, masked_m)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 1223,
    "qualname": "deepgemm_compute_src2dst_triton_kernel",
    "signature": "def deepgemm_compute_src2dst_triton_kernel(topk_ids, reorder_ids, seg_indptr, src2dst, m_max, num_toks, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 1244,
    "qualname": "fill_gateup_input_triton_kernel",
    "signature": "def fill_gateup_input_triton_kernel(input_ptr, scale_ptr, gateup_input_ptr, gateup_input_scale_ptr, src2dst_ptr, topk_ids_ptr, start_expert_id, end_expert_id, topk, m_max, hidden_size, scale_size, BLOCK_SIZE: tl.constexpr)"
  },
  {
    "module": "srt.layers.moe.ep_moe.kernels",
    "file": "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "line": 1288,
    "qualname": "moe_ep_deepgemm_preprocess",
    "signature": "def moe_ep_deepgemm_preprocess(topk_ids: torch.Tensor, num_experts: int, hidden_states: torch.Tensor, top_k: int, start_expert_id, end_expert_id, block_shape, output_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.moe.ep_moe.layer",
    "file": "python/sglang/srt/layers/moe/ep_moe/layer.py",
    "line": 82,
    "qualname": "EPMoE.__init__",
    "signature": "def __init__(self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, num_fused_shared_experts: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str, activation: str, routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_clamp_limit: Optional[float], with_bias: bool)"
  },
  {
    "module": "srt.layers.moe.ep_moe.layer",
    "file": "python/sglang/srt/layers/moe/ep_moe/layer.py",
    "line": 138,
    "qualname": "EPMoE.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, topk_output: TopKOutput)"
  },
  {
    "module": "srt.layers.moe.ep_moe.layer",
    "file": "python/sglang/srt/layers/moe/ep_moe/layer.py",
    "line": 144,
    "qualname": "EPMoE.forward_deepgemm",
    "signature": "def forward_deepgemm(self, hidden_states: torch.Tensor, topk_output: TopKOutput)"
  },
  {
    "module": "srt.layers.moe.ep_moe.layer",
    "file": "python/sglang/srt/layers/moe/ep_moe/layer.py",
    "line": 339,
    "qualname": "DeepEPMoE.__init__",
    "signature": "def __init__(self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, num_fused_shared_experts: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str, activation: str, routed_scaling_factor: Optional[float])"
  },
  {
    "module": "srt.layers.moe.ep_moe.layer",
    "file": "python/sglang/srt/layers/moe/ep_moe/layer.py",
    "line": 421,
    "qualname": "DeepEPMoE.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.moe.ep_moe.layer",
    "file": "python/sglang/srt/layers/moe/ep_moe/layer.py",
    "line": 440,
    "qualname": "DeepEPMoE.dispatch",
    "signature": "def dispatch(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.moe.ep_moe.layer",
    "file": "python/sglang/srt/layers/moe/ep_moe/layer.py",
    "line": 454,
    "qualname": "DeepEPMoE.moe_impl",
    "signature": "def moe_impl(self, dispatch_output: DispatchOutput)"
  },
  {
    "module": "srt.layers.moe.ep_moe.layer",
    "file": "python/sglang/srt/layers/moe/ep_moe/layer.py",
    "line": 475,
    "qualname": "DeepEPMoE.combine",
    "signature": "def combine(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.moe.ep_moe.layer",
    "file": "python/sglang/srt/layers/moe/ep_moe/layer.py",
    "line": 489,
    "qualname": "DeepEPMoE.forward_aiter",
    "signature": "def forward_aiter(self, dispatch_output: Union[DeepEPNormalOutput, DeepEPLLOutput])"
  },
  {
    "module": "srt.layers.moe.ep_moe.layer",
    "file": "python/sglang/srt/layers/moe/ep_moe/layer.py",
    "line": 523,
    "qualname": "DeepEPMoE.forward_deepgemm_contiguous",
    "signature": "def forward_deepgemm_contiguous(self, dispatch_output: DeepEPNormalOutput)"
  },
  {
    "module": "srt.layers.moe.ep_moe.layer",
    "file": "python/sglang/srt/layers/moe/ep_moe/layer.py",
    "line": 647,
    "qualname": "DeepEPMoE.forward_deepgemm_masked",
    "signature": "def forward_deepgemm_masked(self, dispatch_output: DeepEPLLOutput)"
  },
  {
    "module": "srt.layers.moe.ep_moe.layer",
    "file": "python/sglang/srt/layers/moe/ep_moe/layer.py",
    "line": 724,
    "qualname": "DeepEPMoE.forward_npu",
    "signature": "def forward_npu(self, dispatch_output: DeepEPLLOutput)"
  },
  {
    "module": "srt.layers.moe.ep_moe.layer",
    "file": "python/sglang/srt/layers/moe/ep_moe/layer.py",
    "line": 779,
    "qualname": "get_moe_impl_class",
    "signature": "def get_moe_impl_class(quant_config: Optional[QuantizationConfig])"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.__init__",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/__init__.py",
    "line": 19,
    "qualname": "override_config",
    "signature": "def override_config(config)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.__init__",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/__init__.py",
    "line": 27,
    "qualname": "get_config",
    "signature": "def get_config()"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 72,
    "qualname": "write_zeros_to_output",
    "signature": "def write_zeros_to_output(c_ptr, stride_cm, stride_cn, pid_n, N, offs_token, token_mask, BLOCK_SIZE_M, BLOCK_SIZE_N, compute_type)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 92,
    "qualname": "fused_moe_kernel_gptq_awq",
    "signature": "def fused_moe_kernel_gptq_awq(a_ptr, b_ptr, c_ptr, b_scale_ptr, b_zp_ptr, topk_weights_ptr, sorted_token_ids_ptr, expert_ids_ptr, num_tokens_post_padded_ptr, N: tl.constexpr, K: tl.constexpr, EM, num_valid_tokens, stride_am, stride_ak, stride_be, stride_bk, stride_bn, stride_cm, stride_cn, stride_bse, stride_bsk, stride_bsn, stride_bze, stride_bzk, stride_bzn, group_size: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, MUL_ROUTED_WEIGHT: tl.constexpr, top_k: tl.constexpr, compute_type: tl.constexpr, has_zp: tl.constexpr, use_int4_w4a16: tl.constexpr, use_int8_w8a16: tl.constexpr, even_Ks: tl.constexpr)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 323,
    "qualname": "fused_moe_kernel",
    "signature": "def fused_moe_kernel(a_ptr, b_ptr, bias_ptr, c_ptr, a_scale_ptr, b_scale_ptr, topk_weights_ptr, sorted_token_ids_ptr, expert_ids_ptr, num_tokens_post_padded_ptr, N, K, EM, num_valid_tokens, stride_am, stride_ak, stride_be, stride_bk, stride_bn, stride_bias_e, stride_bias_n, stride_cm, stride_cn, stride_asm, stride_ask, stride_bse, stride_bsk, stride_bsn, group_n: tl.constexpr, group_k: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, MUL_ROUTED_WEIGHT: tl.constexpr, top_k: tl.constexpr, compute_type: tl.constexpr, use_fp8_w8a8: tl.constexpr, use_int8_w8a8: tl.constexpr, use_int8_w8a16: tl.constexpr, per_channel_quant: tl.constexpr, even_Ks: tl.constexpr)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 563,
    "qualname": "moe_align_block_size",
    "signature": "def moe_align_block_size(topk_ids: torch.Tensor, block_size: int, num_experts: int)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 636,
    "qualname": "invoke_fused_moe_kernel",
    "signature": "def invoke_fused_moe_kernel(A: torch.Tensor, B: torch.Tensor, bias: Optional[torch.Tensor], C: torch.Tensor, A_scale: Optional[torch.Tensor], B_scale: Optional[torch.Tensor], B_zp: Optional[torch.Tensor], topk_weights: torch.Tensor, topk_ids: torch.Tensor, sorted_token_ids: torch.Tensor, expert_ids: torch.Tensor, num_tokens_post_padded: torch.Tensor, mul_routed_weight: bool, top_k: int, config: Dict[str, Any], compute_type: tl.dtype, use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, block_shape: Optional[List[int]], no_combine: bool)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 813,
    "qualname": "get_config_file_name",
    "signature": "def get_config_file_name(E: int, N: int, dtype: Optional[str], block_shape: Optional[int])"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 825,
    "qualname": "get_moe_configs",
    "signature": "def get_moe_configs(E: int, N: int, dtype: Optional[str], block_n: Optional[int], block_k: Optional[int])"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 898,
    "qualname": "get_default_config",
    "signature": "def get_default_config(M: int, E: int, N: int, K: int, topk: int, dtype: Optional[str], is_marlin: bool, block_shape: Optional[List[int]])"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 955,
    "qualname": "try_get_optimal_moe_config",
    "signature": "def try_get_optimal_moe_config(w1_shape: Tuple[int, ...], w2_shape: Tuple[int, ...], top_k: int, dtype: Optional[str], M: int, is_marlin: bool, block_shape: Optional[List[int]])"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 988,
    "qualname": "get_config_dtype_str",
    "signature": "def get_config_dtype_str(dtype: torch.dtype, use_int8_w8a16: Optional[bool], use_int4_w4a16: Optional[bool], use_fp8_w8a8: Optional[bool], use_int8_w8a8: Optional[bool])"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 1010,
    "qualname": "inplace_fused_experts",
    "signature": "def inplace_fused_experts(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]], routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_limit: Optional[float])"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 1066,
    "qualname": "inplace_fused_experts_fake",
    "signature": "def inplace_fused_experts_fake(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]], routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_limit: Optional[float])"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 1103,
    "qualname": "outplace_fused_experts",
    "signature": "def outplace_fused_experts(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]], no_combine: bool, routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_limit: Optional[float])"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 1160,
    "qualname": "outplace_fused_experts_fake",
    "signature": "def outplace_fused_experts_fake(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]], no_combine: bool, routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_limit: Optional[float])"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 1198,
    "qualname": "fused_experts",
    "signature": "def fused_experts(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]])"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 1329,
    "qualname": "moe_sum_reduce_triton",
    "signature": "def moe_sum_reduce_triton(input: torch.Tensor, output: torch.Tensor, routed_scaling_factor: float)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 1366,
    "qualname": "moe_sum_reduce_torch_compile",
    "signature": "def moe_sum_reduce_torch_compile(x, out, routed_scaling_factor)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 1372,
    "qualname": "swiglu_with_alpha_and_limit",
    "signature": "def swiglu_with_alpha_and_limit(x, gemm1_alpha, gemm1_limit)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 1379,
    "qualname": "fused_experts_impl",
    "signature": "def fused_experts_impl(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor, topk_ids: torch.Tensor, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], inplace: bool, activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]], no_combine: bool, routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_limit: Optional[float])"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.fused_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "line": 1646,
    "qualname": "fused_moe",
    "signature": "def fused_moe(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_output: StandardTopKOutput, moe_runner_config: MoeRunnerConfig, b1: Optional[torch.Tensor], b2: Optional[torch.Tensor], use_fp8_w8a8: bool, use_int8_w8a8: bool, use_int8_w8a16: bool, use_int4_w4a16: bool, per_channel_quant: bool, w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[List[int]])"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.layer",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "line": 119,
    "qualname": "FusedMoE.__init__",
    "signature": "def __init__(self, num_experts: int, hidden_size: int, intermediate_size: int, layer_id: int, top_k: Optional[int], num_fused_shared_experts: int, params_dtype: Optional[torch.dtype], reduce_results: bool, quant_config: Optional[QuantizationConfig], prefix: str, activation: str, apply_router_weight_on_input: bool, use_presharded_weights: bool, inplace: bool, no_combine: bool, routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_clamp_limit: Optional[float], use_weight_loader_fused: bool, with_bias)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.layer",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "line": 459,
    "qualname": "FusedMoE.weight_loader",
    "signature": "def weight_loader(self, param: torch.nn.Parameter, loaded_weight: torch.Tensor, weight_name: str, shard_id: str, expert_id: Optional[int])"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.layer",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "line": 716,
    "qualname": "FusedMoE.weight_loader_fused",
    "signature": "def weight_loader_fused(self, param: torch.nn.Parameter, loaded_weight: torch.Tensor, weight_name: str, shard_id: str)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.layer",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "line": 794,
    "qualname": "FusedMoE.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, topk_output: TopKOutput)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.layer",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "line": 832,
    "qualname": "FusedMoE.make_expert_params_mapping",
    "signature": "def make_expert_params_mapping(cls, ckpt_gate_proj_name: str, ckpt_down_proj_name: str, ckpt_up_proj_name: str, num_experts: int)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.layer",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "line": 861,
    "qualname": "FusedMoE.make_expert_params_mapping_fused",
    "signature": "def make_expert_params_mapping_fused(cls, ckpt_gate_up_proj_name: str, ckpt_down_proj_name: str, ckpt_gate_up_proj_bias_name: str, ckpt_down_proj_bias_name: str)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.layer",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "line": 880,
    "qualname": "FusedMoE.make_expert_params_mapping_fused_mxfp4",
    "signature": "def make_expert_params_mapping_fused_mxfp4(cls, ckpt_gate_up_proj_name: str, ckpt_down_proj_name: str, ckpt_gate_up_proj_bias_name: str, ckpt_down_proj_bias_name: str, ckpt_gate_up_proj_scale_name: str, ckpt_down_proj_scale_name: str)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.layer",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "line": 907,
    "qualname": "FusedMoE.make_expert_input_scale_params_mapping",
    "signature": "def make_expert_input_scale_params_mapping(cls, num_experts: int)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.layer",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "line": 923,
    "qualname": "FusedMoE.should_fuse_routed_scaling_factor_in_topk",
    "signature": "def should_fuse_routed_scaling_factor_in_topk(self)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.layer",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "line": 931,
    "qualname": "FlashInferFusedMoE.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.layer",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "line": 935,
    "qualname": "FlashInferFusedMoE.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, topk_output: TopKOutput)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.layer",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "line": 967,
    "qualname": "FlashInferFP4MoE.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.layer",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "line": 1000,
    "qualname": "FlashInferFP4MoE.forward",
    "signature": "def forward(self, hidden_states: torch.Tensor, topk_output: TopKOutput)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.layer",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "line": 1057,
    "qualname": "get_fused_moe_impl_class",
    "signature": "def get_fused_moe_impl_class()"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.triton_kernels_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py",
    "line": 25,
    "qualname": "quantize",
    "signature": "def quantize(w, dtype, dev)"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.triton_kernels_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py",
    "line": 54,
    "qualname": "triton_kernel_moe_forward",
    "signature": "def triton_kernel_moe_forward(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, per_channel_quant: bool, global_num_experts: int, expert_map: Optional[torch.Tensor], w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[list[int]])"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.triton_kernels_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py",
    "line": 101,
    "qualname": "triton_kernel_fused_experts",
    "signature": "def triton_kernel_fused_experts(hidden_states: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor, routing_data: RoutingData, gather_indx: GatherIndx, scatter_indx: ScatterIndx, inplace: bool, activation: str, apply_router_weight_on_input: bool, use_fp8_w8a8: bool, per_channel_quant: bool, global_num_experts: int, expert_map: Optional[torch.Tensor], w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[list[int]])"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.triton_kernels_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py",
    "line": 189,
    "qualname": "triton_kernel_moe_with_bias_forward",
    "signature": "def triton_kernel_moe_with_bias_forward(hidden_states: torch.Tensor, w1: torch.Tensor, w1_pcg, b1: torch.Tensor, w2: torch.Tensor, w2_pcg, b2: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig, use_fp8_w8a8: bool, per_channel_quant: bool, global_num_experts: int, expert_map: Optional[torch.Tensor], w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[list[int]])"
  },
  {
    "module": "srt.layers.moe.fused_moe_triton.triton_kernels_moe",
    "file": "python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py",
    "line": 242,
    "qualname": "triton_kernel_fused_experts_with_bias",
    "signature": "def triton_kernel_fused_experts_with_bias(hidden_states: torch.Tensor, w1: torch.Tensor, w1_pcg, b1: torch.Tensor, w2: torch.Tensor, w2_pcg, b2: torch.Tensor, routing_data: RoutingData, gather_indx: GatherIndx, scatter_indx: ScatterIndx, inplace: bool, activation: str, use_fp8_w8a8: bool, per_channel_quant: bool, global_num_experts: int, expert_map: Optional[torch.Tensor], w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor], a1_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], block_shape: Optional[list[int]], gemm1_alpha: Optional[float], gemm1_clamp_limit: Optional[float])"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.base_dispatcher",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/base_dispatcher.py",
    "line": 21,
    "qualname": "DispatchOutputChecker.format_is_standard",
    "signature": "def format_is_standard(dispatch_output: DispatchOutput)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.base_dispatcher",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/base_dispatcher.py",
    "line": 27,
    "qualname": "DispatchOutputChecker.format_is_deepep_normal",
    "signature": "def format_is_deepep_normal(dispatch_output: DispatchOutput)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.base_dispatcher",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/base_dispatcher.py",
    "line": 33,
    "qualname": "DispatchOutputChecker.format_is_deepep_ll",
    "signature": "def format_is_deepep_ll(dispatch_output: DispatchOutput)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.base_dispatcher",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/base_dispatcher.py",
    "line": 39,
    "qualname": "DispatchOutputChecker.format_is_deepep",
    "signature": "def format_is_deepep(dispatch_output: DispatchOutput)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.base_dispatcher",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/base_dispatcher.py",
    "line": 45,
    "qualname": "DispatchOutputChecker.format_is_ascent_ll",
    "signature": "def format_is_ascent_ll(dispatch_output: DispatchOutput)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.base_dispatcher",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/base_dispatcher.py",
    "line": 58,
    "qualname": "DispatchOutputFormat.is_standard",
    "signature": "def is_standard(self)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.base_dispatcher",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/base_dispatcher.py",
    "line": 61,
    "qualname": "DispatchOutputFormat.is_deepep_normal",
    "signature": "def is_deepep_normal(self)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.base_dispatcher",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/base_dispatcher.py",
    "line": 64,
    "qualname": "DispatchOutputFormat.is_deepep_ll",
    "signature": "def is_deepep_ll(self)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.base_dispatcher",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/base_dispatcher.py",
    "line": 67,
    "qualname": "DispatchOutputFormat.is_deepep",
    "signature": "def is_deepep(self)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.base_dispatcher",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/base_dispatcher.py",
    "line": 73,
    "qualname": "DispatchOutputFormat.is_ascent_ll",
    "signature": "def is_ascent_ll(self)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.base_dispatcher",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/base_dispatcher.py",
    "line": 82,
    "qualname": "DispatchOutput.format",
    "signature": "def format(self)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.base_dispatcher",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/base_dispatcher.py",
    "line": 95,
    "qualname": "BaseDispatcher.dispatch",
    "signature": "def dispatch(self)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.base_dispatcher",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/base_dispatcher.py",
    "line": 99,
    "qualname": "BaseDispatcher.combine",
    "signature": "def combine(self)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 64,
    "qualname": "DeepEPNormalOutput.format",
    "signature": "def format(self)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 78,
    "qualname": "DeepEPLLOutput.format",
    "signature": "def format(self)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 93,
    "qualname": "AscendDeepEPLLOutput.format",
    "signature": "def format(self)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 115,
    "qualname": "DeepEPBuffer.get_deepep_buffer",
    "signature": "def get_deepep_buffer(cls, group: dist.ProcessGroup, hidden_size: int, param_bytes: int, deepep_mode: DeepEPMode, num_max_dispatch_tokens_per_rank: int, num_experts: int)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 195,
    "qualname": "DeepEPBuffer.clean_buffer",
    "signature": "def clean_buffer(cls)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 205,
    "qualname": "DeepEPBuffer.set_dispatch_mode_as_normal",
    "signature": "def set_dispatch_mode_as_normal(cls)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 209,
    "qualname": "DeepEPBuffer.set_dispatch_mode_as_low_latency",
    "signature": "def set_dispatch_mode_as_low_latency(cls)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 218,
    "qualname": "DeepEPConfig.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 238,
    "qualname": "DeepEPConfig.get_instance",
    "signature": "def get_instance(cls)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 245,
    "qualname": "_DeepEPDispatcherImplBase.__init__",
    "signature": "def __init__(self, group: torch.distributed.ProcessGroup, router_topk: int, permute_fusion: bool, num_experts: int, num_local_experts: int, hidden_size: int, params_dtype: torch.dtype, deepep_mode: DeepEPMode)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 278,
    "qualname": "_DeepEPDispatcherImplBase.dispatch_a",
    "signature": "def dispatch_a(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 286,
    "qualname": "_DeepEPDispatcherImplBase.dispatch_b",
    "signature": "def dispatch_b(self)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 289,
    "qualname": "_DeepEPDispatcherImplBase.combine_a",
    "signature": "def combine_a(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 297,
    "qualname": "_DeepEPDispatcherImplBase.combine_b",
    "signature": "def combine_b(self)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 305,
    "qualname": "_DeepEPDispatcherImplNormal.__init__",
    "signature": "def __init__(self, async_finish: bool)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 311,
    "qualname": "_DeepEPDispatcherImplNormal.dispatch_a",
    "signature": "def dispatch_a(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 330,
    "qualname": "_DeepEPDispatcherImplNormal.dispatch_b",
    "signature": "def dispatch_b(self, hidden_states, topk_idx, topk_weights, previous_event)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 406,
    "qualname": "_DeepEPDispatcherImplNormal.combine_a",
    "signature": "def combine_a(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 441,
    "qualname": "_DeepEPDispatcherImplNormal.combine_b",
    "signature": "def combine_b(self, output, previous_event)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 474,
    "qualname": "_DeepEPDispatcherImplLowLatency.__init__",
    "signature": "def __init__(self, return_recv_hook: bool)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 483,
    "qualname": "_DeepEPDispatcherImplLowLatency.dispatch_a",
    "signature": "def dispatch_a(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 510,
    "qualname": "_DeepEPDispatcherImplLowLatency.dispatch_b",
    "signature": "def dispatch_b(self, hidden_states, topk_idx, topk_weights, masked_m, expected_m, event, hook)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 569,
    "qualname": "_DeepEPDispatcherImplLowLatency.combine_a",
    "signature": "def combine_a(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 582,
    "qualname": "_DeepEPDispatcherImplLowLatency.combine_b",
    "signature": "def combine_b(self, hidden_states, event, hook)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 625,
    "qualname": "DeepEPDispatcher.__init__",
    "signature": "def __init__(self, group: torch.distributed.ProcessGroup, router_topk: int, permute_fusion: bool, num_experts: int, num_local_experts: int, hidden_size: int, params_dtype: torch.dtype, deepep_mode: DeepEPMode, async_finish: bool, return_recv_hook: bool)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 664,
    "qualname": "DeepEPDispatcher.dispatch",
    "signature": "def dispatch(self)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 669,
    "qualname": "DeepEPDispatcher.dispatch_a",
    "signature": "def dispatch_a(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 684,
    "qualname": "DeepEPDispatcher.dispatch_b",
    "signature": "def dispatch_b(self)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 690,
    "qualname": "DeepEPDispatcher.combine",
    "signature": "def combine(self)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 695,
    "qualname": "DeepEPDispatcher.combine_a",
    "signature": "def combine_a(self, hidden_states: torch.Tensor, topk_idx: torch.Tensor, topk_weights: torch.Tensor, forward_batch: ForwardBatch)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.deepep",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/deepep.py",
    "line": 710,
    "qualname": "DeepEPDispatcher.combine_b",
    "signature": "def combine_b(self)"
  },
  {
    "module": "srt.layers.moe.token_dispatcher.standard",
    "file": "python/sglang/srt/layers/moe/token_dispatcher/standard.py",
    "line": 15,
    "qualname": "StandardDispatchOutput.format",
    "signature": "def format(self)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py",
    "line": 64,
    "qualname": "DeviceCapability.as_version_str",
    "signature": "def as_version_str(self)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py",
    "line": 67,
    "qualname": "DeviceCapability.to_int",
    "signature": "def to_int(self)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py",
    "line": 79,
    "qualname": "CompressedTensorsConfig.__init__",
    "signature": "def __init__(self, target_scheme_map: Dict[str, Any], ignore: List[str], quant_format: str, sparsity_scheme_map: Dict[str, SparsityCompressionConfig], sparsity_ignore_list: List[str], kv_cache_scheme: Optional[Dict[str, Any]], config: Optional[Dict[str, Any]], packed_modules_mapping: Dict[str, List[str]])"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py",
    "line": 101,
    "qualname": "CompressedTensorsConfig.get_linear_method",
    "signature": "def get_linear_method(self)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py",
    "line": 104,
    "qualname": "CompressedTensorsConfig.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(cls)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py",
    "line": 108,
    "qualname": "CompressedTensorsConfig.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py",
    "line": 111,
    "qualname": "CompressedTensorsConfig.get_name",
    "signature": "def get_name(self)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py",
    "line": 114,
    "qualname": "CompressedTensorsConfig.get_scaled_act_names",
    "signature": "def get_scaled_act_names(self)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py",
    "line": 117,
    "qualname": "CompressedTensorsConfig.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py",
    "line": 143,
    "qualname": "CompressedTensorsConfig.from_config",
    "signature": "def from_config(cls, config: Dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py",
    "line": 234,
    "qualname": "CompressedTensorsConfig.get_config_filenames",
    "signature": "def get_config_filenames(cls)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py",
    "line": 438,
    "qualname": "CompressedTensorsConfig.get_scheme",
    "signature": "def get_scheme(self, layer: torch.nn.Module, layer_name: Optional[str])"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py",
    "line": 533,
    "qualname": "CompressedTensorsConfig.get_cache_scale",
    "signature": "def get_cache_scale(self, name: str)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py",
    "line": 550,
    "qualname": "CompressedTensorsConfig.supports_cutlass_24",
    "signature": "def supports_cutlass_24(weight_quant: Optional[QuantizationArgs], input_quant: Optional[QuantizationArgs], sparsity_scheme: Optional[SparsityCompressionConfig])"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py",
    "line": 618,
    "qualname": "CompressedTensorsLinearMethod.__init__",
    "signature": "def __init__(self, quantization_config: CompressedTensorsConfig)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py",
    "line": 621,
    "qualname": "CompressedTensorsLinearMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py",
    "line": 624,
    "qualname": "CompressedTensorsLinearMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py",
    "line": 650,
    "qualname": "CompressedTensorsLinearMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors_moe",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors_moe.py",
    "line": 70,
    "qualname": "CompressedTensorsMoEMethod.__new__",
    "signature": "def __new__(cls)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors_moe",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors_moe.py",
    "line": 76,
    "qualname": "CompressedTensorsMoEMethod.get_moe_method",
    "signature": "def get_moe_method(quant_config: CompressedTensorsConfig)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors_moe",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors_moe.py",
    "line": 100,
    "qualname": "CompressedTensorsW8A8Fp8MoEMethod.__init__",
    "signature": "def __init__(self, quant_config: CompressedTensorsConfig)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors_moe",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors_moe.py",
    "line": 109,
    "qualname": "CompressedTensorsW8A8Fp8MoEMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors_moe",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors_moe.py",
    "line": 207,
    "qualname": "CompressedTensorsW8A8Fp8MoEMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: FusedMoE)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors_moe",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors_moe.py",
    "line": 296,
    "qualname": "CompressedTensorsW8A8Fp8MoEMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors_moe",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors_moe.py",
    "line": 346,
    "qualname": "CompressedTensorsWNA16MoEMethod.__init__",
    "signature": "def __init__(self, quant_config: CompressedTensorsConfig)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors_moe",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors_moe.py",
    "line": 369,
    "qualname": "CompressedTensorsWNA16MoEMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors_moe",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors_moe.py",
    "line": 513,
    "qualname": "CompressedTensorsWNA16MoEMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.compressed_tensors_moe",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors_moe.py",
    "line": 643,
    "qualname": "CompressedTensorsWNA16MoEMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.utils",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/utils.py",
    "line": 12,
    "qualname": "is_activation_quantization_format",
    "signature": "def is_activation_quantization_format(format: str)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.utils",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/utils.py",
    "line": 21,
    "qualname": "should_ignore_layer",
    "signature": "def should_ignore_layer(layer_name: Optional[str], ignore: Iterable[str], fused_mapping: Mapping[str, List[str]])"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.utils",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/utils.py",
    "line": 76,
    "qualname": "check_equal_or_regex_match",
    "signature": "def check_equal_or_regex_match(layer_name: str, targets: Iterable[str])"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.utils",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/utils.py",
    "line": 87,
    "qualname": "find_matched_target",
    "signature": "def find_matched_target(layer_name: Optional[str], module: Module, targets: Iterable[str], fused_mapping: Mapping[str, List[str]])"
  },
  {
    "module": "srt.layers.quantization.deep_gemm_wrapper.entrypoint",
    "file": "python/sglang/srt/layers/quantization/deep_gemm_wrapper/entrypoint.py",
    "line": 23,
    "qualname": "grouped_gemm_nt_f8f8bf16_masked",
    "signature": "def grouped_gemm_nt_f8f8bf16_masked(lhs: Tuple[torch.Tensor, torch.Tensor], rhs: Tuple[torch.Tensor, torch.Tensor], out: torch.Tensor, masked_m: torch.Tensor, expected_m: int)"
  },
  {
    "module": "srt.layers.quantization.deep_gemm_wrapper.entrypoint",
    "file": "python/sglang/srt/layers/quantization/deep_gemm_wrapper/entrypoint.py",
    "line": 46,
    "qualname": "grouped_gemm_nt_f8f8bf16_contig",
    "signature": "def grouped_gemm_nt_f8f8bf16_contig(lhs: Tuple[torch.Tensor, torch.Tensor], rhs: Tuple[torch.Tensor, torch.Tensor], out: torch.Tensor, m_indices: torch.Tensor)"
  },
  {
    "module": "srt.layers.quantization.deep_gemm_wrapper.entrypoint",
    "file": "python/sglang/srt/layers/quantization/deep_gemm_wrapper/entrypoint.py",
    "line": 60,
    "qualname": "gemm_nt_f8f8bf16",
    "signature": "def gemm_nt_f8f8bf16(lhs: Tuple[torch.Tensor, torch.Tensor], rhs: Tuple[torch.Tensor, torch.Tensor], out: torch.Tensor)"
  },
  {
    "module": "srt.layers.quantization.deep_gemm_wrapper.entrypoint",
    "file": "python/sglang/srt/layers/quantization/deep_gemm_wrapper/entrypoint.py",
    "line": 78,
    "qualname": "update_deep_gemm_config",
    "signature": "def update_deep_gemm_config(gpu_id: int, server_args: ServerArgs)"
  },
  {
    "module": "srt.layers.quantization.deep_gemm_wrapper.entrypoint",
    "file": "python/sglang/srt/layers/quantization/deep_gemm_wrapper/entrypoint.py",
    "line": 83,
    "qualname": "configure_deep_gemm_num_sms",
    "signature": "def configure_deep_gemm_num_sms(num_sms)"
  },
  {
    "module": "srt.layers.quantization.deep_gemm_wrapper.compile_utils",
    "file": "python/sglang/srt/layers/quantization/deep_gemm_wrapper/compile_utils.py",
    "line": 42,
    "qualname": "update_deep_gemm_config",
    "signature": "def update_deep_gemm_config(gpu_id: int, server_args: ServerArgs)"
  },
  {
    "module": "srt.layers.quantization.deep_gemm_wrapper.compile_utils",
    "file": "python/sglang/srt/layers/quantization/deep_gemm_wrapper/compile_utils.py",
    "line": 142,
    "qualname": "_BaseWarmupExecutor.create",
    "signature": "def create(kernel_type: DeepGemmKernelType)"
  },
  {
    "module": "srt.layers.quantization.deep_gemm_wrapper.compile_utils",
    "file": "python/sglang/srt/layers/quantization/deep_gemm_wrapper/compile_utils.py",
    "line": 149,
    "qualname": "_BaseWarmupExecutor.execute",
    "signature": "def execute(self, m)"
  },
  {
    "module": "srt.layers.quantization.deep_gemm_wrapper.compile_utils",
    "file": "python/sglang/srt/layers/quantization/deep_gemm_wrapper/compile_utils.py",
    "line": 179,
    "qualname": "_NormalWarmupExecutor.__init__",
    "signature": "def __init__(self, max_m: int, n: int, k: int, num_groups: int)"
  },
  {
    "module": "srt.layers.quantization.deep_gemm_wrapper.compile_utils",
    "file": "python/sglang/srt/layers/quantization/deep_gemm_wrapper/compile_utils.py",
    "line": 184,
    "qualname": "_NormalWarmupExecutor.execute",
    "signature": "def execute(self, m)"
  },
  {
    "module": "srt.layers.quantization.deep_gemm_wrapper.compile_utils",
    "file": "python/sglang/srt/layers/quantization/deep_gemm_wrapper/compile_utils.py",
    "line": 193,
    "qualname": "_GroupedContWarmupExecutor.__init__",
    "signature": "def __init__(self, max_m: int, n: int, k: int, num_groups: int)"
  },
  {
    "module": "srt.layers.quantization.deep_gemm_wrapper.compile_utils",
    "file": "python/sglang/srt/layers/quantization/deep_gemm_wrapper/compile_utils.py",
    "line": 199,
    "qualname": "_GroupedContWarmupExecutor.execute",
    "signature": "def execute(self, m)"
  },
  {
    "module": "srt.layers.quantization.deep_gemm_wrapper.compile_utils",
    "file": "python/sglang/srt/layers/quantization/deep_gemm_wrapper/compile_utils.py",
    "line": 209,
    "qualname": "_GroupedMaskedWarmupExecutor.__init__",
    "signature": "def __init__(self, max_m: int, n: int, k: int, num_groups: int)"
  },
  {
    "module": "srt.layers.quantization.deep_gemm_wrapper.compile_utils",
    "file": "python/sglang/srt/layers/quantization/deep_gemm_wrapper/compile_utils.py",
    "line": 217,
    "qualname": "_GroupedMaskedWarmupExecutor.execute",
    "signature": "def execute(self, m)"
  },
  {
    "module": "srt.layers.quantization.deep_gemm_wrapper.compile_utils",
    "file": "python/sglang/srt/layers/quantization/deep_gemm_wrapper/compile_utils.py",
    "line": 229,
    "qualname": "deep_gemm_execution_hook",
    "signature": "def deep_gemm_execution_hook(m: int, n: int, k: int, num_groups: int, kernel_type: DeepGemmKernelType)"
  },
  {
    "module": "srt.layers.quantization.quark.quark",
    "file": "python/sglang/srt/layers/quantization/quark/quark.py",
    "line": 29,
    "qualname": "QuarkConfig.__init__",
    "signature": "def __init__(self, quant_config: dict[str, Any], kv_cache_group: Optional[list[str]], kv_cache_config: Optional[dict[str, Any]], pack_method: str)"
  },
  {
    "module": "srt.layers.quantization.quark.quark",
    "file": "python/sglang/srt/layers/quantization/quark/quark.py",
    "line": 46,
    "qualname": "QuarkConfig.get_linear_method",
    "signature": "def get_linear_method(self)"
  },
  {
    "module": "srt.layers.quantization.quark.quark",
    "file": "python/sglang/srt/layers/quantization/quark/quark.py",
    "line": 50,
    "qualname": "QuarkConfig.get_supported_act_dtypes",
    "signature": "def get_supported_act_dtypes(cls)"
  },
  {
    "module": "srt.layers.quantization.quark.quark",
    "file": "python/sglang/srt/layers/quantization/quark/quark.py",
    "line": 54,
    "qualname": "QuarkConfig.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.quark.quark",
    "file": "python/sglang/srt/layers/quantization/quark/quark.py",
    "line": 57,
    "qualname": "QuarkConfig.get_name",
    "signature": "def get_name(self)"
  },
  {
    "module": "srt.layers.quantization.quark.quark",
    "file": "python/sglang/srt/layers/quantization/quark/quark.py",
    "line": 60,
    "qualname": "QuarkConfig.get_quant_method",
    "signature": "def get_quant_method(self, layer: torch.nn.Module, prefix: str)"
  },
  {
    "module": "srt.layers.quantization.quark.quark",
    "file": "python/sglang/srt/layers/quantization/quark/quark.py",
    "line": 86,
    "qualname": "QuarkConfig.from_config",
    "signature": "def from_config(cls, config: dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.quark.quark",
    "file": "python/sglang/srt/layers/quantization/quark/quark.py",
    "line": 153,
    "qualname": "QuarkConfig.get_config_filenames",
    "signature": "def get_config_filenames(cls)"
  },
  {
    "module": "srt.layers.quantization.quark.quark",
    "file": "python/sglang/srt/layers/quantization/quark/quark.py",
    "line": 289,
    "qualname": "QuarkConfig.get_scheme",
    "signature": "def get_scheme(self, layer: torch.nn.Module, layer_name: str)"
  },
  {
    "module": "srt.layers.quantization.quark.quark",
    "file": "python/sglang/srt/layers/quantization/quark/quark.py",
    "line": 302,
    "qualname": "QuarkConfig.get_scaled_act_names",
    "signature": "def get_scaled_act_names(self)"
  },
  {
    "module": "srt.layers.quantization.quark.quark",
    "file": "python/sglang/srt/layers/quantization/quark/quark.py",
    "line": 308,
    "qualname": "QuarkLinearMethod.__init__",
    "signature": "def __init__(self, quantization_config: QuarkConfig)"
  },
  {
    "module": "srt.layers.quantization.quark.quark",
    "file": "python/sglang/srt/layers/quantization/quark/quark.py",
    "line": 311,
    "qualname": "QuarkLinearMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.quark.quark",
    "file": "python/sglang/srt/layers/quantization/quark/quark.py",
    "line": 314,
    "qualname": "QuarkLinearMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: list[int], input_size: int, output_size: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.quark.quark",
    "file": "python/sglang/srt/layers/quantization/quark/quark.py",
    "line": 340,
    "qualname": "QuarkLinearMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.quark.quark",
    "file": "python/sglang/srt/layers/quantization/quark/quark.py",
    "line": 363,
    "qualname": "QuarkKVCacheMethod.__init__",
    "signature": "def __init__(self, quant_config: QuarkConfig)"
  },
  {
    "module": "srt.layers.quantization.quark.quark",
    "file": "python/sglang/srt/layers/quantization/quark/quark.py",
    "line": 368,
    "qualname": "QuarkKVCacheMethod.validate_kv_cache_config",
    "signature": "def validate_kv_cache_config(kv_cache_config: Optional[dict[str, Any]])"
  },
  {
    "module": "srt.layers.quantization.quark.quark_moe",
    "file": "python/sglang/srt/layers/quantization/quark/quark_moe.py",
    "line": 26,
    "qualname": "QuarkMoEMethod.__new__",
    "signature": "def __new__(cls)"
  },
  {
    "module": "srt.layers.quantization.quark.quark_moe",
    "file": "python/sglang/srt/layers/quantization/quark/quark_moe.py",
    "line": 45,
    "qualname": "QuarkMoEMethod.get_moe_method",
    "signature": "def get_moe_method(quant_config: 'QuarkConfig', module: torch.nn.Module, layer_name: str)"
  },
  {
    "module": "srt.layers.quantization.quark.quark_moe",
    "file": "python/sglang/srt/layers/quantization/quark/quark_moe.py",
    "line": 69,
    "qualname": "QuarkW4A4MXFp4MoEMethod.__init__",
    "signature": "def __init__(self, weight_config: dict[str, Any], input_config: dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.quark.quark_moe",
    "file": "python/sglang/srt/layers/quantization/quark/quark_moe.py",
    "line": 85,
    "qualname": "QuarkW4A4MXFp4MoEMethod.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)"
  },
  {
    "module": "srt.layers.quantization.quark.quark_moe",
    "file": "python/sglang/srt/layers/quantization/quark/quark_moe.py",
    "line": 157,
    "qualname": "QuarkW4A4MXFp4MoEMethod.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.quark.quark_moe",
    "file": "python/sglang/srt/layers/quantization/quark/quark_moe.py",
    "line": 173,
    "qualname": "QuarkW4A4MXFp4MoEMethod.apply",
    "signature": "def apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)"
  },
  {
    "module": "srt.layers.quantization.quark.utils",
    "file": "python/sglang/srt/layers/quantization/quark/utils.py",
    "line": 9,
    "qualname": "deep_compare",
    "signature": "def deep_compare(dict1: Any, dict2: Any)"
  },
  {
    "module": "srt.layers.quantization.quark.utils",
    "file": "python/sglang/srt/layers/quantization/quark/utils.py",
    "line": 22,
    "qualname": "should_ignore_layer",
    "signature": "def should_ignore_layer(layer_name: Optional[str], ignore: Iterable[str], fused_mapping: Mapping[str, list[str]])"
  },
  {
    "module": "srt.layers.quantization.quark.utils",
    "file": "python/sglang/srt/layers/quantization/quark/utils.py",
    "line": 78,
    "qualname": "check_equal_or_regex_match",
    "signature": "def check_equal_or_regex_match(layer_name: str, targets: Iterable[str])"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.schemes.compressed_tensors_scheme",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py",
    "line": 20,
    "qualname": "CompressedTensorsScheme.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.schemes.compressed_tensors_scheme",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py",
    "line": 27,
    "qualname": "CompressedTensorsScheme.create_weights",
    "signature": "def create_weights(self)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.schemes.compressed_tensors_scheme",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py",
    "line": 35,
    "qualname": "CompressedTensorsScheme.apply_weights",
    "signature": "def apply_weights(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.schemes.compressed_tensors_scheme",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py",
    "line": 51,
    "qualname": "CompressedTensorsScheme.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.schemes.compressed_tensors_w8a16_fp8",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py",
    "line": 29,
    "qualname": "apply_fp8_marlin_linear",
    "signature": "def apply_fp8_marlin_linear()"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.schemes.compressed_tensors_w8a16_fp8",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py",
    "line": 32,
    "qualname": "prepare_fp8_layer_for_marlin",
    "signature": "def prepare_fp8_layer_for_marlin()"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.schemes.compressed_tensors_w8a16_fp8",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py",
    "line": 42,
    "qualname": "CompressedTensorsW8A16Fp8.__init__",
    "signature": "def __init__(self, strategy: str, is_static_input_scheme: bool)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.schemes.compressed_tensors_w8a16_fp8",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py",
    "line": 52,
    "qualname": "CompressedTensorsW8A16Fp8.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.schemes.compressed_tensors_w8a16_fp8",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py",
    "line": 59,
    "qualname": "CompressedTensorsW8A16Fp8.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.schemes.compressed_tensors_w8a16_fp8",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py",
    "line": 81,
    "qualname": "CompressedTensorsW8A16Fp8.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, input_size: int, output_partition_sizes: List[int], input_size_per_partition: int, params_dtype: torch.dtype, weight_loader: Callable)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.schemes.compressed_tensors_w8a16_fp8",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py",
    "line": 139,
    "qualname": "CompressedTensorsW8A16Fp8.apply_weights",
    "signature": "def apply_weights(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.schemes.compressed_tensors_w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py",
    "line": 30,
    "qualname": "CompressedTensorsW8A8Fp8.__init__",
    "signature": "def __init__(self, strategy: str, is_static_input_scheme: bool)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.schemes.compressed_tensors_w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py",
    "line": 35,
    "qualname": "CompressedTensorsW8A8Fp8.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.schemes.compressed_tensors_w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py",
    "line": 39,
    "qualname": "CompressedTensorsW8A8Fp8.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.schemes.compressed_tensors_w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py",
    "line": 92,
    "qualname": "CompressedTensorsW8A8Fp8.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, output_partition_sizes: List[int], input_size_per_partition: int, params_dtype: torch.dtype, weight_loader: Callable)"
  },
  {
    "module": "srt.layers.quantization.compressed_tensors.schemes.compressed_tensors_w8a8_fp8",
    "file": "python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py",
    "line": 146,
    "qualname": "CompressedTensorsW8A8Fp8.apply_weights",
    "signature": "def apply_weights(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.quark.schemes.quark_scheme",
    "file": "python/sglang/srt/layers/quantization/quark/schemes/quark_scheme.py",
    "line": 19,
    "qualname": "QuarkScheme.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.quark.schemes.quark_scheme",
    "file": "python/sglang/srt/layers/quantization/quark/schemes/quark_scheme.py",
    "line": 26,
    "qualname": "QuarkScheme.create_weights",
    "signature": "def create_weights(self)"
  },
  {
    "module": "srt.layers.quantization.quark.schemes.quark_scheme",
    "file": "python/sglang/srt/layers/quantization/quark/schemes/quark_scheme.py",
    "line": 34,
    "qualname": "QuarkScheme.apply_weights",
    "signature": "def apply_weights(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.layers.quantization.quark.schemes.quark_scheme",
    "file": "python/sglang/srt/layers/quantization/quark/schemes/quark_scheme.py",
    "line": 50,
    "qualname": "QuarkScheme.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.quark.schemes.quark_w4a4_mxfp4",
    "file": "python/sglang/srt/layers/quantization/quark/schemes/quark_w4a4_mxfp4.py",
    "line": 26,
    "qualname": "QuarkW4A4MXFP4.__init__",
    "signature": "def __init__(self, weight_quant_spec: dict[str, Any], input_quant_spec: dict[str, Any])"
  },
  {
    "module": "srt.layers.quantization.quark.schemes.quark_w4a4_mxfp4",
    "file": "python/sglang/srt/layers/quantization/quark/schemes/quark_w4a4_mxfp4.py",
    "line": 35,
    "qualname": "QuarkW4A4MXFP4.get_min_capability",
    "signature": "def get_min_capability(cls)"
  },
  {
    "module": "srt.layers.quantization.quark.schemes.quark_w4a4_mxfp4",
    "file": "python/sglang/srt/layers/quantization/quark/schemes/quark_w4a4_mxfp4.py",
    "line": 38,
    "qualname": "QuarkW4A4MXFP4.process_weights_after_loading",
    "signature": "def process_weights_after_loading(self, layer: torch.nn.Module)"
  },
  {
    "module": "srt.layers.quantization.quark.schemes.quark_w4a4_mxfp4",
    "file": "python/sglang/srt/layers/quantization/quark/schemes/quark_w4a4_mxfp4.py",
    "line": 50,
    "qualname": "QuarkW4A4MXFP4.create_weights",
    "signature": "def create_weights(self, layer: torch.nn.Module, output_partition_sizes: list[int], input_size_per_partition: int, params_dtype: torch.dtype, weight_loader: Callable)"
  },
  {
    "module": "srt.layers.quantization.quark.schemes.quark_w4a4_mxfp4",
    "file": "python/sglang/srt/layers/quantization/quark/schemes/quark_w4a4_mxfp4.py",
    "line": 90,
    "qualname": "QuarkW4A4MXFP4.apply_weights",
    "signature": "def apply_weights(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])"
  },
  {
    "module": "srt.lora.backend.base_backend",
    "file": "python/sglang/srt/lora/backend/base_backend.py",
    "line": 17,
    "qualname": "BaseLoRABackend.__init__",
    "signature": "def __init__(self, name: str, batch_info: LoRABatchInfo)"
  },
  {
    "module": "srt.lora.backend.base_backend",
    "file": "python/sglang/srt/lora/backend/base_backend.py",
    "line": 21,
    "qualname": "BaseLoRABackend.run_lora_a_sgemm",
    "signature": "def run_lora_a_sgemm(self, x: torch.Tensor, weights: torch.Tensor)"
  },
  {
    "module": "srt.lora.backend.base_backend",
    "file": "python/sglang/srt/lora/backend/base_backend.py",
    "line": 37,
    "qualname": "BaseLoRABackend.run_lora_b_sgemm",
    "signature": "def run_lora_b_sgemm(self, x: torch.Tensor, weights: torch.Tensor)"
  },
  {
    "module": "srt.lora.backend.base_backend",
    "file": "python/sglang/srt/lora/backend/base_backend.py",
    "line": 52,
    "qualname": "BaseLoRABackend.run_qkv_lora",
    "signature": "def run_qkv_lora(self, x: torch.Tensor, qkv_lora_a: torch.Tensor, qkv_lora_b: Union[torch.Tensor, Tuple[torch.Tensor]])"
  },
  {
    "module": "srt.lora.backend.base_backend",
    "file": "python/sglang/srt/lora/backend/base_backend.py",
    "line": 75,
    "qualname": "BaseLoRABackend.run_gate_up_lora",
    "signature": "def run_gate_up_lora(self, x: torch.Tensor, gate_up_lora_a: torch.Tensor, gate_up_lora_b: Union[torch.Tensor, Tuple[torch.Tensor]])"
  },
  {
    "module": "srt.lora.backend.base_backend",
    "file": "python/sglang/srt/lora/backend/base_backend.py",
    "line": 96,
    "qualname": "BaseLoRABackend.set_batch_info",
    "signature": "def set_batch_info(self, batch_info: LoRABatchInfo)"
  },
  {
    "module": "srt.lora.backend.base_backend",
    "file": "python/sglang/srt/lora/backend/base_backend.py",
    "line": 100,
    "qualname": "get_backend_from_name",
    "signature": "def get_backend_from_name(name: str)"
  },
  {
    "module": "srt.lora.backend.triton_backend",
    "file": "python/sglang/srt/lora/backend/triton_backend.py",
    "line": 15,
    "qualname": "TritonLoRABackend.__init__",
    "signature": "def __init__(self, name: str, batch_info: LoRABatchInfo)"
  },
  {
    "module": "srt.lora.backend.triton_backend",
    "file": "python/sglang/srt/lora/backend/triton_backend.py",
    "line": 18,
    "qualname": "TritonLoRABackend.run_lora_a_sgemm",
    "signature": "def run_lora_a_sgemm(self, x: torch.Tensor, weights: torch.Tensor)"
  },
  {
    "module": "srt.lora.backend.triton_backend",
    "file": "python/sglang/srt/lora/backend/triton_backend.py",
    "line": 23,
    "qualname": "TritonLoRABackend.run_lora_b_sgemm",
    "signature": "def run_lora_b_sgemm(self, x: torch.Tensor, weights: torch.Tensor, base_output: torch.Tensor)"
  },
  {
    "module": "srt.lora.backend.triton_backend",
    "file": "python/sglang/srt/lora/backend/triton_backend.py",
    "line": 33,
    "qualname": "TritonLoRABackend.run_qkv_lora",
    "signature": "def run_qkv_lora(self, x: torch.Tensor, qkv_lora_a: torch.Tensor, qkv_lora_b: torch.Tensor, output_offset: torch.Tensor, max_qkv_out_dim: int, base_output: torch.Tensor)"
  },
  {
    "module": "srt.lora.backend.triton_backend",
    "file": "python/sglang/srt/lora/backend/triton_backend.py",
    "line": 61,
    "qualname": "TritonLoRABackend.run_gate_up_lora",
    "signature": "def run_gate_up_lora(self, x: torch.Tensor, gate_up_lora_a: torch.Tensor, gate_up_lora_b: torch.Tensor, base_output: torch.Tensor)"
  },
  {
    "module": "srt.lora.triton_ops.gate_up_lora_b",
    "file": "python/sglang/srt/lora/triton_ops/gate_up_lora_b.py",
    "line": 126,
    "qualname": "gate_up_lora_b_fwd",
    "signature": "def gate_up_lora_b_fwd(x: torch.Tensor, gate_up_lora_b: torch.Tensor, batch_info: LoRABatchInfo, output_dim: int, base_output: torch.Tensor)"
  },
  {
    "module": "srt.lora.triton_ops.qkv_lora_b",
    "file": "python/sglang/srt/lora/triton_ops/qkv_lora_b.py",
    "line": 127,
    "qualname": "qkv_lora_b_fwd",
    "signature": "def qkv_lora_b_fwd(x: torch.Tensor, qkv_lora_b: torch.Tensor, batch_info: LoRABatchInfo, output_offset: torch.Tensor, max_qkv_out_dim: int, base_output: torch.Tensor)"
  },
  {
    "module": "srt.lora.triton_ops.sgemm_lora_a",
    "file": "python/sglang/srt/lora/triton_ops/sgemm_lora_a.py",
    "line": 114,
    "qualname": "sgemm_lora_a_fwd",
    "signature": "def sgemm_lora_a_fwd(x: torch.Tensor, weights: torch.Tensor, batch_info: LoRABatchInfo, stack_num: int)"
  },
  {
    "module": "srt.lora.triton_ops.sgemm_lora_b",
    "file": "python/sglang/srt/lora/triton_ops/sgemm_lora_b.py",
    "line": 118,
    "qualname": "sgemm_lora_b_fwd",
    "signature": "def sgemm_lora_b_fwd(x: torch.Tensor, weights: torch.Tensor, batch_info: LoRABatchInfo, base_output: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.cpp_radix_tree.radix_tree",
    "file": "python/sglang/srt/mem_cache/cpp_radix_tree/radix_tree.py",
    "line": 33,
    "qualname": "RadixTreeCpp.__init__",
    "signature": "def __init__(self, disabled: bool, host_size: Optional[int], page_size: int, write_through_threshold: int)"
  },
  {
    "module": "srt.mem_cache.cpp_radix_tree.radix_tree",
    "file": "python/sglang/srt/mem_cache/cpp_radix_tree/radix_tree.py",
    "line": 52,
    "qualname": "RadixTreeCpp.match_prefix",
    "signature": "def match_prefix(self, prefix: List[int])"
  },
  {
    "module": "srt.mem_cache.cpp_radix_tree.radix_tree",
    "file": "python/sglang/srt/mem_cache/cpp_radix_tree/radix_tree.py",
    "line": 68,
    "qualname": "RadixTreeCpp.evict",
    "signature": "def evict(self, num_tokens: int)"
  },
  {
    "module": "srt.mem_cache.cpp_radix_tree.radix_tree",
    "file": "python/sglang/srt/mem_cache/cpp_radix_tree/radix_tree.py",
    "line": 78,
    "qualname": "RadixTreeCpp.lock_ref",
    "signature": "def lock_ref(self, handle: TreeNodeCpp, lock: bool)"
  },
  {
    "module": "srt.mem_cache.cpp_radix_tree.radix_tree",
    "file": "python/sglang/srt/mem_cache/cpp_radix_tree/radix_tree.py",
    "line": 88,
    "qualname": "RadixTreeCpp.writing_through",
    "signature": "def writing_through(self, key: List[int], indices: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.cpp_radix_tree.radix_tree",
    "file": "python/sglang/srt/mem_cache/cpp_radix_tree/radix_tree.py",
    "line": 104,
    "qualname": "RadixTreeCpp.loading_onboard",
    "signature": "def loading_onboard(self, host_node: TreeNodeCpp, new_device_indices: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.cpp_radix_tree.radix_tree",
    "file": "python/sglang/srt/mem_cache/cpp_radix_tree/radix_tree.py",
    "line": 122,
    "qualname": "RadixTreeCpp.commit_writing_through",
    "signature": "def commit_writing_through(self, handle: IOHandle, success: bool)"
  },
  {
    "module": "srt.mem_cache.cpp_radix_tree.radix_tree",
    "file": "python/sglang/srt/mem_cache/cpp_radix_tree/radix_tree.py",
    "line": 131,
    "qualname": "RadixTreeCpp.commit_loading_onboard",
    "signature": "def commit_loading_onboard(self, handle: IOHandle, success: bool)"
  },
  {
    "module": "srt.mem_cache.cpp_radix_tree.radix_tree",
    "file": "python/sglang/srt/mem_cache/cpp_radix_tree/radix_tree.py",
    "line": 140,
    "qualname": "RadixTreeCpp.evictable_size",
    "signature": "def evictable_size(self)"
  },
  {
    "module": "srt.mem_cache.cpp_radix_tree.radix_tree",
    "file": "python/sglang/srt/mem_cache/cpp_radix_tree/radix_tree.py",
    "line": 149,
    "qualname": "RadixTreeCpp.protected_size",
    "signature": "def protected_size(self)"
  },
  {
    "module": "srt.mem_cache.cpp_radix_tree.radix_tree",
    "file": "python/sglang/srt/mem_cache/cpp_radix_tree/radix_tree.py",
    "line": 158,
    "qualname": "RadixTreeCpp.total_size",
    "signature": "def total_size(self)"
  },
  {
    "module": "srt.mem_cache.cpp_radix_tree.radix_tree",
    "file": "python/sglang/srt/mem_cache/cpp_radix_tree/radix_tree.py",
    "line": 166,
    "qualname": "RadixTreeCpp.reset",
    "signature": "def reset(self)"
  },
  {
    "module": "srt.mem_cache.cpp_radix_tree.radix_tree",
    "file": "python/sglang/srt/mem_cache/cpp_radix_tree/radix_tree.py",
    "line": 172,
    "qualname": "RadixTreeCpp.debug_print",
    "signature": "def debug_print(self)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.client_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/client_hf3fs.py",
    "line": 30,
    "qualname": "rsynchronized",
    "signature": "def rsynchronized()"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.client_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/client_hf3fs.py",
    "line": 42,
    "qualname": "wsynchronized",
    "signature": "def wsynchronized()"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.client_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/client_hf3fs.py",
    "line": 55,
    "qualname": "Hf3fsClient.__init__",
    "signature": "def __init__(self, path: str, size: int, bytes_per_page: int, entries: int)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.client_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/client_hf3fs.py",
    "line": 106,
    "qualname": "Hf3fsClient.batch_read",
    "signature": "def batch_read(self, offsets: List[int], tensors: List[torch.Tensor])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.client_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/client_hf3fs.py",
    "line": 129,
    "qualname": "Hf3fsClient.batch_write",
    "signature": "def batch_write(self, offsets: List[int], tensors: List[torch.Tensor])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.client_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/client_hf3fs.py",
    "line": 151,
    "qualname": "Hf3fsClient.check",
    "signature": "def check(self, offsets: List[int], tensors: List[torch.Tensor])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.client_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/client_hf3fs.py",
    "line": 169,
    "qualname": "Hf3fsClient.get_size",
    "signature": "def get_size(self)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.client_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/client_hf3fs.py",
    "line": 172,
    "qualname": "Hf3fsClient.close",
    "signature": "def close(self)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.client_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/client_hf3fs.py",
    "line": 182,
    "qualname": "Hf3fsClient.flush",
    "signature": "def flush(self)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 26,
    "qualname": "RankMetadata.__init__",
    "signature": "def __init__(self, num_pages: int)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 33,
    "qualname": "RankMetadata.exists_keys",
    "signature": "def exists_keys(self, keys: List[str])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 38,
    "qualname": "RankMetadata.reserve_and_allocate_page_indices",
    "signature": "def reserve_and_allocate_page_indices(self, keys: List[Tuple[str, str]])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 62,
    "qualname": "RankMetadata.confirm_write",
    "signature": "def confirm_write(self, written_keys_to_confirm: List[Tuple[str, int]], pages_to_release: List[int])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 76,
    "qualname": "RankMetadata.delete_keys",
    "signature": "def delete_keys(self, keys: List[str])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 88,
    "qualname": "RankMetadata.clear_all",
    "signature": "def clear_all(self)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 94,
    "qualname": "RankMetadata.get_page_indices",
    "signature": "def get_page_indices(self, keys: List[str])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 103,
    "qualname": "GlobalMetadataState.__init__",
    "signature": "def __init__(self, persistence_path: Optional[str], save_interval: int)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 111,
    "qualname": "GlobalMetadataState.load_from_disk",
    "signature": "def load_from_disk(self)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 139,
    "qualname": "GlobalMetadataState.save_to_disk",
    "signature": "def save_to_disk(self)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 163,
    "qualname": "GlobalMetadataState.schedule_save",
    "signature": "def schedule_save(self)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 170,
    "qualname": "GlobalMetadataState.shutdown",
    "signature": "def shutdown(self)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 183,
    "qualname": "Hf3fsMetadataServer.__init__",
    "signature": "def __init__(self, persistence_path: Optional[str], save_interval: int)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 200,
    "qualname": "Hf3fsMetadataServer.get_rank_metadata",
    "signature": "def get_rank_metadata(self, rank: int)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 210,
    "qualname": "Hf3fsMetadataServer.initialize",
    "signature": "async def initialize(self, rank: int, request: Request)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 228,
    "qualname": "Hf3fsMetadataServer.exists",
    "signature": "async def exists(self, rank: int, request: Request)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 236,
    "qualname": "Hf3fsMetadataServer.reserve_and_allocate_page_indices",
    "signature": "async def reserve_and_allocate_page_indices(self, rank: int, request: Request)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 244,
    "qualname": "Hf3fsMetadataServer.confirm_write",
    "signature": "async def confirm_write(self, rank: int, request: Request)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 257,
    "qualname": "Hf3fsMetadataServer.delete_keys",
    "signature": "async def delete_keys(self, rank: int, request: Request)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 264,
    "qualname": "Hf3fsMetadataServer.clear",
    "signature": "async def clear(self, rank: int)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 270,
    "qualname": "Hf3fsMetadataServer.get_page_indices",
    "signature": "async def get_page_indices(self, rank: int, request: Request)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 278,
    "qualname": "Hf3fsMetadataServer.run",
    "signature": "def run(self, host: str, port: int)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 302,
    "qualname": "Hf3fsGlobalMetadataClient.__init__",
    "signature": "def __init__(self, base_url: str, max_retries: int)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 324,
    "qualname": "Hf3fsGlobalMetadataClient.initialize",
    "signature": "def initialize(self, rank: int, num_pages: int)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 327,
    "qualname": "Hf3fsGlobalMetadataClient.reserve_and_allocate_page_indices",
    "signature": "def reserve_and_allocate_page_indices(self, rank: int, keys: List[Tuple[str, str]])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 335,
    "qualname": "Hf3fsGlobalMetadataClient.confirm_write",
    "signature": "def confirm_write(self, rank: int, written_keys_to_confirm: List[Tuple[str, int]], pages_to_release: List[int])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 349,
    "qualname": "Hf3fsGlobalMetadataClient.delete_keys",
    "signature": "def delete_keys(self, rank: int, keys: List[str])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 352,
    "qualname": "Hf3fsGlobalMetadataClient.exists",
    "signature": "def exists(self, rank: int, keys: List[str])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 356,
    "qualname": "Hf3fsGlobalMetadataClient.clear",
    "signature": "def clear(self, rank: int)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 359,
    "qualname": "Hf3fsGlobalMetadataClient.get_page_indices",
    "signature": "def get_page_indices(self, rank: int, keys: List[str])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 367,
    "qualname": "Hf3fsLocalMetadataClient.__init__",
    "signature": "def __init__(self)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 370,
    "qualname": "Hf3fsLocalMetadataClient.initialize",
    "signature": "def initialize(self, rank: int, num_pages: int)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 373,
    "qualname": "Hf3fsLocalMetadataClient.reserve_and_allocate_page_indices",
    "signature": "def reserve_and_allocate_page_indices(self, rank: int, keys: List[Tuple[str, str]])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 379,
    "qualname": "Hf3fsLocalMetadataClient.confirm_write",
    "signature": "def confirm_write(self, rank: int, written_keys_to_confirm: List[Tuple[str, int]], pages_to_release: List[int])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 388,
    "qualname": "Hf3fsLocalMetadataClient.delete_keys",
    "signature": "def delete_keys(self, rank: int, keys: List[str])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 392,
    "qualname": "Hf3fsLocalMetadataClient.exists",
    "signature": "def exists(self, rank: int, keys: List[str])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 396,
    "qualname": "Hf3fsLocalMetadataClient.clear",
    "signature": "def clear(self, rank: int)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 400,
    "qualname": "Hf3fsLocalMetadataClient.get_page_indices",
    "signature": "def get_page_indices(self, rank: int, keys: List[str])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.mini_3fs_metadata_server",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/mini_3fs_metadata_server.py",
    "line": 405,
    "qualname": "run_metadata_server",
    "signature": "def run_metadata_server(host: str, port: int, persistence_path: Optional[str], save_interval: int)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.test_hf3fs_utils",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/test_hf3fs_utils.py",
    "line": 15,
    "qualname": "test_rw_shm",
    "signature": "def test_rw_shm()"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 24,
    "qualname": "Hf3fsMetadataInterface.initialize",
    "signature": "def initialize(self, rank: int, num_pages: int)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 29,
    "qualname": "Hf3fsMetadataInterface.reserve_and_allocate_page_indices",
    "signature": "def reserve_and_allocate_page_indices(self, rank: int, keys: List[Tuple[str, str]])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 45,
    "qualname": "Hf3fsMetadataInterface.confirm_write",
    "signature": "def confirm_write(self, rank: int, written_keys_to_confirm: List[Tuple[str, int]], pages_to_release: List[int])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 61,
    "qualname": "Hf3fsMetadataInterface.get_page_indices",
    "signature": "def get_page_indices(self, rank: int, keys: List[str])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 74,
    "qualname": "Hf3fsMetadataInterface.delete_keys",
    "signature": "def delete_keys(self, rank: int, keys: List[str])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 79,
    "qualname": "Hf3fsMetadataInterface.exists",
    "signature": "def exists(self, rank: int, keys: List[str])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 84,
    "qualname": "Hf3fsMetadataInterface.clear",
    "signature": "def clear(self, rank: int)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 90,
    "qualname": "AtomicCounter.__init__",
    "signature": "def __init__(self, n: int)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 96,
    "qualname": "AtomicCounter.next",
    "signature": "def next(self)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 103,
    "qualname": "synchronized",
    "signature": "def synchronized()"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 118,
    "qualname": "HiCacheHF3FS.__init__",
    "signature": "def __init__(self, rank: int, file_path: str, file_size: int, numjobs: int, bytes_per_page: int, entries: int, dtype: torch.dtype, metadata_client: Hf3fsMetadataInterface)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 169,
    "qualname": "HiCacheHF3FS.from_env_config",
    "signature": "def from_env_config(bytes_per_page: int, dtype: torch.dtype, storage_config: HiCacheStorageConfig)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 234,
    "qualname": "HiCacheHF3FS.get",
    "signature": "def get(self, key: str, target_location: Optional[Any], target_sizes: Optional[Any])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 247,
    "qualname": "HiCacheHF3FS.batch_get",
    "signature": "def batch_get(self, keys: List[str], target_locations: Optional[Any], target_sizes: Optional[Any])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 294,
    "qualname": "HiCacheHF3FS.set",
    "signature": "def set(self, key: str, value: Optional[Any], target_location: Optional[Any], target_sizes: Optional[Any])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 308,
    "qualname": "HiCacheHF3FS.batch_set",
    "signature": "def batch_set(self, keys: List[str], values: Optional[Any], target_locations: Optional[Any], target_sizes: Optional[Any])"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 367,
    "qualname": "HiCacheHF3FS.delete",
    "signature": "def delete(self, key: str)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 371,
    "qualname": "HiCacheHF3FS.exists",
    "signature": "def exists(self, key: str)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 376,
    "qualname": "HiCacheHF3FS.clear",
    "signature": "def clear(self)"
  },
  {
    "module": "srt.mem_cache.storage.hf3fs.storage_hf3fs",
    "file": "python/sglang/srt/mem_cache/storage/hf3fs/storage_hf3fs.py",
    "line": 379,
    "qualname": "HiCacheHF3FS.close",
    "signature": "def close(self)"
  },
  {
    "module": "srt.mem_cache.storage.mooncake_store.unit_test",
    "file": "python/sglang/srt/mem_cache/storage/mooncake_store/unit_test.py",
    "line": 5,
    "qualname": "test_init_and_warmup",
    "signature": "def test_init_and_warmup()"
  },
  {
    "module": "srt.mem_cache.storage.mooncake_store.unit_test",
    "file": "python/sglang/srt/mem_cache/storage/mooncake_store/unit_test.py",
    "line": 10,
    "qualname": "test_register_buffer",
    "signature": "def test_register_buffer()"
  },
  {
    "module": "srt.mem_cache.storage.mooncake_store.unit_test",
    "file": "python/sglang/srt/mem_cache/storage/mooncake_store/unit_test.py",
    "line": 16,
    "qualname": "test_set_and_get",
    "signature": "def test_set_and_get()"
  },
  {
    "module": "srt.mem_cache.storage.mooncake_store.unit_test",
    "file": "python/sglang/srt/mem_cache/storage/mooncake_store/unit_test.py",
    "line": 28,
    "qualname": "test_exists",
    "signature": "def test_exists()"
  },
  {
    "module": "srt.mem_cache.storage.mooncake_store.mooncake_store",
    "file": "python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py",
    "line": 32,
    "qualname": "MooncakeStoreConfig.from_file",
    "signature": "def from_file()"
  },
  {
    "module": "srt.mem_cache.storage.mooncake_store.mooncake_store",
    "file": "python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py",
    "line": 55,
    "qualname": "MooncakeStoreConfig.load_from_env",
    "signature": "def load_from_env()"
  },
  {
    "module": "srt.mem_cache.storage.mooncake_store.mooncake_store",
    "file": "python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py",
    "line": 78,
    "qualname": "MooncakeStoreConfig.__post_init__",
    "signature": "def __post_init__(self)"
  },
  {
    "module": "srt.mem_cache.storage.mooncake_store.mooncake_store",
    "file": "python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py",
    "line": 87,
    "qualname": "MooncakeStore.__init__",
    "signature": "def __init__(self, storage_config: HiCacheStorageConfig)"
  },
  {
    "module": "srt.mem_cache.storage.mooncake_store.mooncake_store",
    "file": "python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py",
    "line": 132,
    "qualname": "MooncakeStore.warmup",
    "signature": "def warmup(self)"
  },
  {
    "module": "srt.mem_cache.storage.mooncake_store.mooncake_store",
    "file": "python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py",
    "line": 139,
    "qualname": "MooncakeStore.register_buffer",
    "signature": "def register_buffer(self, buffer: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.storage.mooncake_store.mooncake_store",
    "file": "python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py",
    "line": 150,
    "qualname": "MooncakeStore.set",
    "signature": "def set(self, key, value: Optional[Any], target_location: Optional[List[int]], target_sizes: Optional[List[int]])"
  },
  {
    "module": "srt.mem_cache.storage.mooncake_store.mooncake_store",
    "file": "python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py",
    "line": 159,
    "qualname": "MooncakeStore.batch_set",
    "signature": "def batch_set(self, keys: List[str], target_location: Optional[List[int]], target_sizes: Optional[List[int]])"
  },
  {
    "module": "srt.mem_cache.storage.mooncake_store.mooncake_store",
    "file": "python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py",
    "line": 200,
    "qualname": "MooncakeStore.get",
    "signature": "def get(self, key, target_location: Optional[Any], target_sizes: Optional[Any])"
  },
  {
    "module": "srt.mem_cache.storage.mooncake_store.mooncake_store",
    "file": "python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py",
    "line": 208,
    "qualname": "MooncakeStore.batch_get",
    "signature": "def batch_get(self, keys: List[str], target_location: Optional[Any], target_sizes: Optional[Any])"
  },
  {
    "module": "srt.mem_cache.storage.mooncake_store.mooncake_store",
    "file": "python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py",
    "line": 227,
    "qualname": "MooncakeStore.exists",
    "signature": "def exists(self, key)"
  },
  {
    "module": "srt.mem_cache.storage.mooncake_store.mooncake_store",
    "file": "python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py",
    "line": 230,
    "qualname": "MooncakeStore.batch_exists",
    "signature": "def batch_exists(self, keys)"
  },
  {
    "module": "srt.mem_cache.storage.mooncake_store.mooncake_store",
    "file": "python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py",
    "line": 247,
    "qualname": "MooncakeStore.delete",
    "signature": "def delete(self, key)"
  },
  {
    "module": "srt.mem_cache.storage.mooncake_store.mooncake_store",
    "file": "python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py",
    "line": 250,
    "qualname": "MooncakeStore.close",
    "signature": "def close(self)"
  },
  {
    "module": "srt.mem_cache.storage.mooncake_store.mooncake_store",
    "file": "python/sglang/srt/mem_cache/storage/mooncake_store/mooncake_store.py",
    "line": 255,
    "qualname": "MooncakeStore.clear",
    "signature": "def clear(self)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.hicache_nixl",
    "file": "python/sglang/srt/mem_cache/storage/nixl/hicache_nixl.py",
    "line": 29,
    "qualname": "HiCacheNixl.__init__",
    "signature": "def __init__(self, file_path: str, plugin: str)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.hicache_nixl",
    "file": "python/sglang/srt/mem_cache/storage/nixl/hicache_nixl.py",
    "line": 49,
    "qualname": "HiCacheNixl.register_buffers",
    "signature": "def register_buffers(self, buffers: Union[torch.Tensor, List[torch.Tensor], List[tuple]])"
  },
  {
    "module": "srt.mem_cache.storage.nixl.hicache_nixl",
    "file": "python/sglang/srt/mem_cache/storage/nixl/hicache_nixl.py",
    "line": 59,
    "qualname": "HiCacheNixl.register_files",
    "signature": "def register_files(self, file_paths: List[str], open_file: Optional[bool])"
  },
  {
    "module": "srt.mem_cache.storage.nixl.hicache_nixl",
    "file": "python/sglang/srt/mem_cache/storage/nixl/hicache_nixl.py",
    "line": 66,
    "qualname": "HiCacheNixl.register_objects",
    "signature": "def register_objects(self, keys: List[str], sizes: Optional[List[int]])"
  },
  {
    "module": "srt.mem_cache.storage.nixl.hicache_nixl",
    "file": "python/sglang/srt/mem_cache/storage/nixl/hicache_nixl.py",
    "line": 161,
    "qualname": "HiCacheNixl.get",
    "signature": "def get(self, key: str, target_location: Optional[torch.Tensor | int], target_sizes: Optional[int])"
  },
  {
    "module": "srt.mem_cache.storage.nixl.hicache_nixl",
    "file": "python/sglang/srt/mem_cache/storage/nixl/hicache_nixl.py",
    "line": 176,
    "qualname": "HiCacheNixl.batch_get",
    "signature": "def batch_get(self, keys: List[str], target_locations: Optional[List[torch.Tensor | int]], target_sizes: Optional[List[int]])"
  },
  {
    "module": "srt.mem_cache.storage.nixl.hicache_nixl",
    "file": "python/sglang/srt/mem_cache/storage/nixl/hicache_nixl.py",
    "line": 204,
    "qualname": "HiCacheNixl.set",
    "signature": "def set(self, key: str, value: Optional[torch.Tensor], target_location: Optional[int], target_sizes: Optional[int])"
  },
  {
    "module": "srt.mem_cache.storage.nixl.hicache_nixl",
    "file": "python/sglang/srt/mem_cache/storage/nixl/hicache_nixl.py",
    "line": 216,
    "qualname": "HiCacheNixl.batch_set",
    "signature": "def batch_set(self, keys: List[str], values: Optional[List[torch.Tensor]], target_locations: Optional[List[int]], target_sizes: Optional[List[int]])"
  },
  {
    "module": "srt.mem_cache.storage.nixl.hicache_nixl",
    "file": "python/sglang/srt/mem_cache/storage/nixl/hicache_nixl.py",
    "line": 243,
    "qualname": "HiCacheNixl.exists",
    "signature": "def exists(self, key: str)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.nixl_utils",
    "file": "python/sglang/srt/mem_cache/storage/nixl/nixl_utils.py",
    "line": 18,
    "qualname": "NixlBackendSelection.__init__",
    "signature": "def __init__(self, plugin: str)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.nixl_utils",
    "file": "python/sglang/srt/mem_cache/storage/nixl/nixl_utils.py",
    "line": 29,
    "qualname": "NixlBackendSelection.set_bucket",
    "signature": "def set_bucket(self, bucket_name: str)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.nixl_utils",
    "file": "python/sglang/srt/mem_cache/storage/nixl/nixl_utils.py",
    "line": 34,
    "qualname": "NixlBackendSelection.create_backend",
    "signature": "def create_backend(self, agent)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.nixl_utils",
    "file": "python/sglang/srt/mem_cache/storage/nixl/nixl_utils.py",
    "line": 89,
    "qualname": "NixlRegistration.__init__",
    "signature": "def __init__(self, agent)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.nixl_utils",
    "file": "python/sglang/srt/mem_cache/storage/nixl/nixl_utils.py",
    "line": 92,
    "qualname": "NixlRegistration.create_query_tuples",
    "signature": "def create_query_tuples(self, key: str, mem_type: str, file_manager)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.nixl_utils",
    "file": "python/sglang/srt/mem_cache/storage/nixl/nixl_utils.py",
    "line": 145,
    "qualname": "NixlFileManager.__init__",
    "signature": "def __init__(self, base_dir: str)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.nixl_utils",
    "file": "python/sglang/srt/mem_cache/storage/nixl/nixl_utils.py",
    "line": 158,
    "qualname": "NixlFileManager.get_file_path",
    "signature": "def get_file_path(self, key: str)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.nixl_utils",
    "file": "python/sglang/srt/mem_cache/storage/nixl/nixl_utils.py",
    "line": 162,
    "qualname": "NixlFileManager.create_file",
    "signature": "def create_file(self, file_path: str)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.nixl_utils",
    "file": "python/sglang/srt/mem_cache/storage/nixl/nixl_utils.py",
    "line": 174,
    "qualname": "NixlFileManager.open_file",
    "signature": "def open_file(self, file_path: str)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.nixl_utils",
    "file": "python/sglang/srt/mem_cache/storage/nixl/nixl_utils.py",
    "line": 183,
    "qualname": "NixlFileManager.close_file",
    "signature": "def close_file(self, fd: int)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.nixl_utils",
    "file": "python/sglang/srt/mem_cache/storage/nixl/nixl_utils.py",
    "line": 192,
    "qualname": "NixlFileManager.files_to_nixl_tuples",
    "signature": "def files_to_nixl_tuples(self, file_paths: List[str])"
  },
  {
    "module": "srt.mem_cache.storage.nixl.test_hicache_nixl_storage",
    "file": "python/sglang/srt/mem_cache/storage/nixl/test_hicache_nixl_storage.py",
    "line": 20,
    "qualname": "TestNixlUnified.setUp",
    "signature": "def setUp(self)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.test_hicache_nixl_storage",
    "file": "python/sglang/srt/mem_cache/storage/nixl/test_hicache_nixl_storage.py",
    "line": 39,
    "qualname": "TestNixlUnified.tearDown",
    "signature": "def tearDown(self)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.test_hicache_nixl_storage",
    "file": "python/sglang/srt/mem_cache/storage/nixl/test_hicache_nixl_storage.py",
    "line": 46,
    "qualname": "TestNixlUnified.delete_test_file",
    "signature": "def delete_test_file(self, file_path: str)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.test_hicache_nixl_storage",
    "file": "python/sglang/srt/mem_cache/storage/nixl/test_hicache_nixl_storage.py",
    "line": 62,
    "qualname": "TestNixlUnified.verify_tensors_equal",
    "signature": "def verify_tensors_equal(self, expected: torch.Tensor, actual: torch.Tensor)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.test_hicache_nixl_storage",
    "file": "python/sglang/srt/mem_cache/storage/nixl/test_hicache_nixl_storage.py",
    "line": 70,
    "qualname": "TestNixlUnified.verify_tensor_lists_equal",
    "signature": "def verify_tensor_lists_equal(self, expected: List[torch.Tensor], actual: List[torch.Tensor])"
  },
  {
    "module": "srt.mem_cache.storage.nixl.test_hicache_nixl_storage",
    "file": "python/sglang/srt/mem_cache/storage/nixl/test_hicache_nixl_storage.py",
    "line": 82,
    "qualname": "TestNixlUnified.test_single_set_get",
    "signature": "def test_single_set_get(self)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.test_hicache_nixl_storage",
    "file": "python/sglang/srt/mem_cache/storage/nixl/test_hicache_nixl_storage.py",
    "line": 115,
    "qualname": "TestNixlUnified.test_batch_set_get",
    "signature": "def test_batch_set_get(self)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.test_hicache_nixl_storage",
    "file": "python/sglang/srt/mem_cache/storage/nixl/test_hicache_nixl_storage.py",
    "line": 150,
    "qualname": "TestNixlUnified.test_mixed_operations",
    "signature": "def test_mixed_operations(self)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.test_hicache_nixl_storage",
    "file": "python/sglang/srt/mem_cache/storage/nixl/test_hicache_nixl_storage.py",
    "line": 169,
    "qualname": "TestNixlUnified.test_data_integrity",
    "signature": "def test_data_integrity(self)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.test_hicache_nixl_storage",
    "file": "python/sglang/srt/mem_cache/storage/nixl/test_hicache_nixl_storage.py",
    "line": 195,
    "qualname": "TestNixlUnified.test_basic_file_operations",
    "signature": "def test_basic_file_operations(self)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.test_hicache_nixl_storage",
    "file": "python/sglang/srt/mem_cache/storage/nixl/test_hicache_nixl_storage.py",
    "line": 206,
    "qualname": "TestNixlUnified.test_create_nixl_tuples",
    "signature": "def test_create_nixl_tuples(self)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.test_hicache_nixl_storage",
    "file": "python/sglang/srt/mem_cache/storage/nixl/test_hicache_nixl_storage.py",
    "line": 216,
    "qualname": "TestNixlUnified.test_error_handling",
    "signature": "def test_error_handling(self)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.test_hicache_nixl_storage",
    "file": "python/sglang/srt/mem_cache/storage/nixl/test_hicache_nixl_storage.py",
    "line": 226,
    "qualname": "TestNixlUnified.test_register_buffers",
    "signature": "def test_register_buffers(self)"
  },
  {
    "module": "srt.mem_cache.storage.nixl.test_hicache_nixl_storage",
    "file": "python/sglang/srt/mem_cache/storage/nixl/test_hicache_nixl_storage.py",
    "line": 238,
    "qualname": "TestNixlUnified.test_register_files_with_tuples",
    "signature": "def test_register_files_with_tuples(self)"
  },
  {
    "module": "srt.multimodal.processors.clip",
    "file": "python/sglang/srt/multimodal/processors/clip.py",
    "line": 13,
    "qualname": "ClipImageProcessor.__init__",
    "signature": "def __init__(self, hf_config, server_args, _processor)"
  },
  {
    "module": "srt.multimodal.processors.clip",
    "file": "python/sglang/srt/multimodal/processors/clip.py",
    "line": 19,
    "qualname": "ClipImageProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text)"
  },
  {
    "module": "srt.multimodal.processors.deepseek_vl_v2",
    "file": "python/sglang/srt/multimodal/processors/deepseek_vl_v2.py",
    "line": 34,
    "qualname": "DeepseekVL2ImageProcessor.__init__",
    "signature": "def __init__(self, hf_config, server_args, _processor)"
  },
  {
    "module": "srt.multimodal.processors.deepseek_vl_v2",
    "file": "python/sglang/srt/multimodal/processors/deepseek_vl_v2.py",
    "line": 40,
    "qualname": "DeepseekVL2ImageProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text, request_obj, max_req_input_len)"
  },
  {
    "module": "srt.multimodal.processors.gemma3",
    "file": "python/sglang/srt/multimodal/processors/gemma3.py",
    "line": 17,
    "qualname": "Gemma3SGLangImageProcessor.__init__",
    "signature": "def __init__(self, hf_config, server_args, _processor)"
  },
  {
    "module": "srt.multimodal.processors.gemma3",
    "file": "python/sglang/srt/multimodal/processors/gemma3.py",
    "line": 31,
    "qualname": "Gemma3SGLangImageProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self, image_data: List[Union[str, bytes, Dict]], input_text, request_obj)"
  },
  {
    "module": "srt.multimodal.processors.gemma3n",
    "file": "python/sglang/srt/multimodal/processors/gemma3n.py",
    "line": 29,
    "qualname": "Gemma3nSGLangProcessor.__init__",
    "signature": "def __init__(self, hf_config, server_args, _processor)"
  },
  {
    "module": "srt.multimodal.processors.gemma3n",
    "file": "python/sglang/srt/multimodal/processors/gemma3n.py",
    "line": 44,
    "qualname": "Gemma3nSGLangProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self, image_data: Optional[List[Union[str, bytes, Dict]]], audio_data: Optional[List[Union[str, bytes, Dict]]], input_text: str, request_obj)"
  },
  {
    "module": "srt.multimodal.processors.glm4v",
    "file": "python/sglang/srt/multimodal/processors/glm4v.py",
    "line": 22,
    "qualname": "Glm4vImageProcessor.__init__",
    "signature": "def __init__(self, hf_config, server_args, _processor)"
  },
  {
    "module": "srt.multimodal.processors.glm4v",
    "file": "python/sglang/srt/multimodal/processors/glm4v.py",
    "line": 55,
    "qualname": "Glm4vImageProcessor.preprocess_video",
    "signature": "async def preprocess_video(self, vr: VideoReader)"
  },
  {
    "module": "srt.multimodal.processors.glm4v",
    "file": "python/sglang/srt/multimodal/processors/glm4v.py",
    "line": 83,
    "qualname": "Glm4vImageProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text, request_obj)"
  },
  {
    "module": "srt.multimodal.processors.internvl",
    "file": "python/sglang/srt/multimodal/processors/internvl.py",
    "line": 20,
    "qualname": "InternVLImageProcessor.__init__",
    "signature": "def __init__(self, hf_config, server_args, _image_processor)"
  },
  {
    "module": "srt.multimodal.processors.internvl",
    "file": "python/sglang/srt/multimodal/processors/internvl.py",
    "line": 52,
    "qualname": "InternVLImageProcessor.build_transform",
    "signature": "def build_transform(input_size)"
  },
  {
    "module": "srt.multimodal.processors.internvl",
    "file": "python/sglang/srt/multimodal/processors/internvl.py",
    "line": 81,
    "qualname": "InternVLImageProcessor.dynamic_preprocess",
    "signature": "def dynamic_preprocess(image, min_num, max_num, image_size, use_thumbnail)"
  },
  {
    "module": "srt.multimodal.processors.internvl",
    "file": "python/sglang/srt/multimodal/processors/internvl.py",
    "line": 145,
    "qualname": "InternVLImageProcessor.get_index",
    "signature": "def get_index(bound, fps, max_frame, first_idx, num_segments)"
  },
  {
    "module": "srt.multimodal.processors.internvl",
    "file": "python/sglang/srt/multimodal/processors/internvl.py",
    "line": 162,
    "qualname": "InternVLImageProcessor.load_video",
    "signature": "def load_video(video_path, bound, input_size, max_num, num_segments)"
  },
  {
    "module": "srt.multimodal.processors.internvl",
    "file": "python/sglang/srt/multimodal/processors/internvl.py",
    "line": 184,
    "qualname": "InternVLImageProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self, image_data, input_text, request_obj)"
  },
  {
    "module": "srt.multimodal.processors.janus_pro",
    "file": "python/sglang/srt/multimodal/processors/janus_pro.py",
    "line": 14,
    "qualname": "JanusProImageProcessor.__init__",
    "signature": "def __init__(self, hf_config, server_args, _processor)"
  },
  {
    "module": "srt.multimodal.processors.janus_pro",
    "file": "python/sglang/srt/multimodal/processors/janus_pro.py",
    "line": 22,
    "qualname": "JanusProImageProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text, request_obj)"
  },
  {
    "module": "srt.multimodal.processors.kimi_vl",
    "file": "python/sglang/srt/multimodal/processors/kimi_vl.py",
    "line": 15,
    "qualname": "KimiVLImageProcessor.__init__",
    "signature": "def __init__(self, hf_config, server_args, _processor)"
  },
  {
    "module": "srt.multimodal.processors.kimi_vl",
    "file": "python/sglang/srt/multimodal/processors/kimi_vl.py",
    "line": 24,
    "qualname": "KimiVLImageProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self, image_data: List[Union[str, bytes, Dict]], input_text, request_obj)"
  },
  {
    "module": "srt.multimodal.processors.llava",
    "file": "python/sglang/srt/multimodal/processors/llava.py",
    "line": 33,
    "qualname": "LlavaImageProcessor.__init__",
    "signature": "def __init__(self, hf_config, server_args, _processor)"
  },
  {
    "module": "srt.multimodal.processors.llava",
    "file": "python/sglang/srt/multimodal/processors/llava.py",
    "line": 109,
    "qualname": "LlavaImageProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self, image_data: List[Union[str, bytes, ImageData]], input_text, request_obj)"
  },
  {
    "module": "srt.multimodal.processors.llava",
    "file": "python/sglang/srt/multimodal/processors/llava.py",
    "line": 194,
    "qualname": "LlavaMultimodalProcessor.__init__",
    "signature": "def __init__(self, hf_config, server_args, _processor)"
  },
  {
    "module": "srt.multimodal.processors.llava",
    "file": "python/sglang/srt/multimodal/processors/llava.py",
    "line": 210,
    "qualname": "LlavaMultimodalProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self)"
  },
  {
    "module": "srt.multimodal.processors.minicpm",
    "file": "python/sglang/srt/multimodal/processors/minicpm.py",
    "line": 18,
    "qualname": "MiniCPMMultimodalProcessor.__init__",
    "signature": "def __init__(self, hf_config, server_args, _processor)"
  },
  {
    "module": "srt.multimodal.processors.minicpm",
    "file": "python/sglang/srt/multimodal/processors/minicpm.py",
    "line": 36,
    "qualname": "MiniCPMMultimodalProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self, image_data: List[Union[str, bytes]], audio_data: List[Union[str, bytes]], input_text, request_obj)"
  },
  {
    "module": "srt.multimodal.processors.mlama",
    "file": "python/sglang/srt/multimodal/processors/mlama.py",
    "line": 13,
    "qualname": "MllamaImageProcessor.__init__",
    "signature": "def __init__(self, hf_config, server_args, _processor)"
  },
  {
    "module": "srt.multimodal.processors.mlama",
    "file": "python/sglang/srt/multimodal/processors/mlama.py",
    "line": 20,
    "qualname": "MllamaImageProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text)"
  },
  {
    "module": "srt.multimodal.processors.mllama4",
    "file": "python/sglang/srt/multimodal/processors/mllama4.py",
    "line": 21,
    "qualname": "Mllama4ImageProcessor.__init__",
    "signature": "def __init__(self, hf_config, server_args, _processor)"
  },
  {
    "module": "srt.multimodal.processors.mllama4",
    "file": "python/sglang/srt/multimodal/processors/mllama4.py",
    "line": 33,
    "qualname": "Mllama4ImageProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text)"
  },
  {
    "module": "srt.multimodal.processors.phi4mm",
    "file": "python/sglang/srt/multimodal/processors/phi4mm.py",
    "line": 19,
    "qualname": "Phi4MMProcessorAdapter.__init__",
    "signature": "def __init__(self, _processor)"
  },
  {
    "module": "srt.multimodal.processors.phi4mm",
    "file": "python/sglang/srt/multimodal/processors/phi4mm.py",
    "line": 22,
    "qualname": "Phi4MMProcessorAdapter.__call__",
    "signature": "def __call__(self)"
  },
  {
    "module": "srt.multimodal.processors.phi4mm",
    "file": "python/sglang/srt/multimodal/processors/phi4mm.py",
    "line": 50,
    "qualname": "Phi4MMMultimodalProcessor.__init__",
    "signature": "def __init__(self, hf_config, server_args, _processor)"
  },
  {
    "module": "srt.multimodal.processors.phi4mm",
    "file": "python/sglang/srt/multimodal/processors/phi4mm.py",
    "line": 69,
    "qualname": "Phi4MMMultimodalProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self, image_data: List[Union[str, bytes]], audio_data, input_text, request_obj)"
  },
  {
    "module": "srt.multimodal.processors.pixtral",
    "file": "python/sglang/srt/multimodal/processors/pixtral.py",
    "line": 23,
    "qualname": "PixtralProcessor.get_patch_grid_size",
    "signature": "def get_patch_grid_size(self)"
  },
  {
    "module": "srt.multimodal.processors.pixtral",
    "file": "python/sglang/srt/multimodal/processors/pixtral.py",
    "line": 45,
    "qualname": "PixtralProcessor.__init__",
    "signature": "def __init__(self, hf_config, server_args, _processor)"
  },
  {
    "module": "srt.multimodal.processors.pixtral",
    "file": "python/sglang/srt/multimodal/processors/pixtral.py",
    "line": 73,
    "qualname": "PixtralProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text, request_obj)"
  },
  {
    "module": "srt.multimodal.processors.qwen_audio",
    "file": "python/sglang/srt/multimodal/processors/qwen_audio.py",
    "line": 14,
    "qualname": "Qwen2AudioMultimodalProcessor.__init__",
    "signature": "def __init__(self, hf_config, server_args, _processor)"
  },
  {
    "module": "srt.multimodal.processors.qwen_audio",
    "file": "python/sglang/srt/multimodal/processors/qwen_audio.py",
    "line": 34,
    "qualname": "Qwen2AudioMultimodalProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self, audio_data, input_text)"
  },
  {
    "module": "srt.multimodal.processors.qwen_vl",
    "file": "python/sglang/srt/multimodal/processors/qwen_vl.py",
    "line": 37,
    "qualname": "smart_resize",
    "signature": "def smart_resize(height: int, width: int, factor: int, min_pixels: int, max_pixels: int)"
  },
  {
    "module": "srt.multimodal.processors.qwen_vl",
    "file": "python/sglang/srt/multimodal/processors/qwen_vl.py",
    "line": 70,
    "qualname": "resize_image",
    "signature": "def resize_image(image, size_factor: int)"
  },
  {
    "module": "srt.multimodal.processors.qwen_vl",
    "file": "python/sglang/srt/multimodal/processors/qwen_vl.py",
    "line": 85,
    "qualname": "round_by_factor",
    "signature": "def round_by_factor(number: int, factor: int)"
  },
  {
    "module": "srt.multimodal.processors.qwen_vl",
    "file": "python/sglang/srt/multimodal/processors/qwen_vl.py",
    "line": 90,
    "qualname": "ceil_by_factor",
    "signature": "def ceil_by_factor(number: int, factor: int)"
  },
  {
    "module": "srt.multimodal.processors.qwen_vl",
    "file": "python/sglang/srt/multimodal/processors/qwen_vl.py",
    "line": 95,
    "qualname": "floor_by_factor",
    "signature": "def floor_by_factor(number: int, factor: int)"
  },
  {
    "module": "srt.multimodal.processors.qwen_vl",
    "file": "python/sglang/srt/multimodal/processors/qwen_vl.py",
    "line": 100,
    "qualname": "resize_image_async",
    "signature": "async def resize_image_async(image)"
  },
  {
    "module": "srt.multimodal.processors.qwen_vl",
    "file": "python/sglang/srt/multimodal/processors/qwen_vl.py",
    "line": 104,
    "qualname": "smart_nframes",
    "signature": "def smart_nframes(ele: dict, total_frames: int, video_fps: int | float)"
  },
  {
    "module": "srt.multimodal.processors.qwen_vl",
    "file": "python/sglang/srt/multimodal/processors/qwen_vl.py",
    "line": 153,
    "qualname": "preprocess_video",
    "signature": "async def preprocess_video(vr, image_factor: int)"
  },
  {
    "module": "srt.multimodal.processors.qwen_vl",
    "file": "python/sglang/srt/multimodal/processors/qwen_vl.py",
    "line": 204,
    "qualname": "Qwen2_5VLImageProcessor.__init__",
    "signature": "def __init__(self, hf_config, server_args, _processor)"
  },
  {
    "module": "srt.multimodal.processors.qwen_vl",
    "file": "python/sglang/srt/multimodal/processors/qwen_vl.py",
    "line": 225,
    "qualname": "Qwen2_5VLImageProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text, request_obj)"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 24,
    "qualname": "GPUToTensor.forward",
    "signature": "def forward(self, raw_image: Union[np.ndarray, Image.Image])"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 41,
    "qualname": "Step3VisionProcessor.__init__",
    "signature": "def __init__(self, size, interpolation_mode, patch_size)"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 82,
    "qualname": "Step3VisionProcessor.__call__",
    "signature": "def __call__(self, image, is_patch)"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 91,
    "qualname": "ImagePatcher.determine_window_size",
    "signature": "def determine_window_size(self, long: int, short: int)"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 96,
    "qualname": "ImagePatcher.slide_window",
    "signature": "def slide_window(self, width: int, height: int, sizes: list[tuple[int, int]], steps: list[tuple[int, int]], img_rate_thr: float)"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 131,
    "qualname": "ImagePatcher.square_pad",
    "signature": "def square_pad(self, img: Image.Image)"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 140,
    "qualname": "ImagePatcher.get_image_size_for_padding",
    "signature": "def get_image_size_for_padding(self, img_width: int, img_height: int)"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 149,
    "qualname": "ImagePatcher.get_image_size_for_preprocess",
    "signature": "def get_image_size_for_preprocess(self, img_width: int, img_height: int)"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 161,
    "qualname": "ImagePatcher.get_image_size_for_crop",
    "signature": "def get_image_size_for_crop(self, img_width: int, img_height: int, window_size: int)"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 181,
    "qualname": "ImagePatcher.patch_crop",
    "signature": "def patch_crop(self, img: Image.Image, i: int, j: int, th: int, tw: int)"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 185,
    "qualname": "ImagePatcher.get_num_patches",
    "signature": "def get_num_patches(self, img_width: int, img_height: int)"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 210,
    "qualname": "ImagePatcher.__call__",
    "signature": "def __call__(self, img: Image.Image)"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 271,
    "qualname": "Step3VLProcessor.__init__",
    "signature": "def __init__(self, config, tokenizer)"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 298,
    "qualname": "Step3VLProcessor.image_token_id",
    "signature": "def image_token_id(self)"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 301,
    "qualname": "Step3VLProcessor.get_num_image_tokens",
    "signature": "def get_num_image_tokens(self, img_width: int, img_height: int)"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 377,
    "qualname": "Step3VLProcessor.replace_placeholder",
    "signature": "def replace_placeholder(self, text: str, placeholder: str, repls: list[str])"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 392,
    "qualname": "Step3VLProcessor.__call__",
    "signature": "def __call__(self, text: Optional[Union[str, list[str]]], images: Optional[Union[Image.Image, list[Image.Image]]], return_tensors: Optional[Union[str, TensorType]])"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 474,
    "qualname": "Step3VLImageProcessor.__init__",
    "signature": "def __init__(self, hf_config, server_args, _processor)"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 488,
    "qualname": "Step3VLImageProcessor.preprocess",
    "signature": "def preprocess(self, image)"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 491,
    "qualname": "Step3VLImageProcessor.__call__",
    "signature": "def __call__(self, image)"
  },
  {
    "module": "srt.multimodal.processors.step3_vl",
    "file": "python/sglang/srt/multimodal/processors/step3_vl.py",
    "line": 494,
    "qualname": "Step3VLImageProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text: str | List[int], request_obj)"
  },
  {
    "module": "srt.multimodal.processors.vila",
    "file": "python/sglang/srt/multimodal/processors/vila.py",
    "line": 32,
    "qualname": "VILAMultimodalProcessor.__init__",
    "signature": "def __init__(self, hf_config: PretrainedConfig, server_args: ServerArgs, _processor: VILAProcessor)"
  },
  {
    "module": "srt.multimodal.processors.vila",
    "file": "python/sglang/srt/multimodal/processors/vila.py",
    "line": 47,
    "qualname": "VILAMultimodalProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self, image_data: Optional[ImageDataInputItem | List[ImageDataInputItem]], input_text: str | List[int], request_obj: GenerateReqInput | EmbeddingReqInput)"
  },
  {
    "module": "srt.multimodal.processors.base_processor",
    "file": "python/sglang/srt/multimodal/processors/base_processor.py",
    "line": 39,
    "qualname": "BaseMultiModalProcessorOutput.organize_results",
    "signature": "def organize_results(self)"
  },
  {
    "module": "srt.multimodal.processors.base_processor",
    "file": "python/sglang/srt/multimodal/processors/base_processor.py",
    "line": 67,
    "qualname": "MultimodalSpecialTokens.build",
    "signature": "def build(self, processor)"
  },
  {
    "module": "srt.multimodal.processors.base_processor",
    "file": "python/sglang/srt/multimodal/processors/base_processor.py",
    "line": 73,
    "qualname": "MultimodalSpecialTokens.convert_to_str",
    "signature": "def convert_to_str(self, token: Union[str, int], processor)"
  },
  {
    "module": "srt.multimodal.processors.base_processor",
    "file": "python/sglang/srt/multimodal/processors/base_processor.py",
    "line": 80,
    "qualname": "MultimodalSpecialTokens.convert_to_strs",
    "signature": "def convert_to_strs(self, processor)"
  },
  {
    "module": "srt.multimodal.processors.base_processor",
    "file": "python/sglang/srt/multimodal/processors/base_processor.py",
    "line": 88,
    "qualname": "MultimodalSpecialTokens.get_modality_of_token",
    "signature": "def get_modality_of_token(self, token: str)"
  },
  {
    "module": "srt.multimodal.processors.base_processor",
    "file": "python/sglang/srt/multimodal/processors/base_processor.py",
    "line": 110,
    "qualname": "MultimodalSpecialTokens.get_token_id_by_modality",
    "signature": "def get_token_id_by_modality(self, modality: Modality)"
  },
  {
    "module": "srt.multimodal.processors.base_processor",
    "file": "python/sglang/srt/multimodal/processors/base_processor.py",
    "line": 118,
    "qualname": "MultimodalSpecialTokens.parse_regex",
    "signature": "def parse_regex(self)"
  },
  {
    "module": "srt.multimodal.processors.base_processor",
    "file": "python/sglang/srt/multimodal/processors/base_processor.py",
    "line": 126,
    "qualname": "MultimodalSpecialTokens.get_combined_regex",
    "signature": "def get_combined_regex(self)"
  },
  {
    "module": "srt.multimodal.processors.base_processor",
    "file": "python/sglang/srt/multimodal/processors/base_processor.py",
    "line": 151,
    "qualname": "BaseMultimodalProcessor.__init__",
    "signature": "def __init__(self, hf_config, server_args, _processor, transport_mode)"
  },
  {
    "module": "srt.multimodal.processors.base_processor",
    "file": "python/sglang/srt/multimodal/processors/base_processor.py",
    "line": 209,
    "qualname": "BaseMultimodalProcessor.process_mm_data",
    "signature": "def process_mm_data(self, input_text, images, videos, audios)"
  },
  {
    "module": "srt.multimodal.processors.base_processor",
    "file": "python/sglang/srt/multimodal/processors/base_processor.py",
    "line": 252,
    "qualname": "BaseMultimodalProcessor.process_mm_data_async",
    "signature": "async def process_mm_data_async(self, image_data, audio_data, input_text, request_obj)"
  },
  {
    "module": "srt.multimodal.processors.base_processor",
    "file": "python/sglang/srt/multimodal/processors/base_processor.py",
    "line": 262,
    "qualname": "BaseMultimodalProcessor.get_estimated_frames_list",
    "signature": "def get_estimated_frames_list(self, image_data)"
  },
  {
    "module": "srt.multimodal.processors.base_processor",
    "file": "python/sglang/srt/multimodal/processors/base_processor.py",
    "line": 314,
    "qualname": "BaseMultimodalProcessor.submit_data_loading_tasks",
    "signature": "def submit_data_loading_tasks(self, text_parts: List[str], multimodal_tokens: MultimodalSpecialTokens, data_iterators: dict[Modality, Iterator[Any]], discard_alpha_channel: bool, image_estimated_frames_iter: Optional[iter], image_scaling_factor: float, max_image_frames: int, audio_sample_rate: Optional[int])"
  },
  {
    "module": "srt.multimodal.processors.base_processor",
    "file": "python/sglang/srt/multimodal/processors/base_processor.py",
    "line": 385,
    "qualname": "BaseMultimodalProcessor.load_mm_data",
    "signature": "def load_mm_data(self, prompt: str, multimodal_tokens: MultimodalSpecialTokens, image_data: Optional[list], video_data: Optional[list], audio_data: Optional[list], return_text: Optional[bool], discard_alpha_channel: bool, audio_sample_rate: Optional[int])"
  },
  {
    "module": "srt.multimodal.processors.base_processor",
    "file": "python/sglang/srt/multimodal/processors/base_processor.py",
    "line": 498,
    "qualname": "BaseMultimodalProcessor.get_mm_items_offset",
    "signature": "def get_mm_items_offset(input_ids: torch.Tensor, mm_token_id: int)"
  },
  {
    "module": "srt.multimodal.processors.base_processor",
    "file": "python/sglang/srt/multimodal/processors/base_processor.py",
    "line": 515,
    "qualname": "BaseMultimodalProcessor.get_mm_items_offset_by_pair",
    "signature": "def get_mm_items_offset_by_pair(input_ids: torch.Tensor, mm_start_id: int, mm_end_id: int)"
  },
  {
    "module": "srt.multimodal.processors.base_processor",
    "file": "python/sglang/srt/multimodal/processors/base_processor.py",
    "line": 523,
    "qualname": "BaseMultimodalProcessor.collect_mm_items_from_processor_output",
    "signature": "def collect_mm_items_from_processor_output(self, data_dict: dict)"
  },
  {
    "module": "srt.multimodal.processors.base_processor",
    "file": "python/sglang/srt/multimodal/processors/base_processor.py",
    "line": 574,
    "qualname": "BaseMultimodalProcessor.process_and_combine_mm_data",
    "signature": "def process_and_combine_mm_data(self, base_output: BaseMultiModalProcessorOutput, mm_tokens: MultimodalSpecialTokens)"
  },
  {
    "module": "srt.sampling.penaltylib.frequency_penalty",
    "file": "python/sglang/srt/sampling/penaltylib/frequency_penalty.py",
    "line": 14,
    "qualname": "BatchedFrequencyPenalizer.__init__",
    "signature": "def __init__(self, orchestrator: BatchedPenalizerOrchestrator)"
  },
  {
    "module": "srt.sampling.penaltylib.min_new_tokens",
    "file": "python/sglang/srt/sampling/penaltylib/min_new_tokens.py",
    "line": 14,
    "qualname": "BatchedMinNewTokensPenalizer.__init__",
    "signature": "def __init__(self, orchestrator: BatchedPenalizerOrchestrator)"
  },
  {
    "module": "srt.sampling.penaltylib.orchestrator",
    "file": "python/sglang/srt/sampling/penaltylib/orchestrator.py",
    "line": 14,
    "qualname": "BatchedPenalizerOrchestrator.__init__",
    "signature": "def __init__(self, vocab_size: int, batch: ScheduleBatch, penalizers: Set[Type['_BatchedPenalizer']])"
  },
  {
    "module": "srt.sampling.penaltylib.orchestrator",
    "file": "python/sglang/srt/sampling/penaltylib/orchestrator.py",
    "line": 32,
    "qualname": "BatchedPenalizerOrchestrator.batch",
    "signature": "def batch(self)"
  },
  {
    "module": "srt.sampling.penaltylib.orchestrator",
    "file": "python/sglang/srt/sampling/penaltylib/orchestrator.py",
    "line": 36,
    "qualname": "BatchedPenalizerOrchestrator.batch",
    "signature": "def batch(self, value: Optional[ScheduleBatch])"
  },
  {
    "module": "srt.sampling.penaltylib.orchestrator",
    "file": "python/sglang/srt/sampling/penaltylib/orchestrator.py",
    "line": 42,
    "qualname": "BatchedPenalizerOrchestrator.reqs",
    "signature": "def reqs(self)"
  },
  {
    "module": "srt.sampling.penaltylib.orchestrator",
    "file": "python/sglang/srt/sampling/penaltylib/orchestrator.py",
    "line": 45,
    "qualname": "BatchedPenalizerOrchestrator.cumulate_output_tokens",
    "signature": "def cumulate_output_tokens(self, output_ids: torch.Tensor)"
  },
  {
    "module": "srt.sampling.penaltylib.orchestrator",
    "file": "python/sglang/srt/sampling/penaltylib/orchestrator.py",
    "line": 55,
    "qualname": "BatchedPenalizerOrchestrator.apply",
    "signature": "def apply(self, logits: torch.Tensor)"
  },
  {
    "module": "srt.sampling.penaltylib.orchestrator",
    "file": "python/sglang/srt/sampling/penaltylib/orchestrator.py",
    "line": 69,
    "qualname": "BatchedPenalizerOrchestrator.filter",
    "signature": "def filter(self, keep_indices: torch.Tensor)"
  },
  {
    "module": "srt.sampling.penaltylib.orchestrator",
    "file": "python/sglang/srt/sampling/penaltylib/orchestrator.py",
    "line": 95,
    "qualname": "BatchedPenalizerOrchestrator.merge",
    "signature": "def merge(self, their: 'BatchedPenalizerOrchestrator')"
  },
  {
    "module": "srt.sampling.penaltylib.orchestrator",
    "file": "python/sglang/srt/sampling/penaltylib/orchestrator.py",
    "line": 119,
    "qualname": "_BatchedPenalizer.is_prepared",
    "signature": "def is_prepared(self)"
  },
  {
    "module": "srt.sampling.penaltylib.orchestrator",
    "file": "python/sglang/srt/sampling/penaltylib/orchestrator.py",
    "line": 122,
    "qualname": "_BatchedPenalizer.is_required",
    "signature": "def is_required(self)"
  },
  {
    "module": "srt.sampling.penaltylib.orchestrator",
    "file": "python/sglang/srt/sampling/penaltylib/orchestrator.py",
    "line": 125,
    "qualname": "_BatchedPenalizer.prepare",
    "signature": "def prepare(self)"
  },
  {
    "module": "srt.sampling.penaltylib.orchestrator",
    "file": "python/sglang/srt/sampling/penaltylib/orchestrator.py",
    "line": 130,
    "qualname": "_BatchedPenalizer.prepare_if_required",
    "signature": "def prepare_if_required(self)"
  },
  {
    "module": "srt.sampling.penaltylib.orchestrator",
    "file": "python/sglang/srt/sampling/penaltylib/orchestrator.py",
    "line": 137,
    "qualname": "_BatchedPenalizer.teardown",
    "signature": "def teardown(self)"
  },
  {
    "module": "srt.sampling.penaltylib.orchestrator",
    "file": "python/sglang/srt/sampling/penaltylib/orchestrator.py",
    "line": 140,
    "qualname": "_BatchedPenalizer.cumulate_output_tokens",
    "signature": "def cumulate_output_tokens(self, output_ids: torch.Tensor)"
  },
  {
    "module": "srt.sampling.penaltylib.orchestrator",
    "file": "python/sglang/srt/sampling/penaltylib/orchestrator.py",
    "line": 146,
    "qualname": "_BatchedPenalizer.apply",
    "signature": "def apply(self, logits: torch.Tensor)"
  },
  {
    "module": "srt.sampling.penaltylib.orchestrator",
    "file": "python/sglang/srt/sampling/penaltylib/orchestrator.py",
    "line": 152,
    "qualname": "_BatchedPenalizer.filter",
    "signature": "def filter(self, keep_indices: torch.Tensor)"
  },
  {
    "module": "srt.sampling.penaltylib.orchestrator",
    "file": "python/sglang/srt/sampling/penaltylib/orchestrator.py",
    "line": 158,
    "qualname": "_BatchedPenalizer.merge",
    "signature": "def merge(self, their: '_BatchedPenalizer')"
  },
  {
    "module": "srt.sampling.penaltylib.presence_penalty",
    "file": "python/sglang/srt/sampling/penaltylib/presence_penalty.py",
    "line": 14,
    "qualname": "BatchedPresencePenalizer.__init__",
    "signature": "def __init__(self, orchestrator: BatchedPenalizerOrchestrator)"
  }
]