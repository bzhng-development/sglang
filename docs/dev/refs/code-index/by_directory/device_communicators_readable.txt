================================================================================
FUNCTION INDEX: device_communicators module
================================================================================
Total Functions: 115
Documented: 19


============================================================
FILE: python/sglang/srt/distributed/device_communicators/cuda_wrapper.py
Functions: 13
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  37: def find_loaded_library(lib_name)
         ‚Üí Optional[str]
         üìù According to according to https://man7.org/linux/man-pages/man5/proc_p


CLASS: CudaRTLibrary
----------------------------------------
  L 114: __init__(self, so_file: Optional[str])

  L 133: CUDART_CHECK(self, result: cudaError_t)
         ‚Üí None

  L 138: cudaGetErrorString(self, error: cudaError_t)
         ‚Üí str

  L 141: cudaSetDevice(self, device: int)
         ‚Üí None

  L 144: cudaDeviceSynchronize(self)
         ‚Üí None

  L 147: cudaDeviceReset(self)
         ‚Üí None

  L 150: cudaMalloc(self, size: int)
         ‚Üí ctypes.c_void_p

  L 155: cudaFree(self, devPtr: ctypes.c_void_p)
         ‚Üí None

  L 158: cudaMemset(self, devPtr: ctypes.c_void_p, value: int, count: int)
         ‚Üí None

  L 161: cudaMemcpy(self, dst: ctypes.c_void_p, src: ctypes.c_void_p, count: int)
         ‚Üí None

  L 168: cudaIpcGetMemHandle(self, devPtr: ctypes.c_void_p)
         ‚Üí cudaIpcMemHandle_t

  L 175: cudaIpcOpenMemHandle(self, handle: cudaIpcMemHandle_t)
         ‚Üí ctypes.c_void_p


============================================================
FILE: python/sglang/srt/distributed/device_communicators/custom_all_reduce.py
Functions: 13
============================================================


CLASS: CustomAllreduce
----------------------------------------
  L  66: __init__(self, group: ProcessGroup, device: Union[int, str, torch.device], max_size)
         ‚Üí None
         üìù Args:

  L 215: create_shared_buffer(size_in_bytes: int, group: Optional[ProcessGroup])
         ‚Üí List[int]
         üìù Creates a shared buffer and returns a list of pointers

  L 240: free_shared_buffer(pointers: List[int], group: Optional[ProcessGroup])
         ‚Üí None

  L 248: capture(self)
         üìù The main responsibility of this context manager is the

  L 296: register_buffer(self, inp: torch.Tensor)

  L 300: register_graph_buffers(self)

  L 326: should_custom_ar(self, inp: torch.Tensor)

  L 351: all_reduce_reg(self, inp: torch.Tensor, out: torch.Tensor)

  L 358: all_reduce_unreg(self, inp: torch.Tensor, out: torch.Tensor)

  L 364: all_reduce(self, inp: torch.Tensor)
         üìù Performs an out-of-place all reduce.

  L 387: custom_all_reduce(self, input: torch.Tensor)
         ‚Üí Optional[torch.Tensor]
         üìù The main allreduce API that provides support for cuda graph.

  L 412: close(self)

  L 420: __del__(self)


============================================================
FILE: python/sglang/srt/distributed/device_communicators/custom_all_reduce_utils.py
Functions: 8
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  50: def update_environment_variables(envs: Dict[str, str])

  L  62: def producer(batch_src: Sequence[int],
        producer_queue,
        consumer_queue,
        result_queue,
        cuda_visible_devices: Optional[str])

  L  96: def consumer(batch_tgt: Sequence[int],
        producer_queue,
        consumer_queue,
        result_queue,
        cuda_visible_devices: Optional[str])

  L 137: def can_actually_p2p(batch_src: Sequence[int], batch_tgt: Sequence[int])
         ‚Üí Sequence[bool]
         üìù Usually, checking if P2P access is enabled can be done by

  L 237: def gpu_p2p_access_check(src: int, tgt: int)
         ‚Üí bool
         üìù Check if GPU src can access GPU tgt.

  L 312: def with_nvml_context(fn: Callable[_P, _R])
         ‚Üí Callable[_P, _R]

  L 332: def is_full_nvlink(physical_device_ids: List[int], world_size: int)
         ‚Üí bool
         @with_nvml_context

  L 373: def is_weak_contiguous(inp: torch.Tensor)


============================================================
FILE: python/sglang/srt/distributed/device_communicators/hpu_communicator.py
Functions: 3
============================================================


CLASS: HpuCommunicator
----------------------------------------
  L  15: __init__(self, group: ProcessGroup)

  L  23: all_reduce(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  31: all_gather(self, x: torch.Tensor, dim: int)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/distributed/device_communicators/npu_communicator.py
Functions: 3
============================================================


CLASS: NpuCommunicator
----------------------------------------
  L  10: __init__(self, group: ProcessGroup)

  L  18: all_reduce(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  22: all_gather(self, x: torch.Tensor, dim: int)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/distributed/device_communicators/pymscclpp.py
Functions: 8
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  39: def mscclpp_is_weak_contiguous(inp: torch.Tensor)

  L  46: def mscclpp_convert_to_bytes(size_str)
         üìù Converts a human-readable size string (e.g., "1MB", "2.5kb", "3 GB")

  L  87: def mscclpp_bench_time(func, test_niter: int, warmup_niter: int)


CLASS: PyMscclppCommunicator
----------------------------------------
  L 111: __init__(self, group: ProcessGroup, device: Union[int, str, torch.device], max_bytes)
         ‚Üí None
         üìù Args:

  L 243: pre_tune_config(self, dtype)
         ‚Üí bool

  L 273: should_mscclpp_allreduce(self, inp: torch.Tensor, op: ReduceOp)
         ‚Üí bool

  L 289: all_reduce(self, tensor: torch.Tensor, op: ReduceOp)

  L 302: change_state(self, enable: Optional[bool])


============================================================
FILE: python/sglang/srt/distributed/device_communicators/pynccl.py
Functions: 12
============================================================


CLASS: PyNcclCommunicator
----------------------------------------
  L  28: __init__(self, group: Union[ProcessGroup, StatelessProcessGroup], device: Union[int, str, torch.device], library_path: Optional[str])
         üìù Args:

  L 126: all_reduce(self, tensor: torch.Tensor, op: ReduceOp, stream)

  L 150: all_gather(self, output_tensor: torch.Tensor, input_tensor: torch.Tensor, stream, sizes: Optional[list[int]])

  L 196: reduce_scatter(self, output_tensor: torch.Tensor, input_tensor: torch.Tensor, op: ReduceOp, stream, sizes: Optional[list[int]])

  L 245: send(self, tensor: torch.Tensor, dst: int, stream)

  L 263: recv(self, tensor: torch.Tensor, src: int, stream)

  L 281: broadcast(self, tensor: torch.Tensor, src: int, stream)

  L 307: register_comm_window_raw(self, ptr: int, size: int)

  L 310: deregister_comm_window(self, window)

  L 313: group_start(self)

  L 316: group_end(self)

  L 320: change_state(self, enable: Optional[bool], stream: Optional[torch.cuda.Stream])
         üìù A context manager to change the state of the communicator.


============================================================
FILE: python/sglang/srt/distributed/device_communicators/pynccl_allocator.py
Functions: 7
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  34: def is_symmetric_memory_enabled()

  L  38: def set_graph_pool_id(graph_pool_id)

  L  43: def get_nccl_mem_pool()


CLASS: use_symmetric_memory
----------------------------------------
  L  67: __init__(self, group_coordinator: GroupCoordinator)

  L  81: __enter__(self)

  L 104: tag(self, tensor: torch.Tensor)

  L 109: __exit__(self, exc_type, exc_val, exc_tb)


============================================================
FILE: python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py
Functions: 22
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  37: def find_nccl_library()
         ‚Üí str
         üìù We either use the library file specified by the `SGLANG_NCCL_SO_PATH`


CLASS: NCCLLibrary
----------------------------------------
  L 332: __init__(self, so_file: Optional[str])

  L 368: ncclGetErrorString(self, result: ncclResult_t)
         ‚Üí str

  L 371: NCCL_CHECK(self, result: ncclResult_t)
         ‚Üí None

  L 376: ncclGetRawVersion(self)
         ‚Üí int

  L 382: ncclGetVersion(self)
         ‚Üí str

  L 390: ncclGetUniqueId(self)
         ‚Üí ncclUniqueId

  L 395: ncclCommInitRank(self, world_size: int, unique_id: ncclUniqueId, rank: int)
         ‚Üí ncclComm_t

  L 406: ncclAllReduce(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 427: ncclReduce(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, root: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 449: ncclReduceScatter(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, op: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 470: ncclAllGather(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 489: ncclSend(self, sendbuff: buffer_type, count: int, datatype: int, dest: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 502: ncclRecv(self, recvbuff: buffer_type, count: int, datatype: int, src: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 515: ncclBroadcast(self, sendbuff: buffer_type, recvbuff: buffer_type, count: int, datatype: int, root: int, comm: ncclComm_t, stream: cudaStream_t)
         ‚Üí None

  L 531: ncclCommDestroy(self, comm: ncclComm_t)
         ‚Üí None

  L 534: ncclCommWindowRegister(self, comm: ncclComm_t, buff: buffer_type, size: int, win_flags: int)
         ‚Üí ncclWindow_t

  L 545: ncclCommWindowDeregister(self, comm: ncclComm_t, window: ncclWindow_t)
         ‚Üí None

  L 548: ncclGroupStart(self)
         ‚Üí None

  L 551: ncclGroupEnd(self)
         ‚Üí None


CLASS: ncclDataTypeEnum
----------------------------------------
  L 102: from_torch(cls, dtype: torch.dtype)
         ‚Üí int


CLASS: ncclRedOpTypeEnum
----------------------------------------
  L 134: from_torch(cls, op: ReduceOp)
         ‚Üí int


============================================================
FILE: python/sglang/srt/distributed/device_communicators/quick_all_reduce.py
Functions: 8
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  34: def qr_rocm_arch_available()


CLASS: QuickAllReduce
----------------------------------------
  L  73: __init__(self, group: ProcessGroup, device: Union[int, str, torch.device])
         ‚Üí None
         üìù Custom allreduce provides non-destructive acceleration and is

  L 175: init_quick_all_reduce(self)

  L 219: create_shared_buffer(self)
         üìù Creates a shared buffer for quickreduce.

  L 230: should_quick_allreduce(self, inp: torch.Tensor)
         üìù Check if quickreduce is available

  L 254: quick_all_reduce(self, inp: torch.Tensor)
         üìù Performs an out-of-place custom quick all reduce.

  L 265: close(self)

  L 272: __del__(self)


============================================================
FILE: python/sglang/srt/distributed/device_communicators/shm_broadcast.py
Functions: 15
============================================================


CLASS: MessageQueue
----------------------------------------
  L 176: __init__(self, n_reader, n_local_reader, local_reader_ranks: Optional[List[int]], max_chunk_bytes: int, max_chunks: int, connect_ip: Optional[str])

  L 257: export_handle(self)
         ‚Üí Handle

  L 261: create_from_handle(handle: Handle, rank)
         ‚Üí 'MessageQueue'

  L 304: wait_until_ready(self)
         üìù This is a collective operation. All processes (including the

  L 338: acquire_write(self)

  L 391: acquire_read(self)

  L 434: enqueue(self, obj)

  L 449: dequeue(self)

  L 468: broadcast_object(self, obj)

  L 476: create_from_process_group(pg: ProcessGroup, max_chunk_bytes, max_chunks, writer_rank)
         ‚Üí 'MessageQueue'


CLASS: ShmRingBuffer
----------------------------------------
  L  36: __init__(self, n_reader: int, max_chunk_bytes: int, max_chunks: int, name: Optional[str])
         üìù A shared memory ring buffer implementation for broadcast communication

  L 132: __reduce__(self)

  L 143: __del__(self)

  L 150: get_data(self, current_idx: int)

  L 157: get_metadata(self, current_idx: int)


============================================================
FILE: python/sglang/srt/distributed/device_communicators/xpu_communicator.py
Functions: 3
============================================================


CLASS: XpuCommunicator
----------------------------------------
  L  12: __init__(self, group: ProcessGroup)

  L  20: all_reduce(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  24: gather(self, input_: torch.Tensor, rank_in_group: int, dst: int, dim: int)
