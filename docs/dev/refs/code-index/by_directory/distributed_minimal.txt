
# python/sglang/srt/distributed/communication_op.py
tensor_model_parallel_all_reduce(input_)
tensor_model_parallel_all_gather(input_, dim)
tensor_model_parallel_gather(input_, dst, dim)
broadcast_tensor_dict(tensor_dict, Union[torch.Tensor, Any]]], src)

# python/sglang/srt/distributed/naive_distributed.py
  NaiveDistributed.__init__(rank, world_size, rendezvous)
  NaiveDistributed.get_rank()
  NaiveDistributed.get_world_size()
  NaiveDistributed.scatter(tensor, scatter_list, src)
  NaiveDistributed.all_gather_object(obj)
  NaiveDistributed.barrier()
get_naive_distributed()
set_naive_distributed(instance)

# python/sglang/srt/distributed/parallel_state.py
inplace_all_reduce(tensor, group_name)
inplace_all_reduce_fake(tensor, group_name)
outplace_all_reduce(tensor, group_name, outplace_all_reduce_method)
outplace_all_reduce_fake(tensor, group_name, outplace_all_reduce_method)
reg_all_gather_into_tensor(output, input, group_name)
reg_all_gather_into_tensor_fake(output, input, group_name)
  GroupCoordinator.__init__(group_ranks, local_rank, torch_distributed_backend, Backend], use_pynccl, use_pymscclpp, use_custom_allreduce, use_hpu_communicator, use_xpu_communicator, use_npu_communicator, use_message_queue_broadcaster, group_name)
  GroupCoordinator.__repr__()
  GroupCoordinator.first_rank()
  GroupCoordinator.last_rank()
  GroupCoordinator.is_first_rank()
  GroupCoordinator.is_last_rank()
  GroupCoordinator.next_rank()
  GroupCoordinator.prev_rank()
  GroupCoordinator.graph_capture(graph_capture_context)
  GroupCoordinator.all_reduce(input_)
  GroupCoordinator.reduce_scatter_tensor(output, input)
  GroupCoordinator.reduce_scatter(output, input_list)
  GroupCoordinator.reduce_scatterv(input_, output, sizes)
  GroupCoordinator.all_gather_into_tensor(output, input)
  GroupCoordinator.all_gather(input_, dim, output_tensor_list)
  GroupCoordinator.all_gatherv(input_, List[torch.Tensor]], sizes)
  GroupCoordinator.gather(input_, dst, dim)
  GroupCoordinator.broadcast(input_, src)
  GroupCoordinator.broadcast_object(obj, src)
  GroupCoordinator.broadcast_object_list(obj_list, src, group)
  GroupCoordinator.send_object(obj, dst)
  GroupCoordinator.recv_object(src)
  GroupCoordinator.broadcast_tensor_dict(tensor_dict, Union[torch.Tensor, Any]]], src, group, metadata_group)
  GroupCoordinator.send_tensor_dict(tensor_dict, Union[torch.Tensor, Any]], dst, all_gather_group)
  GroupCoordinator.recv_tensor_dict(src, all_gather_group)
  GroupCoordinator.barrier()
  GroupCoordinator.send(tensor, dst)
  GroupCoordinator.recv(size, dtype, src)
  GroupCoordinator.destroy()
get_world_group()
init_world_group(ranks, local_rank, backend)
init_model_parallel_group(group_ranks, local_rank, backend, use_custom_allreduce, use_message_queue_broadcaster, group_name, use_mscclpp_allreduce)
set_pdmux_status(enable_prefill_multiplexing)
get_tp_group()
get_moe_ep_group()
get_moe_tp_group()
get_pp_group()
graph_capture()
set_custom_all_reduce(enable)
set_mscclpp_all_reduce(enable)
init_distributed_environment(world_size, rank, distributed_init_method, local_rank, backend, timeout)
initialize_model_parallel(tensor_model_parallel_size, expert_model_parallel_size, pipeline_model_parallel_size, backend, duplicate_tp_group)
ensure_model_parallel_initialized(tensor_model_parallel_size, expert_model_parallel_size, pipeline_model_parallel_size, backend)
model_parallel_is_initialized()
patch_tensor_parallel_group(tp_group)
get_tensor_model_parallel_world_size()
get_tensor_model_parallel_rank()
get_moe_expert_parallel_world_size()
get_moe_expert_parallel_rank()
get_moe_tensor_parallel_world_size()
get_moe_tensor_parallel_rank()
destroy_model_parallel()
destroy_distributed_environment()
cleanup_dist_env_and_memory(shutdown_ray)
in_the_same_node_as(pg, source_rank)
monkey_patch_vllm_parallel_state(reverse)

# python/sglang/srt/distributed/utils.py
ensure_divisibility(numerator, denominator)
divide(numerator, denominator)
split_tensor_along_last_dim(tensor, num_partitions, contiguous_split_chunks)
get_pp_indices(num_hidden_layers, pp_rank, pp_size)
  StatelessProcessGroup.__post_init__()
  StatelessProcessGroup.send_obj(obj, dst)
  StatelessProcessGroup.expire_data()
  StatelessProcessGroup.recv_obj(src)
  StatelessProcessGroup.broadcast_obj(obj, src)
  StatelessProcessGroup.all_gather_obj(obj)
  StatelessProcessGroup.barrier()
  StatelessProcessGroup.create(host, port, rank, world_size, data_expiration_seconds)