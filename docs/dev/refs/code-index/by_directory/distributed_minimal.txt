
# python/sglang/srt/distributed/communication_op.py
tensor_model_parallel_all_reduce(input_: torch.Tensor) -> torch.Tensor
tensor_model_parallel_all_gather(input_: torch.Tensor, dim: int) -> torch.Tensor
tensor_model_parallel_gather(input_: torch.Tensor, dst: int, dim: int) -> Optional[torch.Tensor]
broadcast_tensor_dict(tensor_dict: Optional[Dict[Any, Union[torch.Tensor, Any]]], src: int)

# python/sglang/srt/distributed/naive_distributed.py
  NaiveDistributed.__init__(rank: int, world_size: int, rendezvous: str)
  NaiveDistributed.get_rank()
  NaiveDistributed.get_world_size()
  NaiveDistributed.scatter(tensor: torch.Tensor, scatter_list: List[torch.Tensor], src: int)
  NaiveDistributed.all_gather_object(obj: Any) -> List[Any]
  NaiveDistributed.barrier()
get_naive_distributed()
set_naive_distributed(instance: NaiveDistributed)

# python/sglang/srt/distributed/parallel_state.py
inplace_all_reduce(tensor: torch.Tensor, group_name: str) -> None
inplace_all_reduce_fake(tensor: torch.Tensor, group_name: str) -> None
outplace_all_reduce(tensor: torch.Tensor, group_name: str, outplace_all_reduce_method: str) -> torch.Tensor
outplace_all_reduce_fake(tensor: torch.Tensor, group_name: str, outplace_all_reduce_method: str) -> torch.Tensor
reg_all_gather_into_tensor(output: torch.Tensor, input: torch.Tensor, group_name: str) -> None
reg_all_gather_into_tensor_fake(output: torch.Tensor, input: torch.Tensor, group_name: str) -> None
  GroupCoordinator.__init__(group_ranks: List[List[int]], local_rank: int, torch_distributed_backend: Union[str, Backend], use_pynccl: bool, use_pymscclpp: bool, use_custom_allreduce: bool, use_hpu_communicator: bool, use_xpu_communicator: bool, use_npu_communicator: bool, use_message_queue_broadcaster: bool, group_name: Optional[str])
  GroupCoordinator.__repr__()
  GroupCoordinator.first_rank()
  GroupCoordinator.last_rank()
  GroupCoordinator.is_first_rank()
  GroupCoordinator.is_last_rank()
  GroupCoordinator.next_rank()
  GroupCoordinator.prev_rank()
  GroupCoordinator.graph_capture(graph_capture_context: Optional[GraphCaptureContext])
  GroupCoordinator.all_reduce(input_: torch.Tensor) -> torch.Tensor
  GroupCoordinator.reduce_scatter_tensor(output: torch.Tensor, input: torch.Tensor) -> None
  GroupCoordinator.reduce_scatter(output: torch.Tensor, input_list: List[torch.Tensor]) -> None
  GroupCoordinator.reduce_scatterv(input_: torch.Tensor, output: Optional[torch.Tensor], sizes: Optional[List[int]]) -> torch.Tensor
  GroupCoordinator.all_gather_into_tensor(output: torch.Tensor, input: torch.Tensor)
  GroupCoordinator.all_gather(input_: torch.Tensor, dim: int, output_tensor_list: Optional[List[torch.Tensor]]) -> torch.Tensor
  GroupCoordinator.all_gatherv(input_: Union[torch.Tensor, List[torch.Tensor]], sizes: Optional[List[int]]) -> Union[torch.Tensor, List[torch.Tensor]]
  GroupCoordinator.gather(input_: torch.Tensor, dst: int, dim: int) -> Optional[torch.Tensor]
  GroupCoordinator.broadcast(input_: torch.Tensor, src: int)
  GroupCoordinator.broadcast_object(obj: Optional[Any], src: int)
  GroupCoordinator.broadcast_object_list(obj_list: List[Any], src: int, group: Optional[ProcessGroup])
  GroupCoordinator.all_gather_object(obj: Any) -> List[Any]
  GroupCoordinator.send_object(obj: Any, dst: int) -> None
  GroupCoordinator.recv_object(src: int) -> Any
  GroupCoordinator.broadcast_tensor_dict(tensor_dict: Optional[Dict[str, Union[torch.Tensor, Any]]], src: int, group: Optional[ProcessGroup], metadata_group: Optional[ProcessGroup]) -> Optional[Dict[str, Union[torch.Tensor, Any]]]
  GroupCoordinator.send_tensor_dict(tensor_dict: Dict[str, Union[torch.Tensor, Any]], dst: Optional[int], all_gather_group: Optional['GroupCoordinator']) -> Optional[Dict[str, Union[torch.Tensor, Any]]]
  GroupCoordinator.recv_tensor_dict(src: Optional[int], all_gather_group: Optional['GroupCoordinator']) -> Optional[Dict[str, Union[torch.Tensor, Any]]]
  GroupCoordinator.barrier()
  GroupCoordinator.send(tensor: torch.Tensor, dst: Optional[int]) -> None
  GroupCoordinator.recv(size: torch.Size, dtype: torch.dtype, src: Optional[int]) -> torch.Tensor
  GroupCoordinator.destroy()
get_world_group() -> GroupCoordinator
init_world_group(ranks: List[int], local_rank: int, backend: str) -> GroupCoordinator
init_model_parallel_group(group_ranks: List[List[int]], local_rank: int, backend: str, use_custom_allreduce: Optional[bool], use_message_queue_broadcaster: bool, group_name: Optional[str], use_mscclpp_allreduce: Optional[bool]) -> GroupCoordinator
set_pdmux_status(enable_prefill_multiplexing: bool)
get_tp_group() -> GroupCoordinator
get_moe_ep_group() -> GroupCoordinator
get_moe_tp_group() -> GroupCoordinator
get_pp_group() -> GroupCoordinator
graph_capture()
set_custom_all_reduce(enable: bool)
set_mscclpp_all_reduce(enable: bool)
init_distributed_environment(world_size: int, rank: int, distributed_init_method: str, local_rank: int, backend: str, timeout: Optional[int])
initialize_model_parallel(tensor_model_parallel_size: int, expert_model_parallel_size: int, pipeline_model_parallel_size: int, backend: Optional[str], duplicate_tp_group: bool) -> None
ensure_model_parallel_initialized(tensor_model_parallel_size: int, expert_model_parallel_size: int, pipeline_model_parallel_size: int, backend: Optional[str]) -> None
model_parallel_is_initialized()
patch_tensor_parallel_group(tp_group: GroupCoordinator)
get_tensor_model_parallel_world_size()
get_tensor_model_parallel_rank()
get_moe_expert_parallel_world_size()
get_moe_expert_parallel_rank()
get_moe_tensor_parallel_world_size()
get_moe_tensor_parallel_rank()
destroy_model_parallel()
destroy_distributed_environment()
cleanup_dist_env_and_memory(shutdown_ray: bool)
in_the_same_node_as(pg: ProcessGroup, source_rank: int) -> List[bool]
monkey_patch_vllm_parallel_state(reverse: bool)

# python/sglang/srt/distributed/utils.py
ensure_divisibility(numerator, denominator)
divide(numerator, denominator)
split_tensor_along_last_dim(tensor: torch.Tensor, num_partitions: int, contiguous_split_chunks: bool) -> Sequence[torch.Tensor]
get_pp_indices(num_hidden_layers: int, pp_rank: int, pp_size: int) -> Tuple[int, int]
  StatelessProcessGroup.__post_init__()
  StatelessProcessGroup.send_obj(obj: Any, dst: int)
  StatelessProcessGroup.expire_data()
  StatelessProcessGroup.recv_obj(src: int) -> Any
  StatelessProcessGroup.broadcast_obj(obj: Optional[Any], src: int) -> Any
  StatelessProcessGroup.all_gather_obj(obj: Any) -> list[Any]
  StatelessProcessGroup.barrier()
  StatelessProcessGroup.create(host: str, port: int, rank: int, world_size: int, data_expiration_seconds: int) -> 'StatelessProcessGroup'
