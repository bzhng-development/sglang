================================================================================
FUNCTION INDEX: wave_ops module
================================================================================
Total Functions: 7
Documented: 0


============================================================
FILE: python/sglang/srt/layers/attention/wave_ops/decode_attention.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  27: def get_wave_kernel(shape: paged_decode_attention_shape,
        max_kv_splits,
        input_dtype,
        output_dtype,
        logit_cap)
         @functools.lru_cache(maxsize=4096)

  L  92: def decode_attention_intermediate_arrays_shapes(num_seqs,
        head_size_kv,
        num_query_heads,
        max_kv_splits)

  L 107: def decode_attention_wave(q,
        k_buffer,
        v_buffer,
        o,
        b_req_idx,
        req_to_token,
        attn_logits,
        attn_logits_max,
        num_kv_splits,
        max_kv_splits,
        sm_scale,
        logit_cap)

  L 159: def decode_attention_fwd(q,
        k_buffer,
        v_buffer,
        o,
        b_req_idx,
        req_to_token,
        attn_logits,
        attn_logits_max,
        num_kv_splits,
        max_kv_splits,
        sm_scale,
        logit_cap)


============================================================
FILE: python/sglang/srt/layers/attention/wave_ops/extend_attention.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  23: def get_wave_kernel(shape: AttentionShape,
        q_shape: tuple[int],
        k_shape: tuple[int],
        v_shape: tuple[int],
        k_cache_shape: tuple[int],
        v_cache_shape: tuple[int],
        o_shape: tuple[int],
        input_dtype: torch.dtype,
        output_dtype: torch.dtype,
        size_dtype: torch.dtype,
        is_causal: bool,
        logit_cap: float,
        layer_scaling: float)
         @functools.lru_cache

  L  83: def extend_attention_wave(q_extend,
        k_extend,
        v_extend,
        k_buffer,
        v_buffer,
        qo_indptr,
        kv_indptr,
        kv_indices,
        custom_mask,
        mask_indptr,
        max_seq_len,
        output,
        is_causal,
        layer_scaling,
        logit_cap)


============================================================
FILE: python/sglang/srt/layers/attention/wave_ops/prefill_attention.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  22: def prefill_attention_wave(q,
        k,
        v,
        o,
        b_start_loc,
        b_seq_len,
        max_seq_len,
        is_causal)
