================================================================================
FUNCTION INDEX: backend module
================================================================================
Total Functions: 12
Documented: 5


============================================================
FILE: python/sglang/srt/lora/backend/base_backend.py
Functions: 7
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 100: def get_backend_from_name(name: str)
         ‚Üí BaseLoRABackend
         üìù Get corresponding backend class from backend's name


CLASS: BaseLoRABackend
----------------------------------------
  L  17: __init__(self, name: str, batch_info: LoRABatchInfo)

  L  21: run_lora_a_sgemm(self, x: torch.Tensor, weights: torch.Tensor)
         ‚Üí torch.Tensor
         üìù Run segment Gemm of lora a modules with current backend.
            The definition of segment Gemm can be referred to https://docs.flashinfer.ai/api/gemm.html.
            Args:
            x: input matrix with shape (s, input_dim), here s is the sum of all sequence lengths
            weights: a set of lora weights with shape (num_lora, c * r, input_dim),
            here r is lora rank, c is a multiplier for stacked modules (e.g., c=3 for qkv_proj, c=2 for gate_up_proj)
            usually input_dim is much larger than r
            Returns:
            result with shape (s, c * r)

  L  37: run_lora_b_sgemm(self, x: torch.Tensor, weights: torch.Tensor)
         ‚Üí torch.Tensor
         üìù Run segment Gemm of lora b modules with current backend.
            The definition of segment Gemm can be referred to https://docs.flashinfer.ai/api/gemm.html.
            Args:
            x: input matrix with shape (s, r), here s is the sum of all sequence lengths, r is lora rank
            weights: a set of lora weights with shape (num_lora, output_dim, r)
            usually output_dim is much larger than r
            Returns:
            result with shape (s, output_dim)

  L  52: run_qkv_lora(self, x: torch.Tensor, qkv_lora_a: torch.Tensor, qkv_lora_b: Union[torch.Tensor, Tuple[torch.Tensor]])
         ‚Üí torch.Tensor
         üìù Run the lora pass for QKV Layer.
            Args:
            x: input matrix with shape (s, input_dim), here s is the sum of all sequence lengths
            qkv_lora_a: lora_a module for qkv, with shape (num_lora, 3 * r, input_dim)
            qkv_lora_b: lora_b module for qkv.
            If passed in as a tensor, its shape should be (num_lora,output_dim_q + 2 * output_dim_kv, r)
            If passed in as a tuple of two tensors, it should contain:
            a lora_b module for q, with shape (1, num_lora, output_dim_q, r)
            and a combined lora_b module for kv, with shape (2, num_lora, output_dim_kv, r)
            Returns:
            result with shape (s, output_dim_q + 2 * output_dim_kv)

  L  75: run_gate_up_lora(self, x: torch.Tensor, gate_up_lora_a: torch.Tensor, gate_up_lora_b: Union[torch.Tensor, Tuple[torch.Tensor]])
         ‚Üí torch.Tensor
         üìù Run the lora pass for gate_up_proj, usually attached to MergedColumnParallelLayer.
            Args:
            x: input matrix with shape (s, input_dim), here s is the sum of all sequence lengths
            gate_up_lora_a: lora_a module for gate_up_proj, with shape (num_lora, 2 * r, input_dim)
            gate_up_lora_b: lora_b module for qkv.
            If passed in as a tensor, its shape should be (num_lora, 2 * output_dim, r)
            If passed in as a tuple, it should contain two tensors with shape (num_lora, output_dim, r)
            Returns:
            result with shape (s, 2 * output_dim)

  L  96: set_batch_info(self, batch_info: LoRABatchInfo)


============================================================
FILE: python/sglang/srt/lora/backend/triton_backend.py
Functions: 5
============================================================


CLASS: TritonLoRABackend
----------------------------------------
  L  15: __init__(self, name: str, batch_info: LoRABatchInfo)

  L  18: run_lora_a_sgemm(self, x: torch.Tensor, weights: torch.Tensor)
         ‚Üí torch.Tensor

  L  23: run_lora_b_sgemm(self, x: torch.Tensor, weights: torch.Tensor, base_output: torch.Tensor)
         ‚Üí torch.Tensor

  L  33: run_qkv_lora(self, x: torch.Tensor, qkv_lora_a: torch.Tensor, qkv_lora_b: torch.Tensor, output_offset: torch.Tensor, max_qkv_out_dim: int, base_output: torch.Tensor)
         ‚Üí torch.Tensor

  L  61: run_gate_up_lora(self, x: torch.Tensor, gate_up_lora_a: torch.Tensor, gate_up_lora_b: torch.Tensor, base_output: torch.Tensor)
         ‚Üí torch.Tensor
