
# python/sglang/srt/lora/layers.py
  BaseLayerWithLoRA.__init__(base_layer, lora_backend)
  BaseLayerWithLoRA.forward(x)
  BaseLayerWithLoRA.set_lora_info()
  BaseLayerWithLoRA.slice_lora_a_weights(A, tp_rank)
  BaseLayerWithLoRA.slice_lora_b_weights(B, tp_rank)
  VocabParallelEmbeddingWithLoRA.__init__(base_layer, lora_backend)
  ColumnParallelLinearWithLoRA.__init__(base_layer, lora_backend)
  ColumnParallelLinearWithLoRA.set_lora_info(A_buffer, B_buffer)
  ColumnParallelLinearWithLoRA.apply_lora(base_output, x)
  ColumnParallelLinearWithLoRA.forward(input_)
  ColumnParallelLinearWithLoRA.slice_lora_a_weights(A, tp_rank)
  ColumnParallelLinearWithLoRA.slice_lora_b_weights(B, tp_rank)
  MergedColumnParallelLinearWithLoRA.__init__(base_layer, lora_backend)
  MergedColumnParallelLinearWithLoRA.set_lora_info(A_buffer, B_buffer)
  MergedColumnParallelLinearWithLoRA.apply_lora(base_output, x)
  MergedColumnParallelLinearWithLoRA.slice_lora_a_weights(A, tp_rank)
  MergedColumnParallelLinearWithLoRA.slice_lora_b_weights(B, tp_rank)
  QKVParallelLinearWithLoRA.__init__(base_layer, lora_backend)
  QKVParallelLinearWithLoRA.set_lora_info(A_buffer_qkv, B_buffer_qkv)
  QKVParallelLinearWithLoRA.apply_lora(base_output, x)
  QKVParallelLinearWithLoRA.slice_lora_a_weights(A, tp_rank)
  QKVParallelLinearWithLoRA.slice_lora_b_weights(B, tp_rank)
  RowParallelLinearWithLoRA.__init__(base_layer, lora_backend)
  RowParallelLinearWithLoRA.set_lora_info(A_buffer, B_buffer)
  RowParallelLinearWithLoRA.apply_lora(base_output, x)
  RowParallelLinearWithLoRA.forward(input_, skip_all_reduce)
  RowParallelLinearWithLoRA.slice_lora_a_weights(A, tp_rank)
  RowParallelLinearWithLoRA.slice_lora_b_weights(B, tp_rank)
get_lora_layer(layer, lora_backend)

# python/sglang/srt/lora/lora.py
  LoRALayer.__init__(config, base_hf_config)
  LoRAAdapter.__init__(uid, config, base_hf_config, load_config, lora_backend)
  LoRAAdapter.initialize_weights()
  LoRAAdapter.normalize_qkv_proj(weight_names, weights, torch.Tensor])
  LoRAAdapter.normalize_gate_up_proj(weight_names, weights, torch.Tensor])

# python/sglang/srt/lora/lora_config.py
  LoRAConfig.__init__(path)
  LoRAConfig.get_lora_config(dummy)

# python/sglang/srt/lora/lora_manager.py
  LoRAManager.__init__(base_model, base_hf_config, max_loras_per_batch, load_config, dtype, lora_backend, tp_size, tp_rank, max_lora_rank, target_modules, lora_paths)
  LoRAManager.init_cuda_graph_batch_info(max_bs_in_cuda_graph)
  LoRAManager.create_lora_update_result(success, error_message)
  LoRAManager.load_lora_adapter(lora_ref)
  LoRAManager.validate_new_adapter(lora_config, lora_ref)
  LoRAManager.unload_lora_adapter(lora_ref)
  LoRAManager.validate_lora_batch(lora_ids)
  LoRAManager.prepare_lora_batch(forward_batch)
  LoRAManager.update_lora_info()
  LoRAManager.init_state(max_lora_rank, target_modules, lora_paths)
  LoRAManager.init_lora_adapters(lora_paths)
  LoRAManager.init_lora_shapes(max_lora_rank, target_modules)
  LoRAManager.load_lora_weights(lora_ref)
  LoRAManager.init_memory_pool()
  LoRAManager.set_lora_module(module_name, module)
  LoRAManager.init_lora_modules()

# python/sglang/srt/lora/lora_registry.py
  LoRARef.__post_init__()
  LoRARef.__str__()
  LoRARegistry.__init__(lora_paths)
  LoRARegistry.register(lora_ref)
  LoRARegistry.unregister(lora_name)
  LoRARegistry.acquire(lora_name, List[str]])
  LoRARegistry.release(lora_id, List[str]])
  LoRARegistry.wait_for_unload(lora_id)
  LoRARegistry.num_registered_loras()

# python/sglang/srt/lora/mem_pool.py
  EmptySlot.__repr__()
  EmptySlot.__new__(cls)
  LoRAMemoryPool.__init__(base_hf_config, max_loras_per_batch, dtype, tp_size, tp_rank, max_lora_rank, target_modules, base_model)
  LoRAMemoryPool.can_support(config, Iterable[LoRAConfig]])
  LoRAMemoryPool.get_lora_A_shape(module_name, base_model, max_lora_dim)
  LoRAMemoryPool.get_lora_B_shape(module_name, base_model, max_lora_dim)
  LoRAMemoryPool.init_buffers(base_model)
  LoRAMemoryPool.prepare_lora_batch(cur_uids, lora_adapters, LoRAAdapter], lora_modules, BaseLayerWithLoRA]], lora_refs, LoRARef])
  LoRAMemoryPool.load_lora_weight_to_buffer(uid, buffer_id, lora_adapter, lora_modules, BaseLayerWithLoRA]])
  LoRAMemoryPool.get_tensor(target_module, layer_id, lora_type)
  LoRAMemoryPool.get_buffer_id(lora_uid)

# python/sglang/srt/lora/utils.py
get_layer_id(name)
get_hidden_dim(module_name, config, base_model)
get_normalized_target_modules(target_modules)
get_stacked_multiply(module_name)
get_target_module_name(full_module_name, target_modules)