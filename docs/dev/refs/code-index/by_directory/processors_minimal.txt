
# python/sglang/srt/multimodal/processors/base_processor.py
  BaseMultiModalProcessorOutput.organize_results() -> List[Tuple[Modality, Any]]
  MultimodalSpecialTokens.build(processor)
  MultimodalSpecialTokens.convert_to_str(token: Union[str, int], processor) -> str
  MultimodalSpecialTokens.convert_to_strs(processor)
  MultimodalSpecialTokens.get_modality_of_token(token: str) -> Optional[Modality]
  MultimodalSpecialTokens.get_token_id_by_modality(modality: Modality) -> Optional[int]
  MultimodalSpecialTokens.parse_regex()
  MultimodalSpecialTokens.get_combined_regex() -> re.Pattern
  BaseMultimodalProcessor.__init__(hf_config, server_args, _processor, transport_mode)
  BaseMultimodalProcessor.process_mm_data(input_text, images, videos, audios) -> dict
  BaseMultimodalProcessor.process_mm_data_async(image_data, audio_data, input_text, request_obj) -> Optional[Dict[str, Any]]
  BaseMultimodalProcessor.get_estimated_frames_list(image_data)
  BaseMultimodalProcessor.submit_data_loading_tasks(text_parts: List[str], multimodal_tokens: MultimodalSpecialTokens, data_iterators: dict[Modality, Iterator[Any]], discard_alpha_channel: bool, image_estimated_frames_iter: Optional[iter], image_scaling_factor: float, max_image_frames: int, audio_sample_rate: Optional[int]) -> Tuple[List, List]
  BaseMultimodalProcessor.load_mm_data(prompt: str, multimodal_tokens: MultimodalSpecialTokens, image_data: Optional[list], video_data: Optional[list], audio_data: Optional[list], return_text: Optional[bool], discard_alpha_channel: bool, audio_sample_rate: Optional[int]) -> BaseMultiModalProcessorOutput
  BaseMultimodalProcessor.get_mm_items_offset(input_ids: torch.Tensor, mm_token_id: int) -> List[Tuple[int, int]]
  BaseMultimodalProcessor.get_mm_items_offset_by_pair(input_ids: torch.Tensor, mm_start_id: int, mm_end_id: int) -> List[Tuple[int, int]]
  BaseMultimodalProcessor.collect_mm_items_from_processor_output(data_dict: dict) -> List[MultimodalDataItem]
  BaseMultimodalProcessor.process_and_combine_mm_data(base_output: BaseMultiModalProcessorOutput, mm_tokens: MultimodalSpecialTokens) -> Tuple[List[MultimodalDataItem], torch.Tensor, dict]

# python/sglang/srt/multimodal/processors/clip.py
  ClipImageProcessor.__init__(hf_config, server_args, _processor)
  ClipImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], input_text)

# python/sglang/srt/multimodal/processors/deepseek_vl_v2.py
  DeepseekVL2ImageProcessor.__init__(hf_config, server_args, _processor)
  DeepseekVL2ImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], input_text, request_obj, max_req_input_len)

# python/sglang/srt/multimodal/processors/gemma3.py
  Gemma3SGLangImageProcessor.__init__(hf_config, server_args, _processor)
  Gemma3SGLangImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes, Dict]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/gemma3n.py
  Gemma3nSGLangProcessor.__init__(hf_config, server_args, _processor)
  Gemma3nSGLangProcessor.process_mm_data_async(image_data: Optional[List[Union[str, bytes, Dict]]], audio_data: Optional[List[Union[str, bytes, Dict]]], input_text: str, request_obj)

# python/sglang/srt/multimodal/processors/glm4v.py
  Glm4vImageProcessor.__init__(hf_config, server_args, _processor)
  Glm4vImageProcessor.preprocess_video(vr: VideoReader)
  Glm4vImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/internvl.py
  InternVLImageProcessor.__init__(hf_config, server_args, _image_processor)
  InternVLImageProcessor.build_transform(input_size)
  InternVLImageProcessor.dynamic_preprocess(image, min_num, max_num, image_size, use_thumbnail)
  InternVLImageProcessor.get_index(bound, fps, max_frame, first_idx, num_segments)
  InternVLImageProcessor.load_video(video_path, bound, input_size, max_num, num_segments)
  InternVLImageProcessor.process_mm_data_async(image_data, input_text, request_obj)

# python/sglang/srt/multimodal/processors/janus_pro.py
  JanusProImageProcessor.__init__(hf_config, server_args, _processor)
  JanusProImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/kimi_vl.py
  KimiVLImageProcessor.__init__(hf_config, server_args, _processor)
  KimiVLImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes, Dict]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/llava.py
  LlavaImageProcessor.__init__(hf_config, server_args, _processor)
  LlavaImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes, ImageData]], input_text, request_obj)
  LlavaMultimodalProcessor.__init__(hf_config, server_args, _processor)
  LlavaMultimodalProcessor.process_mm_data_async()

# python/sglang/srt/multimodal/processors/minicpm.py
  MiniCPMMultimodalProcessor.__init__(hf_config, server_args, _processor)
  MiniCPMMultimodalProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], audio_data: List[Union[str, bytes]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/mlama.py
  MllamaImageProcessor.__init__(hf_config, server_args, _processor)
  MllamaImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], input_text)

# python/sglang/srt/multimodal/processors/mllama4.py
  Mllama4ImageProcessor.__init__(hf_config, server_args, _processor)
  Mllama4ImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], input_text)

# python/sglang/srt/multimodal/processors/phi4mm.py
  Phi4MMProcessorAdapter.__init__(_processor) -> None
  Phi4MMProcessorAdapter.__call__()
  Phi4MMMultimodalProcessor.__init__(hf_config, server_args, _processor)
  Phi4MMMultimodalProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], audio_data, input_text, request_obj)

# python/sglang/srt/multimodal/processors/pixtral.py
  PixtralProcessor.get_patch_grid_size() -> tuple[int, int]
  PixtralProcessor.__init__(hf_config, server_args, _processor)
  PixtralProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/qwen_audio.py
  Qwen2AudioMultimodalProcessor.__init__(hf_config, server_args, _processor)
  Qwen2AudioMultimodalProcessor.process_mm_data_async(audio_data, input_text)

# python/sglang/srt/multimodal/processors/qwen_vl.py
smart_resize(height: int, width: int, factor: int, min_pixels: int, max_pixels: int) -> tuple[int, int]
resize_image(image, size_factor: int) -> Image.Image
round_by_factor(number: int, factor: int) -> int
ceil_by_factor(number: int, factor: int) -> int
floor_by_factor(number: int, factor: int) -> int
resize_image_async(image)
smart_nframes(ele: dict, total_frames: int, video_fps: int | float) -> int
preprocess_video(vr, image_factor: int) -> torch.Tensor
  Qwen2_5VLImageProcessor.__init__(hf_config, server_args, _processor)
  Qwen2_5VLImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], input_text, request_obj)

# python/sglang/srt/multimodal/processors/step3_vl.py
  GPUToTensor.forward(raw_image: Union[np.ndarray, Image.Image]) -> torch.Tensor
  Step3VisionProcessor.__init__(size, interpolation_mode, patch_size)
  Step3VisionProcessor.__call__(image, is_patch)
  ImagePatcher.determine_window_size(long: int, short: int) -> int
  ImagePatcher.slide_window(width: int, height: int, sizes: list[tuple[int, int]], steps: list[tuple[int, int]], img_rate_thr: float) -> tuple[list[tuple[int, int, int, int]], tuple[int, int]]
  ImagePatcher.square_pad(img: Image.Image) -> Image.Image
  ImagePatcher.get_image_size_for_padding(img_width: int, img_height: int) -> tuple[int, int]
  ImagePatcher.get_image_size_for_preprocess(img_width: int, img_height: int) -> tuple[int, int]
  ImagePatcher.get_image_size_for_crop(img_width: int, img_height: int, window_size: int)
  ImagePatcher.patch_crop(img: Image.Image, i: int, j: int, th: int, tw: int)
  ImagePatcher.get_num_patches(img_width: int, img_height: int) -> tuple[int, int]
  ImagePatcher.__call__(img: Image.Image) -> tuple[Image.Image, list[Image.Image], list[bool] | None]
  Step3VLProcessor.__init__(config, tokenizer) -> None
  Step3VLProcessor.image_token_id() -> int
  Step3VLProcessor.get_num_image_tokens(img_width: int, img_height: int) -> int
  Step3VLProcessor.replace_placeholder(text: str, placeholder: str, repls: list[str]) -> str
  Step3VLProcessor.__call__(text: Optional[Union[str, list[str]]], images: Optional[Union[Image.Image, list[Image.Image]]], return_tensors: Optional[Union[str, TensorType]]) -> BatchFeature
  Step3VLImageProcessor.__init__(hf_config, server_args, _processor)
  Step3VLImageProcessor.preprocess(image)
  Step3VLImageProcessor.__call__(image)
  Step3VLImageProcessor.process_mm_data_async(image_data: List[Union[str, bytes]], input_text: str | List[int], request_obj)

# python/sglang/srt/multimodal/processors/vila.py
  VILAMultimodalProcessor.__init__(hf_config: PretrainedConfig, server_args: ServerArgs, _processor: VILAProcessor) -> None
  VILAMultimodalProcessor.process_mm_data_async(image_data: Optional[ImageDataInputItem | List[ImageDataInputItem]], input_text: str | List[int], request_obj: GenerateReqInput | EmbeddingReqInput) -> Optional[Dict[str, Any]]
