
# python/sglang/srt/layers/attention/aiter_backend.py
  AiterAttnBackend.__init__(model_runner, skip_prefill, kv_indptr_buf)
  AiterAttnBackend.init_forward_metadata(forward_batch)
  AiterAttnBackend.init_cuda_graph_state(max_bs, max_num_tokens, kv_indices_buf)
  AiterAttnBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info)
  AiterAttnBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, seq_lens_cpu)
  AiterAttnBackend.get_cuda_graph_seq_len_fill_value()
  AiterAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  AiterAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)
  AiterIndicesUpdaterPrefill.__init__(model_runner, attn_backend)
  AiterIndicesUpdaterPrefill.update(req_pool_indices, seq_lens, seq_lens_sum, prefix_lens, encoder_lens, spec_info)
  AiterIndicesUpdaterPrefill.update_single_wrapper(req_pool_indices, seq_lens, seq_lens_sum, prefix_lens, encoder_lens, spec_info)
  AiterMlaIndicesUpdaterPrefill.__init__(model_runner, attn_backend)
  AiterMlaIndicesUpdaterPrefill.update(req_pool_indices, kv_lens, kv_lens_sum, extend_lens, max_q_len, max_kv_len, spec_info)
  AiterMlaIndicesUpdaterPrefill.update_single_wrapper(req_pool_indices, kv_lens, kv_lens_sum, extend_lens, max_q_len, max_kv_len, spec_info)
  AiterMultiStepDraftBackend.__init__(model_runner, topk, speculative_num_steps)
  AiterMultiStepDraftBackend.common_template(forward_batch, kv_indices_buffer, call_fn)
  AiterMultiStepDraftBackend.init_forward_metadata(forward_batch)
  AiterMultiStepDraftBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  AiterMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(forward_batch)
  AiterMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(forward_batch, bs)

# python/sglang/srt/layers/attention/ascend_backend.py
  AscendAttnBackend.gen_attention_mask(max_seq_len, dtype)
  AscendAttnBackend.__init__(model_runner)
  AscendAttnBackend.init_forward_metadata(forward_batch)
  AscendAttnBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  AscendAttnBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info, EagleVerifyInput]])
  AscendAttnBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, EagleVerifyInput]], seq_lens_cpu)
  AscendAttnBackend.get_cuda_graph_seq_len_fill_value()
  AscendAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  AscendAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache, q_rope, k_rope)

# python/sglang/srt/layers/attention/base_attn_backend.py
  AttentionBackend.init_forward_metadata(forward_batch)
  AttentionBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  AttentionBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info, EagleVerifyInput]])
  AttentionBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, EagleVerifyInput]], seq_lens_cpu)
  AttentionBackend.get_cuda_graph_seq_len_fill_value()
  AttentionBackend.forward(q, k, v, layer, forward_batch, save_kv_cache)
  AttentionBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)
  AttentionBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  AttentionBackend.support_triton()

# python/sglang/srt/layers/attention/cutlass_mla_backend.py
  CutlassMLADecodeMetadata.__init__(workspace, block_kv_indices)
  CutlassMLABackend.__init__(model_runner, skip_prefill, kv_indptr_buf, kv_last_page_len_buf)
  CutlassMLABackend.init_forward_metadata(forward_batch)
  CutlassMLABackend.init_cuda_graph_state(max_bs, max_num_tokens, block_kv_indices)
  CutlassMLABackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info)
  CutlassMLABackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, seq_lens_cpu)
  CutlassMLABackend.get_cuda_graph_seq_len_fill_value()
  CutlassMLABackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache, q_rope, k_rope)

# python/sglang/srt/layers/attention/double_sparsity_backend.py
  DoubleSparseAttnBackend.__init__(model_runner)
  DoubleSparseAttnBackend.init_forward_metadata(forward_batch)
  DoubleSparseAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  DoubleSparseAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)

# python/sglang/srt/layers/attention/dual_chunk_flashattention_backend.py
  DualChunkFlashAttentionBackend.__init__(model_runner)
  DualChunkFlashAttentionBackend.get_sparse_attention_config(layer_idx)
  DualChunkFlashAttentionBackend.init_forward_metadata(forward_batch)
  DualChunkFlashAttentionBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  DualChunkFlashAttentionBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)
  DualChunkFlashAttentionBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  DualChunkFlashAttentionBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info)
  DualChunkFlashAttentionBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, seq_lens_cpu, out_cache_loc)
  DualChunkFlashAttentionBackend.get_cuda_graph_seq_len_fill_value()

# python/sglang/srt/layers/attention/flashattention_backend.py
make_local_attention_virtual_batches(attn_chunk_size, query_start_loc_np, seq_lens_np, block_table, page_size)
cdiv(a, b)
merge_state_v2_wrapper(o, s_a, o_exp, s_b)
  FlashAttentionBackend.__init__(model_runner, skip_prefill, speculative_step_id, topk, speculative_num_steps)
  FlashAttentionBackend.init_forward_metadata(forward_batch)
  FlashAttentionBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache, q_rope, k_rope, sinks)
  FlashAttentionBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache, q_rope, k_rope, sinks)
  FlashAttentionBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  FlashAttentionBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info, EagleVerifyInput]])
  FlashAttentionBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, EagleVerifyInput]], seq_lens_cpu, out_cache_loc)
  FlashAttentionBackend.get_cuda_graph_seq_len_fill_value()
prepare_swa_spec_page_table_triton(page_table_dst, page_table_a, page_table_b, seq_len_a, seq_len_b, speculative_num_draft_tokens)
  FlashAttentionMultiStepBackend.__init__(model_runner, topk, speculative_num_steps)
  FlashAttentionMultiStepBackend.init_forward_metadata(forward_batch)
  FlashAttentionMultiStepBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  FlashAttentionMultiStepBackend.init_forward_metadata_capture_cuda_graph(forward_batch)
  FlashAttentionMultiStepBackend.init_forward_metadata_replay_cuda_graph(forward_batch, bs)
normal_decode_set_metadata(cache_seqlens_int32, cu_seqlens_k, page_table, req_to_token, req_pool_indices, strided_indices, max_seq_pages, seq_lens, seq_len_delta, page_size)

# python/sglang/srt/layers/attention/flashinfer_backend.py
  FlashInferAttnBackend.__init__(model_runner, skip_prefill, kv_indptr_buf, kv_last_page_len_buf)
  FlashInferAttnBackend.init_forward_metadata(forward_batch)
  FlashInferAttnBackend.init_cuda_graph_state(max_bs, max_num_tokens, kv_indices_buf)
  FlashInferAttnBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info, EagleVerifyInput]])
  FlashInferAttnBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, EagleVerifyInput]], seq_lens_cpu)
  FlashInferAttnBackend.get_cuda_graph_seq_len_fill_value()
  FlashInferAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  FlashInferAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)
  FlashInferIndicesUpdaterDecode.__init__(model_runner, attn_backend)
  FlashInferIndicesUpdaterDecode.update(req_pool_indices, seq_lens, seq_lens_cpu, seq_lens_sum, decode_wrappers, encoder_lens, spec_info, EagleVerifyInput]])
  FlashInferIndicesUpdaterDecode.update_single_wrapper(req_pool_indices, seq_lens, seq_lens_cpu, seq_lens_sum, decode_wrappers, encoder_lens, spec_info, EagleVerifyInput]])
  FlashInferIndicesUpdaterDecode.update_sliding_window(req_pool_indices, seq_lens, seq_lens_cpu, seq_lens_sum, decode_wrappers, encoder_lens, spec_info, EagleVerifyInput]])
  FlashInferIndicesUpdaterDecode.update_cross_attention(req_pool_indices, seq_lens, seq_lens_cpu, seq_lens_sum, decode_wrappers, encoder_lens, spec_info, EagleVerifyInput]])
  FlashInferIndicesUpdaterDecode.call_begin_forward(wrapper, req_pool_indices, paged_kernel_lens, paged_kernel_lens_sum, kv_indptr, kv_start_idx, spec_info, EagleVerifyInput]], seq_lens_cpu, use_sliding_window_kv_pool)
  FlashInferIndicesUpdaterPrefill.__init__(model_runner, attn_backend)
  FlashInferIndicesUpdaterPrefill.update(req_pool_indices, seq_lens, seq_lens_cpu, seq_lens_sum, prefix_lens, prefill_wrappers, use_ragged, encoder_lens, spec_info, EagleVerifyInput]])
  FlashInferIndicesUpdaterPrefill.update_single_wrapper(req_pool_indices, seq_lens, seq_lens_cpu, seq_lens_sum, prefix_lens, prefill_wrappers, use_ragged, encoder_lens, spec_info, EagleVerifyInput]])
  FlashInferIndicesUpdaterPrefill.update_sliding_window(req_pool_indices, seq_lens, seq_lens_cpu, seq_lens_sum, prefix_lens, prefill_wrappers, use_ragged, encoder_lens, spec_info, EagleVerifyInput]])
  FlashInferIndicesUpdaterPrefill.update_cross_attention(req_pool_indices, seq_lens, seq_lens_cpu, seq_lens_sum, prefix_lens, prefill_wrappers, use_ragged, encoder_lens, spec_info, EagleVerifyInput]])
  FlashInferIndicesUpdaterPrefill.call_begin_forward(wrapper_ragged, wrapper_paged, req_pool_indices, paged_kernel_lens, paged_kernel_lens_sum, seq_lens, prefix_lens, kv_start_idx, kv_indptr, qo_indptr, use_ragged, spec_info, EagleVerifyInput]], use_sliding_window_kv_pool)
  FlashInferMultiStepDraftBackend.__init__(model_runner, topk, speculative_num_steps)
  FlashInferMultiStepDraftBackend.common_template(forward_batch, kv_indices_buffer, call_fn)
  FlashInferMultiStepDraftBackend.init_forward_metadata(forward_batch)
  FlashInferMultiStepDraftBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  FlashInferMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(forward_batch)
  FlashInferMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(forward_batch, bs)
should_use_tensor_core(kv_cache_dtype, num_attention_heads, num_kv_heads)
fast_decode_plan(indptr, indices, last_page_len, num_qo_heads, num_kv_heads, head_dim, page_size, pos_encoding_mode, window_left, logits_soft_cap, q_data_type, torch.dtype]], kv_data_type, torch.dtype]], data_type, torch.dtype]], sm_scale, rope_scale, rope_theta, non_blocking)

# python/sglang/srt/layers/attention/flashinfer_mla_backend.py
  FlashInferMhaChunkKVRunner.__init__(model_runner, attn_backend)
  FlashInferMhaChunkKVRunner.update_prefix_chunks(num_prefix_chunks)
  FlashInferMhaChunkKVRunner.update_wrapper(forward_batch)
  FlashInferMhaChunkKVRunner.forward(q, k, v, layer, forward_batch)
  FlashInferMLAAttnBackend.__init__(model_runner, skip_prefill, kv_indptr_buf, q_indptr_decode_buf)
  FlashInferMLAAttnBackend.init_forward_metadata(forward_batch)
  FlashInferMLAAttnBackend.init_cuda_graph_state(max_bs, max_num_tokens, kv_indices_buf)
  FlashInferMLAAttnBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info)
  FlashInferMLAAttnBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, seq_lens_cpu)
  FlashInferMLAAttnBackend.get_cuda_graph_seq_len_fill_value()
  FlashInferMLAAttnBackend.init_mha_chunk_metadata(forward_batch)
  FlashInferMLAAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache, q_rope, k_rope)
  FlashInferMLAAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache, q_rope, k_rope)
  FlashInferMLAIndicesUpdaterDecode.__init__(model_runner, attn_backend)
  FlashInferMLAIndicesUpdaterDecode.update(req_pool_indices, seq_lens, seq_lens_sum, decode_wrapper, init_metadata_replay, spec_info, EagleVerifyInput]])
  FlashInferMLAIndicesUpdaterDecode.call_begin_forward(wrapper, req_pool_indices, paged_kernel_lens, paged_kernel_lens_sum, q_indptr, kv_indptr, init_metadata_replay, spec_info, EagleVerifyInput]])
  FlashInferMLAIndicesUpdaterPrefill.__init__(model_runner, attn_backend)
  FlashInferMLAIndicesUpdaterPrefill.update(req_pool_indices, seq_lens, seq_lens_sum, prefix_lens, prefill_wrapper_paged, use_ragged, spec_info, EagleVerifyInput]])
  FlashInferMLAIndicesUpdaterPrefill.call_begin_forward(wrapper_ragged, wrapper_paged, req_pool_indices, paged_kernel_lens, paged_kernel_lens_sum, seq_lens, prefix_lens, kv_indptr, qo_indptr, use_ragged, spec_info, EagleVerifyInput]])
  FlashInferMLAMultiStepDraftBackend.__init__(model_runner, topk, speculative_num_steps)
  FlashInferMLAMultiStepDraftBackend.common_template(forward_batch, kv_indices_buffer, call_fn)
  FlashInferMLAMultiStepDraftBackend.init_forward_metadata(forward_batch)
  FlashInferMLAMultiStepDraftBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  FlashInferMLAMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(forward_batch)
  FlashInferMLAMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(forward_batch, bs)
fast_mla_decode_plan(qo_indptr_cpu, kv_indptr_cpu, kv_indices, kv_len_arr_cpu, num_heads, head_dim_ckv, head_dim_kpe, page_size, causal, sm_scale, q_data_type, kv_data_type)

# python/sglang/srt/layers/attention/flashmla_backend.py
  FlashMLADecodeMetadata.__init__(flashmla_metadata, torch.Tensor]], num_splits, block_kv_indices)
  FlashMLABackend.__init__(model_runner, skip_prefill, kv_indptr_buf, kv_last_page_len_buf)
  FlashMLABackend.init_forward_metadata(forward_batch)
  FlashMLABackend.init_cuda_graph_state(max_bs, max_num_tokens, block_kv_indices)
  FlashMLABackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info)
  FlashMLABackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, seq_lens_cpu)
  FlashMLABackend.get_cuda_graph_seq_len_fill_value()
  FlashMLABackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)
  FlashMLABackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  FlashMLAMultiStepDraftBackend.__init__(model_runner, topk, speculative_num_steps)
  FlashMLAMultiStepDraftBackend.common_template(forward_batch, call_fn)
  FlashMLAMultiStepDraftBackend.init_forward_metadata(forward_batch)
  FlashMLAMultiStepDraftBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  FlashMLAMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(forward_batch)
  FlashMLAMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(forward_batch, bs)

# python/sglang/srt/layers/attention/hybrid_attn_backend.py
  HybridAttnBackend.__init__(model_runner, prefill_backend, decode_backend)
  HybridAttnBackend.init_forward_metadata(forward_batch)
  HybridAttnBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  HybridAttnBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info, EagleVerifyInput]])
  HybridAttnBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, EagleVerifyInput]], seq_lens_cpu)
  HybridAttnBackend.get_cuda_graph_seq_len_fill_value()
  HybridAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)
  HybridAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)

# python/sglang/srt/layers/attention/intel_amx_backend.py
  IntelAMXAttnBackend.__init__(model_runner)
  IntelAMXAttnBackend.init_forward_metadata(forward_batch)
  IntelAMXAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  IntelAMXAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)
  IntelAMXAttnBackend.support_triton()

# python/sglang/srt/layers/attention/merge_state.py
merge_state(prefix_output, prefix_lse, suffix_output, suffix_lse, output, output_lse)

# python/sglang/srt/layers/attention/tbo_backend.py
  TboAttnBackend.__init__(primary, children)
  TboAttnBackend.init_new(cls, creator, AttentionBackend])
  TboAttnBackend.init_forward_metadata(forward_batch)
  TboAttnBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  TboAttnBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info, EagleVerifyInput]])
  TboAttnBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, EagleVerifyInput]], seq_lens_cpu)
  TboAttnBackend.get_cuda_graph_seq_len_fill_value()
  TboAttnBackend.forward_extend()
  TboAttnBackend.forward_decode()

# python/sglang/srt/layers/attention/torch_native_backend.py
  TorchNativeAttnBackend.__init__(model_runner)
  TorchNativeAttnBackend.init_forward_metadata(forward_batch)
  TorchNativeAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  TorchNativeAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)
  TorchNativeAttnBackend.support_triton()

# python/sglang/srt/layers/attention/triton_backend.py
logit_capping_mod(logit_capping_method, logit_cap)
  TritonAttnBackend.__init__(model_runner, skip_prefill, kv_indptr_buf)
  TritonAttnBackend.get_num_kv_splits(num_kv_splits, seq_lens)
  TritonAttnBackend.init_forward_metadata(forward_batch)
  TritonAttnBackend.init_cuda_graph_state(max_bs, max_num_tokens, kv_indices_buf)
  TritonAttnBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info, EagleVerifyInput]])
  TritonAttnBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, EagleVerifyInput]], seq_lens_cpu)
  TritonAttnBackend.get_cuda_graph_seq_len_fill_value()
  TritonAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache, sinks)
  TritonAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache, sinks)
  TritonMultiStepDraftBackend.__init__(model_runner, topk, speculative_num_steps)
  TritonMultiStepDraftBackend.common_template(forward_batch, kv_indices_buffer, call_fn)
  TritonMultiStepDraftBackend.init_forward_metadata(forward_batch)
  TritonMultiStepDraftBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  TritonMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(forward_batch)
  TritonMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(forward_batch, bs)
get_num_kv_splits_triton(num_kv_splits_ptr, seq_lens_ptr, num_seq, num_group, num_head, num_kv_head, max_kv_splits, device_core_count, MAX_NUM_SEQ)
update_sliding_window_buffer(window_kv_indptr, req_to_token, sliding_window_size, seq_lens, req_pool_indices, bs, device, token_to_kv_pool_allocator)
update_sliding_window_buffer_cuda_graph(window_kv_indptr, window_kv_indices, req_to_token, sliding_window_size, seq_lens, req_pool_indices, bs, token_to_kv_pool_allocator)

# python/sglang/srt/layers/attention/trtllm_mha_backend.py
  TRTLLMHAAttnBackend.__init__(model_runner, skip_prefill, kv_indptr_buf, kv_last_page_len_buf, speculative_step_id)
  TRTLLMHAAttnBackend.init_cuda_graph_state(max_bs, max_num_tokens, kv_indices_buf)
  TRTLLMHAAttnBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info)
  TRTLLMHAAttnBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, seq_lens_cpu)
  TRTLLMHAAttnBackend.get_cuda_graph_seq_len_fill_value()
  TRTLLMHAAttnBackend.init_forward_metadata(forward_batch)
  TRTLLMHAAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)
  TRTLLMHAAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  TRTLLMHAAttnMultiStepDraftBackend.__init__(model_runner, topk, speculative_num_steps)
  TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata(forward_batch)
  TRTLLMHAAttnMultiStepDraftBackend.init_cuda_graph_state(max_bs, max_num_tokens)
  TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata_capture_cuda_graph(forward_batch)
  TRTLLMHAAttnMultiStepDraftBackend.init_forward_metadata_replay_cuda_graph(forward_batch, bs)

# python/sglang/srt/layers/attention/trtllm_mla_backend.py
  TRTLLMMLABackend.__init__(model_runner, skip_prefill, kv_indptr_buf, q_indptr_decode_buf)
  TRTLLMMLABackend.init_cuda_graph_state(max_bs, max_num_tokens, kv_indices_buf)
  TRTLLMMLABackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info)
  TRTLLMMLABackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, seq_lens_cpu)
  TRTLLMMLABackend.get_cuda_graph_seq_len_fill_value()
  TRTLLMMLABackend.init_forward_metadata(forward_batch)
  TRTLLMMLABackend.quantize_and_rope_for_fp8(q_nope, q_rope, k_nope, k_rope, forward_batch, cos_sin_cache, is_neox)
  TRTLLMMLABackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache, q_rope, k_rope, cos_sin_cache, is_neox)
  TRTLLMMLAMultiStepDraftBackend.__init__(model_runner, topk, speculative_num_steps)

# python/sglang/srt/layers/attention/utils.py
create_flashinfer_kv_indices_triton(req_to_token_ptr, req_pool_indices_ptr, page_kernel_lens_ptr, kv_indptr, kv_start_idx, kv_indices_ptr, req_to_token_ptr_stride)
create_flashmla_kv_indices_triton(req_to_token_ptr, req_pool_indices_ptr, page_kernel_lens_ptr, kv_start_idx, kv_indices_ptr, req_to_token_ptr_stride, kv_indices_ptr_stride, NUM_PAGE_PER_BLOCK, PAGED_SIZE)

# python/sglang/srt/layers/attention/vision.py
  SingletonCache.set_data(value)
  SingletonCache.get_data()
  SingletonCache.empty()
  VisionSdpaAttention.__init__(head_dim, num_heads, num_kv_heads, dropout, flatten_batch, softmax_in_single_precision)
  VisionSdpaAttention.generate_patch_attention_mask(s, cu_seqlens, flatten_batch)
  VisionSdpaAttention.forward(q, k, v, bsz, cu_seqlens, attention_mask)
  VisionTritonAttention.__init__()
  VisionTritonAttention.forward(q, k, v, cu_seqlens, bsz, seq_len)
  VisionFlash3Attention.__init__()
  VisionFlash3Attention.forward(q, k, v, cu_seqlens, torch.Tensor]], bsz, seq_len)
  VisionAttention.__init__(embed_dim, num_heads, projection_size, use_qkv_parallel, qkv_backend, quant_config, dropout, softmax_in_single_precision, flatten_batch, prefix, proj_bias, num_dummy_heads, qkv_bias, qk_normalization, layer_norm_eps, customized_position_embedding_applier, torch.Tensor, Any, Any], Tuple[torch.Tensor, torch.Tensor]])
  VisionAttention.forward(x, cu_seqlens, position_embeddings, torch.Tensor]], attention_mask)

# python/sglang/srt/layers/attention/vision_utils.py
update_vit_attn_dummy_heads_config(config)
pad_vit_attn_dummy_heads(config, name, loaded_weight)

# python/sglang/srt/layers/attention/wave_backend.py
get_num_kv_splits_triton(num_kv_splits_ptr, seq_lens_ptr, num_seq, num_group, num_head, num_kv_head, max_kv_splits, device_core_count, MAX_NUM_SEQ)
  WaveAttnBackend.__init__(model_runner, skip_prefill, kv_indptr_buf)
  WaveAttnBackend.get_num_kv_splits(num_kv_splits, seq_lens)
  WaveAttnBackend.init_forward_metadata(forward_batch)
  WaveAttnBackend.init_cuda_graph_state(max_bs, max_num_tokens, kv_indices_buf)
  WaveAttnBackend.init_forward_metadata_capture_cuda_graph(bs, num_tokens, req_pool_indices, seq_lens, encoder_lens, forward_mode, spec_info, EagleVerifyInput]])
  WaveAttnBackend.init_forward_metadata_replay_cuda_graph(bs, req_pool_indices, seq_lens, seq_lens_sum, encoder_lens, forward_mode, spec_info, EagleVerifyInput]], seq_lens_cpu)
  WaveAttnBackend.get_cuda_graph_seq_len_fill_value()
  WaveAttnBackend.forward_extend(q, k, v, layer, forward_batch, save_kv_cache)
  WaveAttnBackend.forward_decode(q, k, v, layer, forward_batch, save_kv_cache)