================================================================================
FUNCTION INDEX: root module
================================================================================
Total Functions: 408
Documented: 62


============================================================
FILE: python/sglang/srt/_custom_ops.py
Functions: 26
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  35: def init_custom_ar(ipc_tensors: List[torch.Tensor],
        rank_data: torch.Tensor,
        rank: int,
        full_nvlink: bool)
         → int

  L  43: def all_reduce(fa: int,
        inp: torch.Tensor,
        out: torch.Tensor,
        reg_buffer: int,
        reg_buffer_sz_bytes: int)
         → None

  L  52: def dispose(fa: int)
         → None

  L  55: def meta_size()
         → int

  L  58: def register_buffer(fa: int, ipc_tensors: List[int])
         → None

  L  61: def get_graph_buffer_ipc_meta(fa: int)
         → Tuple[List[int], List[int]]

  L  64: def register_graph_buffers(fa: int,
        handles: List[List[int]],
        offsets: List[List[int]])
         → None

  L  72: def init_custom_ar(meta: torch.Tensor,
        rank_data: torch.Tensor,
        handles: List[str],
        offsets: List[int],
        rank: int,
        full_nvlink: bool)
         → int

  L  84: def all_reduce_reg(fa: int, inp: torch.Tensor, out: torch.Tensor)
         → None

  L  87: def all_reduce_unreg(fa: int,
        inp: torch.Tensor,
        reg_buffer: torch.Tensor,
        out: torch.Tensor)
         → None

  L  92: def dispose(fa: int)
         → None

  L  95: def meta_size()
         → int

  L  98: def register_buffer(fa: int,
        t: torch.Tensor,
        handles: List[str],
        offsets: List[int])
         → None

  L 103: def get_graph_buffer_ipc_meta(fa: int)
         → Tuple[torch.Tensor, List[int]]

  L 106: def register_graph_buffers(fa: int,
        handles: List[str],
        offsets: List[List[int]])
         → None

  L 111: def allocate_meta_buffer(size: int)
         → torch.Tensor

  L 114: def get_meta_buffer_ipc_handle(inp: torch.Tensor)
         → torch.Tensor

  L 119: def init_custom_qr(rank: int, world_size: int, qr_max_size: Optional[int])
         → int

  L 124: def qr_get_handle(fa: int)
         → torch.Tensor

  L 127: def qr_open_handles(fa: int, handles: list[torch.Tensor])
         → None

  L 130: def qr_all_reduce(fa: int,
        inp: torch.Tensor,
        out: torch.Tensor,
        quant_level: int,
        cast_bf2half: bool)
         → None

  L 139: def qr_destroy(fa: int)
         → None

  L 142: def qr_max_size()
         → int

  L 146: def mscclpp_generate_unique_id()
         → bytes

  L 150: def mscclpp_init_context(unique_id: bytes,
        rank: int,
        world_size: int,
        scratch: torch.Tensor,
        put_buffer: torch.Tensor,
        nranks_per_node: int,
        rank_to_node: List[int],
        rank_to_ib: List[int],
        context_selection: int)
         → int

  L 174: def mscclpp_allreduce(context: int,
        inp: torch.Tensor,
        out: torch.Tensor,
        nthreads: int,
        nblocks: int)
         → None


============================================================
FILE: python/sglang/srt/aio_rwlock.py
Functions: 13
============================================================


CLASS: RWLock
----------------------------------------
  L   5: __init__(self)

  L  22: reader_lock(self)
         📝 A context manager for acquiring a shared (reader) lock.

  L  33: writer_lock(self)
         📝 A context manager for acquiring an exclusive (writer) lock.

  L  43: acquire_reader(self)

  L  51: release_reader(self)

  L  59: acquire_writer(self)

  L  72: release_writer(self)


CLASS: _ReaderLock
----------------------------------------
  L  80: __init__(self, rwlock: RWLock)

  L  83: __aenter__(self)

  L  87: __aexit__(self, exc_type, exc_val, exc_tb)


CLASS: _WriterLock
----------------------------------------
  L  92: __init__(self, rwlock: RWLock)

  L  95: __aenter__(self)

  L  99: __aexit__(self, exc_type, exc_val, exc_tb)


============================================================
FILE: python/sglang/srt/bench_utils.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  45: def bench_kineto(fn,
        kernel_names,
        num_tests: int,
        suppress_kineto_output: bool,
        trace_path: str,
        flush_l2: bool,
        with_multiple_kernels: bool)


CLASS: suppress_stdout_stderr
----------------------------------------
  L  10: __enter__(self)

  L  30: __exit__(self)


============================================================
FILE: python/sglang/srt/code_completion_parser.py
Functions: 5
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  58: def register_completion_template(template: CompletionTemplate, override: bool)
         📝 Register a new completion template.

  L  68: def completion_template_exists(template_name: str)
         → bool

  L  72: def is_completion_template_defined()
         → bool

  L  77: def generate_completion_prompt_from_request(request: CompletionRequest)
         → str

  L  87: def generate_completion_prompt(prompt: str, suffix: str, template_name: str)
         → str


============================================================
FILE: python/sglang/srt/conversation.py
Functions: 25
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 465: def register_conv_template(template: Conversation, override: bool)
         📝 Register a new conversation template.

  L 475: def register_conv_template_matching_function(func)

  L 479: def get_conv_template_by_model_path(model_path)

  L 487: def chat_template_exists(template_name: str)
         → bool

  L 491: def generate_embedding_convs(texts: List[str],
        images: List[str],
        template_name: str)
         → List[Conversation]

  L 559: def generate_chat_conv(request: ChatCompletionRequest, template_name: str)
         → Conversation

  L 974: def get_model_type(model_path: str)
         → Optional[str]

  L 987: def match_internvl(model_path: str)
         @register_conv_template_matching_function

  L 995: def match_deepseek_janus_pro(model_path: str)
         @register_conv_template_matching_function

  L1003: def match_vicuna(model_path: str)
         @register_conv_template_matching_function

  L1009: def match_deepseek_vl(model_path: str)
         @register_conv_template_matching_function

  L1017: def match_qwen_chat_ml(model_path: str)
         @register_conv_template_matching_function

  L1027: def match_minicpm(model_path: str)
         @register_conv_template_matching_function

  L1036: def match_phi_4_mm(model_path: str)
         @register_conv_template_matching_function


CLASS: Conversation
----------------------------------------
  L 105: get_prompt(self)
         → str
         📝 Get the prompt for generation.

  L 380: set_system_message(self, system_message: str)
         📝 Set the system message.

  L 384: append_message(self, role: str, message: str)
         📝 Append a new message.

  L 388: append_image(self, image: str, detail: Literal['auto', 'low', 'high'])
         📝 Append a new image.

  L 392: append_video(self, video: str)
         📝 Append a new video.

  L 396: append_audio(self, audio: str)
         📝 Append a new audio.

  L 400: update_last_message(self, message: str)
         📝 Update the last output.

  L 408: to_gradio_chatbot(self)
         📝 Convert the conversation to gradio chatbot format.

  L 418: to_openai_api_messages(self)
         📝 Convert the conversation to OpenAI chat completion format.

  L 433: copy(self)

  L 450: dict(self)


============================================================
FILE: python/sglang/srt/custom_op.py
Functions: 12
============================================================


CLASS: CustomOp
----------------------------------------
  L  13: __init__(self)

  L  21: enter_torch_compile(self, num_tokens: int)

  L  48: leave_torch_compile(self)

  L  58: forward(self)

  L  61: forward_native(self)

  L  64: forward_cuda(self)

  L  67: forward_npu(self)

  L  70: forward_hip(self)

  L  73: forward_xpu(self)

  L  76: forward_hpu(self)

  L  79: forward_cpu(self)

  L  82: dispatch_forward(self)


============================================================
FILE: python/sglang/srt/harmony_parser.py
Functions: 9
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  24: def prefix_hold(text: str, tokens: List[str])
         → Tuple[str, str]
         📝 Holds back the longest suffix of `text` that could be a prefix of any 

  L  46: def iter_tokens(text: str, start_pos: int)
         → Iterator[Token]
         📝 Iterate over structural tokens in left-to-right order.


CLASS: CanonicalStrategy
----------------------------------------
  L 126: __init__(self)

  L 137: parse(self, text: str)
         → Tuple[List[Event], str]


CLASS: HarmonyParser
----------------------------------------
  L 504: __init__(self)

  L 514: parse(self, chunk: str)
         → List[Event]


CLASS: TextStrategy
----------------------------------------
  L 422: __init__(self)

  L 438: set_buffer_context(self, buffer: str)

  L 441: parse(self, text: str)
         → Tuple[List[Event], str]


============================================================
FILE: python/sglang/srt/hf_transformers_utils.py
Functions: 11
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  66: def download_from_hf(model_path: str,
        allow_patterns: Optional[Union[str,
        list]])

  L  79: def get_hf_text_config(config: PretrainedConfig)
         📝 Get the "sub" config relevant to llm for multi modal models.

  L 117: def get_config(model: str,
        trust_remote_code: bool,
        revision: Optional[str],
        model_override_args: Optional[dict])
         @lru_cache_frozenset(maxsize=32)

  L 186: def get_generation_config(model: str,
        trust_remote_code: bool,
        revision: Optional[str])
         @lru_cache_frozenset(maxsize=32)

  L 201: def get_sparse_attention_config(model: str,
        sparse_attention_config_filename: str)
         → Dict[str, Any]

  L 233: def get_context_length(config)
         📝 Get the context length of a model from a huggingface model configs.

  L 257: def get_tokenizer(tokenizer_name: str)
         → Union[PreTrainedTokenizer, PreTrainedTokenizerFast]
         📝 Gets a tokenizer for the given model name via Huggingface.

  L 338: def get_tokenizer_from_processor(processor)

  L 344: def get_processor(tokenizer_name: str)

  L 410: def attach_additional_stop_token_ids(tokenizer)

  L 420: def check_gguf_file(model: Union[str, os.PathLike])
         → bool
         📝 Check if the file is a GGUF model.


============================================================
FILE: python/sglang/srt/host_shared_memory.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  75: def get_host_shared_memory_manager()

  L  80: def set_host_shared_memory_manager(instance: HostSharedMemoryManager)


CLASS: HostSharedMemoryManager
----------------------------------------
  L  18: __init__(self, base_name: str)

  L  23: malloc(self)


============================================================
FILE: python/sglang/srt/jinja_template_utils.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  81: def detect_jinja_template_content_format(chat_template: str)
         → str
         📝 Detect whether a chat template expects 'string' or 'openai' content fo

  L 117: def process_content_for_template_format(msg_dict: dict,
        content_format: str,
        image_data: list,
        video_data: list,
        audio_data: list,
        modalities: list)
         → dict
         📝 Process message content based on detected template format.


============================================================
FILE: python/sglang/srt/model_parallel.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 121: def tensor_parallel(module: torch.nn.Module, device_mesh: Optional[DeviceMesh])
         📝 Tensor parallelize the model across the given device mesh.


============================================================
FILE: python/sglang/srt/offloader.py
Functions: 30
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  50: def get_offloader()

  L  55: def set_offloader(instance: BaseOffloader)

  L  60: def create_offloader_from_server_args(server_args: ServerArgs, dp_rank: int)


CLASS: BaseOffloader
----------------------------------------
  L  30: wrap_modules(self, all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor], whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator])

  L  38: post_init(self)


CLASS: OffloaderV1
----------------------------------------
  L  81: __init__(self, cpu_offload_max_bytes: int)

  L  85: wrap_modules(self, all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor], whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator])

  L  93: maybe_offload_to_cpu(self, module: torch.nn.Module)
         → torch.nn.Module


CLASS: OffloaderV2
----------------------------------------
  L 150: __init__(self, group_size: int, num_in_group: int, prefetch_step: int, mode: str, dp_rank: int, dp_size: int)

  L 189: wrap_modules(self, all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor], whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator])

  L 229: post_init(self)


CLASS: _BaseParamOffloader
----------------------------------------
  L 322: create(mode: str)
         → '_BaseParamOffloader'

  L 330: __init__(self, module, param_name)

  L 338: post_init(self)

  L 341: create_device_tensor(self)


CLASS: _CpuParamOffloader
----------------------------------------
  L 357: __init__(self, module, param_name)

  L 361: create_device_tensor(self)


CLASS: _MetaParamOffloader
----------------------------------------
  L 348: __init__(self, module, param_name)

  L 352: create_device_tensor(self)


CLASS: _ModuleOffloader
----------------------------------------
  L 267: __init__(self, mode: str, module: torch.nn.Module, alt_stream: torch.cuda.Stream, whitelist_param_names: List[str])

  L 296: post_init(self)

  L 300: start_onload(self)

  L 307: offload(self)

  L 311: wait_and_get_device_tensors(self)


CLASS: _ShardedGpuParamOffloader
----------------------------------------
  L 453: __init__(self, module, param_name)

  L 472: post_init(self)

  L 501: create_device_tensor(self)


CLASS: _ShmCpuParamOffloader
----------------------------------------
  L 366: __init__(self, module, param_name)

  L 389: post_init(self)

  L 397: create_device_tensor(self)


============================================================
FILE: python/sglang/srt/operations.py
Functions: 15
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  21: def execute_operations(inputs, operations)

  L  30: def execute_overlapped_operations(inputs_arr: Sequence,
        operations_arr: Sequence,
        delta_stages: Sequence[int])
         → Sequence


CLASS: _StageExecutor
----------------------------------------
  L  76: __init__(self, debug_name: str, stages: List[Stage], inputs: dict)

  L  89: next(self)

  L 114: output(self)

  L 119: done(self)

  L 123: num_stages(self)


CLASS: _StateDict
----------------------------------------
  L 138: __init__(self)

  L 141: __setattr__(self, key, value)

  L 150: __getattr__(self, item)

  L 153: __delattr__(self, item)

  L 156: pop(self, item)

  L 159: update(self, values: Dict[str, Any])

  L 163: get(self, item)

  L 166: clear(self, expect_keys: Sequence[str])


============================================================
FILE: python/sglang/srt/operations_strategy.py
Functions: 2
============================================================


CLASS: OperationsStrategy
----------------------------------------
  L  19: concat(cls, items: List['OperationsStrategy'])
         → 'OperationsStrategy'

  L  31: init_new_tbo(layers: torch.nn.ModuleList, forward_mode: ForwardMode)
         → 'OperationsStrategy'


============================================================
FILE: python/sglang/srt/patch_torch.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  21: def monkey_patch_torch_reductions()
         📝 Monkey patching before Torch https://github.com/pytorch/pytorch/pull/1

  L  75: def monkey_patch_torch_compile()


============================================================
FILE: python/sglang/srt/poll_based_barrier.py
Functions: 3
============================================================


CLASS: PollBasedBarrier
----------------------------------------
  L   7: __init__(self, noop: bool)

  L  11: local_arrive(self)

  L  15: poll_global_arrived(self)
         → bool


============================================================
FILE: python/sglang/srt/reasoning_parser.py
Functions: 13
============================================================


CLASS: BaseReasoningFormatDetector
----------------------------------------
  L  22: __init__(self, think_start_token: str, think_end_token: str, force_reasoning: bool, stream_reasoning: bool)

  L  37: detect_and_parse(self, text: str)
         → StreamingParseResult
         📝 One-time parsing: Detects and parses reasoning sections in the provide

  L  63: parse_streaming_increment(self, new_text: str)
         → StreamingParseResult
         📝 Streaming incremental parsing for reasoning content.


CLASS: DeepSeekR1Detector
----------------------------------------
  L 141: __init__(self, stream_reasoning: bool, force_reasoning: bool)


CLASS: GptOssDetector
----------------------------------------
  L 200: __init__(self, stream_reasoning: bool, force_reasoning: bool)

  L 209: detect_and_parse(self, text: str)
         → StreamingParseResult

  L 232: parse_streaming_increment(self, new_text: str)
         → StreamingParseResult


CLASS: KimiDetector
----------------------------------------
  L 186: __init__(self, stream_reasoning: bool, force_reasoning: bool)


CLASS: Qwen3Detector
----------------------------------------
  L 168: __init__(self, stream_reasoning: bool, force_reasoning: bool)


CLASS: ReasoningParser
----------------------------------------
  L 275: __init__(self, model_type: Optional[str], stream_reasoning: bool, force_reasoning: Optional[bool])

  L 299: parse_non_stream(self, full_text: str)
         → Tuple[Optional[str], Optional[str]]
         📝 Non-streaming call: one-time parsing

  L 304: parse_stream_chunk(self, chunk_text: str)
         → Tuple[Optional[str], Optional[str]]
         📝 Streaming call: incremental parsing


CLASS: StreamingParseResult
----------------------------------------
  L  10: __init__(self, normal_text: Optional[str], reasoning_text: Optional[str])


============================================================
FILE: python/sglang/srt/server_args.py
Functions: 17
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L2341: def prepare_server_args(argv: List[str])
         → ServerArgs
         📝 Prepare the server arguments from the command line arguments.

  L2469: def print_deprecated_warning(message: str)

  L2473: def auto_choose_speculative_params(self: ServerArgs)
         📝 Automatically choose the parameters for speculative decoding.


CLASS: DeprecatedAction
----------------------------------------
  L2460: __init__(self, option_strings, dest, nargs)

  L2465: __call__(self, parser, namespace, values, option_string)


CLASS: LoRAPathAction
----------------------------------------
  L2440: __call__(self, parser, namespace, values, option_string)


CLASS: PortArgs
----------------------------------------
  L2381: init_new(server_args, dp_rank: Optional[int])
         → 'PortArgs'


CLASS: ServerArgs
----------------------------------------
  L 310: __post_init__(self)

  L 731: add_cli_args(parser: argparse.ArgumentParser)

  L2071: from_cli_args(cls, args: argparse.Namespace)

  L2079: url(self)

  L2085: get_hf_config(self)

  L2096: check_server_args(self)

  L2137: check_lora_server_args(self)

  L2220: validate_disagg_tp_size(self, prefill_tp: int, decode_tp: int)

  L2228: model_specific_adjustments(self)

  L2302: adjust_mem_fraction_for_vlm(self, model_config)


============================================================
FILE: python/sglang/srt/torch_memory_saver_adapter.py
Functions: 17
============================================================


CLASS: TorchMemorySaverAdapter
----------------------------------------
  L  21: create(enable: bool)

  L  33: check_validity(self, caller_name)

  L  40: configure_subprocess(self)

  L  43: region(self, tag: str)

  L  46: pause(self, tag: str)

  L  49: resume(self, tag: str)

  L  53: enabled(self)


CLASS: _TorchMemorySaverAdapterNoop
----------------------------------------
  L  79: configure_subprocess(self)

  L  83: region(self, tag: str)

  L  86: pause(self, tag: str)

  L  89: resume(self, tag: str)

  L  93: enabled(self)


CLASS: _TorchMemorySaverAdapterReal
----------------------------------------
  L  60: configure_subprocess(self)

  L  63: region(self, tag: str)

  L  66: pause(self, tag: str)

  L  69: resume(self, tag: str)

  L  73: enabled(self)


============================================================
FILE: python/sglang/srt/two_batch_overlap.py
Functions: 24
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  49: def get_token_num_per_seq(forward_mode: ForwardMode,
        spec_info: Optional[Union[EagleDraftInput,
        EagleVerifyInput]])

  L  65: def compute_split_seq_index(forward_mode: 'ForwardMode',
        num_tokens: int,
        extend_lens: Optional[Sequence[int]],
        token_num_per_seq: Optional[int])
         → Optional[int]

  L 180: def split_spec_info(spec_info: Optional[EagleVerifyInput],
        start_seq_index: int,
        end_seq_index: int,
        start_token_index: int,
        end_token_index: int)

  L 252: def compute_split_token_index(split_seq_index: int,
        forward_mode: 'ForwardMode',
        extend_seq_lens: Optional[Sequence[int]],
        token_num_per_seq: Optional[int])
         → int

  L 273: def compute_split_indices_for_cuda_graph_replay(forward_mode: ForwardMode,
        cuda_graph_num_tokens: int,
        spec_info: Optional[Union[EagleDraftInput,
        EagleVerifyInput]])

  L 785: def model_forward_maybe_tbo(layers,
        enable_tbo: bool,
        positions: torch.Tensor,
        forward_batch: ForwardBatch,
        hidden_states: torch.Tensor,
        input_data_scatter_mode: ScatterMode,
        residual: Optional[torch.Tensor],
        zero_allocator: Optional[BumpAllocator])


CLASS: MaybeTboDeepEPDispatcher
----------------------------------------
  L 964: __init__(self)

  L 973: dispatch(self)
         → DispatchOutput

  L 976: dispatch_a(self)

  L 979: dispatch_b(self)

  L 982: combine(self)
         → torch.Tensor

  L 985: combine_a(self)

  L 988: combine_b(self)


CLASS: TboCudaGraphRunnerPlugin
----------------------------------------
  L 303: __init__(self)

  L 306: capture_one_batch_size(self, batch: ForwardBatch, num_tokens: int)

  L 331: replay_prepare(self, forward_mode: ForwardMode, bs: int, num_token_non_padded: int, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])


CLASS: TboDPAttentionPreparer
----------------------------------------
  L 358: prepare_all_gather(self, local_batch: ScheduleBatch)

  L 404: compute_output(self, partial_global_info)


CLASS: TboForwardBatchPreparer
----------------------------------------
  L 452: prepare(cls, batch: ForwardBatch, is_draft_worker: bool)

  L 464: prepare_raw(cls, batch: ForwardBatch, tbo_children_num_token_non_padded: torch.Tensor)

  L 529: derive_fields_related_to_seq_len_for_two_chunk(cls, batch: ForwardBatch)

  L 591: filter_batch(cls, batch: ForwardBatch)

  L 740: compute_tbo_children_num_token_non_padded(cls, batch: ForwardBatch)

  L 747: compute_tbo_children_num_token_non_padded_raw(cls, tbo_split_token_index: int, num_token_non_padded: int)


============================================================
FILE: python/sglang/srt/utils.py
Functions: 171
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 103: def is_hip()
         → bool

  L 118: def is_cuda()

  L 122: def is_cuda_alike()

  L 126: def is_hpu()
         → bool

  L 130: def is_xpu()
         → bool

  L 134: def is_npu()
         → bool

  L 138: def is_host_cpu_x86()
         → bool

  L 147: def is_cpu()
         → bool

  L 151: def get_cuda_version()

  L 169: def is_blackwell()

  L 176: def is_sm100_supported(device)
         → bool
         @lru_cache(maxsize=1)

  L 183: def is_sm90_supported(device)
         → bool
         @lru_cache(maxsize=1)

  L 192: def get_bool_env_var(name: str, default: str)
         → bool

  L 209: def get_int_env_var(name: str, default: int)
         → int

  L 219: def support_triton(backend: str)
         → bool

  L 233: def cpu_has_amx_support()

  L 237: def use_intel_amx_backend(layer)

  L 241: def is_flashinfer_available()
         📝 Check whether flashinfer is available.

  L 251: def random_uuid()
         → str

  L 312: def enable_show_time_cost()

  L 339: def mark_start(name, interval, color, indent)

  L 349: def mark_end(name)

  L 359: def calculate_time(show, min_cost_ms)

  L 378: def get_available_gpu_memory(device,
        gpu_id,
        distributed,
        empty_cache,
        cpu_group)
         📝 Get available memory for cuda:gpu_id device.

  L 451: def is_pin_memory_available()
         → bool

  L 460: def make_layers(num_hidden_layers: int,
        layer_fn: LayerFn,
        pp_rank: Optional[int],
        pp_size: Optional[int],
        prefix: str,
        return_tuple: bool,
        offloader_kwargs: Dict[str,
        Any])
         → Tuple[int, int, torch.nn.ModuleList]
         📝 Make a list of layers with the given layer function

  L 504: def set_random_seed(seed: int)
         → None
         📝 Set the random seed for all libraries.

  L 513: def find_process_using_port(port: int)
         → Optional[psutil.Process]

  L 525: def wait_port_available(port: int,
        port_name: str,
        timeout_s: int,
        raise_exception: bool)
         → bool

  L 553: def is_port_available(port)
         📝 Return whether a port is available.

  L 567: def get_free_port()

  L 580: def decode_video_base64(video_base64)

  L 659: def load_audio(audio_file: str, sr: Optional[int], mono: bool)
         → np.ndarray

  L 707: def load_image(image_file: Union[Image.Image, str, ImageData, bytes])
         → tuple[Image.Image, tuple[int, int]]

  L 741: def load_video(video_file: Union[str, bytes], use_gpu: bool)

  L 796: def suppress_other_loggers()

  L 816: def assert_pkg_version(pkg: str, min_version: str, message: str)

  L 831: def kill_process_tree(parent_pid, include_parent: bool, skip_pid: int)
         📝 Kill the process and all its child processes.

  L 870: def monkey_patch_p2p_access_check()
         📝 Monkey patch the slow p2p access check.

  L 888: def monkey_patch_vllm_gguf_config()

  L 914: def set_ulimit(target_soft_limit)

  L 938: def add_api_key_middleware(app, api_key: str)

  L 952: def prepare_model_and_tokenizer(model_path: str, tokenizer_path: str)

  L 964: def configure_logger(server_args, prefix: str)

  L 986: def replace_submodule(model: nn.Module, module_name: str, new_module: nn.Module)
         → nn.Module
         📝 Replace a submodule in a model with a new module.

  L 996: def set_weight_attrs(weight: torch.Tensor,
        weight_attrs: Optional[Dict[str,
        Any]])
         📝 Set attributes on a weight tensor.

  L1016: def broadcast_pyobj(data: List[Any],
        rank: int,
        dist_group: Optional[torch.distributed.ProcessGroup],
        src: int,
        force_cpu_device: bool)
         📝 Broadcast inputs from src rank to all other ranks with torch.dist back

  L1063: def point_to_point_pyobj(data: List[Any],
        rank: int,
        group: Optional[torch.distributed.ProcessGroup],
        src: int,
        dst: int)
         📝 Send data from src to dst in group using DeviceToDevice communication.

  L1122: def pytorch_profile(name, func)
         📝 Args:

  L1150: def get_zmq_socket(context: zmq.Context,
        socket_type: zmq.SocketType,
        endpoint: str,
        bind: bool)

  L1191: def dump_to_file(dirpath, name, value)

  L1206: def is_triton_3()

  L1210: def maybe_torch_compile()
         📝 torch.compile does not work for triton 2.2.0, which is needed in xlm1'

  L1224: def delete_directory(dirpath)

  L1237: def set_prometheus_multiproc_dir()

  L1255: def add_prometheus_middleware(app)

  L1268: def bind_port(port)
         📝 Bind to a specific port, assuming it's available.

  L1277: def get_amdgpu_memory_capacity()

  L1310: def get_device_sm()

  L1317: def get_nvgpu_memory_capacity()

  L1356: def get_hpu_memory_capacity()

  L1387: def get_npu_memory_capacity()

  L1396: def get_device_memory_capacity(device: str)

  L1415: def init_custom_process_group(backend,
        init_method,
        timeout,
        world_size,
        rank,
        store,
        group_name,
        pg_options)

  L1484: def crash_on_warnings()

  L1489: def print_warning_once(msg: str)
         → None

  L1495: def print_info_once(msg: str)
         → None
         @functools.lru_cache(None)

  L1499: def get_device_name(device_id: int)
         → str

  L1514: def is_habana_available()
         → bool
         @lru_cache(maxsize=1)

  L1519: def get_device(device_id: Optional[int])
         → str
         @lru_cache(maxsize=8)

  L1561: def get_device_count()
         → int
         @lru_cache(maxsize=1)

  L1586: def get_device_core_count(device_id: int)
         → int

  L1593: def get_device_capability(device_id: int)
         → Tuple[int, int]

  L1618: def get_npu_compiler_config()

  L1627: def get_compiler_backend()
         → str

  L1657: def supports_custom_op()
         → bool

  L1661: def direct_register_custom_op(op_name: str,
        op_func: Callable,
        mutates_args: List[str],
        fake_impl: Optional[Callable],
        target_lib: Optional[Library])
         📝 `torch.library.custom_op` can have significant overhead because it

  L1731: def set_gpu_proc_affinity(tp_size: int, nnodes: int, gpu_id: int)

  L1766: def disable_request_logging()
         → bool
         @lru_cache(maxsize=2)

  L1770: def dataclass_to_string_truncated(data,
        max_length,
        skip_names: Optional[Set[str]])

  L1812: def permute_weight(x: torch.Tensor)
         → torch.Tensor

  L1874: def debug_timing(func)

  L1898: def nullable_str(val: str)

  L1904: def pyspy_dump_schedulers()
         📝 py-spy dump on all scheduler in a local node.

  L1918: def kill_itself_when_parent_died()

  L1928: def set_uvicorn_logging_configs()

  L1941: def get_ip()
         → str

  L1985: def get_open_port()
         → int

  L2009: def is_valid_ipv6_address(address: str)
         → bool

  L2017: def maybe_wrap_ipv6_address(address: str)
         → str

  L2023: def format_tcp_address(ip: str, port: int)
         → str

  L2027: def configure_ipv6(dist_init_addr)

  L2059: def launch_dummy_health_check_server(host, port, enable_metrics)

  L2105: def create_checksum(directory: str)

  L2109: def set_cuda_arch()

  L2116: def next_power_of_2(n: int)

  L2120: def round_up(x: int, y: int)
         → int

  L2135: def empty_context()

  L2139: def add_prefix(name: str, prefix: str)
         → str
         📝 Add a weight path prefix to a module name.

  L2152: def is_remote_url(url: Union[str, Path])
         → bool
         📝 Check if the URL is a remote URL of the format:

  L2165: def parse_connector_type(url: str)
         → str
         📝 Parse the connector type from the URL of the format:

  L2178: def retry(fn,
        max_retry: int,
        initial_delay: float,
        max_delay: float,
        should_retry: Callable[[Any],
        bool])

  L2207: def flatten_nested_list(nested_list)

  L2216: def is_non_idle_and_non_empty(forward_mode, hidden_states)

  L2224: def fast_topk(values, topk, dim)

  L2233: def bind_or_assign(target, source)

  L2241: def get_local_ip_auto()
         → str

  L2250: def get_local_ip_by_nic(interface: str)
         → str

  L2279: def get_local_ip_by_remote()
         → str

  L2307: def is_page_size_one(server_args)

  L2313: def is_no_spec_infer_or_topk_one(server_args)

  L2321: def is_fa3_default_architecture(hf_config)

  L2353: def log_info_on_rank0(logger, msg)

  L2360: def load_json_config(data: str)

  L2367: def dispose_tensor(x: torch.Tensor)

  L2393: def require_mlp_tp_gather(server_args)
         📝 Check if the input of MLP is obtained by all-gather rather than all-re

  L2416: def require_attn_tp_gather(server_args)
         📝 Check if the input of attention is scattered.

  L2430: def require_gathered_buffer(server_args)

  L2434: def require_mlp_sync(server_args)

  L2438: def find_local_repo_dir(repo_id: str, revision: Optional[str])
         → Optional[str]

  L2463: def read_system_prompt_from_file(model_name: str)
         → str
         📝 Read system prompt from a file in the HuggingFace cache directory.

  L2486: def bind_or_assign(target, source)

  L2494: def prepack_weight_if_needed(weight)

  L2506: def dim_is_supported(weight)

  L2578: def dynamic_import(func_path: str)

  L2591: def gc_object_counts()

  L2600: def configure_gc_warning(warn_threshold_secs)

  L2621: def freeze_gc(context: str)

  L2635: def configure_gc_logger()

  L2661: def align(x: int, y: int)
         → int

  L2666: def ceil_div(x: int, y: int)
         → int

  L2670: def parse_lscpu_topology()

  L2690: def get_physical_cpus_by_numa()

  L2725: def get_cpu_ids_by_node()

  L2736: def is_shm_available(dtype, world_size, local_size)

  L2745: def lru_cache_frozenset(maxsize)

  L2790: def apply_module_patch(target_module, target_function, wrappers)

  L2812: def parse_module_path(module_path, function_name, create_dummy)

  L2888: def mxfp_supported()
         📝 Returns whether the current platform supports MX types.

  L3002: def is_triton_kernels_available()
         → bool
         @lru_cache(maxsize=1)

  L3006: def check_cuda_result(raw_output)


CLASS: BumpAllocator
----------------------------------------
  L2342: __init__(self, buffer_size: int, dtype, device)

  L2346: allocate(self, size: int)


CLASS: ConcurrentCounter
----------------------------------------
  L2924: __init__(self, initial: int)
         📝 Initialize the counter with an optional initial value.

  L2934: value(self)
         → int
         📝 Return the current value of the counter.

  L2947: __repr__(self)
         → str
         📝 Return an informative string representation of the counter.

  L2951: increment(self, n: int, notify_all: bool)
         📝 Atomically increment the counter by a given amount and notify all wait

  L2964: decrement(self, n: int, notify_all: bool)
         📝 Atomically decrement the counter by a given amount and notify all wait

  L2977: wait_for(self, condition: Callable[[int], bool])
         📝 Asynchronously wait until the counter satisfies a given condition.

  L2991: wait_for_zero(self)
         📝 Asynchronously wait until the counter reaches zero.


CLASS: DynamicGradMode
----------------------------------------
  L 267: set_inference_mode(mode: bool)

  L 275: __init__(self, mode)

  L 283: __new__(cls, mode_or_orig_func)

  L 288: __enter__(self)
         → None

  L 296: __exit__(self, exc_type: Any, exc_value: Any, traceback: Any)
         → None

  L 302: clone(self)
         → 'DynamicGradMode'
         📝 Create a copy of this class


CLASS: EmptyContextManager
----------------------------------------
  L2128: __enter__(self)

  L2131: __exit__(self, exc_type, exc_value, traceback)


CLASS: LayerFn
----------------------------------------
  L 457: __call__(self, layer_id: int, prefix: str)
         → torch.nn.Module


CLASS: LazyValue
----------------------------------------
  L2566: __init__(self, creator: Callable)

  L2571: value(self)


CLASS: MultiprocessingSerializer
----------------------------------------
  L1834: serialize(obj, output_str: bool)
         📝 Serialize a Python object using ForkingPickler.

  L1857: deserialize(data)
         📝 Deserialize a previously serialized object.


CLASS: PackWeightMethod
----------------------------------------
  L2557: __init__(self, weight_names, transpose_dims)

  L2561: process_weights_after_loading(self, module)
         → None


CLASS: TimeInfo
----------------------------------------
  L 318: __init__(self, name, interval, color, indent)

  L 327: check(self)

  L 333: pretty_print(self)


CLASS: Withable
----------------------------------------
  L2375: __init__(self)

  L2379: value(self)
         → T

  L2383: with_value(self, new_value: T)


============================================================
FILE: python/sglang/srt/warmup.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  16: def warmup(name: str)
         → callable

  L  24: async def execute_warmups(disaggregation_mode: str,
        warmup_names: List[str],
        tokenizer_manager: TokenizerManager)

  L  38: async def voice_chat(disaggregation_mode: str,
        tokenizer_manager: TokenizerManager)
         @warmup('voice_chat')
