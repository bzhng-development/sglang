================================================================================
FUNCTION INDEX: moe module
================================================================================
Total Functions: 71
Documented: 5


============================================================
FILE: python/sglang/srt/layers/moe/cutlass_moe.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  21: def cutlass_fused_experts_fp8(a: torch.Tensor,
        w1_q: torch.Tensor,
        w2_q: torch.Tensor,
        w1_scale: torch.Tensor,
        w2_scale: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        a1_strides: torch.Tensor,
        c1_strides: torch.Tensor,
        a2_strides: torch.Tensor,
        c2_strides: torch.Tensor,
        workspace: torch.Tensor,
        a_ptrs: torch.Tensor,
        b_ptrs: torch.Tensor,
        out_ptrs: torch.Tensor,
        a_scales_ptrs: torch.Tensor,
        b_scales_ptrs: torch.Tensor,
        expert_offsets: torch.Tensor,
        problem_sizes1: torch.Tensor,
        problem_sizes2: torch.Tensor,
        use_fp8_blockscale: bool)
         ‚Üí torch.Tensor
         üìù Performs Fused MoE computation using CUTLASS-like kernels with FP8 wei

  L 214: def cutlass_moe_fp4(a: torch.Tensor,
        a1_gscale: torch.Tensor,
        w1_fp4: torch.Tensor,
        w1_blockscale: torch.Tensor,
        w1_alphas: torch.Tensor,
        a2_gscale: torch.Tensor,
        w2_fp4: torch.Tensor,
        w2_blockscale: torch.Tensor,
        w2_alphas: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        params: CutlassMoEParams,
        apply_router_weight_on_input: bool)
         üìù MoE implementation for FP4 Inputs


============================================================
FILE: python/sglang/srt/layers/moe/cutlass_moe_params.py
Functions: 3
============================================================


CLASS: CutlassMoEParams
----------------------------------------
  L  90: __init__(self, cutlass_moe_type: CutlassMoEType, device: torch.device, num_experts: int, intermediate_size_per_partition: int, hidden_size: int)

  L 143: to_gemm1_args(self)
         ‚Üí dict

  L 157: to_gemm2_args(self)
         ‚Üí dict


============================================================
FILE: python/sglang/srt/layers/moe/cutlass_w4a8_moe.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  20: def cutlass_w4a8_moe(start_expert_id: int,
        end_expert_id: int,
        total_num_experts: int,
        a: torch.Tensor,
        w1_q: torch.Tensor,
        w2_q: torch.Tensor,
        w1_scale: torch.Tensor,
        w2_scale: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids_: torch.Tensor,
        local_topk_ids: torch.Tensor,
        a_strides1: torch.Tensor,
        b_strides1: torch.Tensor,
        c_strides1: torch.Tensor,
        a_strides2: torch.Tensor,
        b_strides2: torch.Tensor,
        c_strides2: torch.Tensor,
        s_strides13: torch.Tensor,
        s_strides2: torch.Tensor,
        expert_offsets: torch.Tensor,
        problem_sizes1: torch.Tensor,
        problem_sizes2: torch.Tensor,
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        apply_router_weight_on_input: bool)
         ‚Üí torch.Tensor
         üìù This function computes a w4a8-quantized Mixture of Experts (MoE) layer


============================================================
FILE: python/sglang/srt/layers/moe/fused_moe_native.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  14: def fused_moe_forward_native(layer: torch.nn.Module,
        x: torch.Tensor,
        topk_output: StandardTopKOutput,
        moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor

  L  41: def moe_forward_native(layer: torch.nn.Module,
        x: torch.Tensor,
        topk_output: StandardTopKOutput,
        moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/moe/rocm_moe_utils.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  23: def rocm_aiter_asm_moe_tkw1_impl(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        fc1_scale: Optional[torch.Tensor],
        fc2_scale: Optional[torch.Tensor],
        fc1_smooth_scale: Optional[torch.Tensor],
        fc2_smooth_scale: Optional[torch.Tensor],
        a16: bool,
        per_tensor_quant_scale: Optional[torch.Tensor],
        expert_mask: Optional[torch.Tensor],
        activation_method: int)
         ‚Üí torch.Tensor

  L  61: def rocm_aiter_asm_moe_tkw1_fake(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        fc1_scale: Optional[torch.Tensor],
        fc2_scale: Optional[torch.Tensor],
        fc1_smooth_scale: Optional[torch.Tensor],
        fc2_smooth_scale: Optional[torch.Tensor],
        a16: bool,
        per_tensor_quant_scale: Optional[torch.Tensor],
        expert_mask: Optional[torch.Tensor],
        activation_method: int)
         ‚Üí torch.Tensor

  L  89: def rocm_fused_experts_tkw1(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[list[int]])
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/moe/router.py
Functions: 10
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  14: def fused_moe_router_kernel(input_ptr,
        moe_router_weight_ptr,
        topk_weights_ptr,
        topk_ids_ptr,
        correction_bias_ptr,
        is_correction_bias: tl.constexpr,
        num_experts: tl.constexpr,
        topk: tl.constexpr,
        moe_softcapping: tl.constexpr,
        moe_renormalize: tl.constexpr,
        hidden_dim: tl.constexpr,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 117: def fused_moe_router_impl(x: torch.Tensor,
        router_weight: torch.Tensor,
        topk: int,
        moe_softcapping: float,
        correction_bias: Optional[torch.Tensor])

  L 160: def fused_moe_router_large_bs_kernel(a_ptr,
        b_ptr,
        topk_weights_ptr,
        topk_ids_ptr,
        bs,
        num_experts: tl.constexpr,
        topk: tl.constexpr,
        moe_softcapping: tl.constexpr,
        moe_renormalize: tl.constexpr,
        K: tl.constexpr,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        BLOCK_SIZE_K: tl.constexpr,
        stride_am: tl.constexpr,
        stride_bn: tl.constexpr)
         @triton.jit

  L 269: def fused_moe_router_large_bs_impl(x: torch.Tensor,
        router_weight: torch.Tensor,
        topk: int,
        moe_softcapping: float,
        BLOCK_SIZE_M: int,
        BLOCK_SIZE_N: int,
        BLOCK_SIZE_K: int)

  L 312: def fused_moe_router_shim(moe_softcapping,
        hidden_states,
        gating_output,
        topk,
        renormalize,
        correction_bias: Optional[torch.Tensor])


CLASS: FusedMoeRouter
----------------------------------------
  L 356: __init__(self, router_linear, topk, moe_softcapping)
         ‚Üí None

  L 361: __call__(self)

  L 364: forward(self, x: torch.Tensor, residual: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 372: forward_cuda(self, x: torch.Tensor, autotune)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 383: forward_vllm(self, x: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


============================================================
FILE: python/sglang/srt/layers/moe/topk.py
Functions: 27
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 345: def fused_topk_torch_native(hidden_states: torch.Tensor,
        gating_output: torch.Tensor,
        topk: int,
        renormalize: bool)

  L 366: def fused_topk_cpu(hidden_states: torch.Tensor,
        gating_output: torch.Tensor,
        topk: int,
        renormalize: bool,
        num_token_non_padded: Optional[torch.Tensor],
        expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo])

  L 385: def apply_topk_weights_cpu(need_apply, topk_weights, inputs)

  L 398: def fused_topk(hidden_states: torch.Tensor,
        gating_output: torch.Tensor,
        topk: int,
        renormalize: bool,
        num_token_non_padded: Optional[torch.Tensor],
        expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo])

  L 429: def grouped_topk_gpu(hidden_states: torch.Tensor,
        gating_output: torch.Tensor,
        topk: int,
        renormalize: bool,
        num_expert_group: Optional[int],
        topk_group: Optional[int],
        num_fused_shared_experts: int,
        routed_scaling_factor: Optional[float],
        num_token_non_padded: Optional[torch.Tensor],
        expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo],
        apply_routed_scaling_factor_on_output: Optional[bool])
         @torch.compile(dynamic=True, backend=get_compiler_backend())

  L 497: def grouped_topk_cpu(hidden_states: torch.Tensor,
        gating_output: torch.Tensor,
        topk: int,
        renormalize: bool,
        num_expert_group: Optional[int],
        topk_group: Optional[int],
        num_fused_shared_experts: int,
        routed_scaling_factor: Optional[float],
        num_token_non_padded: Optional[torch.Tensor],
        expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo],
        apply_routed_scaling_factor_on_output: Optional[bool])

  L 526: def biased_grouped_topk_impl(hidden_states: torch.Tensor,
        gating_output: torch.Tensor,
        correction_bias: torch.Tensor,
        topk: int,
        renormalize: bool,
        num_expert_group: Optional[int],
        topk_group: Optional[int],
        num_fused_shared_experts: int,
        routed_scaling_factor: Optional[float],
        num_token_non_padded: Optional[torch.Tensor],
        expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo],
        apply_routed_scaling_factor_on_output: Optional[bool])
         @torch.compile(dynamic=True, backend=get_compiler_backend(), disable=_is_npu)

  L 599: def is_power_of_two(n)

  L 622: def biased_grouped_topk_gpu(hidden_states: torch.Tensor,
        gating_output: torch.Tensor,
        correction_bias: torch.Tensor,
        topk: int,
        renormalize: bool,
        num_expert_group: Optional[int],
        topk_group: Optional[int],
        num_fused_shared_experts: int,
        routed_scaling_factor: Optional[float],
        num_token_non_padded: Optional[torch.Tensor],
        expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo],
        apply_routed_scaling_factor_on_output: Optional[bool])

  L 701: def biased_grouped_topk_cpu(hidden_states: torch.Tensor,
        gating_output: torch.Tensor,
        correction_bias: torch.Tensor,
        topk: int,
        renormalize: bool,
        num_expert_group: Optional[int],
        topk_group: Optional[int],
        compiled: bool,
        num_fused_shared_experts: int,
        routed_scaling_factor: Optional[float],
        num_token_non_padded: Optional[torch.Tensor],
        expert_location_dispatch_info: Optional[ExpertLocationDispatchInfo],
        apply_routed_scaling_factor_on_output: Optional[bool])

  L 743: def select_experts(hidden_states: torch.Tensor,
        router_logits: torch.Tensor,
        topk_config: TopKConfig)
         ‚Üí StandardTopKOutput


CLASS: BypassedTopKOutput
----------------------------------------
  L 178: format(self)
         ‚Üí TopKOutputFormat


CLASS: StandardTopKOutput
----------------------------------------
  L 152: format(self)
         ‚Üí TopKOutputFormat


CLASS: TopK
----------------------------------------
  L 187: __init__(self, top_k: int)

  L 226: forward_native(self, hidden_states: torch.Tensor, router_logits: torch.Tensor)
         ‚Üí TopKOutput

  L 243: forward_cuda(self, hidden_states: torch.Tensor, router_logits: torch.Tensor)
         ‚Üí TopKOutput

  L 280: forward_cpu(self, hidden_states: torch.Tensor, router_logits: torch.Tensor)
         ‚Üí TopKOutput

  L 296: forward_npu(self, hidden_states: torch.Tensor, router_logits: torch.Tensor)
         ‚Üí TopKOutput

  L 334: empty_topk_output(self, device: torch.device)
         ‚Üí TopKOutput


CLASS: TopKOutput
----------------------------------------
  L 139: format(self)
         ‚Üí TopKOutputFormat
         üìù The format of the output.


CLASS: TopKOutputChecker
----------------------------------------
  L 105: format_is_standard(topk_output: TopKOutput)
         ‚Üí TypeGuard[StandardTopKOutput]

  L 109: format_is_triton_kernel(topk_output: TopKOutput)
         ‚Üí TypeGuard[TritonKernelTopKOutput]

  L 115: format_is_bypassed(topk_output: TopKOutput)
         ‚Üí TypeGuard[BypassedTopKOutput]


CLASS: TopKOutputFormat
----------------------------------------
  L 124: is_standard(self)
         ‚Üí bool

  L 127: is_triton_kernel(self)
         ‚Üí bool

  L 130: is_bypassed(self)
         ‚Üí bool


CLASS: TritonKernelTopKOutput
----------------------------------------
  L 164: format(self)
         ‚Üí TopKOutputFormat


============================================================
FILE: python/sglang/srt/layers/moe/utils.py
Functions: 23
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 110: def initialize_moe_config(server_args: ServerArgs)

  L 130: def get_moe_a2a_backend()
         ‚Üí MoeA2ABackend

  L 138: def get_moe_runner_backend()
         ‚Üí MoeRunnerBackend

  L 146: def get_deepep_mode()
         ‚Üí DeepEPMode

  L 154: def get_deepep_config()
         ‚Üí str

  L 162: def is_tbo_enabled()
         ‚Üí bool

  L 170: def get_tbo_token_distribution_threshold()
         ‚Üí float

  L 181: def should_use_flashinfer_trtllm_moe()
         @lru_cache(maxsize=1)

  L 191: def should_use_flashinfer_cutlass_moe_fp4_allgather()
         üìù Perform FP4 quantize before all-gather for flashinfer cutlass moe to r
         @lru_cache(maxsize=1)


CLASS: DeepEPMode
----------------------------------------
  L  76: enable_normal(self)
         ‚Üí bool

  L  79: enable_low_latency(self)
         ‚Üí bool

  L  82: resolve(self, is_extend_in_batch: bool)
         ‚Üí DeepEPMode

  L  91: is_normal(self)
         ‚Üí bool

  L  94: is_low_latency(self)
         ‚Üí bool

  L  97: is_auto(self)
         ‚Üí bool


CLASS: MoeA2ABackend
----------------------------------------
  L  35: is_none(self)

  L  38: is_deepep(self)


CLASS: MoeRunnerBackend
----------------------------------------
  L  51: is_auto(self)

  L  54: is_triton(self)

  L  57: is_triton_kernel(self)

  L  60: is_flashinfer_trtllm(self)

  L  63: is_flashinfer_cutlass(self)

  L  66: is_flashinfer_mxfp4(self)
