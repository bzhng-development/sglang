
# python/sglang/srt/managers/cache_controller.py
  LayerDoneCounter.__init__(num_layers)
  LayerDoneCounter.next_producer()
  LayerDoneCounter.update_producer()
  LayerDoneCounter.set_consumer(index)
  LayerDoneCounter.increment()
  LayerDoneCounter.wait_until(threshold)
  LayerDoneCounter.reset()
  CacheOperation.__init__(host_indices: torch.Tensor, device_indices: torch.Tensor, node_id: int, priority: Optional[int])
  CacheOperation.merge(other: 'CacheOperation') -> None
  CacheOperation.split(factor) -> List['CacheOperation']
  CacheOperation.__lt__(other: 'CacheOperation')
  TransferBuffer.__init__(stop_event, buffer_count: int, max_buffer_size: int) -> None
  TransferBuffer.full() -> bool
  TransferBuffer.empty() -> bool
  TransferBuffer.put(item, block, timeout) -> None
  TransferBuffer.get(block, timeout) -> Optional[CacheOperation]
  TransferBuffer.clear()
  StorageOperation.__init__(host_indices: torch.Tensor, token_ids: List[int], last_hash: Optional[str], hash_value: Optional[List[str]])
  StorageOperation.__lt__(other: 'StorageOperation')
  PrefetchOperation.__init__(request_id: str, host_indices: torch.Tensor, token_ids: List[int], last_hash: Optional[str])
  PrefetchOperation.increment(num_tokens: int)
  PrefetchOperation.mark_done()
  PrefetchOperation.is_done() -> bool
  HiCacheController.__init__(token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, mem_pool_host: HostKVCache, page_size: int, tp_group: torch.distributed.ProcessGroup, load_cache_event: threading.Event, write_policy: str, io_backend: str, storage_backend: Optional[str], prefetch_threshold: int, model_name: Optional[str], storage_backend_extra_config: Optional[str])
  HiCacheController.reset()
  HiCacheController.write(device_indices: torch.Tensor, priority: Optional[int], node_id: int) -> Optional[torch.Tensor]
  HiCacheController.load(host_indices: torch.Tensor, priority: Optional[int], node_id: int) -> Optional[torch.Tensor]
  HiCacheController.move_indices(host_indices, device_indices)
  HiCacheController.write_thread_func_direct()
  HiCacheController.load_thread_func_layer_by_layer()
  HiCacheController.evict_device(device_indices: torch.Tensor, host_indices: torch.Tensor) -> int
  HiCacheController.evict_host(host_indices: torch.Tensor, backup_only: bool) -> int
  HiCacheController.prefetch(request_id: str, host_indices: torch.Tensor, new_input_tokens: List[int], last_hash: Optional[str]) -> PrefetchOperation
  HiCacheController.terminate_prefetch(operation)
  HiCacheController.append_host_mem_release(host_indices: torch.Tensor)
  HiCacheController.prefetch_io_aux_func()
  HiCacheController.prefetch_rate_limited() -> bool
  HiCacheController.prefetch_thread_func()
  HiCacheController.write_storage(host_indices: torch.Tensor, token_ids: List[int], hash_value: Optional[List[str]]) -> int
  HiCacheController.backup_thread_func()

# python/sglang/srt/managers/data_parallel_controller.py
  LoadBalanceMethod.from_str(cls, method: str)
  DataParallelController.__init__(server_args: ServerArgs, port_args: PortArgs, dp_balance_meta: DPBalanceMeta) -> None
  DataParallelController.launch_dp_schedulers(server_args, port_args)
  DataParallelController.launch_tensor_parallel_group_thread(server_args: ServerArgs, port_args: PortArgs, base_gpu_id: int, dp_rank: int, ready_event: threading.Event)
  DataParallelController.launch_dp_attention_schedulers(server_args, port_args)
  DataParallelController.launch_tensor_parallel_group(server_args: ServerArgs, port_args: PortArgs, base_gpu_id: int, dp_rank: int)
  DataParallelController.round_robin_scheduler(req: Req)
  DataParallelController.shortest_queue_scheduler(input_requests)
  DataParallelController.minimum_tokens_scheduler(req)
  DataParallelController.event_loop()
run_data_parallel_controller_process(server_args: ServerArgs, port_args: PortArgs, pipe_writer)

# python/sglang/srt/managers/detokenizer_manager.py
  DetokenizerManager.__init__(server_args: ServerArgs, port_args: PortArgs)
  DetokenizerManager.event_loop()
  DetokenizerManager.trim_matched_stop(output: Union[str, List[int]], finished_reason: Dict, no_stop_trim: bool)
  DetokenizerManager.handle_batch_embedding_out(recv_obj: BatchEmbeddingOut)
  DetokenizerManager.handle_batch_token_id_out(recv_obj: BatchTokenIDOut)
  DetokenizerManager.handle_multimodal_decode_req(recv_obj: BatchMultimodalDecodeReq)
  DetokenizerManager.handle_freeze_gc_req(recv_req: FreezeGCReq)
  LimitedCapacityDict.__init__(capacity: int)
  LimitedCapacityDict.__setitem__(key, value)
run_detokenizer_process(server_args: ServerArgs, port_args: PortArgs)

# python/sglang/srt/managers/io_struct.py
  GenerateReqInput.contains_mm_input() -> bool
  GenerateReqInput.normalize_batch_and_arguments()
  GenerateReqInput.regenerate_rid()
  GenerateReqInput.__getitem__(i)
  BatchTokenizedGenerateReqInput.__len__()
  BatchTokenizedGenerateReqInput.__getitem__(i)
  BatchTokenizedGenerateReqInput.__iter__()
  EmbeddingReqInput.normalize_batch_and_arguments()
  EmbeddingReqInput.regenerate_rid()
  EmbeddingReqInput.contains_mm_input() -> bool
  EmbeddingReqInput.__getitem__(i)
  BatchTokenizedEmbeddingReqInput.__len__()
  BatchTokenizedEmbeddingReqInput.__getitem__(i)
  BatchTokenizedEmbeddingReqInput.__iter__()
  LoadLoRAAdapterReqInput.to_ref() -> LoRARef
  UnloadLoRAAdapterReqInput.to_ref() -> LoRARef

# python/sglang/srt/managers/mm_utils.py
  TransportProxyTensor.__new__(cls, data: torch.Tensor, name: Optional[str], fields: Optional[Dict[str, Any]], transport_mode: TensorTransportMode)
  TransportProxyTensor.__getstate__()
  TransportProxyTensor.__setstate__(state: Dict[str, Any])
  TransportProxyTensor.name() -> Optional[str]
  TransportProxyTensor.fields() -> Dict[str, Any]
  TransportProxyTensor.transport_mode() -> TensorTransportMode
  MultiModalityDataPaddingPattern.pad_input_tokens(input_ids: List[int], mm_inputs: MultimodalInputs) -> List[int]
  MultiModalityDataPaddingPatternTokenPairs.__init__(data_token_pairs: Optional[List[Tuple[int, int]]], data_start_token_ids: Optional[List[int]]) -> None
  MultiModalityDataPaddingPatternTokenPairs.pad_input_tokens(input_ids: List[int], mm_inputs: MultimodalInputs) -> List[int]
  MultiModalityDataPaddingPatternMultimodalTokens.pad_input_tokens(input_ids: List[int], mm_inputs: MultimodalInputs) -> List[int]
init_embedding_cache(max_size: int)
get_embedding_hash(embedding_items: List[MultimodalDataItem]) -> int
get_embedding_chunk(embedding: torch.Tensor, extend_prefix_len: int, extend_seq_len: int, items_offset: List[Tuple[int, int]]) -> Tuple[torch.Tensor, int, int]
get_embedding_and_mask(data_embedding_func: Callable[[List[MultimodalDataItem]], torch.Tensor], embedding_items: List[MultimodalDataItem], placeholder_tensor: torch.Tensor, input_ids: torch.Tensor, items_size: List[int], prefix_length: List[int], extend_length: List[int], items_offset_list: List[List[Tuple[int, int]]]) -> Tuple[torch.Tensor, torch.Tensor]
embed_mm_inputs(mm_inputs_list: List[MultimodalInputs], extend_prefix_lens: List[int], extend_seq_lens: List[int], input_ids: torch.Tensor, input_embedding: nn.Embedding, multimodal_model: nn.Module, data_embedding_func_mapping: Dict[Modality, Callable[[List[MultimodalDataItem]], torch.Tensor]], placeholder_tokens: dict[Modality, List[int]]) -> Optional[torch.Tensor]
general_mm_embed_routine(input_ids: torch.Tensor, forward_batch: ForwardBatch, language_model: nn.Module, multimodal_model: Optional[nn.Module], data_embedding_funcs: Dict[Modality, Callable[[List[MultimodalDataItem]], torch.Tensor]], placeholder_tokens: Optional[dict[Modality, List[int]]]) -> torch.Tensor
get_multimodal_data_bounds(input_ids: torch.Tensor, pad_values: List[int], token_pairs: List[Tuple[int, int]]) -> torch.Tensor
data_hash(data) -> int
tensor_hash(tensor_list) -> int
hash_feature(f)

# python/sglang/srt/managers/multimodal_processor.py
import_processors()
get_mm_processor(hf_config, server_args: ServerArgs, processor, transport_mode) -> BaseMultimodalProcessor

# python/sglang/srt/managers/schedule_batch.py
  BaseFinishReason.__init__(is_error: bool)
  BaseFinishReason.to_json()
  FINISH_MATCHED_TOKEN.__init__(matched: Union[int, List[int]])
  FINISH_MATCHED_TOKEN.to_json()
  FINISH_MATCHED_STR.__init__(matched: str)
  FINISH_MATCHED_STR.to_json()
  FINISH_LENGTH.__init__(length: int)
  FINISH_LENGTH.to_json()
  FINISH_ABORT.__init__(message, status_code, err_type)
  FINISH_ABORT.to_json()
  Modality.from_str(modality_str: str)
  Modality.all()
  MultimodalDataItem.__getattr__(name: str)
  MultimodalDataItem.__setitem__(key: str, value: Any)
  MultimodalDataItem.set(key: str, value: Any)
  MultimodalDataItem.is_empty_list(l)
  MultimodalDataItem.set_pad_value()
  MultimodalDataItem.is_modality(modality: Modality) -> bool
  MultimodalDataItem.is_audio()
  MultimodalDataItem.is_image()
  MultimodalDataItem.is_video()
  MultimodalDataItem.is_valid() -> bool
  MultimodalDataItem.validate()
  MultimodalDataItem.from_dict(obj: dict)
  MultimodalDataItem.merge(other)
  MultimodalInputs.from_dict(obj: dict)
  MultimodalInputs.contains_image_inputs() -> bool
  MultimodalInputs.contains_video_inputs() -> bool
  MultimodalInputs.contains_audio_inputs() -> bool
  MultimodalInputs.contains_mm_input() -> bool
  MultimodalInputs.merge(other: MultimodalInputs)
  Req.__init__(rid: str, origin_input_text: str, origin_input_ids: List[int], sampling_params: SamplingParams, return_logprob: bool, top_logprobs_num: int, token_ids_logprob: List[int], stream: bool, origin_input_ids_unpadded: Optional[Tuple[int]], lora_id: Optional[str], input_embeds: Optional[List[List[float]]], token_type_ids: List[int], session_id: Optional[str], custom_logit_processor: Optional[str], return_hidden_states: bool, eos_token_ids: Optional[Set[int]], bootstrap_host: Optional[str], bootstrap_port: Optional[int], bootstrap_room: Optional[int], data_parallel_rank: Optional[int], vocab_size: Optional[int])
  Req.seqlen()
  Req.extend_image_inputs(image_inputs)
  Req.finished() -> bool
  Req.init_next_round_input(tree_cache: Optional[BasePrefixCache])
  Req.adjust_max_prefix_ids()
  Req.init_incremental_detokenize()
  Req.check_finished()
  Req.reset_for_retract()
  Req.offload_kv_cache(req_to_token_pool, token_to_kv_pool_allocator)
  Req.load_kv_cache(req_to_token_pool, token_to_kv_pool_allocator)
  Req.log_time_stats()
  Req.set_finish_with_abort(error_msg: str)
  Req.__repr__()
  ScheduleBatch.init_new(cls, reqs: List[Req], req_to_token_pool: ReqToTokenPool, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, tree_cache: BasePrefixCache, model_config: ModelConfig, enable_overlap: bool, spec_algorithm: SpeculativeAlgorithm, chunked_req: Optional[Req])
  ScheduleBatch.batch_size()
  ScheduleBatch.is_empty()
  ScheduleBatch.alloc_req_slots(num_reqs: int)
  ScheduleBatch.alloc_token_slots(num_tokens: int, backup_state: bool)
  ScheduleBatch.alloc_paged_token_slots_extend(prefix_lens: torch.Tensor, seq_lens: torch.Tensor, last_loc: torch.Tensor, extend_num_tokens: int, backup_state: bool)
  ScheduleBatch.alloc_paged_token_slots_decode(seq_lens: torch.Tensor, last_loc: torch.Tensor, backup_state: bool)
  ScheduleBatch.prepare_encoder_info_extend(input_ids: List[int], seq_lens: List[int])
  ScheduleBatch.prepare_for_extend()
  ScheduleBatch.prepare_for_split_prefill()
  ScheduleBatch.mix_with_running(running_batch: 'ScheduleBatch')
  ScheduleBatch.new_page_count_next_decode()
  ScheduleBatch.check_decode_mem(buf_multiplier)
  ScheduleBatch.retract_decode(server_args: ServerArgs)
  ScheduleBatch.prepare_encoder_info_decode()
  ScheduleBatch.prepare_for_idle()
  ScheduleBatch.prepare_for_decode()
  ScheduleBatch.filter_batch(chunked_req_to_exclude: Optional[Union[Req, List[Req]]], keep_indices: Optional[List[int]])
  ScheduleBatch.merge_batch(other: 'ScheduleBatch')
  ScheduleBatch.get_model_worker_batch(seq_lens_cpu_cache: Optional[torch.Tensor]) -> ModelWorkerBatch
  ScheduleBatch.copy()
  ScheduleBatch.__str__()
write_req_to_token_pool_triton(req_to_token_ptr, req_pool_indices, pre_lens, seq_lens, extend_lens, out_cache_loc, req_to_token_ptr_stride: tl.constexpr)
get_last_loc(req_to_token: torch.Tensor, req_pool_indices_tensor: torch.Tensor, prefix_lens_tensor: torch.Tensor) -> torch.Tensor
get_last_loc_torch(req_to_token: torch.Tensor, req_pool_indices_tensor: torch.Tensor, prefix_lens_tensor: torch.Tensor) -> torch.Tensor
get_last_loc_kernel(req_to_token, req_pool_indices_tensor, prefix_lens_tensor, result, num_tokens, req_to_token_stride, BLOCK_SIZE: tl.constexpr)
get_last_loc_triton(req_to_token: torch.Tensor, req_pool_indices_tensor: torch.Tensor, prefix_lens_tensor: torch.Tensor) -> torch.Tensor

# python/sglang/srt/managers/schedule_policy.py
  SchedulePolicy.__init__(policy: str, tree_cache: BasePrefixCache, enable_hierarchical_cache: bool)
  SchedulePolicy.calc_priority(waiting_queue: List[Req]) -> bool
  PrefillAdder.__init__(page_size: int, tree_cache: BasePrefixCache, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, running_batch: ScheduleBatch, new_token_ratio: float, rem_input_tokens: int, rem_chunk_tokens: Optional[int], mixed_with_decode_tokens: int)
  PrefillAdder.rem_total_tokens()
  PrefillAdder.cur_rem_tokens()
  PrefillAdder.ceil_paged_tokens(tokens: int) -> int
  PrefillAdder.budget_state()
  PrefillAdder.add_chunked_req(req: Req)
  PrefillAdder.add_one_req_ignore_eos(req: Req, has_chunked_req: bool)
  PrefillAdder.add_one_req(req: Req, has_chunked_req: bool)

# python/sglang/srt/managers/scheduler.py
  Scheduler.__init__(server_args: ServerArgs, port_args: PortArgs, gpu_id: int, tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int], dp_balance_meta: Optional[DPBalanceMeta])
  Scheduler.init_tokenizer()
  Scheduler.init_memory_pool_and_cache()
  Scheduler.init_disaggregation()
  Scheduler.init_moe_config()
  Scheduler.event_loop_normal()
  Scheduler.event_loop_overlap()
  Scheduler.event_loop_pp()
  Scheduler.recv_requests() -> List[Req]
  Scheduler.process_input_requests(recv_reqs: List)
  Scheduler.handle_generate_request(recv_req: TokenizedGenerateReqInput)
  Scheduler.handle_batch_generate_request(recv_req: BatchTokenizedGenerateReqInput)
  Scheduler.handle_embedding_request(recv_req: TokenizedEmbeddingReqInput)
  Scheduler.handle_batch_embedding_request(recv_req: BatchTokenizedEmbeddingReqInput)
  Scheduler.self_check_during_idle()
  Scheduler.check_memory()
  Scheduler.check_tree_cache()
  Scheduler.get_next_batch_to_run() -> Optional[ScheduleBatch]
  Scheduler.get_num_allocatable_reqs(running_bs)
  Scheduler.get_new_batch_prefill() -> Optional[ScheduleBatch]
  Scheduler.update_running_batch(batch: ScheduleBatch) -> Optional[ScheduleBatch]
  Scheduler.run_batch(batch: ScheduleBatch) -> Union[GenerationBatchResult, EmbeddingBatchResult]
  Scheduler.process_batch_result(batch: ScheduleBatch, result: Union[GenerationBatchResult, EmbeddingBatchResult], launch_done: Optional[threading.Event])
  Scheduler.maybe_send_health_check_signal()
  Scheduler.prepare_mlp_sync_batch(local_batch: ScheduleBatch)
  Scheduler.handle_dp_balance_data(local_batch: ScheduleBatch)
  Scheduler.prepare_mlp_sync_batch_raw(local_batch: ScheduleBatch, dp_size, attn_tp_size: int, tp_group, get_idle_batch, disable_cuda_graph: bool, spec_algorithm, speculative_num_draft_tokens, require_mlp_tp_gather: bool, disable_overlap_schedule: bool)
  Scheduler.get_idle_batch()
  Scheduler.move_ready_grammar_requests()
  Scheduler.set_next_batch_sampling_info_done(batch: ScheduleBatch)
  Scheduler.watchdog_thread()
  Scheduler.flush_cache_wrapped(recv_req: FlushCacheReqInput)
  Scheduler.clear_hicache_storage_wrapped(recv_req: ClearHiCacheReqInput)
  Scheduler.flush_cache()
  Scheduler.get_load()
  Scheduler.get_internal_state(recv_req: GetInternalStateReq)
  Scheduler.set_internal_state(recv_req: SetInternalStateReq)
  Scheduler.handle_rpc_request(recv_req: RpcReqInput)
  Scheduler.abort_request(recv_req: AbortReq)
  Scheduler.load_lora_adapter(recv_req: LoadLoRAAdapterReqInput) -> LoadLoRAAdapterReqOutput
  Scheduler.unload_lora_adapter(recv_req: UnloadLoRAAdapterReqInput) -> UnloadLoRAAdapterReqOutput
  Scheduler.slow_down(recv_req: SlowDownReqInput)
  Scheduler.expert_distribution_handle(recv_req: ExpertDistributionReq)
  Scheduler.open_session(recv_req: OpenSessionReqInput)
  Scheduler.close_session(recv_req: CloseSessionReqInput)
  Scheduler.get_print_prefix()
  Scheduler.current_scheduler_metrics_enabled()
  Scheduler.maybe_sleep_on_idle()
  Scheduler.handle_freeze_gc(recv_req: FreezeGCReq)
  IdleSleeper.__init__(sockets)
  IdleSleeper.maybe_sleep()
is_health_check_generate_req(recv_req)
is_work_request(recv_req)
run_scheduler_process(server_args: ServerArgs, port_args: PortArgs, gpu_id: int, tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int], pipe_writer, balance_meta: Optional[DPBalanceMeta])

# python/sglang/srt/managers/scheduler_input_blocker.py
  SchedulerInputBlocker.__init__(noop: bool)
  SchedulerInputBlocker.handle(recv_reqs: Optional[List[Any]])
input_blocker_guard_region(send_to_scheduler)

# python/sglang/srt/managers/scheduler_metrics_mixin.py
  KvMetrics.__init__()
  SchedulerMetricsMixin.init_metrics(tp_rank: int, pp_rank: int, dp_rank: Optional[int])
  SchedulerMetricsMixin.init_kv_events(kv_events_config: Optional[str])
  SchedulerMetricsMixin.log_prefill_stats(adder: PrefillAdder, can_run_list: List[Req], running_bs: int)
  SchedulerMetricsMixin.log_decode_stats(can_run_cuda_graph: bool, running_batch: ScheduleBatch)

# python/sglang/srt/managers/scheduler_output_processor_mixin.py
  SchedulerOutputProcessorMixin.process_batch_result_prefill(self: Scheduler, batch: ScheduleBatch, result: Union[GenerationBatchResult, EmbeddingBatchResult], launch_done: Optional[threading.Event])
  SchedulerOutputProcessorMixin.process_batch_result_decode(self: Scheduler, batch: ScheduleBatch, result: GenerationBatchResult, launch_done: Optional[threading.Event])
  SchedulerOutputProcessorMixin.add_input_logprob_return_values(self: Scheduler, i: int, req: Req, output: LogitsProcessorOutput, logprob_pt: int, num_input_logprobs: int, last_prefill_chunk: bool)
  SchedulerOutputProcessorMixin.add_logprob_return_values(self: Scheduler, i: int, req: Req, pt: int, next_token_ids: List[int], num_input_logprobs: int, output: LogitsProcessorOutput)
  SchedulerOutputProcessorMixin.stream_output(self: Scheduler, reqs: List[Req], return_logprob: bool, skip_req: Optional[Req])
  SchedulerOutputProcessorMixin.stream_output_generation(self: Scheduler, reqs: List[Req], return_logprob: bool, skip_req: Optional[Req])
  SchedulerOutputProcessorMixin.stream_output_embedding(self: Scheduler, reqs: List[Req])

# python/sglang/srt/managers/scheduler_profiler_mixin.py
  SchedulerProfilerMixin.init_profier()
  SchedulerProfilerMixin.init_profile(output_dir: Optional[str], start_step: Optional[int], num_steps: Optional[int], activities: Optional[List[str]], with_stack: Optional[bool], record_shapes: Optional[bool], profile_by_stage: bool, profile_id: str) -> ProfileReqOutput
  SchedulerProfilerMixin.start_profile(stage: Optional[ForwardMode]) -> ProfileReqOutput | None
  SchedulerProfilerMixin.stop_profile(stage: Optional[ForwardMode]) -> ProfileReqOutput | None
  SchedulerProfilerMixin.profile(recv_req: ProfileReq)

# python/sglang/srt/managers/scheduler_recv_skipper.py
  SchedulerRecvSkipper.maybe_create(server_args: ServerArgs)
  SchedulerRecvSkipper.__init__(server_args: ServerArgs)
  SchedulerRecvSkipper.handle(last_forward_mode: ForwardMode)

# python/sglang/srt/managers/scheduler_update_weights_mixin.py
  SchedulerUpdateWeightsMixin.update_weights_from_disk(recv_req: UpdateWeightFromDiskReqInput)
  SchedulerUpdateWeightsMixin.init_weights_update_group(recv_req: InitWeightsUpdateGroupReqInput)
  SchedulerUpdateWeightsMixin.update_weights_from_distributed(recv_req: UpdateWeightsFromDistributedReqInput) -> Tuple[bool, str]
  SchedulerUpdateWeightsMixin.update_weights_from_tensor(recv_req: UpdateWeightsFromTensorReqInput)
  SchedulerUpdateWeightsMixin.get_weights_by_name(recv_req: GetWeightsByNameReqInput)
  SchedulerUpdateWeightsMixin.release_memory_occupation(recv_req: ReleaseMemoryOccupationReqInput)
  SchedulerUpdateWeightsMixin.resume_memory_occupation(recv_req: ResumeMemoryOccupationReqInput)
  SchedulerUpdateWeightsMixin.save_remote_model(params)
  SchedulerUpdateWeightsMixin.save_sharded_model(params)

# python/sglang/srt/managers/session_controller.py
  SessionReqNode.__init__(req, parent, childs)
  SessionReqNode.clear_childs(req_dict)
  SessionReqNode.clear(req_dict)
  SessionReqNode.abort()
  SessionReqNode.__str__()
  Session.__init__(capacity_of_str_len: int, session_id: Optional[str])
  Session.create_req(req: TokenizedGenerateReqInput, tokenizer)

# python/sglang/srt/managers/template_manager.py
  TemplateManager.__init__()
  TemplateManager.chat_template_name() -> Optional[str]
  TemplateManager.completion_template_name() -> Optional[str]
  TemplateManager.jinja_template_content_format() -> Optional[str]
  TemplateManager.force_reasoning() -> bool
  TemplateManager.load_chat_template(tokenizer_manager, chat_template_arg: Optional[str], model_path: str) -> None
  TemplateManager.guess_chat_template_from_model_path(model_path: str) -> None
  TemplateManager.load_completion_template(completion_template_arg: str) -> None
  TemplateManager.initialize_templates(tokenizer_manager, model_path: str, chat_template: Optional[str], completion_template: Optional[str]) -> None

# python/sglang/srt/managers/tokenizer_manager.py
  TokenizerManager.__init__(server_args: ServerArgs, port_args: PortArgs)
  TokenizerManager.generate_request(obj: Union[GenerateReqInput, EmbeddingReqInput], request: Optional[fastapi.Request])
  TokenizerManager.flush_cache() -> FlushCacheReqOutput
  TokenizerManager.clear_hicache_storage() -> ClearHiCacheReqOutput
  TokenizerManager.abort_request(rid: str, abort_all: bool)
  TokenizerManager.start_profile(output_dir: Optional[str], start_step: Optional[int], num_steps: Optional[int], activities: Optional[List[str]], with_stack: Optional[bool], record_shapes: Optional[bool], profile_by_stage: bool)
  TokenizerManager.stop_profile()
  TokenizerManager.start_expert_distribution_record()
  TokenizerManager.stop_expert_distribution_record()
  TokenizerManager.dump_expert_distribution_record()
  TokenizerManager.pause_generation()
  TokenizerManager.continue_generation()
  TokenizerManager.update_weights_from_disk(obj: UpdateWeightFromDiskReqInput, request: Optional[fastapi.Request]) -> Tuple[bool, str]
  TokenizerManager.init_weights_update_group(obj: InitWeightsUpdateGroupReqInput, request: Optional[fastapi.Request]) -> Tuple[bool, str]
  TokenizerManager.update_weights_from_distributed(obj: UpdateWeightsFromDistributedReqInput, request: Optional[fastapi.Request]) -> Tuple[bool, str]
  TokenizerManager.update_weights_from_tensor(obj: UpdateWeightsFromTensorReqInput, request: Optional[fastapi.Request]) -> Tuple[bool, str]
  TokenizerManager.load_lora_adapter(obj: LoadLoRAAdapterReqInput, _: Optional[fastapi.Request]) -> LoadLoRAAdapterReqOutput
  TokenizerManager.unload_lora_adapter(obj: UnloadLoRAAdapterReqInput, _: Optional[fastapi.Request]) -> UnloadLoRAAdapterReqOutput
  TokenizerManager.get_weights_by_name(obj: GetWeightsByNameReqInput, request: Optional[fastapi.Request])
  TokenizerManager.release_memory_occupation(obj: ReleaseMemoryOccupationReqInput, request: Optional[fastapi.Request])
  TokenizerManager.resume_memory_occupation(obj: ResumeMemoryOccupationReqInput, request: Optional[fastapi.Request])
  TokenizerManager.slow_down(obj: SlowDownReqInput, request: Optional[fastapi.Request])
  TokenizerManager.open_session(obj: OpenSessionReqInput, request: Optional[fastapi.Request])
  TokenizerManager.close_session(obj: CloseSessionReqInput, request: Optional[fastapi.Request])
  TokenizerManager.get_internal_state() -> List[Dict[Any, Any]]
  TokenizerManager.set_internal_state(obj: SetInternalStateReq) -> List[bool]
  TokenizerManager.get_load() -> dict
  TokenizerManager.get_log_request_metadata()
  TokenizerManager.configure_logging(obj: ConfigureLoggingReq)
  TokenizerManager.freeze_gc()
  TokenizerManager.create_abort_task(obj: GenerateReqInput)
  TokenizerManager.auto_create_handle_loop()
  TokenizerManager.dump_requests_before_crash()
  TokenizerManager.sigterm_watchdog()
  TokenizerManager.handle_loop()
  TokenizerManager.convert_logprob_style(meta_info: dict, state: ReqState, top_logprobs_num: int, token_ids_logprob: List[int], return_text_in_logprobs: bool, recv_obj: BatchStrOut, recv_obj_index: int)
  TokenizerManager.detokenize_logprob_tokens(token_logprobs_val: List[float], token_logprobs_idx: List[int], decode_to_text: bool)
  TokenizerManager.detokenize_top_logprobs_tokens(token_logprobs_val: List[float], token_logprobs_idx: List[int], decode_to_text: bool)
  TokenizerManager.collect_metrics(state: ReqState, recv_obj: BatchStrOut, i: int)
  TokenizerManager.dump_requests(state: ReqState, out_dict: dict)
  TokenizerManager.record_request_for_crash_dump(state: ReqState, out_dict: dict)
  TokenizerManager.score_request(query: Optional[Union[str, List[int]]], items: Optional[Union[str, List[str], List[List[int]]]], label_token_ids: Optional[List[int]], apply_softmax: bool, item_first: bool, request: Optional[Any]) -> List[List[float]]
print_exception_wrapper(func)
  SignalHandler.__init__(tokenizer_manager: TokenizerManager)
  SignalHandler.sigterm_handler(signum, frame)
  SignalHandler.running_phase_sigquit_handler(signum, frame)
  _Communicator.__init__(sender, fan_out: int)
  _Communicator.__call__(obj)
  _Communicator.handle_recv(recv_obj: T)

# python/sglang/srt/managers/tp_worker.py
  TpModelWorker.__init__(server_args: ServerArgs, gpu_id: int, tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int], nccl_port: int, is_draft_worker: bool, req_to_token_pool: Optional[ReqToTokenPool], token_to_kv_pool_allocator: Optional[BaseTokenToKVPoolAllocator])
  TpModelWorker.register_hicache_layer_transfer_counter(counter)
  TpModelWorker.set_hicache_consumer(consumer_index)
  TpModelWorker.get_worker_info()
  TpModelWorker.sliding_window_size() -> Optional[int]
  TpModelWorker.is_hybrid() -> bool
  TpModelWorker.get_tokens_per_layer_info()
  TpModelWorker.get_pad_input_ids_func()
  TpModelWorker.get_tp_group()
  TpModelWorker.get_attention_tp_group()
  TpModelWorker.get_attention_tp_cpu_group()
  TpModelWorker.get_memory_pool()
  TpModelWorker.forward_batch_generation(model_worker_batch: ModelWorkerBatch, launch_done: Optional[threading.Event], skip_sample: bool) -> Tuple[Union[LogitsProcessorOutput, torch.Tensor], Optional[torch.Tensor], bool]
  TpModelWorker.forward_batch_embedding(model_worker_batch: ModelWorkerBatch)
  TpModelWorker.update_weights_from_disk(recv_req: UpdateWeightFromDiskReqInput)
  TpModelWorker.init_weights_update_group(recv_req: InitWeightsUpdateGroupReqInput)
  TpModelWorker.update_weights_from_distributed(recv_req: UpdateWeightsFromDistributedReqInput)
  TpModelWorker.update_weights_from_tensor(recv_req: UpdateWeightsFromTensorReqInput)
  TpModelWorker.get_weights_by_name(recv_req: GetWeightsByNameReqInput)
  TpModelWorker.load_lora_adapter(recv_req: LoadLoRAAdapterReqInput)
  TpModelWorker.unload_lora_adapter(recv_req: UnloadLoRAAdapterReqInput)
  TpModelWorker.can_run_lora_batch(lora_ids: list[str]) -> bool

# python/sglang/srt/managers/tp_worker_overlap_thread.py
resolve_future_token_ids(input_ids, future_token_ids_map)
  TpModelWorkerClient.__init__(server_args: ServerArgs, gpu_id: int, tp_rank: int, moe_ep_rank: int, pp_rank: int, dp_rank: Optional[int], nccl_port: int)
  TpModelWorkerClient.register_hicache_layer_transfer_counter(counter)
  TpModelWorkerClient.set_hicache_consumer(consumer_index)
  TpModelWorkerClient.get_worker_info()
  TpModelWorkerClient.get_tokens_per_layer_info()
  TpModelWorkerClient.sliding_window_size() -> Optional[int]
  TpModelWorkerClient.is_hybrid() -> bool
  TpModelWorkerClient.get_pad_input_ids_func()
  TpModelWorkerClient.get_tp_group()
  TpModelWorkerClient.get_attention_tp_group()
  TpModelWorkerClient.get_attention_tp_cpu_group()
  TpModelWorkerClient.get_memory_pool()
  TpModelWorkerClient.get_kv_cache()
  TpModelWorkerClient.forward_thread_func()
  TpModelWorkerClient.forward_thread_func_()
  TpModelWorkerClient.resolve_last_batch_result(launch_done: Optional[threading.Event])
  TpModelWorkerClient.forward_batch_generation(model_worker_batch: ModelWorkerBatch) -> Tuple[None, torch.Tensor, bool]
  TpModelWorkerClient.update_weights_from_disk(recv_req: UpdateWeightFromDiskReqInput)
  TpModelWorkerClient.init_weights_update_group(recv_req: InitWeightsUpdateGroupReqInput)
  TpModelWorkerClient.update_weights_from_distributed(recv_req: UpdateWeightsFromDistributedReqInput)
  TpModelWorkerClient.update_weights_from_tensor(recv_req: UpdateWeightsFromTensorReqInput)
  TpModelWorkerClient.get_weights_by_name(recv_req: GetWeightsByNameReqInput)
  TpModelWorkerClient.load_lora_adapter(recv_req: LoadLoRAAdapterReqInput)
  TpModelWorkerClient.unload_lora_adapter(recv_req: UnloadLoRAAdapterReqInput)
  TpModelWorkerClient.can_run_lora_batch(lora_ids: list[str]) -> bool
  TpModelWorkerClient.__delete__()

# python/sglang/srt/managers/utils.py
validate_input_length(req: Req, max_req_input_len: int, allow_auto_truncate: bool) -> Optional[str]
get_logprob_dict_from_result(result: GenerationBatchResult) -> dict
get_logprob_from_pp_outputs(next_pp_outputs: PPProxyTensors) -> tuple[LogitsProcessorOutput, list[int], list[int]]
  DPBalanceMeta.__init__(num_workers: int)
  DPBalanceMeta.destructor()
  DPBalanceMeta.get_shared_onfly() -> List[Dict[int, int]]
  DPBalanceMeta.set_shared_onfly_info(data: List[Dict[int, int]])
  DPBalanceMeta.get_shared_local_tokens() -> List[int]
  DPBalanceMeta.set_shared_local_tokens(data: List[int])
  DPBalanceMeta.__getstate__()
  DPBalanceMeta.__setstate__(state)
