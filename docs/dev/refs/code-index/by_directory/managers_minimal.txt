
# python/sglang/srt/managers/cache_controller.py
  LayerDoneCounter.__init__(num_layers)
  LayerDoneCounter.next_producer()
  LayerDoneCounter.update_producer()
  LayerDoneCounter.set_consumer(index)
  LayerDoneCounter.increment()
  LayerDoneCounter.wait_until(threshold)
  LayerDoneCounter.reset()
  CacheOperation.__init__(host_indices, device_indices, node_id, priority)
  CacheOperation.merge(other)
  CacheOperation.split(factor)
  CacheOperation.__lt__(other)
  TransferBuffer.__init__(stop_event, buffer_count, max_buffer_size)
  TransferBuffer.full()
  TransferBuffer.empty()
  TransferBuffer.put(item, block, timeout)
  TransferBuffer.get(block, timeout)
  TransferBuffer.clear()
  StorageOperation.__init__(host_indices, token_ids, last_hash, hash_value)
  StorageOperation.__lt__(other)
  PrefetchOperation.__init__(request_id, host_indices, token_ids, last_hash)
  PrefetchOperation.increment(num_tokens)
  PrefetchOperation.mark_done()
  PrefetchOperation.is_done()
  HiCacheController.__init__(token_to_kv_pool_allocator, mem_pool_host, page_size, tp_group, load_cache_event, write_policy, io_backend, storage_backend, prefetch_threshold, model_name, storage_backend_extra_config)
  HiCacheController.reset()
  HiCacheController.write(device_indices, priority, node_id)
  HiCacheController.load(host_indices, priority, node_id)
  HiCacheController.move_indices(host_indices, device_indices)
  HiCacheController.write_thread_func_direct()
  HiCacheController.load_thread_func_layer_by_layer()
  HiCacheController.evict_device(device_indices, host_indices)
  HiCacheController.evict_host(host_indices, backup_only)
  HiCacheController.prefetch(request_id, host_indices, new_input_tokens, last_hash)
  HiCacheController.terminate_prefetch(operation)
  HiCacheController.is_mooncake_backend()
  HiCacheController.prefetch_io_aux_func()
  HiCacheController.prefetch_rate_limit_check()
  HiCacheController.prefetch_thread_func()
  HiCacheController.write_storage(host_indices, token_ids, hash_value)
  HiCacheController.backup_thread_func()

# python/sglang/srt/managers/data_parallel_controller.py
  LoadBalanceMethod.from_str(cls, method)
  DataParallelController.__init__(server_args, port_args, dp_balance_meta)
  DataParallelController.launch_dp_schedulers(server_args, port_args)
  DataParallelController.launch_tensor_parallel_group_thread(server_args, port_args, base_gpu_id, dp_rank, ready_event)
  DataParallelController.launch_dp_attention_schedulers(server_args, port_args)
  DataParallelController.launch_tensor_parallel_group(server_args, port_args, base_gpu_id, dp_rank)
  DataParallelController.round_robin_scheduler(req)
  DataParallelController.shortest_queue_scheduler(input_requests)
  DataParallelController.minimum_tokens_scheduler(req)
  DataParallelController.event_loop()
run_data_parallel_controller_process(server_args, port_args, pipe_writer)

# python/sglang/srt/managers/detokenizer_manager.py
  DetokenizerManager.__init__(server_args, port_args)
  DetokenizerManager.event_loop()
  DetokenizerManager.trim_matched_stop(output, List[int]], finished_reason, no_stop_trim)
  DetokenizerManager.handle_batch_embedding_out(recv_obj)
  DetokenizerManager.handle_batch_token_id_out(recv_obj)
  DetokenizerManager.handle_multimodal_decode_req(recv_obj)
  DetokenizerManager.handle_freeze_gc_req(recv_req)
  LimitedCapacityDict.__init__(capacity)
  LimitedCapacityDict.__setitem__(key, value)
run_detokenizer_process(server_args, port_args)

# python/sglang/srt/managers/io_struct.py
  GenerateReqInput.contains_mm_input()
  GenerateReqInput.normalize_batch_and_arguments()
  GenerateReqInput.regenerate_rid()
  GenerateReqInput.__getitem__(i)
  BatchTokenizedGenerateReqInput.__len__()
  BatchTokenizedGenerateReqInput.__getitem__(i)
  BatchTokenizedGenerateReqInput.__iter__()
  EmbeddingReqInput.normalize_batch_and_arguments()
  EmbeddingReqInput.regenerate_rid()
  EmbeddingReqInput.contains_mm_input()
  EmbeddingReqInput.__getitem__(i)
  BatchTokenizedEmbeddingReqInput.__len__()
  BatchTokenizedEmbeddingReqInput.__getitem__(i)
  BatchTokenizedEmbeddingReqInput.__iter__()
  LoadLoRAAdapterReqInput.to_ref()
  UnloadLoRAAdapterReqInput.to_ref()

# python/sglang/srt/managers/mm_utils.py
  TransportProxyTensor.__new__(cls, data, name, fields, Any]], transport_mode)
  TransportProxyTensor.__getstate__()
  TransportProxyTensor.__setstate__(state, Any])
  TransportProxyTensor.name()
  TransportProxyTensor.fields()
  TransportProxyTensor.transport_mode()
  MultiModalityDataPaddingPattern.pad_input_tokens(input_ids, mm_inputs)
  MultiModalityDataPaddingPatternTokenPairs.__init__(data_token_pairs, int]]], data_start_token_ids)
  MultiModalityDataPaddingPatternTokenPairs.pad_input_tokens(input_ids, mm_inputs)
  MultiModalityDataPaddingPatternMultimodalTokens.pad_input_tokens(input_ids, mm_inputs)
init_embedding_cache(max_size)
get_embedding_hash(embedding_items)
get_embedding_chunk(embedding, extend_prefix_len, extend_seq_len, items_offset, int]])
get_embedding_and_mask(data_embedding_func, torch.Tensor], embedding_items, placeholder_tensor, input_ids, items_size, prefix_length, extend_length, items_offset_list, int]]])
embed_mm_inputs(mm_inputs_list, extend_prefix_lens, extend_seq_lens, input_ids, input_embedding, multimodal_model, data_embedding_func_mapping, Callable[[List[MultimodalDataItem]], torch.Tensor]], placeholder_tokens, List[int]])
general_mm_embed_routine(input_ids, forward_batch, language_model, multimodal_model, data_embedding_funcs, Callable[[List[MultimodalDataItem]], torch.Tensor]], placeholder_tokens, List[int]]])
get_multimodal_data_bounds(input_ids, pad_values, token_pairs, int]])
data_hash(data)
tensor_hash(tensor_list)
hash_feature(f)

# python/sglang/srt/managers/multimodal_processor.py
import_processors()
get_mm_processor(hf_config, server_args, processor, transport_mode)

# python/sglang/srt/managers/schedule_batch.py
  BaseFinishReason.__init__(is_error)
  BaseFinishReason.to_json()
  FINISH_MATCHED_TOKEN.__init__(matched, List[int]])
  FINISH_MATCHED_TOKEN.to_json()
  FINISH_MATCHED_STR.__init__(matched)
  FINISH_MATCHED_STR.to_json()
  FINISH_LENGTH.__init__(length)
  FINISH_LENGTH.to_json()
  FINISH_ABORT.__init__(message, status_code, err_type)
  FINISH_ABORT.to_json()
  Modality.from_str(modality_str)
  Modality.all()
  MultimodalDataItem.__getattr__(name)
  MultimodalDataItem.__setitem__(key, value)
  MultimodalDataItem.set(key, value)
  MultimodalDataItem.is_empty_list(l)
  MultimodalDataItem.set_pad_value()
  MultimodalDataItem.is_modality(modality)
  MultimodalDataItem.is_audio()
  MultimodalDataItem.is_image()
  MultimodalDataItem.is_video()
  MultimodalDataItem.is_valid()
  MultimodalDataItem.validate()
  MultimodalDataItem.from_dict(obj)
  MultimodalDataItem.merge(other)
  MultimodalInputs.from_dict(obj)
  MultimodalInputs.contains_image_inputs()
  MultimodalInputs.contains_video_inputs()
  MultimodalInputs.contains_audio_inputs()
  MultimodalInputs.contains_mm_input()
  MultimodalInputs.merge(other)
  Req.__init__(rid, origin_input_text, origin_input_ids, sampling_params, return_logprob, top_logprobs_num, token_ids_logprob, stream, origin_input_ids_unpadded, lora_id, input_embeds, token_type_ids, session_id, custom_logit_processor, return_hidden_states, eos_token_ids, bootstrap_host, bootstrap_port, bootstrap_room, data_parallel_rank, vocab_size)
  Req.seqlen()
  Req.extend_image_inputs(image_inputs)
  Req.finished()
  Req.init_next_round_input(tree_cache)
  Req.adjust_max_prefix_ids()
  Req.init_incremental_detokenize()
  Req.check_finished()
  Req.reset_for_retract()
  Req.offload_kv_cache(req_to_token_pool, token_to_kv_pool_allocator)
  Req.load_kv_cache(req_to_token_pool, token_to_kv_pool_allocator)
  Req.log_time_stats()
  Req.set_finish_with_abort(error_msg)
  Req.__repr__()
  ScheduleBatch.init_new(cls, reqs, req_to_token_pool, token_to_kv_pool_allocator, tree_cache, model_config, enable_overlap, spec_algorithm, chunked_req)
  ScheduleBatch.batch_size()
  ScheduleBatch.is_empty()
  ScheduleBatch.alloc_req_slots(num_reqs)
  ScheduleBatch.alloc_token_slots(num_tokens, backup_state)
  ScheduleBatch.alloc_paged_token_slots_extend(prefix_lens, seq_lens, last_loc, extend_num_tokens, backup_state)
  ScheduleBatch.alloc_paged_token_slots_decode(seq_lens, last_loc, backup_state)
  ScheduleBatch.prepare_encoder_info_extend(input_ids, seq_lens)
  ScheduleBatch.prepare_for_extend()
  ScheduleBatch.prepare_for_split_prefill()
  ScheduleBatch.mix_with_running(running_batch)
  ScheduleBatch.new_page_count_next_decode()
  ScheduleBatch.check_decode_mem(buf_multiplier)
  ScheduleBatch.retract_decode(server_args)
  ScheduleBatch.prepare_encoder_info_decode()
  ScheduleBatch.prepare_for_idle()
  ScheduleBatch.prepare_for_decode()
  ScheduleBatch.filter_batch(chunked_req_to_exclude, List[Req]]], keep_indices)
  ScheduleBatch.merge_batch(other)
  ScheduleBatch.get_model_worker_batch(seq_lens_cpu_cache)
  ScheduleBatch.copy()
  ScheduleBatch.__str__()
write_req_to_token_pool_triton(req_to_token_ptr, req_pool_indices, pre_lens, seq_lens, extend_lens, out_cache_loc, req_to_token_ptr_stride)
get_last_loc(req_to_token, req_pool_indices_tensor, prefix_lens_tensor)
get_last_loc_torch(req_to_token, req_pool_indices_tensor, prefix_lens_tensor)
get_last_loc_kernel(req_to_token, req_pool_indices_tensor, prefix_lens_tensor, result, num_tokens, req_to_token_stride, BLOCK_SIZE)
get_last_loc_triton(req_to_token, req_pool_indices_tensor, prefix_lens_tensor)

# python/sglang/srt/managers/schedule_policy.py
  SchedulePolicy.__init__(policy, tree_cache, enable_hierarchical_cache)
  SchedulePolicy.calc_priority(waiting_queue)
  PrefillAdder.__init__(page_size, tree_cache, token_to_kv_pool_allocator, running_batch, new_token_ratio, rem_input_tokens, rem_chunk_tokens, mixed_with_decode_tokens)
  PrefillAdder.rem_total_tokens()
  PrefillAdder.cur_rem_tokens()
  PrefillAdder.ceil_paged_tokens(tokens)
  PrefillAdder.budget_state()
  PrefillAdder.add_chunked_req(req)
  PrefillAdder.add_one_req_ignore_eos(req, has_chunked_req)
  PrefillAdder.add_one_req(req, has_chunked_req)

# python/sglang/srt/managers/scheduler.py
  Scheduler.__init__(server_args, port_args, gpu_id, tp_rank, moe_ep_rank, pp_rank, dp_rank, dp_balance_meta)
  Scheduler.init_tokenizer()
  Scheduler.init_memory_pool_and_cache()
  Scheduler.init_disaggregation()
  Scheduler.init_moe_config()
  Scheduler.event_loop_normal()
  Scheduler.event_loop_overlap()
  Scheduler.event_loop_pp()
  Scheduler.recv_requests()
  Scheduler.process_input_requests(recv_reqs)
  Scheduler.handle_generate_request(recv_req)
  Scheduler.handle_batch_generate_request(recv_req)
  Scheduler.handle_embedding_request(recv_req)
  Scheduler.handle_batch_embedding_request(recv_req)
  Scheduler.self_check_during_idle()
  Scheduler.check_memory()
  Scheduler.check_tree_cache()
  Scheduler.get_next_batch_to_run()
  Scheduler.get_num_allocatable_reqs(running_bs)
  Scheduler.get_new_batch_prefill()
  Scheduler.update_running_batch(batch)
  Scheduler.run_batch(batch)
  Scheduler.process_batch_result(batch, result, EmbeddingBatchResult], launch_done)
  Scheduler.maybe_send_health_check_signal()
  Scheduler.prepare_mlp_sync_batch(local_batch)
  Scheduler.handle_dp_balance_data(local_batch)
  Scheduler.prepare_mlp_sync_batch_raw(local_batch, dp_size, attn_tp_size, tp_group, get_idle_batch, disable_cuda_graph, spec_algorithm, speculative_num_draft_tokens, require_mlp_tp_gather, disable_overlap_schedule)
  Scheduler.get_idle_batch()
  Scheduler.move_ready_grammar_requests()
  Scheduler.set_next_batch_sampling_info_done(batch)
  Scheduler.watchdog_thread()
  Scheduler.flush_cache_wrapped(recv_req)
  Scheduler.flush_cache()
  Scheduler.get_load()
  Scheduler.get_internal_state(recv_req)
  Scheduler.set_internal_state(recv_req)
  Scheduler.handle_rpc_request(recv_req)
  Scheduler.abort_request(recv_req)
  Scheduler.load_lora_adapter(recv_req)
  Scheduler.unload_lora_adapter(recv_req)
  Scheduler.slow_down(recv_req)
  Scheduler.expert_distribution_handle(recv_req)
  Scheduler.open_session(recv_req)
  Scheduler.close_session(recv_req)
  Scheduler.get_print_prefix()
  Scheduler.current_scheduler_metrics_enabled()
  Scheduler.maybe_sleep_on_idle()
  Scheduler.handle_freeze_gc(recv_req)
  IdleSleeper.__init__(sockets)
  IdleSleeper.maybe_sleep()
is_health_check_generate_req(recv_req)
is_work_request(recv_req)
run_scheduler_process(server_args, port_args, gpu_id, tp_rank, moe_ep_rank, pp_rank, dp_rank, pipe_writer, balance_meta)

# python/sglang/srt/managers/scheduler_input_blocker.py
  SchedulerInputBlocker.__init__(noop)
  SchedulerInputBlocker.handle(recv_reqs)
input_blocker_guard_region(send_to_scheduler)

# python/sglang/srt/managers/scheduler_metrics_mixin.py
  KvMetrics.__init__()
  SchedulerMetricsMixin.init_metrics(tp_rank, pp_rank, dp_rank)
  SchedulerMetricsMixin.init_kv_events(kv_events_config)
  SchedulerMetricsMixin.log_prefill_stats(adder, can_run_list, running_bs)
  SchedulerMetricsMixin.log_decode_stats(can_run_cuda_graph, running_batch)

# python/sglang/srt/managers/scheduler_output_processor_mixin.py
  SchedulerOutputProcessorMixin.process_batch_result_prefill(batch, result, EmbeddingBatchResult], launch_done)
  SchedulerOutputProcessorMixin.process_batch_result_decode(batch, result, launch_done)
  SchedulerOutputProcessorMixin.add_input_logprob_return_values(i, req, output, logprob_pt, num_input_logprobs, last_prefill_chunk)
  SchedulerOutputProcessorMixin.add_logprob_return_values(i, req, pt, next_token_ids, num_input_logprobs, output)
  SchedulerOutputProcessorMixin.stream_output(reqs, return_logprob, skip_req)
  SchedulerOutputProcessorMixin.stream_output_generation(reqs, return_logprob, skip_req)
  SchedulerOutputProcessorMixin.stream_output_embedding(reqs)

# python/sglang/srt/managers/scheduler_profiler_mixin.py
  SchedulerProfilerMixin.init_profier()
  SchedulerProfilerMixin.init_profile(output_dir, start_step, num_steps, activities, with_stack, record_shapes, profile_by_stage, profile_id)
  SchedulerProfilerMixin.start_profile(stage)
  SchedulerProfilerMixin.stop_profile(stage)
  SchedulerProfilerMixin.profile(recv_req)

# python/sglang/srt/managers/scheduler_recv_skipper.py
  SchedulerRecvSkipper.maybe_create(server_args)
  SchedulerRecvSkipper.__init__(server_args)
  SchedulerRecvSkipper.handle(last_forward_mode)

# python/sglang/srt/managers/scheduler_update_weights_mixin.py
  SchedulerUpdateWeightsMixin.update_weights_from_disk(recv_req)
  SchedulerUpdateWeightsMixin.init_weights_update_group(recv_req)
  SchedulerUpdateWeightsMixin.update_weights_from_distributed(recv_req)
  SchedulerUpdateWeightsMixin.update_weights_from_tensor(recv_req)
  SchedulerUpdateWeightsMixin.get_weights_by_name(recv_req)
  SchedulerUpdateWeightsMixin.release_memory_occupation(recv_req)
  SchedulerUpdateWeightsMixin.resume_memory_occupation(recv_req)
  SchedulerUpdateWeightsMixin.save_remote_model(params)
  SchedulerUpdateWeightsMixin.save_sharded_model(params)

# python/sglang/srt/managers/session_controller.py
  SessionReqNode.__init__(req, parent, childs)
  SessionReqNode.clear_childs(req_dict)
  SessionReqNode.clear(req_dict)
  SessionReqNode.abort()
  SessionReqNode.__str__()
  Session.__init__(capacity_of_str_len, session_id)
  Session.create_req(req, tokenizer)

# python/sglang/srt/managers/template_manager.py
  TemplateManager.__init__()
  TemplateManager.chat_template_name()
  TemplateManager.completion_template_name()
  TemplateManager.jinja_template_content_format()
  TemplateManager.force_reasoning()
  TemplateManager.load_chat_template(tokenizer_manager, chat_template_arg, model_path)
  TemplateManager.guess_chat_template_from_model_path(model_path)
  TemplateManager.load_completion_template(completion_template_arg)
  TemplateManager.initialize_templates(tokenizer_manager, model_path, chat_template, completion_template)

# python/sglang/srt/managers/tokenizer_manager.py
  TokenizerManager.__init__(server_args, port_args)
  TokenizerManager.generate_request(obj, EmbeddingReqInput], request)
  TokenizerManager.flush_cache()
  TokenizerManager.abort_request(rid, abort_all)
  TokenizerManager.start_profile(output_dir, start_step, num_steps, activities, with_stack, record_shapes, profile_by_stage)
  TokenizerManager.stop_profile()
  TokenizerManager.start_expert_distribution_record()
  TokenizerManager.stop_expert_distribution_record()
  TokenizerManager.dump_expert_distribution_record()
  TokenizerManager.pause_generation()
  TokenizerManager.continue_generation()
  TokenizerManager.update_weights_from_disk(obj, request)
  TokenizerManager.init_weights_update_group(obj, request)
  TokenizerManager.update_weights_from_distributed(obj, request)
  TokenizerManager.update_weights_from_tensor(obj, request)
  TokenizerManager.load_lora_adapter(obj, _)
  TokenizerManager.unload_lora_adapter(obj, _)
  TokenizerManager.get_weights_by_name(obj, request)
  TokenizerManager.release_memory_occupation(obj, request)
  TokenizerManager.resume_memory_occupation(obj, request)
  TokenizerManager.slow_down(obj, request)
  TokenizerManager.open_session(obj, request)
  TokenizerManager.close_session(obj, request)
  TokenizerManager.get_internal_state()
  TokenizerManager.set_internal_state(obj)
  TokenizerManager.get_load()
  TokenizerManager.get_log_request_metadata()
  TokenizerManager.configure_logging(obj)
  TokenizerManager.freeze_gc()
  TokenizerManager.create_abort_task(obj)
  TokenizerManager.auto_create_handle_loop()
  TokenizerManager.dump_requests_before_crash()
  TokenizerManager.sigterm_watchdog()
  TokenizerManager.handle_loop()
  TokenizerManager.convert_logprob_style(meta_info, state, top_logprobs_num, token_ids_logprob, return_text_in_logprobs, recv_obj, recv_obj_index)
  TokenizerManager.detokenize_logprob_tokens(token_logprobs_val, token_logprobs_idx, decode_to_text)
  TokenizerManager.detokenize_top_logprobs_tokens(token_logprobs_val, token_logprobs_idx, decode_to_text)
  TokenizerManager.collect_metrics(state, recv_obj, i)
  TokenizerManager.dump_requests(state, out_dict)
  TokenizerManager.record_request_for_crash_dump(state, out_dict)
  TokenizerManager.score_request(query, List[int]]], items, List[str], List[List[int]]]], label_token_ids, apply_softmax, item_first, request)
print_exception_wrapper(func)
  SignalHandler.__init__(tokenizer_manager)
  SignalHandler.sigterm_handler(signum, frame)
  SignalHandler.running_phase_sigquit_handler(signum, frame)
  _Communicator.__init__(sender, fan_out)
  _Communicator.__call__(obj)
  _Communicator.handle_recv(recv_obj)

# python/sglang/srt/managers/tp_worker.py
  TpModelWorker.__init__(server_args, gpu_id, tp_rank, moe_ep_rank, pp_rank, dp_rank, nccl_port, is_draft_worker, req_to_token_pool, token_to_kv_pool_allocator)
  TpModelWorker.register_hicache_layer_transfer_counter(counter)
  TpModelWorker.set_hicache_consumer(consumer_index)
  TpModelWorker.get_worker_info()
  TpModelWorker.sliding_window_size()
  TpModelWorker.is_hybrid()
  TpModelWorker.get_tokens_per_layer_info()
  TpModelWorker.get_pad_input_ids_func()
  TpModelWorker.get_tp_group()
  TpModelWorker.get_attention_tp_group()
  TpModelWorker.get_attention_tp_cpu_group()
  TpModelWorker.get_memory_pool()
  TpModelWorker.forward_batch_generation(model_worker_batch, launch_done, skip_sample)
  TpModelWorker.forward_batch_embedding(model_worker_batch)
  TpModelWorker.update_weights_from_disk(recv_req)
  TpModelWorker.init_weights_update_group(recv_req)
  TpModelWorker.update_weights_from_distributed(recv_req)
  TpModelWorker.update_weights_from_tensor(recv_req)
  TpModelWorker.get_weights_by_name(recv_req)
  TpModelWorker.load_lora_adapter(recv_req)
  TpModelWorker.unload_lora_adapter(recv_req)
  TpModelWorker.can_run_lora_batch(lora_ids)

# python/sglang/srt/managers/tp_worker_overlap_thread.py
resolve_future_token_ids(input_ids, future_token_ids_map)
  TpModelWorkerClient.__init__(server_args, gpu_id, tp_rank, moe_ep_rank, pp_rank, dp_rank, nccl_port)
  TpModelWorkerClient.register_hicache_layer_transfer_counter(counter)
  TpModelWorkerClient.set_hicache_consumer(consumer_index)
  TpModelWorkerClient.get_worker_info()
  TpModelWorkerClient.get_tokens_per_layer_info()
  TpModelWorkerClient.sliding_window_size()
  TpModelWorkerClient.is_hybrid()
  TpModelWorkerClient.get_pad_input_ids_func()
  TpModelWorkerClient.get_tp_group()
  TpModelWorkerClient.get_attention_tp_group()
  TpModelWorkerClient.get_attention_tp_cpu_group()
  TpModelWorkerClient.get_memory_pool()
  TpModelWorkerClient.get_kv_cache()
  TpModelWorkerClient.forward_thread_func()
  TpModelWorkerClient.forward_thread_func_()
  TpModelWorkerClient.resolve_last_batch_result(launch_done)
  TpModelWorkerClient.forward_batch_generation(model_worker_batch)
  TpModelWorkerClient.update_weights_from_disk(recv_req)
  TpModelWorkerClient.init_weights_update_group(recv_req)
  TpModelWorkerClient.update_weights_from_distributed(recv_req)
  TpModelWorkerClient.update_weights_from_tensor(recv_req)
  TpModelWorkerClient.get_weights_by_name(recv_req)
  TpModelWorkerClient.load_lora_adapter(recv_req)
  TpModelWorkerClient.unload_lora_adapter(recv_req)
  TpModelWorkerClient.can_run_lora_batch(lora_ids)
  TpModelWorkerClient.__delete__()

# python/sglang/srt/managers/utils.py
validate_input_length(req, max_req_input_len, allow_auto_truncate)
get_logprob_dict_from_result(result)
get_logprob_from_pp_outputs(next_pp_outputs)
  DPBalanceMeta.__init__(num_workers)
  DPBalanceMeta.destructor()
  DPBalanceMeta.get_shared_onfly()
  DPBalanceMeta.set_shared_onfly_info(data, int]])
  DPBalanceMeta.get_shared_local_tokens()
  DPBalanceMeta.set_shared_local_tokens(data)
  DPBalanceMeta.__getstate__()
  DPBalanceMeta.__setstate__(state)