
# python/sglang/srt/_custom_ops.py
init_custom_ar(ipc_tensors: List[torch.Tensor], rank_data: torch.Tensor, rank: int, full_nvlink: bool) -> int
all_reduce(fa: int, inp: torch.Tensor, out: torch.Tensor, reg_buffer: int, reg_buffer_sz_bytes: int) -> None
dispose(fa: int) -> None
meta_size() -> int
register_buffer(fa: int, ipc_tensors: List[int]) -> None
get_graph_buffer_ipc_meta(fa: int) -> Tuple[List[int], List[int]]
register_graph_buffers(fa: int, handles: List[List[int]], offsets: List[List[int]]) -> None
init_custom_ar(meta: torch.Tensor, rank_data: torch.Tensor, handles: List[str], offsets: List[int], rank: int, full_nvlink: bool) -> int
all_reduce_reg(fa: int, inp: torch.Tensor, out: torch.Tensor) -> None
all_reduce_unreg(fa: int, inp: torch.Tensor, reg_buffer: torch.Tensor, out: torch.Tensor) -> None
dispose(fa: int) -> None
meta_size() -> int
register_buffer(fa: int, t: torch.Tensor, handles: List[str], offsets: List[int]) -> None
get_graph_buffer_ipc_meta(fa: int) -> Tuple[torch.Tensor, List[int]]
register_graph_buffers(fa: int, handles: List[str], offsets: List[List[int]]) -> None
allocate_meta_buffer(size: int) -> torch.Tensor
get_meta_buffer_ipc_handle(inp: torch.Tensor) -> torch.Tensor
init_custom_qr(rank: int, world_size: int, qr_max_size: Optional[int]) -> int
qr_get_handle(fa: int) -> torch.Tensor
qr_open_handles(fa: int, handles: list[torch.Tensor]) -> None
qr_all_reduce(fa: int, inp: torch.Tensor, out: torch.Tensor, quant_level: int, cast_bf2half: bool) -> None
qr_destroy(fa: int) -> None
qr_max_size() -> int
mscclpp_generate_unique_id() -> bytes
mscclpp_init_context(unique_id: bytes, rank: int, world_size: int, scratch: torch.Tensor, put_buffer: torch.Tensor, nranks_per_node: int, rank_to_node: List[int], rank_to_ib: List[int], context_selection: int) -> int
mscclpp_allreduce(context: int, inp: torch.Tensor, out: torch.Tensor, nthreads: int, nblocks: int) -> None

# python/sglang/srt/aio_rwlock.py
  RWLock.__init__()
  RWLock.reader_lock()
  RWLock.writer_lock()
  RWLock.acquire_reader()
  RWLock.release_reader()
  RWLock.acquire_writer()
  RWLock.release_writer()
  _ReaderLock.__init__(rwlock: RWLock)
  _ReaderLock.__aenter__()
  _ReaderLock.__aexit__(exc_type, exc_val, exc_tb)
  _WriterLock.__init__(rwlock: RWLock)
  _WriterLock.__aenter__()
  _WriterLock.__aexit__(exc_type, exc_val, exc_tb)

# python/sglang/srt/bench_utils.py
  suppress_stdout_stderr.__enter__()
  suppress_stdout_stderr.__exit__()
bench_kineto(fn, kernel_names, num_tests: int, suppress_kineto_output: bool, trace_path: str, flush_l2: bool, with_multiple_kernels: bool)

# python/sglang/srt/code_completion_parser.py
register_completion_template(template: CompletionTemplate, override: bool)
completion_template_exists(template_name: str) -> bool
is_completion_template_defined() -> bool
generate_completion_prompt_from_request(request: CompletionRequest) -> str
generate_completion_prompt(prompt: str, suffix: str, template_name: str) -> str

# python/sglang/srt/conversation.py
  Conversation.get_prompt() -> str
  Conversation.set_system_message(system_message: str)
  Conversation.append_message(role: str, message: str)
  Conversation.append_image(image: str, detail: Literal['auto', 'low', 'high'])
  Conversation.append_video(video: str)
  Conversation.append_audio(audio: str)
  Conversation.update_last_message(message: str)
  Conversation.to_gradio_chatbot()
  Conversation.to_openai_api_messages()
  Conversation.copy()
  Conversation.dict()
register_conv_template(template: Conversation, override: bool)
register_conv_template_matching_function(func)
get_conv_template_by_model_path(model_path)
chat_template_exists(template_name: str) -> bool
generate_embedding_convs(texts: List[str], images: List[str], template_name: str) -> List[Conversation]
generate_chat_conv(request: ChatCompletionRequest, template_name: str) -> Conversation
get_model_type(model_path: str) -> Optional[str]
match_internvl(model_path: str)
match_deepseek_janus_pro(model_path: str)
match_vicuna(model_path: str)
match_deepseek_vl(model_path: str)
match_qwen_chat_ml(model_path: str)
match_minicpm(model_path: str)
match_phi_4_mm(model_path: str)

# python/sglang/srt/custom_op.py
  CustomOp.__init__()
  CustomOp.enter_torch_compile(num_tokens: int)
  CustomOp.leave_torch_compile()
  CustomOp.forward()
  CustomOp.forward_native()
  CustomOp.forward_cuda()
  CustomOp.forward_npu()
  CustomOp.forward_hip()
  CustomOp.forward_xpu()
  CustomOp.forward_hpu()
  CustomOp.forward_cpu()
  CustomOp.dispatch_forward()

# python/sglang/srt/harmony_parser.py
prefix_hold(text: str, tokens: List[str]) -> Tuple[str, str]
iter_tokens(text: str, start_pos: int) -> Iterator[Token]
  CanonicalStrategy.__init__()
  CanonicalStrategy.parse(text: str) -> Tuple[List[Event], str]
  TextStrategy.__init__()
  TextStrategy.set_buffer_context(buffer: str)
  TextStrategy.parse(text: str) -> Tuple[List[Event], str]
  HarmonyParser.__init__()
  HarmonyParser.parse(chunk: str) -> List[Event]

# python/sglang/srt/hf_transformers_utils.py
download_from_hf(model_path: str, allow_patterns: Optional[Union[str, list]])
get_hf_text_config(config: PretrainedConfig)
get_config(model: str, trust_remote_code: bool, revision: Optional[str], model_override_args: Optional[dict])
get_generation_config(model: str, trust_remote_code: bool, revision: Optional[str])
get_sparse_attention_config(model: str, sparse_attention_config_filename: str) -> Dict[str, Any]
get_context_length(config)
get_tokenizer(tokenizer_name: str) -> Union[PreTrainedTokenizer, PreTrainedTokenizerFast]
get_tokenizer_from_processor(processor)
get_processor(tokenizer_name: str)
attach_additional_stop_token_ids(tokenizer)
check_gguf_file(model: Union[str, os.PathLike]) -> bool

# python/sglang/srt/host_shared_memory.py
  HostSharedMemoryManager.__init__(base_name: str)
  HostSharedMemoryManager.malloc()
get_host_shared_memory_manager()
set_host_shared_memory_manager(instance: HostSharedMemoryManager)

# python/sglang/srt/jinja_template_utils.py
detect_jinja_template_content_format(chat_template: str) -> str
process_content_for_template_format(msg_dict: dict, content_format: str, image_data: list, video_data: list, audio_data: list, modalities: list) -> dict

# python/sglang/srt/model_parallel.py
tensor_parallel(module: torch.nn.Module, device_mesh: Optional[DeviceMesh])

# python/sglang/srt/offloader.py
  BaseOffloader.wrap_modules(all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor], whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator])
  BaseOffloader.post_init()
get_offloader()
set_offloader(instance: BaseOffloader)
create_offloader_from_server_args(server_args: ServerArgs, dp_rank: int)
  OffloaderV1.__init__(cpu_offload_max_bytes: int)
  OffloaderV1.wrap_modules(all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor], whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator])
  OffloaderV1.maybe_offload_to_cpu(module: torch.nn.Module) -> torch.nn.Module
  OffloaderV2.__init__(group_size: int, num_in_group: int, prefetch_step: int, mode: str, dp_rank: int, dp_size: int)
  OffloaderV2.wrap_modules(all_modules_generator: Generator[torch.nn.Module, None, None], submodule_accessor: Optional[_SubmoduleAccessor], whitelist_param_names_creator: Optional[_WhitelistParamNamesCreator])
  OffloaderV2.post_init()
  _ModuleOffloader.__init__(mode: str, module: torch.nn.Module, alt_stream: torch.cuda.Stream, whitelist_param_names: List[str])
  _ModuleOffloader.post_init()
  _ModuleOffloader.start_onload()
  _ModuleOffloader.offload()
  _ModuleOffloader.wait_and_get_device_tensors()
  _BaseParamOffloader.create(mode: str) -> '_BaseParamOffloader'
  _BaseParamOffloader.__init__(module, param_name)
  _BaseParamOffloader.post_init()
  _BaseParamOffloader.create_device_tensor()
  _MetaParamOffloader.__init__(module, param_name)
  _MetaParamOffloader.create_device_tensor()
  _CpuParamOffloader.__init__(module, param_name)
  _CpuParamOffloader.create_device_tensor()
  _ShmCpuParamOffloader.__init__(module, param_name)
  _ShmCpuParamOffloader.post_init()
  _ShmCpuParamOffloader.create_device_tensor()
  _ShardedGpuParamOffloader.__init__(module, param_name)
  _ShardedGpuParamOffloader.post_init()
  _ShardedGpuParamOffloader.create_device_tensor()

# python/sglang/srt/operations.py
execute_operations(inputs, operations)
execute_overlapped_operations(inputs_arr: Sequence, operations_arr: Sequence, delta_stages: Sequence[int]) -> Sequence
  _StageExecutor.__init__(debug_name: str, stages: List[Stage], inputs: dict)
  _StageExecutor.next()
  _StageExecutor.output()
  _StageExecutor.done()
  _StageExecutor.num_stages()
  _StateDict.__init__()
  _StateDict.__setattr__(key, value)
  _StateDict.__getattr__(item)
  _StateDict.__delattr__(item)
  _StateDict.pop(item)
  _StateDict.update(values: Dict[str, Any])
  _StateDict.get(item)
  _StateDict.clear(expect_keys: Sequence[str])

# python/sglang/srt/operations_strategy.py
  OperationsStrategy.concat(cls, items: List['OperationsStrategy']) -> 'OperationsStrategy'
  OperationsStrategy.init_new_tbo(layers: torch.nn.ModuleList, forward_mode: ForwardMode) -> 'OperationsStrategy'

# python/sglang/srt/patch_torch.py
monkey_patch_torch_reductions()
monkey_patch_torch_compile()

# python/sglang/srt/poll_based_barrier.py
  PollBasedBarrier.__init__(noop: bool)
  PollBasedBarrier.local_arrive()
  PollBasedBarrier.poll_global_arrived() -> bool

# python/sglang/srt/reasoning_parser.py
  StreamingParseResult.__init__(normal_text: Optional[str], reasoning_text: Optional[str])
  BaseReasoningFormatDetector.__init__(think_start_token: str, think_end_token: str, force_reasoning: bool, stream_reasoning: bool)
  BaseReasoningFormatDetector.detect_and_parse(text: str) -> StreamingParseResult
  BaseReasoningFormatDetector.parse_streaming_increment(new_text: str) -> StreamingParseResult
  DeepSeekR1Detector.__init__(stream_reasoning: bool, force_reasoning: bool)
  Qwen3Detector.__init__(stream_reasoning: bool, force_reasoning: bool)
  KimiDetector.__init__(stream_reasoning: bool, force_reasoning: bool)
  GptOssDetector.__init__(stream_reasoning: bool, force_reasoning: bool)
  GptOssDetector.detect_and_parse(text: str) -> StreamingParseResult
  GptOssDetector.parse_streaming_increment(new_text: str) -> StreamingParseResult
  ReasoningParser.__init__(model_type: Optional[str], stream_reasoning: bool, force_reasoning: Optional[bool])
  ReasoningParser.parse_non_stream(full_text: str) -> Tuple[Optional[str], Optional[str]]
  ReasoningParser.parse_stream_chunk(chunk_text: str) -> Tuple[Optional[str], Optional[str]]

# python/sglang/srt/server_args.py
add_load_format_choices(choices)
add_quantization_method_choices(choices)
add_attention_backend_choices(choices)
add_disagg_transfer_backend_choices(choices)
  ServerArgs.__post_init__()
  ServerArgs.add_cli_args(parser: argparse.ArgumentParser)
  ServerArgs.from_cli_args(cls, args: argparse.Namespace)
  ServerArgs.url()
  ServerArgs.get_hf_config()
  ServerArgs.check_server_args()
  ServerArgs.check_lora_server_args()
  ServerArgs.validate_disagg_tp_size(prefill_tp: int, decode_tp: int)
  ServerArgs.model_specific_adjustments()
  ServerArgs.adjust_mem_fraction_for_vlm(model_config)
prepare_server_args(argv: List[str]) -> ServerArgs
  PortArgs.init_new(server_args, dp_rank: Optional[int]) -> 'PortArgs'
  LoRAPathAction.__call__(parser, namespace, values, option_string)
  DeprecatedAction.__init__(option_strings, dest, nargs)
  DeprecatedAction.__call__(parser, namespace, values, option_string)
print_deprecated_warning(message: str)
auto_choose_speculative_params(self: ServerArgs)

# python/sglang/srt/torch_memory_saver_adapter.py
  TorchMemorySaverAdapter.create(enable: bool)
  TorchMemorySaverAdapter.check_validity(caller_name)
  TorchMemorySaverAdapter.configure_subprocess()
  TorchMemorySaverAdapter.region(tag: str)
  TorchMemorySaverAdapter.pause(tag: str)
  TorchMemorySaverAdapter.resume(tag: str)
  TorchMemorySaverAdapter.enabled()
  _TorchMemorySaverAdapterReal.configure_subprocess()
  _TorchMemorySaverAdapterReal.region(tag: str)
  _TorchMemorySaverAdapterReal.pause(tag: str)
  _TorchMemorySaverAdapterReal.resume(tag: str)
  _TorchMemorySaverAdapterReal.enabled()
  _TorchMemorySaverAdapterNoop.configure_subprocess()
  _TorchMemorySaverAdapterNoop.region(tag: str)
  _TorchMemorySaverAdapterNoop.pause(tag: str)
  _TorchMemorySaverAdapterNoop.resume(tag: str)
  _TorchMemorySaverAdapterNoop.enabled()

# python/sglang/srt/two_batch_overlap.py
get_token_num_per_seq(forward_mode: ForwardMode, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
compute_split_seq_index(forward_mode: 'ForwardMode', num_tokens: int, extend_lens: Optional[Sequence[int]], token_num_per_seq: Optional[int]) -> Optional[int]
split_spec_info(spec_info: Optional[EagleVerifyInput], start_seq_index: int, end_seq_index: int, start_token_index: int, end_token_index: int)
compute_split_token_index(split_seq_index: int, forward_mode: 'ForwardMode', extend_seq_lens: Optional[Sequence[int]], token_num_per_seq: Optional[int]) -> int
compute_split_indices_for_cuda_graph_replay(forward_mode: ForwardMode, cuda_graph_num_tokens: int, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  TboCudaGraphRunnerPlugin.__init__()
  TboCudaGraphRunnerPlugin.capture_one_batch_size(batch: ForwardBatch, num_tokens: int)
  TboCudaGraphRunnerPlugin.replay_prepare(forward_mode: ForwardMode, bs: int, num_token_non_padded: int, spec_info: Optional[Union[EagleDraftInput, EagleVerifyInput]])
  TboDPAttentionPreparer.prepare_all_gather(local_batch: ScheduleBatch)
  TboDPAttentionPreparer.compute_output(partial_global_info)
  TboForwardBatchPreparer.prepare(cls, batch: ForwardBatch, is_draft_worker: bool)
  TboForwardBatchPreparer.prepare_raw(cls, batch: ForwardBatch, tbo_children_num_token_non_padded: torch.Tensor)
  TboForwardBatchPreparer.derive_fields_related_to_seq_len_for_two_chunk(cls, batch: ForwardBatch)
  TboForwardBatchPreparer.filter_batch(cls, batch: ForwardBatch)
  TboForwardBatchPreparer.compute_tbo_children_num_token_non_padded(cls, batch: ForwardBatch)
  TboForwardBatchPreparer.compute_tbo_children_num_token_non_padded_raw(cls, tbo_split_token_index: int, num_token_non_padded: int)
model_forward_maybe_tbo(layers, enable_tbo: bool, positions: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor, input_data_scatter_mode: ScatterMode, residual: Optional[torch.Tensor], zero_allocator: Optional[BumpAllocator])
  MaybeTboDeepEPDispatcher.__init__()
  MaybeTboDeepEPDispatcher.dispatch() -> DispatchOutput
  MaybeTboDeepEPDispatcher.dispatch_a()
  MaybeTboDeepEPDispatcher.dispatch_b()
  MaybeTboDeepEPDispatcher.combine() -> torch.Tensor
  MaybeTboDeepEPDispatcher.combine_a()
  MaybeTboDeepEPDispatcher.combine_b()

# python/sglang/srt/utils.py
is_hip() -> bool
is_cuda()
is_cuda_alike()
is_hpu() -> bool
is_xpu() -> bool
is_npu() -> bool
is_host_cpu_x86() -> bool
is_cpu() -> bool
get_cuda_version()
is_blackwell()
is_sm100_supported(device) -> bool
is_sm90_supported(device) -> bool
get_bool_env_var(name: str, default: str) -> bool
get_int_env_var(name: str, default: int) -> int
support_triton(backend: str) -> bool
cpu_has_amx_support()
use_intel_amx_backend(layer)
is_flashinfer_available()
random_uuid() -> str
  DynamicGradMode.set_inference_mode(mode: bool)
  DynamicGradMode.__init__(mode)
  DynamicGradMode.__new__(cls, mode_or_orig_func)
  DynamicGradMode.__enter__() -> None
  DynamicGradMode.__exit__(exc_type: Any, exc_value: Any, traceback: Any) -> None
  DynamicGradMode.clone() -> 'DynamicGradMode'
enable_show_time_cost()
  TimeInfo.__init__(name, interval, color, indent)
  TimeInfo.check()
  TimeInfo.pretty_print()
mark_start(name, interval, color, indent)
mark_end(name)
calculate_time(show, min_cost_ms)
get_available_gpu_memory(device, gpu_id, distributed, empty_cache, cpu_group)
is_pin_memory_available() -> bool
  LayerFn.__call__(layer_id: int, prefix: str) -> torch.nn.Module
make_layers(num_hidden_layers: int, layer_fn: LayerFn, pp_rank: Optional[int], pp_size: Optional[int], prefix: str, return_tuple: bool, offloader_kwargs: Dict[str, Any]) -> Tuple[int, int, torch.nn.ModuleList]
set_random_seed(seed: int) -> None
find_process_using_port(port: int) -> Optional[psutil.Process]
wait_port_available(port: int, port_name: str, timeout_s: int, raise_exception: bool) -> bool
is_port_available(port)
get_free_port()
decode_video_base64(video_base64)
load_audio(audio_file: str, sr: Optional[int], mono: bool) -> np.ndarray
load_image(image_file: Union[Image.Image, str, ImageData, bytes]) -> tuple[Image.Image, tuple[int, int]]
load_video(video_file: Union[str, bytes], use_gpu: bool)
suppress_other_loggers()
assert_pkg_version(pkg: str, min_version: str, message: str)
kill_process_tree(parent_pid, include_parent: bool, skip_pid: int)
monkey_patch_p2p_access_check()
monkey_patch_vllm_gguf_config()
set_ulimit(target_soft_limit)
add_api_key_middleware(app, api_key: str)
prepare_model_and_tokenizer(model_path: str, tokenizer_path: str)
configure_logger(server_args, prefix: str)
replace_submodule(model: nn.Module, module_name: str, new_module: nn.Module) -> nn.Module
set_weight_attrs(weight: torch.Tensor, weight_attrs: Optional[Dict[str, Any]])
broadcast_pyobj(data: List[Any], rank: int, dist_group: Optional[torch.distributed.ProcessGroup], src: int, force_cpu_device: bool)
point_to_point_pyobj(data: List[Any], rank: int, group: Optional[torch.distributed.ProcessGroup], src: int, dst: int)
pytorch_profile(name, func)
get_zmq_socket(context: zmq.Context, socket_type: zmq.SocketType, endpoint: str, bind: bool)
dump_to_file(dirpath, name, value)
is_triton_3()
maybe_torch_compile()
delete_directory(dirpath)
set_prometheus_multiproc_dir()
add_prometheus_middleware(app)
bind_port(port)
get_amdgpu_memory_capacity()
get_device_sm()
get_nvgpu_memory_capacity()
get_hpu_memory_capacity()
get_npu_memory_capacity()
get_device_memory_capacity(device: str)
init_custom_process_group(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)
crash_on_warnings()
print_warning_once(msg: str) -> None
print_info_once(msg: str) -> None
get_device_name(device_id: int) -> str
is_habana_available() -> bool
get_device(device_id: Optional[int]) -> str
get_device_count() -> int
get_device_core_count(device_id: int) -> int
get_device_capability(device_id: int) -> Tuple[int, int]
get_npu_compiler_config()
get_compiler_backend() -> str
supports_custom_op() -> bool
direct_register_custom_op(op_name: str, op_func: Callable, mutates_args: List[str], fake_impl: Optional[Callable], target_lib: Optional[Library])
set_gpu_proc_affinity(tp_size: int, nnodes: int, gpu_id: int)
disable_request_logging() -> bool
dataclass_to_string_truncated(data, max_length, skip_names: Optional[Set[str]])
permute_weight(x: torch.Tensor) -> torch.Tensor
  MultiprocessingSerializer.serialize(obj, output_str: bool)
  MultiprocessingSerializer.deserialize(data)
debug_timing(func)
nullable_str(val: str)
pyspy_dump_schedulers()
kill_itself_when_parent_died()
set_uvicorn_logging_configs()
get_ip() -> str
get_open_port() -> int
is_valid_ipv6_address(address: str) -> bool
maybe_wrap_ipv6_address(address: str) -> str
format_tcp_address(ip: str, port: int) -> str
configure_ipv6(dist_init_addr)
launch_dummy_health_check_server(host, port, enable_metrics)
create_checksum(directory: str)
set_cuda_arch()
next_power_of_2(n: int)
round_up(x: int, y: int) -> int
  EmptyContextManager.__enter__()
  EmptyContextManager.__exit__(exc_type, exc_value, traceback)
empty_context()
add_prefix(name: str, prefix: str) -> str
is_remote_url(url: Union[str, Path]) -> bool
parse_connector_type(url: str) -> str
retry(fn, max_retry: int, initial_delay: float, max_delay: float, should_retry: Callable[[Any], bool])
flatten_nested_list(nested_list)
is_non_idle_and_non_empty(forward_mode, hidden_states)
fast_topk(values, topk, dim)
bind_or_assign(target, source)
get_local_ip_auto() -> str
get_local_ip_by_nic(interface: str) -> str
get_local_ip_by_remote() -> str
is_page_size_one(server_args)
is_no_spec_infer_or_topk_one(server_args)
is_fa3_default_architecture(hf_config)
  BumpAllocator.__init__(buffer_size: int, dtype, device)
  BumpAllocator.allocate(size: int)
log_info_on_rank0(logger, msg)
load_json_config(data: str)
dispose_tensor(x: torch.Tensor)
  Withable.__init__()
  Withable.value() -> T
  Withable.with_value(new_value: T)
require_mlp_tp_gather(server_args)
require_attn_tp_gather(server_args)
require_gathered_buffer(server_args)
require_mlp_sync(server_args)
find_local_repo_dir(repo_id: str, revision: Optional[str]) -> Optional[str]
read_system_prompt_from_file(model_name: str) -> str
bind_or_assign(target, source)
prepack_weight_if_needed(weight)
dim_is_supported(weight)
  PackWeightMethod.__init__(weight_names, transpose_dims)
  PackWeightMethod.process_weights_after_loading(module) -> None
  LazyValue.__init__(creator: Callable)
  LazyValue.value()
dynamic_import(func_path: str)
gc_object_counts()
configure_gc_warning(warn_threshold_secs)
freeze_gc(context: str)
configure_gc_logger()
align(x: int, y: int) -> int
ceil_div(x: int, y: int) -> int
parse_lscpu_topology()
get_physical_cpus_by_numa()
get_cpu_ids_by_node()
is_shm_available(dtype, world_size, local_size)
lru_cache_frozenset(maxsize)
apply_module_patch(target_module, target_function, wrappers)
parse_module_path(module_path, function_name, create_dummy)
mxfp_supported()
  ConcurrentCounter.__init__(initial: int)
  ConcurrentCounter.value() -> int
  ConcurrentCounter.__repr__() -> str
  ConcurrentCounter.increment(n: int, notify_all: bool)
  ConcurrentCounter.decrement(n: int, notify_all: bool)
  ConcurrentCounter.wait_for(condition: Callable[[int], bool])
  ConcurrentCounter.wait_for_zero()
is_triton_kernels_available() -> bool
check_cuda_result(raw_output)

# python/sglang/srt/warmup.py
warmup(name: str) -> callable
execute_warmups(disaggregation_mode: str, warmup_names: List[str], tokenizer_manager: TokenizerManager)
voice_chat(disaggregation_mode: str, tokenizer_manager: TokenizerManager)
