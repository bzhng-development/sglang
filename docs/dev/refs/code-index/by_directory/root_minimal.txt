
# python/sglang/srt/_custom_ops.py
init_custom_ar(ipc_tensors, rank_data, rank, full_nvlink)
all_reduce(fa, inp, out, reg_buffer, reg_buffer_sz_bytes)
dispose(fa)
meta_size()
register_buffer(fa, ipc_tensors)
get_graph_buffer_ipc_meta(fa)
register_graph_buffers(fa, handles, offsets)
init_custom_ar(meta, rank_data, handles, offsets, rank, full_nvlink)
all_reduce_reg(fa, inp, out)
all_reduce_unreg(fa, inp, reg_buffer, out)
dispose(fa)
meta_size()
register_buffer(fa, t, handles, offsets)
get_graph_buffer_ipc_meta(fa)
register_graph_buffers(fa, handles, offsets)
allocate_meta_buffer(size)
get_meta_buffer_ipc_handle(inp)
init_custom_qr(rank, world_size, qr_max_size)
qr_get_handle(fa)
qr_open_handles(fa, handles)
qr_all_reduce(fa, inp, out, quant_level, cast_bf2half)
qr_destroy(fa)
qr_max_size()
mscclpp_generate_unique_id()
mscclpp_init_context(unique_id, rank, world_size, scratch, put_buffer, nranks_per_node, rank_to_node, rank_to_ib, context_selection)
mscclpp_allreduce(context, inp, out, nthreads, nblocks)

# python/sglang/srt/aio_rwlock.py
  RWLock.__init__()
  RWLock.reader_lock()
  RWLock.writer_lock()
  RWLock.acquire_reader()
  RWLock.release_reader()
  RWLock.acquire_writer()
  RWLock.release_writer()
  _ReaderLock.__init__(rwlock)
  _ReaderLock.__aenter__()
  _ReaderLock.__aexit__(exc_type, exc_val, exc_tb)
  _WriterLock.__init__(rwlock)
  _WriterLock.__aenter__()
  _WriterLock.__aexit__(exc_type, exc_val, exc_tb)

# python/sglang/srt/bench_utils.py
  suppress_stdout_stderr.__enter__()
  suppress_stdout_stderr.__exit__()
bench_kineto(fn, kernel_names, num_tests, suppress_kineto_output, trace_path, flush_l2, with_multiple_kernels)

# python/sglang/srt/code_completion_parser.py
register_completion_template(template, override)
completion_template_exists(template_name)
is_completion_template_defined()
generate_completion_prompt_from_request(request)
generate_completion_prompt(prompt, suffix, template_name)

# python/sglang/srt/conversation.py
  Conversation.get_prompt()
  Conversation.set_system_message(system_message)
  Conversation.append_message(role, message)
  Conversation.append_image(image, detail, 'low', 'high'])
  Conversation.append_video(video)
  Conversation.append_audio(audio)
  Conversation.update_last_message(message)
  Conversation.to_gradio_chatbot()
  Conversation.to_openai_api_messages()
  Conversation.copy()
  Conversation.dict()
register_conv_template(template, override)
register_conv_template_matching_function(func)
get_conv_template_by_model_path(model_path)
chat_template_exists(template_name)
generate_embedding_convs(texts, images, template_name)
generate_chat_conv(request, template_name)
get_model_type(model_path)
match_internvl(model_path)
match_deepseek_janus_pro(model_path)
match_vicuna(model_path)
match_deepseek_vl(model_path)
match_qwen_chat_ml(model_path)
match_minicpm(model_path)
match_phi_4_mm(model_path)

# python/sglang/srt/custom_op.py
  CustomOp.__init__()
  CustomOp.enter_torch_compile(num_tokens)
  CustomOp.leave_torch_compile()
  CustomOp.forward()
  CustomOp.forward_native()
  CustomOp.forward_cuda()
  CustomOp.forward_npu()
  CustomOp.forward_hip()
  CustomOp.forward_xpu()
  CustomOp.forward_hpu()
  CustomOp.forward_cpu()
  CustomOp.dispatch_forward()

# python/sglang/srt/harmony_parser.py
prefix_hold(text, tokens)
iter_tokens(text, start_pos)
  CanonicalStrategy.__init__()
  CanonicalStrategy.parse(text)
  TextStrategy.__init__()
  TextStrategy.set_buffer_context(buffer)
  TextStrategy.parse(text)
  HarmonyParser.__init__()
  HarmonyParser.parse(chunk)

# python/sglang/srt/hf_transformers_utils.py
download_from_hf(model_path, allow_patterns, list]])
get_hf_text_config(config)
get_config(model, trust_remote_code, revision, model_override_args)
get_generation_config(model, trust_remote_code, revision)
get_sparse_attention_config(model, sparse_attention_config_filename)
get_context_length(config)
get_tokenizer(tokenizer_name)
get_tokenizer_from_processor(processor)
get_processor(tokenizer_name)
attach_additional_stop_token_ids(tokenizer)
check_gguf_file(model, os.PathLike])

# python/sglang/srt/host_shared_memory.py
  HostSharedMemoryManager.__init__(base_name)
  HostSharedMemoryManager.malloc()
get_host_shared_memory_manager()
set_host_shared_memory_manager(instance)

# python/sglang/srt/jinja_template_utils.py
detect_jinja_template_content_format(chat_template)
process_content_for_template_format(msg_dict, content_format, image_data, video_data, audio_data, modalities)

# python/sglang/srt/model_parallel.py
tensor_parallel(module, device_mesh)

# python/sglang/srt/offloader.py
  BaseOffloader.wrap_modules(all_modules_generator, None, None], submodule_accessor, whitelist_param_names_creator)
  BaseOffloader.post_init()
get_offloader()
set_offloader(instance)
create_offloader_from_server_args(server_args, dp_rank)
  OffloaderV1.__init__(cpu_offload_max_bytes)
  OffloaderV1.wrap_modules(all_modules_generator, None, None], submodule_accessor, whitelist_param_names_creator)
  OffloaderV1.maybe_offload_to_cpu(module)
  OffloaderV2.__init__(group_size, num_in_group, prefetch_step, mode, dp_rank, dp_size)
  OffloaderV2.wrap_modules(all_modules_generator, None, None], submodule_accessor, whitelist_param_names_creator)
  OffloaderV2.post_init()
  _ModuleOffloader.__init__(mode, module, alt_stream, whitelist_param_names)
  _ModuleOffloader.post_init()
  _ModuleOffloader.start_onload()
  _ModuleOffloader.offload()
  _ModuleOffloader.wait_and_get_device_tensors()
  _BaseParamOffloader.create(mode)
  _BaseParamOffloader.__init__(module, param_name)
  _BaseParamOffloader.post_init()
  _BaseParamOffloader.create_device_tensor()
  _MetaParamOffloader.__init__(module, param_name)
  _MetaParamOffloader.create_device_tensor()
  _CpuParamOffloader.__init__(module, param_name)
  _CpuParamOffloader.create_device_tensor()
  _ShmCpuParamOffloader.__init__(module, param_name)
  _ShmCpuParamOffloader.post_init()
  _ShmCpuParamOffloader.create_device_tensor()
  _ShardedGpuParamOffloader.__init__(module, param_name)
  _ShardedGpuParamOffloader.post_init()
  _ShardedGpuParamOffloader.create_device_tensor()

# python/sglang/srt/operations.py
execute_operations(inputs, operations)
execute_overlapped_operations(inputs_arr, operations_arr, delta_stages)
  _StageExecutor.__init__(debug_name, stages, inputs)
  _StageExecutor.next()
  _StageExecutor.output()
  _StageExecutor.done()
  _StageExecutor.num_stages()
  _StateDict.__init__()
  _StateDict.__setattr__(key, value)
  _StateDict.__getattr__(item)
  _StateDict.__delattr__(item)
  _StateDict.pop(item)
  _StateDict.update(values, Any])
  _StateDict.get(item)
  _StateDict.clear(expect_keys)

# python/sglang/srt/operations_strategy.py
  OperationsStrategy.concat(cls, items)
  OperationsStrategy.init_new_tbo(layers, forward_mode)

# python/sglang/srt/patch_torch.py
monkey_patch_torch_reductions()
monkey_patch_torch_compile()

# python/sglang/srt/poll_based_barrier.py
  PollBasedBarrier.__init__(noop)
  PollBasedBarrier.local_arrive()
  PollBasedBarrier.poll_global_arrived()

# python/sglang/srt/reasoning_parser.py
  StreamingParseResult.__init__(normal_text, reasoning_text)
  BaseReasoningFormatDetector.__init__(think_start_token, think_end_token, force_reasoning, stream_reasoning)
  BaseReasoningFormatDetector.detect_and_parse(text)
  BaseReasoningFormatDetector.parse_streaming_increment(new_text)
  DeepSeekR1Detector.__init__(stream_reasoning, force_reasoning)
  Qwen3Detector.__init__(stream_reasoning, force_reasoning)
  KimiDetector.__init__(stream_reasoning, force_reasoning)
  GptOssDetector.__init__(stream_reasoning, force_reasoning)
  GptOssDetector.detect_and_parse(text)
  GptOssDetector.parse_streaming_increment(new_text)
  ReasoningParser.__init__(model_type, stream_reasoning, force_reasoning)
  ReasoningParser.parse_non_stream(full_text)
  ReasoningParser.parse_stream_chunk(chunk_text)

# python/sglang/srt/server_args.py
  ServerArgs.__post_init__()
  ServerArgs.add_cli_args(parser)
  ServerArgs.from_cli_args(cls, args)
  ServerArgs.url()
  ServerArgs.get_hf_config()
  ServerArgs.check_server_args()
  ServerArgs.check_lora_server_args()
  ServerArgs.validate_disagg_tp_size(prefill_tp, decode_tp)
  ServerArgs.model_specific_adjustments()
  ServerArgs.adjust_mem_fraction_for_vlm(model_config)
prepare_server_args(argv)
  PortArgs.init_new(server_args, dp_rank)
  LoRAPathAction.__call__(parser, namespace, values, option_string)
  DeprecatedAction.__init__(option_strings, dest, nargs)
  DeprecatedAction.__call__(parser, namespace, values, option_string)
print_deprecated_warning(message)
auto_choose_speculative_params()

# python/sglang/srt/torch_memory_saver_adapter.py
  TorchMemorySaverAdapter.create(enable)
  TorchMemorySaverAdapter.check_validity(caller_name)
  TorchMemorySaverAdapter.configure_subprocess()
  TorchMemorySaverAdapter.region(tag)
  TorchMemorySaverAdapter.pause(tag)
  TorchMemorySaverAdapter.resume(tag)
  TorchMemorySaverAdapter.enabled()
  _TorchMemorySaverAdapterReal.configure_subprocess()
  _TorchMemorySaverAdapterReal.region(tag)
  _TorchMemorySaverAdapterReal.pause(tag)
  _TorchMemorySaverAdapterReal.resume(tag)
  _TorchMemorySaverAdapterReal.enabled()
  _TorchMemorySaverAdapterNoop.configure_subprocess()
  _TorchMemorySaverAdapterNoop.region(tag)
  _TorchMemorySaverAdapterNoop.pause(tag)
  _TorchMemorySaverAdapterNoop.resume(tag)
  _TorchMemorySaverAdapterNoop.enabled()

# python/sglang/srt/two_batch_overlap.py
get_token_num_per_seq(forward_mode, spec_info, EagleVerifyInput]])
compute_split_seq_index(forward_mode, num_tokens, extend_lens, token_num_per_seq)
split_spec_info(spec_info, start_seq_index, end_seq_index, start_token_index, end_token_index)
compute_split_token_index(split_seq_index, forward_mode, extend_seq_lens, token_num_per_seq)
compute_split_indices_for_cuda_graph_replay(forward_mode, cuda_graph_num_tokens, spec_info, EagleVerifyInput]])
  TboCudaGraphRunnerPlugin.__init__()
  TboCudaGraphRunnerPlugin.capture_one_batch_size(batch, num_tokens)
  TboCudaGraphRunnerPlugin.replay_prepare(forward_mode, bs, num_token_non_padded, spec_info, EagleVerifyInput]])
  TboDPAttentionPreparer.prepare_all_gather(local_batch)
  TboDPAttentionPreparer.compute_output(partial_global_info)
  TboForwardBatchPreparer.prepare(cls, batch, is_draft_worker)
  TboForwardBatchPreparer.prepare_raw(cls, batch, tbo_children_num_token_non_padded)
  TboForwardBatchPreparer.derive_fields_related_to_seq_len_for_two_chunk(cls, batch)
  TboForwardBatchPreparer.filter_batch(cls, batch)
  TboForwardBatchPreparer.compute_tbo_children_num_token_non_padded(cls, batch)
  TboForwardBatchPreparer.compute_tbo_children_num_token_non_padded_raw(cls, tbo_split_token_index, num_token_non_padded)
model_forward_maybe_tbo(layers, enable_tbo, positions, forward_batch, hidden_states, input_data_scatter_mode, residual, zero_allocator)
  MaybeTboDeepEPDispatcher.__init__()
  MaybeTboDeepEPDispatcher.dispatch()
  MaybeTboDeepEPDispatcher.dispatch_a()
  MaybeTboDeepEPDispatcher.dispatch_b()
  MaybeTboDeepEPDispatcher.combine()
  MaybeTboDeepEPDispatcher.combine_a()
  MaybeTboDeepEPDispatcher.combine_b()

# python/sglang/srt/utils.py
is_hip()
is_cuda()
is_cuda_alike()
is_hpu()
is_xpu()
is_npu()
is_host_cpu_x86()
is_cpu()
get_cuda_version()
is_blackwell()
is_sm100_supported(device)
is_sm90_supported(device)
get_bool_env_var(name, default)
get_int_env_var(name, default)
support_triton(backend)
cpu_has_amx_support()
use_intel_amx_backend(layer)
is_flashinfer_available()
random_uuid()
  DynamicGradMode.set_inference_mode(mode)
  DynamicGradMode.__init__(mode)
  DynamicGradMode.__new__(cls, mode_or_orig_func)
  DynamicGradMode.__enter__()
  DynamicGradMode.__exit__(exc_type, exc_value, traceback)
  DynamicGradMode.clone()
enable_show_time_cost()
  TimeInfo.__init__(name, interval, color, indent)
  TimeInfo.check()
  TimeInfo.pretty_print()
mark_start(name, interval, color, indent)
mark_end(name)
calculate_time(show, min_cost_ms)
get_available_gpu_memory(device, gpu_id, distributed, empty_cache, cpu_group)
is_pin_memory_available()
  LayerFn.__call__(layer_id, prefix)
make_layers(num_hidden_layers, layer_fn, pp_rank, pp_size, prefix, return_tuple, offloader_kwargs, Any])
set_random_seed(seed)
find_process_using_port(port)
wait_port_available(port, port_name, timeout_s, raise_exception)
is_port_available(port)
get_free_port()
decode_video_base64(video_base64)
load_audio(audio_file, sr, mono)
load_image(image_file, str, ImageData, bytes])
load_video(video_file, bytes], use_gpu)
suppress_other_loggers()
assert_pkg_version(pkg, min_version, message)
kill_process_tree(parent_pid, include_parent, skip_pid)
monkey_patch_p2p_access_check()
monkey_patch_vllm_gguf_config()
set_ulimit(target_soft_limit)
add_api_key_middleware(app, api_key)
prepare_model_and_tokenizer(model_path, tokenizer_path)
configure_logger(server_args, prefix)
replace_submodule(model, module_name, new_module)
set_weight_attrs(weight, weight_attrs, Any]])
broadcast_pyobj(data, rank, dist_group, src, force_cpu_device)
point_to_point_pyobj(data, rank, group, src, dst)
pytorch_profile(name, func)
get_zmq_socket(context, socket_type, endpoint, bind)
dump_to_file(dirpath, name, value)
is_triton_3()
maybe_torch_compile()
delete_directory(dirpath)
set_prometheus_multiproc_dir()
add_prometheus_middleware(app)
bind_port(port)
get_amdgpu_memory_capacity()
get_device_sm()
get_nvgpu_memory_capacity()
get_hpu_memory_capacity()
get_npu_memory_capacity()
get_device_memory_capacity(device)
init_custom_process_group(backend, init_method, timeout, world_size, rank, store, group_name, pg_options)
crash_on_warnings()
print_warning_once(msg)
print_info_once(msg)
get_device_name(device_id)
is_habana_available()
get_device(device_id)
get_device_count()
get_device_core_count(device_id)
get_device_capability(device_id)
get_npu_compiler_config()
get_compiler_backend()
supports_custom_op()
direct_register_custom_op(op_name, op_func, mutates_args, fake_impl, target_lib)
set_gpu_proc_affinity(tp_size, nnodes, gpu_id)
disable_request_logging()
dataclass_to_string_truncated(data, max_length, skip_names)
permute_weight(x)
  MultiprocessingSerializer.serialize(obj, output_str)
  MultiprocessingSerializer.deserialize(data)
debug_timing(func)
nullable_str(val)
pyspy_dump_schedulers()
kill_itself_when_parent_died()
set_uvicorn_logging_configs()
get_ip()
get_open_port()
is_valid_ipv6_address(address)
maybe_wrap_ipv6_address(address)
format_tcp_address(ip, port)
configure_ipv6(dist_init_addr)
launch_dummy_health_check_server(host, port, enable_metrics)
create_checksum(directory)
set_cuda_arch()
next_power_of_2(n)
round_up(x, y)
  EmptyContextManager.__enter__()
  EmptyContextManager.__exit__(exc_type, exc_value, traceback)
empty_context()
add_prefix(name, prefix)
is_remote_url(url, Path])
parse_connector_type(url)
retry(fn, max_retry, initial_delay, max_delay, should_retry, bool])
flatten_nested_list(nested_list)
is_non_idle_and_non_empty(forward_mode, hidden_states)
fast_topk(values, topk, dim)
bind_or_assign(target, source)
get_local_ip_auto()
get_local_ip_by_nic(interface)
get_local_ip_by_remote()
is_page_size_one(server_args)
is_no_spec_infer_or_topk_one(server_args)
is_fa3_default_architecture(hf_config)
  BumpAllocator.__init__(buffer_size, dtype, device)
  BumpAllocator.allocate(size)
log_info_on_rank0(logger, msg)
load_json_config(data)
dispose_tensor(x)
  Withable.__init__()
  Withable.value()
  Withable.with_value(new_value)
require_mlp_tp_gather(server_args)
require_attn_tp_gather(server_args)
require_gathered_buffer(server_args)
require_mlp_sync(server_args)
find_local_repo_dir(repo_id, revision)
read_system_prompt_from_file(model_name)
bind_or_assign(target, source)
prepack_weight_if_needed(weight)
dim_is_supported(weight)
  PackWeightMethod.__init__(weight_names, transpose_dims)
  PackWeightMethod.process_weights_after_loading(module)
  LazyValue.__init__(creator)
  LazyValue.value()
dynamic_import(func_path)
gc_object_counts()
configure_gc_warning(warn_threshold_secs)
freeze_gc(context)
configure_gc_logger()
align(x, y)
ceil_div(x, y)
parse_lscpu_topology()
get_physical_cpus_by_numa()
get_cpu_ids_by_node()
is_shm_available(dtype, world_size, local_size)
lru_cache_frozenset(maxsize)
apply_module_patch(target_module, target_function, wrappers)
parse_module_path(module_path, function_name, create_dummy)
mxfp_supported()
  ConcurrentCounter.__init__(initial)
  ConcurrentCounter.value()
  ConcurrentCounter.__repr__()
  ConcurrentCounter.increment(n, notify_all)
  ConcurrentCounter.decrement(n, notify_all)
  ConcurrentCounter.wait_for(condition, bool])
  ConcurrentCounter.wait_for_zero()
is_triton_kernels_available()
check_cuda_result(raw_output)

# python/sglang/srt/warmup.py
warmup(name)
execute_warmups(disaggregation_mode, warmup_names, tokenizer_manager)
voice_chat(disaggregation_mode, tokenizer_manager)