
# python/sglang/srt/constrained/triton_ops/bitmask_ops.py
apply_token_bitmask_inplace_kernel(logits_ptr, bitmask_ptr, indices_ptr, num_rows, vocab_size, logits_strides, bitmask_strides, NUM_SMS, BLOCK_SIZE)
apply_token_bitmask_inplace_triton(logits, bitmask, indices, torch.Tensor]])

# python/sglang/srt/layers/attention/triton_ops/decode_attention.py
tanh(x)
decode_attention_fwd_normal(q, k_buffer, v_buffer, o, kv_indptr, kv_indices, attn_logits, attn_lse, num_kv_splits, max_kv_splits, sm_scale, logit_cap, sinks, xai_temperature_len)
decode_attention_fwd_grouped(q, k_buffer, v_buffer, o, kv_indptr, kv_indices, attn_logits, attn_lse, num_kv_splits, max_kv_splits, sm_scale, logit_cap, sinks, xai_temperature_len)
decode_attention_fwd(q, k_buffer, v_buffer, o, kv_indptr, kv_indices, attn_logits, attn_lse, num_kv_splits, max_kv_splits, sm_scale, logit_cap, sinks, xai_temperature_len)

# python/sglang/srt/layers/attention/triton_ops/double_sparsity_attention.py
tanh(x)
flash_decode_stage1(q, k, v, Req_to_tokens, B_req_idx, B_Seqlen, max_len_in_batch, mid_out, mid_out_logsumexp, block_seq)
flash_decode_stage2(mid_out, mid_out_logexpsum, B_Seqlen, O, block_seq)
flash_decode_attention_fwd(q, k_buffer, v_buffer, o, req_to_token, b_req_idx, b_start_loc, b_seq_len, attn_logits, max_len_in_batch, sm_scale, logit_cap)
sparse_flash_decode_stage1(q_label, k_label_buffer, att_out, Req_to_tokens, B_Seqlen, max_len_in_batch, sm_scale, logit_cap)
sparse_flash_decode_stage2(q, k, v, Req_to_tokens, Topk_token_indices, heavy_token_num, mid_out, mid_out_logsumexp, block_seq, sm_scale)
sparse_flash_decode_stage3(Seqlen, mid_out, mid_out_logexpsum, O, block_seq)
flash_decode_sparse_attention_fwd(q, k_buffer, v_buffer, o, q_label, k_label_buffer, req_to_token, b_seq_len, max_len_in_batch, sm_scale, logit_cap, heavy_token_num, att_out_approx, mid_out, mid_o_logexpsum, BLOCK_SEQ)
extend_attention_fwd(q_extend, k_extend, v_extend, o_extend, k_buffer, v_buffer, req_to_tokens, b_req_idx, b_seq_len, b_seq_len_extend, b_start_loc_extend, max_len_extend, sm_scale, logit_cap)

# python/sglang/srt/layers/attention/triton_ops/extend_attention.py
tanh(x)
extend_attention_fwd(q_extend, k_extend, v_extend, o_extend, k_buffer, v_buffer, qo_indptr, kv_indptr, kv_indices, custom_mask, is_causal, mask_indptr, max_len_extend, sm_scale, logit_cap, skip_prefix_custom_mask, sliding_window_size, sinks, window_kv_offsets, xai_temperature_len)
redundant_attention(q_extend, o_extend, k_buffer, v_buffer, b_req_idx, b_start_loc, b_seq_len, b_seq_len_prefix, max_len_in_batch)

# python/sglang/srt/layers/attention/triton_ops/merge_state.py
merge_state_kernel(output, output_lse, prefix_output, prefix_lse, suffix_output, suffix_lse, HEAD_SIZE, PADDED_HEAD_SIZE, OUTPUT_LSE)
merge_state_triton(prefix_output, prefix_lse, suffix_output, suffix_lse, output, output_lse)

# python/sglang/srt/layers/attention/triton_ops/prefill_attention.py
context_attention_fwd(q, k, v, o, b_start_loc, b_seq_len, max_input_len, is_causal)

# python/sglang/srt/layers/attention/triton_ops/rocm_mla_decode_rope.py
is_hip()
tanh(x)
decode_attention_fwd_grouped_rope(q, k_buffer, v_buffer, o, kv_indptr, kv_indices, k_pe_tokens, kv_lora_rank, rotary_dim, cos_sin_cache, positions, attn_logits, num_kv_splits, sm_scale, logit_cap, use_rope, is_neox_style)

# python/sglang/srt/lora/triton_ops/gate_up_lora_b.py
gate_up_lora_b_fwd(x, gate_up_lora_b, batch_info, output_dim, base_output)

# python/sglang/srt/lora/triton_ops/qkv_lora_b.py
qkv_lora_b_fwd(x, qkv_lora_b, batch_info, output_offset, max_qkv_out_dim, base_output)

# python/sglang/srt/lora/triton_ops/sgemm_lora_a.py
sgemm_lora_a_fwd(x, weights, batch_info, stack_num)

# python/sglang/srt/lora/triton_ops/sgemm_lora_b.py
sgemm_lora_b_fwd(x, weights, batch_info, base_output)