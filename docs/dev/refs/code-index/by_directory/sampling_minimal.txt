
# python/sglang/srt/sampling/custom_logit_processor.py
  CustomLogitProcessor.__call__(logits: torch.Tensor, custom_param_list: Optional[List[Dict[str, Any]]]) -> torch.Tensor
  CustomLogitProcessor.to_str(cls) -> str
  CustomLogitProcessor.from_str(cls, json_str: str)
  DisallowedTokensLogitsProcessor.__call__(logits: torch.Tensor, custom_param_list: Optional[List[Dict[str, Any]]]) -> torch.Tensor

# python/sglang/srt/sampling/sampling_batch_info.py
  SamplingBatchInfo.from_schedule_batch(cls, batch: ScheduleBatch, vocab_size: int)
  SamplingBatchInfo.__len__()
  SamplingBatchInfo.update_regex_vocab_mask()
  SamplingBatchInfo.update_penalties()
  SamplingBatchInfo.apply_logits_bias(logits: torch.Tensor)
  SamplingBatchInfo.filter_batch(keep_indices: List[int], keep_indices_device: torch.Tensor)
  SamplingBatchInfo.merge_custom_logit_processor(lhs: Optional[Dict[int, Tuple[CustomLogitProcessor, torch.Tensor]]], rhs: Optional[Dict[int, Tuple[CustomLogitProcessor, torch.Tensor]]], bs1: int, bs2: int, device: str)
  SamplingBatchInfo.merge_batch(other: 'SamplingBatchInfo')
merge_bias_tensor(lhs: Optional[torch.Tensor], rhs: Optional[torch.Tensor], bs1: int, bs2: int, device: str, default: float)

# python/sglang/srt/sampling/sampling_params.py
  SamplingParams.__init__(max_new_tokens: int, stop: Optional[Union[str, List[str]]], stop_token_ids: Optional[List[int]], temperature: float, top_p: float, top_k: int, min_p: float, frequency_penalty: float, presence_penalty: float, repetition_penalty: float, min_new_tokens: int, n: int, json_schema: Optional[str], regex: Optional[str], ebnf: Optional[str], structural_tag: Optional[str], ignore_eos: bool, skip_special_tokens: bool, spaces_between_special_tokens: bool, no_stop_trim: bool, custom_params: Optional[Dict[str, Any]], stream_interval: Optional[int], logit_bias: Optional[Dict[str, float]]) -> None
  SamplingParams.verify(vocab_size)
  SamplingParams.normalize(tokenizer)
