================================================================================
FUNCTION INDEX: fused_moe_triton module
================================================================================
Total Functions: 41
Documented: 7


============================================================
FILE: python/sglang/srt/layers/moe/fused_moe_triton/__init__.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  19: def override_config(config)
         @contextmanager

  L  27: def get_config()
         ‚Üí Optional[Dict[str, Any]]


============================================================
FILE: python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
Functions: 20
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  72: def write_zeros_to_output(c_ptr,
        stride_cm,
        stride_cn,
        pid_n,
        N,
        offs_token,
        token_mask,
        BLOCK_SIZE_M,
        BLOCK_SIZE_N,
        compute_type)
         @triton.jit

  L  92: def fused_moe_kernel_gptq_awq(a_ptr,
        b_ptr,
        c_ptr,
        b_scale_ptr,
        b_zp_ptr,
        topk_weights_ptr,
        sorted_token_ids_ptr,
        expert_ids_ptr,
        num_tokens_post_padded_ptr,
        N: tl.constexpr,
        K: tl.constexpr,
        EM,
        num_valid_tokens,
        stride_am,
        stride_ak,
        stride_be,
        stride_bk,
        stride_bn,
        stride_cm,
        stride_cn,
        stride_bse,
        stride_bsk,
        stride_bsn,
        stride_bze,
        stride_bzk,
        stride_bzn,
        group_size: tl.constexpr,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        BLOCK_SIZE_K: tl.constexpr,
        GROUP_SIZE_M: tl.constexpr,
        MUL_ROUTED_WEIGHT: tl.constexpr,
        top_k: tl.constexpr,
        compute_type: tl.constexpr,
        has_zp: tl.constexpr,
        use_int4_w4a16: tl.constexpr,
        use_int8_w8a16: tl.constexpr,
        even_Ks: tl.constexpr)
         üìù Implements the fused computation for a Mixture of Experts (MOE) using
         @triton.jit

  L 323: def fused_moe_kernel(a_ptr,
        b_ptr,
        bias_ptr,
        c_ptr,
        a_scale_ptr,
        b_scale_ptr,
        topk_weights_ptr,
        sorted_token_ids_ptr,
        expert_ids_ptr,
        num_tokens_post_padded_ptr,
        N,
        K,
        EM,
        num_valid_tokens,
        stride_am,
        stride_ak,
        stride_be,
        stride_bk,
        stride_bn,
        stride_bias_e,
        stride_bias_n,
        stride_cm,
        stride_cn,
        stride_asm,
        stride_ask,
        stride_bse,
        stride_bsk,
        stride_bsn,
        group_n: tl.constexpr,
        group_k: tl.constexpr,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        BLOCK_SIZE_K: tl.constexpr,
        GROUP_SIZE_M: tl.constexpr,
        MUL_ROUTED_WEIGHT: tl.constexpr,
        top_k: tl.constexpr,
        compute_type: tl.constexpr,
        use_fp8_w8a8: tl.constexpr,
        use_int8_w8a8: tl.constexpr,
        use_int8_w8a16: tl.constexpr,
        per_channel_quant: tl.constexpr,
        even_Ks: tl.constexpr)
         üìù Implements the fused computation for a Mixture of Experts (MOE) using
         @triton.jit

  L 563: def moe_align_block_size(topk_ids: torch.Tensor,
        block_size: int,
        num_experts: int)
         ‚Üí Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
         üìù Aligns the token distribution across experts to be compatible with blo

  L 636: def invoke_fused_moe_kernel(A: torch.Tensor,
        B: torch.Tensor,
        bias: Optional[torch.Tensor],
        C: torch.Tensor,
        A_scale: Optional[torch.Tensor],
        B_scale: Optional[torch.Tensor],
        B_zp: Optional[torch.Tensor],
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        sorted_token_ids: torch.Tensor,
        expert_ids: torch.Tensor,
        num_tokens_post_padded: torch.Tensor,
        mul_routed_weight: bool,
        top_k: int,
        config: Dict[str,
        Any],
        compute_type: tl.dtype,
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        block_shape: Optional[List[int]],
        no_combine: bool)
         ‚Üí None

  L 813: def get_config_file_name(E: int,
        N: int,
        dtype: Optional[str],
        block_shape: Optional[int])
         ‚Üí str

  L 825: def get_moe_configs(E: int,
        N: int,
        dtype: Optional[str],
        block_n: Optional[int],
        block_k: Optional[int])
         ‚Üí Optional[Dict[int, Any]]
         üìù Return optimized configurations for the fused MoE kernel.
         @functools.lru_cache

  L 898: def get_default_config(M: int,
        E: int,
        N: int,
        K: int,
        topk: int,
        dtype: Optional[str],
        is_marlin: bool,
        block_shape: Optional[List[int]])
         ‚Üí Dict[str, int]

  L 955: def try_get_optimal_moe_config(w1_shape: Tuple[int,
        ...],
        w2_shape: Tuple[int,
        ...],
        top_k: int,
        dtype: Optional[str],
        M: int,
        is_marlin: bool,
        block_shape: Optional[List[int]])

  L 988: def get_config_dtype_str(dtype: torch.dtype,
        use_int8_w8a16: Optional[bool],
        use_int4_w4a16: Optional[bool],
        use_fp8_w8a8: Optional[bool],
        use_int8_w8a8: Optional[bool])

  L1010: def inplace_fused_experts(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]],
        routed_scaling_factor: Optional[float],
        gemm1_alpha: Optional[float],
        gemm1_limit: Optional[float])
         ‚Üí None

  L1066: def inplace_fused_experts_fake(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]],
        routed_scaling_factor: Optional[float],
        gemm1_alpha: Optional[float],
        gemm1_limit: Optional[float])
         ‚Üí None

  L1103: def outplace_fused_experts(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]],
        no_combine: bool,
        routed_scaling_factor: Optional[float],
        gemm1_alpha: Optional[float],
        gemm1_limit: Optional[float])
         ‚Üí torch.Tensor

  L1160: def outplace_fused_experts_fake(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]],
        no_combine: bool,
        routed_scaling_factor: Optional[float],
        gemm1_alpha: Optional[float],
        gemm1_limit: Optional[float])
         ‚Üí torch.Tensor

  L1198: def fused_experts(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_output: StandardTopKOutput,
        moe_runner_config: MoeRunnerConfig,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]])

  L1329: def moe_sum_reduce_triton(input: torch.Tensor,
        output: torch.Tensor,
        routed_scaling_factor: float)

  L1366: def moe_sum_reduce_torch_compile(x, out, routed_scaling_factor)
         @torch.compile

  L1372: def swiglu_with_alpha_and_limit(x, gemm1_alpha, gemm1_limit)
         @torch.compile

  L1379: def fused_experts_impl(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        inplace: bool,
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]],
        no_combine: bool,
        routed_scaling_factor: Optional[float],
        gemm1_alpha: Optional[float],
        gemm1_limit: Optional[float])

  L1646: def fused_moe(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_output: StandardTopKOutput,
        moe_runner_config: MoeRunnerConfig,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]])
         ‚Üí torch.Tensor
         üìù This function computes a Mixture of Experts (MoE) layer using two sets


============================================================
FILE: python/sglang/srt/layers/moe/fused_moe_triton/layer.py
Functions: 14
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L1057: def get_fused_moe_impl_class()
         üìù Factory function to get the appropriate FusedMoE implementation class.


CLASS: FlashInferFP4MoE
----------------------------------------
  L 967: __init__(self)

  L1000: forward(self, hidden_states: torch.Tensor, topk_output: TopKOutput)
         üìù Forward pass using FP4 TRTLLM kernel.


CLASS: FlashInferFusedMoE
----------------------------------------
  L 931: __init__(self)

  L 935: forward(self, hidden_states: torch.Tensor, topk_output: TopKOutput)


CLASS: FusedMoE
----------------------------------------
  L 119: __init__(self, num_experts: int, hidden_size: int, intermediate_size: int, layer_id: int, top_k: Optional[int], num_fused_shared_experts: int, params_dtype: Optional[torch.dtype], reduce_results: bool, quant_config: Optional[QuantizationConfig], prefix: str, activation: str, apply_router_weight_on_input: bool, use_presharded_weights: bool, inplace: bool, no_combine: bool, routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_clamp_limit: Optional[float], use_weight_loader_fused: bool, with_bias)

  L 459: weight_loader(self, param: torch.nn.Parameter, loaded_weight: torch.Tensor, weight_name: str, shard_id: str, expert_id: Optional[int])
         ‚Üí None

  L 716: weight_loader_fused(self, param: torch.nn.Parameter, loaded_weight: torch.Tensor, weight_name: str, shard_id: str)
         ‚Üí None

  L 794: forward(self, hidden_states: torch.Tensor, topk_output: TopKOutput)

  L 832: make_expert_params_mapping(cls, ckpt_gate_proj_name: str, ckpt_down_proj_name: str, ckpt_up_proj_name: str, num_experts: int)
         ‚Üí List[Tuple[str, str, int, str]]

  L 861: make_expert_params_mapping_fused(cls, ckpt_gate_up_proj_name: str, ckpt_down_proj_name: str, ckpt_gate_up_proj_bias_name: str, ckpt_down_proj_bias_name: str)

  L 880: make_expert_params_mapping_fused_mxfp4(cls, ckpt_gate_up_proj_name: str, ckpt_down_proj_name: str, ckpt_gate_up_proj_bias_name: str, ckpt_down_proj_bias_name: str, ckpt_gate_up_proj_scale_name: str, ckpt_down_proj_scale_name: str)

  L 907: make_expert_input_scale_params_mapping(cls, num_experts: int)
         ‚Üí List[Tuple[str, str, int, str]]

  L 923: should_fuse_routed_scaling_factor_in_topk(self)


============================================================
FILE: python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py
Functions: 5
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  25: def quantize(w, dtype, dev)

  L  54: def triton_kernel_moe_forward(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_output: TopKOutput,
        moe_runner_config: MoeRunnerConfig,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        per_channel_quant: bool,
        global_num_experts: int,
        expert_map: Optional[torch.Tensor],
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[list[int]])
         ‚Üí torch.Tensor

  L 101: def triton_kernel_fused_experts(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        routing_data: RoutingData,
        gather_indx: GatherIndx,
        scatter_indx: ScatterIndx,
        inplace: bool,
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        per_channel_quant: bool,
        global_num_experts: int,
        expert_map: Optional[torch.Tensor],
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[list[int]])
         ‚Üí torch.Tensor

  L 189: def triton_kernel_moe_with_bias_forward(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w1_pcg,
        b1: torch.Tensor,
        w2: torch.Tensor,
        w2_pcg,
        b2: torch.Tensor,
        topk_output: TopKOutput,
        moe_runner_config: MoeRunnerConfig,
        use_fp8_w8a8: bool,
        per_channel_quant: bool,
        global_num_experts: int,
        expert_map: Optional[torch.Tensor],
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[list[int]])
         ‚Üí torch.Tensor

  L 242: def triton_kernel_fused_experts_with_bias(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w1_pcg,
        b1: torch.Tensor,
        w2: torch.Tensor,
        w2_pcg,
        b2: torch.Tensor,
        routing_data: RoutingData,
        gather_indx: GatherIndx,
        scatter_indx: ScatterIndx,
        inplace: bool,
        activation: str,
        use_fp8_w8a8: bool,
        per_channel_quant: bool,
        global_num_experts: int,
        expert_map: Optional[torch.Tensor],
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[list[int]],
        gemm1_alpha: Optional[float],
        gemm1_clamp_limit: Optional[float])
         ‚Üí torch.Tensor
