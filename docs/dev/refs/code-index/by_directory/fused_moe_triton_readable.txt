================================================================================
FUNCTION INDEX: fused_moe_triton module
================================================================================
Total Functions: 41
Documented: 7


============================================================
FILE: python/sglang/srt/layers/moe/fused_moe_triton/__init__.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  19: def override_config(config)
         @contextmanager

  L  27: def get_config()
         ‚Üí Optional[Dict[str, Any]]


============================================================
FILE: python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
Functions: 20
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  72: def write_zeros_to_output(c_ptr,
        stride_cm,
        stride_cn,
        pid_n,
        N,
        offs_token,
        token_mask,
        BLOCK_SIZE_M,
        BLOCK_SIZE_N,
        compute_type)
         @triton.jit

  L  92: def fused_moe_kernel_gptq_awq(a_ptr,
        b_ptr,
        c_ptr,
        b_scale_ptr,
        b_zp_ptr,
        topk_weights_ptr,
        sorted_token_ids_ptr,
        expert_ids_ptr,
        num_tokens_post_padded_ptr,
        N: tl.constexpr,
        K: tl.constexpr,
        EM,
        num_valid_tokens,
        stride_am,
        stride_ak,
        stride_be,
        stride_bk,
        stride_bn,
        stride_cm,
        stride_cn,
        stride_bse,
        stride_bsk,
        stride_bsn,
        stride_bze,
        stride_bzk,
        stride_bzn,
        group_size: tl.constexpr,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        BLOCK_SIZE_K: tl.constexpr,
        GROUP_SIZE_M: tl.constexpr,
        MUL_ROUTED_WEIGHT: tl.constexpr,
        top_k: tl.constexpr,
        compute_type: tl.constexpr,
        has_zp: tl.constexpr,
        use_int4_w4a16: tl.constexpr,
        use_int8_w8a16: tl.constexpr,
        even_Ks: tl.constexpr)
         üìù Implements the fused computation for a Mixture of Experts (MOE) using
            token and expert matrices.
            Key Parameters:
            - A: The input tensor representing tokens with shape (*, K), where '*' can
            be any shape representing batches and K is the feature dimension of
            each token.
            - B: The stacked MOE weight tensor with shape (E, N, K), where E is
            the number of experts, K is the input feature dimension, and N is
            the output feature dimension.
            - C: The output cache tensor with shape (M, topk, N), where M is the
            total number of tokens post padding, topk is the number of times
            each token is repeated, and N is the output feature dimension.
            - sorted_token_ids: A tensor containing the sorted indices of tokens,
            repeated topk times and arranged by the expert index they are
            assigned to.
            - expert_ids: A tensor containing the indices of the expert for each
            block. It determines which expert matrix from B should be used for
            each block in A.
            This kernel performs the multiplication of a token by its corresponding
            expert matrix as determined by `expert_ids`. The sorting of
            `sorted_token_ids` by expert index and padding ensures divisibility by
            BLOCK_SIZE_M, which is necessary to maintain consistency in block matrix
            multiplication across different blocks processed by the same expert.
         @triton.jit

  L 323: def fused_moe_kernel(a_ptr,
        b_ptr,
        bias_ptr,
        c_ptr,
        a_scale_ptr,
        b_scale_ptr,
        topk_weights_ptr,
        sorted_token_ids_ptr,
        expert_ids_ptr,
        num_tokens_post_padded_ptr,
        N,
        K,
        EM,
        num_valid_tokens,
        stride_am,
        stride_ak,
        stride_be,
        stride_bk,
        stride_bn,
        stride_bias_e,
        stride_bias_n,
        stride_cm,
        stride_cn,
        stride_asm,
        stride_ask,
        stride_bse,
        stride_bsk,
        stride_bsn,
        group_n: tl.constexpr,
        group_k: tl.constexpr,
        BLOCK_SIZE_M: tl.constexpr,
        BLOCK_SIZE_N: tl.constexpr,
        BLOCK_SIZE_K: tl.constexpr,
        GROUP_SIZE_M: tl.constexpr,
        MUL_ROUTED_WEIGHT: tl.constexpr,
        top_k: tl.constexpr,
        compute_type: tl.constexpr,
        use_fp8_w8a8: tl.constexpr,
        use_int8_w8a8: tl.constexpr,
        use_int8_w8a16: tl.constexpr,
        per_channel_quant: tl.constexpr,
        even_Ks: tl.constexpr)
         üìù Implements the fused computation for a Mixture of Experts (MOE) using
            token and expert matrices.
            Key Parameters:
            - A: The input tensor representing tokens with shape (*, K), where '*' can
            be any shape representing batches and K is the feature dimension of
            each token.
            - B: The stacked MOE weight tensor with shape (E, N, K), where E is
            the number of experts, K is the input feature dimension, and N is
            the output feature dimension.
            - C: The output cache tensor with shape (M, topk, N), where M is the
            total number of tokens post padding, topk is the number of times
            each token is repeated, and N is the output feature dimension.
            - sorted_token_ids: A tensor containing the sorted indices of tokens,
            repeated topk times and arranged by the expert index they are
            assigned to.
            - expert_ids: A tensor containing the indices of the expert for each
            block. It determines which expert matrix from B should be used for
            each block in A.
            This kernel performs the multiplication of a token by its corresponding
            expert matrix as determined by `expert_ids`. The sorting of
            `sorted_token_ids` by expert index and padding ensures divisibility by
            BLOCK_SIZE_M, which is necessary to maintain consistency in block matrix
            multiplication across different blocks processed by the same expert.
         @triton.jit

  L 563: def moe_align_block_size(topk_ids: torch.Tensor,
        block_size: int,
        num_experts: int)
         ‚Üí Tuple[torch.Tensor, torch.Tensor, torch.Tensor]
         üìù Aligns the token distribution across experts to be compatible with block
            size for matrix multiplication.
            Parameters:
            - topk_ids: A tensor of shape [total_tokens, top_k] representing the
            top-k expert indices for each token.
            - block_size: The block size used in block matrix multiplication.
            - num_experts: The total number of experts.
            Returns:
            - sorted_token_ids: A tensor containing the sorted token indices according
            to their allocated expert.
            - expert_ids: A tensor indicating the assigned expert index for each block.
            - num_tokens_post_padded: The total number of tokens after padding,
            ensuring divisibility by block_size.
            This function pads the number of tokens that each expert needs to process
            so that it is divisible by block_size.
            Padding ensures that during block matrix multiplication, the dimensions
            align correctly.
            Example:
            Given topk_ids = [[2, 3, 4], [1, 2, 4], [1, 3, 4], [1, 2, 3]],
            block_size = 4, and num_experts = 4:
            - We initially have 12 tokens (after repeating 'top_k' times) and 4 experts,
            with each expert needing to process 3 tokens.
            - As block_size is 4, we pad 1 token for each expert.
            - First, flatten topk_ids to [2, 3, 4, 1, 2, 4, 1, 3, 4, 1, 2, 3].
            - Then append padding tokens [12, 12, 12, 12] for each block.
            - After sorting by expert index, we obtain token_ids
            [3, 6, 9, 12, 0, 4, 10, 12, 1, 7, 11, 12, 2, 5, 8, 12].
            Tokens 12 are non-existent (padding) and are ignored in
            the subsequent matrix multiplication.
            - The padding ensures that the total number of tokens is now divisible
            by block_size for proper block matrix operations.

  L 636: def invoke_fused_moe_kernel(A: torch.Tensor,
        B: torch.Tensor,
        bias: Optional[torch.Tensor],
        C: torch.Tensor,
        A_scale: Optional[torch.Tensor],
        B_scale: Optional[torch.Tensor],
        B_zp: Optional[torch.Tensor],
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        sorted_token_ids: torch.Tensor,
        expert_ids: torch.Tensor,
        num_tokens_post_padded: torch.Tensor,
        mul_routed_weight: bool,
        top_k: int,
        config: Dict[str,
        Any],
        compute_type: tl.dtype,
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        block_shape: Optional[List[int]],
        no_combine: bool)
         ‚Üí None

  L 813: def get_config_file_name(E: int,
        N: int,
        dtype: Optional[str],
        block_shape: Optional[int])
         ‚Üí str

  L 825: def get_moe_configs(E: int,
        N: int,
        dtype: Optional[str],
        block_n: Optional[int],
        block_k: Optional[int])
         ‚Üí Optional[Dict[int, Any]]
         üìù Return optimized configurations for the fused MoE kernel.
            The return value will be a dictionary that maps an irregular grid of
            batch sizes to configurations of the fused_moe kernel. To evaluate the
            kernel on a given batch size bs, the closest batch size in the grid should
            be picked and the associated configuration chosen to invoke the kernel.
         @functools.lru_cache

  L 898: def get_default_config(M: int,
        E: int,
        N: int,
        K: int,
        topk: int,
        dtype: Optional[str],
        is_marlin: bool,
        block_shape: Optional[List[int]])
         ‚Üí Dict[str, int]

  L 955: def try_get_optimal_moe_config(w1_shape: Tuple[int,
        ...],
        w2_shape: Tuple[int,
        ...],
        top_k: int,
        dtype: Optional[str],
        M: int,
        is_marlin: bool,
        block_shape: Optional[List[int]])

  L 988: def get_config_dtype_str(dtype: torch.dtype,
        use_int8_w8a16: Optional[bool],
        use_int4_w4a16: Optional[bool],
        use_fp8_w8a8: Optional[bool],
        use_int8_w8a8: Optional[bool])

  L1010: def inplace_fused_experts(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]],
        routed_scaling_factor: Optional[float],
        gemm1_alpha: Optional[float],
        gemm1_limit: Optional[float])
         ‚Üí None

  L1066: def inplace_fused_experts_fake(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]],
        routed_scaling_factor: Optional[float],
        gemm1_alpha: Optional[float],
        gemm1_limit: Optional[float])
         ‚Üí None

  L1103: def outplace_fused_experts(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]],
        no_combine: bool,
        routed_scaling_factor: Optional[float],
        gemm1_alpha: Optional[float],
        gemm1_limit: Optional[float])
         ‚Üí torch.Tensor

  L1160: def outplace_fused_experts_fake(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]],
        no_combine: bool,
        routed_scaling_factor: Optional[float],
        gemm1_alpha: Optional[float],
        gemm1_limit: Optional[float])
         ‚Üí torch.Tensor

  L1198: def fused_experts(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_output: StandardTopKOutput,
        moe_runner_config: MoeRunnerConfig,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]])

  L1329: def moe_sum_reduce_triton(input: torch.Tensor,
        output: torch.Tensor,
        routed_scaling_factor: float)

  L1366: def moe_sum_reduce_torch_compile(x, out, routed_scaling_factor)
         @torch.compile

  L1372: def swiglu_with_alpha_and_limit(x, gemm1_alpha, gemm1_limit)
         @torch.compile

  L1379: def fused_experts_impl(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_weights: torch.Tensor,
        topk_ids: torch.Tensor,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        inplace: bool,
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]],
        no_combine: bool,
        routed_scaling_factor: Optional[float],
        gemm1_alpha: Optional[float],
        gemm1_limit: Optional[float])

  L1646: def fused_moe(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_output: StandardTopKOutput,
        moe_runner_config: MoeRunnerConfig,
        b1: Optional[torch.Tensor],
        b2: Optional[torch.Tensor],
        use_fp8_w8a8: bool,
        use_int8_w8a8: bool,
        use_int8_w8a16: bool,
        use_int4_w4a16: bool,
        per_channel_quant: bool,
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        w1_zp: Optional[torch.Tensor],
        w2_zp: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[List[int]])
         ‚Üí torch.Tensor
         üìù This function computes a Mixture of Experts (MoE) layer using two sets of
            weights, w1 and w2, and top-k gating mechanism.
            Parameters:
            - hidden_states (torch.Tensor): The input tensor to the MoE layer.
            - w1 (torch.Tensor): The first set of expert weights.
            - w2 (torch.Tensor): The second set of expert weights.
            - topk_output (StandardTopKOutput): The top-k output of the experts.
            - moe_runner_config (MoeRunnerConfig): The configuration for the MoE runner.
            - b1 (Optional[torch.Tensor]): Optional bias for w1.
            - b2 (Optional[torch.Tensor]): Optional bias for w2.
            - use_fp8_w8a8 (bool): If True, use fp8 arithmetic to compute the inner
            products for w1 and w2. Defaults to False.
            - use_int8_w8a8 (bool): If True, use int8 arithmetic to compute the inner
            products for w1 and w2. Defaults to False.
            - use_int8_w8a16 (bool): If True, use fp8 arithmetic to compute the inner
            products for w1 and w2. Defaults to False.
            - use_int4_w4a16 (bool): If True, use matmul of int4 weight and bf16/fp16
            activation to compute the inner products for w1 and w2.
            Defaults to False.
            - w1_scale (Optional[torch.Tensor]): Optional scale to be used for
            w1.
            - w2_scale (Optional[torch.Tensor]): Optional scale to be used for
            w2.
            - a1_scale (Optional[torch.Tensor]): Optional scale to be used for
            a1.
            - a2_scale (Optional[torch.Tensor]): Optional scale to be used for
            a2.
            - block_shape: (Optional[List[int]]): Optional block size for block-wise
            quantization.
            - gemm1_alpha (Optional[float]): Optional gemm1_alpha for the activation
            function.
            - gemm1_limit (Optional[float]): Optional gemm1_limit for the swiglu activation
            function.
            Returns:
            - torch.Tensor: The output tensor after applying the MoE layer.


============================================================
FILE: python/sglang/srt/layers/moe/fused_moe_triton/layer.py
Functions: 14
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L1057: def get_fused_moe_impl_class()
         üìù Factory function to get the appropriate FusedMoE implementation class.


CLASS: FlashInferFP4MoE
----------------------------------------
  L 967: __init__(self)

  L1000: forward(self, hidden_states: torch.Tensor, topk_output: TopKOutput)
         üìù Forward pass using FP4 TRTLLM kernel.
            Args:
            hidden_states: Input tensor
            topk_output: TopKOutput object with Bypassed format


CLASS: FlashInferFusedMoE
----------------------------------------
  L 931: __init__(self)

  L 935: forward(self, hidden_states: torch.Tensor, topk_output: TopKOutput)


CLASS: FusedMoE
----------------------------------------
  L 119: __init__(self, num_experts: int, hidden_size: int, intermediate_size: int, layer_id: int, top_k: Optional[int], num_fused_shared_experts: int, params_dtype: Optional[torch.dtype], reduce_results: bool, quant_config: Optional[QuantizationConfig], prefix: str, activation: str, apply_router_weight_on_input: bool, use_presharded_weights: bool, inplace: bool, no_combine: bool, routed_scaling_factor: Optional[float], gemm1_alpha: Optional[float], gemm1_clamp_limit: Optional[float], use_weight_loader_fused: bool, with_bias)

  L 459: weight_loader(self, param: torch.nn.Parameter, loaded_weight: torch.Tensor, weight_name: str, shard_id: str, expert_id: Optional[int])
         ‚Üí None

  L 716: weight_loader_fused(self, param: torch.nn.Parameter, loaded_weight: torch.Tensor, weight_name: str, shard_id: str)
         ‚Üí None

  L 794: forward(self, hidden_states: torch.Tensor, topk_output: TopKOutput)

  L 832: make_expert_params_mapping(cls, ckpt_gate_proj_name: str, ckpt_down_proj_name: str, ckpt_up_proj_name: str, num_experts: int)
         ‚Üí List[Tuple[str, str, int, str]]

  L 861: make_expert_params_mapping_fused(cls, ckpt_gate_up_proj_name: str, ckpt_down_proj_name: str, ckpt_gate_up_proj_bias_name: str, ckpt_down_proj_bias_name: str)

  L 880: make_expert_params_mapping_fused_mxfp4(cls, ckpt_gate_up_proj_name: str, ckpt_down_proj_name: str, ckpt_gate_up_proj_bias_name: str, ckpt_down_proj_bias_name: str, ckpt_gate_up_proj_scale_name: str, ckpt_down_proj_scale_name: str)

  L 907: make_expert_input_scale_params_mapping(cls, num_experts: int)
         ‚Üí List[Tuple[str, str, int, str]]

  L 923: should_fuse_routed_scaling_factor_in_topk(self)


============================================================
FILE: python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py
Functions: 5
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  25: def quantize(w, dtype, dev)

  L  54: def triton_kernel_moe_forward(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        topk_output: TopKOutput,
        moe_runner_config: MoeRunnerConfig,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        per_channel_quant: bool,
        global_num_experts: int,
        expert_map: Optional[torch.Tensor],
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[list[int]])
         ‚Üí torch.Tensor

  L 101: def triton_kernel_fused_experts(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w2: torch.Tensor,
        routing_data: RoutingData,
        gather_indx: GatherIndx,
        scatter_indx: ScatterIndx,
        inplace: bool,
        activation: str,
        apply_router_weight_on_input: bool,
        use_fp8_w8a8: bool,
        per_channel_quant: bool,
        global_num_experts: int,
        expert_map: Optional[torch.Tensor],
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[list[int]])
         ‚Üí torch.Tensor

  L 189: def triton_kernel_moe_with_bias_forward(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w1_pcg,
        b1: torch.Tensor,
        w2: torch.Tensor,
        w2_pcg,
        b2: torch.Tensor,
        topk_output: TopKOutput,
        moe_runner_config: MoeRunnerConfig,
        use_fp8_w8a8: bool,
        per_channel_quant: bool,
        global_num_experts: int,
        expert_map: Optional[torch.Tensor],
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[list[int]])
         ‚Üí torch.Tensor

  L 242: def triton_kernel_fused_experts_with_bias(hidden_states: torch.Tensor,
        w1: torch.Tensor,
        w1_pcg,
        b1: torch.Tensor,
        w2: torch.Tensor,
        w2_pcg,
        b2: torch.Tensor,
        routing_data: RoutingData,
        gather_indx: GatherIndx,
        scatter_indx: ScatterIndx,
        inplace: bool,
        activation: str,
        use_fp8_w8a8: bool,
        per_channel_quant: bool,
        global_num_experts: int,
        expert_map: Optional[torch.Tensor],
        w1_scale: Optional[torch.Tensor],
        w2_scale: Optional[torch.Tensor],
        a1_scale: Optional[torch.Tensor],
        a2_scale: Optional[torch.Tensor],
        block_shape: Optional[list[int]],
        gemm1_alpha: Optional[float],
        gemm1_clamp_limit: Optional[float])
         ‚Üí torch.Tensor
