================================================================================
FUNCTION INDEX: layers module
================================================================================
Total Functions: 266
Documented: 30


============================================================
FILE: python/sglang/srt/layers/activation.py
Functions: 18
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 191: def get_act_fn(act_fn_name: str,
        quant_config: Optional[QuantizationConfig],
        intermediate_size: Optional[int],
        input_is_parallel: bool,
        params_dtype: Optional[torch.dtype])
         ‚Üí nn.Module
         üìù Get an activation function by name.

  L 216: def get_cross_encoder_activation_function(config: PretrainedConfig)


CLASS: GeluAndMul
----------------------------------------
  L  86: __init__(self, approximate)

  L  90: forward_native(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  94: forward_cuda(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: NewGELU
----------------------------------------
  L 108: forward_native(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L 112: forward_cuda(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: QuickGELU
----------------------------------------
  L 129: forward_native(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L 132: forward_cuda(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L 135: forward_hip(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: ReLU2
----------------------------------------
  L 123: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: ScaledActivation
----------------------------------------
  L 147: __init__(self, act_module: nn.Module, intermediate_size: int, input_is_parallel: bool, params_dtype: Optional[torch.dtype])

  L 169: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L 172: weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor)


CLASS: SiluAndMul
----------------------------------------
  L  60: forward_native(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  64: forward_cuda(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  71: forward_cpu(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  80: forward_npu(self, x: torch.Tensor)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/amx_utils.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  10: def amx_process_weight_after_loading(weight)

  L  22: def dim_is_supported(weight)


CLASS: PackWeightMethod
----------------------------------------
  L  79: __init__(self, weight_names, transpose_dims)

  L  83: process_weights_after_loading(self, module)
         ‚Üí None


============================================================
FILE: python/sglang/srt/layers/communicator.py
Functions: 16
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 155: def enable_moe_dense_fully_dp()


CLASS: CommunicateContext
----------------------------------------
  L 315: is_same_group_size(self, a: ScatterMode, b: ScatterMode)

  L 319: init_new(cls)


CLASS: CommunicateSimpleFn
----------------------------------------
  L 341: get_fn(input_mode: ScatterMode, output_mode: ScatterMode, context: CommunicateContext)


CLASS: CommunicateSummableTensorPairFn
----------------------------------------
  L 527: execute(cls, hidden_states_input_mode, residual_input_mode, output_mode, context)

  L 543: get_fn(hidden_states_input_mode: ScatterMode, residual_input_mode: ScatterMode, output_mode: ScatterMode, context: CommunicateContext)


CLASS: CommunicateWithAllReduceAndLayerNormFn
----------------------------------------
  L 388: get_fn(hidden_states_input_mode: ScatterMode, residual_input_mode: ScatterMode, hidden_states_output_mode: ScatterMode, residual_output_mode: ScatterMode, context: CommunicateContext)


CLASS: LayerCommunicator
----------------------------------------
  L 160: __init__(self, layer_scatter_modes: LayerScatterModes, input_layernorm: torch.nn.Module, post_attention_layernorm: torch.nn.Module, allow_reduce_scatter: bool, is_last_layer: bool)

  L 199: prepare_attn(self, hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch)

  L 235: prepare_mlp(self, hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch)

  L 249: postprocess_layer(self, hidden_states: torch.Tensor, residual: torch.Tensor, forward_batch: ForwardBatch)

  L 263: should_use_reduce_scatter(self, forward_batch: ForwardBatch)

  L 271: should_fuse_mlp_allreduce_with_next_layer(self, forward_batch: ForwardBatch)
         ‚Üí bool


CLASS: LayerScatterModes
----------------------------------------
  L  99: init_new(cls)


CLASS: ScatterMode
----------------------------------------
  L  67: model_input_output()
         üìù The scatter mode for model forward pass input and output data


CLASS: _LayerModeComputationContext
----------------------------------------
  L  79: previous_layer(self)


============================================================
FILE: python/sglang/srt/layers/dp_attention.py
Functions: 40
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 123: def set_dp_buffer_len(global_dp_buffer_len: int,
        local_dp_buffer_len: int,
        global_num_tokens: Optional[List[int]])

  L 133: def get_global_dp_buffer()
         ‚Üí torch.Tensor

  L 137: def get_local_dp_buffer()
         ‚Üí torch.Tensor

  L 141: def get_global_dp_buffer_len()
         ‚Üí int

  L 145: def get_local_dp_buffer_len()
         ‚Üí int

  L 149: def get_dp_global_num_tokens()
         ‚Üí List[int]

  L 153: def compute_dp_attention_world_info(enable_dp_attention,
        tp_rank,
        tp_size,
        dp_size)

  L 164: def compute_dp_attention_local_info(enable_dp_attention,
        tp_rank,
        tp_size,
        dp_size,
        moe_dense_tp_size)

  L 181: def initialize_dp_attention(server_args: ServerArgs, model_config: ModelConfig)

  L 241: def is_dp_attention_enabled()
         ‚Üí bool

  L 245: def get_attention_tp_group()
         ‚Üí GroupCoordinator

  L 250: def get_attention_tp_rank()
         ‚Üí int

  L 255: def get_attention_tp_size()
         ‚Üí int

  L 260: def get_attention_dp_rank()
         ‚Üí int

  L 265: def get_attention_dp_size()
         ‚Üí int

  L 270: def get_local_attention_dp_rank()
         ‚Üí int

  L 275: def get_local_attention_dp_size()
         ‚Üí int

  L 281: def disable_dp_size()
         üìù Patch the tp group temporarily until this function ends.
         @contextmanager

  L 301: def get_dp_local_info(forward_batch: ForwardBatch)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 320: def memcpy_triton_kernel(dst_ptr,
        src_ptr,
        offset_ptr,
        sz_ptr,
        offset_src: tl.constexpr,
        chunk_size,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 345: def prod(x)

  L 349: def memcpy_triton(dst, src, dim, offset, sz, offset_src)

  L 431: def dp_gather_partial(global_tokens: torch.Tensor,
        local_tokens: torch.Tensor,
        forward_batch: ForwardBatch)

  L 439: def dp_gather_replicate(global_tokens: torch.Tensor,
        local_tokens: torch.Tensor,
        forward_batch: ForwardBatch)

  L 447: def dp_scatter(local_tokens: torch.Tensor,
        global_tokens: torch.Tensor,
        forward_batch: ForwardBatch)

  L 469: def dp_reduce_scatter_tensor(output: torch.Tensor, input: torch.Tensor)

  L 480: def attn_tp_reduce_scatter_tensor(output: torch.Tensor, input: torch.Tensor)

  L 484: def attn_tp_all_gather_into_tensor(output: torch.Tensor, input: torch.Tensor)

  L 488: def attn_tp_all_gather(output_list: List[torch.Tensor], input: torch.Tensor)


CLASS: DpPaddingMode
----------------------------------------
  L  47: is_max_len(self)

  L  50: is_sum_len(self)

  L  54: get_dp_padding_mode(cls, global_num_tokens: List[int])
         ‚Üí DpPaddingMode

  L  64: get_default_mode_in_cuda_graph(cls)
         ‚Üí DpPaddingMode


CLASS: _DpGatheredBufferWrapper
----------------------------------------
  L  78: set_metadata(cls, hidden_size: int, dtype: torch.dtype, device: torch.device)

  L  84: set_dp_buffer_len(cls, global_dp_buffer_len: int, local_dp_buffer_len: int, global_num_tokens: Optional[List[int]])

  L  95: get_global_dp_buffer(cls)
         ‚Üí torch.Tensor

  L 103: get_local_dp_buffer(cls)
         ‚Üí torch.Tensor

  L 111: get_global_dp_buffer_len(cls)
         ‚Üí int

  L 115: get_local_dp_buffer_len(cls)
         ‚Üí int

  L 119: get_dp_global_num_tokens(cls)
         ‚Üí List[int]


============================================================
FILE: python/sglang/srt/layers/elementwise.py
Functions: 23
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  37: def fused_softcap_kernel(output_ptr,
        input_ptr,
        n_ele,
        softcap_const: tl.constexpr,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L  61: def fused_softcap(x, softcap_const, autotune)

  L 138: def fused_dual_residual_rmsnorm_kernel(output_ptr,
        mid_ptr,
        activ_ptr,
        residual_ptr,
        weight1_ptr,
        weight2_ptr,
        eps: tl.constexpr,
        hidden_dim: tl.constexpr,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 188: def fused_dual_residual_rmsnorm(x, residual, weight1, weight2, eps, autotune)

  L 222: def fused_rmsnorm_kernel(output_ptr,
        activ_ptr,
        weight_ptr,
        eps: tl.constexpr,
        hidden_dim: tl.constexpr,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 252: def fused_rmsnorm(x, weight, eps, autotune, inplace)

  L 329: def experts_combine_kernel(out_hidden_states,
        moe_hidden_states,
        mlp_hidden_states,
        combine_k: tl.constexpr,
        hidden_dim: tl.constexpr,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 359: def experts_combine_triton(moe_hidden_states, mlp_hidden_states, output_buffer)

  L 399: def gelu_and_mul_kernel(out_hidden_states_ptr,
        out_scales_ptr,
        hidden_states_ptr,
        quant_max: tl.constexpr,
        static_scale: tl.constexpr,
        hidden_dim: tl.constexpr,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 436: def gelu_and_mul_triton(hidden_states, scales, quantize, out)

  L 493: def silu_and_mul_kernel(out_hidden_states_ptr,
        out_scales_ptr,
        hidden_states_ptr,
        quant_max: tl.constexpr,
        static_scale: tl.constexpr,
        hidden_dim: tl.constexpr,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 530: def silu_and_mul_triton(hidden_states, scales, quantize, out)


CLASS: FusedDualResidualRMSNorm
----------------------------------------
  L 279: __init__(self, rmsnorm1, rmsnorm2)
         ‚Üí None

  L 286: __call__(self)

  L 289: forward(self, x: torch.Tensor, residual: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 297: forward_cuda(self, x: torch.Tensor, residual: torch.Tensor, autotune)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 309: forward_flashinfer(self, x: torch.Tensor, residual: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 318: forward_native(self, x: torch.Tensor, residual: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: Softcap
----------------------------------------
  L  76: __init__(self, softcap_const: float)

  L  79: __call__(self)

  L  82: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  88: forward_native(self, x: torch.Tensor)
         ‚Üí torch.Tensor

  L  91: forward_cuda(self, x: torch.Tensor, autotune)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/flashinfer_comm_fusion.py
Functions: 7
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  98: def ensure_workspace_initialized(max_token_num: int,
        hidden_dim: int,
        use_fp32_lamport: bool)
         üìù Ensure workspace is initialized

  L 126: def flashinfer_allreduce_residual_rmsnorm(input_tensor: torch.Tensor,
        residual: torch.Tensor,
        weight: torch.Tensor,
        eps: float,
        max_token_num: int,
        use_oneshot: Optional[bool],
        trigger_completion_at_end: bool,
        fp32_acc: bool)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù Use FlashInfer's fused allreduce + residual + RMS norm operation

  L 203: def fake_flashinfer_allreduce_residual_rmsnorm(input_tensor: torch.Tensor,
        residual: torch.Tensor,
        weight: torch.Tensor,
        eps: float,
        max_token_num: int,
        use_oneshot: Optional[bool],
        trigger_completion_at_end: bool,
        fp32_acc: bool)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 227: def cleanup_flashinfer_workspace()


CLASS: FlashInferWorkspaceManager
----------------------------------------
  L  32: __init__(self)

  L  39: initialize(self, world_size: int, rank: int, max_token_num: int, hidden_dim: int, group, use_fp32_lamport: bool)
         üìù Initialize workspace

  L  80: cleanup(self)
         üìù Clean up workspace


============================================================
FILE: python/sglang/srt/layers/layernorm.py
Functions: 14
============================================================


CLASS: Gemma3RMSNorm
----------------------------------------
  L 271: __init__(self, dim: int, eps: float)

  L 279: forward(self, x)

  L 286: extra_repr(self)


CLASS: GemmaRMSNorm
----------------------------------------
  L 226: __init__(self, hidden_size: int, eps: float)
         ‚Üí None

  L 239: forward_native(self, x: torch.Tensor, residual: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]

  L 256: forward_cuda(self, x: torch.Tensor, residual: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]


CLASS: RMSNorm
----------------------------------------
  L  61: __init__(self, hidden_size: int, eps: float, var_hidden_size: Optional[int])
         ‚Üí None

  L  77: forward_cuda(self, x: torch.Tensor, residual: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]

  L  90: forward_npu(self, x: torch.Tensor, residual: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]

  L 102: forward_aiter(self, x: torch.Tensor, residual: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]

  L 121: forward_hip(self, x: torch.Tensor, residual: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]

  L 136: forward_native(self, x: torch.Tensor, residual: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]

  L 175: forward_cpu(self, x: torch.Tensor, residual: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]

  L 192: forward_with_allreduce_fusion(self, x: torch.Tensor, residual: Optional[torch.Tensor])
         ‚Üí Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]
         üìù Forward method with allreduce fusion, prioritizing flashinfer fused op


============================================================
FILE: python/sglang/srt/layers/linear.py
Functions: 26
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  67: def adjust_marlin_shard(param, shard_size, shard_offset)

  L  75: def adjust_bitsandbytes_4bit_shard(param: Parameter,
        shard_offsets: Dict[str,
        Tuple[int,
        int]],
        loaded_shard_id: str)
         ‚Üí Tuple[int, int]
         üìù Adjust the quantization offsets and sizes for BitsAndBytes sharding.

  L  90: def adjust_scalar_to_fused_array(param, loaded_weight, shard_id)
         üìù For fused modules (QKV and MLP) we have an array of length

  L 113: def adjust_shard_offsets(shard_offsets, loaded_weight, dim)


CLASS: ColumnParallelLinear
----------------------------------------
  L 280: __init__(self, input_size: int, output_size: int, bias: bool, gather_output: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], output_sizes: Optional[List[int]], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], use_presharded_weights: bool)

  L 347: weight_loader(self, param: Parameter, loaded_weight: torch.Tensor)

  L 397: weight_loader_v2(self, param: Parameter, loaded_weight: torch.Tensor)

  L 415: forward(self, input_)

  L 429: extra_repr(self)
         ‚Üí str


CLASS: LinearBase
----------------------------------------
  L 139: __init__(self, input_size: int, output_size: int, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str)

  L 162: forward(self, x: torch.Tensor)
         ‚Üí torch.Tensor


CLASS: MergedColumnParallelLinear
----------------------------------------
  L 461: __init__(self, input_size: int, output_sizes: List[int], bias: bool, gather_output: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], use_presharded_weights: bool)

  L 498: weight_loader(self, param: Parameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[int])

  L 693: weight_loader_v2(self, param: BasevLLMParameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[int])


CLASS: QKVParallelLinear
----------------------------------------
  L 773: __init__(self, hidden_size: int, head_size: int, total_num_heads: int, total_num_kv_heads: Optional[int], bias: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], load_presharded_attn: bool)

  L 896: weight_loader_v2(self, param: BasevLLMParameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[str])

  L 934: weight_loader(self, param: Parameter, loaded_weight: torch.Tensor, loaded_shard_id: Optional[str])


CLASS: ReplicatedLinear
----------------------------------------
  L 180: __init__(self, input_size: int, output_size: int, bias: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], prefix: str)

  L 225: weight_loader(self, param: Parameter, loaded_weight: torch.Tensor)

  L 242: forward(self, x: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, Optional[torch.Tensor]]

  L 249: extra_repr(self)
         ‚Üí str


CLASS: RowParallelLinear
----------------------------------------
  L1173: __init__(self, input_size: int, output_size: int, bias: bool, input_is_parallel: bool, skip_bias_add: bool, params_dtype: Optional[torch.dtype], reduce_results: bool, quant_config: Optional[QuantizationConfig], prefix: str, tp_rank: Optional[int], tp_size: Optional[int], use_presharded_weights: bool)

  L1231: weight_loader(self, param: Parameter, loaded_weight: torch.Tensor)

  L1283: weight_loader_v2(self, param: BasevLLMParameter, loaded_weight: torch.Tensor)

  L1304: forward(self, input_, skip_all_reduce)

  L1331: extra_repr(self)
         ‚Üí str


============================================================
FILE: python/sglang/srt/layers/logits_processor.py
Functions: 9
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 607: def fused_softcap_kernel(full_logits_ptr,
        softcapping_value,
        n_elements,
        BLOCK_SIZE: tl.constexpr)
         @triton.jit

  L 634: def fused_softcap(full_logits, final_logit_softcapping)


CLASS: LogitsMetadata
----------------------------------------
  L 123: from_forward_batch(cls, forward_batch: ForwardBatch)

  L 173: compute_dp_attention_metadata(self)


CLASS: LogitsProcessor
----------------------------------------
  L 202: __init__(self, config, skip_all_gather: bool, logit_scale: Optional[float])

  L 235: forward(self, input_ids, hidden_states, lm_head: VocabParallelEmbedding, logits_metadata: Union[LogitsMetadata, ForwardBatch], aux_hidden_states: Optional[torch.Tensor])
         ‚Üí LogitsProcessorOutput

  L 525: get_top_logprobs(all_logprobs: torch.Tensor, logits_metadata: LogitsMetadata)

  L 554: get_token_ids_logprobs(all_logprobs: torch.Tensor, logits_metadata: LogitsMetadata)

  L 577: compute_temp_top_p_normalized_logprobs(last_logits: torch.Tensor, logits_metadata: LogitsMetadata)
         ‚Üí torch.Tensor
         üìù compute logprobs for the output token from the given logits.


============================================================
FILE: python/sglang/srt/layers/multimodal.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  44: def hash_tiles32_kernel_blocked(in_ptr,
        out_ptr,
        n_u32,
        seed1,
        seed2,
        FM_C1: tl.constexpr,
        FM_C2: tl.constexpr,
        POS_A: tl.constexpr,
        POS_B: tl.constexpr,
        TILE: tl.constexpr,
        BLOCK: tl.constexpr,
        USE_CG: tl.constexpr)
         @triton.jit

  L 108: def add_tree_reduce_u64_kernel(in_ptr, out_ptr, n_elems, CHUNK: tl.constexpr)
         @triton.jit

  L 145: def gpu_tensor_hash(tensor: torch.Tensor)
         ‚Üí int
         @torch.inference_mode()


============================================================
FILE: python/sglang/srt/layers/parameter.py
Functions: 31
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 460: def permute_param_layout_(param: BasevLLMParameter,
        input_dim: int,
        output_dim: int)
         ‚Üí BasevLLMParameter
         üìù Permute a parameter's layout to the specified input and output dimensi


CLASS: BasevLLMParameter
----------------------------------------
  L  36: __new__(cls, data: torch.Tensor)

  L  40: __init__(self, data: torch.Tensor, weight_loader: Callable)
         üìù Initialize the BasevLLMParameter

  L  53: weight_loader(self)

  L  60: load_column_parallel_weight(self, loaded_weight: torch.Tensor)

  L  63: load_row_parallel_weight(self, loaded_weight: torch.Tensor)

  L  66: load_merged_column_weight(self, loaded_weight: torch.Tensor)

  L  69: load_qkv_weight(self, loaded_weight: torch.Tensor)


CLASS: PackedColumnParameter
----------------------------------------
  L 383: __init__(self, packed_factor: Union[int, Fraction], packed_dim: int, marlin_tile_size: Optional[int])

  L 396: packed_dim(self)

  L 400: packed_factor(self)

  L 404: marlin_tile_size(self)

  L 407: adjust_shard_indexes_for_packing(self, shard_size, shard_offset)


CLASS: PackedvLLMParameter
----------------------------------------
  L 427: __init__(self, packed_factor: Union[int, Fraction], packed_dim: int, marlin_tile_size: Optional[int])

  L 440: packed_dim(self)

  L 444: packed_factor(self)

  L 448: marlin_tile_size(self)

  L 451: adjust_shard_indexes_for_packing(self, shard_size, shard_offset)


CLASS: PerTensorScaleParameter
----------------------------------------
  L 322: __init__(self)

  L 338: load_row_parallel_weight(self)

  L 343: load_merged_column_weight(self)

  L 346: load_qkv_weight(self)

  L 349: load_column_parallel_weight(self)


CLASS: RowvLLMParameter
----------------------------------------
  L 225: __init__(self, input_dim: int)

  L 230: input_dim(self)

  L 233: load_row_parallel_weight(self, loaded_weight: torch.Tensor, tp_rank: int, use_presharded_weights: bool)


CLASS: _ColumnvLLMParameter
----------------------------------------
  L  84: __init__(self, output_dim: int)

  L  89: output_dim(self)

  L  92: load_column_parallel_weight(self, loaded_weight: torch.Tensor, tp_rank: int, use_presharded_weights: bool)

  L 125: load_merged_column_weight(self, loaded_weight: torch.Tensor)

  L 166: load_qkv_weight(self, loaded_weight: torch.Tensor, tp_rank: int, use_presharded_weights: bool)


============================================================
FILE: python/sglang/srt/layers/pooler.py
Functions: 4
============================================================


CLASS: CrossEncodingPooler
----------------------------------------
  L  71: __init__(self, config: PretrainedConfig, classifier: nn.Module, pooler: Optional[nn.Module])

  L  82: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí EmbeddingPoolerOutput
         üìù Pools sentence pair scores from the hidden_states.


CLASS: Pooler
----------------------------------------
  L  37: __init__(self, pooling_type: PoolingType, normalize: bool)

  L  42: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         ‚Üí EmbeddingPoolerOutput


============================================================
FILE: python/sglang/srt/layers/radix_attention.py
Functions: 2
============================================================


CLASS: RadixAttention
----------------------------------------
  L  44: __init__(self, num_heads: int, head_dim: int, scaling: float, num_kv_heads: int, layer_id: int, logit_cap: float, v_head_dim: int, sliding_window_size: int, is_cross_attention: bool, pos_encoding_mode: str, logit_capping_method: str, quant_config: Optional[QuantizationConfig], attn_type: AttentionType, use_irope: bool, prefix: str)

  L  90: forward(self, q, k, v, forward_batch: ForwardBatch, save_kv_cache: bool)


============================================================
FILE: python/sglang/srt/layers/rotary_embedding.py
Functions: 34
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 646: def yarn_get_mscale(scale: float, mscale: float)
         ‚Üí float

  L1654: def get_rope(head_size: int,
        rotary_dim: int,
        max_position: int,
        base: int,
        is_neox_style: bool,
        rope_scaling: Optional[Dict[str,
        Any]],
        dtype: Optional[torch.dtype],
        partial_rotary_factor: float,
        dual_chunk_attention_config: Optional[Dict[str,
        Any]])
         ‚Üí RotaryEmbedding

  L1872: def rotate_half(x)
         üìù Rotates half the hidden dims of the input.

  L1879: def apply_rotary_pos_emb(q: torch.Tensor,
        k: torch.Tensor,
        cos: torch.Tensor,
        sin: torch.Tensor,
        unsqueeze_dim)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L1902: def get_rope_cpu(head_size: int,
        rotary_dim: int,
        max_position: int,
        base: int,
        is_neox_style: bool,
        rope_scaling: Optional[Dict[str,
        Any]],
        dtype: Optional[torch.dtype],
        partial_rotary_factor: float,
        device: Optional[str])
         ‚Üí RotaryEmbedding

  L1974: def get_rope_wrapper(head_size: int,
        rotary_dim: int,
        max_position: int,
        base: int,
        is_neox_style: bool,
        rope_scaling: Optional[Dict[str,
        Any]],
        dtype: Optional[torch.dtype],
        partial_rotary_factor: float,
        device: Optional[str])


CLASS: DeepseekScalingRotaryEmbedding
----------------------------------------
  L 658: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype)
         ‚Üí None

  L 737: forward_native(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù PyTorch-native implementation equivalent to forward().

  L 778: forward_npu(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 818: forward_cpu(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: DualChunkRotaryEmbedding
----------------------------------------
  L1458: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, chunk_size: int, local_size: int)
         ‚Üí None

  L1560: forward(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L1643: extra_repr(self)
         ‚Üí str


CLASS: DynamicNTKAlphaRotaryEmbedding
----------------------------------------
  L 950: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_alpha: float, dtype: torch.dtype)
         ‚Üí None


CLASS: DynamicNTKScalingRotaryEmbedding
----------------------------------------
  L 357: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype)
         ‚Üí None


CLASS: LinearScalingRotaryEmbedding
----------------------------------------
  L 293: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factors: Union[List[float], float], dtype: torch.dtype)
         ‚Üí None

  L 347: scaling_factor_to_offset(self)
         ‚Üí Dict[float, int]


CLASS: Llama3RotaryEmbedding
----------------------------------------
  L 836: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, scaling_factor: float, low_freq_factor: float, high_freq_factor: float, orig_max_position: int)
         ‚Üí None


CLASS: Llama4VisionRotaryEmbedding
----------------------------------------
  L 883: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype)

  L 926: forward(self, query: torch.Tensor, key: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: MRotaryEmbedding
----------------------------------------
  L 984: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, mrope_section: Optional[List[int]])
         ‚Üí None

  L1033: forward(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù PyTorch-native implementation equivalent to forward().

  L1082: get_rope_index(spatial_merge_size: int, image_token_id: int, video_token_id: int, vision_start_token_id: int, model_type: str, tokens_per_second: Optional[int], input_ids: Optional[torch.LongTensor], image_grid_thw: Optional[torch.LongTensor], video_grid_thw: Optional[torch.LongTensor], second_per_grid_ts: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L1240: get_rope_index_glm4v(input_ids: torch.Tensor, hf_config: Any, image_grid_thw: Union[list[list[int]], torch.Tensor], video_grid_thw: Union[list[list[int]], torch.Tensor], attention_mask: torch.Tensor)
         ‚Üí tuple[torch.Tensor, torch.Tensor]
         üìù Get mrope input positions and delta value for GLM4V.

  L1437: get_next_input_positions(mrope_position_delta: int, context_len: int, seq_len: int)
         ‚Üí torch.Tensor


CLASS: Phi3LongRoPEScaledRotaryEmbedding
----------------------------------------
  L 513: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, original_max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype, short_factor: List[float], long_factor: List[float], short_mscale: Optional[float], long_mscale: Optional[float])

  L 604: forward(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]


CLASS: RotaryEmbedding
----------------------------------------
  L  82: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, dtype: torch.dtype)
         ‚Üí None

  L 139: forward_native(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù A PyTorch-native implementation of forward().

  L 169: forward_npu(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         üìù A PyTorch-npu implementation of forward().

  L 199: forward_cpu(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor])
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 219: forward_cuda(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, offsets: Optional[torch.Tensor], fused_set_kv_buffer_arg)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]

  L 257: extra_repr(self)
         ‚Üí str


CLASS: YaRNScalingRotaryEmbedding
----------------------------------------
  L 444: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype)
         ‚Üí None


============================================================
FILE: python/sglang/srt/layers/sampler.py
Functions: 8
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 157: def top_k_top_p_min_p_sampling_from_probs_torch(probs: torch.Tensor,
        top_ks: torch.Tensor,
        top_ps: torch.Tensor,
        min_ps: torch.Tensor,
        need_min_p_sampling: bool)
         üìù A top-k, top-p and min-p sampling implementation with native pytorch o

  L 184: def sampling_from_probs_torch(probs: torch.Tensor)
         üìù A sampling implementation with native pytorch operations, without

  L 192: def top_p_normalize_probs_torch(probs: torch.Tensor, top_ps: torch.Tensor)

  L 204: def get_top_logprobs(logprobs: torch.Tensor, top_logprobs_nums: List[int])

  L 218: def get_token_ids_logprobs(logprobs: torch.Tensor,
        token_ids_logprobs: List[List[int]])

  L 232: def apply_custom_logit_processor(logits: torch.Tensor,
        sampling_batch_info: SamplingBatchInfo,
        num_tokens_in_batch: int)
         üìù Apply custom logit processors to the logits.


CLASS: Sampler
----------------------------------------
  L  33: __init__(self)

  L  41: forward(self, logits_output: LogitsProcessorOutput, sampling_info: SamplingBatchInfo, return_logprob: bool, top_logprobs_nums: List[int], token_ids_logprobs: List[List[int]])
         üìù Run a sampler & compute logprobs and update logits_output accordingly.


============================================================
FILE: python/sglang/srt/layers/torchao_utils.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  15: def get_gemlite_cache_path()
         ‚Üí str

  L  19: def save_gemlite_cache(print_error: bool)
         ‚Üí bool

  L  31: def proj_filter(module: torch.nn.Module, fqn: str)
         üìù Filter function for quantizing projection layers.

  L  39: def apply_torchao_config_to_model(model: torch.nn.Module,
        torchao_config: str,
        filter_fn: Optional[Callable])
         üìù Quantize a modelwith torchao quantization specified by torchao_config


============================================================
FILE: python/sglang/srt/layers/utils.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  10: def get_layer_id(weight_name)


CLASS: PPMissingLayer
----------------------------------------
  L  25: __init__(self)

  L  29: forward(self)
         üìù Return the first arg from args or the first value from kwargs.


============================================================
FILE: python/sglang/srt/layers/vocab_parallel_embedding.py
Functions: 20
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  44: def pad_vocab_size(vocab_size: int, pad_to: int)
         ‚Üí int
         üìù Pad the vocab size to the given value.

  L  49: def vocab_range_from_per_partition_vocab_size(per_partition_vocab_size: int,
        rank: int,
        offset: int)
         ‚Üí Sequence[int]

  L  57: def vocab_range_from_global_vocab_size(global_vocab_size: int,
        rank: int,
        world_size: int,
        offset: int)
         ‚Üí Sequence[int]

  L 126: def get_masked_input_and_mask(input_: torch.Tensor,
        org_vocab_start_index: int,
        org_vocab_end_index: int,
        num_org_vocab_padding: int,
        added_vocab_start_index: int,
        added_vocab_end_index: int)
         ‚Üí Tuple[torch.Tensor, torch.Tensor]
         @torch.compile(dynamic=True, backend=get_compiler_backend())


CLASS: ParallelLMHead
----------------------------------------
  L 514: __init__(self, num_embeddings: int, embedding_dim: int)

  L 560: tie_weights(self, embed_tokens: VocabParallelEmbedding)
         üìù Tie the weights with word embeddings.

  L 569: forward(self, input_)


CLASS: VocabParallelEmbedding
----------------------------------------
  L 192: __init__(self, num_embeddings: int, embedding_dim: int)

  L 346: get_sharded_to_full_mapping(self)
         ‚Üí Optional[List[int]]
         üìù Get a mapping that can be used to reindex the gathered

  L 411: weight_loader(self, param: Parameter, loaded_weight: torch.Tensor)

  L 462: forward(self, input_)

  L 488: extra_repr(self)
         ‚Üí str


CLASS: VocabParallelEmbeddingShardIndices
----------------------------------------
  L  81: num_org_elements(self)
         ‚Üí int

  L  85: num_added_elements(self)
         ‚Üí int

  L  89: num_org_elements_padded(self)
         ‚Üí int

  L  93: num_added_elements_padded(self)
         ‚Üí int

  L  97: num_org_vocab_padding(self)
         ‚Üí int

  L 101: num_added_vocab_padding(self)
         ‚Üí int

  L 105: num_elements_padded(self)
         ‚Üí int

  L 108: __post_init__(self)
