
# python/sglang/srt/speculative/build_eagle_tree.py
build_tree_kernel_efficient_preprocess(verified_id: torch.Tensor, score_list: List[torch.Tensor], token_list: List[torch.Tensor], parents_list: List[torch.Tensor], num_verify_tokens: int)
build_tree_kernel_efficient(verified_id: torch.Tensor, score_list: List[torch.Tensor], token_list: List[torch.Tensor], parents_list: List[torch.Tensor], seq_lens: torch.Tensor, seq_lens_sum: int, topk: int, spec_steps: int, num_verify_tokens: int, tree_mask_mode: TreeMaskMode, tree_mask_buf: Optional[torch.Tensor], position_buf: Optional[torch.Tensor])
test_build_tree_kernel_efficient()

# python/sglang/srt/speculative/eagle_draft_cuda_graph_runner.py
  EAGLEDraftCudaGraphRunner.__init__(eagle_worker: EAGLEWorker)
  EAGLEDraftCudaGraphRunner.can_run(forward_batch: ForwardBatch)
  EAGLEDraftCudaGraphRunner.capture()
  EAGLEDraftCudaGraphRunner.capture_one_batch_size(num_seqs: int, forward: Callable)
  EAGLEDraftCudaGraphRunner.replay(forward_batch: ForwardBatch)

# python/sglang/srt/speculative/eagle_draft_extend_cuda_graph_runner.py
  EAGLEDraftExtendCudaGraphRunner.__init__(eagle_worker: EAGLEWorker)
  EAGLEDraftExtendCudaGraphRunner.can_run(forward_batch: ForwardBatch)
  EAGLEDraftExtendCudaGraphRunner.capture()
  EAGLEDraftExtendCudaGraphRunner.capture_one_batch_size(bs: int, forward: Callable)
  EAGLEDraftExtendCudaGraphRunner.replay(forward_batch: ForwardBatch)

# python/sglang/srt/speculative/eagle_utils.py
  EagleDraftInput.prepare_for_extend(batch: ScheduleBatch)
  EagleDraftInput.create_idle_input(cls, device: torch.device, hidden_size: int, dtype: torch.dtype, topk: int, capture_hidden_mode: CaptureHiddenMode)
  EagleDraftInput.prepare_extend_after_decode(batch: ScheduleBatch, speculative_num_steps: int)
  EagleDraftInput.generate_attn_arg_prefill(req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, req_to_token: torch.Tensor)
  EagleDraftInput.filter_batch(new_indices: torch.Tensor, has_been_filtered: bool)
  EagleDraftInput.merge_batch(spec_info: EagleDraftInput)
  EagleVerifyInput.create_idle_input(cls, topk: int, spec_steps: int, num_verify_tokens: int)
  EagleVerifyInput.prepare_for_verify(batch: ScheduleBatch, page_size: int)
  EagleVerifyInput.generate_attn_arg_prefill(req_pool_indices: torch.Tensor, paged_kernel_lens: torch.Tensor, paged_kernel_lens_sum: int, req_to_token: torch.Tensor)
  EagleVerifyInput.verify(batch: ScheduleBatch, logits_output: LogitsProcessorOutput, token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator, page_size: int, vocab_mask: Optional[torch.Tensor]) -> torch.Tensor
create_extend_after_decode_spec_info(verified_id, seq_lens, accept_lens, positions, new_verified_id, bs_upper: tl.constexpr)
assign_req_to_token_pool(req_pool_indices, req_to_token, start_offset, end_offset, out_cache_loc, pool_len: tl.constexpr, bs_upper: tl.constexpr)
assign_draft_cache_locs(req_pool_indices, req_to_token, seq_lens, extend_lens, num_new_pages_per_topk, out_cache_loc, pool_len: tl.constexpr, topk: tl.constexpr, speculative_num_steps: tl.constexpr, page_size: tl.constexpr, bs_upper: tl.constexpr, iter_upper: tl.constexpr)
generate_draft_decode_kv_indices(req_pool_indices, req_to_token, paged_kernel_lens, kv_indices, kv_indptr, positions, pool_len: tl.constexpr, kv_indices_stride: tl.constexpr, kv_indptr_stride: tl.constexpr, bs_upper: tl.constexpr, iter_upper: tl.constexpr, num_tokens_upper: tl.constexpr, page_size: tl.constexpr)
align_evict_mask_to_page_size(seq_lens, evict_mask, page_size: tl.constexpr, num_draft_tokens: tl.constexpr, BLOCK_SIZE: tl.constexpr)
get_target_cache_loc(tgt_cache_loc, to_free_slots, accept_length, to_free_num_slots, out_cache_loc, num_verify_tokens: tl.constexpr, num_verify_tokens_upper: tl.constexpr, bs_upper: tl.constexpr)
get_src_tgt_cache_loc(seq_lens: torch.Tensor, out_cache_loc: torch.Tensor, accept_index: torch.Tensor, accept_length: torch.Tensor, draft_token_num: int, page_size: int)
filter_finished_cache_loc_kernel(out_cache_loc, tgt_cache_loc, accept_length, accept_length_filter, bs_upper: tl.constexpr, num_verify_tokens_upper: tl.constexpr)
create_accept_length_filter(accept_length: torch.Tensor, unfinished_index_device: torch.Tensor, seq_lens: torch.Tensor)
select_top_k_tokens(i: int, topk_p: torch.Tensor, topk_index: torch.Tensor, hidden_states: torch.Tensor, scores: torch.Tensor, topk: int)
traverse_tree(retrieve_next_token: torch.Tensor, retrieve_next_sibling: torch.Tensor, draft_tokens: torch.Tensor, grammar: BaseGrammarObject, allocate_token_bitmask: torch.Tensor)
generate_token_bitmask(reqs: List[Req], verify_input: EagleVerifyInput, retrieve_next_token_cpu: torch.Tensor, retrieve_next_sibling_cpu: torch.Tensor, draft_tokens_cpu: torch.Tensor, vocab_size: int)

# python/sglang/srt/speculative/eagle_worker.py
draft_tp_context(tp_group: GroupCoordinator)
  EAGLEWorker.__init__(server_args: ServerArgs, gpu_id: int, tp_rank: int, dp_rank: Optional[int], moe_ep_rank: int, nccl_port: int, target_worker: TpModelWorker)
  EAGLEWorker.init_attention_backend()
  EAGLEWorker.init_cuda_graphs()
  EAGLEWorker.draft_model_runner()
  EAGLEWorker.forward_batch_speculative_generation(batch: ScheduleBatch) -> Tuple[LogitsProcessorOutput, torch.Tensor, int, int, bool]
  EAGLEWorker.check_forward_draft_extend_after_decode(batch: ScheduleBatch)
  EAGLEWorker.forward_target_extend(batch: ScheduleBatch) -> Tuple[LogitsProcessorOutput, torch.Tensor, int, Optional[torch.Tensor]]
  EAGLEWorker.draft(batch: ScheduleBatch)
  EAGLEWorker.draft_forward(forward_batch: ForwardBatch)
  EAGLEWorker.verify(batch: ScheduleBatch, spec_info: EagleVerifyInput)
  EAGLEWorker.add_logprob_values(batch: ScheduleBatch, res: EagleVerifyOutput, logits_output: LogitsProcessorOutput)
  EAGLEWorker.forward_draft_extend(batch: ScheduleBatch, hidden_states: torch.Tensor, next_token_ids: torch.Tensor, seq_lens_cpu: Optional[torch.Tensor])
  EAGLEWorker.forward_draft_extend_after_decode(batch: ScheduleBatch)
  EAGLEWorker.capture_for_decode(logits_output: LogitsProcessorOutput, draft_input: EagleDraftInput)
load_token_map(token_map_path: str) -> List[int]
get_last_loc_large_page_size_top_k_1(req_to_token: torch.Tensor, req_pool_indices: torch.Tensor, seq_lens, speculative_num_steps: int)
get_last_loc_large_page_size_large_top_k(req_to_token: torch.Tensor, req_pool_indices: torch.Tensor, seq_lens: torch.Tensor, speculative_num_steps: int, topk: int, page_size: int)

# python/sglang/srt/speculative/spec_info.py
  SpeculativeAlgorithm.is_none()
  SpeculativeAlgorithm.is_eagle()
  SpeculativeAlgorithm.is_eagle3()
  SpeculativeAlgorithm.from_string(name: str)
