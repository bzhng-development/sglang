================================================================================
FUNCTION INDEX: models module
================================================================================
Total Functions: 1499
Documented: 114


============================================================
FILE: python/sglang/srt/models/arcee.py
Functions: 16
============================================================


CLASS: ArceeAttention
----------------------------------------
  L 105: __init__(self, config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str, bias: bool)
         → None

  L 178: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: ArceeDecoderLayer
----------------------------------------
  L 193: __init__(self, config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 241: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: ArceeForCausalLM
----------------------------------------
  L 393: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 432: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors])
         → LogitsProcessorOutput

  L 468: start_layer(self)

  L 472: end_layer(self)

  L 475: get_input_embeddings(self)
         → nn.Embedding

  L 478: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 528: load_kv_cache_scales(self, quantization_param_path: str)
         → None


CLASS: ArceeMLP
----------------------------------------
  L  63: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str, reduce_results: bool)
         → None

  L  97: forward(self, x, forward_batch)


CLASS: ArceeModel
----------------------------------------
  L 267: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 304: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         → Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]], PPProxyTensors]

  L 350: load_kv_cache_scales(self, quantization_param_path: str)
         → None


============================================================
FILE: python/sglang/srt/models/baichuan.py
Functions: 12
============================================================


CLASS: BaiChuanAttention
----------------------------------------
  L 118: __init__(self, hidden_size: int, num_heads: int, position_embedding: str, rope_theta: float, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], layer_id: int, prefix: str)

  L 202: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: BaiChuanBaseForCausalLM
----------------------------------------
  L 348: __init__(self, config: PretrainedConfig, position_embedding: str, quant_config: Optional[QuantizationConfig], prefix: str)

  L 374: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 385: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: BaiChuanDecoderLayer
----------------------------------------
  L 219: __init__(self, config: PretrainedConfig, position_embedding: str, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 253: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: BaiChuanMLP
----------------------------------------
  L  78: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)

  L 108: forward(self, x)


CLASS: BaiChuanModel
----------------------------------------
  L 280: __init__(self, config: PretrainedConfig, position_embedding: str, quant_config: Optional[QuantizationConfig], prefix: str)

  L 310: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: BaichuanForCausalLM
----------------------------------------
  L 429: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)


============================================================
FILE: python/sglang/srt/models/bailing_moe.py
Functions: 13
============================================================


CLASS: BailingAttention
----------------------------------------
  L  41: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 103: forward(self, hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: BailingMLP
----------------------------------------
  L 119: __init__(self, intermediate_size: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], reduce_results: Optional[bool], prefix: str)
         → None

  L 145: forward(self, x)


CLASS: BailingMoE
----------------------------------------
  L 154: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 201: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: BailingMoeBlock
----------------------------------------
  L 224: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 246: forward(self, hidden_states: torch.Tensor, position_ids: torch.Tensor, residual: Optional[torch.Tensor], forward_batch: ForwardBatch)
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: BailingMoeForCausalLM
----------------------------------------
  L 338: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig])
         → None

  L 356: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor])
         → torch.Tensor

  L 368: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: BailingMoeModel
----------------------------------------
  L 279: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 311: forward(self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor])
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/bert.py
Functions: 24
============================================================


CLASS: BertAttention
----------------------------------------
  L 177: __init__(self, hidden_size: int, num_attention_heads: int, layer_norm_eps: float, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 203: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: BertEmbedding
----------------------------------------
  L  27: __init__(self, config: BertConfig)

  L  51: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: BertEncoder
----------------------------------------
  L 100: __init__(self, config: BertConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 121: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: BertForSequenceClassification
----------------------------------------
  L 439: __init__(self)

  L 458: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 478: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         → torch.Tensor


CLASS: BertIntermediate
----------------------------------------
  L 295: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)

  L 313: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: BertLayer
----------------------------------------
  L 131: __init__(self, config: BertConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 167: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)


CLASS: BertModel
----------------------------------------
  L 351: __init__(self)

  L 375: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         → torch.Tensor

  L 399: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])
         → Set[str]


CLASS: BertOutput
----------------------------------------
  L 321: __init__(self, hidden_size: int, intermediate_size: int, layer_norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str)

  L 341: forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor)
         → torch.Tensor


CLASS: BertPooler
----------------------------------------
  L  81: __init__(self, config: BertConfig)

  L  86: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: BertSelfAttention
----------------------------------------
  L 212: __init__(self, hidden_size: int, num_attention_heads: int, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 257: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: BertSelfOutput
----------------------------------------
  L 268: __init__(self, hidden_size: int, layer_norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str)

  L 285: forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/chatglm.py
Functions: 13
============================================================


CLASS: ChatGLMForCausalLM
----------------------------------------
  L 380: __init__(self, config: ChatGLMConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 397: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 408: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: ChatGLMM
----------------------------------------
  L 321: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 348: forward(self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: GLMAttention
----------------------------------------
  L  50: __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 120: forward(self, hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: GLMBlock
----------------------------------------
  L 193: __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 227: forward(self, hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: GLMMLP
----------------------------------------
  L 147: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 177: forward(self, hidden_states)


CLASS: GLMTransformer
----------------------------------------
  L 268: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 300: forward(self, hidden_states: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/clip.py
Functions: 26
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 518: def monkey_patch_weight_loader()


CLASS: CLIPEncoder
----------------------------------------
  L 219: __init__(self, config: CLIPVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 244: forward(self, inputs_embeds: torch.Tensor, attention_mask: torch.Tensor, causal_attention_mask: torch.Tensor, return_all_hidden_states: bool)
         → Union[torch.Tensor, list[torch.Tensor]]


CLASS: CLIPEncoderLayer
----------------------------------------
  L 139: __init__(self, config: CLIPVisionConfig, act_layer: Type[nn.Module], norm_layer: Type[nn.Module], attn_implementation: Optional[str], quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 180: forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, causal_attention_mask: torch.Tensor)
         → torch.Tensor


CLASS: CLIPMLP
----------------------------------------
  L 108: __init__(self, config, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)

  L 130: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: CLIPModel
----------------------------------------
  L 407: __init__(self, config: CLIPConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 454: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)

  L 486: pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)

  L 490: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: CLIPTextEmbeddings
----------------------------------------
  L  68: __init__(self, config: CLIPTextConfig)

  L  84: forward(self, input_ids: Optional[torch.LongTensor], position_ids: Optional[torch.LongTensor], inputs_embeds: Optional[torch.FloatTensor])
         → torch.Tensor


CLASS: CLIPTextModel
----------------------------------------
  L 307: __init__(self, config: CLIPTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 321: forward(self, input_ids: torch.Tensor, position_ids: torch.Tensor)


CLASS: CLIPTextTransformer
----------------------------------------
  L 266: __init__(self, config: CLIPTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 284: device(self)
         → torch.device

  L 287: forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor], position_ids: Optional[torch.Tensor])


CLASS: CLIPVisionEmbeddings
----------------------------------------
  L  25: __init__(self, config: CLIPVisionConfig)

  L  52: forward(self, pixel_values: torch.Tensor)
         → torch.Tensor


CLASS: CLIPVisionModel
----------------------------------------
  L 387: __init__(self, config: CLIPVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 399: device(self)
         → torch.device

  L 402: forward(self, pixel_values: torch.Tensor)


CLASS: CLIPVisionTransformer
----------------------------------------
  L 331: __init__(self, config: CLIPVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 364: device(self)
         → torch.device

  L 367: forward(self, pixel_values: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/commandr.py
Functions: 15
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  72: def layer_norm_func(hidden_states, weight, variance_epsilon)
         @torch.compile(backend=get_compiler_backend())


CLASS: CohereAttention
----------------------------------------
  L 143: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 228: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: CohereDecoderLayer
----------------------------------------
  L 245: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 271: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: CohereForCausalLM
----------------------------------------
  L 342: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 357: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 372: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: CohereMLP
----------------------------------------
  L 109: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 135: forward(self, x)


CLASS: CohereModel
----------------------------------------
  L 294: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 321: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: LayerNorm
----------------------------------------
  L  83: __init__(self, param_shape, eps)

  L  89: forward(self, hidden_states, residuals)

  L  95: weight_loader(self, param: Parameter, loaded_weight: torch.Tensor)


============================================================
FILE: python/sglang/srt/models/dbrx.py
Functions: 16
============================================================


CLASS: DbrxAttention
----------------------------------------
  L 195: __init__(self, config: DbrxConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 262: forward(self, position_ids: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: DbrxBlock
----------------------------------------
  L 317: __init__(self, config: DbrxConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 333: forward(self, position_ids: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: DbrxExperts
----------------------------------------
  L  90: __init__(self, config: DbrxConfig, quant_config: Optional[QuantizationConfig], params_dtype: Optional[torch.dtype], prefix: str)

  L 146: weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor, weight_name: str)

  L 174: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: DbrxForCausalLM
----------------------------------------
  L 397: __init__(self, config: DbrxConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 420: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 431: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: DbrxFusedNormAttention
----------------------------------------
  L 279: __init__(self, config: DbrxConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 297: forward(self, position_ids: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: DbrxModel
----------------------------------------
  L 350: __init__(self, config: DbrxConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 378: forward(self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


CLASS: DbrxRouter
----------------------------------------
  L  59: __init__(self, config: DbrxConfig, params_dtype: Optional[torch.dtype], prefix: str)

  L  77: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/deepseek.py
Functions: 15
============================================================


CLASS: DeepseekAttention
----------------------------------------
  L 196: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 266: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: DeepseekDecoderLayer
----------------------------------------
  L 282: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 328: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → torch.Tensor


CLASS: DeepseekForCausalLM
----------------------------------------
  L 409: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 429: get_input_embeddings(self)
         → nn.Embedding

  L 433: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor

  L 445: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: DeepseekMLP
----------------------------------------
  L  56: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str)
         → None

  L  88: forward(self, x)


CLASS: DeepseekMoE
----------------------------------------
  L  97: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 152: pack_params(self)

  L 171: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: DeepseekModel
----------------------------------------
  L 357: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 384: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/deepseek_janus_pro.py
Functions: 84
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  82: def named_apply(fn: Callable,
        module: nn.Module,
        name,
        depth_first: bool,
        include_root: bool)
         → nn.Module

  L 105: def VQ_16()

  L 176: def trunc_normal_tf_(tensor: torch.Tensor,
        mean: float,
        std: float,
        a: float,
        b: float)
         📝 Fills the input Tensor with values drawn from a truncated

  L 214: def nchw_to(x: torch.Tensor, fmt: Format)

  L 224: def resample_patch_embed(patch_embed,
        new_size: List[int],
        interpolation: str,
        antialias: bool,
        verbose: bool)
         📝 Resample the weights of the patch embedding kernel to target resolutio

  L 473: def drop_path(x, drop_prob: float, training: bool, scale_by_keep: bool)
         📝 Drop paths (Stochastic Depth) per sample (when applied in main path of

  L 625: def resample_abs_pos_embed(posemb: torch.Tensor,
        new_size: List[int],
        old_size: Optional[List[int]],
        num_prefix_tokens: int,
        interpolation: str,
        antialias: bool,
        verbose: bool)

  L 673: def init_weights(self)

  L 679: def init_weights_vit_timm(module: nn.Module, name: str)
         → None
         📝 ViT weight initialization, original timm impl (for reproducibility)

  L 984: def model_name_to_cls(cls_name)

  L1054: def create_siglip_vit(model_name: str,
        image_size: int,
        select_layer: int,
        ckpt_path: str)

  L1328: def use_fused_attn(experimental: bool)
         → bool

  L1790: def nonlinearity(x)

  L1795: def Normalize(in_channels, norm_type)

  L1847: def compute_entropy_loss(affinity, loss_type, temperature)


CLASS: AttentionPoolLatent
----------------------------------------
  L1342: __init__(self, in_features: int, out_features: int, embed_dim: int, num_heads: int, feat_size: Optional[int], mlp_ratio: float, qkv_bias: bool, qk_norm: bool, latent_len: int, latent_dim: int, pos_embed: str, pool_type: str, norm_layer: Optional[nn.Module], drop: float)

  L1394: init_weights(self)

  L1399: forward(self, x)


CLASS: AttnBlock
----------------------------------------
  L1753: __init__(self, in_channels, norm_type)

  L1763: forward(self, x)


CLASS: CLIPVisionTower
----------------------------------------
  L1138: __init__(self, model_name: str, image_size: Union[Tuple[int, int], int], select_feature: str, select_layer: int, select_layers: list, ckpt_path: str, pixel_mean: Optional[List[float]], pixel_std: Optional[List[float]])

  L1176: device(self)
         → torch.device

  L1180: dtype(self)

  L1183: build_vision_tower(self, vision_tower_params)

  L1201: feature_select(self, image_forward_outs)

  L1220: forward(self, images)
         📝 Args:


CLASS: Decoder
----------------------------------------
  L1524: __init__(self, z_channels, ch, ch_mult, num_res_blocks, norm_type, dropout, resamp_with_conv, out_channels)

  L1586: last_layer(self)

  L1589: forward(self, z)


CLASS: Downsample
----------------------------------------
  L1828: __init__(self, in_channels, with_conv)

  L1837: forward(self, x)


CLASS: DropPath
----------------------------------------
  L 500: __init__(self, drop_prob: float, scale_by_keep: bool)

  L 505: forward(self, x)

  L 508: extra_repr(self)


CLASS: Encoder
----------------------------------------
  L1443: __init__(self, in_channels, ch, ch_mult, num_res_blocks, norm_type, dropout, resamp_with_conv, z_channels)

  L1501: forward(self, x)


CLASS: LayerScale
----------------------------------------
  L1301: __init__(self, dim: int, init_values: float, inplace: bool)
         → None

  L1311: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: Mlp
----------------------------------------
  L 436: __init__(self, in_features, hidden_features, out_features, act_layer, norm_layer, bias, drop, use_conv)

  L 463: forward(self, x)


CLASS: MlpProjector
----------------------------------------
  L1239: __init__(self, cfg)

  L1274: forward(self, x_or_tuple: Union[Tuple[torch.Tensor, torch.Tensor], torch.Tensor])
         📝 Args:


CLASS: MultiModalityCausalLM
----------------------------------------
  L1924: __init__(self, config: MultiModalityConfig, quant_config: Optional[QuantizationConfig])

  L1962: get_image_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor

  L1978: get_input_embeddings(self)
         → nn.Embedding

  L1982: forward(self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)
         → torch.Tensor

  L1999: prepare_gen_img_embeds(self, image_ids: torch.LongTensor)

  L2002: pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)

  L2011: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Normalize
----------------------------------------
  L1116: __init__(self, mean, std, inplace)

  L1123: forward(self, tensor: Tensor)
         → Tensor
         📝 Args:

  L1133: __repr__(self)
         → str


CLASS: PatchDropout
----------------------------------------
  L 573: __init__(self, prob: float, num_prefix_tokens: int, ordered: bool, return_indices: bool)

  L 589: forward(self, x)
         → Union[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]]]


CLASS: PatchEmbed
----------------------------------------
  L 308: __init__(self, img_size: Optional[int], patch_size: int, in_chans: int, embed_dim: int, norm_layer: Optional[Callable], flatten: bool, output_fmt: Optional[str], bias: bool, strict_img_size: bool, dynamic_img_pad: bool)

  L 349: set_input_size(self, img_size: Optional[Union[int, Tuple[int, int]]], patch_size: Optional[Union[int, Tuple[int, int]]])

  L 379: feat_ratio(self, as_scalar)
         → Union[Tuple[int, int], int]

  L 385: dynamic_feat_size(self, img_size: Tuple[int, int])
         → Tuple[int, int]
         📝 Get grid (feature) size for given image size taking account of dynamic

  L 396: forward(self, x)


CLASS: ResnetBlock
----------------------------------------
  L1700: __init__(self, in_channels, out_channels, conv_shortcut, dropout, norm_type)

  L1734: forward(self, x)


CLASS: Upsample
----------------------------------------
  L1806: __init__(self, in_channels, with_conv)

  L1814: forward(self, x)


CLASS: VQModel
----------------------------------------
  L1864: __init__(self, config: ModelArgs)

  L1891: encode(self, x)

  L1897: decode(self, quant)

  L1902: decode_code(self, code_b, shape, channel_first)

  L1907: forward(self, input)


CLASS: VectorQuantizer
----------------------------------------
  L1614: __init__(self, n_e, e_dim, beta, entropy_loss_ratio, l2_norm, show_usage)

  L1633: forward(self, z)

  L1681: get_codebook_entry(self, indices, shape, channel_first)


CLASS: VisionTransformer
----------------------------------------
  L 698: __init__(self, img_size: Union[int, Tuple[int, int]], patch_size: Union[int, Tuple[int, int]], in_chans: int, num_classes: int, global_pool: Literal['', 'avg', 'token', 'map'], embed_dim: int, depth: int, num_heads: int, mlp_ratio: float, qkv_bias: bool, qk_norm: bool, init_values: Optional[float], class_token: bool, no_embed_class: bool, reg_tokens: int, pre_norm: bool, fc_norm: Optional[bool], dynamic_img_size: bool, dynamic_img_pad: bool, drop_rate: float, pos_drop_rate: float, patch_drop_rate: float, proj_drop_rate: float, attn_drop_rate: float, drop_path_rate: float, weight_init: Literal['skip', 'jax', 'jax_nlhb', 'moco', ''], embed_layer: Callable, _norm_layer: Optional[LayerType], _act_layer: Optional[LayerType], block_fn: Type[nn.Module], mlp_layer: Type[nn.Module], ignore_head: bool)
         → None
         📝 Args:

  L 864: init_weights(self, mode: Literal['jax', 'jax_nlhb', 'moco', ''])
         → None

  L 873: no_weight_decay(self)
         → Set

  L 877: group_matcher(self, coarse: bool)
         → Dict

  L 884: get_classifier(self)
         → nn.Module

  L 887: reset_classifier(self, num_classes: int, global_pool)
         → None

  L 957: forward_features(self, x: torch.Tensor)
         → torch.Tensor

  L 966: forward_head(self, x: torch.Tensor, pre_logits: bool)
         → torch.Tensor

  L 977: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: VisionTransformerBlock
----------------------------------------
  L 513: __init__(self, dim: int, num_heads: int, mlp_ratio: float, qkv_bias: bool, qk_norm: bool, proj_drop: float, attn_drop: float, init_values: Optional[float], drop_path: float, act_layer: nn.Module, norm_layer: nn.Module, mlp_layer: nn.Module)
         → None

  L 557: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: vision_head
----------------------------------------
  L1003: __init__(self, params)

  L1013: forward(self, x)


============================================================
FILE: python/sglang/srt/models/deepseek_nextn.py
Functions: 5
============================================================


CLASS: DeepseekModelNextN
----------------------------------------
  L  42: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  80: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


CLASS: DeepseekV3ForCausalLMNextN
----------------------------------------
  L 128: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 155: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 166: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/deepseek_v2.py
Functions: 59
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 736: def yarn_get_mscale(scale: float, mscale: float)
         → float


CLASS: DeepseekV2AttentionMLA
----------------------------------------
  L 746: __init__(self, config: PretrainedConfig, hidden_size: int, num_heads: int, qk_nope_head_dim: int, qk_rope_head_dim: int, v_head_dim: int, q_lora_rank: int, kv_lora_rank: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], reduce_results: bool, layer_id: int, prefix: str, alt_stream: Optional[torch.cuda.Stream])
         → None

  L 970: dispatch_attn_forward_method(self, forward_batch: ForwardBatch)
         → AttnForwardMethod

  L1060: op_prepare(self, state)

  L1068: op_core(self, state)

  L1073: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)

  L1088: forward_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)

  L1130: forward_core(self, intermediate_state)

  L1150: forward_normal_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)

  L1194: forward_normal_core(self, q, k, v, forward_batch)

  L1210: forward_absorb_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)

  L1295: forward_absorb_core(self, q_pe, k_pe, q_nope_out, k_nope, forward_batch, zero_allocator)

  L1382: forward_absorb_fused_mla_rope_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)

  L1496: forward_absorb_fused_mla_rope_cpu_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)

  L1547: forward_absorb_fused_mla_rope_core(self, q_input, key_cache_buf, val_cache_buf, attn_output, kv_indptr, kv_indices, k_pe_output, cos_sin_cache, positions, attn_logits, num_kv_split, sm_scale, enable_rope_fusion, k_input, forward_batch, zero_allocator)

  L1620: forward_absorb_fused_mla_rope_cpu_core(self, q_input, k_input, v_input, forward_batch, zero_allocator)

  L1708: forward_normal_chunked_kv_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, zero_allocator: BumpAllocator)

  L1726: forward_normal_chunked_kv_core(self, q, k, v, forward_batch)


CLASS: DeepseekV2DecoderLayer
----------------------------------------
  L1757: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], is_nextn: bool, prefix: str, alt_stream: Optional[torch.cuda.Stream])
         → None

  L1852: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], zero_allocator: BumpAllocator)
         → torch.Tensor

  L1900: op_comm_prepare_attn(self, state, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], zero_allocator: BumpAllocator, tbo_subbatch_index: Optional[int])

  L1922: op_comm_prepare_mlp(self, state)

  L1931: op_mlp(self, state)

  L1944: op_comm_postprocess_layer(self, state)


CLASS: DeepseekV2ForCausalLM
----------------------------------------
  L2113: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L2158: routed_experts_weights_of_layer(self)

  L2161: determine_num_fused_shared_experts(self, architecture: str)

  L2192: get_input_embeddings(self)
         → nn.Embedding

  L2196: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         → torch.Tensor

  L2216: start_layer(self)

  L2220: end_layer(self)

  L2223: post_load_weights(self, is_nextn, weight_names)

  L2454: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn)

  L2695: get_embed_and_head(self)

  L2698: set_embed_and_head(self, embed, head)

  L2707: get_model_config_for_expert_location(cls, config)


CLASS: DeepseekV2MLP
----------------------------------------
  L 179: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str, tp_rank: Optional[int], tp_size: Optional[int])
         → None

  L 219: forward(self, x, forward_batch, should_allreduce_fusion: bool, use_reduce_scatter: bool)


CLASS: DeepseekV2MoE
----------------------------------------
  L 285: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream], is_nextn: bool)

  L 427: get_moe_weights(self)

  L 434: forward(self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch], should_allreduce_fusion: bool, use_reduce_scatter: bool)
         → torch.Tensor

  L 463: forward_normal_dual_stream(self, hidden_states: torch.Tensor, should_allreduce_fusion: bool, use_reduce_scatter: bool)
         → torch.Tensor

  L 498: forward_normal(self, hidden_states: torch.Tensor, should_allreduce_fusion: bool, use_reduce_scatter: bool)
         → torch.Tensor

  L 537: forward_cpu(self, hidden_states: torch.Tensor, should_allreduce_fusion: bool)
         → torch.Tensor

  L 595: forward_deepep(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 638: op_gate(self, state)

  L 647: op_shared_experts(self, state)

  L 656: op_select_experts(self, state)

  L 680: op_dispatch_a(self, state)

  L 690: op_dispatch_b(self, state)

  L 699: op_experts(self, state)

  L 704: op_combine_a(self, state)

  L 715: op_combine_b(self, state)

  L 723: op_output(self, state)


CLASS: DeepseekV2Model
----------------------------------------
  L1974: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L2031: get_input_embeddings(self)
         → torch.Tensor

  L2034: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         → Union[torch.Tensor, PPProxyTensors]


CLASS: MoEGate
----------------------------------------
  L 238: __init__(self, config, prefix: str, is_nextn: bool)

  L 258: forward(self, hidden_states)


============================================================
FILE: python/sglang/srt/models/deepseek_vl2.py
Functions: 7
============================================================


CLASS: DeepseekVL2ForCausalLM
----------------------------------------
  L 160: __init__(self, config: DeepseekVL2Config, quant_config: Optional[QuantizationConfig])

  L 219: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)

  L 236: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 256: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 260: get_image_feature(self, items: List[MultimodalDataItem])


CLASS: DeepseekVL2MlpProjector
----------------------------------------
  L  26: __init__(self, config: DeepseekVL2MlpProjectorConfig, quant_config: Optional[QuantizationConfig])

  L 111: forward(self, x)


============================================================
FILE: python/sglang/srt/models/ernie4.py
Functions: 14
============================================================


CLASS: Ernie4DecoderLayer
----------------------------------------
  L 148: __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, is_mtp: bool)

  L 212: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: Ernie4Model
----------------------------------------
  L 239: __init__(self, config: Ernie4_5_MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 264: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]]]


CLASS: Ernie4Moe
----------------------------------------
  L  68: __init__(self, config: Ernie4_5_MoeConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 119: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor

  L 122: forward_normal(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: Ernie4_5_ForCausalLM
----------------------------------------
  L 302: __init__(self, config: Ernie4_5_MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 324: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 335: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 358: get_embed_and_head(self)


CLASS: Ernie4_5_MoeForCausalLM
----------------------------------------
  L 363: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: MoEGate
----------------------------------------
  L  49: __init__(self, config, prefix: str)

  L  62: forward(self, hidden_states)


============================================================
FILE: python/sglang/srt/models/ernie4_eagle.py
Functions: 7
============================================================


CLASS: Ernie4ModelMTP
----------------------------------------
  L  39: __init__(self, config: Ernie4_5_MoeConfig, layer_id: int, prefix: str, quant_config: Optional[QuantizationConfig])
         → None

  L  67: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


CLASS: Ernie4_5_MoeForCausalLMMTP
----------------------------------------
  L 102: __init__(self, config: Ernie4_5_MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str, mtp_layer_id: int)
         → None

  L 132: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 143: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 188: get_embed_and_head(self)

  L 191: set_embed_and_head(self, embed, head)


============================================================
FILE: python/sglang/srt/models/exaone.py
Functions: 11
============================================================


CLASS: ExaoneAttention
----------------------------------------
  L  84: __init__(self, config, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 161: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: ExaoneDecoderLayer
----------------------------------------
  L 176: __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 219: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: ExaoneForCausalLM
----------------------------------------
  L 298: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 321: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → LogitsProcessorOutput

  L 335: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: ExaoneGatedMLP
----------------------------------------
  L  46: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  76: forward(self, x)


CLASS: ExaoneModel
----------------------------------------
  L 245: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 273: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/gemma.py
Functions: 12
============================================================


CLASS: GemmaAttention
----------------------------------------
  L  76: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: int, layer_id: int, max_position_embeddings: int, rope_theta: float, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 144: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: GemmaDecoderLayer
----------------------------------------
  L 159: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 190: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: GemmaForCausalLM
----------------------------------------
  L 294: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 309: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor

  L 322: forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)

  L 369: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: GemmaMLP
----------------------------------------
  L  44: __init__(self, hidden_size: int, intermediate_size: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  68: forward(self, x)


CLASS: GemmaModel
----------------------------------------
  L 216: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 242: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/gemma2.py
Functions: 14
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  47: def get_attention_sliding_window_size(config)


CLASS: Gemma2Attention
----------------------------------------
  L  92: __init__(self, layer_id: int, config: PretrainedConfig, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: int, max_position_embeddings: int, rope_theta: float, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 170: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: Gemma2DecoderLayer
----------------------------------------
  L 185: __init__(self, layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 227: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: Gemma2ForCausalLM
----------------------------------------
  L 357: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 372: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor

  L 385: forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)

  L 435: get_attention_sliding_window_size(self)

  L 438: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Gemma2MLP
----------------------------------------
  L  52: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, hidden_activation: str, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  84: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: Gemma2Model
----------------------------------------
  L 255: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 286: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/gemma2_reward.py
Functions: 3
============================================================


CLASS: Gemma2ForSequenceClassification
----------------------------------------
  L  29: __init__(self, config: Gemma2Config, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  48: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         → EmbeddingPoolerOutput

  L  66: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/gemma3_causal.py
Functions: 22
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  52: def get_attention_sliding_window_size(config)

  L  58: def extract_layer_index(prefix: str)
         → int
         📝 Extract the layer index from a prefix string.


CLASS: Gemma3Attention
----------------------------------------
  L 111: __init__(self, layer_id: int, config: Gemma3TextConfig, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 203: naive_attn_with_masks(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, out: torch.Tensor)
         → torch.Tensor

  L 249: forward(self, hidden_states: torch.Tensor, position_embeddings: Tuple[torch.Tensor, torch.Tensor], forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: Gemma3DecoderLayer
----------------------------------------
  L 291: __init__(self, layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 330: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, position_embeddings_global: torch.Tensor, position_embeddings_local: torch.Tensor, forward_batch: ForwardBatch)
         → tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]


CLASS: Gemma3ForCausalLM
----------------------------------------
  L 599: __init__(self, config: Gemma3TextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 624: get_input_embeddings(self)
         → nn.Embedding

  L 627: get_attention_sliding_window_size(self)

  L 630: dtype(self)
         → torch.dtype

  L 634: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → LogitsProcessor

  L 651: forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)

  L 713: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Gemma3MLP
----------------------------------------
  L  72: __init__(self, hidden_size: int, intermediate_size: int, hidden_activation: str, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 103: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: Gemma3RotaryEmbedding
----------------------------------------
  L 372: __init__(self, config: Gemma3TextConfig, device)

  L 418: forward(self, x, position_ids)


CLASS: Gemma3TextModel
----------------------------------------
  L 469: __init__(self, config: Gemma3TextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 513: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


CLASS: Gemma3TextScaledWordEmbedding
----------------------------------------
  L 454: __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int, embed_scale: Optional[float])

  L 464: forward(self, input_ids: torch.Tensor)


============================================================
FILE: python/sglang/srt/models/gemma3_mm.py
Functions: 11
============================================================


CLASS: Gemma3ForConditionalGeneration
----------------------------------------
  L 158: __init__(self, config: Gemma3Config, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 188: pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)
         → List[int]
         📝 Pad input IDs with image tokens.

  L 201: prepare_attn_masks(self, input_ids: torch.Tensor, positions: torch.Tensor, mask_dtype: torch.dtype)
         → Dict
         📝 Prepare attention masks for multimodal inputs.

  L 269: get_input_embeddings(self)
         → nn.Embedding

  L 272: get_attention_sliding_window_size(self)
         📝 This value is used to initialize attention backends in `ForwardBatch`.

  L 278: get_image_feature(self, items: List[MultimodalDataItem])
         📝 Projects the last hidden state from the vision model into language mod

  L 316: forward(self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → LogitsProcessor
         📝     labels (`torch.LongTensor` of shape `(batch_size, sequence_length)

  L 383: tie_weights(self)

  L 386: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Gemma3MultiModalProjector
----------------------------------------
  L  61: __init__(self, config: Gemma3Config)

  L  83: forward(self, vision_outputs: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/gemma3n_audio.py
Functions: 20
============================================================


CLASS: Gemma3nAudioAttention
----------------------------------------
  L 280: __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 388: forward(self, x: torch.Tensor, mask: torch.BoolTensor)
         → torch.Tensor


CLASS: Gemma3nAudioConformerAttention
----------------------------------------
  L 614: __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 646: forward(self, audio_encodings: torch.Tensor, audio_mel_mask: torch.BoolTensor)
         → torch.Tensor


CLASS: Gemma3nAudioConformerBlock
----------------------------------------
  L 793: __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 821: forward(self, audio_encodings: torch.Tensor, audio_mel_mask: torch.BoolTensor)
         → torch.Tensor


CLASS: Gemma3nAudioConformerFeedForward
----------------------------------------
  L 669: __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 702: forward(self, audio_encodings: torch.Tensor)
         → torch.Tensor


CLASS: Gemma3nAudioConformerLightConv1d
----------------------------------------
  L 719: __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 766: forward(self, audio_encodings: torch.Tensor)
         → torch.Tensor


CLASS: Gemma3nAudioEncoder
----------------------------------------
  L 846: __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 868: forward(self, audio_mel: torch.Tensor, audio_mel_mask: torch.BoolTensor)
         → Tuple[torch.Tensor, torch.BoolTensor]
         📝 Encodes a batch of MELs.


CLASS: Gemma3nAudioRelativePositionEmbedding
----------------------------------------
  L 139: __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 227: forward(self, queries: torch.Tensor, keys: torch.Tensor)
         → torch.Tensor


CLASS: Gemma3nAudioSSCPConvBlock
----------------------------------------
  L 490: __init__(self, config: Gemma3nAudioConfig, idx: int, input_freq_dim: int, manual_padding: Tuple[int, int, int, int], quant_config: Optional[QuantizationConfig], prefix: str)

  L 528: forward(self, audio_encodings: torch.Tensor)
         → torch.Tensor


CLASS: Gemma3nAudioSubSampleConvProjection
----------------------------------------
  L 540: __init__(self, config: Gemma3nAudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 602: forward(self, audio_encodings: torch.Tensor)
         → torch.Tensor


CLASS: Gemma3nCumulativeGroupNorm
----------------------------------------
  L  36: __init__(self, num_channels: int, feature_dims: Sequence[int], eps: float)

  L  56: forward(self, x: torch.Tensor, mask: Optional[torch.Tensor])
         → torch.Tensor
         📝 Applies cumulative group norm, optionally using a mask.


============================================================
FILE: python/sglang/srt/models/gemma3n_causal.py
Functions: 29
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  33: def get_attention_sliding_window_size(config)


CLASS: Gemma3nAltUp
----------------------------------------
  L 174: __init__(self, config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 219: compute_router_modalities(self, x: torch.Tensor)
         → torch.Tensor

  L 230: predict(self, hidden_states: torch.Tensor)
         → torch.Tensor
         📝 Predicts the output of a layer using a trainable map.

  L 262: correct(self, predictions: torch.Tensor, activated: torch.Tensor)
         → torch.Tensor
         📝 Corrects the predictions relative to the activated inputs.

  L 293: scale_corrected_output(self, corrected: torch.Tensor)
         → torch.Tensor
         📝 Scales the provided 3D tensor.

  L 297: forward(self, hidden_states: torch.Tensor, activated: torch.Tensor)
         → Tuple[torch.Tensor, torch.Tensor]
         📝 Predicts, correct, and optionally scales the output of a layer using t


CLASS: Gemma3nAttention
----------------------------------------
  L 316: __init__(self, layer_id: int, config: Gemma3nTextConfig, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 434: forward(self, hidden_states: torch.Tensor, positions: Tuple[torch.Tensor, torch.Tensor], forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: Gemma3nDecoderLayer
----------------------------------------
  L 496: __init__(self, layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 567: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, per_layer_input: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: Gemma3nForCausalLM
----------------------------------------
  L 900: __init__(self, config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 927: get_input_embeddings(self)
         → nn.Embedding

  L 930: get_attention_sliding_window_size(self)

  L 933: dtype(self)
         → torch.dtype

  L 937: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, per_layer_inputs: Optional[torch.Tensor])
         → LogitsProcessor

  L 959: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Gemma3nLaurelBlock
----------------------------------------
  L 135: __init__(self, config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 163: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: Gemma3nRMSNorm
----------------------------------------
  L  38: __init__(self, dim: int, eps: float, with_scale: bool)
         → None

  L  53: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: Gemma3nTextMLP
----------------------------------------
  L  66: __init__(self, hidden_size: int, intermediate_size: int, hidden_activation: str, activation_sparsity: float, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 105: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: Gemma3nTextModel
----------------------------------------
  L 629: __init__(self, config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 724: get_input_embeddings(self)
         → nn.Embedding

  L 727: dtype(self)
         → torch.dtype

  L 730: get_per_layer_inputs(self, input_ids: torch.LongTensor)
         → torch.Tensor

  L 738: project_per_layer_inputs(self, inputs_embeds: torch.Tensor, per_layer_inputs: Optional[torch.Tensor])
         → torch.Tensor

  L 765: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, per_layer_inputs: Optional[torch.Tensor])
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/gemma3n_mm.py
Functions: 15
============================================================


CLASS: Gemma3nForConditionalGeneration
----------------------------------------
  L 193: __init__(self, config: Gemma3nConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 245: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)
         → List[int]
         📝 Pad input IDs with image and audio tokens.

  L 254: get_input_embeddings(self)
         → nn.Embedding

  L 257: get_attention_sliding_window_size(self)

  L 260: get_image_feature(self, items: List[MultimodalDataItem])
         📝 Projects the last hidden state from the vision model into language mod

  L 308: get_audio_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor
         📝 Projects the last hidden state from the audio encoder into language mo

  L 388: get_per_layer_inputs(self, input_ids: torch.LongTensor)
         → Optional[torch.Tensor]

  L 393: project_per_layer_inputs(self, inputs_embeds: torch.Tensor, per_layer_inputs: Optional[torch.Tensor])
         → torch.Tensor

  L 403: forward(self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → LogitsProcessor
         📝 Forward pass for multimodal Gemma3n.

  L 449: tie_weights(self)

  L 452: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 499: should_apply_lora(self, module_name: str)
         → bool

  L 502: get_hidden_dim(self, module_name)


CLASS: Gemma3nMultimodalEmbedder
----------------------------------------
  L  62: __init__(self, multimodal_config: Union[Gemma3nAudioConfig, Gemma3nVisionConfig], text_config: Gemma3nTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 108: forward(self, input_ids: Optional[torch.LongTensor], inputs_embeds: Optional[torch.Tensor])
         → torch.Tensor
         📝 Embeds token ids or soft tokens for multimodal content into language m


============================================================
FILE: python/sglang/srt/models/glm4.py
Functions: 11
============================================================


CLASS: Glm4Attention
----------------------------------------
  L  44: __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 111: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: Glm4DecoderLayer
----------------------------------------
  L 137: __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 168: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: Glm4ForCausalLM
----------------------------------------
  L 253: __init__(self, config: Glm4Config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 275: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 286: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Glm4Model
----------------------------------------
  L 197: __init__(self, config: Glm4Config, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 221: get_input_embeddings(self)
         → nn.Embedding

  L 224: dtype(self)
         → torch.dtype

  L 228: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]]]


============================================================
FILE: python/sglang/srt/models/glm4_moe.py
Functions: 20
============================================================


CLASS: Glm4MoeAttention
----------------------------------------
  L 167: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, partial_rotary_factor: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, head_dim: Optional[int], rms_norm_eps: float, attention_bias: bool, quant_config: Optional[QuantizationConfig], use_qk_norm: bool, prefix: str, alt_stream: Optional[torch.cuda.Stream])
         → None

  L 280: op_prepare(self, state)

  L 287: op_core(self, state)

  L 292: forward_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)

  L 308: forward_core(self, intermediate_state)

  L 316: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: Glm4MoeDecoderLayer
----------------------------------------
  L 578: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], is_nextn: bool, prefix: str, alt_stream: Optional[torch.cuda.Stream])
         → None

  L 662: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], zero_allocator: BumpAllocator)
         → torch.Tensor


CLASS: Glm4MoeForCausalLM
----------------------------------------
  L 731: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 764: determine_num_fused_shared_experts(self, architecture: str)

  L 794: get_input_embeddings(self)
         → nn.Embedding

  L 797: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn)


CLASS: Glm4MoeGate
----------------------------------------
  L 331: __init__(self, config, prefix: str, is_nextn: bool)

  L 348: forward(self, hidden_states)


CLASS: Glm4MoeMLP
----------------------------------------
  L 116: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str, tp_rank: Optional[int], tp_size: Optional[int])
         → None

  L 156: forward(self, x, forward_batch, should_allreduce_fusion)


CLASS: Glm4MoeModel
----------------------------------------
  L 694: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None


CLASS: Glm4MoeSparseMoeBlock
----------------------------------------
  L 376: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream], is_nextn: bool)

  L 499: forward_normal_dual_stream(self, hidden_states: torch.Tensor, should_allreduce_fusion: bool, use_reduce_scatter: bool)
         → torch.Tensor

  L 541: forward_normal(self, hidden_states: torch.Tensor, should_allreduce_fusion: bool, use_reduce_scatter: bool)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/glm4_moe_nextn.py
Functions: 5
============================================================


CLASS: Glm4MoeForCausalLMNextN
----------------------------------------
  L 128: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 153: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 164: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Glm4MoeModelNextN
----------------------------------------
  L  42: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  80: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/glm4v.py
Functions: 22
============================================================


CLASS: Glm4vForConditionalGeneration
----------------------------------------
  L 465: __init__(self, config: Glm4vConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 501: get_image_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor

  L 516: get_video_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor

  L 587: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Glm4vPatchMerger
----------------------------------------
  L 145: __init__(self, d_model: int, context_dim: int, quant_config: Optional[QuantizationConfig], bias: bool, prefix: str)
         → None

  L 180: forward(self, x: torch.Tensor)


CLASS: Glm4vRMSNorm
----------------------------------------
  L  38: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: Glm4vVisionBlock
----------------------------------------
  L  80: __init__(self, config: Glm4vVisionConfig, norm_layer: Optional[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)
         → None


CLASS: Glm4vVisionEmbeddings
----------------------------------------
  L 191: __init__(self, config: Glm4vVisionConfig)

  L 207: forward(self, embeddings, lengths, image_shapes, h_coords, w_coords)
         → torch.Tensor


CLASS: Glm4vVisionMLP
----------------------------------------
  L  47: __init__(self, in_features: int, hidden_features: int, bias: bool, quant_config: Optional[QuantizationConfig], prefix: str)

  L  72: forward(self, x: torch.Tensor)


CLASS: Glm4vVisionModel
----------------------------------------
  L 320: __init__(self, vision_config: Glm4vVisionConfig, norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 387: dtype(self)
         → torch.dtype

  L 391: device(self)
         → torch.device

  L 394: rot_pos_emb(self, grid_thw: torch.Tensor)
         → torch.Tensor

  L 426: forward(self, x: torch.Tensor, grid_thw: torch.Tensor)
         → torch.Tensor


CLASS: Glm4vVisionPatchEmbed
----------------------------------------
  L 110: __init__(self, patch_size: int, temporal_patch_size: int, in_channels: int, hidden_size: int)
         → None

  L 132: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: Glm4vVisionRotaryEmbedding
----------------------------------------
  L 282: __init__(self, dim: int, theta: float)
         → None

  L 291: update_freqs_cache(self, seqlen: int)
         → None

  L 314: forward(self, seqlen: int)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/glm4v_moe.py
Functions: 3
============================================================


CLASS: Glm4vMoeForConditionalGeneration
----------------------------------------
  L  34: __init__(self, config: Glm4vMoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  77: determine_num_fused_shared_experts(self, architecture: str)

  L 107: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn)


============================================================
FILE: python/sglang/srt/models/gpt2.py
Functions: 11
============================================================


CLASS: GPT2Attention
----------------------------------------
  L  44: __init__(self, layer_id: int, config: GPT2Config, quant_config: Optional[QuantizationConfig], prefix: str)

  L  84: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: GPT2Block
----------------------------------------
  L 136: __init__(self, layer_id: int, config: GPT2Config, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)

  L 161: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: GPT2LMHeadModel
----------------------------------------
  L 233: __init__(self, config: GPT2Config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 249: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 260: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: GPT2MLP
----------------------------------------
  L  98: __init__(self, intermediate_size: int, config: GPT2Config, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)

  L 124: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: GPT2Model
----------------------------------------
  L 185: __init__(self, config: GPT2Config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 213: forward(self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/gpt_bigcode.py
Functions: 11
============================================================


CLASS: GPTBigCodeAttention
----------------------------------------
  L  43: __init__(self, layer_id: int, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L  94: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: GPTBigCodeBlock
----------------------------------------
  L 151: __init__(self, layer_id: int, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 171: forward(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: GPTBigCodeForCausalLM
----------------------------------------
  L 254: __init__(self, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 273: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 284: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: GPTBigCodeModel
----------------------------------------
  L 194: __init__(self, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 224: forward(self, input_ids: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: GPTBigMLP
----------------------------------------
  L 115: __init__(self, intermediate_size: int, config: GPTBigCodeConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 142: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/gpt_oss.py
Functions: 27
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  99: def get_attention_sliding_window_size(config)


CLASS: GptOssAttention
----------------------------------------
  L 223: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, head_dim: Optional[int], rms_norm_eps: float, attention_bias: bool, quant_config: Optional[QuantizationConfig], prefix: str, sliding_window_size: int, layer_type: str, params_dtype: torch.dtype)
         → None

  L 323: forward_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)

  L 351: forward_core(self, intermediate_state)

  L 363: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: GptOssConfig
----------------------------------------
  L  90: __init__(self)


CLASS: GptOssDecoderLayer
----------------------------------------
  L 378: __init__(self, config: GptOssConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, sliding_window_size: int | None)
         → None

  L 463: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: GptOssForCausalLM
----------------------------------------
  L 598: __init__(self, config: GptOssConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 630: routed_experts_weights_of_layer(self)

  L 634: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         → torch.Tensor

  L 666: start_layer(self)

  L 670: end_layer(self)

  L 741: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], is_nextn: bool, weight_name_mapping: dict)

  L1125: get_embed_and_head(self)

  L1128: set_embed_and_head(self, embed, head)

  L1136: set_eagle3_layers_to_capture(self, layer_ids: Optional[List[int]])

  L1151: get_model_config_for_expert_location(cls, config)

  L1158: get_attention_sliding_window_size(self)


CLASS: GptOssModel
----------------------------------------
  L 505: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str, decoder_layer_type: type[nn.Module])
         → None

  L 548: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         → Union[torch.Tensor, PPProxyTensors]


CLASS: GptOssSparseMoeBlock
----------------------------------------
  L 104: __init__(self, layer_id: int, config: GptOssConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 160: forward(self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch], should_allreduce_fusion: bool)
         → torch.Tensor

  L 171: get_moe_weights(self)

  L 178: forward_normal(self, hidden_states: torch.Tensor, should_allreduce_fusion: bool)
         → torch.Tensor


CLASS: _WeightCreator
----------------------------------------
  L1204: __init__(self, fn)

  L1208: maybe_materialize(obj)


============================================================
FILE: python/sglang/srt/models/granite.py
Functions: 14
============================================================


CLASS: GraniteAttention
----------------------------------------
  L  90: __init__(self, config: GraniteConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 165: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: GraniteDecoderLayer
----------------------------------------
  L 180: __init__(self, config: GraniteConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 225: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: GraniteForCausalLM
----------------------------------------
  L 306: __init__(self, config: GraniteConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 349: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         → LogitsProcessorOutput

  L 366: get_module_name_from_weight_name(self, name)

  L 375: get_num_params(self)

  L 379: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 431: get_weights_by_name(self, name: str, truncate_size: int, tp_size: int)
         → Optional[torch.Tensor]
         📝 Get the weights of the parameter by its name. Similar to `get_paramete


CLASS: GraniteMLP
----------------------------------------
  L  52: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  82: forward(self, x)


CLASS: GraniteModel
----------------------------------------
  L 254: __init__(self, config: GraniteConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 280: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/granitemoe.py
Functions: 12
============================================================


CLASS: GraniteMoeAttention
----------------------------------------
  L  94: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, max_position: int, layer_id: int, rope_theta: float, quant_config: Optional[QuantizationConfig], attention_multiplier: Optional[float], prefix: str)
         → None

  L 165: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: GraniteMoeDecoderLayer
----------------------------------------
  L 181: __init__(self, config: GraniteConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 219: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: GraniteMoeForCausalLM
----------------------------------------
  L 300: __init__(self, config: GraniteConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 331: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         → LogitsProcessorOutput

  L 348: load_weights(self, weights: Iterable[tuple[str, torch.Tensor]])
         → set[str]


CLASS: GraniteMoeMoE
----------------------------------------
  L  40: __init__(self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], tp_size: Optional[int], prefix: str)

  L  82: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: GraniteMoeModel
----------------------------------------
  L 244: __init__(self, config: GraniteConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 271: get_input_embeddings(self, input_ids: torch.Tensor)
         → torch.Tensor

  L 274: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor])
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/grok.py
Functions: 18
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 218: def get_rope_scaling(config)


CLASS: Grok1Attention
----------------------------------------
  L 337: __init__(self, config: PretrainedConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, max_position: int, rope_theta: float, quant_config: Optional[QuantizationConfig], reduce_results: bool, alt_stream: Optional[torch.cuda.Stream], load_presharded_attn: bool, prefix: str)
         → None

  L 457: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: Grok1DecoderLayer
----------------------------------------
  L 538: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], load_presharded_moe: bool, load_presharded_attn: bool, load_presharded_mlp: bool, alt_stream: Optional[torch.cuda.Stream], skip_moe: bool, prefix: str)
         → None

  L 630: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], deferred_norm: Optional[RMSNorm])
         → Tuple[torch.Tensor, torch.Tensor, RMSNorm]

  L 696: moe_with_rmoe(self, x)


CLASS: Grok1ForCausalLM
----------------------------------------
  L 808: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 893: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor

  L 908: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], ignore_parent_name: bool, check_hit_names: bool, model_config: PretrainedConfig | None)
         → dict[str, torch.Tensor]

  L1025: get_num_params_analytical(self)

  L1073: get_num_params_torch(self)


CLASS: Grok1MLP
----------------------------------------
  L  88: __init__(self, hidden_size: int, intermediate_size: int, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, reduce_results, use_presharded_weights: bool, split_gate_up: bool)
         → None

  L 121: forward(self, x)


CLASS: Grok1MoE
----------------------------------------
  L 137: __init__(self, config: PretrainedConfig, layer_id: int, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], tp_size: Optional[int], reduce_results: bool, use_presharded_weights: bool, inplace: bool, no_combine: bool, prefix: str)

  L 201: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: Grok1Model
----------------------------------------
  L 708: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], load_presharded_moe: bool, load_presharded_embedding: bool, load_presharded_attn: bool, load_presharded_mlp: bool, replicate_embedding: bool, prefix: str)
         → None

  L 749: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


CLASS: ScalingRotaryEmbedding
----------------------------------------
  L 247: __init__(self, head_size: int, rotary_dim: int, max_position_embeddings: int, base: int, is_neox_style: bool, scaling_factor: float, dtype: torch.dtype)
         → None


============================================================
FILE: python/sglang/srt/models/hunyuan.py
Functions: 17
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 213: def get_head_dim(config)

  L 224: def check_head_dim(config)


CLASS: HunYuanAttention
----------------------------------------
  L 251: __init__(self, config: PretrainedConfig, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], bias: bool, prefix: str, attention_type: str, layer_id: int)
         → None

  L 351: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, kv_states: Optional[Tuple[torch.Tensor]])
         → torch.Tensor


CLASS: HunYuanDecoderLayer
----------------------------------------
  L 392: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str, layer_id: int)
         → None

  L 462: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], kv_states: Optional[Tuple[torch.Tensor]])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: HunYuanMLP
----------------------------------------
  L  82: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], bias: bool, prefix: str, reduce_results: bool)
         → None

  L 115: forward(self, x)


CLASS: HunYuanMoEV1ForCausalLM
----------------------------------------
  L 585: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig])
         → None

  L 613: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor

  L 642: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 788: load_kv_cache_scales(self, quantization_param_path: str)
         → None


CLASS: HunYuanModel
----------------------------------------
  L 491: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 522: get_input_embeddings(self, input_ids: torch.Tensor)
         → torch.Tensor

  L 525: forward(self, input_ids: Optional[torch.Tensor], positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor])
         → torch.Tensor


CLASS: HunYuanSparseMoeBlock
----------------------------------------
  L 124: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], layer_id: int)

  L 192: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/idefics2.py
Functions: 13
============================================================


CLASS: Idefics2Encoder
----------------------------------------
  L 131: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 151: forward(self, inputs_embeds: torch.Tensor, cu_seqlens: torch.Tensor)
         → torch.Tensor
         📝 Args:


CLASS: Idefics2EncoderLayer
----------------------------------------
  L  69: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  98: forward(self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor)
         → torch.Tensor
         📝 Args:


CLASS: Idefics2VisionEmbeddings
----------------------------------------
  L 189: __init__(self, config: PretrainedConfig)

  L 206: get_position_ids(self, pixel_values: torch.FloatTensor, patch_attention_mask: torch.BoolTensor, tgt_sizes: Optional[torch.IntTensor])

  L 248: forward(self, pixel_values: torch.FloatTensor, patch_attention_mask: torch.BoolTensor, tgt_sizes: Optional[torch.IntTensor])
         → torch.Tensor


CLASS: Idefics2VisionMLP
----------------------------------------
  L  36: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  60: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: Idefics2VisionTransformer
----------------------------------------
  L 270: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], require_post_norm: bool, prefix: str)
         → None

  L 293: get_input_embeddings(self)
         → nn.Embedding

  L 296: compute_cu_seqlens(self, tgt_sizes: Optional[torch.Tensor], input_embeds: Optional[torch.Tensor])
         → torch.Tensor

  L 325: forward(self, pixel_values, patch_attention_mask: Optional[torch.BoolTensor], tgt_sizes: Optional[torch.IntTensor])
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/internlm2.py
Functions: 12
============================================================


CLASS: InternLM2Attention
----------------------------------------
  L  83: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 152: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: InternLM2ForCausalLM
----------------------------------------
  L 276: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 293: get_input_embeddings(self)
         → nn.Embedding

  L 297: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor

  L 309: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: InternLM2MLP
----------------------------------------
  L  45: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  75: forward(self, x)


CLASS: InternLM2Model
----------------------------------------
  L 226: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 251: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


CLASS: InternLMDecoderLayer
----------------------------------------
  L 167: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 200: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


============================================================
FILE: python/sglang/srt/models/internlm2_reward.py
Functions: 3
============================================================


CLASS: InternLM2ForRewardModel
----------------------------------------
  L  29: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  46: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         → EmbeddingPoolerOutput

  L  60: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/interns1.py
Functions: 7
============================================================


CLASS: InternS1ForConditionalGeneration
----------------------------------------
  L  30: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], use_flash_attn)
         → None

  L  95: pixel_shuffle(self, x, scale_factor)

  L 117: extract_feature(self, pixel_values)

  L 135: get_image_feature(self, items: List[MultimodalDataItem])
         📝 Projects the last hidden state from the vision model into language mod

  L 147: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor

  L 167: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 209: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/internvl.py
Functions: 23
============================================================


CLASS: InternAttention
----------------------------------------
  L  36: __init__(self, config, quant_config: QuantizationConfig)

  L  66: forward(self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor)
         → torch.Tensor


CLASS: InternMLP
----------------------------------------
  L 165: __init__(self, config: PretrainedConfig)

  L 172: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: InternRMSNorm
----------------------------------------
  L 151: __init__(self, hidden_size, eps)

  L 156: forward(self, hidden_states)


CLASS: InternVLChatModel
----------------------------------------
  L 406: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], use_flash_attn)
         → None

  L 465: pixel_shuffle(self, x, scale_factor)

  L 487: extract_feature(self, pixel_values)

  L 505: get_image_feature(self, items: List[MultimodalDataItem])
         📝 Projects the last hidden state from the vision model into language mod

  L 517: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor

  L 537: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 547: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: InternVisionEmbeddings
----------------------------------------
  L  77: __init__(self, config: PretrainedConfig)

  L 130: forward(self, pixel_values: torch.FloatTensor)
         → torch.Tensor


CLASS: InternVisionEncoder
----------------------------------------
  L 249: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig])

  L 268: forward(self, inputs_embeds, output_hidden_states: Optional[bool], return_dict: Optional[bool])
         → Union[Tuple, BaseModelOutput]
         📝 Args:


CLASS: InternVisionEncoderLayer
----------------------------------------
  L 187: __init__(self, config: PretrainedConfig, drop_path_rate: float, quant_config: QuantizationConfig)

  L 211: forward(self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor)
         → Tuple[torch.FloatTensor, Optional[torch.FloatTensor], Optional[Tuple[torch.FloatTensor]]]
         📝 Args:


CLASS: InternVisionModel
----------------------------------------
  L 320: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig])

  L 333: resize_pos_embeddings(self, old_size, new_size, patch_size)

  L 356: get_input_embeddings(self)

  L 359: forward(self, pixel_values: Optional[torch.FloatTensor], output_hidden_states: Optional[bool], return_dict: Optional[bool], pixel_embeds: Optional[torch.FloatTensor])
         → Union[Tuple, BaseModelOutputWithPooling]


============================================================
FILE: python/sglang/srt/models/kimi_vl.py
Functions: 8
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 300: def get_spec_layer_idx_from_weight_name(config: DeepseekV2Config,
        weight_name: str)
         → Optional[int]


CLASS: KimiVLForConditionalGeneration
----------------------------------------
  L 122: __init__(self, config: KimiVLConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 145: get_image_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor

  L 160: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 164: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)

  L 183: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: KimiVLMultiModalProjector
----------------------------------------
  L  96: __init__(self, config: KimiVLConfig)

  L 113: forward(self, image_features: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/kimi_vl_moonvit.py
Functions: 25
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  63: def multihead_attention(q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        q_cu_seqlens: Optional[torch.Tensor],
        k_cu_seqlens: Optional[torch.Tensor])
         📝 Multi-head attention using flash attention 2.

  L 115: def sdpa_attention(q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        q_cu_seqlens: Optional[torch.Tensor],
        k_cu_seqlens: Optional[torch.Tensor])
         → torch.Tensor
         📝 Multi-head attention using torch scaled dot product attention.

  L 170: def apply_rope(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor)
         → tuple[torch.Tensor, torch.Tensor]
         📝 Args: (The leading dimensions of all inputs should be the same)

  L 536: def patch_merger(x: torch.Tensor,
        grid_hw: torch.Tensor,
        merge_kernel_size: list[int,
        int])
         → List[torch.Tensor]


CLASS: Learnable2DInterpPosEmb
----------------------------------------
  L 195: __init__(self, height: int, width: int, dim: int, interpolation_mode: str)
         → None

  L 205: reset_parameters(self)

  L 208: forward(self, x: torch.Tensor, grid_hws: torch.Tensor)
         → torch.Tensor


CLASS: MLP2
----------------------------------------
  L 396: __init__(self, dims: list[int], activation, bias)

  L 407: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: MoonVisionPatchEmbed
----------------------------------------
  L 230: __init__(self, out_dim: int, in_dim: int, patch_size: Union[int, Tuple[int, int]], pos_emb_height: int, pos_emb_width: int)

  L 257: forward(self, x: torch.Tensor, grid_hw: torch.Tensor)
         → torch.Tensor
         📝 Args:


CLASS: MoonVitEncoder
----------------------------------------
  L 497: __init__(self, hidden_dim: int, num_layers: int, block_cfg: dict)
         → None

  L 513: forward(self, hidden_states: torch.Tensor, grid_hw: torch.Tensor)
         → torch.Tensor


CLASS: MoonVitEncoderLayer
----------------------------------------
  L 415: __init__(self, num_heads: int, hidden_dim: int, mlp_dim: int)

  L 437: attention_qkvpacked(self, x: torch.Tensor, cu_seqlens: torch.Tensor, rope_freqs_cis: Optional[torch.Tensor])
         📝 Args:

  L 469: forward(self, hidden_states: torch.Tensor, cu_seqlens: torch.Tensor, rope_freqs_cis: Union[torch.Tensor, None])
         → torch.Tensor
         📝 Args:


CLASS: MoonVitPretrainedModel
----------------------------------------
  L 598: __init__(self, config: MoonViTConfig)

  L 623: forward(self, pixel_values: torch.Tensor, grid_hw: torch.Tensor)
         → torch.Tensor
         📝 Args:


CLASS: MoonVitVLProjector
----------------------------------------
  L 567: __init__(self, in_channels: int, merge_kernel_size: list[int, int], hidden_act: str, ln_eps: float, out_dim: int)

  L 583: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: Rope2DPosEmb
----------------------------------------
  L 294: __init__(self, dim: int, max_height: int, max_width: int, theta_base, device)

  L 305: extra_repr(self)

  L 309: precomputed_freqs_cis(self)
         → torch.Tensor
         📝 Calculate the cis(freqs) for each position in the 2D grid.

  L 337: get_freqs_cis_by_seqlens(self, grid_hws: torch.Tensor)
         → torch.Tensor
         📝 Args:

  L 361: get_freqs_cis_by_idx(self, pos_idx: torch.Tensor, pos_idx_mask: torch.Tensor)
         → torch.Tensor
         📝 Args:


============================================================
FILE: python/sglang/srt/models/llama.py
Functions: 25
============================================================


CLASS: LlamaAttention
----------------------------------------
  L 110: __init__(self, config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str, bias: bool)
         → None

  L 188: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: LlamaDecoderLayer
----------------------------------------
  L 203: __init__(self, config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 253: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: LlamaForCausalLM
----------------------------------------
  L 411: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 456: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors])
         → LogitsProcessorOutput

  L 492: forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)
         → Optional[LogitsProcessorOutput]

  L 533: start_layer(self)

  L 537: end_layer(self)

  L 540: get_input_embeddings(self)
         → nn.Embedding

  L 543: get_module_name_from_weight_name(self, name)

  L 552: get_num_params(self)

  L 556: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 624: get_weights_by_name(self, name: str, truncate_size: int, tp_size: int)
         → Optional[torch.Tensor]
         📝 Get the weights of the parameter by its name. Similar to `get_paramete

  L 697: get_embed_and_head(self)

  L 700: set_embed_and_head(self, embed, head)

  L 708: get_embed(self)

  L 711: set_embed(self, embed)

  L 723: load_kv_cache_scales(self, quantization_param_path: str)
         → None

  L 726: set_eagle3_layers_to_capture(self, layer_ids: Optional[List[int]])


CLASS: LlamaMLP
----------------------------------------
  L  62: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str, reduce_results: bool)
         → None

  L  94: forward(self, x, forward_batch, use_reduce_scatter: bool)


CLASS: LlamaModel
----------------------------------------
  L 279: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 316: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         → Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]], PPProxyTensors]

  L 367: load_kv_cache_scales(self, quantization_param_path: str)
         → None


============================================================
FILE: python/sglang/srt/models/llama4.py
Functions: 11
============================================================


CLASS: Llama4Attention
----------------------------------------
  L 193: __init__(self, config: Llama4TextConfig, layer_id: int, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], bias: bool, bias_o_proj: bool, prefix: str)
         → None

  L 317: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: Llama4DecoderLayer
----------------------------------------
  L 353: __init__(self, config: Llama4TextConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 426: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: Llama4ForCausalLM
----------------------------------------
  L 532: __init__(self, config: Llama4TextConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 540: get_input_embeddings(self)


CLASS: Llama4MoE
----------------------------------------
  L  71: custom_routing_function(hidden_states: torch.Tensor, gating_output: torch.Tensor, topk: int, renormalize: bool)
         → Tuple[torch.Tensor, torch.Tensor]

  L  86: __init__(self, config: Llama4TextConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 133: forward(self, hidden_states, forward_batch: ForwardBatch, use_reduce_scatter: bool)


CLASS: Llama4Model
----------------------------------------
  L 465: __init__(self, config: Llama4TextConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 493: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         → Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]]]


============================================================
FILE: python/sglang/srt/models/llama_classification.py
Functions: 3
============================================================


CLASS: LlamaForClassification
----------------------------------------
  L  30: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  49: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         → EmbeddingPoolerOutput

  L  67: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/llama_eagle.py
Functions: 5
============================================================


CLASS: LlamaDecoderLayer
----------------------------------------
  L  40: __init__(self, config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None


CLASS: LlamaForCausalLMEagle
----------------------------------------
  L 114: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 142: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: LlamaModel
----------------------------------------
  L  57: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  84: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/llama_eagle3.py
Functions: 7
============================================================


CLASS: LlamaDecoderLayer
----------------------------------------
  L  43: __init__(self, config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  74: forward(self, positions: torch.Tensor, embeds: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: LlamaForCausalLMEagle3
----------------------------------------
  L 169: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 206: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])
         → None

  L 249: get_hot_token_id(self)


CLASS: LlamaModel
----------------------------------------
  L 104: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 134: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/llama_embedding.py
Functions: 3
============================================================


CLASS: LlamaEmbeddingModel
----------------------------------------
  L  15: __init__(self, config: LlamaConfig, quant_config, prefix: str)
         → None

  L  28: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         → EmbeddingPoolerOutput

  L  42: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/llama_reward.py
Functions: 8
============================================================


CLASS: LlamaForSequenceClassification
----------------------------------------
  L  29: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  48: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         → EmbeddingPoolerOutput

  L  66: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: LlamaForSequenceClassificationWithNormal_Weights
----------------------------------------
  L  85: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  95: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         → EmbeddingPoolerOutput

  L 119: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Weights
----------------------------------------
  L  72: __init__(self, hidden_size, num_label)

  L  82: forward(self, x)


============================================================
FILE: python/sglang/srt/models/llava.py
Functions: 14
============================================================


CLASS: LlavaBaseForCausalLM
----------------------------------------
  L  58: pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)

  L 126: encode_images(self, pixel_values: Union[torch.Tensor, List[torch.Tensor]])
         → torch.Tensor
         📝 encode images by vision tower and multimodal projector

  L 151: forward(self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 440: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 498: num_patches_per_side(self)


CLASS: LlavaForConditionalGeneration
----------------------------------------
  L 615: dtype(self)

  L 618: pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)

  L 670: __init__(self, config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 747: get_image_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor
         📝 Extract features from image inputs.

  L 782: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)

  L 803: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])
         📝 Load weights for LlavaForConditionalGeneration.


CLASS: LlavaLlamaForCausalLM
----------------------------------------
  L 503: __init__(self, config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None


CLASS: LlavaMistralForCausalLM
----------------------------------------
  L 566: __init__(self, config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None


CLASS: LlavaQwenForCausalLM
----------------------------------------
  L 529: __init__(self, config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None


============================================================
FILE: python/sglang/srt/models/llavavid.py
Functions: 6
============================================================


CLASS: LlavaVidForCausalLM
----------------------------------------
  L  33: __init__(self, config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  60: pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)

  L  77: encode_images(self, pixel_values: torch.Tensor)
         → torch.Tensor

  L 109: forward(self, input_ids: torch.LongTensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 221: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 279: num_patches_per_side(self)


============================================================
FILE: python/sglang/srt/models/mimo.py
Functions: 8
============================================================


CLASS: MiMoForCausalLM
----------------------------------------
  L  66: __init__(self, config: MiMoConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  90: get_input_embeddings(self, input_ids: torch.Tensor)
         → torch.Tensor

  L  94: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         → torch.Tensor

  L 110: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 156: get_embed_and_head(self)

  L 159: set_embed_and_head(self, embed, head)

  L 167: load_kv_cache_scales(self, quantization_param_path: str)
         → None


CLASS: MiMoModel
----------------------------------------
  L  32: __init__(self, config: MiMoConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None


============================================================
FILE: python/sglang/srt/models/mimo_mtp.py
Functions: 8
============================================================


CLASS: MiMoMTP
----------------------------------------
  L  84: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 108: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 119: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 173: map_model_name_to_mtp_param_name(self, name: str)
         → str

  L 192: get_embed_and_head(self)

  L 195: set_embed_and_head(self, embed, head)


CLASS: MiMoMultiTokenPredictorLayer
----------------------------------------
  L  25: __init__(self, config: PretrainedConfig, prefix: str, quant_config: Optional[QuantizationConfig])
         → None

  L  47: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/minicpm.py
Functions: 11
============================================================


CLASS: MiniCPMAttention
----------------------------------------
  L  82: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 151: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: MiniCPMDecoderLayer
----------------------------------------
  L 169: __init__(self, config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 205: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: MiniCPMForCausalLM
----------------------------------------
  L 291: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 319: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor

  L 336: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: MiniCPMMLP
----------------------------------------
  L  44: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  74: forward(self, x)


CLASS: MiniCPMModel
----------------------------------------
  L 236: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 265: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/minicpm3.py
Functions: 12
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  86: def input_to_float8(x, dtype)


CLASS: MiniCPM3AttentionMLA
----------------------------------------
  L  97: __init__(self, config: PretrainedConfig, hidden_size: int, num_heads: int, qk_nope_head_dim: int, qk_rope_head_dim: int, v_head_dim: int, q_lora_rank: int, kv_lora_rank: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], layer_id, prefix: str)
         → None

  L 202: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: MiniCPM3DecoderLayer
----------------------------------------
  L 271: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 315: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: MiniCPM3ForCausalLM
----------------------------------------
  L 401: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 429: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor

  L 446: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: MiniCPM3MLP
----------------------------------------
  L  49: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  79: forward(self, x)


CLASS: MiniCPM3Model
----------------------------------------
  L 346: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 375: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/minicpmo.py
Functions: 40
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  71: def apply_spk_emb(input_ids: torch.Tensor,
        spk_emb: torch.Tensor,
        input_embeds: torch.Tensor,
        spk_emb_token_id: int,
        num_spk_embs: int)
         📝 Replace consecutive `num_spk_embs` speaker embedding placeholders in i

  L 126: def make_streaming_chunk_mask_generation(inputs_embeds: torch.Tensor,
        past_seen_tokens: int,
        streaming_tts_text_mask: torch.Tensor,
        streaming_reserved_length: int,
        streaming_audio_chunk_size: int,
        streaming_text_chunk_size: int,
        num_spk_emb: int,
        use_spk_emb: bool)
         → torch.Tensor
         📝 In streaming audio generation, determine which `text` positions the TT


CLASS: ConditionalChatTTS
----------------------------------------
  L 551: __init__(self, config: PretrainedConfig)

  L 610: merge_inputs_embeds(self, input_ids: torch.Tensor, lm_spk_emb_last_hidden_states: Optional[torch.Tensor])
         📝 Merge `input_ids` and `lm_spk_emb_last_hidden_states` to `inputs_embed

  L 656: prefill_text(self, input_ids: torch.Tensor, position_ids: torch.LongTensor, past_key_values: List[Tuple[torch.Tensor, torch.Tensor]], lm_spk_emb_last_hidden_states: Optional[torch.Tensor])
         📝 Prefill a chunk of new text tokens in streaming setting.

  L 728: prefill_audio_ids(self, input_ids: torch.Tensor, past_key_values: List[Tuple[torch.Tensor, torch.Tensor]], streaming_tts_text_mask, add_audio_bos: bool)
         📝 Prefill a chunk of audio ids to the model. Used in sliding-window long

  L 788: generate(self, input_ids: torch.Tensor, past_key_values: List[Tuple[torch.Tensor, torch.Tensor]], temperature: torch.Tensor, eos_token: Union[int, torch.Tensor], streaming_tts_text_mask, force_no_stop, min_new_token, max_new_token, logits_warpers: List[LogitsWarper], logits_processors: List[CustomRepetitionPenaltyLogitsProcessorRepeat], show_tqdm)
         📝 Generate audio codes in streaming setting or non-streaming setting.

  L1051: decode_to_mel_specs(self, result_list: List[torch.Tensor])
         📝 Decode discrete audio codes to mel spectrograms.


CLASS: ConvNeXtBlock
----------------------------------------
  L 203: __init__(self, dim: int, intermediate_dim: int, kernel: int, dilation: int, layer_scale_init_value: float)

  L 232: forward(self, x: torch.Tensor, cond)
         → torch.Tensor


CLASS: CustomRepetitionPenaltyLogitsProcessorRepeat
----------------------------------------
  L 426: __init__(self, penalty: float, max_input_ids: int, past_window: int)

  L 436: __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor)
         → torch.FloatTensor


CLASS: DVAE
----------------------------------------
  L 345: __init__(self)

  L 386: forward(self, inp: torch.Tensor, mode: Literal['encode', 'decode'])
         → torch.Tensor


CLASS: DVAEDecoder
----------------------------------------
  L 257: __init__(self, idim: int, odim: int, n_layer, bn_dim, hidden, kernel, dilation, up)

  L 288: forward(self, x: torch.Tensor, conditioning)
         → torch.Tensor


CLASS: GFSQ
----------------------------------------
  L 302: __init__(self, dim: int, levels: List[int], G: int, R: int, eps, transpose)

  L 331: __call__(self, x: torch.Tensor)
         → torch.Tensor

  L 334: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: MiniCPMO
----------------------------------------
  L1417: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig])
         → None

  L1459: init_tts_module(self)

  L1463: init_audio_module(self)

  L1467: init_llm(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → nn.Module

  L1475: init_vision_module(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L1496: init_resampler(self, embed_dim: int, vision_dim: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → nn.Module

  L1516: pad_input_ids(self, input_ids: List[int], mm_input: MultimodalInputs)

  L1547: get_audio_embedding_streaming(self, items: List[MultimodalDataItem])
         📝 Extract audio embeddings in a streaming manner using cached key-value 

  L1614: subsequent_chunk_mask(self, size: int, chunk_size: int, num_left_chunks: int, device: torch.device, num_lookhead: int)
         → torch.Tensor
         📝 Create mask for subsequent steps (size, size) with chunk size,

  L1647: get_audio_embedding(self, items: List[MultimodalDataItem], chunk_length)
         📝 Extract full audio embeddings with optional chunk-based attention.

  L1746: get_audio_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor

  L1754: get_omni_embedding(self, items: List[MultimodalDataItem], chunk_length, stream_input)
         📝 Args:

  L1778: get_image_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor

  L1817: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L1834: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: MiniCPMWhisperEncoder
----------------------------------------
  L1186: __init__(self, config: WhisperConfig)

  L1195: forward(self, input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, past_key_values: Optional[EncoderDecoderCache], use_cache: Optional[bool])
         📝 Forward pass of the Whisper encoder.


CLASS: MiniCPMWhisperEncoderLayer
----------------------------------------
  L1090: __init__(self, config: WhisperConfig, layer_idx: int)

  L1108: forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, layer_head_mask: torch.Tensor, output_attentions: bool, past_key_values: Optional[EncoderDecoderCache], use_cache: Optional[bool])
         → torch.Tensor
         📝 Args:


CLASS: MultiModalProjector
----------------------------------------
  L1404: __init__(self, in_dim, out_dim)

  L1410: forward(self, audio_features)


============================================================
FILE: python/sglang/srt/models/minicpmv.py
Functions: 27
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  65: def get_1d_sincos_pos_embed_from_grid(embed_dim: int,
        pos: np.ndarray,
        version: Tuple[int,
        int])
         → torch.Tensor
         📝 embed_dim: output dimension for each position

  L  92: def get_2d_sincos_pos_embed_from_grid(embed_dim: int,
        grid: np.ndarray,
        version: Tuple[int,
        int])
         → torch.Tensor

  L 112: def get_2d_sincos_pos_embed(embed_dim: int,
        grid_size: Union[int,
        Tuple[int,
        int]],
        cls_token: bool,
        version: Tuple[int,
        int])
         → torch.Tensor
         📝 grid_size: int of the grid height and width

  L 358: def get_version_by_config(config: PretrainedConfig)
         → Tuple[int, ...]


CLASS: BaseResampler
----------------------------------------
  L 201: __init__(self, num_queries: int, embed_dim: int, num_heads: int, kv_dim: Optional[int], norm_layer: Callable[[int], nn.LayerNorm], do_post_projection: bool, quant_config: Optional[QuantizationConfig], prefix: str)
         → None


CLASS: MiniCPMBaseModel
----------------------------------------
  L 378: __init__(self)

  L 527: get_embedding(self, input_ids: torch.Tensor, image_inputs: Optional[MiniCPMVImageInputs])
         → Tuple[torch.Tensor, torch.Tensor]

  L 563: get_input_embeddings(self)
         → nn.Embedding

  L 566: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 582: init_llm(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)
         → nn.Module

  L 590: init_vision_module(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → nn.Module

  L 598: init_resampler(self, embed_dim: int, vision_dim: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → nn.Module

  L 607: get_vision_embedding(self, pixel_values: List[torch.Tensor], patch_attn_mask: Optional[torch.Tensor], tgt_sizes: Optional[torch.Tensor])
         → torch.Tensor

  L 615: get_image_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor


CLASS: MiniCPMV
----------------------------------------
  L 796: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 824: __getattr__(self, name)

  L 829: __call__(self)

  L 832: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: MiniCPMV2_6
----------------------------------------
  L 659: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 668: init_llm(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)
         → nn.Module

  L 676: init_vision_module(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → nn.Module

  L 692: init_resampler(self, embed_dim: int, vision_dim: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → nn.Module

  L 712: get_vision_embedding(self, pixel_values: List[torch.Tensor], patch_attn_mask: Optional[torch.Tensor], tgt_sizes: Optional[torch.Tensor])
         → torch.Tensor

  L 725: get_image_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor

  L 764: pad_input_ids(self, input_ids: List[int], image_inputs: MultimodalInputs)


CLASS: Resampler2_5
----------------------------------------
  L 260: __init__(self, num_queries: int, embed_dim: int, num_heads: int, kv_dim: Optional[int], norm_layer: Callable[[int], nn.LayerNorm], max_size: Tuple[int, int], quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 309: forward(self, x: torch.Tensor, tgt_sizes: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/mistral.py
Functions: 5
============================================================


CLASS: Mistral3ForConditionalGeneration
----------------------------------------
  L  32: __init__(self)

  L  46: get_image_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor
         📝 Extract features from image inputs.

  L  83: __getattr__(self, name)

  L  86: __hasattr__(self, name)

  L  89: __call__(self)


============================================================
FILE: python/sglang/srt/models/mixtral.py
Functions: 13
============================================================


CLASS: MixtralAttention
----------------------------------------
  L 123: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, max_position: int, rope_theta: float, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 189: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: MixtralDecoderLayer
----------------------------------------
  L 204: __init__(self, config: MixtralConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 239: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → torch.Tensor


CLASS: MixtralForCausalLM
----------------------------------------
  L 341: __init__(self, config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 359: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         → torch.Tensor

  L 383: start_layer(self)

  L 387: end_layer(self)

  L 390: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: MixtralMoE
----------------------------------------
  L  66: __init__(self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], tp_size: Optional[int], prefix: str)

  L 109: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: MixtralModel
----------------------------------------
  L 265: __init__(self, config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 301: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         → Union[torch.Tensor, PPProxyTensors]


============================================================
FILE: python/sglang/srt/models/mixtral_quant.py
Functions: 13
============================================================


CLASS: MixtralAttention
----------------------------------------
  L 173: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, max_position: int, rope_theta: float, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 239: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: MixtralDecoderLayer
----------------------------------------
  L 254: __init__(self, config: MixtralConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 285: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → torch.Tensor


CLASS: MixtralMLP
----------------------------------------
  L  52: __init__(self, num_experts: int, hidden_size: int, intermediate_size: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  90: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: MixtralMoE
----------------------------------------
  L 100: __init__(self, config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 148: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: MixtralModel
----------------------------------------
  L 311: __init__(self, config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 339: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


CLASS: QuantMixtralForCausalLM
----------------------------------------
  L 361: __init__(self, config: MixtralConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 379: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor

  L 391: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/mllama.py
Functions: 32
============================================================


CLASS: ColumnParallelConv2dPatch
----------------------------------------
  L  56: __init__(self, in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Union[int, Tuple[int, int]], bias: bool)
         → None

  L  74: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: MllamaCrossAttentionDecoderLayer
----------------------------------------
  L 591: __init__(self, config: config_mllama.MllamaTextConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 622: forward(self, hidden_states: torch.Tensor, cross_attention_states: torch.Tensor, cross_attention_mask: torch.Tensor, full_text_row_masked_out_mask: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: MllamaForCausalLM
----------------------------------------
  L 740: __init__(self, config: config_mllama.MllamaTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 760: forward(self, input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], cross_attention_states: Optional[torch.LongTensor], cross_attention_mask: Optional[torch.LongTensor], full_text_row_masked_out_mask: Optional[Tuple[torch.Tensor, torch.Tensor]], forward_batch: ForwardBatch, skip_cross_attention: bool)
         → torch.Tensor


CLASS: MllamaForConditionalGeneration
----------------------------------------
  L 804: __init__(self, config: config_mllama.MllamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 840: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 913: flat_encoder_result(self, cross_attention_states: torch.Tensor, encoder_lens_need: List[int])

  L 939: get_full_text_row_masked_out_mask(self, forward_batch: ForwardBatch)

  L 963: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → Union[Tuple, CausalLMOutputWithPast]

  L1027: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: MllamaPrecomputedAspectRatioEmbedding
----------------------------------------
  L  83: __init__(self, config: config_mllama.MllamaVisionConfig, is_gated: bool)

  L  96: forward(self, hidden_state: torch.Tensor, aspect_ratio_ids: torch.Tensor)
         → torch.Tensor


CLASS: MllamaPrecomputedPositionEmbedding
----------------------------------------
  L 110: __init__(self, config: config_mllama.MllamaVisionConfig)

  L 130: forward(self, hidden_state: torch.Tensor, aspect_ratio_ids: torch.Tensor)
         → torch.Tensor


CLASS: MllamaTextCrossAttention
----------------------------------------
  L 499: __init__(self, config: Optional[config_mllama.MllamaTextConfig], layer_id: Optional[int], quant_config: Optional[QuantizationConfig], prefix: str)

  L 557: forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor], cross_attention_states: Optional[torch.Tensor], forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: MllamaTextModel
----------------------------------------
  L 654: __init__(self, config: config_mllama.MllamaTextConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 695: forward(self, input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], cross_attention_states: Optional[torch.LongTensor], cross_attention_mask: Optional[torch.LongTensor], full_text_row_masked_out_mask: Optional[Tuple[torch.Tensor, torch.Tensor]], forward_batch: ForwardBatch, skip_cross_attention: bool)
         → torch.Tensor


CLASS: MllamaTextRMSNorm
----------------------------------------
  L 482: __init__(self, hidden_size, eps)

  L 487: forward(self, hidden_states)

  L 494: extra_repr(self)


CLASS: MllamaVisionEncoder
----------------------------------------
  L 248: __init__(self, config: config_mllama.MllamaVisionConfig, quant_config: Optional[QuantizationConfig], num_layers, is_gated, output_hidden_states, prefix: str)

  L 272: forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor])
         → Union[Tuple, BaseModelOutput]


CLASS: MllamaVisionEncoderLayer
----------------------------------------
  L 185: __init__(self, config: config_mllama.MllamaVisionConfig, quant_config: Optional[QuantizationConfig], is_gated: bool, prefix: str)

  L 225: forward(self, hidden_state: torch.Tensor, attention_mask: Optional[torch.Tensor])


CLASS: MllamaVisionMLP
----------------------------------------
  L 152: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 176: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: MllamaVisionModel
----------------------------------------
  L 294: __init__(self, config: config_mllama.MllamaVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 350: apply_class_embedding(self, hidden_state: torch.Tensor)
         → torch.Tensor

  L 356: forward(self, pixel_values: torch.Tensor, aspect_ratio_ids: torch.Tensor, aspect_ratio_mask: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/mllama4.py
Functions: 27
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  91: def pixel_shuffle(input_tensor, shuffle_ratio)

  L 143: def apply_position_embedding(q, k, freqs_ci, shape)


CLASS: Llama4ForConditionalGeneration
----------------------------------------
  L 425: __init__(self, config: Llama4Config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 524: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 527: get_image_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor

  L 547: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 570: permute_qk_weight_for_rotary(self, name: str, loaded_weight: torch.Tensor)
         → Tuple[str, torch.Tensor]

  L 604: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])
         → Set[str]

  L 935: set_eagle3_layers_to_capture(self, layer_ids: Optional[List[int]])

  L 939: get_embed_and_head(self)

  L 951: set_embed_and_head(self, embed, head)

  L 958: get_embed(self)

  L 961: set_embed(self, embed)


CLASS: Llama4UnfoldConvolution
----------------------------------------
  L 266: __init__(self, config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)

  L 292: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: Llama4VisionEncoder
----------------------------------------
  L 220: __init__(self, config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)

  L 241: forward(self, hidden_states: torch.Tensor, freqs_ci: torch.Tensor)
         → torch.Tensor
         📝 Args:


CLASS: Llama4VisionEncoderLayer
----------------------------------------
  L 156: __init__(self, config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)

  L 197: forward(self, hidden_state: torch.Tensor, freqs_ci: torch.Tensor)


CLASS: Llama4VisionMLP
----------------------------------------
  L  51: __init__(self, input_size: int, intermediate_size: int, output_size: int, bias: bool, output_activation: bool, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)

  L  82: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: Llama4VisionModel
----------------------------------------
  L 332: __init__(self, config: Llama4VisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 377: forward(self, pixel_values: torch.Tensor)
         → torch.Tensor


CLASS: Llama4VisionPixelShuffleMLP
----------------------------------------
  L 118: __init__(self, config, quant_config: Optional[QuantizationConfig], prefix: str, use_data_parallel: bool)

  L 138: forward(self, encoded_patches: torch.Tensor)
         → torch.Tensor


CLASS: Llama4VisionRotaryEmbedding
----------------------------------------
  L 300: __init__(self, config)

  L 326: forward(self, hidden_states)


============================================================
FILE: python/sglang/srt/models/nemotron_nas.py
Functions: 9
============================================================


CLASS: DeciLMDecoderLayer
----------------------------------------
  L  59: __init__(self, config: LlamaConfig, layer_idx: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 127: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: DeciLMForCausalLM
----------------------------------------
  L 295: __init__(self)

  L 341: get_input_embeddings(self, input_ids: torch.Tensor)
         → torch.Tensor

  L 345: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor], get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors])
         → LogitsProcessorOutput

  L 371: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])
         → None


CLASS: DeciModel
----------------------------------------
  L 160: __init__(self)

  L 210: get_input_embeddings(self, input_ids: torch.Tensor)
         → torch.Tensor

  L 213: forward(self, input_ids: Optional[torch.Tensor], positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor], pp_proxy_tensors: Optional[PPProxyTensors])
         → Union[torch.Tensor, PPProxyTensors]


============================================================
FILE: python/sglang/srt/models/olmo.py
Functions: 11
============================================================


CLASS: OlmoAttention
----------------------------------------
  L  51: __init__(self, config: OlmoConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 108: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: OlmoDecoderLayer
----------------------------------------
  L 180: __init__(self, config: OlmoConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 211: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]


CLASS: OlmoForCausalLM
----------------------------------------
  L 299: __init__(self, config: OlmoConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 321: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor

  L 338: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: OlmoMLP
----------------------------------------
  L 131: __init__(self, config: OlmoConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 163: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: OlmoModel
----------------------------------------
  L 233: __init__(self, config: OlmoConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 261: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor
         📝 :param input_ids: A tensor of shape `(batch_size, seq_len)`.


============================================================
FILE: python/sglang/srt/models/olmo2.py
Functions: 11
============================================================


CLASS: Olmo2Attention
----------------------------------------
  L  58: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 148: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: Olmo2DecoderLayer
----------------------------------------
  L 219: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 244: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: Olmo2ForCausalLM
----------------------------------------
  L 330: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 354: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor

  L 371: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Olmo2MLP
----------------------------------------
  L 170: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 202: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: Olmo2Model
----------------------------------------
  L 266: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 292: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor
         📝 :param input_ids: A tensor of shape `(batch_size, seq_len)`.


============================================================
FILE: python/sglang/srt/models/olmoe.py
Functions: 11
============================================================


CLASS: OlmoeAttention
----------------------------------------
  L 109: __init__(self, layer_id: int, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 181: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: OlmoeDecoderLayer
----------------------------------------
  L 198: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 235: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → torch.Tensor


CLASS: OlmoeForCausalLM
----------------------------------------
  L 315: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 335: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor

  L 347: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: OlmoeMoE
----------------------------------------
  L  57: __init__(self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, params_dtype: Optional[torch.dtype], quant_config: Optional[QuantizationConfig], tp_size: Optional[int], layer_id: int, prefix: str)

  L  96: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: OlmoeModel
----------------------------------------
  L 263: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 290: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/persimmon.py
Functions: 13
============================================================


CLASS: PersimmonAttention
----------------------------------------
  L  52: __init__(self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig], prefix: str, layer_id: int)

  L 120: forward(self, position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: PersimmonDecoderLayer
----------------------------------------
  L 147: __init__(self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig], prefix: str, idx: int)

  L 170: forward(self, position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: PersimmonForCausalLM
----------------------------------------
  L 262: __init__(self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 282: get_input_embeddings(self, input_ids: torch.Tensor)
         → torch.Tensor

  L 285: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor])
         → LogitsProcessorOutput

  L 303: load_weights(self, weights: Iterable[tuple[str, torch.Tensor]])


CLASS: PersimmonMLP
----------------------------------------
  L  31: __init__(self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig])

  L  43: forward(self, hidden_states)
         → torch.Tensor


CLASS: PersimmonModel
----------------------------------------
  L 199: __init__(self, config: PersimmonConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 233: get_input_embeddings(self, input_ids: torch.Tensor)
         → torch.Tensor

  L 236: forward(self, input_ids: torch.Tensor, forward_batch: ForwardBatch, positions: torch.Tensor, inputs_embeds: Optional[torch.Tensor])
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/phi.py
Functions: 13
============================================================


CLASS: PhiAttention
----------------------------------------
  L  30: __init__(self, config: PhiConfig, quant_config: Optional[QuantizationConfig], prefix: str, layer_id: int)

  L  84: forward(self, position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: PhiForCausalLM
----------------------------------------
  L 234: __init__(self, config: PhiConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 257: get_input_embeddings(self, input_ids: torch.Tensor)
         → torch.Tensor

  L 260: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor])
         → LogitsProcessorOutput

  L 279: load_weights(self, weights: Iterable[tuple[str, torch.Tensor]])


CLASS: PhiLayer
----------------------------------------
  L 129: __init__(self, config: PhiConfig, quant_config: Optional[QuantizationConfig], prefix: str, idx: int)

  L 148: forward(self, position_ids: torch.Tensor, forward_batch: ForwardBatch, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: PhiMLP
----------------------------------------
  L 100: __init__(self, config: PhiConfig, quant_config: Optional[QuantizationConfig])

  L 120: forward(self, hidden_states)


CLASS: PhiModel
----------------------------------------
  L 168: __init__(self, config: PhiConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 199: get_input_embeddings(self, input_ids: torch.Tensor)
         → torch.Tensor

  L 202: forward(self, input_ids: torch.Tensor, forward_batch: ForwardBatch, positions: torch.Tensor, inputs_embeds: Optional[torch.Tensor])
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/phi3_small.py
Functions: 21
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  32: def quick_gelu(x)
         @torch.jit.script

  L  37: def gegelu(input, limit: Optional[float])
         @torch.jit.script


CLASS: Phi3SmallDecoderLayer
----------------------------------------
  L 236: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 264: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: Phi3SmallForCausalLM
----------------------------------------
  L 364: __init__(self, config: Phi3Config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 406: get_input_embeddings(self, input_ids: torch.Tensor)
         → torch.Tensor

  L 409: set_input_embeddings(self, value)

  L 412: get_output_embeddings(self)

  L 415: set_output_embeddings(self, value)

  L 418: set_decoder(self, decoder)

  L 421: get_decoder(self)

  L 424: compute_logits(self, input_ids: torch.LongTensor, hidden_states: torch.Tensor, sampling_metadata)
         → Optional[torch.Tensor]

  L 437: forward(self, input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor], get_embedding: bool)
         → LogitsProcessorOutput

  L 460: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Phi3SmallMLP
----------------------------------------
  L  54: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  84: forward(self, x)


CLASS: Phi3SmallModel
----------------------------------------
  L 289: __init__(self, config: Phi3Config, quant_config: Optional[QuantizationConfig], prefix: str)

  L 332: get_input_embeddings(self, input_ids: torch.Tensor)
         → torch.Tensor

  L 335: forward(self, input_ids: torch.LongTensor, positions: Optional[torch.LongTensor], forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor])
         → Union[torch.Tensor]


CLASS: Phi3SmallSelfAttention
----------------------------------------
  L  93: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 210: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]


============================================================
FILE: python/sglang/srt/models/phi4mm.py
Functions: 10
============================================================


CLASS: Phi4MMForCausalLM
----------------------------------------
  L 387: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 416: get_image_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor

  L 433: get_audio_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor

  L 454: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 474: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 478: should_apply_lora(self, module_name: str)
         → bool

  L 481: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Phi4MMImageEncoder
----------------------------------------
  L  60: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str, model_dir: str)
         → None

  L 136: get_img_features(self, img_embeds: torch.FloatTensor, attention_mask)
         → torch.FloatTensor

  L 169: forward(self, pixel_values: torch.FloatTensor, image_sizes: torch.Tensor, image_attention_mask: torch.Tensor)
         → list[torch.FloatTensor]
         📝 process image and return vision embeddings.


============================================================
FILE: python/sglang/srt/models/phi4mm_audio.py
Functions: 18
============================================================


CLASS: AudioEmbedding
----------------------------------------
  L1078: __init__(self, config: PretrainedConfig)
         → None

  L1182: set_audio_embeds(self, input_embeds: torch.FloatTensor)
         → None

  L1185: set_audio_embed_sizes(self, audio_embed_sizes: torch.LongTensor)
         → None

  L1188: get_audio_features(self, input_embeds: torch.FloatTensor, audio_attention_mask: torch.Tensor, audio_projection_mode: str)
         → torch.FloatTensor
         📝 arguments:

  L1242: forward(self, audio_features: torch.FloatTensor, audio_attention_mask: torch.Tensor, audio_projection_mode: str)
         → torch.FloatTensor
         📝 arguments:


CLASS: ConformerEncoder
----------------------------------------
  L 778: __init__(self, input_size, chunk_size, left_chunk, num_lang, attention_dim, attention_heads, linear_units, num_blocks, dropout_rate, input_layer, causal, batch_norm, cnn_out, cnn_layer_norm, ext_pw_out_channel, ext_pw_kernel_size, depthwise_seperable_out_channel, depthwise_multiplier, chunk_se, kernel_size, activation, conv_activation, conv_glu_type, bias_in_glu, linear_glu_in_convm, attention_glu_type, export, extra_layer_output_idx, extra_multi_layer_output_idxs, activation_checkpointing, relative_attention_bias_args, time_reduction, use_pt_scaled_dot_product_attention, nemo_conv_settings, conv2d_extra_padding: Literal['feat', 'feat_time', 'none', True], replication_pad_for_subsample_embedding, attention_group_size, encoder_embedding_config)

  L 884: init_relative_attention_bias(self, input_tensor)

  L 888: calculate_hs_mask(self, xs_pad, device, mask)

  L 908: forward(self, xs_pad, masks)
         📝 Conformer Forward function


CLASS: ConformerEncoderLayer
----------------------------------------
  L 148: __init__(self, d_model, ext_pw_out_channel, depthwise_seperable_out_channel, depthwise_multiplier, n_head, d_ffn, ext_pw_kernel_size, kernel_size, dropout_rate, causal, batch_norm, activation, chunk_se, chunk_size, conv_activation, conv_glu_type, bias_in_glu, linear_glu_in_convm, attention_inner_dim, attention_glu_type, activation_checkpointing, export, use_pt_scaled_dot_product_attention, attn_group_sizes: int)

  L 225: forward(self, x, pos_k, pos_v, mask, relative_attention_bias: Optional[Tensor])
         📝 ConformerEncoder forward.


CLASS: TransformerEncoderBase
----------------------------------------
  L 339: __init__(self, input_size, chunk_size, left_chunk, attention_dim, attention_heads, input_layer, cnn_out, cnn_layer_norm, time_reduction, dropout_rate, padding_idx, relative_attention_bias_args, positional_dropout_rate, nemo_conv_settings, conv2d_extra_padding: Literal['feat', 'feat_time', 'none', True], attention_group_size, encoder_embedding_config)

  L 423: compute_lens_change(self, feature_lens)
         📝 feature_lens: int

  L 458: forward(self)
         📝 Abstract forward method implementation.

  L 534: forward_embeddings(self, xs_pad, masks, chunk_size_nc, left_chunk_nc)
         📝 Forwarding the inputs through the top embedding layers

  L 598: get_offset(self)
         📝 Returns offset used when retaining inputs for decoding.


CLASS: WindowQformer
----------------------------------------
  L1001: __init__(self, window_size: int, num_queries: int, num_blocks: int, attention_dim: int, attention_heads: int, linear_units: int, dropout_rate: float, normalize_before: bool)

  L1035: forward(self, audio_embed, mask, embed_len)
         📝 forward decoder


============================================================
FILE: python/sglang/srt/models/phi4mm_utils.py
Functions: 49
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  32: def get_activation(name)
         📝 Select an activation function by name

  L  53: def adaptive_enc_mask(x_len, chunk_start_idx, left_window, right_window)
         📝 The function is very important for Transformer Transducer Streaming mo

  L1586: def calc_length(lengths,
        all_paddings,
        kernel_size,
        stride,
        ceil_mode,
        repeat_num)
         📝 Calculates the output length of a Tensor passed through a convolution 

  L1639: def masked_softmax(scores, mask: Optional[Tensor])

  L1875: def get_offset(input_layer: str, time_reduction: int)
         📝 Get an offset. We will use the offset for determining #frames of a

  L1894: def unfold_tensor(xs_pad, max_seq_len)
         📝 For a given tensor with shape of (N, T, D), if sequence length T is


CLASS: AbsolutePositionalEncoding
----------------------------------------
  L 827: __init__(self, d_model, dropout_rate, max_len)
         📝 Construct an PositionalEncoding object.

  L 837: extend_pe(self, x)
         📝 Reset the positional encodings.

  L 858: forward(self, x: torch.Tensor)
         📝 Add positional encoding.


CLASS: AttBlock
----------------------------------------
  L1634: memory_dims(self, max_len)
         📝 memory dimensions


CLASS: AttModule
----------------------------------------
  L1601: __init__(self)

  L1605: set_export(self, mode)
         📝 set the export mode

  L1609: forward(self, x: Tensor, memory: Optional[Tensor], pos_emb: Optional[Tensor], att_mask: Optional[Tensor])
         → tuple[Tensor, Tensor, Optional[Tensor], Optional[Tensor]]
         📝 AttModule forward


CLASS: BlockBase
----------------------------------------
  L  26: __init__(self, input_size, output_size)


CLASS: CausalConv1D
----------------------------------------
  L 919: __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: Union[str, int], dilation: int, groups: int, bias: bool, padding_mode: str, device, dtype)
         → None

  L 969: update_cache(self, x, cache)

  L 983: forward(self, x, cache)


CLASS: CausalConv2D
----------------------------------------
  L1000: __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: Union[str, int], dilation: int, groups: int, bias: bool, padding_mode: str, device, dtype)
         → None

  L1034: forward(self, x)


CLASS: ConvModule
----------------------------------------
  L 389: __init__(self, input_dim, ext_pw_out_channel, depthwise_seperable_out_channel, ext_pw_kernel_size, kernel_size, depthwise_multiplier, dropout_rate, causal, batch_norm, chunk_se, chunk_size, activation, glu_type, bias_in_glu, linear_glu_in_convm, export)

  L 512: forward(self, x)
         📝 ConvModule Forward.


CLASS: DepthWiseSeperableConv1d
----------------------------------------
  L 286: __init__(self, input_dim, depthwise_seperable_out_channel, kernel_size, depthwise_multiplier, padding)

  L 317: forward(self, x)
         📝 Args:


CLASS: FeedForward
----------------------------------------
  L 622: __init__(self, d_model, d_inner, dropout_rate, activation, bias_in_glu)

  L 643: forward(self, x)
         📝 FeedForward forward function.


CLASS: GLU
----------------------------------------
  L 125: __init__(self, dim: int, act_name: str)
         → None

  L 141: forward(self, x: Tensor)
         → Tensor
         📝 GLU forward


CLASS: GLULinear
----------------------------------------
  L 580: __init__(self, input_dim, output_dim, glu_type, bias_in_glu)

  L 591: forward(self, x)
         📝 GLULinear forward


CLASS: GLUPointWiseConv
----------------------------------------
  L 182: __init__(self, input_dim, output_dim, kernel_size, glu_type, bias_in_glu, causal)

  L 228: forward(self, x)
         📝 Args:


CLASS: MeanVarianceNormLayer
----------------------------------------
  L 886: __init__(self, input_size)

  L 892: forward(self, input_: Tensor)
         → Tensor
         📝 MeanVarianceNormLayer Forward


CLASS: MultiHeadedAttention
----------------------------------------
  L1693: __init__(self, n_head, n_feat, dropout_rate, attention_inner_dim, glu_type, bias_in_glu, use_pt_scaled_dot_product_attention, n_value, group_size: int)

  L1741: forward(self, query: Tensor, key: Tensor, value: Tensor, pos_k: Tensor, pos_v: Tensor, mask: Optional[Tensor], relative_attention_bias: Optional[Tensor])
         📝 Compute 'Scaled Dot Product Attention'.


CLASS: MultiSequential
----------------------------------------
  L1868: forward(self)
         📝 Forward method implementation.


CLASS: NemoConvSubsampling
----------------------------------------
  L1080: __init__(self, feat_in, feat_out, subsampling_factor, subsampling, conv_channels, subsampling_conv_chunking_factor, activation, is_causal)

  L1369: get_sampling_frames(self)

  L1372: get_streaming_cache_size(self)

  L1375: forward(self, x, mask)
         📝 Forward method for NeMo subsampling.

  L1443: reset_parameters(self)

  L1468: conv_split_by_batch(self, x)
         📝 Tries to split input by batch, run conv and concat results

  L1494: conv_split_by_channel(self, x)
         📝 For dw convs, tries to split input by time, run conv and concat

  L1532: channel_chunked_conv(self, conv, chunk_size, x)
         📝 Performs channel chunked convolution

  L1572: change_subsampling_conv_chunking_factor(self, subsampling_conv_chunking_factor: int)


CLASS: Swish
----------------------------------------
  L 108: __init__(self)
         → None

  L 112: forward(self, x: Tensor)
         → Tensor
         📝 Apply Swish function


CLASS: T5RelativeAttentionLogitBias
----------------------------------------
  L 722: __init__(self, num_heads, num_buckets, max_distance, symmetric)

  L 739: forward(self, x)


============================================================
FILE: python/sglang/srt/models/phimoe.py
Functions: 14
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 107: def sparsemixer(scores, jitter_eps)

  L 159: def phimoe_routing_function(hidden_states: torch.Tensor,
        gating_output: torch.Tensor,
        topk: int,
        renormalize: bool)


CLASS: PhiMoE
----------------------------------------
  L 182: __init__(self, num_experts: int, top_k: int, hidden_size: int, intermediate_size: int, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)

  L 221: forward(self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch])
         → torch.Tensor


CLASS: PhiMoEAttention
----------------------------------------
  L 235: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: Optional[int], max_position: int, rope_theta: float, layer_id: int, attention_bias: bool, quant_config: Optional[QuantizationConfig], rope_scaling: Optional[dict], prefix: str)
         → None

  L 315: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: PhiMoEConfig
----------------------------------------
  L  39: __init__(self, vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, head_dim, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, tie_word_embeddings, rope_theta, sliding_window, attention_dropout, num_experts_per_tok, num_local_experts, output_router_logits, router_aux_loss_coef, router_jitter_noise, attention_bias, lm_head_bias)


CLASS: PhiMoEDecoderLayer
----------------------------------------
  L 331: __init__(self, config: PhiMoEConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 372: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, residual: Optional[torch.Tensor], forward_batch: ForwardBatch)
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: PhiMoEForCausalLM
----------------------------------------
  L 455: __init__(self, config: PhiMoEConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 484: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, inputs_embeds: Optional[torch.Tensor], get_embedding: bool)
         → LogitsProcessorOutput

  L 502: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: PhiMoEModel
----------------------------------------
  L 402: __init__(self, config: PhiMoEConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 431: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor])
         → Union[torch.Tensor]


============================================================
FILE: python/sglang/srt/models/pixtral.py
Functions: 13
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 225: def resolve_visual_encoder_outputs(outputs: Union[torch.Tensor,
        List[torch.Tensor]],
        feature_sample_layers: Optional[List[int]],
        post_norm: Optional[nn.Module],
        num_hidden_layers: int)
         → torch.Tensor
         📝 Resolve outputs from visual encoder based on feature_sample_layers.


CLASS: PixtralHFMLP
----------------------------------------
  L  46: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig])
         → None

  L  76: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: PixtralHFTransformer
----------------------------------------
  L 166: __init__(self, config: PixtralVisionConfig, quant_config: Optional[QuantizationConfig])
         → None

  L 192: forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor], position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]], return_all_hidden_states: bool)
         → Union[torch.Tensor, List[torch.Tensor]]
         📝 Forward pass through transformer layers.


CLASS: PixtralHFTransformerBlock
----------------------------------------
  L  90: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig])
         → None

  L 123: forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor], position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]])
         → torch.Tensor


CLASS: PixtralHFVisionModel
----------------------------------------
  L 271: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 274: __init__(self, config: PixtralVisionConfig, quant_config: Optional[QuantizationConfig])
         → None

  L 320: dtype(self)

  L 324: device(self)

  L 327: forward(self, pixel_values: torch.Tensor, image_sizes: list[tuple[int, int]], output_hidden_states: bool, feature_sample_layers: Optional[list[int]])
         → Union[torch.Tensor, tuple]
         📝 Args:

  L 419: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])
         → Set[str]
         📝 Load weights from a HuggingFace checkpoint with proper parameter mappi


============================================================
FILE: python/sglang/srt/models/qwen.py
Functions: 12
============================================================


CLASS: QWenAttention
----------------------------------------
  L  87: __init__(self, hidden_size: int, num_heads: int, max_position_embeddings: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], quant_config: Optional[QuantizationConfig], prefix: str)

  L 141: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: QWenBlock
----------------------------------------
  L 156: __init__(self, config: PretrainedConfig, layer_id, quant_config: Optional[QuantizationConfig], prefix: str)

  L 188: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: QWenLMHeadModel
----------------------------------------
  L 261: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 279: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)

  L 291: forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int])

  L 326: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: QWenMLP
----------------------------------------
  L  47: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)

  L  79: forward(self, x)


CLASS: QWenModel
----------------------------------------
  L 213: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 242: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/qwen2.py
Functions: 22
============================================================


CLASS: Qwen2Attention
----------------------------------------
  L  99: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: Optional[int], layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], dual_chunk_attention_config: Optional[dict[str, Any]], prefix: str)
         → None

  L 174: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: Qwen2DecoderLayer
----------------------------------------
  L 189: __init__(self, config: Qwen2Config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream])
         → None

  L 231: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: Qwen2ForCausalLM
----------------------------------------
  L 409: __init__(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 456: get_input_embedding(self, input_ids: torch.Tensor)
         → torch.Tensor

  L 459: get_input_embeddings(self)
         → nn.Embedding

  L 463: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors])
         → torch.Tensor

  L 491: forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)

  L 532: start_layer(self)

  L 536: end_layer(self)

  L 539: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 608: get_embed_and_head(self)

  L 611: set_embed_and_head(self, embed, head)

  L 619: load_kv_cache_scales(self, quantization_param_path: str)
         → None


CLASS: Qwen2MLP
----------------------------------------
  L  61: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  91: forward(self, x)


CLASS: Qwen2Model
----------------------------------------
  L 257: __init__(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str, decoder_layer_type: type[nn.Module], alt_stream: Optional[torch.cuda.Stream])
         → None

  L 305: get_input_embedding(self, input_ids: torch.Tensor)
         → torch.Tensor

  L 311: get_input_embeddings(self)
         → nn.Embedding

  L 314: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         → Union[torch.Tensor, PPProxyTensors]

  L 368: load_kv_cache_scales(self, quantization_param_path: str)
         → None


============================================================
FILE: python/sglang/srt/models/qwen2_5_vl.py
Functions: 19
============================================================


CLASS: Qwen2_5_VLForConditionalGeneration
----------------------------------------
  L 462: __init__(self, config: Qwen2_5_VLConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 500: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 504: get_image_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor

  L 515: get_video_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor

  L 526: get_input_embeddings(self)

  L 530: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)
         📝 Run forward pass for Qwen2_5-VL.

  L 577: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Qwen2_5_VLMLP
----------------------------------------
  L  66: __init__(self, in_features: int, hidden_features: int, bias: bool, hidden_act, quant_config: Optional[QuantizationConfig], prefix: str)

  L  99: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: Qwen2_5_VisionBlock
----------------------------------------
  L 110: __init__(self, dim: int, intermediate_dim: int, num_heads: int, hidden_act, norm_layer: Type[nn.Module], attn_implementation: Optional[str], quant_config: Optional[QuantizationConfig], prefix: str, num_dummy_heads: int)
         → None

  L 171: forward(self, x: torch.Tensor, cu_seqlens: torch.Tensor, position_embeddings: torch.Tensor)
         → torch.Tensor


CLASS: Qwen2_5_VisionPatchMerger
----------------------------------------
  L 194: __init__(self, dim: int, context_dim: int, spatial_merge_size: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 225: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: Qwen2_5_VisionTransformer
----------------------------------------
  L 238: __init__(self, vision_config: Qwen2_5_VLVisionConfig, norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 292: get_window_index(self, grid_thw)

  L 338: dtype(self)
         → torch.dtype

  L 342: device(self)
         → torch.device

  L 345: rot_pos_emb(self, grid_thw: torch.Tensor)
         → torch.Tensor

  L 377: forward(self, x: torch.Tensor, grid_thw: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/qwen2_audio.py
Functions: 5
============================================================


CLASS: Qwen2AudioForConditionalGeneration
----------------------------------------
  L  88: __init__(self, config: Qwen2AudioConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 115: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 118: get_audio_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor

  L 134: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 153: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/qwen2_eagle.py
Functions: 5
============================================================


CLASS: Qwen2DecoderLayer
----------------------------------------
  L  41: __init__(self, config: Qwen2Config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None


CLASS: Qwen2ForCausalLMEagle
----------------------------------------
  L 115: __init__(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 139: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Qwen2Model
----------------------------------------
  L  58: __init__(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  85: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/qwen2_moe.py
Functions: 17
============================================================


CLASS: Qwen2MoeAttention
----------------------------------------
  L 197: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, qkv_bias: int, quant_config: Optional[QuantizationConfig], dual_chunk_attention_config: Optional[dict[str, Any]], prefix: str)
         → None

  L 278: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: Qwen2MoeDecoderLayer
----------------------------------------
  L 293: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream])
         → None

  L 367: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: Qwen2MoeForCausalLM
----------------------------------------
  L 518: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 541: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         → torch.Tensor

  L 564: forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)

  L 607: start_layer(self)

  L 611: end_layer(self)

  L 614: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 701: get_model_config_for_expert_location(cls, config)


CLASS: Qwen2MoeMLP
----------------------------------------
  L  74: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str)
         → None

  L 105: forward(self, x, use_reduce_scatter: bool)


CLASS: Qwen2MoeModel
----------------------------------------
  L 405: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str, decoder_layer_type: type[nn.Module], alt_stream: Optional[torch.cuda.Stream])
         → None

  L 452: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         → Union[torch.Tensor, PPProxyTensors]


CLASS: Qwen2MoeSparseMoeBlock
----------------------------------------
  L 117: __init__(self, layer_id: int, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 168: forward(self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch], use_reduce_scatter: bool)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/qwen2_rm.py
Functions: 3
============================================================


CLASS: Qwen2ForRewardModel
----------------------------------------
  L  29: __init__(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  52: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         → EmbeddingPoolerOutput

  L  68: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/qwen2_vl.py
Functions: 23
============================================================


CLASS: Qwen2VLForConditionalGeneration
----------------------------------------
  L 445: __init__(self, config: Qwen2VLConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 481: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 485: get_image_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor

  L 496: get_video_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor

  L 514: get_input_embeddings(self)

  L 517: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, get_embedding: bool)
         📝 Run forward pass for Qwen2-VL.

  L 563: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: Qwen2VisionBlock
----------------------------------------
  L 124: __init__(self, dim: int, num_heads: int, mlp_ratio: float, act_layer: Type[nn.Module], norm_layer: Type[nn.Module], attn_implementation: Optional[str], quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 170: forward(self, x: torch.Tensor, cu_seqlens: torch.Tensor, position_embeddings: torch.Tensor)
         → torch.Tensor


CLASS: Qwen2VisionMLP
----------------------------------------
  L  92: __init__(self, in_features: int, hidden_features: int, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)

  L 115: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: Qwen2VisionPatchEmbed
----------------------------------------
  L 191: __init__(self, patch_size: int, temporal_patch_size: int, in_chans: int, embed_dim: int)
         → None

  L 208: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: Qwen2VisionPatchMerger
----------------------------------------
  L 217: __init__(self, d_model: int, context_dim: int, norm_layer: Type[nn.Module], spatial_merge_size: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 251: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: Qwen2VisionRotaryEmbedding
----------------------------------------
  L 264: __init__(self, dim: int, theta: float)
         → None

  L 273: update_freqs_cache(self, seqlen: int)
         → None

  L 292: forward(self, seqlen: int)
         → torch.Tensor


CLASS: Qwen2VisionTransformer
----------------------------------------
  L 299: __init__(self, vision_config: Qwen2VLVisionConfig, norm_eps: float, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 353: dtype(self)
         → torch.dtype

  L 357: device(self)
         → torch.device

  L 360: rot_pos_emb(self, grid_thw: torch.Tensor)
         → torch.Tensor

  L 393: forward(self, x: torch.Tensor, grid_thw: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/qwen3.py
Functions: 16
============================================================


CLASS: Qwen3Attention
----------------------------------------
  L  39: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], head_dim: Optional[int], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], rms_norm_eps: float, attention_bias: bool, prefix: str, alt_stream: Optional[torch.cuda.Stream])
         → None

  L 146: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: Qwen3DecoderLayer
----------------------------------------
  L 162: __init__(self, config: Qwen3Config, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream])
         → None

  L 215: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: Qwen3ForCausalLM
----------------------------------------
  L 281: __init__(self, config: Qwen3Config, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 330: get_input_embeddings(self)
         → nn.Embedding

  L 334: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool, pp_proxy_tensors: Optional[PPProxyTensors])
         → torch.Tensor

  L 370: forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)

  L 411: start_layer(self)

  L 415: end_layer(self)

  L 418: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 487: get_embed_and_head(self)

  L 490: set_embed_and_head(self, embed, head)

  L 498: load_kv_cache_scales(self, quantization_param_path: str)
         → None

  L 501: set_eagle3_layers_to_capture(self, layer_ids: Optional[List[int]])


CLASS: Qwen3Model
----------------------------------------
  L 245: __init__(self, config: Qwen3Config, quant_config: Optional[QuantizationConfig], prefix: str)
         → None


============================================================
FILE: python/sglang/srt/models/qwen3_classification.py
Functions: 3
============================================================


CLASS: Qwen3ForSequenceClassification
----------------------------------------
  L  29: __init__(self, config: Qwen2Config, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  56: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: Optional[torch.Tensor], get_embedding: bool)
         → EmbeddingPoolerOutput

  L  74: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/qwen3_moe.py
Functions: 36
============================================================


CLASS: Qwen3MoeAttention
----------------------------------------
  L 263: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, head_dim: Optional[int], rms_norm_eps: float, attention_bias: bool, quant_config: Optional[QuantizationConfig], prefix: str, dual_chunk_attention_config: Optional[dict[str, Any]], alt_stream: Optional[torch.cuda.Stream])
         → None

  L 373: op_prepare(self, state)

  L 380: op_core(self, state)

  L 385: forward_prepare(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)

  L 400: forward_core(self, intermediate_state)

  L 408: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: Qwen3MoeDecoderLayer
----------------------------------------
  L 423: __init__(self, config: Qwen3MoeConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str, alt_stream: Optional[torch.cuda.Stream])
         → None

  L 505: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]

  L 541: op_comm_prepare_attn(self, state, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor], tbo_subbatch_index: Optional[int])

  L 561: op_comm_prepare_mlp(self, state)

  L 570: op_mlp(self, state)

  L 574: op_comm_postprocess_layer(self, state)


CLASS: Qwen3MoeForCausalLM
----------------------------------------
  L 619: __init__(self, config: Qwen3MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 642: get_input_embeddings(self)
         → nn.Embedding

  L 646: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, pp_proxy_tensors: Optional[PPProxyTensors])
         → torch.Tensor

  L 674: forward_split_prefill(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, split_interval: Tuple[int, int], input_embeds: torch.Tensor)

  L 717: start_layer(self)

  L 721: end_layer(self)

  L 724: get_embed_and_head(self)

  L 727: set_eagle3_layers_to_capture(self, layer_ids: Optional[List[int]])

  L 742: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 857: get_model_config_for_expert_location(cls, config)


CLASS: Qwen3MoeModel
----------------------------------------
  L 600: __init__(self, config: Qwen3MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None


CLASS: Qwen3MoeSparseMoeBlock
----------------------------------------
  L  69: __init__(self, layer_id: int, config: Qwen3MoeConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 118: forward(self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch], use_reduce_scatter: bool)
         → torch.Tensor

  L 130: get_moe_weights(self)

  L 137: forward_normal(self, hidden_states: torch.Tensor, use_reduce_scatter: bool)
         → torch.Tensor

  L 154: forward_deepep(self, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 183: op_gate(self, state)

  L 192: op_select_experts(self, state)

  L 215: op_dispatch_a(self, state)

  L 225: op_dispatch_b(self, state)

  L 234: op_experts(self, state)

  L 239: op_combine_a(self, state)

  L 250: op_combine_b(self, state)

  L 258: op_output(self, state)


============================================================
FILE: python/sglang/srt/models/registry.py
Functions: 3
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  77: def import_model_classes()
         @lru_cache()


CLASS: _ModelRegistry
----------------------------------------
  L  20: get_supported_archs(self)
         → AbstractSet[str]

  L  62: resolve_model_cls(self, architectures: Union[str, List[str]])
         → Tuple[Type[nn.Module], str]


============================================================
FILE: python/sglang/srt/models/roberta.py
Functions: 14
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 192: def create_position_ids_from_input_ids(input_ids,
        padding_idx,
        past_key_values_length)


CLASS: RobertaClassificationHead
----------------------------------------
  L  23: __init__(self, config: RobertaConfig)

  L  28: forward(self, features)


CLASS: RobertaEmbedding
----------------------------------------
  L  38: __init__(self, config: RobertaConfig)

  L  66: forward(self, input_ids: torch.Tensor, seq_lens: torch.Tensor, position_ids: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: XLMRobertaBaseModel
----------------------------------------
  L 115: __init__(self)

  L 135: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         → torch.Tensor

  L 157: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: XLMRobertaForSequenceClassification
----------------------------------------
  L 234: __init__(self)

  L 248: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         → torch.Tensor

  L 265: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: XLMRobertaModel
----------------------------------------
  L 203: __init__(self)

  L 216: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         → torch.Tensor

  L 229: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/siglip.py
Functions: 14
============================================================


CLASS: SiglipEncoder
----------------------------------------
  L 176: __init__(self, config: SiglipVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 201: forward(self, inputs_embeds: torch.Tensor, attention_mask: torch.Tensor, causal_attention_mask: torch.Tensor, return_all_hidden_states: bool)
         → Union[torch.Tensor, list[torch.Tensor]]


CLASS: SiglipEncoderLayer
----------------------------------------
  L  95: __init__(self, config: SiglipVisionConfig, act_layer: Type[nn.Module], norm_layer: Type[nn.Module], attn_implementation: Optional[str], quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 136: forward(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, causal_attention_mask: torch.Tensor)
         → torch.Tensor


CLASS: SiglipMLP
----------------------------------------
  L  63: __init__(self, config, act_layer: Type[nn.Module], quant_config: Optional[QuantizationConfig], prefix: str)

  L  85: forward(self, x: torch.Tensor)
         → torch.Tensor


CLASS: SiglipVisionEmbeddings
----------------------------------------
  L  22: __init__(self, config: SiglipVisionConfig)

  L  48: forward(self, pixel_values: torch.Tensor)
         → torch.Tensor


CLASS: SiglipVisionModel
----------------------------------------
  L 278: __init__(self, config: SiglipVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 290: device(self)
         → torch.device

  L 293: forward(self, pixel_values: torch.Tensor)


CLASS: SiglipVisionTransformer
----------------------------------------
  L 225: __init__(self, config: SiglipVisionConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 255: device(self)
         → torch.device

  L 258: forward(self, pixel_values: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/stablelm.py
Functions: 11
============================================================


CLASS: StableLMEpochModel
----------------------------------------
  L 215: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 241: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


CLASS: StableLmForCausalLM
----------------------------------------
  L 264: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 282: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor

  L 294: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: StablelmAttention
----------------------------------------
  L  83: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 156: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: StablelmDecoderLayer
----------------------------------------
  L 171: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 189: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: StablelmMLP
----------------------------------------
  L  49: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  75: forward(self, x: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/step3_vl.py
Functions: 32
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 496: def get_abs_pos(abs_pos, tgt_size)


CLASS: Step3TextAttention
----------------------------------------
  L 173: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, head_dim: int, share_q_dim: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], rms_norm_eps, prefix: str)
         → None

  L 267: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: Step3TextDecoderLayer
----------------------------------------
  L 284: __init__(self, config: Step3TextConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 388: moe_mlp_forward(self, hidden_states)

  L 397: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: Step3TextMLP
----------------------------------------
  L  75: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 105: forward(self, x)


CLASS: Step3TextMoEMLP
----------------------------------------
  L 114: __init__(self, layer_id: int, config: Step3TextConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 157: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: Step3TextModel
----------------------------------------
  L 432: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 461: get_input_embeddings(self)

  L 464: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


CLASS: Step3VLForConditionalGeneration
----------------------------------------
  L 739: __init__(self, config: Step3VLConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 815: get_image_feature(self, items: List[MultimodalDataItem])
         → torch.Tensor

  L 858: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)

  L 863: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor

  L 884: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])

  L 997: get_model_config_for_expert_location(cls, config: Step3VLConfig)


CLASS: Step3VisionAttention
----------------------------------------
  L 572: __init__(self, dim: int, num_heads: int, qkv_backend, quant_config, prefix: str)
         → None

  L 605: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: Step3VisionEmbeddings
----------------------------------------
  L 612: __init__(self, config: Step3VisionEncoderConfig)

  L 641: forward(self, pixel_values: torch.Tensor)
         → torch.Tensor


CLASS: Step3VisionEncoder
----------------------------------------
  L 716: __init__(self, config: Step3VisionEncoderConfig)

  L 723: forward(self, inputs_embeds)
         → torch.Tensor


CLASS: Step3VisionEncoderLayer
----------------------------------------
  L 665: __init__(self, config, attn_implementation: str)
         → None

  L 680: forward(self, hidden_states)
         → torch.Tensor


CLASS: Step3VisionMLP
----------------------------------------
  L 529: __init__(self, dim: int, intermediate_size: int, bias: bool, hidden_act, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 564: forward(self, hidden_states)
         → torch.Tensor


CLASS: Step3VisionTransformer
----------------------------------------
  L 687: __init__(self, config: Step3VisionEncoderConfig)

  L 695: dtype(self)
         → torch.dtype

  L 698: forward(self, pixel_values: torch.Tensor)


============================================================
FILE: python/sglang/srt/models/torch_native_llama.py
Functions: 16
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  73: def gate_up_proj_weight_loader(self,
        param: Parameter,
        loaded_weight: torch.Tensor,
        loaded_shard_id: int)

  L 142: def qkv_proj_weight_loader(self,
        param: Parameter,
        loaded_weight: torch.Tensor,
        loaded_shard_id: str)


CLASS: LlamaAttention
----------------------------------------
  L 180: __init__(self, config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 253: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: LlamaDecoderLayer
----------------------------------------
  L 268: __init__(self, config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 312: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: LlamaMLP
----------------------------------------
  L 108: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 135: forward(self, x)


CLASS: LlamaModel
----------------------------------------
  L 338: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig])
         → None

  L 361: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


CLASS: TorchNativeLlamaForCausalLM
----------------------------------------
  L 386: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig])
         → None

  L 407: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → LogitsProcessorOutput

  L 419: get_module_name_from_weight_name(self, name)

  L 436: get_num_params(self)

  L 440: load_weights_to_module(self, fqn: str, weights: Iterable[Tuple[str, torch.Tensor]])
         📝 Load weights onto submodule pointed by path `fqn`.

  L 488: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])
         📝 Load weights onto the full model.


============================================================
FILE: python/sglang/srt/models/transformers.py
Functions: 11
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  46: def maybe_prefix(prefix: str, name: str)
         → str
         📝 Add a prefix to a name if the prefix is non-empty.

  L  59: def sglang_flash_attention_forward(module: torch.nn.Module,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        attention_mask: torch.Tensor,
        forward_batch: ForwardBatch,
        scaling: float,
        attention_instances: list[RadixAttention])

  L  97: def replace_linear_class(linear: nn.Linear,
        style: Literal['colwise',
        'rowwise'],
        quant_config: QuantizationConfig)
         → Union[ColumnParallelLinear, RowParallelLinear]
         📝 Replace nn.Linear with one of vLLM's tensor parallel linear classes.


CLASS: HFColumnParallelLinear
----------------------------------------
  L  87: forward(self, input: torch.Tensor)
         → torch.Tensor


CLASS: HFRowParallelLinear
----------------------------------------
  L  93: forward(self, input: torch.Tensor)
         → torch.Tensor


CLASS: TransformersForCausalLM
----------------------------------------
  L 143: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 206: log_replacement(self, name: str, old_module: nn.Module, new_module: nn.Module)

  L 209: tensor_parallel(self, tp_size: int)
         📝 Apply the model's tensor parallelization plan.

  L 238: replace_vocab_embed_class(self, module: nn.Module)

  L 252: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor, get_embedding: bool)
         → LogitsProcessorOutput

  L 277: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/vila.py
Functions: 12
============================================================


CLASS: DownSample3x3BlockFix
----------------------------------------
  L  94: forward(self, x: Tensor)
         → Tensor
         📝 Args:


CLASS: MultimodalProjector
----------------------------------------
  L 130: __init__(self, config: VILAConfig)

  L 160: device(self)
         → torch.device

  L 164: dtype(self)
         → torch.dtype

  L 167: forward(self, x: Tensor)
         → Tensor
         📝 Args:


CLASS: VILAConfig
----------------------------------------
  L  56: __init__(self, text_config: Optional[Dict[str, Any]], vision_config: Optional[Dict[str, Any]])


CLASS: VILAForConditionalGeneration
----------------------------------------
  L 193: __init__(self, config: VILAConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 216: dtype(self)
         → torch.dtype

  L 219: forward(self, input_ids: Tensor, positions: Tensor, forward_batch: ForwardBatch, get_embedding: bool)
         → LogitsProcessorOutput

  L 239: get_image_feature(self, mm_input: List[MultimodalDataItem])
         → Tensor

  L 265: load_weights(self, weights: Iterable[Tuple[str, Tensor]])
         → None

  L 278: pad_input_ids(self, input_ids: List[int], mm_inputs: MultimodalInputs)
         → List[int]


============================================================
FILE: python/sglang/srt/models/xverse.py
Functions: 11
============================================================


CLASS: XverseAttention
----------------------------------------
  L  85: __init__(self, config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], rope_is_neox_style: bool, max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 160: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: XverseDecoderLayer
----------------------------------------
  L 175: __init__(self, config: LlamaConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 222: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → Tuple[torch.Tensor, torch.Tensor]


CLASS: XverseForCausalLM
----------------------------------------
  L 302: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 320: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor

  L 332: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]], name, loaded_weight)


CLASS: XverseMLP
----------------------------------------
  L  47: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  77: forward(self, x)


CLASS: XverseModel
----------------------------------------
  L 248: __init__(self, config: LlamaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 276: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch, input_embeds: torch.Tensor)
         → torch.Tensor


============================================================
FILE: python/sglang/srt/models/xverse_moe.py
Functions: 14
============================================================


CLASS: XverseAttention
----------------------------------------
  L 195: __init__(self, hidden_size: int, num_heads: int, num_kv_heads: int, layer_id: int, rope_theta: float, rope_scaling: Optional[Dict[str, Any]], max_position_embeddings: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 265: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: XverseDecoderLayer
----------------------------------------
  L 281: __init__(self, config: PretrainedConfig, layer_id: int, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 326: forward(self, positions: torch.Tensor, hidden_states: torch.Tensor, forward_batch: ForwardBatch, residual: Optional[torch.Tensor])
         → torch.Tensor


CLASS: XverseMLP
----------------------------------------
  L  53: __init__(self, hidden_size: int, intermediate_size: int, hidden_act: str, quant_config: Optional[QuantizationConfig], reduce_results: bool, prefix: str)
         → None

  L  85: forward(self, x)


CLASS: XverseMoE
----------------------------------------
  L  94: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)

  L 151: pack_params(self)

  L 170: forward(self, hidden_states: torch.Tensor)
         → torch.Tensor


CLASS: XverseModel
----------------------------------------
  L 355: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 383: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor


CLASS: XverseMoeForCausalLM
----------------------------------------
  L 402: __init__(self, config: PretrainedConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L 423: forward(self, input_ids: torch.Tensor, positions: torch.Tensor, forward_batch: ForwardBatch)
         → torch.Tensor

  L 434: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


============================================================
FILE: python/sglang/srt/models/yivl.py
Functions: 4
============================================================


CLASS: YiVLForCausalLM
----------------------------------------
  L  28: __init__(self, config: LlavaConfig, quant_config: Optional[QuantizationConfig], prefix: str)
         → None

  L  41: load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]])


CLASS: YiVLMultiModalProjector
----------------------------------------
  L  93: __init__(self, config: LlavaConfig)

  L 106: forward(self, image_features)
