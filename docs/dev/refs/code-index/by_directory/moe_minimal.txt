
# python/sglang/srt/layers/moe/cutlass_moe.py
cutlass_fused_experts_fp8(a, w1_q, w2_q, w1_scale, w2_scale, topk_weights, topk_ids, a1_strides, c1_strides, a2_strides, c2_strides, workspace, a_ptrs, b_ptrs, out_ptrs, a_scales_ptrs, b_scales_ptrs, expert_offsets, problem_sizes1, problem_sizes2, use_fp8_blockscale)
cutlass_moe_fp4(a, a1_gscale, w1_fp4, w1_blockscale, w1_alphas, a2_gscale, w2_fp4, w2_blockscale, w2_alphas, topk_weights, topk_ids, params, apply_router_weight_on_input)

# python/sglang/srt/layers/moe/cutlass_moe_params.py
  CutlassMoEParams.__init__(cutlass_moe_type, device, num_experts, intermediate_size_per_partition, hidden_size)
  CutlassMoEParams.to_gemm1_args()
  CutlassMoEParams.to_gemm2_args()

# python/sglang/srt/layers/moe/cutlass_w4a8_moe.py
cutlass_w4a8_moe(start_expert_id, end_expert_id, total_num_experts, a, w1_q, w2_q, w1_scale, w2_scale, topk_weights, topk_ids_, local_topk_ids, a_strides1, b_strides1, c_strides1, a_strides2, b_strides2, c_strides2, s_strides13, s_strides2, expert_offsets, problem_sizes1, problem_sizes2, a1_scale, a2_scale, apply_router_weight_on_input)

# python/sglang/srt/layers/moe/fused_moe_native.py
fused_moe_forward_native(layer, x, topk_output, moe_runner_config)
moe_forward_native(layer, x, topk_output, moe_runner_config)

# python/sglang/srt/layers/moe/rocm_moe_utils.py
rocm_aiter_asm_moe_tkw1_impl(hidden_states, w1, w2, topk_weights, topk_ids, fc1_scale, fc2_scale, fc1_smooth_scale, fc2_smooth_scale, a16, per_tensor_quant_scale, expert_mask, activation_method)
rocm_aiter_asm_moe_tkw1_fake(hidden_states, w1, w2, topk_weights, topk_ids, fc1_scale, fc2_scale, fc1_smooth_scale, fc2_smooth_scale, a16, per_tensor_quant_scale, expert_mask, activation_method)
rocm_fused_experts_tkw1(hidden_states, w1, w2, topk_weights, topk_ids, activation, apply_router_weight_on_input, use_fp8_w8a8, per_channel_quant, w1_scale, w2_scale, a1_scale, a2_scale, block_shape)

# python/sglang/srt/layers/moe/router.py
fused_moe_router_kernel(input_ptr, moe_router_weight_ptr, topk_weights_ptr, topk_ids_ptr, correction_bias_ptr, is_correction_bias, num_experts, topk, moe_softcapping, moe_renormalize, hidden_dim, BLOCK_SIZE)
fused_moe_router_impl(x, router_weight, topk, moe_softcapping, correction_bias)
fused_moe_router_large_bs_kernel(a_ptr, b_ptr, topk_weights_ptr, topk_ids_ptr, bs, num_experts, topk, moe_softcapping, moe_renormalize, K, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, stride_am, stride_bn)
fused_moe_router_large_bs_impl(x, router_weight, topk, moe_softcapping, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K)
fused_moe_router_shim(moe_softcapping, hidden_states, gating_output, topk, renormalize, correction_bias)
  FusedMoeRouter.__init__(router_linear, topk, moe_softcapping)
  FusedMoeRouter.__call__()
  FusedMoeRouter.forward(x, residual)
  FusedMoeRouter.forward_cuda(x, autotune)
  FusedMoeRouter.forward_vllm(x)

# python/sglang/srt/layers/moe/topk.py
  TopKOutputChecker.format_is_standard(topk_output)
  TopKOutputChecker.format_is_triton_kernel(topk_output)
  TopKOutputChecker.format_is_bypassed(topk_output)
  TopKOutputFormat.is_standard()
  TopKOutputFormat.is_triton_kernel()
  TopKOutputFormat.is_bypassed()
  TopKOutput.format()
  StandardTopKOutput.format()
  TritonKernelTopKOutput.format()
  BypassedTopKOutput.format()
  TopK.__init__(top_k)
  TopK.forward_native(hidden_states, router_logits)
  TopK.forward_cuda(hidden_states, router_logits)
  TopK.forward_cpu(hidden_states, router_logits)
  TopK.forward_npu(hidden_states, router_logits)
  TopK.empty_topk_output(device)
fused_topk_torch_native(hidden_states, gating_output, topk, renormalize)
fused_topk_cpu(hidden_states, gating_output, topk, renormalize, num_token_non_padded, expert_location_dispatch_info)
apply_topk_weights_cpu(need_apply, topk_weights, inputs)
fused_topk(hidden_states, gating_output, topk, renormalize, num_token_non_padded, expert_location_dispatch_info)
grouped_topk_gpu(hidden_states, gating_output, topk, renormalize, num_expert_group, topk_group, num_fused_shared_experts, routed_scaling_factor, num_token_non_padded, expert_location_dispatch_info, apply_routed_scaling_factor_on_output)
grouped_topk_cpu(hidden_states, gating_output, topk, renormalize, num_expert_group, topk_group, num_fused_shared_experts, routed_scaling_factor, num_token_non_padded, expert_location_dispatch_info, apply_routed_scaling_factor_on_output)
biased_grouped_topk_impl(hidden_states, gating_output, correction_bias, topk, renormalize, num_expert_group, topk_group, num_fused_shared_experts, routed_scaling_factor, num_token_non_padded, expert_location_dispatch_info, apply_routed_scaling_factor_on_output)
is_power_of_two(n)
biased_grouped_topk_gpu(hidden_states, gating_output, correction_bias, topk, renormalize, num_expert_group, topk_group, num_fused_shared_experts, routed_scaling_factor, num_token_non_padded, expert_location_dispatch_info, apply_routed_scaling_factor_on_output)
biased_grouped_topk_cpu(hidden_states, gating_output, correction_bias, topk, renormalize, num_expert_group, topk_group, compiled, num_fused_shared_experts, routed_scaling_factor, num_token_non_padded, expert_location_dispatch_info, apply_routed_scaling_factor_on_output)
select_experts(hidden_states, router_logits, topk_config)

# python/sglang/srt/layers/moe/utils.py
  MoeA2ABackend.is_none()
  MoeA2ABackend.is_deepep()
  MoeRunnerBackend.is_auto()
  MoeRunnerBackend.is_triton()
  MoeRunnerBackend.is_triton_kernel()
  MoeRunnerBackend.is_flashinfer_trtllm()
  MoeRunnerBackend.is_flashinfer_cutlass()
  MoeRunnerBackend.is_flashinfer_mxfp4()
  DeepEPMode.enable_normal()
  DeepEPMode.enable_low_latency()
  DeepEPMode.resolve(is_extend_in_batch)
  DeepEPMode.is_normal()
  DeepEPMode.is_low_latency()
  DeepEPMode.is_auto()
initialize_moe_config(server_args)
get_moe_a2a_backend()
get_moe_runner_backend()
get_deepep_mode()
get_deepep_config()
is_tbo_enabled()
get_tbo_token_distribution_threshold()
should_use_flashinfer_trtllm_moe()
should_use_flashinfer_cutlass_moe_fp4_allgather()