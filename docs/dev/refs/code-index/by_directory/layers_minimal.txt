
# python/sglang/srt/layers/activation.py
  SiluAndMul.forward_native(x)
  SiluAndMul.forward_cuda(x)
  SiluAndMul.forward_cpu(x)
  SiluAndMul.forward_npu(x)
  GeluAndMul.__init__(approximate)
  GeluAndMul.forward_native(x)
  GeluAndMul.forward_cuda(x)
  NewGELU.forward_native(x)
  NewGELU.forward_cuda(x)
  ReLU2.forward(x)
  QuickGELU.forward_native(x)
  QuickGELU.forward_cuda(x)
  QuickGELU.forward_hip(x)
  ScaledActivation.__init__(act_module, intermediate_size, input_is_parallel, params_dtype)
  ScaledActivation.forward(x)
  ScaledActivation.weight_loader(param, loaded_weight)
get_act_fn(act_fn_name, quant_config, intermediate_size, input_is_parallel, params_dtype)
get_cross_encoder_activation_function(config)

# python/sglang/srt/layers/amx_utils.py
amx_process_weight_after_loading(weight)
dim_is_supported(weight)
  PackWeightMethod.__init__(weight_names, transpose_dims)
  PackWeightMethod.process_weights_after_loading(module)

# python/sglang/srt/layers/communicator.py
  ScatterMode.model_input_output()
  _LayerModeComputationContext.previous_layer()
  LayerScatterModes.init_new(cls)
enable_moe_dense_fully_dp()
  LayerCommunicator.__init__(layer_scatter_modes, input_layernorm, post_attention_layernorm, allow_reduce_scatter, is_last_layer)
  LayerCommunicator.prepare_attn(hidden_states, residual, forward_batch)
  LayerCommunicator.prepare_mlp(hidden_states, residual, forward_batch)
  LayerCommunicator.postprocess_layer(hidden_states, residual, forward_batch)
  LayerCommunicator.should_use_reduce_scatter(forward_batch)
  LayerCommunicator.should_fuse_mlp_allreduce_with_next_layer(forward_batch)
  CommunicateContext.is_same_group_size(a, b)
  CommunicateContext.init_new(cls)
  CommunicateSimpleFn.get_fn(input_mode, output_mode, context)
  CommunicateWithAllReduceAndLayerNormFn.get_fn(hidden_states_input_mode, residual_input_mode, hidden_states_output_mode, residual_output_mode, context)
  CommunicateSummableTensorPairFn.execute(cls, hidden_states_input_mode, residual_input_mode, output_mode, context)
  CommunicateSummableTensorPairFn.get_fn(hidden_states_input_mode, residual_input_mode, output_mode, context)

# python/sglang/srt/layers/dp_attention.py
  DpPaddingMode.is_max_len()
  DpPaddingMode.is_sum_len()
  DpPaddingMode.get_dp_padding_mode(cls, global_num_tokens)
  DpPaddingMode.get_default_mode_in_cuda_graph(cls)
  _DpGatheredBufferWrapper.set_metadata(cls, hidden_size, dtype, device)
  _DpGatheredBufferWrapper.set_dp_buffer_len(cls, global_dp_buffer_len, local_dp_buffer_len, global_num_tokens)
  _DpGatheredBufferWrapper.get_global_dp_buffer(cls)
  _DpGatheredBufferWrapper.get_local_dp_buffer(cls)
  _DpGatheredBufferWrapper.get_global_dp_buffer_len(cls)
  _DpGatheredBufferWrapper.get_local_dp_buffer_len(cls)
  _DpGatheredBufferWrapper.get_dp_global_num_tokens(cls)
set_dp_buffer_len(global_dp_buffer_len, local_dp_buffer_len, global_num_tokens)
get_global_dp_buffer()
get_local_dp_buffer()
get_global_dp_buffer_len()
get_local_dp_buffer_len()
get_dp_global_num_tokens()
compute_dp_attention_world_info(enable_dp_attention, tp_rank, tp_size, dp_size)
compute_dp_attention_local_info(enable_dp_attention, tp_rank, tp_size, dp_size, moe_dense_tp_size)
initialize_dp_attention(server_args, model_config)
is_dp_attention_enabled()
get_attention_tp_group()
get_attention_tp_rank()
get_attention_tp_size()
get_attention_dp_rank()
get_attention_dp_size()
get_local_attention_dp_rank()
get_local_attention_dp_size()
disable_dp_size()
get_dp_local_info(forward_batch)
memcpy_triton_kernel(dst_ptr, src_ptr, offset_ptr, sz_ptr, offset_src, chunk_size, BLOCK_SIZE)
prod(x)
memcpy_triton(dst, src, dim, offset, sz, offset_src)
dp_gather_partial(global_tokens, local_tokens, forward_batch)
dp_gather_replicate(global_tokens, local_tokens, forward_batch)
dp_scatter(local_tokens, global_tokens, forward_batch)
dp_reduce_scatter_tensor(output, input)
attn_tp_reduce_scatter_tensor(output, input)
attn_tp_all_gather_into_tensor(output, input)
attn_tp_all_gather(output_list, input)

# python/sglang/srt/layers/elementwise.py
fused_softcap_kernel(output_ptr, input_ptr, n_ele, softcap_const, BLOCK_SIZE)
fused_softcap(x, softcap_const, autotune)
  Softcap.__init__(softcap_const)
  Softcap.__call__()
  Softcap.forward(x)
  Softcap.forward_native(x)
  Softcap.forward_cuda(x, autotune)
fused_dual_residual_rmsnorm_kernel(output_ptr, mid_ptr, activ_ptr, residual_ptr, weight1_ptr, weight2_ptr, eps, hidden_dim, BLOCK_SIZE)
fused_dual_residual_rmsnorm(x, residual, weight1, weight2, eps, autotune)
fused_rmsnorm_kernel(output_ptr, activ_ptr, weight_ptr, eps, hidden_dim, BLOCK_SIZE)
fused_rmsnorm(x, weight, eps, autotune, inplace)
  FusedDualResidualRMSNorm.__init__(rmsnorm1, rmsnorm2)
  FusedDualResidualRMSNorm.__call__()
  FusedDualResidualRMSNorm.forward(x, residual)
  FusedDualResidualRMSNorm.forward_cuda(x, residual, autotune)
  FusedDualResidualRMSNorm.forward_flashinfer(x, residual)
  FusedDualResidualRMSNorm.forward_native(x, residual)
experts_combine_kernel(out_hidden_states, moe_hidden_states, mlp_hidden_states, combine_k, hidden_dim, BLOCK_SIZE)
experts_combine_triton(moe_hidden_states, mlp_hidden_states, output_buffer)
gelu_and_mul_kernel(out_hidden_states_ptr, out_scales_ptr, hidden_states_ptr, quant_max, static_scale, hidden_dim, BLOCK_SIZE)
gelu_and_mul_triton(hidden_states, scales, quantize, out)
silu_and_mul_kernel(out_hidden_states_ptr, out_scales_ptr, hidden_states_ptr, quant_max, static_scale, hidden_dim, BLOCK_SIZE)
silu_and_mul_triton(hidden_states, scales, quantize, out)

# python/sglang/srt/layers/flashinfer_comm_fusion.py
  FlashInferWorkspaceManager.__init__()
  FlashInferWorkspaceManager.initialize(world_size, rank, max_token_num, hidden_dim, group, use_fp32_lamport)
  FlashInferWorkspaceManager.cleanup()
ensure_workspace_initialized(max_token_num, hidden_dim, use_fp32_lamport)
flashinfer_allreduce_residual_rmsnorm(input_tensor, residual, weight, eps, max_token_num, use_oneshot, trigger_completion_at_end, fp32_acc)
fake_flashinfer_allreduce_residual_rmsnorm(input_tensor, residual, weight, eps, max_token_num, use_oneshot, trigger_completion_at_end, fp32_acc)
cleanup_flashinfer_workspace()

# python/sglang/srt/layers/layernorm.py
  RMSNorm.__init__(hidden_size, eps, var_hidden_size)
  RMSNorm.forward_cuda(x, residual)
  RMSNorm.forward_npu(x, residual)
  RMSNorm.forward_aiter(x, residual)
  RMSNorm.forward_hip(x, residual)
  RMSNorm.forward_native(x, residual)
  RMSNorm.forward_cpu(x, residual)
  RMSNorm.forward_with_allreduce_fusion(x, residual)
  GemmaRMSNorm.__init__(hidden_size, eps)
  GemmaRMSNorm.forward_native(x, residual)
  GemmaRMSNorm.forward_cuda(x, residual)
  Gemma3RMSNorm.__init__(dim, eps)
  Gemma3RMSNorm.forward(x)
  Gemma3RMSNorm.extra_repr()

# python/sglang/srt/layers/linear.py
adjust_marlin_shard(param, shard_size, shard_offset)
adjust_bitsandbytes_4bit_shard(param, shard_offsets, Tuple[int, int]], loaded_shard_id)
adjust_scalar_to_fused_array(param, loaded_weight, shard_id)
adjust_shard_offsets(shard_offsets, loaded_weight, dim)
  LinearBase.__init__(input_size, output_size, skip_bias_add, params_dtype, quant_config, prefix)
  LinearBase.forward(x)
  ReplicatedLinear.__init__(input_size, output_size, bias, skip_bias_add, params_dtype, quant_config, prefix)
  ReplicatedLinear.weight_loader(param, loaded_weight)
  ReplicatedLinear.forward(x)
  ReplicatedLinear.extra_repr()
  ColumnParallelLinear.__init__(input_size, output_size, bias, gather_output, skip_bias_add, params_dtype, quant_config, output_sizes, prefix, tp_rank, tp_size, use_presharded_weights)
  ColumnParallelLinear.weight_loader(param, loaded_weight)
  ColumnParallelLinear.weight_loader_v2(param, loaded_weight)
  ColumnParallelLinear.forward(input_)
  ColumnParallelLinear.extra_repr()
  MergedColumnParallelLinear.__init__(input_size, output_sizes, bias, gather_output, skip_bias_add, params_dtype, quant_config, prefix, tp_rank, tp_size, use_presharded_weights)
  MergedColumnParallelLinear.weight_loader(param, loaded_weight, loaded_shard_id)
  MergedColumnParallelLinear.weight_loader_v2(param, loaded_weight, loaded_shard_id)
  QKVParallelLinear.__init__(hidden_size, head_size, total_num_heads, total_num_kv_heads, bias, skip_bias_add, params_dtype, quant_config, prefix, tp_rank, tp_size, load_presharded_attn)
  QKVParallelLinear.weight_loader_v2(param, loaded_weight, loaded_shard_id)
  QKVParallelLinear.weight_loader(param, loaded_weight, loaded_shard_id)
  RowParallelLinear.__init__(input_size, output_size, bias, input_is_parallel, skip_bias_add, params_dtype, reduce_results, quant_config, prefix, tp_rank, tp_size, use_presharded_weights)
  RowParallelLinear.weight_loader(param, loaded_weight)
  RowParallelLinear.weight_loader_v2(param, loaded_weight)
  RowParallelLinear.forward(input_, skip_all_reduce)
  RowParallelLinear.extra_repr()

# python/sglang/srt/layers/logits_processor.py
  LogitsMetadata.from_forward_batch(cls, forward_batch)
  LogitsMetadata.compute_dp_attention_metadata()
  LogitsProcessor.__init__(config, skip_all_gather, logit_scale)
  LogitsProcessor.forward(input_ids, hidden_states, lm_head, logits_metadata, ForwardBatch], aux_hidden_states)
  LogitsProcessor.get_top_logprobs(all_logprobs, logits_metadata)
  LogitsProcessor.get_token_ids_logprobs(all_logprobs, logits_metadata)
  LogitsProcessor.compute_temp_top_p_normalized_logprobs(last_logits, logits_metadata)
fused_softcap_kernel(full_logits_ptr, softcapping_value, n_elements, BLOCK_SIZE)
fused_softcap(full_logits, final_logit_softcapping)

# python/sglang/srt/layers/multimodal.py
hash_tiles32_kernel_blocked(in_ptr, out_ptr, n_u32, seed1, seed2, FM_C1, FM_C2, POS_A, POS_B, TILE, BLOCK, USE_CG)
add_tree_reduce_u64_kernel(in_ptr, out_ptr, n_elems, CHUNK)
gpu_tensor_hash(tensor)

# python/sglang/srt/layers/parameter.py
  BasevLLMParameter.__new__(cls, data)
  BasevLLMParameter.__init__(data, weight_loader)
  BasevLLMParameter.weight_loader()
  BasevLLMParameter.load_column_parallel_weight(loaded_weight)
  BasevLLMParameter.load_row_parallel_weight(loaded_weight)
  BasevLLMParameter.load_merged_column_weight(loaded_weight)
  BasevLLMParameter.load_qkv_weight(loaded_weight)
  _ColumnvLLMParameter.__init__(output_dim)
  _ColumnvLLMParameter.output_dim()
  _ColumnvLLMParameter.load_column_parallel_weight(loaded_weight, tp_rank, use_presharded_weights)
  _ColumnvLLMParameter.load_merged_column_weight(loaded_weight)
  _ColumnvLLMParameter.load_qkv_weight(loaded_weight, tp_rank, use_presharded_weights)
  RowvLLMParameter.__init__(input_dim)
  RowvLLMParameter.input_dim()
  RowvLLMParameter.load_row_parallel_weight(loaded_weight, tp_rank, use_presharded_weights)
  PerTensorScaleParameter.__init__()
  PerTensorScaleParameter.load_row_parallel_weight()
  PerTensorScaleParameter.load_merged_column_weight()
  PerTensorScaleParameter.load_qkv_weight()
  PerTensorScaleParameter.load_column_parallel_weight()
  PackedColumnParameter.__init__(packed_factor, Fraction], packed_dim, marlin_tile_size)
  PackedColumnParameter.packed_dim()
  PackedColumnParameter.packed_factor()
  PackedColumnParameter.marlin_tile_size()
  PackedColumnParameter.adjust_shard_indexes_for_packing(shard_size, shard_offset)
  PackedvLLMParameter.__init__(packed_factor, Fraction], packed_dim, marlin_tile_size)
  PackedvLLMParameter.packed_dim()
  PackedvLLMParameter.packed_factor()
  PackedvLLMParameter.marlin_tile_size()
  PackedvLLMParameter.adjust_shard_indexes_for_packing(shard_size, shard_offset)
permute_param_layout_(param, input_dim, output_dim)

# python/sglang/srt/layers/pooler.py
  Pooler.__init__(pooling_type, normalize)
  Pooler.forward(hidden_states, forward_batch)
  CrossEncodingPooler.__init__(config, classifier, pooler)
  CrossEncodingPooler.forward(hidden_states, forward_batch)

# python/sglang/srt/layers/radix_attention.py
  RadixAttention.__init__(num_heads, head_dim, scaling, num_kv_heads, layer_id, logit_cap, v_head_dim, sliding_window_size, is_cross_attention, pos_encoding_mode, logit_capping_method, quant_config, attn_type, use_irope, prefix)
  RadixAttention.forward(q, k, v, forward_batch, save_kv_cache)

# python/sglang/srt/layers/rotary_embedding.py
  RotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, dtype)
  RotaryEmbedding.forward_native(positions, query, key, offsets)
  RotaryEmbedding.forward_npu(positions, query, key, offsets)
  RotaryEmbedding.forward_cpu(positions, query, key, offsets)
  RotaryEmbedding.forward_cuda(positions, query, key, offsets, fused_set_kv_buffer_arg)
  RotaryEmbedding.extra_repr()
  LinearScalingRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, scaling_factors, float], dtype)
  LinearScalingRotaryEmbedding.scaling_factor_to_offset()
  DynamicNTKScalingRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, scaling_factor, dtype)
  YaRNScalingRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, scaling_factor, dtype)
  Phi3LongRoPEScaledRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, original_max_position_embeddings, base, is_neox_style, dtype, short_factor, long_factor, short_mscale, long_mscale)
  Phi3LongRoPEScaledRotaryEmbedding.forward(positions, query, key, offsets)
yarn_get_mscale(scale, mscale)
  DeepseekScalingRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, scaling_factor, dtype)
  DeepseekScalingRotaryEmbedding.forward_native(positions, query, key, offsets)
  DeepseekScalingRotaryEmbedding.forward_npu(positions, query, key, offsets)
  DeepseekScalingRotaryEmbedding.forward_cpu(positions, query, key, offsets)
  Llama3RotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, dtype, scaling_factor, low_freq_factor, high_freq_factor, orig_max_position)
  Llama4VisionRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, dtype)
  Llama4VisionRotaryEmbedding.forward(query, key)
  DynamicNTKAlphaRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, scaling_alpha, dtype)
  MRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, dtype, mrope_section)
  MRotaryEmbedding.forward(positions, query, key)
  MRotaryEmbedding.get_rope_index(spatial_merge_size, image_token_id, video_token_id, vision_start_token_id, model_type, tokens_per_second, input_ids, image_grid_thw, video_grid_thw, second_per_grid_ts)
  MRotaryEmbedding.get_rope_index_glm4v(input_ids, hf_config, image_grid_thw, torch.Tensor], video_grid_thw, torch.Tensor], attention_mask)
  MRotaryEmbedding.get_next_input_positions(mrope_position_delta, context_len, seq_len)
  DualChunkRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, dtype, chunk_size, local_size)
  DualChunkRotaryEmbedding.forward(positions, query, key, offsets)
  DualChunkRotaryEmbedding.extra_repr()
get_rope(head_size, rotary_dim, max_position, base, is_neox_style, rope_scaling, Any]], dtype, partial_rotary_factor, dual_chunk_attention_config, Any]])
rotate_half(x)
apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim)
get_rope_cpu(head_size, rotary_dim, max_position, base, is_neox_style, rope_scaling, Any]], dtype, partial_rotary_factor, device)
get_rope_wrapper(head_size, rotary_dim, max_position, base, is_neox_style, rope_scaling, Any]], dtype, partial_rotary_factor, device)

# python/sglang/srt/layers/sampler.py
  Sampler.__init__()
  Sampler.forward(logits_output, sampling_info, return_logprob, top_logprobs_nums, token_ids_logprobs)
top_k_top_p_min_p_sampling_from_probs_torch(probs, top_ks, top_ps, min_ps, need_min_p_sampling)
sampling_from_probs_torch(probs)
top_p_normalize_probs_torch(probs, top_ps)
get_top_logprobs(logprobs, top_logprobs_nums)
get_token_ids_logprobs(logprobs, token_ids_logprobs)
apply_custom_logit_processor(logits, sampling_batch_info, num_tokens_in_batch)

# python/sglang/srt/layers/torchao_utils.py
get_gemlite_cache_path()
save_gemlite_cache(print_error)
proj_filter(module, fqn)
apply_torchao_config_to_model(model, torchao_config, filter_fn)

# python/sglang/srt/layers/utils.py
get_layer_id(weight_name)
  PPMissingLayer.__init__()
  PPMissingLayer.forward()

# python/sglang/srt/layers/vocab_parallel_embedding.py
pad_vocab_size(vocab_size, pad_to)
vocab_range_from_per_partition_vocab_size(per_partition_vocab_size, rank, offset)
vocab_range_from_global_vocab_size(global_vocab_size, rank, world_size, offset)
  VocabParallelEmbeddingShardIndices.num_org_elements()
  VocabParallelEmbeddingShardIndices.num_added_elements()
  VocabParallelEmbeddingShardIndices.num_org_elements_padded()
  VocabParallelEmbeddingShardIndices.num_added_elements_padded()
  VocabParallelEmbeddingShardIndices.num_org_vocab_padding()
  VocabParallelEmbeddingShardIndices.num_added_vocab_padding()
  VocabParallelEmbeddingShardIndices.num_elements_padded()
  VocabParallelEmbeddingShardIndices.__post_init__()
get_masked_input_and_mask(input_, org_vocab_start_index, org_vocab_end_index, num_org_vocab_padding, added_vocab_start_index, added_vocab_end_index)
  VocabParallelEmbedding.__init__(num_embeddings, embedding_dim)
  VocabParallelEmbedding.get_sharded_to_full_mapping()
  VocabParallelEmbedding.weight_loader(param, loaded_weight)
  VocabParallelEmbedding.forward(input_)
  VocabParallelEmbedding.extra_repr()
  ParallelLMHead.__init__(num_embeddings, embedding_dim)
  ParallelLMHead.tie_weights(embed_tokens)
  ParallelLMHead.forward(input_)