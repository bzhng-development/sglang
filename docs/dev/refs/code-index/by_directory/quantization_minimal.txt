
# python/sglang/srt/layers/quantization/__init__.py
  DummyConfig.override_quantization_method()
get_quantization_config(quantization)
monkey_patch_isinstance_for_vllm_base_layer(reverse)
monkey_patch_moe_apply(class_obj)
monkey_patch_quant_configs()

# python/sglang/srt/layers/quantization/awq.py
is_layer_skipped_awq(prefix, modules_to_not_convert)
  AWQConfig.__init__(weight_bits, group_size, zero_point, modules_to_not_convert)
  AWQConfig.__repr__()
  AWQConfig.get_scaled_act_names()
  AWQConfig.get_name()
  AWQConfig.get_supported_act_dtypes()
  AWQConfig.get_min_capability(cls)
  AWQConfig.get_config_filenames()
  AWQConfig.from_config(cls, config, Any])
  AWQConfig.get_quant_method(layer, prefix)
  AWQMarlinConfig.__init__(weight_bits, group_size, zero_point, lm_head_quantized, modules_to_not_convert, full_config, Any])
  AWQMarlinConfig.__repr__()
  AWQMarlinConfig.get_scaled_act_names()
  AWQMarlinConfig.get_name(cls)
  AWQMarlinConfig.get_supported_act_dtypes(cls)
  AWQMarlinConfig.get_min_capability(cls)
  AWQMarlinConfig.get_config_filenames(cls)
  AWQMarlinConfig.from_config(cls, config, Any])
  AWQMarlinConfig.override_quantization_method(cls, hf_quant_cfg, user_quant)
  AWQMarlinConfig.get_quant_method(layer, prefix)
  AWQMarlinConfig.is_awq_marlin_compatible(cls, quant_config, Any])
  AWQLinearMethod.__init__(quant_config)
  AWQLinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  AWQLinearMethod.process_weights_after_loading(layer)
  AWQLinearMethod.apply(layer, x, bias)
  AWQMarlinLinearMethod.__init__(quant_config)
  AWQMarlinLinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  AWQMarlinLinearMethod.process_weights_after_loading(layer)
  AWQMarlinLinearMethod.apply(layer, x, bias)
  AWQMoEMethod.__init__(quant_config)
  AWQMoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size_per_partition, params_dtype)
  AWQMoEMethod.process_weights_after_loading(layer)
  AWQMoEMethod.apply(layer, x, topk_output, moe_runner_config)

# python/sglang/srt/layers/quantization/awq_triton.py
awq_dequantize_kernel(qweight_ptr, scales_ptr, zeros_ptr, group_size, result_ptr, num_cols, num_rows, BLOCK_SIZE_X, BLOCK_SIZE_Y)
awq_gemm_kernel(a_ptr, b_ptr, c_ptr, zeros_ptr, scales_ptr, M, N, K, group_size, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, SPLIT_K)
awq_dequantize_triton(qweight, scales, zeros, block_size_x, block_size_y)
awq_gemm_triton(input, qweight, scales, qzeros, split_k_iters, block_size_m, block_size_n, block_size_k)

# python/sglang/srt/layers/quantization/base_config.py
  QuantizeMethodBase.create_weights(layer)
  QuantizeMethodBase.apply(layer)
  QuantizeMethodBase.process_weights_after_loading(layer)
  LinearMethodBase.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  LinearMethodBase.apply(layer, x, bias)
  FusedMoEMethodBase.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype)
  FusedMoEMethodBase.apply(layer, x, topk_output, moe_runner_config)
  QuantizationConfig.__init__()
  QuantizationConfig.get_name()
  QuantizationConfig.get_supported_act_dtypes()
  QuantizationConfig.get_min_capability(cls)
  QuantizationConfig.get_config_filenames()
  QuantizationConfig.from_config(cls, config, Any])
  QuantizationConfig.override_quantization_method(cls, hf_quant_cfg, user_quant)
  QuantizationConfig.get_from_keys(config, Any], keys)
  QuantizationConfig.get_from_keys_or(config, Any], keys, default)
  QuantizationConfig.get_quant_method(layer, prefix)
  QuantizationConfig.get_scaled_act_names()
method_has_implemented_embedding(method_class)

# python/sglang/srt/layers/quantization/blockwise_int8.py
  BlockInt8Config.__init__(is_checkpoint_int8_serialized, activation_scheme, ignored_layers, weight_block_size)
  BlockInt8Config.get_name(cls)
  BlockInt8Config.get_supported_act_dtypes(cls)
  BlockInt8Config.get_min_capability(cls)
  BlockInt8Config.get_config_filenames(cls)
  BlockInt8Config.from_config(cls, config, Any])
  BlockInt8Config.get_quant_method(layer, prefix)
  BlockInt8Config.get_scaled_act_names()
  BlockInt8LinearMethod.__init__(quant_config)
  BlockInt8LinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  BlockInt8LinearMethod.process_weights_after_loading(layer)
  BlockInt8LinearMethod.apply(layer, x, bias)
  BlockInt8MoEMethod.__init__(quant_config)
  BlockInt8MoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype)
  BlockInt8MoEMethod.process_weights_after_loading(layer)
  BlockInt8MoEMethod.apply(layer, x, topk_output, moe_runner_config)

# python/sglang/srt/layers/quantization/fp8.py
dummy_func()
  Fp8Config.__init__(is_checkpoint_fp8_serialized, activation_scheme, ignored_layers, weight_block_size)
  Fp8Config.get_name(cls)
  Fp8Config.get_supported_act_dtypes(cls)
  Fp8Config.get_min_capability(cls)
  Fp8Config.get_config_filenames(cls)
  Fp8Config.from_config(cls, config, Any])
  Fp8Config.get_quant_method(layer, prefix)
  Fp8Config.get_scaled_act_names()
  Fp8LinearMethod.__init__(quant_config, W4AFp8Config])
  Fp8LinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  Fp8LinearMethod.process_weights_after_loading(layer)
  Fp8LinearMethod.apply(layer, x, bias)
get_tile_tokens_dim(num_tokens, top_k, num_experts)
  Fp8MoEMethod.__init__(quant_config)
  Fp8MoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype)
  Fp8MoEMethod.process_weights_after_loading(layer)
  Fp8MoEMethod.process_weights_hip_int4(layer)
  Fp8MoEMethod.process_weights_hip_scale_padding(layer)
  Fp8MoEMethod.apply(layer, x, topk_output, moe_runner_config)
  Fp8MoEMethod.apply_with_router_logits(layer, x, topk_output, moe_runner_config)
  Fp8MoEMethod.maybe_apply_hip_fused_experts(layer, x, topk_output, activation, no_combine)
  Fp8KVCacheMethod.__init__(quant_config)

# python/sglang/srt/layers/quantization/fp8_kernel.py
is_fp8_fnuz()
deep_gemm_fp8_fp8_bf16_nt(A, As, B, Bs, C)
deep_gemm_fp8_fp8_bf16_nt_fake(A, As, B, Bs, C)
per_token_group_quant_8bit(x, group_size, dst_dtype, eps, column_major_scales, scale_tma_aligned, scale_ue8m0, fuse_silu_and_mul, masked_m)
create_per_token_group_quant_fp8_output_scale(x_shape, device, group_size, column_major_scales, scale_tma_aligned, scale_ue8m0)
sglang_per_token_group_quant_fp8(x, group_size, eps, column_major_scales, scale_tma_aligned, scale_ue8m0, fuse_silu_and_mul, masked_m)
sglang_per_token_group_quant_8bit(x, group_size, dst_dtype, eps, column_major_scales, scale_tma_aligned, scale_ue8m0, fuse_silu_and_mul, masked_m)
sglang_per_token_quant_fp8(x, dtype)
static_quant_fp8(x, x_s, repeat_scale)
get_w8a8_block_fp8_configs(N, K, block_n, block_k)
select_w8a8_block_fp8_matmul_kernel(M, N, META)
use_w8a8_block_fp8_matmul_unrolledx4(M, N, META)
select_w8a8_block_fp8_matmul_kernel(M, N, META)
prepare_block_fp8_matmul_inputs(A, B, As, Bs, block_size, output_dtype)
w8a8_block_fp8_matmul_deepgemm(A, B, As, Bs, block_size, output_dtype)
w8a8_block_fp8_matmul_triton(A, B, As, Bs, block_size, output_dtype)
w8a8_block_fp8_matmul(A, B, As, Bs, block_size, output_dtype)
per_tensor_quant_mla_fp8(x, x_s_out, eps)
per_token_group_quant_mla_deep_gemm_masked_fp8(x, group_size, eps, dtype)
scaled_fp8_quant(input, scale, num_token_padding, use_per_token_if_dynamic)
scaled_fp8_quant(input, scale, num_token_padding, use_per_token_if_dynamic)
per_token_group_quant_fp8_hopper_moe_mn_major(A, expert_offsets, problem_sizes, group_size, expert_tokens_alignment)
per_group_transpose(a, expert_offsets, M_ALIGNMENT)
is_weak_contiguous(x)
scaled_mm_kernel(a_ptr, b_ptr, scale_a_ptr, scale_b_ptr, c_ptr, bias_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, ACCUMULATOR_DTYPE, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, BLOCK_SIZE_SCALE_A, BLOCK_SIZE_SCALE_B)
triton_scaled_mm(input, weight, scale_a, scale_b, out_dtype, bias, block_size_m, block_size_n, block_size_k, use_heuristic)

# python/sglang/srt/layers/quantization/fp8_utils.py
use_rowwise_torch_scaled_mm()
cutlass_fp8_supported()
normalize_e4m3fn_to_e4m3fnuz(weight, weight_scale, input_scale)
cutlass_block_fp8_supported()
dispatch_w8a8_block_fp8_linear()
flashinfer_gemm_w8a8_block_fp8_linear(input, weight, block_size, weight_scale, input_scale, bias)
cutlass_w8a8_block_fp8_linear_with_fallback(input, weight, block_size, weight_scale, input_scale, bias)
deepgemm_w8a8_block_fp8_linear_with_fallback(input, weight, block_size, weight_scale, input_scale, bias)
aiter_w8a8_block_fp8_linear(input, weight, block_size, weight_scale, input_scale, bias)
triton_w8a8_block_fp8_linear(input, weight, block_size, weight_scale, input_scale, bias)
dequant_mxfp4(w_block, w_scale, out_dtype)
input_to_float8(x, dtype)
block_quant_to_tensor_quant(x_q_block, x_s, block_size)
block_quant_dequant(x_q_block, x_s, block_size, dtype)
requant_weight_ue8m0_inplace(weight, weight_scale_inv, weight_block_size)
per_block_cast_to_fp8(x)
ceil_to_ue8m0(x)
channel_quant_to_tensor_quant(x_q_channel, x_s)
apply_fp8_linear(input, weight, weight_scale, input_scale, input_scale_ub, bias, cutlass_fp8_supported, use_per_token_if_dynamic, pad_output, compressed_tensor_quant)
can_auto_enable_marlin_fp8()

# python/sglang/srt/layers/quantization/fpgemm_fp8.py
  FBGEMMFp8Config.__init__(ignore_list, input_scale_ub)
  FBGEMMFp8Config.get_name(cls)
  FBGEMMFp8Config.get_supported_act_dtypes(cls)
  FBGEMMFp8Config.get_min_capability(cls)
  FBGEMMFp8Config.get_config_filenames(cls)
  FBGEMMFp8Config.from_config(cls, config, Any])
  FBGEMMFp8Config.get_quant_method(layer, prefix)
  FBGEMMFp8Config.get_scaled_act_names()
  FBGEMMFp8LinearMethod.__init__(quant_config)
  FBGEMMFp8LinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  FBGEMMFp8LinearMethod.process_weights_after_loading(layer)
  FBGEMMFp8LinearMethod.apply(layer, x, bias)

# python/sglang/srt/layers/quantization/gptq.py
check_marlin_format(hf_quant_cfg, Any])
gptq_marlin_moe_repack(b_q_weight, perm, size_k, size_n, num_bits)
  GPTQConfig.__init__(weight_bits, group_size, desc_act, lm_head_quantized, dynamic, Dict[str, Union[int, bool]]])
  GPTQConfig.__repr__()
  GPTQConfig.get_scaled_act_names()
  GPTQConfig.get_name(cls)
  GPTQConfig.get_supported_act_dtypes(cls)
  GPTQConfig.get_min_capability(cls)
  GPTQConfig.get_config_filenames(cls)
  GPTQConfig.from_config(cls, config, Any])
  GPTQConfig.get_quant_method(layer, prefix)
  GPTQMarlinConfig.__init__(weight_bits, group_size, desc_act, is_sym, lm_head_quantized, dynamic, Dict[str, Union[int, bool]]], full_config, Any])
  GPTQMarlinConfig.__repr__()
  GPTQMarlinConfig.get_scaled_act_names()
  GPTQMarlinConfig.get_name(cls)
  GPTQMarlinConfig.get_supported_act_dtypes(cls)
  GPTQMarlinConfig.get_min_capability(cls)
  GPTQMarlinConfig.get_config_filenames(cls)
  GPTQMarlinConfig.from_config(cls, config, Any])
  GPTQMarlinConfig.override_quantization_method(cls, hf_quant_cfg, user_quant)
  GPTQMarlinConfig.get_quant_method(layer, prefix)
  GPTQMarlinConfig.is_gptq_marlin_compatible(cls, quant_config, Any])
  GPTQLinearMethod.__init__(quant_config)
  GPTQLinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  GPTQLinearMethod.process_weights_after_loading(layer)
  GPTQLinearMethod.apply(layer, x, bias)
  GPTQMarlinLinearMethod.__init__(quant_config)
  GPTQMarlinLinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  GPTQMarlinLinearMethod.process_weights_after_loading(layer)
  GPTQMarlinLinearMethod.apply(layer, x, bias)
  GPTQMarlinMoEMethod.__init__(quant_config)
  GPTQMarlinMoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size_per_partition, params_dtype)
  GPTQMarlinMoEMethod.process_weights_after_loading(layer)
  GPTQMarlinMoEMethod.apply(layer, x, topk_output, moe_runner_config)

# python/sglang/srt/layers/quantization/int8_kernel.py
per_token_quant_int8(x, scale_dtype, cal_sum)
per_token_group_quant_int8(x, group_size, eps, dtype)
sglang_per_token_group_quant_int8(x, group_size, eps, dtype)
get_w8a8_block_int8_configs(N, K, block_n, block_k)
w8a8_block_int8_matmul(A, B, As, Bs, block_size, output_dtype)

# python/sglang/srt/layers/quantization/int8_utils.py
apply_w8a8_block_int8_linear(input, weight, block_size, weight_scale, input_scale, bias)
input_to_int8(x, dtype)
block_dequant(x_q_block, x_s, block_size)

# python/sglang/srt/layers/quantization/kv_cache.py
  BaseKVCacheMethod.__init__(quant_config)
  BaseKVCacheMethod.create_weights(layer)
  BaseKVCacheMethod.apply(layer)
  BaseKVCacheMethod.process_weights_after_loading(layer)

# python/sglang/srt/layers/quantization/marlin_utils.py
query_marlin_supported_quant_types(has_zp, include_fp_type, device_capability)
check_marlin_supported(quant_type, group_size, has_zp, device_capability)
verify_marlin_supported(quant_type, group_size, has_zp)
verify_marlin_supports_shape(output_size_per_partition, input_size_per_partition, input_size, group_size)
check_marlin_supports_shape(output_size_per_partition, input_size_per_partition, input_size, group_size)
check_marlin_supports_layer(layer, group_size)
check_moe_marlin_supports_layer(layer, group_size)
marlin_make_workspace(device, max_blocks_per_sm)
marlin_is_k_full(act_order, is_row_parallel)
marlin_repeat_scales_on_all_ranks(act_order, group_size, is_row_parallel)
marlin_make_empty_g_idx(device)
marlin_make_empty_zp(device)
marlin_sort_g_idx(g_idx)
get_scale_perms()
marlin_permute_scales(s, size_k, size_n, group_size)
marlin_permute_bias(s)
marlin_moe_permute_scales(s, size_k, size_n, group_size)
marlin_zero_points(zp, size_k, size_n, num_bits)
awq_to_marlin_zero_points(q_zp_packed, size_k, size_n, num_bits)
moe_awq_to_marlin_zero_points(q_zp_packed, size_k, size_n, num_bits)
maybe_warn_marlin_atomic_add(device, dtype)
maybe_warn_marlin_atomic_add_env()
should_use_atomic_add_reduce(m, n, k, device, dtype)
apply_gptq_marlin_linear(input, weight, weight_scale, weight_zp, g_idx, g_idx_sort_indices, workspace, wtype, output_size_per_partition, input_size_per_partition, is_k_full, bias, use_fp32_reduce)
apply_awq_marlin_linear(input, weight, weight_scale, weight_zp, g_idx, g_idx_sort_indices, workspace, quant_type, output_size_per_partition, input_size_per_partition, bias, use_fp32_reduce)
  MarlinConfig.__init__(group_size, lm_head_quantized)
  MarlinConfig.__repr__()
  MarlinConfig.get_name(cls)
  MarlinConfig.get_supported_act_dtypes(cls)
  MarlinConfig.get_min_capability(cls)
  MarlinConfig.get_config_filenames(cls)
  MarlinConfig.from_config(cls, config, Any])
  MarlinConfig.override_quantization_method(cls, hf_quant_cfg, user_quant)
  MarlinConfig.get_quant_method(layer, prefix)
  MarlinLinearMethod.__init__(quant_config)
  MarlinLinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  MarlinLinearMethod.process_weights_after_loading(layer)
  MarlinLinearMethod.apply(layer, x, bias)

# python/sglang/srt/layers/quantization/marlin_utils_fp8.py
fp8_fused_exponent_bias_into_scales(scales)
apply_fp8_marlin_linear(input, weight, weight_scale, workspace, size_n, size_k, bias, use_fp32_reduce)
prepare_fp8_layer_for_marlin(layer, size_k_first)
prepare_moe_fp8_layer_for_marlin(layer, size_k_first)
pack_fp8_to_int32(fp8_tensor, size_k_first)
marlin_quant_fp8_torch(weight, group_size)

# python/sglang/srt/layers/quantization/modelopt_quant.py
  ModelOptFp8Config.__init__(is_checkpoint_fp8_serialized, kv_cache_quant_method, exclude_modules)
  ModelOptFp8Config.get_name(cls)
  ModelOptFp8Config.get_supported_act_dtypes(cls)
  ModelOptFp8Config.get_min_capability(cls)
  ModelOptFp8Config.get_config_filenames(cls)
  ModelOptFp8Config.from_config(cls, config, Any])
  ModelOptFp8Config.get_quant_method(layer, prefix)
  ModelOptFp8Config.get_scaled_act_names()
  ModelOptFp8LinearMethod.__init__(quant_config)
  ModelOptFp8LinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, params_dtype)
  ModelOptFp8LinearMethod.process_weights_after_loading(layer)
  ModelOptFp8LinearMethod.apply(layer, x, bias)
  ModelOptFp8KVCacheMethod.__init__(quant_config)
  ModelOptFp8MoEMethod.__init__(quant_config)
  ModelOptFp8MoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype)
  ModelOptFp8MoEMethod.process_weights_after_loading(layer)
  ModelOptFp8MoEMethod.apply(layer, x, topk_output, moe_runner_config)
  ModelOptFp4Config.__init__(is_checkpoint_nvfp4_serialized, kv_cache_quant_algo, group_size, exclude_modules)
  ModelOptFp4Config.get_name(cls)
  ModelOptFp4Config.get_supported_act_dtypes(cls)
  ModelOptFp4Config.get_min_capability(cls)
  ModelOptFp4Config.get_config_filenames(cls)
  ModelOptFp4Config.from_config(cls, config, Any])
  ModelOptFp4Config.is_layer_excluded(prefix, exclude_modules)
  ModelOptFp4Config.get_quant_method(layer, prefix)
  ModelOptFp4Config.get_scaled_act_names()
  ModelOptFp4LinearMethod.__init__(quant_config)
  ModelOptFp4LinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  ModelOptFp4LinearMethod.process_weights_after_loading(layer)
  ModelOptFp4LinearMethod.apply(layer, x, bias)
  ModelOptNvFp4FusedMoEMethod.__init__(quant_config)
  ModelOptNvFp4FusedMoEMethod.enable_flashinfer_cutlass_moe()
  ModelOptNvFp4FusedMoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size_per_partition, params_dtype)
  ModelOptNvFp4FusedMoEMethod.swizzle_blockscale(scale)
  ModelOptNvFp4FusedMoEMethod.prepare_static_weights_for_kernel(gemm1_weights, gemm2_weights, gemm1_scales_linear_fp4_bytes, gemm2_scales_linear_fp4_bytes, hidden_size, intermediate_size, num_experts)
  ModelOptNvFp4FusedMoEMethod.process_weights_after_loading(layer)
  ModelOptNvFp4FusedMoEMethod.load_up_proj_weight_first()
  ModelOptNvFp4FusedMoEMethod.apply(layer, x, topk_output, moe_runner_config)

# python/sglang/srt/layers/quantization/moe_wna16.py
get_weight_perm(num_bits)
  MoeWNA16Config.__init__(linear_quant_method, weight_bits, group_size, has_zp, lm_head_quantized, modules_to_not_convert, full_config, Any])
  MoeWNA16Config.get_name(cls)
  MoeWNA16Config.get_supported_act_dtypes(cls)
  MoeWNA16Config.get_min_capability(cls)
  MoeWNA16Config.get_config_filenames(cls)
  MoeWNA16Config.get_scaled_act_names()
  MoeWNA16Config.from_config(cls, config, Any])
  MoeWNA16Config.override_quantization_method(cls, hf_quant_cfg, user_quant)
  MoeWNA16Config.is_moe_wna16_compatible(cls, quant_config, Any])
  MoeWNA16Config.get_quant_method(layer, prefix)
is_layer_skipped_quant(prefix, modules_to_not_convert)
  MoeWNA16Method.__init__(quant_config)
  MoeWNA16Method.create_weights(layer, num_experts, hidden_size, intermediate_size_per_partition, params_dtype)
  MoeWNA16Method.apply(layer, x, topk_output, moe_runner_config)
  MoeWNA16Method.get_weight_loader(layer, weight_loader)

# python/sglang/srt/layers/quantization/mxfp4.py
  Mxfp4Config.__init__(ignored_layers, is_checkpoint_mxfp4_serialized)
  Mxfp4Config.from_config(cls, config)
  Mxfp4Config.get_min_capability(cls)
  Mxfp4Config.get_name(cls)
  Mxfp4Config.get_supported_act_dtypes(cls)
  Mxfp4Config.get_config_filenames(cls)
  Mxfp4Config.is_static_cfg()
  Mxfp4Config.get_quant_method(layer, prefix)
  Mxfp4Config.get_scaled_act_names()
  Mxfp4MoEMethod.__init__(prefix)
  Mxfp4MoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype, with_bias)
  Mxfp4MoEMethod.process_weights_after_loading(layer)
  Mxfp4MoEMethod.apply(layer, x, topk_output, moe_runner_config)
  Mxfp4DynamicQuantMoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size_per_partition, params_dtype)
  Mxfp4DynamicQuantMoEMethod.mxfp4_quantize(w)
  Mxfp4DynamicQuantMoEMethod.process_weights_after_loading(layer)
  Mxfp4DynamicQuantMoEMethod.apply(layer, x, topk_output, moe_runner_config)

# python/sglang/srt/layers/quantization/mxfp4_tensor.py
  MXFP4QuantizeUtil.quantize(cls, input, block_size)
  MXFP4QuantizeUtil.dequantize(cls, quantized_data, dtype, scale, block_sizes)

# python/sglang/srt/layers/quantization/petit.py
  PetitNvFp4Config.__init__(is_checkpoint_nvfp4_serialized, kv_cache_quant_algo, group_size, exclude_modules)
  PetitNvFp4Config.get_name(cls)
  PetitNvFp4Config.get_supported_act_dtypes(cls)
  PetitNvFp4Config.get_min_capability(cls)
  PetitNvFp4Config.get_config_filenames(cls)
  PetitNvFp4Config.from_config(cls, config, Any])
  PetitNvFp4Config.override_quantization_method(cls, hf_quant_cfg, user_quant)
  PetitNvFp4Config.is_petit_nvfp4_compatible(cls, quant_config, Any])
  PetitNvFp4Config.is_layer_excluded(prefix, exclude_modules)
  PetitNvFp4Config.get_quant_method(layer, prefix)
  PetitNvFp4Config.get_scaled_act_names()
  PetitNvFp4LinearMethod.__init__(quant_config)
  PetitNvFp4LinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  PetitNvFp4LinearMethod.process_weights_after_loading(layer)
  PetitNvFp4LinearMethod.apply(layer, x, bias)

# python/sglang/srt/layers/quantization/petit_utils.py
prepare_nvfp4_layer_for_petit(layer)
apply_petit_nvfp4_linear(input, weight, weight_scale, weight_scale_2, size_n, size_k, bias)
verify_petit_nvfp4_supported(quant_method, group_size)
prepare_nvfp4_layer_for_petit(layer)
apply_petit_nvfp4_linear(input, weight, weight_scale, weight_scale_2, size_n, size_k, bias)

# python/sglang/srt/layers/quantization/qoq.py
  QoQConfig.__init__(weight_bits, group_size)
  QoQConfig.__repr__()
  QoQConfig.get_supported_act_dtypes(cls)
  QoQConfig.get_min_capability(cls)
  QoQConfig.get_name(cls)
  QoQConfig.get_config_filenames(cls)
  QoQConfig.from_config(cls, config, Any])
  QoQConfig.get_quant_method(layer, prefix)
  QoQConfig.get_scaled_act_names()
  QoQLinearMethod.__init__(quant_config)
  QoQLinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  QoQLinearMethod.process_weights_after_loading(layer)
  QoQLinearMethod.apply(layer, x, bias)

# python/sglang/srt/layers/quantization/unquant.py
  UnquantizedEmbeddingMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  UnquantizedEmbeddingMethod.apply(layer, x, bias)
  UnquantizedEmbeddingMethod.embedding(layer, input_)
  UnquantizedLinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  UnquantizedLinearMethod.process_weights_after_loading(layer)
  UnquantizedLinearMethod.apply(layer, x, bias)
  UnquantizedFusedMoEMethod.__init__(use_triton_kernels)
  UnquantizedFusedMoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype, with_bias)
  UnquantizedFusedMoEMethod.process_weights_after_loading(layer)
  UnquantizedFusedMoEMethod.apply(layer, x, topk_output, moe_runner_config)
  UnquantizedFusedMoEMethod.forward_cuda(layer, x, topk_output, moe_runner_config)
  UnquantizedFusedMoEMethod.forward_cpu(layer, x, topk_output, moe_runner_config)
  UnquantizedFusedMoEMethod.forward_npu(layer, x, topk_output, moe_runner_config)
  UnquantizedFusedMoEMethod.forward_tpu()

# python/sglang/srt/layers/quantization/utils.py
get_scalar_types()
is_layer_skipped(prefix, ignored_layers, fused_mapping, List[str]])
per_tensor_dequantize(tensor, inv_scale, torch.Tensor])
all_close_1d(x)
convert_to_channelwise(weight_scale, logical_widths)
requantize_with_max_scale(weight, weight_scale, logical_widths)
update_tensor_inplace(old, new)
replace_parameter(mod, name, new, torch.nn.Parameter])
assert_fp8_all_close(a, b)
override_config(config, prefix)
get_dynamic_override(config, layer_name, key, default_value, bool, None])
get_linear_quant_method(config, layer, prefix, linear_method_cls)
get_pack_factor(num_bits)
permute_rows(q_w, w_ref, group_size, test_perm)
pack_cols(q_w, num_bits, size_k, size_n)
pack_rows(q_w, num_bits, size_k, size_n)
unpack_cols(packed_q_w, num_bits, size_k, size_n)
quantize_weights(w, quant_type, group_size, zero_points, ref_zero_points_after_scales)
gptq_quantize_weights(w, quant_type, group_size, act_order, test_perm)
sort_weights(q_w, g_idx)

# python/sglang/srt/layers/quantization/w4afp8.py
  W4AFp8Config.__init__(is_checkpoint_fp8_serialized, is_checkpoint_w4afp8_serialized, linear_activation_scheme, moe_activation_scheme, ignored_layers, weight_block_size, group_size)
  W4AFp8Config.get_name(cls)
  W4AFp8Config.get_supported_act_dtypes(cls)
  W4AFp8Config.get_min_capability(cls)
  W4AFp8Config.get_config_filenames(cls)
  W4AFp8Config.from_config(cls, config, Any])
  W4AFp8Config.get_quant_method(layer, prefix)
  W4AFp8Config.get_scaled_act_names()
  W4AFp8MoEMethod.__init__(quant_config)
  W4AFp8MoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype)
  W4AFp8MoEMethod.process_weights_after_loading(layer)
  W4AFp8MoEMethod.apply(layer, x, topk_output, moe_runner_config)

# python/sglang/srt/layers/quantization/w8a8_fp8.py
  W8A8Fp8Config.__init__(is_checkpoint_fp8_serialized)
  W8A8Fp8Config.get_supported_act_dtypes(cls)
  W8A8Fp8Config.get_min_capability(cls)
  W8A8Fp8Config.get_name()
  W8A8Fp8Config.get_config_filenames(cls)
  W8A8Fp8Config.from_config(cls, config, Any])
  W8A8Fp8Config.get_quant_method(layer, prefix)
  W8A8Fp8Config.get_scaled_act_names()
  W8A8Fp8LinearMethod.__init__(quantization_config)
  W8A8Fp8LinearMethod.process_weights_after_loading(layer)
  W8A8Fp8LinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  W8A8Fp8LinearMethod.apply(layer, x, bias)
  W8A8FP8MoEMethod.__init__(quant_config)
  W8A8FP8MoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype)
  W8A8FP8MoEMethod.process_weights_after_loading(layer)
  W8A8FP8MoEMethod.apply(layer, x, topk_output, moe_runner_config)

# python/sglang/srt/layers/quantization/w8a8_int8.py
npu_wrapper_rmsnorm_init(func)
npu_wrapper_rmsnorm_forward(func)
npu_fused_experts(hidden_states, w13, w13_scale, w2, w2_scale, topk_weights, topk_ids, top_k)
  W8A8Int8Config.__init__(quant_config, Any])
  W8A8Int8Config.get_supported_act_dtypes(cls)
  W8A8Int8Config.get_min_capability(cls)
  W8A8Int8Config.get_name()
  W8A8Int8Config.get_config_filenames(cls)
  W8A8Int8Config.from_config(cls, config, Any])
  W8A8Int8Config.get_quant_method(layer, prefix)
  W8A8Int8Config.is_layer_skipped(prefix, fused_mapping, List[str]])
  W8A8Int8Config.get_scaled_act_names()
  W8A8Int8LinearMethod.__init__(quantization_config)
  W8A8Int8LinearMethod.process_weights_after_loading(layer)
  W8A8Int8LinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  W8A8Int8LinearMethod.apply(layer, x, bias)
  W8A8Int8MoEMethod.__init__(quant_config)
  W8A8Int8MoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype)
  W8A8Int8MoEMethod.process_weights_after_loading(layer)
  W8A8Int8MoEMethod.apply(layer, x, topk_output, moe_runner_config)
  NPU_W8A8LinearMethodImpl.__init__()
  NPU_W8A8LinearMethodImpl.get_weight(input_size, output_size, params_dtype)
  NPU_W8A8LinearMethodImpl.get_pertensor_param(params_dtype)
  NPU_W8A8LinearMethodImpl.get_perchannel_param(output_size, params_dtype)
  NPU_W8A8LinearMethodImpl.apply(layer, x, bias)
  NPU_W8A8LinearMethodImpl.process_weights_after_loading(layer)
  NPU_W8A8LinearMethodMTImpl.__init__()
  NPU_W8A8LinearMethodMTImpl.get_weight(input_size, output_size, params_dtype)
  NPU_W8A8LinearMethodMTImpl.get_pertensor_param(params_dtype)
  NPU_W8A8LinearMethodMTImpl.get_perchannel_param(output_size, params_dtype)
  NPU_W8A8LinearMethodMTImpl.apply(layer, x, bias)
  NPU_W8A8LinearMethodMTImpl.process_weights_after_loading(layer)
  NPU_W8A8LinearMethod.__init__(quantization_config)
  NPU_W8A8LinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  NPU_W8A8LinearMethod.process_weights_after_loading(layer)
  NPU_W8A8LinearMethod.apply(layer, x, bias)
  NPU_W8A8DynamicLinearMethodImpl.__init__()
  NPU_W8A8DynamicLinearMethodImpl.get_weight(input_size, output_size, params_dtype)
  NPU_W8A8DynamicLinearMethodImpl.get_pertensor_param(params_dtype)
  NPU_W8A8DynamicLinearMethodImpl.get_perchannel_param(output_size, params_dtype)
  NPU_W8A8DynamicLinearMethodImpl.apply(layer, x, bias, tp_rank)
  NPU_W8A8DynamicLinearMethodImpl.process_weights_after_loading(layer)
  NPU_W8A8DynamicLinearMethod.__init__(quantization_config)
  NPU_W8A8DynamicLinearMethod.create_weights(layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype)
  NPU_W8A8DynamicLinearMethod.process_weights_after_loading(layer)
  NPU_W8A8DynamicLinearMethod.apply(layer, x, bias)
  NPU_W8A8MoEMethod.__init__(quantization_config)
  NPU_W8A8MoEMethod.create_weights(layer, num_experts, hidden_size, intermediate_size, params_dtype)
  NPU_W8A8MoEMethod.process_weights_after_loading(layer)
  NPU_W8A8MoEMethod.apply(layer, x, topk_output, moe_runner_config)