
# python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py
  CompressedTensorsScheme.get_min_capability(cls)
  CompressedTensorsScheme.create_weights()
  CompressedTensorsScheme.apply_weights(layer, x, bias)
  CompressedTensorsScheme.process_weights_after_loading(layer)

# python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py
apply_fp8_marlin_linear()
prepare_fp8_layer_for_marlin()
  CompressedTensorsW8A16Fp8.__init__(strategy, is_static_input_scheme)
  CompressedTensorsW8A16Fp8.get_min_capability(cls)
  CompressedTensorsW8A16Fp8.process_weights_after_loading(layer)
  CompressedTensorsW8A16Fp8.create_weights(layer, input_size, output_partition_sizes, input_size_per_partition, params_dtype, weight_loader)
  CompressedTensorsW8A16Fp8.apply_weights(layer, x, bias)

# python/sglang/srt/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
  CompressedTensorsW8A8Fp8.__init__(strategy, is_static_input_scheme)
  CompressedTensorsW8A8Fp8.get_min_capability(cls)
  CompressedTensorsW8A8Fp8.process_weights_after_loading(layer)
  CompressedTensorsW8A8Fp8.create_weights(layer, output_partition_sizes, input_size_per_partition, params_dtype, weight_loader)
  CompressedTensorsW8A8Fp8.apply_weights(layer, x, bias)

# python/sglang/srt/layers/quantization/quark/schemes/quark_scheme.py
  QuarkScheme.get_min_capability(cls)
  QuarkScheme.create_weights()
  QuarkScheme.apply_weights(layer, x, bias)
  QuarkScheme.process_weights_after_loading(layer)

# python/sglang/srt/layers/quantization/quark/schemes/quark_w4a4_mxfp4.py
  QuarkW4A4MXFP4.__init__(weight_quant_spec, Any], input_quant_spec, Any])
  QuarkW4A4MXFP4.get_min_capability(cls)
  QuarkW4A4MXFP4.process_weights_after_loading(layer)
  QuarkW4A4MXFP4.create_weights(layer, output_partition_sizes, input_size_per_partition, params_dtype, weight_loader)
  QuarkW4A4MXFP4.apply_weights(layer, x, bias)