
# python/sglang/srt/entrypoints/EngineBase.py
  EngineBase.generate(prompt, str]], sampling_params, Dict]], input_ids, List[int]]], image_data, str]], return_logprob, bool]], logprob_start_len, int]], top_logprobs_num, int]], token_ids_logprob, List[int]]], lora_path, Optional[str]]], custom_logit_processor, str]], return_hidden_states, stream, bootstrap_host, str]], bootstrap_port, int]], bootstrap_room, int]], data_parallel_rank)
  EngineBase.flush_cache()
  EngineBase.update_weights_from_tensor(named_tensors, torch.Tensor]], load_format, flush_cache)
  EngineBase.load_lora_adapter(lora_name, lora_path)
  EngineBase.unload_lora_adapter(lora_name)
  EngineBase.release_memory_occupation()
  EngineBase.resume_memory_occupation()
  EngineBase.shutdown()

# python/sglang/srt/entrypoints/context.py
  ConversationContext.append_output(output)
  ConversationContext.call_tool()
  ConversationContext.need_builtin_tool_call()
  ConversationContext.render_for_completion()
  SimpleContext.__init__()
  SimpleContext.append_output(output)
  SimpleContext.need_builtin_tool_call()
  SimpleContext.call_tool()
  SimpleContext.render_for_completion()
  HarmonyContext.__init__(messages, tool_sessions, Union['ClientSession', Tool]])
  HarmonyContext.append_output(output)
  HarmonyContext.messages()
  HarmonyContext.need_builtin_tool_call()
  HarmonyContext.call_tool()
  HarmonyContext.render_for_completion()
  HarmonyContext.call_search_tool(tool_session, Tool], last_msg)
  HarmonyContext.call_python_tool(tool_session, Tool], last_msg)
  StreamingHarmonyContext.__init__()
  StreamingHarmonyContext.messages()
  StreamingHarmonyContext.append_output(output)
  StreamingHarmonyContext.is_expecting_start()
  StreamingHarmonyContext.is_assistant_action_turn()
  StreamingHarmonyContext.render_for_completion()

# python/sglang/srt/entrypoints/engine.py
  Engine.__init__()
  Engine.generate(prompt, str]], sampling_params, Dict]], input_ids, List[int]]], image_data, audio_data, video_data, return_logprob, bool]], logprob_start_len, int]], top_logprobs_num, int]], token_ids_logprob, List[int]]], lora_path, custom_logit_processor, str]], return_hidden_states, stream, bootstrap_host, str]], bootstrap_port, int]], bootstrap_room, int]], data_parallel_rank)
  Engine.async_generate(prompt, str]], sampling_params, Dict]], input_ids, List[int]]], image_data, audio_data, video_data, return_logprob, bool]], logprob_start_len, int]], top_logprobs_num, int]], token_ids_logprob, List[int]]], lora_path, custom_logit_processor, str]], return_hidden_states, stream, bootstrap_host, str]], bootstrap_port, int]], bootstrap_room, int]], data_parallel_rank)
  Engine.encode(prompt, List[str], List[Dict], List[List[Dict]]], image_data, audio_data, video_data)
  Engine.async_encode(prompt, List[str], List[Dict], List[List[Dict]]], image_data, audio_data, video_data)
  Engine.rerank(prompt)
  Engine.shutdown()
  Engine.__enter__()
  Engine.__exit__(exc_type, exc_value, traceback)
  Engine.flush_cache()
  Engine.start_profile()
  Engine.stop_profile()
  Engine.start_expert_distribution_record()
  Engine.stop_expert_distribution_record()
  Engine.dump_expert_distribution_record()
  Engine.get_server_info()
  Engine.init_weights_update_group(master_address, master_port, rank_offset, world_size, group_name, backend)
  Engine.update_weights_from_distributed(names, dtypes, shapes, group_name, flush_cache)
  Engine.update_weights_from_tensor(named_tensors, torch.Tensor]], load_format, flush_cache)
  Engine.update_weights_from_disk(model_path, load_format)
  Engine.get_weights_by_name(name, truncate_size)
  Engine.load_lora_adapter(lora_name, lora_path, pinned)
  Engine.unload_lora_adapter(lora_name)
  Engine.release_memory_occupation(tags)
  Engine.resume_memory_occupation(tags)
  Engine.freeze_gc()
  Engine.collective_rpc(method)
  Engine.save_remote_model()
  Engine.save_sharded_model()
  Engine.score(query, List[int]]], items, List[str], List[List[int]]]], label_token_ids, apply_softmax, item_first)
  Engine.async_score(query, List[int]]], items, List[str], List[List[int]]]], label_token_ids, apply_softmax, item_first)

# python/sglang/srt/entrypoints/harmony_utils.py
get_encoding()
get_system_message(model_identity, reasoning_effort, 'medium', 'low']], start_date, browser_description, python_description)
get_developer_message(instructions, tools)
get_user_message(content)
parse_response_input(response_msg, prev_responses, ResponseReasoningItem]])
parse_response_output(output)
parse_chat_input(chat_msg)
render_for_completion(messages)
get_stop_tokens_for_assistant_actions()
get_streamable_parser_for_assistant()
parse_output_message(message)
parse_remaining_state(parser)
parse_output_into_messages(token_ids)

# python/sglang/srt/entrypoints/http_server.py
set_global_state(global_state)
lifespan(fast_api_app)
validation_exception_handler(request, exc)
validation_exception_handler(request, exc)
validate_json_request(raw_request)
health_generate(request)
get_model_info()
get_weight_version()
get_server_info()
get_load()
set_internal_state(obj, request)
generate_request(obj, request)
generate_from_file_request(file, request)
encode_request(obj, request)
classify_request(obj, request)
flush_cache()
start_profile_async(obj)
stop_profile_async()
freeze_gc_async()
start_expert_distribution_record_async()
stop_expert_distribution_record_async()
dump_expert_distribution_record_async()
update_weights_from_disk(obj, request)
init_weights_update_group(obj, request)
update_weights_from_tensor(obj, request)
update_weights_from_distributed(obj, request)
update_weight_version(obj, request)
get_weights_by_name(obj, request)
release_memory_occupation(obj, request)
resume_memory_occupation(obj, request)
slow_down(obj, request)
load_lora_adapter(obj, request)
unload_lora_adapter(obj, request)
open_session(obj, request)
close_session(obj, request)
configure_logging(obj, request)
abort_request(obj, request)
parse_function_call_request(obj, request)
separate_reasoning_request(obj, request)
pause_generation(request)
continue_generation(request)
openai_v1_completions(request, raw_request)
openai_v1_chat_completions(request, raw_request)
openai_v1_embeddings(request, raw_request)
available_models()
retrieve_model(model)
v1_score_request(request, raw_request)
v1_responses_request(request, raw_request)
v1_retrieve_responses(response_id, raw_request)
v1_cancel_responses(response_id, raw_request)
v1_rerank_request(request, raw_request)
sagemaker_health()
sagemaker_chat_completions(request, raw_request)
vertex_generate(vertex_req, raw_request)
launch_server(server_args, pipe_finish_writer, launch_callback, None]])

# python/sglang/srt/entrypoints/http_server_engine.py
launch_server_process(server_args)
  HttpServerEngineAdapter.__init__()
  HttpServerEngineAdapter.update_weights_from_tensor(named_tensors, torch.Tensor]], load_format, flush_cache)
  HttpServerEngineAdapter.shutdown()
  HttpServerEngineAdapter.generate(prompt, sampling_params, input_ids, image_data, return_logprob, logprob_start_len, top_logprobs_num, token_ids_logprob, lora_path, custom_logit_processor)
  HttpServerEngineAdapter.release_memory_occupation()
  HttpServerEngineAdapter.resume_memory_occupation()
  HttpServerEngineAdapter.flush_cache()

# python/sglang/srt/entrypoints/tool.py
  Tool.get_result(context)
  HarmonyBrowserTool.__init__()
  HarmonyBrowserTool.get_result(context)
  HarmonyBrowserTool.tool_config()
  HarmonyPythonTool.__init__()
  HarmonyPythonTool.get_result(context)
  HarmonyPythonTool.tool_config()