================================================================================
FUNCTION INDEX: model_loader module
================================================================================
Total Functions: 71
Documented: 23


============================================================
FILE: python/sglang/srt/model_loader/__init__.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  15: def get_model()
         ‚Üí nn.Module


============================================================
FILE: python/sglang/srt/model_loader/loader.py
Functions: 30
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  77: def device_loading_context(module: torch.nn.Module, target_device: torch.device)
         @contextmanager

  L1514: def load_model_with_cpu_quantization(self)
         ‚Üí nn.Module

  L1546: def get_model_loader(load_config: LoadConfig)
         ‚Üí BaseModelLoader
         üìù Get a model loader based on the load format.


CLASS: BaseModelLoader
----------------------------------------
  L 214: __init__(self, load_config: LoadConfig)

  L 218: download_model(self, model_config: ModelConfig)
         ‚Üí None
         üìù Download a model so that it can be immediately loaded.

  L 223: load_model(self)
         ‚Üí nn.Module
         üìù Load a model with the given configurations.


CLASS: BitsAndBytesModelLoader
----------------------------------------
  L 811: __init__(self, load_config: LoadConfig)

  L1243: download_model(self, model_config: ModelConfig)
         ‚Üí None

  L1246: load_model(self)
         ‚Üí nn.Module


CLASS: DefaultModelLoader
----------------------------------------
  L 264: __init__(self, load_config: LoadConfig)

  L 449: download_model(self, model_config: ModelConfig)
         ‚Üí None

  L 454: load_model(self)
         ‚Üí nn.Module

  L 475: load_weights_and_postprocess(model, weights, target_device)


CLASS: DummyModelLoader
----------------------------------------
  L 565: __init__(self, load_config: LoadConfig)

  L 573: download_model(self, model_config: ModelConfig)
         ‚Üí None

  L 576: load_model(self)
         ‚Üí nn.Module


CLASS: GGUFModelLoader
----------------------------------------
  L1271: __init__(self, load_config: LoadConfig)

  L1335: download_model(self, model_config: ModelConfig)
         ‚Üí None

  L1338: load_model(self)
         ‚Üí nn.Module


CLASS: LayeredModelLoader
----------------------------------------
  L 494: __init__(self, load_config: LoadConfig)

  L 499: load_model(self)
         ‚Üí nn.Module


CLASS: RemoteModelLoader
----------------------------------------
  L1372: __init__(self, load_config: LoadConfig)

  L1394: download_model(self, model_config: ModelConfig)
         ‚Üí None

  L1398: save_model(model: torch.nn.Module, model_path: str, url: str)
         ‚Üí None

  L1475: load_model(self)
         ‚Üí nn.Module


CLASS: ShardedStateLoader
----------------------------------------
  L 619: __init__(self, load_config: LoadConfig)

  L 683: download_model(self, model_config: ModelConfig)
         ‚Üí None

  L 686: load_model(self)
         ‚Üí nn.Module

  L 750: save_model(model: torch.nn.Module, path: str, pattern: Optional[str], max_size: Optional[int])
         ‚Üí None


CLASS: Source
----------------------------------------
  L 256: init_new(cls, model_config: ModelConfig, model)


============================================================
FILE: python/sglang/srt/model_loader/utils.py
Functions: 5
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  19: def set_default_torch_dtype(dtype: torch.dtype)
         üìù Sets the default torch dtype to the given dtype.
         @contextlib.contextmanager

  L  27: def resolve_transformers_arch(model_config: ModelConfig,
        architectures: list[str])

  L  82: def get_model_architecture(model_config: ModelConfig)
         ‚Üí Tuple[Type[nn.Module], str]

  L 106: def get_architecture_class_name(model_config: ModelConfig)
         ‚Üí str

  L 110: def post_load_weights(model: nn.Module, model_config: ModelConfig)


============================================================
FILE: python/sglang/srt/model_loader/weight_utils.py
Functions: 35
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  51: def enable_hf_transfer()
         üìù automatically activates hf_transfer

  L  71: def get_lock(model_name_or_path: str, cache_dir: Optional[str])

  L  94: def convert_bin_to_safetensor_file(pt_filename: str, sf_filename: str)
         ‚Üí None

  L 134: def get_quant_config(model_config: ModelConfig,
        load_config: LoadConfig,
        packed_modules_mapping: Dict[str,
        List[str]])
         ‚Üí QuantizationConfig

  L 238: def download_weights_from_hf(model_name_or_path: str,
        cache_dir: Optional[str],
        allow_patterns: List[str],
        revision: Optional[str],
        ignore_patterns: Optional[Union[str,
        List[str]]])
         ‚Üí str
         üìù Download model weights from Hugging Face Hub.
            Args:
            model_name_or_path (str): The model name or path.
            cache_dir (Optional[str]): The cache directory to store the model
            weights. If None, will use HF defaults.
            allow_patterns (List[str]): The allowed patterns for the
            weight files. Files matched by any of the patterns will be
            downloaded.
            revision (Optional[str]): The revision of the model.
            ignore_patterns (Optional[Union[str, List[str]]]): The patterns to
            filter out the weight files. Files matched by any of the patterns
            will be ignored.
            Returns:
            str: The path to the downloaded model weights.

  L 290: def download_safetensors_index_file_from_hf(model_name_or_path: str,
        index_file: str,
        cache_dir: Optional[str],
        revision: Optional[str])
         ‚Üí None
         üìù Download hf safetensors index file from Hugging Face Hub.
            Args:
            model_name_or_path (str): The model name or path.
            cache_dir (Optional[str]): The cache directory to store the model
            weights. If None, will use HF defaults.
            revision (Optional[str]): The revision of the model.

  L 329: def filter_duplicate_safetensors_files(hf_weights_files: List[str],
        hf_folder: str,
        index_file: str)
         ‚Üí List[str]

  L 350: def filter_files_not_needed_for_inference(hf_weights_files: List[str])
         ‚Üí List[str]
         üìù Exclude files that are not needed for inference.
            See https://github.com/huggingface/transformers/blob/v4.34.0/src/transformers/trainer.py#L227-L233

  L 376: def np_cache_weights_iterator(model_name_or_path: str,
        cache_dir: Optional[str],
        hf_folder: str,
        hf_weights_files: List[str])
         ‚Üí Generator[Tuple[str, torch.Tensor], None, None]
         üìù Iterate over the weights in the model np files.
            Will dump the model weights to numpy files if they are not already dumped.

  L 424: def decrypt(fn, key)

  L 428: def safetensors_encrypted_weights_iterator(hf_weights_files: List[str],
        is_all_weights_sharded: bool,
        decryption_key: Optional[str])

  L 436: def safetensors_weights_iterator(hf_weights_files: List[str],
        is_all_weights_sharded: bool,
        decryption_key: Optional[str],
        disable_mmap: bool)
         ‚Üí Generator[Tuple[str, torch.Tensor], None, None]
         üìù Iterate over the weights in the model safetensor files.
            If is_all_weights_sharded is True, it uses more optimize read by reading an
            entire file instead of reading each tensor one by one.

  L 473: def multi_thread_safetensors_weights_iterator(hf_weights_files: List[str],
        is_all_weights_sharded: bool,
        decryption_key: Optional[str],
        max_workers: int,
        disable_mmap: bool)
         ‚Üí Generator[Tuple[str, torch.Tensor], None, None]
         üìù Multi-Thread iterate over the weights in the model safetensor files.
            If is_all_weights_sharded is True, it uses more optimize read by reading an
            entire file instead of reading each tensor one by one.

  L 528: def pt_weights_iterator(hf_weights_files: List[str])
         ‚Üí Generator[Tuple[str, torch.Tensor], None, None]
         üìù Iterate over the weights in the model bin/pt files.

  L 546: def multi_thread_pt_weights_iterator(hf_weights_files: List[str],
        max_workers: int)
         ‚Üí Generator[Tuple[str, torch.Tensor], None, None]
         üìù Multi-Thread iterate over the weights in the model bin/pt files.

  L 579: def get_gguf_extra_tensor_names(gguf_file: str,
        gguf_to_hf_name_map: Dict[str,
        str])
         ‚Üí List[str]

  L 591: def gguf_quant_weights_iterator(gguf_file: str,
        gguf_to_hf_name_map: Dict[str,
        str])
         ‚Üí Generator[Tuple[str, torch.Tensor], None, None]
         üìù Iterate over the quant weights in the model gguf files and convert
            them to torch tensors

  L 625: def convert_pyslice_to_tensor(x: Any)
         ‚Üí torch.Tensor
         üìù convert PySafeSlice object from safetensors to torch.Tensor
            PySafeSlice object supports indexing, which is done before loading the
            actual tensor and can reduce the amount of memory being read into the
            memory. However, it does not support more advanced functionalities
            like `.view()` or `.t()`. Therefore, if we need to modify the loaded
            tensor with these more complicated operators, we need to convert to
            tensor first.

  L 640: def default_weight_loader(param: torch.Tensor, loaded_weight: torch.Tensor)
         ‚Üí None
         üìù Default weight loader.

  L 661: def row_parallel_weight_loader(param: torch.Tensor, loaded_weight: torch.Tensor)
         ‚Üí None
         üìù Load weights that are row-parallelized.

  L 679: def sharded_weight_loader(shard_axis: int)
         ‚Üí LoaderFunction
         üìù Create a weight loader that shards the weights along the given axis

  L 694: def composed_weight_loader(loader: LoaderFunction,
        fn: Callable[[torch.Tensor],
        torch.Tensor])
         ‚Üí LoaderFunction
         üìù Create a weight loader that post-processes the weights after loading

  L 707: def runai_safetensors_weights_iterator(hf_weights_files: List[str])
         ‚Üí Generator[Tuple[str, torch.Tensor], None, None]
         üìù Iterate over the weights in the model safetensor files.

  L 728: def set_runai_streamer_env(load_config: LoadConfig)

  L 752: def initialize_dummy_weights(model: torch.nn.Module,
        low: float,
        high: float,
        seed: int)
         ‚Üí None
         üìù Initialize model weights with random values.
            The model weights must be randomly initialized for accurate performance
            measurements. Additionally, the model weights should not cause NaNs in the
            forward pass. We empirically found that initializing the weights with
            values between -1e-3 and 1e-3 works well for most models.
            We use per-parameter random seed, so that dummy weights are consistent,
            even if the model is partitioned across multiple devices. When the seed
            is fixed, the random values generated by this function only depends on
            the parameter's number of elements and its data type.

  L 784: def maybe_remap_kv_scale_name(name: str, params_dict: dict)
         ‚Üí Optional[str]
         üìù Remap the name of FP8 k/v_scale parameters.
            This function handles the remapping of FP8 k/v_scale parameter names.
            It detects if the given name ends with a suffix and attempts to remap
            it to the expected name format in the model. If the remapped name is not
            found in the params_dict, a warning is printed and None is returned.
            Args:
            name (str): The original loaded checkpoint parameter name.
            params_dict (dict): Dictionary containing the model's named parameters.
            Returns:
            str: The remapped parameter name if successful, or the original name
            if no remapping is needed.
            None: If the remapped name is not found in params_dict.

  L 935: def kv_cache_scales_loader(filename: str,
        tp_rank: int,
        tp_size: int,
        num_hidden_layers: int,
        model_type: Optional[str])
         ‚Üí Iterable[Tuple[int, float]]
         üìù A simple utility to read in KV cache scaling factors that have been
            previously serialized to disk. Used by the model to populate the appropriate
            KV cache scaling factors. The serialization should represent a dictionary
            whose keys are the TP ranks and values are another dictionary mapping layers
            to their KV cache scaling factors.

  L 978: def get_actual_shard_size(shard_size, weight_start, weight_end)

  L 985: def reset_param_data_if_needed(param_data, dim, start, length)

  L 995: def narrow_padded_param_and_loaded_weight(param_data,
        loaded_weight,
        param_data_start,
        weight_start,
        dim,
        shard_size,
        narrow_weight)


CLASS: DisabledTqdm
----------------------------------------
  L  67: __init__(self)


CLASS: KVCacheQuantSchema
----------------------------------------
  L 870: check_is_fp8(self)
         ‚Üí 'KVCacheQuantSchema'

  L 878: check_tp_ranks(self, info: ValidationInfo)
         ‚Üí 'KVCacheQuantSchema'

  L 900: check_current_rank(self, info: ValidationInfo)
         ‚Üí 'KVCacheQuantSchema'


CLASS: QuantParamSchema
----------------------------------------
  L 922: check_model_type(self, info: ValidationInfo)
         ‚Üí 'QuantParamSchema'
