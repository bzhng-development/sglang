================================================================================
FUNCTION INDEX: model_loader module
================================================================================
Total Functions: 70
Documented: 23


============================================================
FILE: python/sglang/srt/model_loader/__init__.py
Functions: 1
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  15: def get_model()
         → nn.Module


============================================================
FILE: python/sglang/srt/model_loader/loader.py
Functions: 30
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  76: def device_loading_context(module: torch.nn.Module, target_device: torch.device)
         @contextmanager

  L1523: def load_model_with_cpu_quantization(self)
         → nn.Module

  L1555: def get_model_loader(load_config: LoadConfig)
         → BaseModelLoader
         📝 Get a model loader based on the load format.


CLASS: BaseModelLoader
----------------------------------------
  L 213: __init__(self, load_config: LoadConfig)

  L 217: download_model(self, model_config: ModelConfig)
         → None
         📝 Download a model so that it can be immediately loaded.

  L 222: load_model(self)
         → nn.Module
         📝 Load a model with the given configurations.


CLASS: BitsAndBytesModelLoader
----------------------------------------
  L 818: __init__(self, load_config: LoadConfig)

  L1250: download_model(self, model_config: ModelConfig)
         → None

  L1253: load_model(self)
         → nn.Module


CLASS: DefaultModelLoader
----------------------------------------
  L 263: __init__(self, load_config: LoadConfig)

  L 448: download_model(self, model_config: ModelConfig)
         → None

  L 453: load_model(self)
         → nn.Module

  L 474: load_weights_and_postprocess(model, weights, target_device)


CLASS: DummyModelLoader
----------------------------------------
  L 564: __init__(self, load_config: LoadConfig)

  L 572: download_model(self, model_config: ModelConfig)
         → None

  L 575: load_model(self)
         → nn.Module


CLASS: GGUFModelLoader
----------------------------------------
  L1278: __init__(self, load_config: LoadConfig)

  L1342: download_model(self, model_config: ModelConfig)
         → None

  L1345: load_model(self)
         → nn.Module


CLASS: LayeredModelLoader
----------------------------------------
  L 493: __init__(self, load_config: LoadConfig)

  L 498: load_model(self)
         → nn.Module


CLASS: RemoteModelLoader
----------------------------------------
  L1379: __init__(self, load_config: LoadConfig)

  L1401: download_model(self, model_config: ModelConfig)
         → None

  L1405: save_model(model: torch.nn.Module, model_path: str, url: str)
         → None

  L1482: load_model(self)
         → nn.Module


CLASS: ShardedStateLoader
----------------------------------------
  L 629: __init__(self, load_config: LoadConfig)

  L 693: download_model(self, model_config: ModelConfig)
         → None

  L 696: load_model(self)
         → nn.Module

  L 757: save_model(model: torch.nn.Module, path: str, pattern: Optional[str], max_size: Optional[int])
         → None


CLASS: Source
----------------------------------------
  L 255: init_new(cls, model_config: ModelConfig, model)


============================================================
FILE: python/sglang/srt/model_loader/utils.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  19: def set_default_torch_dtype(dtype: torch.dtype)
         📝 Sets the default torch dtype to the given dtype.
         @contextlib.contextmanager

  L  27: def resolve_transformers_arch(model_config: ModelConfig,
        architectures: list[str])

  L  82: def get_model_architecture(model_config: ModelConfig)
         → Tuple[Type[nn.Module], str]

  L 106: def get_architecture_class_name(model_config: ModelConfig)
         → str


============================================================
FILE: python/sglang/srt/model_loader/weight_utils.py
Functions: 35
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  51: def enable_hf_transfer()
         📝 automatically activates hf_transfer

  L  71: def get_lock(model_name_or_path: str, cache_dir: Optional[str])

  L  94: def convert_bin_to_safetensor_file(pt_filename: str, sf_filename: str)
         → None

  L 134: def get_quant_config(model_config: ModelConfig,
        load_config: LoadConfig,
        packed_modules_mapping: Dict[str,
        List[str]])
         → QuantizationConfig

  L 238: def download_weights_from_hf(model_name_or_path: str,
        cache_dir: Optional[str],
        allow_patterns: List[str],
        revision: Optional[str],
        ignore_patterns: Optional[Union[str,
        List[str]]])
         → str
         📝 Download model weights from Hugging Face Hub.

  L 290: def download_safetensors_index_file_from_hf(model_name_or_path: str,
        index_file: str,
        cache_dir: Optional[str],
        revision: Optional[str])
         → None
         📝 Download hf safetensors index file from Hugging Face Hub.

  L 329: def filter_duplicate_safetensors_files(hf_weights_files: List[str],
        hf_folder: str,
        index_file: str)
         → List[str]

  L 350: def filter_files_not_needed_for_inference(hf_weights_files: List[str])
         → List[str]
         📝 Exclude files that are not needed for inference.

  L 376: def np_cache_weights_iterator(model_name_or_path: str,
        cache_dir: Optional[str],
        hf_folder: str,
        hf_weights_files: List[str])
         → Generator[Tuple[str, torch.Tensor], None, None]
         📝 Iterate over the weights in the model np files.

  L 424: def decrypt(fn, key)

  L 428: def safetensors_encrypted_weights_iterator(hf_weights_files: List[str],
        is_all_weights_sharded: bool,
        decryption_key: Optional[str])

  L 436: def safetensors_weights_iterator(hf_weights_files: List[str],
        is_all_weights_sharded: bool,
        decryption_key: Optional[str],
        disable_mmap: bool)
         → Generator[Tuple[str, torch.Tensor], None, None]
         📝 Iterate over the weights in the model safetensor files.

  L 473: def multi_thread_safetensors_weights_iterator(hf_weights_files: List[str],
        is_all_weights_sharded: bool,
        decryption_key: Optional[str],
        max_workers: int,
        disable_mmap: bool)
         → Generator[Tuple[str, torch.Tensor], None, None]
         📝 Multi-Thread iterate over the weights in the model safetensor files.

  L 528: def pt_weights_iterator(hf_weights_files: List[str])
         → Generator[Tuple[str, torch.Tensor], None, None]
         📝 Iterate over the weights in the model bin/pt files.

  L 546: def multi_thread_pt_weights_iterator(hf_weights_files: List[str],
        max_workers: int)
         → Generator[Tuple[str, torch.Tensor], None, None]
         📝 Multi-Thread iterate over the weights in the model bin/pt files.

  L 579: def get_gguf_extra_tensor_names(gguf_file: str,
        gguf_to_hf_name_map: Dict[str,
        str])
         → List[str]

  L 591: def gguf_quant_weights_iterator(gguf_file: str,
        gguf_to_hf_name_map: Dict[str,
        str])
         → Generator[Tuple[str, torch.Tensor], None, None]
         📝 Iterate over the quant weights in the model gguf files and convert

  L 625: def convert_pyslice_to_tensor(x: Any)
         → torch.Tensor
         📝 convert PySafeSlice object from safetensors to torch.Tensor

  L 640: def default_weight_loader(param: torch.Tensor, loaded_weight: torch.Tensor)
         → None
         📝 Default weight loader.

  L 661: def row_parallel_weight_loader(param: torch.Tensor, loaded_weight: torch.Tensor)
         → None
         📝 Load weights that are row-parallelized.

  L 679: def sharded_weight_loader(shard_axis: int)
         → LoaderFunction
         📝 Create a weight loader that shards the weights along the given axis

  L 694: def composed_weight_loader(loader: LoaderFunction,
        fn: Callable[[torch.Tensor],
        torch.Tensor])
         → LoaderFunction
         📝 Create a weight loader that post-processes the weights after loading

  L 707: def runai_safetensors_weights_iterator(hf_weights_files: List[str])
         → Generator[Tuple[str, torch.Tensor], None, None]
         📝 Iterate over the weights in the model safetensor files.

  L 728: def set_runai_streamer_env(load_config: LoadConfig)

  L 752: def initialize_dummy_weights(model: torch.nn.Module,
        low: float,
        high: float,
        seed: int)
         → None
         📝 Initialize model weights with random values.

  L 784: def maybe_remap_kv_scale_name(name: str, params_dict: dict)
         → Optional[str]
         📝 Remap the name of FP8 k/v_scale parameters.

  L 935: def kv_cache_scales_loader(filename: str,
        tp_rank: int,
        tp_size: int,
        num_hidden_layers: int,
        model_type: Optional[str])
         → Iterable[Tuple[int, float]]
         📝 A simple utility to read in KV cache scaling factors that have been

  L 978: def get_actual_shard_size(shard_size, weight_start, weight_end)

  L 985: def reset_param_data_if_needed(param_data, dim, start, length)

  L 995: def narrow_padded_param_and_loaded_weight(param_data,
        loaded_weight,
        param_data_start,
        weight_start,
        dim,
        shard_size,
        narrow_weight)


CLASS: DisabledTqdm
----------------------------------------
  L  67: __init__(self)


CLASS: KVCacheQuantSchema
----------------------------------------
  L 870: check_is_fp8(self)
         → 'KVCacheQuantSchema'

  L 878: check_tp_ranks(self, info: ValidationInfo)
         → 'KVCacheQuantSchema'

  L 900: check_current_rank(self, info: ValidationInfo)
         → 'KVCacheQuantSchema'


CLASS: QuantParamSchema
----------------------------------------
  L 922: check_model_type(self, info: ValidationInfo)
         → 'QuantParamSchema'
