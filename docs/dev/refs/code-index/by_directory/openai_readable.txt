================================================================================
FUNCTION INDEX: openai module
================================================================================
Total Functions: 37
Documented: 7


============================================================
FILE: python/sglang/srt/entrypoints/openai/protocol.py
Functions: 4
============================================================


CLASS: ChatCompletionRequest
----------------------------------------
  L 455: set_tool_choice_default(cls, values)


CLASS: CompletionRequest
----------------------------------------
  L 234: validate_max_tokens_positive(cls, v)


CLASS: ResponsesRequest
----------------------------------------
  L 730: to_sampling_params(self, default_max_tokens: int, default_params: Optional[Dict])
         ‚Üí Dict[str, Any]
         üìù Convert to sampling parameters for generation.


CLASS: ResponsesResponse
----------------------------------------
  L 801: from_request(cls, request: ResponsesRequest, sampling_params: Any, model_name: str, created_time: int, output: List[Union[ResponseOutputItem, ResponseReasoningItem, ResponseFunctionToolCall]], status: str, usage: Optional[UsageInfo])
         ‚Üí 'ResponsesResponse'
         üìù Create a response from a request.


============================================================
FILE: python/sglang/srt/entrypoints/openai/serving_base.py
Functions: 4
============================================================


CLASS: OpenAIServingBase
----------------------------------------
  L  21: __init__(self, tokenizer_manager: TokenizerManager)

  L  24: handle_request(self, request: OpenAIServingRequest, raw_request: Request)
         ‚Üí Union[Any, StreamingResponse, ErrorResponse]
         üìù Handle the specific request type with common pattern

  L 120: create_error_response(self, message: str, err_type: str, status_code: int, param: Optional[str])
         ‚Üí ORJSONResponse
         üìù Create an error response

  L 138: create_streaming_error_response(self, message: str, err_type: str, status_code: int)
         ‚Üí str
         üìù Create a streaming error response


============================================================
FILE: python/sglang/srt/entrypoints/openai/serving_chat.py
Functions: 1
============================================================


CLASS: OpenAIServingChat
----------------------------------------
  L  49: __init__(self, tokenizer_manager: TokenizerManager, template_manager: TemplateManager)


============================================================
FILE: python/sglang/srt/entrypoints/openai/serving_completions.py
Functions: 1
============================================================


CLASS: OpenAIServingCompletion
----------------------------------------
  L  34: __init__(self, tokenizer_manager: TokenizerManager, template_manager: TemplateManager)


============================================================
FILE: python/sglang/srt/entrypoints/openai/serving_embedding.py
Functions: 1
============================================================


CLASS: OpenAIServingEmbedding
----------------------------------------
  L  24: __init__(self, tokenizer_manager: TokenizerManager, template_manager: TemplateManager)


============================================================
FILE: python/sglang/srt/entrypoints/openai/serving_responses.py
Functions: 6
============================================================


CLASS: OpenAIServingResponses
----------------------------------------
  L  68: __init__(self, tokenizer_manager: TokenizerManager, template_manager: TemplateManager)
         ‚Üí None

  L 126: create_responses(self, request: ResponsesRequest, raw_request: Optional[Request])
         ‚Üí Union[AsyncGenerator[str, None], ResponsesResponse, ORJSONResponse]

  L 389: responses_full_generator(self, request: ResponsesRequest, sampling_params: Any, result_generator: AsyncIterator[Any], context: ConversationContext, model_name: str, tokenizer: Any, request_metadata: RequestResponseMetadata, created_time: Optional[int])
         ‚Üí Union[ResponsesResponse, ORJSONResponse]

  L 708: retrieve_responses(self, response_id: str)
         ‚Üí Union[ResponsesResponse, ORJSONResponse]

  L 722: cancel_responses(self, response_id: str)
         ‚Üí Union[ResponsesResponse, ORJSONResponse]

  L 771: responses_stream_generator(self, request: ResponsesRequest, sampling_params: Any, result_generator: AsyncIterator[StreamingHarmonyContext], context: StreamingHarmonyContext, model_name: str, tokenizer: Any, request_metadata: RequestResponseMetadata, created_time: Optional[int])
         ‚Üí AsyncGenerator[str, None]


============================================================
FILE: python/sglang/srt/entrypoints/openai/tool_server.py
Functions: 15
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  20: async def list_server_and_tools(server_url: str)

  L  30: def trim_schema(schema: dict)
         ‚Üí dict

  L  55: def post_process_tools_description(list_tools_result: 'ListToolsResult')
         ‚Üí 'ListToolsResult'


CLASS: DemoToolServer
----------------------------------------
  L 145: __init__(self)

  L 160: has_tool(self, tool_name: str)

  L 163: get_tool_description(self, tool_name: str)

  L 174: get_tool_session(self, tool_name: str)


CLASS: MCPToolServer
----------------------------------------
  L  89: __init__(self)

  L  92: add_tool_server(self, server_url: str)

  L 124: has_tool(self, tool_name: str)

  L 127: get_tool_description(self, tool_name: str)

  L 131: get_tool_session(self, tool_name: str)


CLASS: ToolServer
----------------------------------------
  L  76: has_tool(self, tool_name: str)

  L  80: get_tool_description(self, tool_name: str)

  L  84: get_tool_session(self, tool_name: str)
         ‚Üí AbstractAsyncContextManager[Any]


============================================================
FILE: python/sglang/srt/entrypoints/openai/usage_processor.py
Functions: 3
============================================================


CLASS: UsageProcessor
----------------------------------------
  L  18: calculate_response_usage(responses: List[Dict[str, Any]], n_choices: int, enable_cache_report: bool)
         ‚Üí UsageInfo

  L  44: calculate_streaming_usage(prompt_tokens: Mapping[int, int], completion_tokens: Mapping[int, int], cached_tokens: Mapping[int, int], n_choices: int, enable_cache_report: bool)
         ‚Üí UsageInfo

  L  70: calculate_token_usage(prompt_tokens: int, completion_tokens: int, cached_tokens: Optional[Dict[str, int]])
         ‚Üí UsageInfo
         üìù Calculate token usage information


============================================================
FILE: python/sglang/srt/entrypoints/openai/utils.py
Functions: 2
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  13: def to_openai_style_logprobs(input_token_logprobs,
        output_token_logprobs,
        input_top_logprobs,
        output_top_logprobs)

  L  50: def process_hidden_states_from_ret(ret_item: Dict[str,
        Any],
        request: Union[ChatCompletionRequest,
        CompletionRequest])
         ‚Üí Optional[List]
         üìù Process hidden states from a ret item in non-streaming response.
