
# python/sglang/srt/layers/moe/fused_moe_triton/__init__.py
override_config(config)
get_config()

# python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py
write_zeros_to_output(c_ptr, stride_cm, stride_cn, pid_n, N, offs_token, token_mask, BLOCK_SIZE_M, BLOCK_SIZE_N, compute_type)
fused_moe_kernel_gptq_awq(a_ptr, b_ptr, c_ptr, b_scale_ptr, b_zp_ptr, topk_weights_ptr, sorted_token_ids_ptr, expert_ids_ptr, num_tokens_post_padded_ptr, N, K, EM, num_valid_tokens, stride_am, stride_ak, stride_be, stride_bk, stride_bn, stride_cm, stride_cn, stride_bse, stride_bsk, stride_bsn, stride_bze, stride_bzk, stride_bzn, group_size, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, GROUP_SIZE_M, MUL_ROUTED_WEIGHT, top_k, compute_type, has_zp, use_int4_w4a16, use_int8_w8a16, even_Ks)
fused_moe_kernel(a_ptr, b_ptr, bias_ptr, c_ptr, a_scale_ptr, b_scale_ptr, topk_weights_ptr, sorted_token_ids_ptr, expert_ids_ptr, num_tokens_post_padded_ptr, N, K, EM, num_valid_tokens, stride_am, stride_ak, stride_be, stride_bk, stride_bn, stride_bias_e, stride_bias_n, stride_cm, stride_cn, stride_asm, stride_ask, stride_bse, stride_bsk, stride_bsn, group_n, group_k, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, GROUP_SIZE_M, MUL_ROUTED_WEIGHT, top_k, compute_type, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, per_channel_quant, even_Ks)
moe_align_block_size(topk_ids, block_size, num_experts)
invoke_fused_moe_kernel(A, B, bias, C, A_scale, B_scale, B_zp, topk_weights, topk_ids, sorted_token_ids, expert_ids, num_tokens_post_padded, mul_routed_weight, top_k, config, Any], compute_type, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, use_int4_w4a16, per_channel_quant, block_shape, no_combine)
get_config_file_name(E, N, dtype, block_shape)
get_moe_configs(E, N, dtype, block_n, block_k)
get_default_config(M, E, N, K, topk, dtype, is_marlin, block_shape)
try_get_optimal_moe_config(w1_shape, ...], w2_shape, ...], top_k, dtype, M, is_marlin, block_shape)
get_config_dtype_str(dtype, use_int8_w8a16, use_int4_w4a16, use_fp8_w8a8, use_int8_w8a8)
inplace_fused_experts(hidden_states, w1, w2, topk_weights, topk_ids, b1, b2, activation, apply_router_weight_on_input, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, use_int4_w4a16, per_channel_quant, w1_scale, w2_scale, w1_zp, w2_zp, a1_scale, a2_scale, block_shape, routed_scaling_factor, gemm1_alpha, gemm1_limit)
inplace_fused_experts_fake(hidden_states, w1, w2, topk_weights, topk_ids, b1, b2, activation, apply_router_weight_on_input, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, use_int4_w4a16, per_channel_quant, w1_scale, w2_scale, w1_zp, w2_zp, a1_scale, a2_scale, block_shape, routed_scaling_factor, gemm1_alpha, gemm1_limit)
outplace_fused_experts(hidden_states, w1, w2, topk_weights, topk_ids, b1, b2, activation, apply_router_weight_on_input, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, use_int4_w4a16, per_channel_quant, w1_scale, w2_scale, w1_zp, w2_zp, a1_scale, a2_scale, block_shape, no_combine, routed_scaling_factor, gemm1_alpha, gemm1_limit)
outplace_fused_experts_fake(hidden_states, w1, w2, topk_weights, topk_ids, b1, b2, activation, apply_router_weight_on_input, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, use_int4_w4a16, per_channel_quant, w1_scale, w2_scale, w1_zp, w2_zp, a1_scale, a2_scale, block_shape, no_combine, routed_scaling_factor, gemm1_alpha, gemm1_limit)
fused_experts(hidden_states, w1, w2, topk_output, moe_runner_config, b1, b2, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, use_int4_w4a16, per_channel_quant, w1_scale, w2_scale, w1_zp, w2_zp, a1_scale, a2_scale, block_shape)
moe_sum_reduce_triton(input, output, routed_scaling_factor)
moe_sum_reduce_torch_compile(x, out, routed_scaling_factor)
swiglu_with_alpha_and_limit(x, gemm1_alpha, gemm1_limit)
fused_experts_impl(hidden_states, w1, w2, topk_weights, topk_ids, b1, b2, inplace, activation, apply_router_weight_on_input, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, use_int4_w4a16, per_channel_quant, w1_scale, w2_scale, w1_zp, w2_zp, a1_scale, a2_scale, block_shape, no_combine, routed_scaling_factor, gemm1_alpha, gemm1_limit)
fused_moe(hidden_states, w1, w2, topk_output, moe_runner_config, b1, b2, use_fp8_w8a8, use_int8_w8a8, use_int8_w8a16, use_int4_w4a16, per_channel_quant, w1_scale, w2_scale, w1_zp, w2_zp, a1_scale, a2_scale, block_shape)

# python/sglang/srt/layers/moe/fused_moe_triton/layer.py
  FusedMoE.__init__(num_experts, hidden_size, intermediate_size, layer_id, top_k, num_fused_shared_experts, params_dtype, reduce_results, quant_config, prefix, activation, apply_router_weight_on_input, use_presharded_weights, inplace, no_combine, routed_scaling_factor, gemm1_alpha, gemm1_clamp_limit, use_weight_loader_fused, with_bias)
  FusedMoE.weight_loader(param, loaded_weight, weight_name, shard_id, expert_id)
  FusedMoE.weight_loader_fused(param, loaded_weight, weight_name, shard_id)
  FusedMoE.forward(hidden_states, topk_output)
  FusedMoE.make_expert_params_mapping(cls, ckpt_gate_proj_name, ckpt_down_proj_name, ckpt_up_proj_name, num_experts)
  FusedMoE.make_expert_params_mapping_fused(cls, ckpt_gate_up_proj_name, ckpt_down_proj_name, ckpt_gate_up_proj_bias_name, ckpt_down_proj_bias_name)
  FusedMoE.make_expert_params_mapping_fused_mxfp4(cls, ckpt_gate_up_proj_name, ckpt_down_proj_name, ckpt_gate_up_proj_bias_name, ckpt_down_proj_bias_name, ckpt_gate_up_proj_scale_name, ckpt_down_proj_scale_name)
  FusedMoE.make_expert_input_scale_params_mapping(cls, num_experts)
  FusedMoE.should_fuse_routed_scaling_factor_in_topk()
  FlashInferFusedMoE.__init__()
  FlashInferFusedMoE.forward(hidden_states, topk_output)
  FlashInferFP4MoE.__init__()
  FlashInferFP4MoE.forward(hidden_states, topk_output)
get_fused_moe_impl_class()

# python/sglang/srt/layers/moe/fused_moe_triton/triton_kernels_moe.py
quantize(w, dtype, dev)
triton_kernel_moe_forward(hidden_states, w1, w2, topk_output, moe_runner_config, apply_router_weight_on_input, use_fp8_w8a8, per_channel_quant, global_num_experts, expert_map, w1_scale, w2_scale, a1_scale, a2_scale, block_shape)
triton_kernel_fused_experts(hidden_states, w1, w2, routing_data, gather_indx, scatter_indx, inplace, activation, apply_router_weight_on_input, use_fp8_w8a8, per_channel_quant, global_num_experts, expert_map, w1_scale, w2_scale, a1_scale, a2_scale, block_shape)
triton_kernel_moe_with_bias_forward(hidden_states, w1, w1_pcg, b1, w2, w2_pcg, b2, topk_output, moe_runner_config, use_fp8_w8a8, per_channel_quant, global_num_experts, expert_map, w1_scale, w2_scale, a1_scale, a2_scale, block_shape)
triton_kernel_fused_experts_with_bias(hidden_states, w1, w1_pcg, b1, w2, w2_pcg, b2, routing_data, gather_indx, scatter_indx, inplace, activation, use_fp8_w8a8, per_channel_quant, global_num_experts, expert_map, w1_scale, w2_scale, a1_scale, a2_scale, block_shape, gemm1_alpha, gemm1_clamp_limit)