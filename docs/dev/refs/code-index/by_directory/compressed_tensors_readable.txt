================================================================================
FUNCTION INDEX: compressed_tensors module
================================================================================
Total Functions: 32
Documented: 8


============================================================
FILE: python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors.py
Functions: 18
============================================================


CLASS: CompressedTensorsConfig
----------------------------------------
  L  79: __init__(self, target_scheme_map: Dict[str, Any], ignore: List[str], quant_format: str, sparsity_scheme_map: Dict[str, SparsityCompressionConfig], sparsity_ignore_list: List[str], kv_cache_scheme: Optional[Dict[str, Any]], config: Optional[Dict[str, Any]], packed_modules_mapping: Dict[str, List[str]])

  L 101: get_linear_method(self)
         ‚Üí CompressedTensorsLinearMethod

  L 104: get_supported_act_dtypes(cls)
         ‚Üí List[torch.dtype]

  L 108: get_min_capability(cls)
         ‚Üí int

  L 111: get_name(self)
         ‚Üí str

  L 114: get_scaled_act_names(self)
         ‚Üí List[str]

  L 117: get_quant_method(self, layer: torch.nn.Module, prefix: str)
         ‚Üí Optional[QuantizeMethodBase]

  L 143: from_config(cls, config: Dict[str, Any])
         ‚Üí CompressedTensorsConfig

  L 234: get_config_filenames(cls)
         ‚Üí List[str]

  L 438: get_scheme(self, layer: torch.nn.Module, layer_name: Optional[str])
         ‚Üí Optional[CompressedTensorsScheme]
         üìù compressed-tensors supports non uniform in the following way:
            targets of config_groups: There can be N config_groups which each
            have a quantization scheme. Each config_group has a list of targets
            which can be a full layer_name, a regex for a layer_name, or
            an nn.Module name.
            Detect whether a layer_name is found in any target and
            use the quantization scheme corresponding to the matched target
            to select the CompressedTensorsScheme used for infernece.

  L 533: get_cache_scale(self, name: str)
         ‚Üí Optional[str]
         üìù Check whether the param name matches the format for k/v cache scales
            in compressed-tensors. If this is the case, return its equivalent
            param name expected by vLLM
            :param name: param name
            :return: matching param name for KV cache scale in vLLM

  L 550: supports_cutlass_24(weight_quant: Optional[QuantizationArgs], input_quant: Optional[QuantizationArgs], sparsity_scheme: Optional[SparsityCompressionConfig])
         ‚Üí bool
         üìù Check if the layer is supported by the Cutlass 2:4 Kernel
            Conditions:
            - Overarching condition: Sparsity Structure is 2:4
            - Unquantized cases are supported
            - Weight only quantization is not-supported
            - Supported weight quantization strategies are TENSOR and CHANNEL
            - Supported input quantization strategies are TENSOR and TOKEN
            - Only 8 bit quantization is supported
            :return: True if the layer is supported by the Cutlass 2:4 Kernel
            False otherwise


CLASS: CompressedTensorsLinearMethod
----------------------------------------
  L 618: __init__(self, quantization_config: CompressedTensorsConfig)

  L 621: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 624: create_weights(self, layer: torch.nn.Module, input_size_per_partition: int, output_partition_sizes: List[int], input_size: int, output_size: int, params_dtype: torch.dtype)
         üìù Use the CompressedTensorsScheme associated with each layer to create
            the necessary parameters for the layer. See LinearMethodBase for param
            details

  L 650: apply(self, layer: torch.nn.Module, x: torch.Tensor, bias: Optional[torch.Tensor])
         üìù Use the output of create_weights and the CompressedTensorsScheme
            associated with the layer to apply the forward pass with the
            layer input.  See LinearMethodBase for param details


CLASS: DeviceCapability
----------------------------------------
  L  64: as_version_str(self)
         ‚Üí str

  L  67: to_int(self)
         ‚Üí int
         üìù Express device capability as an integer ``<major><minor>``.
            It is assumed that the minor version is always a single digit.


============================================================
FILE: python/sglang/srt/layers/quantization/compressed_tensors/compressed_tensors_moe.py
Functions: 10
============================================================


CLASS: CompressedTensorsMoEMethod
----------------------------------------
  L  70: __new__(cls)

  L  76: get_moe_method(quant_config: CompressedTensorsConfig)
         ‚Üí 'CompressedTensorsMoEMethod'


CLASS: CompressedTensorsW8A8Fp8MoEMethod
----------------------------------------
  L 100: __init__(self, quant_config: CompressedTensorsConfig)

  L 109: create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)

  L 207: process_weights_after_loading(self, layer: FusedMoE)
         ‚Üí None

  L 296: apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor


CLASS: CompressedTensorsWNA16MoEMethod
----------------------------------------
  L 346: __init__(self, quant_config: CompressedTensorsConfig)

  L 369: create_weights(self, layer: torch.nn.Module, num_experts: int, hidden_size: int, intermediate_size_per_partition: int, params_dtype: torch.dtype)

  L 513: process_weights_after_loading(self, layer: torch.nn.Module)
         ‚Üí None

  L 643: apply(self, layer: torch.nn.Module, x: torch.Tensor, topk_output: TopKOutput, moe_runner_config: MoeRunnerConfig)
         ‚Üí torch.Tensor


============================================================
FILE: python/sglang/srt/layers/quantization/compressed_tensors/utils.py
Functions: 4
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  12: def is_activation_quantization_format(format: str)
         ‚Üí bool

  L  21: def should_ignore_layer(layer_name: Optional[str],
        ignore: Iterable[str],
        fused_mapping: Mapping[str,
        List[str]])
         ‚Üí bool

  L  76: def check_equal_or_regex_match(layer_name: str, targets: Iterable[str])
         ‚Üí bool
         üìù Checks whether a layer_name is exactly equal or a regex match for
            if target starts with 're:' to any target in list.

  L  87: def find_matched_target(layer_name: Optional[str],
        module: Module,
        targets: Iterable[str],
        fused_mapping: Mapping[str,
        List[str]])
         ‚Üí str
         üìù Helper function to look up which "target" in the compressed-tensors
            config that a layer corresponds to.
            Recall that a compressed-tensors configs has a concept of
            config_groups, where each layer can be quantized with with a different
            scheme.
            targets in each config_group will be a list of either layer names
            (or regexes corresponding to layer names) or names of torch Modules.
            First, we try to match the layer_name with a target
            Second, we try to match the module's name with a target
            Third, we try to map the layer_name to a list of fused module names.
            *All* component module names must match in order for a match to be
            successful. A successful match returns the first component target
            :param layer_name: layer name
            :param module: torch.nn.Module
            :param targets: list of targets to match the layer against
            :param fused_mapping: map from fused layer names to its components
            :param fused_strategy: either "all" or "any". If using "all", fused
            layers match if "all" of its components match
