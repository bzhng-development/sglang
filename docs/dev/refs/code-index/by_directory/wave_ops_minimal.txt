
# python/sglang/srt/layers/attention/wave_ops/decode_attention.py
get_wave_kernel(shape: paged_decode_attention_shape, max_kv_splits, input_dtype, output_dtype, logit_cap)
decode_attention_intermediate_arrays_shapes(num_seqs, head_size_kv, num_query_heads, max_kv_splits)
decode_attention_wave(q, k_buffer, v_buffer, o, b_req_idx, req_to_token, attn_logits, attn_logits_max, num_kv_splits, max_kv_splits, sm_scale, logit_cap)
decode_attention_fwd(q, k_buffer, v_buffer, o, b_req_idx, req_to_token, attn_logits, attn_logits_max, num_kv_splits, max_kv_splits, sm_scale, logit_cap)

# python/sglang/srt/layers/attention/wave_ops/extend_attention.py
get_wave_kernel(shape: AttentionShape, q_shape: tuple[int], k_shape: tuple[int], v_shape: tuple[int], k_cache_shape: tuple[int], v_cache_shape: tuple[int], o_shape: tuple[int], input_dtype: torch.dtype, output_dtype: torch.dtype, size_dtype: torch.dtype, is_causal: bool, logit_cap: float, layer_scaling: float)
extend_attention_wave(q_extend, k_extend, v_extend, k_buffer, v_buffer, qo_indptr, kv_indptr, kv_indices, custom_mask, mask_indptr, max_seq_len, output, is_causal, layer_scaling, logit_cap)

# python/sglang/srt/layers/attention/wave_ops/prefill_attention.py
prefill_attention_wave(q, k, v, o, b_start_loc, b_seq_len, max_seq_len, is_causal)
