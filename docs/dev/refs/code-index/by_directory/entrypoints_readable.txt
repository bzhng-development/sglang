================================================================================
FUNCTION INDEX: entrypoints module
================================================================================
Total Functions: 145
Documented: 75


============================================================
FILE: python/sglang/srt/entrypoints/EngineBase.py
Functions: 8
============================================================


CLASS: EngineBase
----------------------------------------
  L  14: generate(self, prompt: Optional[Union[List[str], str]], sampling_params: Optional[Union[List[Dict], Dict]], input_ids: Optional[Union[List[List[int]], List[int]]], image_data: Optional[Union[List[str], str]], return_logprob: Optional[Union[List[bool], bool]], logprob_start_len: Optional[Union[List[int], int]], top_logprobs_num: Optional[Union[List[int], int]], token_ids_logprob: Optional[Union[List[List[int]], List[int]]], lora_path: Optional[Union[List[Optional[str]], Optional[str]]], custom_logit_processor: Optional[Union[List[str], str]], return_hidden_states: Optional[bool], stream: Optional[bool], bootstrap_host: Optional[Union[List[str], str]], bootstrap_port: Optional[Union[List[int], int]], bootstrap_room: Optional[Union[List[int], int]], data_parallel_rank: Optional[int])
         → Union[Dict, Iterator[Dict]]
         📝 Generate outputs based on given inputs.

  L  37: flush_cache(self)
         📝 Flush the cache of the engine.

  L  42: update_weights_from_tensor(self, named_tensors: List[Tuple[str, torch.Tensor]], load_format: Optional[str], flush_cache: bool)
         📝 Update model weights with in-memory tensor data.

  L  51: load_lora_adapter(self, lora_name: str, lora_path: str)
         📝 Load a new LoRA adapter without re-launching the engine.

  L  55: unload_lora_adapter(self, lora_name: str)
         📝 Unload a LoRA adapter without re-launching the engine.

  L  60: release_memory_occupation(self)
         📝 Release GPU memory occupation temporarily.

  L  65: resume_memory_occupation(self)
         📝 Resume GPU memory occupation which is previously released.

  L  70: shutdown(self)
         📝 Shutdown the engine and clean up resources.


============================================================
FILE: python/sglang/srt/entrypoints/context.py
Functions: 23
============================================================


CLASS: ConversationContext
----------------------------------------
  L  28: append_output(self, output)
         → None

  L  32: call_tool(self)
         → list[Message]

  L  36: need_builtin_tool_call(self)
         → bool

  L  40: render_for_completion(self)
         → list[int]


CLASS: HarmonyContext
----------------------------------------
  L  64: __init__(self, messages: list, tool_sessions: dict[str, Union['ClientSession', Tool]])

  L  82: append_output(self, output)
         → None

  L 114: messages(self)
         → list

  L 117: need_builtin_tool_call(self)
         → bool

  L 126: call_tool(self)
         → list[Message]

  L 142: render_for_completion(self)
         → list[int]

  L 145: call_search_tool(self, tool_session: Union['ClientSession', Tool], last_msg: Message)
         → list[Message]

  L 158: call_python_tool(self, tool_session: Union['ClientSession', Tool], last_msg: Message)
         → list[Message]


CLASS: SimpleContext
----------------------------------------
  L  46: __init__(self)

  L  49: append_output(self, output)
         → None

  L  52: need_builtin_tool_call(self)
         → bool

  L  55: call_tool(self)
         → list[Message]

  L  58: render_for_completion(self)
         → list[int]


CLASS: StreamingHarmonyContext
----------------------------------------
  L 184: __init__(self)

  L 193: messages(self)
         → list

  L 196: append_output(self, output)
         → None

  L 226: is_expecting_start(self)
         → bool

  L 229: is_assistant_action_turn(self)
         → bool

  L 232: render_for_completion(self)
         → list[int]


============================================================
FILE: python/sglang/srt/entrypoints/engine.py
Functions: 31
============================================================


CLASS: Engine
----------------------------------------
  L 103: __init__(self)
         📝 The arguments of this function is the same as `sglang/srt/server_args.

  L 140: generate(self, prompt: Optional[Union[List[str], str]], sampling_params: Optional[Union[List[Dict], Dict]], input_ids: Optional[Union[List[List[int]], List[int]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat], return_logprob: Optional[Union[List[bool], bool]], logprob_start_len: Optional[Union[List[int], int]], top_logprobs_num: Optional[Union[List[int], int]], token_ids_logprob: Optional[Union[List[List[int]], List[int]]], lora_path: Optional[List[Optional[str]]], custom_logit_processor: Optional[Union[List[str], str]], return_hidden_states: bool, stream: bool, bootstrap_host: Optional[Union[List[str], str]], bootstrap_port: Optional[Union[List[int], int]], bootstrap_room: Optional[Union[List[int], int]], data_parallel_rank: Optional[int])
         → Union[Dict, Iterator[Dict]]
         📝 The arguments of this function is the same as `sglang/srt/managers/io_

  L 221: async_generate(self, prompt: Optional[Union[List[str], str]], sampling_params: Optional[Union[List[Dict], Dict]], input_ids: Optional[Union[List[List[int]], List[int]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat], return_logprob: Optional[Union[List[bool], bool]], logprob_start_len: Optional[Union[List[int], int]], top_logprobs_num: Optional[Union[List[int], int]], token_ids_logprob: Optional[Union[List[List[int]], List[int]]], lora_path: Optional[List[Optional[str]]], custom_logit_processor: Optional[Union[List[str], str]], return_hidden_states: bool, stream: bool, bootstrap_host: Optional[Union[List[str], str]], bootstrap_port: Optional[Union[List[int], int]], bootstrap_room: Optional[Union[List[int], int]], data_parallel_rank: Optional[int])
         → Union[Dict, AsyncIterator[Dict]]
         📝 The arguments of this function is the same as `sglang/srt/managers/io_

  L 293: encode(self, prompt: Union[str, List[str], List[Dict], List[List[Dict]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat])
         → Dict
         📝 The arguments of this function is the same as `sglang/srt/managers/io_

  L 315: async_encode(self, prompt: Union[str, List[str], List[Dict], List[List[Dict]]], image_data: Optional[MultimodalDataInputFormat], audio_data: Optional[MultimodalDataInputFormat], video_data: Optional[MultimodalDataInputFormat])
         → Dict
         📝 Asynchronous version of encode method.

  L 337: rerank(self, prompt: Union[List[List[str]]])
         → Dict
         📝 The arguments of this function is the same as `sglang/srt/managers/io_

  L 351: shutdown(self)
         📝 Shutdown the engine

  L 355: __enter__(self)

  L 358: __exit__(self, exc_type, exc_value, traceback)

  L 362: flush_cache(self)

  L 366: start_profile(self)

  L 370: stop_profile(self)

  L 374: start_expert_distribution_record(self)

  L 380: stop_expert_distribution_record(self)

  L 386: dump_expert_distribution_record(self)

  L 392: get_server_info(self)

  L 404: init_weights_update_group(self, master_address: str, master_port: int, rank_offset: int, world_size: int, group_name: str, backend: str)
         📝 Initialize parameter update group.

  L 427: update_weights_from_distributed(self, names: list[str], dtypes: list[str], shapes: list[list[int]], group_name: str, flush_cache: bool)
         📝 Update weights from distributed source.

  L 448: update_weights_from_tensor(self, named_tensors: List[Tuple[str, torch.Tensor]], load_format: Optional[str], flush_cache: bool)
         📝 Update weights from distributed source. If there are going to be more 

  L 474: update_weights_from_disk(self, model_path: str, load_format: Optional[str])
         📝 Update the weights from disk inplace without re-launching the engine.

  L 495: get_weights_by_name(self, name: str, truncate_size: int)
         📝 Get weights by parameter name.

  L 503: load_lora_adapter(self, lora_name: str, lora_path: str, pinned: bool)
         📝 Load a new LoRA adapter without re-launching the engine.

  L 517: unload_lora_adapter(self, lora_name: str)
         📝 Unload a LoRA adapter without re-launching the engine.

  L 527: release_memory_occupation(self, tags: Optional[List[str]])

  L 534: resume_memory_occupation(self, tags: Optional[List[str]])

  L 541: freeze_gc(self)
         📝 To maintain a high performance server with low latency, we want to red

  L 561: collective_rpc(self, method: str)

  L 568: save_remote_model(self)

  L 571: save_sharded_model(self)

  L 574: score(self, query: Optional[Union[str, List[int]]], items: Optional[Union[str, List[str], List[List[int]]]], label_token_ids: Optional[List[int]], apply_softmax: bool, item_first: bool)
         → List[List[float]]
         📝 Score the probability of specified token IDs appearing after the given

  L 625: async_score(self, query: Optional[Union[str, List[int]]], items: Optional[Union[str, List[str], List[List[int]]]], label_token_ids: Optional[List[int]], apply_softmax: bool, item_first: bool)
         → List[List[float]]
         📝 Asynchronous version of score method.


============================================================
FILE: python/sglang/srt/entrypoints/harmony_utils.py
Functions: 13
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  54: def get_encoding()

  L  61: def get_system_message(model_identity: Optional[str],
        reasoning_effort: Optional[Literal['high',
        'medium',
        'low']],
        start_date: Optional[str],
        browser_description: Optional[str],
        python_description: Optional[str])
         → Message

  L  86: def get_developer_message(instructions: Optional[str],
        tools: Optional[list[Tool]])
         → Message

  L 118: def get_user_message(content: str)
         → Message

  L 122: def parse_response_input(response_msg: ResponseInputOutputItem,
        prev_responses: list[Union[ResponseOutputItem,
        ResponseReasoningItem]])
         → Message

  L 174: def parse_response_output(output: ResponseOutputItem)
         → Message

  L 190: def parse_chat_input(chat_msg)
         → Message

  L 202: def render_for_completion(messages: list[Message])
         → list[int]

  L 210: def get_stop_tokens_for_assistant_actions()
         → list[int]

  L 214: def get_streamable_parser_for_assistant()
         → StreamableParser

  L 218: def parse_output_message(message: Message)

  L 324: def parse_remaining_state(parser: StreamableParser)

  L 368: def parse_output_into_messages(token_ids: Iterable[int])


============================================================
FILE: python/sglang/srt/entrypoints/http_server.py
Functions: 55
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 128: def set_global_state(global_state: _GlobalState)

  L 134: async def lifespan(fast_api_app: FastAPI)
         @asynccontextmanager

  L 212: async def validation_exception_handler(request: Request, exc: HTTPException)
         📝 Enrich HTTP exception with status code and other details
         @app.exception_handler(HTTPException)

  L 225: async def validation_exception_handler(request: Request,
        exc: RequestValidationError)
         📝 Override FastAPI's default 422 validation error with 400
         @app.exception_handler(RequestValidationError)

  L 247: async def validate_json_request(raw_request: Request)
         📝 Validate that the request content-type is application/json.

  L 268: async def health_generate(request: Request)
         → Response
         📝 Check the health of the inference server by sending a special request 
         @app.get('/health')
         @app.get('/health_generate')

  L 339: async def get_model_info()
         📝 Get the model information.
         @app.get('/get_model_info')

  L 352: async def get_weight_version()
         📝 Get the current weight version.
         @app.get('/get_weight_version')

  L 360: async def get_server_info()
         @app.get('/get_server_info')

  L 374: async def get_load()
         @app.get('/get_load')

  L 381: async def set_internal_state(obj: SetInternalStateReq, request: Request)
         @app.api_route('/set_internal_state', methods=['POST', 'PUT'])

  L 388: async def generate_request(obj: GenerateReqInput, request: Request)
         📝 Handle a generate request.
         @app.api_route('/generate', methods=['POST', 'PUT'])

  L 425: async def generate_from_file_request(file: UploadFile, request: Request)
         📝 Handle a generate request, this is purely to work with input_embeds.
         @app.api_route('/generate_from_file', methods=['POST'])

  L 449: async def encode_request(obj: EmbeddingReqInput, request: Request)
         📝 Handle an embedding request.
         @app.api_route('/encode', methods=['POST', 'PUT'])

  L 461: async def classify_request(obj: EmbeddingReqInput, request: Request)
         📝 Handle a reward model request. Now the arguments and return values are
         @app.api_route('/classify', methods=['POST', 'PUT'])

  L 473: async def flush_cache()
         📝 Flush the radix cache.
         @app.api_route('/flush_cache', methods=['GET', 'POST'])

  L 484: async def start_profile_async(obj: Optional[ProfileReqInput])
         📝 Start profiling.
         @app.api_route('/start_profile', methods=['GET', 'POST'])

  L 505: async def stop_profile_async()
         📝 Stop profiling.
         @app.api_route('/stop_profile', methods=['GET', 'POST'])

  L 515: async def freeze_gc_async()
         📝 See engine.freeze_gc for more details.
         @app.api_route('/freeze_gc', methods=['GET', 'POST'])

  L 527: async def start_expert_distribution_record_async()
         📝 Start recording the expert distribution. Clear the previous record if 
         @app.api_route('/start_expert_distribution_record', methods=['GET', 'POST'])

  L 537: async def stop_expert_distribution_record_async()
         📝 Stop recording the expert distribution.
         @app.api_route('/stop_expert_distribution_record', methods=['GET', 'POST'])

  L 547: async def dump_expert_distribution_record_async()
         📝 Dump expert distribution record.
         @app.api_route('/dump_expert_distribution_record', methods=['GET', 'POST'])

  L 557: async def update_weights_from_disk(obj: UpdateWeightFromDiskReqInput,
        request: Request)
         📝 Update the weights from disk inplace without re-launching the server.
         @app.post('/update_weights_from_disk')

  L 586: async def init_weights_update_group(obj: InitWeightsUpdateGroupReqInput,
        request: Request)
         📝 Initialize the parameter update group.
         @app.post('/init_weights_update_group')

  L 601: async def update_weights_from_tensor(obj: UpdateWeightsFromTensorReqInput,
        request: Request)
         📝 Update the weights from tensor inplace without re-launching the server
         @app.post('/update_weights_from_tensor')

  L 627: async def update_weights_from_distributed(obj: UpdateWeightsFromDistributedReqInput,
        request: Request)
         📝 Update model parameter from distributed online.
         @app.post('/update_weights_from_distributed')

  L 650: async def update_weight_version(obj: UpdateWeightVersionReqInput,
        request: Request)
         📝 Update the weight version. This operation requires no active requests.
         @app.post('/update_weight_version')

  L 680: async def get_weights_by_name(obj: GetWeightsByNameReqInput, request: Request)
         📝 Get model parameter by name.
         @app.api_route('/get_weights_by_name', methods=['GET', 'POST'])

  L 693: async def release_memory_occupation(obj: ReleaseMemoryOccupationReqInput,
        request: Request)
         📝 Release GPU memory occupation temporarily.
         @app.api_route('/release_memory_occupation', methods=['GET', 'POST'])

  L 704: async def resume_memory_occupation(obj: ResumeMemoryOccupationReqInput,
        request: Request)
         📝 Resume GPU memory occupation.
         @app.api_route('/resume_memory_occupation', methods=['GET', 'POST'])

  L 715: async def slow_down(obj: SlowDownReqInput, request: Request)
         📝 Slow down the system deliberately. Only for testing. Example scenario:
         @app.api_route('/slow_down', methods=['GET', 'POST'])

  L 728: async def load_lora_adapter(obj: LoadLoRAAdapterReqInput, request: Request)
         📝 Load a new LoRA adapter without re-launching the server.
         @app.api_route('/load_lora_adapter', methods=['POST'])

  L 745: async def unload_lora_adapter(obj: UnloadLoRAAdapterReqInput, request: Request)
         📝 Load a new LoRA adapter without re-launching the server.
         @app.api_route('/unload_lora_adapter', methods=['POST'])

  L 762: async def open_session(obj: OpenSessionReqInput, request: Request)
         📝 Open a session, and return its unique session id.
         @app.api_route('/open_session', methods=['GET', 'POST'])

  L 776: async def close_session(obj: CloseSessionReqInput, request: Request)
         📝 Close the session.
         @app.api_route('/close_session', methods=['GET', 'POST'])

  L 786: async def configure_logging(obj: ConfigureLoggingReq, request: Request)
         📝 Configure the request logging options.
         @app.api_route('/configure_logging', methods=['GET', 'POST'])

  L 793: async def abort_request(obj: AbortReq, request: Request)
         📝 Abort a request.
         @app.post('/abort_request')

  L 805: async def parse_function_call_request(obj: ParseFunctionCallReq,
        request: Request)
         📝 A native API endpoint to parse function calls from a text.
         @app.post('/parse_function_call')

  L 827: async def separate_reasoning_request(obj: SeparateReasoningReqInput,
        request: Request)
         📝 A native API endpoint to separate reasoning from a text.
         @app.post('/separate_reasoning')

  L 847: async def pause_generation(request: Request)
         📝 Pause generation.
         @app.post('/pause_generation')

  L 857: async def continue_generation(request: Request)
         📝 Continue generation.
         @app.post('/continue_generation')

  L 870: async def openai_v1_completions(request: CompletionRequest,
        raw_request: Request)
         📝 OpenAI-compatible text completion endpoint.
         @app.post('/v1/completions', dependencies=[Depends(validate_json_request)])

  L 878: async def openai_v1_chat_completions(request: ChatCompletionRequest,
        raw_request: Request)
         📝 OpenAI-compatible chat completion endpoint.
         @app.post('/v1/chat/completions', dependencies=[Depends(validate_json_request)])

  L 892: async def openai_v1_embeddings(request: EmbeddingRequest, raw_request: Request)
         📝 OpenAI-compatible embeddings endpoint.
         @app.post('/v1/embeddings', response_class=ORJSONResponse, dependencies=[Depends(validate_json_request)])

  L 900: async def available_models()
         📝 Show available models. OpenAI-compatible endpoint.
         @app.get('/v1/models', response_class=ORJSONResponse)

  L 916: async def retrieve_model(model: str)
         📝 Retrieves a model instance, providing basic information about the mode
         @app.get('/v1/models/{model:path}', response_class=ORJSONResponse)

  L 941: async def v1_score_request(request: ScoringRequest, raw_request: Request)
         📝 Endpoint for the decoder-only scoring API. See Engine.score() for deta
         @app.post('/v1/score', dependencies=[Depends(validate_json_request)])

  L 949: async def v1_responses_request(request: dict, raw_request: Request)
         📝 Endpoint for the responses API with reasoning support.
         @app.post('/v1/responses', dependencies=[Depends(validate_json_request)])

  L 969: async def v1_retrieve_responses(response_id: str, raw_request: Request)
         📝 Retrieve a response by ID.
         @app.get('/v1/responses/{response_id}')

  L 977: async def v1_cancel_responses(response_id: str, raw_request: Request)
         📝 Cancel a background response.
         @app.post('/v1/responses/{response_id}/cancel')

  L 987: async def v1_rerank_request(request: V1RerankReqInput, raw_request: Request)
         📝 Endpoint for reranking documents based on query relevance.
         @app.api_route('/v1/rerank', methods=['POST', 'PUT'], dependencies=[Depends(validate_json_request)])

  L 996: async def sagemaker_health()
         → Response
         📝 Check the health of the http server.
         @app.get('/ping')

  L1002: async def sagemaker_chat_completions(request: ChatCompletionRequest,
        raw_request: Request)
         📝 OpenAI-compatible chat completion endpoint.
         @app.post('/invocations')

  L1013: async def vertex_generate(vertex_req: VertexGenerateReqInput,
        raw_request: Request)
         @app.post(os.environ.get('AIP_PREDICT_ROUTE', '/vertex_generate'))

  L1051: def launch_server(server_args: ServerArgs,
        pipe_finish_writer: Optional[multiprocessing.connection.Connection],
        launch_callback: Optional[Callable[[],
        None]])
         📝 Launch SRT (SGLang Runtime) Server.


============================================================
FILE: python/sglang/srt/entrypoints/http_server_engine.py
Functions: 8
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  20: def launch_server_process(server_args: ServerArgs)
         → multiprocessing.Process


CLASS: HttpServerEngineAdapter
----------------------------------------
  L  58: __init__(self)

  L  78: update_weights_from_tensor(self, named_tensors: List[Tuple[str, torch.Tensor]], load_format: Optional[str], flush_cache: bool)
         📝 Update model weights from tensor data. The HTTP server will only post 

  L 102: shutdown(self)

  L 105: generate(self, prompt, sampling_params, input_ids, image_data, return_logprob, logprob_start_len, top_logprobs_num, token_ids_logprob, lora_path, custom_logit_processor)

  L 135: release_memory_occupation(self)

  L 138: resume_memory_occupation(self)

  L 141: flush_cache(self)


============================================================
FILE: python/sglang/srt/entrypoints/tool.py
Functions: 7
============================================================


CLASS: HarmonyBrowserTool
----------------------------------------
  L  25: __init__(self)

  L  45: get_result(self, context: 'ConversationContext')
         → Any

  L  56: tool_config(self)
         → Any


CLASS: HarmonyPythonTool
----------------------------------------
  L  62: __init__(self)

  L  75: get_result(self, context: 'ConversationContext')
         → Any

  L  86: tool_config(self)
         → Any


CLASS: Tool
----------------------------------------
  L  19: get_result(self, context: 'ConversationContext')
         → Any
