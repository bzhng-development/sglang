================================================================================
FUNCTION INDEX: model_executor module
================================================================================
Total Functions: 97
Documented: 15


============================================================
FILE: python/sglang/srt/model_executor/cuda_graph_runner.py
Functions: 16
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  73: def get_is_capture_mode()

  L  78: def model_capture_mode()
         @contextmanager

  L  88: def freeze_gc(enable_cudagraph_gc: bool)
         üìù Optimize garbage collection during CUDA graph capture.
            Clean up, then freeze all remaining objects from being included
            in future collections if GC is disabled during capture.
         @contextmanager

  L 117: def patch_model(model: torch.nn.Module,
        enable_compile: bool,
        num_tokens: int,
        tp_group: GroupCoordinator)
         üìù Patch the model to make it compatible with with torch.compile
         @contextmanager

  L 149: def set_torch_compile_config()

  L 165: def get_batch_sizes_to_capture(model_runner: ModelRunner)

  L 228: def get_global_graph_memory_pool()

  L 232: def set_global_graph_memory_pool(val)


CLASS: CudaGraphRunner
----------------------------------------
  L 240: __init__(self, model_runner: ModelRunner)

  L 393: can_run(self, forward_batch: ForwardBatch)

  L 445: capture(self)
         ‚Üí None

  L 520: capture_one_batch_size(self, bs: int, forward: Callable)

  L 675: recapture_if_needed(self, forward_batch: ForwardBatch)

  L 706: replay_prepare(self, forward_batch: ForwardBatch, pp_proxy_tensors: Optional[PPProxyTensors])

  L 796: replay(self, forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí Union[LogitsProcessorOutput, PPProxyTensors]

  L 826: get_spec_info(self, num_tokens: int)


============================================================
FILE: python/sglang/srt/model_executor/forward_batch_info.py
Functions: 44
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L 875: def enable_num_token_non_padded(server_args)

  L 909: def compute_position(attn_backend: str,
        extend_prefix_lens: torch.Tensor,
        extend_seq_lens: torch.Tensor,
        extend_seq_lens_sum: int)

  L 928: def compute_position_triton(extend_prefix_lens: torch.Tensor,
        extend_seq_lens: torch.Tensor,
        extend_seq_lens_sum)
         üìù Compute positions. It is a fused version of `compute_position_torch`.

  L 955: def compute_position_kernel(positions,
        extend_start_loc,
        extend_prefix_lens,
        extend_seq_lens,
        has_prefix: tl.constexpr)
         @triton.jit

  L 984: def compute_position_torch(extend_prefix_lens: torch.Tensor,
        extend_seq_lens: torch.Tensor)

  L1002: def clamp_position(seq_lens)
         @torch.compile(dynamic=True, backend=get_compiler_backend(), disable=_is_npu)

  L1007: def create_chunked_prefix_cache_kv_indices(req_to_token_ptr,
        req_pool_indices_ptr,
        chunk_start_idx_ptr,
        chunk_seq_lens_ptr,
        chunk_cu_seq_lens_ptr,
        chunk_kv_indices_ptr,
        req_to_token_ptr_stride: tl.constexpr)
         @triton.jit


CLASS: CaptureHiddenMode
----------------------------------------
  L 151: need_capture(self)

  L 154: is_full(self)

  L 157: is_last(self)

  L 160: __lt__(self, other)


CLASS: ForwardBatch
----------------------------------------
  L 310: init_new(cls, batch: ModelWorkerBatch, model_runner: ModelRunner)

  L 456: merge_mm_inputs(self)
         ‚Üí Optional[MultimodalInputs]
         üìù Merge all multimodal inputs in the batch into a single MultiModalInputs object.
            Returns:
            if none, current batch contains no multimodal input

  L 479: contains_image_inputs(self)
         ‚Üí bool

  L 487: contains_audio_inputs(self)
         ‚Üí bool

  L 495: contains_video_inputs(self)
         ‚Üí bool

  L 503: contains_mm_inputs(self)
         ‚Üí bool

  L 568: get_max_chunk_capacity(self)

  L 573: set_prefix_chunk_idx(self, idx: int)

  L 576: set_attn_attend_prefix_cache(self, attn_attend_prefix_cache: bool)

  L 579: prepare_chunked_kv_indices(self, device: torch.device)

  L 617: prepare_mlp_sync_batch(self, model_runner: ModelRunner)

  L 733: post_forward_mlp_sync_batch(self, logits_output: LogitsProcessorOutput)

  L 789: get_prefix_chunk_seq_lens(self, prefix_lens: torch.Tensor, num_prefix_chunks: int, prefix_chunk_len: int)

  L 812: prepare_chunked_prefix_cache_info(self, device: torch.device)

  L 871: can_run_tbo(self)


CLASS: ForwardMode
----------------------------------------
  L  92: is_prefill(self)

  L  95: is_extend(self)

  L 103: is_decode(self)

  L 106: is_mixed(self)

  L 109: is_idle(self)

  L 112: is_decode_or_idle(self)

  L 115: is_target_verify(self)

  L 118: is_draft_extend(self)

  L 121: is_extend_or_draft_extend_or_mixed(self)

  L 128: is_cuda_graph(self)

  L 135: is_dummy_first(self)

  L 138: is_split_prefill(self)


CLASS: PPProxyTensors
----------------------------------------
  L 883: __init__(self, tensors)

  L 890: __getitem__(self, key: Union[str, slice])

  L 896: __setitem__(self, key: str, value: torch.Tensor)

  L 899: __len__(self)

  L 902: __eq__(self, other: object)

  L 905: __repr__(self)
         ‚Üí str


============================================================
FILE: python/sglang/srt/model_executor/model_runner.py
Functions: 35
============================================================


CLASS: LocalSerializedTensor
----------------------------------------
  L1912: get(self, rank: int)


CLASS: ModelRunner
----------------------------------------
  L 162: __init__(self, model_config: ModelConfig, mem_fraction_static: float, gpu_id: int, tp_rank: int, tp_size: int, moe_ep_rank: int, moe_ep_size: int, pp_rank: int, pp_size: int, nccl_port: int, server_args: ServerArgs, dp_rank: Optional[int], is_draft_worker: bool, req_to_token_pool: Optional[ReqToTokenPool], token_to_kv_pool_allocator: Optional[BaseTokenToKVPoolAllocator])

  L 255: initialize(self, min_per_gpu_memory: float)

  L 385: model_specific_adjustment(self)

  L 554: init_torch_distributed(self)

  L 650: load_model(self)

  L 757: update_expert_location(self, new_expert_location_metadata: ExpertLocationMetadata, update_layer_ids: List[int])

  L 770: update_weights_from_disk(self, model_path: str, load_format: str)
         ‚Üí tuple[bool, str]
         üìù Update engine weights in-place from the disk.

  L 825: init_weights_update_group(self, master_address, master_port, rank_offset, world_size, group_name, backend)
         üìù Initialize the Torch process group for model parameter updates.
            `_model_update_group` is used in the RLHF workflow, where rank
            0 is the actor model in the training engine, and the other ranks are
            the inference engine, which is used for rollout.
            In the RLHF workflow, the training engine updates the model
            weights/parameters online, and broadcasts them to the inference
            engine through the `_model_update_group` process group.

  L 870: update_weights_from_distributed(self, names, dtypes, shapes, group_name)
         üìù Update specific parameter in the model weights online
            through `_model_update_group` process group.
            Args:
            name: the name of the parameter to be updated.
            dtype: the data type of the parameter to be updated.
            shape: the shape of the parameter to be updated.

  L 918: update_weights_from_tensor(self, named_tensors: List[Tuple[str, Union[torch.Tensor, 'LocalSerializedTensor']]], load_format: Optional[str])

  L 981: get_weights_by_name(self, name: str, truncate_size: int)
         ‚Üí Optional[torch.Tensor]
         üìù Get the weights of the parameter by its name. Similar to `get_parameter` in Hugging Face.
            Only used for unit test with an unoptimized performance.
            For optimized performance, please use torch.save and torch.load.

  L 998: init_lora_manager(self)

  L1013: load_lora_adapter(self, lora_ref: LoRARef)
         üìù Load a new lora adapter from disk or huggingface.

  L1030: unload_lora_adapter(self, lora_ref: LoRARef)
         üìù Unload a lora adapter that was previously loaded during initialization or dynamic loading.

  L1047: profile_max_num_token(self, total_gpu_memory: int)

  L1082: set_num_token_hybrid(self)

  L1164: init_memory_pool(self, total_gpu_memory: int, max_num_reqs: Optional[int], max_total_tokens: Optional[int])

  L1417: init_cublas(self)
         üìù We need to run a small matmul to init cublas. Otherwise, it will raise some errors later.

  L1426: init_attention_backend(self)
         üìù Init attention kernel backend.

  L1588: init_double_sparsity_channel_config(self, selected_channel)

  L1605: init_device_graphs(self)
         üìù Capture cuda graphs.

  L1632: init_threads_binding(self)

  L1656: apply_torch_tp(self)

  L1663: forward_decode(self, forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors)
         ‚Üí LogitsProcessorOutput

  L1682: forward_extend(self, forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors)
         ‚Üí LogitsProcessorOutput

  L1705: forward_idle(self, forward_batch: ForwardBatch, pp_proxy_tensors)
         ‚Üí LogitsProcessorOutput

  L1718: forward_split_prefill(self, forward_batch: ForwardBatch, reinit_attn_backend: bool, forward_count: int)
         ‚Üí LogitsProcessorOutput

  L1739: forward(self, forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors: Optional[PPProxyTensors], reinit_attn_backend: bool, split_forward_count: int)
         ‚Üí Tuple[Union[LogitsProcessorOutput, PPProxyTensors], bool]

  L1833: sample(self, logits_output: LogitsProcessorOutput, forward_batch: ForwardBatch)
         ‚Üí torch.Tensor
         üìù Sample and compute logprobs and update logits_output.
            Args:
            logits_output: The logits output from the model forward
            forward_batch: The forward batch that generates logits_output
            Returns:
            A list of next_token_ids

  L1867: model_is_mrope(self)
         ‚Üí bool
         üìù Detect if the model has "mrope" rope_scaling type.
            mrope requires keep "rope_deltas" between prompt and decoding phases.

  L1876: save_remote_model(self, url: str)

  L1882: save_sharded_model(self, path: str, pattern: Optional[str], max_size: Optional[int])


CLASS: RankZeroFilter
----------------------------------------
  L 149: __init__(self, is_rank_zero)

  L 153: filter(self, record)


============================================================
FILE: python/sglang/srt/model_executor/npu_graph_runner.py
Functions: 2
============================================================


CLASS: NPUGraphRunner
----------------------------------------
  L  38: __init__(self, model_runner: ModelRunner)

  L  62: replay(self, forward_batch: ForwardBatch, skip_attn_backend_init: bool, pp_proxy_tensors: Optional[PPProxyTensors])
         ‚Üí Union[LogitsProcessorOutput, PPProxyTensors]
