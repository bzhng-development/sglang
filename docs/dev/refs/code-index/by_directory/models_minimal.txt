
# python/sglang/srt/models/arcee.py
  ArceeMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix, reduce_results)
  ArceeMLP.forward(x, forward_batch)
  ArceeAttention.__init__(config, hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], rope_is_neox_style, max_position_embeddings, quant_config, prefix, bias)
  ArceeAttention.forward(positions, hidden_states, forward_batch)
  ArceeDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  ArceeDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  ArceeModel.__init__(config, quant_config, prefix)
  ArceeModel.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  ArceeModel.load_kv_cache_scales(quantization_param_path)
  ArceeForCausalLM.__init__(config, quant_config, prefix)
  ArceeForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, get_embedding, pp_proxy_tensors)
  ArceeForCausalLM.start_layer()
  ArceeForCausalLM.end_layer()
  ArceeForCausalLM.get_input_embeddings()
  ArceeForCausalLM.load_weights(weights, torch.Tensor]])
  ArceeForCausalLM.load_kv_cache_scales(quantization_param_path)

# python/sglang/srt/models/baichuan.py
  BaiChuanMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  BaiChuanMLP.forward(x)
  BaiChuanAttention.__init__(hidden_size, num_heads, position_embedding, rope_theta, max_position_embeddings, quant_config, layer_id, prefix)
  BaiChuanAttention.forward(positions, hidden_states, forward_batch)
  BaiChuanDecoderLayer.__init__(config, position_embedding, layer_id, quant_config, prefix)
  BaiChuanDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  BaiChuanModel.__init__(config, position_embedding, quant_config, prefix)
  BaiChuanModel.forward(input_ids, positions, forward_batch)
  BaiChuanBaseForCausalLM.__init__(config, position_embedding, quant_config, prefix)
  BaiChuanBaseForCausalLM.forward(input_ids, positions, forward_batch)
  BaiChuanBaseForCausalLM.load_weights(weights, torch.Tensor]])
  BaichuanForCausalLM.__init__(config, quant_config, prefix)

# python/sglang/srt/models/bailing_moe.py
  BailingAttention.__init__(config, layer_id, quant_config, prefix)
  BailingAttention.forward(hidden_states, position_ids, forward_batch)
  BailingMLP.__init__(intermediate_size, config, quant_config, reduce_results, prefix)
  BailingMLP.forward(x)
  BailingMoE.__init__(config, layer_id, quant_config, prefix)
  BailingMoE.forward(hidden_states)
  BailingMoeBlock.__init__(config, layer_id, quant_config, prefix)
  BailingMoeBlock.forward(hidden_states, position_ids, residual, forward_batch)
  BailingMoeModel.__init__(config, quant_config, prefix)
  BailingMoeModel.forward(input_ids, position_ids, forward_batch, input_embeds)
  BailingMoeForCausalLM.__init__(config, quant_config)
  BailingMoeForCausalLM.forward(input_ids, positions, forward_batch, inputs_embeds)
  BailingMoeForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/bert.py
  BertEmbedding.__init__(config)
  BertEmbedding.forward(input_ids, positions, forward_batch)
  BertPooler.__init__(config)
  BertPooler.forward(hidden_states, forward_batch)
  BertEncoder.__init__(config, quant_config, prefix)
  BertEncoder.forward(hidden_states, forward_batch)
  BertLayer.__init__(config, layer_id, quant_config, prefix)
  BertLayer.forward(hidden_states, forward_batch)
  BertAttention.__init__(hidden_size, num_attention_heads, layer_norm_eps, layer_id, quant_config, prefix)
  BertAttention.forward(hidden_states, forward_batch)
  BertSelfAttention.__init__(hidden_size, num_attention_heads, layer_id, quant_config, prefix)
  BertSelfAttention.forward(hidden_states, forward_batch)
  BertSelfOutput.__init__(hidden_size, layer_norm_eps, quant_config, prefix)
  BertSelfOutput.forward(hidden_states, input_tensor)
  BertIntermediate.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  BertIntermediate.forward(hidden_states)
  BertOutput.__init__(hidden_size, intermediate_size, layer_norm_eps, quant_config, prefix)
  BertOutput.forward(hidden_states, input_tensor)
  BertModel.__init__()
  BertModel.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  BertModel.load_weights(weights, torch.Tensor]])
  BertForSequenceClassification.__init__()
  BertForSequenceClassification.load_weights(weights, torch.Tensor]])
  BertForSequenceClassification.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)

# python/sglang/srt/models/chatglm.py
  GLMAttention.__init__(config, layer_id, quant_config, prefix)
  GLMAttention.forward(hidden_states, position_ids, forward_batch)
  GLMMLP.__init__(config, quant_config, prefix)
  GLMMLP.forward(hidden_states)
  GLMBlock.__init__(config, layer_id, quant_config, prefix)
  GLMBlock.forward(hidden_states, position_ids, forward_batch)
  GLMTransformer.__init__(config, quant_config, prefix)
  GLMTransformer.forward(hidden_states, position_ids, forward_batch)
  ChatGLMM.__init__(config, quant_config, prefix)
  ChatGLMM.forward(input_ids, position_ids, forward_batch)
  ChatGLMForCausalLM.__init__(config, quant_config, prefix)
  ChatGLMForCausalLM.forward(input_ids, positions, forward_batch)
  ChatGLMForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/clip.py
  CLIPVisionEmbeddings.__init__(config)
  CLIPVisionEmbeddings.forward(pixel_values)
  CLIPTextEmbeddings.__init__(config)
  CLIPTextEmbeddings.forward(input_ids, position_ids, inputs_embeds)
  CLIPMLP.__init__(config, act_layer, quant_config, prefix)
  CLIPMLP.forward(x)
  CLIPEncoderLayer.__init__(config, act_layer, norm_layer, attn_implementation, quant_config, prefix)
  CLIPEncoderLayer.forward(hidden_states, attention_mask, causal_attention_mask)
  CLIPEncoder.__init__(config, quant_config, prefix)
  CLIPEncoder.forward(inputs_embeds, attention_mask, causal_attention_mask, return_all_hidden_states)
  CLIPTextTransformer.__init__(config, quant_config, prefix)
  CLIPTextTransformer.device()
  CLIPTextTransformer.forward(input_ids, attention_mask, position_ids)
  CLIPTextModel.__init__(config, quant_config, prefix)
  CLIPTextModel.forward(input_ids, position_ids)
  CLIPVisionTransformer.__init__(config, quant_config, prefix)
  CLIPVisionTransformer.device()
  CLIPVisionTransformer.forward(pixel_values)
  CLIPVisionModel.__init__(config, quant_config, prefix)
  CLIPVisionModel.device()
  CLIPVisionModel.forward(pixel_values)
  CLIPModel.__init__(config, quant_config, prefix)
  CLIPModel.forward(input_ids, positions, forward_batch, get_embedding)
  CLIPModel.pad_input_ids(input_ids, image_inputs)
  CLIPModel.load_weights(weights, torch.Tensor]])
monkey_patch_weight_loader()

# python/sglang/srt/models/commandr.py
layer_norm_func(hidden_states, weight, variance_epsilon)
  LayerNorm.__init__(param_shape, eps)
  LayerNorm.forward(hidden_states, residuals)
  LayerNorm.weight_loader(param, loaded_weight)
  CohereMLP.__init__(config, quant_config, prefix)
  CohereMLP.forward(x)
  CohereAttention.__init__(config, layer_id, quant_config, prefix)
  CohereAttention.forward(positions, hidden_states, forward_batch)
  CohereDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  CohereDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  CohereModel.__init__(config, quant_config, prefix)
  CohereModel.forward(input_ids, positions, forward_batch)
  CohereForCausalLM.__init__(config, quant_config, prefix)
  CohereForCausalLM.forward(input_ids, positions, forward_batch)
  CohereForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/dbrx.py
  DbrxRouter.__init__(config, params_dtype, prefix)
  DbrxRouter.forward(hidden_states)
  DbrxExperts.__init__(config, quant_config, params_dtype, prefix)
  DbrxExperts.weight_loader(param, loaded_weight, weight_name)
  DbrxExperts.forward(hidden_states)
  DbrxAttention.__init__(config, layer_id, quant_config, prefix)
  DbrxAttention.forward(position_ids, hidden_states, forward_batch)
  DbrxFusedNormAttention.__init__(config, layer_id, quant_config, prefix)
  DbrxFusedNormAttention.forward(position_ids, hidden_states, forward_batch)
  DbrxBlock.__init__(config, layer_id, quant_config, prefix)
  DbrxBlock.forward(position_ids, hidden_states, forward_batch)
  DbrxModel.__init__(config, quant_config, prefix)
  DbrxModel.forward(input_ids, position_ids, forward_batch, input_embeds)
  DbrxForCausalLM.__init__(config, quant_config, prefix)
  DbrxForCausalLM.forward(input_ids, positions, forward_batch)
  DbrxForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/deepseek.py
  DeepseekMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, reduce_results, prefix)
  DeepseekMLP.forward(x)
  DeepseekMoE.__init__(config, quant_config, prefix)
  DeepseekMoE.pack_params()
  DeepseekMoE.forward(hidden_states)
  DeepseekAttention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, prefix)
  DeepseekAttention.forward(positions, hidden_states, forward_batch)
  DeepseekDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  DeepseekDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  DeepseekModel.__init__(config, quant_config, prefix)
  DeepseekModel.forward(input_ids, positions, forward_batch, input_embeds)
  DeepseekForCausalLM.__init__(config, quant_config, prefix)
  DeepseekForCausalLM.get_input_embeddings()
  DeepseekForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  DeepseekForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/deepseek_janus_pro.py
named_apply(fn, module, name, depth_first, include_root)
VQ_16()
trunc_normal_tf_(tensor, mean, std, a, b)
nchw_to(x, fmt)
resample_patch_embed(patch_embed, new_size, interpolation, antialias, verbose)
  PatchEmbed.__init__(img_size, patch_size, in_chans, embed_dim, norm_layer, flatten, output_fmt, bias, strict_img_size, dynamic_img_pad)
  PatchEmbed.set_input_size(img_size, Tuple[int, int]]], patch_size, Tuple[int, int]]])
  PatchEmbed.feat_ratio(as_scalar)
  PatchEmbed.dynamic_feat_size(img_size, int])
  PatchEmbed.forward(x)
  Mlp.__init__(in_features, hidden_features, out_features, act_layer, norm_layer, bias, drop, use_conv)
  Mlp.forward(x)
drop_path(x, drop_prob, training, scale_by_keep)
  DropPath.__init__(drop_prob, scale_by_keep)
  DropPath.forward(x)
  DropPath.extra_repr()
  VisionTransformerBlock.__init__(dim, num_heads, mlp_ratio, qkv_bias, qk_norm, proj_drop, attn_drop, init_values, drop_path, act_layer, norm_layer, mlp_layer)
  VisionTransformerBlock.forward(x)
  PatchDropout.__init__(prob, num_prefix_tokens, ordered, return_indices)
  PatchDropout.forward(x)
resample_abs_pos_embed(posemb, new_size, old_size, num_prefix_tokens, interpolation, antialias, verbose)
init_weights()
init_weights_vit_timm(module, name)
  VisionTransformer.__init__(img_size, Tuple[int, int]], patch_size, Tuple[int, int]], in_chans, num_classes, global_pool, 'avg', 'token', 'map'], embed_dim, depth, num_heads, mlp_ratio, qkv_bias, qk_norm, init_values, class_token, no_embed_class, reg_tokens, pre_norm, fc_norm, dynamic_img_size, dynamic_img_pad, drop_rate, pos_drop_rate, patch_drop_rate, proj_drop_rate, attn_drop_rate, drop_path_rate, weight_init, 'jax', 'jax_nlhb', 'moco', ''], embed_layer, _norm_layer, _act_layer, block_fn, mlp_layer, ignore_head)
  VisionTransformer.init_weights(mode, 'jax_nlhb', 'moco', ''])
  VisionTransformer.no_weight_decay()
  VisionTransformer.group_matcher(coarse)
  VisionTransformer.get_classifier()
  VisionTransformer.reset_classifier(num_classes, global_pool)
  VisionTransformer.forward_features(x)
  VisionTransformer.forward_head(x, pre_logits)
  VisionTransformer.forward(x)
model_name_to_cls(cls_name)
  vision_head.__init__(params)
  vision_head.forward(x)
create_siglip_vit(model_name, image_size, select_layer, ckpt_path)
  Normalize.__init__(mean, std, inplace)
  Normalize.forward(tensor)
  Normalize.__repr__()
  CLIPVisionTower.__init__(model_name, image_size, int], int], select_feature, select_layer, select_layers, ckpt_path, pixel_mean, pixel_std)
  CLIPVisionTower.device()
  CLIPVisionTower.dtype()
  CLIPVisionTower.build_vision_tower(vision_tower_params)
  CLIPVisionTower.feature_select(image_forward_outs)
  CLIPVisionTower.forward(images)
  MlpProjector.__init__(cfg)
  MlpProjector.forward(x_or_tuple, torch.Tensor], torch.Tensor])
  LayerScale.__init__(dim, init_values, inplace)
  LayerScale.forward(x)
use_fused_attn(experimental)
  AttentionPoolLatent.__init__(in_features, out_features, embed_dim, num_heads, feat_size, mlp_ratio, qkv_bias, qk_norm, latent_len, latent_dim, pos_embed, pool_type, norm_layer, drop)
  AttentionPoolLatent.init_weights()
  AttentionPoolLatent.forward(x)
  Encoder.__init__(in_channels, ch, ch_mult, num_res_blocks, norm_type, dropout, resamp_with_conv, z_channels)
  Encoder.forward(x)
  Decoder.__init__(z_channels, ch, ch_mult, num_res_blocks, norm_type, dropout, resamp_with_conv, out_channels)
  Decoder.last_layer()
  Decoder.forward(z)
  VectorQuantizer.__init__(n_e, e_dim, beta, entropy_loss_ratio, l2_norm, show_usage)
  VectorQuantizer.forward(z)
  VectorQuantizer.get_codebook_entry(indices, shape, channel_first)
  ResnetBlock.__init__(in_channels, out_channels, conv_shortcut, dropout, norm_type)
  ResnetBlock.forward(x)
  AttnBlock.__init__(in_channels, norm_type)
  AttnBlock.forward(x)
nonlinearity(x)
Normalize(in_channels, norm_type)
  Upsample.__init__(in_channels, with_conv)
  Upsample.forward(x)
  Downsample.__init__(in_channels, with_conv)
  Downsample.forward(x)
compute_entropy_loss(affinity, loss_type, temperature)
  VQModel.__init__(config)
  VQModel.encode(x)
  VQModel.decode(quant)
  VQModel.decode_code(code_b, shape, channel_first)
  VQModel.forward(input)
  MultiModalityCausalLM.__init__(config, quant_config)
  MultiModalityCausalLM.get_image_feature(items)
  MultiModalityCausalLM.get_input_embeddings()
  MultiModalityCausalLM.forward(input_ids, positions, forward_batch, get_embedding)
  MultiModalityCausalLM.prepare_gen_img_embeds(image_ids)
  MultiModalityCausalLM.pad_input_ids(input_ids, image_inputs)
  MultiModalityCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/deepseek_nextn.py
  DeepseekModelNextN.__init__(config, quant_config, prefix)
  DeepseekModelNextN.forward(input_ids, positions, forward_batch, input_embeds)
  DeepseekV3ForCausalLMNextN.__init__(config, quant_config, prefix)
  DeepseekV3ForCausalLMNextN.forward(input_ids, positions, forward_batch)
  DeepseekV3ForCausalLMNextN.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/deepseek_v2.py
  DeepseekV2MLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, reduce_results, prefix, tp_rank, tp_size)
  DeepseekV2MLP.forward(x, forward_batch, should_allreduce_fusion, use_reduce_scatter)
  MoEGate.__init__(config, prefix, is_nextn)
  MoEGate.forward(hidden_states)
  DeepseekV2MoE.__init__(config, layer_id, quant_config, prefix, alt_stream, is_nextn)
  DeepseekV2MoE.get_moe_weights()
  DeepseekV2MoE.forward(hidden_states, forward_batch, should_allreduce_fusion, use_reduce_scatter)
  DeepseekV2MoE.forward_normal_dual_stream(hidden_states, should_allreduce_fusion, use_reduce_scatter)
  DeepseekV2MoE.forward_normal(hidden_states, should_allreduce_fusion, use_reduce_scatter)
  DeepseekV2MoE.forward_cpu(hidden_states, should_allreduce_fusion)
  DeepseekV2MoE.forward_deepep(hidden_states, forward_batch)
  DeepseekV2MoE.op_gate(state)
  DeepseekV2MoE.op_shared_experts(state)
  DeepseekV2MoE.op_select_experts(state)
  DeepseekV2MoE.op_dispatch_a(state)
  DeepseekV2MoE.op_dispatch_b(state)
  DeepseekV2MoE.op_experts(state)
  DeepseekV2MoE.op_combine_a(state)
  DeepseekV2MoE.op_combine_b(state)
  DeepseekV2MoE.op_output(state)
yarn_get_mscale(scale, mscale)
  DeepseekV2AttentionMLA.__init__(config, hidden_size, num_heads, qk_nope_head_dim, qk_rope_head_dim, v_head_dim, q_lora_rank, kv_lora_rank, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, reduce_results, layer_id, prefix, alt_stream)
  DeepseekV2AttentionMLA.dispatch_attn_forward_method(forward_batch)
  DeepseekV2AttentionMLA.op_prepare(state)
  DeepseekV2AttentionMLA.op_core(state)
  DeepseekV2AttentionMLA.forward(positions, hidden_states, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_prepare(positions, hidden_states, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_core(intermediate_state)
  DeepseekV2AttentionMLA.forward_normal_prepare(positions, hidden_states, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_normal_core(q, k, v, forward_batch)
  DeepseekV2AttentionMLA.forward_absorb_prepare(positions, hidden_states, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_absorb_core(q_pe, k_pe, q_nope_out, k_nope, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_prepare(positions, hidden_states, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_cpu_prepare(positions, hidden_states, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_core(q_input, key_cache_buf, val_cache_buf, attn_output, kv_indptr, kv_indices, k_pe_output, cos_sin_cache, positions, attn_logits, num_kv_split, sm_scale, enable_rope_fusion, k_input, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_absorb_fused_mla_rope_cpu_core(q_input, k_input, v_input, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_normal_chunked_kv_prepare(positions, hidden_states, forward_batch, zero_allocator)
  DeepseekV2AttentionMLA.forward_normal_chunked_kv_core(q, k, v, forward_batch)
  DeepseekV2DecoderLayer.__init__(config, layer_id, quant_config, is_nextn, prefix, alt_stream)
  DeepseekV2DecoderLayer.forward(positions, hidden_states, forward_batch, residual, zero_allocator)
  DeepseekV2DecoderLayer.op_comm_prepare_attn(state, positions, hidden_states, forward_batch, residual, zero_allocator, tbo_subbatch_index)
  DeepseekV2DecoderLayer.op_comm_prepare_mlp(state)
  DeepseekV2DecoderLayer.op_mlp(state)
  DeepseekV2DecoderLayer.op_comm_postprocess_layer(state)
  DeepseekV2Model.__init__(config, quant_config, prefix)
  DeepseekV2Model.get_input_embeddings()
  DeepseekV2Model.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  DeepseekV2ForCausalLM.__init__(config, quant_config, prefix)
  DeepseekV2ForCausalLM.routed_experts_weights_of_layer()
  DeepseekV2ForCausalLM.determine_num_fused_shared_experts(architecture)
  DeepseekV2ForCausalLM.get_input_embeddings()
  DeepseekV2ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  DeepseekV2ForCausalLM.start_layer()
  DeepseekV2ForCausalLM.end_layer()
  DeepseekV2ForCausalLM.post_load_weights(is_nextn, weight_names)
  DeepseekV2ForCausalLM.load_weights(weights, torch.Tensor]], is_nextn)
  DeepseekV2ForCausalLM.get_embed_and_head()
  DeepseekV2ForCausalLM.set_embed_and_head(embed, head)
  DeepseekV2ForCausalLM.get_model_config_for_expert_location(cls, config)

# python/sglang/srt/models/deepseek_vl2.py
  DeepseekVL2MlpProjector.__init__(config, quant_config)
  DeepseekVL2MlpProjector.forward(x)
  DeepseekVL2ForCausalLM.__init__(config, quant_config)
  DeepseekVL2ForCausalLM.forward(input_ids, positions, forward_batch)
  DeepseekVL2ForCausalLM.load_weights(weights, torch.Tensor]])
  DeepseekVL2ForCausalLM.pad_input_ids(input_ids, mm_inputs)
  DeepseekVL2ForCausalLM.get_image_feature(items)

# python/sglang/srt/models/ernie4.py
  MoEGate.__init__(config, prefix)
  MoEGate.forward(hidden_states)
  Ernie4Moe.__init__(config, layer_id, quant_config, prefix)
  Ernie4Moe.forward(hidden_states)
  Ernie4Moe.forward_normal(hidden_states)
  Ernie4DecoderLayer.__init__(config, layer_id, quant_config, prefix, is_mtp)
  Ernie4DecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  Ernie4Model.__init__(config, quant_config, prefix)
  Ernie4Model.forward(input_ids, positions, forward_batch, input_embeds)
  Ernie4_5_ForCausalLM.__init__(config, quant_config, prefix)
  Ernie4_5_ForCausalLM.forward(input_ids, positions, forward_batch)
  Ernie4_5_ForCausalLM.load_weights(weights, torch.Tensor]])
  Ernie4_5_ForCausalLM.get_embed_and_head()
  Ernie4_5_MoeForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/ernie4_eagle.py
  Ernie4ModelMTP.__init__(config, layer_id, prefix, quant_config)
  Ernie4ModelMTP.forward(input_ids, positions, forward_batch, input_embeds)
  Ernie4_5_MoeForCausalLMMTP.__init__(config, quant_config, prefix, mtp_layer_id)
  Ernie4_5_MoeForCausalLMMTP.forward(input_ids, positions, forward_batch)
  Ernie4_5_MoeForCausalLMMTP.load_weights(weights, torch.Tensor]])
  Ernie4_5_MoeForCausalLMMTP.get_embed_and_head()
  Ernie4_5_MoeForCausalLMMTP.set_embed_and_head(embed, head)

# python/sglang/srt/models/exaone.py
  ExaoneGatedMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  ExaoneGatedMLP.forward(x)
  ExaoneAttention.__init__(config, hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], rope_is_neox_style, max_position_embeddings, quant_config, prefix)
  ExaoneAttention.forward(positions, hidden_states, forward_batch)
  ExaoneDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  ExaoneDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  ExaoneModel.__init__(config, quant_config, prefix)
  ExaoneModel.forward(input_ids, positions, forward_batch, input_embeds)
  ExaoneForCausalLM.__init__(config, quant_config, prefix)
  ExaoneForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  ExaoneForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/gemma.py
  GemmaMLP.__init__(hidden_size, intermediate_size, quant_config, prefix)
  GemmaMLP.forward(x)
  GemmaAttention.__init__(hidden_size, num_heads, num_kv_heads, head_dim, layer_id, max_position_embeddings, rope_theta, quant_config, prefix)
  GemmaAttention.forward(positions, hidden_states, forward_batch)
  GemmaDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  GemmaDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  GemmaModel.__init__(config, quant_config, prefix)
  GemmaModel.forward(input_ids, positions, forward_batch, input_embeds)
  GemmaForCausalLM.__init__(config, quant_config, prefix)
  GemmaForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  GemmaForCausalLM.forward_split_prefill(input_ids, positions, forward_batch, split_interval, int], input_embeds)
  GemmaForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/gemma2.py
get_attention_sliding_window_size(config)
  Gemma2MLP.__init__(hidden_size, intermediate_size, hidden_act, hidden_activation, quant_config, prefix)
  Gemma2MLP.forward(x)
  Gemma2Attention.__init__(layer_id, config, hidden_size, num_heads, num_kv_heads, head_dim, max_position_embeddings, rope_theta, quant_config, prefix)
  Gemma2Attention.forward(positions, hidden_states, forward_batch)
  Gemma2DecoderLayer.__init__(layer_id, config, quant_config, prefix)
  Gemma2DecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  Gemma2Model.__init__(config, quant_config, prefix)
  Gemma2Model.forward(input_ids, positions, forward_batch, input_embeds)
  Gemma2ForCausalLM.__init__(config, quant_config, prefix)
  Gemma2ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  Gemma2ForCausalLM.forward_split_prefill(input_ids, positions, forward_batch, split_interval, int], input_embeds)
  Gemma2ForCausalLM.get_attention_sliding_window_size()
  Gemma2ForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/gemma2_reward.py
  Gemma2ForSequenceClassification.__init__(config, quant_config, prefix)
  Gemma2ForSequenceClassification.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  Gemma2ForSequenceClassification.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/gemma3_causal.py
get_attention_sliding_window_size(config)
extract_layer_index(prefix)
  Gemma3MLP.__init__(hidden_size, intermediate_size, hidden_activation, quant_config, prefix)
  Gemma3MLP.forward(x)
  Gemma3Attention.__init__(layer_id, config, max_position_embeddings, quant_config, prefix)
  Gemma3Attention.naive_attn_with_masks(q, k, v, out)
  Gemma3Attention.forward(hidden_states, position_embeddings, torch.Tensor], forward_batch)
  Gemma3DecoderLayer.__init__(layer_id, config, quant_config, prefix)
  Gemma3DecoderLayer.forward(positions, hidden_states, position_embeddings_global, position_embeddings_local, forward_batch)
  Gemma3RotaryEmbedding.__init__(config, device)
  Gemma3RotaryEmbedding.forward(x, position_ids)
  Gemma3TextScaledWordEmbedding.__init__(num_embeddings, embedding_dim, padding_idx, embed_scale)
  Gemma3TextScaledWordEmbedding.forward(input_ids)
  Gemma3TextModel.__init__(config, quant_config, prefix)
  Gemma3TextModel.forward(input_ids, positions, forward_batch, input_embeds)
  Gemma3ForCausalLM.__init__(config, quant_config, prefix)
  Gemma3ForCausalLM.get_input_embeddings()
  Gemma3ForCausalLM.get_attention_sliding_window_size()
  Gemma3ForCausalLM.dtype()
  Gemma3ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  Gemma3ForCausalLM.forward_split_prefill(input_ids, positions, forward_batch, split_interval, int], input_embeds)
  Gemma3ForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/gemma3_mm.py
  Gemma3MultiModalProjector.__init__(config)
  Gemma3MultiModalProjector.forward(vision_outputs)
  Gemma3ForConditionalGeneration.__init__(config, quant_config, prefix)
  Gemma3ForConditionalGeneration.pad_input_ids(input_ids, image_inputs)
  Gemma3ForConditionalGeneration.prepare_attn_masks(input_ids, positions, mask_dtype)
  Gemma3ForConditionalGeneration.get_input_embeddings()
  Gemma3ForConditionalGeneration.get_attention_sliding_window_size()
  Gemma3ForConditionalGeneration.get_image_feature(items)
  Gemma3ForConditionalGeneration.forward(input_ids, positions, forward_batch, input_embeds)
  Gemma3ForConditionalGeneration.tie_weights()
  Gemma3ForConditionalGeneration.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/gemma3n_audio.py
  Gemma3nCumulativeGroupNorm.__init__(num_channels, feature_dims, eps)
  Gemma3nCumulativeGroupNorm.forward(x, mask)
  Gemma3nAudioRelativePositionEmbedding.__init__(config, quant_config, prefix)
  Gemma3nAudioRelativePositionEmbedding.forward(queries, keys)
  Gemma3nAudioAttention.__init__(config, quant_config, prefix)
  Gemma3nAudioAttention.forward(x, mask)
  Gemma3nAudioSSCPConvBlock.__init__(config, idx, input_freq_dim, manual_padding, int, int, int], quant_config, prefix)
  Gemma3nAudioSSCPConvBlock.forward(audio_encodings)
  Gemma3nAudioSubSampleConvProjection.__init__(config, quant_config, prefix)
  Gemma3nAudioSubSampleConvProjection.forward(audio_encodings)
  Gemma3nAudioConformerAttention.__init__(config, quant_config, prefix)
  Gemma3nAudioConformerAttention.forward(audio_encodings, audio_mel_mask)
  Gemma3nAudioConformerFeedForward.__init__(config, quant_config, prefix)
  Gemma3nAudioConformerFeedForward.forward(audio_encodings)
  Gemma3nAudioConformerLightConv1d.__init__(config, quant_config, prefix)
  Gemma3nAudioConformerLightConv1d.forward(audio_encodings)
  Gemma3nAudioConformerBlock.__init__(config, quant_config, prefix)
  Gemma3nAudioConformerBlock.forward(audio_encodings, audio_mel_mask)
  Gemma3nAudioEncoder.__init__(config, quant_config, prefix)
  Gemma3nAudioEncoder.forward(audio_mel, audio_mel_mask)

# python/sglang/srt/models/gemma3n_causal.py
get_attention_sliding_window_size(config)
  Gemma3nRMSNorm.__init__(dim, eps, with_scale)
  Gemma3nRMSNorm.forward(x)
  Gemma3nTextMLP.__init__(hidden_size, intermediate_size, hidden_activation, activation_sparsity, quant_config, prefix)
  Gemma3nTextMLP.forward(x)
  Gemma3nLaurelBlock.__init__(config, quant_config, prefix)
  Gemma3nLaurelBlock.forward(x)
  Gemma3nAltUp.__init__(config, quant_config, prefix)
  Gemma3nAltUp.compute_router_modalities(x)
  Gemma3nAltUp.predict(hidden_states)
  Gemma3nAltUp.correct(predictions, activated)
  Gemma3nAltUp.scale_corrected_output(corrected)
  Gemma3nAltUp.forward(hidden_states, activated)
  Gemma3nAttention.__init__(layer_id, config, max_position_embeddings, quant_config, prefix)
  Gemma3nAttention.forward(hidden_states, positions, torch.Tensor], forward_batch)
  Gemma3nDecoderLayer.__init__(layer_id, config, quant_config, prefix)
  Gemma3nDecoderLayer.forward(positions, hidden_states, per_layer_input, forward_batch)
  Gemma3nTextModel.__init__(config, quant_config, prefix)
  Gemma3nTextModel.get_input_embeddings()
  Gemma3nTextModel.dtype()
  Gemma3nTextModel.get_per_layer_inputs(input_ids)
  Gemma3nTextModel.project_per_layer_inputs(inputs_embeds, per_layer_inputs)
  Gemma3nTextModel.forward(input_ids, positions, forward_batch, input_embeds, per_layer_inputs)
  Gemma3nForCausalLM.__init__(config, quant_config, prefix)
  Gemma3nForCausalLM.get_input_embeddings()
  Gemma3nForCausalLM.get_attention_sliding_window_size()
  Gemma3nForCausalLM.dtype()
  Gemma3nForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, per_layer_inputs)
  Gemma3nForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/gemma3n_mm.py
  Gemma3nMultimodalEmbedder.__init__(multimodal_config, Gemma3nVisionConfig], text_config, quant_config, prefix)
  Gemma3nMultimodalEmbedder.forward(input_ids, inputs_embeds)
  Gemma3nForConditionalGeneration.__init__(config, quant_config, prefix)
  Gemma3nForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)
  Gemma3nForConditionalGeneration.get_input_embeddings()
  Gemma3nForConditionalGeneration.get_attention_sliding_window_size()
  Gemma3nForConditionalGeneration.get_image_feature(items)
  Gemma3nForConditionalGeneration.get_audio_feature(items)
  Gemma3nForConditionalGeneration.get_per_layer_inputs(input_ids)
  Gemma3nForConditionalGeneration.project_per_layer_inputs(inputs_embeds, per_layer_inputs)
  Gemma3nForConditionalGeneration.forward(input_ids, positions, forward_batch, input_embeds)
  Gemma3nForConditionalGeneration.tie_weights()
  Gemma3nForConditionalGeneration.load_weights(weights, torch.Tensor]])
  Gemma3nForConditionalGeneration.should_apply_lora(module_name)
  Gemma3nForConditionalGeneration.get_hidden_dim(module_name)

# python/sglang/srt/models/glm4.py
  Glm4Attention.__init__(config, layer_id, quant_config, prefix)
  Glm4Attention.forward(positions, hidden_states, forward_batch)
  Glm4DecoderLayer.__init__(config, layer_id, quant_config, prefix)
  Glm4DecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  Glm4Model.__init__(config, quant_config, prefix)
  Glm4Model.get_input_embeddings()
  Glm4Model.dtype()
  Glm4Model.forward(input_ids, positions, forward_batch, input_embeds)
  Glm4ForCausalLM.__init__(config, quant_config, prefix)
  Glm4ForCausalLM.forward(input_ids, positions, forward_batch)
  Glm4ForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/glm4_moe.py
  Glm4MoeMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, reduce_results, prefix, tp_rank, tp_size)
  Glm4MoeMLP.forward(x, forward_batch, should_allreduce_fusion)
  Glm4MoeAttention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, partial_rotary_factor, rope_scaling, Any]], max_position_embeddings, head_dim, rms_norm_eps, attention_bias, quant_config, use_qk_norm, prefix, alt_stream)
  Glm4MoeAttention.op_prepare(state)
  Glm4MoeAttention.op_core(state)
  Glm4MoeAttention.forward_prepare(positions, hidden_states, forward_batch)
  Glm4MoeAttention.forward_core(intermediate_state)
  Glm4MoeAttention.forward(positions, hidden_states, forward_batch)
  Glm4MoeGate.__init__(config, prefix, is_nextn)
  Glm4MoeGate.forward(hidden_states)
  Glm4MoeSparseMoeBlock.__init__(config, layer_id, quant_config, prefix, alt_stream, is_nextn)
  Glm4MoeSparseMoeBlock.forward_normal_dual_stream(hidden_states, should_allreduce_fusion, use_reduce_scatter)
  Glm4MoeSparseMoeBlock.forward_normal(hidden_states, should_allreduce_fusion, use_reduce_scatter)
  Glm4MoeDecoderLayer.__init__(config, layer_id, quant_config, is_nextn, prefix, alt_stream)
  Glm4MoeDecoderLayer.forward(positions, hidden_states, forward_batch, residual, zero_allocator)
  Glm4MoeModel.__init__(config, quant_config, prefix)
  Glm4MoeForCausalLM.__init__(config, quant_config, prefix)
  Glm4MoeForCausalLM.determine_num_fused_shared_experts(architecture)
  Glm4MoeForCausalLM.get_input_embeddings()
  Glm4MoeForCausalLM.load_weights(weights, torch.Tensor]], is_nextn)

# python/sglang/srt/models/glm4_moe_nextn.py
  Glm4MoeModelNextN.__init__(config, quant_config, prefix)
  Glm4MoeModelNextN.forward(input_ids, positions, forward_batch, input_embeds)
  Glm4MoeForCausalLMNextN.__init__(config, quant_config, prefix)
  Glm4MoeForCausalLMNextN.forward(input_ids, positions, forward_batch)
  Glm4MoeForCausalLMNextN.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/glm4v.py
  Glm4vRMSNorm.forward(x)
  Glm4vVisionMLP.__init__(in_features, hidden_features, bias, quant_config, prefix)
  Glm4vVisionMLP.forward(x)
  Glm4vVisionBlock.__init__(config, norm_layer, quant_config, prefix)
  Glm4vVisionPatchEmbed.__init__(patch_size, temporal_patch_size, in_channels, hidden_size)
  Glm4vVisionPatchEmbed.forward(x)
  Glm4vPatchMerger.__init__(d_model, context_dim, quant_config, bias, prefix)
  Glm4vPatchMerger.forward(x)
  Glm4vVisionEmbeddings.__init__(config)
  Glm4vVisionEmbeddings.forward(embeddings, lengths, image_shapes, h_coords, w_coords)
  Glm4vVisionRotaryEmbedding.__init__(dim, theta)
  Glm4vVisionRotaryEmbedding.update_freqs_cache(seqlen)
  Glm4vVisionRotaryEmbedding.forward(seqlen)
  Glm4vVisionModel.__init__(vision_config, norm_eps, quant_config, prefix)
  Glm4vVisionModel.dtype()
  Glm4vVisionModel.device()
  Glm4vVisionModel.rot_pos_emb(grid_thw)
  Glm4vVisionModel.forward(x, grid_thw)
  Glm4vForConditionalGeneration.__init__(config, quant_config, prefix)
  Glm4vForConditionalGeneration.get_image_feature(items)
  Glm4vForConditionalGeneration.get_video_feature(items)
  Glm4vForConditionalGeneration.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/glm4v_moe.py
  Glm4vMoeForConditionalGeneration.__init__(config, quant_config, prefix)
  Glm4vMoeForConditionalGeneration.determine_num_fused_shared_experts(architecture)
  Glm4vMoeForConditionalGeneration.load_weights(weights, torch.Tensor]], is_nextn)

# python/sglang/srt/models/gpt2.py
  GPT2Attention.__init__(layer_id, config, quant_config, prefix)
  GPT2Attention.forward(hidden_states, forward_batch)
  GPT2MLP.__init__(intermediate_size, config, act_layer, quant_config, prefix)
  GPT2MLP.forward(hidden_states)
  GPT2Block.__init__(layer_id, config, act_layer, quant_config, prefix)
  GPT2Block.forward(hidden_states, forward_batch)
  GPT2Model.__init__(config, quant_config, prefix)
  GPT2Model.forward(input_ids, position_ids, forward_batch)
  GPT2LMHeadModel.__init__(config, quant_config, prefix)
  GPT2LMHeadModel.forward(input_ids, positions, forward_batch)
  GPT2LMHeadModel.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/gpt_bigcode.py
  GPTBigCodeAttention.__init__(layer_id, config, quant_config, prefix)
  GPTBigCodeAttention.forward(hidden_states, forward_batch)
  GPTBigMLP.__init__(intermediate_size, config, quant_config, prefix)
  GPTBigMLP.forward(hidden_states)
  GPTBigCodeBlock.__init__(layer_id, config, quant_config, prefix)
  GPTBigCodeBlock.forward(hidden_states, forward_batch)
  GPTBigCodeModel.__init__(config, quant_config, prefix)
  GPTBigCodeModel.forward(input_ids, position_ids, forward_batch)
  GPTBigCodeForCausalLM.__init__(config, quant_config, prefix)
  GPTBigCodeForCausalLM.forward(input_ids, positions, forward_batch)
  GPTBigCodeForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/gpt_oss.py
  GptOssConfig.__init__()
get_attention_sliding_window_size(config)
  GptOssSparseMoeBlock.__init__(layer_id, config, quant_config, prefix)
  GptOssSparseMoeBlock.forward(hidden_states, forward_batch, should_allreduce_fusion)
  GptOssSparseMoeBlock.get_moe_weights()
  GptOssSparseMoeBlock.forward_normal(hidden_states, should_allreduce_fusion)
  GptOssAttention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], max_position_embeddings, head_dim, rms_norm_eps, attention_bias, quant_config, prefix, sliding_window_size, layer_type, params_dtype)
  GptOssAttention.forward_prepare(positions, hidden_states, forward_batch)
  GptOssAttention.forward_core(intermediate_state)
  GptOssAttention.forward(positions, hidden_states, forward_batch)
  GptOssDecoderLayer.__init__(config, layer_id, quant_config, prefix, sliding_window_size)
  GptOssDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  GptOssModel.__init__(config, quant_config, prefix, decoder_layer_type)
  GptOssModel.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  GptOssForCausalLM.__init__(config, quant_config, prefix)
  GptOssForCausalLM.routed_experts_weights_of_layer()
  GptOssForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  GptOssForCausalLM.start_layer()
  GptOssForCausalLM.end_layer()
  GptOssForCausalLM.load_weights(weights, torch.Tensor]], is_nextn, weight_name_mapping)
  GptOssForCausalLM.get_embed_and_head()
  GptOssForCausalLM.set_embed_and_head(embed, head)
  GptOssForCausalLM.set_eagle3_layers_to_capture(layer_ids)
  GptOssForCausalLM.get_model_config_for_expert_location(cls, config)
  GptOssForCausalLM.get_attention_sliding_window_size()
  _WeightCreator.__init__(fn)
  _WeightCreator.maybe_materialize(obj)

# python/sglang/srt/models/granite.py
  GraniteMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  GraniteMLP.forward(x)
  GraniteAttention.__init__(config, hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], rope_is_neox_style, max_position_embeddings, quant_config, prefix)
  GraniteAttention.forward(positions, hidden_states, forward_batch)
  GraniteDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  GraniteDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  GraniteModel.__init__(config, quant_config, prefix)
  GraniteModel.forward(input_ids, positions, forward_batch, input_embeds)
  GraniteForCausalLM.__init__(config, quant_config, prefix)
  GraniteForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  GraniteForCausalLM.get_module_name_from_weight_name(name)
  GraniteForCausalLM.get_num_params()
  GraniteForCausalLM.load_weights(weights, torch.Tensor]])
  GraniteForCausalLM.get_weights_by_name(name, truncate_size, tp_size)

# python/sglang/srt/models/granitemoe.py
  GraniteMoeMoE.__init__(num_experts, top_k, hidden_size, intermediate_size, layer_id, params_dtype, quant_config, tp_size, prefix)
  GraniteMoeMoE.forward(hidden_states)
  GraniteMoeAttention.__init__(hidden_size, num_heads, num_kv_heads, max_position, layer_id, rope_theta, quant_config, attention_multiplier, prefix)
  GraniteMoeAttention.forward(positions, hidden_states, forward_batch)
  GraniteMoeDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  GraniteMoeDecoderLayer.forward(positions, hidden_states, forward_batch)
  GraniteMoeModel.__init__(config, quant_config, prefix)
  GraniteMoeModel.get_input_embeddings(input_ids)
  GraniteMoeModel.forward(input_ids, positions, forward_batch, inputs_embeds)
  GraniteMoeForCausalLM.__init__(config, quant_config, prefix)
  GraniteMoeForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  GraniteMoeForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/grok.py
  Grok1MLP.__init__(hidden_size, intermediate_size, layer_id, quant_config, prefix, reduce_results, use_presharded_weights, split_gate_up)
  Grok1MLP.forward(x)
  Grok1MoE.__init__(config, layer_id, num_experts, top_k, hidden_size, intermediate_size, params_dtype, quant_config, tp_size, reduce_results, use_presharded_weights, inplace, no_combine, prefix)
  Grok1MoE.forward(hidden_states)
get_rope_scaling(config)
  ScalingRotaryEmbedding.__init__(head_size, rotary_dim, max_position_embeddings, base, is_neox_style, scaling_factor, dtype)
  Grok1Attention.__init__(config, hidden_size, num_heads, num_kv_heads, layer_id, max_position, rope_theta, quant_config, reduce_results, alt_stream, load_presharded_attn, prefix)
  Grok1Attention.forward(positions, hidden_states, forward_batch)
  Grok1DecoderLayer.__init__(config, layer_id, quant_config, load_presharded_moe, load_presharded_attn, load_presharded_mlp, alt_stream, skip_moe, prefix)
  Grok1DecoderLayer.forward(positions, hidden_states, forward_batch, residual, deferred_norm)
  Grok1DecoderLayer.moe_with_rmoe(x)
  Grok1Model.__init__(config, quant_config, load_presharded_moe, load_presharded_embedding, load_presharded_attn, load_presharded_mlp, replicate_embedding, prefix)
  Grok1Model.forward(input_ids, positions, forward_batch, input_embeds)
  Grok1ForCausalLM.__init__(config, quant_config, prefix)
  Grok1ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  Grok1ForCausalLM.load_weights(weights, torch.Tensor]], ignore_parent_name, check_hit_names, model_config)
  Grok1ForCausalLM.get_num_params_analytical()
  Grok1ForCausalLM.get_num_params_torch()

# python/sglang/srt/models/hunyuan.py
  HunYuanMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, bias, prefix, reduce_results)
  HunYuanMLP.forward(x)
  HunYuanSparseMoeBlock.__init__(config, quant_config, layer_id)
  HunYuanSparseMoeBlock.forward(hidden_states)
get_head_dim(config)
check_head_dim(config)
  HunYuanAttention.__init__(config, hidden_size, num_heads, num_kv_heads, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, bias, prefix, attention_type, layer_id)
  HunYuanAttention.forward(positions, hidden_states, forward_batch, kv_states)
  HunYuanDecoderLayer.__init__(config, quant_config, prefix, layer_id)
  HunYuanDecoderLayer.forward(positions, hidden_states, forward_batch, residual, kv_states)
  HunYuanModel.__init__(config, quant_config, prefix)
  HunYuanModel.get_input_embeddings(input_ids)
  HunYuanModel.forward(input_ids, positions, forward_batch, input_embeds)
  HunYuanMoEV1ForCausalLM.__init__(config, quant_config)
  HunYuanMoEV1ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  HunYuanMoEV1ForCausalLM.load_weights(weights, torch.Tensor]])
  HunYuanMoEV1ForCausalLM.load_kv_cache_scales(quantization_param_path)

# python/sglang/srt/models/idefics2.py
  Idefics2VisionMLP.__init__(config, quant_config, prefix)
  Idefics2VisionMLP.forward(hidden_states)
  Idefics2EncoderLayer.__init__(config, quant_config, prefix)
  Idefics2EncoderLayer.forward(hidden_states, cu_seqlens)
  Idefics2Encoder.__init__(config, quant_config, prefix)
  Idefics2Encoder.forward(inputs_embeds, cu_seqlens)
  Idefics2VisionEmbeddings.__init__(config)
  Idefics2VisionEmbeddings.get_position_ids(pixel_values, patch_attention_mask, tgt_sizes)
  Idefics2VisionEmbeddings.forward(pixel_values, patch_attention_mask, tgt_sizes)
  Idefics2VisionTransformer.__init__(config, quant_config, require_post_norm, prefix)
  Idefics2VisionTransformer.get_input_embeddings()
  Idefics2VisionTransformer.compute_cu_seqlens(tgt_sizes, input_embeds)
  Idefics2VisionTransformer.forward(pixel_values, patch_attention_mask, tgt_sizes)

# python/sglang/srt/models/internlm2.py
  InternLM2MLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  InternLM2MLP.forward(x)
  InternLM2Attention.__init__(hidden_size, num_heads, num_kv_heads, rope_theta, rope_scaling, Any]], max_position_embeddings, layer_id, quant_config, prefix)
  InternLM2Attention.forward(positions, hidden_states, forward_batch)
  InternLMDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  InternLMDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  InternLM2Model.__init__(config, quant_config, prefix)
  InternLM2Model.forward(input_ids, positions, forward_batch, input_embeds)
  InternLM2ForCausalLM.__init__(config, quant_config, prefix)
  InternLM2ForCausalLM.get_input_embeddings()
  InternLM2ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  InternLM2ForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/internlm2_reward.py
  InternLM2ForRewardModel.__init__(config, quant_config, prefix)
  InternLM2ForRewardModel.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  InternLM2ForRewardModel.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/interns1.py
  InternS1ForConditionalGeneration.__init__(config, quant_config, use_flash_attn)
  InternS1ForConditionalGeneration.pixel_shuffle(x, scale_factor)
  InternS1ForConditionalGeneration.extract_feature(pixel_values)
  InternS1ForConditionalGeneration.get_image_feature(items)
  InternS1ForConditionalGeneration.forward(input_ids, positions, forward_batch, input_embeds)
  InternS1ForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)
  InternS1ForConditionalGeneration.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/internvl.py
  InternAttention.__init__(config, quant_config)
  InternAttention.forward(hidden_states, cu_seqlens)
  InternVisionEmbeddings.__init__(config)
  InternVisionEmbeddings.forward(pixel_values)
  InternRMSNorm.__init__(hidden_size, eps)
  InternRMSNorm.forward(hidden_states)
  InternMLP.__init__(config)
  InternMLP.forward(hidden_states)
  InternVisionEncoderLayer.__init__(config, drop_path_rate, quant_config)
  InternVisionEncoderLayer.forward(hidden_states, cu_seqlens)
  InternVisionEncoder.__init__(config, quant_config)
  InternVisionEncoder.forward(inputs_embeds, output_hidden_states, return_dict)
  InternVisionModel.__init__(config, quant_config)
  InternVisionModel.resize_pos_embeddings(old_size, new_size, patch_size)
  InternVisionModel.get_input_embeddings()
  InternVisionModel.forward(pixel_values, output_hidden_states, return_dict, pixel_embeds)
  InternVLChatModel.__init__(config, quant_config, use_flash_attn)
  InternVLChatModel.pixel_shuffle(x, scale_factor)
  InternVLChatModel.extract_feature(pixel_values)
  InternVLChatModel.get_image_feature(items)
  InternVLChatModel.forward(input_ids, positions, forward_batch, input_embeds)
  InternVLChatModel.pad_input_ids(input_ids, mm_inputs)
  InternVLChatModel.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/kimi_vl.py
  KimiVLMultiModalProjector.__init__(config)
  KimiVLMultiModalProjector.forward(image_features)
  KimiVLForConditionalGeneration.__init__(config, quant_config, prefix)
  KimiVLForConditionalGeneration.get_image_feature(items)
  KimiVLForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)
  KimiVLForConditionalGeneration.forward(input_ids, positions, forward_batch, get_embedding)
  KimiVLForConditionalGeneration.load_weights(weights, torch.Tensor]])
get_spec_layer_idx_from_weight_name(config, weight_name)

# python/sglang/srt/models/kimi_vl_moonvit.py
multihead_attention(q, k, v, q_cu_seqlens, k_cu_seqlens)
sdpa_attention(q, k, v, q_cu_seqlens, k_cu_seqlens)
apply_rope(xq, xk, freqs_cis)
  Learnable2DInterpPosEmb.__init__(height, width, dim, interpolation_mode)
  Learnable2DInterpPosEmb.reset_parameters()
  Learnable2DInterpPosEmb.forward(x, grid_hws)
  MoonVisionPatchEmbed.__init__(out_dim, in_dim, patch_size, Tuple[int, int]], pos_emb_height, pos_emb_width)
  MoonVisionPatchEmbed.forward(x, grid_hw)
  Rope2DPosEmb.__init__(dim, max_height, max_width, theta_base, device)
  Rope2DPosEmb.extra_repr()
  Rope2DPosEmb.precomputed_freqs_cis()
  Rope2DPosEmb.get_freqs_cis_by_seqlens(grid_hws)
  Rope2DPosEmb.get_freqs_cis_by_idx(pos_idx, pos_idx_mask)
  MLP2.__init__(dims, activation, bias)
  MLP2.forward(x)
  MoonVitEncoderLayer.__init__(num_heads, hidden_dim, mlp_dim)
  MoonVitEncoderLayer.attention_qkvpacked(x, cu_seqlens, rope_freqs_cis)
  MoonVitEncoderLayer.forward(hidden_states, cu_seqlens, rope_freqs_cis, None])
  MoonVitEncoder.__init__(hidden_dim, num_layers, block_cfg)
  MoonVitEncoder.forward(hidden_states, grid_hw)
patch_merger(x, grid_hw, merge_kernel_size, int])
  MoonVitVLProjector.__init__(in_channels, merge_kernel_size, int], hidden_act, ln_eps, out_dim)
  MoonVitVLProjector.forward(hidden_states)
  MoonVitPretrainedModel.__init__(config)
  MoonVitPretrainedModel.forward(pixel_values, grid_hw)

# python/sglang/srt/models/llama.py
  LlamaMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix, reduce_results)
  LlamaMLP.forward(x, forward_batch, use_reduce_scatter)
  LlamaAttention.__init__(config, hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], rope_is_neox_style, max_position_embeddings, quant_config, prefix, bias)
  LlamaAttention.forward(positions, hidden_states, forward_batch)
  LlamaDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  LlamaDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  LlamaModel.__init__(config, quant_config, prefix)
  LlamaModel.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  LlamaModel.load_kv_cache_scales(quantization_param_path)
  LlamaForCausalLM.__init__(config, quant_config, prefix)
  LlamaForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, get_embedding, pp_proxy_tensors)
  LlamaForCausalLM.forward_split_prefill(input_ids, positions, forward_batch, split_interval, int], input_embeds)
  LlamaForCausalLM.start_layer()
  LlamaForCausalLM.end_layer()
  LlamaForCausalLM.get_input_embeddings()
  LlamaForCausalLM.get_module_name_from_weight_name(name)
  LlamaForCausalLM.get_num_params()
  LlamaForCausalLM.load_weights(weights, torch.Tensor]])
  LlamaForCausalLM.get_weights_by_name(name, truncate_size, tp_size)
  LlamaForCausalLM.get_embed_and_head()
  LlamaForCausalLM.set_embed_and_head(embed, head)
  LlamaForCausalLM.get_embed()
  LlamaForCausalLM.set_embed(embed)
  LlamaForCausalLM.load_kv_cache_scales(quantization_param_path)
  LlamaForCausalLM.set_eagle3_layers_to_capture(layer_ids)

# python/sglang/srt/models/llama4.py
  Llama4MoE.custom_routing_function(hidden_states, gating_output, topk, renormalize)
  Llama4MoE.__init__(config, layer_id, quant_config, prefix)
  Llama4MoE.forward(hidden_states, forward_batch, use_reduce_scatter)
  Llama4Attention.__init__(config, layer_id, hidden_size, num_heads, num_kv_heads, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, bias, bias_o_proj, prefix)
  Llama4Attention.forward(positions, hidden_states, forward_batch)
  Llama4DecoderLayer.__init__(config, layer_id, quant_config, prefix)
  Llama4DecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  Llama4Model.__init__(config, quant_config, prefix)
  Llama4Model.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  Llama4ForCausalLM.__init__(config, quant_config, prefix)
  Llama4ForCausalLM.get_input_embeddings()

# python/sglang/srt/models/llama_classification.py
  LlamaForClassification.__init__(config, quant_config, prefix)
  LlamaForClassification.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  LlamaForClassification.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/llama_eagle.py
  LlamaDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  LlamaModel.__init__(config, quant_config, prefix)
  LlamaModel.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  LlamaForCausalLMEagle.__init__(config, quant_config, prefix)
  LlamaForCausalLMEagle.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/llama_eagle3.py
  LlamaDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  LlamaDecoderLayer.forward(positions, embeds, hidden_states, forward_batch, residual)
  LlamaModel.__init__(config, quant_config, prefix)
  LlamaModel.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  LlamaForCausalLMEagle3.__init__(config, quant_config, prefix)
  LlamaForCausalLMEagle3.load_weights(weights, torch.Tensor]])
  LlamaForCausalLMEagle3.get_hot_token_id()

# python/sglang/srt/models/llama_embedding.py
  LlamaEmbeddingModel.__init__(config, quant_config, prefix)
  LlamaEmbeddingModel.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  LlamaEmbeddingModel.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/llama_reward.py
  LlamaForSequenceClassification.__init__(config, quant_config, prefix)
  LlamaForSequenceClassification.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  LlamaForSequenceClassification.load_weights(weights, torch.Tensor]])
  Weights.__init__(hidden_size, num_label)
  Weights.forward(x)
  LlamaForSequenceClassificationWithNormal_Weights.__init__(config, quant_config, prefix)
  LlamaForSequenceClassificationWithNormal_Weights.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  LlamaForSequenceClassificationWithNormal_Weights.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/llava.py
  LlavaBaseForCausalLM.pad_input_ids(input_ids, image_inputs)
  LlavaBaseForCausalLM.encode_images(pixel_values, List[torch.Tensor]])
  LlavaBaseForCausalLM.forward(input_ids, positions, forward_batch)
  LlavaBaseForCausalLM.load_weights(weights, torch.Tensor]])
  LlavaBaseForCausalLM.num_patches_per_side()
  LlavaLlamaForCausalLM.__init__(config, quant_config, prefix)
  LlavaQwenForCausalLM.__init__(config, quant_config, prefix)
  LlavaMistralForCausalLM.__init__(config, quant_config, prefix)
  LlavaForConditionalGeneration.dtype()
  LlavaForConditionalGeneration.pad_input_ids(input_ids, image_inputs)
  LlavaForConditionalGeneration.__init__(config, quant_config, prefix)
  LlavaForConditionalGeneration.get_image_feature(items)
  LlavaForConditionalGeneration.forward(input_ids, positions, forward_batch, get_embedding)
  LlavaForConditionalGeneration.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/llavavid.py
  LlavaVidForCausalLM.__init__(config, quant_config, prefix)
  LlavaVidForCausalLM.pad_input_ids(input_ids, image_inputs)
  LlavaVidForCausalLM.encode_images(pixel_values)
  LlavaVidForCausalLM.forward(input_ids, positions, forward_batch)
  LlavaVidForCausalLM.load_weights(weights, torch.Tensor]])
  LlavaVidForCausalLM.num_patches_per_side()

# python/sglang/srt/models/mimo.py
  MiMoModel.__init__(config, quant_config, prefix)
  MiMoForCausalLM.__init__(config, quant_config, prefix)
  MiMoForCausalLM.get_input_embeddings(input_ids)
  MiMoForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  MiMoForCausalLM.load_weights(weights, torch.Tensor]])
  MiMoForCausalLM.get_embed_and_head()
  MiMoForCausalLM.set_embed_and_head(embed, head)
  MiMoForCausalLM.load_kv_cache_scales(quantization_param_path)

# python/sglang/srt/models/mimo_mtp.py
  MiMoMultiTokenPredictorLayer.__init__(config, prefix, quant_config)
  MiMoMultiTokenPredictorLayer.forward(input_ids, positions, forward_batch, input_embeds)
  MiMoMTP.__init__(config, quant_config, prefix)
  MiMoMTP.forward(input_ids, positions, forward_batch)
  MiMoMTP.load_weights(weights, torch.Tensor]])
  MiMoMTP.map_model_name_to_mtp_param_name(name)
  MiMoMTP.get_embed_and_head()
  MiMoMTP.set_embed_and_head(embed, head)

# python/sglang/srt/models/minicpm.py
  MiniCPMMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  MiniCPMMLP.forward(x)
  MiniCPMAttention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, prefix)
  MiniCPMAttention.forward(positions, hidden_states, forward_batch)
  MiniCPMDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  MiniCPMDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  MiniCPMModel.__init__(config, quant_config, prefix)
  MiniCPMModel.forward(input_ids, positions, forward_batch, input_embeds)
  MiniCPMForCausalLM.__init__(config, quant_config, prefix)
  MiniCPMForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  MiniCPMForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/minicpm3.py
  MiniCPM3MLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  MiniCPM3MLP.forward(x)
input_to_float8(x, dtype)
  MiniCPM3AttentionMLA.__init__(config, hidden_size, num_heads, qk_nope_head_dim, qk_rope_head_dim, v_head_dim, q_lora_rank, kv_lora_rank, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, layer_id, prefix)
  MiniCPM3AttentionMLA.forward(positions, hidden_states, forward_batch)
  MiniCPM3DecoderLayer.__init__(config, layer_id, quant_config, prefix)
  MiniCPM3DecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  MiniCPM3Model.__init__(config, quant_config, prefix)
  MiniCPM3Model.forward(input_ids, positions, forward_batch, input_embeds)
  MiniCPM3ForCausalLM.__init__(config, quant_config, prefix)
  MiniCPM3ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  MiniCPM3ForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/minicpmo.py
apply_spk_emb(input_ids, spk_emb, input_embeds, spk_emb_token_id, num_spk_embs)
make_streaming_chunk_mask_generation(inputs_embeds, past_seen_tokens, streaming_tts_text_mask, streaming_reserved_length, streaming_audio_chunk_size, streaming_text_chunk_size, num_spk_emb, use_spk_emb)
  ConvNeXtBlock.__init__(dim, intermediate_dim, kernel, dilation, layer_scale_init_value)
  ConvNeXtBlock.forward(x, cond)
  DVAEDecoder.__init__(idim, odim, n_layer, bn_dim, hidden, kernel, dilation, up)
  DVAEDecoder.forward(x, conditioning)
  GFSQ.__init__(dim, levels, G, R, eps, transpose)
  GFSQ.__call__(x)
  GFSQ.forward(x)
  DVAE.__init__()
  DVAE.forward(inp, mode, 'decode'])
  CustomRepetitionPenaltyLogitsProcessorRepeat.__init__(penalty, max_input_ids, past_window)
  CustomRepetitionPenaltyLogitsProcessorRepeat.__call__(input_ids, scores)
  ConditionalChatTTS.__init__(config)
  ConditionalChatTTS.merge_inputs_embeds(input_ids, lm_spk_emb_last_hidden_states)
  ConditionalChatTTS.prefill_text(input_ids, position_ids, past_key_values, torch.Tensor]], lm_spk_emb_last_hidden_states)
  ConditionalChatTTS.prefill_audio_ids(input_ids, past_key_values, torch.Tensor]], streaming_tts_text_mask, add_audio_bos)
  ConditionalChatTTS.generate(input_ids, past_key_values, torch.Tensor]], temperature, eos_token, torch.Tensor], streaming_tts_text_mask, force_no_stop, min_new_token, max_new_token, logits_warpers, logits_processors, show_tqdm)
  ConditionalChatTTS.decode_to_mel_specs(result_list)
  MiniCPMWhisperEncoderLayer.__init__(config, layer_idx)
  MiniCPMWhisperEncoderLayer.forward(hidden_states, attention_mask, layer_head_mask, output_attentions, past_key_values, use_cache)
  MiniCPMWhisperEncoder.__init__(config)
  MiniCPMWhisperEncoder.forward(input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, past_key_values, use_cache)
  MultiModalProjector.__init__(in_dim, out_dim)
  MultiModalProjector.forward(audio_features)
  MiniCPMO.__init__(config, quant_config)
  MiniCPMO.init_tts_module()
  MiniCPMO.init_audio_module()
  MiniCPMO.init_llm(config, quant_config, prefix)
  MiniCPMO.init_vision_module(config, quant_config, prefix)
  MiniCPMO.init_resampler(embed_dim, vision_dim, quant_config, prefix)
  MiniCPMO.pad_input_ids(input_ids, mm_input)
  MiniCPMO.get_audio_embedding_streaming(items)
  MiniCPMO.subsequent_chunk_mask(size, chunk_size, num_left_chunks, device, num_lookhead)
  MiniCPMO.get_audio_embedding(items, chunk_length)
  MiniCPMO.get_audio_feature(items)
  MiniCPMO.get_omni_embedding(items, chunk_length, stream_input)
  MiniCPMO.get_image_feature(items)
  MiniCPMO.forward(input_ids, positions, forward_batch)
  MiniCPMO.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/minicpmv.py
get_1d_sincos_pos_embed_from_grid(embed_dim, pos, version, int])
get_2d_sincos_pos_embed_from_grid(embed_dim, grid, version, int])
get_2d_sincos_pos_embed(embed_dim, grid_size, Tuple[int, int]], cls_token, version, int])
  BaseResampler.__init__(num_queries, embed_dim, num_heads, kv_dim, norm_layer, nn.LayerNorm], do_post_projection, quant_config, prefix)
  Resampler2_5.__init__(num_queries, embed_dim, num_heads, kv_dim, norm_layer, nn.LayerNorm], max_size, int], quant_config, prefix)
  Resampler2_5.forward(x, tgt_sizes)
get_version_by_config(config)
  MiniCPMBaseModel.__init__()
  MiniCPMBaseModel.get_embedding(input_ids, image_inputs)
  MiniCPMBaseModel.get_input_embeddings()
  MiniCPMBaseModel.forward(input_ids, positions, forward_batch)
  MiniCPMBaseModel.init_llm(config, quant_config, prefix)
  MiniCPMBaseModel.init_vision_module(config, quant_config, prefix)
  MiniCPMBaseModel.init_resampler(embed_dim, vision_dim, quant_config, prefix)
  MiniCPMBaseModel.get_vision_embedding(pixel_values, patch_attn_mask, tgt_sizes)
  MiniCPMBaseModel.get_image_feature(items)
  MiniCPMV2_6.__init__(config, quant_config, prefix)
  MiniCPMV2_6.init_llm(config, quant_config, prefix)
  MiniCPMV2_6.init_vision_module(config, quant_config, prefix)
  MiniCPMV2_6.init_resampler(embed_dim, vision_dim, quant_config, prefix)
  MiniCPMV2_6.get_vision_embedding(pixel_values, patch_attn_mask, tgt_sizes)
  MiniCPMV2_6.get_image_feature(items)
  MiniCPMV2_6.pad_input_ids(input_ids, image_inputs)
  MiniCPMV.__init__(config, quant_config, prefix)
  MiniCPMV.__getattr__(name)
  MiniCPMV.__call__()
  MiniCPMV.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/mistral.py
  Mistral3ForConditionalGeneration.__init__()
  Mistral3ForConditionalGeneration.get_image_feature(items)
  Mistral3ForConditionalGeneration.__getattr__(name)
  Mistral3ForConditionalGeneration.__hasattr__(name)
  Mistral3ForConditionalGeneration.__call__()

# python/sglang/srt/models/mixtral.py
  MixtralMoE.__init__(num_experts, top_k, hidden_size, intermediate_size, layer_id, params_dtype, quant_config, tp_size, prefix)
  MixtralMoE.forward(hidden_states)
  MixtralAttention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, max_position, rope_theta, quant_config, prefix)
  MixtralAttention.forward(positions, hidden_states, forward_batch)
  MixtralDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  MixtralDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  MixtralModel.__init__(config, quant_config, prefix)
  MixtralModel.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  MixtralForCausalLM.__init__(config, quant_config, prefix)
  MixtralForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  MixtralForCausalLM.start_layer()
  MixtralForCausalLM.end_layer()
  MixtralForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/mixtral_quant.py
  MixtralMLP.__init__(num_experts, hidden_size, intermediate_size, quant_config, prefix)
  MixtralMLP.forward(hidden_states)
  MixtralMoE.__init__(config, quant_config, prefix)
  MixtralMoE.forward(hidden_states)
  MixtralAttention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, max_position, rope_theta, quant_config, prefix)
  MixtralAttention.forward(positions, hidden_states, forward_batch)
  MixtralDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  MixtralDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  MixtralModel.__init__(config, quant_config, prefix)
  MixtralModel.forward(input_ids, positions, forward_batch, input_embeds)
  QuantMixtralForCausalLM.__init__(config, quant_config, prefix)
  QuantMixtralForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  QuantMixtralForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/mllama.py
  ColumnParallelConv2dPatch.__init__(in_channels, out_channels, kernel_size, Tuple[int, int]], stride, Tuple[int, int]], bias)
  ColumnParallelConv2dPatch.forward(x)
  MllamaPrecomputedAspectRatioEmbedding.__init__(config, is_gated)
  MllamaPrecomputedAspectRatioEmbedding.forward(hidden_state, aspect_ratio_ids)
  MllamaPrecomputedPositionEmbedding.__init__(config)
  MllamaPrecomputedPositionEmbedding.forward(hidden_state, aspect_ratio_ids)
  MllamaVisionMLP.__init__(config, quant_config, prefix)
  MllamaVisionMLP.forward(hidden_states)
  MllamaVisionEncoderLayer.__init__(config, quant_config, is_gated, prefix)
  MllamaVisionEncoderLayer.forward(hidden_state, attention_mask)
  MllamaVisionEncoder.__init__(config, quant_config, num_layers, is_gated, output_hidden_states, prefix)
  MllamaVisionEncoder.forward(hidden_states, attention_mask)
  MllamaVisionModel.__init__(config, quant_config, prefix)
  MllamaVisionModel.apply_class_embedding(hidden_state)
  MllamaVisionModel.forward(pixel_values, aspect_ratio_ids, aspect_ratio_mask)
  MllamaTextRMSNorm.__init__(hidden_size, eps)
  MllamaTextRMSNorm.forward(hidden_states)
  MllamaTextRMSNorm.extra_repr()
  MllamaTextCrossAttention.__init__(config, layer_id, quant_config, prefix)
  MllamaTextCrossAttention.forward(hidden_states, attention_mask, cross_attention_states, forward_batch)
  MllamaCrossAttentionDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  MllamaCrossAttentionDecoderLayer.forward(hidden_states, cross_attention_states, cross_attention_mask, full_text_row_masked_out_mask, forward_batch)
  MllamaTextModel.__init__(config, quant_config, prefix)
  MllamaTextModel.forward(input_ids, positions, cross_attention_states, cross_attention_mask, full_text_row_masked_out_mask, torch.Tensor]], forward_batch, skip_cross_attention)
  MllamaForCausalLM.__init__(config, quant_config, prefix)
  MllamaForCausalLM.forward(input_ids, positions, cross_attention_states, cross_attention_mask, full_text_row_masked_out_mask, torch.Tensor]], forward_batch, skip_cross_attention)
  MllamaForConditionalGeneration.__init__(config, quant_config, prefix)
  MllamaForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)
  MllamaForConditionalGeneration.flat_encoder_result(cross_attention_states, encoder_lens_need)
  MllamaForConditionalGeneration.get_full_text_row_masked_out_mask(forward_batch)
  MllamaForConditionalGeneration.forward(input_ids, positions, forward_batch)
  MllamaForConditionalGeneration.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/mllama4.py
  Llama4VisionMLP.__init__(input_size, intermediate_size, output_size, bias, output_activation, quant_config, prefix, use_data_parallel)
  Llama4VisionMLP.forward(hidden_states)
pixel_shuffle(input_tensor, shuffle_ratio)
  Llama4VisionPixelShuffleMLP.__init__(config, quant_config, prefix, use_data_parallel)
  Llama4VisionPixelShuffleMLP.forward(encoded_patches)
apply_position_embedding(q, k, freqs_ci, shape)
  Llama4VisionEncoderLayer.__init__(config, quant_config, prefix, use_data_parallel)
  Llama4VisionEncoderLayer.forward(hidden_state, freqs_ci)
  Llama4VisionEncoder.__init__(config, quant_config, prefix, use_data_parallel)
  Llama4VisionEncoder.forward(hidden_states, freqs_ci)
  Llama4UnfoldConvolution.__init__(config, quant_config, prefix, use_data_parallel)
  Llama4UnfoldConvolution.forward(hidden_states)
  Llama4VisionRotaryEmbedding.__init__(config)
  Llama4VisionRotaryEmbedding.forward(hidden_states)
  Llama4VisionModel.__init__(config, quant_config, prefix)
  Llama4VisionModel.forward(pixel_values)
  Llama4ForConditionalGeneration.__init__(config, quant_config, prefix)
  Llama4ForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)
  Llama4ForConditionalGeneration.get_image_feature(items)
  Llama4ForConditionalGeneration.forward(input_ids, positions, forward_batch)
  Llama4ForConditionalGeneration.permute_qk_weight_for_rotary(name, loaded_weight)
  Llama4ForConditionalGeneration.load_weights(weights, torch.Tensor]])
  Llama4ForConditionalGeneration.set_eagle3_layers_to_capture(layer_ids)
  Llama4ForConditionalGeneration.get_embed_and_head()
  Llama4ForConditionalGeneration.set_embed_and_head(embed, head)
  Llama4ForConditionalGeneration.get_embed()
  Llama4ForConditionalGeneration.set_embed(embed)

# python/sglang/srt/models/nemotron_nas.py
  DeciLMDecoderLayer.__init__(config, layer_idx, quant_config, prefix)
  DeciLMDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  DeciModel.__init__()
  DeciModel.get_input_embeddings(input_ids)
  DeciModel.forward(input_ids, positions, forward_batch, inputs_embeds, pp_proxy_tensors)
  DeciLMForCausalLM.__init__()
  DeciLMForCausalLM.get_input_embeddings(input_ids)
  DeciLMForCausalLM.forward(input_ids, positions, forward_batch, inputs_embeds, get_embedding, pp_proxy_tensors)
  DeciLMForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/olmo.py
  OlmoAttention.__init__(config, layer_id, quant_config, prefix)
  OlmoAttention.forward(positions, hidden_states, forward_batch)
  OlmoMLP.__init__(config, quant_config, prefix)
  OlmoMLP.forward(x)
  OlmoDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  OlmoDecoderLayer.forward(positions, hidden_states, forward_batch)
  OlmoModel.__init__(config, quant_config, prefix)
  OlmoModel.forward(input_ids, positions, forward_batch, input_embeds)
  OlmoForCausalLM.__init__(config, quant_config, prefix)
  OlmoForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  OlmoForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/olmo2.py
  Olmo2Attention.__init__(config, layer_id, quant_config, prefix)
  Olmo2Attention.forward(positions, hidden_states, forward_batch)
  Olmo2MLP.__init__(config, quant_config, prefix)
  Olmo2MLP.forward(x)
  Olmo2DecoderLayer.__init__(config, layer_id, quant_config, prefix)
  Olmo2DecoderLayer.forward(positions, hidden_states, forward_batch)
  Olmo2Model.__init__(config, quant_config, prefix)
  Olmo2Model.forward(input_ids, positions, forward_batch, input_embeds)
  Olmo2ForCausalLM.__init__(config, quant_config, prefix)
  Olmo2ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  Olmo2ForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/olmoe.py
  OlmoeMoE.__init__(num_experts, top_k, hidden_size, intermediate_size, params_dtype, quant_config, tp_size, layer_id, prefix)
  OlmoeMoE.forward(hidden_states)
  OlmoeAttention.__init__(layer_id, hidden_size, num_heads, num_kv_heads, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, prefix)
  OlmoeAttention.forward(positions, hidden_states, forward_batch)
  OlmoeDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  OlmoeDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  OlmoeModel.__init__(config, quant_config, prefix)
  OlmoeModel.forward(input_ids, positions, forward_batch, input_embeds)
  OlmoeForCausalLM.__init__(config, quant_config, prefix)
  OlmoeForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  OlmoeForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/persimmon.py
  PersimmonMLP.__init__(config, quant_config)
  PersimmonMLP.forward(hidden_states)
  PersimmonAttention.__init__(config, quant_config, prefix, layer_id)
  PersimmonAttention.forward(position_ids, forward_batch, hidden_states)
  PersimmonDecoderLayer.__init__(config, quant_config, prefix, idx)
  PersimmonDecoderLayer.forward(position_ids, forward_batch, hidden_states)
  PersimmonModel.__init__(config, quant_config, prefix)
  PersimmonModel.get_input_embeddings(input_ids)
  PersimmonModel.forward(input_ids, forward_batch, positions, inputs_embeds)
  PersimmonForCausalLM.__init__(config, quant_config, prefix)
  PersimmonForCausalLM.get_input_embeddings(input_ids)
  PersimmonForCausalLM.forward(input_ids, positions, forward_batch, inputs_embeds)
  PersimmonForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/phi.py
  PhiAttention.__init__(config, quant_config, prefix, layer_id)
  PhiAttention.forward(position_ids, forward_batch, hidden_states)
  PhiMLP.__init__(config, quant_config)
  PhiMLP.forward(hidden_states)
  PhiLayer.__init__(config, quant_config, prefix, idx)
  PhiLayer.forward(position_ids, forward_batch, hidden_states)
  PhiModel.__init__(config, quant_config, prefix)
  PhiModel.get_input_embeddings(input_ids)
  PhiModel.forward(input_ids, forward_batch, positions, inputs_embeds)
  PhiForCausalLM.__init__(config, quant_config, prefix)
  PhiForCausalLM.get_input_embeddings(input_ids)
  PhiForCausalLM.forward(input_ids, positions, forward_batch, inputs_embeds)
  PhiForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/phi3_small.py
quick_gelu(x)
gegelu(input, limit)
  Phi3SmallMLP.__init__(config, quant_config, prefix)
  Phi3SmallMLP.forward(x)
  Phi3SmallSelfAttention.__init__(config, layer_id, quant_config, prefix)
  Phi3SmallSelfAttention.forward(positions, hidden_states, forward_batch)
  Phi3SmallDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  Phi3SmallDecoderLayer.forward(positions, hidden_states, forward_batch)
  Phi3SmallModel.__init__(config, quant_config, prefix)
  Phi3SmallModel.get_input_embeddings(input_ids)
  Phi3SmallModel.forward(input_ids, positions, forward_batch, inputs_embeds)
  Phi3SmallForCausalLM.__init__(config, quant_config, prefix)
  Phi3SmallForCausalLM.get_input_embeddings(input_ids)
  Phi3SmallForCausalLM.set_input_embeddings(value)
  Phi3SmallForCausalLM.get_output_embeddings()
  Phi3SmallForCausalLM.set_output_embeddings(value)
  Phi3SmallForCausalLM.set_decoder(decoder)
  Phi3SmallForCausalLM.get_decoder()
  Phi3SmallForCausalLM.compute_logits(input_ids, hidden_states, sampling_metadata)
  Phi3SmallForCausalLM.forward(input_ids, positions, forward_batch, inputs_embeds, get_embedding)
  Phi3SmallForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/phi4mm.py
  Phi4MMImageEncoder.__init__(config, quant_config, prefix, model_dir)
  Phi4MMImageEncoder.get_img_features(img_embeds, attention_mask)
  Phi4MMImageEncoder.forward(pixel_values, image_sizes, image_attention_mask)
  Phi4MMForCausalLM.__init__(config, quant_config, prefix)
  Phi4MMForCausalLM.get_image_feature(items)
  Phi4MMForCausalLM.get_audio_feature(items)
  Phi4MMForCausalLM.forward(input_ids, positions, forward_batch)
  Phi4MMForCausalLM.pad_input_ids(input_ids, mm_inputs)
  Phi4MMForCausalLM.should_apply_lora(module_name)
  Phi4MMForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/phi4mm_audio.py
  ConformerEncoderLayer.__init__(d_model, ext_pw_out_channel, depthwise_seperable_out_channel, depthwise_multiplier, n_head, d_ffn, ext_pw_kernel_size, kernel_size, dropout_rate, causal, batch_norm, activation, chunk_se, chunk_size, conv_activation, conv_glu_type, bias_in_glu, linear_glu_in_convm, attention_inner_dim, attention_glu_type, activation_checkpointing, export, use_pt_scaled_dot_product_attention, attn_group_sizes)
  ConformerEncoderLayer.forward(x, pos_k, pos_v, mask, relative_attention_bias)
  TransformerEncoderBase.__init__(input_size, chunk_size, left_chunk, attention_dim, attention_heads, input_layer, cnn_out, cnn_layer_norm, time_reduction, dropout_rate, padding_idx, relative_attention_bias_args, positional_dropout_rate, nemo_conv_settings, conv2d_extra_padding, 'feat_time', 'none', True], attention_group_size, encoder_embedding_config)
  TransformerEncoderBase.compute_lens_change(feature_lens)
  TransformerEncoderBase.forward()
  TransformerEncoderBase.forward_embeddings(xs_pad, masks, chunk_size_nc, left_chunk_nc)
  TransformerEncoderBase.get_offset()
  ConformerEncoder.__init__(input_size, chunk_size, left_chunk, num_lang, attention_dim, attention_heads, linear_units, num_blocks, dropout_rate, input_layer, causal, batch_norm, cnn_out, cnn_layer_norm, ext_pw_out_channel, ext_pw_kernel_size, depthwise_seperable_out_channel, depthwise_multiplier, chunk_se, kernel_size, activation, conv_activation, conv_glu_type, bias_in_glu, linear_glu_in_convm, attention_glu_type, export, extra_layer_output_idx, extra_multi_layer_output_idxs, activation_checkpointing, relative_attention_bias_args, time_reduction, use_pt_scaled_dot_product_attention, nemo_conv_settings, conv2d_extra_padding, 'feat_time', 'none', True], replication_pad_for_subsample_embedding, attention_group_size, encoder_embedding_config)
  ConformerEncoder.init_relative_attention_bias(input_tensor)
  ConformerEncoder.calculate_hs_mask(xs_pad, device, mask)
  ConformerEncoder.forward(xs_pad, masks)
  WindowQformer.__init__(window_size, num_queries, num_blocks, attention_dim, attention_heads, linear_units, dropout_rate, normalize_before)
  WindowQformer.forward(audio_embed, mask, embed_len)
  AudioEmbedding.__init__(config)
  AudioEmbedding.set_audio_embeds(input_embeds)
  AudioEmbedding.set_audio_embed_sizes(audio_embed_sizes)
  AudioEmbedding.get_audio_features(input_embeds, audio_attention_mask, audio_projection_mode)
  AudioEmbedding.forward(audio_features, audio_attention_mask, audio_projection_mode)

# python/sglang/srt/models/phi4mm_utils.py
  BlockBase.__init__(input_size, output_size)
get_activation(name)
adaptive_enc_mask(x_len, chunk_start_idx, left_window, right_window)
  Swish.__init__()
  Swish.forward(x)
  GLU.__init__(dim, act_name)
  GLU.forward(x)
  GLUPointWiseConv.__init__(input_dim, output_dim, kernel_size, glu_type, bias_in_glu, causal)
  GLUPointWiseConv.forward(x)
  DepthWiseSeperableConv1d.__init__(input_dim, depthwise_seperable_out_channel, kernel_size, depthwise_multiplier, padding)
  DepthWiseSeperableConv1d.forward(x)
  ConvModule.__init__(input_dim, ext_pw_out_channel, depthwise_seperable_out_channel, ext_pw_kernel_size, kernel_size, depthwise_multiplier, dropout_rate, causal, batch_norm, chunk_se, chunk_size, activation, glu_type, bias_in_glu, linear_glu_in_convm, export)
  ConvModule.forward(x)
  GLULinear.__init__(input_dim, output_dim, glu_type, bias_in_glu)
  GLULinear.forward(x)
  FeedForward.__init__(d_model, d_inner, dropout_rate, activation, bias_in_glu)
  FeedForward.forward(x)
  T5RelativeAttentionLogitBias.__init__(num_heads, num_buckets, max_distance, symmetric)
  T5RelativeAttentionLogitBias.forward(x)
  AbsolutePositionalEncoding.__init__(d_model, dropout_rate, max_len)
  AbsolutePositionalEncoding.extend_pe(x)
  AbsolutePositionalEncoding.forward(x)
  MeanVarianceNormLayer.__init__(input_size)
  MeanVarianceNormLayer.forward(input_)
  CausalConv1D.__init__(in_channels, out_channels, kernel_size, stride, padding, int], dilation, groups, bias, padding_mode, device, dtype)
  CausalConv1D.update_cache(x, cache)
  CausalConv1D.forward(x, cache)
  CausalConv2D.__init__(in_channels, out_channels, kernel_size, stride, padding, int], dilation, groups, bias, padding_mode, device, dtype)
  CausalConv2D.forward(x)
  NemoConvSubsampling.__init__(feat_in, feat_out, subsampling_factor, subsampling, conv_channels, subsampling_conv_chunking_factor, activation, is_causal)
  NemoConvSubsampling.get_sampling_frames()
  NemoConvSubsampling.get_streaming_cache_size()
  NemoConvSubsampling.forward(x, mask)
  NemoConvSubsampling.reset_parameters()
  NemoConvSubsampling.conv_split_by_batch(x)
  NemoConvSubsampling.conv_split_by_channel(x)
  NemoConvSubsampling.channel_chunked_conv(conv, chunk_size, x)
  NemoConvSubsampling.change_subsampling_conv_chunking_factor(subsampling_conv_chunking_factor)
calc_length(lengths, all_paddings, kernel_size, stride, ceil_mode, repeat_num)
  AttModule.__init__()
  AttModule.set_export(mode)
  AttModule.forward(x, memory, pos_emb, att_mask)
  AttBlock.memory_dims(max_len)
masked_softmax(scores, mask)
  MultiHeadedAttention.__init__(n_head, n_feat, dropout_rate, attention_inner_dim, glu_type, bias_in_glu, use_pt_scaled_dot_product_attention, n_value, group_size)
  MultiHeadedAttention.forward(query, key, value, pos_k, pos_v, mask, relative_attention_bias)
  MultiSequential.forward()
get_offset(input_layer, time_reduction)
unfold_tensor(xs_pad, max_seq_len)

# python/sglang/srt/models/phimoe.py
  PhiMoEConfig.__init__(vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, head_dim, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, tie_word_embeddings, rope_theta, sliding_window, attention_dropout, num_experts_per_tok, num_local_experts, output_router_logits, router_aux_loss_coef, router_jitter_noise, attention_bias, lm_head_bias)
sparsemixer(scores, jitter_eps)
phimoe_routing_function(hidden_states, gating_output, topk, renormalize)
  PhiMoE.__init__(num_experts, top_k, hidden_size, intermediate_size, layer_id, quant_config, prefix)
  PhiMoE.forward(hidden_states, forward_batch)
  PhiMoEAttention.__init__(hidden_size, num_heads, num_kv_heads, head_dim, max_position, rope_theta, layer_id, attention_bias, quant_config, rope_scaling, prefix)
  PhiMoEAttention.forward(positions, hidden_states, forward_batch)
  PhiMoEDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  PhiMoEDecoderLayer.forward(positions, hidden_states, residual, forward_batch)
  PhiMoEModel.__init__(config, quant_config, prefix)
  PhiMoEModel.forward(input_ids, positions, forward_batch, input_embeds)
  PhiMoEForCausalLM.__init__(config, quant_config, prefix)
  PhiMoEForCausalLM.forward(input_ids, positions, forward_batch, inputs_embeds, get_embedding)
  PhiMoEForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/pixtral.py
  PixtralHFMLP.__init__(config, quant_config)
  PixtralHFMLP.forward(x)
  PixtralHFTransformerBlock.__init__(config, layer_id, quant_config)
  PixtralHFTransformerBlock.forward(hidden_states, attention_mask, position_embeddings, torch.Tensor]])
  PixtralHFTransformer.__init__(config, quant_config)
  PixtralHFTransformer.forward(x, attention_mask, position_embeddings, torch.Tensor]], return_all_hidden_states)
resolve_visual_encoder_outputs(outputs, List[torch.Tensor]], feature_sample_layers, post_norm, num_hidden_layers)
  PixtralHFVisionModel.pad_input_ids(input_ids, mm_inputs)
  PixtralHFVisionModel.__init__(config, quant_config)
  PixtralHFVisionModel.dtype()
  PixtralHFVisionModel.device()
  PixtralHFVisionModel.forward(pixel_values, image_sizes, int]], output_hidden_states, feature_sample_layers)
  PixtralHFVisionModel.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/qwen.py
  QWenMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  QWenMLP.forward(x)
  QWenAttention.__init__(hidden_size, num_heads, max_position_embeddings, layer_id, rope_theta, rope_scaling, Any]], quant_config, prefix)
  QWenAttention.forward(positions, hidden_states, forward_batch)
  QWenBlock.__init__(config, layer_id, quant_config, prefix)
  QWenBlock.forward(positions, hidden_states, forward_batch)
  QWenModel.__init__(config, quant_config, prefix)
  QWenModel.forward(input_ids, positions, forward_batch)
  QWenLMHeadModel.__init__(config, quant_config, prefix)
  QWenLMHeadModel.forward(input_ids, positions, forward_batch)
  QWenLMHeadModel.forward_split_prefill(input_ids, positions, forward_batch, split_interval, int])
  QWenLMHeadModel.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/qwen2.py
  Qwen2MLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  Qwen2MLP.forward(x)
  Qwen2Attention.__init__(hidden_size, num_heads, num_kv_heads, head_dim, layer_id, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, dual_chunk_attention_config, Any]], prefix)
  Qwen2Attention.forward(positions, hidden_states, forward_batch)
  Qwen2DecoderLayer.__init__(config, layer_id, quant_config, prefix, alt_stream)
  Qwen2DecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  Qwen2Model.__init__(config, quant_config, prefix, decoder_layer_type, alt_stream)
  Qwen2Model.get_input_embedding(input_ids)
  Qwen2Model.get_input_embeddings()
  Qwen2Model.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  Qwen2Model.load_kv_cache_scales(quantization_param_path)
  Qwen2ForCausalLM.__init__(config, quant_config, prefix)
  Qwen2ForCausalLM.get_input_embedding(input_ids)
  Qwen2ForCausalLM.get_input_embeddings()
  Qwen2ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, get_embedding, pp_proxy_tensors)
  Qwen2ForCausalLM.forward_split_prefill(input_ids, positions, forward_batch, split_interval, int], input_embeds)
  Qwen2ForCausalLM.start_layer()
  Qwen2ForCausalLM.end_layer()
  Qwen2ForCausalLM.load_weights(weights, torch.Tensor]])
  Qwen2ForCausalLM.get_embed_and_head()
  Qwen2ForCausalLM.set_embed_and_head(embed, head)
  Qwen2ForCausalLM.load_kv_cache_scales(quantization_param_path)

# python/sglang/srt/models/qwen2_5_vl.py
  Qwen2_5_VLMLP.__init__(in_features, hidden_features, bias, hidden_act, quant_config, prefix)
  Qwen2_5_VLMLP.forward(x)
  Qwen2_5_VisionBlock.__init__(dim, intermediate_dim, num_heads, hidden_act, norm_layer, attn_implementation, quant_config, prefix, num_dummy_heads)
  Qwen2_5_VisionBlock.forward(x, cu_seqlens, position_embeddings)
  Qwen2_5_VisionPatchMerger.__init__(dim, context_dim, spatial_merge_size, quant_config, prefix)
  Qwen2_5_VisionPatchMerger.forward(x)
  Qwen2_5_VisionTransformer.__init__(vision_config, norm_eps, quant_config, prefix)
  Qwen2_5_VisionTransformer.get_window_index(grid_thw)
  Qwen2_5_VisionTransformer.dtype()
  Qwen2_5_VisionTransformer.device()
  Qwen2_5_VisionTransformer.rot_pos_emb(grid_thw)
  Qwen2_5_VisionTransformer.forward(x, grid_thw)
  Qwen2_5_VLForConditionalGeneration.__init__(config, quant_config, prefix)
  Qwen2_5_VLForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)
  Qwen2_5_VLForConditionalGeneration.get_image_feature(items)
  Qwen2_5_VLForConditionalGeneration.get_video_feature(items)
  Qwen2_5_VLForConditionalGeneration.get_input_embeddings()
  Qwen2_5_VLForConditionalGeneration.forward(input_ids, positions, forward_batch, get_embedding)
  Qwen2_5_VLForConditionalGeneration.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/qwen2_audio.py
  Qwen2AudioForConditionalGeneration.__init__(config, quant_config, prefix)
  Qwen2AudioForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)
  Qwen2AudioForConditionalGeneration.get_audio_feature(items)
  Qwen2AudioForConditionalGeneration.forward(input_ids, positions, forward_batch)
  Qwen2AudioForConditionalGeneration.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/qwen2_eagle.py
  Qwen2DecoderLayer.__init__(config, layer_id, quant_config, prefix)
  Qwen2Model.__init__(config, quant_config, prefix)
  Qwen2Model.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  Qwen2ForCausalLMEagle.__init__(config, quant_config, prefix)
  Qwen2ForCausalLMEagle.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/qwen2_moe.py
  Qwen2MoeMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, reduce_results, prefix)
  Qwen2MoeMLP.forward(x, use_reduce_scatter)
  Qwen2MoeSparseMoeBlock.__init__(layer_id, config, quant_config, prefix)
  Qwen2MoeSparseMoeBlock.forward(hidden_states, forward_batch, use_reduce_scatter)
  Qwen2MoeAttention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], max_position_embeddings, qkv_bias, quant_config, dual_chunk_attention_config, Any]], prefix)
  Qwen2MoeAttention.forward(positions, hidden_states, forward_batch)
  Qwen2MoeDecoderLayer.__init__(config, layer_id, quant_config, prefix, alt_stream)
  Qwen2MoeDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  Qwen2MoeModel.__init__(config, quant_config, prefix, decoder_layer_type, alt_stream)
  Qwen2MoeModel.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  Qwen2MoeForCausalLM.__init__(config, quant_config, prefix)
  Qwen2MoeForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  Qwen2MoeForCausalLM.forward_split_prefill(input_ids, positions, forward_batch, split_interval, int], input_embeds)
  Qwen2MoeForCausalLM.start_layer()
  Qwen2MoeForCausalLM.end_layer()
  Qwen2MoeForCausalLM.load_weights(weights, torch.Tensor]])
  Qwen2MoeForCausalLM.get_model_config_for_expert_location(cls, config)

# python/sglang/srt/models/qwen2_rm.py
  Qwen2ForRewardModel.__init__(config, quant_config, prefix)
  Qwen2ForRewardModel.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  Qwen2ForRewardModel.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/qwen2_vl.py
  Qwen2VisionMLP.__init__(in_features, hidden_features, act_layer, quant_config, prefix)
  Qwen2VisionMLP.forward(x)
  Qwen2VisionBlock.__init__(dim, num_heads, mlp_ratio, act_layer, norm_layer, attn_implementation, quant_config, prefix)
  Qwen2VisionBlock.forward(x, cu_seqlens, position_embeddings)
  Qwen2VisionPatchEmbed.__init__(patch_size, temporal_patch_size, in_chans, embed_dim)
  Qwen2VisionPatchEmbed.forward(x)
  Qwen2VisionPatchMerger.__init__(d_model, context_dim, norm_layer, spatial_merge_size, quant_config, prefix)
  Qwen2VisionPatchMerger.forward(x)
  Qwen2VisionRotaryEmbedding.__init__(dim, theta)
  Qwen2VisionRotaryEmbedding.update_freqs_cache(seqlen)
  Qwen2VisionRotaryEmbedding.forward(seqlen)
  Qwen2VisionTransformer.__init__(vision_config, norm_eps, quant_config, prefix)
  Qwen2VisionTransformer.dtype()
  Qwen2VisionTransformer.device()
  Qwen2VisionTransformer.rot_pos_emb(grid_thw)
  Qwen2VisionTransformer.forward(x, grid_thw)
  Qwen2VLForConditionalGeneration.__init__(config, quant_config, prefix)
  Qwen2VLForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)
  Qwen2VLForConditionalGeneration.get_image_feature(items)
  Qwen2VLForConditionalGeneration.get_video_feature(items)
  Qwen2VLForConditionalGeneration.get_input_embeddings()
  Qwen2VLForConditionalGeneration.forward(input_ids, positions, forward_batch, get_embedding)
  Qwen2VLForConditionalGeneration.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/qwen3.py
  Qwen3Attention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], head_dim, max_position_embeddings, quant_config, rms_norm_eps, attention_bias, prefix, alt_stream)
  Qwen3Attention.forward(positions, hidden_states, forward_batch)
  Qwen3DecoderLayer.__init__(config, layer_id, quant_config, prefix, alt_stream)
  Qwen3DecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  Qwen3Model.__init__(config, quant_config, prefix)
  Qwen3ForCausalLM.__init__(config, quant_config, prefix)
  Qwen3ForCausalLM.get_input_embeddings()
  Qwen3ForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, get_embedding, pp_proxy_tensors)
  Qwen3ForCausalLM.forward_split_prefill(input_ids, positions, forward_batch, split_interval, int], input_embeds)
  Qwen3ForCausalLM.start_layer()
  Qwen3ForCausalLM.end_layer()
  Qwen3ForCausalLM.load_weights(weights, torch.Tensor]])
  Qwen3ForCausalLM.get_embed_and_head()
  Qwen3ForCausalLM.set_embed_and_head(embed, head)
  Qwen3ForCausalLM.load_kv_cache_scales(quantization_param_path)
  Qwen3ForCausalLM.set_eagle3_layers_to_capture(layer_ids)

# python/sglang/srt/models/qwen3_classification.py
  Qwen3ForSequenceClassification.__init__(config, quant_config, prefix)
  Qwen3ForSequenceClassification.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  Qwen3ForSequenceClassification.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/qwen3_moe.py
  Qwen3MoeSparseMoeBlock.__init__(layer_id, config, quant_config, prefix)
  Qwen3MoeSparseMoeBlock.forward(hidden_states, forward_batch, use_reduce_scatter)
  Qwen3MoeSparseMoeBlock.get_moe_weights()
  Qwen3MoeSparseMoeBlock.forward_normal(hidden_states, use_reduce_scatter)
  Qwen3MoeSparseMoeBlock.forward_deepep(hidden_states, forward_batch)
  Qwen3MoeSparseMoeBlock.op_gate(state)
  Qwen3MoeSparseMoeBlock.op_select_experts(state)
  Qwen3MoeSparseMoeBlock.op_dispatch_a(state)
  Qwen3MoeSparseMoeBlock.op_dispatch_b(state)
  Qwen3MoeSparseMoeBlock.op_experts(state)
  Qwen3MoeSparseMoeBlock.op_combine_a(state)
  Qwen3MoeSparseMoeBlock.op_combine_b(state)
  Qwen3MoeSparseMoeBlock.op_output(state)
  Qwen3MoeAttention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], max_position_embeddings, head_dim, rms_norm_eps, attention_bias, quant_config, prefix, dual_chunk_attention_config, Any]], alt_stream)
  Qwen3MoeAttention.op_prepare(state)
  Qwen3MoeAttention.op_core(state)
  Qwen3MoeAttention.forward_prepare(positions, hidden_states, forward_batch)
  Qwen3MoeAttention.forward_core(intermediate_state)
  Qwen3MoeAttention.forward(positions, hidden_states, forward_batch)
  Qwen3MoeDecoderLayer.__init__(config, layer_id, quant_config, prefix, alt_stream)
  Qwen3MoeDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  Qwen3MoeDecoderLayer.op_comm_prepare_attn(state, positions, hidden_states, forward_batch, residual, tbo_subbatch_index)
  Qwen3MoeDecoderLayer.op_comm_prepare_mlp(state)
  Qwen3MoeDecoderLayer.op_mlp(state)
  Qwen3MoeDecoderLayer.op_comm_postprocess_layer(state)
  Qwen3MoeModel.__init__(config, quant_config, prefix)
  Qwen3MoeForCausalLM.__init__(config, quant_config, prefix)
  Qwen3MoeForCausalLM.get_input_embeddings()
  Qwen3MoeForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, pp_proxy_tensors)
  Qwen3MoeForCausalLM.forward_split_prefill(input_ids, positions, forward_batch, split_interval, int], input_embeds)
  Qwen3MoeForCausalLM.start_layer()
  Qwen3MoeForCausalLM.end_layer()
  Qwen3MoeForCausalLM.get_embed_and_head()
  Qwen3MoeForCausalLM.set_eagle3_layers_to_capture(layer_ids)
  Qwen3MoeForCausalLM.load_weights(weights, torch.Tensor]])
  Qwen3MoeForCausalLM.get_model_config_for_expert_location(cls, config)

# python/sglang/srt/models/registry.py
  _ModelRegistry.get_supported_archs()
  _ModelRegistry.resolve_model_cls(architectures, List[str]])
import_model_classes()

# python/sglang/srt/models/roberta.py
  RobertaClassificationHead.__init__(config)
  RobertaClassificationHead.forward(features)
  RobertaEmbedding.__init__(config)
  RobertaEmbedding.forward(input_ids, seq_lens, position_ids, forward_batch)
  XLMRobertaBaseModel.__init__()
  XLMRobertaBaseModel.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  XLMRobertaBaseModel.load_weights(weights, torch.Tensor]])
create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length)
  XLMRobertaModel.__init__()
  XLMRobertaModel.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  XLMRobertaModel.load_weights(weights, torch.Tensor]])
  XLMRobertaForSequenceClassification.__init__()
  XLMRobertaForSequenceClassification.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  XLMRobertaForSequenceClassification.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/siglip.py
  SiglipVisionEmbeddings.__init__(config)
  SiglipVisionEmbeddings.forward(pixel_values)
  SiglipMLP.__init__(config, act_layer, quant_config, prefix)
  SiglipMLP.forward(x)
  SiglipEncoderLayer.__init__(config, act_layer, norm_layer, attn_implementation, quant_config, prefix)
  SiglipEncoderLayer.forward(hidden_states, attention_mask, causal_attention_mask)
  SiglipEncoder.__init__(config, quant_config, prefix)
  SiglipEncoder.forward(inputs_embeds, attention_mask, causal_attention_mask, return_all_hidden_states)
  SiglipVisionTransformer.__init__(config, quant_config, prefix)
  SiglipVisionTransformer.device()
  SiglipVisionTransformer.forward(pixel_values)
  SiglipVisionModel.__init__(config, quant_config, prefix)
  SiglipVisionModel.device()
  SiglipVisionModel.forward(pixel_values)

# python/sglang/srt/models/stablelm.py
  StablelmMLP.__init__(config, quant_config, prefix)
  StablelmMLP.forward(x)
  StablelmAttention.__init__(config, layer_id, quant_config, prefix)
  StablelmAttention.forward(positions, hidden_states, forward_batch)
  StablelmDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  StablelmDecoderLayer.forward(positions, hidden_states, forward_batch)
  StableLMEpochModel.__init__(config, quant_config, prefix)
  StableLMEpochModel.forward(input_ids, positions, forward_batch, input_embeds)
  StableLmForCausalLM.__init__(config, quant_config, prefix)
  StableLmForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  StableLmForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/step3_vl.py
  Step3TextMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  Step3TextMLP.forward(x)
  Step3TextMoEMLP.__init__(layer_id, config, quant_config, prefix)
  Step3TextMoEMLP.forward(hidden_states)
  Step3TextAttention.__init__(hidden_size, num_heads, num_kv_heads, head_dim, share_q_dim, layer_id, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, rms_norm_eps, prefix)
  Step3TextAttention.forward(positions, hidden_states, forward_batch)
  Step3TextDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  Step3TextDecoderLayer.moe_mlp_forward(hidden_states)
  Step3TextDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  Step3TextModel.__init__(config, quant_config, prefix)
  Step3TextModel.get_input_embeddings()
  Step3TextModel.forward(input_ids, positions, forward_batch, input_embeds)
get_abs_pos(abs_pos, tgt_size)
  Step3VisionMLP.__init__(dim, intermediate_size, bias, hidden_act, quant_config, prefix)
  Step3VisionMLP.forward(hidden_states)
  Step3VisionAttention.__init__(dim, num_heads, qkv_backend, quant_config, prefix)
  Step3VisionAttention.forward(hidden_states)
  Step3VisionEmbeddings.__init__(config)
  Step3VisionEmbeddings.forward(pixel_values)
  Step3VisionEncoderLayer.__init__(config, attn_implementation)
  Step3VisionEncoderLayer.forward(hidden_states)
  Step3VisionTransformer.__init__(config)
  Step3VisionTransformer.dtype()
  Step3VisionTransformer.forward(pixel_values)
  Step3VisionEncoder.__init__(config)
  Step3VisionEncoder.forward(inputs_embeds)
  Step3VLForConditionalGeneration.__init__(config, quant_config, prefix)
  Step3VLForConditionalGeneration.get_image_feature(items)
  Step3VLForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)
  Step3VLForConditionalGeneration.forward(input_ids, positions, forward_batch, input_embeds)
  Step3VLForConditionalGeneration.load_weights(weights, torch.Tensor]])
  Step3VLForConditionalGeneration.get_model_config_for_expert_location(cls, config)

# python/sglang/srt/models/torch_native_llama.py
gate_up_proj_weight_loader(param, loaded_weight, loaded_shard_id)
  LlamaMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  LlamaMLP.forward(x)
qkv_proj_weight_loader(param, loaded_weight, loaded_shard_id)
  LlamaAttention.__init__(config, hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], rope_is_neox_style, max_position_embeddings, quant_config, prefix)
  LlamaAttention.forward(positions, hidden_states, forward_batch)
  LlamaDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  LlamaDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  LlamaModel.__init__(config, quant_config)
  LlamaModel.forward(input_ids, positions, forward_batch, input_embeds)
  TorchNativeLlamaForCausalLM.__init__(config, quant_config)
  TorchNativeLlamaForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  TorchNativeLlamaForCausalLM.get_module_name_from_weight_name(name)
  TorchNativeLlamaForCausalLM.get_num_params()
  TorchNativeLlamaForCausalLM.load_weights_to_module(fqn, weights, torch.Tensor]])
  TorchNativeLlamaForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/transformers.py
maybe_prefix(prefix, name)
sglang_flash_attention_forward(module, query, key, value, attention_mask, forward_batch, scaling, attention_instances)
  HFColumnParallelLinear.forward(input)
  HFRowParallelLinear.forward(input)
replace_linear_class(linear, style, 'rowwise'], quant_config)
  TransformersForCausalLM.__init__(config, quant_config, prefix)
  TransformersForCausalLM.log_replacement(name, old_module, new_module)
  TransformersForCausalLM.tensor_parallel(tp_size)
  TransformersForCausalLM.replace_vocab_embed_class(module)
  TransformersForCausalLM.forward(input_ids, positions, forward_batch, input_embeds, get_embedding)
  TransformersForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/vila.py
  VILAConfig.__init__(text_config, Any]], vision_config, Any]])
  DownSample3x3BlockFix.forward(x)
  MultimodalProjector.__init__(config)
  MultimodalProjector.device()
  MultimodalProjector.dtype()
  MultimodalProjector.forward(x)
  VILAForConditionalGeneration.__init__(config, quant_config, prefix)
  VILAForConditionalGeneration.dtype()
  VILAForConditionalGeneration.forward(input_ids, positions, forward_batch, get_embedding)
  VILAForConditionalGeneration.get_image_feature(mm_input)
  VILAForConditionalGeneration.load_weights(weights, Tensor]])
  VILAForConditionalGeneration.pad_input_ids(input_ids, mm_inputs)

# python/sglang/srt/models/xverse.py
  XverseMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, prefix)
  XverseMLP.forward(x)
  XverseAttention.__init__(config, hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], rope_is_neox_style, max_position_embeddings, quant_config, prefix)
  XverseAttention.forward(positions, hidden_states, forward_batch)
  XverseDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  XverseDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  XverseModel.__init__(config, quant_config, prefix)
  XverseModel.forward(input_ids, positions, forward_batch, input_embeds)
  XverseForCausalLM.__init__(config, quant_config, prefix)
  XverseForCausalLM.forward(input_ids, positions, forward_batch, input_embeds)
  XverseForCausalLM.load_weights(weights, torch.Tensor]], name, loaded_weight)

# python/sglang/srt/models/xverse_moe.py
  XverseMLP.__init__(hidden_size, intermediate_size, hidden_act, quant_config, reduce_results, prefix)
  XverseMLP.forward(x)
  XverseMoE.__init__(config, quant_config, prefix)
  XverseMoE.pack_params()
  XverseMoE.forward(hidden_states)
  XverseAttention.__init__(hidden_size, num_heads, num_kv_heads, layer_id, rope_theta, rope_scaling, Any]], max_position_embeddings, quant_config, prefix)
  XverseAttention.forward(positions, hidden_states, forward_batch)
  XverseDecoderLayer.__init__(config, layer_id, quant_config, prefix)
  XverseDecoderLayer.forward(positions, hidden_states, forward_batch, residual)
  XverseModel.__init__(config, quant_config, prefix)
  XverseModel.forward(input_ids, positions, forward_batch)
  XverseMoeForCausalLM.__init__(config, quant_config, prefix)
  XverseMoeForCausalLM.forward(input_ids, positions, forward_batch)
  XverseMoeForCausalLM.load_weights(weights, torch.Tensor]])

# python/sglang/srt/models/yivl.py
  YiVLForCausalLM.__init__(config, quant_config, prefix)
  YiVLForCausalLM.load_weights(weights, torch.Tensor]])
  YiVLMultiModalProjector.__init__(config)
  YiVLMultiModalProjector.forward(image_features)