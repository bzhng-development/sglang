================================================================================
FUNCTION INDEX: processors module
================================================================================
Total Functions: 91
Documented: 17


============================================================
FILE: python/sglang/srt/multimodal/processors/base_processor.py
Functions: 18
============================================================


CLASS: BaseMultiModalProcessorOutput
----------------------------------------
  L  39: organize_results(self)
         ‚Üí List[Tuple[Modality, Any]]
         üìù :return: a list of results, with their corresponding modalities


CLASS: BaseMultimodalProcessor
----------------------------------------
  L 151: __init__(self, hf_config, server_args, _processor, transport_mode)

  L 209: process_mm_data(self, input_text, images, videos, audios)
         ‚Üí dict
         üìù process multimodal data with transformers AutoProcessor

  L 252: process_mm_data_async(self, image_data, audio_data, input_text, request_obj)
         ‚Üí Optional[Dict[str, Any]]

  L 262: get_estimated_frames_list(self, image_data)
         üìù estimate the total frame count from all visual input

  L 314: submit_data_loading_tasks(self, text_parts: List[str], multimodal_tokens: MultimodalSpecialTokens, data_iterators: dict[Modality, Iterator[Any]], discard_alpha_channel: bool, image_estimated_frames_iter: Optional[iter], image_scaling_factor: float, max_image_frames: int, audio_sample_rate: Optional[int])
         ‚Üí Tuple[List, List]
         üìù load multimodal data parallelly using iterators.

  L 385: load_mm_data(self, prompt: str, multimodal_tokens: MultimodalSpecialTokens, image_data: Optional[list], video_data: Optional[list], audio_data: Optional[list], return_text: Optional[bool], discard_alpha_channel: bool, audio_sample_rate: Optional[int])
         ‚Üí BaseMultiModalProcessorOutput
         üìù Each frame of video/image will be replaced by a single image token

  L 498: get_mm_items_offset(input_ids: torch.Tensor, mm_token_id: int)
         ‚Üí List[Tuple[int, int]]
         üìù Get a set of range for mm_items from input_ids

  L 515: get_mm_items_offset_by_pair(input_ids: torch.Tensor, mm_start_id: int, mm_end_id: int)
         ‚Üí List[Tuple[int, int]]

  L 523: collect_mm_items_from_processor_output(self, data_dict: dict)
         ‚Üí List[MultimodalDataItem]
         üìù Create mm_items directly from processor output.

  L 574: process_and_combine_mm_data(self, base_output: BaseMultiModalProcessorOutput, mm_tokens: MultimodalSpecialTokens)
         ‚Üí Tuple[List[MultimodalDataItem], torch.Tensor, dict]
         üìù Process multimodal data and return the combined multimodal items and i


CLASS: MultimodalSpecialTokens
----------------------------------------
  L  67: build(self, processor)

  L  73: convert_to_str(self, token: Union[str, int], processor)
         ‚Üí str

  L  80: convert_to_strs(self, processor)

  L  88: get_modality_of_token(self, token: str)
         ‚Üí Optional[Modality]
         üìù :return: the modality associated with the given token, if the token is

  L 110: get_token_id_by_modality(self, modality: Modality)
         ‚Üí Optional[int]

  L 118: parse_regex(self)

  L 126: get_combined_regex(self)
         ‚Üí re.Pattern
         üìù Builds and returns a regex, used to split input str into tokens (with 


============================================================
FILE: python/sglang/srt/multimodal/processors/clip.py
Functions: 2
============================================================


CLASS: ClipImageProcessor
----------------------------------------
  L  13: __init__(self, hf_config, server_args, _processor)

  L  19: process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text)


============================================================
FILE: python/sglang/srt/multimodal/processors/deepseek_vl_v2.py
Functions: 2
============================================================


CLASS: DeepseekVL2ImageProcessor
----------------------------------------
  L  34: __init__(self, hf_config, server_args, _processor)

  L  40: process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text, request_obj, max_req_input_len)


============================================================
FILE: python/sglang/srt/multimodal/processors/gemma3.py
Functions: 2
============================================================


CLASS: Gemma3SGLangImageProcessor
----------------------------------------
  L  17: __init__(self, hf_config, server_args, _processor)

  L  31: process_mm_data_async(self, image_data: List[Union[str, bytes, Dict]], input_text, request_obj)


============================================================
FILE: python/sglang/srt/multimodal/processors/gemma3n.py
Functions: 2
============================================================


CLASS: Gemma3nSGLangProcessor
----------------------------------------
  L  29: __init__(self, hf_config, server_args, _processor)

  L  44: process_mm_data_async(self, image_data: Optional[List[Union[str, bytes, Dict]]], audio_data: Optional[List[Union[str, bytes, Dict]]], input_text: str, request_obj)
         üìù Process multimodal data including images and audio.


============================================================
FILE: python/sglang/srt/multimodal/processors/glm4v.py
Functions: 3
============================================================


CLASS: Glm4vImageProcessor
----------------------------------------
  L  22: __init__(self, hf_config, server_args, _processor)

  L  55: preprocess_video(self, vr: VideoReader)
         üìù Preprocess video using VideoReader from Decord backend.

  L  83: process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text, request_obj)


============================================================
FILE: python/sglang/srt/multimodal/processors/internvl.py
Functions: 6
============================================================


CLASS: InternVLImageProcessor
----------------------------------------
  L  20: __init__(self, hf_config, server_args, _image_processor)

  L  52: build_transform(input_size)

  L  81: dynamic_preprocess(image, min_num, max_num, image_size, use_thumbnail)

  L 145: get_index(bound, fps, max_frame, first_idx, num_segments)

  L 162: load_video(video_path, bound, input_size, max_num, num_segments)

  L 184: process_mm_data_async(self, image_data, input_text, request_obj)


============================================================
FILE: python/sglang/srt/multimodal/processors/janus_pro.py
Functions: 2
============================================================


CLASS: JanusProImageProcessor
----------------------------------------
  L  14: __init__(self, hf_config, server_args, _processor)

  L  22: process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text, request_obj)


============================================================
FILE: python/sglang/srt/multimodal/processors/kimi_vl.py
Functions: 2
============================================================


CLASS: KimiVLImageProcessor
----------------------------------------
  L  15: __init__(self, hf_config, server_args, _processor)

  L  24: process_mm_data_async(self, image_data: List[Union[str, bytes, Dict]], input_text, request_obj)


============================================================
FILE: python/sglang/srt/multimodal/processors/llava.py
Functions: 4
============================================================


CLASS: LlavaImageProcessor
----------------------------------------
  L  33: __init__(self, hf_config, server_args, _processor)

  L 109: process_mm_data_async(self, image_data: List[Union[str, bytes, ImageData]], input_text, request_obj)


CLASS: LlavaMultimodalProcessor
----------------------------------------
  L 194: __init__(self, hf_config, server_args, _processor)

  L 210: process_mm_data_async(self)


============================================================
FILE: python/sglang/srt/multimodal/processors/minicpm.py
Functions: 2
============================================================


CLASS: MiniCPMMultimodalProcessor
----------------------------------------
  L  18: __init__(self, hf_config, server_args, _processor)

  L  36: process_mm_data_async(self, image_data: List[Union[str, bytes]], audio_data: List[Union[str, bytes]], input_text, request_obj)


============================================================
FILE: python/sglang/srt/multimodal/processors/mlama.py
Functions: 2
============================================================


CLASS: MllamaImageProcessor
----------------------------------------
  L  13: __init__(self, hf_config, server_args, _processor)

  L  20: process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text)


============================================================
FILE: python/sglang/srt/multimodal/processors/mllama4.py
Functions: 2
============================================================


CLASS: Mllama4ImageProcessor
----------------------------------------
  L  21: __init__(self, hf_config, server_args, _processor)

  L  33: process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text)


============================================================
FILE: python/sglang/srt/multimodal/processors/phi4mm.py
Functions: 4
============================================================


CLASS: Phi4MMMultimodalProcessor
----------------------------------------
  L  50: __init__(self, hf_config, server_args, _processor)

  L  69: process_mm_data_async(self, image_data: List[Union[str, bytes]], audio_data, input_text, request_obj)


CLASS: Phi4MMProcessorAdapter
----------------------------------------
  L  19: __init__(self, _processor)
         ‚Üí None

  L  22: __call__(self)


============================================================
FILE: python/sglang/srt/multimodal/processors/pixtral.py
Functions: 3
============================================================


CLASS: PixtralProcessor
----------------------------------------
  L  23: get_patch_grid_size(self)
         ‚Üí tuple[int, int]

  L  45: __init__(self, hf_config, server_args, _processor)

  L  73: process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text, request_obj)


============================================================
FILE: python/sglang/srt/multimodal/processors/qwen_audio.py
Functions: 2
============================================================


CLASS: Qwen2AudioMultimodalProcessor
----------------------------------------
  L  14: __init__(self, hf_config, server_args, _processor)

  L  34: process_mm_data_async(self, audio_data, input_text)


============================================================
FILE: python/sglang/srt/multimodal/processors/qwen_vl.py
Functions: 10
============================================================

MODULE FUNCTIONS:
----------------------------------------
  L  37: def smart_resize(height: int,
        width: int,
        factor: int,
        min_pixels: int,
        max_pixels: int)
         ‚Üí tuple[int, int]
         üìù Rescales the image so that the following conditions are met:

  L  70: def resize_image(image, size_factor: int)
         ‚Üí Image.Image

  L  85: def round_by_factor(number: int, factor: int)
         ‚Üí int
         üìù Returns the closest integer to 'number' that is divisible by 'factor'.

  L  90: def ceil_by_factor(number: int, factor: int)
         ‚Üí int
         üìù Returns the smallest integer greater than or equal to 'number' that is

  L  95: def floor_by_factor(number: int, factor: int)
         ‚Üí int
         üìù Returns the largest integer less than or equal to 'number' that is div

  L 100: async def resize_image_async(image)

  L 104: def smart_nframes(ele: dict, total_frames: int, video_fps: int | float)
         ‚Üí int
         üìù calculate the number of frames for video used for model inputs.

  L 153: async def preprocess_video(vr, image_factor: int)
         ‚Üí torch.Tensor


CLASS: Qwen2_5VLImageProcessor
----------------------------------------
  L 204: __init__(self, hf_config, server_args, _processor)

  L 225: process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text, request_obj)


============================================================
FILE: python/sglang/srt/multimodal/processors/step3_vl.py
Functions: 21
============================================================


CLASS: GPUToTensor
----------------------------------------
  L  24: forward(self, raw_image: Union[np.ndarray, Image.Image])
         ‚Üí torch.Tensor


CLASS: ImagePatcher
----------------------------------------
  L  91: determine_window_size(self, long: int, short: int)
         ‚Üí int

  L  96: slide_window(self, width: int, height: int, sizes: list[tuple[int, int]], steps: list[tuple[int, int]], img_rate_thr: float)
         ‚Üí tuple[list[tuple[int, int, int, int]], tuple[int, int]]

  L 131: square_pad(self, img: Image.Image)
         ‚Üí Image.Image

  L 140: get_image_size_for_padding(self, img_width: int, img_height: int)
         ‚Üí tuple[int, int]

  L 149: get_image_size_for_preprocess(self, img_width: int, img_height: int)
         ‚Üí tuple[int, int]

  L 161: get_image_size_for_crop(self, img_width: int, img_height: int, window_size: int)

  L 181: patch_crop(self, img: Image.Image, i: int, j: int, th: int, tw: int)

  L 185: get_num_patches(self, img_width: int, img_height: int)
         ‚Üí tuple[int, int]

  L 210: __call__(self, img: Image.Image)
         ‚Üí tuple[Image.Image, list[Image.Image], list[bool] | None]


CLASS: Step3VLImageProcessor
----------------------------------------
  L 474: __init__(self, hf_config, server_args, _processor)

  L 488: preprocess(self, image)

  L 491: __call__(self, image)

  L 494: process_mm_data_async(self, image_data: List[Union[str, bytes]], input_text: str | List[int], request_obj)


CLASS: Step3VLProcessor
----------------------------------------
  L 271: __init__(self, config, tokenizer)
         ‚Üí None

  L 298: image_token_id(self)
         ‚Üí int

  L 301: get_num_image_tokens(self, img_width: int, img_height: int)
         ‚Üí int

  L 377: replace_placeholder(self, text: str, placeholder: str, repls: list[str])
         ‚Üí str

  L 392: __call__(self, text: Optional[Union[str, list[str]]], images: Optional[Union[Image.Image, list[Image.Image]]], return_tensors: Optional[Union[str, TensorType]])
         ‚Üí BatchFeature


CLASS: Step3VisionProcessor
----------------------------------------
  L  41: __init__(self, size, interpolation_mode, patch_size)

  L  82: __call__(self, image, is_patch)


============================================================
FILE: python/sglang/srt/multimodal/processors/vila.py
Functions: 2
============================================================


CLASS: VILAMultimodalProcessor
----------------------------------------
  L  32: __init__(self, hf_config: PretrainedConfig, server_args: ServerArgs, _processor: VILAProcessor)
         ‚Üí None

  L  47: process_mm_data_async(self, image_data: Optional[ImageDataInputItem | List[ImageDataInputItem]], input_text: str | List[int], request_obj: GenerateReqInput | EmbeddingReqInput)
         ‚Üí Optional[Dict[str, Any]]
