
# python/sglang/srt/distributed/device_communicators/cuda_wrapper.py
find_loaded_library(lib_name)
  CudaRTLibrary.__init__(so_file)
  CudaRTLibrary.CUDART_CHECK(result)
  CudaRTLibrary.cudaGetErrorString(error)
  CudaRTLibrary.cudaSetDevice(device)
  CudaRTLibrary.cudaDeviceSynchronize()
  CudaRTLibrary.cudaDeviceReset()
  CudaRTLibrary.cudaMalloc(size)
  CudaRTLibrary.cudaFree(devPtr)
  CudaRTLibrary.cudaMemset(devPtr, value, count)
  CudaRTLibrary.cudaMemcpy(dst, src, count)
  CudaRTLibrary.cudaIpcGetMemHandle(devPtr)
  CudaRTLibrary.cudaIpcOpenMemHandle(handle)

# python/sglang/srt/distributed/device_communicators/custom_all_reduce.py
  CustomAllreduce.__init__(group, device, str, torch.device], max_size)
  CustomAllreduce.create_shared_buffer(size_in_bytes, group)
  CustomAllreduce.free_shared_buffer(pointers, group)
  CustomAllreduce.capture()
  CustomAllreduce.register_buffer(inp)
  CustomAllreduce.register_graph_buffers()
  CustomAllreduce.should_custom_ar(inp)
  CustomAllreduce.all_reduce_reg(inp, out)
  CustomAllreduce.all_reduce_unreg(inp, out)
  CustomAllreduce.all_reduce(inp)
  CustomAllreduce.custom_all_reduce(input)
  CustomAllreduce.close()
  CustomAllreduce.__del__()

# python/sglang/srt/distributed/device_communicators/custom_all_reduce_utils.py
update_environment_variables(envs, str])
producer(batch_src, producer_queue, consumer_queue, result_queue, cuda_visible_devices)
consumer(batch_tgt, producer_queue, consumer_queue, result_queue, cuda_visible_devices)
can_actually_p2p(batch_src, batch_tgt)
gpu_p2p_access_check(src, tgt)
with_nvml_context(fn, _R])
is_full_nvlink(physical_device_ids, world_size)
is_weak_contiguous(inp)

# python/sglang/srt/distributed/device_communicators/hpu_communicator.py
  HpuCommunicator.__init__(group)
  HpuCommunicator.all_reduce(x)
  HpuCommunicator.all_gather(x, dim)

# python/sglang/srt/distributed/device_communicators/npu_communicator.py
  NpuCommunicator.__init__(group)
  NpuCommunicator.all_reduce(x)
  NpuCommunicator.all_gather(x, dim)

# python/sglang/srt/distributed/device_communicators/pymscclpp.py
mscclpp_is_weak_contiguous(inp)
mscclpp_convert_to_bytes(size_str)
mscclpp_bench_time(func, test_niter, warmup_niter)
  PyMscclppCommunicator.__init__(group, device, str, torch.device], max_bytes)
  PyMscclppCommunicator.pre_tune_config(dtype)
  PyMscclppCommunicator.should_mscclpp_allreduce(inp, op)
  PyMscclppCommunicator.all_reduce(tensor, op)
  PyMscclppCommunicator.change_state(enable)

# python/sglang/srt/distributed/device_communicators/pynccl.py
  PyNcclCommunicator.__init__(group, StatelessProcessGroup], device, str, torch.device], library_path)
  PyNcclCommunicator.all_reduce(tensor, op, stream)
  PyNcclCommunicator.all_gather(output_tensor, input_tensor, stream, sizes)
  PyNcclCommunicator.reduce_scatter(output_tensor, input_tensor, op, stream, sizes)
  PyNcclCommunicator.send(tensor, dst, stream)
  PyNcclCommunicator.recv(tensor, src, stream)
  PyNcclCommunicator.broadcast(tensor, src, stream)
  PyNcclCommunicator.register_comm_window_raw(ptr, size)
  PyNcclCommunicator.deregister_comm_window(window)
  PyNcclCommunicator.group_start()
  PyNcclCommunicator.group_end()
  PyNcclCommunicator.change_state(enable, stream)

# python/sglang/srt/distributed/device_communicators/pynccl_allocator.py
is_symmetric_memory_enabled()
set_graph_pool_id(graph_pool_id)
get_nccl_mem_pool()
  use_symmetric_memory.__init__(group_coordinator)
  use_symmetric_memory.__enter__()
  use_symmetric_memory.tag(tensor)
  use_symmetric_memory.__exit__(exc_type, exc_val, exc_tb)

# python/sglang/srt/distributed/device_communicators/pynccl_wrapper.py
find_nccl_library()
  ncclDataTypeEnum.from_torch(cls, dtype)
  ncclRedOpTypeEnum.from_torch(cls, op)
  NCCLLibrary.__init__(so_file)
  NCCLLibrary.ncclGetErrorString(result)
  NCCLLibrary.NCCL_CHECK(result)
  NCCLLibrary.ncclGetRawVersion()
  NCCLLibrary.ncclGetVersion()
  NCCLLibrary.ncclGetUniqueId()
  NCCLLibrary.ncclCommInitRank(world_size, unique_id, rank)
  NCCLLibrary.ncclAllReduce(sendbuff, recvbuff, count, datatype, op, comm, stream)
  NCCLLibrary.ncclReduce(sendbuff, recvbuff, count, datatype, op, root, comm, stream)
  NCCLLibrary.ncclReduceScatter(sendbuff, recvbuff, count, datatype, op, comm, stream)
  NCCLLibrary.ncclAllGather(sendbuff, recvbuff, count, datatype, comm, stream)
  NCCLLibrary.ncclSend(sendbuff, count, datatype, dest, comm, stream)
  NCCLLibrary.ncclRecv(recvbuff, count, datatype, src, comm, stream)
  NCCLLibrary.ncclBroadcast(sendbuff, recvbuff, count, datatype, root, comm, stream)
  NCCLLibrary.ncclCommDestroy(comm)
  NCCLLibrary.ncclCommWindowRegister(comm, buff, size, win_flags)
  NCCLLibrary.ncclCommWindowDeregister(comm, window)
  NCCLLibrary.ncclGroupStart()
  NCCLLibrary.ncclGroupEnd()

# python/sglang/srt/distributed/device_communicators/quick_all_reduce.py
qr_rocm_arch_available()
  QuickAllReduce.__init__(group, device, str, torch.device])
  QuickAllReduce.init_quick_all_reduce()
  QuickAllReduce.create_shared_buffer()
  QuickAllReduce.should_quick_allreduce(inp)
  QuickAllReduce.quick_all_reduce(inp)
  QuickAllReduce.close()
  QuickAllReduce.__del__()

# python/sglang/srt/distributed/device_communicators/shm_broadcast.py
  ShmRingBuffer.__init__(n_reader, max_chunk_bytes, max_chunks, name)
  ShmRingBuffer.__reduce__()
  ShmRingBuffer.__del__()
  ShmRingBuffer.get_data(current_idx)
  ShmRingBuffer.get_metadata(current_idx)
  MessageQueue.__init__(n_reader, n_local_reader, local_reader_ranks, max_chunk_bytes, max_chunks, connect_ip)
  MessageQueue.export_handle()
  MessageQueue.create_from_handle(handle, rank)
  MessageQueue.wait_until_ready()
  MessageQueue.acquire_write()
  MessageQueue.acquire_read()
  MessageQueue.enqueue(obj)
  MessageQueue.dequeue()
  MessageQueue.broadcast_object(obj)
  MessageQueue.create_from_process_group(pg, max_chunk_bytes, max_chunks, writer_rank)

# python/sglang/srt/distributed/device_communicators/xpu_communicator.py
  XpuCommunicator.__init__(group)
  XpuCommunicator.all_reduce(x)
  XpuCommunicator.gather(input_, rank_in_group, dst, dim)