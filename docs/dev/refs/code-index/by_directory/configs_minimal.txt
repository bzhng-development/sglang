
# python/sglang/srt/configs/chatglm.py
  ChatGLMConfig.__init__(num_layers, padded_vocab_size, hidden_size, ffn_hidden_size, kv_channels, num_attention_heads, seq_length, hidden_dropout, attention_dropout, layernorm_epsilon, rmsnorm, apply_residual_connection_post_layernorm, post_layer_norm, add_bias_linear, add_qkv_bias, interleaved_qkv, bias_dropout_fusion, multi_query_attention, multi_query_group_num, apply_query_key_layer_scaling, attention_softmax_in_fp32, fp32_residual_connection, quantization_bit, pre_seq_len, prefix_projection)

# python/sglang/srt/configs/dbrx.py
  DbrxAttentionConfig.__init__(attn_pdrop: float, clip_qkv: Optional[float], kv_n_heads: int, rope_theta: float)
  DbrxAttentionConfig.from_pretrained(cls, pretrained_model_name_or_path: str) -> 'PretrainedConfig'
  DbrxFFNConfig.__init__(ffn_act_fn: Optional[dict], ffn_hidden_size: int, moe_num_experts: int, moe_top_k: int, moe_jitter_eps: Optional[float], moe_loss_weight: float, moe_normalize_expert_weights: Optional[float], uniform_expert_assignment: bool)
  DbrxFFNConfig.from_pretrained(cls, pretrained_model_name_or_path: str) -> 'PretrainedConfig'
  DbrxConfig.__init__(d_model: int, n_heads: int, n_layers: int, max_seq_len: int, vocab_size: int, resid_pdrop: float, emb_pdrop: float, attn_config: Optional[DbrxAttentionConfig], ffn_config: Optional[DbrxFFNConfig], use_cache: bool, initializer_range: float, output_router_logits: bool, router_aux_loss_coef: float)

# python/sglang/srt/configs/deepseekvl2.py
select_best_resolution(image_size, candidate_resolutions)
  DictOutput.items()
  DictOutput.keys()
  DictOutput.__getitem__(item)
  DictOutput.__contains__(key)
  DictOutput.__setitem__(key, value)
  VLChatProcessorOutput.__len__()
  ImageTransform.__init__(mean: Optional[Tuple[float, float, float]], std: Optional[Tuple[float, float, float]], normalize: bool)
  ImageTransform.__call__(pil_img: Image.Image)
  DeepseekVLV2Processor.__init__(tokenizer: LlamaTokenizerFast, candidate_resolutions: Tuple[Tuple[int, int]], patch_size: int, downsample_ratio: int, image_mean: Tuple[float, float, float], image_std: Tuple[float, float, float], normalize: bool, image_token: str, pad_token: str, add_special_token: bool, sft_format: str, mask_prompt: bool, ignore_id: int)
  DeepseekVLV2Processor.format_messages_v2(messages, pil_images, max_req_input_len)
  DeepseekVLV2Processor.bos_id()
  DeepseekVLV2Processor.eos_id()
  DeepseekVLV2Processor.pad_id()
  DeepseekVLV2Processor.encode(text: str, bos: bool, eos: bool)
  DeepseekVLV2Processor.decode(t: List[int]) -> str
  DeepseekVLV2Processor.process_one(prompt: str, conversations: List[Dict[str, str]], images: List[Image.Image], apply_sft_format: bool, inference_mode: bool, system_prompt: str, max_req_input_len: int)
  DeepseekVLV2Processor.__call__()
  DeepseekVLV2Processor.find_all_indices(messages, target_value)
  DeepseekVLV2Processor.tokenize_with_images(conversation: str, images: List[Image.Image], bos: bool, eos: bool, cropping: bool, max_req_input_len: int)
  DeepseekVL2VisionEncoderConfig.__init__(model_name: str, image_size: int, patch_size: int, width: int, layers: int, heads: int, mlp_ratio: int, global_pool: str, ignore_head: bool, class_token: bool, num_classes: int, use_checkpoint: bool)
  DeepseekVL2MlpProjectorConfig.__init__(projector_type: str, input_dim: int, n_embed: int, depth: int, mlp_ratio: int, downsample_ratio: int)
  DeepseekV2Config.__init__(vocab_size, hidden_size, intermediate_size, moe_intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, n_shared_experts, n_routed_experts, ep_size, routed_scaling_factor, kv_lora_rank, q_lora_rank, qk_rope_head_dim, v_head_dim, qk_nope_head_dim, topk_method, n_group, topk_group, num_experts_per_tok, moe_layer_freq, first_k_dense_replace, norm_topk_prob, scoring_func, aux_loss_alpha, seq_aux, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, pretraining_tp, tie_word_embeddings, rope_theta, rope_scaling, attention_bias, attention_dropout, use_mla)
  DeepseekVL2Config.__init__(tile_tag: str, global_view_pos: str, candidate_resolutions: Tuple[Tuple[int, int]])

# python/sglang/srt/configs/device_config.py
  DeviceConfig.__init__(device: str) -> None

# python/sglang/srt/configs/exaone.py
  ExaoneConfig.__init__(vocab_size, max_position_embeddings, hidden_size, num_layers, num_attention_heads, num_key_value_heads, intermediate_size, activation_function, rope_theta, rope_scaling, embed_dropout, attention_dropout, layer_norm_epsilon, initializer_range, use_cache, bos_token_id, eos_token_id, tie_word_embeddings)

# python/sglang/srt/configs/internvl.py
  InternLM2Config.__init__(vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, tie_word_embeddings, bias, rope_theta, rope_scaling, attn_implementation)
  InternVisionConfig.__init__(num_channels, patch_size, image_size, qkv_bias, hidden_size, num_attention_heads, intermediate_size, qk_normalization, num_hidden_layers, use_flash_attn, hidden_act, layer_norm_eps, dropout, drop_path_rate, attention_dropout, initializer_range, initializer_factor)
  InternVisionConfig.from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike]) -> 'PretrainedConfig'
  InternVLChatConfig.__init__(vision_config, llm_config, use_backbone_lora, use_llm_lora, pad2square, select_layer, force_image_size, downsample_ratio, template, dynamic_image_size, use_thumbnail, ps_version, min_dynamic_patch, max_dynamic_patch)
  InternVLChatConfig.to_dict()
  InternLM2Tokenizer.__init__(vocab_file, unk_token, bos_token, eos_token, pad_token, sp_model_kwargs: Optional[Dict[str, Any]], add_bos_token, add_eos_token, decode_with_prefix_space, clean_up_tokenization_spaces)
  InternLM2Tokenizer.no_prefix_space_tokens()
  InternLM2Tokenizer.vocab_size()
  InternLM2Tokenizer.bos_token_id() -> Optional[int]
  InternLM2Tokenizer.eos_token_id() -> Optional[int]
  InternLM2Tokenizer.get_vocab()
  InternLM2Tokenizer.convert_tokens_to_string(tokens)
  InternLM2Tokenizer.save_vocabulary(save_directory, filename_prefix: Optional[str]) -> Tuple[str]
  InternLM2Tokenizer.build_inputs_with_special_tokens(token_ids_0, token_ids_1)
  InternLM2Tokenizer.get_special_tokens_mask(token_ids_0: List[int], token_ids_1: Optional[List[int]], already_has_special_tokens: bool) -> List[int]
  InternLM2Tokenizer.create_token_type_ids_from_sequences(token_ids_0: List[int], token_ids_1: Optional[List[int]]) -> List[int]

# python/sglang/srt/configs/janus_pro.py
  DictToObject.__init__(dictionary)
  VisionConfig.__init__()
  GenAlignerConfig.__init__()
  GenHeadConfig.__init__()
  AlignerConfig.__init__()
  GenVisionConfig.__init__()
  MultiModalityConfig.__init__()
  VLMImageProcessor.__init__(image_size: int, min_size: int, image_mean: Union[Tuple[float, float, float], List[float]], image_std: Union[Tuple[float, float, float], List[float]], rescale_factor: float, do_normalize: bool)
  VLMImageProcessor.resize(pil_img: Image) -> np.ndarray
  VLMImageProcessor.preprocess(images, return_tensors: str) -> BatchFeature
  VLMImageProcessor.default_shape()
  DictOutput.items()
  DictOutput.keys()
  DictOutput.__getitem__(item)
  DictOutput.__contains__(key)
  DictOutput.__setitem__(key, value)
  VLChatProcessorOutput.__len__()
  VLChatProcessor.__init__(image_processor: VLMImageProcessor, tokenizer: LlamaTokenizerFast, image_tag: str, image_start_tag: str, image_end_tag: str, pad_tag: str, num_image_tokens: int, add_special_token: bool, sft_format: str, mask_prompt: bool, ignore_id: int)
  VLChatProcessor.image_token()
  VLChatProcessor.image_id() -> int
  VLChatProcessor.image_start_id()
  VLChatProcessor.image_end_id()
  VLChatProcessor.image_start_token()
  VLChatProcessor.image_end_token()
  VLChatProcessor.pad_id()
  VLChatProcessor.add_image_token(image_indices: List[int], input_ids: torch.LongTensor)
  VLChatProcessor.process_one(prompt: str, images: List[Image])
  VLChatProcessor.__call__()
  VLChatProcessor.batchify(prepare_list: List[VLChatProcessorOutput]) -> BatchedVLChatProcessorOutput
  VLMImageProcessorConfig.__init__(image_size: int, min_size: int, image_mean: Union[Tuple[float, float, float], List[float]], image_std: Union[Tuple[float, float, float], List[float]], rescale_factor: float, do_normalize: bool)

# python/sglang/srt/configs/kimi_vl.py
  KimiVLConfig.__init__(vision_config: Optional[Union[dict, MoonViTConfig]], text_config: Optional[Union[dict, DeepseekV2Config]], ignore_index: int, media_placeholder_token_id: int, pad_token_id: int)

# python/sglang/srt/configs/kimi_vl_moonvit.py
  MoonViTConfig.__init__(patch_size: int, init_pos_emb_height: int, init_pos_emb_width: int, num_attention_heads: int, num_hidden_layers: int, hidden_size: int, intermediate_size: int, merge_kernel_size: tuple[int, int])

# python/sglang/srt/configs/load_config.py
  LoadConfig.__post_init__()

# python/sglang/srt/configs/longcat_flash.py
  LongcatFlashConfig.__init__(vocab_size, hidden_size, intermediate_size, ffn_hidden_size, expert_ffn_hidden_size, num_layers, num_hidden_layers, num_attention_heads, ep_size, kv_lora_rank, q_lora_rank, qk_rope_head_dim, qk_nope_head_dim, v_head_dim, n_routed_experts, moe_topk, norm_topk_prob, max_position_embeddings, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, pretraining_tp, tie_word_embeddings, rope_theta, rope_scaling, attention_bias, attention_dropout, mla_scale_q_lora, mla_scale_kv_lora, torch_dtype, params_dtype, rounter_params_dtype, router_bias, topk_method, routed_scaling_factor, zero_expert_num, zero_expert_type, nextn_use_scmoe, num_nextn_predict_layers)

# python/sglang/srt/configs/model_config.py
  ModelConfig.__init__(model_path: str, trust_remote_code: bool, revision: Optional[str], context_length: Optional[int], model_override_args: str, is_embedding: Optional[bool], enable_multimodal: Optional[bool], dtype: str, quantization: Optional[str], override_config_file: Optional[str], is_draft_model: bool, hybrid_kvcache_ratio: Optional[float], model_impl: Union[str, ModelImpl]) -> None
  ModelConfig.from_server_args(server_args: ServerArgs, model_path: str)
  ModelConfig.get_total_num_attention_heads() -> int
  ModelConfig.get_num_attention_heads(tensor_parallel_size) -> int
  ModelConfig.get_total_num_kv_heads() -> int
  ModelConfig.get_num_kv_heads(tensor_parallel_size) -> int
  ModelConfig.get_hf_eos_token_id() -> Optional[Set[int]]
  ModelConfig.maybe_pull_model_tokenizer_from_remote() -> None
is_generation_model(model_architectures: List[str], is_embedding: bool)
is_multimodal_model(model_architectures: List[str])
is_multimodal_gen_model(model_architectures: List[str])
is_image_gen_model(model_architectures: List[str])
is_audio_model(model_architectures: List[str])
is_encoder_decoder_model(model_architectures: List[str])
is_multimodal_chunked_prefill_supported(model_architectures: List[str])
yarn_get_mscale(scale: float, mscale: float) -> float
is_hybrid_model(model_architectures: List[str], hybrid_kvcache_ratio: Optional[float], context_length: Optional[int], attention_chunk_size: Optional[int])
get_hybrid_layer_ids(model_architectures: List[str], num_hidden_layers: int)

# python/sglang/srt/configs/step3_vl.py
  Step3VisionEncoderConfig.__init__(hidden_size, intermediate_size, output_hidden_size, num_hidden_layers, num_attention_heads, num_channels, image_size, patch_size, hidden_act, layer_norm_eps)
  Step3TextConfig.__init__(hidden_size: int, intermediate_size: int, num_attention_heads: int, num_attention_groups: int, num_hidden_layers: int, max_seq_len: int, vocab_size: int, rms_norm_eps: float, moe_intermediate_size: int, moe_num_experts: int, moe_top_k: int, rope_theta: float, rope_scaling: Optional[dict[str, Any]], max_position_embedding: int, share_expert_dim: int, share_q_dim: int, head_dim: int, norm_expert_weight: bool, moe_layers_enum: tuple[int]) -> None
  Step3VLConfig.__init__(vision_config: Optional[Union[dict, Step3VisionEncoderConfig]], text_config: Optional[Union[dict, Step3TextConfig]], understand_projector_stride: int, projector_bias: bool, image_token_id: int) -> None

# python/sglang/srt/configs/update_config.py
may_get_weight_block_size(model_config, load_config)
get_moe_padding_size(weight_block_size)
get_num_heads_padding_size(tp_size, weight_block_size)
update_intermediate_size(model_config, attr_name, intermediate_padding_size)
adjust_config_with_unaligned_cpu_tp(model_config: ModelConfig, load_config: LoadConfig, tp_size: int) -> ModelConfig

# python/sglang/srt/configs/utils.py
register_image_processor(config: Type[PretrainedConfig], image_processor: Type[BaseImageProcessor])
register_processor(config: Type[PretrainedConfig], processor: Type[ProcessorMixin])
