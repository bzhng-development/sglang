
# python/sglang/srt/configs/chatglm.py
  ChatGLMConfig.__init__(num_layers, padded_vocab_size, hidden_size, ffn_hidden_size, kv_channels, num_attention_heads, seq_length, hidden_dropout, attention_dropout, layernorm_epsilon, rmsnorm, apply_residual_connection_post_layernorm, post_layer_norm, add_bias_linear, add_qkv_bias, interleaved_qkv, bias_dropout_fusion, multi_query_attention, multi_query_group_num, apply_query_key_layer_scaling, attention_softmax_in_fp32, fp32_residual_connection, quantization_bit, pre_seq_len, prefix_projection)

# python/sglang/srt/configs/dbrx.py
  DbrxAttentionConfig.__init__(attn_pdrop, clip_qkv, kv_n_heads, rope_theta)
  DbrxAttentionConfig.from_pretrained(cls, pretrained_model_name_or_path)
  DbrxFFNConfig.__init__(ffn_act_fn, ffn_hidden_size, moe_num_experts, moe_top_k, moe_jitter_eps, moe_loss_weight, moe_normalize_expert_weights, uniform_expert_assignment)
  DbrxFFNConfig.from_pretrained(cls, pretrained_model_name_or_path)
  DbrxConfig.__init__(d_model, n_heads, n_layers, max_seq_len, vocab_size, resid_pdrop, emb_pdrop, attn_config, ffn_config, use_cache, initializer_range, output_router_logits, router_aux_loss_coef)

# python/sglang/srt/configs/deepseekvl2.py
select_best_resolution(image_size, candidate_resolutions)
  DictOutput.items()
  DictOutput.keys()
  DictOutput.__getitem__(item)
  DictOutput.__contains__(key)
  DictOutput.__setitem__(key, value)
  VLChatProcessorOutput.__len__()
  ImageTransform.__init__(mean, float, float]], std, float, float]], normalize)
  ImageTransform.__call__(pil_img)
  DeepseekVLV2Processor.__init__(tokenizer, candidate_resolutions, int]], patch_size, downsample_ratio, image_mean, float, float], image_std, float, float], normalize, image_token, pad_token, add_special_token, sft_format, mask_prompt, ignore_id)
  DeepseekVLV2Processor.format_messages_v2(messages, pil_images, max_req_input_len)
  DeepseekVLV2Processor.bos_id()
  DeepseekVLV2Processor.eos_id()
  DeepseekVLV2Processor.pad_id()
  DeepseekVLV2Processor.encode(text, bos, eos)
  DeepseekVLV2Processor.decode(t)
  DeepseekVLV2Processor.process_one(prompt, conversations, str]], images, apply_sft_format, inference_mode, system_prompt, max_req_input_len)
  DeepseekVLV2Processor.__call__()
  DeepseekVLV2Processor.find_all_indices(messages, target_value)
  DeepseekVLV2Processor.tokenize_with_images(conversation, images, bos, eos, cropping, max_req_input_len)
  DeepseekVL2VisionEncoderConfig.__init__(model_name, image_size, patch_size, width, layers, heads, mlp_ratio, global_pool, ignore_head, class_token, num_classes, use_checkpoint)
  DeepseekVL2MlpProjectorConfig.__init__(projector_type, input_dim, n_embed, depth, mlp_ratio, downsample_ratio)
  DeepseekV2Config.__init__(vocab_size, hidden_size, intermediate_size, moe_intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, n_shared_experts, n_routed_experts, ep_size, routed_scaling_factor, kv_lora_rank, q_lora_rank, qk_rope_head_dim, v_head_dim, qk_nope_head_dim, topk_method, n_group, topk_group, num_experts_per_tok, moe_layer_freq, first_k_dense_replace, norm_topk_prob, scoring_func, aux_loss_alpha, seq_aux, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, pretraining_tp, tie_word_embeddings, rope_theta, rope_scaling, attention_bias, attention_dropout, use_mla)
  DeepseekVL2Config.__init__(tile_tag, global_view_pos, candidate_resolutions, int]])

# python/sglang/srt/configs/device_config.py
  DeviceConfig.__init__(device)

# python/sglang/srt/configs/exaone.py
  ExaoneConfig.__init__(vocab_size, max_position_embeddings, hidden_size, num_layers, num_attention_heads, num_key_value_heads, intermediate_size, activation_function, rope_theta, rope_scaling, embed_dropout, attention_dropout, layer_norm_epsilon, initializer_range, use_cache, bos_token_id, eos_token_id, tie_word_embeddings)

# python/sglang/srt/configs/internvl.py
  InternLM2Config.__init__(vocab_size, hidden_size, intermediate_size, num_hidden_layers, num_attention_heads, num_key_value_heads, hidden_act, max_position_embeddings, initializer_range, rms_norm_eps, use_cache, pad_token_id, bos_token_id, eos_token_id, tie_word_embeddings, bias, rope_theta, rope_scaling, attn_implementation)
  InternVisionConfig.__init__(num_channels, patch_size, image_size, qkv_bias, hidden_size, num_attention_heads, intermediate_size, qk_normalization, num_hidden_layers, use_flash_attn, hidden_act, layer_norm_eps, dropout, drop_path_rate, attention_dropout, initializer_range, initializer_factor)
  InternVisionConfig.from_pretrained(cls, pretrained_model_name_or_path, os.PathLike])
  InternVLChatConfig.__init__(vision_config, llm_config, use_backbone_lora, use_llm_lora, pad2square, select_layer, force_image_size, downsample_ratio, template, dynamic_image_size, use_thumbnail, ps_version, min_dynamic_patch, max_dynamic_patch)
  InternVLChatConfig.to_dict()
  InternLM2Tokenizer.__init__(vocab_file, unk_token, bos_token, eos_token, pad_token, sp_model_kwargs, Any]], add_bos_token, add_eos_token, decode_with_prefix_space, clean_up_tokenization_spaces)
  InternLM2Tokenizer.no_prefix_space_tokens()
  InternLM2Tokenizer.vocab_size()
  InternLM2Tokenizer.bos_token_id()
  InternLM2Tokenizer.eos_token_id()
  InternLM2Tokenizer.get_vocab()
  InternLM2Tokenizer.convert_tokens_to_string(tokens)
  InternLM2Tokenizer.save_vocabulary(save_directory, filename_prefix)
  InternLM2Tokenizer.build_inputs_with_special_tokens(token_ids_0, token_ids_1)
  InternLM2Tokenizer.get_special_tokens_mask(token_ids_0, token_ids_1, already_has_special_tokens)
  InternLM2Tokenizer.create_token_type_ids_from_sequences(token_ids_0, token_ids_1)

# python/sglang/srt/configs/janus_pro.py
  DictToObject.__init__(dictionary)
  VisionConfig.__init__()
  GenAlignerConfig.__init__()
  GenHeadConfig.__init__()
  AlignerConfig.__init__()
  GenVisionConfig.__init__()
  MultiModalityConfig.__init__()
  VLMImageProcessor.__init__(image_size, min_size, image_mean, float, float], List[float]], image_std, float, float], List[float]], rescale_factor, do_normalize)
  VLMImageProcessor.resize(pil_img)
  VLMImageProcessor.preprocess(images, return_tensors)
  VLMImageProcessor.default_shape()
  DictOutput.items()
  DictOutput.keys()
  DictOutput.__getitem__(item)
  DictOutput.__contains__(key)
  DictOutput.__setitem__(key, value)
  VLChatProcessorOutput.__len__()
  VLChatProcessor.__init__(image_processor, tokenizer, image_tag, image_start_tag, image_end_tag, pad_tag, num_image_tokens, add_special_token, sft_format, mask_prompt, ignore_id)
  VLChatProcessor.image_token()
  VLChatProcessor.image_id()
  VLChatProcessor.image_start_id()
  VLChatProcessor.image_end_id()
  VLChatProcessor.image_start_token()
  VLChatProcessor.image_end_token()
  VLChatProcessor.pad_id()
  VLChatProcessor.add_image_token(image_indices, input_ids)
  VLChatProcessor.process_one(prompt, images)
  VLChatProcessor.__call__()
  VLChatProcessor.batchify(prepare_list)
  VLMImageProcessorConfig.__init__(image_size, min_size, image_mean, float, float], List[float]], image_std, float, float], List[float]], rescale_factor, do_normalize)

# python/sglang/srt/configs/kimi_vl.py
  KimiVLConfig.__init__(vision_config, MoonViTConfig]], text_config, DeepseekV2Config]], ignore_index, media_placeholder_token_id, pad_token_id)

# python/sglang/srt/configs/kimi_vl_moonvit.py
  MoonViTConfig.__init__(patch_size, init_pos_emb_height, init_pos_emb_width, num_attention_heads, num_hidden_layers, hidden_size, intermediate_size, merge_kernel_size, int])

# python/sglang/srt/configs/load_config.py
  LoadConfig.__post_init__()

# python/sglang/srt/configs/model_config.py
  ModelConfig.__init__(model_path, trust_remote_code, revision, context_length, model_override_args, is_embedding, enable_multimodal, dtype, quantization, override_config_file, is_draft_model, hybrid_kvcache_ratio, model_impl, ModelImpl])
  ModelConfig.from_server_args(server_args, model_path)
  ModelConfig.get_total_num_attention_heads()
  ModelConfig.get_num_attention_heads(tensor_parallel_size)
  ModelConfig.get_total_num_kv_heads()
  ModelConfig.get_num_kv_heads(tensor_parallel_size)
  ModelConfig.get_hf_eos_token_id()
  ModelConfig.maybe_pull_model_tokenizer_from_remote()
is_generation_model(model_architectures, is_embedding)
is_multimodal_model(model_architectures)
is_multimodal_gen_model(model_architectures)
is_image_gen_model(model_architectures)
is_audio_model(model_architectures)
is_encoder_decoder_model(model_architectures)
is_multimodal_chunked_prefill_supported(model_architectures)
yarn_get_mscale(scale, mscale)
is_hybrid_model(model_architectures, hybrid_kvcache_ratio, context_length, attention_chunk_size)
get_hybrid_layer_ids(model_architectures, num_hidden_layers)

# python/sglang/srt/configs/step3_vl.py
  Step3VisionEncoderConfig.__init__(hidden_size, intermediate_size, output_hidden_size, num_hidden_layers, num_attention_heads, num_channels, image_size, patch_size, hidden_act, layer_norm_eps)
  Step3TextConfig.__init__(hidden_size, intermediate_size, num_attention_heads, num_attention_groups, num_hidden_layers, max_seq_len, vocab_size, rms_norm_eps, moe_intermediate_size, moe_num_experts, moe_top_k, rope_theta, rope_scaling, Any]], max_position_embedding, share_expert_dim, share_q_dim, head_dim, norm_expert_weight, moe_layers_enum)
  Step3VLConfig.__init__(vision_config, Step3VisionEncoderConfig]], text_config, Step3TextConfig]], understand_projector_stride, projector_bias, image_token_id)

# python/sglang/srt/configs/update_config.py
may_get_weight_block_size(model_config, load_config)
get_moe_padding_size(weight_block_size)
get_num_heads_padding_size(tp_size, weight_block_size)
update_intermediate_size(model_config, attr_name, intermediate_padding_size)
adjust_config_with_unaligned_cpu_tp(model_config, load_config, tp_size)

# python/sglang/srt/configs/utils.py
register_image_processor(config, image_processor)
register_processor(config, processor)