<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 15.0.4"/>
    <title>sglang.srt.configs API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:.75rem center;margin-bottom:1rem;}.pdoc .alert > em{display:none;}.pdoc .alert > *:last-child{margin-bottom:0;}.pdoc .alert.note{color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .alert.tip{color:#0a3622;background-color:#d1e7dd;border-color:#a3cfbb;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%230a3622%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2%206a6%206%200%201%201%2010.174%204.31c-.203.196-.359.4-.453.619l-.762%201.769A.5.5%200%200%201%2010.5%2013a.5.5%200%200%201%200%201%20.5.5%200%200%201%200%201l-.224.447a1%201%200%200%201-.894.553H6.618a1%201%200%200%201-.894-.553L5.5%2015a.5.5%200%200%201%200-1%20.5.5%200%200%201%200-1%20.5.5%200%200%201-.46-.302l-.761-1.77a2%202%200%200%200-.453-.618A5.98%205.98%200%200%201%202%206m6-5a5%205%200%200%200-3.479%208.592c.263.254.514.564.676.941L5.83%2012h4.342l.632-1.467c.162-.377.413-.687.676-.941A5%205%200%200%200%208%201%22/%3E%3C/svg%3E");}.pdoc .alert.important{color:#055160;background-color:#cff4fc;border-color:#9eeaf9;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23055160%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2%200a2%202%200%200%200-2%202v12a2%202%200%200%200%202%202h12a2%202%200%200%200%202-2V2a2%202%200%200%200-2-2zm6%204c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%204.995A.905.905%200%200%201%208%204m.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2%22/%3E%3C/svg%3E");}.pdoc .alert.warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .alert.caution{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M11.46.146A.5.5%200%200%200%2011.107%200H4.893a.5.5%200%200%200-.353.146L.146%204.54A.5.5%200%200%200%200%204.893v6.214a.5.5%200%200%200%20.146.353l4.394%204.394a.5.5%200%200%200%20.353.146h6.214a.5.5%200%200%200%20.353-.146l4.394-4.394a.5.5%200%200%200%20.146-.353V4.893a.5.5%200%200%200-.146-.353zM8%204c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%204.995A.905.905%200%200%201%208%204m.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2%22/%3E%3C/svg%3E");}.pdoc .alert.danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .decorator-deprecated{color:#842029;}.pdoc .decorator-deprecated ~ span{filter:grayscale(1) opacity(0.8);}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../srt.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;sglang.srt</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="class" href="#ExaoneConfig">ExaoneConfig</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ExaoneConfig.__init__">ExaoneConfig</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.model_type">model_type</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.keys_to_ignore_at_inference">keys_to_ignore_at_inference</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.attribute_map">attribute_map</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.vocab_size">vocab_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.max_position_embeddings">max_position_embeddings</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.hidden_size">hidden_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.num_layers">num_layers</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.num_attention_heads">num_attention_heads</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.num_hidden_layers">num_hidden_layers</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.num_key_value_heads">num_key_value_heads</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.activation_function">activation_function</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.embed_dropout">embed_dropout</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.attention_dropout">attention_dropout</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.layer_norm_epsilon">layer_norm_epsilon</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.initializer_range">initializer_range</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.use_cache">use_cache</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.rope_theta">rope_theta</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.rope_scaling">rope_scaling</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.bos_token_id">bos_token_id</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExaoneConfig.eos_token_id">eos_token_id</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#ChatGLMConfig">ChatGLMConfig</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ChatGLMConfig.__init__">ChatGLMConfig</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.model_type">model_type</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.attribute_map">attribute_map</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.num_layers">num_layers</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.vocab_size">vocab_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.padded_vocab_size">padded_vocab_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.hidden_size">hidden_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.ffn_hidden_size">ffn_hidden_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.kv_channels">kv_channels</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.num_attention_heads">num_attention_heads</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.seq_length">seq_length</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.max_position_embeddings">max_position_embeddings</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.hidden_dropout">hidden_dropout</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.attention_dropout">attention_dropout</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.layernorm_epsilon">layernorm_epsilon</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.rmsnorm">rmsnorm</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.apply_residual_connection_post_layernorm">apply_residual_connection_post_layernorm</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.post_layer_norm">post_layer_norm</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.add_bias_linear">add_bias_linear</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.add_qkv_bias">add_qkv_bias</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.bias_dropout_fusion">bias_dropout_fusion</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.multi_query_attention">multi_query_attention</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.multi_query_group_num">multi_query_group_num</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.apply_query_key_layer_scaling">apply_query_key_layer_scaling</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.attention_softmax_in_fp32">attention_softmax_in_fp32</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.fp32_residual_connection">fp32_residual_connection</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.quantization_bit">quantization_bit</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.pre_seq_len">pre_seq_len</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.prefix_projection">prefix_projection</a>
                        </li>
                        <li>
                                <a class="variable" href="#ChatGLMConfig.interleaved_qkv">interleaved_qkv</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#DbrxConfig">DbrxConfig</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#DbrxConfig.__init__">DbrxConfig</a>
                        </li>
                        <li>
                                <a class="variable" href="#DbrxConfig.model_type">model_type</a>
                        </li>
                        <li>
                                <a class="variable" href="#DbrxConfig.attribute_map">attribute_map</a>
                        </li>
                        <li>
                                <a class="variable" href="#DbrxConfig.d_model">d_model</a>
                        </li>
                        <li>
                                <a class="variable" href="#DbrxConfig.n_heads">n_heads</a>
                        </li>
                        <li>
                                <a class="variable" href="#DbrxConfig.n_layers">n_layers</a>
                        </li>
                        <li>
                                <a class="variable" href="#DbrxConfig.max_seq_len">max_seq_len</a>
                        </li>
                        <li>
                                <a class="variable" href="#DbrxConfig.vocab_size">vocab_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#DbrxConfig.resid_pdrop">resid_pdrop</a>
                        </li>
                        <li>
                                <a class="variable" href="#DbrxConfig.emb_pdrop">emb_pdrop</a>
                        </li>
                        <li>
                                <a class="variable" href="#DbrxConfig.use_cache">use_cache</a>
                        </li>
                        <li>
                                <a class="variable" href="#DbrxConfig.initializer_range">initializer_range</a>
                        </li>
                        <li>
                                <a class="variable" href="#DbrxConfig.output_router_logits">output_router_logits</a>
                        </li>
                        <li>
                                <a class="variable" href="#DbrxConfig.router_aux_loss_coef">router_aux_loss_coef</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#DeepseekVL2Config">DeepseekVL2Config</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#DeepseekVL2Config.__init__">DeepseekVL2Config</a>
                        </li>
                        <li>
                                <a class="variable" href="#DeepseekVL2Config.model_type">model_type</a>
                        </li>
                        <li>
                                <a class="variable" href="#DeepseekVL2Config.vision_config">vision_config</a>
                        </li>
                        <li>
                                <a class="variable" href="#DeepseekVL2Config.projector_config">projector_config</a>
                        </li>
                        <li>
                                <a class="variable" href="#DeepseekVL2Config.language_config">language_config</a>
                        </li>
                        <li>
                                <a class="variable" href="#DeepseekVL2Config.tile_tag">tile_tag</a>
                        </li>
                        <li>
                                <a class="variable" href="#DeepseekVL2Config.global_view_pos">global_view_pos</a>
                        </li>
                        <li>
                                <a class="variable" href="#DeepseekVL2Config.candidate_resolutions">candidate_resolutions</a>
                        </li>
                        <li>
                                <a class="variable" href="#DeepseekVL2Config.architectures">architectures</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#MultiModalityConfig">MultiModalityConfig</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MultiModalityConfig.__init__">MultiModalityConfig</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiModalityConfig.model_type">model_type</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiModalityConfig.vision_config">vision_config</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiModalityConfig.aligner_config">aligner_config</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiModalityConfig.gen_vision_config">gen_vision_config</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiModalityConfig.gen_aligner_config">gen_aligner_config</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiModalityConfig.gen_head_config">gen_head_config</a>
                        </li>
                        <li>
                                <a class="variable" href="#MultiModalityConfig.language_config">language_config</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#KimiVLConfig">KimiVLConfig</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#KimiVLConfig.__init__">KimiVLConfig</a>
                        </li>
                        <li>
                                <a class="variable" href="#KimiVLConfig.model_type">model_type</a>
                        </li>
                        <li>
                                <a class="variable" href="#KimiVLConfig.vision_config">vision_config</a>
                        </li>
                        <li>
                                <a class="variable" href="#KimiVLConfig.text_config">text_config</a>
                        </li>
                        <li>
                                <a class="variable" href="#KimiVLConfig.ignore_index">ignore_index</a>
                        </li>
                        <li>
                                <a class="variable" href="#KimiVLConfig.media_placeholder_token_id">media_placeholder_token_id</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#MoonViTConfig">MoonViTConfig</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MoonViTConfig.__init__">MoonViTConfig</a>
                        </li>
                        <li>
                                <a class="variable" href="#MoonViTConfig.model_type">model_type</a>
                        </li>
                        <li>
                                <a class="variable" href="#MoonViTConfig.patch_size">patch_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#MoonViTConfig.init_pos_emb_height">init_pos_emb_height</a>
                        </li>
                        <li>
                                <a class="variable" href="#MoonViTConfig.init_pos_emb_width">init_pos_emb_width</a>
                        </li>
                        <li>
                                <a class="variable" href="#MoonViTConfig.num_hidden_layers">num_hidden_layers</a>
                        </li>
                        <li>
                                <a class="variable" href="#MoonViTConfig.num_attention_heads">num_attention_heads</a>
                        </li>
                        <li>
                                <a class="variable" href="#MoonViTConfig.hidden_size">hidden_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#MoonViTConfig.intermediate_size">intermediate_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#MoonViTConfig.merge_kernel_size">merge_kernel_size</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#Step3VLConfig">Step3VLConfig</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#Step3VLConfig.__init__">Step3VLConfig</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3VLConfig.model_type">model_type</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3VLConfig.vision_config">vision_config</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3VLConfig.text_config">text_config</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3VLConfig.understand_projector_stride">understand_projector_stride</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3VLConfig.projector_bias">projector_bias</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3VLConfig.hidden_size">hidden_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3VLConfig.image_token_id">image_token_id</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#Step3TextConfig">Step3TextConfig</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#Step3TextConfig.__init__">Step3TextConfig</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.model_type">model_type</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.architectures">architectures</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.hidden_size">hidden_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.intermediate_size">intermediate_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.num_attention_heads">num_attention_heads</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.num_attention_groups">num_attention_groups</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.num_hidden_layers">num_hidden_layers</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.max_seq_len">max_seq_len</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.vocab_size">vocab_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.rms_norm_eps">rms_norm_eps</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.moe_intermediate_size">moe_intermediate_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.moe_num_experts">moe_num_experts</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.moe_top_k">moe_top_k</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.rope_theta">rope_theta</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.rope_scaling">rope_scaling</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.max_position_embedding">max_position_embedding</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.share_expert_dim">share_expert_dim</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.share_q_dim">share_q_dim</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.head_dim">head_dim</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.norm_expert_weight">norm_expert_weight</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3TextConfig.moe_layers_enum">moe_layers_enum</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#Step3VisionEncoderConfig">Step3VisionEncoderConfig</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#Step3VisionEncoderConfig.__init__">Step3VisionEncoderConfig</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3VisionEncoderConfig.model_type">model_type</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3VisionEncoderConfig.hidden_size">hidden_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3VisionEncoderConfig.intermediate_size">intermediate_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3VisionEncoderConfig.output_hidden_size">output_hidden_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3VisionEncoderConfig.num_hidden_layers">num_hidden_layers</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3VisionEncoderConfig.num_attention_heads">num_attention_heads</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3VisionEncoderConfig.num_channels">num_channels</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3VisionEncoderConfig.patch_size">patch_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3VisionEncoderConfig.image_size">image_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3VisionEncoderConfig.layer_norm_eps">layer_norm_eps</a>
                        </li>
                        <li>
                                <a class="variable" href="#Step3VisionEncoderConfig.hidden_act">hidden_act</a>
                        </li>
                </ul>

            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
sglang<wbr>.<a href="./../srt.html">srt</a><wbr>.configs    </h1>

                
                        <input id="mod-configs-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-configs-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos"> 1</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">sglang.srt.configs.chatglm</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatGLMConfig</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos"> 2</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">sglang.srt.configs.dbrx</span><span class="w"> </span><span class="kn">import</span> <span class="n">DbrxConfig</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos"> 3</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">sglang.srt.configs.deepseekvl2</span><span class="w"> </span><span class="kn">import</span> <span class="n">DeepseekVL2Config</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos"> 4</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">sglang.srt.configs.exaone</span><span class="w"> </span><span class="kn">import</span> <span class="n">ExaoneConfig</span>
</span><span id="L-5"><a href="#L-5"><span class="linenos"> 5</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">sglang.srt.configs.janus_pro</span><span class="w"> </span><span class="kn">import</span> <span class="n">MultiModalityConfig</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos"> 6</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">sglang.srt.configs.kimi_vl</span><span class="w"> </span><span class="kn">import</span> <span class="n">KimiVLConfig</span>
</span><span id="L-7"><a href="#L-7"><span class="linenos"> 7</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">sglang.srt.configs.kimi_vl_moonvit</span><span class="w"> </span><span class="kn">import</span> <span class="n">MoonViTConfig</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos"> 8</span></a><span class="kn">from</span><span class="w"> </span><span class="nn">sglang.srt.configs.step3_vl</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos"> 9</span></a>    <span class="n">Step3TextConfig</span><span class="p">,</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos">10</span></a>    <span class="n">Step3VisionEncoderConfig</span><span class="p">,</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos">11</span></a>    <span class="n">Step3VLConfig</span><span class="p">,</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos">12</span></a><span class="p">)</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos">13</span></a>
</span><span id="L-14"><a href="#L-14"><span class="linenos">14</span></a><span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos">15</span></a>    <span class="s2">&quot;ExaoneConfig&quot;</span><span class="p">,</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos">16</span></a>    <span class="s2">&quot;ChatGLMConfig&quot;</span><span class="p">,</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos">17</span></a>    <span class="s2">&quot;DbrxConfig&quot;</span><span class="p">,</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos">18</span></a>    <span class="s2">&quot;DeepseekVL2Config&quot;</span><span class="p">,</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos">19</span></a>    <span class="s2">&quot;MultiModalityConfig&quot;</span><span class="p">,</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos">20</span></a>    <span class="s2">&quot;KimiVLConfig&quot;</span><span class="p">,</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos">21</span></a>    <span class="s2">&quot;MoonViTConfig&quot;</span><span class="p">,</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos">22</span></a>    <span class="s2">&quot;Step3VLConfig&quot;</span><span class="p">,</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos">23</span></a>    <span class="s2">&quot;Step3TextConfig&quot;</span><span class="p">,</span>
</span><span id="L-24"><a href="#L-24"><span class="linenos">24</span></a>    <span class="s2">&quot;Step3VisionEncoderConfig&quot;</span><span class="p">,</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos">25</span></a><span class="p">]</span>
</span></pre></div>


            </section>
                <section id="ExaoneConfig">
                            <input id="ExaoneConfig-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">ExaoneConfig</span><wbr>(<span class="base">transformers.configuration_utils.PretrainedConfig</span>):

                <label class="view-source-button" for="ExaoneConfig-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ExaoneConfig"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ExaoneConfig-30"><a href="#ExaoneConfig-30"><span class="linenos"> 30</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ExaoneConfig</span><span class="p">(</span><span class="n">PretrainedConfig</span><span class="p">):</span>
</span><span id="ExaoneConfig-31"><a href="#ExaoneConfig-31"><span class="linenos"> 31</span></a><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="ExaoneConfig-32"><a href="#ExaoneConfig-32"><span class="linenos"> 32</span></a><span class="sd">    This is the configuration class to store the configuration of a :class:`~transformers.ExaoneModel`. It is used to</span>
</span><span id="ExaoneConfig-33"><a href="#ExaoneConfig-33"><span class="linenos"> 33</span></a><span class="sd">    instantiate a EXAONE model according to the specified arguments, defining the model architecture. Instantiating a</span>
</span><span id="ExaoneConfig-34"><a href="#ExaoneConfig-34"><span class="linenos"> 34</span></a><span class="sd">    configuration with the defaults will yield a similar configuration to that of the Exaone</span>
</span><span id="ExaoneConfig-35"><a href="#ExaoneConfig-35"><span class="linenos"> 35</span></a>
</span><span id="ExaoneConfig-36"><a href="#ExaoneConfig-36"><span class="linenos"> 36</span></a><span class="sd">    Configuration objects inherit from :class:`~transformers.PretrainedConfig` and can be used to control the model</span>
</span><span id="ExaoneConfig-37"><a href="#ExaoneConfig-37"><span class="linenos"> 37</span></a><span class="sd">    outputs. Read the documentation from :class:`~transformers.PretrainedConfig` for more information.</span>
</span><span id="ExaoneConfig-38"><a href="#ExaoneConfig-38"><span class="linenos"> 38</span></a>
</span><span id="ExaoneConfig-39"><a href="#ExaoneConfig-39"><span class="linenos"> 39</span></a>
</span><span id="ExaoneConfig-40"><a href="#ExaoneConfig-40"><span class="linenos"> 40</span></a><span class="sd">    Args:</span>
</span><span id="ExaoneConfig-41"><a href="#ExaoneConfig-41"><span class="linenos"> 41</span></a><span class="sd">        vocab_size (:obj:`int`, `optional`, defaults to 102400):</span>
</span><span id="ExaoneConfig-42"><a href="#ExaoneConfig-42"><span class="linenos"> 42</span></a><span class="sd">            Vocabulary size of the EXAONE model. Defines the number of different tokens that can be represented by the</span>
</span><span id="ExaoneConfig-43"><a href="#ExaoneConfig-43"><span class="linenos"> 43</span></a><span class="sd">            :obj:`inputs_ids` passed when calling :class:`~transformers.ExaoneModel`. Vocabulary size of the model.</span>
</span><span id="ExaoneConfig-44"><a href="#ExaoneConfig-44"><span class="linenos"> 44</span></a><span class="sd">            Defines the different tokens that can be represented by the `inputs_ids` passed to the forward method of</span>
</span><span id="ExaoneConfig-45"><a href="#ExaoneConfig-45"><span class="linenos"> 45</span></a><span class="sd">            :class:`~transformers.EXAONEModel`.</span>
</span><span id="ExaoneConfig-46"><a href="#ExaoneConfig-46"><span class="linenos"> 46</span></a><span class="sd">        max_position_embeddings (:obj:`int`, `optional`, defaults to 2048):</span>
</span><span id="ExaoneConfig-47"><a href="#ExaoneConfig-47"><span class="linenos"> 47</span></a><span class="sd">            The maximum sequence length that this model might ever be used with. Typically set this to something large</span>
</span><span id="ExaoneConfig-48"><a href="#ExaoneConfig-48"><span class="linenos"> 48</span></a><span class="sd">            just in case (e.g., 512 or 1024 or 2048).</span>
</span><span id="ExaoneConfig-49"><a href="#ExaoneConfig-49"><span class="linenos"> 49</span></a><span class="sd">        hidden_size (:obj:`int`, `optional`, defaults to 2048):</span>
</span><span id="ExaoneConfig-50"><a href="#ExaoneConfig-50"><span class="linenos"> 50</span></a><span class="sd">            Dimensionality of the encoder layers and the pooler layer.</span>
</span><span id="ExaoneConfig-51"><a href="#ExaoneConfig-51"><span class="linenos"> 51</span></a><span class="sd">        num_layers (:obj:`int`, `optional`, defaults to 32):</span>
</span><span id="ExaoneConfig-52"><a href="#ExaoneConfig-52"><span class="linenos"> 52</span></a><span class="sd">            Number of hidden layers in the Transformer encoder.</span>
</span><span id="ExaoneConfig-53"><a href="#ExaoneConfig-53"><span class="linenos"> 53</span></a><span class="sd">        num_attention_heads (:obj:`int`, `optional`, defaults to 32):</span>
</span><span id="ExaoneConfig-54"><a href="#ExaoneConfig-54"><span class="linenos"> 54</span></a><span class="sd">            Number of attention heads for each attention layer in the Transformer decoder.</span>
</span><span id="ExaoneConfig-55"><a href="#ExaoneConfig-55"><span class="linenos"> 55</span></a><span class="sd">        num_key_value_heads (:obj:`int`, `optional`):</span>
</span><span id="ExaoneConfig-56"><a href="#ExaoneConfig-56"><span class="linenos"> 56</span></a><span class="sd">            This is the number of key_value heads that should be used to implement Grouped Query Attention. If</span>
</span><span id="ExaoneConfig-57"><a href="#ExaoneConfig-57"><span class="linenos"> 57</span></a><span class="sd">            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if</span>
</span><span id="ExaoneConfig-58"><a href="#ExaoneConfig-58"><span class="linenos"> 58</span></a><span class="sd">            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When</span>
</span><span id="ExaoneConfig-59"><a href="#ExaoneConfig-59"><span class="linenos"> 59</span></a><span class="sd">            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed</span>
</span><span id="ExaoneConfig-60"><a href="#ExaoneConfig-60"><span class="linenos"> 60</span></a><span class="sd">            by meanpooling all the original heads within that group. For more details checkout [this</span>
</span><span id="ExaoneConfig-61"><a href="#ExaoneConfig-61"><span class="linenos"> 61</span></a><span class="sd">            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to</span>
</span><span id="ExaoneConfig-62"><a href="#ExaoneConfig-62"><span class="linenos"> 62</span></a><span class="sd">            `num_attention_heads`.</span>
</span><span id="ExaoneConfig-63"><a href="#ExaoneConfig-63"><span class="linenos"> 63</span></a><span class="sd">        intermediate_size (:obj:`int`, `optional`, defaults to `hidden_size * 4`):</span>
</span><span id="ExaoneConfig-64"><a href="#ExaoneConfig-64"><span class="linenos"> 64</span></a><span class="sd">            Dimensionality of the &quot;intermediate&quot; (i.e., feed-forward) layer in the Transformer encoder.</span>
</span><span id="ExaoneConfig-65"><a href="#ExaoneConfig-65"><span class="linenos"> 65</span></a><span class="sd">        activation_function (:obj:`str` or :obj:`function`, `optional`, defaults to :obj:`&quot;silu&quot;`):</span>
</span><span id="ExaoneConfig-66"><a href="#ExaoneConfig-66"><span class="linenos"> 66</span></a><span class="sd">            The non-linear activation function (function or string) in the decoder.</span>
</span><span id="ExaoneConfig-67"><a href="#ExaoneConfig-67"><span class="linenos"> 67</span></a><span class="sd">        rope_theta (:obj:`float`, `optional`, defaults to 10000.0):</span>
</span><span id="ExaoneConfig-68"><a href="#ExaoneConfig-68"><span class="linenos"> 68</span></a><span class="sd">            The base period of the RoPE embeddings.</span>
</span><span id="ExaoneConfig-69"><a href="#ExaoneConfig-69"><span class="linenos"> 69</span></a><span class="sd">        rope_scaling (:obj:`Dict`, `optional`):</span>
</span><span id="ExaoneConfig-70"><a href="#ExaoneConfig-70"><span class="linenos"> 70</span></a><span class="sd">            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type</span>
</span><span id="ExaoneConfig-71"><a href="#ExaoneConfig-71"><span class="linenos"> 71</span></a><span class="sd">            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value</span>
</span><span id="ExaoneConfig-72"><a href="#ExaoneConfig-72"><span class="linenos"> 72</span></a><span class="sd">            accordingly.</span>
</span><span id="ExaoneConfig-73"><a href="#ExaoneConfig-73"><span class="linenos"> 73</span></a><span class="sd">            Expected contents:</span>
</span><span id="ExaoneConfig-74"><a href="#ExaoneConfig-74"><span class="linenos"> 74</span></a><span class="sd">                `rope_type` (:obj:`str`):</span>
</span><span id="ExaoneConfig-75"><a href="#ExaoneConfig-75"><span class="linenos"> 75</span></a><span class="sd">                    The sub-variant of RoPE to use. Can be one of [&#39;default&#39;, &#39;linear&#39;, &#39;dynamic&#39;, &#39;yarn&#39;, &#39;longrope&#39;,</span>
</span><span id="ExaoneConfig-76"><a href="#ExaoneConfig-76"><span class="linenos"> 76</span></a><span class="sd">                    &#39;llama3&#39;], with &#39;default&#39; being the original RoPE implementation.</span>
</span><span id="ExaoneConfig-77"><a href="#ExaoneConfig-77"><span class="linenos"> 77</span></a><span class="sd">                `factor` (:obj:`float`, `optional`):</span>
</span><span id="ExaoneConfig-78"><a href="#ExaoneConfig-78"><span class="linenos"> 78</span></a><span class="sd">                    Used with all rope types except &#39;default&#39;. The scaling factor to apply to the RoPE embeddings. In</span>
</span><span id="ExaoneConfig-79"><a href="#ExaoneConfig-79"><span class="linenos"> 79</span></a><span class="sd">                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *</span>
</span><span id="ExaoneConfig-80"><a href="#ExaoneConfig-80"><span class="linenos"> 80</span></a><span class="sd">                    original maximum pre-trained length.</span>
</span><span id="ExaoneConfig-81"><a href="#ExaoneConfig-81"><span class="linenos"> 81</span></a><span class="sd">                `original_max_position_embeddings` (:obj:`int`, `optional`):</span>
</span><span id="ExaoneConfig-82"><a href="#ExaoneConfig-82"><span class="linenos"> 82</span></a><span class="sd">                    Used with &#39;dynamic&#39;, &#39;longrope&#39; and &#39;llama3&#39;. The original max position embeddings used during</span>
</span><span id="ExaoneConfig-83"><a href="#ExaoneConfig-83"><span class="linenos"> 83</span></a><span class="sd">                    pretraining.</span>
</span><span id="ExaoneConfig-84"><a href="#ExaoneConfig-84"><span class="linenos"> 84</span></a><span class="sd">                `attention_factor` (:obj:`float`, `optional`):</span>
</span><span id="ExaoneConfig-85"><a href="#ExaoneConfig-85"><span class="linenos"> 85</span></a><span class="sd">                    Used with &#39;yarn&#39; and &#39;longrope&#39;. The scaling factor to be applied on the attention</span>
</span><span id="ExaoneConfig-86"><a href="#ExaoneConfig-86"><span class="linenos"> 86</span></a><span class="sd">                    computation. If unspecified, it defaults to value recommended by the implementation, using the</span>
</span><span id="ExaoneConfig-87"><a href="#ExaoneConfig-87"><span class="linenos"> 87</span></a><span class="sd">                    `factor` field to infer the suggested value.</span>
</span><span id="ExaoneConfig-88"><a href="#ExaoneConfig-88"><span class="linenos"> 88</span></a><span class="sd">                `beta_fast` (:obj:`float`, `optional`):</span>
</span><span id="ExaoneConfig-89"><a href="#ExaoneConfig-89"><span class="linenos"> 89</span></a><span class="sd">                    Only used with &#39;yarn&#39;. Parameter to set the boundary for extrapolation (only) in the linear</span>
</span><span id="ExaoneConfig-90"><a href="#ExaoneConfig-90"><span class="linenos"> 90</span></a><span class="sd">                    ramp function. If unspecified, it defaults to 32.</span>
</span><span id="ExaoneConfig-91"><a href="#ExaoneConfig-91"><span class="linenos"> 91</span></a><span class="sd">                `beta_slow` (:obj:`float`, `optional`):</span>
</span><span id="ExaoneConfig-92"><a href="#ExaoneConfig-92"><span class="linenos"> 92</span></a><span class="sd">                    Only used with &#39;yarn&#39;. Parameter to set the boundary for interpolation (only) in the linear</span>
</span><span id="ExaoneConfig-93"><a href="#ExaoneConfig-93"><span class="linenos"> 93</span></a><span class="sd">                    ramp function. If unspecified, it defaults to 1.</span>
</span><span id="ExaoneConfig-94"><a href="#ExaoneConfig-94"><span class="linenos"> 94</span></a><span class="sd">                `short_factor` (:obj:`List[float]`, `optional`):</span>
</span><span id="ExaoneConfig-95"><a href="#ExaoneConfig-95"><span class="linenos"> 95</span></a><span class="sd">                    Only used with &#39;longrope&#39;. The scaling factor to be applied to short contexts (&lt;</span>
</span><span id="ExaoneConfig-96"><a href="#ExaoneConfig-96"><span class="linenos"> 96</span></a><span class="sd">                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden</span>
</span><span id="ExaoneConfig-97"><a href="#ExaoneConfig-97"><span class="linenos"> 97</span></a><span class="sd">                    size divided by the number of attention heads divided by 2</span>
</span><span id="ExaoneConfig-98"><a href="#ExaoneConfig-98"><span class="linenos"> 98</span></a><span class="sd">                `long_factor` (:obj:`List[float]`, `optional`):</span>
</span><span id="ExaoneConfig-99"><a href="#ExaoneConfig-99"><span class="linenos"> 99</span></a><span class="sd">                    Only used with &#39;longrope&#39;. The scaling factor to be applied to long contexts (&lt;</span>
</span><span id="ExaoneConfig-100"><a href="#ExaoneConfig-100"><span class="linenos">100</span></a><span class="sd">                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden</span>
</span><span id="ExaoneConfig-101"><a href="#ExaoneConfig-101"><span class="linenos">101</span></a><span class="sd">                    size divided by the number of attention heads divided by 2</span>
</span><span id="ExaoneConfig-102"><a href="#ExaoneConfig-102"><span class="linenos">102</span></a><span class="sd">                `low_freq_factor` (:obj:`float`, `optional`):</span>
</span><span id="ExaoneConfig-103"><a href="#ExaoneConfig-103"><span class="linenos">103</span></a><span class="sd">                    Only used with &#39;llama3&#39;. Scaling factor applied to low frequency components of the RoPE</span>
</span><span id="ExaoneConfig-104"><a href="#ExaoneConfig-104"><span class="linenos">104</span></a><span class="sd">                `high_freq_factor` (:obj:`float`, `optional`):</span>
</span><span id="ExaoneConfig-105"><a href="#ExaoneConfig-105"><span class="linenos">105</span></a><span class="sd">                    Only used with &#39;llama3&#39;. Scaling factor applied to high frequency components of the RoPE</span>
</span><span id="ExaoneConfig-106"><a href="#ExaoneConfig-106"><span class="linenos">106</span></a><span class="sd">        embed_dropout (:obj:`float`, `optional`, defaults to 0.0):</span>
</span><span id="ExaoneConfig-107"><a href="#ExaoneConfig-107"><span class="linenos">107</span></a><span class="sd">            The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.</span>
</span><span id="ExaoneConfig-108"><a href="#ExaoneConfig-108"><span class="linenos">108</span></a><span class="sd">        attention_dropout (:obj:`float`, `optional`, defaults to 0.0):</span>
</span><span id="ExaoneConfig-109"><a href="#ExaoneConfig-109"><span class="linenos">109</span></a><span class="sd">            The dropout ratio for the attention probabilities.</span>
</span><span id="ExaoneConfig-110"><a href="#ExaoneConfig-110"><span class="linenos">110</span></a><span class="sd">        layer_norm_epsilon (:obj:`float`, `optional`, defaults to 1e-5):</span>
</span><span id="ExaoneConfig-111"><a href="#ExaoneConfig-111"><span class="linenos">111</span></a><span class="sd">            The epsilon used by the layer normalization layers.</span>
</span><span id="ExaoneConfig-112"><a href="#ExaoneConfig-112"><span class="linenos">112</span></a><span class="sd">        initializer_range (:obj:`float`, `optional`, defaults to 0.02):</span>
</span><span id="ExaoneConfig-113"><a href="#ExaoneConfig-113"><span class="linenos">113</span></a><span class="sd">            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</span>
</span><span id="ExaoneConfig-114"><a href="#ExaoneConfig-114"><span class="linenos">114</span></a><span class="sd">        use_cache (:obj:`bool`, `optional`, defaults to :obj:`True`):</span>
</span><span id="ExaoneConfig-115"><a href="#ExaoneConfig-115"><span class="linenos">115</span></a><span class="sd">            Whether or not the model should return the last key/values attentions (not used by all models). Only</span>
</span><span id="ExaoneConfig-116"><a href="#ExaoneConfig-116"><span class="linenos">116</span></a><span class="sd">            relevant if ``configs.is_decoder=True``.</span>
</span><span id="ExaoneConfig-117"><a href="#ExaoneConfig-117"><span class="linenos">117</span></a><span class="sd">        bos_token_id (:obj:`int`, `optional`, defaults to 0):</span>
</span><span id="ExaoneConfig-118"><a href="#ExaoneConfig-118"><span class="linenos">118</span></a><span class="sd">            Beginning of stream token id.</span>
</span><span id="ExaoneConfig-119"><a href="#ExaoneConfig-119"><span class="linenos">119</span></a><span class="sd">        eos_token_id (:obj:`int`, `optional`, defaults to 2):</span>
</span><span id="ExaoneConfig-120"><a href="#ExaoneConfig-120"><span class="linenos">120</span></a><span class="sd">            End of stream token id.</span>
</span><span id="ExaoneConfig-121"><a href="#ExaoneConfig-121"><span class="linenos">121</span></a><span class="sd">        tie_word_embeddings (:obj:`bool`, `optional`, defaults to :obj:`True`):</span>
</span><span id="ExaoneConfig-122"><a href="#ExaoneConfig-122"><span class="linenos">122</span></a><span class="sd">            Whether to tie weight embeddings</span>
</span><span id="ExaoneConfig-123"><a href="#ExaoneConfig-123"><span class="linenos">123</span></a><span class="sd">        gradient_checkpointing (:obj:`bool`, `optional`, defaults to :obj:`False`):</span>
</span><span id="ExaoneConfig-124"><a href="#ExaoneConfig-124"><span class="linenos">124</span></a><span class="sd">            If True, use gradient checkpointing to save memory at the expense of slower backward pass.</span>
</span><span id="ExaoneConfig-125"><a href="#ExaoneConfig-125"><span class="linenos">125</span></a>
</span><span id="ExaoneConfig-126"><a href="#ExaoneConfig-126"><span class="linenos">126</span></a><span class="sd">        Example::</span>
</span><span id="ExaoneConfig-127"><a href="#ExaoneConfig-127"><span class="linenos">127</span></a>
</span><span id="ExaoneConfig-128"><a href="#ExaoneConfig-128"><span class="linenos">128</span></a><span class="sd">            &gt;&gt;&gt; from transformers import EXAONEModel, ExaoneConfig</span>
</span><span id="ExaoneConfig-129"><a href="#ExaoneConfig-129"><span class="linenos">129</span></a>
</span><span id="ExaoneConfig-130"><a href="#ExaoneConfig-130"><span class="linenos">130</span></a><span class="sd">            &gt;&gt;&gt; # Initializing a EXAONE configuration</span>
</span><span id="ExaoneConfig-131"><a href="#ExaoneConfig-131"><span class="linenos">131</span></a><span class="sd">            &gt;&gt;&gt; configuration = ExaoneConfig()</span>
</span><span id="ExaoneConfig-132"><a href="#ExaoneConfig-132"><span class="linenos">132</span></a>
</span><span id="ExaoneConfig-133"><a href="#ExaoneConfig-133"><span class="linenos">133</span></a><span class="sd">            &gt;&gt;&gt; # Initializing a model from configuration</span>
</span><span id="ExaoneConfig-134"><a href="#ExaoneConfig-134"><span class="linenos">134</span></a><span class="sd">            &gt;&gt;&gt; model = EXAONEModel(configuration)</span>
</span><span id="ExaoneConfig-135"><a href="#ExaoneConfig-135"><span class="linenos">135</span></a>
</span><span id="ExaoneConfig-136"><a href="#ExaoneConfig-136"><span class="linenos">136</span></a><span class="sd">            &gt;&gt;&gt; # Accessing the model configuration</span>
</span><span id="ExaoneConfig-137"><a href="#ExaoneConfig-137"><span class="linenos">137</span></a><span class="sd">            &gt;&gt;&gt; configuration = model.configs</span>
</span><span id="ExaoneConfig-138"><a href="#ExaoneConfig-138"><span class="linenos">138</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="ExaoneConfig-139"><a href="#ExaoneConfig-139"><span class="linenos">139</span></a>
</span><span id="ExaoneConfig-140"><a href="#ExaoneConfig-140"><span class="linenos">140</span></a>    <span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;exaone&quot;</span>
</span><span id="ExaoneConfig-141"><a href="#ExaoneConfig-141"><span class="linenos">141</span></a>    <span class="n">keys_to_ignore_at_inference</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span>
</span><span id="ExaoneConfig-142"><a href="#ExaoneConfig-142"><span class="linenos">142</span></a>    <span class="n">attribute_map</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;num_hidden_layers&quot;</span><span class="p">:</span> <span class="s2">&quot;num_layers&quot;</span><span class="p">}</span>
</span><span id="ExaoneConfig-143"><a href="#ExaoneConfig-143"><span class="linenos">143</span></a>
</span><span id="ExaoneConfig-144"><a href="#ExaoneConfig-144"><span class="linenos">144</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ExaoneConfig-145"><a href="#ExaoneConfig-145"><span class="linenos">145</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ExaoneConfig-146"><a href="#ExaoneConfig-146"><span class="linenos">146</span></a>        <span class="n">vocab_size</span><span class="o">=</span><span class="mi">102400</span><span class="p">,</span>
</span><span id="ExaoneConfig-147"><a href="#ExaoneConfig-147"><span class="linenos">147</span></a>        <span class="n">max_position_embeddings</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
</span><span id="ExaoneConfig-148"><a href="#ExaoneConfig-148"><span class="linenos">148</span></a>        <span class="n">hidden_size</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
</span><span id="ExaoneConfig-149"><a href="#ExaoneConfig-149"><span class="linenos">149</span></a>        <span class="n">num_layers</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
</span><span id="ExaoneConfig-150"><a href="#ExaoneConfig-150"><span class="linenos">150</span></a>        <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
</span><span id="ExaoneConfig-151"><a href="#ExaoneConfig-151"><span class="linenos">151</span></a>        <span class="n">num_key_value_heads</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="ExaoneConfig-152"><a href="#ExaoneConfig-152"><span class="linenos">152</span></a>        <span class="n">intermediate_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="ExaoneConfig-153"><a href="#ExaoneConfig-153"><span class="linenos">153</span></a>        <span class="n">activation_function</span><span class="o">=</span><span class="s2">&quot;silu&quot;</span><span class="p">,</span>
</span><span id="ExaoneConfig-154"><a href="#ExaoneConfig-154"><span class="linenos">154</span></a>        <span class="n">rope_theta</span><span class="o">=</span><span class="mf">10000.0</span><span class="p">,</span>
</span><span id="ExaoneConfig-155"><a href="#ExaoneConfig-155"><span class="linenos">155</span></a>        <span class="n">rope_scaling</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="ExaoneConfig-156"><a href="#ExaoneConfig-156"><span class="linenos">156</span></a>        <span class="n">embed_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
</span><span id="ExaoneConfig-157"><a href="#ExaoneConfig-157"><span class="linenos">157</span></a>        <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
</span><span id="ExaoneConfig-158"><a href="#ExaoneConfig-158"><span class="linenos">158</span></a>        <span class="n">layer_norm_epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
</span><span id="ExaoneConfig-159"><a href="#ExaoneConfig-159"><span class="linenos">159</span></a>        <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
</span><span id="ExaoneConfig-160"><a href="#ExaoneConfig-160"><span class="linenos">160</span></a>        <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="ExaoneConfig-161"><a href="#ExaoneConfig-161"><span class="linenos">161</span></a>        <span class="n">bos_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="ExaoneConfig-162"><a href="#ExaoneConfig-162"><span class="linenos">162</span></a>        <span class="n">eos_token_id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
</span><span id="ExaoneConfig-163"><a href="#ExaoneConfig-163"><span class="linenos">163</span></a>        <span class="n">tie_word_embeddings</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="ExaoneConfig-164"><a href="#ExaoneConfig-164"><span class="linenos">164</span></a>        <span class="o">**</span><span class="n">kwargs</span>
</span><span id="ExaoneConfig-165"><a href="#ExaoneConfig-165"><span class="linenos">165</span></a>    <span class="p">):</span>
</span><span id="ExaoneConfig-166"><a href="#ExaoneConfig-166"><span class="linenos">166</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
</span><span id="ExaoneConfig-167"><a href="#ExaoneConfig-167"><span class="linenos">167</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="n">max_position_embeddings</span>
</span><span id="ExaoneConfig-168"><a href="#ExaoneConfig-168"><span class="linenos">168</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span id="ExaoneConfig-169"><a href="#ExaoneConfig-169"><span class="linenos">169</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
</span><span id="ExaoneConfig-170"><a href="#ExaoneConfig-170"><span class="linenos">170</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">num_attention_heads</span>
</span><span id="ExaoneConfig-171"><a href="#ExaoneConfig-171"><span class="linenos">171</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
</span><span id="ExaoneConfig-172"><a href="#ExaoneConfig-172"><span class="linenos">172</span></a>        <span class="k">if</span> <span class="n">num_key_value_heads</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ExaoneConfig-173"><a href="#ExaoneConfig-173"><span class="linenos">173</span></a>            <span class="n">num_key_value_heads</span> <span class="o">=</span> <span class="n">num_attention_heads</span>
</span><span id="ExaoneConfig-174"><a href="#ExaoneConfig-174"><span class="linenos">174</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span> <span class="o">=</span> <span class="n">num_key_value_heads</span>
</span><span id="ExaoneConfig-175"><a href="#ExaoneConfig-175"><span class="linenos">175</span></a>        <span class="k">if</span> <span class="n">intermediate_size</span><span class="p">:</span>
</span><span id="ExaoneConfig-176"><a href="#ExaoneConfig-176"><span class="linenos">176</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">intermediate_size</span>
</span><span id="ExaoneConfig-177"><a href="#ExaoneConfig-177"><span class="linenos">177</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="ExaoneConfig-178"><a href="#ExaoneConfig-178"><span class="linenos">178</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">4</span>
</span><span id="ExaoneConfig-179"><a href="#ExaoneConfig-179"><span class="linenos">179</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">=</span> <span class="n">activation_function</span>
</span><span id="ExaoneConfig-180"><a href="#ExaoneConfig-180"><span class="linenos">180</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dropout</span> <span class="o">=</span> <span class="n">embed_dropout</span>
</span><span id="ExaoneConfig-181"><a href="#ExaoneConfig-181"><span class="linenos">181</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">attention_dropout</span>
</span><span id="ExaoneConfig-182"><a href="#ExaoneConfig-182"><span class="linenos">182</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_epsilon</span> <span class="o">=</span> <span class="n">layer_norm_epsilon</span>
</span><span id="ExaoneConfig-183"><a href="#ExaoneConfig-183"><span class="linenos">183</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">initializer_range</span> <span class="o">=</span> <span class="n">initializer_range</span>
</span><span id="ExaoneConfig-184"><a href="#ExaoneConfig-184"><span class="linenos">184</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span>
</span><span id="ExaoneConfig-185"><a href="#ExaoneConfig-185"><span class="linenos">185</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">rope_theta</span> <span class="o">=</span> <span class="n">rope_theta</span>
</span><span id="ExaoneConfig-186"><a href="#ExaoneConfig-186"><span class="linenos">186</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">rope_scaling</span> <span class="o">=</span> <span class="n">rope_scaling</span>
</span><span id="ExaoneConfig-187"><a href="#ExaoneConfig-187"><span class="linenos">187</span></a>
</span><span id="ExaoneConfig-188"><a href="#ExaoneConfig-188"><span class="linenos">188</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bos_token_id</span> <span class="o">=</span> <span class="n">bos_token_id</span>
</span><span id="ExaoneConfig-189"><a href="#ExaoneConfig-189"><span class="linenos">189</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span>
</span><span id="ExaoneConfig-190"><a href="#ExaoneConfig-190"><span class="linenos">190</span></a>
</span><span id="ExaoneConfig-191"><a href="#ExaoneConfig-191"><span class="linenos">191</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ExaoneConfig-192"><a href="#ExaoneConfig-192"><span class="linenos">192</span></a>            <span class="n">bos_token_id</span><span class="o">=</span><span class="n">bos_token_id</span><span class="p">,</span>
</span><span id="ExaoneConfig-193"><a href="#ExaoneConfig-193"><span class="linenos">193</span></a>            <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
</span><span id="ExaoneConfig-194"><a href="#ExaoneConfig-194"><span class="linenos">194</span></a>            <span class="n">tie_word_embeddings</span><span class="o">=</span><span class="n">tie_word_embeddings</span><span class="p">,</span>
</span><span id="ExaoneConfig-195"><a href="#ExaoneConfig-195"><span class="linenos">195</span></a>            <span class="o">**</span><span class="n">kwargs</span>
</span><span id="ExaoneConfig-196"><a href="#ExaoneConfig-196"><span class="linenos">196</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>This is the configuration class to store the configuration of a <code>~transformers.ExaoneModel</code>. It is used to
instantiate a EXAONE model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the Exaone</p>

<p>Configuration objects inherit from <code>~transformers.PretrainedConfig</code> and can be used to control the model
outputs. Read the documentation from <code>~transformers.PretrainedConfig</code> for more information.</p>

<p>Args:
    vocab_size (<code>int</code>, <code>optional</code>, defaults to 102400):
        Vocabulary size of the EXAONE model. Defines the number of different tokens that can be represented by the
        <code>inputs_ids</code> passed when calling <code>~transformers.ExaoneModel</code>. Vocabulary size of the model.
        Defines the different tokens that can be represented by the <code>inputs_ids</code> passed to the forward method of
        <code>~transformers.EXAONEModel</code>.
    max_position_embeddings (<code>int</code>, <code>optional</code>, defaults to 2048):
        The maximum sequence length that this model might ever be used with. Typically set this to something large
        just in case (e.g., 512 or 1024 or 2048).
    hidden_size (<code>int</code>, <code>optional</code>, defaults to 2048):
        Dimensionality of the encoder layers and the pooler layer.
    num_layers (<code>int</code>, <code>optional</code>, defaults to 32):
        Number of hidden layers in the Transformer encoder.
    num_attention_heads (<code>int</code>, <code>optional</code>, defaults to 32):
        Number of attention heads for each attention layer in the Transformer decoder.
    num_key_value_heads (<code>int</code>, <code>optional</code>):
        This is the number of key_value heads that should be used to implement Grouped Query Attention. If
        <code>num_key_value_heads=num_attention_heads</code>, the model will use Multi Head Attention (MHA), if
        <code>num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When
        converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed
        by meanpooling all the original heads within that group. For more details checkout [this
        paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to
</code>num_attention_heads<code>.
    intermediate_size (</code>int<code>,</code>optional<code>, defaults to</code>hidden_size * 4<code>):
        Dimensionality of the "intermediate" (i.e., feed-forward) layer in the Transformer encoder.
    activation_function (</code>str<code>or</code>function<code>,</code>optional<code>, defaults to</code>"silu"<code>):
        The non-linear activation function (function or string) in the decoder.
    rope_theta (</code>float<code>,</code>optional<code>, defaults to 10000.0):
        The base period of the RoPE embeddings.
    rope_scaling (</code>Dict<code>,</code>optional<code>):
        Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type
        and you expect the model to work on longer</code>max_position_embeddings<code>, we recommend you to update this value
        accordingly.
        Expected contents:
</code>rope_type<code>(</code>str<code>):
                The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',
                'llama3'], with 'default' being the original RoPE implementation.
</code>factor<code>(</code>float<code>,</code>optional<code>):
                Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In
                most scaling types, a</code>factor<code>of x will enable the model to handle sequences of length x *
                original maximum pre-trained length.
</code>original_max_position_embeddings<code>(</code>int<code>,</code>optional<code>):
                Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during
                pretraining.
</code>attention_factor<code>(</code>float<code>,</code>optional<code>):
                Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention
                computation. If unspecified, it defaults to value recommended by the implementation, using the
</code>factor<code>field to infer the suggested value.
</code>beta_fast<code>(</code>float<code>,</code>optional<code>):
                Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear
                ramp function. If unspecified, it defaults to 32.
</code>beta_slow<code>(</code>float<code>,</code>optional<code>):
                Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear
                ramp function. If unspecified, it defaults to 1.
</code>short_factor<code>(</code>List[float]<code>,</code>optional<code>):
                Only used with 'longrope'. The scaling factor to be applied to short contexts (&lt;
</code>original_max_position_embeddings<code>). Must be a list of numbers with the same length as the hidden
                size divided by the number of attention heads divided by 2
</code>long_factor<code>(</code>List[float]<code>,</code>optional<code>):
                Only used with 'longrope'. The scaling factor to be applied to long contexts (&lt;
</code>original_max_position_embeddings<code>). Must be a list of numbers with the same length as the hidden
                size divided by the number of attention heads divided by 2
</code>low_freq_factor<code>(</code>float<code>,</code>optional<code>):
                Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE
</code>high_freq_factor<code>(</code>float<code>,</code>optional<code>):
                Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE
    embed_dropout (</code>float<code>,</code>optional<code>, defaults to 0.0):
        The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.
    attention_dropout (</code>float<code>,</code>optional<code>, defaults to 0.0):
        The dropout ratio for the attention probabilities.
    layer_norm_epsilon (</code>float<code>,</code>optional<code>, defaults to 1e-5):
        The epsilon used by the layer normalization layers.
    initializer_range (</code>float<code>,</code>optional<code>, defaults to 0.02):
        The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
    use_cache (</code>bool<code>,</code>optional<code>, defaults to</code>True<code>):
        Whether or not the model should return the last key/values attentions (not used by all models). Only
        relevant if ``configs.is_decoder=True``.
    bos_token_id (</code>int<code>,</code>optional<code>, defaults to 0):
        Beginning of stream token id.
    eos_token_id (</code>int<code>,</code>optional<code>, defaults to 2):
        End of stream token id.
    tie_word_embeddings (</code>bool<code>,</code>optional<code>, defaults to</code>True<code>):
        Whether to tie weight embeddings
    gradient_checkpointing (</code>bool<code>,</code>optional<code>, defaults to</code>False`):
        If True, use gradient checkpointing to save memory at the expense of slower backward pass.</p>

<pre><code>Example::

    &gt;&gt;&gt; from transformers import EXAONEModel, ExaoneConfig

    &gt;&gt;&gt; # Initializing a EXAONE configuration
    &gt;&gt;&gt; configuration = ExaoneConfig()

    &gt;&gt;&gt; # Initializing a model from configuration
    &gt;&gt;&gt; model = EXAONEModel(configuration)

    &gt;&gt;&gt; # Accessing the model configuration
    &gt;&gt;&gt; configuration = model.configs
</code></pre>
</div>


                            <div id="ExaoneConfig.__init__" class="classattr">
                                        <input id="ExaoneConfig.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">ExaoneConfig</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">vocab_size</span><span class="o">=</span><span class="mi">102400</span>,</span><span class="param">	<span class="n">max_position_embeddings</span><span class="o">=</span><span class="mi">2048</span>,</span><span class="param">	<span class="n">hidden_size</span><span class="o">=</span><span class="mi">2048</span>,</span><span class="param">	<span class="n">num_layers</span><span class="o">=</span><span class="mi">32</span>,</span><span class="param">	<span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">32</span>,</span><span class="param">	<span class="n">num_key_value_heads</span><span class="o">=</span><span class="kc">None</span>,</span><span class="param">	<span class="n">intermediate_size</span><span class="o">=</span><span class="kc">None</span>,</span><span class="param">	<span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;silu&#39;</span>,</span><span class="param">	<span class="n">rope_theta</span><span class="o">=</span><span class="mf">10000.0</span>,</span><span class="param">	<span class="n">rope_scaling</span><span class="o">=</span><span class="kc">None</span>,</span><span class="param">	<span class="n">embed_dropout</span><span class="o">=</span><span class="mf">0.0</span>,</span><span class="param">	<span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.0</span>,</span><span class="param">	<span class="n">layer_norm_epsilon</span><span class="o">=</span><span class="mf">1e-05</span>,</span><span class="param">	<span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span>,</span><span class="param">	<span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span>,</span><span class="param">	<span class="n">bos_token_id</span><span class="o">=</span><span class="mi">0</span>,</span><span class="param">	<span class="n">eos_token_id</span><span class="o">=</span><span class="mi">2</span>,</span><span class="param">	<span class="n">tie_word_embeddings</span><span class="o">=</span><span class="kc">True</span>,</span><span class="param">	<span class="o">**</span><span class="n">kwargs</span></span>)</span>

                <label class="view-source-button" for="ExaoneConfig.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ExaoneConfig.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ExaoneConfig.__init__-144"><a href="#ExaoneConfig.__init__-144"><span class="linenos">144</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ExaoneConfig.__init__-145"><a href="#ExaoneConfig.__init__-145"><span class="linenos">145</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-146"><a href="#ExaoneConfig.__init__-146"><span class="linenos">146</span></a>        <span class="n">vocab_size</span><span class="o">=</span><span class="mi">102400</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-147"><a href="#ExaoneConfig.__init__-147"><span class="linenos">147</span></a>        <span class="n">max_position_embeddings</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-148"><a href="#ExaoneConfig.__init__-148"><span class="linenos">148</span></a>        <span class="n">hidden_size</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-149"><a href="#ExaoneConfig.__init__-149"><span class="linenos">149</span></a>        <span class="n">num_layers</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-150"><a href="#ExaoneConfig.__init__-150"><span class="linenos">150</span></a>        <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-151"><a href="#ExaoneConfig.__init__-151"><span class="linenos">151</span></a>        <span class="n">num_key_value_heads</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-152"><a href="#ExaoneConfig.__init__-152"><span class="linenos">152</span></a>        <span class="n">intermediate_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-153"><a href="#ExaoneConfig.__init__-153"><span class="linenos">153</span></a>        <span class="n">activation_function</span><span class="o">=</span><span class="s2">&quot;silu&quot;</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-154"><a href="#ExaoneConfig.__init__-154"><span class="linenos">154</span></a>        <span class="n">rope_theta</span><span class="o">=</span><span class="mf">10000.0</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-155"><a href="#ExaoneConfig.__init__-155"><span class="linenos">155</span></a>        <span class="n">rope_scaling</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-156"><a href="#ExaoneConfig.__init__-156"><span class="linenos">156</span></a>        <span class="n">embed_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-157"><a href="#ExaoneConfig.__init__-157"><span class="linenos">157</span></a>        <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-158"><a href="#ExaoneConfig.__init__-158"><span class="linenos">158</span></a>        <span class="n">layer_norm_epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-159"><a href="#ExaoneConfig.__init__-159"><span class="linenos">159</span></a>        <span class="n">initializer_range</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-160"><a href="#ExaoneConfig.__init__-160"><span class="linenos">160</span></a>        <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-161"><a href="#ExaoneConfig.__init__-161"><span class="linenos">161</span></a>        <span class="n">bos_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-162"><a href="#ExaoneConfig.__init__-162"><span class="linenos">162</span></a>        <span class="n">eos_token_id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-163"><a href="#ExaoneConfig.__init__-163"><span class="linenos">163</span></a>        <span class="n">tie_word_embeddings</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-164"><a href="#ExaoneConfig.__init__-164"><span class="linenos">164</span></a>        <span class="o">**</span><span class="n">kwargs</span>
</span><span id="ExaoneConfig.__init__-165"><a href="#ExaoneConfig.__init__-165"><span class="linenos">165</span></a>    <span class="p">):</span>
</span><span id="ExaoneConfig.__init__-166"><a href="#ExaoneConfig.__init__-166"><span class="linenos">166</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
</span><span id="ExaoneConfig.__init__-167"><a href="#ExaoneConfig.__init__-167"><span class="linenos">167</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="n">max_position_embeddings</span>
</span><span id="ExaoneConfig.__init__-168"><a href="#ExaoneConfig.__init__-168"><span class="linenos">168</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span id="ExaoneConfig.__init__-169"><a href="#ExaoneConfig.__init__-169"><span class="linenos">169</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
</span><span id="ExaoneConfig.__init__-170"><a href="#ExaoneConfig.__init__-170"><span class="linenos">170</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">num_attention_heads</span>
</span><span id="ExaoneConfig.__init__-171"><a href="#ExaoneConfig.__init__-171"><span class="linenos">171</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
</span><span id="ExaoneConfig.__init__-172"><a href="#ExaoneConfig.__init__-172"><span class="linenos">172</span></a>        <span class="k">if</span> <span class="n">num_key_value_heads</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="ExaoneConfig.__init__-173"><a href="#ExaoneConfig.__init__-173"><span class="linenos">173</span></a>            <span class="n">num_key_value_heads</span> <span class="o">=</span> <span class="n">num_attention_heads</span>
</span><span id="ExaoneConfig.__init__-174"><a href="#ExaoneConfig.__init__-174"><span class="linenos">174</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span> <span class="o">=</span> <span class="n">num_key_value_heads</span>
</span><span id="ExaoneConfig.__init__-175"><a href="#ExaoneConfig.__init__-175"><span class="linenos">175</span></a>        <span class="k">if</span> <span class="n">intermediate_size</span><span class="p">:</span>
</span><span id="ExaoneConfig.__init__-176"><a href="#ExaoneConfig.__init__-176"><span class="linenos">176</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">intermediate_size</span>
</span><span id="ExaoneConfig.__init__-177"><a href="#ExaoneConfig.__init__-177"><span class="linenos">177</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="ExaoneConfig.__init__-178"><a href="#ExaoneConfig.__init__-178"><span class="linenos">178</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">4</span>
</span><span id="ExaoneConfig.__init__-179"><a href="#ExaoneConfig.__init__-179"><span class="linenos">179</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span> <span class="o">=</span> <span class="n">activation_function</span>
</span><span id="ExaoneConfig.__init__-180"><a href="#ExaoneConfig.__init__-180"><span class="linenos">180</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dropout</span> <span class="o">=</span> <span class="n">embed_dropout</span>
</span><span id="ExaoneConfig.__init__-181"><a href="#ExaoneConfig.__init__-181"><span class="linenos">181</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">attention_dropout</span>
</span><span id="ExaoneConfig.__init__-182"><a href="#ExaoneConfig.__init__-182"><span class="linenos">182</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_epsilon</span> <span class="o">=</span> <span class="n">layer_norm_epsilon</span>
</span><span id="ExaoneConfig.__init__-183"><a href="#ExaoneConfig.__init__-183"><span class="linenos">183</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">initializer_range</span> <span class="o">=</span> <span class="n">initializer_range</span>
</span><span id="ExaoneConfig.__init__-184"><a href="#ExaoneConfig.__init__-184"><span class="linenos">184</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span>
</span><span id="ExaoneConfig.__init__-185"><a href="#ExaoneConfig.__init__-185"><span class="linenos">185</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">rope_theta</span> <span class="o">=</span> <span class="n">rope_theta</span>
</span><span id="ExaoneConfig.__init__-186"><a href="#ExaoneConfig.__init__-186"><span class="linenos">186</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">rope_scaling</span> <span class="o">=</span> <span class="n">rope_scaling</span>
</span><span id="ExaoneConfig.__init__-187"><a href="#ExaoneConfig.__init__-187"><span class="linenos">187</span></a>
</span><span id="ExaoneConfig.__init__-188"><a href="#ExaoneConfig.__init__-188"><span class="linenos">188</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bos_token_id</span> <span class="o">=</span> <span class="n">bos_token_id</span>
</span><span id="ExaoneConfig.__init__-189"><a href="#ExaoneConfig.__init__-189"><span class="linenos">189</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">eos_token_id</span> <span class="o">=</span> <span class="n">eos_token_id</span>
</span><span id="ExaoneConfig.__init__-190"><a href="#ExaoneConfig.__init__-190"><span class="linenos">190</span></a>
</span><span id="ExaoneConfig.__init__-191"><a href="#ExaoneConfig.__init__-191"><span class="linenos">191</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ExaoneConfig.__init__-192"><a href="#ExaoneConfig.__init__-192"><span class="linenos">192</span></a>            <span class="n">bos_token_id</span><span class="o">=</span><span class="n">bos_token_id</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-193"><a href="#ExaoneConfig.__init__-193"><span class="linenos">193</span></a>            <span class="n">eos_token_id</span><span class="o">=</span><span class="n">eos_token_id</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-194"><a href="#ExaoneConfig.__init__-194"><span class="linenos">194</span></a>            <span class="n">tie_word_embeddings</span><span class="o">=</span><span class="n">tie_word_embeddings</span><span class="p">,</span>
</span><span id="ExaoneConfig.__init__-195"><a href="#ExaoneConfig.__init__-195"><span class="linenos">195</span></a>            <span class="o">**</span><span class="n">kwargs</span>
</span><span id="ExaoneConfig.__init__-196"><a href="#ExaoneConfig.__init__-196"><span class="linenos">196</span></a>        <span class="p">)</span>
</span></pre></div>


    

                            </div>
                            <div id="ExaoneConfig.model_type" class="classattr">
                                <div class="attr variable">
            <span class="name">model_type</span>        =
<span class="default_value">&#39;exaone&#39;</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.model_type"></a>
    
    

                            </div>
                            <div id="ExaoneConfig.keys_to_ignore_at_inference" class="classattr">
                                <div class="attr variable">
            <span class="name">keys_to_ignore_at_inference</span>        =
<span class="default_value">[&#39;past_key_values&#39;]</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.keys_to_ignore_at_inference"></a>
    
    

                            </div>
                            <div id="ExaoneConfig.attribute_map" class="classattr">
                                <div class="attr variable">
            <span class="name">attribute_map</span>        =
<span class="default_value">{&#39;num_hidden_layers&#39;: &#39;num_layers&#39;}</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.attribute_map"></a>
    
    

                            </div>
                            <div id="ExaoneConfig.vocab_size" class="classattr">
                                <div class="attr variable">
            <span class="name">vocab_size</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.vocab_size"></a>
    
    

                            </div>
                            <div id="ExaoneConfig.max_position_embeddings" class="classattr">
                                <div class="attr variable">
            <span class="name">max_position_embeddings</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.max_position_embeddings"></a>
    
    

                            </div>
                            <div id="ExaoneConfig.hidden_size" class="classattr">
                                <div class="attr variable">
            <span class="name">hidden_size</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.hidden_size"></a>
    
    

                            </div>
                            <div id="ExaoneConfig.num_layers" class="classattr">
                                <div class="attr variable">
            <span class="name">num_layers</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.num_layers"></a>
    
    

                            </div>
                            <div id="ExaoneConfig.num_attention_heads" class="classattr">
                                <div class="attr variable">
            <span class="name">num_attention_heads</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.num_attention_heads"></a>
    
    

                            </div>
                            <div id="ExaoneConfig.num_hidden_layers" class="classattr">
                                <div class="attr variable">
            <span class="name">num_hidden_layers</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.num_hidden_layers"></a>
    
    

                            </div>
                            <div id="ExaoneConfig.num_key_value_heads" class="classattr">
                                <div class="attr variable">
            <span class="name">num_key_value_heads</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.num_key_value_heads"></a>
    
    

                            </div>
                            <div id="ExaoneConfig.activation_function" class="classattr">
                                <div class="attr variable">
            <span class="name">activation_function</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.activation_function"></a>
    
    

                            </div>
                            <div id="ExaoneConfig.embed_dropout" class="classattr">
                                <div class="attr variable">
            <span class="name">embed_dropout</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.embed_dropout"></a>
    
    

                            </div>
                            <div id="ExaoneConfig.attention_dropout" class="classattr">
                                <div class="attr variable">
            <span class="name">attention_dropout</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.attention_dropout"></a>
    
    

                            </div>
                            <div id="ExaoneConfig.layer_norm_epsilon" class="classattr">
                                <div class="attr variable">
            <span class="name">layer_norm_epsilon</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.layer_norm_epsilon"></a>
    
    

                            </div>
                            <div id="ExaoneConfig.initializer_range" class="classattr">
                                <div class="attr variable">
            <span class="name">initializer_range</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.initializer_range"></a>
    
    

                            </div>
                            <div id="ExaoneConfig.use_cache" class="classattr">
                                <div class="attr variable">
            <span class="name">use_cache</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.use_cache"></a>
    
    

                            </div>
                            <div id="ExaoneConfig.rope_theta" class="classattr">
                                <div class="attr variable">
            <span class="name">rope_theta</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.rope_theta"></a>
    
    

                            </div>
                            <div id="ExaoneConfig.rope_scaling" class="classattr">
                                <div class="attr variable">
            <span class="name">rope_scaling</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.rope_scaling"></a>
    
    

                            </div>
                            <div id="ExaoneConfig.bos_token_id" class="classattr">
                                <div class="attr variable">
            <span class="name">bos_token_id</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.bos_token_id"></a>
    
    

                            </div>
                            <div id="ExaoneConfig.eos_token_id" class="classattr">
                                <div class="attr variable">
            <span class="name">eos_token_id</span>

        
    </div>
    <a class="headerlink" href="#ExaoneConfig.eos_token_id"></a>
    
    

                            </div>
                </section>
                <section id="ChatGLMConfig">
                            <input id="ChatGLMConfig-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">ChatGLMConfig</span><wbr>(<span class="base">transformers.configuration_utils.PretrainedConfig</span>):

                <label class="view-source-button" for="ChatGLMConfig-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ChatGLMConfig"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ChatGLMConfig-13"><a href="#ChatGLMConfig-13"><span class="linenos">13</span></a><span class="k">class</span><span class="w"> </span><span class="nc">ChatGLMConfig</span><span class="p">(</span><span class="n">PretrainedConfig</span><span class="p">):</span>
</span><span id="ChatGLMConfig-14"><a href="#ChatGLMConfig-14"><span class="linenos">14</span></a>    <span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;chatglm&quot;</span>
</span><span id="ChatGLMConfig-15"><a href="#ChatGLMConfig-15"><span class="linenos">15</span></a>    <span class="n">attribute_map</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="ChatGLMConfig-16"><a href="#ChatGLMConfig-16"><span class="linenos">16</span></a>        <span class="s2">&quot;num_hidden_layers&quot;</span><span class="p">:</span> <span class="s2">&quot;num_layers&quot;</span><span class="p">,</span>
</span><span id="ChatGLMConfig-17"><a href="#ChatGLMConfig-17"><span class="linenos">17</span></a>        <span class="s2">&quot;n_head_kv&quot;</span><span class="p">:</span> <span class="s2">&quot;multi_query_group_num&quot;</span><span class="p">,</span>
</span><span id="ChatGLMConfig-18"><a href="#ChatGLMConfig-18"><span class="linenos">18</span></a>    <span class="p">}</span>
</span><span id="ChatGLMConfig-19"><a href="#ChatGLMConfig-19"><span class="linenos">19</span></a>
</span><span id="ChatGLMConfig-20"><a href="#ChatGLMConfig-20"><span class="linenos">20</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ChatGLMConfig-21"><a href="#ChatGLMConfig-21"><span class="linenos">21</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ChatGLMConfig-22"><a href="#ChatGLMConfig-22"><span class="linenos">22</span></a>        <span class="n">num_layers</span><span class="o">=</span><span class="mi">28</span><span class="p">,</span>
</span><span id="ChatGLMConfig-23"><a href="#ChatGLMConfig-23"><span class="linenos">23</span></a>        <span class="n">padded_vocab_size</span><span class="o">=</span><span class="mi">65024</span><span class="p">,</span>
</span><span id="ChatGLMConfig-24"><a href="#ChatGLMConfig-24"><span class="linenos">24</span></a>        <span class="n">hidden_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
</span><span id="ChatGLMConfig-25"><a href="#ChatGLMConfig-25"><span class="linenos">25</span></a>        <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">13696</span><span class="p">,</span>
</span><span id="ChatGLMConfig-26"><a href="#ChatGLMConfig-26"><span class="linenos">26</span></a>        <span class="n">kv_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
</span><span id="ChatGLMConfig-27"><a href="#ChatGLMConfig-27"><span class="linenos">27</span></a>        <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
</span><span id="ChatGLMConfig-28"><a href="#ChatGLMConfig-28"><span class="linenos">28</span></a>        <span class="n">seq_length</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
</span><span id="ChatGLMConfig-29"><a href="#ChatGLMConfig-29"><span class="linenos">29</span></a>        <span class="n">hidden_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
</span><span id="ChatGLMConfig-30"><a href="#ChatGLMConfig-30"><span class="linenos">30</span></a>        <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
</span><span id="ChatGLMConfig-31"><a href="#ChatGLMConfig-31"><span class="linenos">31</span></a>        <span class="n">layernorm_epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
</span><span id="ChatGLMConfig-32"><a href="#ChatGLMConfig-32"><span class="linenos">32</span></a>        <span class="n">rmsnorm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="ChatGLMConfig-33"><a href="#ChatGLMConfig-33"><span class="linenos">33</span></a>        <span class="n">apply_residual_connection_post_layernorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="ChatGLMConfig-34"><a href="#ChatGLMConfig-34"><span class="linenos">34</span></a>        <span class="n">post_layer_norm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="ChatGLMConfig-35"><a href="#ChatGLMConfig-35"><span class="linenos">35</span></a>        <span class="n">add_bias_linear</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="ChatGLMConfig-36"><a href="#ChatGLMConfig-36"><span class="linenos">36</span></a>        <span class="n">add_qkv_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="ChatGLMConfig-37"><a href="#ChatGLMConfig-37"><span class="linenos">37</span></a>        <span class="n">interleaved_qkv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="ChatGLMConfig-38"><a href="#ChatGLMConfig-38"><span class="linenos">38</span></a>        <span class="n">bias_dropout_fusion</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="ChatGLMConfig-39"><a href="#ChatGLMConfig-39"><span class="linenos">39</span></a>        <span class="n">multi_query_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="ChatGLMConfig-40"><a href="#ChatGLMConfig-40"><span class="linenos">40</span></a>        <span class="n">multi_query_group_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ChatGLMConfig-41"><a href="#ChatGLMConfig-41"><span class="linenos">41</span></a>        <span class="n">apply_query_key_layer_scaling</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="ChatGLMConfig-42"><a href="#ChatGLMConfig-42"><span class="linenos">42</span></a>        <span class="n">attention_softmax_in_fp32</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="ChatGLMConfig-43"><a href="#ChatGLMConfig-43"><span class="linenos">43</span></a>        <span class="n">fp32_residual_connection</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="ChatGLMConfig-44"><a href="#ChatGLMConfig-44"><span class="linenos">44</span></a>        <span class="n">quantization_bit</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="ChatGLMConfig-45"><a href="#ChatGLMConfig-45"><span class="linenos">45</span></a>        <span class="n">pre_seq_len</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="ChatGLMConfig-46"><a href="#ChatGLMConfig-46"><span class="linenos">46</span></a>        <span class="n">prefix_projection</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="ChatGLMConfig-47"><a href="#ChatGLMConfig-47"><span class="linenos">47</span></a>        <span class="o">**</span><span class="n">kwargs</span>
</span><span id="ChatGLMConfig-48"><a href="#ChatGLMConfig-48"><span class="linenos">48</span></a>    <span class="p">):</span>
</span><span id="ChatGLMConfig-49"><a href="#ChatGLMConfig-49"><span class="linenos">49</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
</span><span id="ChatGLMConfig-50"><a href="#ChatGLMConfig-50"><span class="linenos">50</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">padded_vocab_size</span>
</span><span id="ChatGLMConfig-51"><a href="#ChatGLMConfig-51"><span class="linenos">51</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">padded_vocab_size</span> <span class="o">=</span> <span class="n">padded_vocab_size</span>
</span><span id="ChatGLMConfig-52"><a href="#ChatGLMConfig-52"><span class="linenos">52</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span id="ChatGLMConfig-53"><a href="#ChatGLMConfig-53"><span class="linenos">53</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">ffn_hidden_size</span> <span class="o">=</span> <span class="n">ffn_hidden_size</span>
</span><span id="ChatGLMConfig-54"><a href="#ChatGLMConfig-54"><span class="linenos">54</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kv_channels</span> <span class="o">=</span> <span class="n">kv_channels</span>
</span><span id="ChatGLMConfig-55"><a href="#ChatGLMConfig-55"><span class="linenos">55</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">num_attention_heads</span>
</span><span id="ChatGLMConfig-56"><a href="#ChatGLMConfig-56"><span class="linenos">56</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_length</span>
</span><span id="ChatGLMConfig-57"><a href="#ChatGLMConfig-57"><span class="linenos">57</span></a>        <span class="c1"># It is to be compatible with long lora.</span>
</span><span id="ChatGLMConfig-58"><a href="#ChatGLMConfig-58"><span class="linenos">58</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="n">seq_length</span>
</span><span id="ChatGLMConfig-59"><a href="#ChatGLMConfig-59"><span class="linenos">59</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dropout</span> <span class="o">=</span> <span class="n">hidden_dropout</span>
</span><span id="ChatGLMConfig-60"><a href="#ChatGLMConfig-60"><span class="linenos">60</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">attention_dropout</span>
</span><span id="ChatGLMConfig-61"><a href="#ChatGLMConfig-61"><span class="linenos">61</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_epsilon</span> <span class="o">=</span> <span class="n">layernorm_epsilon</span>
</span><span id="ChatGLMConfig-62"><a href="#ChatGLMConfig-62"><span class="linenos">62</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">rmsnorm</span> <span class="o">=</span> <span class="n">rmsnorm</span>
</span><span id="ChatGLMConfig-63"><a href="#ChatGLMConfig-63"><span class="linenos">63</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_residual_connection_post_layernorm</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ChatGLMConfig-64"><a href="#ChatGLMConfig-64"><span class="linenos">64</span></a>            <span class="n">apply_residual_connection_post_layernorm</span>
</span><span id="ChatGLMConfig-65"><a href="#ChatGLMConfig-65"><span class="linenos">65</span></a>        <span class="p">)</span>
</span><span id="ChatGLMConfig-66"><a href="#ChatGLMConfig-66"><span class="linenos">66</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">post_layer_norm</span> <span class="o">=</span> <span class="n">post_layer_norm</span>
</span><span id="ChatGLMConfig-67"><a href="#ChatGLMConfig-67"><span class="linenos">67</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">add_bias_linear</span> <span class="o">=</span> <span class="n">add_bias_linear</span>
</span><span id="ChatGLMConfig-68"><a href="#ChatGLMConfig-68"><span class="linenos">68</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">add_qkv_bias</span> <span class="o">=</span> <span class="n">add_qkv_bias</span>
</span><span id="ChatGLMConfig-69"><a href="#ChatGLMConfig-69"><span class="linenos">69</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias_dropout_fusion</span> <span class="o">=</span> <span class="n">bias_dropout_fusion</span>
</span><span id="ChatGLMConfig-70"><a href="#ChatGLMConfig-70"><span class="linenos">70</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">multi_query_attention</span> <span class="o">=</span> <span class="n">multi_query_attention</span>
</span><span id="ChatGLMConfig-71"><a href="#ChatGLMConfig-71"><span class="linenos">71</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">multi_query_group_num</span> <span class="o">=</span> <span class="n">multi_query_group_num</span>
</span><span id="ChatGLMConfig-72"><a href="#ChatGLMConfig-72"><span class="linenos">72</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_query_key_layer_scaling</span> <span class="o">=</span> <span class="n">apply_query_key_layer_scaling</span>
</span><span id="ChatGLMConfig-73"><a href="#ChatGLMConfig-73"><span class="linenos">73</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention_softmax_in_fp32</span> <span class="o">=</span> <span class="n">attention_softmax_in_fp32</span>
</span><span id="ChatGLMConfig-74"><a href="#ChatGLMConfig-74"><span class="linenos">74</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">fp32_residual_connection</span> <span class="o">=</span> <span class="n">fp32_residual_connection</span>
</span><span id="ChatGLMConfig-75"><a href="#ChatGLMConfig-75"><span class="linenos">75</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">quantization_bit</span> <span class="o">=</span> <span class="n">quantization_bit</span>
</span><span id="ChatGLMConfig-76"><a href="#ChatGLMConfig-76"><span class="linenos">76</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pre_seq_len</span> <span class="o">=</span> <span class="n">pre_seq_len</span>
</span><span id="ChatGLMConfig-77"><a href="#ChatGLMConfig-77"><span class="linenos">77</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">prefix_projection</span> <span class="o">=</span> <span class="n">prefix_projection</span>
</span><span id="ChatGLMConfig-78"><a href="#ChatGLMConfig-78"><span class="linenos">78</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">interleaved_qkv</span> <span class="o">=</span> <span class="n">interleaved_qkv</span>
</span><span id="ChatGLMConfig-79"><a href="#ChatGLMConfig-79"><span class="linenos">79</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as
methods for loading/downloading/saving configurations.</p>

<p><Tip></p>

<p>A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to
initialize a model does <strong>not</strong> load the model weights. It only affects the model's configuration.</p>

<p></Tip></p>

<p>Class attributes (overridden by derived classes):</p>

<ul>
<li><strong>model_type</strong> (<code>str</code>) -- An identifier for the model type, serialized into the JSON file, and used to recreate
the correct object in [<code>~transformers.AutoConfig</code>].</li>
<li><strong>has_no_defaults_at_init</strong> (<code>bool</code>) -- Whether the config class can be initialized without providing input arguments.
Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,
(but not necessarily) such as [<code>~transformers.EncoderDecoderConfig</code>] or [<code>~RagConfig</code>]. They have to be initialized from
two or more configs of type [<code>~transformers.PretrainedConfig</code>].</li>
<li><strong>keys_to_ignore_at_inference</strong> (<code>list[str]</code>) -- A list of keys to ignore by default when looking at dictionary
outputs of the model during inference.</li>
<li><strong>attribute_map</strong> (<code>dict[str, str]</code>) -- A dict that maps model specific attribute names to the standardized
naming of attributes.</li>
<li><strong>base_model_tp_plan</strong> (<code>dict[str, Any]</code>) -- A dict that maps sub-modules FQNs of a base model to a tensor
parallel plan applied to the sub-module when <code>model.tensor_parallel</code> is called.</li>
<li><strong>base_model_pp_plan</strong> (<code>dict[str, tuple[list[str]]]</code>) -- A dict that maps child-modules of a base model to a
pipeline parallel plan that enables users to place the child-module on the appropriate device.</li>
</ul>

<p>Common attributes (present in all subclasses):</p>

<ul>
<li><strong>vocab_size</strong> (<code>int</code>) -- The number of tokens in the vocabulary, which is also the first dimension of the
embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).</li>
<li><strong>hidden_size</strong> (<code>int</code>) -- The hidden size of the model.</li>
<li><strong>num_attention_heads</strong> (<code>int</code>) -- The number of attention heads used in the multi-head attention layers of the
model.</li>
<li><strong>num_hidden_layers</strong> (<code>int</code>) -- The number of blocks in the model.</li>
</ul>

<p><Tip warning={true}&gt;</p>

<p>Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading
some of them will still be possible, but attempting to overwrite them will throw an exception -- you should set
them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more
information about the individual parameters.</p>

<p></Tip></p>

<p>Arg:
    name_or_path (<code>str</code>, <em>optional</em>, defaults to <code>""</code>):
        Store the string that was passed to [<code>PreTrainedModel.from_pretrained</code>] or
        [<code>TFPreTrainedModel.from_pretrained</code>] as <code>pretrained_model_name_or_path</code> if the configuration was created
        with such a method.
    output_hidden_states (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether or not the model should return all hidden-states.
    output_attentions (<code>bool</code>, *optional*, defaults to <code>False</code>):
        Whether or not the model should returns all attentions.
    return_dict (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
        Whether or not the model should return a [<code>~transformers.utils.ModelOutput</code>] instead of a plain tuple.
    is_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether the model is used as an encoder/decoder or not.
    is_decoder (<code>bool</code>, *optional*, defaults to <code>False</code>):
        Whether to only use the decoder in an encoder-decoder architecture, otherwise it has no effect on
        decoder-only or encoder-only architectures.
    cross_attention_hidden_size (<code>bool</code>, <em>optional</em>):
        The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder
        setting and the cross-attention hidden dimension differs from <code>self.config.hidden_size</code>.
    add_cross_attention (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether cross-attention layers should be added to the model. Note, this option is only relevant for models
        that can be used as decoder models within the [<code>EncoderDecoderModel</code>] class, which consists of all models
        in <code>AUTO_MODELS_FOR_CAUSAL_LM</code>.
    tie_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder
        and decoder model to have the exact same parameter names.
    prune_heads (<code>dict[int, list[int]]</code>, <em>optional</em>, defaults to <code>{}</code>):
        Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of
        heads to prune in said layer.</p>

<pre><code>    For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.
chunk_size_feed_forward (`int`, *optional*, defaults to `0`):
    The chunk size of all feed forward layers in the residual attention blocks. A chunk size of `0` means that
    the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes `n` &lt;
    sequence_length embeddings at a time. For more information on feed forward chunking, see [How does Feed
    Forward Chunking work?](../glossary.html#feed-forward-chunking).

&gt; Parameters for fine-tuning tasks

architectures (`list[str]`, *optional*):
    Model architectures that can be used with the model pretrained weights.
finetuning_task (`str`, *optional*):
    Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow
    or PyTorch) checkpoint.
id2label (`dict[int, str]`, *optional*):
    A map from index (for instance prediction index, or target index) to label.
label2id (`dict[str, int]`, *optional*):
    A map from label to index for the model.
num_labels (`int`, *optional*):
    Number of labels to use in the last layer added to the model, typically for a classification task.
task_specific_params (`dict[str, Any]`, *optional*):
    Additional keyword arguments to store for the current task.
problem_type (`str`, *optional*):
    Problem type for `XxxForSequenceClassification` models. Can be one of `"regression"`,
    `"single_label_classification"` or `"multi_label_classification"`.

&gt; Parameters linked to the tokenizer

tokenizer_class (`str`, *optional*):
    The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the
    model by default).
prefix (`str`, *optional*):
    A specific prompt that should be added at the beginning of each text before calling the model.
bos_token_id (`int`, *optional*):
    The id of the _beginning-of-stream_ token.
pad_token_id (`int`, *optional*):
    The id of the _padding_ token.
eos_token_id (`int`, *optional*):
    The id of the _end-of-stream_ token.
decoder_start_token_id (`int`, *optional*):
    If an encoder-decoder model starts decoding with a different token than _bos_, the id of that token.
sep_token_id (`int`, *optional*):
    The id of the _separation_ token.

&gt; PyTorch specific parameters

torchscript (`bool`, *optional*, defaults to `False`):
    Whether or not the model should be used with Torchscript.
tie_word_embeddings (`bool`, *optional*, defaults to `True`):
    Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the
    model has a output word embedding layer.
torch_dtype (`str`, *optional*):
    The `dtype` of the weights. This attribute can be used to initialize the model to a non-default `dtype`
    (which is normally `float32`) and thus allow for optimal storage allocation. For example, if the saved
    model is `float16`, ideally we want to load it back using the minimal amount of memory needed to load
    `float16` weights. Since the config object is stored in plain text, this attribute contains just the
    floating type string without the `torch.` prefix. For example, for `torch.float16` ``torch_dtype` is the
    `"float16"` string.

    This attribute is currently not being used during model loading time, but this may change in the future
    versions. But we can already start preparing for the future by saving the dtype with save_pretrained.
</code></pre>
</div>


                            <div id="ChatGLMConfig.__init__" class="classattr">
                                        <input id="ChatGLMConfig.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">ChatGLMConfig</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">num_layers</span><span class="o">=</span><span class="mi">28</span>,</span><span class="param">	<span class="n">padded_vocab_size</span><span class="o">=</span><span class="mi">65024</span>,</span><span class="param">	<span class="n">hidden_size</span><span class="o">=</span><span class="mi">4096</span>,</span><span class="param">	<span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">13696</span>,</span><span class="param">	<span class="n">kv_channels</span><span class="o">=</span><span class="mi">128</span>,</span><span class="param">	<span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">32</span>,</span><span class="param">	<span class="n">seq_length</span><span class="o">=</span><span class="mi">2048</span>,</span><span class="param">	<span class="n">hidden_dropout</span><span class="o">=</span><span class="mf">0.0</span>,</span><span class="param">	<span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.0</span>,</span><span class="param">	<span class="n">layernorm_epsilon</span><span class="o">=</span><span class="mf">1e-05</span>,</span><span class="param">	<span class="n">rmsnorm</span><span class="o">=</span><span class="kc">True</span>,</span><span class="param">	<span class="n">apply_residual_connection_post_layernorm</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">post_layer_norm</span><span class="o">=</span><span class="kc">True</span>,</span><span class="param">	<span class="n">add_bias_linear</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">add_qkv_bias</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">interleaved_qkv</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">bias_dropout_fusion</span><span class="o">=</span><span class="kc">True</span>,</span><span class="param">	<span class="n">multi_query_attention</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">multi_query_group_num</span><span class="o">=</span><span class="mi">1</span>,</span><span class="param">	<span class="n">apply_query_key_layer_scaling</span><span class="o">=</span><span class="kc">True</span>,</span><span class="param">	<span class="n">attention_softmax_in_fp32</span><span class="o">=</span><span class="kc">True</span>,</span><span class="param">	<span class="n">fp32_residual_connection</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="n">quantization_bit</span><span class="o">=</span><span class="mi">0</span>,</span><span class="param">	<span class="n">pre_seq_len</span><span class="o">=</span><span class="kc">None</span>,</span><span class="param">	<span class="n">prefix_projection</span><span class="o">=</span><span class="kc">False</span>,</span><span class="param">	<span class="o">**</span><span class="n">kwargs</span></span>)</span>

                <label class="view-source-button" for="ChatGLMConfig.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ChatGLMConfig.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ChatGLMConfig.__init__-20"><a href="#ChatGLMConfig.__init__-20"><span class="linenos">20</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="ChatGLMConfig.__init__-21"><a href="#ChatGLMConfig.__init__-21"><span class="linenos">21</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-22"><a href="#ChatGLMConfig.__init__-22"><span class="linenos">22</span></a>        <span class="n">num_layers</span><span class="o">=</span><span class="mi">28</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-23"><a href="#ChatGLMConfig.__init__-23"><span class="linenos">23</span></a>        <span class="n">padded_vocab_size</span><span class="o">=</span><span class="mi">65024</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-24"><a href="#ChatGLMConfig.__init__-24"><span class="linenos">24</span></a>        <span class="n">hidden_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-25"><a href="#ChatGLMConfig.__init__-25"><span class="linenos">25</span></a>        <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">13696</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-26"><a href="#ChatGLMConfig.__init__-26"><span class="linenos">26</span></a>        <span class="n">kv_channels</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-27"><a href="#ChatGLMConfig.__init__-27"><span class="linenos">27</span></a>        <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-28"><a href="#ChatGLMConfig.__init__-28"><span class="linenos">28</span></a>        <span class="n">seq_length</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-29"><a href="#ChatGLMConfig.__init__-29"><span class="linenos">29</span></a>        <span class="n">hidden_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-30"><a href="#ChatGLMConfig.__init__-30"><span class="linenos">30</span></a>        <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-31"><a href="#ChatGLMConfig.__init__-31"><span class="linenos">31</span></a>        <span class="n">layernorm_epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-32"><a href="#ChatGLMConfig.__init__-32"><span class="linenos">32</span></a>        <span class="n">rmsnorm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-33"><a href="#ChatGLMConfig.__init__-33"><span class="linenos">33</span></a>        <span class="n">apply_residual_connection_post_layernorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-34"><a href="#ChatGLMConfig.__init__-34"><span class="linenos">34</span></a>        <span class="n">post_layer_norm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-35"><a href="#ChatGLMConfig.__init__-35"><span class="linenos">35</span></a>        <span class="n">add_bias_linear</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-36"><a href="#ChatGLMConfig.__init__-36"><span class="linenos">36</span></a>        <span class="n">add_qkv_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-37"><a href="#ChatGLMConfig.__init__-37"><span class="linenos">37</span></a>        <span class="n">interleaved_qkv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-38"><a href="#ChatGLMConfig.__init__-38"><span class="linenos">38</span></a>        <span class="n">bias_dropout_fusion</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-39"><a href="#ChatGLMConfig.__init__-39"><span class="linenos">39</span></a>        <span class="n">multi_query_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-40"><a href="#ChatGLMConfig.__init__-40"><span class="linenos">40</span></a>        <span class="n">multi_query_group_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-41"><a href="#ChatGLMConfig.__init__-41"><span class="linenos">41</span></a>        <span class="n">apply_query_key_layer_scaling</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-42"><a href="#ChatGLMConfig.__init__-42"><span class="linenos">42</span></a>        <span class="n">attention_softmax_in_fp32</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-43"><a href="#ChatGLMConfig.__init__-43"><span class="linenos">43</span></a>        <span class="n">fp32_residual_connection</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-44"><a href="#ChatGLMConfig.__init__-44"><span class="linenos">44</span></a>        <span class="n">quantization_bit</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-45"><a href="#ChatGLMConfig.__init__-45"><span class="linenos">45</span></a>        <span class="n">pre_seq_len</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-46"><a href="#ChatGLMConfig.__init__-46"><span class="linenos">46</span></a>        <span class="n">prefix_projection</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
</span><span id="ChatGLMConfig.__init__-47"><a href="#ChatGLMConfig.__init__-47"><span class="linenos">47</span></a>        <span class="o">**</span><span class="n">kwargs</span>
</span><span id="ChatGLMConfig.__init__-48"><a href="#ChatGLMConfig.__init__-48"><span class="linenos">48</span></a>    <span class="p">):</span>
</span><span id="ChatGLMConfig.__init__-49"><a href="#ChatGLMConfig.__init__-49"><span class="linenos">49</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
</span><span id="ChatGLMConfig.__init__-50"><a href="#ChatGLMConfig.__init__-50"><span class="linenos">50</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">padded_vocab_size</span>
</span><span id="ChatGLMConfig.__init__-51"><a href="#ChatGLMConfig.__init__-51"><span class="linenos">51</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">padded_vocab_size</span> <span class="o">=</span> <span class="n">padded_vocab_size</span>
</span><span id="ChatGLMConfig.__init__-52"><a href="#ChatGLMConfig.__init__-52"><span class="linenos">52</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span id="ChatGLMConfig.__init__-53"><a href="#ChatGLMConfig.__init__-53"><span class="linenos">53</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">ffn_hidden_size</span> <span class="o">=</span> <span class="n">ffn_hidden_size</span>
</span><span id="ChatGLMConfig.__init__-54"><a href="#ChatGLMConfig.__init__-54"><span class="linenos">54</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kv_channels</span> <span class="o">=</span> <span class="n">kv_channels</span>
</span><span id="ChatGLMConfig.__init__-55"><a href="#ChatGLMConfig.__init__-55"><span class="linenos">55</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">num_attention_heads</span>
</span><span id="ChatGLMConfig.__init__-56"><a href="#ChatGLMConfig.__init__-56"><span class="linenos">56</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_length</span>
</span><span id="ChatGLMConfig.__init__-57"><a href="#ChatGLMConfig.__init__-57"><span class="linenos">57</span></a>        <span class="c1"># It is to be compatible with long lora.</span>
</span><span id="ChatGLMConfig.__init__-58"><a href="#ChatGLMConfig.__init__-58"><span class="linenos">58</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="n">seq_length</span>
</span><span id="ChatGLMConfig.__init__-59"><a href="#ChatGLMConfig.__init__-59"><span class="linenos">59</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dropout</span> <span class="o">=</span> <span class="n">hidden_dropout</span>
</span><span id="ChatGLMConfig.__init__-60"><a href="#ChatGLMConfig.__init__-60"><span class="linenos">60</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">attention_dropout</span>
</span><span id="ChatGLMConfig.__init__-61"><a href="#ChatGLMConfig.__init__-61"><span class="linenos">61</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_epsilon</span> <span class="o">=</span> <span class="n">layernorm_epsilon</span>
</span><span id="ChatGLMConfig.__init__-62"><a href="#ChatGLMConfig.__init__-62"><span class="linenos">62</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">rmsnorm</span> <span class="o">=</span> <span class="n">rmsnorm</span>
</span><span id="ChatGLMConfig.__init__-63"><a href="#ChatGLMConfig.__init__-63"><span class="linenos">63</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_residual_connection_post_layernorm</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="ChatGLMConfig.__init__-64"><a href="#ChatGLMConfig.__init__-64"><span class="linenos">64</span></a>            <span class="n">apply_residual_connection_post_layernorm</span>
</span><span id="ChatGLMConfig.__init__-65"><a href="#ChatGLMConfig.__init__-65"><span class="linenos">65</span></a>        <span class="p">)</span>
</span><span id="ChatGLMConfig.__init__-66"><a href="#ChatGLMConfig.__init__-66"><span class="linenos">66</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">post_layer_norm</span> <span class="o">=</span> <span class="n">post_layer_norm</span>
</span><span id="ChatGLMConfig.__init__-67"><a href="#ChatGLMConfig.__init__-67"><span class="linenos">67</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">add_bias_linear</span> <span class="o">=</span> <span class="n">add_bias_linear</span>
</span><span id="ChatGLMConfig.__init__-68"><a href="#ChatGLMConfig.__init__-68"><span class="linenos">68</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">add_qkv_bias</span> <span class="o">=</span> <span class="n">add_qkv_bias</span>
</span><span id="ChatGLMConfig.__init__-69"><a href="#ChatGLMConfig.__init__-69"><span class="linenos">69</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias_dropout_fusion</span> <span class="o">=</span> <span class="n">bias_dropout_fusion</span>
</span><span id="ChatGLMConfig.__init__-70"><a href="#ChatGLMConfig.__init__-70"><span class="linenos">70</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">multi_query_attention</span> <span class="o">=</span> <span class="n">multi_query_attention</span>
</span><span id="ChatGLMConfig.__init__-71"><a href="#ChatGLMConfig.__init__-71"><span class="linenos">71</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">multi_query_group_num</span> <span class="o">=</span> <span class="n">multi_query_group_num</span>
</span><span id="ChatGLMConfig.__init__-72"><a href="#ChatGLMConfig.__init__-72"><span class="linenos">72</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_query_key_layer_scaling</span> <span class="o">=</span> <span class="n">apply_query_key_layer_scaling</span>
</span><span id="ChatGLMConfig.__init__-73"><a href="#ChatGLMConfig.__init__-73"><span class="linenos">73</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">attention_softmax_in_fp32</span> <span class="o">=</span> <span class="n">attention_softmax_in_fp32</span>
</span><span id="ChatGLMConfig.__init__-74"><a href="#ChatGLMConfig.__init__-74"><span class="linenos">74</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">fp32_residual_connection</span> <span class="o">=</span> <span class="n">fp32_residual_connection</span>
</span><span id="ChatGLMConfig.__init__-75"><a href="#ChatGLMConfig.__init__-75"><span class="linenos">75</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">quantization_bit</span> <span class="o">=</span> <span class="n">quantization_bit</span>
</span><span id="ChatGLMConfig.__init__-76"><a href="#ChatGLMConfig.__init__-76"><span class="linenos">76</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pre_seq_len</span> <span class="o">=</span> <span class="n">pre_seq_len</span>
</span><span id="ChatGLMConfig.__init__-77"><a href="#ChatGLMConfig.__init__-77"><span class="linenos">77</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">prefix_projection</span> <span class="o">=</span> <span class="n">prefix_projection</span>
</span><span id="ChatGLMConfig.__init__-78"><a href="#ChatGLMConfig.__init__-78"><span class="linenos">78</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">interleaved_qkv</span> <span class="o">=</span> <span class="n">interleaved_qkv</span>
</span><span id="ChatGLMConfig.__init__-79"><a href="#ChatGLMConfig.__init__-79"><span class="linenos">79</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                            <div id="ChatGLMConfig.model_type" class="classattr">
                                <div class="attr variable">
            <span class="name">model_type</span>        =
<span class="default_value">&#39;chatglm&#39;</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.model_type"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.attribute_map" class="classattr">
                                <div class="attr variable">
            <span class="name">attribute_map</span>        =
<span class="default_value">{&#39;num_hidden_layers&#39;: &#39;num_layers&#39;, &#39;n_head_kv&#39;: &#39;multi_query_group_num&#39;}</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.attribute_map"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.num_layers" class="classattr">
                                <div class="attr variable">
            <span class="name">num_layers</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.num_layers"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.vocab_size" class="classattr">
                                <div class="attr variable">
            <span class="name">vocab_size</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.vocab_size"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.padded_vocab_size" class="classattr">
                                <div class="attr variable">
            <span class="name">padded_vocab_size</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.padded_vocab_size"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.hidden_size" class="classattr">
                                <div class="attr variable">
            <span class="name">hidden_size</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.hidden_size"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.ffn_hidden_size" class="classattr">
                                <div class="attr variable">
            <span class="name">ffn_hidden_size</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.ffn_hidden_size"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.kv_channels" class="classattr">
                                <div class="attr variable">
            <span class="name">kv_channels</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.kv_channels"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.num_attention_heads" class="classattr">
                                <div class="attr variable">
            <span class="name">num_attention_heads</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.num_attention_heads"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.seq_length" class="classattr">
                                <div class="attr variable">
            <span class="name">seq_length</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.seq_length"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.max_position_embeddings" class="classattr">
                                <div class="attr variable">
            <span class="name">max_position_embeddings</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.max_position_embeddings"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.hidden_dropout" class="classattr">
                                <div class="attr variable">
            <span class="name">hidden_dropout</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.hidden_dropout"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.attention_dropout" class="classattr">
                                <div class="attr variable">
            <span class="name">attention_dropout</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.attention_dropout"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.layernorm_epsilon" class="classattr">
                                <div class="attr variable">
            <span class="name">layernorm_epsilon</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.layernorm_epsilon"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.rmsnorm" class="classattr">
                                <div class="attr variable">
            <span class="name">rmsnorm</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.rmsnorm"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.apply_residual_connection_post_layernorm" class="classattr">
                                <div class="attr variable">
            <span class="name">apply_residual_connection_post_layernorm</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.apply_residual_connection_post_layernorm"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.post_layer_norm" class="classattr">
                                <div class="attr variable">
            <span class="name">post_layer_norm</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.post_layer_norm"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.add_bias_linear" class="classattr">
                                <div class="attr variable">
            <span class="name">add_bias_linear</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.add_bias_linear"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.add_qkv_bias" class="classattr">
                                <div class="attr variable">
            <span class="name">add_qkv_bias</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.add_qkv_bias"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.bias_dropout_fusion" class="classattr">
                                <div class="attr variable">
            <span class="name">bias_dropout_fusion</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.bias_dropout_fusion"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.multi_query_attention" class="classattr">
                                <div class="attr variable">
            <span class="name">multi_query_attention</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.multi_query_attention"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.multi_query_group_num" class="classattr">
                                <div class="attr variable">
            <span class="name">multi_query_group_num</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.multi_query_group_num"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.apply_query_key_layer_scaling" class="classattr">
                                <div class="attr variable">
            <span class="name">apply_query_key_layer_scaling</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.apply_query_key_layer_scaling"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.attention_softmax_in_fp32" class="classattr">
                                <div class="attr variable">
            <span class="name">attention_softmax_in_fp32</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.attention_softmax_in_fp32"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.fp32_residual_connection" class="classattr">
                                <div class="attr variable">
            <span class="name">fp32_residual_connection</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.fp32_residual_connection"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.quantization_bit" class="classattr">
                                <div class="attr variable">
            <span class="name">quantization_bit</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.quantization_bit"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.pre_seq_len" class="classattr">
                                <div class="attr variable">
            <span class="name">pre_seq_len</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.pre_seq_len"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.prefix_projection" class="classattr">
                                <div class="attr variable">
            <span class="name">prefix_projection</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.prefix_projection"></a>
    
    

                            </div>
                            <div id="ChatGLMConfig.interleaved_qkv" class="classattr">
                                <div class="attr variable">
            <span class="name">interleaved_qkv</span>

        
    </div>
    <a class="headerlink" href="#ChatGLMConfig.interleaved_qkv"></a>
    
    

                            </div>
                </section>
                <section id="DbrxConfig">
                            <input id="DbrxConfig-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">DbrxConfig</span><wbr>(<span class="base">transformers.configuration_utils.PretrainedConfig</span>):

                <label class="view-source-button" for="DbrxConfig-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#DbrxConfig"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DbrxConfig-166"><a href="#DbrxConfig-166"><span class="linenos">166</span></a><span class="k">class</span><span class="w"> </span><span class="nc">DbrxConfig</span><span class="p">(</span><span class="n">PretrainedConfig</span><span class="p">):</span>
</span><span id="DbrxConfig-167"><a href="#DbrxConfig-167"><span class="linenos">167</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Configuration class for Dbrx.</span>
</span><span id="DbrxConfig-168"><a href="#DbrxConfig-168"><span class="linenos">168</span></a>
</span><span id="DbrxConfig-169"><a href="#DbrxConfig-169"><span class="linenos">169</span></a><span class="sd">    [`DbrxModel`]. It is used to instantiate a Dbrx model according to the</span>
</span><span id="DbrxConfig-170"><a href="#DbrxConfig-170"><span class="linenos">170</span></a><span class="sd">    specified arguments, defining the model architecture.</span>
</span><span id="DbrxConfig-171"><a href="#DbrxConfig-171"><span class="linenos">171</span></a>
</span><span id="DbrxConfig-172"><a href="#DbrxConfig-172"><span class="linenos">172</span></a><span class="sd">    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the</span>
</span><span id="DbrxConfig-173"><a href="#DbrxConfig-173"><span class="linenos">173</span></a><span class="sd">    documentation from [`PretrainedConfig`] for more information.</span>
</span><span id="DbrxConfig-174"><a href="#DbrxConfig-174"><span class="linenos">174</span></a>
</span><span id="DbrxConfig-175"><a href="#DbrxConfig-175"><span class="linenos">175</span></a>
</span><span id="DbrxConfig-176"><a href="#DbrxConfig-176"><span class="linenos">176</span></a><span class="sd">    Args:</span>
</span><span id="DbrxConfig-177"><a href="#DbrxConfig-177"><span class="linenos">177</span></a><span class="sd">        d_model (`int`, *optional*, defaults to 6144):</span>
</span><span id="DbrxConfig-178"><a href="#DbrxConfig-178"><span class="linenos">178</span></a><span class="sd">            Dimensionality of the embeddings and hidden states.</span>
</span><span id="DbrxConfig-179"><a href="#DbrxConfig-179"><span class="linenos">179</span></a><span class="sd">        n_heads (`int`, *optional*, defaults to 48):</span>
</span><span id="DbrxConfig-180"><a href="#DbrxConfig-180"><span class="linenos">180</span></a><span class="sd">            Number of attention heads for each attention layer in the Transformer encoder.</span>
</span><span id="DbrxConfig-181"><a href="#DbrxConfig-181"><span class="linenos">181</span></a><span class="sd">        n_layers (`int`, *optional*, defaults to 40):</span>
</span><span id="DbrxConfig-182"><a href="#DbrxConfig-182"><span class="linenos">182</span></a><span class="sd">            Number of hidden layers in the Transformer encoder.</span>
</span><span id="DbrxConfig-183"><a href="#DbrxConfig-183"><span class="linenos">183</span></a><span class="sd">        max_seq_len (`int`, *optional*, defaults to 32768):</span>
</span><span id="DbrxConfig-184"><a href="#DbrxConfig-184"><span class="linenos">184</span></a><span class="sd">            The maximum sequence length of the model.</span>
</span><span id="DbrxConfig-185"><a href="#DbrxConfig-185"><span class="linenos">185</span></a><span class="sd">        vocab_size (`int`, *optional*, defaults to 100352):</span>
</span><span id="DbrxConfig-186"><a href="#DbrxConfig-186"><span class="linenos">186</span></a><span class="sd">            Vocabulary size of the Dbrx model. Defines the maximum number of different tokens that can be represented by</span>
</span><span id="DbrxConfig-187"><a href="#DbrxConfig-187"><span class="linenos">187</span></a><span class="sd">            the `inputs_ids` passed when calling [`DbrxModel`].</span>
</span><span id="DbrxConfig-188"><a href="#DbrxConfig-188"><span class="linenos">188</span></a><span class="sd">        resid_pdrop (`float`, *optional*, defaults to 0.0):</span>
</span><span id="DbrxConfig-189"><a href="#DbrxConfig-189"><span class="linenos">189</span></a><span class="sd">            The dropout probability applied to the attention output before combining with residual.</span>
</span><span id="DbrxConfig-190"><a href="#DbrxConfig-190"><span class="linenos">190</span></a><span class="sd">        emb_pdrop (`float`, *optional*, defaults to 0.0):</span>
</span><span id="DbrxConfig-191"><a href="#DbrxConfig-191"><span class="linenos">191</span></a><span class="sd">            The dropout probability for the embedding layer.</span>
</span><span id="DbrxConfig-192"><a href="#DbrxConfig-192"><span class="linenos">192</span></a><span class="sd">        attn_config (`dict`, *optional*):</span>
</span><span id="DbrxConfig-193"><a href="#DbrxConfig-193"><span class="linenos">193</span></a><span class="sd">            A dictionary used to configure the model&#39;s attention module.</span>
</span><span id="DbrxConfig-194"><a href="#DbrxConfig-194"><span class="linenos">194</span></a><span class="sd">        ffn_config (`dict`, *optional*):</span>
</span><span id="DbrxConfig-195"><a href="#DbrxConfig-195"><span class="linenos">195</span></a><span class="sd">            A dictionary used to configure the model&#39;s FFN module.</span>
</span><span id="DbrxConfig-196"><a href="#DbrxConfig-196"><span class="linenos">196</span></a><span class="sd">        use_cache (`bool`, *optional*, defaults to `False`):</span>
</span><span id="DbrxConfig-197"><a href="#DbrxConfig-197"><span class="linenos">197</span></a><span class="sd">            Whether or not the model should return the last key/values attentions (not used by all models).</span>
</span><span id="DbrxConfig-198"><a href="#DbrxConfig-198"><span class="linenos">198</span></a><span class="sd">        initializer_range (`float`, *optional*, defaults to 0.02):</span>
</span><span id="DbrxConfig-199"><a href="#DbrxConfig-199"><span class="linenos">199</span></a><span class="sd">            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</span>
</span><span id="DbrxConfig-200"><a href="#DbrxConfig-200"><span class="linenos">200</span></a><span class="sd">        output_router_logits (`bool`, *optional*, defaults to `False`):</span>
</span><span id="DbrxConfig-201"><a href="#DbrxConfig-201"><span class="linenos">201</span></a><span class="sd">            Whether or not the router logits should be returned by the model. Enabling this will also</span>
</span><span id="DbrxConfig-202"><a href="#DbrxConfig-202"><span class="linenos">202</span></a><span class="sd">            allow the model to output the auxiliary loss. See [here]() for more details</span>
</span><span id="DbrxConfig-203"><a href="#DbrxConfig-203"><span class="linenos">203</span></a><span class="sd">        router_aux_loss_coef (`float`, *optional*, defaults to 0.001):</span>
</span><span id="DbrxConfig-204"><a href="#DbrxConfig-204"><span class="linenos">204</span></a><span class="sd">            The aux loss factor for the total loss.</span>
</span><span id="DbrxConfig-205"><a href="#DbrxConfig-205"><span class="linenos">205</span></a>
</span><span id="DbrxConfig-206"><a href="#DbrxConfig-206"><span class="linenos">206</span></a>
</span><span id="DbrxConfig-207"><a href="#DbrxConfig-207"><span class="linenos">207</span></a><span class="sd">    Example:</span>
</span><span id="DbrxConfig-208"><a href="#DbrxConfig-208"><span class="linenos">208</span></a><span class="sd">    ```python</span>
</span><span id="DbrxConfig-209"><a href="#DbrxConfig-209"><span class="linenos">209</span></a><span class="sd">    &gt;&gt;&gt; from transformers import DbrxConfig, DbrxModel</span>
</span><span id="DbrxConfig-210"><a href="#DbrxConfig-210"><span class="linenos">210</span></a>
</span><span id="DbrxConfig-211"><a href="#DbrxConfig-211"><span class="linenos">211</span></a><span class="sd">    &gt;&gt;&gt; # Initializing a Dbrx configuration</span>
</span><span id="DbrxConfig-212"><a href="#DbrxConfig-212"><span class="linenos">212</span></a><span class="sd">    &gt;&gt;&gt; configuration = DbrxConfig()</span>
</span><span id="DbrxConfig-213"><a href="#DbrxConfig-213"><span class="linenos">213</span></a>
</span><span id="DbrxConfig-214"><a href="#DbrxConfig-214"><span class="linenos">214</span></a><span class="sd">    &gt;&gt;&gt; # Initializing a model (with random weights) from the configuration</span>
</span><span id="DbrxConfig-215"><a href="#DbrxConfig-215"><span class="linenos">215</span></a><span class="sd">    &gt;&gt;&gt; model = DbrxModel(configuration)</span>
</span><span id="DbrxConfig-216"><a href="#DbrxConfig-216"><span class="linenos">216</span></a>
</span><span id="DbrxConfig-217"><a href="#DbrxConfig-217"><span class="linenos">217</span></a><span class="sd">    &gt;&gt;&gt; # Accessing the model configuration</span>
</span><span id="DbrxConfig-218"><a href="#DbrxConfig-218"><span class="linenos">218</span></a><span class="sd">    &gt;&gt;&gt; configuration = model.config</span>
</span><span id="DbrxConfig-219"><a href="#DbrxConfig-219"><span class="linenos">219</span></a><span class="sd">    ```</span>
</span><span id="DbrxConfig-220"><a href="#DbrxConfig-220"><span class="linenos">220</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="DbrxConfig-221"><a href="#DbrxConfig-221"><span class="linenos">221</span></a>
</span><span id="DbrxConfig-222"><a href="#DbrxConfig-222"><span class="linenos">222</span></a>    <span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;dbrx&quot;</span>
</span><span id="DbrxConfig-223"><a href="#DbrxConfig-223"><span class="linenos">223</span></a>    <span class="n">attribute_map</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="DbrxConfig-224"><a href="#DbrxConfig-224"><span class="linenos">224</span></a>        <span class="s2">&quot;num_attention_heads&quot;</span><span class="p">:</span> <span class="s2">&quot;n_heads&quot;</span><span class="p">,</span>
</span><span id="DbrxConfig-225"><a href="#DbrxConfig-225"><span class="linenos">225</span></a>        <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="s2">&quot;d_model&quot;</span><span class="p">,</span>
</span><span id="DbrxConfig-226"><a href="#DbrxConfig-226"><span class="linenos">226</span></a>        <span class="s2">&quot;num_hidden_layers&quot;</span><span class="p">:</span> <span class="s2">&quot;n_layers&quot;</span><span class="p">,</span>
</span><span id="DbrxConfig-227"><a href="#DbrxConfig-227"><span class="linenos">227</span></a>        <span class="s2">&quot;max_position_embeddings&quot;</span><span class="p">:</span> <span class="s2">&quot;max_seq_len&quot;</span><span class="p">,</span>
</span><span id="DbrxConfig-228"><a href="#DbrxConfig-228"><span class="linenos">228</span></a>    <span class="p">}</span>
</span><span id="DbrxConfig-229"><a href="#DbrxConfig-229"><span class="linenos">229</span></a>
</span><span id="DbrxConfig-230"><a href="#DbrxConfig-230"><span class="linenos">230</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="DbrxConfig-231"><a href="#DbrxConfig-231"><span class="linenos">231</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="DbrxConfig-232"><a href="#DbrxConfig-232"><span class="linenos">232</span></a>        <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
</span><span id="DbrxConfig-233"><a href="#DbrxConfig-233"><span class="linenos">233</span></a>        <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
</span><span id="DbrxConfig-234"><a href="#DbrxConfig-234"><span class="linenos">234</span></a>        <span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span>
</span><span id="DbrxConfig-235"><a href="#DbrxConfig-235"><span class="linenos">235</span></a>        <span class="n">max_seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
</span><span id="DbrxConfig-236"><a href="#DbrxConfig-236"><span class="linenos">236</span></a>        <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32000</span><span class="p">,</span>
</span><span id="DbrxConfig-237"><a href="#DbrxConfig-237"><span class="linenos">237</span></a>        <span class="n">resid_pdrop</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="DbrxConfig-238"><a href="#DbrxConfig-238"><span class="linenos">238</span></a>        <span class="n">emb_pdrop</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="DbrxConfig-239"><a href="#DbrxConfig-239"><span class="linenos">239</span></a>        <span class="n">attn_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DbrxAttentionConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="DbrxConfig-240"><a href="#DbrxConfig-240"><span class="linenos">240</span></a>        <span class="n">ffn_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DbrxFFNConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="DbrxConfig-241"><a href="#DbrxConfig-241"><span class="linenos">241</span></a>        <span class="n">use_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="DbrxConfig-242"><a href="#DbrxConfig-242"><span class="linenos">242</span></a>        <span class="n">initializer_range</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">,</span>
</span><span id="DbrxConfig-243"><a href="#DbrxConfig-243"><span class="linenos">243</span></a>        <span class="n">output_router_logits</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="DbrxConfig-244"><a href="#DbrxConfig-244"><span class="linenos">244</span></a>        <span class="n">router_aux_loss_coef</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
</span><span id="DbrxConfig-245"><a href="#DbrxConfig-245"><span class="linenos">245</span></a>        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
</span><span id="DbrxConfig-246"><a href="#DbrxConfig-246"><span class="linenos">246</span></a>    <span class="p">):</span>
</span><span id="DbrxConfig-247"><a href="#DbrxConfig-247"><span class="linenos">247</span></a>        <span class="k">if</span> <span class="n">attn_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="DbrxConfig-248"><a href="#DbrxConfig-248"><span class="linenos">248</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">attn_config</span> <span class="o">=</span> <span class="n">DbrxAttentionConfig</span><span class="p">()</span>
</span><span id="DbrxConfig-249"><a href="#DbrxConfig-249"><span class="linenos">249</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attn_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
</span><span id="DbrxConfig-250"><a href="#DbrxConfig-250"><span class="linenos">250</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">attn_config</span> <span class="o">=</span> <span class="n">DbrxAttentionConfig</span><span class="p">(</span><span class="o">**</span><span class="n">attn_config</span><span class="p">)</span>
</span><span id="DbrxConfig-251"><a href="#DbrxConfig-251"><span class="linenos">251</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="DbrxConfig-252"><a href="#DbrxConfig-252"><span class="linenos">252</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">attn_config</span> <span class="o">=</span> <span class="n">attn_config</span>
</span><span id="DbrxConfig-253"><a href="#DbrxConfig-253"><span class="linenos">253</span></a>
</span><span id="DbrxConfig-254"><a href="#DbrxConfig-254"><span class="linenos">254</span></a>        <span class="k">if</span> <span class="n">ffn_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="DbrxConfig-255"><a href="#DbrxConfig-255"><span class="linenos">255</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_config</span> <span class="o">=</span> <span class="n">DbrxFFNConfig</span><span class="p">()</span>
</span><span id="DbrxConfig-256"><a href="#DbrxConfig-256"><span class="linenos">256</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ffn_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
</span><span id="DbrxConfig-257"><a href="#DbrxConfig-257"><span class="linenos">257</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_config</span> <span class="o">=</span> <span class="n">DbrxFFNConfig</span><span class="p">(</span><span class="o">**</span><span class="n">ffn_config</span><span class="p">)</span>
</span><span id="DbrxConfig-258"><a href="#DbrxConfig-258"><span class="linenos">258</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="DbrxConfig-259"><a href="#DbrxConfig-259"><span class="linenos">259</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_config</span> <span class="o">=</span> <span class="n">ffn_config</span>
</span><span id="DbrxConfig-260"><a href="#DbrxConfig-260"><span class="linenos">260</span></a>
</span><span id="DbrxConfig-261"><a href="#DbrxConfig-261"><span class="linenos">261</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
</span><span id="DbrxConfig-262"><a href="#DbrxConfig-262"><span class="linenos">262</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
</span><span id="DbrxConfig-263"><a href="#DbrxConfig-263"><span class="linenos">263</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
</span><span id="DbrxConfig-264"><a href="#DbrxConfig-264"><span class="linenos">264</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="o">=</span> <span class="n">max_seq_len</span>
</span><span id="DbrxConfig-265"><a href="#DbrxConfig-265"><span class="linenos">265</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
</span><span id="DbrxConfig-266"><a href="#DbrxConfig-266"><span class="linenos">266</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">resid_pdrop</span> <span class="o">=</span> <span class="n">resid_pdrop</span>
</span><span id="DbrxConfig-267"><a href="#DbrxConfig-267"><span class="linenos">267</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">emb_pdrop</span> <span class="o">=</span> <span class="n">emb_pdrop</span>
</span><span id="DbrxConfig-268"><a href="#DbrxConfig-268"><span class="linenos">268</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span>
</span><span id="DbrxConfig-269"><a href="#DbrxConfig-269"><span class="linenos">269</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">initializer_range</span> <span class="o">=</span> <span class="n">initializer_range</span>
</span><span id="DbrxConfig-270"><a href="#DbrxConfig-270"><span class="linenos">270</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_router_logits</span> <span class="o">=</span> <span class="n">output_router_logits</span>
</span><span id="DbrxConfig-271"><a href="#DbrxConfig-271"><span class="linenos">271</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">router_aux_loss_coef</span> <span class="o">=</span> <span class="n">router_aux_loss_coef</span>
</span><span id="DbrxConfig-272"><a href="#DbrxConfig-272"><span class="linenos">272</span></a>
</span><span id="DbrxConfig-273"><a href="#DbrxConfig-273"><span class="linenos">273</span></a>        <span class="n">tie_word_embeddings</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;tie_word_embeddings&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
</span><span id="DbrxConfig-274"><a href="#DbrxConfig-274"><span class="linenos">274</span></a>        <span class="k">if</span> <span class="n">tie_word_embeddings</span><span class="p">:</span>
</span><span id="DbrxConfig-275"><a href="#DbrxConfig-275"><span class="linenos">275</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;tie_word_embeddings is not supported for Dbrx models.&quot;</span><span class="p">)</span>
</span><span id="DbrxConfig-276"><a href="#DbrxConfig-276"><span class="linenos">276</span></a>
</span><span id="DbrxConfig-277"><a href="#DbrxConfig-277"><span class="linenos">277</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="DbrxConfig-278"><a href="#DbrxConfig-278"><span class="linenos">278</span></a>            <span class="n">tie_word_embeddings</span><span class="o">=</span><span class="n">tie_word_embeddings</span><span class="p">,</span>
</span><span id="DbrxConfig-279"><a href="#DbrxConfig-279"><span class="linenos">279</span></a>            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span id="DbrxConfig-280"><a href="#DbrxConfig-280"><span class="linenos">280</span></a>        <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Configuration class for Dbrx.</p>

<p>[<code>DbrxModel</code>]. It is used to instantiate a Dbrx model according to the
specified arguments, defining the model architecture.</p>

<p>Configuration objects inherit from [<code>PretrainedConfig</code>] and can be used to control the model outputs. Read the
documentation from [<code>PretrainedConfig</code>] for more information.</p>

<p>Args:
    d_model (<code>int</code>, *optional*, defaults to 6144):
        Dimensionality of the embeddings and hidden states.
    n_heads (<code>int</code>, <em>optional</em>, defaults to 48):
        Number of attention heads for each attention layer in the Transformer encoder.
    n_layers (<code>int</code>, *optional*, defaults to 40):
        Number of hidden layers in the Transformer encoder.
    max_seq_len (<code>int</code>, *optional*, defaults to 32768):
        The maximum sequence length of the model.
    vocab_size (<code>int</code>, <em>optional</em>, defaults to 100352):
        Vocabulary size of the Dbrx model. Defines the maximum number of different tokens that can be represented by
        the <code>inputs_ids</code> passed when calling [<code>DbrxModel</code>].
    resid_pdrop (<code>float</code>, *optional*, defaults to 0.0):
        The dropout probability applied to the attention output before combining with residual.
    emb_pdrop (<code>float</code>, <em>optional</em>, defaults to 0.0):
        The dropout probability for the embedding layer.
    attn_config (<code>dict</code>, *optional*):
        A dictionary used to configure the model's attention module.
    ffn_config (<code>dict</code>, <em>optional</em>):
        A dictionary used to configure the model's FFN module.
    use_cache (<code>bool</code>, *optional*, defaults to <code>False</code>):
        Whether or not the model should return the last key/values attentions (not used by all models).
    initializer_range (<code>float</code>, <em>optional</em>, defaults to 0.02):
        The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
    output_router_logits (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether or not the router logits should be returned by the model. Enabling this will also
        allow the model to output the auxiliary loss. See <a href="">here</a> for more details
    router_aux_loss_coef (<code>float</code>, <em>optional</em>, defaults to 0.001):
        The aux loss factor for the total loss.</p>

<p>Example:</p>

<div class="pdoc-code codehilite">
<pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DbrxConfig</span><span class="p">,</span> <span class="n">DbrxModel</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Initializing a Dbrx configuration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">configuration</span> <span class="o">=</span> <span class="n">DbrxConfig</span><span class="p">()</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Initializing a model (with random weights) from the configuration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">DbrxModel</span><span class="p">(</span><span class="n">configuration</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Accessing the model configuration</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">configuration</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
</code></pre>
</div>
</div>


                            <div id="DbrxConfig.__init__" class="classattr">
                                        <input id="DbrxConfig.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">DbrxConfig</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span>,</span><span class="param">	<span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span>,</span><span class="param">	<span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span>,</span><span class="param">	<span class="n">max_seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span>,</span><span class="param">	<span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32000</span>,</span><span class="param">	<span class="n">resid_pdrop</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>,</span><span class="param">	<span class="n">emb_pdrop</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>,</span><span class="param">	<span class="n">attn_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">sglang</span><span class="o">.</span><span class="n">srt</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">dbrx</span><span class="o">.</span><span class="n">DbrxAttentionConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">ffn_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">sglang</span><span class="o">.</span><span class="n">srt</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">dbrx</span><span class="o">.</span><span class="n">DbrxFFNConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">use_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">initializer_range</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.02</span>,</span><span class="param">	<span class="n">output_router_logits</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">router_aux_loss_coef</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span>,</span><span class="param">	<span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span></span>)</span>

                <label class="view-source-button" for="DbrxConfig.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#DbrxConfig.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DbrxConfig.__init__-230"><a href="#DbrxConfig.__init__-230"><span class="linenos">230</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="DbrxConfig.__init__-231"><a href="#DbrxConfig.__init__-231"><span class="linenos">231</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="DbrxConfig.__init__-232"><a href="#DbrxConfig.__init__-232"><span class="linenos">232</span></a>        <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
</span><span id="DbrxConfig.__init__-233"><a href="#DbrxConfig.__init__-233"><span class="linenos">233</span></a>        <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
</span><span id="DbrxConfig.__init__-234"><a href="#DbrxConfig.__init__-234"><span class="linenos">234</span></a>        <span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">24</span><span class="p">,</span>
</span><span id="DbrxConfig.__init__-235"><a href="#DbrxConfig.__init__-235"><span class="linenos">235</span></a>        <span class="n">max_seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
</span><span id="DbrxConfig.__init__-236"><a href="#DbrxConfig.__init__-236"><span class="linenos">236</span></a>        <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32000</span><span class="p">,</span>
</span><span id="DbrxConfig.__init__-237"><a href="#DbrxConfig.__init__-237"><span class="linenos">237</span></a>        <span class="n">resid_pdrop</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="DbrxConfig.__init__-238"><a href="#DbrxConfig.__init__-238"><span class="linenos">238</span></a>        <span class="n">emb_pdrop</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
</span><span id="DbrxConfig.__init__-239"><a href="#DbrxConfig.__init__-239"><span class="linenos">239</span></a>        <span class="n">attn_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DbrxAttentionConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="DbrxConfig.__init__-240"><a href="#DbrxConfig.__init__-240"><span class="linenos">240</span></a>        <span class="n">ffn_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">DbrxFFNConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="DbrxConfig.__init__-241"><a href="#DbrxConfig.__init__-241"><span class="linenos">241</span></a>        <span class="n">use_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="DbrxConfig.__init__-242"><a href="#DbrxConfig.__init__-242"><span class="linenos">242</span></a>        <span class="n">initializer_range</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">,</span>
</span><span id="DbrxConfig.__init__-243"><a href="#DbrxConfig.__init__-243"><span class="linenos">243</span></a>        <span class="n">output_router_logits</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="DbrxConfig.__init__-244"><a href="#DbrxConfig.__init__-244"><span class="linenos">244</span></a>        <span class="n">router_aux_loss_coef</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
</span><span id="DbrxConfig.__init__-245"><a href="#DbrxConfig.__init__-245"><span class="linenos">245</span></a>        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
</span><span id="DbrxConfig.__init__-246"><a href="#DbrxConfig.__init__-246"><span class="linenos">246</span></a>    <span class="p">):</span>
</span><span id="DbrxConfig.__init__-247"><a href="#DbrxConfig.__init__-247"><span class="linenos">247</span></a>        <span class="k">if</span> <span class="n">attn_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="DbrxConfig.__init__-248"><a href="#DbrxConfig.__init__-248"><span class="linenos">248</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">attn_config</span> <span class="o">=</span> <span class="n">DbrxAttentionConfig</span><span class="p">()</span>
</span><span id="DbrxConfig.__init__-249"><a href="#DbrxConfig.__init__-249"><span class="linenos">249</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attn_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
</span><span id="DbrxConfig.__init__-250"><a href="#DbrxConfig.__init__-250"><span class="linenos">250</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">attn_config</span> <span class="o">=</span> <span class="n">DbrxAttentionConfig</span><span class="p">(</span><span class="o">**</span><span class="n">attn_config</span><span class="p">)</span>
</span><span id="DbrxConfig.__init__-251"><a href="#DbrxConfig.__init__-251"><span class="linenos">251</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="DbrxConfig.__init__-252"><a href="#DbrxConfig.__init__-252"><span class="linenos">252</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">attn_config</span> <span class="o">=</span> <span class="n">attn_config</span>
</span><span id="DbrxConfig.__init__-253"><a href="#DbrxConfig.__init__-253"><span class="linenos">253</span></a>
</span><span id="DbrxConfig.__init__-254"><a href="#DbrxConfig.__init__-254"><span class="linenos">254</span></a>        <span class="k">if</span> <span class="n">ffn_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="DbrxConfig.__init__-255"><a href="#DbrxConfig.__init__-255"><span class="linenos">255</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_config</span> <span class="o">=</span> <span class="n">DbrxFFNConfig</span><span class="p">()</span>
</span><span id="DbrxConfig.__init__-256"><a href="#DbrxConfig.__init__-256"><span class="linenos">256</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ffn_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
</span><span id="DbrxConfig.__init__-257"><a href="#DbrxConfig.__init__-257"><span class="linenos">257</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_config</span> <span class="o">=</span> <span class="n">DbrxFFNConfig</span><span class="p">(</span><span class="o">**</span><span class="n">ffn_config</span><span class="p">)</span>
</span><span id="DbrxConfig.__init__-258"><a href="#DbrxConfig.__init__-258"><span class="linenos">258</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="DbrxConfig.__init__-259"><a href="#DbrxConfig.__init__-259"><span class="linenos">259</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">ffn_config</span> <span class="o">=</span> <span class="n">ffn_config</span>
</span><span id="DbrxConfig.__init__-260"><a href="#DbrxConfig.__init__-260"><span class="linenos">260</span></a>
</span><span id="DbrxConfig.__init__-261"><a href="#DbrxConfig.__init__-261"><span class="linenos">261</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
</span><span id="DbrxConfig.__init__-262"><a href="#DbrxConfig.__init__-262"><span class="linenos">262</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
</span><span id="DbrxConfig.__init__-263"><a href="#DbrxConfig.__init__-263"><span class="linenos">263</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
</span><span id="DbrxConfig.__init__-264"><a href="#DbrxConfig.__init__-264"><span class="linenos">264</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="o">=</span> <span class="n">max_seq_len</span>
</span><span id="DbrxConfig.__init__-265"><a href="#DbrxConfig.__init__-265"><span class="linenos">265</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
</span><span id="DbrxConfig.__init__-266"><a href="#DbrxConfig.__init__-266"><span class="linenos">266</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">resid_pdrop</span> <span class="o">=</span> <span class="n">resid_pdrop</span>
</span><span id="DbrxConfig.__init__-267"><a href="#DbrxConfig.__init__-267"><span class="linenos">267</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">emb_pdrop</span> <span class="o">=</span> <span class="n">emb_pdrop</span>
</span><span id="DbrxConfig.__init__-268"><a href="#DbrxConfig.__init__-268"><span class="linenos">268</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span>
</span><span id="DbrxConfig.__init__-269"><a href="#DbrxConfig.__init__-269"><span class="linenos">269</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">initializer_range</span> <span class="o">=</span> <span class="n">initializer_range</span>
</span><span id="DbrxConfig.__init__-270"><a href="#DbrxConfig.__init__-270"><span class="linenos">270</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_router_logits</span> <span class="o">=</span> <span class="n">output_router_logits</span>
</span><span id="DbrxConfig.__init__-271"><a href="#DbrxConfig.__init__-271"><span class="linenos">271</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">router_aux_loss_coef</span> <span class="o">=</span> <span class="n">router_aux_loss_coef</span>
</span><span id="DbrxConfig.__init__-272"><a href="#DbrxConfig.__init__-272"><span class="linenos">272</span></a>
</span><span id="DbrxConfig.__init__-273"><a href="#DbrxConfig.__init__-273"><span class="linenos">273</span></a>        <span class="n">tie_word_embeddings</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;tie_word_embeddings&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
</span><span id="DbrxConfig.__init__-274"><a href="#DbrxConfig.__init__-274"><span class="linenos">274</span></a>        <span class="k">if</span> <span class="n">tie_word_embeddings</span><span class="p">:</span>
</span><span id="DbrxConfig.__init__-275"><a href="#DbrxConfig.__init__-275"><span class="linenos">275</span></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;tie_word_embeddings is not supported for Dbrx models.&quot;</span><span class="p">)</span>
</span><span id="DbrxConfig.__init__-276"><a href="#DbrxConfig.__init__-276"><span class="linenos">276</span></a>
</span><span id="DbrxConfig.__init__-277"><a href="#DbrxConfig.__init__-277"><span class="linenos">277</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="DbrxConfig.__init__-278"><a href="#DbrxConfig.__init__-278"><span class="linenos">278</span></a>            <span class="n">tie_word_embeddings</span><span class="o">=</span><span class="n">tie_word_embeddings</span><span class="p">,</span>
</span><span id="DbrxConfig.__init__-279"><a href="#DbrxConfig.__init__-279"><span class="linenos">279</span></a>            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span id="DbrxConfig.__init__-280"><a href="#DbrxConfig.__init__-280"><span class="linenos">280</span></a>        <span class="p">)</span>
</span></pre></div>


    

                            </div>
                            <div id="DbrxConfig.model_type" class="classattr">
                                <div class="attr variable">
            <span class="name">model_type</span>        =
<span class="default_value">&#39;dbrx&#39;</span>

        
    </div>
    <a class="headerlink" href="#DbrxConfig.model_type"></a>
    
    

                            </div>
                            <div id="DbrxConfig.attribute_map" class="classattr">
                                <div class="attr variable">
            <span class="name">attribute_map</span>        =
<input id="DbrxConfig.attribute_map-view-value" class="view-value-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
            <label class="view-value-button pdoc-button" for="DbrxConfig.attribute_map-view-value"></label><span class="default_value">{&#39;num_attention_heads&#39;: &#39;n_heads&#39;, &#39;hidden_size&#39;: &#39;d_model&#39;, &#39;num_hidden_layers&#39;: &#39;n_layers&#39;, &#39;max_position_embeddings&#39;: &#39;max_seq_len&#39;}</span>

        
    </div>
    <a class="headerlink" href="#DbrxConfig.attribute_map"></a>
    
    

                            </div>
                            <div id="DbrxConfig.d_model" class="classattr">
                                <div class="attr variable">
            <span class="name">d_model</span>

        
    </div>
    <a class="headerlink" href="#DbrxConfig.d_model"></a>
    
    

                            </div>
                            <div id="DbrxConfig.n_heads" class="classattr">
                                <div class="attr variable">
            <span class="name">n_heads</span>

        
    </div>
    <a class="headerlink" href="#DbrxConfig.n_heads"></a>
    
    

                            </div>
                            <div id="DbrxConfig.n_layers" class="classattr">
                                <div class="attr variable">
            <span class="name">n_layers</span>

        
    </div>
    <a class="headerlink" href="#DbrxConfig.n_layers"></a>
    
    

                            </div>
                            <div id="DbrxConfig.max_seq_len" class="classattr">
                                <div class="attr variable">
            <span class="name">max_seq_len</span>

        
    </div>
    <a class="headerlink" href="#DbrxConfig.max_seq_len"></a>
    
    

                            </div>
                            <div id="DbrxConfig.vocab_size" class="classattr">
                                <div class="attr variable">
            <span class="name">vocab_size</span>

        
    </div>
    <a class="headerlink" href="#DbrxConfig.vocab_size"></a>
    
    

                            </div>
                            <div id="DbrxConfig.resid_pdrop" class="classattr">
                                <div class="attr variable">
            <span class="name">resid_pdrop</span>

        
    </div>
    <a class="headerlink" href="#DbrxConfig.resid_pdrop"></a>
    
    

                            </div>
                            <div id="DbrxConfig.emb_pdrop" class="classattr">
                                <div class="attr variable">
            <span class="name">emb_pdrop</span>

        
    </div>
    <a class="headerlink" href="#DbrxConfig.emb_pdrop"></a>
    
    

                            </div>
                            <div id="DbrxConfig.use_cache" class="classattr">
                                <div class="attr variable">
            <span class="name">use_cache</span>

        
    </div>
    <a class="headerlink" href="#DbrxConfig.use_cache"></a>
    
    

                            </div>
                            <div id="DbrxConfig.initializer_range" class="classattr">
                                <div class="attr variable">
            <span class="name">initializer_range</span>

        
    </div>
    <a class="headerlink" href="#DbrxConfig.initializer_range"></a>
    
    

                            </div>
                            <div id="DbrxConfig.output_router_logits" class="classattr">
                                <div class="attr variable">
            <span class="name">output_router_logits</span>

        
    </div>
    <a class="headerlink" href="#DbrxConfig.output_router_logits"></a>
    
    

                            </div>
                            <div id="DbrxConfig.router_aux_loss_coef" class="classattr">
                                <div class="attr variable">
            <span class="name">router_aux_loss_coef</span>

        
    </div>
    <a class="headerlink" href="#DbrxConfig.router_aux_loss_coef"></a>
    
    

                            </div>
                </section>
                <section id="DeepseekVL2Config">
                            <input id="DeepseekVL2Config-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">DeepseekVL2Config</span><wbr>(<span class="base">transformers.configuration_utils.PretrainedConfig</span>):

                <label class="view-source-button" for="DeepseekVL2Config-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#DeepseekVL2Config"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DeepseekVL2Config-652"><a href="#DeepseekVL2Config-652"><span class="linenos">652</span></a><span class="k">class</span><span class="w"> </span><span class="nc">DeepseekVL2Config</span><span class="p">(</span><span class="n">PretrainedConfig</span><span class="p">):</span>
</span><span id="DeepseekVL2Config-653"><a href="#DeepseekVL2Config-653"><span class="linenos">653</span></a>    <span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;deepseek_vl_v2&quot;</span>
</span><span id="DeepseekVL2Config-654"><a href="#DeepseekVL2Config-654"><span class="linenos">654</span></a>    <span class="n">vision_config</span><span class="p">:</span> <span class="n">DeepseekVL2VisionEncoderConfig</span>
</span><span id="DeepseekVL2Config-655"><a href="#DeepseekVL2Config-655"><span class="linenos">655</span></a>    <span class="n">projector_config</span><span class="p">:</span> <span class="n">DeepseekVL2MlpProjectorConfig</span>
</span><span id="DeepseekVL2Config-656"><a href="#DeepseekVL2Config-656"><span class="linenos">656</span></a>    <span class="n">language_config</span><span class="p">:</span> <span class="n">DeepseekV2Config</span>
</span><span id="DeepseekVL2Config-657"><a href="#DeepseekVL2Config-657"><span class="linenos">657</span></a>
</span><span id="DeepseekVL2Config-658"><a href="#DeepseekVL2Config-658"><span class="linenos">658</span></a>    <span class="n">tile_tag</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;2D&quot;</span>
</span><span id="DeepseekVL2Config-659"><a href="#DeepseekVL2Config-659"><span class="linenos">659</span></a>    <span class="n">global_view_pos</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;head&quot;</span>
</span><span id="DeepseekVL2Config-660"><a href="#DeepseekVL2Config-660"><span class="linenos">660</span></a>    <span class="n">candidate_resolutions</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">((</span><span class="mi">384</span><span class="p">,</span> <span class="mi">384</span><span class="p">),)</span>
</span><span id="DeepseekVL2Config-661"><a href="#DeepseekVL2Config-661"><span class="linenos">661</span></a>
</span><span id="DeepseekVL2Config-662"><a href="#DeepseekVL2Config-662"><span class="linenos">662</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="DeepseekVL2Config-663"><a href="#DeepseekVL2Config-663"><span class="linenos">663</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="DeepseekVL2Config-664"><a href="#DeepseekVL2Config-664"><span class="linenos">664</span></a>        <span class="n">tile_tag</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;tile_tag&quot;</span><span class="p">,</span>
</span><span id="DeepseekVL2Config-665"><a href="#DeepseekVL2Config-665"><span class="linenos">665</span></a>        <span class="n">global_view_pos</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;head&quot;</span><span class="p">,</span>
</span><span id="DeepseekVL2Config-666"><a href="#DeepseekVL2Config-666"><span class="linenos">666</span></a>        <span class="n">candidate_resolutions</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">((</span><span class="mi">384</span><span class="p">,</span> <span class="mi">384</span><span class="p">),),</span>
</span><span id="DeepseekVL2Config-667"><a href="#DeepseekVL2Config-667"><span class="linenos">667</span></a>        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span id="DeepseekVL2Config-668"><a href="#DeepseekVL2Config-668"><span class="linenos">668</span></a>    <span class="p">):</span>
</span><span id="DeepseekVL2Config-669"><a href="#DeepseekVL2Config-669"><span class="linenos">669</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="DeepseekVL2Config-670"><a href="#DeepseekVL2Config-670"><span class="linenos">670</span></a>
</span><span id="DeepseekVL2Config-671"><a href="#DeepseekVL2Config-671"><span class="linenos">671</span></a>        <span class="n">vision_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;vision_config&quot;</span><span class="p">,</span> <span class="p">{})</span>
</span><span id="DeepseekVL2Config-672"><a href="#DeepseekVL2Config-672"><span class="linenos">672</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vision_config</span> <span class="o">=</span> <span class="n">DeepseekVL2VisionEncoderConfig</span><span class="p">(</span><span class="o">**</span><span class="n">vision_config</span><span class="p">)</span>
</span><span id="DeepseekVL2Config-673"><a href="#DeepseekVL2Config-673"><span class="linenos">673</span></a>
</span><span id="DeepseekVL2Config-674"><a href="#DeepseekVL2Config-674"><span class="linenos">674</span></a>        <span class="n">projector_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;projector_config&quot;</span><span class="p">,</span> <span class="p">{})</span>
</span><span id="DeepseekVL2Config-675"><a href="#DeepseekVL2Config-675"><span class="linenos">675</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">projector_config</span> <span class="o">=</span> <span class="n">DeepseekVL2MlpProjectorConfig</span><span class="p">(</span><span class="o">**</span><span class="n">projector_config</span><span class="p">)</span>
</span><span id="DeepseekVL2Config-676"><a href="#DeepseekVL2Config-676"><span class="linenos">676</span></a>
</span><span id="DeepseekVL2Config-677"><a href="#DeepseekVL2Config-677"><span class="linenos">677</span></a>        <span class="n">language_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;language_config&quot;</span><span class="p">,</span> <span class="p">{})</span>
</span><span id="DeepseekVL2Config-678"><a href="#DeepseekVL2Config-678"><span class="linenos">678</span></a>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">language_config</span><span class="p">,</span> <span class="n">DeepseekV2Config</span><span class="p">):</span>
</span><span id="DeepseekVL2Config-679"><a href="#DeepseekVL2Config-679"><span class="linenos">679</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">language_config</span> <span class="o">=</span> <span class="n">language_config</span>
</span><span id="DeepseekVL2Config-680"><a href="#DeepseekVL2Config-680"><span class="linenos">680</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="DeepseekVL2Config-681"><a href="#DeepseekVL2Config-681"><span class="linenos">681</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">language_config</span> <span class="o">=</span> <span class="n">DeepseekV2Config</span><span class="p">(</span><span class="o">**</span><span class="n">language_config</span><span class="p">)</span>
</span><span id="DeepseekVL2Config-682"><a href="#DeepseekVL2Config-682"><span class="linenos">682</span></a>
</span><span id="DeepseekVL2Config-683"><a href="#DeepseekVL2Config-683"><span class="linenos">683</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">tile_tag</span> <span class="o">=</span> <span class="n">tile_tag</span>
</span><span id="DeepseekVL2Config-684"><a href="#DeepseekVL2Config-684"><span class="linenos">684</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">global_view_pos</span> <span class="o">=</span> <span class="n">global_view_pos</span>
</span><span id="DeepseekVL2Config-685"><a href="#DeepseekVL2Config-685"><span class="linenos">685</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">candidate_resolutions</span> <span class="o">=</span> <span class="n">candidate_resolutions</span>
</span><span id="DeepseekVL2Config-686"><a href="#DeepseekVL2Config-686"><span class="linenos">686</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">architectures</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;DeepseekVL2ForCausalLM&quot;</span><span class="p">]</span>
</span></pre></div>


            <div class="docstring"><p>Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as
methods for loading/downloading/saving configurations.</p>

<p><Tip></p>

<p>A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to
initialize a model does <strong>not</strong> load the model weights. It only affects the model's configuration.</p>

<p></Tip></p>

<p>Class attributes (overridden by derived classes):</p>

<ul>
<li><strong>model_type</strong> (<code>str</code>) -- An identifier for the model type, serialized into the JSON file, and used to recreate
the correct object in [<code>~transformers.AutoConfig</code>].</li>
<li><strong>has_no_defaults_at_init</strong> (<code>bool</code>) -- Whether the config class can be initialized without providing input arguments.
Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,
(but not necessarily) such as [<code>~transformers.EncoderDecoderConfig</code>] or [<code>~RagConfig</code>]. They have to be initialized from
two or more configs of type [<code>~transformers.PretrainedConfig</code>].</li>
<li><strong>keys_to_ignore_at_inference</strong> (<code>list[str]</code>) -- A list of keys to ignore by default when looking at dictionary
outputs of the model during inference.</li>
<li><strong>attribute_map</strong> (<code>dict[str, str]</code>) -- A dict that maps model specific attribute names to the standardized
naming of attributes.</li>
<li><strong>base_model_tp_plan</strong> (<code>dict[str, Any]</code>) -- A dict that maps sub-modules FQNs of a base model to a tensor
parallel plan applied to the sub-module when <code>model.tensor_parallel</code> is called.</li>
<li><strong>base_model_pp_plan</strong> (<code>dict[str, tuple[list[str]]]</code>) -- A dict that maps child-modules of a base model to a
pipeline parallel plan that enables users to place the child-module on the appropriate device.</li>
</ul>

<p>Common attributes (present in all subclasses):</p>

<ul>
<li><strong>vocab_size</strong> (<code>int</code>) -- The number of tokens in the vocabulary, which is also the first dimension of the
embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).</li>
<li><strong>hidden_size</strong> (<code>int</code>) -- The hidden size of the model.</li>
<li><strong>num_attention_heads</strong> (<code>int</code>) -- The number of attention heads used in the multi-head attention layers of the
model.</li>
<li><strong>num_hidden_layers</strong> (<code>int</code>) -- The number of blocks in the model.</li>
</ul>

<p><Tip warning={true}&gt;</p>

<p>Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading
some of them will still be possible, but attempting to overwrite them will throw an exception -- you should set
them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more
information about the individual parameters.</p>

<p></Tip></p>

<p>Arg:
    name_or_path (<code>str</code>, <em>optional</em>, defaults to <code>""</code>):
        Store the string that was passed to [<code>PreTrainedModel.from_pretrained</code>] or
        [<code>TFPreTrainedModel.from_pretrained</code>] as <code>pretrained_model_name_or_path</code> if the configuration was created
        with such a method.
    output_hidden_states (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether or not the model should return all hidden-states.
    output_attentions (<code>bool</code>, *optional*, defaults to <code>False</code>):
        Whether or not the model should returns all attentions.
    return_dict (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
        Whether or not the model should return a [<code>~transformers.utils.ModelOutput</code>] instead of a plain tuple.
    is_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether the model is used as an encoder/decoder or not.
    is_decoder (<code>bool</code>, *optional*, defaults to <code>False</code>):
        Whether to only use the decoder in an encoder-decoder architecture, otherwise it has no effect on
        decoder-only or encoder-only architectures.
    cross_attention_hidden_size (<code>bool</code>, <em>optional</em>):
        The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder
        setting and the cross-attention hidden dimension differs from <code>self.config.hidden_size</code>.
    add_cross_attention (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether cross-attention layers should be added to the model. Note, this option is only relevant for models
        that can be used as decoder models within the [<code>EncoderDecoderModel</code>] class, which consists of all models
        in <code>AUTO_MODELS_FOR_CAUSAL_LM</code>.
    tie_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder
        and decoder model to have the exact same parameter names.
    prune_heads (<code>dict[int, list[int]]</code>, <em>optional</em>, defaults to <code>{}</code>):
        Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of
        heads to prune in said layer.</p>

<pre><code>    For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.
chunk_size_feed_forward (`int`, *optional*, defaults to `0`):
    The chunk size of all feed forward layers in the residual attention blocks. A chunk size of `0` means that
    the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes `n` &lt;
    sequence_length embeddings at a time. For more information on feed forward chunking, see [How does Feed
    Forward Chunking work?](../glossary.html#feed-forward-chunking).

&gt; Parameters for fine-tuning tasks

architectures (`list[str]`, *optional*):
    Model architectures that can be used with the model pretrained weights.
finetuning_task (`str`, *optional*):
    Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow
    or PyTorch) checkpoint.
id2label (`dict[int, str]`, *optional*):
    A map from index (for instance prediction index, or target index) to label.
label2id (`dict[str, int]`, *optional*):
    A map from label to index for the model.
num_labels (`int`, *optional*):
    Number of labels to use in the last layer added to the model, typically for a classification task.
task_specific_params (`dict[str, Any]`, *optional*):
    Additional keyword arguments to store for the current task.
problem_type (`str`, *optional*):
    Problem type for `XxxForSequenceClassification` models. Can be one of `"regression"`,
    `"single_label_classification"` or `"multi_label_classification"`.

&gt; Parameters linked to the tokenizer

tokenizer_class (`str`, *optional*):
    The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the
    model by default).
prefix (`str`, *optional*):
    A specific prompt that should be added at the beginning of each text before calling the model.
bos_token_id (`int`, *optional*):
    The id of the _beginning-of-stream_ token.
pad_token_id (`int`, *optional*):
    The id of the _padding_ token.
eos_token_id (`int`, *optional*):
    The id of the _end-of-stream_ token.
decoder_start_token_id (`int`, *optional*):
    If an encoder-decoder model starts decoding with a different token than _bos_, the id of that token.
sep_token_id (`int`, *optional*):
    The id of the _separation_ token.

&gt; PyTorch specific parameters

torchscript (`bool`, *optional*, defaults to `False`):
    Whether or not the model should be used with Torchscript.
tie_word_embeddings (`bool`, *optional*, defaults to `True`):
    Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the
    model has a output word embedding layer.
torch_dtype (`str`, *optional*):
    The `dtype` of the weights. This attribute can be used to initialize the model to a non-default `dtype`
    (which is normally `float32`) and thus allow for optimal storage allocation. For example, if the saved
    model is `float16`, ideally we want to load it back using the minimal amount of memory needed to load
    `float16` weights. Since the config object is stored in plain text, this attribute contains just the
    floating type string without the `torch.` prefix. For example, for `torch.float16` ``torch_dtype` is the
    `"float16"` string.

    This attribute is currently not being used during model loading time, but this may change in the future
    versions. But we can already start preparing for the future by saving the dtype with save_pretrained.
</code></pre>
</div>


                            <div id="DeepseekVL2Config.__init__" class="classattr">
                                        <input id="DeepseekVL2Config.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">DeepseekVL2Config</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">tile_tag</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;tile_tag&#39;</span>,</span><span class="param">	<span class="n">global_view_pos</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;head&#39;</span>,</span><span class="param">	<span class="n">candidate_resolutions</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">((</span><span class="mi">384</span><span class="p">,</span> <span class="mi">384</span><span class="p">),)</span>,</span><span class="param">	<span class="o">**</span><span class="n">kwargs</span></span>)</span>

                <label class="view-source-button" for="DeepseekVL2Config.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#DeepseekVL2Config.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="DeepseekVL2Config.__init__-662"><a href="#DeepseekVL2Config.__init__-662"><span class="linenos">662</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="DeepseekVL2Config.__init__-663"><a href="#DeepseekVL2Config.__init__-663"><span class="linenos">663</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="DeepseekVL2Config.__init__-664"><a href="#DeepseekVL2Config.__init__-664"><span class="linenos">664</span></a>        <span class="n">tile_tag</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;tile_tag&quot;</span><span class="p">,</span>
</span><span id="DeepseekVL2Config.__init__-665"><a href="#DeepseekVL2Config.__init__-665"><span class="linenos">665</span></a>        <span class="n">global_view_pos</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;head&quot;</span><span class="p">,</span>
</span><span id="DeepseekVL2Config.__init__-666"><a href="#DeepseekVL2Config.__init__-666"><span class="linenos">666</span></a>        <span class="n">candidate_resolutions</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">((</span><span class="mi">384</span><span class="p">,</span> <span class="mi">384</span><span class="p">),),</span>
</span><span id="DeepseekVL2Config.__init__-667"><a href="#DeepseekVL2Config.__init__-667"><span class="linenos">667</span></a>        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span id="DeepseekVL2Config.__init__-668"><a href="#DeepseekVL2Config.__init__-668"><span class="linenos">668</span></a>    <span class="p">):</span>
</span><span id="DeepseekVL2Config.__init__-669"><a href="#DeepseekVL2Config.__init__-669"><span class="linenos">669</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="DeepseekVL2Config.__init__-670"><a href="#DeepseekVL2Config.__init__-670"><span class="linenos">670</span></a>
</span><span id="DeepseekVL2Config.__init__-671"><a href="#DeepseekVL2Config.__init__-671"><span class="linenos">671</span></a>        <span class="n">vision_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;vision_config&quot;</span><span class="p">,</span> <span class="p">{})</span>
</span><span id="DeepseekVL2Config.__init__-672"><a href="#DeepseekVL2Config.__init__-672"><span class="linenos">672</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vision_config</span> <span class="o">=</span> <span class="n">DeepseekVL2VisionEncoderConfig</span><span class="p">(</span><span class="o">**</span><span class="n">vision_config</span><span class="p">)</span>
</span><span id="DeepseekVL2Config.__init__-673"><a href="#DeepseekVL2Config.__init__-673"><span class="linenos">673</span></a>
</span><span id="DeepseekVL2Config.__init__-674"><a href="#DeepseekVL2Config.__init__-674"><span class="linenos">674</span></a>        <span class="n">projector_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;projector_config&quot;</span><span class="p">,</span> <span class="p">{})</span>
</span><span id="DeepseekVL2Config.__init__-675"><a href="#DeepseekVL2Config.__init__-675"><span class="linenos">675</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">projector_config</span> <span class="o">=</span> <span class="n">DeepseekVL2MlpProjectorConfig</span><span class="p">(</span><span class="o">**</span><span class="n">projector_config</span><span class="p">)</span>
</span><span id="DeepseekVL2Config.__init__-676"><a href="#DeepseekVL2Config.__init__-676"><span class="linenos">676</span></a>
</span><span id="DeepseekVL2Config.__init__-677"><a href="#DeepseekVL2Config.__init__-677"><span class="linenos">677</span></a>        <span class="n">language_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;language_config&quot;</span><span class="p">,</span> <span class="p">{})</span>
</span><span id="DeepseekVL2Config.__init__-678"><a href="#DeepseekVL2Config.__init__-678"><span class="linenos">678</span></a>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">language_config</span><span class="p">,</span> <span class="n">DeepseekV2Config</span><span class="p">):</span>
</span><span id="DeepseekVL2Config.__init__-679"><a href="#DeepseekVL2Config.__init__-679"><span class="linenos">679</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">language_config</span> <span class="o">=</span> <span class="n">language_config</span>
</span><span id="DeepseekVL2Config.__init__-680"><a href="#DeepseekVL2Config.__init__-680"><span class="linenos">680</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="DeepseekVL2Config.__init__-681"><a href="#DeepseekVL2Config.__init__-681"><span class="linenos">681</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">language_config</span> <span class="o">=</span> <span class="n">DeepseekV2Config</span><span class="p">(</span><span class="o">**</span><span class="n">language_config</span><span class="p">)</span>
</span><span id="DeepseekVL2Config.__init__-682"><a href="#DeepseekVL2Config.__init__-682"><span class="linenos">682</span></a>
</span><span id="DeepseekVL2Config.__init__-683"><a href="#DeepseekVL2Config.__init__-683"><span class="linenos">683</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">tile_tag</span> <span class="o">=</span> <span class="n">tile_tag</span>
</span><span id="DeepseekVL2Config.__init__-684"><a href="#DeepseekVL2Config.__init__-684"><span class="linenos">684</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">global_view_pos</span> <span class="o">=</span> <span class="n">global_view_pos</span>
</span><span id="DeepseekVL2Config.__init__-685"><a href="#DeepseekVL2Config.__init__-685"><span class="linenos">685</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">candidate_resolutions</span> <span class="o">=</span> <span class="n">candidate_resolutions</span>
</span><span id="DeepseekVL2Config.__init__-686"><a href="#DeepseekVL2Config.__init__-686"><span class="linenos">686</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">architectures</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;DeepseekVL2ForCausalLM&quot;</span><span class="p">]</span>
</span></pre></div>


    

                            </div>
                            <div id="DeepseekVL2Config.model_type" class="classattr">
                                <div class="attr variable">
            <span class="name">model_type</span>        =
<span class="default_value">&#39;deepseek_vl_v2&#39;</span>

        
    </div>
    <a class="headerlink" href="#DeepseekVL2Config.model_type"></a>
    
    

                            </div>
                            <div id="DeepseekVL2Config.vision_config" class="classattr">
                                <div class="attr variable">
            <span class="name">vision_config</span><span class="annotation">: sglang.srt.configs.deepseekvl2.DeepseekVL2VisionEncoderConfig</span>

        
    </div>
    <a class="headerlink" href="#DeepseekVL2Config.vision_config"></a>
    
    

                            </div>
                            <div id="DeepseekVL2Config.projector_config" class="classattr">
                                <div class="attr variable">
            <span class="name">projector_config</span><span class="annotation">: sglang.srt.configs.deepseekvl2.DeepseekVL2MlpProjectorConfig</span>

        
    </div>
    <a class="headerlink" href="#DeepseekVL2Config.projector_config"></a>
    
    

                            </div>
                            <div id="DeepseekVL2Config.language_config" class="classattr">
                                <div class="attr variable">
            <span class="name">language_config</span><span class="annotation">: sglang.srt.configs.deepseekvl2.DeepseekV2Config</span>

        
    </div>
    <a class="headerlink" href="#DeepseekVL2Config.language_config"></a>
    
    

                            </div>
                            <div id="DeepseekVL2Config.tile_tag" class="classattr">
                                <div class="attr variable">
            <span class="name">tile_tag</span><span class="annotation">: str</span>        =
<span class="default_value">&#39;2D&#39;</span>

        
    </div>
    <a class="headerlink" href="#DeepseekVL2Config.tile_tag"></a>
    
    

                            </div>
                            <div id="DeepseekVL2Config.global_view_pos" class="classattr">
                                <div class="attr variable">
            <span class="name">global_view_pos</span><span class="annotation">: str</span>        =
<span class="default_value">&#39;head&#39;</span>

        
    </div>
    <a class="headerlink" href="#DeepseekVL2Config.global_view_pos"></a>
    
    

                            </div>
                            <div id="DeepseekVL2Config.candidate_resolutions" class="classattr">
                                <div class="attr variable">
            <span class="name">candidate_resolutions</span><span class="annotation">: Tuple[Tuple[int, int]]</span>        =
<span class="default_value">((384, 384),)</span>

        
    </div>
    <a class="headerlink" href="#DeepseekVL2Config.candidate_resolutions"></a>
    
    

                            </div>
                            <div id="DeepseekVL2Config.architectures" class="classattr">
                                <div class="attr variable">
            <span class="name">architectures</span>

        
    </div>
    <a class="headerlink" href="#DeepseekVL2Config.architectures"></a>
    
    

                            </div>
                </section>
                <section id="MultiModalityConfig">
                            <input id="MultiModalityConfig-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">MultiModalityConfig</span><wbr>(<span class="base">transformers.configuration_utils.PretrainedConfig</span>):

                <label class="view-source-button" for="MultiModalityConfig-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiModalityConfig"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiModalityConfig-125"><a href="#MultiModalityConfig-125"><span class="linenos">125</span></a><span class="k">class</span><span class="w"> </span><span class="nc">MultiModalityConfig</span><span class="p">(</span><span class="n">PretrainedConfig</span><span class="p">):</span>
</span><span id="MultiModalityConfig-126"><a href="#MultiModalityConfig-126"><span class="linenos">126</span></a>    <span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;multi_modality&quot;</span>
</span><span id="MultiModalityConfig-127"><a href="#MultiModalityConfig-127"><span class="linenos">127</span></a>    <span class="n">vision_config</span><span class="p">:</span> <span class="n">VisionConfig</span>
</span><span id="MultiModalityConfig-128"><a href="#MultiModalityConfig-128"><span class="linenos">128</span></a>    <span class="n">aligner_config</span><span class="p">:</span> <span class="n">AlignerConfig</span>
</span><span id="MultiModalityConfig-129"><a href="#MultiModalityConfig-129"><span class="linenos">129</span></a>
</span><span id="MultiModalityConfig-130"><a href="#MultiModalityConfig-130"><span class="linenos">130</span></a>    <span class="n">gen_vision_config</span><span class="p">:</span> <span class="n">GenVisionConfig</span>
</span><span id="MultiModalityConfig-131"><a href="#MultiModalityConfig-131"><span class="linenos">131</span></a>    <span class="n">gen_aligner_config</span><span class="p">:</span> <span class="n">GenAlignerConfig</span>
</span><span id="MultiModalityConfig-132"><a href="#MultiModalityConfig-132"><span class="linenos">132</span></a>    <span class="n">gen_head_config</span><span class="p">:</span> <span class="n">GenHeadConfig</span>
</span><span id="MultiModalityConfig-133"><a href="#MultiModalityConfig-133"><span class="linenos">133</span></a>
</span><span id="MultiModalityConfig-134"><a href="#MultiModalityConfig-134"><span class="linenos">134</span></a>    <span class="n">language_config</span><span class="p">:</span> <span class="n">LlamaConfig</span>
</span><span id="MultiModalityConfig-135"><a href="#MultiModalityConfig-135"><span class="linenos">135</span></a>
</span><span id="MultiModalityConfig-136"><a href="#MultiModalityConfig-136"><span class="linenos">136</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span id="MultiModalityConfig-137"><a href="#MultiModalityConfig-137"><span class="linenos">137</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="MultiModalityConfig-138"><a href="#MultiModalityConfig-138"><span class="linenos">138</span></a>        <span class="n">vision_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;vision_config&quot;</span><span class="p">,</span> <span class="p">{})</span>
</span><span id="MultiModalityConfig-139"><a href="#MultiModalityConfig-139"><span class="linenos">139</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vision_config</span> <span class="o">=</span> <span class="n">VisionConfig</span><span class="p">(</span><span class="o">**</span><span class="n">vision_config</span><span class="p">)</span>
</span><span id="MultiModalityConfig-140"><a href="#MultiModalityConfig-140"><span class="linenos">140</span></a>
</span><span id="MultiModalityConfig-141"><a href="#MultiModalityConfig-141"><span class="linenos">141</span></a>        <span class="n">aligner_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;aligner_config&quot;</span><span class="p">,</span> <span class="p">{})</span>
</span><span id="MultiModalityConfig-142"><a href="#MultiModalityConfig-142"><span class="linenos">142</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">aligner_config</span> <span class="o">=</span> <span class="n">AlignerConfig</span><span class="p">(</span><span class="o">**</span><span class="n">aligner_config</span><span class="p">)</span>
</span><span id="MultiModalityConfig-143"><a href="#MultiModalityConfig-143"><span class="linenos">143</span></a>
</span><span id="MultiModalityConfig-144"><a href="#MultiModalityConfig-144"><span class="linenos">144</span></a>        <span class="n">gen_vision_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;gen_vision_config&quot;</span><span class="p">,</span> <span class="p">{})</span>
</span><span id="MultiModalityConfig-145"><a href="#MultiModalityConfig-145"><span class="linenos">145</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gen_vision_config</span> <span class="o">=</span> <span class="n">GenVisionConfig</span><span class="p">(</span><span class="o">**</span><span class="n">gen_vision_config</span><span class="p">)</span>
</span><span id="MultiModalityConfig-146"><a href="#MultiModalityConfig-146"><span class="linenos">146</span></a>
</span><span id="MultiModalityConfig-147"><a href="#MultiModalityConfig-147"><span class="linenos">147</span></a>        <span class="n">gen_aligner_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;gen_aligner_config&quot;</span><span class="p">,</span> <span class="p">{})</span>
</span><span id="MultiModalityConfig-148"><a href="#MultiModalityConfig-148"><span class="linenos">148</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gen_aligner_config</span> <span class="o">=</span> <span class="n">GenAlignerConfig</span><span class="p">(</span><span class="o">**</span><span class="n">gen_aligner_config</span><span class="p">)</span>
</span><span id="MultiModalityConfig-149"><a href="#MultiModalityConfig-149"><span class="linenos">149</span></a>
</span><span id="MultiModalityConfig-150"><a href="#MultiModalityConfig-150"><span class="linenos">150</span></a>        <span class="n">gen_head_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;gen_head_config&quot;</span><span class="p">,</span> <span class="p">{})</span>
</span><span id="MultiModalityConfig-151"><a href="#MultiModalityConfig-151"><span class="linenos">151</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gen_head_config</span> <span class="o">=</span> <span class="n">GenHeadConfig</span><span class="p">(</span><span class="o">**</span><span class="n">gen_head_config</span><span class="p">)</span>
</span><span id="MultiModalityConfig-152"><a href="#MultiModalityConfig-152"><span class="linenos">152</span></a>
</span><span id="MultiModalityConfig-153"><a href="#MultiModalityConfig-153"><span class="linenos">153</span></a>        <span class="n">language_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;language_config&quot;</span><span class="p">,</span> <span class="p">{})</span>
</span><span id="MultiModalityConfig-154"><a href="#MultiModalityConfig-154"><span class="linenos">154</span></a>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">language_config</span><span class="p">,</span> <span class="n">LlamaConfig</span><span class="p">):</span>
</span><span id="MultiModalityConfig-155"><a href="#MultiModalityConfig-155"><span class="linenos">155</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">language_config</span> <span class="o">=</span> <span class="n">language_config</span>
</span><span id="MultiModalityConfig-156"><a href="#MultiModalityConfig-156"><span class="linenos">156</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="MultiModalityConfig-157"><a href="#MultiModalityConfig-157"><span class="linenos">157</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">language_config</span> <span class="o">=</span> <span class="n">LlamaConfig</span><span class="p">(</span><span class="o">**</span><span class="n">language_config</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as
methods for loading/downloading/saving configurations.</p>

<p><Tip></p>

<p>A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to
initialize a model does <strong>not</strong> load the model weights. It only affects the model's configuration.</p>

<p></Tip></p>

<p>Class attributes (overridden by derived classes):</p>

<ul>
<li><strong>model_type</strong> (<code>str</code>) -- An identifier for the model type, serialized into the JSON file, and used to recreate
the correct object in [<code>~transformers.AutoConfig</code>].</li>
<li><strong>has_no_defaults_at_init</strong> (<code>bool</code>) -- Whether the config class can be initialized without providing input arguments.
Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,
(but not necessarily) such as [<code>~transformers.EncoderDecoderConfig</code>] or [<code>~RagConfig</code>]. They have to be initialized from
two or more configs of type [<code>~transformers.PretrainedConfig</code>].</li>
<li><strong>keys_to_ignore_at_inference</strong> (<code>list[str]</code>) -- A list of keys to ignore by default when looking at dictionary
outputs of the model during inference.</li>
<li><strong>attribute_map</strong> (<code>dict[str, str]</code>) -- A dict that maps model specific attribute names to the standardized
naming of attributes.</li>
<li><strong>base_model_tp_plan</strong> (<code>dict[str, Any]</code>) -- A dict that maps sub-modules FQNs of a base model to a tensor
parallel plan applied to the sub-module when <code>model.tensor_parallel</code> is called.</li>
<li><strong>base_model_pp_plan</strong> (<code>dict[str, tuple[list[str]]]</code>) -- A dict that maps child-modules of a base model to a
pipeline parallel plan that enables users to place the child-module on the appropriate device.</li>
</ul>

<p>Common attributes (present in all subclasses):</p>

<ul>
<li><strong>vocab_size</strong> (<code>int</code>) -- The number of tokens in the vocabulary, which is also the first dimension of the
embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).</li>
<li><strong>hidden_size</strong> (<code>int</code>) -- The hidden size of the model.</li>
<li><strong>num_attention_heads</strong> (<code>int</code>) -- The number of attention heads used in the multi-head attention layers of the
model.</li>
<li><strong>num_hidden_layers</strong> (<code>int</code>) -- The number of blocks in the model.</li>
</ul>

<p><Tip warning={true}&gt;</p>

<p>Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading
some of them will still be possible, but attempting to overwrite them will throw an exception -- you should set
them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more
information about the individual parameters.</p>

<p></Tip></p>

<p>Arg:
    name_or_path (<code>str</code>, <em>optional</em>, defaults to <code>""</code>):
        Store the string that was passed to [<code>PreTrainedModel.from_pretrained</code>] or
        [<code>TFPreTrainedModel.from_pretrained</code>] as <code>pretrained_model_name_or_path</code> if the configuration was created
        with such a method.
    output_hidden_states (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether or not the model should return all hidden-states.
    output_attentions (<code>bool</code>, *optional*, defaults to <code>False</code>):
        Whether or not the model should returns all attentions.
    return_dict (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
        Whether or not the model should return a [<code>~transformers.utils.ModelOutput</code>] instead of a plain tuple.
    is_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether the model is used as an encoder/decoder or not.
    is_decoder (<code>bool</code>, *optional*, defaults to <code>False</code>):
        Whether to only use the decoder in an encoder-decoder architecture, otherwise it has no effect on
        decoder-only or encoder-only architectures.
    cross_attention_hidden_size (<code>bool</code>, <em>optional</em>):
        The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder
        setting and the cross-attention hidden dimension differs from <code>self.config.hidden_size</code>.
    add_cross_attention (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether cross-attention layers should be added to the model. Note, this option is only relevant for models
        that can be used as decoder models within the [<code>EncoderDecoderModel</code>] class, which consists of all models
        in <code>AUTO_MODELS_FOR_CAUSAL_LM</code>.
    tie_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder
        and decoder model to have the exact same parameter names.
    prune_heads (<code>dict[int, list[int]]</code>, <em>optional</em>, defaults to <code>{}</code>):
        Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of
        heads to prune in said layer.</p>

<pre><code>    For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.
chunk_size_feed_forward (`int`, *optional*, defaults to `0`):
    The chunk size of all feed forward layers in the residual attention blocks. A chunk size of `0` means that
    the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes `n` &lt;
    sequence_length embeddings at a time. For more information on feed forward chunking, see [How does Feed
    Forward Chunking work?](../glossary.html#feed-forward-chunking).

&gt; Parameters for fine-tuning tasks

architectures (`list[str]`, *optional*):
    Model architectures that can be used with the model pretrained weights.
finetuning_task (`str`, *optional*):
    Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow
    or PyTorch) checkpoint.
id2label (`dict[int, str]`, *optional*):
    A map from index (for instance prediction index, or target index) to label.
label2id (`dict[str, int]`, *optional*):
    A map from label to index for the model.
num_labels (`int`, *optional*):
    Number of labels to use in the last layer added to the model, typically for a classification task.
task_specific_params (`dict[str, Any]`, *optional*):
    Additional keyword arguments to store for the current task.
problem_type (`str`, *optional*):
    Problem type for `XxxForSequenceClassification` models. Can be one of `"regression"`,
    `"single_label_classification"` or `"multi_label_classification"`.

&gt; Parameters linked to the tokenizer

tokenizer_class (`str`, *optional*):
    The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the
    model by default).
prefix (`str`, *optional*):
    A specific prompt that should be added at the beginning of each text before calling the model.
bos_token_id (`int`, *optional*):
    The id of the _beginning-of-stream_ token.
pad_token_id (`int`, *optional*):
    The id of the _padding_ token.
eos_token_id (`int`, *optional*):
    The id of the _end-of-stream_ token.
decoder_start_token_id (`int`, *optional*):
    If an encoder-decoder model starts decoding with a different token than _bos_, the id of that token.
sep_token_id (`int`, *optional*):
    The id of the _separation_ token.

&gt; PyTorch specific parameters

torchscript (`bool`, *optional*, defaults to `False`):
    Whether or not the model should be used with Torchscript.
tie_word_embeddings (`bool`, *optional*, defaults to `True`):
    Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the
    model has a output word embedding layer.
torch_dtype (`str`, *optional*):
    The `dtype` of the weights. This attribute can be used to initialize the model to a non-default `dtype`
    (which is normally `float32`) and thus allow for optimal storage allocation. For example, if the saved
    model is `float16`, ideally we want to load it back using the minimal amount of memory needed to load
    `float16` weights. Since the config object is stored in plain text, this attribute contains just the
    floating type string without the `torch.` prefix. For example, for `torch.float16` ``torch_dtype` is the
    `"float16"` string.

    This attribute is currently not being used during model loading time, but this may change in the future
    versions. But we can already start preparing for the future by saving the dtype with save_pretrained.
</code></pre>
</div>


                            <div id="MultiModalityConfig.__init__" class="classattr">
                                        <input id="MultiModalityConfig.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">MultiModalityConfig</span><span class="signature pdoc-code condensed">(<span class="param"><span class="o">**</span><span class="n">kwargs</span></span>)</span>

                <label class="view-source-button" for="MultiModalityConfig.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MultiModalityConfig.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MultiModalityConfig.__init__-136"><a href="#MultiModalityConfig.__init__-136"><span class="linenos">136</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span id="MultiModalityConfig.__init__-137"><a href="#MultiModalityConfig.__init__-137"><span class="linenos">137</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="MultiModalityConfig.__init__-138"><a href="#MultiModalityConfig.__init__-138"><span class="linenos">138</span></a>        <span class="n">vision_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;vision_config&quot;</span><span class="p">,</span> <span class="p">{})</span>
</span><span id="MultiModalityConfig.__init__-139"><a href="#MultiModalityConfig.__init__-139"><span class="linenos">139</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vision_config</span> <span class="o">=</span> <span class="n">VisionConfig</span><span class="p">(</span><span class="o">**</span><span class="n">vision_config</span><span class="p">)</span>
</span><span id="MultiModalityConfig.__init__-140"><a href="#MultiModalityConfig.__init__-140"><span class="linenos">140</span></a>
</span><span id="MultiModalityConfig.__init__-141"><a href="#MultiModalityConfig.__init__-141"><span class="linenos">141</span></a>        <span class="n">aligner_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;aligner_config&quot;</span><span class="p">,</span> <span class="p">{})</span>
</span><span id="MultiModalityConfig.__init__-142"><a href="#MultiModalityConfig.__init__-142"><span class="linenos">142</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">aligner_config</span> <span class="o">=</span> <span class="n">AlignerConfig</span><span class="p">(</span><span class="o">**</span><span class="n">aligner_config</span><span class="p">)</span>
</span><span id="MultiModalityConfig.__init__-143"><a href="#MultiModalityConfig.__init__-143"><span class="linenos">143</span></a>
</span><span id="MultiModalityConfig.__init__-144"><a href="#MultiModalityConfig.__init__-144"><span class="linenos">144</span></a>        <span class="n">gen_vision_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;gen_vision_config&quot;</span><span class="p">,</span> <span class="p">{})</span>
</span><span id="MultiModalityConfig.__init__-145"><a href="#MultiModalityConfig.__init__-145"><span class="linenos">145</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gen_vision_config</span> <span class="o">=</span> <span class="n">GenVisionConfig</span><span class="p">(</span><span class="o">**</span><span class="n">gen_vision_config</span><span class="p">)</span>
</span><span id="MultiModalityConfig.__init__-146"><a href="#MultiModalityConfig.__init__-146"><span class="linenos">146</span></a>
</span><span id="MultiModalityConfig.__init__-147"><a href="#MultiModalityConfig.__init__-147"><span class="linenos">147</span></a>        <span class="n">gen_aligner_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;gen_aligner_config&quot;</span><span class="p">,</span> <span class="p">{})</span>
</span><span id="MultiModalityConfig.__init__-148"><a href="#MultiModalityConfig.__init__-148"><span class="linenos">148</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gen_aligner_config</span> <span class="o">=</span> <span class="n">GenAlignerConfig</span><span class="p">(</span><span class="o">**</span><span class="n">gen_aligner_config</span><span class="p">)</span>
</span><span id="MultiModalityConfig.__init__-149"><a href="#MultiModalityConfig.__init__-149"><span class="linenos">149</span></a>
</span><span id="MultiModalityConfig.__init__-150"><a href="#MultiModalityConfig.__init__-150"><span class="linenos">150</span></a>        <span class="n">gen_head_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;gen_head_config&quot;</span><span class="p">,</span> <span class="p">{})</span>
</span><span id="MultiModalityConfig.__init__-151"><a href="#MultiModalityConfig.__init__-151"><span class="linenos">151</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">gen_head_config</span> <span class="o">=</span> <span class="n">GenHeadConfig</span><span class="p">(</span><span class="o">**</span><span class="n">gen_head_config</span><span class="p">)</span>
</span><span id="MultiModalityConfig.__init__-152"><a href="#MultiModalityConfig.__init__-152"><span class="linenos">152</span></a>
</span><span id="MultiModalityConfig.__init__-153"><a href="#MultiModalityConfig.__init__-153"><span class="linenos">153</span></a>        <span class="n">language_config</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;language_config&quot;</span><span class="p">,</span> <span class="p">{})</span>
</span><span id="MultiModalityConfig.__init__-154"><a href="#MultiModalityConfig.__init__-154"><span class="linenos">154</span></a>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">language_config</span><span class="p">,</span> <span class="n">LlamaConfig</span><span class="p">):</span>
</span><span id="MultiModalityConfig.__init__-155"><a href="#MultiModalityConfig.__init__-155"><span class="linenos">155</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">language_config</span> <span class="o">=</span> <span class="n">language_config</span>
</span><span id="MultiModalityConfig.__init__-156"><a href="#MultiModalityConfig.__init__-156"><span class="linenos">156</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="MultiModalityConfig.__init__-157"><a href="#MultiModalityConfig.__init__-157"><span class="linenos">157</span></a>            <span class="bp">self</span><span class="o">.</span><span class="n">language_config</span> <span class="o">=</span> <span class="n">LlamaConfig</span><span class="p">(</span><span class="o">**</span><span class="n">language_config</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                            <div id="MultiModalityConfig.model_type" class="classattr">
                                <div class="attr variable">
            <span class="name">model_type</span>        =
<span class="default_value">&#39;multi_modality&#39;</span>

        
    </div>
    <a class="headerlink" href="#MultiModalityConfig.model_type"></a>
    
    

                            </div>
                            <div id="MultiModalityConfig.vision_config" class="classattr">
                                <div class="attr variable">
            <span class="name">vision_config</span><span class="annotation">: sglang.srt.configs.janus_pro.VisionConfig</span>

        
    </div>
    <a class="headerlink" href="#MultiModalityConfig.vision_config"></a>
    
    

                            </div>
                            <div id="MultiModalityConfig.aligner_config" class="classattr">
                                <div class="attr variable">
            <span class="name">aligner_config</span><span class="annotation">: sglang.srt.configs.janus_pro.AlignerConfig</span>

        
    </div>
    <a class="headerlink" href="#MultiModalityConfig.aligner_config"></a>
    
    

                            </div>
                            <div id="MultiModalityConfig.gen_vision_config" class="classattr">
                                <div class="attr variable">
            <span class="name">gen_vision_config</span><span class="annotation">: sglang.srt.configs.janus_pro.GenVisionConfig</span>

        
    </div>
    <a class="headerlink" href="#MultiModalityConfig.gen_vision_config"></a>
    
    

                            </div>
                            <div id="MultiModalityConfig.gen_aligner_config" class="classattr">
                                <div class="attr variable">
            <span class="name">gen_aligner_config</span><span class="annotation">: sglang.srt.configs.janus_pro.GenAlignerConfig</span>

        
    </div>
    <a class="headerlink" href="#MultiModalityConfig.gen_aligner_config"></a>
    
    

                            </div>
                            <div id="MultiModalityConfig.gen_head_config" class="classattr">
                                <div class="attr variable">
            <span class="name">gen_head_config</span><span class="annotation">: sglang.srt.configs.janus_pro.GenHeadConfig</span>

        
    </div>
    <a class="headerlink" href="#MultiModalityConfig.gen_head_config"></a>
    
    

                            </div>
                            <div id="MultiModalityConfig.language_config" class="classattr">
                                <div class="attr variable">
            <span class="name">language_config</span><span class="annotation">: transformers.models.llama.configuration_llama.LlamaConfig</span>

        
    </div>
    <a class="headerlink" href="#MultiModalityConfig.language_config"></a>
    
    

                            </div>
                </section>
                <section id="KimiVLConfig">
                            <input id="KimiVLConfig-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">KimiVLConfig</span><wbr>(<span class="base">transformers.configuration_utils.PretrainedConfig</span>):

                <label class="view-source-button" for="KimiVLConfig-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#KimiVLConfig"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="KimiVLConfig-12"><a href="#KimiVLConfig-12"><span class="linenos">12</span></a><span class="k">class</span><span class="w"> </span><span class="nc">KimiVLConfig</span><span class="p">(</span><span class="n">PretrainedConfig</span><span class="p">):</span>
</span><span id="KimiVLConfig-13"><a href="#KimiVLConfig-13"><span class="linenos">13</span></a>    <span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;kimi_vl&quot;</span>
</span><span id="KimiVLConfig-14"><a href="#KimiVLConfig-14"><span class="linenos">14</span></a>
</span><span id="KimiVLConfig-15"><a href="#KimiVLConfig-15"><span class="linenos">15</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="KimiVLConfig-16"><a href="#KimiVLConfig-16"><span class="linenos">16</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="KimiVLConfig-17"><a href="#KimiVLConfig-17"><span class="linenos">17</span></a>        <span class="n">vision_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">MoonViTConfig</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="KimiVLConfig-18"><a href="#KimiVLConfig-18"><span class="linenos">18</span></a>        <span class="n">text_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">DeepseekV2Config</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="KimiVLConfig-19"><a href="#KimiVLConfig-19"><span class="linenos">19</span></a>        <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span>
</span><span id="KimiVLConfig-20"><a href="#KimiVLConfig-20"><span class="linenos">20</span></a>        <span class="n">media_placeholder_token_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">163605</span><span class="p">,</span>
</span><span id="KimiVLConfig-21"><a href="#KimiVLConfig-21"><span class="linenos">21</span></a>        <span class="n">pad_token_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="KimiVLConfig-22"><a href="#KimiVLConfig-22"><span class="linenos">22</span></a>        <span class="o">**</span><span class="n">kwargs</span>
</span><span id="KimiVLConfig-23"><a href="#KimiVLConfig-23"><span class="linenos">23</span></a>    <span class="p">):</span>
</span><span id="KimiVLConfig-24"><a href="#KimiVLConfig-24"><span class="linenos">24</span></a>        <span class="k">if</span> <span class="n">vision_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="KimiVLConfig-25"><a href="#KimiVLConfig-25"><span class="linenos">25</span></a>            <span class="n">vision_config</span> <span class="o">=</span> <span class="n">MoonViTConfig</span><span class="p">()</span>
</span><span id="KimiVLConfig-26"><a href="#KimiVLConfig-26"><span class="linenos">26</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vision_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
</span><span id="KimiVLConfig-27"><a href="#KimiVLConfig-27"><span class="linenos">27</span></a>            <span class="n">vision_config</span> <span class="o">=</span> <span class="n">MoonViTConfig</span><span class="p">(</span><span class="o">**</span><span class="n">vision_config</span><span class="p">)</span>
</span><span id="KimiVLConfig-28"><a href="#KimiVLConfig-28"><span class="linenos">28</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vision_config</span> <span class="o">=</span> <span class="n">vision_config</span>
</span><span id="KimiVLConfig-29"><a href="#KimiVLConfig-29"><span class="linenos">29</span></a>
</span><span id="KimiVLConfig-30"><a href="#KimiVLConfig-30"><span class="linenos">30</span></a>        <span class="k">if</span> <span class="n">text_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="KimiVLConfig-31"><a href="#KimiVLConfig-31"><span class="linenos">31</span></a>            <span class="n">text_config</span> <span class="o">=</span> <span class="n">DeepseekV2Config</span><span class="p">()</span>
</span><span id="KimiVLConfig-32"><a href="#KimiVLConfig-32"><span class="linenos">32</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
</span><span id="KimiVLConfig-33"><a href="#KimiVLConfig-33"><span class="linenos">33</span></a>            <span class="n">text_config</span> <span class="o">=</span> <span class="n">DeepseekV2Config</span><span class="p">(</span><span class="o">**</span><span class="n">text_config</span><span class="p">)</span>
</span><span id="KimiVLConfig-34"><a href="#KimiVLConfig-34"><span class="linenos">34</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">text_config</span> <span class="o">=</span> <span class="n">text_config</span>
</span><span id="KimiVLConfig-35"><a href="#KimiVLConfig-35"><span class="linenos">35</span></a>
</span><span id="KimiVLConfig-36"><a href="#KimiVLConfig-36"><span class="linenos">36</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span> <span class="o">=</span> <span class="n">ignore_index</span>
</span><span id="KimiVLConfig-37"><a href="#KimiVLConfig-37"><span class="linenos">37</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">media_placeholder_token_id</span> <span class="o">=</span> <span class="n">media_placeholder_token_id</span>
</span><span id="KimiVLConfig-38"><a href="#KimiVLConfig-38"><span class="linenos">38</span></a>
</span><span id="KimiVLConfig-39"><a href="#KimiVLConfig-39"><span class="linenos">39</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as
methods for loading/downloading/saving configurations.</p>

<p><Tip></p>

<p>A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to
initialize a model does <strong>not</strong> load the model weights. It only affects the model's configuration.</p>

<p></Tip></p>

<p>Class attributes (overridden by derived classes):</p>

<ul>
<li><strong>model_type</strong> (<code>str</code>) -- An identifier for the model type, serialized into the JSON file, and used to recreate
the correct object in [<code>~transformers.AutoConfig</code>].</li>
<li><strong>has_no_defaults_at_init</strong> (<code>bool</code>) -- Whether the config class can be initialized without providing input arguments.
Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,
(but not necessarily) such as [<code>~transformers.EncoderDecoderConfig</code>] or [<code>~RagConfig</code>]. They have to be initialized from
two or more configs of type [<code>~transformers.PretrainedConfig</code>].</li>
<li><strong>keys_to_ignore_at_inference</strong> (<code>list[str]</code>) -- A list of keys to ignore by default when looking at dictionary
outputs of the model during inference.</li>
<li><strong>attribute_map</strong> (<code>dict[str, str]</code>) -- A dict that maps model specific attribute names to the standardized
naming of attributes.</li>
<li><strong>base_model_tp_plan</strong> (<code>dict[str, Any]</code>) -- A dict that maps sub-modules FQNs of a base model to a tensor
parallel plan applied to the sub-module when <code>model.tensor_parallel</code> is called.</li>
<li><strong>base_model_pp_plan</strong> (<code>dict[str, tuple[list[str]]]</code>) -- A dict that maps child-modules of a base model to a
pipeline parallel plan that enables users to place the child-module on the appropriate device.</li>
</ul>

<p>Common attributes (present in all subclasses):</p>

<ul>
<li><strong>vocab_size</strong> (<code>int</code>) -- The number of tokens in the vocabulary, which is also the first dimension of the
embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).</li>
<li><strong>hidden_size</strong> (<code>int</code>) -- The hidden size of the model.</li>
<li><strong>num_attention_heads</strong> (<code>int</code>) -- The number of attention heads used in the multi-head attention layers of the
model.</li>
<li><strong>num_hidden_layers</strong> (<code>int</code>) -- The number of blocks in the model.</li>
</ul>

<p><Tip warning={true}&gt;</p>

<p>Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading
some of them will still be possible, but attempting to overwrite them will throw an exception -- you should set
them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more
information about the individual parameters.</p>

<p></Tip></p>

<p>Arg:
    name_or_path (<code>str</code>, <em>optional</em>, defaults to <code>""</code>):
        Store the string that was passed to [<code>PreTrainedModel.from_pretrained</code>] or
        [<code>TFPreTrainedModel.from_pretrained</code>] as <code>pretrained_model_name_or_path</code> if the configuration was created
        with such a method.
    output_hidden_states (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether or not the model should return all hidden-states.
    output_attentions (<code>bool</code>, *optional*, defaults to <code>False</code>):
        Whether or not the model should returns all attentions.
    return_dict (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
        Whether or not the model should return a [<code>~transformers.utils.ModelOutput</code>] instead of a plain tuple.
    is_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether the model is used as an encoder/decoder or not.
    is_decoder (<code>bool</code>, *optional*, defaults to <code>False</code>):
        Whether to only use the decoder in an encoder-decoder architecture, otherwise it has no effect on
        decoder-only or encoder-only architectures.
    cross_attention_hidden_size (<code>bool</code>, <em>optional</em>):
        The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder
        setting and the cross-attention hidden dimension differs from <code>self.config.hidden_size</code>.
    add_cross_attention (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether cross-attention layers should be added to the model. Note, this option is only relevant for models
        that can be used as decoder models within the [<code>EncoderDecoderModel</code>] class, which consists of all models
        in <code>AUTO_MODELS_FOR_CAUSAL_LM</code>.
    tie_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder
        and decoder model to have the exact same parameter names.
    prune_heads (<code>dict[int, list[int]]</code>, <em>optional</em>, defaults to <code>{}</code>):
        Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of
        heads to prune in said layer.</p>

<pre><code>    For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.
chunk_size_feed_forward (`int`, *optional*, defaults to `0`):
    The chunk size of all feed forward layers in the residual attention blocks. A chunk size of `0` means that
    the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes `n` &lt;
    sequence_length embeddings at a time. For more information on feed forward chunking, see [How does Feed
    Forward Chunking work?](../glossary.html#feed-forward-chunking).

&gt; Parameters for fine-tuning tasks

architectures (`list[str]`, *optional*):
    Model architectures that can be used with the model pretrained weights.
finetuning_task (`str`, *optional*):
    Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow
    or PyTorch) checkpoint.
id2label (`dict[int, str]`, *optional*):
    A map from index (for instance prediction index, or target index) to label.
label2id (`dict[str, int]`, *optional*):
    A map from label to index for the model.
num_labels (`int`, *optional*):
    Number of labels to use in the last layer added to the model, typically for a classification task.
task_specific_params (`dict[str, Any]`, *optional*):
    Additional keyword arguments to store for the current task.
problem_type (`str`, *optional*):
    Problem type for `XxxForSequenceClassification` models. Can be one of `"regression"`,
    `"single_label_classification"` or `"multi_label_classification"`.

&gt; Parameters linked to the tokenizer

tokenizer_class (`str`, *optional*):
    The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the
    model by default).
prefix (`str`, *optional*):
    A specific prompt that should be added at the beginning of each text before calling the model.
bos_token_id (`int`, *optional*):
    The id of the _beginning-of-stream_ token.
pad_token_id (`int`, *optional*):
    The id of the _padding_ token.
eos_token_id (`int`, *optional*):
    The id of the _end-of-stream_ token.
decoder_start_token_id (`int`, *optional*):
    If an encoder-decoder model starts decoding with a different token than _bos_, the id of that token.
sep_token_id (`int`, *optional*):
    The id of the _separation_ token.

&gt; PyTorch specific parameters

torchscript (`bool`, *optional*, defaults to `False`):
    Whether or not the model should be used with Torchscript.
tie_word_embeddings (`bool`, *optional*, defaults to `True`):
    Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the
    model has a output word embedding layer.
torch_dtype (`str`, *optional*):
    The `dtype` of the weights. This attribute can be used to initialize the model to a non-default `dtype`
    (which is normally `float32`) and thus allow for optimal storage allocation. For example, if the saved
    model is `float16`, ideally we want to load it back using the minimal amount of memory needed to load
    `float16` weights. Since the config object is stored in plain text, this attribute contains just the
    floating type string without the `torch.` prefix. For example, for `torch.float16` ``torch_dtype` is the
    `"float16"` string.

    This attribute is currently not being used during model loading time, but this may change in the future
    versions. But we can already start preparing for the future by saving the dtype with save_pretrained.
</code></pre>
</div>


                            <div id="KimiVLConfig.__init__" class="classattr">
                                        <input id="KimiVLConfig.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">KimiVLConfig</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">vision_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n"><a href="#MoonViTConfig">MoonViTConfig</a></span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">text_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">sglang</span><span class="o">.</span><span class="n">srt</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">deepseekvl2</span><span class="o">.</span><span class="n">DeepseekV2Config</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span>,</span><span class="param">	<span class="n">media_placeholder_token_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">163605</span>,</span><span class="param">	<span class="n">pad_token_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>,</span><span class="param">	<span class="o">**</span><span class="n">kwargs</span></span>)</span>

                <label class="view-source-button" for="KimiVLConfig.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#KimiVLConfig.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="KimiVLConfig.__init__-15"><a href="#KimiVLConfig.__init__-15"><span class="linenos">15</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="KimiVLConfig.__init__-16"><a href="#KimiVLConfig.__init__-16"><span class="linenos">16</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="KimiVLConfig.__init__-17"><a href="#KimiVLConfig.__init__-17"><span class="linenos">17</span></a>        <span class="n">vision_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">MoonViTConfig</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="KimiVLConfig.__init__-18"><a href="#KimiVLConfig.__init__-18"><span class="linenos">18</span></a>        <span class="n">text_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">DeepseekV2Config</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="KimiVLConfig.__init__-19"><a href="#KimiVLConfig.__init__-19"><span class="linenos">19</span></a>        <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span>
</span><span id="KimiVLConfig.__init__-20"><a href="#KimiVLConfig.__init__-20"><span class="linenos">20</span></a>        <span class="n">media_placeholder_token_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">163605</span><span class="p">,</span>
</span><span id="KimiVLConfig.__init__-21"><a href="#KimiVLConfig.__init__-21"><span class="linenos">21</span></a>        <span class="n">pad_token_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="KimiVLConfig.__init__-22"><a href="#KimiVLConfig.__init__-22"><span class="linenos">22</span></a>        <span class="o">**</span><span class="n">kwargs</span>
</span><span id="KimiVLConfig.__init__-23"><a href="#KimiVLConfig.__init__-23"><span class="linenos">23</span></a>    <span class="p">):</span>
</span><span id="KimiVLConfig.__init__-24"><a href="#KimiVLConfig.__init__-24"><span class="linenos">24</span></a>        <span class="k">if</span> <span class="n">vision_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="KimiVLConfig.__init__-25"><a href="#KimiVLConfig.__init__-25"><span class="linenos">25</span></a>            <span class="n">vision_config</span> <span class="o">=</span> <span class="n">MoonViTConfig</span><span class="p">()</span>
</span><span id="KimiVLConfig.__init__-26"><a href="#KimiVLConfig.__init__-26"><span class="linenos">26</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vision_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
</span><span id="KimiVLConfig.__init__-27"><a href="#KimiVLConfig.__init__-27"><span class="linenos">27</span></a>            <span class="n">vision_config</span> <span class="o">=</span> <span class="n">MoonViTConfig</span><span class="p">(</span><span class="o">**</span><span class="n">vision_config</span><span class="p">)</span>
</span><span id="KimiVLConfig.__init__-28"><a href="#KimiVLConfig.__init__-28"><span class="linenos">28</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vision_config</span> <span class="o">=</span> <span class="n">vision_config</span>
</span><span id="KimiVLConfig.__init__-29"><a href="#KimiVLConfig.__init__-29"><span class="linenos">29</span></a>
</span><span id="KimiVLConfig.__init__-30"><a href="#KimiVLConfig.__init__-30"><span class="linenos">30</span></a>        <span class="k">if</span> <span class="n">text_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="KimiVLConfig.__init__-31"><a href="#KimiVLConfig.__init__-31"><span class="linenos">31</span></a>            <span class="n">text_config</span> <span class="o">=</span> <span class="n">DeepseekV2Config</span><span class="p">()</span>
</span><span id="KimiVLConfig.__init__-32"><a href="#KimiVLConfig.__init__-32"><span class="linenos">32</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
</span><span id="KimiVLConfig.__init__-33"><a href="#KimiVLConfig.__init__-33"><span class="linenos">33</span></a>            <span class="n">text_config</span> <span class="o">=</span> <span class="n">DeepseekV2Config</span><span class="p">(</span><span class="o">**</span><span class="n">text_config</span><span class="p">)</span>
</span><span id="KimiVLConfig.__init__-34"><a href="#KimiVLConfig.__init__-34"><span class="linenos">34</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">text_config</span> <span class="o">=</span> <span class="n">text_config</span>
</span><span id="KimiVLConfig.__init__-35"><a href="#KimiVLConfig.__init__-35"><span class="linenos">35</span></a>
</span><span id="KimiVLConfig.__init__-36"><a href="#KimiVLConfig.__init__-36"><span class="linenos">36</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span> <span class="o">=</span> <span class="n">ignore_index</span>
</span><span id="KimiVLConfig.__init__-37"><a href="#KimiVLConfig.__init__-37"><span class="linenos">37</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">media_placeholder_token_id</span> <span class="o">=</span> <span class="n">media_placeholder_token_id</span>
</span><span id="KimiVLConfig.__init__-38"><a href="#KimiVLConfig.__init__-38"><span class="linenos">38</span></a>
</span><span id="KimiVLConfig.__init__-39"><a href="#KimiVLConfig.__init__-39"><span class="linenos">39</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">pad_token_id</span><span class="o">=</span><span class="n">pad_token_id</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                            <div id="KimiVLConfig.model_type" class="classattr">
                                <div class="attr variable">
            <span class="name">model_type</span>        =
<span class="default_value">&#39;kimi_vl&#39;</span>

        
    </div>
    <a class="headerlink" href="#KimiVLConfig.model_type"></a>
    
    

                            </div>
                            <div id="KimiVLConfig.vision_config" class="classattr">
                                <div class="attr variable">
            <span class="name">vision_config</span>

        
    </div>
    <a class="headerlink" href="#KimiVLConfig.vision_config"></a>
    
    

                            </div>
                            <div id="KimiVLConfig.text_config" class="classattr">
                                <div class="attr variable">
            <span class="name">text_config</span>

        
    </div>
    <a class="headerlink" href="#KimiVLConfig.text_config"></a>
    
    

                            </div>
                            <div id="KimiVLConfig.ignore_index" class="classattr">
                                <div class="attr variable">
            <span class="name">ignore_index</span>

        
    </div>
    <a class="headerlink" href="#KimiVLConfig.ignore_index"></a>
    
    

                            </div>
                            <div id="KimiVLConfig.media_placeholder_token_id" class="classattr">
                                <div class="attr variable">
            <span class="name">media_placeholder_token_id</span>

        
    </div>
    <a class="headerlink" href="#KimiVLConfig.media_placeholder_token_id"></a>
    
    

                            </div>
                </section>
                <section id="MoonViTConfig">
                            <input id="MoonViTConfig-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">MoonViTConfig</span><wbr>(<span class="base">transformers.configuration_utils.PretrainedConfig</span>):

                <label class="view-source-button" for="MoonViTConfig-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MoonViTConfig"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MoonViTConfig-7"><a href="#MoonViTConfig-7"><span class="linenos"> 7</span></a><span class="k">class</span><span class="w"> </span><span class="nc">MoonViTConfig</span><span class="p">(</span><span class="n">PretrainedConfig</span><span class="p">):</span>
</span><span id="MoonViTConfig-8"><a href="#MoonViTConfig-8"><span class="linenos"> 8</span></a>    <span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;moonvit&quot;</span>
</span><span id="MoonViTConfig-9"><a href="#MoonViTConfig-9"><span class="linenos"> 9</span></a>
</span><span id="MoonViTConfig-10"><a href="#MoonViTConfig-10"><span class="linenos">10</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="MoonViTConfig-11"><a href="#MoonViTConfig-11"><span class="linenos">11</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="MoonViTConfig-12"><a href="#MoonViTConfig-12"><span class="linenos">12</span></a>        <span class="n">patch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">14</span><span class="p">,</span>
</span><span id="MoonViTConfig-13"><a href="#MoonViTConfig-13"><span class="linenos">13</span></a>        <span class="n">init_pos_emb_height</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
</span><span id="MoonViTConfig-14"><a href="#MoonViTConfig-14"><span class="linenos">14</span></a>        <span class="n">init_pos_emb_width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
</span><span id="MoonViTConfig-15"><a href="#MoonViTConfig-15"><span class="linenos">15</span></a>        <span class="n">num_attention_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
</span><span id="MoonViTConfig-16"><a href="#MoonViTConfig-16"><span class="linenos">16</span></a>        <span class="n">num_hidden_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">27</span><span class="p">,</span>
</span><span id="MoonViTConfig-17"><a href="#MoonViTConfig-17"><span class="linenos">17</span></a>        <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1152</span><span class="p">,</span>
</span><span id="MoonViTConfig-18"><a href="#MoonViTConfig-18"><span class="linenos">18</span></a>        <span class="n">intermediate_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4304</span><span class="p">,</span>
</span><span id="MoonViTConfig-19"><a href="#MoonViTConfig-19"><span class="linenos">19</span></a>        <span class="n">merge_kernel_size</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
</span><span id="MoonViTConfig-20"><a href="#MoonViTConfig-20"><span class="linenos">20</span></a>        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span id="MoonViTConfig-21"><a href="#MoonViTConfig-21"><span class="linenos">21</span></a>    <span class="p">):</span>
</span><span id="MoonViTConfig-22"><a href="#MoonViTConfig-22"><span class="linenos">22</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="MoonViTConfig-23"><a href="#MoonViTConfig-23"><span class="linenos">23</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span>
</span><span id="MoonViTConfig-24"><a href="#MoonViTConfig-24"><span class="linenos">24</span></a>        <span class="c1"># Positional embedding config</span>
</span><span id="MoonViTConfig-25"><a href="#MoonViTConfig-25"><span class="linenos">25</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">init_pos_emb_height</span> <span class="o">=</span> <span class="n">init_pos_emb_height</span>
</span><span id="MoonViTConfig-26"><a href="#MoonViTConfig-26"><span class="linenos">26</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">init_pos_emb_width</span> <span class="o">=</span> <span class="n">init_pos_emb_width</span>
</span><span id="MoonViTConfig-27"><a href="#MoonViTConfig-27"><span class="linenos">27</span></a>        <span class="c1"># Transformer config</span>
</span><span id="MoonViTConfig-28"><a href="#MoonViTConfig-28"><span class="linenos">28</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="n">num_hidden_layers</span>
</span><span id="MoonViTConfig-29"><a href="#MoonViTConfig-29"><span class="linenos">29</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">num_attention_heads</span>
</span><span id="MoonViTConfig-30"><a href="#MoonViTConfig-30"><span class="linenos">30</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span id="MoonViTConfig-31"><a href="#MoonViTConfig-31"><span class="linenos">31</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">intermediate_size</span>
</span><span id="MoonViTConfig-32"><a href="#MoonViTConfig-32"><span class="linenos">32</span></a>        <span class="c1"># Patch merger config</span>
</span><span id="MoonViTConfig-33"><a href="#MoonViTConfig-33"><span class="linenos">33</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">merge_kernel_size</span> <span class="o">=</span> <span class="n">merge_kernel_size</span>
</span></pre></div>


            <div class="docstring"><p>Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as
methods for loading/downloading/saving configurations.</p>

<p><Tip></p>

<p>A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to
initialize a model does <strong>not</strong> load the model weights. It only affects the model's configuration.</p>

<p></Tip></p>

<p>Class attributes (overridden by derived classes):</p>

<ul>
<li><strong>model_type</strong> (<code>str</code>) -- An identifier for the model type, serialized into the JSON file, and used to recreate
the correct object in [<code>~transformers.AutoConfig</code>].</li>
<li><strong>has_no_defaults_at_init</strong> (<code>bool</code>) -- Whether the config class can be initialized without providing input arguments.
Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,
(but not necessarily) such as [<code>~transformers.EncoderDecoderConfig</code>] or [<code>~RagConfig</code>]. They have to be initialized from
two or more configs of type [<code>~transformers.PretrainedConfig</code>].</li>
<li><strong>keys_to_ignore_at_inference</strong> (<code>list[str]</code>) -- A list of keys to ignore by default when looking at dictionary
outputs of the model during inference.</li>
<li><strong>attribute_map</strong> (<code>dict[str, str]</code>) -- A dict that maps model specific attribute names to the standardized
naming of attributes.</li>
<li><strong>base_model_tp_plan</strong> (<code>dict[str, Any]</code>) -- A dict that maps sub-modules FQNs of a base model to a tensor
parallel plan applied to the sub-module when <code>model.tensor_parallel</code> is called.</li>
<li><strong>base_model_pp_plan</strong> (<code>dict[str, tuple[list[str]]]</code>) -- A dict that maps child-modules of a base model to a
pipeline parallel plan that enables users to place the child-module on the appropriate device.</li>
</ul>

<p>Common attributes (present in all subclasses):</p>

<ul>
<li><strong>vocab_size</strong> (<code>int</code>) -- The number of tokens in the vocabulary, which is also the first dimension of the
embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).</li>
<li><strong>hidden_size</strong> (<code>int</code>) -- The hidden size of the model.</li>
<li><strong>num_attention_heads</strong> (<code>int</code>) -- The number of attention heads used in the multi-head attention layers of the
model.</li>
<li><strong>num_hidden_layers</strong> (<code>int</code>) -- The number of blocks in the model.</li>
</ul>

<p><Tip warning={true}&gt;</p>

<p>Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading
some of them will still be possible, but attempting to overwrite them will throw an exception -- you should set
them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more
information about the individual parameters.</p>

<p></Tip></p>

<p>Arg:
    name_or_path (<code>str</code>, <em>optional</em>, defaults to <code>""</code>):
        Store the string that was passed to [<code>PreTrainedModel.from_pretrained</code>] or
        [<code>TFPreTrainedModel.from_pretrained</code>] as <code>pretrained_model_name_or_path</code> if the configuration was created
        with such a method.
    output_hidden_states (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether or not the model should return all hidden-states.
    output_attentions (<code>bool</code>, *optional*, defaults to <code>False</code>):
        Whether or not the model should returns all attentions.
    return_dict (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
        Whether or not the model should return a [<code>~transformers.utils.ModelOutput</code>] instead of a plain tuple.
    is_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether the model is used as an encoder/decoder or not.
    is_decoder (<code>bool</code>, *optional*, defaults to <code>False</code>):
        Whether to only use the decoder in an encoder-decoder architecture, otherwise it has no effect on
        decoder-only or encoder-only architectures.
    cross_attention_hidden_size (<code>bool</code>, <em>optional</em>):
        The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder
        setting and the cross-attention hidden dimension differs from <code>self.config.hidden_size</code>.
    add_cross_attention (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether cross-attention layers should be added to the model. Note, this option is only relevant for models
        that can be used as decoder models within the [<code>EncoderDecoderModel</code>] class, which consists of all models
        in <code>AUTO_MODELS_FOR_CAUSAL_LM</code>.
    tie_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder
        and decoder model to have the exact same parameter names.
    prune_heads (<code>dict[int, list[int]]</code>, <em>optional</em>, defaults to <code>{}</code>):
        Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of
        heads to prune in said layer.</p>

<pre><code>    For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.
chunk_size_feed_forward (`int`, *optional*, defaults to `0`):
    The chunk size of all feed forward layers in the residual attention blocks. A chunk size of `0` means that
    the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes `n` &lt;
    sequence_length embeddings at a time. For more information on feed forward chunking, see [How does Feed
    Forward Chunking work?](../glossary.html#feed-forward-chunking).

&gt; Parameters for fine-tuning tasks

architectures (`list[str]`, *optional*):
    Model architectures that can be used with the model pretrained weights.
finetuning_task (`str`, *optional*):
    Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow
    or PyTorch) checkpoint.
id2label (`dict[int, str]`, *optional*):
    A map from index (for instance prediction index, or target index) to label.
label2id (`dict[str, int]`, *optional*):
    A map from label to index for the model.
num_labels (`int`, *optional*):
    Number of labels to use in the last layer added to the model, typically for a classification task.
task_specific_params (`dict[str, Any]`, *optional*):
    Additional keyword arguments to store for the current task.
problem_type (`str`, *optional*):
    Problem type for `XxxForSequenceClassification` models. Can be one of `"regression"`,
    `"single_label_classification"` or `"multi_label_classification"`.

&gt; Parameters linked to the tokenizer

tokenizer_class (`str`, *optional*):
    The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the
    model by default).
prefix (`str`, *optional*):
    A specific prompt that should be added at the beginning of each text before calling the model.
bos_token_id (`int`, *optional*):
    The id of the _beginning-of-stream_ token.
pad_token_id (`int`, *optional*):
    The id of the _padding_ token.
eos_token_id (`int`, *optional*):
    The id of the _end-of-stream_ token.
decoder_start_token_id (`int`, *optional*):
    If an encoder-decoder model starts decoding with a different token than _bos_, the id of that token.
sep_token_id (`int`, *optional*):
    The id of the _separation_ token.

&gt; PyTorch specific parameters

torchscript (`bool`, *optional*, defaults to `False`):
    Whether or not the model should be used with Torchscript.
tie_word_embeddings (`bool`, *optional*, defaults to `True`):
    Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the
    model has a output word embedding layer.
torch_dtype (`str`, *optional*):
    The `dtype` of the weights. This attribute can be used to initialize the model to a non-default `dtype`
    (which is normally `float32`) and thus allow for optimal storage allocation. For example, if the saved
    model is `float16`, ideally we want to load it back using the minimal amount of memory needed to load
    `float16` weights. Since the config object is stored in plain text, this attribute contains just the
    floating type string without the `torch.` prefix. For example, for `torch.float16` ``torch_dtype` is the
    `"float16"` string.

    This attribute is currently not being used during model loading time, but this may change in the future
    versions. But we can already start preparing for the future by saving the dtype with save_pretrained.
</code></pre>
</div>


                            <div id="MoonViTConfig.__init__" class="classattr">
                                        <input id="MoonViTConfig.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">MoonViTConfig</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">patch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">14</span>,</span><span class="param">	<span class="n">init_pos_emb_height</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span>,</span><span class="param">	<span class="n">init_pos_emb_width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span>,</span><span class="param">	<span class="n">num_attention_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span>,</span><span class="param">	<span class="n">num_hidden_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">27</span>,</span><span class="param">	<span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1152</span>,</span><span class="param">	<span class="n">intermediate_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4304</span>,</span><span class="param">	<span class="n">merge_kernel_size</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>,</span><span class="param">	<span class="o">**</span><span class="n">kwargs</span></span>)</span>

                <label class="view-source-button" for="MoonViTConfig.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#MoonViTConfig.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="MoonViTConfig.__init__-10"><a href="#MoonViTConfig.__init__-10"><span class="linenos">10</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="MoonViTConfig.__init__-11"><a href="#MoonViTConfig.__init__-11"><span class="linenos">11</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="MoonViTConfig.__init__-12"><a href="#MoonViTConfig.__init__-12"><span class="linenos">12</span></a>        <span class="n">patch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">14</span><span class="p">,</span>
</span><span id="MoonViTConfig.__init__-13"><a href="#MoonViTConfig.__init__-13"><span class="linenos">13</span></a>        <span class="n">init_pos_emb_height</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
</span><span id="MoonViTConfig.__init__-14"><a href="#MoonViTConfig.__init__-14"><span class="linenos">14</span></a>        <span class="n">init_pos_emb_width</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
</span><span id="MoonViTConfig.__init__-15"><a href="#MoonViTConfig.__init__-15"><span class="linenos">15</span></a>        <span class="n">num_attention_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
</span><span id="MoonViTConfig.__init__-16"><a href="#MoonViTConfig.__init__-16"><span class="linenos">16</span></a>        <span class="n">num_hidden_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">27</span><span class="p">,</span>
</span><span id="MoonViTConfig.__init__-17"><a href="#MoonViTConfig.__init__-17"><span class="linenos">17</span></a>        <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1152</span><span class="p">,</span>
</span><span id="MoonViTConfig.__init__-18"><a href="#MoonViTConfig.__init__-18"><span class="linenos">18</span></a>        <span class="n">intermediate_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4304</span><span class="p">,</span>
</span><span id="MoonViTConfig.__init__-19"><a href="#MoonViTConfig.__init__-19"><span class="linenos">19</span></a>        <span class="n">merge_kernel_size</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
</span><span id="MoonViTConfig.__init__-20"><a href="#MoonViTConfig.__init__-20"><span class="linenos">20</span></a>        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span id="MoonViTConfig.__init__-21"><a href="#MoonViTConfig.__init__-21"><span class="linenos">21</span></a>    <span class="p">):</span>
</span><span id="MoonViTConfig.__init__-22"><a href="#MoonViTConfig.__init__-22"><span class="linenos">22</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span id="MoonViTConfig.__init__-23"><a href="#MoonViTConfig.__init__-23"><span class="linenos">23</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span>
</span><span id="MoonViTConfig.__init__-24"><a href="#MoonViTConfig.__init__-24"><span class="linenos">24</span></a>        <span class="c1"># Positional embedding config</span>
</span><span id="MoonViTConfig.__init__-25"><a href="#MoonViTConfig.__init__-25"><span class="linenos">25</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">init_pos_emb_height</span> <span class="o">=</span> <span class="n">init_pos_emb_height</span>
</span><span id="MoonViTConfig.__init__-26"><a href="#MoonViTConfig.__init__-26"><span class="linenos">26</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">init_pos_emb_width</span> <span class="o">=</span> <span class="n">init_pos_emb_width</span>
</span><span id="MoonViTConfig.__init__-27"><a href="#MoonViTConfig.__init__-27"><span class="linenos">27</span></a>        <span class="c1"># Transformer config</span>
</span><span id="MoonViTConfig.__init__-28"><a href="#MoonViTConfig.__init__-28"><span class="linenos">28</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="n">num_hidden_layers</span>
</span><span id="MoonViTConfig.__init__-29"><a href="#MoonViTConfig.__init__-29"><span class="linenos">29</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">num_attention_heads</span>
</span><span id="MoonViTConfig.__init__-30"><a href="#MoonViTConfig.__init__-30"><span class="linenos">30</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span id="MoonViTConfig.__init__-31"><a href="#MoonViTConfig.__init__-31"><span class="linenos">31</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">intermediate_size</span>
</span><span id="MoonViTConfig.__init__-32"><a href="#MoonViTConfig.__init__-32"><span class="linenos">32</span></a>        <span class="c1"># Patch merger config</span>
</span><span id="MoonViTConfig.__init__-33"><a href="#MoonViTConfig.__init__-33"><span class="linenos">33</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">merge_kernel_size</span> <span class="o">=</span> <span class="n">merge_kernel_size</span>
</span></pre></div>


    

                            </div>
                            <div id="MoonViTConfig.model_type" class="classattr">
                                <div class="attr variable">
            <span class="name">model_type</span>        =
<span class="default_value">&#39;moonvit&#39;</span>

        
    </div>
    <a class="headerlink" href="#MoonViTConfig.model_type"></a>
    
    

                            </div>
                            <div id="MoonViTConfig.patch_size" class="classattr">
                                <div class="attr variable">
            <span class="name">patch_size</span>

        
    </div>
    <a class="headerlink" href="#MoonViTConfig.patch_size"></a>
    
    

                            </div>
                            <div id="MoonViTConfig.init_pos_emb_height" class="classattr">
                                <div class="attr variable">
            <span class="name">init_pos_emb_height</span>

        
    </div>
    <a class="headerlink" href="#MoonViTConfig.init_pos_emb_height"></a>
    
    

                            </div>
                            <div id="MoonViTConfig.init_pos_emb_width" class="classattr">
                                <div class="attr variable">
            <span class="name">init_pos_emb_width</span>

        
    </div>
    <a class="headerlink" href="#MoonViTConfig.init_pos_emb_width"></a>
    
    

                            </div>
                            <div id="MoonViTConfig.num_hidden_layers" class="classattr">
                                <div class="attr variable">
            <span class="name">num_hidden_layers</span>

        
    </div>
    <a class="headerlink" href="#MoonViTConfig.num_hidden_layers"></a>
    
    

                            </div>
                            <div id="MoonViTConfig.num_attention_heads" class="classattr">
                                <div class="attr variable">
            <span class="name">num_attention_heads</span>

        
    </div>
    <a class="headerlink" href="#MoonViTConfig.num_attention_heads"></a>
    
    

                            </div>
                            <div id="MoonViTConfig.hidden_size" class="classattr">
                                <div class="attr variable">
            <span class="name">hidden_size</span>

        
    </div>
    <a class="headerlink" href="#MoonViTConfig.hidden_size"></a>
    
    

                            </div>
                            <div id="MoonViTConfig.intermediate_size" class="classattr">
                                <div class="attr variable">
            <span class="name">intermediate_size</span>

        
    </div>
    <a class="headerlink" href="#MoonViTConfig.intermediate_size"></a>
    
    

                            </div>
                            <div id="MoonViTConfig.merge_kernel_size" class="classattr">
                                <div class="attr variable">
            <span class="name">merge_kernel_size</span>

        
    </div>
    <a class="headerlink" href="#MoonViTConfig.merge_kernel_size"></a>
    
    

                            </div>
                </section>
                <section id="Step3VLConfig">
                            <input id="Step3VLConfig-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">Step3VLConfig</span><wbr>(<span class="base">transformers.configuration_utils.PretrainedConfig</span>):

                <label class="view-source-button" for="Step3VLConfig-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Step3VLConfig"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Step3VLConfig-144"><a href="#Step3VLConfig-144"><span class="linenos">144</span></a><span class="k">class</span><span class="w"> </span><span class="nc">Step3VLConfig</span><span class="p">(</span><span class="n">PretrainedConfig</span><span class="p">):</span>
</span><span id="Step3VLConfig-145"><a href="#Step3VLConfig-145"><span class="linenos">145</span></a>    <span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;step3_vl&quot;</span>
</span><span id="Step3VLConfig-146"><a href="#Step3VLConfig-146"><span class="linenos">146</span></a>
</span><span id="Step3VLConfig-147"><a href="#Step3VLConfig-147"><span class="linenos">147</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="Step3VLConfig-148"><a href="#Step3VLConfig-148"><span class="linenos">148</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="Step3VLConfig-149"><a href="#Step3VLConfig-149"><span class="linenos">149</span></a>        <span class="n">vision_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">Step3VisionEncoderConfig</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="Step3VLConfig-150"><a href="#Step3VLConfig-150"><span class="linenos">150</span></a>        <span class="n">text_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">Step3TextConfig</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="Step3VLConfig-151"><a href="#Step3VLConfig-151"><span class="linenos">151</span></a>        <span class="n">understand_projector_stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="Step3VLConfig-152"><a href="#Step3VLConfig-152"><span class="linenos">152</span></a>        <span class="n">projector_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="Step3VLConfig-153"><a href="#Step3VLConfig-153"><span class="linenos">153</span></a>        <span class="n">image_token_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128001</span><span class="p">,</span>
</span><span id="Step3VLConfig-154"><a href="#Step3VLConfig-154"><span class="linenos">154</span></a>        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span id="Step3VLConfig-155"><a href="#Step3VLConfig-155"><span class="linenos">155</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="Step3VLConfig-156"><a href="#Step3VLConfig-156"><span class="linenos">156</span></a>        <span class="k">if</span> <span class="n">vision_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="Step3VLConfig-157"><a href="#Step3VLConfig-157"><span class="linenos">157</span></a>            <span class="n">vision_config</span> <span class="o">=</span> <span class="n">Step3VisionEncoderConfig</span><span class="p">()</span>
</span><span id="Step3VLConfig-158"><a href="#Step3VLConfig-158"><span class="linenos">158</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vision_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
</span><span id="Step3VLConfig-159"><a href="#Step3VLConfig-159"><span class="linenos">159</span></a>            <span class="n">vision_config</span> <span class="o">=</span> <span class="n">Step3VisionEncoderConfig</span><span class="p">(</span><span class="o">**</span><span class="n">vision_config</span><span class="p">)</span>
</span><span id="Step3VLConfig-160"><a href="#Step3VLConfig-160"><span class="linenos">160</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vision_config</span> <span class="o">=</span> <span class="n">vision_config</span>
</span><span id="Step3VLConfig-161"><a href="#Step3VLConfig-161"><span class="linenos">161</span></a>
</span><span id="Step3VLConfig-162"><a href="#Step3VLConfig-162"><span class="linenos">162</span></a>        <span class="k">if</span> <span class="n">text_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="Step3VLConfig-163"><a href="#Step3VLConfig-163"><span class="linenos">163</span></a>            <span class="n">text_config</span> <span class="o">=</span> <span class="n">Step3TextConfig</span><span class="p">()</span>
</span><span id="Step3VLConfig-164"><a href="#Step3VLConfig-164"><span class="linenos">164</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
</span><span id="Step3VLConfig-165"><a href="#Step3VLConfig-165"><span class="linenos">165</span></a>            <span class="n">text_config</span> <span class="o">=</span> <span class="n">Step3TextConfig</span><span class="p">(</span><span class="o">**</span><span class="n">text_config</span><span class="p">)</span>
</span><span id="Step3VLConfig-166"><a href="#Step3VLConfig-166"><span class="linenos">166</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">text_config</span> <span class="o">=</span> <span class="n">text_config</span>
</span><span id="Step3VLConfig-167"><a href="#Step3VLConfig-167"><span class="linenos">167</span></a>
</span><span id="Step3VLConfig-168"><a href="#Step3VLConfig-168"><span class="linenos">168</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">understand_projector_stride</span> <span class="o">=</span> <span class="n">understand_projector_stride</span>
</span><span id="Step3VLConfig-169"><a href="#Step3VLConfig-169"><span class="linenos">169</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">projector_bias</span> <span class="o">=</span> <span class="n">projector_bias</span>
</span><span id="Step3VLConfig-170"><a href="#Step3VLConfig-170"><span class="linenos">170</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">text_config</span><span class="o">.</span><span class="n">hidden_size</span>
</span><span id="Step3VLConfig-171"><a href="#Step3VLConfig-171"><span class="linenos">171</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">image_token_id</span> <span class="o">=</span> <span class="n">image_token_id</span>
</span><span id="Step3VLConfig-172"><a href="#Step3VLConfig-172"><span class="linenos">172</span></a>
</span><span id="Step3VLConfig-173"><a href="#Step3VLConfig-173"><span class="linenos">173</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as
methods for loading/downloading/saving configurations.</p>

<p><Tip></p>

<p>A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to
initialize a model does <strong>not</strong> load the model weights. It only affects the model's configuration.</p>

<p></Tip></p>

<p>Class attributes (overridden by derived classes):</p>

<ul>
<li><strong>model_type</strong> (<code>str</code>) -- An identifier for the model type, serialized into the JSON file, and used to recreate
the correct object in [<code>~transformers.AutoConfig</code>].</li>
<li><strong>has_no_defaults_at_init</strong> (<code>bool</code>) -- Whether the config class can be initialized without providing input arguments.
Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,
(but not necessarily) such as [<code>~transformers.EncoderDecoderConfig</code>] or [<code>~RagConfig</code>]. They have to be initialized from
two or more configs of type [<code>~transformers.PretrainedConfig</code>].</li>
<li><strong>keys_to_ignore_at_inference</strong> (<code>list[str]</code>) -- A list of keys to ignore by default when looking at dictionary
outputs of the model during inference.</li>
<li><strong>attribute_map</strong> (<code>dict[str, str]</code>) -- A dict that maps model specific attribute names to the standardized
naming of attributes.</li>
<li><strong>base_model_tp_plan</strong> (<code>dict[str, Any]</code>) -- A dict that maps sub-modules FQNs of a base model to a tensor
parallel plan applied to the sub-module when <code>model.tensor_parallel</code> is called.</li>
<li><strong>base_model_pp_plan</strong> (<code>dict[str, tuple[list[str]]]</code>) -- A dict that maps child-modules of a base model to a
pipeline parallel plan that enables users to place the child-module on the appropriate device.</li>
</ul>

<p>Common attributes (present in all subclasses):</p>

<ul>
<li><strong>vocab_size</strong> (<code>int</code>) -- The number of tokens in the vocabulary, which is also the first dimension of the
embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).</li>
<li><strong>hidden_size</strong> (<code>int</code>) -- The hidden size of the model.</li>
<li><strong>num_attention_heads</strong> (<code>int</code>) -- The number of attention heads used in the multi-head attention layers of the
model.</li>
<li><strong>num_hidden_layers</strong> (<code>int</code>) -- The number of blocks in the model.</li>
</ul>

<p><Tip warning={true}&gt;</p>

<p>Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading
some of them will still be possible, but attempting to overwrite them will throw an exception -- you should set
them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more
information about the individual parameters.</p>

<p></Tip></p>

<p>Arg:
    name_or_path (<code>str</code>, <em>optional</em>, defaults to <code>""</code>):
        Store the string that was passed to [<code>PreTrainedModel.from_pretrained</code>] or
        [<code>TFPreTrainedModel.from_pretrained</code>] as <code>pretrained_model_name_or_path</code> if the configuration was created
        with such a method.
    output_hidden_states (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether or not the model should return all hidden-states.
    output_attentions (<code>bool</code>, *optional*, defaults to <code>False</code>):
        Whether or not the model should returns all attentions.
    return_dict (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
        Whether or not the model should return a [<code>~transformers.utils.ModelOutput</code>] instead of a plain tuple.
    is_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether the model is used as an encoder/decoder or not.
    is_decoder (<code>bool</code>, *optional*, defaults to <code>False</code>):
        Whether to only use the decoder in an encoder-decoder architecture, otherwise it has no effect on
        decoder-only or encoder-only architectures.
    cross_attention_hidden_size (<code>bool</code>, <em>optional</em>):
        The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder
        setting and the cross-attention hidden dimension differs from <code>self.config.hidden_size</code>.
    add_cross_attention (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether cross-attention layers should be added to the model. Note, this option is only relevant for models
        that can be used as decoder models within the [<code>EncoderDecoderModel</code>] class, which consists of all models
        in <code>AUTO_MODELS_FOR_CAUSAL_LM</code>.
    tie_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder
        and decoder model to have the exact same parameter names.
    prune_heads (<code>dict[int, list[int]]</code>, <em>optional</em>, defaults to <code>{}</code>):
        Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of
        heads to prune in said layer.</p>

<pre><code>    For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.
chunk_size_feed_forward (`int`, *optional*, defaults to `0`):
    The chunk size of all feed forward layers in the residual attention blocks. A chunk size of `0` means that
    the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes `n` &lt;
    sequence_length embeddings at a time. For more information on feed forward chunking, see [How does Feed
    Forward Chunking work?](../glossary.html#feed-forward-chunking).

&gt; Parameters for fine-tuning tasks

architectures (`list[str]`, *optional*):
    Model architectures that can be used with the model pretrained weights.
finetuning_task (`str`, *optional*):
    Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow
    or PyTorch) checkpoint.
id2label (`dict[int, str]`, *optional*):
    A map from index (for instance prediction index, or target index) to label.
label2id (`dict[str, int]`, *optional*):
    A map from label to index for the model.
num_labels (`int`, *optional*):
    Number of labels to use in the last layer added to the model, typically for a classification task.
task_specific_params (`dict[str, Any]`, *optional*):
    Additional keyword arguments to store for the current task.
problem_type (`str`, *optional*):
    Problem type for `XxxForSequenceClassification` models. Can be one of `"regression"`,
    `"single_label_classification"` or `"multi_label_classification"`.

&gt; Parameters linked to the tokenizer

tokenizer_class (`str`, *optional*):
    The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the
    model by default).
prefix (`str`, *optional*):
    A specific prompt that should be added at the beginning of each text before calling the model.
bos_token_id (`int`, *optional*):
    The id of the _beginning-of-stream_ token.
pad_token_id (`int`, *optional*):
    The id of the _padding_ token.
eos_token_id (`int`, *optional*):
    The id of the _end-of-stream_ token.
decoder_start_token_id (`int`, *optional*):
    If an encoder-decoder model starts decoding with a different token than _bos_, the id of that token.
sep_token_id (`int`, *optional*):
    The id of the _separation_ token.

&gt; PyTorch specific parameters

torchscript (`bool`, *optional*, defaults to `False`):
    Whether or not the model should be used with Torchscript.
tie_word_embeddings (`bool`, *optional*, defaults to `True`):
    Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the
    model has a output word embedding layer.
torch_dtype (`str`, *optional*):
    The `dtype` of the weights. This attribute can be used to initialize the model to a non-default `dtype`
    (which is normally `float32`) and thus allow for optimal storage allocation. For example, if the saved
    model is `float16`, ideally we want to load it back using the minimal amount of memory needed to load
    `float16` weights. Since the config object is stored in plain text, this attribute contains just the
    floating type string without the `torch.` prefix. For example, for `torch.float16` ``torch_dtype` is the
    `"float16"` string.

    This attribute is currently not being used during model loading time, but this may change in the future
    versions. But we can already start preparing for the future by saving the dtype with save_pretrained.
</code></pre>
</div>


                            <div id="Step3VLConfig.__init__" class="classattr">
                                        <input id="Step3VLConfig.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">Step3VLConfig</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">vision_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n"><a href="#Step3VisionEncoderConfig">Step3VisionEncoderConfig</a></span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">text_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n"><a href="#Step3TextConfig">Step3TextConfig</a></span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">understand_projector_stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>,</span><span class="param">	<span class="n">projector_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>,</span><span class="param">	<span class="n">image_token_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128001</span>,</span><span class="param">	<span class="o">**</span><span class="n">kwargs</span></span>)</span>

                <label class="view-source-button" for="Step3VLConfig.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Step3VLConfig.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Step3VLConfig.__init__-147"><a href="#Step3VLConfig.__init__-147"><span class="linenos">147</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="Step3VLConfig.__init__-148"><a href="#Step3VLConfig.__init__-148"><span class="linenos">148</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="Step3VLConfig.__init__-149"><a href="#Step3VLConfig.__init__-149"><span class="linenos">149</span></a>        <span class="n">vision_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">Step3VisionEncoderConfig</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="Step3VLConfig.__init__-150"><a href="#Step3VLConfig.__init__-150"><span class="linenos">150</span></a>        <span class="n">text_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">Step3TextConfig</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="Step3VLConfig.__init__-151"><a href="#Step3VLConfig.__init__-151"><span class="linenos">151</span></a>        <span class="n">understand_projector_stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="Step3VLConfig.__init__-152"><a href="#Step3VLConfig.__init__-152"><span class="linenos">152</span></a>        <span class="n">projector_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
</span><span id="Step3VLConfig.__init__-153"><a href="#Step3VLConfig.__init__-153"><span class="linenos">153</span></a>        <span class="n">image_token_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128001</span><span class="p">,</span>
</span><span id="Step3VLConfig.__init__-154"><a href="#Step3VLConfig.__init__-154"><span class="linenos">154</span></a>        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span id="Step3VLConfig.__init__-155"><a href="#Step3VLConfig.__init__-155"><span class="linenos">155</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="Step3VLConfig.__init__-156"><a href="#Step3VLConfig.__init__-156"><span class="linenos">156</span></a>        <span class="k">if</span> <span class="n">vision_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="Step3VLConfig.__init__-157"><a href="#Step3VLConfig.__init__-157"><span class="linenos">157</span></a>            <span class="n">vision_config</span> <span class="o">=</span> <span class="n">Step3VisionEncoderConfig</span><span class="p">()</span>
</span><span id="Step3VLConfig.__init__-158"><a href="#Step3VLConfig.__init__-158"><span class="linenos">158</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vision_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
</span><span id="Step3VLConfig.__init__-159"><a href="#Step3VLConfig.__init__-159"><span class="linenos">159</span></a>            <span class="n">vision_config</span> <span class="o">=</span> <span class="n">Step3VisionEncoderConfig</span><span class="p">(</span><span class="o">**</span><span class="n">vision_config</span><span class="p">)</span>
</span><span id="Step3VLConfig.__init__-160"><a href="#Step3VLConfig.__init__-160"><span class="linenos">160</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vision_config</span> <span class="o">=</span> <span class="n">vision_config</span>
</span><span id="Step3VLConfig.__init__-161"><a href="#Step3VLConfig.__init__-161"><span class="linenos">161</span></a>
</span><span id="Step3VLConfig.__init__-162"><a href="#Step3VLConfig.__init__-162"><span class="linenos">162</span></a>        <span class="k">if</span> <span class="n">text_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="Step3VLConfig.__init__-163"><a href="#Step3VLConfig.__init__-163"><span class="linenos">163</span></a>            <span class="n">text_config</span> <span class="o">=</span> <span class="n">Step3TextConfig</span><span class="p">()</span>
</span><span id="Step3VLConfig.__init__-164"><a href="#Step3VLConfig.__init__-164"><span class="linenos">164</span></a>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">text_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
</span><span id="Step3VLConfig.__init__-165"><a href="#Step3VLConfig.__init__-165"><span class="linenos">165</span></a>            <span class="n">text_config</span> <span class="o">=</span> <span class="n">Step3TextConfig</span><span class="p">(</span><span class="o">**</span><span class="n">text_config</span><span class="p">)</span>
</span><span id="Step3VLConfig.__init__-166"><a href="#Step3VLConfig.__init__-166"><span class="linenos">166</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">text_config</span> <span class="o">=</span> <span class="n">text_config</span>
</span><span id="Step3VLConfig.__init__-167"><a href="#Step3VLConfig.__init__-167"><span class="linenos">167</span></a>
</span><span id="Step3VLConfig.__init__-168"><a href="#Step3VLConfig.__init__-168"><span class="linenos">168</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">understand_projector_stride</span> <span class="o">=</span> <span class="n">understand_projector_stride</span>
</span><span id="Step3VLConfig.__init__-169"><a href="#Step3VLConfig.__init__-169"><span class="linenos">169</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">projector_bias</span> <span class="o">=</span> <span class="n">projector_bias</span>
</span><span id="Step3VLConfig.__init__-170"><a href="#Step3VLConfig.__init__-170"><span class="linenos">170</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">text_config</span><span class="o">.</span><span class="n">hidden_size</span>
</span><span id="Step3VLConfig.__init__-171"><a href="#Step3VLConfig.__init__-171"><span class="linenos">171</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">image_token_id</span> <span class="o">=</span> <span class="n">image_token_id</span>
</span><span id="Step3VLConfig.__init__-172"><a href="#Step3VLConfig.__init__-172"><span class="linenos">172</span></a>
</span><span id="Step3VLConfig.__init__-173"><a href="#Step3VLConfig.__init__-173"><span class="linenos">173</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                            <div id="Step3VLConfig.model_type" class="classattr">
                                <div class="attr variable">
            <span class="name">model_type</span>        =
<span class="default_value">&#39;step3_vl&#39;</span>

        
    </div>
    <a class="headerlink" href="#Step3VLConfig.model_type"></a>
    
    

                            </div>
                            <div id="Step3VLConfig.vision_config" class="classattr">
                                <div class="attr variable">
            <span class="name">vision_config</span>

        
    </div>
    <a class="headerlink" href="#Step3VLConfig.vision_config"></a>
    
    

                            </div>
                            <div id="Step3VLConfig.text_config" class="classattr">
                                <div class="attr variable">
            <span class="name">text_config</span>

        
    </div>
    <a class="headerlink" href="#Step3VLConfig.text_config"></a>
    
    

                            </div>
                            <div id="Step3VLConfig.understand_projector_stride" class="classattr">
                                <div class="attr variable">
            <span class="name">understand_projector_stride</span>

        
    </div>
    <a class="headerlink" href="#Step3VLConfig.understand_projector_stride"></a>
    
    

                            </div>
                            <div id="Step3VLConfig.projector_bias" class="classattr">
                                <div class="attr variable">
            <span class="name">projector_bias</span>

        
    </div>
    <a class="headerlink" href="#Step3VLConfig.projector_bias"></a>
    
    

                            </div>
                            <div id="Step3VLConfig.hidden_size" class="classattr">
                                <div class="attr variable">
            <span class="name">hidden_size</span>

        
    </div>
    <a class="headerlink" href="#Step3VLConfig.hidden_size"></a>
    
    

                            </div>
                            <div id="Step3VLConfig.image_token_id" class="classattr">
                                <div class="attr variable">
            <span class="name">image_token_id</span>

        
    </div>
    <a class="headerlink" href="#Step3VLConfig.image_token_id"></a>
    
    

                            </div>
                </section>
                <section id="Step3TextConfig">
                            <input id="Step3TextConfig-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">Step3TextConfig</span><wbr>(<span class="base">transformers.configuration_utils.PretrainedConfig</span>):

                <label class="view-source-button" for="Step3TextConfig-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Step3TextConfig"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Step3TextConfig-37"><a href="#Step3TextConfig-37"><span class="linenos"> 37</span></a><span class="k">class</span><span class="w"> </span><span class="nc">Step3TextConfig</span><span class="p">(</span><span class="n">PretrainedConfig</span><span class="p">):</span>
</span><span id="Step3TextConfig-38"><a href="#Step3TextConfig-38"><span class="linenos"> 38</span></a>    <span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;step3_text&quot;</span>
</span><span id="Step3TextConfig-39"><a href="#Step3TextConfig-39"><span class="linenos"> 39</span></a>    <span class="n">architectures</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Step3TextForCausalLM&quot;</span><span class="p">]</span>
</span><span id="Step3TextConfig-40"><a href="#Step3TextConfig-40"><span class="linenos"> 40</span></a>
</span><span id="Step3TextConfig-41"><a href="#Step3TextConfig-41"><span class="linenos"> 41</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="Step3TextConfig-42"><a href="#Step3TextConfig-42"><span class="linenos"> 42</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="Step3TextConfig-43"><a href="#Step3TextConfig-43"><span class="linenos"> 43</span></a>        <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">7168</span><span class="p">,</span>
</span><span id="Step3TextConfig-44"><a href="#Step3TextConfig-44"><span class="linenos"> 44</span></a>        <span class="n">intermediate_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">18432</span><span class="p">,</span>
</span><span id="Step3TextConfig-45"><a href="#Step3TextConfig-45"><span class="linenos"> 45</span></a>        <span class="n">num_attention_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
</span><span id="Step3TextConfig-46"><a href="#Step3TextConfig-46"><span class="linenos"> 46</span></a>        <span class="n">num_attention_groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="Step3TextConfig-47"><a href="#Step3TextConfig-47"><span class="linenos"> 47</span></a>        <span class="n">num_hidden_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">61</span><span class="p">,</span>
</span><span id="Step3TextConfig-48"><a href="#Step3TextConfig-48"><span class="linenos"> 48</span></a>        <span class="n">max_seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">65536</span><span class="p">,</span>
</span><span id="Step3TextConfig-49"><a href="#Step3TextConfig-49"><span class="linenos"> 49</span></a>        <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128815</span><span class="p">,</span>
</span><span id="Step3TextConfig-50"><a href="#Step3TextConfig-50"><span class="linenos"> 50</span></a>        <span class="n">rms_norm_eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
</span><span id="Step3TextConfig-51"><a href="#Step3TextConfig-51"><span class="linenos"> 51</span></a>        <span class="n">moe_intermediate_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5120</span><span class="p">,</span>
</span><span id="Step3TextConfig-52"><a href="#Step3TextConfig-52"><span class="linenos"> 52</span></a>        <span class="n">moe_num_experts</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">48</span><span class="p">,</span>
</span><span id="Step3TextConfig-53"><a href="#Step3TextConfig-53"><span class="linenos"> 53</span></a>        <span class="n">moe_top_k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
</span><span id="Step3TextConfig-54"><a href="#Step3TextConfig-54"><span class="linenos"> 54</span></a>        <span class="n">rope_theta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">500000</span><span class="p">,</span>
</span><span id="Step3TextConfig-55"><a href="#Step3TextConfig-55"><span class="linenos"> 55</span></a>        <span class="n">rope_scaling</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="Step3TextConfig-56"><a href="#Step3TextConfig-56"><span class="linenos"> 56</span></a>        <span class="n">max_position_embedding</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">65536</span><span class="p">,</span>
</span><span id="Step3TextConfig-57"><a href="#Step3TextConfig-57"><span class="linenos"> 57</span></a>        <span class="n">share_expert_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5120</span><span class="p">,</span>
</span><span id="Step3TextConfig-58"><a href="#Step3TextConfig-58"><span class="linenos"> 58</span></a>        <span class="n">share_q_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
</span><span id="Step3TextConfig-59"><a href="#Step3TextConfig-59"><span class="linenos"> 59</span></a>        <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
</span><span id="Step3TextConfig-60"><a href="#Step3TextConfig-60"><span class="linenos"> 60</span></a>        <span class="n">norm_expert_weight</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="Step3TextConfig-61"><a href="#Step3TextConfig-61"><span class="linenos"> 61</span></a>        <span class="n">moe_layers_enum</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="Step3TextConfig-62"><a href="#Step3TextConfig-62"><span class="linenos"> 62</span></a>            <span class="mi">4</span><span class="p">,</span>
</span><span id="Step3TextConfig-63"><a href="#Step3TextConfig-63"><span class="linenos"> 63</span></a>            <span class="mi">5</span><span class="p">,</span>
</span><span id="Step3TextConfig-64"><a href="#Step3TextConfig-64"><span class="linenos"> 64</span></a>            <span class="mi">6</span><span class="p">,</span>
</span><span id="Step3TextConfig-65"><a href="#Step3TextConfig-65"><span class="linenos"> 65</span></a>            <span class="mi">7</span><span class="p">,</span>
</span><span id="Step3TextConfig-66"><a href="#Step3TextConfig-66"><span class="linenos"> 66</span></a>            <span class="mi">8</span><span class="p">,</span>
</span><span id="Step3TextConfig-67"><a href="#Step3TextConfig-67"><span class="linenos"> 67</span></a>            <span class="mi">9</span><span class="p">,</span>
</span><span id="Step3TextConfig-68"><a href="#Step3TextConfig-68"><span class="linenos"> 68</span></a>            <span class="mi">10</span><span class="p">,</span>
</span><span id="Step3TextConfig-69"><a href="#Step3TextConfig-69"><span class="linenos"> 69</span></a>            <span class="mi">11</span><span class="p">,</span>
</span><span id="Step3TextConfig-70"><a href="#Step3TextConfig-70"><span class="linenos"> 70</span></a>            <span class="mi">12</span><span class="p">,</span>
</span><span id="Step3TextConfig-71"><a href="#Step3TextConfig-71"><span class="linenos"> 71</span></a>            <span class="mi">13</span><span class="p">,</span>
</span><span id="Step3TextConfig-72"><a href="#Step3TextConfig-72"><span class="linenos"> 72</span></a>            <span class="mi">14</span><span class="p">,</span>
</span><span id="Step3TextConfig-73"><a href="#Step3TextConfig-73"><span class="linenos"> 73</span></a>            <span class="mi">15</span><span class="p">,</span>
</span><span id="Step3TextConfig-74"><a href="#Step3TextConfig-74"><span class="linenos"> 74</span></a>            <span class="mi">16</span><span class="p">,</span>
</span><span id="Step3TextConfig-75"><a href="#Step3TextConfig-75"><span class="linenos"> 75</span></a>            <span class="mi">17</span><span class="p">,</span>
</span><span id="Step3TextConfig-76"><a href="#Step3TextConfig-76"><span class="linenos"> 76</span></a>            <span class="mi">18</span><span class="p">,</span>
</span><span id="Step3TextConfig-77"><a href="#Step3TextConfig-77"><span class="linenos"> 77</span></a>            <span class="mi">19</span><span class="p">,</span>
</span><span id="Step3TextConfig-78"><a href="#Step3TextConfig-78"><span class="linenos"> 78</span></a>            <span class="mi">20</span><span class="p">,</span>
</span><span id="Step3TextConfig-79"><a href="#Step3TextConfig-79"><span class="linenos"> 79</span></a>            <span class="mi">21</span><span class="p">,</span>
</span><span id="Step3TextConfig-80"><a href="#Step3TextConfig-80"><span class="linenos"> 80</span></a>            <span class="mi">22</span><span class="p">,</span>
</span><span id="Step3TextConfig-81"><a href="#Step3TextConfig-81"><span class="linenos"> 81</span></a>            <span class="mi">23</span><span class="p">,</span>
</span><span id="Step3TextConfig-82"><a href="#Step3TextConfig-82"><span class="linenos"> 82</span></a>            <span class="mi">24</span><span class="p">,</span>
</span><span id="Step3TextConfig-83"><a href="#Step3TextConfig-83"><span class="linenos"> 83</span></a>            <span class="mi">25</span><span class="p">,</span>
</span><span id="Step3TextConfig-84"><a href="#Step3TextConfig-84"><span class="linenos"> 84</span></a>            <span class="mi">26</span><span class="p">,</span>
</span><span id="Step3TextConfig-85"><a href="#Step3TextConfig-85"><span class="linenos"> 85</span></a>            <span class="mi">27</span><span class="p">,</span>
</span><span id="Step3TextConfig-86"><a href="#Step3TextConfig-86"><span class="linenos"> 86</span></a>            <span class="mi">28</span><span class="p">,</span>
</span><span id="Step3TextConfig-87"><a href="#Step3TextConfig-87"><span class="linenos"> 87</span></a>            <span class="mi">29</span><span class="p">,</span>
</span><span id="Step3TextConfig-88"><a href="#Step3TextConfig-88"><span class="linenos"> 88</span></a>            <span class="mi">30</span><span class="p">,</span>
</span><span id="Step3TextConfig-89"><a href="#Step3TextConfig-89"><span class="linenos"> 89</span></a>            <span class="mi">31</span><span class="p">,</span>
</span><span id="Step3TextConfig-90"><a href="#Step3TextConfig-90"><span class="linenos"> 90</span></a>            <span class="mi">32</span><span class="p">,</span>
</span><span id="Step3TextConfig-91"><a href="#Step3TextConfig-91"><span class="linenos"> 91</span></a>            <span class="mi">33</span><span class="p">,</span>
</span><span id="Step3TextConfig-92"><a href="#Step3TextConfig-92"><span class="linenos"> 92</span></a>            <span class="mi">34</span><span class="p">,</span>
</span><span id="Step3TextConfig-93"><a href="#Step3TextConfig-93"><span class="linenos"> 93</span></a>            <span class="mi">35</span><span class="p">,</span>
</span><span id="Step3TextConfig-94"><a href="#Step3TextConfig-94"><span class="linenos"> 94</span></a>            <span class="mi">36</span><span class="p">,</span>
</span><span id="Step3TextConfig-95"><a href="#Step3TextConfig-95"><span class="linenos"> 95</span></a>            <span class="mi">37</span><span class="p">,</span>
</span><span id="Step3TextConfig-96"><a href="#Step3TextConfig-96"><span class="linenos"> 96</span></a>            <span class="mi">38</span><span class="p">,</span>
</span><span id="Step3TextConfig-97"><a href="#Step3TextConfig-97"><span class="linenos"> 97</span></a>            <span class="mi">39</span><span class="p">,</span>
</span><span id="Step3TextConfig-98"><a href="#Step3TextConfig-98"><span class="linenos"> 98</span></a>            <span class="mi">40</span><span class="p">,</span>
</span><span id="Step3TextConfig-99"><a href="#Step3TextConfig-99"><span class="linenos"> 99</span></a>            <span class="mi">41</span><span class="p">,</span>
</span><span id="Step3TextConfig-100"><a href="#Step3TextConfig-100"><span class="linenos">100</span></a>            <span class="mi">42</span><span class="p">,</span>
</span><span id="Step3TextConfig-101"><a href="#Step3TextConfig-101"><span class="linenos">101</span></a>            <span class="mi">43</span><span class="p">,</span>
</span><span id="Step3TextConfig-102"><a href="#Step3TextConfig-102"><span class="linenos">102</span></a>            <span class="mi">44</span><span class="p">,</span>
</span><span id="Step3TextConfig-103"><a href="#Step3TextConfig-103"><span class="linenos">103</span></a>            <span class="mi">45</span><span class="p">,</span>
</span><span id="Step3TextConfig-104"><a href="#Step3TextConfig-104"><span class="linenos">104</span></a>            <span class="mi">46</span><span class="p">,</span>
</span><span id="Step3TextConfig-105"><a href="#Step3TextConfig-105"><span class="linenos">105</span></a>            <span class="mi">47</span><span class="p">,</span>
</span><span id="Step3TextConfig-106"><a href="#Step3TextConfig-106"><span class="linenos">106</span></a>            <span class="mi">48</span><span class="p">,</span>
</span><span id="Step3TextConfig-107"><a href="#Step3TextConfig-107"><span class="linenos">107</span></a>            <span class="mi">49</span><span class="p">,</span>
</span><span id="Step3TextConfig-108"><a href="#Step3TextConfig-108"><span class="linenos">108</span></a>            <span class="mi">50</span><span class="p">,</span>
</span><span id="Step3TextConfig-109"><a href="#Step3TextConfig-109"><span class="linenos">109</span></a>            <span class="mi">51</span><span class="p">,</span>
</span><span id="Step3TextConfig-110"><a href="#Step3TextConfig-110"><span class="linenos">110</span></a>            <span class="mi">52</span><span class="p">,</span>
</span><span id="Step3TextConfig-111"><a href="#Step3TextConfig-111"><span class="linenos">111</span></a>            <span class="mi">53</span><span class="p">,</span>
</span><span id="Step3TextConfig-112"><a href="#Step3TextConfig-112"><span class="linenos">112</span></a>            <span class="mi">54</span><span class="p">,</span>
</span><span id="Step3TextConfig-113"><a href="#Step3TextConfig-113"><span class="linenos">113</span></a>            <span class="mi">55</span><span class="p">,</span>
</span><span id="Step3TextConfig-114"><a href="#Step3TextConfig-114"><span class="linenos">114</span></a>            <span class="mi">56</span><span class="p">,</span>
</span><span id="Step3TextConfig-115"><a href="#Step3TextConfig-115"><span class="linenos">115</span></a>            <span class="mi">57</span><span class="p">,</span>
</span><span id="Step3TextConfig-116"><a href="#Step3TextConfig-116"><span class="linenos">116</span></a>            <span class="mi">58</span><span class="p">,</span>
</span><span id="Step3TextConfig-117"><a href="#Step3TextConfig-117"><span class="linenos">117</span></a>            <span class="mi">59</span><span class="p">,</span>
</span><span id="Step3TextConfig-118"><a href="#Step3TextConfig-118"><span class="linenos">118</span></a>        <span class="p">),</span>
</span><span id="Step3TextConfig-119"><a href="#Step3TextConfig-119"><span class="linenos">119</span></a>        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span id="Step3TextConfig-120"><a href="#Step3TextConfig-120"><span class="linenos">120</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="Step3TextConfig-121"><a href="#Step3TextConfig-121"><span class="linenos">121</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span id="Step3TextConfig-122"><a href="#Step3TextConfig-122"><span class="linenos">122</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">intermediate_size</span>
</span><span id="Step3TextConfig-123"><a href="#Step3TextConfig-123"><span class="linenos">123</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">num_attention_heads</span>
</span><span id="Step3TextConfig-124"><a href="#Step3TextConfig-124"><span class="linenos">124</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_groups</span> <span class="o">=</span> <span class="n">num_attention_groups</span>
</span><span id="Step3TextConfig-125"><a href="#Step3TextConfig-125"><span class="linenos">125</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="n">num_hidden_layers</span>
</span><span id="Step3TextConfig-126"><a href="#Step3TextConfig-126"><span class="linenos">126</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="o">=</span> <span class="n">max_seq_len</span>
</span><span id="Step3TextConfig-127"><a href="#Step3TextConfig-127"><span class="linenos">127</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
</span><span id="Step3TextConfig-128"><a href="#Step3TextConfig-128"><span class="linenos">128</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">rms_norm_eps</span> <span class="o">=</span> <span class="n">rms_norm_eps</span>
</span><span id="Step3TextConfig-129"><a href="#Step3TextConfig-129"><span class="linenos">129</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">moe_intermediate_size</span> <span class="o">=</span> <span class="n">moe_intermediate_size</span>
</span><span id="Step3TextConfig-130"><a href="#Step3TextConfig-130"><span class="linenos">130</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">moe_num_experts</span> <span class="o">=</span> <span class="n">moe_num_experts</span>
</span><span id="Step3TextConfig-131"><a href="#Step3TextConfig-131"><span class="linenos">131</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">moe_top_k</span> <span class="o">=</span> <span class="n">moe_top_k</span>
</span><span id="Step3TextConfig-132"><a href="#Step3TextConfig-132"><span class="linenos">132</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">rope_theta</span> <span class="o">=</span> <span class="n">rope_theta</span>
</span><span id="Step3TextConfig-133"><a href="#Step3TextConfig-133"><span class="linenos">133</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">rope_scaling</span> <span class="o">=</span> <span class="n">rope_scaling</span>
</span><span id="Step3TextConfig-134"><a href="#Step3TextConfig-134"><span class="linenos">134</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embedding</span> <span class="o">=</span> <span class="n">max_position_embedding</span>
</span><span id="Step3TextConfig-135"><a href="#Step3TextConfig-135"><span class="linenos">135</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">share_expert_dim</span> <span class="o">=</span> <span class="n">share_expert_dim</span>
</span><span id="Step3TextConfig-136"><a href="#Step3TextConfig-136"><span class="linenos">136</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">share_q_dim</span> <span class="o">=</span> <span class="n">share_q_dim</span>
</span><span id="Step3TextConfig-137"><a href="#Step3TextConfig-137"><span class="linenos">137</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">head_dim</span>
</span><span id="Step3TextConfig-138"><a href="#Step3TextConfig-138"><span class="linenos">138</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm_expert_weight</span> <span class="o">=</span> <span class="n">norm_expert_weight</span>
</span><span id="Step3TextConfig-139"><a href="#Step3TextConfig-139"><span class="linenos">139</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">moe_layers_enum</span> <span class="o">=</span> <span class="n">moe_layers_enum</span>
</span><span id="Step3TextConfig-140"><a href="#Step3TextConfig-140"><span class="linenos">140</span></a>
</span><span id="Step3TextConfig-141"><a href="#Step3TextConfig-141"><span class="linenos">141</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as
methods for loading/downloading/saving configurations.</p>

<p><Tip></p>

<p>A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to
initialize a model does <strong>not</strong> load the model weights. It only affects the model's configuration.</p>

<p></Tip></p>

<p>Class attributes (overridden by derived classes):</p>

<ul>
<li><strong>model_type</strong> (<code>str</code>) -- An identifier for the model type, serialized into the JSON file, and used to recreate
the correct object in [<code>~transformers.AutoConfig</code>].</li>
<li><strong>has_no_defaults_at_init</strong> (<code>bool</code>) -- Whether the config class can be initialized without providing input arguments.
Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,
(but not necessarily) such as [<code>~transformers.EncoderDecoderConfig</code>] or [<code>~RagConfig</code>]. They have to be initialized from
two or more configs of type [<code>~transformers.PretrainedConfig</code>].</li>
<li><strong>keys_to_ignore_at_inference</strong> (<code>list[str]</code>) -- A list of keys to ignore by default when looking at dictionary
outputs of the model during inference.</li>
<li><strong>attribute_map</strong> (<code>dict[str, str]</code>) -- A dict that maps model specific attribute names to the standardized
naming of attributes.</li>
<li><strong>base_model_tp_plan</strong> (<code>dict[str, Any]</code>) -- A dict that maps sub-modules FQNs of a base model to a tensor
parallel plan applied to the sub-module when <code>model.tensor_parallel</code> is called.</li>
<li><strong>base_model_pp_plan</strong> (<code>dict[str, tuple[list[str]]]</code>) -- A dict that maps child-modules of a base model to a
pipeline parallel plan that enables users to place the child-module on the appropriate device.</li>
</ul>

<p>Common attributes (present in all subclasses):</p>

<ul>
<li><strong>vocab_size</strong> (<code>int</code>) -- The number of tokens in the vocabulary, which is also the first dimension of the
embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).</li>
<li><strong>hidden_size</strong> (<code>int</code>) -- The hidden size of the model.</li>
<li><strong>num_attention_heads</strong> (<code>int</code>) -- The number of attention heads used in the multi-head attention layers of the
model.</li>
<li><strong>num_hidden_layers</strong> (<code>int</code>) -- The number of blocks in the model.</li>
</ul>

<p><Tip warning={true}&gt;</p>

<p>Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading
some of them will still be possible, but attempting to overwrite them will throw an exception -- you should set
them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more
information about the individual parameters.</p>

<p></Tip></p>

<p>Arg:
    name_or_path (<code>str</code>, <em>optional</em>, defaults to <code>""</code>):
        Store the string that was passed to [<code>PreTrainedModel.from_pretrained</code>] or
        [<code>TFPreTrainedModel.from_pretrained</code>] as <code>pretrained_model_name_or_path</code> if the configuration was created
        with such a method.
    output_hidden_states (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether or not the model should return all hidden-states.
    output_attentions (<code>bool</code>, *optional*, defaults to <code>False</code>):
        Whether or not the model should returns all attentions.
    return_dict (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
        Whether or not the model should return a [<code>~transformers.utils.ModelOutput</code>] instead of a plain tuple.
    is_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether the model is used as an encoder/decoder or not.
    is_decoder (<code>bool</code>, *optional*, defaults to <code>False</code>):
        Whether to only use the decoder in an encoder-decoder architecture, otherwise it has no effect on
        decoder-only or encoder-only architectures.
    cross_attention_hidden_size (<code>bool</code>, <em>optional</em>):
        The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder
        setting and the cross-attention hidden dimension differs from <code>self.config.hidden_size</code>.
    add_cross_attention (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether cross-attention layers should be added to the model. Note, this option is only relevant for models
        that can be used as decoder models within the [<code>EncoderDecoderModel</code>] class, which consists of all models
        in <code>AUTO_MODELS_FOR_CAUSAL_LM</code>.
    tie_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder
        and decoder model to have the exact same parameter names.
    prune_heads (<code>dict[int, list[int]]</code>, <em>optional</em>, defaults to <code>{}</code>):
        Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of
        heads to prune in said layer.</p>

<pre><code>    For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.
chunk_size_feed_forward (`int`, *optional*, defaults to `0`):
    The chunk size of all feed forward layers in the residual attention blocks. A chunk size of `0` means that
    the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes `n` &lt;
    sequence_length embeddings at a time. For more information on feed forward chunking, see [How does Feed
    Forward Chunking work?](../glossary.html#feed-forward-chunking).

&gt; Parameters for fine-tuning tasks

architectures (`list[str]`, *optional*):
    Model architectures that can be used with the model pretrained weights.
finetuning_task (`str`, *optional*):
    Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow
    or PyTorch) checkpoint.
id2label (`dict[int, str]`, *optional*):
    A map from index (for instance prediction index, or target index) to label.
label2id (`dict[str, int]`, *optional*):
    A map from label to index for the model.
num_labels (`int`, *optional*):
    Number of labels to use in the last layer added to the model, typically for a classification task.
task_specific_params (`dict[str, Any]`, *optional*):
    Additional keyword arguments to store for the current task.
problem_type (`str`, *optional*):
    Problem type for `XxxForSequenceClassification` models. Can be one of `"regression"`,
    `"single_label_classification"` or `"multi_label_classification"`.

&gt; Parameters linked to the tokenizer

tokenizer_class (`str`, *optional*):
    The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the
    model by default).
prefix (`str`, *optional*):
    A specific prompt that should be added at the beginning of each text before calling the model.
bos_token_id (`int`, *optional*):
    The id of the _beginning-of-stream_ token.
pad_token_id (`int`, *optional*):
    The id of the _padding_ token.
eos_token_id (`int`, *optional*):
    The id of the _end-of-stream_ token.
decoder_start_token_id (`int`, *optional*):
    If an encoder-decoder model starts decoding with a different token than _bos_, the id of that token.
sep_token_id (`int`, *optional*):
    The id of the _separation_ token.

&gt; PyTorch specific parameters

torchscript (`bool`, *optional*, defaults to `False`):
    Whether or not the model should be used with Torchscript.
tie_word_embeddings (`bool`, *optional*, defaults to `True`):
    Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the
    model has a output word embedding layer.
torch_dtype (`str`, *optional*):
    The `dtype` of the weights. This attribute can be used to initialize the model to a non-default `dtype`
    (which is normally `float32`) and thus allow for optimal storage allocation. For example, if the saved
    model is `float16`, ideally we want to load it back using the minimal amount of memory needed to load
    `float16` weights. Since the config object is stored in plain text, this attribute contains just the
    floating type string without the `torch.` prefix. For example, for `torch.float16` ``torch_dtype` is the
    `"float16"` string.

    This attribute is currently not being used during model loading time, but this may change in the future
    versions. But we can already start preparing for the future by saving the dtype with save_pretrained.
</code></pre>
</div>


                            <div id="Step3TextConfig.__init__" class="classattr">
                                        <input id="Step3TextConfig.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">Step3TextConfig</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">7168</span>,</span><span class="param">	<span class="n">intermediate_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">18432</span>,</span><span class="param">	<span class="n">num_attention_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span>,</span><span class="param">	<span class="n">num_attention_groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>,</span><span class="param">	<span class="n">num_hidden_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">61</span>,</span><span class="param">	<span class="n">max_seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">65536</span>,</span><span class="param">	<span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128815</span>,</span><span class="param">	<span class="n">rms_norm_eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-05</span>,</span><span class="param">	<span class="n">moe_intermediate_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5120</span>,</span><span class="param">	<span class="n">moe_num_experts</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">48</span>,</span><span class="param">	<span class="n">moe_top_k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span>,</span><span class="param">	<span class="n">rope_theta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">500000</span>,</span><span class="param">	<span class="n">rope_scaling</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">max_position_embedding</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">65536</span>,</span><span class="param">	<span class="n">share_expert_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5120</span>,</span><span class="param">	<span class="n">share_q_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span>,</span><span class="param">	<span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span>,</span><span class="param">	<span class="n">norm_expert_weight</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">moe_layers_enum</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">29</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">34</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">37</span><span class="p">,</span> <span class="mi">38</span><span class="p">,</span> <span class="mi">39</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">46</span><span class="p">,</span> <span class="mi">47</span><span class="p">,</span> <span class="mi">48</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">52</span><span class="p">,</span> <span class="mi">53</span><span class="p">,</span> <span class="mi">54</span><span class="p">,</span> <span class="mi">55</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">57</span><span class="p">,</span> <span class="mi">58</span><span class="p">,</span> <span class="mi">59</span><span class="p">)</span>,</span><span class="param">	<span class="o">**</span><span class="n">kwargs</span></span>)</span>

                <label class="view-source-button" for="Step3TextConfig.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Step3TextConfig.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Step3TextConfig.__init__-41"><a href="#Step3TextConfig.__init__-41"><span class="linenos"> 41</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="Step3TextConfig.__init__-42"><a href="#Step3TextConfig.__init__-42"><span class="linenos"> 42</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-43"><a href="#Step3TextConfig.__init__-43"><span class="linenos"> 43</span></a>        <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">7168</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-44"><a href="#Step3TextConfig.__init__-44"><span class="linenos"> 44</span></a>        <span class="n">intermediate_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">18432</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-45"><a href="#Step3TextConfig.__init__-45"><span class="linenos"> 45</span></a>        <span class="n">num_attention_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-46"><a href="#Step3TextConfig.__init__-46"><span class="linenos"> 46</span></a>        <span class="n">num_attention_groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-47"><a href="#Step3TextConfig.__init__-47"><span class="linenos"> 47</span></a>        <span class="n">num_hidden_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">61</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-48"><a href="#Step3TextConfig.__init__-48"><span class="linenos"> 48</span></a>        <span class="n">max_seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">65536</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-49"><a href="#Step3TextConfig.__init__-49"><span class="linenos"> 49</span></a>        <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128815</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-50"><a href="#Step3TextConfig.__init__-50"><span class="linenos"> 50</span></a>        <span class="n">rms_norm_eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-51"><a href="#Step3TextConfig.__init__-51"><span class="linenos"> 51</span></a>        <span class="n">moe_intermediate_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5120</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-52"><a href="#Step3TextConfig.__init__-52"><span class="linenos"> 52</span></a>        <span class="n">moe_num_experts</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">48</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-53"><a href="#Step3TextConfig.__init__-53"><span class="linenos"> 53</span></a>        <span class="n">moe_top_k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-54"><a href="#Step3TextConfig.__init__-54"><span class="linenos"> 54</span></a>        <span class="n">rope_theta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">500000</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-55"><a href="#Step3TextConfig.__init__-55"><span class="linenos"> 55</span></a>        <span class="n">rope_scaling</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-56"><a href="#Step3TextConfig.__init__-56"><span class="linenos"> 56</span></a>        <span class="n">max_position_embedding</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">65536</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-57"><a href="#Step3TextConfig.__init__-57"><span class="linenos"> 57</span></a>        <span class="n">share_expert_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5120</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-58"><a href="#Step3TextConfig.__init__-58"><span class="linenos"> 58</span></a>        <span class="n">share_q_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-59"><a href="#Step3TextConfig.__init__-59"><span class="linenos"> 59</span></a>        <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-60"><a href="#Step3TextConfig.__init__-60"><span class="linenos"> 60</span></a>        <span class="n">norm_expert_weight</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-61"><a href="#Step3TextConfig.__init__-61"><span class="linenos"> 61</span></a>        <span class="n">moe_layers_enum</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
</span><span id="Step3TextConfig.__init__-62"><a href="#Step3TextConfig.__init__-62"><span class="linenos"> 62</span></a>            <span class="mi">4</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-63"><a href="#Step3TextConfig.__init__-63"><span class="linenos"> 63</span></a>            <span class="mi">5</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-64"><a href="#Step3TextConfig.__init__-64"><span class="linenos"> 64</span></a>            <span class="mi">6</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-65"><a href="#Step3TextConfig.__init__-65"><span class="linenos"> 65</span></a>            <span class="mi">7</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-66"><a href="#Step3TextConfig.__init__-66"><span class="linenos"> 66</span></a>            <span class="mi">8</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-67"><a href="#Step3TextConfig.__init__-67"><span class="linenos"> 67</span></a>            <span class="mi">9</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-68"><a href="#Step3TextConfig.__init__-68"><span class="linenos"> 68</span></a>            <span class="mi">10</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-69"><a href="#Step3TextConfig.__init__-69"><span class="linenos"> 69</span></a>            <span class="mi">11</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-70"><a href="#Step3TextConfig.__init__-70"><span class="linenos"> 70</span></a>            <span class="mi">12</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-71"><a href="#Step3TextConfig.__init__-71"><span class="linenos"> 71</span></a>            <span class="mi">13</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-72"><a href="#Step3TextConfig.__init__-72"><span class="linenos"> 72</span></a>            <span class="mi">14</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-73"><a href="#Step3TextConfig.__init__-73"><span class="linenos"> 73</span></a>            <span class="mi">15</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-74"><a href="#Step3TextConfig.__init__-74"><span class="linenos"> 74</span></a>            <span class="mi">16</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-75"><a href="#Step3TextConfig.__init__-75"><span class="linenos"> 75</span></a>            <span class="mi">17</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-76"><a href="#Step3TextConfig.__init__-76"><span class="linenos"> 76</span></a>            <span class="mi">18</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-77"><a href="#Step3TextConfig.__init__-77"><span class="linenos"> 77</span></a>            <span class="mi">19</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-78"><a href="#Step3TextConfig.__init__-78"><span class="linenos"> 78</span></a>            <span class="mi">20</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-79"><a href="#Step3TextConfig.__init__-79"><span class="linenos"> 79</span></a>            <span class="mi">21</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-80"><a href="#Step3TextConfig.__init__-80"><span class="linenos"> 80</span></a>            <span class="mi">22</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-81"><a href="#Step3TextConfig.__init__-81"><span class="linenos"> 81</span></a>            <span class="mi">23</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-82"><a href="#Step3TextConfig.__init__-82"><span class="linenos"> 82</span></a>            <span class="mi">24</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-83"><a href="#Step3TextConfig.__init__-83"><span class="linenos"> 83</span></a>            <span class="mi">25</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-84"><a href="#Step3TextConfig.__init__-84"><span class="linenos"> 84</span></a>            <span class="mi">26</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-85"><a href="#Step3TextConfig.__init__-85"><span class="linenos"> 85</span></a>            <span class="mi">27</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-86"><a href="#Step3TextConfig.__init__-86"><span class="linenos"> 86</span></a>            <span class="mi">28</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-87"><a href="#Step3TextConfig.__init__-87"><span class="linenos"> 87</span></a>            <span class="mi">29</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-88"><a href="#Step3TextConfig.__init__-88"><span class="linenos"> 88</span></a>            <span class="mi">30</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-89"><a href="#Step3TextConfig.__init__-89"><span class="linenos"> 89</span></a>            <span class="mi">31</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-90"><a href="#Step3TextConfig.__init__-90"><span class="linenos"> 90</span></a>            <span class="mi">32</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-91"><a href="#Step3TextConfig.__init__-91"><span class="linenos"> 91</span></a>            <span class="mi">33</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-92"><a href="#Step3TextConfig.__init__-92"><span class="linenos"> 92</span></a>            <span class="mi">34</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-93"><a href="#Step3TextConfig.__init__-93"><span class="linenos"> 93</span></a>            <span class="mi">35</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-94"><a href="#Step3TextConfig.__init__-94"><span class="linenos"> 94</span></a>            <span class="mi">36</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-95"><a href="#Step3TextConfig.__init__-95"><span class="linenos"> 95</span></a>            <span class="mi">37</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-96"><a href="#Step3TextConfig.__init__-96"><span class="linenos"> 96</span></a>            <span class="mi">38</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-97"><a href="#Step3TextConfig.__init__-97"><span class="linenos"> 97</span></a>            <span class="mi">39</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-98"><a href="#Step3TextConfig.__init__-98"><span class="linenos"> 98</span></a>            <span class="mi">40</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-99"><a href="#Step3TextConfig.__init__-99"><span class="linenos"> 99</span></a>            <span class="mi">41</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-100"><a href="#Step3TextConfig.__init__-100"><span class="linenos">100</span></a>            <span class="mi">42</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-101"><a href="#Step3TextConfig.__init__-101"><span class="linenos">101</span></a>            <span class="mi">43</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-102"><a href="#Step3TextConfig.__init__-102"><span class="linenos">102</span></a>            <span class="mi">44</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-103"><a href="#Step3TextConfig.__init__-103"><span class="linenos">103</span></a>            <span class="mi">45</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-104"><a href="#Step3TextConfig.__init__-104"><span class="linenos">104</span></a>            <span class="mi">46</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-105"><a href="#Step3TextConfig.__init__-105"><span class="linenos">105</span></a>            <span class="mi">47</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-106"><a href="#Step3TextConfig.__init__-106"><span class="linenos">106</span></a>            <span class="mi">48</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-107"><a href="#Step3TextConfig.__init__-107"><span class="linenos">107</span></a>            <span class="mi">49</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-108"><a href="#Step3TextConfig.__init__-108"><span class="linenos">108</span></a>            <span class="mi">50</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-109"><a href="#Step3TextConfig.__init__-109"><span class="linenos">109</span></a>            <span class="mi">51</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-110"><a href="#Step3TextConfig.__init__-110"><span class="linenos">110</span></a>            <span class="mi">52</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-111"><a href="#Step3TextConfig.__init__-111"><span class="linenos">111</span></a>            <span class="mi">53</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-112"><a href="#Step3TextConfig.__init__-112"><span class="linenos">112</span></a>            <span class="mi">54</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-113"><a href="#Step3TextConfig.__init__-113"><span class="linenos">113</span></a>            <span class="mi">55</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-114"><a href="#Step3TextConfig.__init__-114"><span class="linenos">114</span></a>            <span class="mi">56</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-115"><a href="#Step3TextConfig.__init__-115"><span class="linenos">115</span></a>            <span class="mi">57</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-116"><a href="#Step3TextConfig.__init__-116"><span class="linenos">116</span></a>            <span class="mi">58</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-117"><a href="#Step3TextConfig.__init__-117"><span class="linenos">117</span></a>            <span class="mi">59</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-118"><a href="#Step3TextConfig.__init__-118"><span class="linenos">118</span></a>        <span class="p">),</span>
</span><span id="Step3TextConfig.__init__-119"><a href="#Step3TextConfig.__init__-119"><span class="linenos">119</span></a>        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span id="Step3TextConfig.__init__-120"><a href="#Step3TextConfig.__init__-120"><span class="linenos">120</span></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="Step3TextConfig.__init__-121"><a href="#Step3TextConfig.__init__-121"><span class="linenos">121</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span id="Step3TextConfig.__init__-122"><a href="#Step3TextConfig.__init__-122"><span class="linenos">122</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">intermediate_size</span>
</span><span id="Step3TextConfig.__init__-123"><a href="#Step3TextConfig.__init__-123"><span class="linenos">123</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">num_attention_heads</span>
</span><span id="Step3TextConfig.__init__-124"><a href="#Step3TextConfig.__init__-124"><span class="linenos">124</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_groups</span> <span class="o">=</span> <span class="n">num_attention_groups</span>
</span><span id="Step3TextConfig.__init__-125"><a href="#Step3TextConfig.__init__-125"><span class="linenos">125</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="n">num_hidden_layers</span>
</span><span id="Step3TextConfig.__init__-126"><a href="#Step3TextConfig.__init__-126"><span class="linenos">126</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="o">=</span> <span class="n">max_seq_len</span>
</span><span id="Step3TextConfig.__init__-127"><a href="#Step3TextConfig.__init__-127"><span class="linenos">127</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
</span><span id="Step3TextConfig.__init__-128"><a href="#Step3TextConfig.__init__-128"><span class="linenos">128</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">rms_norm_eps</span> <span class="o">=</span> <span class="n">rms_norm_eps</span>
</span><span id="Step3TextConfig.__init__-129"><a href="#Step3TextConfig.__init__-129"><span class="linenos">129</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">moe_intermediate_size</span> <span class="o">=</span> <span class="n">moe_intermediate_size</span>
</span><span id="Step3TextConfig.__init__-130"><a href="#Step3TextConfig.__init__-130"><span class="linenos">130</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">moe_num_experts</span> <span class="o">=</span> <span class="n">moe_num_experts</span>
</span><span id="Step3TextConfig.__init__-131"><a href="#Step3TextConfig.__init__-131"><span class="linenos">131</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">moe_top_k</span> <span class="o">=</span> <span class="n">moe_top_k</span>
</span><span id="Step3TextConfig.__init__-132"><a href="#Step3TextConfig.__init__-132"><span class="linenos">132</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">rope_theta</span> <span class="o">=</span> <span class="n">rope_theta</span>
</span><span id="Step3TextConfig.__init__-133"><a href="#Step3TextConfig.__init__-133"><span class="linenos">133</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">rope_scaling</span> <span class="o">=</span> <span class="n">rope_scaling</span>
</span><span id="Step3TextConfig.__init__-134"><a href="#Step3TextConfig.__init__-134"><span class="linenos">134</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embedding</span> <span class="o">=</span> <span class="n">max_position_embedding</span>
</span><span id="Step3TextConfig.__init__-135"><a href="#Step3TextConfig.__init__-135"><span class="linenos">135</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">share_expert_dim</span> <span class="o">=</span> <span class="n">share_expert_dim</span>
</span><span id="Step3TextConfig.__init__-136"><a href="#Step3TextConfig.__init__-136"><span class="linenos">136</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">share_q_dim</span> <span class="o">=</span> <span class="n">share_q_dim</span>
</span><span id="Step3TextConfig.__init__-137"><a href="#Step3TextConfig.__init__-137"><span class="linenos">137</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">head_dim</span>
</span><span id="Step3TextConfig.__init__-138"><a href="#Step3TextConfig.__init__-138"><span class="linenos">138</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm_expert_weight</span> <span class="o">=</span> <span class="n">norm_expert_weight</span>
</span><span id="Step3TextConfig.__init__-139"><a href="#Step3TextConfig.__init__-139"><span class="linenos">139</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">moe_layers_enum</span> <span class="o">=</span> <span class="n">moe_layers_enum</span>
</span><span id="Step3TextConfig.__init__-140"><a href="#Step3TextConfig.__init__-140"><span class="linenos">140</span></a>
</span><span id="Step3TextConfig.__init__-141"><a href="#Step3TextConfig.__init__-141"><span class="linenos">141</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                            <div id="Step3TextConfig.model_type" class="classattr">
                                <div class="attr variable">
            <span class="name">model_type</span>        =
<span class="default_value">&#39;step3_text&#39;</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.model_type"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.architectures" class="classattr">
                                <div class="attr variable">
            <span class="name">architectures</span>        =
<span class="default_value">[&#39;Step3TextForCausalLM&#39;]</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.architectures"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.hidden_size" class="classattr">
                                <div class="attr variable">
            <span class="name">hidden_size</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.hidden_size"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.intermediate_size" class="classattr">
                                <div class="attr variable">
            <span class="name">intermediate_size</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.intermediate_size"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.num_attention_heads" class="classattr">
                                <div class="attr variable">
            <span class="name">num_attention_heads</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.num_attention_heads"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.num_attention_groups" class="classattr">
                                <div class="attr variable">
            <span class="name">num_attention_groups</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.num_attention_groups"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.num_hidden_layers" class="classattr">
                                <div class="attr variable">
            <span class="name">num_hidden_layers</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.num_hidden_layers"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.max_seq_len" class="classattr">
                                <div class="attr variable">
            <span class="name">max_seq_len</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.max_seq_len"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.vocab_size" class="classattr">
                                <div class="attr variable">
            <span class="name">vocab_size</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.vocab_size"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.rms_norm_eps" class="classattr">
                                <div class="attr variable">
            <span class="name">rms_norm_eps</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.rms_norm_eps"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.moe_intermediate_size" class="classattr">
                                <div class="attr variable">
            <span class="name">moe_intermediate_size</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.moe_intermediate_size"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.moe_num_experts" class="classattr">
                                <div class="attr variable">
            <span class="name">moe_num_experts</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.moe_num_experts"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.moe_top_k" class="classattr">
                                <div class="attr variable">
            <span class="name">moe_top_k</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.moe_top_k"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.rope_theta" class="classattr">
                                <div class="attr variable">
            <span class="name">rope_theta</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.rope_theta"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.rope_scaling" class="classattr">
                                <div class="attr variable">
            <span class="name">rope_scaling</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.rope_scaling"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.max_position_embedding" class="classattr">
                                <div class="attr variable">
            <span class="name">max_position_embedding</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.max_position_embedding"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.share_expert_dim" class="classattr">
                                <div class="attr variable">
            <span class="name">share_expert_dim</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.share_expert_dim"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.share_q_dim" class="classattr">
                                <div class="attr variable">
            <span class="name">share_q_dim</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.share_q_dim"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.head_dim" class="classattr">
                                <div class="attr variable">
            <span class="name">head_dim</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.head_dim"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.norm_expert_weight" class="classattr">
                                <div class="attr variable">
            <span class="name">norm_expert_weight</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.norm_expert_weight"></a>
    
    

                            </div>
                            <div id="Step3TextConfig.moe_layers_enum" class="classattr">
                                <div class="attr variable">
            <span class="name">moe_layers_enum</span>

        
    </div>
    <a class="headerlink" href="#Step3TextConfig.moe_layers_enum"></a>
    
    

                            </div>
                </section>
                <section id="Step3VisionEncoderConfig">
                            <input id="Step3VisionEncoderConfig-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
            
    <span class="def">class</span>
    <span class="name">Step3VisionEncoderConfig</span><wbr>(<span class="base">transformers.configuration_utils.PretrainedConfig</span>):

                <label class="view-source-button" for="Step3VisionEncoderConfig-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Step3VisionEncoderConfig"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Step3VisionEncoderConfig-7"><a href="#Step3VisionEncoderConfig-7"><span class="linenos"> 7</span></a><span class="k">class</span><span class="w"> </span><span class="nc">Step3VisionEncoderConfig</span><span class="p">(</span><span class="n">PretrainedConfig</span><span class="p">):</span>
</span><span id="Step3VisionEncoderConfig-8"><a href="#Step3VisionEncoderConfig-8"><span class="linenos"> 8</span></a>    <span class="n">model_type</span> <span class="o">=</span> <span class="s2">&quot;step3_vision_encoder&quot;</span>
</span><span id="Step3VisionEncoderConfig-9"><a href="#Step3VisionEncoderConfig-9"><span class="linenos"> 9</span></a>
</span><span id="Step3VisionEncoderConfig-10"><a href="#Step3VisionEncoderConfig-10"><span class="linenos">10</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="Step3VisionEncoderConfig-11"><a href="#Step3VisionEncoderConfig-11"><span class="linenos">11</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig-12"><a href="#Step3VisionEncoderConfig-12"><span class="linenos">12</span></a>        <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1792</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig-13"><a href="#Step3VisionEncoderConfig-13"><span class="linenos">13</span></a>        <span class="n">intermediate_size</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig-14"><a href="#Step3VisionEncoderConfig-14"><span class="linenos">14</span></a>        <span class="n">output_hidden_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig-15"><a href="#Step3VisionEncoderConfig-15"><span class="linenos">15</span></a>        <span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">63</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig-16"><a href="#Step3VisionEncoderConfig-16"><span class="linenos">16</span></a>        <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig-17"><a href="#Step3VisionEncoderConfig-17"><span class="linenos">17</span></a>        <span class="n">num_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig-18"><a href="#Step3VisionEncoderConfig-18"><span class="linenos">18</span></a>        <span class="n">image_size</span><span class="o">=</span><span class="mi">728</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig-19"><a href="#Step3VisionEncoderConfig-19"><span class="linenos">19</span></a>        <span class="n">patch_size</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig-20"><a href="#Step3VisionEncoderConfig-20"><span class="linenos">20</span></a>        <span class="n">hidden_act</span><span class="o">=</span><span class="s2">&quot;quick_gelu&quot;</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig-21"><a href="#Step3VisionEncoderConfig-21"><span class="linenos">21</span></a>        <span class="n">layer_norm_eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig-22"><a href="#Step3VisionEncoderConfig-22"><span class="linenos">22</span></a>        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig-23"><a href="#Step3VisionEncoderConfig-23"><span class="linenos">23</span></a>    <span class="p">):</span>
</span><span id="Step3VisionEncoderConfig-24"><a href="#Step3VisionEncoderConfig-24"><span class="linenos">24</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span id="Step3VisionEncoderConfig-25"><a href="#Step3VisionEncoderConfig-25"><span class="linenos">25</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">intermediate_size</span>
</span><span id="Step3VisionEncoderConfig-26"><a href="#Step3VisionEncoderConfig-26"><span class="linenos">26</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_hidden_size</span> <span class="o">=</span> <span class="n">output_hidden_size</span>
</span><span id="Step3VisionEncoderConfig-27"><a href="#Step3VisionEncoderConfig-27"><span class="linenos">27</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="n">num_hidden_layers</span>
</span><span id="Step3VisionEncoderConfig-28"><a href="#Step3VisionEncoderConfig-28"><span class="linenos">28</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">num_attention_heads</span>
</span><span id="Step3VisionEncoderConfig-29"><a href="#Step3VisionEncoderConfig-29"><span class="linenos">29</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_channels</span> <span class="o">=</span> <span class="n">num_channels</span>
</span><span id="Step3VisionEncoderConfig-30"><a href="#Step3VisionEncoderConfig-30"><span class="linenos">30</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span>
</span><span id="Step3VisionEncoderConfig-31"><a href="#Step3VisionEncoderConfig-31"><span class="linenos">31</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">image_size</span> <span class="o">=</span> <span class="n">image_size</span>
</span><span id="Step3VisionEncoderConfig-32"><a href="#Step3VisionEncoderConfig-32"><span class="linenos">32</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_eps</span> <span class="o">=</span> <span class="n">layer_norm_eps</span>
</span><span id="Step3VisionEncoderConfig-33"><a href="#Step3VisionEncoderConfig-33"><span class="linenos">33</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_act</span> <span class="o">=</span> <span class="n">hidden_act</span>
</span><span id="Step3VisionEncoderConfig-34"><a href="#Step3VisionEncoderConfig-34"><span class="linenos">34</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as
methods for loading/downloading/saving configurations.</p>

<p><Tip></p>

<p>A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to
initialize a model does <strong>not</strong> load the model weights. It only affects the model's configuration.</p>

<p></Tip></p>

<p>Class attributes (overridden by derived classes):</p>

<ul>
<li><strong>model_type</strong> (<code>str</code>) -- An identifier for the model type, serialized into the JSON file, and used to recreate
the correct object in [<code>~transformers.AutoConfig</code>].</li>
<li><strong>has_no_defaults_at_init</strong> (<code>bool</code>) -- Whether the config class can be initialized without providing input arguments.
Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,
(but not necessarily) such as [<code>~transformers.EncoderDecoderConfig</code>] or [<code>~RagConfig</code>]. They have to be initialized from
two or more configs of type [<code>~transformers.PretrainedConfig</code>].</li>
<li><strong>keys_to_ignore_at_inference</strong> (<code>list[str]</code>) -- A list of keys to ignore by default when looking at dictionary
outputs of the model during inference.</li>
<li><strong>attribute_map</strong> (<code>dict[str, str]</code>) -- A dict that maps model specific attribute names to the standardized
naming of attributes.</li>
<li><strong>base_model_tp_plan</strong> (<code>dict[str, Any]</code>) -- A dict that maps sub-modules FQNs of a base model to a tensor
parallel plan applied to the sub-module when <code>model.tensor_parallel</code> is called.</li>
<li><strong>base_model_pp_plan</strong> (<code>dict[str, tuple[list[str]]]</code>) -- A dict that maps child-modules of a base model to a
pipeline parallel plan that enables users to place the child-module on the appropriate device.</li>
</ul>

<p>Common attributes (present in all subclasses):</p>

<ul>
<li><strong>vocab_size</strong> (<code>int</code>) -- The number of tokens in the vocabulary, which is also the first dimension of the
embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).</li>
<li><strong>hidden_size</strong> (<code>int</code>) -- The hidden size of the model.</li>
<li><strong>num_attention_heads</strong> (<code>int</code>) -- The number of attention heads used in the multi-head attention layers of the
model.</li>
<li><strong>num_hidden_layers</strong> (<code>int</code>) -- The number of blocks in the model.</li>
</ul>

<p><Tip warning={true}&gt;</p>

<p>Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading
some of them will still be possible, but attempting to overwrite them will throw an exception -- you should set
them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more
information about the individual parameters.</p>

<p></Tip></p>

<p>Arg:
    name_or_path (<code>str</code>, <em>optional</em>, defaults to <code>""</code>):
        Store the string that was passed to [<code>PreTrainedModel.from_pretrained</code>] or
        [<code>TFPreTrainedModel.from_pretrained</code>] as <code>pretrained_model_name_or_path</code> if the configuration was created
        with such a method.
    output_hidden_states (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether or not the model should return all hidden-states.
    output_attentions (<code>bool</code>, *optional*, defaults to <code>False</code>):
        Whether or not the model should returns all attentions.
    return_dict (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
        Whether or not the model should return a [<code>~transformers.utils.ModelOutput</code>] instead of a plain tuple.
    is_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether the model is used as an encoder/decoder or not.
    is_decoder (<code>bool</code>, *optional*, defaults to <code>False</code>):
        Whether to only use the decoder in an encoder-decoder architecture, otherwise it has no effect on
        decoder-only or encoder-only architectures.
    cross_attention_hidden_size (<code>bool</code>, <em>optional</em>):
        The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder
        setting and the cross-attention hidden dimension differs from <code>self.config.hidden_size</code>.
    add_cross_attention (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether cross-attention layers should be added to the model. Note, this option is only relevant for models
        that can be used as decoder models within the [<code>EncoderDecoderModel</code>] class, which consists of all models
        in <code>AUTO_MODELS_FOR_CAUSAL_LM</code>.
    tie_encoder_decoder (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
        Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder
        and decoder model to have the exact same parameter names.
    prune_heads (<code>dict[int, list[int]]</code>, <em>optional</em>, defaults to <code>{}</code>):
        Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of
        heads to prune in said layer.</p>

<pre><code>    For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.
chunk_size_feed_forward (`int`, *optional*, defaults to `0`):
    The chunk size of all feed forward layers in the residual attention blocks. A chunk size of `0` means that
    the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes `n` &lt;
    sequence_length embeddings at a time. For more information on feed forward chunking, see [How does Feed
    Forward Chunking work?](../glossary.html#feed-forward-chunking).

&gt; Parameters for fine-tuning tasks

architectures (`list[str]`, *optional*):
    Model architectures that can be used with the model pretrained weights.
finetuning_task (`str`, *optional*):
    Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow
    or PyTorch) checkpoint.
id2label (`dict[int, str]`, *optional*):
    A map from index (for instance prediction index, or target index) to label.
label2id (`dict[str, int]`, *optional*):
    A map from label to index for the model.
num_labels (`int`, *optional*):
    Number of labels to use in the last layer added to the model, typically for a classification task.
task_specific_params (`dict[str, Any]`, *optional*):
    Additional keyword arguments to store for the current task.
problem_type (`str`, *optional*):
    Problem type for `XxxForSequenceClassification` models. Can be one of `"regression"`,
    `"single_label_classification"` or `"multi_label_classification"`.

&gt; Parameters linked to the tokenizer

tokenizer_class (`str`, *optional*):
    The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the
    model by default).
prefix (`str`, *optional*):
    A specific prompt that should be added at the beginning of each text before calling the model.
bos_token_id (`int`, *optional*):
    The id of the _beginning-of-stream_ token.
pad_token_id (`int`, *optional*):
    The id of the _padding_ token.
eos_token_id (`int`, *optional*):
    The id of the _end-of-stream_ token.
decoder_start_token_id (`int`, *optional*):
    If an encoder-decoder model starts decoding with a different token than _bos_, the id of that token.
sep_token_id (`int`, *optional*):
    The id of the _separation_ token.

&gt; PyTorch specific parameters

torchscript (`bool`, *optional*, defaults to `False`):
    Whether or not the model should be used with Torchscript.
tie_word_embeddings (`bool`, *optional*, defaults to `True`):
    Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the
    model has a output word embedding layer.
torch_dtype (`str`, *optional*):
    The `dtype` of the weights. This attribute can be used to initialize the model to a non-default `dtype`
    (which is normally `float32`) and thus allow for optimal storage allocation. For example, if the saved
    model is `float16`, ideally we want to load it back using the minimal amount of memory needed to load
    `float16` weights. Since the config object is stored in plain text, this attribute contains just the
    floating type string without the `torch.` prefix. For example, for `torch.float16` ``torch_dtype` is the
    `"float16"` string.

    This attribute is currently not being used during model loading time, but this may change in the future
    versions. But we can already start preparing for the future by saving the dtype with save_pretrained.
</code></pre>
</div>


                            <div id="Step3VisionEncoderConfig.__init__" class="classattr">
                                        <input id="Step3VisionEncoderConfig.__init__-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="name">Step3VisionEncoderConfig</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">hidden_size</span><span class="o">=</span><span class="mi">1792</span>,</span><span class="param">	<span class="n">intermediate_size</span><span class="o">=</span><span class="mi">3072</span>,</span><span class="param">	<span class="n">output_hidden_size</span><span class="o">=</span><span class="mi">4096</span>,</span><span class="param">	<span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">63</span>,</span><span class="param">	<span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">16</span>,</span><span class="param">	<span class="n">num_channels</span><span class="o">=</span><span class="mi">3</span>,</span><span class="param">	<span class="n">image_size</span><span class="o">=</span><span class="mi">728</span>,</span><span class="param">	<span class="n">patch_size</span><span class="o">=</span><span class="mi">14</span>,</span><span class="param">	<span class="n">hidden_act</span><span class="o">=</span><span class="s1">&#39;quick_gelu&#39;</span>,</span><span class="param">	<span class="n">layer_norm_eps</span><span class="o">=</span><span class="mf">1e-05</span>,</span><span class="param">	<span class="o">**</span><span class="n">kwargs</span></span>)</span>

                <label class="view-source-button" for="Step3VisionEncoderConfig.__init__-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#Step3VisionEncoderConfig.__init__"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="Step3VisionEncoderConfig.__init__-10"><a href="#Step3VisionEncoderConfig.__init__-10"><span class="linenos">10</span></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
</span><span id="Step3VisionEncoderConfig.__init__-11"><a href="#Step3VisionEncoderConfig.__init__-11"><span class="linenos">11</span></a>        <span class="bp">self</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig.__init__-12"><a href="#Step3VisionEncoderConfig.__init__-12"><span class="linenos">12</span></a>        <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1792</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig.__init__-13"><a href="#Step3VisionEncoderConfig.__init__-13"><span class="linenos">13</span></a>        <span class="n">intermediate_size</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig.__init__-14"><a href="#Step3VisionEncoderConfig.__init__-14"><span class="linenos">14</span></a>        <span class="n">output_hidden_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig.__init__-15"><a href="#Step3VisionEncoderConfig.__init__-15"><span class="linenos">15</span></a>        <span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">63</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig.__init__-16"><a href="#Step3VisionEncoderConfig.__init__-16"><span class="linenos">16</span></a>        <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig.__init__-17"><a href="#Step3VisionEncoderConfig.__init__-17"><span class="linenos">17</span></a>        <span class="n">num_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig.__init__-18"><a href="#Step3VisionEncoderConfig.__init__-18"><span class="linenos">18</span></a>        <span class="n">image_size</span><span class="o">=</span><span class="mi">728</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig.__init__-19"><a href="#Step3VisionEncoderConfig.__init__-19"><span class="linenos">19</span></a>        <span class="n">patch_size</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig.__init__-20"><a href="#Step3VisionEncoderConfig.__init__-20"><span class="linenos">20</span></a>        <span class="n">hidden_act</span><span class="o">=</span><span class="s2">&quot;quick_gelu&quot;</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig.__init__-21"><a href="#Step3VisionEncoderConfig.__init__-21"><span class="linenos">21</span></a>        <span class="n">layer_norm_eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig.__init__-22"><a href="#Step3VisionEncoderConfig.__init__-22"><span class="linenos">22</span></a>        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
</span><span id="Step3VisionEncoderConfig.__init__-23"><a href="#Step3VisionEncoderConfig.__init__-23"><span class="linenos">23</span></a>    <span class="p">):</span>
</span><span id="Step3VisionEncoderConfig.__init__-24"><a href="#Step3VisionEncoderConfig.__init__-24"><span class="linenos">24</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
</span><span id="Step3VisionEncoderConfig.__init__-25"><a href="#Step3VisionEncoderConfig.__init__-25"><span class="linenos">25</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">intermediate_size</span>
</span><span id="Step3VisionEncoderConfig.__init__-26"><a href="#Step3VisionEncoderConfig.__init__-26"><span class="linenos">26</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_hidden_size</span> <span class="o">=</span> <span class="n">output_hidden_size</span>
</span><span id="Step3VisionEncoderConfig.__init__-27"><a href="#Step3VisionEncoderConfig.__init__-27"><span class="linenos">27</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="n">num_hidden_layers</span>
</span><span id="Step3VisionEncoderConfig.__init__-28"><a href="#Step3VisionEncoderConfig.__init__-28"><span class="linenos">28</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">num_attention_heads</span>
</span><span id="Step3VisionEncoderConfig.__init__-29"><a href="#Step3VisionEncoderConfig.__init__-29"><span class="linenos">29</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_channels</span> <span class="o">=</span> <span class="n">num_channels</span>
</span><span id="Step3VisionEncoderConfig.__init__-30"><a href="#Step3VisionEncoderConfig.__init__-30"><span class="linenos">30</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span>
</span><span id="Step3VisionEncoderConfig.__init__-31"><a href="#Step3VisionEncoderConfig.__init__-31"><span class="linenos">31</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">image_size</span> <span class="o">=</span> <span class="n">image_size</span>
</span><span id="Step3VisionEncoderConfig.__init__-32"><a href="#Step3VisionEncoderConfig.__init__-32"><span class="linenos">32</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm_eps</span> <span class="o">=</span> <span class="n">layer_norm_eps</span>
</span><span id="Step3VisionEncoderConfig.__init__-33"><a href="#Step3VisionEncoderConfig.__init__-33"><span class="linenos">33</span></a>        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_act</span> <span class="o">=</span> <span class="n">hidden_act</span>
</span><span id="Step3VisionEncoderConfig.__init__-34"><a href="#Step3VisionEncoderConfig.__init__-34"><span class="linenos">34</span></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span></pre></div>


    

                            </div>
                            <div id="Step3VisionEncoderConfig.model_type" class="classattr">
                                <div class="attr variable">
            <span class="name">model_type</span>        =
<span class="default_value">&#39;step3_vision_encoder&#39;</span>

        
    </div>
    <a class="headerlink" href="#Step3VisionEncoderConfig.model_type"></a>
    
    

                            </div>
                            <div id="Step3VisionEncoderConfig.hidden_size" class="classattr">
                                <div class="attr variable">
            <span class="name">hidden_size</span>

        
    </div>
    <a class="headerlink" href="#Step3VisionEncoderConfig.hidden_size"></a>
    
    

                            </div>
                            <div id="Step3VisionEncoderConfig.intermediate_size" class="classattr">
                                <div class="attr variable">
            <span class="name">intermediate_size</span>

        
    </div>
    <a class="headerlink" href="#Step3VisionEncoderConfig.intermediate_size"></a>
    
    

                            </div>
                            <div id="Step3VisionEncoderConfig.output_hidden_size" class="classattr">
                                <div class="attr variable">
            <span class="name">output_hidden_size</span>

        
    </div>
    <a class="headerlink" href="#Step3VisionEncoderConfig.output_hidden_size"></a>
    
    

                            </div>
                            <div id="Step3VisionEncoderConfig.num_hidden_layers" class="classattr">
                                <div class="attr variable">
            <span class="name">num_hidden_layers</span>

        
    </div>
    <a class="headerlink" href="#Step3VisionEncoderConfig.num_hidden_layers"></a>
    
    

                            </div>
                            <div id="Step3VisionEncoderConfig.num_attention_heads" class="classattr">
                                <div class="attr variable">
            <span class="name">num_attention_heads</span>

        
    </div>
    <a class="headerlink" href="#Step3VisionEncoderConfig.num_attention_heads"></a>
    
    

                            </div>
                            <div id="Step3VisionEncoderConfig.num_channels" class="classattr">
                                <div class="attr variable">
            <span class="name">num_channels</span>

        
    </div>
    <a class="headerlink" href="#Step3VisionEncoderConfig.num_channels"></a>
    
    

                            </div>
                            <div id="Step3VisionEncoderConfig.patch_size" class="classattr">
                                <div class="attr variable">
            <span class="name">patch_size</span>

        
    </div>
    <a class="headerlink" href="#Step3VisionEncoderConfig.patch_size"></a>
    
    

                            </div>
                            <div id="Step3VisionEncoderConfig.image_size" class="classattr">
                                <div class="attr variable">
            <span class="name">image_size</span>

        
    </div>
    <a class="headerlink" href="#Step3VisionEncoderConfig.image_size"></a>
    
    

                            </div>
                            <div id="Step3VisionEncoderConfig.layer_norm_eps" class="classattr">
                                <div class="attr variable">
            <span class="name">layer_norm_eps</span>

        
    </div>
    <a class="headerlink" href="#Step3VisionEncoderConfig.layer_norm_eps"></a>
    
    

                            </div>
                            <div id="Step3VisionEncoderConfig.hidden_act" class="classattr">
                                <div class="attr variable">
            <span class="name">hidden_act</span>

        
    </div>
    <a class="headerlink" href="#Step3VisionEncoderConfig.hidden_act"></a>
    
    

                            </div>
                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.kind) {
                    case "function":
                        if (doc.fullname.endsWith(".__init__")) {
                            heading = `<span class="name">${doc.fullname.replace(/\.__init__$/, "")}</span>${doc.signature}`;
                        } else {
                            heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span>${doc.signature}`;
                        }
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value"> = ${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.kind}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>